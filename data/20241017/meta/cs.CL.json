[
    {
        "title": "A Hitchhiker's Guide to Scaling Law Estimation",
        "authors": "Leshem ChoshenYang ZhangJacob Andreas",
        "links": "http://arxiv.org/abs/2410.11840v1",
        "entry_id": "http://arxiv.org/abs/2410.11840v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11840v1",
        "summary": "Scaling laws predict the loss of a target machine learning model by\nextrapolating from easier-to-train models with fewer parameters or smaller\ntraining sets. This provides an efficient way for practitioners and researchers\nalike to compare pretraining decisions involving optimizers, datasets, and\nmodel architectures. Despite the widespread use of scaling laws to model the\ndynamics of language model training, there has been little work on\nunderstanding how to best estimate and interpret them. We collect (and release)\na large-scale dataset containing losses and downstream evaluations for 485\npreviously published pretrained models. We use these to estimate more than 1000\nscaling laws, then derive a set of best practices for estimating scaling laws\nin new model families. We find that fitting scaling laws to intermediate\ncheckpoints of training runs (and not just their final losses) substantially\nimproves accuracy, and that -- all else equal -- estimates of performance are\ngenerally most accurate when derived from other models of similar sizes.\nHowever, because there is a significant degree of variability across model\nseeds, training multiple small models is sometimes more useful than training a\nsingle large one. Moreover, while different model families differ scaling\nbehavior, they are often similar enough that a target model's behavior can be\npredicted from a single model with the same architecture, along with scaling\nparameter estimates derived from other model families.",
        "updated": "2024-10-15 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11840v1"
    },
    {
        "title": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models",
        "authors": "Han HanTong ZhuXiang ZhangMengsong WuHao XiongWenliang Chen",
        "links": "http://arxiv.org/abs/2410.11805v1",
        "entry_id": "http://arxiv.org/abs/2410.11805v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11805v1",
        "summary": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack of relevant data instances. To address this problem, we\nintroduce NesTools to bridge the current gap in comprehensive nested tool\nlearning evaluations. NesTools comprises a novel automatic data generation\nmethod to construct large-scale nested tool calls with different nesting\nstructures. With manual review and refinement, the dataset is in high quality\nand closely aligned with real-world scenarios. Therefore, NesTools can serve as\na new benchmark to evaluate the nested tool learning abilities of LLMs. We\nconduct extensive experiments on 22 LLMs, and provide in-depth analyses with\nNesTools, which shows that current LLMs still suffer from the complex nested\ntool learning task.",
        "updated": "2024-10-15 17:33:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11805v1"
    },
    {
        "title": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability",
        "authors": "Tsz Ting ChungLeyang CuiLemao LiuXinting HuangShuming ShiDit-Yan Yeung",
        "links": "http://arxiv.org/abs/2410.11786v1",
        "entry_id": "http://arxiv.org/abs/2410.11786v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11786v1",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in a\nwide range of natural language processing tasks when leveraging in-context\nlearning. To mitigate the additional computational and financial costs\nassociated with in-context learning, several prompt compression methods have\nbeen proposed to compress the in-context learning prompts. Despite their\nsuccess, these methods face challenges with transferability due to\nmodel-specific compression, or rely on external training data, such as GPT-4.\nIn this paper, we investigate the ability of LLMs to develop a unified\ncompression method that discretizes uninformative tokens, utilizing a\nself-supervised pre-training technique. By introducing a small number of\nparameters during the continual pre-training, the proposed Selection-p produces\na probability for each input token, indicating whether to preserve or discard\nit. Experiments show Selection-p achieves state-of-the-art performance across\nnumerous classification tasks, achieving compression rates of up to 10 times\nwhile experiencing only a marginal 0.8% decrease in performance. Moreover, it\nexhibits superior transferability to different models compared to prior work.\nAdditionally, we further analyze how Selection-p helps maintain performance on\nin-context learning with long contexts.",
        "updated": "2024-10-15 17:05:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11786v1"
    },
    {
        "title": "MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation",
        "authors": "Chenxi WangXiang ChenNingyu ZhangBozhong TianHaoming XuShumin DengHuajun Chen",
        "links": "http://arxiv.org/abs/2410.11779v1",
        "entry_id": "http://arxiv.org/abs/2410.11779v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11779v1",
        "summary": "Multimodal Large Language Models (MLLMs) frequently exhibit hallucination\nphenomena, but the underlying reasons remain poorly understood. In this paper,\nwe present an empirical analysis and find that, although MLLMs incorrectly\ngenerate the objects in the final output, they are actually able to recognize\nvisual objects in the preceding layers. We speculate that this may be due to\nthe strong knowledge priors of the language model suppressing the visual\ninformation, leading to hallucinations. Motivated by this, we propose a novel\ndynamic correction decoding method for MLLMs (DeCo), which adaptively selects\nthe appropriate preceding layers and proportionally integrates knowledge into\nthe final layer to adjust the output logits. Note that DeCo is model agnostic\nand can be seamlessly incorporated with various classic decoding strategies and\napplied to different MLLMs. We evaluate DeCo on widely-used benchmarks,\ndemonstrating that it can reduce hallucination rates by a large margin compared\nto baselines, highlighting its potential to mitigate hallucinations. Code is\navailable at https://github.com/zjunlp/DeCo.",
        "updated": "2024-10-15 16:57:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11779v1"
    },
    {
        "title": "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models",
        "authors": "Kai YaoPenlei GaoLichun LiYuan ZhaoXiaofeng WangWei WangJianke Zhu",
        "links": "http://arxiv.org/abs/2410.11772v1",
        "entry_id": "http://arxiv.org/abs/2410.11772v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11772v1",
        "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant\npopularity for adapting pre-trained Large Language Models (LLMs) to downstream\ntasks, primarily due to their potential to significantly reduce memory and\ncomputational overheads. However, a common limitation in most PEFT approaches\nis their application of a uniform architectural design across all layers. This\nuniformity involves identical trainable modules and ignores the varying\nimportance of each layer, leading to sub-optimal fine-tuning results. To\novercome the above limitation and obtain better performance, we develop a novel\napproach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent\nsparsity and select the most important subset of full layers with effective\nlayer-wise importance scoring. The proposed IST is a versatile and\nplug-and-play technique compatible with various PEFT methods that operate on a\nper-layer basis. By leveraging the estimated importance scores, IST dynamically\nupdates these selected layers in PEFT modules, leading to reduced memory\ndemands. We further provide theoretical proof of convergence and empirical\nevidence of superior performance to demonstrate the advantages of IST over\nuniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,\nand downstream tasks substantiate the effectiveness of our proposed method,\nshowcasing IST's capacity to enhance existing layer-based PEFT methods. Our\ncode is available at https://github.com/Kaiseem/IST.",
        "updated": "2024-10-15 16:53:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11772v1"
    }
]