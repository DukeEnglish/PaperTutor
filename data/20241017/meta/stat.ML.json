[
    {
        "title": "Mitigating Suboptimality of Deterministic Policy Gradients in Complex Q-functions",
        "authors": "Ayush JainNorio KosakaXinhu LiKyung-Min KimErdem BıyıkJoseph J. Lim",
        "links": "http://arxiv.org/abs/2410.11833v1",
        "entry_id": "http://arxiv.org/abs/2410.11833v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11833v1",
        "summary": "In reinforcement learning, off-policy actor-critic approaches like DDPG and\nTD3 are based on the deterministic policy gradient. Herein, the Q-function is\ntrained from off-policy environment data and the actor (policy) is trained to\nmaximize the Q-function via gradient ascent. We observe that in complex tasks\nlike dexterous manipulation and restricted locomotion, the Q-value is a complex\nfunction of action, having several local optima or discontinuities. This poses\na challenge for gradient ascent to traverse and makes the actor prone to get\nstuck at local optima. To address this, we introduce a new actor architecture\nthat combines two simple insights: (i) use multiple actors and evaluate the\nQ-value maximizing action, and (ii) learn surrogates to the Q-function that are\nsimpler to optimize with gradient-based methods. We evaluate tasks such as\nrestricted locomotion, dexterous manipulation, and large discrete-action space\nrecommender systems and show that our actor finds optimal actions more\nfrequently and outperforms alternate actor architectures.",
        "updated": "2024-10-15 17:58:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11833v1"
    },
    {
        "title": "Bayesian Experimental Design via Contrastive Diffusions",
        "authors": "Jacopo IolloChristophe HeinkeléPierre AlliezFlorence Forbes",
        "links": "http://arxiv.org/abs/2410.11826v1",
        "entry_id": "http://arxiv.org/abs/2410.11826v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11826v1",
        "summary": "Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the\ncost of running a sequence of experiments. When based on the Expected\nInformation Gain (EIG), design optimization corresponds to the maximization of\nsome intractable expected {\\it contrast} between prior and posterior\ndistributions. Scaling this maximization to high dimensional and complex\nsettings has been an issue due to BOED inherent computational complexity. In\nthis work, we introduce an {\\it expected posterior} distribution with\ncost-effective sampling properties and provide a tractable access to the EIG\ncontrast maximization via a new EIG gradient expression. Diffusion-based\nsamplers are used to compute the dynamics of the expected posterior and ideas\nfrom bi-level optimization are leveraged to derive an efficient joint\nsampling-optimization loop, without resorting to lower bound approximations of\nthe EIG. The resulting efficiency gain allows to extend BOED to the well-tested\ngenerative capabilities of diffusion models. By incorporating generative models\ninto the BOED framework, we expand its scope and its use in scenarios that were\npreviously impractical. Numerical experiments and comparison with\nstate-of-the-art methods show the potential of the approach.",
        "updated": "2024-10-15 17:53:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11826v1"
    },
    {
        "title": "Solving The Dynamic Volatility Fitting Problem: A Deep Reinforcement Learning Approach",
        "authors": "Emmanuel GnabeyeuOmar KarkarImad Idboufous",
        "links": "http://arxiv.org/abs/2410.11789v1",
        "entry_id": "http://arxiv.org/abs/2410.11789v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11789v1",
        "summary": "The volatility fitting is one of the core problems in the equity derivatives\nbusiness. Through a set of deterministic rules, the degrees of freedom in the\nimplied volatility surface encoding (parametrization, density, diffusion) are\ndefined. Whilst very effective, this approach widespread in the industry is not\nnatively tailored to learn from shifts in market regimes and discover\nunsuspected optimal behaviors. In this paper, we change the classical paradigm\nand apply the latest advances in Deep Reinforcement Learning(DRL) to solve the\nfitting problem. In particular, we show that variants of Deep Deterministic\nPolicy Gradient (DDPG) and Soft Actor Critic (SAC) can achieve at least as good\nas standard fitting algorithms. Furthermore, we explain why the reinforcement\nlearning framework is appropriate to handle complex objective functions and is\nnatively adapted for online learning.",
        "updated": "2024-10-15 17:10:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11789v1"
    },
    {
        "title": "On the Training Convergence of Transformers for In-Context Classification",
        "authors": "Wei ShenRuida ZhouJing YangCong Shen",
        "links": "http://arxiv.org/abs/2410.11778v1",
        "entry_id": "http://arxiv.org/abs/2410.11778v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11778v1",
        "summary": "While transformers have demonstrated impressive capacities for in-context\nlearning (ICL) in practice, theoretical understanding of the underlying\nmechanism enabling transformers to perform ICL is still in its infant stage.\nThis work aims to theoretically study the training dynamics of transformers for\nin-context classification tasks. We demonstrate that, for in-context\nclassification of Gaussian mixtures under certain assumptions, a single-layer\ntransformer trained via gradient descent converges to a globally optimal model\nat a linear rate. We further quantify the impact of the training and testing\nprompt lengths on the ICL inference error of the trained transformer. We show\nthat when the lengths of training and testing prompts are sufficiently large,\nthe prediction of the trained transformer approaches the Bayes-optimal\nclassifier. Experimental results corroborate the theoretical findings.",
        "updated": "2024-10-15 16:57:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11778v1"
    },
    {
        "title": "l_inf-approximation of localized distributions",
        "authors": "Tiangang CuiShuigen LiuXin Tong",
        "links": "http://arxiv.org/abs/2410.11771v1",
        "entry_id": "http://arxiv.org/abs/2410.11771v1",
        "pdf_url": "http://arxiv.org/pdf/2410.11771v1",
        "summary": "Distributions in spatial model often exhibit localized features. Intuitively,\nthis locality implies a low intrinsic dimensionality, which can be exploited\nfor efficient approximation and computation of complex distributions. However,\nexisting approximation theory mainly considers the joint distributions, which\ndoes not guarantee that the marginal errors are small. In this work, we\nestablish a dimension independent error bound for the marginals of approximate\ndistributions. This $\\ell_\\infty$-approximation error is obtained using Stein's\nmethod, and we propose a $\\delta$-locality condition that quantifies the degree\nof localization in a distribution. We also show how $\\delta$-locality can be\nderived from different conditions that characterize the distribution's\nlocality. Our $\\ell_\\infty$ bound motivates the localization of existing\napproximation methods to respect the locality. As examples, we show how to use\nlocalized likelihood-informed subspace method and localized score matching,\nwhich not only avoid dimension dependence in the approximation error, but also\nsignificantly reduce the computational cost due to the local and parallel\nimplementation based on the localized structure.",
        "updated": "2024-10-15 16:47:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.11771v1"
    }
]