High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion
JunhwaHur*, CharlesHerrmann*, SaurabhSaxena, JanneKontkanen, Wei-ShengLai,
YichangShih, MichaelRubinstein, DavidJ.Fleet†, DeqingSun
Google
Abstract high-resolutionframeinterpolation,whichenablesthesyn-
thesis of new frames between existing ones to enhance a
Despite the recent progress, existing frame interpolation video’s frame rate. Despite the progress, the latest tech-
methods still struggle with processing extremely high reso-
niques struggle in the high resolution setting, where chal-
lution input and handling challenging cases such as repeti-
lengingcaseslikerepetitivetextures,detailedorthinobjects
tivetextures,thinobjects,andlargemotion.Toaddressthese
becomemorecommonplace.
issues,weintroduceapatch-based cascadedpixeldiffusion
modelforframeinterpolation,HiFI,thatexcelsinthesesce- Existing methods often design models using strong do-
narioswhileachievingcompetitiveperformanceonstandard main knowledge, e.g., correspondence matching (Ranjan
benchmarks. Cascades, which generate a series of images and Black 2017; Ilg et al. 2017; Sun et al. 2018; Teed and
fromlow-tohigh-resolution,canhelpsignificantlywithlarge Deng2020)andsynthesisbasedonwarping(Huetal.2022;
or complex motion that require both global context for a Jiangetal.2018;NiklausandLiu2020;Park,Lee,andKim
coarsesolutionanddetailedcontextforhighresolutionout- 2021; Xue et al. 2019). Domain knowledge enables small
put. However, contrary to prior work on cascaded diffusion
modelstoperformwellwhentrainedonasmallamountof
modelswhichperformdiffusiononincreasinglylargeresolu-
data but may restrict their capabilities. For example, when
tions,weuseasinglemodelthatalwaysperformsdiffusion
motioncuesareincorporatedintothemodel,thefinalqual-
atthesameresolutionandupsamplesbyprocessingpatches
ityareboundedbytheaccuracyofthemotion.Thisispar-
oftheinputsandthepriorsolution.Weshowthatthistech-
nique drastically reduces memory usage at inference time ticularlyevidentonhighresolutioninputswithlargemotion,
and also allows us to use a single model at test time, solv- repetitivetexture,andthinstructures,wheremotionestima-
ingbothframeinterpolation(basemodel’stask)andspatial tionoftenstruggles(seeFig.1).
up-sampling,savingtrainingcost.WeshowthatHiFIhelps To address these challenges, we advocate a domain-
significantlywithhighresolutionandcomplexrepeatedtex-
agnosticdiffusionapproach,relyingonmodelcapacityand
turesthatrequireglobalcontext.HiFIdemonstratescompara-
trainingdataatscaleforperformancegainsandgeneraliza-
bleorbeyondstate-of-the-artperformanceonmultiplebench-
tion. Some recent work have explored diffusion for frame
marks(Vimeo,Xiph,X-Test,andSEPE-8K).Onournewly
interpolation but towards generative aspect, e.g. better per-
introduced dataset that focuses on particularly challenging
cases,HiFIalsosignificantlyoutperformsotherbaselineson ceptual quality (Danier, Zhang, and Bull 2024) or complex
these cases. Please visit our project page for video results: andnon-linearmotion(Jainetal.2024)betweentwoframes
https://hifi-diffusion.github.io veryfurtherapartintime.However,theirperformancefalls
behind in the classical setting: predicting an intermediate
framewithqualitymeasuredbyPSNRorSSIM.
Introduction
We instead introduce a patch-based cascaded pixel dif-
In a short amount of time, smartphone cameras have be- fusion approach for High resolution Frame Interpolation,
come both ubiquitous and significantly higher quality, cap- dubbedHiFI.HiFIgeneralizesacrossdiverseresolutionsup
turing spatially higher resolution images and videos. How- to 8K images, a wide range of scene motions, and a broad
ever,thetemporalresolution—i.e.videoframerate—ofcap- spectrum of challenging scenes. The diffusion framework
turedvideoshaslaggedbehindthespatialresolution,dueto allows us to scale both the model capacity and data size,
acombinationofcomputationalandmemorycostsandlim- and we show that our model can effectively utilize large-
itedexposuretime. scalevideodatasets.Whilecascadesoffersignificantbene-
The conflict between increased user interest in creative fitsforhandlingdiverseinputresolutionsanddifferentlevels
video content and technical limitations for capturing high ofmotion,standardcascades,whichdenoisetheentirehigh-
frame-rate video has increased interest in techniques for resolutionimagejointly,oftenstrugglewithmemoryissues
atveryhighresolutionssuchas8K.Tosavememoryduring
*Theseauthorscontributedequally. inference,weproposeanewpatch-basedcascadeforframe
†DF is also affiliated with the University of Toronto and the interpolation,whichalwaysdenoisesthesameresolutionbut
VectorInstitute. isappliedtopatchesofhighresolutionframes.Inaddition,
4202
tcO
51
]VC.sc[
1v83811.0142:viXraInputoverlay M2M EMA-VFI HiFI(Ours) GroundTruth
Figure1:QualitativecomparisononchallengingcasesonourproposedLaMoRdataset(rows1and2)andX-TEST(row3).
Forchallengingcases,suchaslargemotionorrepetitivetextures,theproposedHiFIsubstantiallyoutperformsotherbaselines.
thisallowsustousethesamemodelforbothbaseestimation pose to estimate an intermediate frame in a phase-based
andsuper-resolution,savingtrainingtimeanddiskspaceat representation instead of the conventional pixel domain.
inferencetime. Kernel-basedapproaches(ChengandChen2020;Leeetal.
The proposed HiFI method achieves state-of-the-art ac- 2020;Niklaus,Mai,andWang2021;Niklaus,Mai,andLiu
curacy on challenging high-resolution public benchmark 2017a,b)presentsimplesingle-stageformulationsthatesti-
datasets, Xiph (Niklaus and Liu 2020), X-TEST (Sim, mateper-pixeln nkernelsandsynthesizetheintermediate
×
Oh, and Kim 2021) and SEPE (Al Shoura et al. 2023), frameusingconvolutiononinputpatches.Bothapproaches
anddemonstratesstrongperformanceonchallengingcorner avoidrelianceonmotionestimator,buttheydonotusually
cases,e.g.,repetitivetexturesandlargemotion.Wealsoin- perform well on high resolution input with large motion,
troduceanewevaluationdataset,LargeMotionandRepet- evenwithdeformableconvolution(ChengandChen2020).
itive texture (LaMoR), which specifically highlights these
Generic architecture for interpolation. Some methods
challenging cases and demonstrate that HiFI significantly
exploreusingagenericarchitecturewithoutdomainknowl-
outperformsexistingbaselines.
edge, such as attention (Choi et al. 2020), transformer (Shi
Relatedwork et al. 2022), 3D convolution under multi-frame input setup
(Kallurietal.2023;Shietal.2022).However,bothattention
Domain-specific architecture for interpolation. One
and3Dconvolutionarecomputationallyexpensiveandthus
representativelineofworkismotion-basedapproach.Given
prohibitiveat4Kor8Kresolution.
estimated bi-directional optical flow between two nearby
frames, these methods synthesize an intermediate frame Diffusion models for computer vision. Recently diffu-
through forward splatting (Hu et al. 2022; Jin et al. 2023; sionmodelshavedemonstratedtheirstrengthongenerative
Niklaus and Liu 2018, 2020) or backward warping (Huang computervisionapplicationssuchasimage(Hoetal.2022a;
et al. 2022; Jiang et al. 2018; Park, Lee, and Kim 2021; PeeblesandXie2023;Rombachetal.2022)andvideogen-
Park et al. 2020; Sim, Oh, and Kim 2021), followed by a eration (Blattmann et al. 2023; Ge et al. 2023; Ho et al.
refinement module that improves visual quality. Their per- 2022b), image editing (Brooks, Holynski, and Efros 2023;
formance depends on the accuracy of motion estimations Yang, Hwang, and Ye 2023), 3D generation (Qian et al.
sinceinaccuraciesinthemotioncauseartifactsinthesplat- 2024; Shi et al. 2024b), etc. Beyond generation, diffusion
ting/warpingprocess.Asaresult,theystruggleoninputsfor has also shown to be effective for dense computer vision
whichopticalflowestimationisproblematic,i.e.,largemo- tasksandhasbeenbecomethestate-of-the-arttechniquefor
tion,occlusion,andthinobjects. classicalproblemssuchasdepthprediction(Keetal.2024;
Phase-based approaches (Meyer et al. 2015, 2018) pro- Saxena et al. 2023a), optical flow prediction (Saxena et al.2023a),correspondencematching(Nametal.2024),seman- Sampled time step<latexit sha1_base64="ObOz7MDSvnPxARcJMTWIRdlhyPo=">AAACAXicbVBNS8NAEN34WetX1IvgZbEIHqQkItVj0YvHCqYtJKFstpt26e4m7G6EEurFv+LFgyJe/Rfe/Ddu2hy09cHA470ZZuZFKaNKO863tbS8srq2Xtmobm5t7+zae/ttlWQSEw8nLJHdCCnCqCCeppqRbioJ4hEjnWh0U/idByIVTcS9Hqck5GggaEwx0kbq2YcaBopyGHCkhxix3Jv4zhl0w55dc+rOFHCRuCWpgRKtnv0V9BOccSI0Zkgp33VSHeZIaooZmVSDTJEU4REaEN9QgThRYT79YAJPjNKHcSJNCQ2n6u+JHHGlxjwyncWhat4rxP88P9PxVZhTkWaaCDxbFGcM6gQWccA+lQRrNjYEYUnNrRAPkURYm9CqJgR3/uVF0j6vu4164+6i1rwu46iAI3AMToELLkET3IIW8AAGj+AZvII368l6sd6tj1nrklXOHIA/sD5/AOaLlec=</latexit>t ⇠U[0,1] Timestep embedding
ticsegmentation(Baranchuketal.2022;Xuetal.2023),etc.
Diffusion
Diffusion models for interpolation. Two recent works forward process
Target intermediate Noisy inputz<latexit sha1_base64="IV+ShggXRDda8GPnRdLoDHXNSRg=">AAAB+XicbVBPS8MwHE3nvzn/VT16CQ7B02hFpsehF48T3BxspaRpuoWlaUl+Hcyyb+LFgyJe/Sbe/DamWw+6+SDk8d7vR15ekAquwXG+rcra+sbmVnW7trO7t39gHx51dZIpyjo0EYnqBUQzwSXrAAfBeqliJA4EewzGt4X/OGFK80Q+wDRlXkyGkkecEjCSb9uDIBGhnsbmyp9mPvh23Wk4c+BV4pakjkq0fftrECY0i5kEKojWfddJwcuJAk4Fm9UGmWYpoWMyZH1DJYmZ9vJ58hk+M0qIo0SZIwHP1d8bOYl1Ec5MxgRGetkrxP+8fgbRtZdzmWbAJF08FGUCQ4KLGnDIFaMgpoYQqrjJiumIKELBlFUzJbjLX14l3YuG22w07y/rrZuyjio6QafoHLnoCrXQHWqjDqJogp7RK3qzcuvFerc+FqMVq9w5Rn9gff4AaA2ULg==</latexit>t
also explore diffusion for video frame interpolation from a frameI<latexit sha1_base64="QHraVnX2X5KTgd5LLSXGkhqrKnc=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi1WXRje4q2Ac0oUymk3boZBLmIZTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJU86Udt1vp7S2vrG5Vd6u7Ozu7R9UD486KjGS0DZJeCJ7IVaUM0HbmmlOe6mkOA457YaT29zvPlGpWCIe9TSlQYxHgkWMYG0l34+xHodRdj8beINqza27c6BV4hWkBgVag+qXP0yIianQhGOl+p6b6iDDUjPC6aziG0VTTCZ4RPuWChxTFWTzzDN0ZpUhihJpn9Borv7eyHCs1DQO7WSeUS17ufif1zc6ug4yJlKjqSCLQ5HhSCcoLwANmaRE86klmEhmsyIyxhITbWuq2BK85S+vks5F3WvUGw+XteZNUUcZTuAUzsGDK2jCHbSgDQRSeIZXeHOM8+K8Ox+L0ZJT7BzDHzifP+YakZw=</latexit>1 Base model Denoised prediction ˆI<latexit sha1_base64="eMjKR5v1GK+iMKkvicfniGDXBHg=">AAAB+3icbVDLSsNAFJ3UV62vWJdugkVwVRKR6rLoRncV7AOaECbTSTt0MgkzN2IJ+RU3LhRx64+482+ctFlo64GBwzn3cs+cIOFMgW1/G5W19Y3Nrep2bWd3b//APKz3VJxKQrsk5rEcBFhRzgTtAgNOB4mkOAo47QfTm8LvP1KpWCweYJZQL8JjwUJGMGjJN+vuBEPmRhgmQZjd5bnv+GbDbtpzWKvEKUkDlej45pc7ikkaUQGEY6WGjp2Al2EJjHCa19xU0QSTKR7ToaYCR1R52Tx7bp1qZWSFsdRPgDVXf29kOFJqFgV6sgiplr1C/M8bphBeeRkTSQpUkMWhMOUWxFZRhDVikhLgM00wkUxntcgES0xA11XTJTjLX14lvfOm02q27i8a7euyjio6RifoDDnoErXRLeqgLiLoCT2jV/Rm5MaL8W58LEYrRrlzhP7A+PwBRC+Umg==</latexit>1
generative perspective. LDMVFI (Danier, Zhang, and Bull via v<latexit sha1_base64="kzu8rA5oY3M30grBP2LTyOJvLoA=">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIVJdFNy4r2Ae0Y8lk0jY0kwxJplKG/ocbF4q49V/c+Tdm2llo64GQwzn3kpMTxJxp47rfTmFtfWNzq7hd2tnd2z8oHx61tEwUoU0iuVSdAGvKmaBNwwynnVhRHAWctoPxbea3J1RpJsWDmcbUj/BQsAEj2FjpsRdIHuppZK90MuuXK27VnQOtEi8nFcjR6Je/eqEkSUSFIRxr3fXc2PgpVoYRTmelXqJpjMkYD2nXUoEjqv10nnqGzqwSooFU9giD5urvjRRHOotmJyNsRnrZy8T/vG5iBtd+ykScGCrI4qFBwpGRKKsAhUxRYvjUEkwUs1kRGWGFibFFlWwJ3vKXV0nrourVqrX7y0r9Jq+jCCdwCufgwRXU4Q4a0AQCCp7hFd6cJ+fFeXc+FqMFJ985hj9wPn8AU9STEg==</latexit> -parameterization
2024) proposes using a conditional latent diffusion model
and optimizes it for perceptual frame interpolation quality, Input framesI<latexit sha1_base64="4PyLlaUBdIaPiGR+HeeaIyhQ4ds=">AAACA3icbVDLSsNAFL2pr1pfUXe6GSyCCylJkeqy6EZ3FewD2hAm00k7dPJgZiKUUHDjr7hxoYhbf8Kdf+OkzaK2Hhg4c8693HuPF3MmlWX9GIWV1bX1jeJmaWt7Z3fP3D9oySgRhDZJxCPR8bCknIW0qZjitBMLigOP07Y3usn89iMVkkXhgxrH1AnwIGQ+I1hpyTWPegFWQ89P7yaudY7mflXXLFsVawq0TOyclCFHwzW/e/2IJAENFeFYyq5txcpJsVCMcDop9RJJY0xGeEC7moY4oNJJpzdM0KlW+siPhH6hQlN1viPFgZTjwNOV2Y5y0cvE/7xuovwrJ2VhnCgaktkgP+FIRSgLBPWZoETxsSaYCKZ3RWSIBSZKx1bSIdiLJy+TVrVi1yq1+4ty/TqPowjHcAJnYMMl1OEWGtAEAk/wAm/wbjwbr8aH8TkrLRh5zyH8gfH1Czk4l0Q=</latexit>0,I2
butthePSNRorSSIMmetricofthepredictedframestends Figure 2: Our base model is conditioned on two input
tobelowerthanthatbystateoftheart.VIDIM(Jain etal. frames, I 0 and I 2, and predicts the intermediate frame I 1.
2024)usesacascadedpixeldiffusionmodelbutfocuseson Themodelusesv-parameterization(SalimansandHo2022;
ataskclosertotheconditionalvideogeneration.Giventwo Saxenaetal.2023b)forbothmodeloutputandloss.
temporally-far-apart frames, the method generates a base
videoof7framesat64 64resolutionandthenupsamples present in the low resolution answer that require commu-
×
themto256 256.Itisunclearwhetheradiffusion-basedap- nicationbetweenpatchesforconsistencyandcoherency.In
×
proachcanachievecompetitiveresultsontheclassicalframe contrast,estimationtasksbenefitfromastrongconditioning
interpolationproblem,wheretheinputframescomefroma signal(inputframes),whichprovidethiscontextandmake
videowithhighFPSandcanbeupto8Kresolution. theproblemmorelocalized.Thisdifferenceinfocusallows
ustoexploredifferentarchitectureanddesignchoices.
Cascaded diffusion models. Beginning with CDM (Ho
In the estimation community, the most similar to ours is
et al. 2022a), cascades have become the default technique
DDVM(Saxenaetal.2023a)whichuses“tiling”toimprove
forscalinguptheoutputresolutionofpixeldiffusionmod-
theresolution.Afterthebasemodelrunsatacoarsesolution,
els.Diffusioncascadesconsistofa“base”modelforanini-
the output is upsampled by partially denoising tiles taken
tiallow-resolutionsolutionandanumberofseparate“super-
from the coarse solution and input frames. In a similar set-
resolution” models to produce a higher-resolution output
ting,frameinterpolation,weshowthattilingperformsworse
conditioned on the low resolution output. Despite its abil-
thanourproposedpatch-basedcascade.
ity to produce high-resolution output, the minimum mem-
ory cost increases proportionally with the resolution since
Diffusionforhigh-resolutionframeinterp.
each super-resolution model performs diffusion at the res-
olution it produces. Even with hand-designing specialized Weexploreapixeldiffusionapproachforclassicalframein-
super-resolutionarchitectures(Hoetal.2022a;Sahariaetal. terpolationandproposeapatch-basedcascadestrategythat
2022),theincreasedmemoryproblemforeachupsampling upsamplesbydoingdiffusionatasingleresolutionandpro-
stage still persists, especially as the target resolution in- cessing patches of high resolution inputs. Our patch-based
creasessignificantly,suchasfrom1Kto8K. cascade allows us to produce a high resolution output with
low memory usage at inference time, and use the same
High resolution diffusion. Recent works in high- modelforbothbaseestimationandupsampling.
resolution image generation have introduced training-free
approaches through merging the score functions of nearby Architecture
patches (Bar-Tal et al. 2023; Liu et al. 2023) or expanding
Our method adopts a conditional image diffusion frame-
thenetwork(Shietal.2024a;Kim,Hwang,andPark2024).
work.GivenaconcatenationoftemporallynearbyframesI
More recently papers have begun to explore using mod- 0
andI asaconditioningsignal,ourmodelaimstoestimate
elswhichareexplicitlytrainedtodenoisepatches(ortiles) 2
anintermediateframeˆI asareversediffusionprocessinthe
and then merge their results. Any-size-diffusion (Zheng 1
pixel space, as illustrated in Fig. 2. We take a generic effi-
et al. 2024) introduces an implicit overlap scheme, where
cientU-NetarchitecturefromDDVM(Saxenaetal.2023a)
high-resolution image is broken into a series of non-
with v-parameterization (Salimans and Ho 2022; Saxena
overlapping tiles with changing location during sampling
et al. 2023b) for both model output and loss. The U-Net
process. Patched denoising diffusion models (Ding et al.
includes self-attention layers at two bottom levels. Given a
2024)createsasetofhierarchicalpatchesbutthenusesboth
noisyimagez = α x+σ ϵasaninput,thenetworkpre-
the score value as well as the feature maps to encourage t t t
dictsvˆ,wherexisthetargetimage(i.e.,I ),sampledran-
consistency. In video generation, Hierarchical Patch Diffu- 1
domnoiseϵ (0,I),sampledtimestept [0,1],and
sion(Skorokhodovetal.2024)proposesahierarchicalpatch α2 + σ2 = ∼ 1.N We directly apply L1 loss on∼ vU parameter
structure to improve training time and save on memory at t t
space,i.e., vˆ v ,wherev =α ϵ σ x.Thepredicted
inferencetime.However,duetotheirtask,theyrequirespe- || − ||1 t − t
cializedmodulestoensureglobalconsistency. imageisrecoveredbyxˆ =α tz t σ tvˆ,wherexˆ =ˆI 1.
−
In contrast to these efforts, we focus on frame interpo-
Patch-basedcascademodel
lation, an estimation task, and target improved memory-
efficiency at inference time for extremely high resolutions Sinceourmainfocusisonextremelyhighresolutions(2K,
(4Kor8K).Infact,patch-basedtechniquesareparticularly 4K,8K),standardcascades,whichjointlydenoisetheentire
well-suitedforestimationtasks.Themainissuewithpatch- high resolution image, would require either a considerable
basedmethodsforgenerationaretheexistenceofdetailsnot amountofmemoryatinferencetimeoracarefularchitectureFigure 3: Patch-based cascade model. Given a low-resolution intermediate from the previous level, patch-based cascade
createspatchesfrombi-linearlyupsampledlow-resolutionintermediateandtwoinputframesandusesthesepatchesascondi-
tioning for a diffusion process. It then combines denoised patches to form the whole image. At inference time, only a single
weight-sharedmodelisrecursivelyusedacrossdifferentimagescalesasinFig.4.Two-stagecascadeisshownforsimplicity.
At each pyramid level, we upscale prediction via patch-
based cascade, as shown in Fig. 3. For refinement at scale
s , we take the prediction from the prior scale and then
N−2
upsampleittomatchs .Ateachlevel,weperformthree
N−2
stages:(i)patchify,wherewecropoverlappingpatchesfrom
theupsampledintermediatepredictionandtheinputatthat
scale,(ii)denoisepatches,wherewerundiffusiontoobtain
the prediction for each patch, and (iii) accumulate patches,
whereweuseMultiDiffusion(Bar-Taletal.2023)tomerge
results from different patches for the predictionˆIsN−2. We
2
then upsampleˆIsN−2 to level s and repeat this process
2 N−3
untiln=0,theoriginalinputscale.
Training setup. For the patch-based cascade model, we
Figure 4: Upsampling strategy. Similar to a standard cas-
wanttotrainadiffusionmodelthatisconditionedonapair
cade, we process the image from coarse to fine, but we al-
of input images and a half resolution representation of the
waysdenoiseatthesameresolution,asindicatedbythered
target we aim to predict. We first predict the intermediate
box.DetailsoneachstepofthecascadeareinFig.3.
frame at a half resolutionˆI1/2 by feeding downsampled in-
1
puts to a pre-trained base model (Fig. 2) computed offline.
searchtoreducethememorycost.Weinsteadadvocatefora
Then we upsample this intermediate frame to the original
patch-basedcascadeapproachthatperformsdiffusionatthe
scaleanduseitasaconditioningimagealongwiththeorig-
sameresolutiononpatchesoftheinputframes.Thisavoids
inal inputs and follow the standard diffusion procedure to
bothoftheseissues,keepingthepeakmemoryusageatin-
train the patch-based cascade model. Note this inference is
ferencetimenearconstantandallowingustousethesame
doneonceofflinetoimprovetrainingefficiency.
architectureforeveryupsamplelevel.Wealsofindthatcan
re-usethesamemodelinboththebaseandsuper-resolution Single model for all stages. By conditioning on the low
settings,savingtrainingtimeanddiskspaceatinference.
resolution estimate but using dropout 50% of the time dur-
ingtraining,wefindthatwecanusethesamemodelforall
Approach. For our overall strategy, shown in Fig. 4, we
stages of the cascade, including base and super-resolution;
adopt the well-known coarse-to-fine idea for cascades and
base generation is done by passing zeros as the low reso-
build an N-level image pyramid. Starting from the lowest
lution condition. Note, while this shares some similarities
scale s
N−1
(where s
n
1/2n), we downsample the input
with CFG (Ho and Salimans 2022), in practice, we do not
conditioning images by≡ a factor of s (i.e., IsN−1 and
N−1 0 combine unconditional and conditional estimations at in-
IsN−1)andpredictanintermediateimageˆIsN−1 atthesame ference time. Empirically we find that a single model for
2 1
scale s N−1. We then apply 2 bilinear upsampling to this all stages performs slightly better than having a dedicated
intermediate image
up(ˆIsN−1)×
and use it as a conditioning super-resolution model; it also has the benefits of substan-
1
signalforadenoisingprocessatthescales . tially reducing training time (only one model is trained in-
N−2steadofmultipledifferentonesfromscratch)anddiskspace showninFig.5.Wewillanalyzekeycomponentsthatcon-
atinferencetime(onlyonemodelneedstobesaved).Inter- tributetotheperformanceintheablationstudybelow.
estingly,weobservethatadedicatedsuper-resolutionmodel
SEPE. We also test HiFI on SEPE, which consists of 8K
trained without dropout on the coarse estimation does not
resolutionvideos.Mostmethodswetestedranoutofmem-
worksinceittakestheshortcutofupsamplingthelowreso-
ory except M2M with PSNR 28.34 (dB) and SSIM 0.883,
lutioninsteadofattendingtothehighresolutioninputs.
comparedwithPSNR29.61(dB)andSSIM0.899byHiFI.
Pleaseviewthesuppl.materialforvisualcomparison.
Experiments
Implementation details. Similar to previous diffusion- LargeMotionandRepetitivetexturedataset
based methods (Jain et al. 2024; Danier, Zhang, and Bull
The public benchmark datasets present a variety of scenes
2024),weutilizealarge-scalevideodatasetfortraining,to
butdonotfullycapturethefailuremodesofcurrentmethod,
testthescalabilityofthediffusionmodelbetter.Thedataset
especially large motion or repetitive texture cases often
contains up to 30 M videos with 40 frames, collected from
present in videos in the wild. To better evaluate existing
theinternetandothersources.Wefirsttrainourbasemodel
methods and further innovation, we introduce a Large Mo-
onthedataset,andthenweadditionallyincludeVimeo-90K
tion and Repetitive texture (LaMoR) dataset that includes
triplet (Xue et al. 2019) and X-TRAIN (Sim, Oh, and Kim
such19challengingexamplesat4Kresolutioninbothpor-
2021)tofinetunethecascademodel.Forafaircomparison,
traitandlandscapemodes.Figure6demonstratesafewchal-
we also prepare a model that is trained on Vimeo-90K and
lenging examples from our dataset. Table 2 compares the
X-TRAINonlyfromthescratch.Weuseamini-batchsizeof
accuracy of state-of-the-art methods including ours on the
256andtrainthebasemodelfor3Miterationstepsandthe
dataset. Figure 7 shows qualitative results where HiFI per-
patch-basedcascademodelfor200kiterationsteps.Weuse
forms particularly well for challenging cases of repetitive
theAdamoptimizer(KingmaandBa2014)withaconstant
texturesandlargemotion.
learningrate1e−4withwarmupatthebeginning.Forinfer-
ence,weuse3-stagepatch-basedcascadesetupwithapatch
Ablationstudy
sizeof512 768unlessspecified.Duringsampling,weav-
erage4sam× plesthatareestimatedvia8samplingsteps. Dedicatedupsamplemodelvssinglemodel. InTable3,
Our data augmentation includes random crop and hori- we compare the accuracy of the base model, two distinct
zontal,vertical,andtemporalflipwithaprobabilityof50%. modelsforbaseandupsample,andourfinalsettingofusing
Weuseacropsizeof352 480forlarge-scalebasemodel thesamemodelforbaseandupsample.Bothcascadestrate-
training and 224 288 for× the cascade model training. We gies are effective for handling large motion, substantially
use a multi-resolu× tion crop augmentation that crops an im- improvingaccuracyonX-TEST.Usingthesamemodelfor
age patch with a random rectangular crop size between the bothbaseandupsampleperformson-parorevenbetterthan
originalresolutionandthefinalcropsizeandthenresizeitto havingadedicatedupsamplemodel,especiallyonchalleng-
the final crop size. While commonly used, we find random ingX-TEST.Thisvalidatesthestrengthofre-usingthesame
90◦rotationaugmentationandphotometricaugmentationto model for both over the more expensive dedicate model
belesseffective,soweoptnottousethem. setup. Increasing the number of upsample stages improves
Moredetailsareinthesupplementarymaterial. theaccuracybutsaturatesoverthree.
Comparison with coarse-to-fine refinement. Coarse-to-
Publicbenchmarkevaluation
finetilingrefinementfromDDVM(Saxenaetal.2023a)first
WefirstevaluateHiFIonthreepopularbenchmarkdatasets,
predicts the target at low resolution, bilinearly upsamples
Vimeo-90Ktriplet,Xiph,andX-TEST,asshowninTable1,
it to the target resolution, and refines it from an intermedi-
aswellasan8Kdataset,SEPE(AlShouraetal.2023).
ate sampling step in a patch-wise manner. Our patch-based
Vimeo-90K. Thelow-resolution(256 448)Vimeo-90Kis cascadeperformsconsistentlybetterthanthecoarse-to-fine
oneofthemostheavilystudiedbenchm× ark,wherenumbers tilingrefinementontheX-TESTbenchmark;32.98(dB)vs
arehighlysaturatedamongdifferentmethods.HiFIachieves 32.54(dB)on4K,and33.79(dB)vs.33.03(dB)on2K.
competitiveaccuracywithagenerictrainingprocedure.
Architecture. In Table 4, we analyze where the major
Xiph and X-TEST. Both Xiph and X-TEST have high gain originates from by ablating attention layers or diffu-
resolution(2Kand4K).ThemotionofX-TESTcanbeover sionprocessinthebasemodel,giventhesame training as-
400pixelsatthe4Kresolution,particularlychallengingfor sets(e.g.,datasets,computations,etc.).Usingattentionlay-
existing methods. For X-TEST, we follow the evaluation ers brings about moderate performance gains on both the
protocoldiscussedin(Sim,Oh,andKim2021)thatinterpo- small (Vimeo) and large (X-TEST) motion datasets. Re-
lates7intermediateframes.Whentrainedonacombination moving the diffusion process leads to significant perfor-
of Vimeo and X-TRAIN, HiFI performs favorably against mancedegradation.Wealsotestonewidely-usedtraditional
stateoftheartonXiphandX-TESTdatasets,bothin2Kand method FILM (Reda et al. 2022), which relies on a “scale-
4Kresolutions.Pre-trainingonalargevideodatasetsignifi- agnostic” motion estimator to handle large motion. FILM
cantlybooststheperformanceofHiFIonXiphandX-TEST, trainedonthesamelargedatasetissubstantiallyworsethan
settinganewstateoftheart.Visually,HiFIcanbetterinter- HiFI,suggestingthattraditional,hand-designedmethodsdo
polate fine details with large motion at high resolution, as notscaleupwellw.r.t.data.Inputoverlay M2M EMA-VFI UPR-Net HiFI(Ours) Groundtruth
Figure 5: Qualitative examples for public datasets. Our method performs well even in cases of large motion and complex
texturessuchasathinobjectonthetopandtheplatenumberatthebottom.
Table 1: Results on public benchmark datasets: HiFI performs favorably on the highly-saturated Vimeo-90K (Xue et al.
2019)andissubstantiallymoreaccuratethanexistingtwo-framemethodsonhigh-resolutionXiph(NiklausandLiu2020)and
X-TEST(Sim,Oh,andKim2021)datasets. Best and second-best arehighlightedincolor.
Xiph X-TEST
Vimeo-90K
Method Trainingdataset 2K 4K 2K 4K
PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM
M2M(Huetal.2022) Vimeo 35.47 0.978 36.44 0.967 33.92 0.945 32.07 0.923 30.81 0.912
FILM(Redaetal.2022) Vimeo 36.06 0.970 36.66 0.951 33.78 0.906 31.61 0.916 26.98 0.839
AMT(Lietal.2023) Vimeo 36.53 0.982 36.38 0.941 34.63 0.904 - - - -
UPR-Net(Jinetal.2023) Vimeo 36.42 0.982 - - - - - - 30.68 0.909
FITUG(Placketal.2023) Vimeo 36.34 0.981 - - - - - - - -
TCL(Zhouetal.2023) Vimeo 36.85 0.982 - - - - - - - -
IQ-VFI(Huetal.2024) Vimeo 36.60 0.982 36.68 0.942 34.72 0.905 - - - -
EMA-VFI(Zhangetal.2023) Vimeo(+septuplet) 36.64 0.982 36.90 0.945 34.67 0.907 32.85 0.930 31.46 0.916
XVFI(Sim,Oh,andKim2021) Vimeo/X-TRAIN 35.07 0.976 - - - - 30.85 0.913 30.12 0.870
BiFormer(Park,Kim,andKim2023) Vimeo+X-TRAIN - - - - 34.48 0.927 - - 31.32 0.921
Vimeo+X-TRAIN 35.70 0.979 36.64 0.967 34.45 0.948 33.03 0.927 32.03 0.918
HiFI(Ours)
Vimeo+X-TRAIN+Rawvideos 36.12 0.980 37.36 0.969 35.40 0.953 33.94 0.941 32.92 0.931
Table2:ResultsonLaMoR.HiFIissignificantlymoreac-
curatethanstate-of-the-artmethods.
Method PSNR SSIM
LDMVFI(Danier,Zhang,andBull2024) 21.952 0.828
EMA-VFI(Zhangetal.2023) 22.327 0.845
M2M(Huetal.2022) 24.995 0.884
UPR-Net(Jinetal.2023) 25.856 0.892
BiFormer(Park,Kim,andKim2023) 26.330 0.893
Figure 6: A few examples from our LaMoR dataset that HiFI(Ours) 28.141 0.912
includes challenging scenes, such as repetitive texture and
largemotionwheretypicalmethodsfail.
cascade model. In general, we find that the model needs
more sampling steps for large motion (e.g., X-TEST) than
Numberofsamplingsteps. Theoptimalnumberofsam- forsmallmotion(e.g.,Vimeo-90K(Xueetal.2019)).How-
plingstepsalsodifferbetweenthebaseandthepatch-based
K2hpiX
K4TSET-XInputoverlay M2M EMA-VFI UPR-Net HiFI(Ours) Groundtruth
Figure7:QualitativecomparisononLaMoR.TheproposedHiFIisparticularlyeffectiveatverychallengingcasesincluding
repetitivetexturesandlargemotion.
Table 3: Base vs. cascade models. Our patch-based self- Table5:EffectofthesamplingstepsonPSNRforthebase
cascadeformulationsubstantiallyoutperformsthebasewith modelandpatch-basedcascademodel.Morestepstendsto
thesamenumberofparameters.Throughmodelsharing,our bebetterforhigherresolutionandlargemotiondatasets.
self-cascade generalizes better on the X-TEST dataset than
(a)Basemodel (b)Patch-basedcascade
thestandardcascadebutwithhalfoftheparameters.
Steps Vimeo-90K X-TEST4K Steps Vimeo-90K X-TEST4K
Vimeo-90K X-TEST2K X-TEST4K
Method Modelsize
1 34.87 27.95 1 36.13 32.32
PSNRSSIMPSNRSSIMPSNRSSIM
2 35.37 27.92 2 36.15 32.83
Base 647M 35.44 0.978 30.32 0.879 28.57 0.876 4 35.44 28.57 4 36.12 32.92
8 35.21 29.67 8 36.06 32.92
Standardtwo-stagecascade 1294M 36.12 0.980 33.86 0.939 32.48 0.926
16 34.58 30.34 16 35.98 32.84
Patch-basedself-cascade×2 647M 36.12 0.980 33.93 0.941 32.77 0.930
32 33.53 30.40 32 35.92 32.68
Patch-basedself-cascade×3 647M 36.12 0.980 33.94 0.941 32.92 0.931
64 32.68 30.02 64 35.86 32.64
Discussions While HiFI demonstrates significantly bet-
ter performance there are still limitations. Some extremely
Table 4: Architecture analysis. Both attention layers and
complicated motion types, e.g. fluid dynamics, are still be-
diffusion process contribute to substantial accuracy gain.
yondthecapabilityofourmodel.Diffusionmodelsarecom-
Comparingtoadomain-specificarchitecture,FILM,ourap-
putationallyheavyandmayneeddistillation(Salimansand
proach scales up better when training on the same large-
Ho2022)forapplicationswithalimitedcomputationalbud-
scalevideodataset.
get.
Vimeo-90K X-TEST2K X-TEST4K
Method
PSNR SSIM PSNR SSIM PSNR SSIM Conclusion
Ours,basemodel 35.44 0.978 30.32 0.879 28.57 0.876
w/oattentionlayers 35.13 0.977 29.73 0.861 27.75 0.854 Wehaveintroducedadiffusion-basedmethodforhighreso-
w/odiffusion 33.78 0.965 28.05 0.852 27.56 0.861 lutionframeinterpolation,namedHiFI.Ourproposedpatch-
basedcascadeachievesstate-of-the-artperformanceonsev-
FILM(Redaetal.2022) 34.02 0.970 28.15 0.854 27.24 0.856
eral high-resolution frame interpolation benchmarks, while
improvingefficiencyfortrainingandinference.Wealsoes-
ever,thepatch-basedcascademodelisabletoachievebetter tablish a new benchmark, LaMoR, with large motion and
numbersacrossdifferentdatasetswithfewersamplingsteps. repeatedtexturesathighresolution.References Jain,S.;Watson,D.;Tabellion,E.;Hołyn´ski,A.;Poole,B.;
andKontkanen,J.2024. VideoInterpolationwithDiffusion
Al Shoura, T.; Dehaghi, A. M.; Razavi, R.; Far, B.; and
Models. InCVPR.
Moshirpour,M.2023. SEPEDataset:8KVideoSequences
and Images for Analysis and Development. In Conference Jiang, H.; Sun, D.; Jampani, V.; Yang, M.-H.; Learned-
onACMMultimediaSystems,463–468. Miller, E.; and Kautz, J. 2018. Super SloMo: High quality
estimation of multiple intermediate frames for video inter-
Bar-Tal,O.;Yariv,L.;Lipman,Y.;andDekel,T.2023. Mul-
polation. InCVPR,9000–9008.
tiDiffusion:Fusingdiffusionpathsforcontrolledimagegen-
Jin,X.;Wu,L.;Chen,J.;Chen,Y.;Koo,J.;andHahm,C.-h.
eration. InICML.
2023. Aunifiedpyramidrecurrentnetworkforvideoframe
Baranchuk,D.;Voynov,A.;Rubachev,I.;Khrulkov,V.;and
interpolation. InCVPR,1578–1587.
Babenko,A.2022. Label-EfficientSemanticSegmentation
Kalluri,T.;Pathak,D.;Chandraker,M.;andTran,D.2023.
withDiffusionModels. InICLR.
FLAVR:Flow-agnosticvideorepresentationsforfastframe
Blattmann, A.; Dockhorn, T.; Kulal, S.; Mendelevitch, D.; interpolation. InWACV,2071–2082.
Kilian, M.; Lorenz, D.; Levi, Y.; English, Z.; Voleti, V.;
Ke,B.;Obukhov,A.;Huang,S.;Metzger,N.;Daudt,R.C.;
Letts,A.;Jampani,V.;andRombach,R.2023. Stablevideo
andSchindler,K.2024. Repurposingdiffusion-basedimage
diffusion: Scaling latent video diffusion models to large
generatorsformonoculardepthestimation. InCVPR.
datasets. arXiv:2311.15127[cs.CV].
Kiefhaber, S.; Niklaus, S.; Liu, F.; and Schaub-Meyer,
Brooks, T.; Holynski, A.; and Efros, A. A. 2023. Instruct-
S. 2024. Benchmarking Video Frame Interpolation.
Pix2Pix: Learning to follow image editing instructions. In
arXiv:2403.17128[cs.CV].
CVPR,18392–18402.
Kim, Y.; Hwang, G.; and Park, E. 2024. Diffuse-
Cheng,X.;andChen,Z.2020.Videoframeinterpolationvia High: Training-free Progressive High-Resolution Image
deformableseparableconvolution. InAAAI,10607–10614. Synthesis through Structure Guidance. arXiv preprint
Choi, M.; Kim, H.; Han, B.; Xu, N.; and Lee, K. M. 2020. arXiv:2406.18459.
Channelattentionisallyouneedforvideoframeinterpola- Kingma, D. P.; and Ba, J. 2014. Adam: A method for
tion. InAAAI,10663–10671. stochasticoptimization. InICLR.
Danier,D.;Zhang,F.;andBull,D.2024. LDMVFI:Video Lee, H.; Kim, T.; Chung, T.-y.; Pak, D.; Ban, Y.; and Lee,
frame interpolation with latent diffusion models. In AAAI, S.2020. AdaCof:Adaptivecollaborationofflowsforvideo
1472–1480. frameinterpolation. InCVPR,5316–5325.
Ding,Z.;Zhang,M.;Wu,J.;andTu,Z.2024. Patchedde- Li, Z.; Zhu, Z.-L.; Han, L.-H.; Hou, Q.; Guo, C.-L.; and
noising diffusion models for high-resolution image synthe- Cheng,M.-M.2023. AMT:All-pairsmulti-fieldtransforms
sis. InICLR. forefficientframeinterpolation. InCVPR,9801–9810.
Liu, Y.; Lin, C.; Zeng, Z.; Long, X.; Liu, L.; Komura, T.;
Ge, S.; Nah, S.; Liu, G.; Poon, T.; Tao, A.; Catanzaro, B.;
and Wang, W. 2023. Syncdreamer: Generating multiview-
Jacobs, D.; Huang, J.-B.; Liu, M.-Y.; and Balaji, Y. 2023.
consistentimagesfromasingle-viewimage. arXivpreprint
Preserve your own correlation: A noise prior for video dif-
arXiv:2309.03453.
fusionmodels. InICCV,22930–22941.
Meyer,S.;Djelouah,A.;McWilliams,B.;Sorkine-Hornung,
Ho,J.;Saharia,C.;Chan,W.;Fleet,D.J.;Norouzi,M.;and
A.; Gross, M.; and Schroers, C. 2018. PhaseNet for video
Salimans,T.2022a. Cascadeddiffusionmodelsforhighfi-
frameinterpolation. InCVPR,498–507.
delityimagegeneration. JMLR,23(47):1–33.
Meyer,S.;Wang,O.;Zimmer,H.;Grosse,M.;andSorkine-
Ho,J.;andSalimans,T.2022.Classifier-freediffusionguid-
Hornung, A. 2015. Phase-based frame interpolation for
ance. arXivpreprintarXiv:2207.12598.
video. InCVPR,1410–1418.
Ho,J.;Salimans,T.;Gritsenko,A.;Chan,W.;Norouzi,M.;
Nam, J.; Lee, G.; Kim, S.; Kim, H.; Cho, H.; Kim, S.; and
and Fleet, D. J. 2022b. Video diffusion models. NeurIPS,
Kim, S. 2024. Diffusion Model for Dense Matching. In
35:8633–8646.
ICLR.
Hu,M.;Jiang,K.;Zhong,Z.;Wang,Z.;andZheng,Y.2024. Niklaus, S.; and Liu, F. 2018. Context-aware synthesis for
IQ-VFI: Implicit Quadratic Motion Estimation for Video videoframeinterpolation. InCVPR,1701–1710.
FrameInterpolation. InCVPR,6410–6419.
Niklaus, S.; and Liu, F. 2020. Softmax splatting for video
Hu,P.;Niklaus,S.;Sclaroff,S.;andSaenko,K.2022.Many- frameinterpolation. InCVPR,5437–5446.
to-manysplattingforefficientvideoframeinterpolation. In Niklaus,S.;Mai,L.;andLiu,F.2017a. Videoframeinter-
CVPR,3553–3562. polationviaadaptiveconvolution. InCVPR,670–679.
Huang,Z.;Zhang,T.;Heng,W.;Shi,B.;andZhou,S.2022. Niklaus,S.;Mai,L.;andLiu,F.2017b. Videoframeinter-
Real-time intermediate flow estimation for video frame in- polationviaadaptiveseparableconvolution. InICCV,261–
terpolation. InECCV,624–642. 270.
Ilg, E.; Mayer, N.; Saikia, T.; Keuper, M.; Dosovitskiy, A.; Niklaus,S.;Mai,L.;andWang,O.2021.Revisitingadaptive
and Brox, T. 2017. FlowNet 2.0: Evolution of optical flow convolutionsforvideoframeinterpolation.InWACV,1099–
estimationwithdeepnetworks. InCVPR. 1109.Park,J.;Kim,J.;andKim,C.-S.2023. BiFormer:Learning Skorokhodov,I.;Menapace,W.;Siarohin,A.;andTulyakov,
bilateralmotion estimationviabilateral transformerfor 4K S. 2024. Hierarchical Patch Diffusion Models for High-
videoframeinterpolation. InCVPR,1568–1577. ResolutionVideoGeneration. InCVPR,7569–7579.
Park, J.; Ko, K.; Lee, C.; and Kim, C.-S. 2020. BMBC: Sun, D.; Yang, X.; Liu, M.-Y.; and Kautz, J. 2018. PWC-
Bilateral motion estimation with bilateral cost volume for Net:CNNsforopticalflowusingpyramid,warping,andcost
videointerpolation. InECCV,109–125. volume. InCVPR,8934–8943.
Park,J.;Lee,C.;andKim,C.-S.2021.Asymmetricbilateral Teed,Z.;andDeng,J.2020.RAFT:Recurrentall-pairsfield
motion estimation for video frame interpolation. In ICCV, transformsforopticalflow. InECCV,402–419.
14539–14548. Xu, J.; Liu, S.; Vahdat, A.; Byeon, W.; Wang, X.; and
DeMello,S.2023.Open-vocabularypanopticsegmentation
Peebles, W.; and Xie, S. 2023. Scalable diffusion models
withtext-to-imagediffusionmodels. InCVPR,2955–2966.
withtransformers. InICCV,4195–4205.
Xue, T.; Chen, B.; Wu, J.; Wei, D.; and Freeman, W. T.
Plack, M.; Briedis, K. M.; Djelouah, A.; Hullin, M. B.;
2019. Video enhancement with task-oriented flow. IJCV,
Gross, M.; and Schroers, C. 2023. Frame Interpolation
127:1106–1125.
Transformer and Uncertainty Guidance. In CVPR, 9811–
9821. Yang, S.; Hwang, H.; and Ye, J. C. 2023. Zero-shot con-
trastive loss for text-guided diffusion image style transfer.
Qian, G.; Mai, J.; Hamdi, A.; Ren, J.; Siarohin, A.; Li, B.;
InICCV,22873–22882.
Lee, H.-Y.; Skorokhodov, I.; Wonka, P.; Tulyakov, S.; and
Ghanem, B. 2024. Magic123: One Image to High-Quality Zhang,G.;Zhu,Y.;Wang,H.;Chen,Y.;Wu,G.;andWang,
3DObjectGenerationUsingBoth2Dand3DDiffusionPri- L.2023. Extractingmotionandappearanceviainter-frame
ors. InICLR. attention for efficient video frame interpolation. In CVPR,
5682–5692.
Ranjan,A.;andBlack,M.J.2017. OpticalFlowEstimation
Zheng,Q.;Guo,Y.;Deng,J.;Han,J.;Li,Y.;Xu,S.;andXu,
usingaSpatialPyramidNetwork. InCVPR.
H. 2024. Any-size-diffusion: Toward efficient text-driven
Reda, F.; Kontkanen, J.; Tabellion, E.; Sun, D.; Pantofaru,
synthesisforany-sizeHDimages. InAAAI,7571–7578.
C.; and Curless, B. 2022. FILM: Frame interpolation for
Zhou,K.;Li,W.;Han,X.;andLu,J.2023. Exploringmo-
largemotion. InECCV,250–266.
tion ambiguity and alignment for high-quality video frame
Rombach,R.;Blattmann,A.;Lorenz,D.;Esser,P.;andOm- interpolation. InCVPR,22169–22179.
mer, B. 2022. High-resolution image synthesis with latent
diffusionmodels. InCVPR,10684–10695.
Saharia,C.;Chan,W.;Saxena,S.;Li,L.;Whang,J.;Denton,
E. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan,
B.;Salimans,T.;Ho,J.;Fleet,D.J.;andNorouzi,M.2022.
Photorealistictext-to-imagediffusionmodelswithdeeplan-
guageunderstanding. NeurIPS,36479–36494.
Salimans, T.; and Ho, J. 2022. Progressive Distillation for
FastSamplingofDiffusionModels. InICLR.
Saxena, S.; Herrmann, C.; Hur, J.; Kar, A.; Norouzi, M.;
Sun,D.;andFleet,D.J.2023a. Thesurprisingeffectiveness
of diffusion models for optical flow and monocular depth
estimation. InNeurIPS.
Saxena,S.;Hur,J.;Herrmann,C.;Sun,D.;andFleet,D.J.
2023b. Zero-ShotMetricDepthwithaField-of-ViewCon-
ditionedDiffusionModel. arXiv:2312.13252[cs.CV].
Shi, S.; Li, W.; Zhang, Y.; He, J.; Gong, B.; and Zheng, Y.
2024a. ResMaster:MasteringHigh-ResolutionImageGen-
eration via Structural and Fine-Grained Guidance. arXiv
preprintarXiv:2406.16476.
Shi,Y.;Wang,P.;Ye,J.;Mai,L.;Li,K.;andYang,X.2024b.
MVDream: Multi-view Diffusion for 3D Generation. In
ICLR.
Shi, Z.; Xu, X.; Liu, X.; Chen, J.; and Yang, M.-H. 2022.
Video frame interpolation transformer. In CVPR, 17482–
17491.
Sim, H.; Oh, J.; and Kim, M. 2021. XVFI: extreme video
frameinterpolation. InICCV,14489–14498.Appendix Table6:Effectofdifferentpatchsize:Theusageofdiffer-
entpatchsizesdoesnotshowsignificantaccuracydifference
Overview onX-TEST4Kdataset.
Thanks for viewing the appendix, which provides fur- Patchsize PSNR SSIM
ther implementation details, analysis of dropout for patch-
256×384 32.70 0.931
based cascade training, and further discussions on results
384×576 32.82 0.932
on Vimeo-90K and SEPE 8K benchmark datasets. We also
512×768 32.92 0.931
provide more qualitative comparison including interactive
640×960 32.79 0.930
tools and videos with state-of-the-art methods in our web- 768×1152 32.75 0.930
site. For more details, please browse our project webpage:
https://hifi-diffusion.github.io
Implementationdetails
We include further implementation details continuing from
themainpaper.OurimplementationisbasedonJAXframe-
work. We use a fixed random seed for reproducibility. We
use 256 TPUv5e with 16 GB memory for training. For in-
ference, our model runs on one A100 40GB and processes
up to 8K resolution without a memory problem. Our novel
HiFIcascadeenablesthishigh-resolutionprocessingwhere
mostofthemethodshavedifficultiesin.
Effectofthepatchsize Overlaidinputs Intermediate
We provide an analysis on how patch size in the cascade
modelaffectstheaccuracyduringinference.Duetotheself-
attentionlayersattwobottomlevels,theperformanceofour
methodcouldvary,whenapatchsizethatisdifferentfrom
thetrainingresolution(i.e.,224 288)isused.Also,bigger
×
patch sizes can give better accuracy due to a larger context
window, but it is not so clear if it always holds. Given our
standardsetup(i.e.,athree-stagecascade,8samplingsteps,
an average of 4 samples), we try different patch size and
evaluateonX-TEST4Kdataset(Sim,Oh,andKim2021).
Table6reportsPSNRandSSIMonX-TEST4Kdataset.
Althoughthetrainingresolutionisat224 288,themethod W/odropout W/dropout
×
isnotverysensitivetothechoiceofpatchsizeatinference
Figure 8: Dropout forces the network to use both the low
time.Thesmallestandthebiggestpatchsize(i.e.,256 384
× resolutionintermediateandtheinputs.Withoutdropout,the
and 768 1152) show marginal difference in both PSNR
× networkjustupsamplestheintermediateandmissesdetails.
andSSIMmetrics.Thepatchsize512 768givesthebest
× Eachsecondrowshowscloseviewsofhighlightedareasin
accuracyonX-TEST4K.
eachfirstrow.
Dropoutforpatch-basedcascadetraining
andseethemodels’behaviorbyinputtingadifferentimage
We find that the image-level dropout is crucial for making
asthelowresolutionintermediateduringtheinference.We
the patch-based cascade model behave as intended, espe-
testatwo-stagecascademodelasinFig.9c.
ciallyforchallenginglargemotionscenes,asinFig.8.With-
outdropout,themodelfindsashortcut,sharpeningthelow The model with dropout (i.e., Fig. 9d) successfully out-
resolution intermediate from the base model without refer- puts high-resolution prediction when actual low resolution
ringtoinputimages.Thisresultsinlosingfinedetails,e.g., intermediatesareinputted.Whenadifferentintermediateis
thin structures and letters. With dropout, the model refers inputted, the model tries to add appearance (e.g., color or
to both the low resolution intermediate for coarse structure texture) from input frames on top of object structures from
and the high-resolution input for fine details. In this study, the low resolution intermediate. Though it produces non-
wetrainthemodelonX-TRAIN(Sim,Oh,andKim2021) sensibleoutput,thisprovesthatthemodelisabletoexploit
only,asthenetworkwithoutdropoutdoesnotconvergewith bothinputframeandlowresolutionintermediateduringthe
a full training dataset. This suggests that the dropout also inference.
stabilizeslargescaletrainingforthecascademodel. On the other hand, the model trained without dropout
To further analyze the effect of the image-level dropout, (i.e.,Fig.9e)onlyreferstothelowresolutionintermediate.
wepreparetwomodelsthatarewithorwithoutthedropout EvenwheninputtingadifferentimageasthelowresolutionNoise image
Downsampled Base model Stackable model
frames
UpsampledˆI<latexit sha1_base64="50HAbLDX44rTD3ymoHq3OOrw054=">AAACAXicbVDLSsNAFJ3UV62vqBvBzWARXNWkSHVZdKO7CvYBTQyT6aQdOpmEmYlQQtz4K25cKOLWv3Dn3zhps9DWAxcO59zLvff4MaNSWda3UVpaXlldK69XNja3tnfM3b2OjBKBSRtHLBI9H0nCKCdtRRUjvVgQFPqMdP3xVe53H4iQNOJ3ahITN0RDTgOKkdKSZx44I6RSJ0Rq5AfpTZbdp/ZpPfNsz6xaNWsKuEjsglRBgZZnfjmDCCch4QozJGXftmLlpkgoihnJKk4iSYzwGA1JX1OOQiLddPpBBo+1MoBBJHRxBafq74kUhVJOQl935qfKeS8X//P6iQou3JTyOFGE49miIGFQRTCPAw6oIFixiSYIC6pvhXiEBMJKh1bRIdjzLy+STr1mN2qN27Nq87KIowwOwRE4ATY4B01wDVqgDTB4BM/gFbwZT8aL8W58zFpLRjGzD/7A+PwBN3yWvg==</latexit>1 1/2
Noisy inputz<latexit sha1_base64="l4mIDscYXa+Yfc8n9JqUIeztc3U=">AAAB+XicbVBPS8MwHP11/pvzX9Wjl+IQPI1WZHrwMPDicYKbg62UNE23sDQpSTqYZd/EiwdFvPpNvPltTLcedPNByOO934+8vDBlVGnX/bYqa+sbm1vV7drO7t7+gX141FUik5h0sGBC9kKkCKOcdDTVjPRSSVASMvIYjm8L/3FCpKKCP+hpSvwEDTmNKUbaSIFtD0LBIjVNzJU/zQId2HW34c7hrBKvJHUo0Q7sr0EkcJYQrjFDSvU9N9V+jqSmmJFZbZApkiI8RkPSN5SjhCg/nyefOWdGiZxYSHO4dubq740cJaoIZyYTpEdq2SvE/7x+puNrP6c8zTThePFQnDFHC6eowYmoJFizqSEIS2qyOniEJMLalFUzJXjLX14l3YuG12w07y/rrZuyjiqcwCmcgwdX0II7aEMHMEzgGV7hzcqtF+vd+liMVqxy5xj+wPr8AWY/lCg=</latexit> t
(a)Inputoverlay
Upsampled low resolution Denoised predictionˆI<latexit sha1_base64="eMjKR5v1GK+iMKkvicfniGDXBHg=">AAAB+3icbVDLSsNAFJ3UV62vWJdugkVwVRKR6rLoRncV7AOaECbTSTt0MgkzN2IJ+RU3LhRx64+482+ctFlo64GBwzn3cs+cIOFMgW1/G5W19Y3Nrep2bWd3b//APKz3VJxKQrsk5rEcBFhRzgTtAgNOB4mkOAo47QfTm8LvP1KpWCweYJZQL8JjwUJGMGjJN+vuBEPmRhgmQZjd5bnv+GbDbtpzWKvEKUkDlej45pc7ikkaUQGEY6WGjp2Al2EJjHCa19xU0QSTKR7ToaYCR1R52Tx7bp1qZWSFsdRPgDVXf29kOFJqFgV6sgiplr1C/M8bphBeeRkTSQpUkMWhMOUWxFZRhDVikhLgM00wkUxntcgES0xA11XTJTjLX14lvfOm02q27i8a7euyjio6RifoDDnoErXRLeqgLiLoCT2jV/Rm5MaL8W58LEYrRrlzhP7A+PwBRC+Umg==</latexit> 1
Intermediate ˆI<latexit sha1_base64="50HAbLDX44rTD3ymoHq3OOrw054=">AAACAXicbVDLSsNAFJ3UV62vqBvBzWARXNWkSHVZdKO7CvYBTQyT6aQdOpmEmYlQQtz4K25cKOLWv3Dn3zhps9DWAxcO59zLvff4MaNSWda3UVpaXlldK69XNja3tnfM3b2OjBKBSRtHLBI9H0nCKCdtRRUjvVgQFPqMdP3xVe53H4iQNOJ3ahITN0RDTgOKkdKSZx44I6RSJ0Rq5AfpTZbdp/ZpPfNsz6xaNWsKuEjsglRBgZZnfjmDCCch4QozJGXftmLlpkgoihnJKk4iSYzwGA1JX1OOQiLddPpBBo+1MoBBJHRxBafq74kUhVJOQl935qfKeS8X//P6iQou3JTyOFGE49miIGFQRTCPAw6oIFixiSYIC6pvhXiEBMJKh1bRIdjzLy+STr1mN2qN27Nq87KIowwOwRE4ATY4B01wDVqgDTB4BM/gFbwZT8aL8W58zFpLRjGzD/7A+PwBN3yWvg==</latexit> 1 / 2
1
Input frames I<latexit sha1_base64="4PyLlaUBdIaPiGR+HeeaIyhQ4ds=">AAACA3icbVDLSsNAFL2pr1pfUXe6GSyCCylJkeqy6EZ3FewD2hAm00k7dPJgZiKUUHDjr7hxoYhbf8Kdf+OkzaK2Hhg4c8693HuPF3MmlWX9GIWV1bX1jeJmaWt7Z3fP3D9oySgRhDZJxCPR8bCknIW0qZjitBMLigOP07Y3usn89iMVkkXhgxrH1AnwIGQ+I1hpyTWPegFWQ89P7yaudY7mflXXLFsVawq0TOyclCFHwzW/e/2IJAENFeFYyq5txcpJsVCMcDop9RJJY0xGeEC7moY4oNJJpzdM0KlW+siPhH6hQlN1viPFgZTjwNOV2Y5y0cvE/7xuovwrJ2VhnCgaktkgP+FIRSgLBPWZoETxsSaYCKZ3RWSIBSZKx1bSIdiLJy+TVrVi1yq1+4ty/TqPowjHcAJnYMMl1OEWGtAEAk/wAm/wbjwbr8aH8TkrLRh5zyH8gfH1Czk4l0Q=</latexit> 0,I2
(b)Groundtruth (c)Modeldiagram
Oracle Different Oracle Different
(d)Modelwithdropout (e)Modelwithoutdropout
Figure 9: Effect of dropout. In the two-stage cascade formulation, we train our cascade model with or without dropout and
visualize results when inputting oracle/different low resolution intermediate respectively. These inputs are downsampled and
upsampledbacktotheoriginalresolutiontomimicthelowresolutionintermediatefromthepreviouslevel.(d)Withdropout
themodeleffectivelyutilizescoarsestructurefromtheintermediateandfinedetailsfromthehighresolutioninput.Thisholds
trueevenwithdifferentlowresolutionintermediate:themodeladdcolorandtexturetothecoarsestructure.(e)Ontheother
hand,themodelwithoutdropoutsolelyreliesontheintermediate,primarilysharpeningit.Thisleadstoalossoffinedetails,
e.g.aroundobjectboundaries(seetheerrormap).Thebehaviorbecomeslookingmoreapparentwithadifferentintermediate;
themodelignoresinputframes.
condition (e.g., the right column in Fig. 9e), the denoised DiscussiononVimeo-90K
prediction completely ignores the input frames and takes a
shortcut to sharpen the low resolution condition (i.e., tree While HiFI achieves the best accuracy on multiple high
image). Our probabilistic image-level dropout prevents the resolution benchmark datasets, it performs comparably on
model from taking this shortcut and learns to refer to both highly-saturated Vimeo-90K benchmark (Xue et al. 2019).
inputandconditioncues. To analyze the behavior, in Fig. 10 we show random-
sampled results of our method that gives high errors on
Vimeo-90K. As visualized in the error map, erroneous
prediction mostly arises from motion boundaries even if
ser-woL
desioneD
pamrorrE
etaidemretni
noitciderpInputoverlay Ours Groundtruth Errormap
Figure 10: Error map visualization on Vimeo-90K: We randomly sample ours results with high errors on Vimeo-90k and
visualize them with input overlay, ground truth, and error map. The error mostly originates from motion boundaries where
thepredictedobjects’motionsometimesdonotalignwellwithtruemotion.ThisisbecauseVimeo-90Kdatasetpreferslinear
motionpredictionwhereasourmodelcanpredictplausiblenon-linearmotion(Kiefhaberetal.2024).
HiFI is able to interpolate frames with fine details, as dictionyieldsasubtlemisalignmentbetweenpredictedposi-
showninthesecondcolumn.Thisisspecificallyduetothe tionandtruepositionofobject,anditcausesintensitydiffer-
dataset property: Vimeo-90K prefers linear motion predic- encemostlynearimageedges.Thuserrorsmostlyarisenear
tion(Kiefhaberetal.2024)whereasourmodelpredictsdi- objectormotionboundaries.InFig.11b,wevisualizemul-
verse non-linear motion of objects. Non-linear motion pre- tiplenon-linearmotionexamplesthatourmodelproduces.(a)Twoinputframes(above)andgroundtruth(below) (b)Multiplesampleswith64samplingsteps
Average
sample
Error
map
Variance
map
Sampling
1 4 8 16 64
steps
(c)Averagesample,errormap,andvariancemapw.r.t.thedifferentnumberofsamplingsteps.
Figure11:Effectofthenumberofsamplingsteps:Wevisualizehowthenumberofsamplingstepsaffectsresults.(a)Given
two input frames, we try different sampling steps and visualize (b) each individual sample as well as (c) averaged sample,
errormap,andvariancemap.Withmoresamplingsteps,themodelpredictsmultipleplausiblediversesampleswithnon-linear
motion(i.e.,thefastmovingstick).
Effectofthenumberofsamplingsteps Table 7: Results on SEPE 8K dataset. HiFI outperforms
M2M(Huetal.2022).MostofthemethodshavetheOOM
In Fig. 11, we visualize how the number of sampling steps
(outofmemory)problemat8Kresolution.
affects results. The input frame in Fig. 11a shows a person
using a stick, and the stick moves fast between the frames.
Method PSNR SSIM
The variance map in Fig. 11c shows that with more sam-
pling steps, the model outputs more diverse motion of the LDMVFI(Danier,Zhang,andBull2024) OOM
stick and the hand of the person, as highlighted in the var- EMA-VFI(Zhangetal.2023) OOM
UPR-Net(Jinetal.2023) OOM
ious map. With the lower number of sampling steps, our
BiFormer(Park,Kim,andKim2023) OOM
model produces close-to-mean prediction. Fig. 11b visual-
M2M(Huetal.2022) 28.34 0.883
izes the four samples drawn from 64 sampling steps; our
HiFI(Ours) 29.78 0.900
model predicts diverse, plausible samples with non-linear
motion,suchasindifferenttrajectoriesoratdifferentaccel-
eration and deceleration rate. This follows the same obser- such as video quality assessment, super-resolution, com-
vationfromDDVM(Saxenaetal.2023a)thatthediffusion pression,etc.Toutilizethedatasetforbenchmarkingframe
modelisabletopredictplausiblemulti-modesamples. interpolation methods, especially for high resolution with
large motion, we select a triple of frames (145th, 150th,
EvaluationonSEPE8Kbenchmark and 155th) from each video and target to predict the mid-
dleframefromtheresttwoframesasinput.
SEPE 8K dataset (Al Shoura et al. 2023) provides 40 raw
videos with 300 frames, 8K resolution, and 29.97 FPS for Table7includesallstate-of-the-artmethodsthatwecom-
benchmarking various downstream computer vision tasks pareonSEPE8Kbenchmark.ExceptM2M,theothermeth-Inputoverlay M2M HiFI(Ours) GroundTruth
Figure12:QualitativecomparisononSEPE8K:WeprovidequalitativecomparisonbetweenourmethodandM2M(Huetal.
2022) which is able to run at 8K resolution except other methods. Compared to M2M (Hu et al. 2022) having a difficulty in
handlinglargemotion,ourmethodisabletorecoverfinedetailsonchallengingcasesat8Kresolution.
ods are not able to process 8K resolution image due to the showsafewexamplesfromtheSEPE8Kdataset.Incaseof
out-of-memory(OOM)problemonA10040GBGPU.Fig- fluidmotioninthefirstrow,HiFItendstooutputblurryre-
ure12 providequalitativecomparison betweenour method sults;moregenerativesolutioncanbepreferred.Alsoasin
and M2M (Hu et al. 2022). Unlike M2M (Hu et al. 2022), thesecondrow,whenmotionisextremelylarger(e.g.1500
ourmethodisabletorecoverfinedetailsonsuchchalleng- pixels) than the patch size, most of the content goes out of
inglargemotioncasesevenat8Kresolution. image boundary. HiFI cannot establish reliable correspon-
dence,andthusisnotabletointerpolatetheirmotion.
Challenges
Our method achieves the state of the art on multiple high-
resolutionbenchmarkdatasets,yettherestillexistssomeex-
tremelydifficultcasesthatchallengeourmethod.Figure13Groundtruth HiFI(Ours)
Inputoverlay Groundtruth HiFI(Ours)
closeview closeview
Figure13:Someextremelychallengingcasesthatevenourmodelstruggleswith:(above)fluidmotionwheremoregenerative
solutioncanbepreferredor(below)extremelylargemotion,around1500pixelsintheexample,thatismuchbiggerthanthe
patchsizeatinferencetime.