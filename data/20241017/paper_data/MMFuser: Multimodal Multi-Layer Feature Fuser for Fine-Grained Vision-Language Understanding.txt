1
MMFuser: Multimodal Multi-Layer Feature Fuser
for Fine-Grained Vision-Language Understanding
Yue Cao, Yangzhou Liu, Zhe Chen, Guangchen Shi, Wenhai Wang, Danhuai Zhao, and Tong Lu
Abstract—Despite significant advancements in Multimodal Cosine Similarity 1.000
Large Language Models (MLLMs) for understanding complex 1.00 0.896
human intentions through cross-modal interactions, capturing 0.704
intricate image details remains challenging. Previous methods 0.68
0.551
integrating multiple vision encoders to enhance visual detail
0.425
introduce redundancy and computational overhead. We observe
0.39 0.318
that most MLLMs utilize only the last-layer feature map of
0.221
the vision encoder for visual representation, neglecting the
rich fine-grained information in shallow feature maps. To ad- 0.15 0.0820.133
dress this issue, we propose MMFuser, a simple yet effective 0.0020.012 -0.013-0.002-0.0040.0020.0140.0220.032
multi-layer feature fuser that efficiently integrates deep and
-0.05
shallow features from Vision Transformers (ViTs). Specifically,
it leverages semantically aligned deep features as queries to
dynamically extract missing details from shallow features, thus (a) Cosine similarity between different feature maps.
preserving semantic alignment while enriching the representa-
tion with fine-grained information. Applied to the LLaVA-1.5
model, MMFuser achieves significant improvements in visual
representation and benchmark performance, providing a more
flexible and lightweight solution compared to multi-encoder Input Image CLIP (L23) ConeNeXt DINOv2 EVA02 SigLIP
ensemble methods. The code and model have been released at
https://github.com/yuecao0119/MMFuser.
Index Terms—Multimodal large language model, visual per-
ception, feature fusion, Transformer. CLIP (L2) CLIP (L6) CLIP (L10) CLIP (L14) CLIP (L18) CLIP (L22)
(b) Visualization of different feature maps.
I. INTRODUCTION Fig.1. Comparisonoffeaturemapsfromdifferentvisionencodersandvarious
layers of CLIP-ViT. (a) Cosine similarity is computed between the feature
IN recent years, Multimodal Large Language Models maps from various vision encoders, including CLIP-ViT-L [17], ConvNeXt-
(MLLMs) [1]–[7] have emerged as a research hotspot in XXL[18],DINOv2-L[19],EVA02-L[20],andSigLIP-L[21],andthefinal-
layerfeaturemapofCLIP-ViT-L.(b)Visualizationofdifferentfeaturemaps.
thefieldofArtificialGeneralIntelligence(AGI).Thesemodels
Theseresultsindicatesignificantfeaturedifferencesnotonlybetweenvarious
have made significant strides in understanding and expressing visionencodersbutalsoacrossdifferentlayerswithinthesamevisionencoder.
complex human intent through cross-modal interaction and Thisobservationmotivatesustofullyexplorethepotentialofindividualvision
encodersfordevelopingMLLMs.
learning. Building on rapid advancements in Large Language
Models (LLMs) [8]–[16], MLLMs utilize pre-trained vision
encoders to extract image features and integrate them with
advancedLLMs,demonstratingremarkablecapabilitiesacross encoder, feeding the outputs from its final or penultimate
a wide range of vision-language tasks. layer into LLMs as visual representations. In this manner,
Currently,themainstreamapproach[2]–[5],[22]–[24]inthe these features with rich high-1level semantic information are
community involves using a pre-trained Vision Transformer effectivelytransformedfromanimagespacetoasemantictext
(ViT) [17], [25] or its variants [18], [19], [26] as the vision space.However,duetothelossoflow-levelimageinformation
in deep features, current MLLMs encounter challenges in ac-
This work was supported in part by the China Mobile Zijin Innovation curatelyinterpretingdetails,resultinginissuessuchasOptical
Insititute under Grant NR2310J7M, in part by the National Natural Science
CharacterRecognition(OCR)errorsandobjecthallucinations.
FoundationofChinaunderGrant62372223,andinpartbytheYouthStudent
BasicResearchProjectoftheNationalNaturalScienceFoundationforPhD To address these issues, recent studies [24], [27] indicate
Students under Grant 623B2050. (Yue Cao and Yangzhou Liu are co-first
that the ability of vision encoders to learn fine-grained image
authors.)(Correspondingauthor:TongLu.)
Yue Cao, Yangzhou Liu, Zhe Chen, Guangchen Shi, and features has become a bottleneck for MLLMs. Consequently,
Tong Lu are with the State Key Laboratory for Novel Software someresearchers[15],[28]–[30]believethatsolelyrelyingon
Technology, Nanjing University, Nanjing 210023, China, (e-mail:
features of a single vision encoder may not be optimal. They
caoyue0119@gmail.com; lyzlll2343@gmail.com; czcz94cz@gmail.com;
guangchenshi@smail.nju.edu.cn;lutong@nju.edu.cn). propose integrating multiple pre-trained vision encoders, such
Wenhai Wang is with the Chinese University of Hong Kong, Hong Kong as CLIP [17], DINOv2 [19], and ConvNext [18], to enhance
999077,China,(e-mail:wangwenhai362@163.com).
fine-grained visual representations in a complementary man-
DanhuaiZhaoiswiththeChinaMobileZijinInnovationInsititute,Nanjing
211899,China,(e-mail:zhaodanhuai@chinamobile.com). ner. While these ensemble-based methods achieve promising
4202
tcO
51
]VC.sc[
1v92811.0142:viXra
tXeNvnoC 2vONID 20AVE PILgiS 2L PILC 3L PILC 4L PILC 5L PILC 6L PILC 8L PILC 01L PILC 21L PILC 41L PILC 61L PILC 81L PILC 02L PILC 22L PILC 32L PILC2
Qwen-VL-Chat LLaVA-1.5-7B LLaVA-1.5-7B + MMFuser InstructBLIP LLaVA-1.5-13B LLaVA-1.5-13B + MMFuser
VQA-v2 VQA-v2
79.1
80
GQA MMVet GQA MMVet
62.8 78.278.5 32.6 63.3 80.1
62 63.4 35.436.6
30.5
Vizwiz 53.4 57.5 65.5LLaVA-Wild Vizwiz 575.34.6 70.7 71.8LLaVA-Wild
50 63.4
49.5
68.7 38.9 60.8 71.6 33.425.6 58.2 61.6
SQA-IMG68.2 66.8 58.2 58.6 SEED SQA-IMG 71.2 63.1 62 SEED
50.7 1212.8
58.8 56.7
58.2 60.6 586.30.1 61.3 78.9 63.6
TextVQA61.5 MMB-CN TextVQA 59.9 63.8MMB-CN
6 64 7.3 .5 85.9 67.7
86.3 1479.7 1531.3
85.9 1487.5 87.5 69.9
POPE 1510.7 MMB POPE 1585.2 MMB
MME MME
(a) Comparison of 7B models (b) Comparison of 13B models
Fig.2. Performancecomparisonacrossdifferentmodelsizes.(a)Among7Bmodels,includingQwen-VL-Chat[5],LLaVA-1.5-7B[3],ourmodelsurpasses
LLaVA-1.5-7B on 11 out of 12 benchmarks, with an average score of 61.8 compared to LLaVA-1.5-7B’s 60.3. (b) Among 13B models, including
InstructBLIP [1] and LLaVA-1.5-13B [3], our model also outperforms LLaVA-1.5-13B on 10 out of 12 benchmarks, achieving an average score of 64.1
comparedtoLLaVA-1.5-13B’s63.2.TheseresultsindicatethatMMFusercaneffectivelyimprovetheperformanceofLLaVA-1.5models.
results, they unfortunately introduce model redundancy and features. This insight inspired us to propose a simple yet
increase computational overhead. Therefore, the necessity of effective method called MMFuser (see Fig. 3), which uses
employing multiple vision encoders remains contentious. deepfeaturesasqueriestodynamicallyextractmissingdetails
As a matter of fact, even with a single vision encoder, fromshallowfeatures.Itminimizestheriskofshallowfeatures
learned visual representations are diverse, as shown in Fig. 1. disrupting semantic alignment, maintaining the coherence of
The visualizations indicate that deep features are effective at deep features while enriching them with fine-grained infor-
extracting high-level semantic information, whereas shallow mation. By leveraging multi-layer features, MMFuser can
features are better suited for capturing low-level details such enhance the overall performance of MLLM in processing
as edges and textures, which have not been fully leveraged in images and videos.
current MLLMs. Looking back at classic image and video To validate the effectiveness of MMFuser, we applied it to
tasks such as object detection and semantic segmentation, the recent well-known model, LLaVA-1.5 [3]. As shown in
multi-layer features are widely used [31]–[35], where the Fig. 2, our MMFuser significantly enhances the visual repre-
combination of shallow and deep visual features provides sentations input into the MLLM, thereby improving LLaVA-
a more comprehensive understanding of images or videos. 1.5’s performance on most multimodal benchmarks. Specifi-
However,theapplicationofmulti-layerfeaturesinthismanner cally,our7BmodeloutperformedLLaVA-1.5-7Bin10outof
remains relatively uncommon within the field of MLLMs. 12 benchmarks, and our 13B model outperformed LLaVA-
It is natural to extend this idea to the visual representation 1.5-13B in 10 out of 12 benchmarks. Besides, our model
ofMLLMs.Wehaveexperimentedwithsomestraightforward demonstratedsuperiorperformanceinfine-grainedrecognition
methods to combine multiple layers of features from a single tasks, including OCR and visual grounding.
ViT [17] for enhancing image detail, such as element-wise In summary, our main contributions are as follows:
averagingorchannel-wiseconcatenation.However,thesesim- • We reveal that the expressive potential of single vision
ple fusion methods have a little improvement in performance. encoders in MLLMs is underutilized. Shallow features,
Through further exploration, we find that while shallow fea- rich in detail, suffer from poor semantic alignment with
turescapturericherlow-leveldetails,theirsemanticalignment text features, indicating that simple fusion methods are
with the text feature space is insufficient. This deficiency inadequate and require more advanced design.
impairs the model’s ability to understand images and offsets • We introduce MMFuser, which enhances the visual rep-
the benefits introduced by combining shallow and deep visual resentationsofasinglevisionencoderbydynamicallyin-
features, indicating that multi-layer feature fusion in MLLMs tegratingfine-graineddetailsfromshallowfeatureswhile
is vital and requires more thoughtful designs. maintaining the semantic coherence of deep features.
Further, previous work [36] suggests that LLMs excel in • Applying MMFuser to LLaVA-1.5 models, we achieve
understanding deep features of ViTs, which are fully aligned significant performance improvements. Our 13B model
with text feature spaces. Conversely, while shallow features surpasses LLaVA-1.5 by 3.8, 53.9, and 2.2 points on the
are rich in details, they exhibit poor semantic alignment, VizWiz, MME, and MMBench-EN, respectively, demon-
making it challenging for LLMs to effectively interpret these strating the efficacy of our method.3
Additionally, other vision foundation models are employed
… … …
to construct MLLMs. For instance, CogVLM [41] leveraged
Fusion Module 𝑄 the pre-trained EVA2-CLIP-E [50] model for visual repre-
Layer 𝑛
Layer 𝑛 sentations. ConvLLaVA [26] incorporated ConvNeXt [18], a
… Layer 𝑛 Layer 𝑛 … 𝐾/𝑉 re su convolution-based hierarchical model, as its vision encoder.
Layer 2 Layer 𝑚 …
FM
M
In DeepSeek-VL [15], SigLIP [21] and SAM-ViT [51] were
… … … 𝐾/𝑉 utilized as vision encoders. Furthermore, InternVL [4], [6]
Layer 1 Layer 1 Layer 1 Layer 1 employed InternViT-6B, a vision foundation model trained
on web-scale image-text data. These works typically use the
Vision Encoder Vision Encoder Vision Encoder Vision Encoder
feature map from the final layer of the vision encoder as the
(a) (b) (c)
visual representation, as shown in Fig. 3 (a). In contrast, our
Fig.3. Previousmethodsvs.theproposedMMFuser.(a)Previousmethods approachaimstoexplorethepotentialbenefitsofusingfeature
typicallyutilizevisualfeaturesfromthefinalorpenultimatelayerofthevision maps from the shallow and intermediate layers of the vision
encoder. For example, the LLaVA series [2], [3] adopted this approach. (b)
encoder for vision-language tasks.
Somemodelsintegratevisualfeaturesfrommultiplevisionencoders,suchas
MouSi[47],DeepSeek-VL[15],andLLaVA-HR[28].(c)OurMMFuserfuses
visual features from different layers of a single vision encoder, providing
richerdetailandbettersemanticalignmentwithtext. C. Enhanced Visual Representation in MLLMs
Many works are dedicated to enhancing visual representa-
II. RELATEDWORK tions in MLLMs, including:
1) Scaling Up the Vision Encoder: PaLI [48] increased the
A. Multimodal Large Language Model
parameters of its vision encoder to 4 billion. In PaLI-17B,
Multimodal Large Language Models (MLLMs) integrate
the vision encoder, ViT-e, accounts for approximately 25% of
visualrepresentationsfromimageswithlinguisticembeddings
the total parameters. InternVL [4] scaled its vision foundation
from text, thereby enhancing the models’ capabilities in com-
model to 6 billion parameters, progressively aligning it with a
prehending and generating language descriptions of visual
large language model. PaLM-E [52] achieved a scale of 562
content. Most open-source MLLMs employ architectures that
billion parameters by integrating the 540 billion parameter
include a pre-trained vision encoder, an LLM, and a cross-
PaLM [53] LLM with the ViT-22B [54].
modal connector. Early models, such as the BLIP series [1],
2) Integrating Multiple Vision Encoders: As shown in
[36], utilized the Q-Former module to align text and images,
Fig. 3 (b), this method enhances visual representations by in-
thus improving multimodal capabilities. Flamingo [37] em-
tegrating multiple vision encoders. For example, MMVP [27]
ployed a gated cross-attention mechanism to integrate images
employed a Mixture of Features (MoF) approach to integrate
andtext.LLaVA-1.5[3]adoptedanMLPprojectortoconnect
image features from CLIP-ViT and DINOv2 [19]. Notably,
thepre-trainedvisionencoderwiththeLLM.InternVL[4],[6]
DINOv2 was a self-supervised vision model trained without
employed a dynamic resolution strategy, segmenting images
any language guidance. Similarly, MouSi [47] utilized an
into tiles and encoding both the tiles and the thumbnail view
ensemble technique to synergize the capabilities of individual
together. It then uses a pixel shuffle operation to reduce the
vision encoders. This method introduced a fusion network to
number of visual tokens before integrating these features with
unifytheprocessingofoutputsfromdifferentvisionencoders,
the LLM through an MLP projector.
including CLIP, DINOv2, and SAM [51]. LLaVA-HR [28]
Additionally, private MLLMs such as the Gemini se-
integratedimagefeaturesfromtwovisionencodersofdifferent
ries [38], [39], GPT-4V [7], and Claude-3V series [40], along
resolutions: 336px from CLIP-ViT and 1024px from CLIP-
withopen-sourceMLLMslikeMiniGPT-4[23],Qwen-VL[5],
ConvNeXt[18].Thisapproachleveragedthestrengthsofboth
CogVLM [41], the VisionLLM series [42], [43], and the All-
resolution inputs to enhance visual understanding. DeepSeek-
Seeingseries[44],[45],amongothers[46],havedemonstrated
VL [15] employed a hybrid vision encoder design, effec-
robust multimodal capabilities. These models exemplify the
tively encoding images by combining SigLIP-L [21] for low-
powerful ability of MLLMs to understand, generalize, and
resolution inputs and SAM-B [51] for high-resolution inputs.
reason with multimodal information, consistently setting new
3) Feature Fusion: MEP3P [55] enhanced the original
benchmarks in multimodal tasks.
visual features input into MLLMs with image depth features
and pseudo-3D positions. VCMR [56] utilized deformable at-
B. Vision Encoder in MLLMs
tention to process multi-granularity image features and obtain
The vision encoder plays a pivotal role in MLLMs, where fine-grained information, thereby improving performance in
notable models like CLIP-ViT [17] have been widely used in subsequent cross-modal tasks. Our concurrent work, Dense
this field. CLIP [17] leveraged contrastive learning on large- Connector [57], integrated features from multiple layers, en-
scale image-text pairs for pre-training, resulting in a vision riching the visual inputs for MLLMs by capturing multi-level
encoder that learns rich and general visual representations. representations from the vision encoder.
This capability enhances the understanding of the relationship Overall, these methods demonstrated significant perfor-
between vision and language. Several models, including the mance boosts for MLLMs. However, the potential of a single
LLaVA series [2], [3], PaLI [48], and Qwen-VL [5], adopted vision encoder remained underexplored. To address this, we
CLIP-ViT [17], [49] as their default vision encoder. proposed MMFuser, to integrate feature maps from multiple4
Vision Encoder
<image>
❄ How many small balls are there in the picture, with
<text>
two yellow rings and one blue ring?
… … … … … 𝐹&’$(#)
🔥
Projector Tokenizer
MMFuser🔥 <response>
Concatenate
🔥 Learnable 𝑋 𝐹* There are
❄ three small
Frozen Input Features Query Feature 𝐹"# 𝐅$#
🔥
b pa icl tls
u
rin
e
,t whe
it h
Cross Attention
Image Feature Large Language Model two yellow
𝐹$%
# (Vicuna-7B / 13B) rings and one
Addition Self Attention blue ring.
Fig. 4. Overview of MMFuser. In MMFuser, feature maps from different layers of the vision encoder are strategically integrated to enhance the visual
representations. Deep feature maps are employed as query elements, while shallow and intermediate feature maps are concatenated to form key and value
elements.Throughadynamicattention-basedfusion,MMFusercombinesfine-graineddetailsandhigher-levelsemanticinformation.Thefusedfeaturesare
thenalignedwithtextusingaprojectorandsubsequentlypassedasinputstoLLMs.
layers of the vision encoder, as shown in Fig. 3 (c). This TABLEI
approach allowed us to obtain more powerful visual repre- COMPARISONBETWEENTHELLAVA-1.5BASELINE,FOURSIMPLE
FEATUREFUSIONMETHODS,ANDOURMMFUSER.
sentations, thereby enhancing the performance of MLLMs.
VizWiz POPE MME MMBCN MMVet
III. METHOD Method [61] [62] [63] [64] [65] Avg.
Inthissection,weaddressthechallengeofmissingdetailed LLaVA-1.5-13B[3] 53.6 85.9 1531.3 63.6 35.4 63.0
information in current visual representations for MLLMs.
w/Concatenation 52.1 86.9 1537.5 63.7 35.8 63.5
Initial attempts using some simple fusion methods demon- w/Average 54.7 87.1 1527.9 63.6 35.7 63.0
strate that shallow features suffer from inadequate semantic w/WeightedAverage 54.4 87.0 1532.8 62.5 34.6 63.1
w/FPN[31] 53.7 87.3 1553.2 63.4 37.3 63.9
alignment. To overcome this, we propose the MMFuser for
w/MMFuser(Ours) 57.4 87.5 1585.2 63.8 36.6 64.9
MLLMs, designed to effectively integrate multi-layer features
while maintaining semantic alignment.
A. Analysis of Visual Representations for MLLMs
Currently, most mainstream MLLMs [2]–[4], [23], [58], create the fused feature map, i.e. Concat(F ,F ,...,F ). (2)
1 2 L
[59] adopt CLIP-ViT [17] as their vision encoder, typically Average: Compute the element-wise average of all feature
selectingasinglefeaturemapfromthefinallayersasthevisual maps to obtain the fused feature map, i.e. 1 (cid:80)L F . (3)
L i=1 i
representation. Prior studies [60] suggest that in deeper layers Weighted Average: Assign learnable weights to each feature
of ViT, the receptive fields of attention heads become pre- map,thencomputetheweightedaverageas(cid:80)L w F ,where
i=1 i i
dominantly global, while the shallow layers retain both local w is the learnable weight associated with F . (4) Feature
i i
andglobalinformation.Consequently,thelackoflocaldetails PyramidNetwork(FPN)[31]:FeedallfeaturemapsintoFPN
in deep feature maps can lead to suboptimal performance in for multi-scale feature learning, then compute the weighted
fine-grained visual recognition tasks. average of the FPN outputs to obtain the fused feature map.
Different from existing approaches that integrate multiple
encoders [15], [28]–[30], we consider that the visual infor- We applied the fused feature maps as the visual represen-
mation captured by the CLIP-ViT itself is not fully leveraged tations in LLaVA-1.5 [3] and followed its original settings to
in MLLMs. As illustrated in Fig. 1, shallow layers capture evaluate model performance. However, as shown in Table I,
fine-grained details, which are often underutilized. Therefore, none of the four fusion methods consistently improved model
we argue that combining shallow and deep features can performance. We attribute this to the semantic misalignment
significantly enhance MLLM performance. between deep and shallow features. As shown in Fig. 1,
To validate our point, we explored several feature map fu- although shallow features capture more fine-grained details,
sionmethods.WeselectedLfeaturemapsfromvariousdepths their alignment with text is considerably weaker than that of
of ViT, denoted as F = [F ,F ,...,F ], where F ∈ RN×D, deep features. In deeper layers, the features that correspond
1 2 L i
withi∈[1,L].Here,N representsthenumberofViTpatches, tothetextareprominentlyhighlighted,whileshallowfeatures
andDdenotesthedimensionoftheimagefeatureembeddings. lackthisclearcorrespondence.Bysimplyfusingthetwotypes
The four fusion methods considered are: (1) Concatenation: of image features, the model struggles to effectively leverage
Concatenate feature maps along the channel dimension to the complementary strengths of each feature.
Block Block Block Block Block5
TABLEII
COMPARISONWITHSTATE-OF-THE-ARTMLLMSON12GENERALMULTIMODALBENCHMARKS.THEBENCHMARKSINCLUDE:VQAV2[67],GQA[68],
VIZWIZ[61],SCIENCEQA-IMG[69];TEXTVQA[70],POPE[62],MME[63],MMBENCH-EN[64],MMBENCH-CN[64],SEED-BENCH[71],
LLAVA-BENCH-IN-THE-WILD[2],ANDMMVET[65].∗THETRAININGANNOTATIONSOFTHEDATASETSAREOBSERVEDDURINGTRAINING.THE
BESTRESULTSAREMARKEDINBOLD,ANDTHESECONDBESTRESULTSAREUNDERLINED.
Method LLM VQAv2 GQA VizWiz SQAI VQAT POPE MME MMB MMBCN SEED LLaVAW MMVet
InstructBLIP[1] Vicuna-7B – 49.2 34.5 60.5 50.1 – – 36.0 23.7 53.4 60.9 26.2
IDEFICS-9B[72] LLaMA-7B 50.9 38.4 35.5 – 25.9 – – 48.2 25.2 – – –
Qwen-VL[5] Qwen-7B 78.8∗ 59.3∗ 35.2 67.1 63.8∗ – – 38.2 7.4 56.3 – –
Qwen-VL-Chat[5] Qwen-7B 78.2∗ 57.5∗ 38.9 68.2 61.5∗ – 1487.5 60.6 56.7 58.2 – –
LLaVA-1.5-7B[3] Vicuna-7B 78.5∗ 62.0∗ 50.0 66.8 58.2 85.9 1510.7 64.3 58.3 58.6 63.4 30.5
LLaVA-1.5-7B
Vicuna-7B 79.1∗ 62.8∗ 53.4 68.7 58.8 86.3 1479.7 67.5 60.1 60.8 65.5 32.6
+MMFuser(Ours)
BLIP-2[36] Vicuna-13B 65.0 41.0 19.6 61.0 42.5 85.3 1293.8 – – 46.4 38.1 22.4
InstructBLIP[1] Vicuna-13B – 49.5 33.4 63.1 50.7 78.9 1212.8 – – – 58.2 25.6
IDEFICS-80B[72] LLaMA-65B 60.0 45.2 36.0 – 30.9 – – 54.5 38.1 – – –
Shikra[73] Vicuna-13B 77.4∗ – – – – – – 58.8 – – – –
LLaVA-1.5-13B[3] Vicuna-13B 80.0∗ 63.3∗ 53.6 71.6 61.3 85.9 1531.3 67.7 63.6 61.6 70.7 35.4
LLaVA-1.5-13B
Vicuna-13B 80.1∗ 63.4∗ 57.4 71.2 59.9 87.5 1585.2 69.9 63.8 62.0 71.8 36.6
+MMFuser(Ours)
B. MMFuser: Multimodal Multi-Layer Feature Fuser C. Overall Framework Design
Building on the insights from the previous sections, we Our proposed MMFuser can be integrated into main-
observe that the shallow and deep features of ViT can com- stream open-source MLLMs, particularly within the “ViT-
plement each other. To harness this potential, we propose a MLP-LLM” architecture [2]–[4], [23]. As a case study, we
multi-layerfeaturefusionmodule,MMFuser.Itcanserveasa demonstrate this integration using the LLaVA-1.5 model. In
bridge between the vision encoder and the LLM. The overall this framework, MMFuser is positioned between the ViT and
architecture of MMFuser is shown in Fig. 4. the MLP projector to fuse multi-layer feature maps from the
Specifically, we extract L feature maps from the ViT, ViT. The overall architecture, illustrated in Fig. 4, consists
denoted as F = [F ,F ,...,F ]. Since the strong semantic of four key components: the vision encoder (ViT), MMFuser,
1 2 L
MLP projector, and the large language model.
alignmentbetweendeepvisualfeaturesandtextspace,weuse
the deep feature F as queries to dynamically extract missing The input image is first fed into the ViT to extract multi-
L
details from shallow features X =Concat(F ,F ,...,F ), layer visual features. Then, our MMFuser leverages these
1 2 L−1
multi-layer features to obtain a fused feature with richer local
through a cross-attention operation. This results in a visual
feature F ∈ RN×D with richer fine-grained features. This information. Following the pipeline in LLaVA-1.5 [3], the
ca
fused features are then aligned with the text embedding space
process can be formulated as:
through a trainable MLP projector. Similarly, the input text
F =Attention(norm(F ), norm(X)), (1) is transformed into a text embedding via a tokenizer. Finally,
ca L
the image and text embeddings are concatenated and fed into
whereAttention(·)denotestheattentionmechanism,norm(·) an LLM, such as Vicuna-7B [8]. The LLM then answers the
means layer normalization [66], and Concat(·) represents the user’s questions based on the image features. This framework
concatenation operation. canalsobereadilyadaptedforprocessingvideodata.Overall,
To effectively facilitate feature interaction and emphasize existingmainstreamMLLMscaneasilyadoptourMMFuserto
salientfeatures,weincorporateaself-attentionmechanisminto enhance their visual feature extraction capabilities.
the feature map F , formulated as:
ca
IV. EXPERIMENT
F′ =Attention(norm(F ), norm(F )),
sa ca ca A. Implementation Details
F =F +γ F′ , (2)
sa ca 2 sa We adopt LLaVA-1.5 [3] as the baseline to study the
where γ ∈ RD is a learnable vector that adjusts the visual representations of MLLMs. The model comprises
2
contribution of F relative to F′ . Subsequently, for the three components: the pre-trained vision encoder CLIP-ViT-
ca sa
L-336px [17], the pre-trained LLM Vicuna-v1.5 [8], and a
resulting feature map F , we introduce another learnable
sa
vector γ ∈RD to modulate the integration of F and F : two-layer MLP projector. To fully leverage the potential of
1 L sa
a single vision encoder, we use the proposed MMFuser to
F =F +γ F . (3) fuse multi-layer features from the vision encoder, replacing
visual L 1 sa
the original single-layer image feature for the LLM.
Through the aforementioned steps, we derive the enhanced 1) Architecture Settings: In MMFuser, the number of fea-
visual feature F . Unlike the original visual feature F , ture layers L selected from the ViT is set to 5 by default. The
visual L
F integrates richer fine-grained information, making it a parameters γ and γ , which control the weights of features
visual 1 2
superior alternative for the visual input to the LLM. from different layers, are both initialized to 0. Deformable6
TABLEIII TABLEIV
RESULTONOCRBENCH[82].RECOG.:TEXTRECOGNITION,VQAS: RESULTSOFREGIONCAPTIONING.RESULTSAREREPORTEDWITHTHE
SCENETEXT-CENTRICVQA;VQAD:DOCUMENT-ORIENTEDVQA; CIDERSCORE.
KIE:KEYINFORMATIONEXTRACTION;HMER:HANDWRITTEN
MATHEMATICALEXPRESSION;FINAL:OVERALLSCOREACROSSALL RefCOCO RefCOCO+ RefCOCOg
FIVECATEGORIES. Model val testA testB val testA testB val test Avg.
Method Recog. VQAS VQAD KIE HMER Final LLaVA-1.5-7B 30.4 16.0 42.0 30.2 20.3 39.1 60.5 58.9 37.2
+MMFuser 33.6 17.7 45.9 33.6 21.2 42.6 61.5 61.6 39.7
MiniGPT4V2[83] 124 29 4 0 0 157
LLaVA-1.5-13B 33.1 16.7 45.2 33.4 19.8 41.6 61.6 59.9 38.9
BLIP2[36] 154 71 10 0 0 235
+MMFuser 38.2 19.5 53.6 36.8 22.8 43.8 64.5 63.4 42.8
InstructBLIP[1] 168 93 14 1 0 276
BLIVA[84] 165 103 22 1 0 291
LLaVA-1.5-7B[3] 160 117 15 5 0 297 TABLEV
+MMFuser(Ours) 159 128 20 8 0 315↑18 RESULTSOFREFERRINGEXPRESSIONCOMPREHENSION.RESULTSARE
REPORTEDWITHTHEPRECISION@0.5SCORE.
LLaVA-1.5-13B[3] 176 129 19 7 0 331
+MMFuser(Ours) 171 136 25 11 0 343↑12
RefCOCO RefCOCO+ RefCOCOg
Model Avg.
val testA testB val testA testB val test
LLaVA-1.5-7B 56.2 64.4 47.5 50.0 59.2 39.0 48.8 48.4 51.7
attention [74] is used as the default attention mechanism in +MMFuser 62.0 70.7 52.4 55.6 65.1 44.2 54.0 54.8 57.4
MMFuser. In this setup, the number of sampling points is
LLaVA-1.5-13B 66.5 73.9 55.7 59.8 67.9 48.7 57.3 56.0 60.7
fixed at 4, with the attention layer employing 16 heads. +MMFuser 66.6 73.9 56.3 61.3 68.8 49.2 56.5 56.5 61.1
2) Training Settings: For a fair comparison, we adopt the
same two-stage training recipes as LLaVA-1.5 [3]:
Pre-training. During the pre-training stage, we utilize the 2) Results on Comprehensive Multimodal Benchmarks:
LLaVA-LCS-558K dataset [2], which comprises 558K image- In the multimodal benchmarks, our 7B and 13B models
caption pairs. In this phase, the vision encoder and the LLM demonstrate significant performance improvements compared
are kept frozen. Training is focused solely on the MLP to the corresponding LLaVA-1.5 models. Notably, our 13B
projector and MMFuser, with the objective of aligning the modelsubstantiallyoutperformsLLaVA-1.5-13Bacrossseven
visual features with the input space of the LLM. different benchmarks. Specifically, our 13B model achieves
Fine-tuning. During the fine-tuning stage, we employ scores of 1585.2 on the MME benchmark and 69.9 on the
the LLaVA-Instruct-665K dataset [3], which includes 665K MMBenchbenchmark,representingenhancementsof53.9and
instruction-following data from sources such as GQA [68], 2.2pointsoverLLaVA-1.5-13B,respectively.Additionally,our
TextCaps [75], ShareGPT [76], and others [67], [77]–[81]. In model exhibits strong performance across other multimodal
thisphase,theMLPprojector,MMFuser,andLLMaretrained benchmarks, such as POPE, SEED-Bench, and MMVet.
end-to-end, while the vision encoder remains frozen.
We train our model using the same experimental settings
C. Results on OCRBench
and hyperparameters as LLaVA-1.5. We adopt the AdamW
optimizer and use a cosine decay learning rate scheduler with OCRBench [82] serves as a comprehensive OCR bench-
an initial warmup ratio of 0.03 and no weight decay. The mark,comprising1,000manuallycuratedandcorrectedOCR-
globalbatchsizeissetto256forpre-trainingand128forfine- relatedVQAinstructions.Thebenchmarkissystematicallydi-
tuning. The learning rate is set to 1e-3 during the pre-training vided into five distinct categories: Text Recognition (Recog.),
phase and reduced to 2e-5 during the fine-tuning phase. Both Scene Text-Centric VQA (VQAS), Document-Oriented VQA
phases are trained for a single epoch. (VQAD), Key Information Extraction (KIE), and Handwritten
Mathematical Expression Recognition (HMER).
As detailed in Table III, our models, with 7B and 13B
B. Results on General Multimodal Benchmarks
parameters, exhibit an average improvement of 15 points over
We evaluated MMFuser using a comprehensive suite of LLaVA-1.5. This substantial gain underscores MMFuser’s en-
12 benchmarks, including both academic visual question an- hancedcapabilityinrefiningthegranularityofvisualrepresen-
swering (VQA) benchmarks and comprehensive multimodal tations, thereby contributing to more accurate text recognition
benchmarks, to assess its performance across multiple dimen- and superior OCR performance.
sions. As shown in Table II, our model exhibits substantial
performance improvements on these benchmarks.
D. Results on Region-level Benchmarks
1) Results on Academic VQA Benchmarks: On the aca-
demic VQA benchmarks, our 7B model consistently out- Toassessregionalunderstandingandgroundingcapabilities,
performs LLaVA-1.5-7B across all five benchmarks. Simi- we evaluate MMFuser on two representative regional-level
larly, the 13B version of our model surpasses LLaVA-1.5- tasks. (1) Regional Captioning [81], [85]: This task requires
13B on the VQAv2, GQA, and VizWiz benchmarks, with the model to generate a description for an object in the
a particularly notable improvement of 3.8 points on VizWiz. image based on a given region. (2) Referring Expression
Furthermore, our model achieves comparable performance on Comprehension [80], [85]: This task requires the model to
the ScienceQA and TextVQA benchmarks. locatetargetobjectsinanimagebasedonagivendescription.7
Input Image Key/Value Query MMFuser Output Input Image Key/Value Query MMFuser Output
Fig.5. FeaturemapvisualizationofMMFuser.Foreachimage,weprovidethreetypesofoutputfeaturemaps.Theterm“Key/Value”referstotheaveraged
feature maps from four selected shallow and intermediate layers of the ViT—specifically, layers 3, 8, 13, and 18—used as the key and value inputs in
MMFuser.“Query”denotesthefeaturemapfromthepenultimatelayeroftheViT,servingasthequeryinputinMMFuserandasthevisualrepresentations
in prior MLLMs. “MMFuser Output” represents the feature map generated after applying the proposed MMFuser. As can be seen, the proposed MMFuser
capturesfine-graineddetailsfromshallowandintermediateViTlayers,enrichingthevisualrepresentationsfortheLLM.
TABLEVI TABLEVII
ABLATIONONLAYERCOMBINATION. ABLATIONSONATTENTIONMECHANISMS.“CPLX.”INDICATESTHE
COMPLEXITYTYPEOFTHEATTENTIONMECHANISM,ENCOMPASSING
SelectedLayers BOTHQUADRATIC(QUAD.)ANDLINEARVARIANTS.
VizWiz POPE MME MMB SEED MMVet Avg.
Q K,V
AttentionType Cplx. VizWizPOPEMMEMMBSEEDMMVetAvg.
– – 53.6 85.9 1531.3 67.7 61.6 35.4 63.5
LLaVA-1.5-13B[3] – 53.6 85.9 1531.3 67.7 61.6 35.4 63.5
23 [1,3,5,7] 54.3 87.3 1582.2 69.5 61.8 35.1 64.5
23 [9,11,13,15] 52.4 87.0 1560.3 69.1 61.7 35.6 64.0 GlobalAttn[86] Quad. 52.9 87.6 1566.3 68.6 62.2 35.3 64.2
23 [17,19,21,24] 54.7 87.3 1591.2 69.3 62.0 35.8 64.8 LinearSRA[87] Linear 54.3 87.0 1581.6 68.9 61.9 34.7 64.3
23 [5,8,11,20] 54.7 87.2 1584.0 69.3 62.5 36.2 64.9 DeformableAttn[74]Linear 57.4 87.5 1585.2 69.9 62.0 36.6 65.4
23 [3,8,13,18] 57.4 87.5 1585.2 69.9 62.0 36.6 65.4
TABLEVIII
1) Results of Region Captioning: On region captioning ABLATIONSONINTERNALMODULEDESIGNS.
tasks, our model shows significant improvements. As shown
Module VizWiz POPE MME MMB SEED MMVet Avg.
in Table IV, compared to LLaVA-1.5, the 7B model of
LLaVA-1.5-13B 53.6 85.9 1531.3 67.7 61.6 35.4 63.5
MMFuser surpasses LLaVA-1.5 by 2.5 points on average,
while the 13B version improves by 3.9 points. This indicates +Cross-Attn 54.6 87.2 1557.1 68.2 62.2 36.1 64.4
++Self-Attn 57.4 87.5 1585.2 69.9 62.0 36.6 65.4
that MMFuser captures fine-grained information, enhancing
caption accuracy and richness.
2) Results of Referring Expression Comprehension (REC):
The results demonstrate that incorporating feature maps
We also employ REC tasks to evaluate the model’s grounding
from multiple layers significantly improves the model’s visual
capabilities. As shown in Table V, our model consistently
representation capabilities. The feature maps from different
outperforms LLaVA-1.5 models across all benchmarks, with
layers capture information at varying receptive field sizes,
an especially notable average improvement of 5.7 points for
each offering distinct advantages depending on the task.
the7BmodelcomparedtoLLaVA-1.5-7B.Thishighlightsthat
Notably, uniform sampling across layers, which aggregates
the visual representations generated by MMFuser are more
information from diverse receptive fields, achieves the highest
detailedandcomprehensive,enhancingspatiallocalizationand
performance. Therefore, MMFuser employs this strategy to
significantly boosting performance in grounding tasks.
optimize visual representation for visual-language tasks.
2) AblationonAttentionMechanisms: Theattentionmech-
E. Ablation Study
anism in MMFuser is modular and can be replaced with
1) Ablation Study on Layer Combination: To evaluate the differentvariants.Weexperimentwiththreetypesofattention
contribution of feature maps from different layers of ViT [17] mechanisms using the 13B model of MMFuser. As shown
to model performance, we conduct an ablation study on the in Table VII, our framework consistently enhances the visual
MMFuser-13B model by sampling feature maps from various representation capability of the MLLM, regardless of the at-
depths. As detailed in Table VI, we utilize the feature map tention mechanism employed. This confirms the effectiveness
from the 23rd layer of ViT as the query, while the key and and adaptability of our feature fusion module.
valueareformedbyconcatenatingfeaturemapssampledfrom Compared to global attention mechanisms [86], sparse at-
different layers: shallow (row 2), intermediate (row 3), deep tention mechanisms with linear complexity not only deliver
(row 4), non-uniform (row 5), and uniform sampling (row 6). superior performance in MMFuser but also offer substantial
18
User: Please provide a detailed description of the image.
MMFuser (Ours): The image features a box of Annie's Homegrown Organic Chocolate Chip Cookie Bites.
The box is purple and has a picture of a cookie on it. The cookies are made with organic ingredients and are
free from synthetic flavors, synthetic colors, and synthetic preservatives. The box contains a total of 12
cookies, making it a perfect snack option for those who enjoy organic and healthier alternatives.
LLaVA-1.5: The image features a box of Annie's Organic Chocolate Chip Cookie Bites. The box is purple and
has a picture of a cookie on it. The cookies are made with organic ingredients and are gluten-free, making
them a healthier alternative to traditional chocolate chip cookies. The box contains a dozen cookies, each
with a bite-sized portion, perfect for snacking or sharing with friends and family.
(a) Qualitative example of the complex captioning capability.
User: Please identify the text in the picture.
MMFuser (Ours): The image displays a series of statistics related to common data science roles. The
first statistic shows that 53% of companies still rely on their chief executive officer (CEO) for their
data and analytics strategy. The second statistic indicates that 57% of businesses say they have a
chief data officer (CDO) to lead their data and analytics efforts. The third statistic highlights that 58%
of companies believe their system administrator should be responsible for their data and analytics
strategy.
LLaVA-1.5: The image features a pie chart with various data points related to common data science roles. The chart is divided into four
sections. The first section is titled "53% of companies still rely on their chief executive officer for their data and analytics agenda." The
second section is titled "57% of companies say they have a system administrator." The third section is titled "59% of companies say
they have a business intelligence officer." The fourth section is titled "58% of companies say they have a chief data officer."
(b) Qualitative example of the OCR capability.
User: User:
Please provide the bounding box Please provide the bounding box
coordinates of the area described coordinates of the area described
in this sentence: Traffic signs. in this sentence: Glass water cup.
MMFuser (Ours): IoU: 0.76 MMFuser (Ours): IoU: 0.74
[0.69, 0.40, 0.76, 0.49] [0.83, 0.88, 0.99, 1.00]
LLaVA-1.5: IoU: 0.53 LLaVA-1.5: IoU: 0.63
Ground-truth: Ground-truth:
[0.69, 0.41, 0.78, 0.51] [0.81, 0.87, 0.97, 0.99]
[0.70, 0.41, 0.76, 0.49] [0.83, 0.89, 0.96, 1.00]
(c) Qualitative example of the visual grounding capability.
Fig.6. QualitativecomparisonsbetweenMMFuserandLLaVA-1.5.ForthecomplexcaptioningandOCRtasks,thetextgeneratedbyeachmodeliscolor-
codedtomatchthemodelname,indicatingcontentthataccuratelyreflectstheinformationpresentintheimage.Textinreddenoteserrorsorhallucinations.
For visual grounding examples, the predicted bounding boxes are also color-coded according to the respective model names, while yellow bounding boxes
denotethegroundtruth.TheIntersectionoverUnion(IoU)metricisusedtoevaluatetheoverlapbetweenpredictedboundingboxesandthegroundtruth.A
higherIoUvalueindicatesamoreaccurateprediction,asitreflectsalargerintersectionarearelativetotheunionarea.
improvements in computational efficiency. Among the linear cross-attentionmechanismstoextractfine-grainedinformation
attention mechanisms, deformable attention [74] provides the from various ViT layers leads to a marked performance
most significant performance gains. Therefore, deformable improvement.Specifically,ourmodeloutperformsthebaseline
attention is adopted as the default mechanism in MMFuser. by 1.3 points on POPE and 0.7 points on MMVet. Moreover,
It is noteworthy that future research could explore more the addition of self-attention to these fine-grained features
advanced attention mechanisms to further enhance the visual further enhances the model’s capability to capture relevant
representation capabilities of MLLMs. information, resulting in gains of 2.8 points on VizWiz, 28.1
points on MME, and 1.7 points on MMB. These results
3) Ablation on Internal Module Designs: To validate the
collectivelydemonstratethatourdesignsignificantlyenhances
key contributions of our MMFuser architecture, we incre-
the visual representation abilities of MLLMs, highlighting the
mentally enhance the LLaVA-1.5-13B baseline [3] with our
critical role of each component in our model.
proposed designs. As illustrated in Table VIII, integrating9
4) Visual Representation Visualization: To intuitively vali- for MLLMs. Overall, MMFuser improves fine-grained detail
datetheimpactofMMFuseronvisualfeatures,wepresentthe capture and semantic understanding in MLLMs, and we hope
input and output feature map visualizations for four example it will contribute to the community’s efforts in developing
imagesinFig.5.Fortheinputimage,theshallowfeaturemaps more robust and efficient multimodal models.
convey richer fine-grained information, but this information is
messy and the semantic information is difficult to distinguish.
REFERENCES
For instance, the lower-left image contains complex infor-
mation, making it challenging to intuitively discern semantic [1] W. Dai, J. Li, D. Li, A. Huat, J. Zhao, W. Wang, B. Li, P. Fung, and
contentfromtheshallowfeatures.Butaftertheattentionmech- S.Hoi,“Instructblip:Towardsgeneral-purposevision-languagemodels
withinstructiontuning,”inProc.Adv.NeuralInf.Process.Syst.,vol.36,
anism of MMFuser, the foreground characters in the picture
2023.
are well highlighted, and the semantic information is aligned [2] H.Liu,C.Li,Q.Wu,andY.J.Lee,“Visualinstructiontuning,”inProc.
with the last layer feature map of ViT. The aligned detailed Adv.NeuralInf.Process.Syst.,vol.36,2024.
[3] H. Liu, C. Li, Y. Li, and Y. J. Lee, “Improved baselines with visual
features can effectively enhance the fine-grained perception
instruction tuning,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern
capability of MLLMs. Recognit.(CVPR),2024,pp.26296–26306.
[4] Z.Chen,J.Wu,W.Wang,W.Su,G.Chen,S.Xing,M.Zhong,Q.Zhang,
X.Zhu,L.Luetal.,“Internvl:Scalingupvisionfoundationmodelsand
F. Qualitative Comparison aligning for generic visual-linguistic tasks,” in Proc. IEEE/CVF Conf.
Comput.Vis.PatternRecognit.(CVPR),2024,pp.24185–24198.
In Fig. 6, we compare MMFuser and LLaVA-1.5 across
[5] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou,
three case studies related to fine-grained analysis, including and J. Zhou, “Qwen-vl: A frontier large vision-language model with
complex captioning, OCR, and visual grounding. The visual- versatileabilities,”arXivpreprintarXiv:2308.12966,2023.
[6] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu,
ization results show that the rich fine-grained information in
J. Luo, Z. Ma et al., “How far are we to gpt-4v? closing the gap to
the shallow feature maps significantly enhances the ability of commercialmultimodalmodelswithopen-sourcesuites,”arXivpreprint
MLLMs to capture and process image details. arXiv:2404.16821,2024.
[7] OpenAI, “Gpt-4v(ision) system card,” 2023. [Online]. Available:
Forthecomplexcaptioningexample,MMFusershowsgreat
https://api.semanticscholar.org/CorpusID:263218031
improvement in recognizing small text and small objects. [8] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng,
Compared to LLaVA-1.5, MMFuser even successfully identi- S.Zhuang,Y.Zhuang,J.E.Gonzalezetal.,“Vicuna:Anopen-source
chatbot impressing gpt-4 with 90%* chatgpt quality,” Mar. 2023.
fiesthesmallandblurredgreenbackgroundtextdescribingthe
[Online].Available:https://lmsys.org/blog/2023-03-30-vicuna/
healthyingredientsofthecookie:“freefromsyntheticflavors, [9] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
synthetic colors, and synthetic preservatives.” T. Lacroix, B. Rozie`re, N. Goyal, E. Hambro, F. Azhar et al.,
“Llama:Openandefficientfoundationlanguagemodels,”arXivpreprint
For the OCR task, MMFuser shows enhanced recognition
arXiv:2302.13971,2023.
of small texts, effectively mitigating hallucinations that often
[10] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
occur in LLaVA-1.5. The latter struggles with misaligned N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama
and hallucinated content due to inadequate text recognition 2: Open foundation and fine-tuned chat models,” arXiv preprint
arXiv:2307.09288,2023.
capabilities, leading it to supplement details from its prior
[11] OpenAI,“Chatgpt,”2022.[Online].Available:https://chat.openai.com/
knowledge rather than the image itself. [12] J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman,
In the visual grounding example, demonstrates a more D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4
technicalreport,”arXivpreprintarXiv:2303.08774,2023.
accurate delineation of object boundaries, especially when
[13] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,
adjacentobjectshavesimilarcolors.Thisimprovementismore A. Mathur, A. Schelten, A. Yang, A. Fan et al., “The llama 3 herd of
pronounced with small objects, such as the positioning of the models,”arXivpreprintarXiv:2407.21783,2024.
[14] I. Team, “Internlm: A multilingual language model with progressively
traffic sign in the left-side example, where MMFuser shows a
enhanced capabilities,” 2023. [Online]. Available: https://github.com/
23% increase in the IoU metric. InternLM/InternLM
In summary, these results further validate the effectiveness [15] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren,
Z. Li, Y. Sun et al., “Deepseek-vl: towards real-world vision-language
of MMFuser in addressing the issue of missing fine-grained
understanding,”arXivpreprintarXiv:2403.05525,2024.
image details in MLLMs. [16] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge,
Y. Han, F. Huang et al., “Qwen technical report,” arXiv preprint
arXiv:2309.16609,2023.
V. CONCLUSION
[17] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
In this paper, we introduced MMFuser, a novel multimodal G.Sastry,A.Askell,P.Mishkin,J.Clarketal.,“Learningtransferable
visual models from natural language supervision,” in Proc. Int. Conf.
multi-layer feature fuser designed to improve visual represen-
Mach.Learn.(ICML),2021.
tation in Multimodal Large Language Models (MLLMs). By [18] Z.Liu,H.Mao,C.-Y.Wu,C.Feichtenhofer,T.Darrell,andS.Xie,“A
integrating both shallow and deep features from the vision convnetforthe2020s,”inProc.IEEE/CVFConf.Comput.Vis.Pattern
Recognit.(CVPR),2022,pp.11976–11986.
encoder, MMFuser addresses the limitations of relying solely
[19] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,
on deep features from a single vision encoder, which often P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., “Dinov2:
results in the loss of fine-grained details. Our experiments Learning robust visual features without supervision,” Trans. Mach.
Learn.Res.,pp.1–31,2024.
showthatMMFuserenhancestheperformanceoftheLLaVA-
[20] Y.Fang,Q.Sun,X.Wang,T.Huang,X.Wang,andY.Cao,“Eva-02:
1.5 model across various benchmarks, enriching visual repre- A visual representation for neon genesis,” Image Vision Comput., vol.
sentation without the redundancy and computational overhead 149,p.105171,2024.
[21] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, “Sigmoid loss for
ofensemblemodels.Thisapproachmaximizesthepotentialof
language image pre-training,” in Proc. IEEE/CVF Int. Conf. Comput.
asingleViTencoder,offeringanefficientandflexiblesolution Vis.(ICCV),2023,pp.11975–11986.10
[22] K. Zheng, X. He, and X. E. Wang, “Minigpt-5: Interleaved recognition and understanding of the open world,” in Proc. Int. Conf.
vision-and-language generation via generative vokens,” arXiv preprint Learn.Represent.(ICLR),2024.
arXiv:2310.02239,2023. [45] W. Wang, Y. Ren, H. Luo, T. Li, C. Yan, Z. Chen, W. Wang, Q. Li,
[23] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: L.Lu,X.Zhuetal.,“Theall-seeingprojectv2:Towardsgeneralrelation
Enhancingvision-languageunderstandingwithadvancedlargelanguage comprehension of the open world,” arXiv preprint arXiv:2402.19474,
models,”arXivpreprintarXiv:2304.10592,2023. 2024.
[24] R.Xu,Y.Yao,Z.Guo,J.Cui,Z.Ni,C.Ge,T.-S.Chua,Z.Liu,M.Sun, [46] Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, and F. Wei,
andG.Huang,“Llava-uhd:anlmmperceivinganyaspectratioandhigh- “Kosmos-2:Groundingmultimodallargelanguagemodelstotheworld,”
resolutionimages,”arXivpreprintarXiv:2403.11703,2024. arXivpreprintarXiv:2306.14824,2023.
[25] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, [47] X.Fan,T.Ji,C.Jiang,S.Li,S.Jin,S.Song,J.Wang,B.Hong,L.Chen,
T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gellyetal., G. Zheng et al., “Mousi: Poly-visual-expert vision-language models,”
“An image is worth 16x16 words: Transformers for image recognition arXivpreprintarXiv:2401.17221,2024.
atscale,”inProc.Int.Conf.Learn.Represent.(ICLR),2021. [48] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski,
[26] C.Ge,S.Cheng,Z.Wang,J.Yuan,Y.Gao,J.Song,S.Song,G.Huang, D.Salz,S.Goodman,A.Grycner,B.Mustafa,L.Beyeretal.,“Pali:A
andB.Zheng,“Convllava:Hierarchicalbackbonesasvisualencoderfor jointly-scaled multilingual language-image model,” in Proc. Int. Conf.
largemultimodalmodels,”arXivpreprintarXiv:2405.15738,2024. Learn.Represent.(ICLR),2023.
[27] S. Tong, Z. Liu, Y. Zhai, Y. Ma, Y. LeCun, and S. Xie, “Eyes wide [49] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini,
shut? exploring the visual shortcomings of multimodal llms,” in Proc. R. Taori, A. Dave, V. Shankar, H. Namkoong, J. Miller, H. Hajishirzi,
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2024, pp. A.Farhadi,andL.Schmidt,“Openclip,”Jul.2021.[Online].Available:
9568–9578. https://doi.org/10.5281/zenodo.5143773
[28] G. Luo, Y. Zhou, Y. Zhang, X. Zheng, X. Sun, and R. Ji, “Feast your [50] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao, “Eva-clip: Improved
eyes: Mixture-of-resolution adaptation for multimodal large language trainingtechniquesforclipatscale,”arXivpreprintarXiv:2303.15389,
models,”arXivpreprintarXiv:2403.03003,2024. 2023.
[29] D. Jiang, Y. Liu, S. Liu, J. Zhao, H. Zhang, Z. Gao, X. Zhang, J. Li, [51] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,
andH.Xiong,“Fromcliptodino:Visualencodersshoutinmulti-modal T.Xiao,S.Whitehead,A.C.Berg,W.-Y.Loetal.,“Segmentanything,”
largelanguagemodels,”arXivpreprintarXiv:2310.08825,2024. in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), 2023, pp. 4015–
[30] Y.Li,Y.Zhang,C.Wang,Z.Zhong,Y.Chen,R.Chu,S.Liu,andJ.Jia, 4026.
“Mini-gemini: Mining the potential of multi-modality vision language [52] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
models,”arXivpreprintarXiv:2403.18814,2024. A.Wahid,J.Tompson,Q.Vuong,T.Yuetal.,“Palm-e:Anembodied
[31] T.-Y.Lin,P.Dolla´r,R.Girshick,K.He,B.Hariharan,andS.Belongie, multimodallanguagemodel,”inProc.Int.Conf.Mach.Learn.(ICML),
“Feature pyramid networks for object detection,” in Proc. IEEE/CVF vol.202,2023,pp.8469–8488.
Conf.Comput.Vis.PatternRecognit.(CVPR),2017,pp.2117–2125. [53] A.Chowdhery,S.Narang,J.Devlin,M.Bosma,G.Mishra,A.Roberts,
[32] S.Xu, H.Zhang, X.Xu,X. Hu,Y. Xu,L.Dai, K.-S.Choi, andP.-A. P.Barham,H.W.Chung,C.Sutton,S.Gehrmannetal.,“Palm:Scaling
Heng,“Representativefeaturealignmentforadaptiveobjectdetection,” language modeling with pathways,” J. Mach. Learn. Res., vol. 24, no.
IEEETrans.CircuitsSyst.VideoTechnol.,vol.33,no.2,pp.689–700, 240,pp.1–113,2023.
2023. [54] M.Dehghani,J.Djolonga,B.Mustafa,P.Padlewski,J.Heek,J.Gilmer,
[33] W. Gao, G. Liao, S. Ma, G. Li, Y. Liang, and W. Lin, “Unified A.P. Steiner,M.Caron,R. Geirhos,I.Alabdulmohsin etal.,“Scaling
information fusion network for multi-modal rgb-d and rgb-t salient visiontransformersto22billionparameters,”inProc.Int.Conf.Mach.
object detection,” IEEE Trans. Circuits Syst. Video Technol., vol. 32, Learn.(ICML),2023,pp.7480–7512.
no.4,pp.2091–2106,2022. [55] J. Zhu, H. Wang, and M. Shi, “Multi-modal large language model
[34] Z. Chen, H. Ji, Y. Zhang, Z. Zhu, and Y. Li, “High-resolution feature enhanced pseudo 3d perception framework for visual commonsense
pyramidnetworkforsmallobjectdetectionondroneview,”IEEETrans. reasoning,”IEEETrans.CircuitsSyst.VideoTechnol.,pp.1–1,2024.
CircuitsSyst.VideoTechnol.,vol.34,no.1,pp.475–489,2024. [56] X. Zhang, P. Zhao, J. Ji, X. Lu, and Y. Yin, “Video corpus moment
[35] S. Wang, H. Lu, and Z. Deng, “Fast object detection in compressed retrievalviadeformablemultigranularityfeaturefusionandadversarial
video,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), 2019, pp. training,”IEEETrans.CircuitsSyst.VideoTechnol.,pp.1–1,2023.
7104–7113. [57] H.Yao,W.Wu,T.Yang,Y.Song,M.Zhang,H.Feng,Y.Sun,Z.Li,
[36] J.Li,D.Li,S.Savarese,andS.Hoi,“Blip-2:Bootstrappinglanguage- W.Ouyang,andJ.Wang,“Denseconnectorformllms,”arXivpreprint
image pre-training with frozen image encoders and large language arXiv:2405.13800,2024.
models,” in Proc. Int. Conf. Mach. Learn. (ICML), 2023, pp. 19730– [58] B.McKinzie,Z.Gan,J.-P.Fauconnier,S.Dodge,B.Zhang,P.Dufter,
19742. D. Shah, X. Du, F. Peng, F. Weers et al., “Mm1: Methods, anal-
[37] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, ysis & insights from multimodal llm pre-training,” arXiv preprint
K. Lenc, A. Mensch, K. Millican, M. Reynolds et al., “Flamingo: a arXiv:2403.09611,2024.
visuallanguagemodelforfew-shotlearning,”inProc.Adv.NeuralInf. [59] Z.Li,B.Yang,Q.Liu,Z.Ma,S.Zhang,J.Yang,Y.Sun,Y.Liu,and
Process.Syst.,vol.35,2022,pp.23716–23736. X.Bai,“Monkey:Imageresolutionandtextlabelareimportantthings
[38] G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut, forlargemulti-modalmodels,”inProc.IEEE/CVFConf.Comput.Vis.
J.Schalkwyk,A.M.Dai,A.Hauthetal.,“Gemini:afamilyofhighly PatternRecognit.(CVPR),2024,pp.26763–26773.
capablemultimodalmodels,”arXivpreprintarXiv:2312.11805,2023. [60] M.Raghu,T.Unterthiner,S.Kornblith,C.Zhang,andA.Dosovitskiy,
[39] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. “Do vision transformers see like convolutional neural networks?” in
Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser et al., Proc.Adv.NeuralInf.Process.Syst.,vol.34,2021,pp.12116–12128.
“Gemini 1.5: Unlocking multimodal understanding across millions of [61] D.Gurari,Q.Li,A.J.Stangl,A.Guo,C.Lin,K.Grauman,J.Luo,and
tokensofcontext,”arXivpreprintarXiv:2403.05530,2024. J.P.Bigham,“Vizwizgrandchallenge:Answeringvisualquestionsfrom
[40] Anthropic, “The claude 3 model family: Opus, sonnet, haiku,” 2024. blindpeople,”inProc.IEEE/CVFConf.Comput.Vis.PatternRecognit.
[Online].Available:https://www.anthropic.com (CVPR),2018,pp.3608–3617.
[41] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, [62] Y.Li,Y.Du,K.Zhou,J.Wang,W.X.Zhao,andJ.-R.Wen,“Evaluating
L.Zhao,X.Songetal.,“Cogvlm:Visualexpertforpretrainedlanguage object hallucination in large vision-language models,” Proc. Empirical
models,”arXivpreprintarXiv:2311.03079,2023. MethodsNaturalLang.Process.,pp.292–305,2023.
[42] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo, T. Lu, [63] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu,
J. Zhou, Y. Qiao et al., “Visionllm: Large language model is also an W. Lin, J. Yang, X. Zheng et al., “Mme: A comprehensive evaluation
open-endeddecoderforvision-centrictasks,”inProc.Adv.NeuralInf. benchmark for multimodal large language models,” arXiv preprint
Process.Syst.,vol.36,2024. arXiv:2306.13394,2023.
[43] J.Wu,M.Zhong,S.Xing,Z.Lai,Z.Liu,W.Wang,Z.Chen,X.Zhu, [64] Y.Liu,H.Duan,Y.Zhang,B.Li,S.Zhang,W.Zhao,Y.Yuan,J.Wang,
L.Lu,T.Luetal.,“Visionllmv2:Anend-to-endgeneralistmultimodal C.He,Z.Liuetal.,“Mmbench:Isyourmulti-modalmodelanall-around
large language model for hundreds of vision-language tasks,” arXiv player?”arXivpreprintarXiv:2307.06281,2023.
preprintarXiv:2406.08394,2024. [65] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and
[44] W.Wang,M.Shi,Q.Li,W.Wang,Z.Huang,L.Xing,Z.Chen,H.Li, L.Wang,“Mm-vet:Evaluatinglargemultimodalmodelsforintegrated
X.Zhu,Z.Caoetal.,“Theall-seeingproject:Towardspanopticvisual capabilities,”arXivpreprintarXiv:2308.02490,2023.11
[66] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv
preprintarXiv:1607.06450,2016.
[67] Y.Goyal,T.Khot,D.Summers-Stay,D.Batra,andD.Parikh,“Making
thevinvqamatter:Elevatingtheroleofimageunderstandinginvisual
question answering,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern
Recognit.(CVPR),2017,pp.6325–6334.
[68] D. A. Hudson and C. D. Manning, “Gqa: A new dataset for real-
worldvisualreasoningandcompositionalquestionanswering,”inProc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2019, pp.
6700–6709.
[69] P.Lu,S.Mishra,T.Xia,L.Qiu,K.-W.Chang,S.-C.Zhu,O.Tafjord,
P. Clark, and A. Kalyan, “Learn to explain: Multimodal reasoning via
thought chains for science question answering,” in Proc. Adv. Neural
Inf.Process.Syst.,2022.
[70] A.Singh,V.Natarajan,M.Shah,Y.Jiang,X.Chen,D.Batra,D.Parikh,
and M. Rohrbach, “Towards vqa models that can read,” in Proc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2019, pp.
8317–8326.
[71] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan, “Seed-bench:
Benchmarkingmultimodalllmswithgenerativecomprehension,”arXiv
preprintarXiv:2307.16125,2023.
[72] IDEFICS, “Introducing idefics: An open reproduction of state-of-
the-art visual language model,” 2023. [Online]. Available: https:
//huggingface.co/blog/idefics
[73] K.Chen,Z.Zhang,W.Zeng,R.Zhang,F.Zhu,andR.Zhao,“Shikra:
Unleashingmultimodalllm’sreferentialdialoguemagic,”arXivpreprint
arXiv:2306.15195,2023.
[74] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:
Deformabletransformersforend-to-endobjectdetection,”inProc.Int.
Conf.Learn.Represent.(ICLR),2020.
[75] O. Sidorov, R. Hu, M. Rohrbach, and A. Singh, “Textcaps: a dataset
forimagecaptioningwithreadingcomprehension,”inProc.Eur.Conf.
Comput.Vis.(ECCV). Springer,2020,pp.742–758.
[76] ShareGPT,2023.[Online].Available:https://sharegpt.com/
[77] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi, “Ok-vqa: A
visualquestionansweringbenchmarkrequiringexternalknowledge,”in
Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2019,
pp.3195–3204.
[78] A. Mishra, S. Shekhar, A. K. Singh, and A. Chakraborty, “Ocr-vqa:
Visual question answering by reading text in images,” in Int. J. Doc.
Anal.Recog.,2019,pp.947–952.
[79] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi,
“A-okvqa: A benchmark for visual question answering using world
knowledge,”inProc.Eur.Conf.Comput.Vis.(ECCV),2022,pp.146–
162.
[80] S.Kazemzadeh,V.Ordonez,M.Matten,andT.Berg,“Referitgame:Re-
ferringtoobjectsinphotographsofnaturalscenes,”inProc.Empirical
MethodsNaturalLang.Process.,2014,pp.787–798.
[81] R.Krishna,Y.Zhu,O.Groth,J.Johnson,K.Hata,J.Kravitz,S.Chen,
Y.Kalantidis,L.-J.Li,D.A.Shammaetal.,“Visualgenome:Connecting
languageandvisionusingcrowdsourceddenseimageannotations,”Int.
J.Comput.Vis.,pp.32–73,2017.
[82] Y. Liu, Z. Li, M. Huang, B. Yang, W. Yu, C. Li, X. Yin, C. lin Liu,
L. Jin, and X. Bai, “Ocrbench: On the hidden mystery of ocr in large
multimodalmodels,”arXivpreprintarXiv:2305.07895,2024.
[83] J.Chen,D.Zhu,X.Shen,X.Li,Z.Liu,P.Zhang,R.Krishnamoorthi,
V. Chandra, Y. Xiong, and M. Elhoseiny, “Minigpt-v2: large language
model as a unified interface for vision-language multi-task learning,”
arXivpreprintarXiv:2310.09478,2023.
[84] W. Hu, Y. Xu, Y. Li, W. Li, Z. Chen, and Z. Tu, “Bliva: A simple
multimodalllmforbetterhandlingoftext-richvisualquestions,”inProc.
AAAIConf.Artif.Intell.(AAAI),vol.38,no.3,2024,pp.2256–2264.
[85] J.Mao,J.Huang,A.Toshev,O.Camburu,A.L.Yuille,andK.Murphy,
“Generationandcomprehensionofunambiguousobjectdescriptions,”in
Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2016,
pp.11–20.
[86] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,
Ł.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”inProc.Adv.
NeuralInf.Process.Syst.,2017.
[87] W.Wang,E.Xie,X.Li,D.-P.Fan,K.Song,D.Liang,T.Lu,P.Luo,and
L.Shao,“Pvtv2:Improvedbaselineswithpyramidvisiontransformer,”
ComputationalVisualMedia,vol.8,no.3,pp.415–424,2022.