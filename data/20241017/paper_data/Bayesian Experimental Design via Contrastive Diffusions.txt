Underreview
BAYESIAN EXPERIMENTAL DESIGN
VIA CONTRASTIVE DIFFUSIONS
JacopoIollo1∗,ChristopheHeinkelé2,PierreAlliez3,andFlorenceForbes1∗
1InriaGrenobleRhône-Alpes,Grenoble,France
2Cerema,Endsum-Strasbourg,France
3InriaSophiaAntipolis,SophiaAntipolis,France
{jacopo.iollo, florence.forbes}@inria.fr
ABSTRACT
Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce
the cost of running a sequence of experiments. When based on the Expected
Information Gain (EIG), design optimization corresponds to the maximization
ofsomeintractableexpectedcontrastbetweenpriorandposteriordistributions.
Scalingthismaximizationtohighdimensionalandcomplexsettingshasbeenan
issueduetoBOEDinherentcomputationalcomplexity. Inthiswork,weintroduce
an expected posterior distribution with cost-effective sampling properties and
provideatractableaccesstotheEIGcontrastmaximizationviaanewEIGgradient
expression. Diffusion-basedsamplersareusedtocomputethedynamicsofthe
expected posterior and ideas from bi-level optimization are leveraged to derive
anefficientjointsampling-optimizationloop, withoutresortingtolowerbound
approximationsoftheEIG.TheresultingefficiencygainallowstoextendBOED
tothewell-testedgenerativecapabilitiesofdiffusionmodels. Byincorporating
generativemodelsintotheBOEDframework,weexpanditsscopeanditsusein
scenariosthatwerepreviouslyimpractical.Numericalexperimentsandcomparison
withstate-of-the-artmethodsshowthepotentialoftheapproach.
1 INTRODUCTION
Designingoptimalexperimentscanbecriticalinnumerousappliedcontextswhereexperimentsare
constrainedintermsofresourcesormoregenerallycostlyandlimited.Inthiswork,designisassumed
tobecharacterizedbysomecontinuousparametersξ ∈ E ⊂Rd,whichreferstotheexperimental
part,suchasthechoiceofameasurementlocation,thatcanbecontrolledtooptimizetheexperimental
outcome. WeconsideraBayesiansettinginwhichtheparametersofinterestisθ ∈Θ⊂Rm and
designisoptimizedtomaximizetheinformationgainonθ. Bayesianoptimalexperimentaldesign
(BOED)isnotanewtopicinstatistics,seee.g. ChalonerandVerdinelli(1995);SebastianiandWynn
(2000);Amzaletal.(2006)buthasrecentlygainednewinterestwiththeuseofmachinelearning
techniques,seeRainforthetal.(2024);Huanetal.(2024)forrecentreviews. Themostcommon
approachconsistsofmaximizingtheso-calledexpectedinformationgain(EIG),whichisamutual
informationcriterionthataccountsforinformationviatheShannon’sentropy. Letp(θ)denotea
priorprobabilitydistributionandp(y|θ,ξ)alikelihooddefiningtheobservationy ∈Y generating
process. Thepriorisassumedtobeindependentonξandp(y|θ,ξ)availableinclosed-form. Toour
knowledge,allpreviousBOEDapproachesalsoassumethatthepriorisavailableinclosed-form,a
settingthatwewillrefertoasdensity-basedBOED.Inthiswork,wewillalsoconsiderdata-based
BOED,asettingwhereonlysamplesfromthepriorareavailable. TheEIG,denotedbelowbyI,
admitsseveralequivalentexpressions,seee.g. Fosteretal.(2019). Itcanbewrittenastheexpected
lossinentropywhenaccountingforanobservationyatξ(eq.(1))orasamutualinformation(MI)
orexpectedKullback-Leibler(KL)divergence(eq. (2)). Denotingp (θ,y) = p(θ,y|ξ)thejoint
ξ
distributionof(θ,Y)andusingp(θ,y|ξ)=p(θ|y,ξ)p(y|ξ)=p(y|θ,ξ)p(θ),itcomes,
I(ξ)=E [H(p(θ))−H(p(θ|Y,ξ)] (1)
p(y|ξ)
=E [KL(p(θ|y,ξ),p(θ))] = MI(p ), (2)
p(y|ξ) ξ
∗Correspondingauthor
1
4202
tcO
51
]LM.tats[
1v62811.0142:viXraUnderreview
whererandomvariablesareindicatedwithuppercaseletters,E [·]orE [·]denotestheexpectation
p(·) p
with respect to p and H(p(θ)) = −E [logp(θ)] is the entropy of p. The joint distribution p
p(θ) ξ
completelydeterminesallotherdistributions,marginal(prior)andconditional(posterior)distributions,
sothatthemutualinformation,whichistheKLbetweenthejointandtheproductofitsmarginal
distributions,canbewrittenasafunctionofp ∈P(Θ×Y)only. InthefollowingP(Θ×Y),resp.
ξ
P(Θ),resp. P(Y),denotesthesetofprobabilitymeasuresonΘ×Y,resp. Θ,resp. Y.
InBOED,welookforξ∗satisfying
ξ∗ ∈argmaxI(ξ)=argmaxMI(p ). (3)
ξ
ξ∈Rd ξ∈Rd
Theaboveoptimizationisusuallyreferredtoasstaticdesignoptimization. Themainchallengein
EIG-basedBOEDisthatboththeEIGanditsgradientwithrespecttoξaredoublyintractable. Their
respectiveexpressionsinvolveanexpectationofanintractableintegrandoveraposteriordistribution
which is itself not straightforward to sample from. The posterior distribution is generally only
accessiblethroughaniterativealgorithmprovidingapproximatesamples. Inpractice,theinference
problemisfurthercomplicatedasdesignoptimizationisconsideredinasequentialcontext,inwhich
aseriesofexperimentsisplannedsequentiallyandeachsuccessivedesignhastobeaccountedfor.
Inordertoremovetheintegrandintractabilityissue,solutionshavebeenproposedwhichoptimize
anEIGlowerbound(Fosteretal.,2019). Thislowerboundcanbeexpressedasanexpectationofa
tractableintegrandandbecomestightwithincreasedsimulationbudgets. Theremainingposterior
samplingissuehasthenbeensolvedindifferentways. Asetofapproachesconsistsofapproximating
theproblematicposteriordistribution,eitherwithvariationaltechniques(Fosteretal.,2019)orwith
efficientsequentialMonteCarlo(SMC)sampling(Iolloetal.,2024;Drovandietal.,2013). Other
approachesavoidposteriorestimation,usingreinforcementlearning(RL)andoff-linepolicylearning
tobypasstheneedforsampling(Fosteretal.,2021;Ivanovaetal.,2021;Blauetal.,2022). However,
somestudieshaveshownthatestimatingtheposteriorwasbeneficial,e.g. Iolloetal.(2024)and
Ivanovaetal.(2024),whichimprovesonFosteretal.(2021)byintroducingposteriorestimationsteps
inordertorefinethelearnedpolicy. Inaddition,oneshouldkeepinmindthatposteriorinferenceis
centralinBOEDastheultimategoalisnotdesignpersebuttogaininformationontheparameterof
interest. Thisischallengingespeciallyinasequentialcontext. Previousattemptsthatprovideboth
candidatedesignandestimatesoftheposteriordistribution,suchasFosteretal.(2019);Iolloetal.
(2024),arethusessentiallyin2alternatingstages,approximatedesignoptimizationbeingdependent
onapproximateposteriorsamplingandvice-versa.
In this work, we propose a novel 1-stage approach which leverages a sampling-as-optimization
setting(KorbaandSalim,2022;Marionetal.,2024)wheresamplingisseenasanoptimizationtask
overthespaceofprobabilitydistributions. WeintroduceanewEIGgradientexpression(Section
3),whichhighlightstheEIGgradientasafunctionofboththedesignandsomesamplingoutcome.
Thisfitsintoabi-leveloptimizationframeworkadaptedtoBOEDinSection4. Sodoing,ateach
step,bothanestimationoftheoptimaldesignandsamplesfromthecurrentposteriordistribution
canbeprovidedinasingleloopdescribedinSection5. Itresultsanefficientprocedurethatcan
handle both traditional density-based samplers and data-based samplers such as provided by the
highlysuccessfuldiffusion-basedgenerativemodels. TheresultingefficiencygainenablesBOED
applicationsatsignificantlylargerscalesthanpreviouslyfeasible. Notably,thisopensthefirstaccess
todata-basedBOED,beyonddensity-basedapproaches. Figure1isanillustrationona28×28image
θ reconstructionproblemfrom7×7sub-imagescenteredatlocationsξ tobeselected,detailsin
Section6. Forsimplernotation,wefirstpresentourapproachinthestaticdesigncase. Adaptationto
thesequentialcaseisspecifiedinSection6andallnumericalexamplesareinthesequentialsetting.
Figure1:28×28Imageθ(1stcolumn)reconstructionfromseven7×7sub-imagesy=A θ+ηcenteredat
ξ
sevencentralpixelsξ(designs)selectedsequentially.Optimizedvs.randomdesigns:measuredoutcomey(2nd
vs.3rdcolumn)andparameterθestimates(reconstruction)withhighestweights(uppervs.lowersub-row).
2Underreview
2 RELATED WORK
We focus on gradient-based BOED for continuous problems. Applying a first-order method to
solve (3) requires computing gradients of the EIG I, which are no more tractable than I itself.
Gradient-basedBOEDisgenerallybasedonstochasticgradient-typealgorithms(seeSection4.3.2.
inHuanetal.(2024)). Thisrequiresinprincipleunbiasedgradientestimators,althoughstochastic
approximation solutions using biased oracles have also been investigated, see e.g. Demidovich
etal.(2023);LiuandTajbakhsh(2024). Tomeetthisrequirement,moststochasticgradient-based
approachesstartfromanEIGlowerboundthatyieldstractableunbiasedgradientestimators. More
specifically,EIGlowerboundshaveusuallytheadvantagetoremovethenestedexpectationissue,see
e.g. Fosteretal.(2019). Incontrast,veryfewapproachesfocusondirectEIGgradientestimators. To
ourknowledge,thisisonlythecaseinGodaetal.(2022)andAoandLi(2024). Godaetal.(2022)
proposeanunbiasedestimatoroftheEIGgradientusingarandomizedversionofamultilevelnested
MonteCarlo(MLMC)estimatorfromRheeandGlynn(2015). Adifferentestimatorisproposed
byAoandLi(2024),whouseMCMCsamplersleadingtobiasedestimators,forwhichtheauthors
showempiricallythatthebiascouldbemadenegligible. Inthiswork,wefirstshow,inSection3,
thattheirtwoapparentlydifferentsolutionsactuallyonlydifferinthewaytheintractableposterior
distributionisapproximated. WethenproposeathirdwaytocomputeEIGgradientsthatismore
computationallyefficientandscalesbettertolargerdatavolumesandsequentialdesigncontexts.
Thisnewexpressionmakesuseofadistributionthatweintroduceandnametheexpectedposterior
distribution. This latter distribution has interesting sampling features that allows us to leverage
score-basedsamplingtechniquesandconnecttotheso-calledimplicitdiffusionframeworkofMarion
etal.(2024). OursingleloopprocedureinSection5isinspiredbyMarionetal.(2024)andother
recentdevelopmentsinbi-leveloptimization(Yangetal.,2021;Dagréouetal.,2022;Hongetal.,
2023). However, theselattersettingsdonotcoverdoublyintractableobjectivessuchastheEIG,
whichrequiresbothappropriategradientestimatorsandsamplingoperators,seeourSections3and4.
InBOED,efficientsingleloopprocedureshavebeenproposedbyFosteretal.(2020)buttheyrely
heavilyonvariationalapproximations,whichmaylimitaccuracyinscenarioswithcomplexposterior
distributions. Additionally,theyfocusonlower-boundmaximizationwhichmightsacrificeprecision.
3 EXPECTED-POSTERIOR ESTIMATION OF THE EIG GRADIENT
EfficientEIGgradientestimatorsarecentralforaccuratescalableBOED.Gradientsderivedfromthe
reparameterizationtrickareoftenpreferred,overtheonesobtainedwithscore-basedtechniques,as
theyhavebeenreportedtoexhibitlowervariance(Xuetal.,2019).
EIGgradientviaareparametrizationtrick. Assumingp(y|θ,ξ)issuchthatY canberewritten
asY =T (U)withT invertiblesothatU =T−1(Y)andU isarandomvariableindependent
ξ,θ ξ,θ ξ,θ
onθandξwithatractabledistributionp (U). TheexistenceofT isstraightforwardifthedirect
U ξ,θ
modelcorrespondstoanadditiveGaussiannoiseasthetransformationisthenlinearinU. Results
existtoguaranteetheexistenceofsuchatransformationinmoregeneralsituations(Papamakarios
etal.,2021). Usingthischangeofvariable,twoexpressionsoftheEIGgradient,(5)and(6)below,
can be derived. Detailed steps are given in Appendix A. With p denoting the joint distribution
ξ
p(θ,y|ξ),gaquantityrelatedtothescoreg(ξ,y,θ,θ′) = ∇ logp(T (u)|θ′,ξ) and
ξ ξ,θ |u=T−1(y)
ξ,θ
denotingh(ξ,y,θ,θ′)=∇ p(T (u)|θ′,ξ) ,afirstexpressionis
ξ ξ,θ |u=T−1(y)
ξ,θ
(cid:34) E (cid:2) h(ξ,Y,θ,θ′)(cid:3)(cid:35)
∇ I(ξ)=E g(ξ,Y,θ,θ)− p(θ′) . (4)
ξ pξ E (cid:2) p(Y|θ′,ξ)(cid:3)
p(θ′)
Considering importance sampling formulations for the second term of (4), with an importance
distributionq ∈P(Θ),potentiallydependingony,θandξ,furtherleadsto
 E (cid:104) p(θ′) h(ξ,Y,θ,θ′)(cid:105)
∇ ξI(ξ)=E pξg(ξ,Y,θ,θ)− Eq(θ′|Y,θ,ξ) q (cid:104)(θ′|Y p(, θθ ′, )ξ) p(Y|θ′,ξ)(cid:105)  . (5)
q(θ′|Y,θ,ξ) q(θ′|Y,θ,ξ)
InGodaetal.(2022),thislatterexpressionisusedinarandomizedMLMCprocedurewithqsettoa
Laplaceapproximationoftheposteriordistribution,withoutjustificationforthisspecificchoiceofq.
3Underreview
Itresultsanestimatorwhichisnotunbiasedbutcanbede-biasedfollowingRheeandGlynn(2015).
Alternatively,asecondexpressionoftheEIGgradientisthestartingpointofAoandLi(2024),
∇ I(ξ)=E (cid:2) g(ξ,Y,θ,θ)−E (cid:2) g(ξ,Y,θ,θ′)(cid:3)(cid:3) . (6)
ξ pξ p(θ′|Y,ξ)
ItfollowsanestedMonteCarloestimator(30)giveninAppendixA,usingsamples{(y ,θ )}
i i i=1:N
fromthejointp andforeachy ,samples{θ′ } fromanMCMCprocedureapproximating
ξ i i,j j=1:M
theintractableposteriorp(θ′|y ,ξ). Interestingly,expression(6)canalsoberecoveredbysettingthe
i
importanceproposalq(θ′|y,θ,ξ)top(θ′|y,ξ)in(5),whichprovidesaclearjustificationofwhythe
choiceofqmadeinGodaetal.(2022)isrelevant. ApproachesbyGodaetal.(2022)andAoandLi
(2024)thusmainlydifferintheirchoiceofapproximationsfortheposteriordistribution. Usinga
LaplaceapproximationasinGodaetal.(2022)isrelevantonlyiftheposteriorisunimodal,which
maynotbethecaseinpractice. TheMCMCversionofAoandLi(2024)isthenpotentiallymore
generalbutalsomorecostlyasitrequiresrunningN timesaMCMCsampler,targetingeachtimea
differentposteriorp(θ|y ,ξ). Inthenextparagraph,weintroducetheexpectedposteriordistribution
i
andderiveanother,morecomputationallyefficient,gradientexpression.
ImportancesamplingEIGgradientestimatorwithanexpectedposteriorproposal. Intheir
work, Ao and Li (2024) consider only static design, which hides the fact that for more realistic
sequentialdesigncontexts,theirsolutionisnottractableduetoitscomputationalcomplexity. Their
solutionfacesthestandardissueofnestedestimation(Rainforthetal.,2018). Toavoidthisissuewe
proposetouseanimportancesamplingexpressionforthesecondtermin(6),whichhastheadvantage
tomovethedependenceony(andθ)fromthesamplingparttotheintegrandpart. Weconsidera
proposaldistributionq ∈P(θ)thatdoesnotdependonY norθ. Itcomes,
(cid:20) (cid:20) p(θ′|Y,ξ) (cid:21)(cid:21)
∇ I(ξ) = E g(ξ,Y,θ,θ)−E g(ξ,Y,θ,θ′) , (7)
ξ pξ q(θ′|ξ) q(θ′|ξ)
andanapproximategradientcanbeobtainedas
1 (cid:88)N (cid:20)
g(ξ,y ,θ ,θ )−E
(cid:20) p(θ′|y i,ξ)
g(ξ,y ,θ
,θ′)(cid:21)(cid:21)
. (8)
N i i i q(θ′|ξ) q(θ′|ξ) i i
i=1
Thesecondtermin(8)stillrequiresN importancesamplingapproximationswhosequalitydepends
onthechoiceoftheproposaldistributionq. Theidealproposalqiseasytosimulate,withcomputable
weightsatleastuptoaconstant,andsothatqandthemultipletargetdistributionsp(·|y ,ξ)arenot
i
toofarapart. GivenN samples{(θ ,y )} fromp ,weproposethustotakeq =q where
i i i=1:N ξ ξ,N
q
isthefollowinglogarithmicpoolingorgeometricmixture,with(cid:80)N
ν =1,
ξ,N i=1 i
N N
(cid:89) (cid:89)
q (θ)∝ p(θ|y ,ξ)νi ∝p(θ) p(y |θ,ξ)νi . (9)
ξ,N i i
i=1 i=1
Werefertoq astheexpectedposteriordistribution, definedinamoregeneralwayin(13). It
ξ,N
allowstoassesstheeffectofacandidatedesignξonsamplesfromtheprior. Itdiffersfromastandard
posteriorasnorealdatayobtainedbyrunningtheexperimentξisavailableduringtheoptimization.
Weonlyhaveaccesstosamples{(θ ,y )} fromthejointp . Theexpectedposteriorcanbe
i i i=1:N ξ
seenasadistributionthattakesintoaccountallpossibleoutcomesofacandidateexperimentξgiven
thesamples{θ } fromtheprior. Thischoiceofq isjustifiedinAppendixB,usingLemma
i i=1:N ξ,N
2,provedtherein.
Lemma2showsthat,for(cid:80)N
ν =1,q isthedistributionqthatminimizesthe
i=1 i ξ,N
weightedsumoftheKLsagainsteachposteriorp(θ|y ,ξ),i.e.
(cid:80)N
ν KL(q,p(θ|y ,ξ)),leadingto
i i=1 i i
anefficientimportancesamplingproposal. Itfollowsournewgradientestimator,
 
N M
1 (cid:88) 1 (cid:88)
∇ ξI(ξ)≈
N
g(ξ,y i,θ i,θ i)−
M
w
i,j
g(ξ,y i,θ i,θ′ j) , (10)
i=1 j=1
where {(θ ,y )} follow p , {θ′} follow q and w = p(θ′ j|yi,ξ) denotes the
i i i=1:N ξ i j=1:M ξ,N i,j qξ,N(θ′ j)
importancesamplingweight. Whenthisfractioncanonlybeevaluateduptoaconstant,weconsider
4Underreview
selfnormalizedimportancesampling(SNIS)usingp˜,q˜ theunnormalizedversionsofpandq ,
ξ,N ξ,N
w˜ =
p˜(θ′ j|y i,ξ)
=
p(y i|θ′ j,ξ)
and w =
w˜
i,j . (11)
i,j q˜ ξ,N(θ′ j) (cid:81)N ℓ=1p(y ℓ|θ′ j,ξ)νℓ i,j (cid:80)M j=1w˜
i,j
Althoughwithareducedcomputationalcost,computinggradientswith(10)stillrequiresaniterative
samplingalgorithmideallyrunforalargenumberofiterationstoreachsatisfyingapproximations
of the joint p and the expected posterior q . In static design, sampling from the joint is not
ξ ξ,N
generallydifficultasthepriorandthelikelihoodareassumedavailablebutthisbecomesproblematic
insequentialdesign,asfurtherdetailedinSection6.1. Sequentialdesignisthesettingtobekeptin
mindinthispaperandinpractice,theexactdistributionsarerarelyreached. Toassesstheimpacton
gradientapproximations,itisconvenienttointroduce,asinMarionetal.(2024),gradientoperators.
Inthenextsection,weshowhowtoadapttheformalismofMarionetal.(2024)toourBOEDtask.
4 EIG OPTIMIZATION THROUGH SAMPLING
TheintractabilityoftheEIGanditsgradientlinksoptimizationandsamplingstepsinanestedand
costlyway. Marionetal.(2024),inadifferentcontext,proposetoadoptabi-leveloptimizationpoint
ofviewtointegratesamplingasanadditionaloptimizationstepandderivesimplerprocedures.
Estimationofgradientsthroughsampling. DenoteΓafunctionfromP(Θ×Y)×P(Θ)×Rd
toRd,definedas,
(cid:20) (cid:20) p(θ′|Y) (cid:21)(cid:21)
Γ(p,q,ξ)=E g(ξ,Y,θ,θ)−E g(ξ,Y,θ,θ′) . (12)
p q q(θ′)
Expression(7)showsthat∇ I(ξ) = Γ(p ,q,ξ), whereq isadistributionq(θ′|ξ)onθ′ possibly
ξ ξ
dependingonξ. Thegradientestimator(10)correspondsthento∇ I(ξ)≈Γ(pˆ ,qˆ ,ξ),where
ξ ξ ξ,N
pˆ =
(cid:80)N
δ and qˆ =
(cid:80)M
δ . In general, sampling from p , or its sequential
ξ i=1 (θi,yi) ξ,N j=1 θ′ j ξ
counterpart,andq ischallengingandonlypossiblethroughaniterativeprocedure. However,an
ξ,N
interestingfeatureofourexpectedposterioristhatitdoesnotaddadditionalsamplingdifficulties.
Expectedposteriordistribution. Moregenerally(detailsinAppendixB),wedefine,
q (θ)∝exp(E [logp(θ|Y,ξ)]) (13)
ξ,ρ ρ
whereρisameasureonY.
Whenρ(y)=(cid:80)N
ν δ
(y)with(cid:80)N
ν =1,werecoverq (θ)=
i=1 i yi i=1 i ξ,ρ
q (θ)in(9). Thespecialstructureoftheexpectedposteriorallowstosamplefromitusingthe
ξ,N
samealgorithmicstructuretosamplefromasingleposteriorp(θ|y,ξ). Indeed,thescoreofq (θ)
ξ,ρ
islinkedtotheposteriorscore,
∇ logq (θ)=E [∇ logp(θ|Y,ξ)], (14)
θ ξ,ρ ρ θ
whichforq
simplifiesinto(cid:80)N
ν ∇ logp(θ|y ,ξ). Inpractice,weconsidertheoperationof
ξ,N i=1 i θ i
samplingastheoutputofastochasticprocessiteratingaso-calledsamplingoperator.
Iterativesamplingoperators. Iterativesamplingoperators,asintroducedinMarionetal.(2024),
aremappingsfromaspace,tobespecifiedbelow,toaspaceofprobabilities. InourBOEDsetting,
weconsidertwosuchoperators. Thefirstoneisdefined,foreachξ,throughasequenceoversof
parameterizedfunctionsfromP(Θ×Y)toP(Θ×Y)anddenotedbyΣY,θ(p,ξ). Samplingis
s
definedastheoutcomeinthelimits→∞orforsomefinites=S ofthefollowingprocessstarting
fromp(0) ∈P(Θ×Y)anditerating
p(s+1) =ΣY,θ(p(s),ξ). (15)
s
Thisformalismincludesnormalizingflows(Papamakariosetal.,2021)forwhichp(s) isexplicit.
More generally, we also consider situations in which p(s) is represented by a random variable
5Underreview
Figure2: Sourcelocalisationexample.Prior(left)andexpectedposterior(right)samplesatexperimentk.Final
ξ∗ (orangecross)attheendoftheoptimizationsequenceξ ,·,ξ (bluecrosses).Thisoptimization"contrasts"
k 0 T
thetwodistributionsbymakingtheexpectedposterior"asdifferentaspossible"fromtheprior.
X ∼p(s). Forexample,indensity-basedBOED,whenconsideringaLangevindiffusionconverging
s
top ,foragivenξ,p(s+1) =ΣY,θ(p(s),ξ)isashorthandforitsEulerdiscretization,
ξ s
y(s+1) =y(s)−γ ∇
V(y(s),θ(s),ξ)+(cid:112)
2γ B
s y s y,s
θ(s+1) =θ(s)−γ ∇ V(y(s),θ(s),ξ)+(cid:112) 2γ B
s θ s θ,s
whereB andB arerealizationsofindependentstandardGaussianvariables,γ isastep-size
y,s θ,s s
andV(y,θ,ξ)=−logp(θ)−logp(y|θ,ξ)isthep potential,p (y,θ)∝exp(−V(y,θ,ξ)). The
ξ ξ
dynamicsinducedleadtosamplesfromp fors→∞. Inthefollowing,wewillthususethenotation
ξ
ΣY,θ tomeanthatwehaveaccesstosamplesfromp(s+1), whichisequivalenttoapplyΣY,θ to
s s
anempiricalversionofp(s)builtfromsamples{(y(s),θ(s))} . Similarlyforagivenposterior
i i i=1:N
p(θ|y ,ξ),wecanproducesamples{θ′(s+1)} ,withafixedy ,usingthefollowingupdating,
i j j=1:M i
θ′(s+1) =θ′(s)−γ′∇ V(y ,θ′(s),ξ)+(cid:112) 2γ′B , (16)
s θ i s θ′,s
whiletheexpectedposteriorq canbesampledwith
ξ,N
N
θ′(s+1) =θ′(s)−γ′ (cid:88) ν ∇ V(y ,θ′(s),ξ)+(cid:112) 2γ′B . (17)
s i θ i s θ′,s
i=1
Forasamplingoperatoroftheexpectedposteriorgeneralform(13),weneedtoextendthedefinition
inMarionetal.(2024)byaddingadependenceonsomedistributionρ∈P(Y)fortheconditioning
part. Thesecondsamplingoperatorisdefined,forsomegivenξandρ,throughasequenceoversof
parameterizedfunctionsfromP(Θ)toP(Θ)anddenotedbyΣθ′(q,ξ,ρ). Thesamplingoperatoris
s
definedastheoutcomeofthefollowingprocessstartingfromq(0) ∈P(Θ)anditerating
q(s+1) =Σθ′ (q(s),ξ,ρ). (18)
s
Forinstance,whenρ=(cid:80)N ν δ ,q(s+1) =Σθ′(q(s),ξ,ρ)canthenbeashorthandfor(17).
i=1 i yi s
5 SINGLE LOOP CONTRASTIVE EIG OPTIMIZATION
Theperspectiveofoptimizationthroughsamplingleadsnaturallytoanestedloopprocedure. An
innerloopisperformed,iteratingsamplingoperators,toreachgoodapproximationsofp andq
ξ ξ,N
viasamplesusedtoevaluategradient(10). Itresultsanestedloopalgorithminwhichthedesignis
updatedonlyaftercompletionoftheinnerloop,whichinourBOEDcontextinvolvestwosamplers,
as specified in Section 4 and summarized in Algorithm 1. More efficient procedures consist of
iteratingalternatelybetweenthesamplingoperator,e.g. oneLangevinstep,andthedesignupdate,
6Underreview
viaoneiterationofstochasticgradientdescent(SGD)oranotheroptimizer. Inparticular,Marionetal.
(2024)pointoutthelinktobi-leveloptimizationwhensamplingcanbecastasanoptimizationover
thespaceofdistributions. Forinstance,theLangevindiffusiondynamicsfollowagradientflowof
theKLwithrespecttotheWasserstein-2distance(Jordanetal.,1998). Thispointofviewsuggests
totransformthenestedloopAlgorithm1intoasingleloopsummarizedinAlgorithm2. Ateach
optimizationstepoverξ,thesamplingoperatorsareappliedonlyonceusingthecurrentξ,whichis
updatedinturn,etc. Algorithms1and2areinageneralform. Inthissection,wespecifythesteps
thatarefurtherillustratedinSection6. Regardingthechoiceofsamplingoperators,thisapproach
canhandlebothtraditionaldensity-basedsamplingwhereanexpressionofthetargetdistributionis
availableanddata-basedsamplingwhereonlytrainingsamplesareavailableandagenerativemodel
isusedtogenerateadditionalsimilarsamples. Examplesofbothcasesarespecifiedbelow.
Algorithm1:Nested-loopoptimization Algorithm2:Singleloopoptimization
Result: Optimaldesignξ∗ Result: Optimaldesignξ∗
Initialisation: ξ ∈Rd Initialisation: ξ ∈Rd,p(0)←p ,q(0)←q
0 0 0 0
fort=0:T-1(outerξoptimizationloop)do fort=0:T-1(sampling-optimizationloop)do
p(0)←p andq(0)←q p(t+1) =ΣY,θ(p(t),ξ )
t 0 t 0 t t
fors=0:S-1(p ξ innersampling)do ρˆ
t+1
←p( yt+1)(p(t+1)marginalovery)
p( ts+1) =ΣY s ,θ(p( ts),ξ t) q(t+1) =Σθ t′(q(t),ξ t,ρˆ t+1)
end Compute
pˆ ←p(S) ∇ I(ξ )=Γ(p(t+1),q(t+1),ξ )in(12)
ξ t t ξ t t
ρˆ ←pˆ (y)(pˆ marginalovery) Updateξ withSGDoranotheroptimizer
t ξ t ξ t t
fors’=1:S’-1(q innersampling)do end
ξ,ρ
q(s′+1) =Σθ′(q(s′),ξ
,ρˆ)
return ξ T;
t s′ t t t
end
qˆ
←q(S′)
ξ t t
Compute∇ I(ξ )=Γ(pˆ ,qˆ ,ξ )in(12)
ξ t ξ t ξ t t
Updateξ withSGDoranotheroptimizer
t
end
return ξ ;
T
SamplingoperatorsexamplesforBOED. Amongdensity-basedsamplers,wecanmentionscore-
basedMCMCsamplers,includingLangevindynamicsviatheUnadjustedLangevinAlgorithm(ULA)
andMetropolisAdjustedLangevinAlgorithm(MALA)(RobertsandTweedie,1996),Hamiltonian
MonteCarlo(HMC)samplers(HoffmanandGelman,2014). InSection6.2,anillustrationisgiven
withLangevinandsequentialMonteCarlo(SMC)tohandleasequentialdensity-basedBOEDtask.
Examples of data-based samplers are generative models such as denoising diffusions and others
(Sohl-Dicksteinetal.,2015;Hoetal.,2020;Songetal.,2021)thatusescore-matchingtechniques
(Hyvärinen, 2005; Vincent, 2011). Diffusion models, over a time index t, learn a score
function∇ logp (θ(t))usinganeuralnetworks (θ(t),t)withparametersϕ. Giventhislearned
θ t ϕ
approximation,itispossibletogeneratenewsamplesofθbyrunningabackwarddiffusionprocess
usings (θ(t),t)insteadofthetruescore. InBOED,weareinterestedinsamplingfromaconditional
ϕ
distribution p(θ|y,ξ) with the following objectives. First we need to sample from the expected
posteriorq (θ)whichrequiresconditioningontheobservationy. Second,insequentialdesign
ξ,N
problems(seesection6.1andAppendixD),weneedtoconditiononthehistoryofobservations
D andproducesamplesfromp(θ|D ). Boththeseissuescanbetackledintheframework
k−1 k−1
of denoising diffusions with linear inverse problems (Daras et al., 2024). When the likelihood
correspondstoameasurementY withY =A θ+ηandη∼N(0,Σ),inspiringrecentattempts,
ξ
such as (Corenflos et al., 2024; Cardoso et al., 2024), have addressed the problem of sampling
efficientlyfromp(θ|y,ξ)usingonlythelearnedscores (θ(t),t),withouttheneedforanykindof
ϕ
retraining. Denoisingdiffusionsincludeseveralvariantsbutsomeformulationsconsistofrunninga
stochasticdifferentialequation(SDE),seeAppendixC.Forconditionalsampling,thiswouldmean
runninganSDEwithaconditionalscore∇ logp(θ(t)|y,ξ),whichisintractable. See(42)andmore
θ
detailsinAppendixC.2. Asasolution,DouandSong(2024)proposeamethodnamedFPSthat
approximatesp(θ(t)|y,ξ)byp(θ(t)|y(t),ξ)withy(t)thenoisedobservationattimet. Asthescore
7Underreview
∇ logp(θ(t)|y(t),ξ)canbewrittenas∇ logp(θ(t))+∇ logp(y(t)|θ(t),ξ),wecanleveragethe
θ θ θ
learnedscores (θ(t),t)andtheclosedformof∇ logp(y(t)|θ(t),ξ)tosampleapproximatelyfrom
ϕ θ
p(θ|y,ξ)usingabackwardSDEwiththeapproximatescore,see(44)inAppendixC.2. Thisallows
to sample efficiently from p(θ|D ) and, using ∇ logq (θ′) = (cid:80)N ν ∇ logp(θ′|y ,ξ),
k−1 θ ξ,N i=1 i θ i
fromtheexpectedposteriorwiththeextensionof(44)below,wherey(t)isthenoisedy attimet,
i i
(cid:34) N (cid:35)
dθ′(t)
=
−β(t) θ′(t)−β(t)(cid:88)
ν ∇
logp(θ′(t)|y(t),ξ) dt+(cid:112)
β(t)dB . (19)
2 i θ i t
i=1
AnadditionalresamplingSMC-likestepcanalsobeaddedasexplainedinAppendixE.Theapproach
allowstohandlenewsequentialdata-basedBOEDtasksasillustratedinSection6.3.
ContrastiveOptimization. Optimizingξusingthegradientexpression(10)encouragestoselect
a ξ that gives either high probability p(y |θ ,ξ) to samples (θ ,y ) from p or low probability
i i i i ξ
p(y |θ′,ξ) to samples θ′ from q . This contrastive behaviour is also visible in (2) where the
j j j ξ,N
EIGisdefinedasthemeanovertheexperimentoutcomesoftheKLbetweenposteriorandprior
distributions. The expected posterior q is then used as a proxy to the intractable posterior, to
ξ,N
performthiscontrastiveoptimization. Figure2providesavisualizationofthiscontrastivebehavior
inthesourcelocalizationexampleofSection6.2. Itcorrespondstosetthenextdesignξtoavalue
thateliminatesthemostparameterθvalues(rightplot)amongthepossibleonesapriori(leftplot).
ThisisanalogoustoNoiseConstrastiveEstimation(GutmannandHyvärinen,2010)methodswhere
modelparametersarecomputedsothatthedatasamplesareasdifferentaspossiblefromthenoise
samples. AdditionalillustrationsaregiveninAppendixFigure5.
6 NUMERICAL EXPERIMENTS
Two sequential density-based (Section 6.2) and data-based (Section 6.3) BOED examples are
consideredtoillustratethatourmethodextendstothesequentialcaseinbothsettings.
6.1 SEQUENTIALBAYESIANEXPERIMENTALDESIGN
Inthesequentialsetting,asequenceofK experimentsisplannedwhilegraduallyaccountingforthe
successivelycollecteddata. Atstepk,wewishtopickthebestdesignξ givenpreviousoutcomes
k
D ={(y ,ξ ),...,(y ,ξ )}. Theexpectedinformationgaininthisscenarioisgivenby:
k−1 1 1 k−1 k−1
I (ξ,D )=E [KL(p(θ|y,ξ,D ),p(θ|D ))] ,
k k−1 p(y|ξ,Dk−1) k−1 k−1
where p(θ|D ) and p(θ|y,ξ,D ) act respectively as prior and posterior analogues to the
k−1 k−1
static case (2). See Appendix D for more detailed explanations. The main difference is that
we no longer have direct access to samples from the step k prior p(θ|D ). However, as
k−1
p(θ|D ) ∝
p(θ)(cid:81)k−1p(y
|θ,ξ ) and p(θ|y,ξ,D ) ∝
p(θ)p(y|θ,ξ)(cid:81)k−1p(y
|θ,ξ )
k−1 n=1 n n k−1 n=1 n n
we can still compute the score of these distributions and run sampling operators similar to (15)
and(18). ToemphasizetheirdependenceonD , theyaredenotedbyΣY,θ|Dk−1(p(s),ξ)and
k−1 s
Σθ′|Dk−1(q(s),ξ,ρ).
Examplesoftheseoperatorsareprovidedin(20)and(21)inSection6.2.
s
Evaluationmetricsandcomparison. WerefertoourmethodasCoDiff. InSection6.2,comparison
isprovidedwithotherrecentapproaches,namelyareinforcementlearning-basedapproachRL-BOED
fromBlauetal.(2022),thevariationalpriorcontrastiveestimationVPCEofFosteretal.(2020)
andarecentapproachnamedPASOA(Iolloetal.,2024)basedontemperedsequentialMonteCarlo
samplers. Wealsocomparewithanontemperedversionofthislatterapproach(SMC)andwitha
randombaseline,wheretheobservations{y ,·,y }aresimulatedwithdesignsgeneratedrandomly.
1 K
More details about these methods are given in Appendix F.2. To compare methods in terms of
informationgains,weusethesequentialpriorcontrastiveestimation(SPCE)andsequentialnested
MonteCarlo(SNMC)boundsintroducedinFosteretal.(2021)andusedinBlauetal.(2022). These
quantitiesallowtocomparemethodsontheproduceddesignsequencesonly,viatheir[SPCE,SNMC]
intervalswhichcontainthetotalEIG.TheirexpressionsaregiveninAppendixF.1. Wealsoprovide
theL Wassersteindistancebetweentheproducedsamplesandthetrueparameterθ.Formethodsthat
2
donotprovideposteriorestimationsorpoorqualityones(RL-BOED,VPCE,Random),wecompute
8Underreview
WassersteindistancesonposteriorsamplesobtainedbyusingtemperedSMContheirdesignand
observationsequences. Incontrast,SMCWassersteindistancesarecomputedontheSMCposterior
samples. InSection6.3,ourevaluationismainlyqualitative. Thepreviousmethodsdonotapplyand
wearenotawareofexistingattemptsthatcouldhandlesuchagenerativesetting.
6.2 SOURCESLOCATIONFINDING
WepresentasourcelocalizationexampleinspiredbyFosteretal.(2021);Blauetal.(2022).Thesetup
involvesC sourcesinR2,withunknownpositionsθ ={θ ,...,θ }. Thechallengeistodetermine
1 C
optimalmeasurementlocationstoaccuratelyinferthesourcespositions. Whenameasurementis
takenatlocationξ ∈ R2,thesignalstrengthisdefinedasµ(θ,ξ) = b+(cid:80)C αc where
c=1 m+∥θc−ξ∥2
2
α ,b,andmarepredefinedconstants. WeassumeastandardGaussianpriorforeachsourcelocation,
c
θ ∼N(0,I ),andmodelthelikelihoodaslog-normal: (logy |θ,ξ)∼N(logµ(θ,ξ),σ),withσ
c 2
representingthestandarddeviation. Forthisexperiment,wesetC =2,α =α =1,m=10−4,
1 2
b = 10−1, σ = 0.5, and plan K = 30 sequential design optimizations. In the notation of the
single loop Algorithm 2, we consider ΣY,θ|Dk−1(p(t),ξ ) and Σθ′|Dk−1(q(t),ξ ,ρˆ ) operators
t t t t t+1
that correspond respectively to the update of batch samples of size N = 200 and M = 200
{(y(t),θ(t))} and {θ′(t)} with ρˆ = (cid:80)N ν δ using Langevin diffusions.
i i i=1:N j j=1:M t+1 i=1 i y(t+1)
i
Makinguseoftheavailabilityofthelikelihoodinthisexample,samplingfromit,isstraightforward
andsamplingoperatoriterationssimplifyinto,fori=1:N andj =1:M
θ(t+1) =θ(t)−γ ∇ logp(θ(t)|D )+(cid:112) 2γ B and y(t+1) ∼p(y|θ(t+1),ξ ) (20)
i i t θ i k−1 t θ,t i i t
N
θ′(t+1) =θ′(t) −γ′(cid:88) ν ∇ logp(θ′(t) |y(t+1),ξ ,D )+(cid:112) 2γ′B . (21)
j j t i θ j i t k−1 t θ′,t
i=1
Inpractice,Langevindiffusioncangettrappedinlocalminimaandcausesthesamplingtobetoo
slowtokeeppacewiththeoptimizationprocess. Toaddressthis,weaugmenttheLangevindiffusion
withtheDiffusiveGibbs(DiGS)MCMCkernelproposedbyChenetal.(2024). DiGSisanauxiliary
variableMCMCmethodwheretheauxiliaryvariableX˜ isanoisyversionoftheoriginalvariableX.
DiGSenhancesmixingandhelpsescapelocalmodesbyalternatelysamplingfromthedistributions
p(x˜|x),whichintroducesnoiseviaGaussianconvolution,andp(x|x˜),whichdenoisesthesample
backtotheoriginalspaceusingascore-basedupdate(hereaLangevindiffusion). With400total
samples,eachmeasurementsteptakes2.9s. Thisnumberofsamplesisinsightfulasitisusuallythe
amountofsamplesonecanaffordtocomputeinthediffusionmodelsofSection6.3. Thewhole
experimentisrepeated100timeswithrandomsourcelocationseachtime. Figure3shows, with
respecttok,themedianforSPCE,theL Wassersteindistancesbetweenweightedsamplesandthe
2
true source locations and SNMC. CoDiff clearly outperforms all other methods, with significant
improvement,bothintermsofinformationgainandposteriorestimation. Itimprovesby30%the
non-myopic RL-BOED results on SPCE and provides much higher SNMC. The L Wasserstein
2
distanceistwoorderofmagnitudelower,suggestingthehigherqualityofourmeasurements.
Figure3: Sourcelocation. Medianover100rolloutsforSPCE,L Wassersteindistance(logarithmic
2
scale)andSNMCwithrespecttothenumberofexperimentsk. NumberofsamplesN +M =400
9Underreview
6.3 IMAGERECONSTRUCTIONWITHDIFFUSIONMODELS
Webuildanartificialexperimentaldesigntasktoillustratetheabilityofourmethodtohandledesign
parametersrelatedtoinverseproblemswheretheparameterθofinterestishighdimensional. More
specifically, we consider the task that consists of recovering an hidden image from only partial
observationsofitspixels. Theimagetoberecoveredisdenotedbyθ. Anexperimentcorresponds
tothechoiceofapixelξ aroundwhichanobservationmaskiscenteredandtheimagebecomes
visible. Themeasuredobservationyisthenamaskedversionofθ. Thelikelihoodderivesfromthe
modelY =A θ+ηwhereA isasquaremaskcenteredatξandηsomeGaussianvariable. For
ξ ξ
theimageprior,weconsideradiffusionmodeltrainedforgenerationoftheMNISTdataset(LeCun
etal.,1998). Thegoalisthustoselectsequentiallythebestcentralpixellocationsfor7×7masks
soastoreconstructanentire28×28MNISTimageinthesmallestnumberofexperiments. The
smallerthemaskthemoreinterestingitbecomestooptimallyselectthemaskcenters. Algorithm2is
usedwithdiffusion-basedsamplingoperatorsspecifiedinAppendixF.2.2. Thegaininoptimizingthe
maskplacementsisillustratedinFigure1andAppendixFigure8. Progressiveimagereconstructions
areshowninFigure4andAppendixFigures6and7. Thedigittoberecoveredisshowninthe1st
column. Thesuccessivelyselectedmasksareshown(redlinesquares)inthe2ndcolumnwiththe
resultinggraduallydiscoveredpartoftheimage. Thereconstructionpersecanbeestimatedfromthe
posteriorsamplesshowninthelast16columns. Ateachexperiment,theuppersub-rowshowsthe16
most-likelyreconstructedimages,whilethelowersub-rowshowsthe16lessprobableones. Asthe
numberofexperimentsincreasestheposteriorsamplesgraduallyconcentrateontherightdigit.
Figure4: Imagereconstruction.First6experiments(rows):imagegroundtruth,measurementatexperimentk,
samplesfromcurrentpriorp(θ|D ),withbest(resp. worst)weightsinupper(resp. lower)sub-row. The
k−1
samplesincorporatepastmeasurementinformationastheprocedureadvances.Eachdesignstepstakes∼7.3s
7 CONCLUSION
Wepresentedanewapproachtogradient-basedBOEDthatallowsveryefficientimplementations.
Introducingtheconceptofexpectedposteriordistribution,weproposedanewEIGgradientestimator,
which allows the EIG maximization to be performed in a bi-level optimization framework and
withoutresortingtolowerbounds. Theperformancewasillustratedinatraditionaldensity-based
settingwithsuperioraccuracyandlowercomputationalcostcomparedtostate-of-the-artmethods.
10Underreview
In addition, the possibility of our method to also handle data-based sampling represents, to our
knowledge,thefirstextensionofBOEDtodiffusion-basedgenerativemodels. Byintegratingthe
highlysuccessfulframeworkofdiffusionmodelsforoursamplingoperators,wewereabletooptimize
adesignparameterξconcurrentlywiththediffusionprocess.Thiswasillustratedinanewapplication
forBOEDinvolvinghighdimensionalimageparameters. Thefoundationofourapproachlieson
conditionaldiffusionmodelsandtheirapplicationtoinverseproblems. Thankstothisadvancement,
there are as many new potential applications of BOED as there are trained diffusion models for
specificinverseproblemtasks. Currently,mostmodelsassumealinearforwardmodel. However,
thenon-linearsettingisanactivefieldofresearch,andadvancementsinthisareacouldbedirectly
appliedtoourframework. Theapplicabilityofourmethodcouldalsobeextendedbyconsidering
settingswithnoexplicitexpressionofthelikelihoodandinvestigatingsimulation-basedinference
such as developed by Ivanova et al. (2021); Kleinegesse and Gutmann (2021); Kleinegesse et al.
(2020). Inaddition,althoughindensity-basedBOED,wehaveshownthatgreedyapproachescould
outperform long-sighted reinforcement learning procedures, in a data-based setting, it would be
interestingtoinvestigateanextensiontononmyopicapproachessuchasIqbaletal.(2024).
REFERENCES
Alquier,P.(2024). User-friendlyIntroductiontoPAC-BayesBounds. FoundationsandTrendsin
MachineLearning,17(2):174–303.
Amzal,B.,Bois,F.,Parent,E.,andRobert,C.P.(2006). Bayesian-OptimalDesignviaInteracting
ParticleSystems. JournaloftheAmericanStatisticalAssociation,101(474):773–785.
Anderson,B.D.(1982). Reverse-timediffusionequationmodels. StochasticProcessesandtheir
Applications,12(3):313–326.
Ao,Z.andLi,J.(2024). OnEstimatingtheGradientoftheExpectedInformationGaininBayesian
ExperimentalDesign. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,
pages20311–20319.
Babuschkin,I.,Baumli,K.,Bell,A.,Bhupatiraju,S.,Bruce,J.,Buchlovsky,P.,Budden,D.,Cai,T.,
Clark,A.,Danihelka,I.,Dedieu,A.,Fantacci,C.,Godwin,J.,Jones,C.,Hemsley,R.,Hennigan,
T.,Hessel,M.,Hou,S.,Kapturowski,S.,Keck,T.,Kemaev,I.,King,M.,Kunesch,M.,Martens,
L.,Merzic,H.,Mikulik,V.,Norman,T.,Papamakarios,G.,Quan,J.,Ring,R.,Ruiz,F.,Sanchez,
A.,Schneider,R.,Sezener,E.,Spencer,S.,Srinivasan,S.,Stokowiec,W.,Wang,L.,Zhou,G.,and
Viola,F.(2020). TheDeepMindJAXEcosystem.
Blau,T.,Bonilla,E.V.,Chades,I.,andDezfouli,A.(2022). Optimizingsequentialexperimental
designwithdeepreinforcementlearning. InProceedingsofthe39thInternationalConferenceon
MachineLearning(ICML),volume162,pages2107–2128.PMLR.
Bradbury,J.,Frostig,R.,Hawkins,P.,Johnson,M.J.,Leary,C.,Maclaurin,D.,Necula,G.,Paszke,
A.,VanderPlas,J.,Wanderman-Milne,S.,andZhang,Q.(2020). JAX:composabletransformations
ofPython+NumPyprograms.
Cardoso,G.,elidrissi,Y.J.,Corff,S.L.,andMoulines,E.(2024). MonteCarloguidedDenoising
DiffusionmodelsforBayesianlinearinverseproblems. InTheTwelfthInternationalConference
onLearningRepresentations(ICLR).
Carvalho,L.,Villela,D.,Coelho,F.,andBastos,L.(2022). BayesianInferencefortheWeightsin
LogarithmicPooling. BayesianAnalysis,1(1):1–29.
Chaloner,K.andVerdinelli,I.(1995). Bayesianexperimentaldesign: Areview. StatisticalScience,
10(3):273–304.
Chatterjee,S.andDiaconis,P.(2018). Thesamplesizerequiredinimportancesampling. TheAnnals
ofAppliedProbability,28(2):1099–1135.
Chen,W.,Zhang,M.,Paige,B.,Hernández-Lobato,J.M.,andBarber,D.(2024). DiffusiveGibbs
sampling. InProceedingsofthe41stInternationalConferenceonMachineLearning(ICML),
volume235,pages7731–7747.PMLR.
11Underreview
Corenflos,A.,Zhao,Z.,Särkkä,S.,Sjölund,J.,andSchön,T.B.(2024). Conditioningdiffusion
modelsbyexplicitforward-backwardbridging. https://arxiv.org/abs/2405.13794.
Dagréou,M.,Ablin,P.,Vaiter,S.,andMoreau,T.(2022). Aframeworkforbileveloptimizationthat
enablesstochasticandglobalvariancereductionalgorithms. InAdvancesinNeuralInformation
ProcessingSystems.
Daras, G., Chung, H., Lai, C.-H., Mitsufuji, Y., Milanfar, P., Dimakis, A. G., Ye,
C., and Delbracio, M. (2024). A survey on diffusion models for inverse problems.
https://giannisdaras.github.io/publications/diffusion_survey.pdf.
Demidovich,Y.,Malinovsky,G.,Sokolov,I.,andRichtárik,P.(2023). AGuideThroughtheZooof
BiasedSGD. InThirty-seventhConferenceonNeuralInformationProcessingSystems.
Dhariwal,P.andNichol,A.(2021). DiffusionmodelsbeatGANxonimagesynthesis. Advancesin
neuralinformationprocessingsystems,34:8780–8794.
Donsker,M.andVaradhan,S.(1976). AsymptoticevaluationofcertainMarkovprocessexpectations
forlargetime—III. CommunicationsonPureandAppliedMathematics,29(4):389–461.
Dou,Z.andSong,Y.(2024). Diffusionposteriorsamplingforlinearinverseproblemsolving: A
filteringperspective. InTheTwelfthInternationalConferenceonLearningRepresentations(ICLR).
Drovandi, C. C., McGree, J., and Pettitt, A. N. (2013). Sequential Monte Carlo for Bayesian
sequentiallydesignedexperimentsfordiscretedata. ComputationalStatistics&DataAnalysis,
57(1):320–335.
Foster,A.,Ivanova,D.R.,Malik,I.,andRainforth,T.(2021). DeepAdaptiveDesign: Amortizing
SequentialBayesianExperimentalDesign. InProceedingsofthe38thInternationalConference
onMachineLearning(ICML),volume161,pages3384–3395.PMLR.
Foster,A.,Jankowiak,M.,Bingham,E.,Horsfall,P.,Teh,Y.W.,Rainforth,T.,andGoodman,N.
(2019). VariationalBayesianOptimalExperimentalDesign. InAdvancesinNeuralInformation
ProcessingSystems,pages14059–14070.
Foster,A.,Jankowiak,M.,O’Meara,M.,Teh,Y.W.,andRainforth,T.(2020). AUnifiedStochastic
Gradient Approach to Designing Bayesian-Optimal Experiments. In Proceedings of the 23rd
InternationalConferenceinArtificialIntelligenceandStatistics,volume108,pages2959–2969.
Goda,T.,Hironaka,T.,Kitade,W.,andFoster,A.(2022). UnbiasedMLMCStochasticGradient-
BasedOptimizationofBayesianExperimentalDesigns. SIAMJournalonScientificComputing,
44(1):A286–A311.
Gutmann,M.andHyvärinen,A.(2010). Noise-contrastiveestimation: Anewestimationprinciple
forunnormalizedstatisticalmodels. InProceedingsofthethirteenthinternationalconferenceon
artificialintelligenceandstatistics,pages297–304.JMLRWorkshopandConferenceProceedings.
Ho,J.,Jain,A.,andAbbeel,P.(2020). Denoisingdiffusionprobabilisticmodels. InProceedingsof
the34thInternationalConferenceonNeuralInformationProcessingSystems.
Hoffman,M.D.andGelman,A.(2014). TheNo-U-TurnSampler: AdaptivelySettingPathLengths
inHamiltonianMonteCarlo. JournalofMachineLearningResearch,15(47):1593–1623.
Hong,M.,Wai,H.,Wang,Z.,andYang,Z.(2023). Atwo-timescalestochasticalgorithmframework
forbileveloptimization: Complexityanalysisandapplicationtoactor-critic. SIAMJournalon
Optimization,33:147–180.
Huan, X., Jagalur, J., and Marzouk, Y. (2024). Optimal experimental design: Formulations and
computations. ActaNumerica,33:715–840.
Hyvärinen,A.(2005). EstimationofNon-NormalizedStatisticalModelsbyScoreMatching. Journal
ofMachineLearningResearch,6(24):695–709.
12Underreview
Iollo,J.,Heinkelé,C.,Alliez,P.,andForbes,F.(2024). PASOA-PArticlebaSedBayesianOptimal
Adaptive design. In Proceedings of the 41st International Conference on Machine Learning
(ICML),volume235,pages21020–21046.PMLR.
Iqbal, S., Corenflos, A., Särkkä, S., and Abdulsamad, H. (2024). Nesting particle filters for
experimentaldesignindynamicalsystems. InProceedingsofthe41stInternationalConferenceon
MachineLearning(ICML),volume235.PMLR.
Ivanova, D. R., Foster, A., Kleinegesse, S., Gutmann, M. U., and Rainforth, T. (2021). Implicit
deepadaptivedesign: Policy-basedexperimentaldesignwithoutlikelihoods. Advancesinneural
informationprocessingsystems,34:25785–25798.
Ivanova, D. R., Hedman, M., Guan, C., and Rainforth, T. (2024). Step-DAD: Semi-Amortized
Policy-BasedBayesianExperimentalDesign. ICLR2024WorkshoponData-centricMachine
LearningResearch(DMLR).
Jordan,R.,Kinderlehrer,D.,andOtto,F.(1998). ThevariationalformulationoftheFokker–Planck
equation. SIAMjournalonmathematicalanalysis,29(1):1–17.
Kingma, D. and Ba, J. (2015). Adam: A method for stochastic optimization. In International
ConferenceonLearningRepresentations(ICLR),SanDiega,CA,USA.
Kleinegesse, S., Drovandi, C., and Gutmann, M. U. (2020). Sequential Bayesian Experimental
DesignforImplicitModelsviaMutualInformation. BayesianAnalysis,16:773–802.
Kleinegesse, S. and Gutmann, M. U. (2021). Gradient-based Bayesian Experimental Design for
ImplicitModelsusingMutualInformationLowerBounds. ArXiv,abs/2105.04379.
Knoblauch,J.,Jewson,J.,andDamoulas,T.(2022). AnOptimization-CentricViewonBayes’Rule:
ReviewingandGeneralizingVariationalInference. JournalofMachineLearningResearch,23(1).
Korba,A.andSalim,A.(2022). Samplingasfirst-orderoptimizationoveraspaceofprobability
measures. Tutorialatthe39thInternationalConferenceonMachineLearning(ICML).
Kullback,S.(1959). InformationTheoryandStatistics. Wiley.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to
documentrecognition. ProceedingsoftheIEEE,86(11):2278–2324.
Liu, Y. and Tajbakhsh, S. D. (2024). Stochastic Optimization Algorithms for Problems with
ControllableBiasedOracles. https://arxiv.org/abs/2306.07810.
Marion, P., Korba, A., Bartlett, P., Blondel, M., Bortoli, V. D., Doucet, A., Llinares-López, F.,
Paquette,C.,andBerthet,Q.(2024). Implicitdiffusion: Efficientoptimizationthroughstochastic
sampling. https://arxiv.org/abs/2402.05468.
Minka,T.(2005). Divergencemeasuresandmessagepassing. Technicalreport,Research,Microsoft.
Papamakarios,G.,Nalisnick,E.,Rezende,D.J.,Mohamed,S.,andLakshminarayanan,B.(2021).
Normalizingflowsforprobabilisticmodelingandinference.JournalofMachineLearningResearch,
22(1).
Rainforth,T.,Cornish,R.,Yang,H.,Warrington,A.,andWood,F.(2018). OnnestingMonteCarlo
estimators. InInternationalConferenceonMachineLearning(ICML),pages4267–4276.PMLR.
Rainforth, T., Foster, A., Ivanova, D. R., and Bickford Smith, F. (2024). Modern Bayesian
ExperimentalDesign. StatisticalScience,39(1):100–114.
Rhee,C.-h.andGlynn,P.W.(2015). UnbiasedestimationwithsquarerootconvergenceforSDE
models. OperationsResearch,63(5):1026–1043.
Roberts,G.O.andTweedie,R.L.(1996). ExponentialconvergenceofLangevindistributionsand
theirdiscreteapproximations. Bernoulli,2(4):341–363.
13Underreview
Sebastiani,P.andWynn,H.P.(2000).MaximumentropysamplingandoptimalBayesianexperimental
design. JournaloftheRoyalStatisticalSociety: SeriesB(StatisticalMethodology).
Sohl-Dickstein,J.,Weiss,E.,Maheswaranathan,N.,andGanguli,S.(2015). DeepUnsupervised
Learning using Nonequilibrium Thermodynamics. In Proceedings of the 32nd International
ConferenceonMachineLearning(ICML),volume37,pages2256–2265.PMLR.
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Ermon,S.,andPoole,B.(2021). Score-
basedgenerativemodelingthroughstochasticdifferentialequations. InTheNinthInternational
ConferenceonLearningRepresentations(ICLR).
Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural
Computation,23(7):1661–1674.
Xu, M., Quiroz, M., Kohn, R., and Sisson, S. A. (2019). Variance reduction properties of the
reparameterization trick. In The 22nd international conference on artificial intelligence and
statistics,pages2711–2720.PMLR.
Yang, J., Ji, K., and Liang, Y. (2021). Provably Faster Algorithms for Bilevel Optimization. In
AdvancesinNeuralInformationProcessingSystems.
14Underreview
A TWO EXPRESSIONS FOR THE EIG GRADIENT
Both approaches presented below, that of Goda et al. (2022) and Ao and Li (2024), start from
EIG gradient expressions derived using a reparameterization trick. Using the change of variable
Y =T (U),wecanderivethefollowingexpressionfortheEIGgradient,
ξ,θ
∇ I(ξ)=E [∇ logp(T (U)|θ;ξ)]−E [∇ logp(T (U)|ξ)] . (22)
ξ pU(u)p(θ) ξ ξ,θ pU(u)p(θ) ξ ξ,θ
Thefirsttermin(22)involvesonlytheknownlikelihoodandisgenerallynotproblematic. Forthe
secondterm,wecanuse,
∇ p(T (U)|ξ)
∇ logp(T (U)|ξ)= ξ ξ,θ (23)
ξ ξ,θ p(T (U)|ξ)
ξ,θ
withp(T (U)|ξ)=E (cid:2) p(T (U)|θ′,ξ)(cid:3) and
ξ,θ p(θ′) ξ,θ
∇ p(T (U)|ξ)=∇ E (cid:2) p(T (U)|θ′,ξ)(cid:3)
ξ ξ,θ ξ p(θ′) ξ,θ
=E (cid:2) ∇ p(T (U)|θ′,ξ)(cid:3) (24)
p(θ′) ξ ξ,θ
=E (cid:2) p(T (U)|θ′,ξ)∇ logp(T (U)|θ′,ξ)(cid:3) . (25)
p(θ′) ξ,θ ξ ξ,θ
Subsequently,twoexpressionsoftheEIGgradientcanbederiveddependingonwhichof(24)or(25)
isused. Using(24)andp(T (U)|ξ)=E (cid:2) p(T (U)|θ′,ξ)(cid:3) ,itcomes
ξ,θ p(θ′) ξ,θ
(cid:34) E (cid:2) ∇ p(T (U)|θ′,ξ)(cid:3)(cid:35)
∇ I(ξ)=E ∇ logp(T (U)|θ;ξ)− p(θ′) ξ ξ,θ
ξ pU(u)p(θ) ξ ξ,θ E (cid:2) p(T (U)|θ′,ξ)(cid:3)
p(θ′) ξ,θ
(cid:34) E (cid:2) h(ξ,Y,θ,θ′)(cid:3)(cid:35)
=E g(ξ,Y,θ,θ)− p(θ′) . (26)
pξ E (cid:2) p(Y|θ′,ξ)(cid:3)
p(θ′)
with
g(ξ,y,θ,θ′)=∇ logp(T (u)|θ′,ξ)
ξ ξ,θ |u=T−1(y)
ξ,θ
and
h(ξ,y,θ,θ′)=∇ p(T (u)|θ′,ξ) .
ξ ξ,θ |u=T−1(y)
ξ,θ
Considering, in the second term, an additional importance distribution q(θ′|y,θ,ξ) leads to the
expressionusedinGodaetal.(2022),
 E (cid:104) p(θ′) h(ξ,Y,θ,θ′)(cid:105)
∇ ξI(ξ)=E pξg(ξ,Y,θ,θ)− Eq(θ′|Y,θ,ξ) q (cid:104)(θ′|Y p(, θθ ′, )ξ) p(Y|θ′,ξ)(cid:105)  . (27)
q(θ′|Y,θ,ξ) q(θ′|Y,θ,ξ)
Itcanbeusedtoderiveestimatorsoftheform,
N  1 (cid:80)M p(θ′ i,j) h(ξ,y ,θ ,θ′ )
∇ ξI(ξ)≈ N1 (cid:88) g(ξ,y i,θ i,θ i)− M
1
(cid:80)j= M1 q(θ′ i,j| py (i θ,θ
′
i,i j, )ξ) p(yi |θ′i ,ξi ),j  , (28)
i=1 M j=1 q(θ′ i,j|yi,θi,ξ) i i,j
where {(y ,θ )} are simulated from the joint distribution p and for each i = 1 : N,
i i i=1:N ξ
{θ′ } is a sample from q(·|y ,θ ,ξ). Goda et al. (2022) use (28) with N = 1. Even
i,j j=1:M i i
withperfectsampling,thisestimatorisnotunbiasedduetotheratiointhesecondtermbutcanbe
de-biasedfollowingRheeandGlynn(2015). TherandomizedMLMCprocedureofRheeandGlynn
(2015)isapost-hocgeneralprocedurethatcanbemoregenerallyappliedtode-biasasequenceof
possiblybiasedestimators,providedtheestimatorsareconsistent.
Alternatively,using(25)instead,anotherexpressionoftheEIGgradientcanbederived. Replacing
(25)in(23),itcomes,
(cid:20) p(T (U)|θ′,ξ)∇ logp(T (U)|θ′,ξ)(cid:21)
∇ logp(T (U)|ξ)=E ξ,θ ξ ξ,θ
ξ ξ,θ p(θ′) p(T (U)|ξ)
ξ,θ
=E (cid:2) ∇ logp(T (U)|θ′,ξ)(cid:3) ,
p(θ′|Tξ,θ(U),ξ) ξ ξ,θ
15Underreview
which,withthedefinitionofgabove,leadsto
∇ I(ξ)=E (cid:2) g(ξ,Y,θ,θ)−E (cid:2) g(ξ,Y,θ,θ′)(cid:3)(cid:3) . (29)
ξ pξ p(θ′|Y,ξ)
Thisalternativeexpression(29)isthestartingpointofAoandLi(2024),whosubsequentlyusethe
followingestimator,
N
∇ I(ξ)≈ 1 (cid:88)(cid:2) g(ξ,y ,θ ,θ )−E (cid:2) g(ξ,y ,θ ,θ′)(cid:3)(cid:3) ,
ξ N i i i q(θ′|yi,ξ) i i
i=1
where{(y ,θ )} isasbeforeasamplefromthejointdistributionp andwhereforeachy ,
i i i=1:N ξ i
q(θ′|y ,ξ)isatractableapproximationoftheintractableposteriorp(θ′|y ,ξ). Morespecifically,Ao
i i
andLi(2024)proposetoapproximateeachposteriordistributionbyq(θ′|y ,ξ) = 1 (cid:80)M δ ,
i M j=1 θ′ i,j
usingasample{θ′ } fromanMCMCprocedure. ItfollowsthenestedMonteCarloestimator
i,j j=1:M
below,
 
N M
1 (cid:88) 1 (cid:88)
∇ ξI(ξ)≈
N
g(ξ,y i,θ i,θ i)−
M
g(ξ,y i,θ i,θ i,j) . (30)
i=1 j=1
B LOGARITHMIC POOLING AS A GOOD IMPORTANCE SAMPLING PROPOSAL
Whenconsideringimportancesamplingwithaproposaldistributionq andatargetdistributionp,
ChatterjeeandDiaconis(2018)provedthatundercertainconditions,thenumberofsimulationdraws
requiredforbothimportancesamplingandselfnormalizedimportancesampling(SNIS)estimatorsto
havesmallL errorwithhighprobabilitywasroughlyexp(KL(p,q)),seeTheorem1.2inChatterjee
1
and Diaconis (2018) for SNIS. Similarly, selecting a proposal distribution which minimizes the
importancesamplingestimatorvarianceisequivalenttofindingadistributionwithsmallχ2-distance
top,seee.g. AppendixEofMinka(2005). Moregenerally,findingagoodproposalqislinkedtothe
problemofminimizingα-divergencesorf-divergencebetweenpandq,whicharejointlyconvexin
pandq,seeMinka(2005). Inthiswork,weconsiderKL(q,p)asameasureofproximitybetweenp
andq. Thischoiceisultimatelyarbitrarybuthastheadvantageofleadingtoaninterpretableproposal
withinterestingsamplingproperties. Tojustifytheexpectedposteriorq in(9)anditsusein(10),
ξ,N
wethenuseLemma2belowtoshowthatfor(cid:80)N ν = 1,thedistributionq∗ thatminimizesthe
i=1 i
weightedsumoftheKLagainsteachposteriorp(θ|y ,ξ),i.e.
(cid:80)N
ν KL(q,p(θ|y ,ξ))is
i i=1 i i
N
(cid:89)
q∗(θ) ∝ p(θ) p(y |θ,ξ)νi (31)
i
i=1
N
(cid:89)
∝ p(θ|y ,ξ)νi , (32)
i
i=1
which is the logarithmic pooling (or geometric mixture) of the respective posterior distributions
p(θ|y ,ξ). Lemma2resultsfromanapplicationofalemmamentionedbyAlquier(2024)(Lemma
i
2.2 therein), and recalled below in Lemma 1. This Lemma 1 has been known since Kullback
(Kullback,1959)inthecaseofafiniteparameterspaceΘ,butthegeneralcaseisduetoDonskerand
Varadhan(DonskerandVaradhan,1976). RecallthatP(Θ)denotesthesetofprobabilitymeasures
onΘandpagivenprobabilitymeasureinP(Θ).
Lemma1(DonskerandVaradhan’svariationalformula) Foranymeasurable,boundedfunction
f :Θ→R,thesupremumwithrespecttoq ∈P(Θ)of
E [f(θ)]−KL(q,p)
q
isthefollowingGibbsmeasurep definedbyitsdensitywithrespecttop,
f
exp(f(θ))
dp = dp.
f E [exp(f(θ))]
p
ThefollowingLemma2isanapplicationofLemma1.
16Underreview
Lemma2 For a given probability measure p ∈ P(Θ) and a measure ρ on Y (not necessarily a
probabilitymeasure),defineforanyprobabilitymeasureq ∈P(Θ)
ℓ(q) = E [E [logp(Y|θ)]]−KL(q,p). (33)
q Y∼ρ
ItresultsfromtheDonskerandVaradhan’svariationalformulaLemma1thatthesupremumofℓ(q)
withrespecttoqisreachedfortheGibbsmeasureq∗definedbyitsdensitywithrespecttop,
q∗(θ)∝p(θ) exp(E [logp(Y|θ)]).
Y∼ρ
Inadditionmaximizingℓisequivalenttominimising
E [KL(q(θ),p(θ|Y))]
Y∼ρ
whichmeansthatq∗isthemeasurethatminimizestheKLtoeachp(θ|y)onaveragewithrespectto
y.
ProofofLemma2 Theexpressionofq∗resultsfromadirectapplicationofLemma1tof(θ)=
E [logp(Y|θ)] assuming it is measurable and bounded as a function of θ (to be checked in
Y∼ρ
practice). Thesecondpartresultsfromrewritingℓas
(cid:20) (cid:20) (cid:21)(cid:21)
log(p(Y|θ)p(θ))
ℓ(q) = E E
Y∼ρ θ∼q logq(θ)
= −E [KL(q,p(θ|Y)]+E [logp(Y)] .
Y∼ρ Y∼ρ
Example: asalreadymentionedourexpectedposteriorq correspondstotheapplicationofthis
ξ,N
resulttoρ=(cid:80)N
ν δ
with(cid:80)N
ν =1.
i=1 i yi i=1 i
Remark 1: If ρ = δ , or ρ =
(cid:80)N
δ , we recover the standard variational formulation of the
y i=1 yi
posterior distribution (see e.g. Table 1 in (Knoblauch et al., 2022)). The posterior distribution
p(θ|y ,...,y ) differs from the logarithmic pooling (for which the weights ν sum to 1) in the
1 N i
relativeweightgiventotheprior. Theresultisvalidforverygeneralℓnotnecessarilyexpressedas
anexpectation.
Remark2: Regardinglogarithmicpooling,theresultissimilartoaresultinCarvalhoetal.(2022)
(Remark3.1therein)byshowingthat,inthecaseofthesumoftheKL,(cid:80)N
KL(q,p(θ|y )),the
i=1 i
optimalpoolingweightsareequal,ν = 1.
i N
Remark3: Theexpectedposteriordistributioncanalsoberecoveredasaconstrainedmeanfield
solution. Indeed,itiseasytoshowthatq∗ isalsothemeasurethatminimizestheKLbetweenthe
jointdistributionandaproductformapproximationwhereoneofthefactorisfixedtoρ(y),
q∗ =arg min KL(q(θ)ρ(y),p(θ,Y)).
q∈P(Θ)
C DIFFUSION-BASED GENERATIVE MODELS
C.1 DENOISINGDIFFUSIONMODELS
Givenadistributionp ∈P(Θ)onlyavailablethroughasetofsamplesofθ,diffusionmodelsare
0
based onthe additionof noise tothe availablesamples insuch a mannerthat allows to learnthe
reverseprocessthat"denoises"thesamples. Thislearnedprocesscanthenbeexploitedtogenerate
newsamplesbydenoisingrandomnoisesamplesuntilwegetbacktotheoriginaldatadistribution.
Asanappropriatenoisingprocess,inourexperimentswerantheVariancePreservingSDEfrom
DhariwalandNichol(2021):
dθ˜(t) =−β(t) θ˜(t) dt+(cid:112) β(t)dB˜ (34)
2 t
whereβ(t)>0isalinearnoiseschedulethatcontrolstheamountofnoiseaddedattimet. Solving
SDE(34)leadsto
(θ˜(t) |θ˜(0)
)∼N(cid:18)
θ˜(0)
exp(−1(cid:90) t
β(s)ds),
(1−exp(−(cid:90) t β(s)ds))I(cid:19)
, (35)
2
0 0
17Underreview
whichcanbewrittenas
√ √ (cid:90) t
θ˜(t)
= α¯
θ˜(0)
+ 1−α¯ ϵ with α¯ =exp(− β(s)ds) and (36)
t t t
0
whereϵ ∼ N(0,I)isastandardGaussianrandomvariable. Samplesfromp aretransformedto
0
samplesfromastandardGaussiandistributionaftersomelargetimeT.
Thereversedenoisingprocesscanthenbewrittenasthereverseofthediffusionprocess(34),which
asstatedbyAnderson(1982)is:
(cid:20) (cid:21)
dθ(t) = −β(t) θ(t)−β(t)∇ logp (θ(t)) dt+(cid:112) β(t)dB , (37)
2 θ t t
wherep isthedistributionofθ˜(t) from(34). SolvingthisreverseSDE,thedistributionofθ(T) is
t
closedtop forlargeT,whichallowsapproximatesamplingfromp .
0 0
Thescorefunction∇ logp (θ(t))ofthenoisydatadistributionattimetisintractableandisthen
θ t
estimated by learning a neural network s (θ,t) with parameters ϕ. Score matching (Hyvärinen,
ϕ
2005)isamethodtotrains byminimizingthefollowingloss:
ϕ
E (cid:2) ||s (θ,t)−∇ logp (θ)||2(cid:3) . (38)
pt(θ) ϕ θ t
Asp (θ)isstillunknownandonlysamplesfromp
(θ|θ˜(0)
)in(35)areavailable,Songetal.(2021)
t t
rewritethislossfunctionas:
(cid:104) (cid:105)
E E E λ(t)||s (θ,t)−∇ logp (θ|θ(0))||2 (39)
t∼U[0,T] p0(θ(0)) pt(θ|θ(0)) ϕ θ t
whereλ(t)>0isaweightingfunctionthatallowstofocusmoreoncertaintimestepsthanothers. It
iscommontotakeλ(t)inverselyproportionaltothevarianceof(35)attimet.
Oncetheneuralnetworks hasbeentrainedbyminimizing(39), itcanbeusedtogeneratenew
ϕ
samplesapproximatelydistributedasthetargetdistributionp byrunninganumericalschemeonthe
0
reverseSDE(37). ByrunningforexampletheEuler-Maruyamaschemeon(37),wegetthefollowing
updatestepforthereverseprocess:
θ(t−∆t) =θ(t)+ β(t) θ(t)∆t+β(t)s (θ(t),t)∆t+(cid:112) β(t)∆tϵ. (40)
2 ϕ
Wecanthengeneratesamplesapproximatelyfromp byrunningthereverseprocess(40)witha
0
smallenough∆t.
C.2 CONDITIONALDIFFUSIONMODELS
Conditionaldiffusionmodelsarisewhen,forsomemeasurementy,wewanttoproducesamplesfrom
someconditionaldistributionp (θ|y). Samplingfromconditionaldistributionsisaproblemthat
0
arisesininverseproblems. Whenusingdiffusionmodels,numeroussolutionshavebeeninvestigated
asmentionedinaveryrecentreview(Darasetal.,2024). Wespecifyinthissectiontheapproach
adoptedforourapplications. Withtheapplicationtoexperimentaldesigninmind,weassumehere
that
Y =A θ+η (41)
ξ
whereη ∼N(0,σ2I)isthemeasurementnoise,A istheoperatorthatrepresentstheexperimentat
ξ
ξ.
Samplingfromtheconditionaldistributionp(θ|y,ξ)canbedonebyrunningthereversediffusion
processontheconditionalSDE:
(cid:20) (cid:21)
dθ(t) = −β(t) θ(t)−β(t)∇ logp(θ(t)|y,ξ) dt+(cid:112) β(t)dB , (42)
2 θ t
withtheusualscore∇ logp (θ(t))replacedbytheconditionnalscore∇ logp (θ(t)|y,ξ). The
θ t θ t
mainobjectiveofconditionalSDEistogeneratesamplesfromtheconditionaldistributionp(θ|y,ξ)
18Underreview
without retraining a new neural network s for the new conditional score. By noting that the
ϕ
conditionalscorecanbewrittenas:
∇ logp (θ(t)|y,ξ)=∇ logp (y|θ(t),ξ)+∇ logp (θ(t)) (43)
θ t θ t θ t
we can leverage a pre-computed neural network s that was trained to estimate the score
ϕ
∇ logp (θ(t)) in the unconditional case. If we know how to evaluate the first term
θ t
∇ logp (y|θ(t),ξ), we can then run the reverse process (42) to generate samples from the
θ t
conditionaldistributionp(θ|y,ξ). Unfortunatelythistermdoesnothaveaclosedformexpression.
Asasolution,DouandSong(2024)proposetoapproximatetheintractable∇ logp (y|θ(t),ξ)by
θ t
thetractable∇ logp (y(t)|θ(t),ξ)wherey(t)isanoisyversionofyattimet. Then,thefollowing
θ t
backwardSDEcanberuntogeneratesamplesfromtheconditionaldistributionp(θ|y,ξ):
(cid:20) (cid:21)
dθ(t) = −β(t) θ(t)−β(t)∇ logp (y(t)|θ(t),ξ)−β(t)∇ logp (θ(t)) dt+(cid:112) β(t)dB (44)
2 θ t θ t t
Thesequenceofnoisyy(t)canbegeneratedwithanoisingprocesslike(36),
√ √ (cid:90) t
y(t) = α¯ y+ 1−α¯ A ϵ with α¯ =exp(− β(s)ds), (45)
t t ξ t
0
whichusingtheforwardmodel(41)canbewrittenas:
√
y(t) =A θ(t)+ α¯ η . (46)
ξ t
Wecanthenevaluate∇ logp (y(t)|θ(t),ξ)as:
θ t
1
∇ logp (y(t)|θ(t),ξ)= AT(y(t)−A θ(t)). (47)
θ t σ2α¯ ξ ξ
t
D SEQUENTIAL BAYESIAN EXPERIMENTAL DESIGN
Inthisframework,experimentalconditionsaredeterminedsequentially,makinguseofmeasurements
that are gradually made. This sequential view is referred to as sequential or iterated design.
In a sequential setting, we assume that we plan a sequence of K experiments. For each
experiment, we wish to pick the best ξ using the data that has already been observed D =
k k−1
{(y ,ξ ),...,(y ,ξ )}. Given this design, we conduct an experiment using ξ and obtain
1 1 k−1 k−1 k
outcomey . Bothξ andy arethenaddedtoD foranewsetD =D ∪(y ,ξ ). After
k k k k−1 k k−1 k k
eachstep,ourbeliefaboutθisupdatedandsummarisedbythecurrentposteriorp(θ|D ),which
k
actsasthenextprioratstepk+1. Whentheobservationsareassumedconditionallyindependent,it
comes,
k
(cid:89)
p(θ|D )∝p(θ) p(y |θ,ξ ) (48)
k n n
n=1
and
k−1
(cid:89)
p(y,θ|ξ,D )∝p(θ)p(y|θ,ξ) p(y |θ,ξ ). (49)
k−1 n n
n=1
Agreedydesigncanbeseenaschoosingeachdesignξ asifitwasthelastone. Thismeansthatξ
k k
ischosenasξ∗ thevaluethatmaximizes
k
ξ∗ =argmaxI (ξ,D )
k k k−1
ξ
where
(cid:34) (cid:35)
pk(θ,Y)
I (ξ,D )=E log ξ = MI(pk) (50)
k k−1 pk ξ p(Y|ξ,D k−1)p(θ|D k−1) ξ
19Underreview
withpk denotingthejointdistributionp(y,θ|ξ,D ) = p(y|θ,ξ) p(θ|D ). Distributionpk
ξ k−1 k−1 ξ
involvesthecurrentpriorp(θ|D ),whichisnotavailableinclosed-formandisnotstraightforward
k−1
tosamplefrom. Distributionpk canbewrittenasaGibbsdistributionbydefiningthepotentialV as
ξ k
pk(y,θ)∝exp(−V (y,θ,ξ))
ξ k
k−1
(cid:88)
withV (y,θ,ξ)=−logp(θ)−logp(y|θ,ξ)− logp(y |θ,ξ )
k n n
n=1
=V(y,θ,ξ)+V˜ (θ),
k
where V(y,θ,ξ) has been already defined in Section 4. Note that the marginal in θ of pk is the
ξ
posterioratstepk−1orequivalentlythecurrentpriorp(θ|D )andthemarginalinyis
k−1
p(y|ξ,D )=E [p(y|θ,D )].
k−1 p(θ|Dk−1) k−1
Once a new ξ is computed and a new observation y is performed, the posterior at step k is
k k
p(θ|y ,ξ ,D )whichistheconditionaldistributionofp(y ,θ|ξ ,D ).
k k k−1 k k k−1
E SEQUENTIAL MONTE CARLO (SMC)-STYLE RESAMPLING
SMCisanessentialadditionwhendealingwithsequentialBOED.Indensity-basedBOED,ithas
beenalreadyexploitedinthesequentialcontextshowingarealimprovementinthequalityofthe
generatedsamples(Iolloetal.,2024). SMCisalsousefulinsimplerstaticcasesasitcanimprove
thequalityofthegeneratedθandcontrastiveθ′samples,thatinturnimprovestheaccuracyofthe
gradientestimator(30). AparticularlycentralstepinSMCistheresamplingstep,firstrecalledbelow
inthedensity-basedcase. Usingourframework,isitalsopossibletoderiveaSMC-styleresampling
scheme in the data-based case. This is becoming a popular strategy in the context of generative
models(DouandSong,2024;Cardosoetal.,2024).
Density-basedBOED. Instaticdensity-basedBOED,thepriorp(θ)andthelikelihoodp(y|θ,ξ)
areavailableinclosed-form. Inthesequentialexperimentcontext,wewanttogenerateN samples
θ ,...,θ from the current prior p(θ|D ) and M samples θ′,...,θ′ from the expected
1 N k−1 1 M
posteriorq ξ,N(θ′). Asbothp(θ|D k−1)andq ξ,N(θ′)∝(cid:81)N i=1p(θ′|y i,ξ,D k−1)νi canbeevaluated
uptoanormalizingconstant,itisstraightforwardtoextendthesamplingoperatorsofSection4and
addaresamplingsteptothesamplesθ ,...,θ andθ′,...,θ′ withweightsw andw′:
1 N 1 M i j
w˜
w = i with w˜ =p˜(θ )
i (cid:80)N
w˜
i i
i=1 i
w˜′
w′ = j with w˜′ =q˜(θ′)
j (cid:80)M w˜′ j j
j=1 j
wherep˜andq˜aretheunnormalizedversionsofp(θ|D )andq (θ′)respectively.
k−1 ξ,N
Data-basedBOED. Inthesettingofdata-basedBOED,weassumeaccesstoaconditionaldiffusion
model that allows to generate samples from p(θ|D ) and q (θ′). The resampling scheme
k−1 ξ,N
proposedin(DouandSong,2024)canbeusedasis, toimprovethequalityofthesamplesfrom
p(θ|D )asthisisanusualconditionaldistribution. TheresamplingschemeisbasedontheFPS
k−1
update:
p(θ(t−1)|θ(t),y(t−1),ξ,D )∝p (θ(t−1)|θ(t),D )p(y(t−1)|θ(t−1),ξ) (51)
k−1 t k−1
where p (θ(t−1)|θ(t),D ) is given in closed form by the unconditional diffusion model
t k−1
and p(y(t−1)|θ(t−1),ξ) is given by (46). As both these distributions are Gaussian,
p(θ(t−1)|θ(t),y(t−1),ξ,D )canbewritteninclosedformandresamplingweightscanbewritten
k−1
as:
w˜
w = i with w˜ =p(y(t−1)|θ(t),ξ) (52)
i (cid:80)N
w˜
i i
i=1 i
20Underreview
wherep(y(t−1)|θ(t),ξ)istractable(seeDouandSong(2024)formoredetails).
i
Fortheexpectedposteriorq ξ,N(θ′)∝(cid:81)N i=1p(θ′|y i,ξ,D k−1)νi,update(51)takestheform:
N N
(cid:89) p(θ(t−1)|θ(t),y(t−1),ξ,D )νi ∝p (θ(t−1)|θ(t),D ) (cid:89) p(y(t−1)|θ(t−1),ξ)νi
i k−1 t k−1 i
i=1 i=1
N
∝(cid:89)(cid:16)
p(θ(t−1)|θ(t),D
)p(y(t−1)|θ(t−1),ξ)(cid:17)νi
(53)
k−1 i
i=1
whichleadstothefollowingresamplingweights:
w′ =
w˜ j′
with w˜′
=(cid:89)N
p(y(t−1)|θ′(t),ξ)νi .
j (cid:80)M w˜′ j i j
j=1 j i=1
F NUMERICAL EXPERIMENTS
F.1 SEQUENTIALPRIORCONTRASTIVEESTIMATION(SPCE)ANDSEQUENTIALNESTED
MONTECARLO(SNMC)CRITERIA
TheSPCEintroducedbyFosteretal.(2021)isatractablequantitytoassessthedesignsequence
quality. ForanumberK ofexperiments,D ={(y ,ξ ),·,(y ,ξ )}andLcontrastivevariables,
K 1 1 K K
SPCEisdefinedas
 
K
(cid:81)
p(y |θ ,ξ )
 k 0 k 
SPCE(ξ ,·,ξ )=E log k=1  . (54)
1 K (cid:81)K p(yk|ξ k,θ0) (cid:81)L p(θℓ)  1 (cid:80)L (cid:81)K p(y |θ ,ξ ) 
k=1 ℓ=0 L+1 k ℓ k
ℓ=0k=1
SPCEisalowerboundofthetotalEIGwhichistheexpectedinformationgainedfromtheentire
sequenceofdesignparametersξ ,...,ξ anditbecomestightwhenLtendsto∞. Inaddition,
1 K
SPCE has the advantage to use only samples from the prior p(θ) and not from the successive
posteriordistributions. Itmakesitafaircriteriontocomparemethodsondesignsequencesonly.
Consideringatrueparametervaluedenotedbyθ∗,givenasequenceofdesignvalues{ξ } ,
k k=1:K
observations{y } aresimulatedusingp(y|θ∗,ξ )respectively. Therefore,foragivenD ,
k k=1:K k k
thecorrespondingSPCEisestimatednumericallybysamplingθ ,·,θ fromtheprior,
1 L
 
K
SPCE(D )=
1
(cid:88)N

log
k(cid:81) =1p(y k|θ∗,ξ k) 
.
K N (cid:18) K L K (cid:19)
i=1 L+1
1
(cid:81) p(y k|θ∗,ξ k)+ (cid:80) (cid:81) p(y k|θi ℓ,ξ k) 
k=1 ℓ=1k=1
Similarly,anupperboundonthetotalEIGhasalsobeenintroducedbyFosteretal.(2021)andnamed
theSequentialnestedMonteCarlo(SNMC)criterion,
 
K
(cid:81)
p(y |θ ,ξ )
 k 0 k 
SNMC(ξ ,·,ξ )=E log k=1  .
1 K (cid:81)K p(yk|ξ k,θ0) (cid:81)L p(θℓ)  1 (cid:80)L (cid:81)K p(y |θ ,ξ ) 
k=1 ℓ=0 L k ℓ k
ℓ=1k=1
AsshowninFosteretal.(2021)(AppendixA),SPCEincreaseswithLtoreachthetotalEIGwhen
L→∞atarateO(L−1)ofconvergence. ItisalsoshowninFosteretal.(2021)thatforagivenL,
SPCEisboundedbylog(L+1)whiletheupperboundSNMCbelowispotentiallyunbounded. Asin
Blauetal.(2022),ifweuseL=107tocomputeSPCEandSNMC,theboundislog(L+1)=16.12
forSPCE.Inpracticethisdoesnotimpactthenumericalmethodscomparisonastheintervals[SPCE,
SNMC]containingthetotalEIGremainclearlydistinct.
21Underreview
F.2 IMPLEMENTATIONDETAILS
F.2.1 SOURCEEXAMPLE
ForVPCE(Fosteretal.,2020)andRL-BOED(Blauetal.,2022), weusedthecodeavailableat
github.com/csiro-mlai/RL-BOED,usingthesettingsrecommendedthereintoreproducetheresultsin
therespectivepapers. VPCEoptimizesanEIGlowerboundinamyopicmannerestimatingposterior
distributionswithvariationalapproximations. RL-BOEDisanon-myopicapproachwhichdoesnot
provideposteriordistributions. Fromtheobtainedsequencesofobservationsanddesignvalues,we
computedSPCEandSNMCasexplainedaboveandretrievedthesameresultsasintheirrespective
papers. ForPASOAandSMCprocedures,weusedthecodeavailableatgithub.com/iolloj/pasoa.
PASOAisamyopicapproach,optimizinganEIGlowerboundusingsequentialMonteCarlo(SMC)
samplersandtemperingtoalsoprovideposteriorestimations. ThemethodreferedtoasSMCisa
variantwithouttempering.
ForCoDiff, theν ’sintheexpectedposteriordistributionweresettoν = 1. Thecurrentprior
i i N
andposteriordistributionsatexperimentalstepkwereinitializedusingrespectivelythepriorand
posterior samples at step k −1. Design optimization was performed using the Adam optimizer
with an exponential learning rate decay schedule with initial learning rate 10−2 and decay rate
0.98. TheLangevinstep-sizeintheDiGSmethodChenetal.(2024)wassetto10−2. Thejoint
optimization-samplingloopwasrunfor5000steps.
Forthisexample,theadditionalFigure5showssamplesfromthecurrentpriorp(θ|D )atsome
k−1
intermediatestepk,whichgraduallyconcentratesaroundthetruesources.Samplesfromtheexpected
posteriordistributionareshownincomparisontoillustrateitscontrastivenature.
F.2.2 MNISTEXAMPLE
ForthenumericalexampleofSection6.3,weusedtheMNISTdataset(LeCunetal.,1998),thetime
varyingSDE(34)withanoiseschedule(b −b )/(T −t )+(b −b )t /(T −t )(with
max min 0 min max 0 0
b =5,b =0.2,t =0,T =2). Thetrainingoftheusualscorematchingwasdonefor3000
max min 0
epochswithabatchsizeof256andusingAdamoptimizerKingmaandBa(2015). Weusedgradient
clippingandthetrainingwasdoneonasingleA100GPU.
UpdateequationsforthesamplingoperatorswerederivedfromSDE(19)forthecontrastivesamples
oftheexpectedposteriorq (θ′)and(44)forsamplesfromthecurrentpriorp(θ|D ),where
ξ,N k−1
D canbeaddedintheconditioningpartwithoutdifficulty. Thoseupdatesareequivalentto(53)
k−1
and(51)respectively. TheresamplingweightswerecomputedasinSectionE.
Figures6and7showadditionalimagereconstructionprocesses.Thedigittoberecoveredisshownin
thefirstcolumn. Thesuccessivelyselectedmasksareshown(redlinesquares)inthesecondcolumn
withtheresultinggraduallydiscoveredpartoftheimage. Thereconstructionpersecanbeestimated
fromtheposteriorsamplesshowninthelast16columns. Ateachexperiment,theuppersub-row
showsthe16most-likelyreconstructedimages,whilethelowersub-rowshowsthe16less-probable
ones. Asthenumberofexperimentsincreasestheposteriorsamplesgraduallyconcentrateonthe
rightdigit.
Figure8thenshowsthatdesignoptimizationiseffectivebyshowingbetteroutcomeswhenmasks
locations are optimized (second column) than when masks are selected at random centers (third
column). The highest posterior weight samples in the last 14 columns also clearly show more
resemblancewiththetruedigitintheoptimizedcase.
F.3 HARDWAREDETAILS
Thesourceexample6.2canberunlocally. ItwastestedonanAppleM1Pro16Gbchipbutfaster
runningtimescanbeachievedonGPU.TheMNISTexample6.3wasrunonasingleA10080Gb
GPU.
22Underreview
Figure5: Severalsourcelocalisationexamples. Prior(left)andexpectedposterior(right)samplesat
experimentk. Finalξ∗ value(orangecross)attheendoftheoptimizationsequenceξ ,·,ξ (blue
k 0 T
crosses). Thisoptimization"contrasts"thetwodistributionsbymakingtheexpectedposterior"as
differentaspossible"fromtheprior.
23Underreview
Figure6: Imagereconstruction. First7experiments(rows): imagegroundtruth,measurementat
experimentk,samplesfromcurrentpriorp(θ|D ),withbest(resp. worst)weightsinupper(resp.
k−1
lower)sub-row. Thesamplesincorporatepastmeasurementinformationastheprocedureadvances.
F.4 SOFTWAREDETAILS
OurcodeisimplementedinJaxBradburyetal.(2020)andusesFlaxasaNeuralNetworklibraryand
OptaxasoptimizationoneBabuschkinetal.(2020).
24Underreview
Figure7: Imagereconstruction. First7experiments(rows): imagegroundtruth,measurementat
experimentk,samplesfromcurrentpriorp(θ|D ),withbest(resp. worst)weightsinupper(resp.
k−1
lower)sub-row. Thesamplesincorporatepastmeasurementinformationastheprocedureadvances.
Figure8:Imageθ(1stcolumn)reconstructionfrom7sub-imagesy =A θ+ηselectedsequentially
ξ
at7centralpixelξ. Optimizedvs. randomdesigns: measuredoutcomey(2ndvs. 3rdcolumn)and
parameterθestimates(reconstruction)withhighestweights(uppervs. lowersub-row).
25