Learning Smooth Humanoid Locomotion
through Lipschitz-Constrained Policies
Zixuan Chen∗1 Xialin He∗2 Yen-Jen Wang∗3 Qiayuan Liao3 Yanjie Ze4 Zhongyu Li3
S. Shankar Sastry3 Jiajun Wu4 Koushil Sreenath3 Saurabh Gupta2 Xue Bin Peng1,5
1Simon Fraser University 2UIUC 3UC Berkeley 4Stanford University 5NVIDIA ∗Equal Contribution
lipschitz-constrained-policy.github.io
Fig. 1: Lipschitz-constrained policies (LCP) provide a simple and general method for training policies to produce smooth
behaviors, which can be directly deployed on a wide range of real-world humanoid robots. Our policies exhibit robust
behaviors that can recover from external forces and walk across irregular terrain. For full videos, please visit the project
website.
Abstract—Reinforcementlearningcombinedwithsim-to-real need for smoothing rewards or low-pass filters and can be
transfer offers a general framework for developing locomotion easilyintegratedintotrainingframeworksformanydistincthu-
controllersforleggedrobots.Tofacilitatesuccessfuldeployment manoidrobots.WeextensivelyevaluateLCPinbothsimulation
intherealworld,smoothingtechniques,suchaslow-passfilters andreal-worldhumanoidrobots,producingsmoothandrobust
andsmoothnessrewards,areoftenemployedtodeveloppolicies locomotion controllers. All simulation and deployment code,
with smooth behaviors. However, because these techniques are along with complete checkpoints, is available on our project
non-differentiableandusuallyrequiretedioustuningofalarge page: https://lipschitz-constrained-policy.github.io.
set of hyperparameters, they tend to require extensive manual
tuningforeachroboticplatform.Toaddressthischallengeand
I. INTRODUCTION
establish a general technique for enforcing smooth behaviors,
we propose a simple and effective method that imposes a
Humanoid research aims to develop intelligent, human-
Lipschitz constraint on a learned policy, which we refer to
like machines capable of autonomously operating in ev-
as Lipschitz-Constrained Policies (LCP). We show that the
Lipschitz constraint can be implemented in the form of a eryday environments [1]–[4]. One of the most fundamental
gradient penalty, which provides a differentiable objective challenges in this field is achieving reliable mobility. De-
that can be easily incorporated with automatic differentiation velopingrobustlocomotioncontrollersand adaptingthemto
frameworks.Wedemonstratethat LCPeffectivelyreplacesthe
real robots would greatly improve their capabilities.
4202
tcO
61
]OR.sc[
2v52811.0142:viXraTraditional model-based methods, such as Model Predic- II. RELATEDWORK
tive Control (MPC), necessitate precise system structure and
Legged robot locomotion has long been a crucial yet
dynamicsmodeling,whichislabor-intensiveandchallenging
challenging problem in robotics due to legged systems’
to design. In contrast, model-free reinforcement learning
highdimensionalityandinstability.Classicmodel-basedcon-
provides a straightforward end-to-end approach to devel-
trol methods have achieved impressive behaviors on legged
oping robust controllers, significantly alleviating the neces-
robots [12]–[14]. In recent years, learning-based methods
sity for meticulous dynamics modeling and system design.
have shown great potential to automate the controller devel-
However, because model-free RL requires a large number
opment process, providing a general approach to building
of samples through trial-and-error during training, which
robust controllers for quadrupedal locomotion [15]–[18],
cannot be performed in the real world, sim-to-real transfer
bipedal locomotion [11], [19]–[21], and humanoid locomo-
techniques are utilized to enable the successful deployment
tion [7], [22]–[24].
of controllers in real-world environments. Combined with
sim-to-real techniques, model-free RL-based methods have a) Sim-to-RealTransfer: Oneofthemainchallengesin
achieved great success in controlling quadruped robots and RL-based methods is sim-to-real transfer, where policies are
humanoid robots [5]–[7]. first trained in simulation and then deployed in real-world
environments. Substantial effort is often necessary to bridge
However, due to the simplified dynamics and actuation thedomaingapbetweensimulationsandtherealworld,such
models used in simulation, the resulting models tend to as developing high-fidelity simulators [25], [26], and incor-
be nearly idealized, meaning that the motors can produce porating domain randomization techniques during training
the desired torques at any state. As a result, RL-based [18], [22], [27], [28]. Another widely adopted approach is
policies trained in simulation are susceptible to developing the teacher-student framework, where a privileged teacher
jittery behaviors akin to bang-bang control [8]. This results policy, with access to full state information, is trained first,
in significant differences between actions in consecutive followed by the training of an observation-based student
timesteps, leading to excessively high output torques that policy through distillation [15], [20], [22], [24], [29]–[31].
realactuatorscannotproduce.Assuch,thesebehaviorsoften To further facilitate sim-to-real transfer, our framework also
fail to transfer to real robots. Therefore, enforcing smooth leverages a teacher-student framework [6], [16], [23], which
behaviors is crucial for successful sim-to-real transfer. trains a latent representation of the dynamics based on the
observation history. These methods have been successful in
Previous systems use smoothing methods to enforce
transferring controllers for both quadruped robots [9], [32],
smooth behaviors from a learned policy, such as smooth-
[33],andhumanoidrobots[7],[23].Someworkalsoexplores
ness rewards or low-pass filters. Incorporating smoothness
utilizing a single policy to control robots with different
rewards during training can be an effective approach to elic-
morphologies zero-shot in real world [34]. However, the
iting smoother behaviors. In robot locomotion, researchers
policy’s performance on real humanoid robots has yet to be
typically penalize joint velocities, joint accelerations, and
validated,anditisnoteasytoplugintoanyexistingtraining
energyconsumption[9].Otherapproachesattempttosmooth
pipeline.
policybehaviorbyapplyinglow-passfilters[10],[11].How-
b) Learning Smooth Behaviors: Due to the simplified
ever, smoothness rewards require careful tuning of weights
dynamics of simulators, policies trained in simulation often
to balance smooth behavior with task completion, and low-
exhibit jittery behaviors that cannot be transferred to the
pass filters often dampen or limit exploration, causing extra
real world. Therefore, smooth policy behaviors are criti-
effortwhentrainingcontrollersforanewrobot.Additionally,
cal for successful sim-to-real transfer. Common smoothing
thenon-differentiablenatureofsmoothnessrewardsandlow-
techniques include the use of smoothness rewards, such as
pass filters presents another limitation.
penalizing sudden changes in actions, degree of freedom
In this work, we introduce Lipschitz-Constrained Policies (DoF) velocities, DoF accelerations [24], [29], [31], [32],
(LCP), a general and differentiable method for encouraging [35], [36], and energy consumption [6], [9]. In addition to
RL policies to develop smooth behaviors. LCP enforces a smoothness rewards, low-pass filters have also been applied
Lipschitz constraint on the output actions of a policy with to the output actions of a policy to ensure smoother be-
respect to the input observations through a differentiable haviors [10], [11], [18], [37]. However, smoothness rewards
gradient penalty. LCP can be implemented with only a typically require careful manual design and tuning, while
few lines of code and easily incorporated into existing low-pass filters often dampen policy exploration, resulting
RL frameworks. We demonstrate that this approach can be in sub-optimal policies. These techniques are also generally
directly applied to train control policies for a diverse suite not directly differentiable, requiring sample-based gradient
of humanoid robots. Our experiments show that LCP can be estimators to optimize, such policy gradients.
an alternative to non-differentiable smoothness techniques c) GradientPenalty: Inthiswork,weproposeasimple
such as smoothness rewards and low-pass filters. We also and differentiable method to train RL policies that produce
demonstrate that LCP can be deployed zero-shot to several smooth behaviors by leveraging a gradient penalty. Gradient
real-worldrobotswithdifferentmorphologies,indicatingthe penalty is a common technique for stabilizing training of
generalization of our method. generative adversarial network (GAN), which is susceptibleLipschitz Continuity Training with Smoothness Rewards
y=Kx
y=f(x)
y=-Kx
smoothness rewards
w.o. smoothness rewards
x Training Iterations
Fig. 2: Lipschitz continuity is a method of quantifying the Fig.3:Gradientofpoliciestrainedwithandwithoutsmooth-
smoothness functions. A Lipschitz continuous function is a ness rewards. Policies with smoother behaviors also exhibit
function whose rate-of-change is bounded by a constant K. smaller gradient magnitudes.
to vanishing or exploding gradients. Arjovsky et al. [38]
Any such K is referred to as a Lipschitz constant of
proposed the Wasserstein GAN (WGAN) using weight clip-
the function f [45]. A corollary that arises from Lipschitz
ping to stabilize training. However, weight clipping still
Continuity is that if the gradient of a function is bounded:
often results in poor model performance and convergence
issues. Gulrajani et al. [39] introduced the gradient penalty ∥∇ xf(x)∥≤K, (2)
(WGAN-GP) as an alternative to weight clipping, which
then this function f is Lipschitz continuous. However, it is
penalizes the norm of the discriminator’s gradient. Since its
worth noting that the converse is not true.
introduction, the gradient penalty has become a widely used
regularization technique for GANs [40], [41]. For motion B. Reinforcement Learning
control, gradient penalty has been an effective technique for
Inthiswork,ourcontrollersaretrainedthroughreinforce-
improving the stability of adversarial imitation learning. For
ment learning, in which an agent interacts with the envi-
example, AMP [42], CALM [43], and ASE [44] all apply
ronment according to a policy π to maximize an objective
a gradient penalty to regularize an adversarial discriminator,
function[46].Ateachtimestept,theagentobservesthestate
which then enables a policy to imitate a large variety of
s of the environment, and takes an action a according to
t t
challengingmotions.Whilethesepriorsystemsdemonstrated
thepolicyπ(a |s ).Thisactionthenleadstoanewstateac-
t t
the effectiveness of gradient penalties as a regularizer for
cording to the dynamics of the environment p(s | s ,a ).
t+1 t t
discriminators, in this work, we show that a similar gradient
Theagentreceivesarewardr =r(s ,s ,a )ateachstep.
t t+1 t t
penalty can also be an effective regularizer to encourage
The agent’s goal is to maximize its expected return:
policies to produce smooth behaviors, which are then more
(cid:34)T−1 (cid:35)
amenable for real-world transfer. (cid:88)
J(π)=E γtr , (3)
p(τ|π) t
III. BACKGROUND t=0
Our method leverages ideas from Lipschitz continuity wherep(τ|π)representsthelikelihoodofthetrajectoryτ,T
to train reinforcement learning policies to produce smooth denotes the time horizon, and γ is the discount factor.
behaviors. This section will review some fundamental con-
IV. LIPSCHITZ-CONSTRAINEDPOLICIES
cepts for Lipschitz continuity and reinforcement learning
to provide a comprehensive background of our proposed In this section, we introduce Lipschitz-Constrained Poli-
method. cies(LCP),amethodfortrainingpoliciestoproducesmooth
behaviors by incorporating a Lipschitz constraint during
A. Lipschitz Continuity
training. We begin with a simple experiment to illustrate the
Intuitively, Lipschitz continuity is a property that limits motivation behind our method. This is then followed by a
how fast a function can change. This property is a good detailed description of our proposed method.
way of characterizing the smoothness of a function. An
A. Motivating Example
intuitive visualization is shown in Fig. 2. Formally, we give
the definition of Lipschitz continuity as follows: WewillfirstillustratethemotivationofLCPwithasimple
experiment. We know that RL-based policies are prone to
Definition III.1 (Lipschitz Continuity). Given two metric
producing jittery behaviors, and the most common method
spaces (X,d ) and (Y,d ), where d denotes the metric
X Y X for mitigating these behaviors is to incorporate smoothness
on the set X and d is the metric on set Y, a function
Y rewards during training. The smoothness of a function is
f :X →Y is deemed Lipschitz continuous if there exists
typically evaluated using the first derivative. Therefore, we
a real constant K such that, for all x and x in X,
1 2 compare the ℓ2-norm of the gradient of policies trained
d (f(x ),f(x ))≤Kd (x ,x ). (1) with and without smoothness rewards. Although no specific
Y 1 2 X 1 2
y
yciloP
fo
tneidarGtechnique is explicitly applied to regularize the policies’ V. TRAININGSETUP
gradient, the gradient trained with smoothness rewards is
Toevaluatetheeffectivenessofourmethod,weapplyLCP
significantly smaller than that of a policy trained without
to train policies for a variety of humanoid robots, where
smoothness rewards, as illustrated in Fig. 3. This fact in-
the task is for the robots to walk while following steering
spires our proposed method, which explicitly regularizes the
commands.
gradientofthepolicy.Weshowthatthissimplemethodleads
a) Observations: The input observations to the policy
to smooth behaviors, which can then facilitate successful
o = [ϕ ,c ,srobot,a ] consists of a gait phase variable
transfer to the real world. t t t t t−1
ϕ ∈R2 (a periodic clock signal represented by its sine and
t
B. Lipschitz Constraint as a Differentiable Objective cosine components), command c t, measured joint positions
and velocities srobot, and the previous output action of the
While smoothness rewards can mitigate jittery behaviors, t
policy a . To enable robust sim-to-real transfer, the policy
theserewardfunctionscanbecomplextodesign,withalarge t−1
also takes privileged information e as input, which consists
numberofhyperparametersthatrequiretuning.Furthermore, t
of the base mass, center of mass, motor strengths, and
these smoothness rewards are non-differentiable since they
root linear velocity. Observations o are normalized with a
are implemented as part of the underlying environment. t
running mean and standard deviation before being passed as
Therefore,theyoftenneedtobeoptimizedthroughsampling-
input to the policy.
basedmethods,suchaspolicygradients.Thisworkproposes
b) Commands: The command input to the policy c =
a simple and differentiable smoothness objective for policy t
[vcmd,vcmd,vcmd] consists of the desired linear velocities
optimization based on Lipschitz continuity. x y yaw
along x-axis vcmd ∈ [0m/s,0.8m/s] and y-axis vcmd ∈
Equation 2 stipulates that any function with bounded x y
[−0.4m/s,0.4m/s], and the desired yaw velocity vcmd ∈
gradients is Lipschitz continuous. Therefore, we can formu- yaw
[−0.6rad/s,0.6rad/s], both are in the robot frame. During
late a constrained policy optimization problem that enforces
training,commandsarerandomlysampledfromtheirrespec-
Lipschitz continuity through a gradient constraint:
tive ranges every 150 timestep or when the environment is
max J(π) reset.
π
s.t. max(cid:2) ∥∇ logπ(a|s)∥2(cid:3) ≤K2 (4) c) Actions: The policy’s output actions specify target
s,a s joint rotations for all joints in the robot’s body, which are
then converted to torque commands by PD controllers with
where K is a constant and J(π) is the RL objective defined
manually specified PD gains.
inEquation3.Sincecalculatingthemaximumgradientnorm
d) Training: All policies are modeled using neural
acrossallstatesisintractable,weapproximatethisconstraint
networks and trained using the PPO algorithm [48]. The
with an expectation over samples collected from policy
policiesaretrainedsolelyinsimulationwithdomainrandom-
rollouts, following the heuristic from Schulman et al. [47]:
ization and then deployed directly on the real robots [27].
max J(π) Sim-to-real transfer is performed using Regularized Online
π (5)
s.t. E (cid:2) ∥∇ logπ(a|s)∥2(cid:3) ≤K2, Adaptation (ROA) [6], [32].
s,a∼D s
where D is a dataset consisting of state-action pairs (s ,a ) VI. EXPERIMENTS
t t
collectedfromthepolicy.Next,tofacilitateoptimizationwith LCP’s effectiveness is evaluated on a set of diverse hu-
gradient-based methods, the constraint can be reformulated manoidrobotstoshowitsgeneralizationability.Weconduct
into a penalty by introducing a Lagrange multiplier λ: an extensive suite of simulation and real-world experiments,
minmax
J(π)−λ(cid:0)E (cid:2)
∥∇
logπ(a|s)∥2(cid:3) −K2(cid:1)
.
comparing LCP to commonly used smoothing techniques
λ≥0 π s,a∼D s from prior systems.
(6)
To further simplify the objective, we set λ as a manually A. Robot Platforms
gp
specified coefficient, and since K is a constant, this leads to
We evaluate our framework on three real-world robots:
a simple differentiable gradient penalty (GP) on the policy:
the human-sized Fourier GR1T1, Fourier GR1T2, Unitree
max J(π)−λ E (cid:2) ∥∇ logπ(a|s)∥2(cid:3) . (7) H1, and the smaller Berkeley Humanoid. We will first
gp s,a∼D s
π provideanoverviewofeachrobot’sbodystructure.Then,our
This gradient penalty can be easily implemented in any experimentsshowthatLCPisageneralsmoothingtechnique
reinforcement learning framework, requiring only a few that can be applied widely to several distinct robots.
lines of code. The gradient penalty provides a simple and a) Fourier GR1T1 & Fourier GR1T2: The Fourier
differentiable alternative to smoothness rewards or low-pass GR1T1andFourierGR1T2havethesamemechanicalstruc-
filters,whicharenotdifferentiablewithrespecttothepolicy ture.Theybothcomprise21joints,with12jointsinthelower
parameters. Our experiments show that LCP provides an body and 9 in the upper body. Notice that the torque limit
effective alternative to non-differentiable smoothing tech- for the ankle roll joint is minimal; we treat this as a passive
niques and can be directly used to train robust locomotion joint during training and deploying. This means we control
controllers for a diverse cast of robots. 19 joints of GR1 in total.Action Rate↓ DoF Acceleration↓ DoF Velocity↓ Energy↓
5000
60
16 40
2500
30
8 20
250M 500M 250M 500M 250M 500M 250M 500M
Samples No Smoothing Smoothness Rewards LCP (Ours)
Fig. 4: Smoothness metrics recorded over the course of training. LCP produces smooth behaviors that are comparable to
policies that are trained with explicit smoothness rewards.
Smoothness Methods GP Weights
20
20
No Smoothing
Low-pass Filter
Smoothness Rew λgp=0.0 λgp=0.002 λgp=0.01
LCP(Ours) λgp=0.001 λgp=0.005
0 0
0 200M 400M 600M 0 200M 400M 600M
Fig. 5: Task returns of different smoothing methods. Fig. 6: Task returns of LCP with different λ . Exces-
gp
LCP provides an effective alternative to other tech- sively large λ may hinder policy learning.
gp
niques.
b) Unitree H1: The Unitree H1 has 19 joints, with 10 [50].Wealsorecordthemeantaskreturn,whichiscalculated
joints in the lower body, 9 in the upper body, and 1 ankle using the linear and angular velocity tracking rewards.
joint per leg. All joints are actively controlled. a) Is LCP effective for producing smooth behaviors?:
c) Berkeley Humanoid: Berkeley Humanoid is a small We train policies with LCP using a GP coefficient of λ =
gp
robot with a height of 0.85m [49]. It has 12 degrees of 0.002. We track various smoothness metrics throughout the
freedom,with6jointsineachlegand2jointsineachankle. training process, including energy consumption, degrees-
of-freedom (DoF) velocities, DoF accelerations, and ac-
B. Results
tion rates. We then compare these metrics against policies
To evaluate the effectiveness of LCP, we compare our
trained with and without smoothness rewards. The results
method to the following baselines:
are recorded in Fig. 4. While LCP is not trained with
• No smoothing: No smoothing techniques are applied reward functions that directly minimize these smoothness
duringtraining.Thisbaselinedemonstratesthenecessity metrics, LCP nonetheless produces smooth behaviors that
of smoothing techniques for sim-to-real transfer; aresimilartopoliciestrainedwithsmoothnessrewards.This
• Smoothness rewards: Smoothness rewards are the demonstrates that LCP can be an effective substitute for
most commonly used smoothing method, where addi- traditional smoothness rewards.
tional reward terms are incorporated into the reward b) How does LCP affect task performance?: In TA-
function to encourage smooth behaviors. These reward BLE I(a) and Fig. 5, we compare the task performance of
functions are not directly differentiable. LCP with policies trained with other smoothing methods.
• Low-pass Filters: Low-pass filters are commonly used LCPachievessimilartaskperformancecomparedtopolicies
for action smoothing, where a filter is applied to the trainedsolelywithsmoothnessrewards.Policiestrainedwith
policy’s output actions before the action is applied to low-pass filters tend to exhibit lower task returns, which
the environment. Low-pass filters are also not easily may be due to the damping introduced by low-pass filters
differentiable for policy training. that can in turn impair exploration. Policies trained without
To evaluate the effectiveness of various smoothing tech- smoothingtechniquestendtoachievethehighesttaskreturns
niques, we record a suite of smoothness metrics, including but exhibit highly jittery behaviors unsuitable for real-world
mean DoF velocities (rad/s), mean energy (N·rad/s), action deployment.
rate (rad/s), robot base acceleration (m/s2), action jitter c) What is the effect of the GP coefficient λ ?:
gp
(rad/s3), and DoF position jitter (rad/s3). Action rate is the TABLE I(b) shows the performance of LCP with different
first derivative of output actions over time. The jitter metrics GP coefficients λ . Incorporating a gradient penalty leads
gp
represent the third derivative of their respective quantities to significantly smoother behaviors. However, with small
↑
snruteR
ksaT
↑
snruteR
ksaTTABLEI:AblationStudies.Allpoliciesaretrainedwiththreerandomseedsandtestedin1000environmentsfor500steps,
corresponding to 10 seconds clock time.
Method ActionJitter↓ DoFPosJitter↓ DoFVelocity↓ Energy↓ BaseAcc↓ TaskReturn↑
(a)AblationonSmoothMethods
LCP(ours) 3.21±0.11 0.17±0.01 10.65±0.37 24.57±1.17 0.06±0.002 26.03±1.51
SmoothnessReward 5.74±0.08 0.19±0.002 11.35±0.51 25.92±0.84 0.06±0.002 26.56±0.26
Low-passFilter 7.86±3.00 0.23±0.04 11.72±0.14 32.83±5.50 0.06±0.002 24.98±1.29
NoSmoothness 42.19±4.72 0.41±0.08 12.92±0.99 42.68±10.27 0.09±0.01 28.87±0.85
(b)AblationonGPWeights(λgp)
LCPw.λgp=0.0 42.19±4.72 0.41±0.08 12.92±0.99 42.68±10.27 0.09±0.01 28.87±0.85
LCPw.λgp=0.001 3.69±0.31 0.21±0.05 11.44±1.18 27.09±4.44 0.06±0.01 26.32±1.20
LCPw.λgp=0.002(ours) 3.21±0.11 0.17±0.01 10.65±0.37 24.57±1.17 0.06±0.002 26.03±1.51
LCPw.λgp=0.005 2.10±0.05 0.15±0.01 10.44±0.70 26.24±3.50 0.05±0.002 23.92±2.05
LCPw.λgp=0.01 0.17±0.01 0.07±0.00 2.75±0.12 5.89±0.28 0.007±0.00 16.11±2.76
(c)AblationonGPInputs
LCPw.GPonwholeobs(ours) 3.21±0.11 0.17±0.01 10.65±0.37 24.57±1.17 0.06±0.002 26.03±1.51
LCPw.GPoncurrentobs 7.16±0.60 0.35±0.03 13.70±1.50 35.18±4.84 0.09±0.005 25.44±3.73
TABLE II: Sim-to-sim perfomance when transferring policies trained in IsaacGym to Mujoco. All policies are trained with
three random seeds and tested for 3 trials with 500 steps, corresponding to 10 seconds per trial.
ActionJitter↓ DoFPosJitter↓ DoFVelocity↓ Energy↓ BaseAcc↓ TaskReturn↑
FourierGR1 1.47±0.43 0.34±0.07 9.54±1.53 36.38±2.97 0.08±0.004 24.33±1.25
UnitreeH1 0.44±0.03 0.10±0.007 9.12±0.38 76.22±5.81 0.04±0.005 21.74±1.40
BerkeleyHumanoid 1.77±0.32 0.12±0.01 7.92±0.21 19.99±0.36 0.06±0.00 26.50±0.57
As with other smoothing techniques, some care is required
to tune the GP coefficient for a better performance.
d) Which components of the observation should GP be
appliedto?: SincethepoliciesaretrainedusingtheROAfor
sim-to-real transfer, the policy’s input consists of the current
observation and a history of past observations. TABLE I(c)
comparestheperformanceofLCPwhenthegradientpenalty
is applied to the whole input observation or only to the
current observation. We find that applying GP on the whole
observation achieves the best performance. Regularizing the
policy only with respect to the current observation can still
lead to non-smooth behaviors due to changes in the history.
e) Sim-to-Sim Transfer: Before deploying models we
test our models in a different simulator Mujoco [25]. As
shown in TABLE II, we observe a slight decrease in task
return compared to IsaacGym for full-sized robots such as
FourierGR1andUnitreeH1,suggestingthatthedomaingap
ismoresignificantforlargerrobots.Theoverallresultsshow
that LCP performs well in sim-to-sim transfer, providing
confidence for subsequent real-world deployments.
Fig.7:Real-worlddeployment.LCPisabletotraineffective
C. Real World Deployment
locomotionpoliciesonawiderangeofrobots,whichcanbe
directly transferred to the real world. We deploy LCP models trained with the same reward
functionsandλ =0.002onfourdistinctrobots.Asshown
gp
coefficients(e.g.,λ =0.001),thepolicycandevelopjittery in Fig. 1, LCP effectively enables different robots to walk
gp
behaviors that are dangerous to deploy in the real world. in the real world. Fig. 7 shows snapshots of the robots’
However, excessively larger coefficients (e.g., λ = 0.01) behaviors over the course of one gait cycle.
gp
lead to a substantial decline in task return due to overly Terrains To evaluate the robustness of the learned policies,
smooth and sluggish behaviors. As shown in Fig. 6, large we apply the policies in the real world to walk on three
values of λ may also lead to slower learning speeds. Our types of terrain: smooth, soft, and rough plane. We measure
gp
experiments suggest that λ = 0.002 strikes an effective the jitter metrics to evaluate LCP’s performance, as shown
gp
balance between policy smoothness and task performance. in TABLE III. The behaviors of our policies remain smoothTABLEIII:Performanceduringreal-worlddeployment.Per-
[5] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, “Learning to walk
formance for each method is calculated across 3 models in minutes using massively parallel deep reinforcement learning,” in
from different training runs. Each model is executed for 10 ConferenceonRobotLearning. PMLR,2022,pp.91–100.
[6] Z.Fu,X.Cheng,andD.Pathak,“Deepwhole-bodycontrol:learning
seconds. Standard deviation is recorded for each test.
aunifiedpolicyformanipulationandlocomotion,”inConferenceon
RobotLearning. PMLR,2023,pp.138–149.
Robot ActionJitter↓ DoFPosJitter↓ DoFVelocity↓
[7] X. Cheng, Y. Ji, J. Chen, R. Yang, G. Yang, and X. Wang, “Ex-
(a)SmoothPlane pressive whole-body control for humanoid robots,” arXiv preprint
FourierGR1 1.12±0.16 0.28±0.13 10.82±1.58 arXiv:2402.16796,2024.
UnitreeH1 1.11±0.07 0.14±0.01 10.95±0.53 [8] J. Lasalle, “The ‘bang-bang’ principle,” IFAC Proceedings Volumes,
BerkeleyHumanoid 1.56±0.10 0.10±0.01 4.99±0.60
vol. 1, no. 1, pp. 503–507, 1960, 1st International IFAC
(b)SoftPlane Congress on Automatic and Remote Control,Moscow, USSR, 1960.
FourierGR1 1.18±0.17 0.24±0.09 10.45±1.42 [Online]. Available: https://www.sciencedirect.com/science/article/pii/
UnitreeH1 1.18±0.09 0.15±0.01 11.80±0.57 S147466701770095X
BerkeleyHumanoid 1.66±0.03 0.12±0.01 6.78±1.57
[9] Z. Fu, A. Kumar, J. Malik, and D. Pathak, “Minimizing energy
(c)RoughPlane consumption leads to the emergence of gaits in legged robots,” in
FourierGR1 1.18±0.22 0.26±0.11 11.61±1.64 ConferenceonRobotLearning(CoRL),2021.
UnitreeH1 1.20±0.09 0.14±0.01 11.68±0.84 [10] Y.Ji,Z.Li,Y.Sun,X.B.Peng,S.Levine,G.Berseth,andK.Sreenath,
BerkeleyHumanoid 1.63±0.11 0.11±0.01 5.02±0.48
“Hierarchicalreinforcementlearningforprecisesoccershootingskills
usingaquadrupedalrobot,”in2022IEEE/RSJInternationalConfer-
in the presence of variations in the terrain and is able to ence on Intelligent Robots and Systems (IROS). IEEE, 2022, pp.
1479–1486.
effective traverse the different surfaces.
[11] Z. Li, X. Cheng, X. B. Peng, P. Abbeel, S. Levine, G. Berseth,
External Forces To further test the robustness of our poli- and K. Sreenath, “Reinforcement learning for robust parameterized
cies, we apply external forces to the robot in the real world locomotion control of bipedal robots,” in 2021 IEEE International
Conference on Robotics and Automation (ICRA). IEEE, 2021, pp.
Fig. 1. The recovery behaviors of our models are shown
2811–2817.
in the supplementary video. The LCP models can robustly [12] H. Miura and I. Shimoyama, “Dynamic walk of a biped,” The
recover from unexpected external perturbations. InternationalJournalofRoboticsResearch,vol.3,no.2,pp.60–74,
1984.
VII. CONCLUSION [13] K.Sreenath,H.-W.Park,I.Poulakakis,andJ.W.Grizzle,“Acompliant
hybrid zero dynamics controller for stable, efficient and fast bipedal
In this work, we present Lipschitz Constrained Policies walking on mabel,” The International Journal of Robotics Research,
(LCP), a simple and general method for training controllers vol.30,no.9,pp.1170–1193,2011.
[14] H. Geyer, A. Seyfarth, and R. Blickhan, “Positive force feedback in
to produce smooth behaviors amenable to sim-to-real trans-
bouncinggaits?”ProceedingsoftheRoyalSocietyofLondon.Series
fer. LCP approximates a Lipschitz constraint on the policy, B:BiologicalSciences,vol.270,no.1529,pp.2173–2183,2003.
implemented in the form of a differentiable gradient penalty [15] X.Cheng,K.Shi,A.Agarwal,andD.Pathak,“Extremeparkourwith
leggedrobots,”arXivpreprintarXiv:2309.14341,2023.
applied during training. Through extensive simulation and
[16] A. Kumar, Z. Fu, D. Pathak, and J. Malik, “Rma: Rapid motor
real-world experiments, we show the effectiveness of LCP adaptationforleggedrobots,”arXivpreprintarXiv:2107.04034,2021.
in training locomotion controllers for a wide range of real [17] H.Lai,W.Zhang,X.He,C.Yu,Z.Tian,Y.Yu,andJ.Wang,“Sim-
to-real transfer for quadrupedal locomotion via terrain transformer,”
humanoid robots.While LCP has demonstratedits effective-
in2023IEEEInternationalConferenceonRoboticsandAutomation
ness in real-world locomotion experiments, our results are (ICRA). IEEE,2023,pp.5141–5147.
still limited to basic walking behaviors. Evaluating LCP on [18] X.B.Peng,E.Coumans,T.Zhang,T.-W.Lee,J.Tan,andS.Levine,
“Learningagileroboticlocomotionskillsbyimitatinganimals,”arXiv
more dynamic skills, such as running and jumping, would
preprintarXiv:2004.00784,2020.
help further validate this method’s generality. [19] Z.Li,X.B.Peng,P.Abbeel,S.Levine,G.Berseth,andK.Sreenath,
“Robustandversatilebipedaljumpingcontrolthroughreinforcement
ACKNOWLEDGEMENT learning,”arXivpreprintarXiv:2302.09450,2023.
[20] A.Kumar,Z.Li,J.Zeng,D.Pathak,K.Sreenath,andJ.Malik,“Adapt-
We extend our gratitude to Fourier Intelligence for their ing rapid motor adaptation for bipedal robots,” in 2022 IEEE/RSJ
hardware support, Jiaze Cai, Yiyang Shao, Junfeng Long, International Conference on Intelligent Robots and Systems (IROS).
IEEE,2022,pp.1161–1168.
Haoru Xue, and Renzhi Tao for their assistance with real-
[21] H.Duan,B.Pandit,M.S.Gadde,B.J.vanMarum,J.Dao,C.Kim,
world experiments, and Mintae Kim for his valuable advice andA.Fern,“Learningvision-basedbipedallocomotionforchalleng-
on the mathematical formulas. This work was supported in ingterrain,”arXivpreprintarXiv:2309.14594,2023.
[22] I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and
part by an NSERC Discovery Grant (DGECR-2023-00280).
K. Sreenath, “Real-world humanoid locomotion with reinforcement
learning,”ScienceRobotics,vol.9,no.89,p.eadi9579,2024.
REFERENCES
[23] X.Gu,Y.-J.Wang,X.Zhu,C.Shi,Y.Guo,Y.Liu,andJ.Chen,“Ad-
[1] X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang, “Open-television: vancing humanoid locomotion: Mastering challenging terrains with
Teleoperationwithimmersiveactivevisualfeedback,”arXivpreprint denoising world model learning,” arXiv preprint arXiv:2408.14472,
arXiv:2407.01512,2024. 2024.
[2] Y. Ze, Z. Chen, W. Wang, T. Chen, X. He, Y. Yuan, X. B. Peng, [24] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani,
andJ.Wu,“Generalizablehumanoidmanipulationwithimproved3d C. Liu, and G. Shi, “Omnih2o: Universal and dexterous human-
diffusionpolicies,”arXivpreprintarXiv:2410.10803,2024. to-humanoid whole-body teleoperation and learning,” arXiv preprint
[3] I. Radosavovic, S. Kamat, T. Darrell, and J. Malik, “Learning arXiv:2406.08858,2024.
humanoid locomotion over challenging terrain,” 2024. [Online]. [25] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for
Available:https://arxiv.org/abs/2410.03654 model-basedcontrol,”in2012IEEE/RSJinternationalconferenceon
[4] F.Liu,Z.Gu,Y.Cai,Z.Zhou,S.Zhao,H.Jung,S.Ha,Y.Chen,D.Xu, intelligentrobotsandsystems. IEEE,2012,pp.5026–5033.
and Y. Zhao, “Opt2skill: Imitating dynamically-feasible whole-body [26] V.Makoviychuk,L.Wawrzyniak,Y.Guo,M.Lu,K.Storey,M.Mack-
trajectoriesforversatilehumanoidloco-manipulation,”arXivpreprint lin,D.Hoeller,N.Rudin,A.Allshire,A.Handa,etal.,“Isaacgym:
arXiv:2409.20514,2024. High performance gpu-based physics simulation for robot learning,”
arXivpreprintarXiv:2108.10470,2021.[27] X.B.Peng,M.Andrychowicz,W.Zaremba,andP.Abbeel,“Sim-to- “Improved training of wasserstein gans,” 2017. [Online]. Available:
realtransferofroboticcontrolwithdynamicsrandomization,”in2018 https://arxiv.org/abs/1704.00028
IEEEInternationalConferenceonRoboticsandAutomation(ICRA), [40] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing
May2018,pp.1–8. ofgansforimprovedquality,stability,andvariation,”2018.[Online].
[28] I.Radosavovic,B.Zhang,B.Shi,J.Rajasegaran,S.Kamat,T.Darrell, Available:https://arxiv.org/abs/1710.10196
K. Sreenath, and J. Malik, “Humanoid locomotion as next token [41] A. Brock, J. Donahue, and K. Simonyan, “Large scale gan training
prediction,”arXivpreprintarXiv:2402.19469,2024. for high fidelity natural image synthesis,” 2019. [Online]. Available:
[29] X. Gu, Y.-J. Wang, and J. Chen, “Humanoid-gym: Reinforcement https://arxiv.org/abs/1809.11096
learning for humanoid robot with zero-shot sim2real transfer,” arXiv [42] X. B. Peng, Z. Ma, P. Abbeel, S. Levine, and A. Kanazawa,
preprintarXiv:2404.05695,2024. “Amp: adversarial motion priors for stylized physics-based character
[30] Z.Zhuang,S.Yao,andH.Zhao,“Humanoidparkourlearning,”arXiv control,”ACMTransactionsonGraphics,vol.40,no.4,p.1–20,July
preprintarXiv:2406.10759,2024. 2021.[Online].Available:http://dx.doi.org/10.1145/3450626.3459670
[31] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, “Humanplus: [43] C.Tessler,Y.Kasten,Y.Guo,S.Mannor,G.Chechik,andX.B.Peng,
Humanoidshadowingandimitationfromhumans,”inarXiv,2024. “Calm: Conditional adversarial latent models for directable virtual
[32] M. Liu, Z. Chen, X. Cheng, Y. Ji, R. Yang, and X. Wang, “Visual characters,” in ACM SIGGRAPH 2023 Conference Proceedings,
whole-body control for legged loco-manipulation,” arXiv preprint ser. SIGGRAPH ’23. New York, NY, USA: Association for
arXiv:2403.16967,2024. Computing Machinery, 2023. [Online]. Available: https://doi.org/10.
[33] X.Cheng,A.Kumar,andD.Pathak,“Legsasmanipulator:Pushing 1145/3588432.3591541
quadrupedalagilitybeyondlocomotion,”in2023IEEEInternational [44] X. B. Peng, Y. Guo, L. Halper, S. Levine, and S. Fidler, “Ase:
Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. large-scale reusable adversarial skill embeddings for physically
5106–5112. simulatedcharacters,”ACMTransactionsonGraphics,vol.41,no.4,
[34] N. Bohlinger, G. Czechmanowski, M. Krupka, P. Kicki, K. Walas, p. 1–17, July 2022. [Online]. Available: http://dx.doi.org/10.1145/
J. Peters, and D. Tateo, “One policy to run them all: an end-to-end 3528223.3530110
learning approach to multi-embodiment locomotion,” arXiv preprint [45] M.O’Searcoid,Metricspaces. SpringerScience&BusinessMedia,
arXiv:2409.06366,2024. 2006.
[35] T. He, Z. Luo, W. Xiao, C. Zhang, K. Kitani, C. Liu, and G. Shi, [46] R.S.SuttonandA.G.Barto,Reinforcementlearning:Anintroduction.
“Learning human-to-humanoid real-time whole-body teleoperation,” MITpress,2018.
arXivpreprintarXiv:2403.04436,2024. [47] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel,
[36] C. Zhang, W. Xiao, T. He, and G. Shi, “Wococo: Learning whole- “Trustregionpolicyoptimization,”CoRR,vol.abs/1502.05477,2015.
body humanoid control with sequential contacts,” arXiv preprint [Online].Available:http://arxiv.org/abs/1502.05477
arXiv:2406.06005,2024. [48] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
[37] G.Feng,H.Zhang,Z.Li,X.B.Peng,B.Basireddy,L.Yue,Z.Song, “Proximal policy optimization algorithms,” arXiv preprint
L.Yang,Y.Liu,K.Sreenath,etal.,“Genloco:Generalizedlocomotion arXiv:1707.06347,2017.
controllersforquadrupedalrobots,”inConferenceonRobotLearning. [49] Q. Liao, B. Zhang, X. Huang, X. Huang, Z. Li, and K. Sreenath,
PMLR,2023,pp.1893–1903. “Berkeleyhumanoid:Aresearchplatformforlearning-basedcontrol,”
[38] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” 2017. arXivpreprintarXiv:2407.21781,2024.
[Online].Available:https://arxiv.org/abs/1701.07875 [50] T.FlashandN.Hogan,“Thecoordinationofarmmovements:anex-
[39] I.Gulrajani,F.Ahmed,M.Arjovsky,V.Dumoulin,andA.Courville, perimentallyconfirmedmathematicalmodel,”Journalofneuroscience,
vol.5,no.7,pp.1688–1703,1985.