On the Training Convergence of Transformers for
In-Context Classification
Wei Shen∗ Ruida Zhou∗
University of Virginia University of California, Los Angeles
Jing Yang Cong Shen
Pennsylvania State University University of Virginia
Abstract
While transformers have demonstrated impressive capacities for in-context learning (ICL) in practice,
theoretical understanding of the underlying mechanism enabling transformers to perform ICL is still in its
infant stage. This work aims to theoretically study the training dynamics of transformers for in-context
classification tasks. We demonstrate that, for in-context classification of Gaussian mixtures under certain
assumptions, a single-layer transformer trained via gradient descent converges to a globally optimal model
at a linear rate. We further quantify the impact of the training and testing prompt lengths on the ICL
inference error of the trained transformer. We show that when the lengths of training and testing prompts
are sufficiently large, the prediction of the trained transformer approaches the Bayes-optimal classifier.
Experimental results corroborate the theoretical findings.
1 Introduction
Large language models (LLMs) based on the transformer architecture [Vaswani et al., 2017] have demonstrated
remarkable in-context learning (ICL) abilities [Brown et al., 2020]. When given a prompt consisting of examples
ofalearningtask,thesemodelscanlearntosolvethistaskfornewtestexampleswithoutanyparameterupdating.
This behavior has been empirically demonstrated in state-of-the-art models on real-world tasks [OpenAI, 2023,
Touvron et al., 2023].
This impressive capacity of transformer-based models has inspired many recent works aiming to understand
the ICL abilities of transformers. A more comprehensive literature review can be found in Appendix A. Garg
et al. [2022] was the first to study the ICL abilities of transformers for various function classes. They empirically
showedthattransformerscanlearnlinearregressionmodelsincontext. Lateron,alineofresearchwasdeveloped
to theoretically explain how transformers perform in-context linear regression. For example, Akyu¨rek et al.
[2022], Von Oswald et al. [2023], Bai et al. [2024], Fu et al. [2023], Giannou et al. [2024] showed by construction
that, some specially-designed transformers can perform linear regression in context. Moreover, some recent works
like Zhang et al. [2023a], Huang et al. [2023], Chen et al. [2024] studied the training dynamics of a single-layer
transformer for in-context linear regression. They proved the convergence of their single-layer transformers and
showed their trained transformer are able to perform linear regression in context.
Building on the earlier works that largely focus on linear regression problems, several recent papers have
started to investigate the ICL capabilities of transformers for non-linear problems such as classification. For
∗Firsttwoauthorscontributedequally.
1
4202
tcO
51
]GL.sc[
1v87711.0142:viXrainstance, Bai et al. [2024] showed that, by construction, multi-layer transformers can be approximately viewed
as multiple steps of gradient descents for logistic regression. Giannou et al. [2024] further showcased that the
constructed transformers can approximately perform Newton’s method for logistic regression. Recently, Li et al.
[2024] studied the training dynamics of transformers for in-context binary classification. However, their analysis
requires the data to be pairwise orthogonal and the possible distribution of their data is highly limited. The
learning dynamics of transformers for more general in-context classification problems is not well understood.
Moreover, to the best of our knowledge, existing literature [Bai et al., 2024, Giannou et al., 2024, Li et al.,
2024] studying the in-context classification of transformers focus only on binary classification. How transformers
perform in-context multi-class classification remains unexplored.
In this work, we study the learning dynamics of a singly-layer transformer for both in-context binary
and multi-class classification of Gaussian mixtures, a fundamental problem in machine learning. Our main
contributions can be summarized as follows:
• To the best of our knowledge, we are the first to study the learning dynamics of transformers for in-context
classification of Gaussian mixtures, and we are the first to prove the training convergence of transformers for
in-context multi-class classification. We prove that with appropriately distributed training data (Assumptions
3.1, 4.1), a single-layer transformer trained via gradient descent will converge to its global minimizer at a
linear rate (Theorems 3.1, 4.1) for both in-context binary or multi-class classification problems.
• Due to the high non-linearity of our loss function, we cannot directly find the closed-form expression of the
global minimizer. Instead, we prove an important property that the global minimizer consists of a constant
plus an error term that is induced by the finite training prompt length (N). We further show that the max
norm of this error term is bounded, and converges to zero at a rate of O(1/N).
• With properly distributed test prompts (Assumptions 3.2, 4.2), we establish an upper bound of the inference
error (defined in Equation (3)) of the trained transformer and quantify the impact of the training and testing
prompt lengths on this error. We further prove that when the lengths of training prompts (N) and testing
√
prompts (M) approach infinity, this error converges to zero at a rate of O(1/N +1/ M) (Theorems 3.2,
4.2), and the prediction of the trained transformer is Bayes-optimal, i.e., the optimal classifier given the data
distribution.
2 Preliminaries
Notations. We denote [n] = {1,2,...,n}. For a matrix A ∈ Rm×n, we denote its Frobenius norm as ∥A∥ ,
F
and its max norm as ∥A∥ =max |A |. We use A (or A ) to represent the element of matrix A
max i∈[m],j∈[n] ij a,b ab
at the a-th row and b-th column, and use A to represent a vector of dimension c−a+1 whose i-th element
a:c,b
is A . We denote the l norm of a vector as ∥·∥ . We denote the all-zero vector of size n as 0 and the
(a+i−1),b 2 2 n
all-zeromatrixofsizem×nas0 . Weuseσ(x):=1/(1+exp(−x))todenotethesigmoidfunction. Wedefine
m×n
softmax(·):Rk →(0,1)k, and its i-th element as softmax(·) , where softmax(x) =exp(x )/((cid:80)k exp(x )).
i i i j=1 j
2.1 Single-layer transformer
Given an input embedding matrix E ∈Rde×dn, a single head self-attention module F
SA
[Vaswani et al., 2017]
with width d will output
e
(cid:18) (WKE)⊤WQE(cid:19)
F (E;WV,WK,WQ)=E+WVE·f , (1)
SA attn ρ
whereWV,WK,WQ ∈Rde×de arethevalue,key,andqueryweightmatrices,respectively,ρ>0isanormalization
factor, and f is an activation function for attention. There are different choices of f ; for example Vaswani
attn attn
et al. [2017] adopts softmax.
In this work, similar to Zhang et al. [2023a], Wu et al. [2023], we set f (x) = x and define WKQ =
attn
2(WK)⊤WQ ∈Rde×de. WeuseF todenotethissimplifiedmodel. Then,theoutputofF withaninputembedding
matrix E ∈Rde×dn can be expressed as
E⊤WKQE
F(E;WV,WKQ)=E+WVE· . (2)
ρ
In the following theoretical study and the subsequent experiments (Section 5.2), we show that this simplified
transformer model has sufficient capability to approach the Bayes-optimal classifier for in-context classification
of Gaussian mixtures.
2.2 In-context learning framework
We adopt a framework for in-context learning similar to that used in Bai et al. [2024]. Under this framework,
i.i.d.
the model receives a prompt P =(D,x ) comprising a set of demonstrations D ={(x ,y )} ∼ P and
query i i i∈[N]
a query x ∼P , where P is the joint distribution of (x,y) and P is the marginal distribution of x. Here,
query x x
x ∈ Rd is an in-context example, and y is the corresponding label for x . For instance, in regression tasks,
i i i
y ∈R is a scalar. In this paper, we focus on classification tasks. Thus, the range of y can be any set containing
i i
cdifferentelements, suchas{1,...,c}, forclassificationproblemsinvolvingcclasses. Theobjectiveistogenerate
an output y that approximates the target y ∼P .
(cid:98)query query y|xquery
Sincey isadiscreterandomvariable,weusethetotalvariationdistancetomeasurethedifferencebetween
query
y and y :
(cid:98)query query
∆(y ,y )= sup |P(y =z)−P(y =z)|, (3)
query (cid:98)query query (cid:98)query
z∈R(yquery)
where R(y ) is the range of y . When ∆(y ,y )=0, y has the same distribution as y , which
query query query (cid:98)query (cid:98)query query
means the output of the model perfectly approximates y .
query
Unlike standard supervised learning, in ICL, each prompt P can be sampled from a different distribution
τ
P . We say that a model has the ICL capability if it can approximate y for a broad range of P ’s with
τ τ,query τ
fixed parameters.
3 In-context binary classification
In this section, we study the learning dynamics of a single-layer transformer for in-context binary classification.
It is a special case of the general multi-class classification. As a result, the analysis is more concise. The general
in-context multi-class classification problem is studied in Section 4.
We first introduce the prompt and the transformer structure we will use for in-context binary classification.
The prompt for in-context binary classification is denoted as P =(x ,y ,...,x ,y ,x ), where x ∈Rd and
1 1 N N query i
y ∈ {−1,1}. We can convert this prompt P into its corresponding embedding matrix E(P) in the following
i
form:
(cid:18) (cid:19)
x x ··· x x
E =E(P)= 1 2 N query ∈R(d+1)×(N+1). (4)
y y ··· y 0
1 2 N
Similar to Huang et al. [2023], Wu et al. [2023], Ahn et al. [2024], we set some of the parameters in our model
to 0 or 1 to simplify the optimization problem, and consider the parameters of our model (WV,WKQ) in the
following sparse form:
(cid:18) (cid:19) (cid:18) (cid:19)
0 0 W 0
WV = d×d d , WKQ = d , (5)
0⊤ 1 0⊤ 0
d d
where W ∈Rd×d. We set the normalization factor ρ equal to the length of the prompt N. Let F(E(P);W) be
the output matrix of the transformer. We then read out the bottom-right entry of the output matrix through
3a sigmoid function, and denote this output as y . The output y of the transformer with prompt P and
(cid:98)out (cid:98)out
parameters W can be expressed as
(cid:0) (cid:1)
y =σ [F(E(P);W)]
(cid:98)out (d+1),(N+1)
=σ(cid:32) (cid:0) 0⊤ 1(cid:1)(cid:32) N1 (cid:80)N i=1x ix⊤ i + N1x queryx⊤ query N1 (cid:80)N i=1x iy i(cid:33)(cid:18) W 0 d(cid:19)(cid:18) x query(cid:19)(cid:33)
d 1 (cid:80)N x⊤y 1 (cid:80)N y2 0⊤ 0 0
N i=1 i i N i=1 i d
(cid:32)(cid:32) N (cid:33) (cid:33)
1 (cid:88)
=σ y x⊤ Wx .
N i i query
i=1
We denote the prediction of our model for x as y , which is a random variable depending on y .
query (cid:98)query (cid:98)out
Consider generating a random variable u uniformly on [0,1]. If u≤y , we output y =1; if u>y , we
(cid:98)out (cid:98)query (cid:98)out
output y =−1. Then, we have P(y =1)=y , P(y =−1)=1−y .
(cid:98)query (cid:98)query (cid:98)out (cid:98)query (cid:98)out
3.1 Training procedure
We study the binary classification of two Gaussian mixtures and use the following definition.
Definition 3.1 We say a data pair (x,y)∼Pb(µ ,µ ,Λ) if y follows a Bernoulli distribution with P(y =−1)=
0 1
P(y =1) = 1/2 and f(x|y = −1) = N(µ ,Λ), f(x|y = 1) = N(µ ,Λ), where µ ,µ ∈ Rd and Λ ∈ Rd×d is a
0 1 0 1
positive definite matrix.
WeconsiderthecaseofB trainingtasksindexedbyτ ∈[B]. Eachtrainingtaskτ isassociatedwithaprompt
P =(x ,y ,...,x ,y ,x ) and a corresponding label y . We make the following assumption in
τ τ,1 τ,1 τ,N τ,N τ,query τ,query
this section.
Assumption 3.1 For each learning task τ ∈[B], we assume
(1) {x ,y }N ,{x ,y }i. ∼i.d. Pb(µ ,µ ,Λ).
τ,i τ,i i=1 τ,query τ,query τ,0 τ,1
(2) µ is randomly sampled from N(0,I ), and µ = U µ where U = Λ1/2U Λ−1/2, and U is
τ,0 d τ,1 τ,Λ τ,0 τ,Λ τ τ
uniformly distributed over the closed set of real unitary matrices such that U U⊤ =I .
τ τ d
We denote the distribution of (µ ,µ ) as Pb(Λ). Note that U = Λ1/2U Λ−1/2 can be viewed as
τ,0 τ,1 Ω τ,Λ τ
a linear transformation that preserves the inner product of vectors in Λ−1-weighted norm, and we have
µ⊤ Λ−1µ −µ⊤ Λ−1µ =0.
τ,0 τ,0 τ,1 τ,1
Lety =σ([F(E(P );W)] )betheoutputofourtransformerfortaskτ. Wedefinetheempirical
(cid:98)τ,out τ (d+1),(N+1)
risk over B independent tasks as
B
1 (cid:88)
L(cid:98)(W)=
2B
−(1+y τ,query)log(y (cid:98)τ,out)−(1−y τ,query)log(1−y (cid:98)τ,out). (6)
τ=1
Taking the limit of infinite training tasks B →∞, the expected training loss can be defined as
1
L(W)= Bl →im ∞L(cid:98)(W)=− 2E[(1+y τ,query)log(y (cid:98)τ,out)+(1−y τ,query)log(1−y (cid:98)τ,out)], (7)
where the expectation is taken over (µ ,µ )∼Pb(Λ), {x ,y }N ,{x ,y }i. ∼i.d. Pb(µ ,µ ,Λ).
τ,0 τ,1 Ω τ,i τ,i i=1 τ,query τ,query τ,0 τ,1
Applying gradient descent over the expected training loss (7), we have the following theorem.
Theorem 3.1 Under Assumption 3.1, the following statements hold.
(1) Optimizing the training loss L(W) in (7) with training prompt length N via gradient descent Wt+1 =
Wt−η∇L(Wt), we have that for any t≥1
∥Wt−W∗∥2 ≤exp(−t/κ)∥W0−W∗∥2, (8)
F F
4where W0 is the initial parameter and W∗ is the global minimizer of L(W), and κ=l/α. Here α,l are
constants satisfying
0<α≤λ (∇2L(W))≤λ (∇2L(W))≤l, for all W ∈R , (9)
min max W
where R ={W ∈Rd×d|∥W −W∗∥ ≤∥W0−W∗∥ }.
W F F
(2) Denote W∗ = 2(Λ−1+G), q = x , µ = µ −µ , u = 2(µ +µ ), a = µ⊤Λ−1q for simplicity.
τ,query τ,1 τ,0 τ,1 τ,0
Then we have
∥G∥ ≤ 1 (cid:13) (cid:13)S−1E(cid:2) σ′(a)(cid:18) 4qq⊤+ 1 uu⊤Λ−1qq⊤(cid:19)
max N(cid:13) 4
+σ′′(a)(cid:18)
1
(u⊤Λ−1q)2µq⊤+2q⊤Λ−1qµq⊤)(cid:3)(cid:19)(cid:13)
(cid:13)
+o(cid:18)
1
(cid:19)
, (10)
8 (cid:13) max N
whereS =4∇2L(cid:101)(2Λ−1),L(cid:101)(2Λ−1)=lim N→∞L(2Λ−1),σ′(·)andσ′′(·)arethefirst-andsecond-orderderiva-
tives of σ(·), respectively, and the expectation is taken over (µ , µ )∼Pb(Λ), x ∼Pb(µ ,µ ,Λ).
τ,0 τ,1 Ω τ,query x τ,0 τ,1
The detailed proof of Theorem 3.1 can be found in Appendix D. In the following, we provide a brief proof
sketch to highlight the key ideas.
Proof sketch for Theorem 3.1. As a first step, we prove in Lemma D.2 that the expected loss function
L(W) in (7) is strictly convex with respect to (w.r.t.) W and is strongly convex in any compact set of Rd×d.
Moreover, we prove L(W) has one unique global minimizer W∗. Since the loss function L(W) we consider is
highlynon-linear,wecannotdirectlyfindtheclosed-formexpressionofW∗,asisoftendoneinthepriorliterature.
We address this technical challenge via the following method. First, in Lemma D.3, by analyzing the Taylor
expansion of L(W), we prove that as N →∞, our loss function L(W) converges to L(cid:101)(W) pointwisely (defined in
(25)), and the global minimizer W∗ converges to 2Λ−1. Thus, we denote W∗ =2(Λ−1+G), and prove ∥G∥
max
is bounded and scales as ∥G∥ =O(N−1/2). Next, in Lemma D.4, by further analyzing the Taylor expansion
max
of the equation ∇L(W∗)=0 at the point 2Λ−1, we establish a tighter bound ∥G∥ =O(N−1). In Lemma
max
D.5, we prove that our loss function is l-smooth and provide an upper bound for l. Thus, in a compact set R ,
W
our loss function is α-strongly convex and l-smooth. Finally, leveraging the standard results from the convex
optimization, we prove Theorem 3.1.
According to Theorem 3.1, we have Wt =W∗+Ht where ∥Ht∥ ≤exp(−t/(2κ))∥W0−W∗∥ . If we set
max F
T ≥2κlog(N·∥W0−W∗∥ F),wehave∥HT∥
max
≤1/N. DenotingW(cid:99) =WT,wehaveW(cid:99) =2(Λ−1+G+HT/2)=
2(Λ−1 +G(cid:98)), where G(cid:98) = G+HT/2,∥G(cid:98)∥
max
≤ ∥G∥
max
+∥HT∥
max
= O(1/N). Thus, we have the following
corollary.
Corollary 3.1 If we optimize the expected loss L(W) in (7) via gradient descent with training prompt length N,
initial parameters W0, and learning rate η =1/l, then, under Assumption 3.1, after T ≥2κlog(N∥W0−W∗∥ )
F
steps, the updated model W(cid:99) satisfies
W(cid:99) =2(Λ−1+G(cid:98)), (11)
where ∥G(cid:98)∥
max
=O(1/N), κ=l/α, and α,l are constants defined in (9).
Theorem 3.1 and Corollary 3.1 show that training a single-layer transformer with properly distributed data
(Assumption 3.1) for binary classification via gradient descent can linearly converge to its global minimum
W∗ =2(Λ−1+G). Furthermore, when the prompt length N grows, this global minimum W∗ will converge to
2Λ−1 at a rate of O(1/N).
53.2 In-context inference
Next, we analyze the performance of the trained transformer (11) for in-context binary classification tasks. We
make the following assumption.
Assumption 3.2 For an in-context test prompt P =(x ,y ,...,x ,y ,x ), we assume
test 1 1 M M query
(1) {x ,y }M i. ∼i.d. Pb(µ ,µ ,Λ), x ∈Rd.
i i i=1 0 1 query
(2) µ⊤Λ−1µ =µ⊤Λ−1µ .
0 0 1 1
With this assumption, for y ∼Pb (µ ,µ ,Λ), according to the Bayes’ theorem, we have
query y|xquery 0 1
f(x |y =1)P(y =1)
P(y =1|x )= query query query =σ((µ −µ )⊤Λ−1x ).
query query (cid:80) f(x |y =z)P(y =z) 1 0 query
z∈{±1} query query query
If we test the trained transformer with parameters W(cid:99) in (11) and P test, by a simple calculation, we have
(cid:32)(cid:32) M (cid:33) (cid:33)
2 (cid:88)
y
(cid:98)out
=σ
M
y ix⊤
i
(Λ−1+G(cid:98))x
query
. (12)
i=1
Intuitively, whenthetrainingpromptlengthN →∞, wehaveG(cid:98) →0, andwhenthetestpromptlengthM →∞,
we have 2 (cid:80)M y x⊤ →(µ −µ )⊤. Thus, when N,M →∞, P(y =1)=y →σ((µ −µ )⊤Λ−1x )=
M i=1 i i 1 0 (cid:98)query (cid:98)out 1 0 query
P(y =1|x ), and the prediction of the trained transformer y perfectly matches with the distribution
query query (cid:98)query
of the ground truth label y .
query
By analyzing the Taylor expansion of y at point σ((µ −µ )⊤Λ−1x ), we formally present the afore-
(cid:98)out 1 0 query
mentioned intuition in the following theorem, which establishes an upper bound of the total variation distance
between y and y .
query (cid:98)query
Theorem 3.2 Consider a test prompt P satisfying Assumption 3.2, and let y ∼Pb (µ ,µ ,Λ). Let
test query y|xquery 0 1
y
(cid:98)query
be the prediction of the trained transformer with parameters W(cid:99) in (11). Then, for the inference error
defined in (3), we have
E[∆(y ,y )]
query (cid:98)query
  √ 
(cid:18) (cid:19)
≤σ′(µ⊤Λ−1q)∥G(cid:98)∥ max (cid:88) |µ iq j|+ √1
M
 21 |u⊤Λ−1q|+ 2 √ π2 (cid:88) |Λ− ij1/2q j|+o N1 + √1
M
,
i,j∈[d] i,j∈[d]
where µ=µ −µ , u=2(µ +µ ), q =x , and the expectation is taken over {x ,y }M i. ∼i.d. Pb(µ ,µ ,Λ).
1 0 1 0 query i i i=1 0 1
The proof of Theorem 3.2 can be found in Appendix E. Since ∥G(cid:98)∥
max
=O(1/N), Theorem 3.2 suggests that
if we ignore the constants regarding µ ,µ ,Λ,x , the expected total variation distance between y and
√ 0 1 query query
y is at most O(1/N +1/ M). On the other hand, for data pair (x,y)∼Pb(µ ,µ ,Λ), the Bayes-optimal
(cid:98)query 0 1
classifier is P(y =1|x)=f(x|y)P(y =1)/f(x)=σ((µ −µ )⊤Λ−1x), which corresponds to the logistic regression
1 0
model σ(w⊤x+b) with parameters w =Λ−1(µ −µ ) and b=0. Therefore, when N,M →∞, the prediction
1 0
of the trained transformer is Bayes-optimal, and is equivalent to the optimal logistic regressor for binary
classification problems with distribution Pb(µ ,µ ,Λ). Note that different from Assumption 3.1 which states
0 1
that µ ,µ ,x are sampled according to some specific distributions during training, Assumption 3.2 does
τ,0 τ,1 τ,query
not impose strong distributional constraints on µ ,µ and x , which shows the strong generalization ability
0 1 query
of the trained transformer. We also discuss the consequences when Assumption 3.2 does not hold in Remark E.1,
which highlights the necessity of Assumption 3.2. Moreover, even if M →∞, the distribution variation between
y and y does not disappear unless N → ∞. Thus, the ICL ability of trained transformers for binary
query (cid:98)query
classification is limited by the finite length of training prompts. Similar behaviors have also been observed in
Zhang et al. [2023a] for in-context linear regression.
64 In-context multi-class classification
We now extend the study of the learning dynamics of a single-layer transformer to in-context multi-class
classification, generalizing the results of the previous section. We will present the detailed formulation and then
focus on the main differences to binary classification.
We first introduce the prompt and the transformer structure that will be used for in-context multi-class
classification. The prompt for in-context multi-class classification involving c≥2 classes can be expressed as
P =(x ,y ,...,x ,y ,x ), where x ∈Rd, y ∈{e ,e ,...,e }, and e is the i-th standard unit vector of
1 1 N N query i i 1 2 c i
Rc. Its embedding matrix can be formulated as
(cid:18) (cid:19)
x x ··· x x
E =E(P)= 1 2 N query ∈R(d+c)×(N+1). (13)
y y ··· y 0
1 2 N c
Similartothebinarycase, wesetsomeoftheparametersinourmodelas0and1tosimplifytheoptimization
problem and consider the parameters of our model (WV,WKQ) in the following sparse form:
(cid:18) (cid:19) (cid:18) (cid:19)
0 0 W 0
WV = d×d d×c , WKQ = d×c , (14)
0 I 0 0
c×d c c×d c×c
where W ∈ Rd×d. We set the normalization factor ρ equal to the length of the prompt N. We read out the
bottom-rightc-dimensionalcolumnvectorfromtheoutputmatrixwithasoftmaxfunctionastheoutput,denoted
as y . With parameters W and a prompt P =(x ,y ,...,x ,y ,x ), the output can be expressed as
(cid:98)out 1 1 N N query
(cid:32)(cid:32) N (cid:33) (cid:33)
y =softmax(cid:0) [F(E(P);W)] (cid:1) =softmax 1 (cid:88) y x⊤ Wx .
(cid:98)out (d+1):(d+c),(N+1) N i i query
i=1
We denote the prediction of the model for x as y , which is a random variable depending on y .
query (cid:98)query (cid:98)out
Randomly sample a random variable u that is uniformly distributed on [0,1]. If
u∈(cid:2)(cid:80)i−1(y
)
,(cid:80)i
(y )
(cid:1)
,
j=1 (cid:98)out j j=1 (cid:98)out j
where (y ) is the j-th element of y , we let y =e . Thus, P(y =e )=(y ) .
(cid:98)out j (cid:98)out (cid:98)query i (cid:98)query i (cid:98)out i
4.1 Training procedure
We focus on the multi-class classification of Gaussian mixtures and use the following definition.
Definition 4.1 We say a data pair (x,y)∼Pm(µ,Λ) if P(y =e )=1/c and f(x|y =e )=N(µ ,Λ) for i∈[c],
i i i
where µ=(µ ,...,µ )∈Rd×c and Λ∈Rd×d is a positive definite matrix.
1 c
WeconsiderthecaseofB trainingtasksindexedbyτ ∈[B]. Eachtrainingtaskτ isassociatedwithaprompt
P =(x ,y ,...,x ,y ,x ) and a corresponding label y . We make the following assumption in
τ τ,1 τ,1 τ,N τ,N τ,query τ,query
this section.
Assumption 4.1 For each learning task τ ∈[B], we assume
(1) {x ,y }N ,{x ,y }i. ∼i.d. Pm(µ =(µ ,...,µ ),Λ).
τ,i τ,i i=1 τ,query τ,query τ τ,1 τ,c
(2) µ is sampled from N(0,I ), and µ = U µ ,k = 2,3,...,c, where U = Λ1/2U Λ−1/2, and
τ,1 d τ,k τ,k,Λ τ,1 τ,k,Λ τ,k
U are uniformly distributed over the closed set of real unitary matrices such that U U⊤ =I .
τ,k τ,k τ,k d
We denote the distribution of µ as Pm(Λ). Note that U = Λ1/2U Λ−1/2 can be viewed as linear
τ Ω τ,k,Λ τ,k
transformations that preserve the inner product of vectors in the Λ−1 weighted norm, and we have µ⊤ Λ−1µ =
τ,i τ,i
µ⊤ Λ−1µ ,fori,j ∈[c]. Lety =softmax(cid:0) [F(E(P );W)] (cid:1) betheoutputofthetransformer
τ,j τ,j (cid:98)τ,out τ (d+1):(d+c),(N+1)
for task τ. We define the empirical risk over B independent tasks as
B c
1 (cid:88)(cid:88)
L(cid:98)(W)=
B
−(y τ,query) klog((y (cid:98)τ,out) k). (15)
τ=1k=1
7Taking the limit of infinite training tasks B →∞, the expected training loss can be defined as
(cid:34) c (cid:35)
(cid:88)
L(W)= lim L(cid:98)(W)=−E (y τ,query) klog((y (cid:98)τ,out) k) , (16)
B→∞
k=1
where the expectation is taken over µ ∼Pm(Λ), {x ,y }N ,{x ,y }i. ∼i.d. Pm(µ ,Λ).
τ Ω τ,i τ,i i=1 τ,query τ,query τ
Applying gradient descent over the expected training loss (16), we have the following theorem.
Theorem 4.1 (Informal)
Under Assumption 4.1, the following statements hold.
(1) Optimizing training loss L(W) in (16) with training prompt length N via gradient descent Wt+1 =
Wt−η∇L(Wt), for any t≥1, we have
∥Wt−W∗∥2 ≤exp(−t/κ)∥W0−W∗∥2, (17)
F F
where W0 is the initial parameter and W∗ is the global minimizer of L(W), κ = l/α. Here, α,l are
constants such that
0<α≤λ (∇2L(W))≤λ (∇2L(W))≤l, for all W ∈R , (18)
min max W
where R ={W ∈Rd×d|∥W −W∗∥ ≤∥W0−W∗∥ }.
W F F
(2) Denoting W∗ =c(Λ−1+G), we have ∥G∥ =O(c/N).
max
(3) After T ≥2κlog(N ·∥W0−W∗∥ F) steps, denoting the updated model W(cid:99) satisfies
W(cid:99) =c(Λ−1+G(cid:98)), (19)
where ∥G(cid:98)∥
max
=O(c/N).
The formal statement and proof of Theorem 4.1 can be found in Appendix F. Technically, the proof of
Theorem 4.1 builds on that of Theorem 3.1, but the more complicated cross terms in the Taylor expansions
of the softmax functions, which are due to the nature of multi-class classification, bring new challenges to the
analysis. To address these issues, we derived new bounds on the expected errors of the cross terms in Lemma
F.1, F.2, which may be of independent interest to other similar problems.
Theorem 4.1 shows that training a single-layer transformer with properly distributed data (Assumption
4.1) for in-context multi-class classification via gradient descent can linearly converge to its global minimum
W∗ =c(Λ−1+G). When the prompt length N grows, this global minimum W∗ will converge to cΛ−1 at a rate
of O(c/N). Compared to the binary case, the new results establish the scaling behavior w.r.t. the number of
classes c.
4.2 In-context inference
Assumption 4.2 For an in-context test prompt P =(x ,y ,...,x ,y ,x ), we assume
test 1 1 M M query
(1) {x ,y }M i. ∼i.d. Pm(µ,Λ), µ=(µ ,...,µ )∈Rd×c, x ∈Rd.
i i i=1 1 c query
(2) µ⊤Λ−1µ =µ⊤Λ−1µ , for i,j ∈[c].
i i j j
With this assumption, for y ∼Pm (µ,Λ), according to the Bayes’ theorem, we have
query y|xquery
f(x |y =e )P(y =e )
P(y =e |x )= query query k query k =softmax(µ⊤Λ−1x ) .
query k query (cid:80)c f(x |y =e )P(y =e ) query k
j=1 query query j query j
8If we test the trained transformer with parameters W(cid:99) in (19) and prompt P test, by a simple calculation, we have
(cid:32)(cid:32) M (cid:33) (cid:33)
c (cid:88)
y
(cid:98)out
=softmax
M
y ix⊤
i
(Λ−1+G(cid:98))x
query
. (20)
i=1
Note that, when the training prompt length N →∞, we have G(cid:98) →0, and when the test prompt length M →∞,
we have c (cid:80)M y x⊤ → µ⊤. Thus, when N,M → ∞, P(y =e ) = (y ) → softmax(µ⊤Λ−1x ) =
M i=1 i i (cid:98)query k (cid:98)out k query k
P(y =e |x ), i.e., the prediction of the trained transformer y matches the ground truth label y .
query k query (cid:98)query query
By analyzing the Taylor expansion of y at point softmax(µ⊤Λ−1x ), we crystallize the aforementioned
(cid:98)out query
intuition in the following theorem, which establishes an upper bound of the total variation distance between
y and y .
query (cid:98)query
Theorem 4.2 (Informal) Let P satisfy Assumption 4.2 and y ∼ Pm (µ,Λ). Denote y as the
test query y|xquery (cid:98)query
prediction of the trained transformer with parameter W(cid:99) in (19). Then, for the inference error defined in (3), we
have
E[∆(y ,y )]=O(c2N−1+c3/2M−1/2),
query (cid:98)query
where the expectation is taken over {x ,y }M i. ∼i.d. Pm(µ,Λ).
i i i=1
TheformalstatementandproofofTheorem4.2canbefoundinAppendixG.Wecanseethattheconvergence
rateoftheinferenceerrorinmulti-classclassificationw.r.t. N andM issimilartothatinthebinaryclassification,
except for the constant coefficient c. This suggests that classification tasks with more classes may have higher
errors than those with fewer classes. On the other hand, for data pair (x,y) ∼ Pm(µ,Λ), the Bayes-optimal
classifierisP(y =e |x)=f(x|y)P(y =e )/f(x)=softmax(µ⊤Λ−1x) ,whichcorrespondstoasoftmaxregression
k k k
modelsoftmax(Wx+b)withparametersW =µ⊤Λ−1 andb=0. WhenN,M →∞,thepredictionofthetrained
transformer is Bayes-optimal, and is equivalent to the optimal softmax regressor for multi-class classification
problems with distribution Pm(µ,Λ). Note that different from Assumption 4.1 which states that µ ,x are
τ τ,query
sampled according to some specific distributions during training, Assumption 4.2 impose strong distributional
constraints on µ or x , which shows the strong generalization ability of the trained transformer. We also
query
discuss the consequences when Assumption 4.2 does not hold in Remark G.1, which highlights the necessity
of Assumption 4.2. Moreover, even if M → ∞, the distribution variation between y and y does not
query (cid:98)query
disappear unless N → ∞. Thus, the ICL ability of the trained transformers for multi-class classification is
limitedbythefinitelengthoftrainingprompts. SimilarbehaviorshavealsobeenobservedinZhangetal.[2023a]
for in-context linear regression and in Section 3.2 for in-context binary classification.
5 Experiments
In this section, we carry out experiments to verify the theoretical claims and compare the ICL performances of
transformers with other machine learning algorithms. Detailed experimental settings can be found in Appendix
H.
95.1 ICL performances of the single-layer transformers
(a) c=10 (b) N =80
(c) log-log axes (d) log-log axes
Figure 1: Inference errors of single-layer transformers. (a): Models trained on different training prompt lengths N on
classification tasks involving c=10 classes. (b): Models trained on different classification tasks involving c classes with a
fixed training prompt length N =80. (c): Relationship between the inference error and the test prompt length M in
log-log axes. Training prompt length N =2000 and number of classes c=6. (d): Relationship between the inference
error and the training prompt length N in log-log axes. Test prompt length M =2000 and number of classes c=6.
We first train single-layer transformers for in-context classification of Gaussian mixtures with different numbers
of classes c, different lengths of training prompts N, and test them with different test prompt lengths M. The
results are reported in Figure 1. We can see from Figure 1 (a,b) that the inference errors decrease as N and M
increase, and they increase as c increases. In Figure 1 (c,d), we first fix the training prompt length (test prompt
length) to a large number 2000, and then vary the test prompt length (training prompt length) from 20 to 2000.
The results show that, as M and N become sufficiently large, the inference error, which is an approximation
of E[∆(y ,y )] (see Appendix H for detailed definitions), decreases to near-zero. This indicates that the
query (cid:98)query
prediction of the trained transformer approaches the Bayes-optimal classifier. All these experimental results
corroborate our theoretical claims.
5.2 Comparison of transformers with other machine learning algorithms
Additionally, we conduct experiments comparing the ICL performances of the transformers with other machine
learning algorithms for the classification of three Gaussian mixtures. Again, the detailed experimental setting
can be found in Appendix H. Form Figure 2, we can see that, when the prompt length is less than 100, all three
transformer models have comparable or better performances than the traditional methods (softmax regression,
SVM, K-nearest neighbor), which demonstrates the strong ICL capacities of transformers. When the prompt
length is larger than 100, a 3-layer transformer with the GPT-2 architecture shows a decline in performance,
probably because our transformer models were only trained with a small prompt length of N =100. Similar
10Figure 2: Inference errors for the classification of three Gaussian mixtures. Colored lines correspond to different models
as follows: a single-layer transformer defined in Section 4 (1-layer, sparse), a single-layer transformer with full parameters
(59) (1-layer, full), a 3-layer transformer with a GPT2 architecture (3-layer, GPT2), softmax regression (softmax), SVM
with linear kernel (SVM, linear), SVM with Gaussian kernel (SVM, gaussian), 1-nearest neighbor (1-NN), and 3-nearest
neighbor (3-NN). All three transformers are trained with prompt length N =100.
declined performance when the training prompt length is smaller than the test prompt length has also been
observed for in-context linear regression tasks; see e.g. Zhang et al. [2023a]. However, even when the prompt
length is larger than 100, the single-layer transformer defined in Section 4 (1-layer, sparse) and the single-layer
transformer with full parameters (59) (1-layer, full) still significantly outperform the traditional methods. This
also indicates that, though the model (1-layer, sparse) we studied in this paper is relatively simple, this model is
already sufficiently complex for in-context classification of Gaussian mixtures, both in theory and practice.
6 Conclusion
We studied the learning dynamics of transformers for in-context classification of Gaussian mixtures, and showed
thatwithproperlydistributeddata, asingle-layertransformertrainedviagradientdescentconvergestoitsglobal
minimum. Moreover, we established the upper bounds of the inference errors of the trained transformers and
discussed how the training and test prompt lengths influence the performance of the model. Experimental results
alsocorroboratedthetheoreticalclaims. Therearesomedirectionsworthfurtherexploring. Onepotentialavenue
is to investigate whether the assumptions regrading the training and test prompts can be relaxed. Additionally,
we have only examined single-layer transformers with linear attention and sparse parameters. The learning
dynamicsofmulti-layertransformerswithnonlinearattention(e.g.,softmax)forin-contextclassificationproblems
remain an interesting area for future investigation.
References
KwangjunAhn,XiangCheng,HadiDaneshmand,andSuvritSra.Transformerslearntoimplementpreconditioned
gradient descent for in-context learning. Advances in Neural Information Processing Systems, 36, 2024.
Ekin Akyu¨rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is
in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.
YuBai,FanChen,HuanWang,CaimingXiong,andSongMei. Transformersasstatisticians: Provablein-context
learning with in-context algorithm selection. Advances in neural information processing systems, 36, 2024.
11Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901, 2020.
S´ebastien Bubeck. Convex optimization: Algorithms and complexity, 2015.
Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head softmax
attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442,
2024.
Xiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent to learn
non-linear functions in context. arXiv preprint arXiv:2312.06528, 2023.
Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L
McClelland, and Felix Hill. Language models show human-like content effects on reasoning. arXiv preprint
arXiv:2207.07051, 2022.
Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order optimization methods
for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086, 2023.
Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a
case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583–30598,
2022.
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos.
Looped transformers as programmable computers. In International Conference on Machine Learning, pages
11398–11442. PMLR, 2023.
AngelikiGiannou,LiuYang,TianhaoWang,DimitrisPapailiopoulos,andJasonDLee. Howwellcantransformers
emulate in-context newton’s method? arXiv preprint arXiv:2403.03183, 2024.
Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai. How do transformers
learn in-context beyond simple functions? a case study on learning with representations. arXiv preprint
arXiv:2310.10616, 2023.
Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint
arXiv:2310.05249, 2023.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods
underthepolyak-l(cid:32)ojasiewiczcondition. InMachine Learning and Knowledge Discovery in Databases: European
Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16, pages
795–811. Springer, 2016.
Juno Kim and Taiji Suzuki. Transformers learn nonlinear features in context: Nonconvex mean-field dynamics
on the attention landscape. arXiv preprint arXiv:2402.01258, 2024.
Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin-Yu Chen. Training nonlinear transformers for
efficientin-contextlearning: Atheoreticallearningandgeneralizationanalysis.arXivpreprintarXiv:2402.15607,
2024.
YingcongLi,MuhammedEmrullahIldiz,DimitrisPapailiopoulos,andSametOymak. Transformersasalgorithms:
Generalization and stability in in-context learning. In International Conference on Machine Learning, pages
19565–19594. PMLR, 2023a.
Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic
understanding. In International Conference on Machine Learning, pages 19689–19729. PMLR, 2023b.
12Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context reinforcement learning
via supervised pretraining. arXiv preprint arXiv:2310.08566, 2023.
ArvindMahankali,TatsunoriBHashimoto,andTengyuMa. Onestepofgradientdescentisprovablytheoptimal
in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023.
Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent.
arXiv preprint arXiv:2402.14735, 2024.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David
Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate
computation with language models. arXiv preprint arXiv:2112.00114, 2021.
OpenAI. GPT-4 technical report, 2023.
Reese Pathak, Rajat Sen, Weihao Kong, and Abhimanyu Das. Transformers can optimally learn regression
mixture models. arXiv preprint arXiv:2311.08362, 2023.
Alec Radford. Improving language understanding by generative pre-training. 2018.
Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support
vector machines. arXiv preprint arXiv:2308.16898, 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat
models. arXiv preprint arXiv:2307.09288, 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L(cid:32) ukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joa˜o Sacramento, Alexander Mordvintsev, Andrey
Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International
Conference on Machine Learning, pages 35151–35174. PMLR, 2023.
Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are
implicitly topic models: Explaining and finding good demonstrations for in-context learning. In Workshop on
Efficient Systems for Foundation Models@ ICML2023, 2023.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint
arXiv:2206.07682, 2022.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language
processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system
demonstrations, pages 38–45, 2020.
Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett. How many
pretraining tasks are needed for in-context learning of linear regression? arXiv preprint arXiv:2310.08391,
2023.
Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as
implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.
Tong Yang, Yu Huang, Yingbin Liang, and Yuejie Chi. In-context learning with representations: Contextual
generalization of trained transformers. arXiv preprint arXiv:2408.10147, 2024.
13Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv
preprint arXiv:2306.09927, 2023a.
Ruiqi Zhang, Jingfeng Wu, and Peter L Bartlett. In-context learning of a linear transformer block: benefits of
the mlp component and one-step gd initialization. arXiv preprint arXiv:2402.14951, 2024.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068, 2022.
Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning
learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420,
2023b.
14Appendix
The Appendix is organized as follows. In Section A, we provide a literature review of the related works that
studied the ICL abilities of transformers. In Section B, we introduce the additional notations for the proofs in
the Appendix. In Section C, we introduce some useful Lemmas we adopt from previous literature. In Sections D,
E, F, G, we present the proofs of Theorem 3.1, 3.2, 4.1, 4.2 respectively. In Section H, we provide additional
details of our experiments.
A Related work
It has been observed that transformer-based models have impressive ICL abilities in natural language processing
[Brownetal.,2020,Nyeetal.,2021,Weietal.,2022,Dasguptaetal.,2022,Zhangetal.,2022]. Gargetal.[2022]
first initiated the study of the ICL abilities of transformers in a mathematical framework and they empirically
showed that transformers can in-context learn linear regression, two-layer ReLU networks, and decision trees.
Subsequently, numerous works have been developed to explain the ICL capacities of transformers in solving
in-context mathematical problems. These works mainly use two approaches: constructing specific transformers
capable of performing certain in-context learning tasks, and studying the training dynamics of transformers for
such tasks.
Constructions of transformers. Akyu¨rek et al. [2022], Von Oswald et al. [2023] showed by construction
that multi-layer transformers can be viewed as multiple steps of gradient descent for linear regression. Akyu¨rek
et al. [2022] also showed that constructed transformers can implement closed-form ridge regression. Guo et al.
[2023] showed that constructed transformers can perform in-context learning with representations. Bai et al.
[2024] proved that constructed transformers can perform various statistical machine learning algorithms through
in-context gradient descent and showed that constructed transformers can perform in-context model selection.
Lin et al. [2023] demonstrated that constructed transformers can approximate several in-context reinforcement
learning algorithms. Fu et al. [2023], Giannou et al. [2024] further proved that constructed transformers
can perform higher-order optimization algorithms like Newton’s method. Pathak et al. [2023] showed that
transformers can learn mixtures of linear regressions. Giannou et al. [2023] proved that looped transformers
that can emulate various in-context learning algorithms. Cheng et al. [2023] showed that transformers can
perform functional gradient descent for learning non-linear functions in context. Zhang et al. [2024] showed that
a linear attention layer followed by a linear layer can learn and encode a mean signal vector for in-context linear
regression.
Training dynamics of transformers. Mahankali et al. [2023], Ahn et al. [2024] proved that the global
minimizerofthein-contextlearninglossoflineartransformercanbeequivalentlyviewedasone-steppreconditioned
gradientdescentforlinearregression. Zhangetal.[2023a]provedtheconvergenceofgradientflowonasingle-layer
linear transformer and discussed how training and test prompt length will influence the prediction error of
transformersforlinearregression. Huangetal.[2023]provedtheconvergenceofgradientdescentonasingle-layer
transformer with softmax attention with certain orthogonality assumptions on the data features. Li et al. [2023b]
showed that trained transformers can learn topic structure. Wu et al. [2023] analyzed the task complexity bound
for pretraining single-layer linear transformers on in-context linear regression tasks. Tarzanagh et al. [2023] built
the connections between single-layer transformers and support vector machines (SVMs). Nichani et al. [2024]
showed that transformers trained via gradient descent can learn causal structure. Chen et al. [2024] proved the
convergence of gradient flow on a multi-head softmax attention model for in-context multi-task linear regression.
Kim and Suzuki [2024], Yang et al. [2024] proved that trained transformers can learn nonlinear features in
context.
Recently,Lietal.[2024]studiedthetrainingdynamicsofasinglelayertransformerforin-contextclassification
problems. However, they only studied the binary classification tasks with finite patterns. They generated their
data as x = µ +κv , where {µ }M1 are in-domain-relevant patterns and {ν }M2 are in-domain-irrelevant
j k j j=1 k k=1
patterns, M ≥M and these patterns are all pairwise orthogonal. Thus, the possible distribution of their data
1 2
15is finite and highly limited. In contrast, our work explores the ICL capabilities of transformers for both binary
and multi-class classification of Gaussian mixtures. Specifically, our data is drawn according to Pb(µ ,µ ,Λ)
0 1
or Pm(µ,Λ), and the range and possible distributions of our data are infinite. Furthermore, the transformer
architectures analyzed in their work also differ from those in our study, thereby highlighting the distinct
contributions and independent interests of our work.
Some works also studied the ICL from other perspectives. To name a few, Xie et al. [2021] explained the
ICL as implicit Bayesian inference; Wang et al. [2023] explained the LLMs as latent variable models; Zhang
et al. [2023b] explained the ICL abilities of transformers as implicitly implementing a Bayesian model averaging
algorithm; and Li et al. [2023a] studied the generalization and stability of the ICL abilities of transformers.
B Additional notations
We denote X ∼Bin(n,p) if a random variable X follows the binomial distribution with parameters n∈N and
p ∈ [0,1], which means P(X =k) = n! pk(1−p)n−k. We denote X ∼ Multin(n,p) if random variables
k!(n−k)!
X =(X ,X ,...,X ) follow the Multinomial distribution with parameters n∈N and p =p =···=p =1/k,
1 2 k 1 2 k
which means P(X =(x ,x ,...,x ))= n! k−n. We denote ζ (x)=softmax(x) =exp(x )/((cid:80)k exp(x ))
for simplicity. We
defi1 ne2
δ =
1k
, δ
=(cid:81)k i= 01 ,x ik ̸=!
j. For x ∈
N,i
we define t (x)
=i ⌊(x−1i )/d⌋+j= 11
,t
(x)j
=
ii ij 1 2
((x−1) mod d)+1.
C Useful lemmas
Lemma C.1 ([Karimi et al., 2016]) If f :Rd →R is µ-strongly convex, then
µ
f(x)−minf(x)≥ ∥x∗−x∥2
x 2 2
where x∗ =argmin f(x).
x
Lemma C.2 ([Bubeck, 2015]) Suppose f :Rd →R is α-strongly convex and β-smooth for some 0<α≤β.
Then, the gradient descent iterating wt+1 =wt−η∇f(wt) with learning rate η =1/β and initialization w0 ∈Rd
satisfies that for any t≥1,
∥wt−w∗∥2 ≤exp(−t/κ)∥w0−w∗∥2
2 2
where κ=β/α is the condition number of f, and w∗ =argmin f(w) is the minimizer of f.
w∈Rd
D Training procedure for in-context binary classification
In this section, we present the proof of Theorem 3.1.
D.1 Proof sketch
First, we prove in Lemma D.2 that the expected loss function L(W) in (7) is strictly convex w.r.t. W and
is strongly convex in a compact set of Rd×d. Moreover, we prove L(W) has one unique global minimizer W∗.
Then, in Lemma D.3, by analyzing the Taylor expansion of L(W), we prove that as N →∞, our loss function
L(W) point wisely converges to L(cid:101)(W) (defined in (25)), and the global minimizer W∗ converge to 2Λ−1. We
denote W∗ =2(Λ−1+G), and prove ∥G∥ =O(N−1/2). Next, in Lemma D.4, by further analyzing the Taylor
max
expansion of the equation ∇L(W∗)=0 at the point 2Λ−1, we establish a tighter bound ∥G∥ =O(N−1). In
max
Lemma D.5, we prove that our loss function is l-smooth and provide an upper bound for l. Thus, in a compact
16setR , ourlossfunctionisα-stronglyconvexandl-smooth. Finally, leveragingthestandardresultsfromconvex
W
optimization, we prove Theorem 3.1 in subsection D.4.
In this section, we use the following notations.
D.2 Notations
Recall the expected loss function (7) is
1
L(W)=− E[(1+y )log(y )+(1−y )log(1−y )], (21)
2 τ,query (cid:98)τ,out τ,query (cid:98)τ,out
where
(cid:32)(cid:32) N (cid:33) (cid:33)
2 (cid:88) W
y =σ y x⊤ x
(cid:98)τ,out N τ,i τ,i 2 τ,query
i=1
is the output of the transformer, and the label of the data follows the distribution
P(y =1|x )=σ((µ −µ )⊤Λ−1x )).
τ,query τ,query τ,1 τ,0 τ,query
In this section, we introduce the following notations to analyze (7). We denote µ = µ , µ = µ , µ = µ
τ 1 τ,1 0 τ,0
and q = x . Then with probability P(y =1) = 1/2 we have q = µ + v, and with probability
τ,query τ,query 1
P(y =0) = 1/2 we have q = µ +v, where v ∼ N(0,Λ). We define p = 2 (cid:80)N y x . Since with
τ,query 0 N i=1 τ,i τ,i
probability P(y =1) = 1/2 we have x = µ +v , and with probability P(y =0) = 1/2 we have x =
τ,i τ,i 1 i τ,i τ,i
µ +v , where v ∼N(0,Λ), we known p=2N µ /N −2N µ /N +g, where g = 2 (cid:80)N v , g ∼N(0,4Λ/N),
0 i i 1 1 0 0 N i=1 i
N ∼Bin(N,1/2). Defining h=N /N −1/2, u=2(µ +µ ), we have N /N =1/2−h and
1 1 1 0 0
p=µ+hu+g. (22)
Then, the expected loss function (7) can be expressed as
L(W)=E[−σ(µ⊤Λ−1q)log(σ(p⊤Wq/2))−(1−σ(µ⊤Λ−1q))log(1−σ(p⊤Wq/2))]. (23)
The gradient of the loss function (7) can be expressed as
1
∇L(W)= E[(σ(p⊤Wq/2)−σ(µ⊤Λ−1q))pq⊤]. (24)
2
Moreover, we define a function L(cid:101)(W) as
L(cid:101)(W)=E[−σ(µ⊤Λ−1q)log(σ(µ⊤Wq/2))−(1−σ(µ⊤Λ−1q))log(1−σ(µ⊤Wq/2))]. (25)
In Lemma D.3, we show that as N →∞, L(W) will point wisely converge to L(cid:101)(W).
17D.3 Lemmas
Lemma D.1 Suppose N ∼Bin(N,1/2). Defining h=N /N −1/2, we have
1 1
E[h]=0
1
E[h2]=
4N
E[h3]=0
E[hn]=O(N−2), for n≥4
1
E[|h|]≤
2N1/2
E[|h3|]=O(N−3/2).
Proof Since N ∼Bin(N,1/2), the moment-generating function of N is
1 1
(cid:18)
1 1
(cid:19)N
M (t)= + exp(t) .
N1 2 2
We can compute the moment-generating function of h as follows:
(cid:18) t(cid:19) (cid:18) t (cid:19) (cid:18)exp −t +exp t (cid:19)N (cid:18) (cid:18) t (cid:19)(cid:19)N
M (t)=exp − M = 2N 2N = cosh
h 2 N1 N 2 2N
(cid:32) t2 (cid:88)∞ t2i (cid:33)N
= 1+ + .
8N2 (2i)!(2N)2i
i=2
Thus, we know the coefficients of t, t2, t3 are 0,1/(8N),0 respectively, and the coefficients of tn,n ≥ 4 are
O(1/N2). We have
E[h]=0
1
E[h2]=
4N
E[h3]=0
E[hn]=O(1/N2), for n≥4.
Moreover, according to the Jensen’s inequality, we have
E[|h|]≤(cid:0)E[h2](cid:1)1/2
=
1
2N1/2
E[|h3|]≤(cid:0)E[h4](cid:1)3/4 =O(N−3/2).
■
Lemma D.2 For the loss function L(W) (7), we have ∇2L(W)≻0. For any compact set R of Rd×d, when
W
W ∈ R , we have ∇2L(W) ≻ γI for some γ > 0. Additionally, L(W) has one unique global minimizer on
W d
Rd×d.
For L(cid:101)(W) defined in (25), we also have ∇2L(cid:101)(W)≻0. For any compact set R
W
of Rd×d, when W ∈R W, we
have ∇2L(cid:98)(W)≻γI
d
for some γ >0. Additionally, L(cid:101)(W) has one unique global minimizer on Rd×d.
Proof We vectorize W as Vec(W) ∈ Rd2, where Vec(W) = W , t (x) = ⌊(x−1)/d⌋+1,t (x) =
i t1(i),t2(i) 1 2
18((x−1) mod d)+1. Then, we have
(cid:20) (cid:21)
1
(∇L(W)) =E (σ(p⊤Wq/2)−σ(µ⊤Λ−1q))p q . (26)
i p,q 2 t1(i) t2(i)
The Hessian matrix of the loss function (7) is
(cid:20) (cid:21)
1
(∇2L(W)) =E σ(p⊤Wq/2)(1−σ(p⊤Wq/2))p q p q .
ij p,q 4 t1(i) t2(i) t1(j) t2(j)
Considering z ∈Rd2 such that z ̸=0, we have
(cid:34) (cid:35)
1 (cid:88)
z⊤∇2L(W)z =E σ(p⊤Wq/2)(1−σ(p⊤Wq/2)) z z p q p q
q,p 4 a b t1(a) t2(a) t1(b) t2(b)
ab
 2
(cid:90) 1 (cid:88)
= 4σ(p⊤Wq/2)(1−σ(p⊤Wq/2)) z ap t1(a)q t2(a) f pq(p,q)dpdq,
a∈[d2]
where f (p,q) are the probability density function (PDF) function of p,q. Since for any p,q, σ(p⊤Wq/2)(1−
pq
σ(p⊤Wq/2))>0, we have z⊤∇2L(W)z ≥0. Thus, ∇2L(W)⪰0 and L(W) is convex.
Moreover, for any z ̸= 0, we denote z = z ,i,j ∈ [d]. Suppose a,b ∈ argmax |z |, we consider
ij ((i−1)d+j) i,j ij
a set of constants {c ,c },{c ,c },i,j ∈ [d], where c = d,c = d+1, c = d,c = d+1, and
1pi 2pi 1qi 2qi 1pa 2pa 1qb 2qb
c =1/16,c =1/8,i̸=a, c =1/16,c =1/8,j ̸=b. Then, for any c ∈[c ,c ],c ∈[c ,c ]. We
1pi 2pi 1qj 2qj pi 1pi 2pi qj 1qj 2qj
have
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:88) z ijc pic qj(cid:12) (cid:12)≥(cid:2) d2−2(d+1)(d−1)/8−(d−1)2/64(cid:3) max|z ij|≥d2max|z ij|/2.
(cid:12) (cid:12) ij ij
(cid:12)i,j∈[d] (cid:12)
Then, we define region
Ω(a,b)≜{p=(cid:80)
c e , q
=(cid:80)
c e ,c ∈[c ,c ],c ∈[c ,c ]}. We have
i pi i j qj j pi 1pi 2pi qj 1qj 2qj
 2
(cid:88)
Ωm (ai ,n b) z cp t1(c)q t2(c) ≥d4m ia jx|z ij|2/4≥∥z∥2 2/4.
c∈[d2]
Defining
(cid:90)
C(Ω)= min f (p,q)dpdq,
pq
a∈[d],b∈[d] Ω(a,b)
(cid:26) (cid:27)
1
S(Ω,W)= min min σ(p⊤Wq/2)(1−σ(p⊤Wq/2)) ,
a∈[d],b∈[d]Ω(a,b) 4
wehaveS(Ω,W)>0. SincewithprobabilityP(y =1)=1/2,q =µ +v,withprobabilityP(y =0)=
τ,query 1 τ,query
1/2, q =µ +v, where v ∼N(0,Λ) and p=µ+hu+g, where g ∼N(0,4Λ/N),v ∼N(0,Λ),µ ∼N(0,I ), the
0 0 d
covariance matrices of p,q are positive definite and we have f (p,q)>0 for all p,q ∈Rd. Moreover, Ω(a,b) are
pq
non-zero measures on Rd×d. Thus, we have C(Ω)>0. Then, for any z ̸=0, we have
(cid:32) (cid:33)2
(cid:90) 1 (cid:88)
z⊤∇2L(W)z ≥ σ(p⊤Wq/2)(1−σ(p⊤Wq/2)) z p q f (p,q)dpdq
4 l t1(l) t2(l) pq
Ω(a,b) l
≥C(Ω)S(Ω,W)∥z∥2/4
2
>0.
19Thus, we have ∇2L(W)≻0. L(W) is strictly convex.
Moreover, for any compact set R of Rd×d, for any W ∈R , we have
W W
(cid:26) (cid:27)
1
S(Ω)= min min min σ(p⊤Wq/2)(1−σ(p⊤Wq/2)) >0.
W∈RW a∈[d],b∈[d]Ω(a,b) 4
Then, for any W ∈R , for any z ̸=0, we have
W
(cid:32) (cid:33)2
(cid:90) 1 (cid:88)
z⊤∇2L(W)z ≥ σ(p⊤Wq/2)(1−σ(p⊤Wq/2)) z p q f (p,q)dpdq
4 l t1(l) t2(l) pq
Ω(a,b) l
1
≥ C(Ω)S(Ω)∥z∥2.
4 2
Thus, when W ∈ R , where R is a compact set, we have ∇2L(W) ≻ C(Ω)S(Ω)I /4 and the loss function
W W d
L(W) is γ−strongly convex, where γ =C(Ω)S(Ω)/4.
Because our loss function is strictly convex in Rd×d, it has at most one global minimizer in Rd×d. Next,
we prove all level sets of our loss function are compact, i.e. V = {W ∈ Rd×d|L(W) ≤ α} is compact for all
α
α. We prove it by contradiction. Suppose V is not compact for some α. Since our loss function is continuous
α
and convex, V is an unbounded convex set. Since the dimension of V is d2, consider a point Wα ∈V , there
α α α
must exists a Wk ̸=0 such that {Wα+tWk|t=[0,∞)}∈V . For this Wk ̸=0 , there must exist a set
d×d α d×d
of constants 0<c <c ,0<c <c such that for any c ∈[c ,c ],c ∈[c ,c ], we have
3pi 4pi 3qj 4qj pi 3pi 4pi qj 3qj 4qj
(cid:88)
| c c Wk|≠ 0.
pi qj ij
ij
Thus, we have
(cid:88)
lim | c c (Wα+tWk)|=∞.
pi qj ij ij
t→∞
ij
We define Ω = {p = (cid:80) c e , q = (cid:80) c e , c ∈ [c ,c ],c ∈ [c ,c ],∥µ∥2 ≤ (cid:80) c2 +c2 }. Then,
0 i pi i j qj j pi 3pi 4pi qj 3qj 4qj 2 i 4pi 4qj
defining
(cid:90)
C(Ω )= f (p,q)dpdq,
0 pq
Ω0
S(Ω
)=min(cid:8) min{σ(µ⊤Λ−1q),(1−σ(µ⊤Λ−1q))}(cid:9)
,
0
Ω0
20we have S(Ω )>0. Since Ω are non-zero measures for p,q, we have C(Ω )>0. Then, we have
0 0 0
lim L(Wα+tWk)
t→∞
= lim E[−σ(µ⊤Λ−1q)log(σ(p⊤(Wα+tWk)q/2))−(1−σ(µ⊤Λ−1q))log(1−σ(p⊤(Wα+tWk)q/2))]
t→∞
(cid:90)
(cid:88)
≥ lim [−σ(µ⊤Λ−1q)log(σ( c c (Wα+tWk)/2))]f (p,q)dpdq
pi qj ij ij pq
t→∞
Ω0 ij
(cid:90)
(cid:88)
+ lim [−(1−σ(µ⊤Λ−1q))log(1−σ( c c (Wα+tWk)/2))]f (p,q)dpdq
pi qj ij ij pq
t→∞
Ω0 ij
 
 (cid:88) 
≥C(Ω )S(Ω )·min lim[−log(σ( c c (Wα+tWk)/2))]
0 0 pi qj ij ij
Ω0 t→∞
ij

 
 (cid:88) 
+C(Ω )S(Ω )·min lim[−log(1−σ( c c (Wα+tWk)/2))]
0 0 pi qj ij ij
Ω0 t→∞
ij

=∞.
This contradicts the assumption L(Wα+tWk)≤α. Thus, all level sets of the loss function L(W) are compact,
which means there exists a global minimizer for L(W). Together with the fact that L(W) is strictly convex,
L(W) has one unique global minimizer on Rd×d.
Similarly, we can prove the same conclusions for L(cid:101)(W). ■
Lemma D.3 Denoting the global minimizer of the loss function (7) as W∗, we have W∗ =2(Λ−1+G), where
∥G∥ =O(N−1/2).
max
Proof Let a=µ⊤Λ−1q, s=µ⊤Wq/2, r =(hu+g)⊤Wq/2. Performing the Taylor expansion on (7), we have
L(W)=E[−σ(a)log(σ(s+r))−(1−σ(a))log(1−σ(s+r))]
=E[−σ(a)log(σ(s))−(1−σ(a))log(1−σ(s))]−E[(σ(a)(1−σ(s))−(1−σ(a))σ(s)))r]
+E(cid:2) σ(ξ(s,r))(1−σ(ξ(s,r)))r2/2(cid:3)
=L(cid:101)(W)−E[(σ(a)(1−σ(s))−(1−σ(a))σ(s)))r]+E(cid:2) σ(ξ(s,r))(1−σ(ξ(s,r)))r2/2(cid:3)
,
whereξ(s,r)arerealnumbersbetweensands+r.
AccordingtoLemmaD.1,wehaveE[r]=E(cid:2) (hu+g)⊤Wq/2(cid:3)
=
0. Thus, we have
E[(σ(a)(1−σ(s))−(1−σ(a))σ(s)))r]=E [(σ(a)(1−σ(s))−(1−σ(a))σ(s)))E [r]]=0.
µ,u,q g,h
Moreover, we have
E(cid:2) σ(ξ(s,r))(1−σ(ξ(s,r))r2/2(cid:3) ≤E(cid:2) r2(cid:3)
=E[h2u⊤Wqu⊤Wq+g⊤Wqg⊤Wq]
( =a)E[u⊤Wqu⊤Wq/(4N)+4(ΛWq)⊤Wq/N]
≤C ∥W∥2 /N,
l max
where (a) is due to Lemma D.1, g⊤Wqg⊤Wq = (cid:80) g W q g W q = (cid:80) g g W q W q =
i,j,k,l∈[d] i ij j k kl l i,j,k,l∈[d] i k kl l ij j
(gg⊤Wq)⊤Wq and E[gg⊤]=4Λ/N. C is a constant independent of N and W. Thus, we have
l
(cid:12) (cid:12)
(cid:12) (cid:12)L(cid:101)(W)−L(W)(cid:12) (cid:12)≤C l∥W∥2 max/N.
21This shows that L(W) point wisely converges to L(cid:101)(W).
According to Lemma D.2, L(cid:101)(W) has one unique global minimizer. Consider the equation:
∇L(cid:101)(W)=E[σ(µ⊤Wq/2)−σ(µ⊤Λ−1q)]=0.
We can easily find that ∇L(cid:101)(2Λ−1)=0 and W =2Λ−1 is the global minimizer of L(cid:101)(W).
Considering a compact set R = {W |∥W −2Λ−1∥ ≤ ρ }, we have ∥W∥ ≤ C for W ∈ R . Here
W F W max W W
ρ ,C are some positive finite constants. Then, we have
W W
(cid:12) (cid:12)
(cid:12) (cid:12)L(cid:101)(W)−L(W)(cid:12) (cid:12)≤C l′/N, W ∈R W,
where C′ = C C2 is a constant independent of N and W. This shows that, for W ∈ R , our loss function
l l W W
L(W) uniformly converge to L(cid:101)(W).
Denote W∗ as the global minimizer of the loss function L(W) with prompt length N. Then, we show
that, when N is sufficiently large, W∗ ∈ R . We first denote ∂R = {W |∥W − 2Λ−1∥ = ρ } and
W W F W
∆=min
W∈∂RW
L(cid:101)(W)−L(cid:101)(2Λ−1)>0. Then, for N ≥4C l′/∆, and for any W ∈R W, we have
(cid:12) (cid:12)
(cid:12)L(cid:101)(W)−L(W)(cid:12)≤∆/4,
(cid:12) (cid:12)
This means
min L(W)− min L(W)
W∈∂RW W∈RW
≥ min L(W)−L(2Λ−1)
W∈∂RW
≥ min L(cid:101)(W)−L(cid:101)(2Λ−1)−∆/2
W∈∂RW
≥∆/2
>0.
Since L(W) is strictly convex, we have W∗ =argmin L(W)∈R .
W W
Then, we have
|L(cid:101)(W∗)−L(W∗)|≤C′/N
l
|L(cid:101)(2Λ−1)−L(2Λ−1)|≤C′/N
l
and
L(cid:101)(W∗)≤L(W∗)+C′/N ≤L(2Λ−1)+C′/N ≤L(cid:101)(2Λ−1)+2C′/N.
l l l
According to Lemma D.2, for W ∈R W, we have ∇2L(cid:101)(W)≻γI d, where γ is a positive constant independent of
N. Thus, L(cid:101)(W) is γ-strongly convex in R W. According to Lemma C.1, we have
2 4C′
∥W∗−2Λ−1∥2 ≤ (L(cid:101)(W∗)−L(cid:101)(2Λ−1))≤ l.
F γ γN
√
Thus, when N →∞, we have W∗ →2Λ−1. Denoting W∗ =2(Λ−1+G), we have ∥G∥ =O(1/ N). ■
max
22Lemma D.4 The global minimizer of the loss function (7) is W∗ =2(Λ−1+G), where
1
∥G∥ ≤ ∥S−1(E[σ′(a)(4qq⊤+uu⊤Λ−1qq⊤/4)
max N
+σ′′(a)((u⊤Λ−1q)2µq⊤/8+2q⊤Λ−1qµq⊤)])∥ +o(1/N),
max
a=µ⊤Λ−1q, S =4∇2L(cid:101)(2Λ−1).
Proof According to Lemma D.2, the loss function L(W) has a unique global minimizer W∗. We have
∇L(W∗)=E(cid:2) (σ(p⊤W∗q/2)−σ(µ⊤Λ−1q))pq⊤(cid:3)
=0. (27)
Let W∗ =2(Λ−1+G), a=µ⊤Λ−1q, b=(µ+hu+g)⊤Gq+(hu+g)⊤Λ−1q. We have
p⊤W∗q/2
=(µ+hu+g)⊤(Λ−1+G)q
=(µ+hu+g)⊤Gq+(hu+g)⊤Λ−1q+µ⊤Λ−1q
=a+b.
The Taylor expansion of σ(a+b) at point a with an Lagrange form of remainder is
σ′′(a) σ′′′(ξ(a,b))
σ(a+b)pq⊤ =σ(a)pq⊤+σ′(a)bpq⊤+ b2pq⊤+ b3pq⊤,
2 3!
where ξ(a,b) are real numbers between a and a+b. Thus, our equation (27) become
(cid:20) σ′′(a) σ′′′(ξ(a,b)) (cid:21)
E σ′(a)bpq⊤+ b2pq⊤+ b3pq⊤ =0. (28)
µ,u,g,h,q 2 3!
Note that E[σ′(a)bpq⊤] = E (cid:2) σ′(a)E (cid:2) bpq⊤(cid:3)(cid:3) . For E [bpq⊤], according to Lemma D.1 and g ∼
µ,u,q g,h g,h
N(0,4Λ/N), we have
E [bpq⊤]=E[µ⊤Gqµq⊤+g⊤Λ−1qgq⊤+g⊤Gqgq⊤+h2u⊤Gquq⊤+h2u⊤Λ−1quq⊤]
g,h
=µµ⊤Gqq⊤+4qq⊤/N +4ΛGqq⊤/N +uu⊤Gqq⊤/(4N)+uu⊤Λ−1qq⊤/(4N). (29)
Then, we have
∥E [σ′(a)(4ΛGqq⊤/N +uu⊤Gqq⊤/(4N))]∥ ≤c ∥G∥ /N,
µ,u,q max 1 max
where c =max |E[(cid:80) 4σ′(a)(Λ q q )+(cid:80) σ′(a)(u u q q /4)]| is a constant independent of N. According
1 ij kl √ ik l j kl i k l j
to Lemma D.3, ∥G∥ =O(1/ N)=o(1), we have
max
∥E [σ′(a)(4ΛGqq⊤/N +uu⊤Gqq⊤/(4N))]∥ =o(1/N), (30)
µ,u,q max
23Similarly for E[σ′′(a)b2pq⊤/2], we have
E [b2pq⊤]
g,h
=E[µ⊤Gqµ⊤Gqµq⊤+h2u⊤Gqu⊤Gqµq⊤+g⊤Gqg⊤Gqµq⊤+2h2u⊤Gqµ⊤Gquq⊤+2g⊤Gqµ⊤Gqgq⊤]
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
+E[2h2u⊤Gqu⊤Λ−1qµq⊤+2g⊤Gqg⊤Λ−1qµq⊤+2h2µ⊤Gqu⊤Λ−1quq⊤+2µ⊤Gqg⊤Λ−1qgq⊤]
(cid:124) (cid:123)(cid:122) (cid:125)
(ii)
+E[h2u⊤Λ−1qu⊤Λ−1qµq⊤+g⊤Λ−1qg⊤Λ−1qµq⊤].
(cid:124) (cid:123)(cid:122) (cid:125)
(iii)
Each term in (i) contains two G’s. Thus, their max norms are at most O(∥G∥2 ). For each term in (ii), it
max
contains one G and h2 or contains one G and two g. According to E[h2]=1/(4N) in Lemma D.1, the max norm
of terms with one G and h2 are smaller than O(∥G∥ /N). Defining g¯=N1/2Λ−1/2g/2, we have g¯∼N(0,I )
max d
and g =2N−1/2Λ1/2g¯. Thus, converting two g to g¯, we have a coefficient of N−1. Therefore, the max norms of
terms with one G and two g are also smaller than O(∥G∥ /N). Therefore, for terms (i),(ii), we have
max
∥E[σ′′(a)(i)/2]∥ ≤O(∥G∥2 )=o(∥G∥ ), (31)
max max max
∥E[σ′′(a)(ii)/2]∥ ≤O(∥G∥ /N)=o(1/N). (32)
max max
For term (iii), according to Lemma D.1 and g ∼N(0,4Λ/N), we have
∥E[σ′′(a)(iii)/2]∥
max
=∥E(cid:2) σ′′(a)(h2u⊤Λ−1qu⊤Λ−1qµq⊤+g⊤Λ−1qg⊤Λ−1qµq⊤)/2(cid:3)
∥ (33)
max
=
1 ∥E(cid:2) σ′′(a)((u⊤Λ−1q)2µq⊤/8+2q⊤Λ−1qµq⊤)(cid:3)
∥ . (34)
N max
For E[σ′′′(ξ(a,b))b3pq⊤/3!], we have
∥E[σ′′′(ξ(a,b))b3pq⊤/3!]∥
max
≤max|σ′′′(z)|/3!·maxE(cid:2)(cid:12) (cid:12)b3p iq j(cid:12) (cid:12)(cid:3)
z∈R ij
(cid:20)
≤O(1)·maxE (cid:88) (cid:12) (cid:12)ϕ⊤ 1Gqϕ⊤ 2Gqϕ⊤ 3Gqp iq j(cid:12) (cid:12)
ij
ϕ1,ϕ2,ϕ3∈{µ,hu,g}
(cid:124) (cid:123)(cid:122) (cid:125)
(∗)
+ (cid:88) (cid:12) (cid:12)ϕ⊤ 1Gqϕ⊤ 2Gqϕ⊤ 3Λ−1qp iq j(cid:12) (cid:12)
ϕ1,ϕ2∈{µ,hu,g},ϕ3∈{hu,g}
(cid:124) (cid:123)(cid:122) (cid:125)
(∗)
+ (cid:88) (cid:12) (cid:12)ϕ⊤ 1Gqϕ⊤ 2Λ−1qϕ⊤ 3Λ−1qp iq j(cid:12) (cid:12)
ϕ1∈{µ,hu,g},ϕ2,ϕ3∈{hu,g}
(cid:124) (cid:123)(cid:122) (cid:125)
(∗∗)
(cid:21)
+ (cid:88) (cid:12) (cid:12)ϕ⊤ 1Λ−1qϕ⊤ 2Λ−1qϕ⊤ 3Λ−1qp iq j(cid:12) (cid:12) .
ϕ1,ϕ2,ϕ3∈{hu,g}
(cid:124) (cid:123)(cid:122) (cid:125)
(∗∗∗)
For terms in (∗) containing two or three G, these terms’ expected absolute values are at most smaller than
O(∥G∥2 ). For terms in (∗∗) containing one G, these terms must contain n number of h and n number of
max 1 2
elements of g, where n +n = 2,3,4,n ,n ∈ N. According to Lemma D.1, we know that for n = 1,2,3,4,
1 2 1 2 1
E|hn1| ≤ O(N−n1/2). Defining g¯= N1/2Λ−1/2g/2, we have g¯∼ N(0,I d) and g = 2N−1/2Λ1/2g¯. Converting g
24to g¯, we have a coefficient of N−n2/2. Thus, for terms in (∗∗), these terms’ expected absolute values are at
most smaller than O(∥G∥ maxN−(n1+n2)/2)≤O(∥G∥ maxN−1). For terms in (∗∗∗) without G, these terms must
contain n number of h and n number of elements of g, we have n +n = 3,4,n ,n ∈ N. Similarly, these
1 2 1 2 1 2
term’s expected absolute values are at most smaller than O(N−(n1+n2)/2)≤O(N−3/2). Therefore, we have
∥E[σ′′′(ξ(a,b))b3pq⊤/3!]∥
max
≤maxE(cid:2)(cid:12) (cid:12)b3p iq j(cid:12) (cid:12)(cid:3) ·max|σ′′′(z)|/3!
ij z
=O(∥G∥2 )+O(∥G∥ /N)+O(1/N−3/2)
max max
=o(∥G∥ )+o(1/N). (35)
max
Moreover, we have
(cid:26) (cid:27)
(cid:88)
E [σ′(a)µµ⊤Gqq⊤] = s G , (36)
µ,u,q ijkl kl
ij kl
where s = Eσ′(a)µ µ q q . We vectorize G as Vec(G) = G . Define S ∈ Rd2×d2, where S =
ijkl i k l j i t1(i),t2(i) ij
s =Eσ′(a)µ q µ q . Then (36) can be expressed as
t1(i),t2(i),t1(j),t2(j) t1(i) t2(i) t1(j) t2(j)
(cid:26) (cid:27)
E [σ′(a)µµ⊤Gqq⊤] =SG. (37)
µ,v
Note that S =4∇2L(cid:101)(2Λ−1). According to Lemma D.2, S is positive definite. Thus, combining (28), (29), (30),
(31), (32), (34), (35), (37), we have
∥G∥ ≤
1 ∥S−1(cid:0)E[σ′(a)(4qq⊤+uu⊤Λ−1qq⊤/4)+σ′′(a)((u⊤Λ−1q)2µq⊤/8+2q⊤Λ−1qµq⊤)](cid:1)
∥ +o(1/N).
max N max
■
Lemma D.5 The loss function (7) is l-smooth, where l≤ 1(cid:80) E[(p q )2].
4 i∈[d2] t1(i) t2(i)
Proof The Hessian matrix of the loss function is
1
(∇2L(W)) = E[σ(p⊤Wq/2)(1−σ(p⊤Wq/2))p q p q ].
ij 4 t1(i) t2(i) t1(j) t2(j)
Considering z ∈Rd2 such that z ̸=0, we have
(cid:34) (cid:35)
1 (cid:88)
z⊤∇2L(W)z =E σ(p⊤Wq/2)(1−σ(p⊤Wq/2)) z z p q p q
4 a b t1(a) t2(a) t1(b) t2(b)
ab
  2
1 (cid:88)
=E 4σ(p⊤Wq/2)(1−σ(p⊤Wq/2)) z ap t1(a)q t2(a)  
a∈[d2]
  2
1 (cid:88)
≤E 4 z ap t1(a)q t2(a)  
a∈[d2]
(a)1 (cid:88)
≤ ∥z∥2 E[(p q )2]
4 2 t1(i) t2(i)
i∈[d2]
where (a) is due to the Cauchy–Schwarz inequality. Thus, ∇2L(W)⪯lI and L(W) is l-smooth, where l is a
d
constant smaller than 1(cid:80) E[(p q )2]. ■
4 i∈[d2] t1(i) t2(i)
25D.4 Proof of Theorem 3.1
Proof According to Lemma D.4, the global minimizer of L(W) is W∗ =2(Λ−1+G), where
1 1
∥G∥ ≤ ∥S−1(E[σ′(a)(4qq⊤+ uu⊤Λ−1qq⊤)
max N 4
1
+σ′′(a)( (u⊤Λ−1q)2µq⊤+2q⊤Λ−1qµq⊤)])∥ +o(1/N). (38)
8 max
Define R ={W ∈Rd×d|∥W−W∗∥ ≤∥W0−W∗∥ }. R is acompactset. Then, according to Lemma D.2,
W F F W
for W ∈ R , we have ∇2L(W) ⪰ αI . Here α > 0 is a positive constant number. Thus, L(W) is α-strongly
W d
convex in R . Moreover, according to Lemma D.5, L(W) is l-smooth. Then according to Lemma C.2, applying
W
gradient descent with η =1/l, for any t≥1, we have
∥Wt−W∗∥2 ≤exp(−t/κ)·∥W0−W∗∥2,
F F
where κ=l/α. ■
E In-context inference of binary classification
E.1 Notations
In this section, we use the following notations. We denote µ = µ −µ , u = 2(µ +µ ), q = x . Define
1 0 1 0 query
p = 2 (cid:80)M y x . Since with probability P(y =1) = 1/2, x = µ +v , with probability P(y =0) = 1/2,
M i=1 i i i i 1 i i
x =µ +v ,wherev ∼N(0,Λ),wehavep=2M µ /M−2M µ /M+g,whereg = 2 (cid:80)M v ,g ∼N(0,4Λ/M),
i 0 i i 1 1 0 0 M i=1 i
M ∼Bin(M,1/2). Defining h=M /N −1/2, u=2(µ +µ ), we have M /N =1/2−h and
1 1 1 0 0
p=µ+hu+g. (39)
E.2 Proof of Theorem 3.2
Proof The output of the trained transformer is
(cid:32)(cid:32) M (cid:33) (cid:33)
2 (cid:88)
y
(cid:98)out
=σ
M
y ix⊤
i
(Λ−1+G(cid:98))x
query
=σ(p⊤(Λ−1+G(cid:98))q). (40)
i=1
The probability of y =1 given x is
query query
P(y =1|x )=σ((µ −µ )⊤Λ−1x )=σ(µ⊤Λ−1q).
query query 1 0 query
Defining a=µ⊤Λ−1q, b=(µ+hu+g)⊤G(cid:98)q+(hu+g)⊤Λ−1q, we have
p⊤(Λ−1+G(cid:98))q
=(µ+hu+g)⊤(Λ−1+G(cid:98))q
=(µ+hu+g)⊤G(cid:98)q+(hu+g)⊤Λ−1q+µ⊤Λ−1q =a+b,
and
(cid:104) (cid:105)
E σ(p⊤(Λ−1+G(cid:98))q) =E[σ(a+b)]=E[σ(a)+σ′(a)b+σ′′(ξ(a,b))b2/2],
26where ξ are real numbers between a and a+b. Thus, we have
E[|σ(a+b)−σ(a)|]
≤E[(cid:12) (cid:12)σ′(a)b+σ′′(ξ(a,b))b2/2(cid:12)
(cid:12)]
≤σ′(a)E[|b|]+E[b2]
We first consider the term σ′(a)E[|b|]. Defining g¯=Λ−1/2M1/2g/2, we have
σ′(a)E[|b|]
(cid:104) (cid:105)
≤σ′(a) |µ⊤G(cid:98)q|+E[|hu⊤G(cid:98)q|]+E[|g⊤G(cid:98)q|]+E[|hu⊤Λ−1q|]+E[|g⊤Λ−1q|]
(cid:20) (cid:21)
(a) 1 2 1 2
≤σ′(a) |µ⊤G(cid:98)q|+ |u⊤G(cid:98)q|+ E[|g¯⊤Λ1/2G(cid:98)q|]+ |u⊤Λ−1q|+ E[|g¯⊤Λ−1/2q|]
2M1/2 M1/2 2M1/2 M1/2
  √ 
(cid:18) (cid:19)
( ≤b) σ′(a)∥G(cid:98)∥
max
(cid:88) |µ iq j|+ M1
1/2
1 2|u⊤Λ−1q|+ 2 √ π2 (cid:88) |Λ− ij1/2q j|+o N1 + √1
M
,
i,j∈[d] i,j∈[d]
√ √
where (a) is due to E[|h|]≤1/(2M1/2) in Lemma D.1. (b) is because that g¯ ∼N(0,1) and E[|g¯|]= 2/ π, for
i i
i∈[d].
For E[b2], we have
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
E[b2]≤E [(µ+hu+g)⊤G(cid:98)q]2 +E [(hu+g)⊤Λ−1q]2 +2E (µ+hu+g)⊤G(cid:98)q(hu+g)⊤Λ−1q .
(cid:104) (cid:105)
Notice that terms in E [(µ+hu+g)⊤G(cid:98)q]2 contain two G(cid:98). Thus, they are at most smaller than O(∥G(cid:98)∥2 )=
max
(cid:104) (cid:105)
O(1/N2). Terms in E [(hu+g)⊤Λ−1q]2 /2 contain two h, or two g, or one h and one g. According to Lemma
√
D.1, we have E[|h|] = O(1/ M), E[h2] = 1/(4M). Moreover, g = 2M−1/2Λ1/2g¯. Converting one g to g¯, we
(cid:104) (cid:105)
have a coefficient of M−1/2. Thus, terms in E [(hu+g)⊤Λ−1q]2 /2 contain two h, or two g, or one h and
(cid:104) (cid:105)
one g are O(1/M). Terms in E (µ+hu+g)⊤G(cid:98)q(hu+g)⊤Λ−1q contain at least one G(cid:98) and one h or one
√ √
G(cid:98) and one g. Thus, they are at √most smaller than O √(∥G(cid:98)∥ max/ M) = O(1/(N M)). Therefore, we have
E[b2|]/2=O(1/N2+1/M +1/(N M))=o(1/N +1/ M).
Finally, we have
E[∆(y ,y )]=E[|y −P(y =1|x )|]=E[|σ(a+b)−σ(a)|]≤σ′(a)E[|b|]+E[b2]
query (cid:98)query (cid:98)out query query
  √ 
(cid:18) (cid:19)
≤σ′(a)∥G(cid:98)∥
max
(cid:88) |µ iq j|+ M1
1/2
1 2|u⊤Λ−1q|+ 2 √ π2 (cid:88) |Λ− ij1/2q j|+o N1 + √1
M
.
i,j∈[d] i,j∈[d]
■
Remark E.1 We note that Theorem 3.2 requires Assumption 3.2 to hold. For example, we need the covariance
Λ in training and testing to be the same. A similar consistency requirement of the covariance Λ in training and
testing had also been observed for in-context linear regression in Zhang et al. [2023a].
Here, we discuss the consequences when Assumption 3.2 does not hold. For example, suppose the labels of
our data in test prompts are not balanced where P(y =1)=p ,P(y =−1)=p . Besides, µ ,µ do not have the
1 0 0 1
same Λ−1 weighted norm, and the covariance matrix of test data is Γ̸=Λ. Then, as N,M →∞, we have
M
2 (cid:88)
y x⊤ →2(p µ −p µ )⊤,
M i i 1 1 0 0
i=1
27and
P(y =1)→σ(2(p µ −p µ )⊤Λ−1x ).
(cid:98)query 1 1 0 0 query
On the other hand, the distribution of the ground truth label is
P(y =1)=σ((µ −µ )⊤Γ−1x +(µ⊤Λ−1µ −µ⊤Λ−1µ )/2+log(p /p )).
query 1 0 query 1 1 0 0 1 0
Define z ≜ (µ −µ )⊤Γ−1x +(µ⊤Λ−1µ −µ⊤Λ−1µ )/2+log(p /p ) and zˆ ≜ 2(p µ −p µ )⊤Λ−1x .
1 0 query 1 1 0 0 1 0 1 1 0 0 query
Then, we can notice that unless zˆ= z or |σ(zˆ)−σ(z)| is sufficiently small, the transformer cannot correctly
perform the in-context binary classification.
F Training procedure for in-context multi-class classification
In this section, we present the proof of Theorem 4.1.
F.1 Proof sketch
First, we prove in Lemma F.3 that the expected loss function L(W) (16) is strictly convex w.r.t. W and is
strongly convex in a compact set of Rd×d. Moreover, we prove L(W) has one unique global minimizer W∗. Then,
in Lemma F.4, by analyzing the Taylor expansion of L(W), we prove that as N →∞, our loss function L(W)
point wisely converges to L(cid:101)(W) (defined in (44)), and the global minimizer W∗ converge to 2Λ−1. Thus, we
denote W∗ =2(Λ−1+G), and prove ∥G∥ =O(N−1/4). Next, in Lemma F.5, by further analyzing the Taylor
max
expansion of the equation ∇L(W∗)=0 at the point 2Λ−1, we establish a tighter bound ∥G∥ =O(cN−1). In
max
Lemma F.6, we prove that our loss function is l-smooth and provide an upper bound for l. Thus, in a compact
set R , our loss function is α-strongly convex and l-smooth. Finally, leveraging the standard results from the
W
convex optimization, we prove Theorem 4.1 in subsection F.3.
In this section, we use the following notations.
F.2 Notations
Recall the expected loss function (16) is
(cid:34) c (cid:35)
(cid:88)
L(W)=−E (y ) log((y ) ) , (41)
τ,query k (cid:98)τ,out k
k=1
where
(cid:32) (cid:32) N (cid:33) (cid:33)
1 c (cid:88)
(y ) =softmax y x⊤ Wx
(cid:98)τ,out k c N τ,i τ,i τ,query
i=1 k
is the output of the transformer, and the label of the data follows the distribution
P(y =e |x )=softmax(µ⊤Λ−1x )) .
τ,query k τ,query τ τ,query k
Inthissection,weintroducethefollowingnotationstoanalyze(16). Wedenoteµ =µ ,µ=(µ ,µ ,...,µ )∈
k τ,k 1 2 k
Rd×c and q =x . Then with probability P(y =e )=1/c, q =µ +v, where v ∼N(0,Λ). We define
τ,query τ,query k k
p = c (cid:80)N (y ) x ∈Rd and P =(p ,p ,...,p )∈Rd×c. We have P⊤ = c (cid:80)N y x⊤ ∈Rc×d. Since with
k N i=1 τ,i k τ,i 1 2 c N i=1 i τ,i
probability P(y =e )=1/c we have x =µ +v , where v ∼N(0,Λ), we known p = c (cid:80)N (y ) x =
τ,i k τ,i k i i k N i=1 τ,i k τ,i
cN µ /N +g , where g = c (cid:80) v , g ∼ N(0,c2N Λ/N2) and (N ,N ,...,N ) ∼ Multin(n,1/c).
k k k k N i∈{i|yτ,i=ek} i k k 1 2 c
Defining h =N /N −1/c, we have N /N =1/c+h and p =µ +ch µ +g . Defining g¯ =Λ−1/2g , we
k k k k k k k k k k k
28have g¯ ∼ N(0,c2N I /N2). Defining µ = (h µ ,h µ ,...,h µ ) ∈ Rd×c and g = (g ,g ,...,g ) ∈ Rd×c, we
k k d h 1 1 2 2 k k 1 2 k
have P =µ+cµ +g.
h
Then, the expected loss function (16) can be expressed as
(cid:34) c (cid:35)
(cid:88)
L(W)=E −softmax(µ⊤Λ−1q) log(softmax(P⊤Wq/c) ) . (42)
k k
k=1
The gradient of the loss function (16) can be expressed as
(cid:34) c (cid:35)
∇L(W)=E (cid:88)(cid:2) (softmax(P⊤Wq/c) −softmax(µ⊤Λ−1q) )p q⊤/c(cid:3) . (43)
k k k
k=1
Moreover, we define a function L(cid:101)(W) as
c
(cid:88)
L(cid:101)(W)=E[ −softmax(µ⊤Λ−1q) klog(softmax(µ⊤Wq/c) k)]. (44)
k=1
In Lemma F.4, we show that as N →∞, L(W) will point wisely converge to L(cid:101)(W).
Lemma F.1 Suppose (N ,N ,...,N )∼Multin(N,1/c). Defining h =N /N −1/c, we have
1 2 c k k
E[h ]=0
k
(cid:18) (cid:19)
1 1 1
E[h2]= −
k N c c2
1
E[h h ]=− ,i̸=j
i j Nc2
c
E[(cid:89) hnk]=O(cid:0) N−2(cid:1) ,(cid:88)
n ≥3
k k
k=1 k
E[|h |]≤N−1/2c−1/2(1−1/c)1/2
j
E[|h h |]=O(N−1)
i j
(cid:16) (cid:17)
E[|h h h |]=O N−3/2
i j k
E[|h h h h |]=O(cid:0) N−2(cid:1) ,
i j k l
where i,j,k,l∈[c].
Proof Since (N ,N ,...,N )∼Multin(N,1/c), the moment-generating function of (N ,N ,...,N ) is
1 2 c 1 2 c
(cid:32) c (cid:33)N
1(cid:88)
M (t)= exp(t )
N c i
i=1
29We can compute the moment-generating function of h=(h ,h ,...,h ) as follows:
1 2 c
(cid:32) c (cid:33)  c   c N
(cid:88) 1(cid:88) 1 (cid:88)
M h(t)=exp − t i/c M N(t/N)=
c
exp
N
t i− t j/c
i=1 i=1 j=1
     2
c c c c
1 (cid:88) (cid:88) 1 (cid:88) (cid:88)
= 1+ Nc t i−c t j/c+ 2N2c  t i− t j/c  
i=1 j=1 i=1 j=1
  kN
∞ c c
(cid:88) 1 (cid:88) (cid:88)
+ k!Nkc

t i− t j/c  

k=3 i=1 j=1
   k
c ∞ c c
(cid:88) 1 (cid:88) 1 (cid:88) 1 (cid:88) (cid:88)
= 1+ 2N(1/c−1/c2)t2
i
− 2Nc2t it
j
+ k!Nkc

t i− t j/c  

i=1 i̸=j∈[c] k=3 i=1 j=1
Observing the coefficients of h, we have
E[h ]=0
k
(cid:18) (cid:19)
1 1 1
E[h2]= −
k N c c2
1
E[h h ]=− ,i̸=j
i j Nc2
c
E[(cid:89) hnk]=O(cid:0) N−2(cid:1) ,(cid:88)
n ≥3,
k k
k=1 k
where i,j,k ∈[c].
Iteratively applying the H¨older’s inequality, we have
E[|h |]≤(cid:0)E[h2](cid:1)1/2 =N−1/2c−1/2(1−1/c)1/2
j j
E[|h h |]≤(cid:0)E[h2h2](cid:1)1/2 =O(N−1)
i j i j
E[|h |3]≤E[|h |4]3/4 =(N−3/2)
i i
(cid:16) (cid:17)
E[|h h h |]≤E[|h |3]1/3E[|h |3]1/3E[|h |3]1/3 =O N−3/2
i j k i j k
E[|h h h h |]≤E[|h |4]1/4E[|h |4]1/4E[|h |4]1/4E[|h |4]1/4 =O(cid:0) N−2(cid:1)
i j k l i j k l
where i,j,k,l∈[c]. ■
Lemma F.2 Suppose g ∼ N(0,c2N Λ/N2) and (N ,N ,...,N ) ∼ Multin(N,1/c), define g¯ = Λ−1/2g and
k k 1 2 c k k
N /N =1/c+h , we have
k k
E[(g¯ ) ]=0
k i
E[(g¯ ) (g¯) ]=δ δ c/N
k i l j kl ij
E[(g¯ ) (g¯ ) (g¯ ) ]=0
k1 ii k2 i2 k3 i3
E[(g¯ )4]=E[3c2/N2(1+ch )2]=O(N−2)
k i k
E[h (g¯ ) (g¯) ]=E[c2δ δ h h /N]=O(N−2)
m k i l j kl ij m k
E[h h (g¯ ) ]=0
m l k i
30where i,j,i ,i ,i ∈[d],k,l,m,k ,k ,k ∈[c].
1 2 3 1 2 3
(cid:80) (cid:80)
For any n , n satisfying n + n =1,2,3, we have
1k 2ki k∈[c] 1k k∈[c],i∈[d] 2ki
(cid:89)
E[ hn1k(g¯ )n2ki]=O(N−1)
k k i
k∈[c],i∈[d]
Moreover, we have
E[|(g¯ ) |]≤E[(g¯ )2]1/2 =N−1/2c1/2
k i k i
E[|(g¯ ) |3]≤E[(g¯ )4]3/4 =O(N−3/2)
k i k i
where i∈[d],k ∈[c].
(cid:80) (cid:80)
For any n , n satisfying n + n =n, n=1,2,3,4, we have
1k 2ki k∈[c] 1k k∈[c],i∈[d] 2ki
(cid:89)
E[ |hn1k(g¯ )n2ki|]=O(N−n/2)
k k i
k∈[c],i∈[d]
Proof Since g ∼N(0,c2N Λ/N2) and g¯ ∼N(0,c2N I /N2)=N(0,(c/N +c2h /N)I ), we have
k k k k d k d
E[(g¯ ) ]=0
k i
E[(g¯ ) (g¯) ]=δ δ c/N
k i l j kl ij
E[(g¯ ) (g¯ ) (g¯ ) ]=0
k1 ii k2 i2 k3 i3
E[(g¯ )4]=E[3c2/N2(1+ch )2]=O(N−2)
k i k
E[h (g¯ ) (g¯) ]=E[c2δ δ h h /N]=O(N−2)
m k i l j kl ij m k
E[h h (g¯ ) ]=0
m l k i
where i,j,i ,i ,i ∈ [d],k,l,m,k ,k ,k ∈ [c]. Thus, with the results from Lemma F.1, for any n , n
1 2 3 1 2 3 1k 2ki
(cid:80) (cid:80)
satisfying n + n =1,2,3, we have
k∈[c] 1k k∈[c],i∈[d] 2ki
(cid:89)
E[ hn1k(g¯ )n2ki]=O(N−1)
k k i
k∈[c],i∈[d]
Moreover, according to the Jensen’s inequality, we have
E[|(g¯ ) |]≤E[(g¯ )2]1/2 =N−1/2c1/2
k i k i
E[|(g¯ ) |3]≤E[(g¯ )4]3/4 =O(N−3/2)
k i k i
(cid:80)
where i ∈ [d],k ∈ [c]. Thus, with the results from Lemma F.1, for any n , n satisfying n +
1k 2ki k∈[c] 1k
(cid:80)
n =n, n=1,2,3,4, we have
k∈[c],i∈[d] 2ki
(cid:89) (cid:89)
E[ |hn k1k(g¯ k)n i2ki|]≤ E[|hn k|]n1k/nE[|(g¯ k)n i|]n2ki/n =O(N−n/2).
k∈[c],i∈[d] k∈[c],i∈[d]
■
Lemma F.3 For the loss function L(W) (16), we have ∇2L(W)≻0. For any compact set R , when W ∈R ,
W W
we have ∇2L(W)≻γI for some γ >0. Additionally, L(W) has one unique global minimizer on Rd×d.
d
31For L(cid:101)(W) defined in (44), we also have ∇2L(cid:101)(W)≻0. For any compact set R W, when W ∈R W, we have
∇2L(cid:98)(W)≻γI
d
for some γ >0. Additionally, L(cid:101)(W) has one unique global minimizer on Rd×d.
Proof We vectorize W as Vec(W) ∈ Rd2, where Vec(W) = W , t (x) = ⌊(x−1)/d⌋+1,t (x) =
i t1(i),t2(i) 1 2
((x−1) mod d)+1. Then, we have
(cid:34) c (cid:35)
(∇L(W)) =E (cid:88)(cid:2) (softmax(P⊤Wq/c) −softmax(µ⊤Λ−1q) )(p ) q /c(cid:3) (45)
i k k k t1(i) t2(i)
k=1
Note that
softmax(P⊤Wq/c) =σ(a )
k k
∇softmax(P⊤Wq/c) =σ(a )(1−σ(a ))∇a ,
k k k k
(cid:80)
where a =−log( exp((p −p )Wq/c)). For ∇a , we have
k l=1,...,c,l̸=k l k k
(cid:80) exp(cid:0) (p −p )⊤Wq/c(cid:1) (p −p )q⊤/c
∇a = l=1,...,c,l̸=k l k k l
k (cid:80) exp((p −p )⊤Wq/c)
l=1,...,c,l̸=k l k
(cid:80) exp(cid:0) p⊤Wq/c(cid:1) (p −p )q⊤/c
= l=1,...,c,l̸=k l k l .
(cid:80) exp(cid:0) p⊤Wq/c(cid:1)
l=1,...,c,l̸=k l
Then we have
(cid:80) exp(cid:0) p⊤Wq(cid:1) (p −p )q⊤/c
∇softmax(P⊤Wq/c) =softmax(P⊤Wq/c) l=1,...,c,l̸=k l k l
k k (cid:80) exp(p⊤Wq/c)
n=1,...,c n
(cid:88)
= softmax(P⊤Wq/c) softmax(P⊤Wq/c) (p −p )q⊤/c
k l k l
l=1,...,c,l̸=k
and
(cid:88)
(∇softmax(P⊤Wq/c) ) = softmax(P⊤Wq/c) softmax(P⊤Wq/c) (p −p ) q /c.
k j k l k l t1(j) t2(j)
l=1,...,c,l̸=k
We can express the Hessian matrix of the loss function with the following form:
 
c
(cid:88) (cid:88)
(∇2L(W)) ij =E  softmax(P⊤Wq/c) ksoftmax(P⊤Wq/c) l(p k) t1(i)q t2(i)(p k−p l) t1(j)q t2(j)/c2 
k=1l=1,...,c,l̸=k
(cid:34) c k−1 (cid:35)
(cid:88)(cid:88)
=E softmax(P⊤Wq/c) softmax(P⊤Wq/c) (p −p ) q (p −p ) q /c2 .
k l k l t1(i) t2(i) k l t1(j) t2(j)
k=2l=1
Considering z ∈Rd2 such that z ̸=0, we have
(cid:34) c k−1 (cid:35)
1 (cid:88)(cid:88) (cid:88)
z⊤∇2L(W)z =E softmax(P⊤Wq/c) softmax(P⊤Wq/c) z z (p −p ) q (p −p ) q
c2 k l a b k l t1(a) t2(a) k l t1(b) t2(b)
k=2l=1 ab
  2
c k−1
1 (cid:88)(cid:88) (cid:88)
=E c2 softmax(P⊤Wq/c) ksoftmax(P⊤Wq/c) l z a(p k−p l) t1(a)q t2(a)  
k=2l=1 a∈[d2]
32SinceforanyP,q,k,l,softmax(P⊤Wq/c) softmax(P⊤Wq/c) >0,wehavez⊤∇2L(W)z ≥0. Thus,∇2L(W)⪰
k l
0 and L(W) is convex.
Defining p˜=p −p , we have
1 2
z⊤∇2L(W)z
  2
1 (cid:88)
≥E c2softmax(P⊤Wq/c) 1softmax(P⊤Wq/c) 2 z a(p 1−p 2) t1(a)q t2(a)  
a∈[d2]
 2
(cid:90) 1 (cid:88)
= c2softmax(P⊤Wq/c) 1softmax(P⊤Wq/c) 2 z ap˜ t1(a)q t2(a) f Pq(P,q)dPdq
a∈[d2]
where f (P,q) are the PDF function of P,q. For any z ̸= 0, we denote z = z , suppose a,b ∈
Pq ij ((i−1)d+j)
argmax |z |, we consider a set of constants {c ,c },{c ,c },i,j ∈ [d], where c = d,c = d+1,
i,j ij 1pi 2pi 1qi 2qi 1pa 2pa
c = d,c = d+1, and c = 1/16,c = 1/8,i ̸= a, c = 1/16,c = 1/8,j ≠ b. Then, for any
1qb 2qb 1pi 2pi 1qj 2qj
c ∈[c ,c ],c ∈[c ,c ], we have
pi 1pi 2pi qj 1qj 2qj
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:88) z ijc pic qj(cid:12) (cid:12)≥(cid:2) d2−2(d+1)(d−1)/8−(d−1)2/64(cid:3) max|z ij|≥d2max|z ij|/2.
(cid:12) (cid:12) ij ij
(cid:12)i,j∈[d] (cid:12)
Then, we define region Ω(a,b) = {p˜ = (cid:80) c e , q = (cid:80) c e ,c ∈ [c ,c ],c ∈ [c ,c ],∥P∥2 ≤
i pi i j qj j pi 1pi 2pi qj 1qj 2qj F
c2((cid:80) c2 +c2 )}. We have
i 2pi 2qj
 2
(cid:88)
Ωm (ai ,n b) z lp˜ t1(l)q t2(l) ≥d4m ia jx|z ij|2/4≥∥z∥2 2/4.
l∈[d2]
Defining
(cid:90)
C(Ω)= min f (P,q)dPdq,
Pq
a∈[d],b∈[d] Ω(a,b)
(cid:26) (cid:27)
1
S(Ω,W)= min min softmax(P⊤Wq/c) softmax(P⊤Wq/c) ,
a∈[d],b∈[d]Ω(a,b) c2 1 2
we have S(Ω,W)>0. Since we have f (P,q)>0 for all P,q and Ω(a,b) are non-zero measures for P,q. Thus,
Pq
we have C(Ω)>0. Then, for any z ̸=0, we have
z⊤∇2L(W)z
 2
(cid:90) 1 (cid:88)
≥ c2softmax(P⊤Wq/c) 1softmax(P⊤Wq/c) 2 z lp˜ t1(l)q t2(l) f Pq(P,q)dPdq
Ω(a,b) l∈[d2]
≥C(Ω)S(Ω,W)∥z∥2/4>0
2
Thus, we have ∇2L(W)≻0. L(W) is strictly convex.
Moreover, for any compact set R of Rd×d, for any W ∈R , we have
W W
(cid:26) (cid:27)
1
S(Ω)= min min min softmax(P⊤Wq/c) softmax(P⊤Wq/c) >0.
W∈RW a∈[d],b∈[d]Ω(a,b) c2 1 2
33Then, for any W ∈R , for any z ̸=0, we have
W
z⊤∇2L(W)z
 2
(cid:90) 1 (cid:88)
≥ c2softmax(P⊤Wq/c) 1softmax(P⊤Wq/c) 2 z lp˜ t1(l)q t2(l) f Pq(P,q)dPdq
Ω(a,b) l∈[d2]
≥C(Ω)S(Ω)∥z∥2/4.
2
Thus, when W ∈R , R is a compact set, we have ∇2L(W)≻C(Ω)S(Ω)I /4, our loss function is γ−strongly
W W d
convex, where γ =C(Ω)S(Ω)/4.
Because our loss function is strictly convex in Rd×d, it has at most one global minimizer in Rd×d. Next, we
prove all level sets of our loss function are compact, i.e. V ={W ∈Rd×d|L(W)≤α} is compact for all α. We
α
prove it by contradiction. Suppose V is not compact for some α. Since our loss function is continuous and
α
convex, V is an unbounded convex set. Since the dimension of V is d2, consider a point Wα ∈V , there must
α α α
exists a Wk ̸=0 that {Wα+tWk|t=[0,∞)}∈V . For this Wk ̸=0 , there must exist a set of constants
d×d α d×d
0<c <c ,0<c <c such that for any c ∈[c ,c ],c ∈[c ,c ], we have
3pi 4pi 3qj 4qj pi 3pi 4pi qj 3qj 4qj
(cid:88)
| c c Wk|≠ 0.
pi qj ij
ij
Thus, we have
(cid:88)
lim | c c (Wα+tWk)|=∞.
pi qj ij ij
t→∞
ij
WedefineΩ ={p˜=(cid:80) c e ,q =(cid:80) c e ,c ∈[c ,c ],c ∈[c ,c ],∥P∥2 ≤c2((cid:80) c2 +c2 ),∥µ∥2 ≤
0 i pi i j qj j pi 3pi 4pi qj 3qj 4qj F i 4pi 4qj F
c2((cid:80) c2 +c2 )}. Then, defining
i 4pi 4qj
(cid:90)
C(Ω )= f (P,q)dPdq,
0 Pq
Ω0
S(Ω )=min(cid:8) min{softmax(µ⊤Wq/c) ,softmax(µ⊤Wq/c) }(cid:9)
0 1 2
Ω0
34we have S(Ω )>0. Since Ω are non-zero measures for P,q, we have C(Ω )>0. Then, we have
0 0 0
lim L(Wα+tWk)
t→∞
c
(cid:88)
= lim E[ −softmax(µ⊤Λ−1q) log(softmax(P⊤(Wα+tWk)q/c) )]
l l
t→∞
l=1
(cid:90)
≥ lim [−softmax(µ⊤Λ−1q) log(softmax(P⊤(Wα+tWk)q/c) )]f (P,q)dPdq
1 1 Pq
t→∞
Ω0
(cid:90)
+ lim [−softmax(µ⊤Λ−1q) log(softmax(P⊤(Wα+tWk)q/c) )]f (P,q)dPdq
2 2 Pq
t→∞
Ω0
(cid:90)
≥ lim [−softmax(µ⊤Λ−1q) log(σ(p˜⊤(Wα+tWk)q/c))]f (P,q)dPdq
1 Pq
t→∞
Ω0
(cid:90)
+ lim [−softmax(µ⊤Λ−1q) log(σ(−p˜⊤(Wα+tWk)q/c))]f (P,q)dPdq
2 Pq
t→∞
Ω0
 
 (cid:88) 
≥C(Ω )S(Ω )·min lim[−log(σ( c c (Wα+tWk)/c))]
0 0 pi qj ij ij
Ω0 t→∞
ij

 
 (cid:88) 
+C(Ω )S(Ω )·min lim[−log(σ(− c c (Wα+tWk)/c))]
0 0 pi qj ij ij
Ω0 t→∞
ij

=∞
This contradicts the assumption L(Wα+tWk)≤α. Thus, all level sets of the loss function L(W) are compact,
which means there exists a a global minimizer for L(W). Together with the fact that L(W) is strictly convex,
L(W) has one unique a global minimizer on Rd×d.
Similarly, we can prove the same conclusions for L(cid:101)(W). ■
Lemma F.4 Denoting the global minimizer of our loss function (16) as W∗, we have W∗ =c(Λ−1+G), where
∥G∥ =O(N−1/4).
max
Proof Let a = µ⊤Λ−1q, s = µ⊤Wq/c, r = (µ +g)⊤Wq/c, a = µ⊤Λ−1q, s = µ⊤Wq/c, r = (ch µ +
h k k k k k k k
g )⊤Wq/c. Performing the Taylor expansion on (16), we have
k
(cid:34) c (cid:35)
(cid:88)
L(W)=E −ζ (a)log(ζ (s+r))
k k
k=1
 
c c
(cid:88) (cid:88)
=E  −ζ k(a)log(ζ k(s))− ζ k(a)R kl(s,r)r l
k=1 k,l=1
 
c
(cid:88)
=L(cid:101)(W)−E  ζ k(a)R kl(s,r)r l
k,l=1
35where |R (s,r)|≤sup |∂log(ζk(y))|sup | 1 ∂ζk(y)|=sup |δ −ζ (y)|≤1. Thus, we have
kl y ∂yl y ζk(y) ∂yl y kl l
(cid:12) (cid:12) (cid:88)c
(cid:12) (cid:12)L(cid:101)(W)−L(W)(cid:12) (cid:12)≤c E[|r l|]
l=1
c
≤(cid:88) cE(cid:2)
|h
µ⊤Wq|(cid:3) +E(cid:2) |g⊤Wq|(cid:3)
l l l
l=1
≤O(1)∥W∥ E[|h |]+O(1)∥W∥ E[|(g¯) |]
max l max l i
≤C ∥W∥ N−1/2
l max
where the last inequality is due to Lemma F.1, F.2. C is a constant independent of N and W. This shows that
l
L(W) point wisely converge to L(cid:101)(W).
According to Lemma D.2, L(cid:101)(W) has one unique global minimizer. Considering the equation:
(cid:34) c (cid:35)
(cid:88)
∇L(cid:101)(W)=E −softmax(µ⊤Λ−1q) klog(softmax(µ⊤Wq/c) k) =0
k=1
We can easily find that ∇L(cid:101)(cΛ−1)=0 and W =cΛ−1 is the global minimizer of L(cid:101)(W).
Considering a compact set R = {W |∥W −2Λ−1∥ ≤ ρ }, we have ∥W∥ ≤ C for any W ∈ R .
W F W max W W
Here ρ ,C are some positive finite constants. Then, we have
W W
(cid:12) (cid:12)
(cid:12) (cid:12)L(cid:101)(W)−L(W)(cid:12) (cid:12)≤C l′N−1/2, W ∈R
W
where C′ =C C is a constant independent of N and W. This shows that, for any W ∈R , L(W) uniformly
l l W W
converge to L(cid:101)(W).
DenoteW∗astheglobalminimizerofL(W)withpromptlengthN. Then,weshowthat,whenN issufficiently
large, W∗ ∈ R W. We first denote ∂R
W
= {W |∥W −cΛ−1∥
F
= ρ W} , ∆ = min
W∈∂RW
L(cid:101)(W)−L(cid:101)(cΛ−1) > 0.
Then, for N ≥(4C′/∆)2, and for any W ∈R , we have
l W
(cid:12) (cid:12)
(cid:12)L(cid:101)(W)−L(W)(cid:12)≤∆/4
(cid:12) (cid:12)
min L(W)− min L(W)≥ min L(W)−L(cΛ−1)≥∆/2>0
W∈∂RW W∈RW W∈∂RW
Since L(W) is strictly convex, we have W∗ =argmin L(W)∈R .
W W
Then, we have
|L(cid:101)(W∗)−L(W∗)|≤C′/N
l
|L(cid:101)(cΛ−1)−L(cΛ−1)|≤C′/N
l
L(cid:101)(W∗)≤L(W∗)+C′/N ≤L(cΛ−1)+C′/N ≤L(cid:101)(cΛ−1)+2C′N−1/2
l l l
According to Lemma D.2, for W ∈R W, we have ∇2L(cid:101)(W)≻γI d, where γ is a positive constant independent of
N. Thus, L(cid:101)(W) is γ-strongly convex in R W. According to Lemma C.1, we have
2 4C′
∥W∗−cΛ−1∥2 ≤ (L(cid:101)(W∗)−L(cid:101)(cΛ−1))≤ l
F γ γN1/2
36Thus, when N →∞, we have W∗ →cΛ−1. Denoting W∗ =c(Λ−1+G), we have ∥G∥ =O(N−1/4). ■
max
Lemma F.5 The global minimizer of the loss function (16) is W∗ =c(Λ−1+G). We have
∥G∥ ≤
1 (cid:13) (cid:13) (cid:13)S−1E(cid:34) (cid:88)c ∂ζ k(a)
(cδ −1)µ
µ⊤Λ−1qq⊤+(cid:88)c ∂ζ k(a)
cqq⊤
max N(cid:13) ∂a kl k l ∂a
(cid:13) l k
k,l=1 k=1
+
(cid:88)c ∂2ζ k(a)
(cδ −1)µ⊤Λ−1qµ⊤Λ−1qµ q⊤/2+
(cid:88)c ∂2ζ k(a)
cq⊤Λ−1qµ
q⊤/2(cid:35)(cid:13)
(cid:13)
(cid:13)
∂a ∂a ln l n k ∂a2 k (cid:13)
k,l,n=1 l n k,l=1 l (cid:13) max
+o(1/N),
where a=µ⊤Λ−1q, a
k
=µ⊤ kΛ−1q, S =c2∇2L(cid:101)(cΛ−1). Ignoring constants other than c,N, we have ∥G∥
max
≤
O(c/N).
Proof According to Lemma F.3, the loss function L(W) has a unique global minimizer W∗. We have
(cid:34) c (cid:35)
∇L(W∗)=E (cid:88)(cid:2) (ζ (P⊤W∗q/c)−ζ (µ⊤Λ−1q))p q⊤/c(cid:3) =0. (46)
k k k
k=1
Let W∗ = c(Λ−1 + G), a = µ⊤Λ−1q, a = µ⊤Λ−1q, b = (µ + cµ + g)⊤Gq + (cµ + g)⊤Λ−1q, b =
k k h h k
(µ +ch µ +g )⊤Gq+(ch µ +g )⊤Λ−1q. The Taylor expansion of ζ (a+b) at point a is
k k k k k k k k
ζ (a+b)=ζ
(a)+(cid:88)c ∂ζ k(a)
b +
(cid:88)c ∂2ζ k(a)
b b /2!+
(cid:88)c
R (a,b)b b b /3!,
k k ∂a l ∂a ∂a l n klnm l n m
l l n
l=1 l,n=1 l,n,m=1
where |R (a,b)|≤sup |
∂3ζk(x)
|. Thus, our equation (46) become
klnm x ∂xl∂xn∂xm
 
E
(cid:88)c ∂ζ
∂k
a(a)
b lp kq⊤+
(cid:88)c ∂∂ a2ζ ∂( aa)
b lb np kq⊤/2!+
(cid:88)c
R klnm(a,b)b lb nb mp kq⊤/3!=0. (47)
l l n
k,l=1 k,l,n=1 k,l,n,m=1
For the first term (cid:80)c ∂ζk(a)b p q⊤, according to Lemma F.1, we have
k,l=1 ∂al l k
 
c
E (cid:88) ∂ζ ∂k a(a) b lp kq⊤ 
l
k,l=1
 
c
=E (cid:88) ∂ζ ∂k a(a)(cid:2) µ⊤ l Gqµ kq⊤+c2h lh kµ⊤ l Gqµ kq⊤+c2h lh kµ⊤ l Λ−1qµ kq⊤+g l⊤Λ−1qg kq⊤+g l⊤Gqg kq⊤(cid:3) 
l
k,l=1
(cid:34) c
=E (cid:88) ∂ζ k(a)(cid:0) µ µ⊤Gqq⊤+(cδ −1)µ µ⊤Gqq⊤/N +(cδ −1)µ µ⊤Λ−1qq⊤/N(cid:1)
∂a k l kl k l kl k l
l
k,l=1
c (cid:35)
+(cid:88)∂ζ k(a)(cid:0) cqq⊤/N +cΛGqq⊤/N(cid:1) . (48)
∂a
k
k=1
37According to Lemma F.4, O(∥G∥ )=O(N−1/4)=o(1), we have
max
(cid:13) (cid:13) (cid:13)E(cid:34) (cid:88)c ∂ζ k(a)
(cδ −1)µ µ⊤Gqq⊤/N
+(cid:88)c ∂ζ k(a) cΛGqq⊤/N(cid:35)(cid:13) (cid:13)
(cid:13)
(cid:13) ∂a kl k l ∂a (cid:13)
(cid:13) l k (cid:13)
k,l=1 k=1 max
≤O(∥G∥ /N)=o(1/N) (49)
max
For the second term (cid:80)c ∂2ζk(a)b b p q⊤/2!, we have
k,l,n=1 ∂al∂an l n k
 
E 
(cid:88)c ∂ ∂2 aζ
k
∂( aa)
b lb np kq⊤/2!
l n
k,l,n=1
=1 E(cid:34) (cid:88)c ∂2ζ k(a)(cid:32) (cid:88)
ϕ⊤Gqϕ⊤Gqp q⊤
2 ∂a ∂a 1 2 k
l n
k,l,n=1 ϕ1∈{µl,chlµl,gl},ϕ2∈{µn,chnµn,gn}
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
(cid:88)
+ 2ϕ⊤Gqϕ⊤Λ−1qp q⊤
1 2 k
ϕ1∈{µl,chlµl,gl},ϕ2∈{chnµn,gn}
(cid:124) (cid:123)(cid:122) (cid:125)
(ii)
(cid:33)(cid:35)
(cid:88)
+ ϕ⊤Λ−1qϕ⊤Λ−1qp q⊤ .
1 2 k
ϕ1∈{chlµl,gl},ϕ2∈{chnµn,gn}
(cid:124) (cid:123)(cid:122) (cid:125)
(iii)
For terms (i) having two G, their max norms are at most smaller than O(∥G∥2 ). For terms (ii) having
max
one G, define g¯ = Λ−1/2g , these terms must contain n number of h and n number of (g¯ ) , we have
l l 1j j 2ji j i
(cid:80)
n +n =n ,n =1,2,3. According to Lemma F.2, we know that for n =1,2,3,
j∈[c],i∈[d] 1j 2ji t t t
E[ (cid:89) hn1j(g¯ )n2ji]=O(N−1)
j j i
j∈[c],i∈[d]
Thus, the max norm of expectations of terms in (ii) are at most smaller than O(∥G∥ N−1). Therefore, for
max
terms (i),(ii), we have
∥E[(i)]∥ ≤O(∥G∥2 )=o(∥G∥ ) (50)
max max max
∥E[(ii)]∥ ≤O(∥G∥ /N)=o(1/N) (51)
max max
For terms (iii) without G, we have
∥E[(iii)]∥
max
=(cid:13) (cid:13) (cid:13)E(cid:34) (cid:88)c ∂2ζ k(a)
c2h h µ⊤Λ−1qµ⊤Λ−1qµ q⊤/2+
(cid:88)c ∂2ζ k(a)
g⊤Λ−1qg⊤Λ−1qµ q⊤/2
(cid:13) ∂a ∂a l n l n k ∂a2 l l k
(cid:13) k,l,n=1 l n k,l=1 l
+
(cid:88)c ∂2ζ k(a)
ch µ⊤Λ−1qg⊤Λ−1qg q⊤+
(cid:88)c ∂2ζ k(a)
c3h h h µ⊤Λ−1qµ⊤Λ−1qµ
q⊤/2(cid:35)(cid:13)
(cid:13)
(cid:13)
∂a ∂a l l k k ∂a ∂a l n k l n k (cid:13)
l k l n (cid:13)
k,l=1 k,l,n=1 max
(cid:13)  (cid:13)
≤
21 N(cid:13)
(cid:13) (cid:13)E 
(cid:88)c ∂ ∂2 aζ
k
∂( aa)
(cδ ln−1)µ⊤ l Λ−1qµ⊤ nΛ−1qµ kq⊤+
(cid:88)c ∂2 ∂ζ ak( 2a)
cq⊤Λ−1qµ kq⊤
(cid:13)
(cid:13) (cid:13)
(cid:13) k,l,n=1 l n k,l=1 l (cid:13) max
+O(1/N2) (52)
38where the last inequity is due to Lemma F.1, F.2.
For the third term (cid:80)c R (a,b)b b b p q⊤/3!, we have
k,l,n,m=1 klnm l n m k
(cid:13)  (cid:13)
(cid:13) (cid:88)c (cid:13)
(cid:13) (cid:13)E  R klnm(a,b)b lb nb mp kq⊤/3!(cid:13) (cid:13)
(cid:13) (cid:13)
k,l,n,m=1 max
(cid:88)
≤O(1) max E[ |b b b (p ) q |]
l,m∈[d]
k1 k2 k3 k4 l m
k1,k2,k3,k4∈[c]
(cid:34)
(cid:88) (cid:88)
≤O(1)E ϕ⊤Gqϕ⊤Gqϕ⊤Gq(p ) q
1 2 3 k4 l m
k1,k2,k3,k4∈[c] ϕ1∈{µk1,chk1µk1,gk1},ϕ2∈{µk2,chk2µk2,gk2},ϕ3∈{µk3,chk3µk3,gk3}
(cid:124) (cid:123)(cid:122) (cid:125)
(∗)
(cid:88)
+ ϕ⊤Gqϕ⊤Gqϕ⊤Λ−1q(p ) q
1 2 3 k4 l m
ϕ1∈{µk1,chk1µk1,gk1},ϕ2∈{µk2,chk2µk2,gk2},ϕ3∈{chk3µk3,gk3}
(cid:124) (cid:123)(cid:122) (cid:125)
(∗)
(cid:88)
+ ϕ⊤Gqϕ⊤Λ−1qϕ⊤Λ−1q(p ) q
1 2 3 k4 l m
ϕ1∈{µk1,chk1µk1,gk1},ϕ2∈{chk2µk2,gk2},ϕ3∈{chk3µk3,gk3}
(cid:124) (cid:123)(cid:122) (cid:125)
(∗∗)
(cid:35)
(cid:88)
+ ϕ⊤Λ−1qϕ⊤Λ−1qϕ⊤Λ−1q(p ) q .
1 2 3 k4 l m
ϕ1∈{chk1µk1,gk1},ϕ2∈{chk2µk2,gk2},ϕ3∈{chk3µk3,gk3}
(cid:124) (cid:123)(cid:122) (cid:125)
(∗∗∗)
Fortermsin(∗)havingtwoorthreeG,theseterms’expectedabsolutevaluesareatmostsmallerthanO(∥G∥2 ).
max
For terms in (∗∗) having one G, these terms must contain n number of h and n number of (g¯ ) , we have
1j j 2ji j i
(cid:80)
n +n =n ,n =2,3,4. According to Lemma F.2, for n =2,3,4, we have
j∈[c],i∈[d] 1j 2ji t t t
E[ (cid:89) |hn1k(g¯ )n2ji|]=O(N−nt/2)=O(N−1)
k j i
j∈[c],i∈[d]
Thus, these term’s expected absolute values are at most smaller than O(∥G∥ N−1). For terms in (∗∗∗)
max
(cid:80)
withoutG, thesetermsmustcontainn numberofh andn numberof(g¯ ) , wehave n +n =
1j j 2ji j i j∈[c],i∈[d] 1j 2ji
n ,n =3,4. According to Lemma F.2, for n =3,4, we have
t t t
E[ (cid:89) |hn1k(g¯ )n2ji|]=O(N−nt/2)=O(N−3/2)
k j i
j∈[c],i∈[d]
Thus, these term’s expected absolute values are at most smaller than O(N−3/2). Therefore, we have
(cid:13)  (cid:13)
(cid:13) (cid:88)c (cid:13)
(cid:13) (cid:13)E  R klnm(a,b)b lb nb mp kq⊤/3!(cid:13) (cid:13)
(cid:13) (cid:13)
k,l,n,m=1 max
(cid:88)
≤O(1) max E[ |b b b (p ) q |]
l,m∈[d]
k1 k2 k3 k4 l m
k1,k2,k3,k4∈[c]
≤O(∥G∥2 )+O(∥G∥ N−1)+O(N−3/2)
max max
≤o(∥G∥ )+o(1/N). (53)
max
39Moreover, we have
(cid:40)  c (cid:41)
E (cid:88) ∂ζ ∂k a(a) µ kµ⊤ l Gqq⊤ 
l
k,l=1 ij
(cid:40)  c c (cid:41)
(cid:88) (cid:88)
= E  ζ k(a)(1−ζ k(a))µ kµ⊤ kGqq⊤− ζ k(a)ζ l(a)µ kµ⊤ l Gqq⊤ 
k=1 k,l=1,k̸=l ij
(cid:40)  c (cid:41)
(cid:88)
= E  ζ k(a)ζ l(a)µ k(µ k−µ l)⊤Gqq⊤ 
k,l=1,k̸=l ij
(cid:40) (cid:34) c k−1 (cid:35)(cid:41)
(cid:88)(cid:88)
= E ζ (a)ζ (a)(µ −µ )(µ −µ )⊤Gqq⊤
k l k l k l
k=2l=1 ij
d
(cid:88)
= s G , (54)
ijnm nm
n,m=1
(cid:104) (cid:105)
wheres =E (cid:80)c (cid:80)k−1ζ (a)ζ (a)(µ −µ ) (µ −µ ) q q . WevectorizeGasVec(G) =G . De-
ijnm k=2 l=1 k l k l i k l n m j i t1(i),t2(i)
(cid:104) (cid:105)
fineS ∈Rd2×d2,whereS =s =E (cid:80)c (cid:80)k−1ζ (a)ζ (a)(µ −µ ) q (µ −µ ) q ,
ij t1(i),t2(i),t1(j),t2(j) k=2 l=1 k l k l t1(i) t2(i) k l t1(j) t2(j)
(54) can be expressed as
 
c
E (cid:88) ∂ζ ∂k a(a) µ kµ⊤
l
Gqq⊤ =SG. (55)
l
k,l=1
Note that S =c2∇2L(cid:101)(cΛ−1). According to Lemma F.3, S is positive definite. Thus, combining (47), (48), (49),
(50), (51), (52), (53), (55), we have
∥G∥ ≤
1 (cid:13) (cid:13) (cid:13)S−1E(cid:34) (cid:88)c ∂ζ k(a)
(cδ −1)µ
µ⊤Λ−1qq⊤+(cid:88)c ∂ζ k(a)
cqq⊤
max N(cid:13) ∂a kl k l ∂a
(cid:13) l k
k,l=1 k=1
+
(cid:88)c ∂2ζ k(a)
(cδ −1)µ⊤Λ−1qµ⊤Λ−1qµ q⊤/2+
(cid:88)c ∂2ζ k(a)
cq⊤Λ−1qµ
q⊤/2(cid:35)(cid:13)
(cid:13)
(cid:13)
∂a ∂a ln l n k ∂a2 k (cid:13)
k,l,n=1 l n k,l=1 l (cid:13) max
+o(1/N).
Ignoring constants other than c,N, we have ∥G∥ ≤O(c/N). ■
max
Lemma F.6 The loss function (7) is l-smooth, where l≤ 1 (cid:80)c (cid:80)k−1(cid:80) E[((p −p ) q )2].
c2 k=2 l=1 i∈[d2] k l t1(i) t2(i)
Proof The Hessian matrix of the loss function is
(cid:34) c k−1 (cid:35)
(cid:88)(cid:88)
(∇2L(W)) =E softmax(P⊤Wq/c) softmax(P⊤Wq/c) (p −p ) q (p −p ) q /c2 .
ij k l k l t1(i) t2(i) k l t1(j) t2(j)
k=2l=1
40Considering z ∈Rd2 such that z ̸=0, we have
  2
c k−1
1 (cid:88)(cid:88) (cid:88)
z⊤∇2L(W)z =E c2 softmax(P⊤Wq/c) ksoftmax(P⊤Wq/c) l z a(p k−p l) t1(a)q t2(a)  
k=2l=1 a∈[d2]
c k−1
(a) 1 (cid:88)(cid:88) (cid:88)
≤ ∥z∥2 E[((p −p ) q )2]
c2 2 k l t1(i) t2(i)
k=2l=1i∈[d2]
where (a) is due to the Cauchy–Schwarz inequality. Thus, ∇2L(W)⪯lI and L(W) is l-smooth, where l is a
d
constant smaller than 1 (cid:80)c (cid:80)k−1(cid:80) E[((p −p ) q )2]. ■
c2 k=2 l=1 i∈[d2] k l t1(i) t2(i)
Theorem F.1 (Formal statement of Theorem 4.1) The following statements hold.
(1) OptimizingtraininglossL(W)(16)withtrainingpromptlengthN viagradientdescentWt+1 =Wt−η∇L(Wt),
we have for any t
∥Wt−W∗∥2 ≤exp(−t/κ)∥W0−W∗∥2,
F F
where W0 is the initial parameter and W∗ is the global minimizer of L(W), κ=l/α. α,l are constants such
that
0<α≤λ (∇2L(W))≤λ (∇2L(W))≤l, for all W ∈R , (56)
min max W
where R ={W ∈Rd×d|∥W −W∗∥ ≤∥W0−W∗∥ }.
W F F
(2) Denoting W∗ =c(Λ−1+G), we have
∥G∥ ≤
1 (cid:13) (cid:13) (cid:13)S−1E(cid:34) (cid:88)c ∂ζ k(a)
(cδ −1)µ
µ⊤Λ−1qq⊤+(cid:88)c ∂ζ k(a)
cqq⊤
max N(cid:13) ∂a kl k l ∂a
(cid:13) l k
k,l=1 k=1
+
1 (cid:88)c ∂2ζ k(a)
(cδ −1)µ⊤Λ−1qµ⊤Λ−1qµ q⊤+
1 (cid:88)c ∂2ζ k(a)
cq⊤Λ−1qµ
q⊤(cid:35)(cid:13)
(cid:13)
(cid:13)
2 ∂a ∂a ln l n k 2 ∂a2 k (cid:13)
k,l,n=1 l n k,l=1 l (cid:13) max
+o(1/N)
=O(c/N)
where S = c2∇2L(cid:101)(2Λ−1), L(cid:101)(2Λ−1) = lim N→∞L(2Λ−1). The expectation is taken over µ
τ
∼ P Ωm(Λ),
x ∼Pm(µ ,Λ).
τ,query x τ
(3) After T ≥2κlog(N ·∥W0−W∗∥ F) gradient steps, denoting W(cid:99) as the final model, we have
W(cid:99) =c(Λ−1+G(cid:98)), (57)
where ∥G(cid:98)∥
max
=O(c/N).
41F.3 Proof of Theorem 4.1
Proof According to Lemma F.5, the global minimizer of L(W) is W∗ =c(Λ−1+G), where
∥G∥ ≤
1 (cid:13) (cid:13) (cid:13)S−1E(cid:34) (cid:88)c ∂ζ k(a)
(cδ −1)µ
µ⊤Λ−1qq⊤+(cid:88)c ∂ζ k(a)
cqq⊤
max N(cid:13) ∂a kl k l ∂a
(cid:13) l k
k,l=1 k=1
+
(cid:88)c ∂2ζ k(a)
(cδ −1)µ⊤Λ−1qµ⊤Λ−1qµ q⊤/2+
(cid:88)c ∂2ζ k(a)
cq⊤Λ−1qµ
q⊤/2(cid:35)(cid:13)
(cid:13)
(cid:13)
∂a ∂a ln l n k ∂a2 k (cid:13)
k,l,n=1 l n k,l=1 l (cid:13) max
+o(1/N).
Ignoring constants other than c,N, we have ∥G∥ ≤O(c/N).
max
Define R = {W ∈ Rd×d|∥W −W∗∥ ≤ ∥W0−W∗∥ }, and R is a compact set. Then, according to
W F F W
Lemma F.3, for W ∈R , we have ∇2L(W)⪰αI . Here α>0 is a positive constant number. Thus, L(W) is
W d
α-strongly convex in R . Moreover, according to Lemma F.6, L(W) is l-smooth. Then according to Lemma
W
C.2, applying gradient descent with η =1/l, for any t≥1, we have
∥Wt−W∗∥2 ≤exp(−t/κ)·∥W0−W∗∥2,
F F
where κ=l/α.
AfterT ≥2κlog(N·∥W0−W∗∥ F)gradientsteps,wehaveW(cid:99) =WT =c(Λ−1+G+HT/c)=2(Λ−1+G(cid:98)),where
G(cid:98) =G+HT/c, ∥HT∥
max
≤exp(−T/κ)·∥W0−W∗∥2
F
≤1/N. Thus, ∥G(cid:98)∥
max
≤∥G∥ max+∥HT∥
max
=O(c/N).
■
G In-context inference of multi-class classification
G.1 Notations
In this section, we use the following notations. We denote µ = (µ ,µ ,...,µ ), q = x . Define p =
1 2 c query k
c (cid:80)M (y ) x , and define P = (p ,p ,...,p ) ∈ Rd×c. We have P⊤ = c (cid:80)M y x⊤ ∈ Rc×d. Since with
M i=1 i k i 1 2 c M i=1 i τ,i
probability P(y =e ) = 1/c, x = µ + v , where v ∼ N(0,Λ), we have p = c (cid:80)M (y ) x =
τ,i k τ,i k i i k M i=1 τ,i k τ,i
cM µ /M+g , where g = c (cid:80) v , g ∼N(0,c2M Λ/M2) and(M ,M ,...,M )∼Multin(M,1/c).
k k k k M i∈{i|yτ,i=ek} i k k 1 2 c
Defining h =M /M −1/c, we have M /M =1/c+h and p =µ +ch µ +g .
k k k k k k k k k
Theorem G.1 (Formal statement of Theorem 4.2) Let y be the prediction of the trained transformer
(cid:98)query
with parameters W(cid:99) in (19) and P
test
satisfying Assumption 4.2, and let y
query
∼ P ym |xquery(µ,Λ). Then, for the
inference error defined in (3), we have
E[∆(y ,y )]
query (cid:98)query
   
≤m k∈a [cx ] (cid:88)c ∂ζ ∂k a( la) ∥G(cid:98)∥
max
(cid:88) |(µ l) iq j|+ M1
1/2
(cid:112) c(1−1/c)|µ⊤
l
Λ−1q|+√ c (cid:88) |Λ− ij1/2q j|

l=1 i,j∈[d] i,j∈[d]
(cid:18) (cid:19)
1 1
+o + √ ,
N M
where a=µ⊤Λ−1q, a =µ⊤Λ−1q. The expectation is taken over {x ,y }M i. ∼i.d. Pm(µ,Λ).
k k i i i=1
42G.2 Proof of Theorem 4.2
Proof The output of the trained transformer is
(cid:32)(cid:32) M (cid:33) (cid:33)
c (cid:88)
y
(cid:98)out
=softmax
M
y ix⊤
i
(Λ−1+G(cid:98))x
query
=softmax(P⊤(Λ−1+G(cid:98))q) (58)
i=1
The probability of y =e given x is
query k query
P(y =e |x )=softmax(µ⊤Λ−1x ) =softmax(µ⊤Λ−1q)
query k query query k k
Defining a = µ⊤Λ−1q, b = (µ+µ h+g)⊤G(cid:98)q+(µ h+g)⊤Λ−1q, a
k
= µ⊤ kΛ−1q, b
k
= (µ
k
+ch kµ
k
+g k)⊤G(cid:98)q+
(ch µ +g )⊤Λ−1q, we have
k k k
c c
E(cid:104) softmax(P⊤(Λ−1+G(cid:98))q) k(cid:105) =E[ζ k(a+b)]=E[ζ k(a)+(cid:88)∂ζ ∂k a(a) b l+ (cid:88) R kln(a,b)b lb n/2]
l
l=1 l,n=1
where |R (a,b)|≤sup
|∂2ζk(x)|.
Thus, we have
kln x ∂xl∂xn
(cid:34) c (cid:12) (cid:12)(cid:35) (cid:12) (cid:12) c (cid:12) (cid:12)
E[|ζ k(a+b)−ζ k(a)|]≤E (cid:88)(cid:12) (cid:12) (cid:12)∂ζ ∂k a(a) b l(cid:12) (cid:12)
(cid:12)
+E (cid:12) (cid:12) (cid:12)(cid:88) R kln(a,b)b lb n/2(cid:12) (cid:12) (cid:12).
l=1 l (cid:12)l,n=1 (cid:12)
(cid:104) (cid:12) (cid:12)(cid:105)
We first consider the term E (cid:80)c (cid:12)∂ζk(a)b (cid:12) . Defining g¯ =Λ−1/2g , we have
l=1(cid:12) ∂al l(cid:12) l l
(cid:34) c (cid:12) (cid:12)(cid:35)
E (cid:88)(cid:12) (cid:12) (cid:12)∂ζ ∂k a(a) b l(cid:12) (cid:12)
(cid:12)
l
l=1
c
≤(cid:88)∂ζ
∂k
a(a)(cid:16)
|µ⊤
l
G(cid:98)q|+E[|ch lµ⊤
l
G(cid:98)q|]+E[|g l⊤G(cid:98)q|]+E[|ch lµ⊤
l
Λ−1q|]+E[|g
l⊤Λ−1q|](cid:17)
l
l=1
c (cid:32) (cid:112) (cid:112) (cid:33)
( ≤a)(cid:88)∂ζ k(a)
|µ⊤G(cid:98)q|+
c(1−1/c)
|µ⊤G(cid:98)q|+E[|g¯⊤Λ1/2G(cid:98)q|]+
c(1−1/c)
|µ⊤Λ−1q|+E[|g¯⊤Λ−1/2q|]
∂a l M1/2 l l M1/2 l l
l
l=1
  
( ≤b)(cid:88)c ∂ζ ∂k a(a) ∥G(cid:98)∥
max
(cid:88) |(µ l) iq j|+ M1
1/2
(cid:112) c(1−1/c)|µ⊤
l
Λ−1q|+√ c (cid:88) |Λ− ij1/2q j|
l
l=1 i,j∈[d] i,j∈[d]
(cid:18) (cid:19)
1 1
+o + √ ,
N M
where (a) is due to Lemma F.1 that E[|h|]≤M−1/2c−1/2(1−1/c)1/2. (b) is because that g¯ ∼N(0,c2M I /M2),
l l d
E[|(g¯) |]≤E[(g¯)2]1/2 =(c/M)1/2, for l∈[c],i∈[d].
l i l i
43(cid:104)(cid:12) (cid:12)(cid:105)
For E (cid:12)(cid:80)c R (a,b)b b /2(cid:12) , we have
(cid:12) l,n=1 kln l n (cid:12)
E (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:88)c R kln(a,b)b lb n/2(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) =O(1)E(cid:34) (cid:88)c (cid:32) (cid:88) (cid:12) (cid:12) (cid:12)ϕ⊤ 1G(cid:98)qϕ⊤ 2G(cid:98)q(cid:12) (cid:12) (cid:12)
(cid:12)l,n=1 (cid:12) l,n=1 ϕ1∈{µl,chlµl,gl},ϕ2∈{µn,chnµn,gn}
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
(cid:33)(cid:35)
+ (cid:88) (cid:12) (cid:12) (cid:12)2ϕ⊤ 1G(cid:98)qϕ⊤ 2Λ−1q(cid:12) (cid:12) (cid:12)+ (cid:88) (cid:12) (cid:12)ϕ⊤ 1Λ−1qϕ⊤ 2Λ−1q(cid:12) (cid:12) .
ϕ1∈{µl,chlµl,gl},ϕ2∈{chnµn,gn} ϕ1∈{chlµl,gl},ϕ2∈{chnµn,gn}
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(ii) (iii)
For terms (i) having two G(cid:98), they are at most smaller than O(∥G(cid:98)∥2 )=O(1/N2). For terms (ii) having one G,
max
(cid:80)
thesetermsmustcontainn numberofh andn numberof(g¯ ) ,wehave n +n =n ,n =1,2.
1j j 2ji j i j∈[c],i∈[d] 1j 2ji t t
According to Lemma F.2, we know that for n =1,2,
t
E[ (cid:89) (cid:12) (cid:12)hn j1j(g¯ j)n i2ji(cid:12) (cid:12)]=O(M−1/2).
j∈[c],i∈[d]
√
Thus, terms in (ii) are at most smaller than O(∥G∥ M−1/2) = O(1/(N M)). For terms (iii) without G,
max
(cid:80)
these terms must contain n number of h and n number of (g¯ ) , we have n +n =n ,n =2.
1j j 2ji j i j∈[c],i∈[d] 1j 2ji t t
According to Lemma F.2, for n =2, we have
t
E[ (cid:89) |hn1k(g¯ )n2ji|]=O(M−nt/2)=O(M−1).
k j i
j∈[c],i∈[d]
(cid:104)(cid:12) (cid:12)(cid:105) √
Thus,thesetermareO(M−1). Therefore,wehaveE (cid:12)(cid:80)c R (a,b)b b /2(cid:12) =O(1/N2+1/M+1/(N M))=
√ (cid:12) l,n=1 kln l n (cid:12)
o(1/N +1/ M).
Finally, we have
E[∆(y ,y )]=max{E[|softmax(a+b) −softmax(a) |]}
query (cid:98)query k k
k
   
≤m k∈a [cx ] (cid:88)c ∂ζ ∂k a( la) ∥G(cid:98)∥
max
(cid:88) |(µ l) iq j|+ M1
1/2
(cid:112) c(1−1/c)|µ⊤
l
Λ−1q|+√ c (cid:88) |Λ− ij1/2q j|

l=1 i,j∈[d] i,j∈[d]
(cid:18) (cid:19)
1 1
+o + √ .
N M
■
Remark G.1 We note that Theorem 4.2 requires Assumption 4.2 to hold. For example, we need the covariance
Λ in training and testing to be the same. A similar consistency requirement of the covariance Λ in training and
testing had also been observed for in-context linear regression in Zhang et al. [2023a] and for in-context binary
classification in the previous section 3.2.
Here, we discuss the consequences when Assumption 4.2 does not hold. For example, suppose the labels
of our data in test prompts are not balanced P(y =e ) = p , µ do not have the same Λ−1 weighted norm
k k
µ⊤Λ−1µ ≜Ψ , and the covariance matrix of test data is Γ̸=Λ, then as N,M →∞, we have
k k k
M
c (cid:88)
y x⊤ →c(p µ ,p µ ,...,p µ )⊤,
M i i 1 1 2 2 c c
i=1
44and
P(y =1)→softmax(c(p µ ,p µ ,...,p µ )⊤Λ−1x ).
(cid:98)query 1 1 2 2 c c query
Denote Ψ=(Ψ ,...,Ψ )⊤, Φ=(log(p ),...,log(p ))⊤ and z =µ⊤Γ−1x −Ψ/2+Φ. Then distribution of
1 c 1 c query
the ground truth label is
P(y =e )=softmax(z) .
query k k
Define zˆ=c(p µ ,p µ ,...,p µ )⊤Λ−1x . Then, unless zˆ=z or ∥softmax(zˆ)−softmax(z)∥ is sufficiently
1 1 2 2 c c query 2
small, the transformer cannot correctly perform the in-context multi-class classification.
H Experiment details
For all tasks, we set d=20 and we randomly generate a covariance matrix Λ=diag(λ ,...,λ ), where λ =|λˆ |
1 d i i
andλˆ i. ∼i.d. N(3,1). ForeachtrainingdatasetwithdifferenttrainingpromptlengthsN,anddifferentclassnumbers
i
c, we randomly generate B training samples. Training prompts P ,τ ∈[B] and their corresponding labels y
τ τ,query
aregeneratedaccordingtoAssumption4.1. Moreover,wealsogeneratetestingdatasets. Forexample,foreachtest-
ingdataset, wefirstrandomlygenerate20pairsof(µ ,x ,y ),j ∈[20], where(µ )i. ∼i.d. Pm(Λ), x ∼
j j,query j,prob j Ω j,query
Pm(µ ,Λ). y =softmax(µ⊤Λ−1x ) are the corresponding probability distributions of the ground truth
x j j,prob j j,query
label y . For each j, we generate 100 testing prompts P =(x ,y ,...,x ,y ,x ), where
j,query jk jk,1 jk,1 jk,M jk,M j,query
(x ,y ) i. ∼i.d. Pm(µ Λ),j ∈ [20],k ∈ [100],i ∈ [M]. We denote a model’s output for testing prompts P
jk,i jk,i j jk
(cid:12) (cid:12)
as y . We calculate its inference error with 1 (cid:80) max (cid:12)(cid:98)(y ) −(y ) (cid:12), which serves an
(cid:98)jk 20×100 j∈[20],k∈[100] l∈[c](cid:12) jk l j,prob l(cid:12)
approximation of the expected total variation distance we defined in (3).
For experiments in Figure 1, we train the single-layer transformers with the sparse-form parameters and
structures defined in Section 4. We set the size of the training dataset to B =10,000 and set the batch size to
50. We train the transformers using SGD with learning rate {0.1,0.5,1} for 10 epochs, and get the best model
on each training dataset. Then, we test these trained models on different testing datasets. Each experiment is
repeated 10 times with different random seeds. The mean results and standard deviation error bars of these 10
experiments are plotted in Figure 1.
For experiments in Figure 2, the structure of the transformer with full parameters (1-layer, full) is defined as
E⊤WKQE
F(E;WV,WKQ)=E+WVE· , (59)
ρ
whereWV,WKQ ∈R(d+c)×(d+c) aretheparametersforoptimization. Forthe(3-layer, GPT2)model, weusethe
GPT2architecture[Radford,2018]with64embeddingsizes,3layersand2headsasimplementedbyHuggingFace
[Wolf et al., 2020], and we use the similar embedding method proposed by Garg et al. [2022]. Note that, for
the GPT2 model trained on prompt length N = 100, it will produce predictions for the labels y , i ∈ [101].
(cid:98)i
Assume the ground truth labels for the prompt is y , i ∈ [101], usually, for decoder models like GPT2, we
i
should define the loss function as
Loss=(cid:80)101
CrossEntropyLoss(y ,y ), which can be viewed to some extent
i=1 (cid:98)i i
as the model is trained with prompts length ranging from 0 to 100. However, for in-context classification of
Gaussian mixtures, longer prompts are significantly more helpful for training models than shorter prompts. For
example, considering an extreme case where the classification task involves c classes and the training prompt
length is smaller than c, then it is nearly impossible for the model to learn to classify c classes Gaussian mixtures
in context with these short prompts. Our Theorems 3.1 and 4.1 also clearly demonstrate that the training
error is inversely proportional to the prompt length with a scaling of O(1/N). Thus, if we define the loss
function for GPT2 as
Loss=(cid:80)101
CrossEntropyLoss(y ,y ), the underlying short prompts will be detrimental
i=1 (cid:98)i i
to the model training. Therefore, in our experiments, we define the loss function for our GPT2 model as
Loss=(cid:80)101
CrossEntropyLoss(y ,y ), which can be viewed to some extent as the model that is trained with
i=41 (cid:98)i i
prompts length ranging from 40 to 100. For all three transformer models, we set the size of the training dataset
45to B =400,000 and set the batch size to 50. We train the (1-layer, sparse) and (1-layer, full) using Adam with
learning rate 0.001 for 5 epochs, and train the GPT2 model using Adam with learning rate 0.0001 for 5 epochs.
Each experiment is repeated 3 times with different random seeds. The mean results and standard deviation
error bars of these 3 experiments are plotted in Figure 2.
46