GaVaMoE: Gaussian-Variational Gated Mixture of Experts
for Explainable Recommendation
FeiTang YongliangShen HangZhang
ZhejiangUniversity ZhejiangUniversity ZhejiangUniversity
flysugar@zju.edu.cn syl@zju.edu.cn 22451046@zju.edu.cn
ZeqiTan WenqiZhang GuiyangHou
ZhejiangUniversity ZhejiangUniversity ZhejiangUniversity
zqtan@zju.edu.cn zhangwenqi@zju.edu.cn gyhou@zju.edu.cn
KaitaoSong WeimingLu YuetingZhuang
MicrosoftResearchAsia ZhejiangUniversity ZhejiangUniversity
kaitaosong@microsoft.com luwm@zju.edu.cn yzhuang@zju.edu.cn
Abstract 1 Introduction
Largelanguagemodel-basedexplainablerecommendation(LLM- Recommendationsystemshavebecomeubiquitousinthedigital
basedER)systemsshowpromiseingeneratinghuman-likeexplana- age,offeringpersonalizedsuggestionstohelpusersnavigatevast
tionsforrecommendations.However,theyfacechallengesinmodel- amountsofinformation[36].Asthesesystemsevolve,theabilityto
inguser-itemcollaborativepreferences,personalizingexplanations, explainrecommendationshasemergedasacriticalfactorinenhanc-
andhandlingsparseuser-iteminteractions.Toaddresstheseissues, ingusertrust,decision-making,andsatisfaction[44,45].Clearand
weproposeGaVaMoE,anovelGaussian-VariationalGatedMixture meaningfulexplanationsnotonlyaidusersinmakingbetterchoices
ofExpertsframeworkforexplainablerecommendation.GaVaMoE butalsoincreasetheirconfidenceintherecommendationprocess.
introducestwokeycomponents:(1)aratingreconstructionmod- Consequently,developingeffectiveexplainablerecommendation
ulethatemploysVariationalAutoencoder(VAE)withaGaussian systemshasbecomeakeyresearchfocus[9],drivingthecreation
MixtureModel(GMM)tocapturecomplexuser-itemcollaborative ofmoreuser-centricandtrustworthyAI-drivenexperiences.
preferences,servingasapre-trainedmulti-gatingmechanism;and Recentadvancements[6,35,37]inlargelanguagemodelshave
(2)asetoffine-grainedexpertmodelscoupledwiththemulti-gating openednewavenuesforgeneratinghuman-likeexplanationsinrec-
mechanismforgeneratinghighlypersonalizedexplanations.The ommendationsystems,leadingtothedevelopmentofLLM-based
VAEcomponentmodelslatentfactorsinuser-iteminteractions, ERsystems[2,10,23,27,42].RecentworkssuchasPEPLER[23],
whiletheGMMclustersuserswithsimilarbehaviors.Eachcluster LLM2ER[42],andXRec[27]havedemonstratedthepotentialof
correspondstoagateinthemulti-gatingmechanism,routinguser- LLMsingeneratingcontextuallyrichandcoherentexplanations.PE-
itempairstoappropriateexpertmodels.Thisarchitectureenables PLER[23]pioneeredthisfieldbyutilizingGPT-2[29]anduser-item
GaVaMoEtogeneratetailoredexplanationsforspecificusertypes IDsasprompts,demonstratingthepotentialoftransferlearning
andpreferences,mitigatingdatasparsitybyleveragingusersimi- inrecommendationsystems.LLM2ER[42]builtuponthisfoun-
larities.Extensiveexperimentsonthreereal-worlddatasetsdemon- dationbyintroducingapersonalizedpromptmodule,addressing
stratethatGaVaMoEsignificantlyoutperformsexistingmethods thelimitationsofsolelyrelyingonuser-itemIDs.XRec[27]fur-
inexplanationquality,personalization,andconsistency.Notably, theradvancedthefieldbyincorporatinguseranditemprofiles
GaVaMoEexhibitsrobustperformanceinscenarioswithsparse andintegratingcollaborativefilteringsignals,bridgingthegapbe-
user-iteminteractions,maintaininghigh-qualityexplanationseven tweenconventionalrecommendationtechniquesandLLM-based
foruserswithlimitedhistoricaldata1. approaches.
Despitetheirpromisingpotential,currentLLM-basedERsystems
Keywords faceseveralcriticalchallengesthatlimittheireffectiveness.First,
RecommenderSystems,LargeLanguageModel,MixtureofExperts thesesystemsstruggletoadequatelymodeluser-itemcollaborative
preferences,particularlyincapturingthecomplex,non-linearrela-
1Ourprojectisavailableathttps://github.com/sugarandgugu/GaVaMoE. tionshipsthatexistbetweenusersanditems.Second,thegenerated
explanationsfrequentlylacksufficientpersonalization,failingto
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor reflectthenuancedpreferencesandbehaviorsofindividualusers.
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
Thisresultsingenericrecommendationsthatmaynotresonate
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe withusers’specificinterestsorneeds.Third,LLM-basedERap-
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or proachesperformpoorlywhenconfrontedwithsparseuser-item
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. interactions,acommonscenarioinreal-worldrecommendation
Conferenceacronym’XX,June03–05,2018,Woodstock,NY systems.Thissparsityproblemsignificantlyhamperstheability
©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. ofthesemodelstogenerateaccurateandmeaningfulexplanations,
ACMISBN978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX
4202
tcO
51
]RI.sc[
1v14811.0142:viXracaption：user-item interaction进入先前的方法，生成的可解释文本个性化
程度不高，比较通用化。在我们提出的方法中，user-item interaction首先
经过VAE进行编码并通过GMM得到不同的聚类，接着通过Multi-gating路
由到细腻度的专家并进行激活处理，从而生成的文本个性化程度高、文
本质量好。
Conferenceacronym’XX,June03–05,2018,Woodstock,NY Tangetal.
togeneratemeaningfulexplanations,evenforthosewithlimited
... Previous LLM-Based ER
System Prompt + This shirt is very interactionhistory.
User-Item Interaction { {U Is tee mr E Em +mb ·be ·ed dd din ing g} } + tren gd ry e a an t d fi th .as a Ourmaincontributionscanbesummarizedasfollows:
Multi-gating Fine-Grained MoE Personlization • WeintroduceGaVaMoE,annovelframeworkthatintegrates
VAE Cluster 1 Gate 1 Text Quality aVAEwithGMM(VAE-GMM)forratingreconstructionand
Encoder Gate 2 Consistency
Activated aMulti-gatingMixtureofExpertsforexplanationgeneration.
Thisdesignallowsformoreaccuratemodelingofuser-item
Cluster 2 This blue number collaborativepreferencesandgenerateshighlypersonalized,
GMM Gate 3 10 training jersey is
ideal for you. Stay contextuallyrelevantexplanations.
Cluster 3 motivated and train
like a pro. • We develop a dynamic multi-gating mechanism that effi-
ciently routes user-item pairs to expert models based on
Figure1:ComparisonofGaVaMoEwithpreviousLLM-based
clustereduserbehaviors.Thismechanismenhancesthesys-
ERsystemsforexplanationgeneration.GaVaMoEprocesses
tem’s ability to handle sparse data and generate tailored
user-iteminteractionsthroughVAEencodingandGMMclus-
explanations for diverse user groups, ensuring that even
tering,thenroutesthemtofine-grainedexpertsviamulti-
userswithlimitedinteractionhistoriesreceivemeaningful,
gating,generatinghighlypersonalized,high-qualityexplana-
high-qualityrecommendationsandexplanations.
tions.Incontrast,previousmethodstypicallygeneratemore
• Weconductextensiveexperimentsonthreereal-worlddatasets,
genericexplanationswithlimitedpersonalization.
demonstratingthatGaVaMoEsignificantlyoutperformspre-
viousmethodsacrossmultiplemetrics,includingexplana-
tionquality,personalization,andconsistency,andeffectively
particularlyforuserswithlimitedhistoricaldataorfornewor
addressingdatasparsitychallenges.
nicheitemsinthesystem.
To address these challenges, we propose GaVaMoE, a novel
2 RelatedWorks
Gaussian-VariationalGatedMixtureofExpertsframeworkforex-
plainablerecommendation.GaVaMoEintroducestwokeycompo- Explainable Recommendation System. Explainable recom-
nentsthatworktogethertoimprovepersonalizationandeffectively mendationprovidesexplanationsthatclarifywhycertainitems
mitigatedatasparsity.Thefirstcomponent,aratingreconstruction arerecommended,therebyenhancingthetransparencyandper-
module,employsVariationalAutoencoder(VAE)[17]inconjunc- suasivenessoftherecommendationsystems[36,45].Explainable
tionwithaGaussianMixtureModel(GMM)[16]tocapturedeep recommendationscanbepresentedinvariousformat,suchaspre-
user-itemcollaborativepreferences.Thismodulemapsuser-item definedtemplates[19,33,46],reasoningrules[3,32].However,
pairs into a latent space and clusters users with similar behav- thesemethodsareexpensivetomaintainandfailtoproducedi-
iors,uncoveringunderlyingpatternseveninsparsedatascenarios. verse,personalizedexplanations,resultinginpoorgeneralization.
Buildingupontheselearnedrepresentationsanduserclusters,the RelatedworksofExplainablerecommendationbasedonitemfea-
secondcomponentimplementsamulti-gatedmixtureofexperts.A tures[11,39],knowledgegraphpaths[1,8,40],andrankedtext
dynamicmulti-gatingmechanism,informedbytheGMMcluster- [20,21]alsosimilarlyfacetheproblemoflowgeneralizationand
ing,routesuser-itempairstotheappropriatefine-grainedexperts, poorpersonalization.Toaddressthesechallenges,moreandmore
ensuringhighlypersonalizedandcontextuallyrelevantexplana- explainablerecommendationsystemsbasedonnaturallanguage
tions.AsillustratedinFigure1,GaVaMoEfirstprocessesuser-item processingtechniqueshavebeenstudied.Theseworksfocusonus-
interactionsthroughVAEencodingandGMMclustering,thenuses inggenerativemodelstodirectlyobtainpersonalizedexplanations.
themulti-gatingmechanismtoactivatethemostsuitableexperts NRT[24]simultaneouslyperformsaccuratescorepredictionwhile
forexplanationgeneration.ThisarchitectureenablesGaVaMoEto generatinghigh-qualitysummarizedpromptsbyintegratinguser
producemorepersonalizedandcontextuallyrelevantexplanations anditemlatentfactors.Co-AttentiveMulti-TaskLearning(CAML
comparedtopreviousLLM-basedERsystems,whichtypicallyuse )[4]integratesamulti-tasklearningmechanismandadoptsthe
LLMswithgenericprompts. jointattentionproposedin[34].PETER[22]employsasmall,unpre-
GaVaMoEofferssignificantadvantagesinthreekeyareas:(1)En- trainedTransformer,connectinguseranditemIDswithgenerated
hancedCollaborativePreferenceModeling:TheVAEexplicitly textthroughadesignedcontextpredictiontaskforpersonalized
modelslatentfactorsinfluencinguser-iteminteractions,allowing textgeneration.
GaVaMoEtocapturecomplex,non-linearrelationshipsandencode Recently,researchonLLM-basedERsystemshasgainedsignifi-
user-itemcollaborativepreferencesdirectlyintothelatentspace. cantattention.ReXPlugisanend-to-endexplainablerecommenda-
(2)ImprovedPersonalization:TheGMMclusteringidentifies tionframeworkthatgenerateshigh-qualitypersonalizednatural
distinctusergroupsbasedontheirratingbehaviors,enablingmore languagereviewsforusersbyplugginginandutilizingaplug-and-
nuanceduserrepresentation.Themulti-gatingmechanismensures playlanguagemodel.PEPLER[23]utilizesapretrainedlanguage
thatexplanationsaretailoredtospecificusertypesandpreferences, modelGPT2[29]togenerateexplainablerecommendationsbyin-
witheachgatespecializinginroutinguser-itempairstosuitable corporatinguseranditemIDvectorsintoprompts.LLM2ER[42]
experts.(3)EffectiveHandlingofDataSparsity:Byemploying utilizesapersonalizedpromptlearningmoduletomatchuserpref-
GMMforclusteringuser-iteminteractionsandutilizingmultiple erenceandfine-tunesthemodelwithreinforcementlearningusing
expertmodels,thesystemcanleveragesimilaritiesamongusers twoinnovativeexplainabilityqualityrewardmodelstogenerateGaVaMoE:Gaussian-VariationalGatedMixtureofExperts
forExplainableRecommendation Conferenceacronym’XX,June03–05,2018,Woodstock,NY
explanations.Xrec[27]seamlesslyintegratesthecapabilitiesof theyreflecttheattributesofanitemunderspecificcircumstances
largelanguagemodelswithagraph-basedcollaborativefiltering [22].FollowingLLM2ER[42],weincorporatethesefeaturewords
paradigmtogeneratetext.Toaddressthelow-qualityissuescaused intotheLLMprompttoenhanceexplanationgeneration.Insum-
by hallucination in large language models, PEVAE [2] employs mary,theinputconsistsofuserID𝑢,itemID𝑖,user’srating𝑟 𝑢,𝑖,
personalizedvariationalautoencoderstotacklespecificqualitycon- featurewords 𝑓 𝑢,𝑖.Giventhisinput,ourmodelaimstogenerate
cerns,whilePRAG[41]enhancesperformancebyincorporating explainabletext𝑒ˆ𝑢,𝑖.
retrievalaugmentationtoresolvecertainlow-qualityissues.
3.2 VAE-GMMforRatingReconstruction
MixtureofExperts. MixtureofExperts(MoE)technologywas
Toeffectivelycapturedeepcollaborativepreferencesandcluster
firstintroducedby[14],wheredifferentexpertsareusedtocom-
userswithsimilarbehaviors,weproposeaVariationalAutoencoder
plete various subtasks. With the development of deep learning,
(VAE) integrated with a Gaussian Mixture Model (GMM). This
theTransformer[38]hasbeenwidelyusedinvariousnaturallan-
componentservesasthefoundationformodelingcomplexuser-
guage processing tasks. In the process, numerous studies have
itemcollaborativepreferencesandactsasaroutingstrategyforour
attemptedtoreplacethefeed-forwardneuralnetwork(FFN)layer
multi-gatingmechanism,addressingthechallengeofdatasparsity.
withinTransformerwithMoE.Sparsely-GatedMoE[30]introduces
asparselygatedmixtureofexpertslayer,whichcontainsmultiple 3.2.1 RatingReconstructionwithVariationalAutoencoder. Weem-
feed-forwardlayersandscalesupto137Bparameters.ThisMoE ployaVAEtolearncompactrepresentationsofuser-iteminter-
implementationisappliedconvolutelybetweenstackedLSTM[13] actions.Givenauser-itempair(𝑢,𝑖)where𝑢 ∈𝑈 and𝑖 ∈ 𝐼,the
layers.GShard[18]isapioneerinscalingMoElanguagemodelsto encoder𝐸oftheVAEpredictsthemean𝜇andlog-variance𝑙𝑜𝑔(𝜎)2
ultra-largesizesusinglearnabletop-2ortop-1routingstrategies. ofthelatentrepresentation:
SwitchTransformer[7]introducestheconceptofexpertcapacity
[𝜇,𝑙𝑜𝑔(𝜎)2] =𝐸(𝑢,𝑖) (1)
andsimplifiestheexpertroutingalgorithm,summarizingasetof
MoEtrainingexperiences.TheST-MoE[48]modelisastableand Toenablebackpropagationduringtraining,weutilizetherepa-
transferablesparseexpertmodelthataddressestraininginstabil- rameterizationtrick[25,31],whichtransformstherandomsam-
ityissuesbyintroducingrouterz-loss.Withthedevelopmentof plingprocessintoadifferentiableoperation:
large language models, researchers have made significant inno- 𝑧=𝜇+𝜀𝜎, 𝜀 ∼𝑁(0,𝐼) (2)
vationsinMoE.Mixtral8x7B[15]isasparsemixtureofexperts
languagemodelwith8feed-forwardblocks,achievingperformance
where𝜀israndomlysampledfromastandardnormaldistribution.
comparabletoa70Bparametermodelacrossmultiplebenchmarks.
Thedecoder𝐷thenreconstructstheratingfromthelatent𝑧:
LLaMA-MoE[47]constructsasparseMoEmodelbypartitioning 𝑟ˆ𝑢,𝑖 =𝐷(𝑧) (3)
theFFNparametersoftheexistinglargelanguagemodelLLaMA
Thisreconstructionprocessallowsthemodeltolearnacom-
[37]intomultipleexperts.DeepSeekMoE[5]innovativelyproposes
pactrepresentationofuser-iteminteractions,effectivelymodeling
fine-grainedexpertsegmentationandsharedexpertisolationstrate-
complexcollaborativepreferences.
giestoaddresstheproblemsofknowledgemixingandredundancy,
enhancingmodelperformance. 3.2.2 User Clustering with Gaussian Mixture Model. To cluster
userswithsimilarcollaborativepreferences,weextendtheabove
3 Method
VAEbyincorporatingaGaussianMixtureModel(GMM).Following
Inthissection,wepresentGaVaMoE,anovelexplainablerecom- theapproachin[16],weassumethatuser-itempairembeddings
mendationsystembasedonaMixtureofExpertsmodel.Webegin followamixedGaussiandistribution,whichallowsustocluster
byprovidingadetaileddefinitionoftheexplainablerecommenda- userswithsimilarcollaborativepreferences.WedefineaGaussian
tiontask(Section3.1).Then,weintroduceourproposedframework, mixturedistribution𝑃 =GMM(𝜋,𝜇¯,(𝜎¯)2),where𝜋 representsthe
illustratedinFigure2,whichconsistsoftwomaincomponents: priordistribution,and𝜇¯and(𝜎¯)2representthemeanandvariance
VAE-GMMforRatingReconstruction(Section3.2),andMulti-gating oftheGaussianmixturedistribution,respectively.
MixtureofExperts(Section3.3).Finally,wedetailthetwo-stage Assuming𝐾 clusters,theprocessofsamplingandclustering
trainingprocessofourframework(Section3.4),demonstratinghow fromthelatentspacecanberepresentedas:
thesecomponentsworkinconcerttoproducepersonalizedand 𝑝(𝑥,𝑧,𝑐)=𝑝(𝑥|𝑧)𝑝(𝑧|𝑐)𝑝(𝑐) (4)
contextuallyrelevantexplanations.
where𝑥 representstheobservedsample,𝑧istheembeddingofthe
3.1 TaskFormulation
user-itempair(𝑢,𝑖),and𝑐istheclusterassignment.Theprobabili-
tiesaredefinedasfollows:
Theexplainablerecommendationtaskinvolvesgeneratingperson-
alizeditemrecommendationsalongwithhuman-readableexpla- 𝑝(𝑐)=Cat(𝑐|𝜋) (5)
nations.Formally,givenauser-itempair𝑦 ∈ 𝑌,whichconsists 𝑝(𝑧|𝑐)=N(𝑧|𝜇¯𝑐,(𝜎¯𝑐)2𝐼) (6)
ofauserID𝑢 ∈ 𝑈 andanitemID𝑖 ∈ 𝐼,weconsidertherating
𝑟
𝑢,𝑖
∈𝑅 >0asanindicatorofuser𝑢’spositiveattitudetowardsitem 𝑝(𝑥|𝑧)=𝐵𝑒𝑟(𝑥|𝜇 𝑥) (7)
𝑖.Eachitem𝑖 isassociatedwithasetoffeatures 𝑓 𝑢,𝑖 ∈ 𝐹,which whereCat(𝜋)isacategoricaldistributionparameterizedby𝜋,and
describeitscharacteristics(e.g.,"thrilling"or"comedy"foramovie). Ber(𝑥|𝜇 𝑥)denotesamultivariateBernoullidistributionfortheout-
Thesefeaturesplayacrucialroleingeneratingexplainabletext,as putofdecoder𝐷.Conferenceacronym’XX,June03–05,2018,Woodstock,NY Tangetal.
VAE-GMM for Rating Reconstruction Multi-gating Mixture of Experts
Rating Reconstruction Explaination Generation Output: Explaination
KL Divegence Fine-grained MoE
VAE VAE Expert 1 Expert 2 Expert ··· Expert rN-1 Expert rN
z
Encoder Decoder
GMM VAE-GMM & Select
Multi-Gating Mechanism RRoouuteterr Top-2 Experts
Router 2
Gating
Cluster 1 Cluster 2 Cluster 3 Cluste Cluster N Input: User Item Sent. Feat.
r ··· System Prompt: You are a professional explainer, Your assignment involves providing
users with a detailed explanation regarding a specific item.
Input Prompt: The user has a {positive/negative} experience with the item, the item has
Router 1 Router 2 Router 3 Router ··· Router N {good} features, please provide an explanation for recommending {item} to {user}.
Output: {explaination text}
Stage 1: Rating Reconstruction Training Stage 2: Explaination Generation Training
Figure2:ThearchitectureofGaVaMoE.ThemodelcomprisesNstackedGaVaMoEBlocks,eachfeaturingtwokeycomponets:
(1)VAE-GMMforRatingReconstruction,whichusesVAEandGMMtoreconstructuser-itemratings,capturingcollaborative
signalsandclusteringuserswithsimilarpreferences;and(2)Multi-gatingMixtureofExperts,whichemploysaMulti-gating
mechanismtorouteuser-itempairstoappropriatefine-grainedexpertsforexplanationgeneration.GaVaMoE’strainingoccurs
intwostages:RatingReconstructionTraining,whichfocusesonlearninguser-itemcollaborativepreferencesandclustering,
followedbyExplanationGenerationTraining,whichutilizesthelearnedrepresentationstoproducepersonalizedexplanations.
ByoptimizingtheEvidenceLowerBound(ELBO)(detailedin identifiedbytheVAE-GMMisassociatedwithaspecificgateinthe
AppendixA),wetrainthisVAE-GMMmodeltosimultaneously multi-gatingmechanism.Thenumberofgatesinourmulti-gating
learnlatentrepresentationsandclusterassignments.Eachresult- mechanismisthereforeequaltothenumberofclustersKidentified
ingclustercorrespondstoagateinourMulti-gatingMechanism, intheVAE-GMMstage.
providing GaVaMoE with a routing strategy. By directing user- Todeterminewhichgateshouldprocesstheinput,weselectthe
itempairstocluster-specificgates,weensurethattheexplanation clusterwiththehighestprobability:
generationprocessistailoredtothespecificpreferencepatterns
𝑐¯=𝑎𝑟𝑔𝑚𝑎𝑥 𝑐𝛾 𝑐 (9)
identifiedwithineachcluster.
where𝑐¯representstheindexoftheselectedgate.Given𝐾 clusters
3.3 Multi-gatingMixtureofExperts andacorrespondingsetof𝐾 gates𝐺 =𝐺 1,...,𝐺 𝐾,weselectthe
BuildingupontheVAE-GMMcomponent,weintroduceanovel specificgate𝐺 𝑐¯forprocessingtheinput.
multi-gatingmixtureofexpertsarchitecturetogenerateexplana- Multi-gating mechanism ensures that users within the same
tions.Thisarchitecturecombinesadynamicroutingmechanism clusteraredirectedtothesamegate,guaranteeingthepreferences
withfine-grainedexpertstoenhancepersonalizationandefficiency. andstylesofspecificusergroups.Thisisparticularlybeneficialfor
OurapproachextendsthetraditionalMixtureofExperts(MoE)con- handlingsparsedata,asthemodelcanleverageknowledgefrom
ceptbyintroducingamulti-gatingmechanisminformedbyuser userswithinthesameclustertodiscoverimplicitpreferencesfor
clusteringandafine-grainedexpertstrategy. thosewithlimitedinteractionhistory.
3.3.1 Multi-gatingMechanismforExpertSelection. Ourmulti-gating 3.3.2 Fine-grainedMixtureofExperts. Toaddressissuesofknowl-
mechanismleveragestheclusteringinformationobtainedfromthe edgehybridityandknowledgeredundancy[5],wemodifytheorig-
VAE-GMMprocess(Section3.2)torouteuser-itempairstothemost inalstructurebydividingthefullyconnectednetworklayersinto
appropriateexperts.Foragiveninputsamplerepresentedbyits smaller,morespecializedunits.Specifically,wereducetheinterme-
latentembeddingz,wecomputetheprobabilityofitbelongingto diatehiddendimensionsofthefullyconnectedlayersto 𝑟1 oftheir
eachcluster𝑐usingthefollowingequation: originalsize,effectivelysplittingeachexpertFeed-ForwardNet-
𝜋
𝑐
·N(z|𝜇¯𝑐,(𝜎¯𝑐)2)
work(FFN)into𝑟smallerexperts.Assumingtheoriginalnumberof
𝛾 𝑐 = (cid:205) 𝑐𝐾 ′=1𝜋
𝑐′
·N(z|𝜇¯𝑐′,(𝜎¯𝑐′)2) (8) e fix np ee -r gt rs ai is ne𝑁 d, eth xpe en ru tm stb rae tr eo gf ye ax lp loe wrts si ts heno mw o𝑟 d𝑁 elta oft ae cr hr ie ed vu ec it nio cn re. aT sh ei ds
Thisprobabilitycalculationallowsustoidentifythemostlikely specializationwithoutadditionalcomputationalcost.
clusterforeachinputsample.Ourdesignincorporatesaone-to-one Givenaninput𝑥,afterdeterminingthecorrespondinggate𝐺 𝑐¯
correspondencebetweenclustersandgates,whereeachcluster throughthemulti-gatingmechanism,weproceedtoselectthemost
Users
Items
Pred
Rating
N
x
GaVaMoE
BlockGaVaMoE:Gaussian-VariationalGatedMixtureofExperts
forExplainableRecommendation Conferenceacronym’XX,June03–05,2018,Woodstock,NY
relevantexpertswithinthisgate.Toachievethis,wecomputelogits Table1:StatisticsoftheDatasets
𝑓(𝑥)=W·𝑥,whereWistheweightmatrixassociatedwithgate
𝐺 𝑐¯.Theselogitsrepresenttherelevancescoresofeachexpertfor TripAdvisor Amazon Yelp
thegiveninputwithintheselectedgate.Toobtainnormalizedprob-
#users 9,765 7,506 27,147
abilitiesforexpertselection,weapplyasoftmaxfunctionoverall
all𝑟𝑁 experts: #items 6,280 7,360 20,266
𝑒𝑓(𝑥)𝑖 #records 320,023 441,783 1,293,247
𝐺 𝑐¯(𝑥)𝑖 =
(cid:205)𝑟𝑁 𝑒𝑓(𝑥)𝑗
(10) #features 5,069 5,399 7,340
𝑗=1
where𝐺 𝑐¯(𝑥)𝑖 representsthegatingvalueforthe𝑖-thexpert.
Tofurtherenhanceefficiencyandspecialization,weselectonly Thistwo-stageapproachallowsGaVaMoEtoexcelinbothcol-
thetop𝑘expertsbasedontheirgatingvalues.LetΩrepresentthe laborativefilteringandpersonalizedexplanationgeneration.By
setofindicesforthesetop𝑘experts.Thefinaloutputoftheexpert firstbuildingaVAE-GMMinunderstandinguser-iteminteractions,
layeristhencomputedasaweightedcombinationoftheselected andthenleveragingthisunderstandingfortailoredexplanations,
experts’outputs: wecreateamodelcapableofprovidinghighlypersonalizedand
𝑟𝑁 contextuallyrelevantrecommendationsandexplanations.
∑︁
𝐺 𝑐¯(𝑥)𝑖 ·𝐸 𝑖(𝑥) (11)
𝑖∈Ω 4 Experiment
where𝐸
𝑖(𝑥)
istheoutputofthe𝑖-thexpertnetworkforinput𝑥.By
4.1 Datasets
combiningthemulti-gatingmechanismwithfine-grainedmixture
TocomprehensivelyevaluateGaVaMoE,weutilizedthreediverse
ofexperts,ourapproacheffectivelyaddressesthechallengesof
real-worlddatasets:TripAdvisor2,Amazon(Movies&TV)3,and
personalizationanddatasparsityinexplainablerecommendation
Yelp4.TripAdvisorrepresentsthetravelandhospitalitysectorwith
systems.Itenablesthemodeltogeneratehighlytailoredexplana-
hotelandattractionreviews,Amazonfocusesontheentertainment
tionsbyroutinginputstothemostappropriatespecializedexperts,
domainwithmovieandTVshowratings,whileYelpencompasses
eveninscenarioswithlimiteduserinteractiondata.
abroadspectrumoflocalbusinessesincludingrestaurants,services,
3.4 Two-stageTrainingObjective and retail. As shown in Table 1, these datasets exhibit varying
scalesandpotentialsparsitylevels,withYelpbeingthelargestand
ToeffectivelytraintheGaVaMoEmodel,weemployatwo-stage
potentiallymostsparse,followedbyAmazonandTripAdvisor.This
training process. This approach allows us to first capture deep
diversityindatacharacteristicsenablesustoevaluateGaVaMoE’s
collaborativepreferencesandthenleveragethisinformationfor
adaptability and robustness in handling different user behavior
personalizedexplanationgeneration.Thetwostagesareasfollows:
patternsanddatasparsitychallenges.Forourexperiments,each
3.4.1 Stage1:RatingReconstructionTraining. Thefirststagefo- datasetwasrandomlysplitintotraining,validation,andtestsetsat
cusesontrainingtheVAE-GMMbyratingreconstruction.Impor- aratioof8:1:1.
tantly,theVAE-GMMservesasakeycomponentforthemulti-
gatingmechanism.Ourgoalhereistocapturenuanceduser-item 4.2 ImplementationDetail
collaborativepreferencesandcreateuserclusters.Weoptimizethe GaVaMoEemploysatwo-layerTransformerencoderandatwo-
EvidenceLowerBound(ELBO)asourobjectivefunction: layerMLPforthedecoderintheVAE.Thelatentspacesizeis128,
L ELBO(𝑥,𝛽)=E𝑞(𝑧,𝑐|𝑥)[log𝑝(𝑥|𝑧)]−𝛽·KL(𝑞(𝑧,𝑐|𝑥)||𝑝(𝑧,𝑐)) withuseranditemembeddingsof768.Wesetbatchsizeto4096,
(12)
learningrateto1e-5,𝛽to0.1and30trainingepochs.Forexplana-
Thisfunctionbalancestwokeyaspects:thereconstructionofuser- tiongenerationtraining,wesetthebatchsizeto1,learningtateto
iteminteractionsandtheorganizationoftheseinteractionsinto 3e-5.ThetrainingprocessutilizesAdamWwithagradientaccumu-
distinctclusters.Thefirsttermencouragesaccuratereconstruction, lationof8andaclippingnormof0.3.GaVaMoEusesLLaMA3.1-8B
whilethesecondterm,controlledbythehyperparameter𝛽,ensures [6]with32GaVaMoEblocks,replacingtheFFNwithMoE(hidden
thatourlatentrepresentationsarewell-structuredandinformative. size1280,12experts,2activatedperpass).Thenumberofgates
matchesuserclusters,andexplanationgenerationtraininglasts3
3.4.2 Stage2:ExplanationGenerationTraining. Withourcollabo- forepochs.
rativepreferencescaptured,thesecondstagefocusesontraining
theMulti-gatingMixtureofExpertstogeneratepersonalizedexpla- 4.3 ComparedMethods
nations.Weleveragetheuserclustersandlatentrepresentations
Wecompareourmodel’sperformanceagainstthefollowingcom-
learnedinthefirststage.Ourobjectivefunctionforthisstagecom-
petiablebaselinesinexplainablerecommendation:
binesratingpredictionaccuracywithexplanationquality:
• NETE[19]:presentsagatedfusionrecurrentunitthatlever-
L total=𝛼·L ELBO+(1−𝛼)·L explanation (13) agesneuraltemplatestogeneratehigh-quality,explainable
Here,𝛼 balances the importance of accurate rating predictions textforrecommendationsystems.
L ELBOwiththequalityofgeneratedexplanationsL explanation.The
2https://www.tripadvisor.com
L explanationisthenegativelog-likelihoodofgeneratingthecorrect 3http://jmcauley.ucsd.edu/data/amazon
explanation. 4https://www.yelp.com/datasetConferenceacronym’XX,June03–05,2018,Woodstock,NY Tangetal.
Table2:Performancecomparisonofexplanationgenerationmethodsacrossthreedatasets.MetricsincludeBLEUandROUGE
fortextquality,Distinctforpersonalization,andBERTScoreforsemanticconsistency.
BLEU(%) ROUGE(%) Distinct(%) BERTScore
Method BLEU-1 BLEU-4 ROUGE-1 ROUGE-L Distinct-1 Distinct-2 BERTScore-P BERTScore-R BERTScore-F
Amazon(Movie&TV)
PEPLER 12.564 0.991 14.005 10.816 17.134 61.190 0.369 0.362 0.364
PETER 14.796 1.201 15.335 11.614 17.431 62.790 0.384 0.376 0.380
PEVAE 16.349 1.503 19.255 15.853 18.440 60.610 0.349 0.340 0.345
NETE 14.965 1.152 16.470 12.982 14.503 47.312 0.344 0.344 0.344
XRec 20.259 1.730 24.971 17.737 20.370 64.444 0.409 0.398 0.401
GaVaMoE 21.370 1.751 25.445 17.747 21.254 65.784 0.416 0.410 0.412
TripAdvisor
PEPLER 13.392 0.965 15.382 12.078 18.124 64.923 0.345 0.303 0.343
PETER 14.957 0.881 16.514 13.002 19.400 65.652 0.354 0.325 0.350
PEVAE 15.705 1.401 18.675 15.411 18.182 66.414 0.369 0.363 0.368
NETE 13.847 1.186 14.891 11.880 14.854 48.431 0.300 0.293 0.295
XRec 19.642 1.650 23.672 16.871 21.113 65.332 0.386 0.380 0.384
GaVaMoE 21.142 1.891 25.465 18.118 22.784 68.398 0.397 0.392 0.394
Yelp
PEPLER 9.448 0.623 12.604 9.769 13.321 43.577 0.317 0.294 0.305
PETER 8.072 0.574 12.965 9.850 14.240 52.891 0.298 0.271 0.281
PEVAE 14.250 1.217 16.229 13.891 16.890 58.510 0.328 0.323 0.326
NETE 11.314 0.823 14.041 9.226 13.245 44.426 0.222 0.137 0.144
XRec 19.379 1.601 22.870 16.531 17.560 61.190 0.395 0.351 0.373
GaVaMoE 19.586 1.616 23.259 17.416 19.483 63.847 0.406 0.374 0.381
• PETER[22]:utilizesTransformertointegrateuseranditem used,theyhavelimitationsincapturingtruesemanticinformation,
IDsintogeneratedexplanations. relyingprimarilyonn-gramoverlap.BERTScoreaddressesthislim-
• PEPLER[23]:adoptsapre-trainedGPT-2modelwithprompt itationwithamoresophisticatedapproachtoevaluatingsemantic
tuningforexplanationgeneration. consistency.Thisdiversesetofmetricsallowsforacomprehensive
• PEVAE[2]:extendshierarchicalVAEstoaddressdataspar- evaluationofGaVaMoE’sperformanceacrossvariousaspectsof
sityinLLM-basedrecommendationsystems,aligningwith explanationgeneration,assessingnotonlylinguisticqualitybut
ourfocusonsparseinteractionscenarios. alsopersonalizationandsemanticconsistency.
• XRec[27]:generatesexplanatorytextsbyintegratinghigh-
order collaborative signals encoded by graph neural net-
worksandthetextgenerationcapabilitiesofLLMs. 4.5 ResultsandAnalysis
4.5.1 MainResults. Table2presentstheperformancecomparison
Thesebaselineswerecarefullyselectedtorepresenttheprogres-
ofvariousexplanationgenerationmethodsacrossTripAdvisor,Yelp,
sionofexplainablerecommendationsystems,fromtemplate-based
andAmazon(Movie&TV)datasets.GaVaMoEconsistentlyout-
approaches(NETE)toadvancedLLM-basedmodels(XRec).
performsallbaselines,includingthestate-of-the-artXRecmodel,
across all metrics and datasets. Specifically, GaVaMoE achieves
4.4 EvaluationMetrics
significantimprovementsoverXRec:6.39%onTripAdvisor,4.46%
TocomprehensivelyevaluateGaVaMoE’sperformance,weemploy onYelp,and2.57%onAmazononaverage.Intermsoftextqual-
metricsassessingthreekeydimensions:explanationquality,per- ity,GaVaMoEachievesthehighestBLEU-4andROUGE-Lscores
sonalization,andconsistency.Forexplanationquality,weuseBLEU acrossalldatasets.Forinstance,ontheAmazondataset,GaVaMoE’s
[28](BLEU-1andBLEU-4)andROUGE[26](ROUGE-1andROUGE- BLEU-4scoreof1.75%surpassesXRec’s1.73%.Themodel’ssuperior
L)toassessfluency,grammaticalcorrectness,andcontentoverlap. performanceinthesemetricsindicatesitsabilitytogeneratehigh-
Forpersonalization,weemployDistinct-1andDistinct-2,evaluat- quality,fluentexplanations.GaVaMoEalsodemonstratesenhanced
ingunigramandbigramdiversityingeneratedtext.Forconsistency, consistency,asmeasuredbyBERTScore.OntheAmazondataset,
weutilizeBERTScore[43],capturingdeepersemanticsimilarities GaVaMoEachievesaBERTScore-Fof0.412,comparedtoXRec’s
usingcontextualembeddings.WhileBLEUandROUGEarewidely 0.401.Notably,onthesparserYelpdataset,GaVaMoE’sBERTScore-FGaVaMoE:Gaussian-VariationalGatedMixtureofExperts
forExplainableRecommendation Conferenceacronym’XX,June03–05,2018,Woodstock,NY
GaVaMoE[LLaMA3.1] Base[LLaMA3.1] GaVaMoE[LLaMA3.1] XRec PEPLER
GaVaMoE[LLaMA3.1] w/o Multi-gating GaVaMoE[GPT2]
GaVaMoE[LLaMA3.1] w/o Fine-grained experts 1.2
0.4
2 1.0
0.4
0.8 0.3
1
0.2 0.6
0.2
ds1 ds2 ds3 ds1 ds2 ds3
0 0.0
TripAdvisor Amazon Yelp TripAdvisor Amazon Yelp
Figure4:Performancecomparisonacrossdifferentsparsity
Figure3:AblationstudyresultscomparingGaVaMoEvari-
levels(ds1,ds2,ds3).GaVaMoEdemonstratesconsistentsu-
antsacrossTripAdvisor,Amazon,andYelpdatasets.
periorityandstabilityacrossallsparsitylevels.
ontheAmazondataset,GaVaMoE[LLaMA3.1]achievedaBLEU-
of0.381showsa2.14%increaseoverXRec,highlightingitsrobust-
4 score of 1.89, compared to 1.82 for the variant without multi-
nessinhandlingsparsedatascenarios.Forpersonalization,mea-
gatingand1.85forthevariantwithoutfine-grainedexperts.This
suredbyDistinctscores,GaVaMoEconsistentlygeneratesmore
demonstratesthecrucialroleofbothcomponentsinenhancing
diverseexplanations.OnTripAdvisor,GaVaMoE’sDistinct-1and
thequalityandrelevanceofgeneratedexplanations.(2)Comple-
Distinct-2scoresof22.784%and68.398%respectivelyoutperform
mentaryEffectsofArchitecturalComponents:ThefullGaVa-
XRec’s21.113%and65.332%.
MoE[LLaMA3.1]modeldemonstratedsubstantialimprovements
TheseimprovementscanbeattributedtoGaVaMoE’skeycom-
overBase[LLaMA3.1],withBERTScoreincreasesof7.6%,8.1%,and
ponents.TheVAEenablesdeepcollaborativepreferencemodeling,
6.2%onTripAdvisor,Amazon,andYelpdatasets,respectively.This
capturingnuanceduser-iteminteractionsformoreaccurateand
highlightsthecomplementarybenefitsofthemulti-gatingmech-
personalizedexplanations.Thisisenhancedbytheintegrationof
anismforpreciseroutingandtheMoEstructureforspecialized
GMM,whichrefinesusergroupingbasedonsimilarpreferences.
processing.(3)BaseModelIndependence:Interestingly,GaVa-
Theresultingclustersinformamulti-gatingmechanismthateffi-
MoE[GPT2]showedcompetitiveperformance,outperformingmost
cientlyroutesuser-itempairstothemostsuitableexperts,ensuring
baselinemodelsexceptXRec.Thisindicatesthatourmulti-gating
eachexplanationisgeneratedbyanexpertfamiliarwiththespecific
andMoEcomponentsprovidesubstantialimprovementsevenwith
interactionpattern.Thesefine-grainedexperts,focusingonspecific
smaller,lessadvancedlanguagemodels.(4)Scalability:Perfor-
usersubsets,allowfortargetedandpersonalizedexplanationgen-
manceremainedstrongacrossdifferentbasemodelsizes.GaVa-
eration.Thiscombinationofdeeppreferencemodeling,clustering,
MoE[GPT2]’sBLEU-4scoreswereonlyslightlylowerthanGaVa-
efficientrouting,andspecializedgenerationenablesGaVaMoEto
MoE[LLaMA3.1](e.g.,1.76vs.1.89onAmazon),showcasingthe
producehigherquality,personalized,andconsistentexplanations.
architecture’sflexibilityacrossvariouscomputationalsettings.
4.5.2 Ablation Study. To further evaluate the efficacy of GaVa-
4.5.3 AnalysisofMulti-gatingMechanism. Toevaluatetheimpact
MoE’scomponents,weconductedacomprehensiveablationstudy
ofourmulti-gatingmechanism,wevariedthenumberofgatesfrom
acrossthreediversedatasets.Wecomparedthefollowingvariants:
2to5acrossthreedatasets.Table3showsdistinctoptimalgate
• GaVaMoE[LLaMA3.1]:Ourfullmodel,incorporatingboth configurationsforeachdataset:3gatesforTripAdvisor,2gatesfor
themulti-gatingmechanismandtheenhancedMoEarchi- Amazon,and3gatesforYelp.
tecture. Thesevariationsinoptimalgatenumbersreflecttheintricatere-
• GaVaMoE[LLaMA3.1]w/oMulti-gating:Avariantthat lationshipbetweendatasetcharacteristicsandmodelperformance.
retainstheMoEarchitecturebutremovesthemulti-gating Withfewergates,user-itemcollaborativepreferenceswithineach
mechanism. clustertendtobemoreconsistent,enablingbetterpatternlearning
• GaVaMoE[LLaMA3.1]w/oFine-grainedexperts:Avari- andgeneralization.However,asthenumberofgatesincreases,data
antthatretainsthemulti-gatingmechanismbutremoves dispersionmayintroducenoise,potentiallyreducingthemodel’s
thefine-grainedexperts. generalizationability.Theeffectivenessofthemulti-gatingmecha-
• Base[LLaMA3.1]:AbaselineversionusingonlytheLLaMA nismalsodependsonbalanceddatadistributionacrossgates.An
3.1languagemodel,withoutthemulti-gatingmechanism imbalance,suchasonegatereceivingsignificantlyfeweritems,
andMoEarchitecture. canleadtoincompletetrainingofboththegateanditsassociated
• GaVaMoE[GPT2]:UsesGPT2asthebaselanguagemodel experts,impactingoverallperformance.
whileretainingourmulti-gatingandMoEcomponents. Ouranalysisunderscorestheimportanceoftuningthenumber
ofgatesbasedondataset-specificcharacteristics.Theoptimalcon-
Figure3presentsourablationstudyresults,comparingBLEU-4and
figurationbalancescapturingdiverseuser-itempreferenceswith
BERTScoremetricsacrossthreedatasets.Keyfindingsinclude:(1)
maintainingsufficientdataconsistencywithinclusters.
EffectivenessofMulti-gatingMechanismandFine-grained
Experts:GaVaMoE[LLaMA3.1]consistentlyoutperformeditsvari- 4.5.4 AnalysisofDataSparsity. ToevaluateGaVaMoE’srobustness
ants without multi-gating or fine-grained experts. For instance, againstdatasparsity,weconductedacomprehensiveevaluation
erocS
4-UELB
erocSTREB
4-UELB
erocSTREBConferenceacronym’XX,June03–05,2018,Woodstock,NY Tangetal.
Table3:PerformanceofGaVaMoEwithdifferentnumbersofgatescrossthreedatasets.
Router BLEU-1 BLEU-4 ROUGE-1 ROUGE-L DISTINCT-1 DISTINCT-2 BERTScore-P BERTScore-R BERTScore-F
TripAdvisor
2 17.270 1.735 25.314 18.392 20.466 65.332 0.301 0.296 0.263
3 21.142 1.891 25.465 18.118 22.784 68.398 0.397 0.392 0.395
4 19.980 1.862 25.040 18.192 21.284 67.190 0.313 0.299 0.285
5 16.038 1.748 24.886 18.034 20.130 66.036 0.290 0.259 0.240
Amazon(Movie&TV)
2 17.465 1.521 21.072 15.271 17.113 60.074 0.311 0.290 0.304
3 19.586 1.616 23.259 17.416 19.483 63.847 0.406 0.374 0.381
4 16.224 1.398 22.697 16.875 17.470 61.454 0.368 0.350 0.353
5 16.047 1.326 22.067 16.644 17.201 61.241 0.360 0.334 0.347
Yelp
2 17.465 1.521 21.072 15.271 17.113 60.074 0.311 0.290 0.304
3 19.586 1.616 23.259 17.416 19.483 63.847 0.406 0.374 0.381
4 16.224 1.398 22.697 16.875 17.470 61.454 0.368 0.350 0.353
5 16.047 1.326 22.067 16.644 17.201 61.241 0.360 0.334 0.347
Table4:ThehumanevaluationresultsontheYelpdataset.
30 1.0 40 2.0
20 0.8 20 1.5 Method P Q U HumanScore
10 0.6
0 0 1.0 GaVaMoE 4.524 4.553 4.541 4.538
0.4
10 20 0.5 XRec 4.390 4.287 4.252 4.317
20 0.2
PEPLER 3.766 3.652 3.773 3.734
30 40 20 0 20 40 0.0 40 25 0 25 50 0.0
3.0 4
40 2.5 40 clusteringapproachiscrucialforaddressingdatasparsity:when
3 encounteringuserswithlimitedinteractions,themodelleverages
20 2.0 20
informationfromsimilaruserswithinthesamecluster,enabling
0 1.5 0 2
personalizedexplanationsevenforsparseusers.
20 1.0 20 1
40 0.5 40 4.6 HumanEvaluation
50 25 0 25 50 0.0 50 25 0 25 50 0
TovalidateGaVaMoE’seffectivenessinreal-worldscenarios,we
Figure5:VisualizationoflatentspacedistributionsinGaVa- conductedahumanevaluationstudyusing2,000randomlysampled
MoEfordifferentnumbersofuserclusters. entriesfromtheYelpdataset.WecomparedGaVaMoEagainstXRec
andPEPLER,employingthreeprofessionalstoassessthegenerated
explanations based on personalization (P), text quality (Q), and
ofGaVaMoE’sperformanceacrossvaryinglevelsofuser-itemin- usersatisfaction(U).Eachcriterionwasscoredona1-to-5scale,
teractionsparsity.Followingmethodologiesfrom[27]and[2],we withdetailsprovidedinAppendixB.Wecomputedacomposite
dividedthetestdataintothreeequalsubsets(ds1,ds2,ds3)based Human-Scoreusingaweightedformula:
onuserappearancefrequencyinthetrainingdata.Thisdivision
Human-Score=𝜆 𝑝𝑃 𝑠𝑐𝑜𝑟𝑒 +𝜆 𝑞𝑄 𝑠𝑐𝑜𝑟𝑒 +𝜆 𝑢𝑈 𝑠𝑐𝑜𝑟𝑒 (14)
createsaspectrumofsparsity,withds1containingthemostfre-
quentusersandds3theleastfrequent,allowingustoassessmodel Where𝜆 𝑝 =0.4,𝜆 𝑞,and𝜆 𝑢 =0.3.ThescoresP,Q,andUrepresent
performanceunderincreasinglysparseconditions. themetricsforpersonalization,textquality,andusersatisfaction,
AsshowninFigure4,GaVaMoEconsistentlyoutperformsXRec respectively.
and PEPLER across all sparsity levels in terms of BLEU-4 and Table4presentstheseresults,showingthatGaVaMoEconsis-
BERTScoremetrics.Notably,GaVaMoE’sperformanceremainssta- tentlyoutperformedbaselinesacrossallmetrics.GaVaMoEachieved
blefromds1tods3,indicatingitseffectivenessinhandlingsparse thehighestoverallHuman-Scoreof4.538,significantlysurpassing
datascenarios. XRec(4.317)andPEPLER(3.734).Thesehumanevaluationresults
TherobustnessofGaVaMoEcanbeattributedtoitsclustering corroborateourcomputationalfindings,demonstratingGaVaMoE’s
andfine-grainedexpertmodels.Figure5visualizesthelatentspace abilitytogeneratehigh-quality,personalizedexplanationsthatres-
distribution,demonstratingclearseparationofusercategories.This onatewithusersinreal-worldscenarios.GaVaMoE:Gaussian-VariationalGatedMixtureofExperts
forExplainableRecommendation Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Table5:ComparisonofexplanationsgeneratedbybaselinesandGaVaMoEunderdifferentuserinteractionscenarios.Case1
representsauserwithlimitedinteractionhistory,whileCase2representsauserwithsufficientinteractions.
Case1:Limiteduser-iteminteractions Case2:Sufficientuser-iteminteractions
GroundTruth
Text:Whileundoubtedlyawell-doneandstunningmovie. Text:J.K.Rowling’smagicalworldisbeautifullybroughttolifein
thisfantasyfilm.
Features:undoubtedly,stunning,movie Features:J.K.Rowling,magic,fantasy,books
GeneratedExplanations
PETER:Ithinkthisfilmisagoodmovie. PETER:ThisfantasymovieisbasedonJ.K.Rowling’sbooks.
PEPLER:Thefilmisaverygoodmovie. PEPLER:ThefilmisfantasybasedonJ.K.Rowling’sbooks.
XRec:Whileawell-doneandstunningmovie,itlacksdepthand XRec:J.K.Rowling’smagicalworldisvividlybroughttolife,of-
failstoengage. feringarichandimmersiveexperience.
GaVaMoE:Whileundoubtedlyawell-doneandstunningmovie, GaVaMoE:J.K.Rowling’smagicalworldisbeautifullybroughtto
thevisualsandperformancesareexcellent. lifeinthisfantasyfilm,withstunningvisualsandacaptivating
story.
5 CaseStudy reconstructionmoduleemployingVAEwithGMMforcapturing
TofurtherevaluateGaVaMoE’seffectivenessingeneratingperson- complexuser-itemcollaborativepreferences,and(2)amulti-gating
alizedandrelevantexplanations,weconductedadetailedcasestudy mechanismcoupledwithexpertmodelsforgeneratingpersonal-
comparingourmodel’sperformancewithbaselinemethodsunder ized explanations. Our approach enhances collaborative prefer-
differentuserinteractionscenarios.Table5presentsacomparative encemodeling,improvespersonalization,andeffectivelyhandles
analysisofexplanationsgeneratedbyGaVaMoEandbaselinemeth- datasparsity.Extensiveexperimentsonthreereal-worlddatasets
odsundertwodistinctscenarios:limitedandsufficientuser-item demonstratethatGaVaMoEsignificantlyoutperformsstate-of-the-
interactions.GaVaMoEdemonstratessuperiorperformanceacross artmethodsacrossmultiplemetrics,includingexplanationquality,
bothcases,effectivelyaddressingthechallengesofdatasparsity personalization,andconsistency.Notably,GaVaMoEexhibitsro-
andpersonalization.Inthelimitedinteractionscenario(Case1), bustperformanceinsparsedatascenarios,maintaininghigh-quality
GaVaMoEcapturesallkeyitemfeatures("undoubtedly,""stunning," explanationsevenforuserswithlimitedhistoricaldata.
"movie"),showcasingitsrobustnessinsparsedataconditions.The
model’sexplanationcloselymirrorsthegroundtruthwhileadding References
relevantdetailsabout"visualsandperformances,"providingamore [1] QingyaoAi,VahidAzizi,XuChen,andYongfengZhang. Learningheteroge-
comprehensiveinsight.Similarly,inthesufficientinteractioncase
neousknowledgebaseembeddingsforexplainablerecommendation.Algorithms,
11(9):137,2018.
(Case2),GaVaMoEaccuratelycapturestheessenceofJ.K.Rowling’s [2] ZefengCaiandZeruiCai.Pevae:Ahierarchicalvaeforpersonalizedexplainable
magicalworldandthefantasygenre,demonstratingitsabilityto recommendation.InProceedingsofthe45thInternationalACMSIGIRConference
onResearchandDevelopmentinInformationRetrieval,pages692–702,2022.
leveragericheruserhistory.Acrossbothscenarios,GaVaMoEmain-
[3] HanxiongChen,ShaoyunShi,YunqiLi,andYongfengZhang.Neuralcollabo-
tainsappropriatesentimentandcontext,avoidingtheintroduction rativereasoning. InProceedingsoftheWebConference2021,pages1516–1527,
ofirrelevantorinaccurateinformation-apitfallobservedinsome 2021.
[4] ZhongxiaChen,XitingWang,XingXie,TongWu,GuoqingBu,YiningWang,and
baselinemethodslikeXRec.Incontrast,PETERandPEPLERcon-
EnhongChen.Co-attentivemulti-tasklearningforexplainablerecommendation.
sistentlyproducegenericstatementslackingspecificdescriptors, InIJCAI,volume2019,pages2137–2143,2019.
highlightingGaVaMoE’ssuperiorfeatureintegrationandcontex- [5] DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,HuazuoGao,DeliChen,
JiashiLi,WangdingZeng,XingkaiYu,YWu,etal. Deepseekmoe:Towards
tual understanding. These observations underscore GaVaMoE’s ultimateexpertspecializationinmixture-of-expertslanguagemodels. arXiv
effectivenessingeneratinghighlypersonalized,contextuallyrel- preprintarXiv:2401.06066,2024.
[6] AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,Ahmad
evant,andinformativeexplanations,regardlessoftheextentof
Al-Dahle,AieshaLetman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,
userinteractionhistory.Themodel’sperformancealignswithour etal.Thellama3herdofmodels.arXivpreprintarXiv:2407.21783,2024.
initialgoalsofenhancingexplainablerecommendationsthrough [7] WilliamFedus,BarretZoph,andNoamShazeer.Switchtransformers:Scalingto
trillionparametermodelswithsimpleandefficientsparsity.JournalofMachine
improvedcollaborativepreferencemodelingandpersonalization, LearningResearch,23(120):1–39,2022.
particularlyinscenarioswithsparseuser-iteminteractions. [8] ZuohuiFu,YikunXian,RuoyuanGao,JieyuZhao,QiaoyingHuang,Yingqiang
Ge,ShuyuanXu,ShijieGeng,ChiragShah,YongfengZhang,etal.Fairness-aware
explainablerecommendationoverknowledgegraphs.InProceedingsofthe43rd
internationalACMSIGIRconferenceonresearchanddevelopmentininformation
6 Conclusion retrieval,pages69–78,2020.
[9] JingyueGao,XitingWang,YashaWang,andXingXie.Explainablerecommenda-
Inthispaper,weintroducedGaVaMoE,anovelGaussian-Variational tionthroughattentivemulti-viewlearning.InProceedingsoftheAAAIConference
GatedMixtureofExpertsframeworkforexplainablerecommen- onArtificialIntelligence,volume33,pages3622–3629,2019.
[10] DeepeshVHadaandShirishKShevade.Rexplug:Explainablerecommendation
dationsystems.GaVaMoEaddressescriticalchallengesinexisting usingplug-and-playlanguagemodel. InProceedingsofthe44thInternational
LLM-basedERsystemsthroughtwokeyinnovations:(1)arating ACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,Conferenceacronym’XX,June03–05,2018,Woodstock,NY Tangetal.
pages81–91,2021. [36] NavaTintarevandJudithMasthoff.Explainingrecommendations:Designand
[11] XiangnanHe,TaoChen,Min-YenKan,andXiaoChen.Trirank:Review-aware evaluation.InRecommendersystemshandbook,pages353–382.Springer,2015.
explainablerecommendationbymodelingaspects.InProceedingsofthe24thACM [37] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne
internationalonconferenceoninformationandknowledgemanagement,pages Lachaux,TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,Faisal
1661–1670,2015. Azhar,etal. Llama:Openandefficientfoundationlanguagemodels. arXiv
[12] IrinaHiggins,LoicMatthey,ArkaPal,ChristopherPBurgess,XavierGlorot, preprintarXiv:2302.13971,2023.
MatthewMBotvinick,ShakirMohamed,andAlexanderLerchner. beta-vae: [38] AVaswani.Attentionisallyouneed.AdvancesinNeuralInformationProcessing
Learningbasicvisualconceptswithaconstrainedvariationalframework.ICLR Systems,2017.
(Poster),3,2017. [39] XiangWang,XiangnanHe,FuliFeng,LiqiangNie,andTat-SengChua.Tem:Tree-
[13] SHochreiter.Longshort-termmemory.NeuralComputationMIT-Press,1997. enhancedembeddingmodelforexplainablerecommendation.InProceedingsof
[14] RobertAJacobs,MichaelIJordan,StevenJNowlan,andGeoffreyEHinton. the2018worldwidewebconference,pages1543–1552,2018.
Adaptivemixturesoflocalexperts.Neuralcomputation,3(1):79–87,1991. [40] YikunXian,ZuohuiFu,ShanMuthukrishnan,GerardDeMelo,andYongfeng
[15] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,Blanche Zhang.Reinforcementknowledgegraphreasoningforexplainablerecommenda-
Savary,ChrisBamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBou tion.InProceedingsofthe42ndinternationalACMSIGIRconferenceonresearch
Hanna,FlorianBressand,etal.Mixtralofexperts.arXivpreprintarXiv:2401.04088, anddevelopmentininformationretrieval,pages285–294,2019.
2024. [41] ZhouhangXie,SameerSingh,JulianMcAuley,andBodhisattwaPrasadMajumder.
[16] ZhuxiJiang,YinZheng,HuachunTan,BangshengTang,andHanningZhou.Vari- Factualandinformativereviewgenerationforexplainablerecommendation.In
ationaldeepembedding:Anunsupervisedandgenerativeapproachtoclustering. ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume37,pages
arXivpreprintarXiv:1611.05148,2016. 13816–13824,2023.
[17] Diederik P Kingma. Auto-encoding variational bayes. arXiv preprint [42] MengyuanYang,MengyingZhu,YanWang,LinxunChen,YileiZhao,Xiuyuan
arXiv:1312.6114,2013. Wang,BingHan,XiaolinZheng,andJianweiYin.Fine-tuninglargelanguage
[18] DmitryLepikhin,HyoukJoongLee,YuanzhongXu,DehaoChen,OrhanFirat, modelbasedexplainablerecommendationwithexplainablequalityreward.In
YanpingHuang,MaximKrikun,NoamShazeer,andZhifengChen. Gshard: ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages
Scalinggiantmodelswithconditionalcomputationandautomaticsharding. 9250–9259,2024.
arXivpreprintarXiv:2006.16668,2020. [43] TianyiZhang,VarshaKishore,FelixWu,KilianQWeinberger,andYoavArtzi.
[19] LeiLi,YongfengZhang,andLiChen. Generateneuraltemplateexplanations Bertscore:Evaluatingtextgenerationwithbert.arXivpreprintarXiv:1904.09675,
forrecommendation.InProceedingsofthe29thACMInternationalConferenceon 2019.
Information&KnowledgeManagement,pages755–764,2020. [44] YongfengZhang.Tutorialonexplainablerecommendationandsearch.InPro-
[20] LeiLi,YongfengZhang,andLiChen.Extra:Explanationrankingdatasetsfor ceedingsofthe2019ACMSIGIRInternationalConferenceonTheoryofInformation
explainablerecommendation,2021. Retrieval,pages255–256,2019.
[21] LeiLi,YongfengZhang,andLiChen.Extra:Explanationrankingdatasetsfor [45] YongfengZhang,XuChen,etal.Explainablerecommendation:Asurveyand
explainablerecommendation.InProceedingsofthe44thInternationalACMSIGIR newperspectives.FoundationsandTrends®inInformationRetrieval,14(1):1–101,
conferenceonResearchandDevelopmentinInformationRetrieval,pages2463–2469, 2020.
2021. [46] YongfengZhang,GuokunLai,MinZhang,YiZhang,YiqunLiu,andShaopingMa.
[22] LeiLi,YongfengZhang,andLiChen.Personalizedtransformerforexplainable Explicitfactormodelsforexplainablerecommendationbasedonphrase-level
recommendation.arXivpreprintarXiv:2105.11601,2021. sentimentanalysis.InProceedingsofthe37thinternationalACMSIGIRconference
[23] LeiLi,YongfengZhang,andLiChen.Personalizedpromptlearningforexplain- onResearch&developmentininformationretrieval,pages83–92,2014.
ablerecommendation. ACMTransactionsonInformationSystems,41(4):1–26, [47] TongZhu,XiaoyeQu,DaizeDong,JiachengRuan,JingqiTong,ConghuiHe,and
2023. YuCheng.Llama-moe:Buildingmixture-of-expertsfromllamawithcontinual
[24] PijiLi,ZihaoWang,ZhaochunRen,LidongBing,andWaiLam.Neuralrating pre-training.arXivpreprintarXiv:2406.16554,2024.
regressionwithabstractivetipsgenerationforrecommendation.InProceedings [48] BarretZoph,IrwanBello,SameerKumar,NanDu,YanpingHuang,JeffDean,
ofthe40thInternationalACMSIGIRconferenceonResearchandDevelopmentin NoamShazeer,andWilliamFedus.St-moe:Designingstableandtransferable
InformationRetrieval,pages345–354,2017. sparseexpertmodels.arXivpreprintarXiv:2202.08906,2022.
[25] DawenLiang,RahulGKrishnan,MatthewDHoffman,andTonyJebara.Varia-
tionalautoencodersforcollaborativefiltering.InProceedingsofthe2018world
widewebconference,pages689–698,2018.
[26] Chin-YewLin.Rouge:Apackageforautomaticevaluationofsummaries.InText
summarizationbranchesout,pages74–81,2004.
[27] QiyaoMa,XubinRen,andChaoHuang. Xrec:Largelanguagemodelsforex-
plainablerecommendation.arXivpreprintarXiv:2406.02377,2024.
[28] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.Bleu:amethod
forautomaticevaluationofmachinetranslation.InProceedingsofthe40thannual
meetingoftheAssociationforComputationalLinguistics,pages311–318,2002.
[29] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,
etal.Languagemodelsareunsupervisedmultitasklearners.OpenAIblog,1(8):9,
2019.
[30] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,Ge-
offreyHinton,andJeffDean.Outrageouslylargeneuralnetworks:Thesparsely-
gatedmixture-of-expertslayer.arXivpreprintarXiv:1701.06538,2017.
[31] IlyaShenbin,AntonAlekseev,ElenaTutubalina,ValentinMalykh,andSergeyI
Nikolenko.Recvae:Anewvariationalautoencoderfortop-nrecommendations
withimplicitfeedback.InProceedingsofthe13thinternationalconferenceonweb
searchanddatamining,pages528–536,2020.
[32] ShaoyunShi,HanxiongChen,WeizhiMa,JiaxinMao,MinZhang,andYongfeng
Zhang. Neurallogicreasoning. InProceedingsofthe29thACMInternational
ConferenceonInformation&KnowledgeManagement,pages1365–1374,2020.
[33] JuntaoTan,ShuyuanXu,YingqiangGe,YunqiLi,XuChen,andYongfengZhang.
Counterfactualexplainablerecommendation. InProceedingsofthe30thACM
InternationalConferenceonInformation&KnowledgeManagement,pages1784–
1793,2021.
[34] YiTay,AnhTuanLuu,andSiuCheungHui.Multi-pointerco-attentionnetworks
forrecommendation. InProceedingsofthe24thACMSIGKDDinternational
conferenceonknowledgediscovery&datamining,pages2309–2318,2018.
[35] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-Baptiste
Alayrac,JiahuiYu,RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,
etal. Gemini:afamilyofhighlycapablemultimodalmodels. arXivpreprint
arXiv:2312.11805,2023.GaVaMoE:Gaussian-VariationalGatedMixtureofExperts
forExplainableRecommendation Conferenceacronym’XX,June03–05,2018,Woodstock,NY
A ELBOinGaVaMoE • User Satisfaction (U): Assess whether the explanation
InGaVaMoE,ourobjectiveistomaximizethelikelihoodofuser- wouldlikelymotivatetheusertoengagefurtherwiththe
itempairs,equivalenttomaximizinglog𝑝(𝑥).AdoptingJensen’s recommendeditemortheplatform.Considerfactorssuch
inequality,wederivethevariationalevidencelowerbound(ELBO) asinformativeness,persuasiveness,andappeal.
asfollows: Adetailedscoringguidewithspecificcriteriaforeachscorelevel
(cid:20) 𝑝(𝑐,𝑧,𝑥)(cid:21) isprovidedinTable6,ensuringconsistencyandreliabilityinthe
L𝐸𝐿𝐵𝑂(𝑥)=E 𝑞(𝑧,𝑐|𝑥) log 𝑞(𝑧,𝑐|𝑥) humanevaluationprocess.
Toensurecomprehensiveandpreciseevaluations,humanex-
=E
𝑞(𝑧,𝑐|𝑥)
[log𝑝(𝑥|𝑧)+log𝑝(𝑧|𝑐)+log𝑝(𝑐)]−
(15) pertsareencouragedtoutilizethefullrangeofthescale,including
E
𝑞(𝑧,𝑐|𝑥)
[log𝑞(𝑧|𝑥)+log𝑞(𝑐|𝑥)]
half-pointscores(e.g.,3.5)whenexplanationsfallbetweentwo
(cid:20) 𝑝(𝑧,𝑐) (cid:21) scoredescriptions.Thisgranularapproachallowsforamorenu-
=E
𝑞(𝑧,𝑐|𝑥)
log𝑝(𝑥|𝑧)+log
𝑞(𝑧,𝑐|𝑥) ancedassessmentoftheexplanations’qualityandeffectiveness.
Thefirsttermrepresentsthereconstructionloss,allowingthe
Table6:ScoringCriteriaforHumanEvaluation
modeltodiscoverdeepleveluser-iteminteractions.Thesecond
termcalculatestheKullback-Leiblerdivergencebetweenthemixed
Gaussian prior distribution𝑞(𝑧|𝑥) and the variational posterior Score CriteriaDescription
𝑝(𝑧,𝑐).Tolearnmoredisentangledrepresentations,accordingto
Personalization(P)
𝛽-VAE[12]approach,weaddedahyperparameter𝛽 totheKLdi- 5 Explanationperfectlyalignswithuserpreferences,demon-
vergenceterm,rewritingtheformulaasfollows: stratingdeepunderstandingofindividualtastes.
4 Explanationshowsstrongalignmentwithuserpreferences,
L ELBO(𝑥,𝛽)=E 𝑞(𝑧,𝑐|𝑥) [log𝑝(𝑥|𝑧)]−𝛽·KL(𝑞(𝑧,𝑐|𝑥)||𝑝(𝑧,𝑐)) withminormisalignments.
(16)
3 Explanationmoderatelyalignswithuserpreferences,with
where𝛽balancesthereconstructionlossandtheKLregularization
somenoticeablemisalignments.
term.TheKLregularizationtermisderivedas: 2 Explanationshowsweakalignmentwithuserpreferences,with
significantmisalignments.
KL(𝑞(𝑧,𝑐|𝑥)||𝑝(𝑧,𝑐)) 1 Explanationshowsnoalignmentwithuserpreferences.
=−
21∑︁ 𝑐𝐾
=1𝛾 𝑖,𝑐
𝑑∑︁𝐷 =1(cid:32) log(𝜎¯𝑐,𝑑)2+(cid:18) 𝜎𝜎 ¯𝑐𝑖, ,𝑑 𝑑(cid:19)2 +∑︁ 𝑐𝐾
=1𝛾 𝑖,𝑐log
𝛾𝜋 𝑖𝑐 ,𝑐(cid:33)
(17)
Te 5xtQua
T
nl
e
oi xt ety
ri
r(
s
oQ
e
r)
x s.ceptionallyclear,coherent,andwell-structured,with
+ 21 𝑑∑︁𝐷 =1(cid:16) log(cid:0)𝜎 𝑖,𝑑(cid:1)2(cid:17) − 21∑︁ 𝑐𝐾 =1𝛾 𝑖,𝑐 𝑑∑︁𝐷 =1(cid:18)(𝜇 𝑖,𝑑 𝜎¯− 𝑐,𝑑𝜇¯𝑐,𝑑)(cid:19)2 + 21 4
3
T Ter ee r xx o tt r ii s ss . gcl ee na er ra an llyd cc lo eh ae rr be un tt, hw asit sh om mi en ii sm sua el sst wru itc htu cr oa hl ei rs es nu ce eso or
r
structure.
Inthisformulation,𝐾representsthenumberofclusters,𝐷isthe
2 Texthasnotableissueswithclarity,coherence,orstructure.
dimensionalityofthelatentspace,𝛾 𝑖,𝑐 istheprobabilitythatthe 1 Textisunclear,incoherent,orpoorlystructured.
𝑖-thsamplebelongstocluster𝑐,and𝜋 𝑐 isthepriorprobabilityof
UserSatisfaction(U)
cluster𝑐.Theparameters𝜇 𝑖,𝑑 and𝜎 𝑖,𝑑 arethemeanandstandard 5 Explanationishighlyengaging,likelytopromptimmediate
deviationofthe𝑖-thsampleinthed-thdimension,while𝜇¯𝑐,𝑑and
userinteractionoraction.
𝜎¯𝑐,𝑑arethemeanandstandarddeviationofcluster𝑐inthed-th
4 Explanationisengagingandlikelytopromptuserinteraction.
dimension. 3 Explanationissomewhatengaging,maypromptuserinterac-
ByoptimizingthisELBO,GaVaMoEcaneffectivelylearnastruc- tion.
turedlatentspacethatcapturesbothuser-iteminteractionsand 2 Explanationisunlikelytopromptuserinteraction.
userclusteringinformation,whichiscrucialforthesubsequent 1 Explanationislikelytodiscourageuserinteraction.
multi-gatingmechanismandpersonalizedexplanationgeneration.
B HumanscoringCriteria Received20February2007;revised12March2009;accepted5June2009
Thisguideprovidesdetailedinstructionsforevaluatinggenerated
explanationsona1-to-5scaleacrossthreedimensions:Personal-
ization,TextQuality,andUserSatisfaction:
• Personalization(P):Considertheuser’spastinteractions,
statedpreferences,andbehavioralpatternswhenassessing
alignment.Lookforspecificmentionsofuser-relevantfea-
turesorexperiences.
• TextQuality(Q):Evaluatetheexplanation’sgrammatical
correctness,logicalflow,andappropriateuseoflanguage.
Considerwhetherthetextwouldbeeasilyunderstoodby
theaverageuser.