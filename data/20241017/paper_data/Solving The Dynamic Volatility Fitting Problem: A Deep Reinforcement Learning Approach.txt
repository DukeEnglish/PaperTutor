Solving The Dynamic Volatility Fitting Problem:
A Deep Reinforcement Learning Approach
Emmanuel Gnabeyeu ∗ Omar Karkar † Imad Idboufous ‡
London, UK. London, UK. London, UK.
October 16, 2024
Abstract quasi-orthogonal parameters. This functional form,
The volatility fitting is one of the core often referred to as volatility parametrization, allows
problems in the equity derivatives business. users to describe the dynamic properties (movements
Through a set of deterministic rules, the and deformations) of the implied volatility surface in
degrees of freedom in the implied volatility an easy way and helps in generating predictive signals
surface encoding (parametrization, density, for pricing and risk management of equity derivatives.
diffusion)aredefined. Whilstveryeffective,
this approach widespread in the industry is The process through which the coefficients of the
not natively tailored to learn from shifts in volatility parametrization are determined is called
market regimes and discover unsuspected calibration and is often performed automatically 1
optimalbehaviors. Inthispaper,wechange by an algorithm tailored to optimize pre-determined
the classical paradigm and apply the lat- objective functions. For instance, most algorithms aim
est advances in Deep Reinforcement Learn- at matching the mid implied volatility at all expiries
ing(DRL) to solve the fitting problem. In where there is a visible market under non-arbitrage
particular, we show that variants of Deep constraints. Thefittingalgorithmsusedintheindustry
DeterministicPolicyGradient(DDPG)and often account for large sets of conditions such as
SoftActorCritic(SAC)canachieveatleast liquidity, presence of outlier quotes or even earnings
as-goodasstandardfittingalgorithms. Fur- and macro events however all those algorithms have
thermore,weexplainwhythereinforcement native rigidity in them as they follow preset rules for
learningframeworkisappropriatetohandle every market conditions. By design, the ”deterministic
complex objective functions and is natively algorithms” cannot find optimal solutions outside
adapted for online learning. of their definition scope and cannot re-use past
accumulated experiences as every output is flashed
out.
Keywords: Volatility Fitting, Continuous State
Action Spaces, Stochastic and Continuous Control,
In this paper, we look at the fitting problem from
Actor-Critic, Sequential Decision Making and Deep
the angle of Reinforcement Learning (RL). In this
Reinforcement Learning in Stochastic Environment.
new paradigm, we assimilate the fitting-algorithms to
agents evolving in a stochastic environment where: (a)
1 INTRODUCTION states are the collection of observable market quotes
and anterior volatility surfaces 2 (b) actions are the
The implied volatility, which is one of the most bumps to apply in order to shift the prior volatility
important risk factors in the equity business, is usually parametrization and (c) rewards are the opposite of
encoded in a functional form with a limited set of welldesignederrors. ByadoptingaRLframework,one
∗emmanuel.gnabeyeu-mbiada@polytechnique.edu
1Itcanalsobeperformedonalessregularbasisdepending
†omar.karkar@citi.com
ontheliquidityoftheasset.
‡imad.idboufous@citi.com
2Statescanalsoaccountforspotand/orforwardmovements.
1
4202
tcO
51
]PC.nif-q[
1v98711.0142:viXraAdvanced Volatility Fitting
can benefit from the native exploration/exploitation 3. To keep the action space small, we restrict ourselves
trade-off where optimal actions are taken with some to 3 parameters and consider extending to a larger
margin for exploration that could unveil new optimal set in a future study. Using 3 parameters is often
decisions. Furthermore, in continuous states (which not enough to fit the market properly, as there are
is the case for the volatility fitting problem), one can not enough degrees of freedom to sufficiently bend
heavilybenefitfromreplaybuffers(see. B.3)whichare the model implied volatility, this is why we will be
massive logs with historical or synthetic experiences. adding to the RL results a benchmark generated from
Finally, by learning in a stochastic environment where a classical optimizer.
marketquotesarecontinuouslyevolvingRLagentscan
learn to track market dynamics and better stabilize Traditionally, once the vehicle encoding the im-
the volatility parametrization coefficients. plied volatility is selected, the choice of the target is
defined. Inmostcases, practitionersaimatpositioning
Our findings show that the RL framework offers the model implied volatility at the mid 4 factoring
a powerful and adaptative approach to solving the into account several effects such as the presence of
continuous fitting problem. Leveraging Actor-Critic arbitrage, smoothness of the term-structure, stability
RL based techniques, we observed that the RL agent between consecutive fits and so on. A careful selection
was capable of adjusting to the evolving market of the reward is thus important when translating
dynamics with minimal intervention, thus offering a the classical model-based approach into a model-free
more flexible and adaptable solution than traditional reinforcement learning architecture. The fitting logic
deterministic models. itself is driven by a set of deterministic rules where
market data is massaged to generate aggregated
The remainder of the paper is organized as fol- quotes, the parametrization coefficients adjusted to
lows. In section 2 , we present the fitting problem, approach the mid volatilities for some strikes and the
discuss traditional approaches and introduce our main output coefficients set cleaned from any bias induced
RL alternative. Section 3 provides some foundational by the numerical method.
concepts and background on RL techniques with a fo-
cus on actor-critic methods, while section 4 outlines A first and natural alternative would be to con-
our modelling of the state-action space and details our sider the volatility fitting problem as a predictive
proposed methodology. In section 5 we introduce some exercise where the input x is the collection of all
illustrativetoyexamplestoshowcasehowtheRLagent available information at time t and the output ∆y
adapts in environments of varying complexities. This is the variation of the parametrization’s parameters
is followed by the numerical results and their analysis between t and t+ϵ. In such setting, the goal is to
in section 6. approximate the application ϕ such that ∆y = ϕ(x).
A very famous family of approximators are the feed
forward neural networks which can learn large set of
2 VOLATILITY FITTING
functions. While it is a viable and intuitive approach,
Many methods exist in the industry to encode the solving the problem with such design has several
implied volatility information extracted from the downsides: on the one hand, the training set used to
market. For example, some practitioners can use calibrate the weights of the neural networks captures
closed or semi-closed parametric forms to define the solely a finite number of market regimes. This would
variance (or the volatility) at every strike (see for require either a frequent re-calibration or training
example [27]). Other approaches consist in modeling from the user to track changes in market regimes or
the implied density of the asset directly (see [26]) or an immense dataset to expand the coverage of the
even use a diffusion style model to fit the market at training set. On the other hand, labeling all the inputs
every expiry. x assumes implicitly a choice of the objective function
For a survey of methodologies for constructing implied (for example low error to the mid). This means the
volatility surface (IVS) in practice, the reader can refer calibrated neural network cannot be re-used when the
to [30]. user has a different objective function in mind 5 .
In this paper, we restrict ourselves to the modeling
of the variance using a parametric function. The 3Somemarketprovidersuseparametrizationswithupto20
techniques developed to solve the calibration of the coefficients.
volatility surface can be perfectly scaled to all the
4Themidiscomputedfromtheaggregatedbid/askvolatilities.
5Theusercouldbuilddifferentneuralnetstoovercomethis
other parametrization choices. Parametrization of the
limitationbutitalsomeanshavingdistincttrainingsetswhich
variance can take small tovery large sets of coefficients couldbaretimeandmonetarycostchallenges.Advanced Volatility Fitting
The second alternative, which we introduce in Denoting by γ ∈ [0,1] a discounting factor, it is
this paper, is to swap the ”deterministic algorithms” standard to define the return from a state as the sum
with the Deep Reinforcement Learning framework. of discounted future reward from following the policy
On the one hand, the actor neural network ϕ π:
reward
provides a direct link between the inputs x and the
output ∆y. On the other hand, the replay buffer is
T
a dynamic training set which can swiftly track the R =Rπ(s )=(cid:88) γ(i−t)r(s ,a ). (1)
t t i i
market evolution. Furthermore, when the objective
i=t
function considered for the fitting is complex (e.g:
desk PnL) and the standard optimizer too heavy to Likewise, we define the state-action value function 7 or
run 6, the RL approach can learn how to interactively Q-value as:
adjust the volatility surface through intelligent cues
(the rewards) freely available (e.g: in risk systems).
Qπ(s,a)=Eπ[R |s =s,a =a] (2)
As we will showcase in the coming sections, the t t t
Deep Reinforcement Learning (DRL) techniques T
(cid:88)
are effective and can produce the same results as = E (si,ai)∼ρπ[γ(i−t)r(s i,a i)]. (3)
standard optimizers. They can be trained offline (for i=t
example outside market hours) or online after an and the value or state value function:
initial warm-up phase.
Vπ(s)=Eπ[R |s =s]=max Qπ(s,a). (4)
t t a∈A
TheobjectiveofReinforcementLearningistolearn
3 PRELIMINARIES
a policy π (i.e. a sequence of actions over time) which
maximizestheexpectedsumofdiscountedrewardfrom
3.1 Basics On Reinforcement Learning
the system (value functions). In the case of Markov
Reinforcement Learning refers to a goal-directed learn- Decision Processes (MDP), an agent chooses its next
ing and decision-making process where an agent acting action a =π(s ) according to its policy π depending
t t
within an evolving system, learns to make optimal on its current state s ; it then gets into a next state
t
decisions through repeated experiences gained by in- s = E(s ,a ), depending on the response of the
t+1 t t
teracting with the environment without relying on su- environment and gets the instantaneous reward r =
t
pervision or complete model. (See Bertsekas(2005) [3], r(s ,a ) 8.
t t
Sutton & Barto(2018) [4]). That agent reinforced with awards based on its
We consider a stochastic environment E, assumed interaction with the environment would learn to op-
fully-observed and thus formulate the problem of deci- erate optimally in that environment to maximize the
sion making of an agent acting within an environment cumulative rewards (trials and errors).
overafinitetimehorizon,asaMarkovDecisionProcess The problem is thus formulated mathematically by
(S,A,r,p), where at each time, the agent observes a defining the optimal value function for each state s as
currentstates fromthecontinuousstatespaceS ⊂Rd, follow:
t
takes an action a based on that current state from
t (cid:20) T (cid:12) (cid:21)
the continuous action space A ⊂ Rn and receives a V∗(s)=supVπ(s)=supEπ (cid:88) γ(i−t)r(s i,a i)(cid:12) (cid:12)s
t
=s .
scalar reward r t denoting numerical feedback from the π π i=t (cid:12)
stochastic scalar-valued reward function r =r(a t,s t) . (5)
The transition dynamics is given by p(s |s ,a )
t+1 t t
andrepresentstheprobabilitydensityofthenextstate. The standard approach to solve this optimization
Theagentbehaviourisdefinedbyapolicyπi.e. actions makes use of the well-known Dynamic Programming
are drawn from π : S → A (resp. P(A)) for a deter- Principle (DPP) to derive the following variation of
ministic policy (resp. for a randomized policy which the recursive linear Bellman equation:
maps the current state s to a probability distribution
t
over the action space). (cid:20) (cid:21)
Wewilluseρ π(s t)andρ π(s t,a t)todenotethestate V∗(s)= sup E[r(s,a)]+γE s′∼p(s′|s,π(s))[V∗(s′)]
and state-action marginals of the trajectory distribu- a∈A
(6)
tion induced by a policy π.
74and2provideapredictionofhowgoodeachstateoreach
6Multiplerepricingcanberequiredintheoptimizationrou-
state/actionpairis.
tines.
8ThefunctionsE ,randπ mayhaverandomcomponents.Advanced Volatility Fitting
It follows from the literature that, in order to solve equation:
thisproblem,wecanperformeithervalueiteration[15],
Q(s ,a )=r(s ,a )+γQ(cid:0) s ,πD(s ,θπ)(cid:1) ,
policy iteration [9, 18] or a combinasion of the two in t t t t t+1 t+1
actor-critic methods [12].
The actor is trained to maximize the critic’s es-
timated Q-values by back-propagating through both
3.2 Deep Reinforcement Learning Al- networks.
gorithms To encourage exploration, it uses the following ac-
tion (stochastic policy):
Combining the above-mentioned ideas with Deep
Learning has lead to significant performance im- a ∼πD(s ,θπ)+N (7)
t t
provement and the spreading of the so called Deep
where N is either an Ornstein-Uhlenbeck (Ornstein
Reinforcement Learning (DRL).
and Uhlenbeck, 1930) noise process N = OU(0,σ2)
10 (correlated) or an additive Gaussian noise N =
In fact, when we are in the setting of continuous,
N(0,σ2I)(uncorrelatedandtime-independent),chosen
high dimensional action spaces, it is known from
according to the environment11. It thus treats the
the literature [1] that classical tabular value-based
problemofexplorationindependentlyfromthelearning
and policy-based methods are challenging, we rather
consider parametrized value functions Q(s,a;θQ) and algorithm.
V(s,θV) or policies (π(s,a;θπ)) and then use Neural
Networks as function approximators.9. There are
several popular neural network architectures amongst
which the fully-connected neural networks used in this
study and for which the reader is invited to refer to
the appendix B.1 and B.2 for more details.
The combination of value-based and policy-based
methods leads to Actor-Critic algorithms. This class Figure 1: DDPG Framework.
of RL algorithms alternates between policy evalua-
tion by computing the value function for a policy and
However,thisalgorithmisknowntosufferfromhigh
policy improvement by using the value function to
sample complexity and sensibility to hyper-parameters
obtain a better policy. It can be extended to neural 12. Subsequently, we also consider an extension known
Actor-Critic algorithms by using neural networks for
as SAC.
functional approximations. Deep Deterministic Pol-
icy Gradient (DDPG) and Soft Actor-Critic (SAC)
Soft Actor Critic (SAC): It is an off-policy actor-
algorithms are part of that class of RL algorithms
critic deep RL algorithm based on the maximum en-
whichhavedemonstratedstate-of-artperformanceona
tropy reinforcement learning framework. It was intro-
range of challenging decision-making and control tasks.
ducedin[11]andextendstheDDPGsettingbyallowing
Those algorithms use a replay buffer to improve the
the actor to maximize the expected reward while also
performance of neural networks (see the B.3 for more
maximizing the entropy. The off-policy formulation
details).
enables reuse of previously collected data for efficiency,
and the entropy maximization enables stability and
Deep Deterministic Policy Gradient (DDPG): provides a substantial improvement in exploration and
It is a model-free off-policy Actor-Critic algorithm, robustness. It favors stochastic policies by considering
first introduced in [10], which combines the Deep Q- a more general and augmented entropy objective:
Network (DQN) [25] and Deep Policy Gradient (DPG)
algorithms. It extends the DQN to continuous action
T
spaces by incorporating DPG to learn a deterministic (cid:88)
J = E [γ(i−t)(r(s ,a )+αH(π(·|s ))].
strategy πD. The critic estimates the Q-value func- π (si,ai)∼ρπ i i i
i=t
tion using off-policy data and the recursive Bellman
(8)
9Thanks to the universal approximation theorem, Neural √
networksareknownfortheirabilitytoapproximateawiderange
10xt=xt−1+θ(µ−xt−1)dt+randn(size(µ))σ dt.
11WewillusedanadditiveGaussiannoisewithadecreasing
of non-linear functions in high dimension and to fit to many
standarddeviationtodampenexplorationastheagentislearning
non-linearregressiontask[34,31,32]
theoptimalpolicy.
12Wewitnesseditbychangingdifferentshyperparameters.Advanced Volatility Fitting
π∗ =argmaxJ . a =f (ϵ ;s )=µ (s )+ϵ σ (s ), (9)
π t ϕ t t ϕ t t ϕ t
π∈Π
where ϵ is an input noise vector such as a spherical
where H is the entropy functional, Π a tractable t
Gaussian (ϵ ∼N(0,I) )
family of distributions and α is the temperature pa- t
Choosing the optimal temperature is non-trivial
rameter which determines the relative importance of
since it needs to be tuned for each task. We overcome
the entropy term against the reward, and thus controls
this issue as proposed in [13] by formulating a different
the stochasticity of the optimal policy.
maximum entropy RL objective, where the entropy is
We want to project the improved policy into the
treated as a constraint.
desired set of policies in order to account for the con-
Our aim is to find a stochastic policy with maxi-
straint that π ∈Π. We will rather use the information
malexpectedreturnthatsatisfiesaminimumexpected
projection defined in terms of the Kullback-Leibler
divergence and redefined the actor loss function 13. entropy constraint. Formally, we need to solve the
entropy-constrained maximum expected return objec-
Let’s denote by Q the critic network, parameter-
θ
tive problem.
ized by θ and π the actor network, parameterized by
ϕ
ϕ which outputs a probability distribution over the
action space i.e. the best action to take from state (cid:34) T (cid:35)
(cid:88)
s πt (i ·s |sth ).en sampled from the probability distribution m πta :TxE ρπ γ(i−t)r(s i,a i)
ϕ t i=t
We redefined the policy loss as the KL divergence s.t. E [−log(π (a |s ))]≥H¯ ∀t
or the gap between the probability distribution over
(st,at)∼ρπ t t t
theactionspaceproposedbytheactornetworkandthe where H¯ is a predefined or desired minimum policy
probability distribution induced by the exponentiated expected entropy threshold 16.
current Q function normalized by the factor Z 14: We use the Lagrangian relaxation[33] to transform
θ
In fact, the high values of the latter indicates the the entropy constraint into a penalty, yielding an ex-
areasoftheactionspacewherethecumulativeexpected pression that is easier to solve.
sumofrewardsisapproximatedtobehigh, minimizing We solve the optimal dual variable (also known
the KL divergence means getting an efficient actor as “Lagrange multiplier”) at different steps backward
network associated to actions yielding highly rewarded in time, α∗ (the optimal temperature at step t) after
t
trajectories. solving for Q∗ and π∗:
t t
With the definition of the KL divergence, we have: By repeating this process, we can learn the optimal
temperature parameter in every step by minimizing
(cid:34) (cid:32) (cid:12) (cid:12)(cid:12)
(cid:12)
exp(cid:0)1Q
(s
,·)(cid:1)(cid:33)(cid:35) the same objective function:
J π(ϕ)=E st∼D D KL π ϕ(·|s t) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12)
(cid:12)
Zα θ(sθ t)t α t∗ =argm αi tnE
at∼π
t∗(cid:2) −α tlogπ t∗(a t|s t;α t)−α tH¯(cid:3) .
(10)
(cid:34) (cid:35)
(cid:90) π (a |s )
=E π (a |s )log ϕ t t da
st∼D at ϕ t t exp(cid:0) α1Q θ(s t,a t)(cid:1) /Z θ(s t) t Inthefinanceindustry,newideascomingfromrein-
(cid:104) (cid:105) forcementlearninghavebeendevelopedandadaptedto
=E E [αlogπ (a |s )−Q (s ,a )] +Cste
st∼D at∼πϕ(·|st) ϕ t t θ t t financial problems with many successes. For surveys in
the literature on RL applications in finance, the reader
Weuseare-parametrizationtrickfortheprobability
is invited to refer to [1, 7, 23, 17, 19, 20, 21]. This has
law according to which actions are drawn: 15 i.e.
attracted a lot of attention in applying RL techniques
13We choose KL divergence to project the improved policy toimprovedecision-makinginvariousfinancialmarkets
intoΠ. Inprinciplewecouldchooseothersprojectionsamong and thus has instigated the following natural extension
whichtheWassersteinprojection.
to the volatility fitting problem.
14Thepolicy’sdensityhastheformoftheBoltzmanndistribu-
tion,wheretheQ-functionservesasthenegativeenergy. Itcan
beshowthatitisanoptimalsolutionforthemaximum-entropy
16wechoosetheentropytargetH¯ tobe-dim(A)asproposed
RLobjective8.
15Insteadofthestandardlikelihoodratiomethod. Thismay bythearticle.
resultinalowervarianceestimatorandstillisanopenquestion
onwhichnoconsensusisfoundintheliterature.Advanced Volatility Fitting
4 MODELING SPECIFICATION parameters denoted by θ⃗ = (cid:0) θ1,...,θK(cid:1) . The
ti ti ti
market is moving over time, and at time t , this
i+1
In this section, we frame the fitting problem into an fitting problem consists of finding the optimal vector
RL setting, translating the classicaldefinitions into the θ⃗∗ which maximizes an objective function. The
reinforcement learning framework. We conclude this pt ai+ ra1 metrisation Ψvol is classically optimized to be
section by detailing the implementation of the Actor- close to the mid θ volatilities (cid:8) σMid(t ,κ )(cid:9)
criticalgorithms(DDPGandSAC)andexplaininghow i j j∈[1,...,n]
they were adapted to the fitting game. with σMid(t i,κ j) = σAsk(ti,κj)+ 2σBid(ti,κj) (See Figure
2 ) .
4.1 Problem Statement In Reinforce-
In the RL framework, we leave the problem open
ment Learning Framework
to more than just matching the mid as much as
Parametrisationandmarketobservables: Letus possible. In fact, our goal is to maximize a certain
fixamaturityTandobservethemarketattimet ,i.e. a reward defined in term of an objective function: with
i
(cid:110) (cid:111) a market move at time t , the challenge for an
collection σAsk (t ,κ ),σBid (t ,κ ) i+1
Call/Put i j Call/Put i j j∈[1,...,n] agent is to bump the parameters of our volatility slice
of markets quotes for different moneyness (κ 1,...,κ n). parametrization Ψvol in order to maximize a certain
θ⃗
reward.
Subsequently,forarangeofmoneyness,andstarting
from a prior parametrized volatility surface Ψvol ( i.e.
θ⃗
ti
the latest fit is the prior input), the problem is to find
the shifted vector ∆θ⃗ =(cid:0) ∆θ1,...,∆θK(cid:1) to bump the
ti ti ti
old parameters θ⃗ in order to maximize our selected
ti
reward function.
State and Action spaces: Our state is the set of
tuple consisting of the observed markets quotes and
the prior volatility parameters:
(cid:16) (cid:17)
s = σBid(t ,κ ),σAsk(t ,κ ),θ1 ,...,θK ∈RN
ti i j i j ti−1 ti−1
j∈[1,n]
(with N ≥ 2n+K), where we consider the bid and
ask market implied volatility, i.e. the state space is
continuous 18.
From the RL perspective, the fitting cycle looks as
follows: At time t− ,
i+1
1. The agent sees the state s of our environment
ti+1
built on the market bid and ask volatilities (ho-
mogeneousvariable)andtheoldvolatilitysurface
Figure2: Asnapshotoftheagentactinginthemarket:
parameters θ⃗ (endogenous variable),
The old volatility surface is bumped to a new one ti
following the move of the market, conditional on prior 2. It takes some action a =∆θ⃗ =θ −θ
volatility surface.
ti+1 ti+1 ti+1 ti
on the adjustment of the old parameters θ⃗ , and
ti
receives some reward r(s ,a =∆θ⃗ ).
We consider a setting with a parametrization
ti+1 ti+1 ti+1
Ψvol 17 of the volatility slice with a vector of K 3. The resulting volatility slice maximizes the re-
θ⃗
ward and it boils down a new state s for the
17WecanthinkabouttheStochasticvolatilityinspired(SVI)
next move.
ti+2
parameterizationofthetotalimpliedvarianceforafixedtime
tomaturityproposedbyGatheral[28,26,29]. 18We may also rather consider
(cid:16) (cid:17)
ΨS θ⃗VI(k)2=a+b(ρ(k−m)+(cid:113) (k−m)2+σ2) Nσ ≥Mi 2d n(t +i, Kκj )) ,, wσ hsp erre ea σd s( pti r, eaκ dj () t, iθ ,t1 κi− j)1, :=..., σθ AtK i s− k1 (ti,j κj)∈ −σR BN id(t( iw ,κit jh
)
wherek isthelog-forwardmoneyness,andθ⃗=(a,b,ρ,m,σ). isthebid-askvolatilityspread.Advanced Volatility Fitting
The error term between our volatility surface and preference (decaying the standard deviation of the ex-
the mid market is denoted ξ(θ⃗ ) . ploration noise and the temperature coefficient with
ti+1
Since the vector of bumps ∆θ⃗ ( resp. the actions) steps as we draw close to the optimal reward). We also
can take any values in RK ( resp. RK) space, it draws choose in the static and sequential scenarios to store
down a continuous actions spaces. only transitions which improve the replay memory (i.e.
for which the reward is greater that the worst reward
in the replay buffer21).
Reward functions: We use two metrics to measure
the goodness of the fit: 19 Note that the sampled minibatch from the replay
buffer in our algorithms is a state-action-reward-next
• Mean squared error (MSE) defined by : state tuple of the form
n (s ,a ,r ,s ),...,(s ,a ,r ,s ).
ξ(θ⃗ ):=(cid:88) (σMid(t ,κ )−Ψvol(k ))2 (11) t1 t1 t1 t2 tK tK tK tK+1
ti i j θ⃗
ti
j
Using the observed markets quotes and the prior
j=1
volatility parameters to represent the market state
It is used to penalise larger deviations from the present the challenge of feature scaling regarding their
true values, making it an effective metric for order of magnitude. In this context, unlike supervised
assessing the quality of fit. learning, wecannotsimplyre-scaledthefeaturesinthe
naivewaysincethea-prioridistributionofstatesunder
• Black Scholes Vega weighted Mean squared error
the optimal policy is unknown. We used batch nor-
(BMSE) defined by :
malization (dynamic feature scaling or running means
and standard deviation) of states as solution to this
n
ξ(θ⃗ ):=(cid:88) vega (κ )(σMid(t ,κ )−Ψvol(κ ))2 problem.
ti BS j i j θ⃗
ti
j
The critic and actor networks are then updated
j=1
(12) accordingtothesesampleswithanormalizationoflayer
inputs as proposed in [24] to reduce the internal co-
It is used to penalise markets quotes located at variate shift. The reader should refer to the appendix
the neighbourhood of the at-the-money region. D for the hyper-parameters used for the training.
Remark: We can also use the Scaled Mean squared
Deep Deterministic Policy Gradient (DDPG):
error (SMSE) 20 to give more importance to market
In our setting (see section 5 ), we perform exploration
quotes with small bid-ask spread.
using a Gaussian Noise (GN) instead of Ornstein Uh-
Withinallthesecases, wedefinedtherewardasthe
lenbeck (OU) action noise as GN is time-independent
opposite of that error term:
r(s ,a =∆θ⃗ )=−ξ(θ⃗ ) while OU noise presents some auto-correlation.
ti ti ti ti
4.2 Algorithms Overview For Volatility
Fitting
In this setting, the state-action value can be written:
T
Qπ(s ,a )=−(cid:88) E [γ(k−ti)ξ(θ⃗ )] (14)
ti ti (sk,ak)∼ρπ k
k=ti
We adapt the framework of DRL technique and
provide a natural extension of DDPG and SAC to the
volatility fitting problem, and adjust our exploration
Figure 3: Replay Memory and Noise in the DDPG
19Wecanalsodesignarewardfunctionaltakingintoaccounta
framework in the static and sequential toy problems.
setofconstrainsliketheliquidity,macro-events,term-structure,
stability.
20Settingσspread(ti,κj):=σ tA is ,κk j−σ tB i,i κd
j
thevolatilityspread, We choose to dampen exploration by powerly de-
wedefinetheSMSE: caying the standard deviation of the Gaussian Noise
ξ(θ⃗ ti):= j(cid:88) =n 1(σMid σ(t si p, rκ eaj d) (− ti,Ψ κv θ⃗ jto )il(κj) )2 (13) (F 2i 1g s. ee3 5) .1.
and5.2. Inthedynamicscenario,wejustremovethe
oldesttransitioninthereplaymemoryAdvanced Volatility Fitting
Algorithm 1 DDPG variant for Volatility fitting Soft Actor Critic (SAC): We use two soft Q-
function to mitigate positive bias in the policy im-
1: Input: Randomly initialize an actor network
πD(s;θ), a critic network Q(s,a;ϕ),with param- provement step. We also learn the optimal tempera-
eters θ(0) and ϕ(0) ture coefficient at each step by minimizing the dual
2: Initialize target networks π¯D and Q¯ with parame- objective in Equation 10
ters ϕ¯(0) ←ϕ(0) and θ¯(0) ←θ(0),
3: Initialize the replay buffer RB Algorithm 2 SAC variant for Volatility Fitting
4: for n=0,...,N −1 do 1: Input: Randomly initialize an actor network
5: Initialize a gaussian process N for action explo- π(s;ϕ), two critics networks Q(s,a;θ ) and
1
ration and fix LearningFlag= True. Q(s,a;θ ), with parameters ϕ(0), θ(0) and θ(0).
2 1 2
6: Initializestates 0withaflatvolatilityparameters Initialize α(0) and set H¯ =−dim(RK)=−K
7: for t=1,...,M do 2: Initialize target networks Q¯ with weights
8: Select the bumps a t ∼ πD(s t;θ(n))+ϵ t with θ¯(0) ←θ(0) and θ¯(0) ←θ(0),
ϵ ∼N(0,σ ), σ =max(σ (1− n)4,σ ) 1 1 2 2
t n n 0 N min 3: Initialize an empty replay pool
9: ExecutethebumpsπD(s t;θ(n))(deterministic) RB ←∅ and fix LearningFlag= True.
and a t(exploration), then receive reward r tD, 4: for n=0,...,N −1 do
r t and observe new state s t+1 5: Initializestates 0withaflatvolatilityparameters
10: SmartlyStorethetransition(s t,a t,r t,s t+1)in
RB , (see Fig.3) 6: for t=1,...,M do
11: if LearningFlag then 7: Sample the vector of bumps from the policy
12: Sample a random mini-batch of N batch tran- a t ∼π ϕ(a t|s t):=µ ϕ(s t)+ϵ tσ ϕ(s t)
sitions {(s (i),a (i),r (i),s (i+1))}N i=b 1atch from 8: Execute the bumps µ ϕ(s t) (deterministic) and
RB a (exploration) , then receive rewards rD, r
t t t
13: Compute the target and observe new state s ∼p(s |s ,a )
Y i =r i+γQ¯(s i+1,π¯D(s i+1;θ¯(n));ϕ¯(n)). 9: Store the transition in tht+ e1 replayt p+ o1 olt t
14: Update the critic by minimizing the loss: RB ←RB∪{(s ,a ,r(s ,a ),s )}
t t t t t+1
ϕ(n+1) =ϕ(n)−β∇ ϕL DDPG(ϕ(n)) 10: if LearningFlag then
15: Update the actor by using the sampled pol- 11: Sample a random mini-batch of N batch tran-
icy gradient: θ(n+1) = θ(n) + α∇ θJ(θ(n)) sitions {(s ,a ,r ,s )}Nbatch from
(i) (i) (i) (i+1) i=1
RB
16: Update the target networks via polyak av- 12: Compute the target y i =
eraging: r +γ(cid:0) Q¯ (s ,a )−α(n)logπ(a |s ;ϕ(n))(cid:1) .
i θ i+1 i+1 i i
13: Update the Q-function parameters: 24for
j ∈{1,2} θ(n+1) ←θ(n)−λ ∇ˆ J (θ(n))
ϕ¯(n+1) ← τϕ(n+1)+(1−τ)ϕ¯(n) j j Q θj Qj j
14: Update policy weights: 25
θ¯(n+1) ← τθ(n+1)+(1−τ)θ¯(n) ϕ(n+1) ←ϕ(n)−λ ∇ˆ J (ϕ(n))
π ϕ π
15: Adjust temperature:
17: end if α(n+1) ←α(n)−λ∇ˆ J(α(n))
18: Update LearningFlag wrt to a preset crite- α
rion22 16: Update the target networks weights via
polyak averaging:
19: end for
θ¯(n+1) ←τθ(n+1)+(1−τ)θ¯(n) forj ∈{1,2}
20: end for j j j
17: end if
18: Update LearningFlag wrt to a criterion26
∇ J(θ)≈ 1 (cid:80) ∇ Q(s,a;ϕ)∇ πD(s;θ)|
θ Nbatch i a θ s=si,a=ai 19: end for
23
20: end for
L DDPG(ϕ)= Nba1
tch
(cid:80)N i=b 1atch(Y i−Q(s i,a i;ϕ))2 Ensure: Optimized parameters θ 1∗, θ 2∗ and ϕ∗
22r tD > R0 (in the static and sequential toy case) or r tm >
rR m0( tin heth me ead nyn ra ewm aic rdca ofse t) hewh eve are luR at0 ionis ea pir se ow da er (d seeth 5r .e 3sh ).old and 24by minimizing the MSE loss against Bellman backup
t 23thisanalyticalgradientisuselessinpracticewithcommon JQj(θ j(n))= Nba1
tch
(cid:80)N i=b 1atch(yi−Q¯(si,ai;θ j(n)))2forj∈{1,2}
deeplearningframeworkssuchaspytorchsincetheselibraries 25byminimizingtheEntropy-regularizedpolicyloss:
aresuchthatwritingthepolicylossisenough: thegradientis Jπ(ϕ)=E st∼D(cid:2)E ϵ∼N(0,I)[αlogπ ϕ(f ϕ(ϵ;st)|st)−Q θ(st,f ϕ(ϵ;st))](cid:3)
automaticallycomputedwhenthebackwardpassisperformed. Q θ(si,ai)=min(Q(si,ai;θ 1(n)),Q(si,ai;θ 2(n))).
26sameas22Advanced Volatility Fitting
5 INSIGHTS FROM TOY-MARKETS • The success criteria is more subtle as it doesn’t
only consist in approaching a single static termi-
Inordertoassesstheperformanceofthereinforcement nal curve but to have the agent approach and
learningalgorithmsforthevolatilityfittingproblem,we track the market for several steps.
considertoymarketsofincreasingcomplexity. Thedata
used for the toy markets is generated synthetically to In this setting, we perform three successive operations:
reflect real market configurations. As we will detail in
1. Training Phase: we calibrate the hyper-
thecominglines, westartwithasimplescenariowhere
parameters (e.g: learning rates, volatility noise,
market data is static. We then relax this hypothesis to
replay buffer size, etc) to increase the average re-
slowly account for the market dynamics.
ward in the evaluation episodes. The evaluation
episodes are modes where all the randomness of
5.1 Static Scenario
theDRLalgorithmsisremovedandnoupdatesto
the neural networks is performed. This training
In this scenario, the market quotes are static and do
phasehasasidebenefitasithelpstodeterminea
not change during the full experiment. In particular,
stopping threshold (for learning) for our agents.
theinitialstateisaflatvolatilitysurfaceandthefitting
episode ends after one step. The state space is degen-
2. ValidationPhase: inthisphase, withtheoptimal
erated into a single state but the number of actions is
configuration of hyper-parameters and the stop-
infinite. The goal of the agent is to immediately detect
pingcriterionforlearning,wetrainseveralagents
the right parametrization bumps to apply in order to
anduponcompletionoftraining, wecomparethe
approach the market mid.
performance of different successful agents under
different seeds to select the most promising can-
5.2 Sequential Scenario didate.
In this scenario, the market quotes are static and do 3. Testing Phase: we assess the performance of the
not change during the full experiment. In particular, best agent in a test episode.
theinitialstateisaflatvolatilitysurfaceandthefitting
episodeendsafterseveralsteps(e.g: 50steps27 ). Both
6 RESULTS AND ANALYSIS
the state and action spaces are infinite. The goal of
the agent is to detect the right parametrization bumps
In this section, we evaluate the performance obtained
to apply in order to approach the market mid in an by the reinforcement learning algorithms 29 for the toy
episode. Due to the effect of the discount factor, the
problems described in 5 . The algorithms performance
agentisencouragedtodetecttherightparametershifts
is tested against different market configurations that
from the first step.
can be encountered in live trading activity.
5.3 Quasi-Dynamic Scenario
6.1 Static Market
In this scenario, we allow the market to evolve freely
We compare the performance of the DDPG and SAC
during a full episode. In particular, the mids and
(averaged across multiple random seeds) against a clas-
spreads (for different moneyness) are random quanti-
sical optimizer (Benchmark) in 5.1. We first show
ties with distinct marginals and a common joint distri-
a summary table with the final rewards for different
bution28. The means, variances and correlations are
market configurations before displaying the implied
calibrated using ”real-market” data for a full trading
volatility at convergence.
day. This scenario is a clear cut versus the static and
sequential market as:
MSE Reward Summary:
• The dimensionality of the state space is higher
as the market quotes components is changing at
As can be seen from table 1 and 2, the final
every step.
rewards achieved by the variants of the actor-
27Thisorderofmagnitudeisinlinewiththenumberoffitsthat critic algorithms is close to the one coming from
canhappenbetweentheopenandthecloseifafitisscheduled the optimizer. The performance is stable across
every10mn. the different market configurations considerered.
28Themarginalsareassimilatedtonormaldistributionsand
thejoint-dynamicistreatedasaGaussiancopula 29Note: The DDPG and SAC are variants of the seminal
algorithmsadjustedforthevolatilityfittingproblem.Advanced Volatility Fitting
Table 1: DDPG MSE Rewards 6.1.2 Skew Market (MSE)
type Skew High Smile Inv. Smile
Bench -0.011913 -0.000022 -0.000044
seeds’ avg -0.012145 -0.000163 -0.000315
Table 2: SAC MSE Rewards
type Skew High Smile Inv. Smile
Figure 6: Implied volatility snapshot in a ”Skewed”
Bench -0.011913 -0.000022 -0.000044
configuration with DDPG: A Monte-Carlo on 5 differ-
seeds’ avg -0.011945 -0.000221 -0.001800
ents random seeds is performed with a power decaying
noise. Werepresentin(green)thebestresponseofthe
agent amongst the last 1000 episodes and in (blue) the
mean of the last 1000 volatility slices.
6.1.1 High Smile Market (MSE)
Figure 7: Implied volatility snapshot in a ”Skewed”
Figure 4: Implied volatility snapshot in a ”High Smile” configuration with SAC: A Monte-Carlo on 5 different
configuration with DDPG: A Monte-Carlo on 5 differ- random seeds is performed with automatic entropy
ents random seeds is performed with a power decaying adjustement. Werepresentin(green)thebestresponse
noise. Werepresentin(green)thebestresponseofthe of the agent amongst the last 1000 episodes and in
agent amongst the last 1000 episodes and in (blue) the (blue) the mean of the last 1000 volatility slices.
mean of the last 1000 volatility slices.
6.1.3 Inverse Smile Market (MSE)
Figure5: Impliedvolatilitysnapshotina”HighSmile”
configuration with SAC: A Monte-Carlo on 5 different Figure 8: Implied volatility snapshot in a ”Inverse
random seeds is performed with automatic entropy Smile” configuration with DDPG: A Monte-Carlo on
adjustement. Werepresentin(green)thebestresponse 5 differents random seeds is performed with a power
of the agent amongst the last 1000 episodes and in decaying noise. We represent in (green) the best re-
(blue) the mean of the last 1000 volatility slices. sponseoftheagentamongstthelast1000episodesand
in (blue) the mean of the last 1000 volatility slices.Advanced Volatility Fitting
Table 4: SAC MSE Rewards
type Skew High Smile Inv. Smile
Bench -0.011913 -0.0000220 -0.0000436
seeds’ avg -0.017717 -0.0006141 -0.0018047
6.2.1 Skew Market (MSE)
Figure 9: Implied volatility snapshot in a ”Inverse
Smile” configuration with SAC: A Monte-Carlo on 5
different random seeds is performed with automatic
entropy adjustement. We represent in (green) the best
response of the agent amongst the last 1000 episodes
andin(blue)themeanofthelast1000volatilityslices.
For more details on the black-scholes vega weighted
reward, please refer to the appendix A.1
6.2 Sequential Scenario
We compare the performance of the DDPG and SAC
(averaged across multiple random seeds) against a clas-
sical optimizer (Benchmark) in 5.2. In this scenario,
more than one step is allowed per episode. We first
Figure 10: Episodic agent’s Rewards and Implied
show a summary table with the final rewards for differ-
volatility evolution in a ”Skewed” configuration with
entmarketconfigurationsbeforedisplayingtheimplied
DDPG.
volatility at convergence.
The figure shows that, the agent is detecting the right
parameter shifts from the first step.
MSE Reward Summary:
As can be seen from table 3 and 4, the final rewards
achieved by the variants of the actor-critic algorithms
is close to the one coming from the optimizer. The
performance is stable across the different market
configuration considerered.
Table 3: DDPG MSE Rewards Figure 11: Implied volatility snapshot in a ”Skewed”
configuration with DDPG: A Monte-Carlo on 5 differ-
type Skew High Smile Inv. Smile ents random seeds is performed with a power decaying
noise. We represent in (green) the best response of
Bench -0.011913 -0.000022 -0.000044
the agent amongst the last 50 episodes and in (blue)
seeds’ avg -0.017231 -0.005591 -0.001318
the mean of the final volatility slices for the last 50
episodes.Advanced Volatility Fitting
Figure 15: Implied volatility snapshot in a ”High
Figure 12: Implied volatility snapshot in a ”Skewed”
Smile” configuration with SAC: A Monte-Carlo on 5
configurationwithSAC:AMonte-Carloon5differents
different random seeds is performed with automatic
random seeds is performed with automatic entropy
entropy adjustement. We represent in (green) the best
adjustement. Werepresentin(green)thebestresponse
response of the agent amongst the last 50 episodes and
of the agent amongst the last 50 episodes and in (blue)
in (blue) the mean of the final volatility slices for the
the mean of the final volatility slices for the last 50
last 50 episodes.
episodes.
6.2.2 Smile Market (MSE) 6.2.3 Inverse Smile Market (MSE)
Figure 16: Episodic agent’s Rewards and Implied
Figure 13: Episodic agent’s Rewards and Implied
volatility evolution in an ”Inverse Smile” configuration
volatility evolution in a ”Smile” configuration with
with DDPG.
DDPG.
The figure shows that, the agent is detecting the right
The figure shows that, the agent is detecting the right
parameter shifts from the first step.
parameter shifts from the first step.
Figure 17: Implied volatility snapshot in an ”Inverse
Figure14: Impliedvolatilitysnapshotina”HighSmile”
Smile” configuration with DDPG: A Monte-Carlo on
configuration with DDPG: A Monte-Carlo on 5 differ-
5 different random seeds is performed with a power
ent random seeds is performed with a power decaying
decaying noise. We represent in (green) the best re-
noise. We represent in (green) the best response of
sponse of the agent amongst the last 50 episodes and
the agent amongst the last 50 episodes and in (blue)
in (blue) the mean of the final volatility slices for the
the mean of the final volatility slices for the last 50
last 50 episodes.
episodes.Advanced Volatility Fitting
quantile excluded) during the evaluation process. This
method enables us to identify the optimal set of hyper-
parameters, and we define the stopping criterion for
training as the highest average reward obtained for the
optimal configuration. (see 20).
Figure 18: Implied volatility snapshot in an ”Inverse
Smile” configuration with SAC: A Monte-Carlo on 5
different random seeds is performed with automatic
entropy adjustement. We represent in (green) the best
response of the agent amongst the last 50 episodes and
in (blue) the mean of the final volatility slices for the
last 50 episodes.
For more details onthe black-scholesvega weighted
reward, please refer to the appendix A.2 Figure 19: Consistency of agent performance across
multiple random seeds during the training phase.
Remarks: SACtypicallyconvergesfasterandmore
consistently across different environments. Moreover
the introduction of entropy helps mitigate over fitting
during training. While robust, its require handling
both a stochastic policy and an entropy regularization
term. In our settings where fine continuous control
over actions is important, DDPG version might be a
better choice since it may be more straightforward to
integrate and tune for a quasi dynamic environment
(which might be more sensitive to precise adjustments).
It also has less computational overhead and resource
usage.
6.3 Quasi-dynamic scenario Figure 20: Cumulative mean rewards of evaluation
episodes (for 5 tuples of hyper-parameters in the grid
In this section we present the training, validation and search).
testing phases of RL agents using DDPG algorithm.
Theagentsaretrainedondatageneratedfortwostocks
(one with a wide spread and the other with a tight
spread) across several episodes, each consisting of 50
steps. WhiletheconvergencerateofDDPGcanbeslow
due to its sensibility to hyperparameters, Fine-tuning
is often crucial for successful training.
Training Phase: The agents are trained to maxi-
mize the average reward during evaluation episodes
withinagame. Inthoseevaluationepisodes,allsources
of randomness in the DRL algorithms are turned off
and no updates to the neural networks are made. We
construct a hypercube of hyper-parameters (e.g: learn-
ing rates, volatility noise, replay buffer size, batch
Figure21: Comparisonofmeanepisodicrewardsacross
size, etc). For each tuple or hyper-parameter combina-
multiple random seeds for agents trained with default
tion, we track the evolution of average rewards (25%
versus optimized hyper-parameters.Advanced Volatility Fitting
As illustrated in Figure21, we compute the average We compute the mean reward for each agent, as
evaluation rewards across multiple random seeds to showninthefiguresbelow(22and23). Thisvalidation
identify the optimal set of hyper-parameters and estab- phase allows us to identify agents that successfully
lishthestoppingcriterion. ThecurvesshowninFigure reach or surpass the pre-determined reward threshold.
20 represent the mean results across seeds, ensuring a
robust, stable and consistent process throughout the Testing Phase: In the final testing phase, we select
validation phase. the agent with the best validation performance and
Once the optimal hyper-parameters are deter- assess its behavior in a test episode of 50 steps for
mined, we can compare the mean testing episodic re- different random seed. Figures 24 and 25 display the
wards(averaged across a different set of random seeds) evolution of agent’s rewards and implied volatility(of
for agents trained with default versus optimized hyper- one selected step) during the testing phase for stocks
parameters. Figure21demonstratesthesubstantialim- with wide and tight spreads respectively.
provementachievedwhenfine-tuninghyper-parameters
compared to the default setup.
Validation Phase: upon completion of training, we
evaluate the performance of distinct RL agents using
different random seeds within a validation episode.
Figure 24: Evolution of agent’s episodic rewards and
implied volatility (of one selected step) for a wide
spread stock during testing with DDPG.
Figure 22: Evolution of mean episodic rewards(over
several random seeds) and implied volatility(of one
selected step) for a wide spread stock with DDPG.
Notice that some agents (like Agent 5) don’t reach the
threshold.
Figure 25: Evolution of agent’s episodic rewards and
implied volatility (of one selected step) for a tight
spread stock during testing with DDPG.
As can be seen from figures 24 and 25, the per-
formance of the best agent under both tigh and wide
spread is satisfactory. The agent is able to track the
market through time keeping the reward high (close
to the optimal level for the number of parametriza-
Figure 23: Evolution of mean episodic rewards(over tion coefficients used in the test). It is also important
several random seeds) and implied volatility(of one to highlight that there is also a good margin of im-
selected step) for a tight spread stock with DDPG. provement by deepening the hyperparametes search,Advanced Volatility Fitting
extending the training period and optimization the References
scoring metric in the evaluation episodes.
[1] B. Hambly, R. Xu, and H. Yang, Recent
Advances in Reinforcement Learning in Finance,
7 CONCLUSION AND PROSPECTS
arXiv:2112.04553, 2021.
In this paper, we proposed a model-free deep rein- [2] H. Robbins, and S. Monro, A stochastic
forcement learning architecture to solve the dynamic approximation method., Ann. Math. Statistics,
volatilityfittingproblemincontinuousstateandaction 22:400–407, 1951.
spaces. We showed that DRL algorithms are natively
tailored for the volatility fitting problem as they [3] Bertsekas, D.P. Dynamic programming and opti-
possess (a) native exploration (b) effective catalog of mal control, volume 1. Athena scientific Belmont,
pastexperiencesand(c)goodpredictivepowerinlarge MA, 2005.
spaces. Using toy problems with increasing complexity,
[4] Sutton, R. S. and Barto, A. G. Reinforcement
weshowedthatdeepreinforcementlearningalgorithms
learning: An introduction. MIT press, 2018.
can achieve satisfactory performance.
[5] A. Charpentier, R. Elie, and C. Rem-
Traditional methods such as least squares fit- linger, Reinforcement learning in economics
ting and gradient-based optimisation techniques, often and finance, Computational Economics, (2021),
struggle with complex state spaces and non-linearities. pp. 1–38.
In contract RL’s ability to learn from the interac-
tions with the continuous environment allows it to [6] P. N. Kolm, and G. Ritter, Modern per-
progressively improve its performance by exploring spectives on reinforcement learning in finance,
and exploiting patterns that emerges from the data. Modern Perspectives on Reinforcement Learning
Moreover, our approach highlights the benefits of in Finance (September 6, 2019). The Journal of
rewardsshapingandcarefulenvironmentdesign,which Machine Learning in Finance, 1 (2020).
allow the RL agent to learn efficiently in complex
[7] P. Murray, B. Wood, H. Buehler,
market conditions.
M. Wiese, and M. S. Pakkanen, Deep Hedg-
ing: Continuous Reinforcement Learning for
This paper therefore lays the groundwork for a
Hedging of General Portfolios across Multiple
full AI-based dynamic volatility fitting. Future work
Risk Aversions, arXiv:2207.07467v1, 2022.
can extend the fitting to larger parametrizations
and reflects stylistic effects in volatility surface [8] M. F. Dixon, I. Halperin, and P. Bilokon,
term-structure. Machine Learning in Finance, Springer, 2020.
[9] R. S. Sutton, D. A. McAllester, S. P.
ACKNOWLEDGMENTS: The authors would
Singh, and Y. Mansour,Policy gradient meth-
like to thank all the members of Equities Derivatives
ods for reinforcement learning with function ap-
Markets Quantitative Analysis (MQA) of Citigroup
proximation, in Advances in Neural Information
Global Market Limited for fruitful and insightful dis-
Processing Systems, 2000, pp. 1057–1063.
cussions, remarks and suggestions for this study.
We are particularly grateful to Truong Nguyen, [10] T. P. Lillicrap, J. J. Hunt, A. Pritzel,
Thomas Fouret and El Mostafa Ezzine for their feed- N.Heess, T.Erez, Y.Tassa, D.Silver, and
back and encouragements throughout this research D. Wierstra, Continuous control with deep re-
project.
inforcement learning, in4thInternationalConfer-
ence on Learning Representations (ICLR), 2016.
DISCLAIMER: The article contains the personal
views of the authors, which are not necessarily those [11] T. Haarnoja, A. Zhou, P. Abbeel, and
of Citi. This is not a product of Citi Research. S. Levine, Soft actor-critic: Off-policy maxi-
mum entropy deep reinforcement learning with a
stochastic actor, in International Conference on
Machine Learning, PMLR, 2018, pp. 1861–1870.
[12] V. R. Konda and J. N. Tsitsiklis, Actor-
critic algorithms, in Advances in Neural Informa-
tion Processing Systems, 2000, pp. 1008–1014.Advanced Volatility Fitting
[13] T. Haarnoja, A. Zhou,K. Hartikainen, [24] S. Ioffe and C. Szegedy, Batch Nor-
G. Tucker, S. Ha, J. Tan, V. Kumar, malization: Accelerating Deep Network Train-
H. Zhu, A. Gupta, P. Abbeel, and ing by Reducing Internal Covariate Shift,
S. Levine, Soft Actor-Critic Algorithms and Ap- arXiv:1502.03167, (2015).
plications, in International Conference on Ma-
[25] V. Mnih, K. Kavukcuoglu, D. Silver,
chine Learning, PMLR, 2019.
A. A. Rusu, J. Veness, M. G. Bellemare,
[14] J. D. Abernethy and S. Kale, Adaptive mar- A. Graves, M. Riedmiller, A. K. Fidje-
ket making via online learning, inNIPS,Citeseer, land, G. Ostrovski, et al., Human-level con-
2013, pp. 2058–2066. trol through deep reinforcement learning, Nature,
518 (2015), pp. 529–533.
[15] C. J. Watkins and P. Dayan, Q-learning, Ma-
chine learning, 8 (1992), pp. 279–292. [26] J. Gatheral, A. Jacquier., Arbitrage-Free
SVI Volatility Surfaces, Quantitative Finance,
[16] T. Spooner, J. Fearnley, R. Savani, and
Vol. 14, No. 1, 59-71, 2014.
A. Koukorinis, Market making via reinforce-
ment learning, in International Foundation for [27] V. Zetocha., Sculpting implied volatility sur-
Autonomous Agents and Multiagent Systems, faces of illiquid assets, (April 11, 2022).
AAMAS ’18, 2018, pp. 434–442.
[28] J.Gatheral.,Aparsimoniousarbitrage-freeim-
[17] M. Zhao and V. Linetsky, High frequency au- plied volatility parameterization with application
tomated market making algorithms with adverse to the valuation of volatility derivatives., (Presen-
selection risk control via reinforcement learning, tation at Global Derivatives and Risk Manage-
in Proceedings of the Second ACM International ment, Madrid, 2004).
Conference on AI in Finance, 2021, pp. 1–9.
[29] G. Guo, A. Jacquier, C. Martini, and
[18] R. J. Williams, Simple statistical gradient- L. Neufcourt G. Cybenko., Generalised
following algorithms for connectionist reinforce- arbitrage-free SVI volatility surfaces., Computa-
ment learning, Machine Learning, 8 (1992), tional Finance, (May 27, 2016).
pp. 229–256.
[30] C. Homescu., Implied volatility surface: con-
[19] S. Ganesh, N. Vadori, M. Xu, H. Zheng, struction methodologies and characteristics.,
P. Reddy, and M. Veloso, Reinforcement (arXiv, July 9, 2011).
learning for market making in a multi-agent
dealer market, arXiv preprint arXiv:1911.05892, [31] G. Cybenko., Approximation by superpositions
(2019). of a sigmoidal function., Math. Control Signals
Systems, 2(4):303–314, 1989.
[20] T. Spooner and R. Savani, Robust Market
Making via Adversarial Reinforcement Learning, [32] Y. LeCun, Y. Bengio and G. Hinton., Deep
in Proc. of the 29th International Joint Confer- learning., Nature, 2015.
ence on Artificial Intelligence, IJCAI-20, 7 2020,
[33] D.Knowles.,LagrangianDualityforDummies.,
pp. 4590–4596.
Stanford University, 2010.
[21] Z. Ye, W. Deng, S. Zhou, Y. Xu, and
[34] K. Hornik, M. Stinchcombe and H. White.,
J. Guan, Optimal trade execution based on deep
Multilayer feedforward networks are universal ap-
deterministic policy gradient, in Database Sys-
proximators., Neural Networks, Vol. 2, 359-366.
tems for Advanced Applications, Springer Inter-
national Publishing, 2020, pp. 638–654.
[22] S. Lin and P. A. Beling, An end-to-end opti-
mal trade execution framework based on proximal
policy optimization, in IJCAI, 2020, pp. 4548–
4554.
[23] J. Cao, J. Chen, J. Hull, and Z. Poulos,
Deep hedging of derivatives using reinforcement
learning, The Journal of Financial Data Science,
3 (2021), pp. 10–27.Advanced Volatility Fitting
A Black Scholes Vega Weighted MSE re-
ward
A.1 Static case
As can be seen from table 5 and 6, the final rewards
achieved by the variants of the actor-critic algorithms
is close to the one coming from the optimizer. The
performance is stable across the different market
configuration considerered.
Table 5: DDPG MSE Rewards Figure27: Impliedvolatilitysnapshotina”HighSmile”
configuration with SAC: A Monte-Carlo on 5 differ-
type Skew High Smile Inv. Smile ents random seeds. We represent in (green) the best
response of the agent amongst the last 1000 episodes
Bench -0.0217 -0.000041 -0.0000383
andin(blue)themeanofthelast1000volatilityslices.
seeds’ avg -0.0114 -0.001279 -0.0001314
A.1.2 Skew Market
Table 6: SAC MSE Rewards
type Skew High Smile Inv. Smile
Bench -0.0217 -0.000041 -0.0000383
seeds’ avg -0.0319 -0.003708 -0.000254
A.1.1 High Smile Market
Figure28: Impliedvolatilitysnapshotina”skewSmile”
configuration with DDPG: A Monte-Carlo on 5 differ-
ents random seeds is performed with a power decaying
noise. Werepresentin(green)thebestresponseofthe
agent amongst the last 1000 episodes and in (blue) the
mean of the last 1000 volatility slices.
Figure26: Impliedvolatilitysnapshotina”HighSmile”
configuration with DDPG: A Monte-Carlo on 5 differ-
ents random seeds is performed with a power decaying
noise. Werepresentin(green)thebestresponseofthe
agent amongst the last 1000 episodes and in (blue) the
mean of the last 1000 volatility slices.
Figure 29: Implied volatility snapshot in a ”Skew”
configuration with SAC: A Monte-Carlo on 5 differ-
ents random seeds. We represent in (green) the best
response of the agent amongst the last 1000 episodes
andin(blue)themeanofthelast1000volatilityslices.Advanced Volatility Fitting
A.1.3 Inverse Smile Market A.2.1 Smile Market
Figure 32: Implied volatility snapshot in a ”Smile”
configuration with DDPG: A Monte-Carlo on 5 differ-
Figure 30: Implied volatility snapshot in an ”Inverse
ents random seeds is performed with a power decaying
Smile” configuration with DDPG: A Monte-Carlo on
noise. We represent in (green) the best response of
5 differents random seeds is performed with a power
the agent amongst the last 50 episodes and in (blue)
decaying noise. We represent in (green) the best re-
the mean of the final volatility slices for the last 50
sponseoftheagentamongstthelast1000episodesand
episodes.
in (blue) the mean of the last 1000 volatility slices.
A.2.2 Skew Market
Figure 31: Implied volatility snapshot in an ”Inverse Figure33: Impliedvolatilitysnapshotina”Skew”con-
Smile” configuration with SAC: A Monte-Carlo on figuration with DDPG: A Monte-Carlo on 5 differents
5 differents random seeds. We represent in (green) randomseedsisperformedwithapowerdecayingnoise.
the best response of the agent amongst the last 1000 We represent in (green) the best response of the agent
episodesandin(blue)themeanofthelast1000volatil- amongst the last 50 episodes and in (blue) the mean
ity slices. of the final volatility slices for the last 50 episodes.
A.2.3 Inverse Smile Market (MSE)
A.2 Sequential case
As can be seen from table 7, the final rewards achieved
by the variants of the actor-critic algorithms is close to
the one coming from the optimizer. The performance
is stable across the different market configuration
considerered.
Table 7: DDPG MSE Rewards
type Skew High Smile Inv. Smile Figure 34: Implied volatility snapshot in an ”Inverse
Smile” configuration with DDPG: A Monte-Carlo on
Bench -0.0217 -0.000041 -0.0000383
5 differents random seeds is performed with a power
seeds’ avg -0.0791 -0.000818 -0.000516
decaying noise. We represent in (green) the best re-
sponse of the agent amongst the last 50 episodes and
in (blue) the mean of the final volatility slices for the
last 50 episodes.Advanced Volatility Fitting
B Deep Reinforcement Learning
DRL comes from using neural networks thanks to the Φ =Φ − γ n+1 (cid:88) ∂ F (x )∇˙L(F (x )−y )
n+1 n N Φ Φ i Φ i i
universal approximation theorem, as nonlinear func- batch i∈In+1
tional approximation to the value and policy functions.
Using Neural networks for reinforcement learning
involve some (above-mentioned-like) optimization algo-
B.1 Fully Connected Artificial Neural
rithm which assumes that samples are independently
Networks (FCNN)
andidenticallydistributed(IID).Thisassumptiondoes
not hold any more when the samples are generated
FCNN refers to a neural network architecture where
from exploring sequentially in an environment, this is
any given neuron is connected to all neurons in the
challenging in the context of RL. we used a Replay
previous layer.
Let us consider data samples x
i
∈ Rdin and y
i
∈ Buffer to address that issue and improve the learning
Rdout for1≤i≤M associatedtoaregressionproblem, performance.
where(x )aretheinputsand(y )aretheoutputs. That
i i
is, we look for a function F :Rdin →Rdout which fits B.3 Replay Buffer
to the data i.e. ∀1≤i≤M,F(x )∼y
i i The agent learns over time to select his actions based
Let K +1, K ∈ N be the number of layers of the
on his past experiences (exploitation) and/or by trying
neural network and for k =0,...,K; let d ∈N be the
k new choices (exploration). The past experiences are
size of the kth layer with d =d and d =d .
0 in K out storedina”Replay Buffer” mainlyusedtoimprovethe
Let ϕ:R→R be a non-linear function, generally
performance of neural networks by providing nearly
chosen to be a sigmoid-type or a ReLU-type function
non correlated samples at each parameters update.
(called activation function in the neural networks lit-
Essentially, it is a finite size cache memory, which
erature), For k = 1,...,K, x ∈ Rdk−1 and for θ
k
∈
temporarily saves the agent observations during the
M dk,dk−1((R)) (weight matrices) and b
k
∈ Rdk(bias
learning process.
vectors).
Before the training process, it is full of tuples or
We define the below vector in Rdk where the scalar
transitions (s ,a ,r ,s ) sampled from the environ-
function ϕ is applied to the vector θ x+b coordinate t t t t+1
k k mentaccordingtotheinitialexplorationpolicy. During
by coordinate:
the training process and at each time step, we update
the replay buffer by discarding the transitions with the
worst reward or the oldest reward and storing a new
ϕ (x):=[ϕ([θ x+b ] )] (15)
θk,bk k k i 1≤i≤dk
tuple(s ,a ,r ,s )sampledfromtheenvironmentac-
t t t t+1
Writing Φ = (θ ,b ,...,θ ,b ) = (θ ,b ) , the cording to the current policy. In this work, the weights
1 1 K K k k k=1,...,K
output of the neural network with
(cid:80)K
d ×d pa-
of our Neural Networks are initialized from uniform
l=1 l−1 l
distributions (Xavier initialisation).
rameters, is given by:
The first motivation of the replay memory in DQN
[25]wastoalleviatetheproblemsofcorrelateddataand
F (x)=θ (ϕ ◦...◦ϕ (u))+b (16) non-stationary distributions by smoothing the train-
Φ K θK−1,bK−1 θ1,b1 K
ing distribution over many past experiences. In fact,
learning processes are usually in mini-batches i.e. the
B.2 Training of Neural Networks
parameters of the neural networks are updated by uni-
The objective is to extract a model from the empirical formlychoosingaminibatchofsamplesfromthebuffer
data. We look for a function F in a family of func- and performing batch gradient descent (a more stable
tions parametrized by a finite-dimension parameter: version and still computationally efficient of Stochastic
(cid:8) F Φ,Φ∈Rd1×din ×Rd1 ×...×RdK×dout ×RdK(cid:9) gradient descent). Such algorithm assume each batch
ForalossfunctionL:Rdout →R+,whichmeasures sample to be (IID) from a fixed distribution. However,
the error between the prediction F (x ) and the true explorationfromtheenvironmentclearlygeneratescor-
Φ i
data y , the regression problem is the minimization related samples, subject to distributional shift. Thus
i
of the average loss over a small batch of data I . we consider a ”Replay Memory” with large enough
n+1
The parameters Φ are updated in the descent direction size, to store past experiences and uniformly sample
of L using either Stochastic algorithms as introduced a mini batch at each update of Neural Networks pa-
by Robbins and Monro [2] or a more stable version rameters in order to minimize the correlations between
and still computationally efficient, the batch gradient samples allowing the algorithms to learn across a set
descent and further their extension: of uncorrelated transitions.Advanced Volatility Fitting
C Supplementary Results
The below figures display the evolution of some important quantities during the training of the RL agents in
several market type and MSE reward: A Monte-Carlo on 5 differents random seeds is performed with a power
decayingnoiseforDDPGandautomaticentropyadjustmentforSAC.Werepresentin(green)thebestresponse
of the agent amongst the last 1000 steps and in (blue) the mean of the final volatility slices for the last 1000 steps.
As we can see, the reward is converging and the replay buffer is improving over time. Moreover, the exploration
parameters (std noise and temperature coefficient) are decreasing as the agent is converging to the optimal
solution.
DDPG Static, MSE reward, Xavier init = True, Monte Carlo size = 5 , threshold=0.013, elapsed time =00:40:00,clip_grad=True, smart_rb=False
Average scores/reward per episode volatilities slices
0 0.50 a bv eg st l a las st t 1 10 00 00 0 v vo ol_ l_c cu ur rv ve e
benchmark_fit
1 0.45 Mid
2 0.40
0
3 1 0.35
2
4 3 0.30
4
Mean reward
5 Mean reward 0.25
best atteignable reward
6 0.20
0 10000 20000 30000 40000 50000 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
epoch Moneyness
Statistics of the Replay Buffer Reward Evolution of Exploration Noise's standard deviation
0 Std Exploration Noise
0.14
1
0.12
2
0.10
3 0.08
4 0.06
5 0.04
6 M Me ea an n o of f A Mv ine ir mag ue ms rr ee ww aa rr dd RR BB 0.02
0 10000 20000 30000 40000 50000 0 10000 20000 30000 40000 50000
epoch
Figure 35: A snapshot of the training in a ”Skew” configuration with DDPG and a power decaying noise.
SAC, MSE reward, Xavier init = True, Monte Carlo size = 5 , Lr=2.5e-05 (A2C), elapsed time =02:33:18,clip_grad=False, smart_rb=False
Average scores/reward per episode volatilities
0 avg last 1000 vol_curve
0.50 best last 1000 vol_curve
10 benchmark_fit
0.45 Mid
20
0.40
30 0.35
40 0.30
50 Mean reward over seeds 0.25
deterministic reward Mean
best atteignable reward
60 0.20
0 20000 40000 60000 80000 100000 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
Statistics Reward Replay Buffer Evolution of temperature Coef.
0 1.0 Temperature Coef.
10
0.8
20
0.6
30
0.4
40
0.2
50
Mean of Means reward RB
Mean of Minimum reward RB 0.0
60
0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000
Figure 36: A snapshot of the training in a ”Skew” configuration with SAC and automatic entropy adjustement.
drawer lov
.lpmi
esion
dtsAdvanced Volatility Fitting
DDPG Static, MSE reward, Xavier init = True, Monte Carlo size = 5 , threshold=0.0005, elapsed time =00:24:54,clip_grad=True, smart_rb=False
Average scores/reward per episode volatilities slices
0 avg last 1000 vol_curve
0.675 best last 1000 vol_curve
benchmark_fit
5 0.650 Mid
0.625
10 0.600
0.575
15
0.550
20 r be ew sta r ad tt M eie ga nn able reward 0.525
0 2500 5000 7500 10000 12500 15000 17500 20000 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
epoch Moneyness
Statistics of the Replay Buffer Reward Evolution of Exploration Noise's standard deviation
0 Std Exploration Noise
0.14
2
0.12
4
0.10
6 0.08
8 0.06
10 0.04
12 M Me ea an n o of f A Mv ine ir mag ue ms rr ee ww aa rr dd RR BB 0.02
0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000
epoch
Figure 37: A snapshot of the training in a ”High Smile” configuration with DDPG and a power decaying noise.
SAC, MSE reward, Xavier init = True, Monte Carlo size = 5 , Lr=2.5e-05 (A2C), elapsed time =01:36:19,clip_grad=False, smart_rb=False
Average scores/reward per episode volatilities
0 avg last 1000 vol_curve
0.675 best last 1000 vol_curve
10 benchmark_fit
0.650 Mid
20 0.625
30 0.600
0.575
40
0.550
50 Mean reward over seeds
deterministic reward Mean 0.525
best atteignable reward
60
0 20000 40000 60000 80000 100000 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
Statistics Reward Replay Buffer Evolution of temperature Coef.
0 1.0 Temperature Coef.
10
0.8
20
0.6
30
0.4
40
0.2
50
Mean of Means reward RB
Mean of Minimum reward RB 0.0
60
0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000
Figure 38: A snapshot of the training in a ”High Smile” configuration with SAC and automatic entropy
adjustement.
drawer lov
.lpmi
esion
dtsAdvanced Volatility Fitting
DDPG Static, MSE reward, Xavier init = True, Monte Carlo size = 5 , threshold=0.001, elapsed time =00:57:19,clip_grad=True, smart_rb=False
Average scores/reward per episode volatilities slices
0.925
0
0.900
10 0.875
0.850
20
0.825
30 0.800
0.775 avg last 1000 vol_curve
40 0.750 best last 1000 vol_curve
reward Mean benchmark_fit
best atteignable reward 0.725 Mid
50
0 10000 20000 30000 40000 50000 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
epoch Moneyness
Statistics of the Replay Buffer Reward Evolution of Exploration Noise's standard deviation
0 Std Exploration Noise
0.14
2
0.12
4
0.10
6
0.08
8
0.06
10
0.04
12 Mean of Averages reward RB 0.02
Mean of Minimum reward RB
14
0 10000 20000 30000 40000 50000 0 10000 20000 30000 40000 50000
epoch
Figure 39: A snapshot of the training in a ”Inverse Smile” configuration with DDPG and a power decaying
noise.
SAC, MSE reward, Xavier init = True, Monte Carlo size = 5 , Lr=2.5e-05 (A2C), elapsed time =02:01:50,clip_grad=False, smart_rb=False
Average scores/reward per episode volatilities
0 0.925
0.900
10
0.875
20 0.850
0.825
30
0.800
40
0.775
avg last 1000 vol_curve
50 Mean reward over seeds 0.750 best last 1000 vol_curve
deterministic reward Mean benchmark_fit
best atteignable reward 0.725 Mid
60
0 20000 40000 60000 80000 100000 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
Statistics Reward Replay Buffer Evolution of temperature Coef.
0 1.0 Temperature Coef.
10
0.8
20
0.6
30
0.4
40
0.2
50
Mean of Means reward RB
Mean of Minimum reward RB 0.0
60
0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000
Figure 40: A snapshot of the training in a ”Inverse Smile” configuration with SAC and automatic entropy
adjustement.
drawer lov
.lpmi
esion
dtsAdvanced Volatility Fitting
D Experiment Details and Hyperparameters
Below, we provide the different hyper-parameters during our experiments(Dynamic setting in brackets).
D.1 DDPG variant Algorithm for Volatility fitting
Table 8 lists the DDPG parameters used in our experiments.
Table 8: DDPG Hyperparameters
Parameter Value
optimizer Adam
learning rate Actor and Critic 0.0025 (2.5·10−5,2.5·10−4)
discount (γ) 0.99
replay buffer size 103 (2.103)
number of hidden layers (all networks) 2
number of hidden units per layer 256
number of samples per minibatch (batch size) 64 (252)
Power decaying noise with std bounded in [0.01,0.15]
nonlinearity ReLU, Tanh
target smoothing coefficient (τ) 0.001
gradient steps 1
Neural Network initialisation Xavier initialisation
D.2 SAC variant Algorithm for Volatility fitting
Table 9 lists the SAC parameters used in our experiments.
Table 9: SAC Hyperparameters
Parameter Value
optimizer Adam
learning rate Actor and Critic 2.5·10−5,2.5·10−4
discount (γ) 0.99
replay buffer size 103
number of hidden layers (all networks) 2
number of hidden units per layer 256
number of samples per minibatch (batch size) 64
entropy target −dim(A)=−K
automatic entropy tuning True
nonlinearity ReLU, Tanh
target smoothing coefficient (τ) 0.001
Neural Network initialisation Xavier initialisation
E Hardware and Computational Ressources
Allexperimentsin thispaperwereconductedon amachine withthefollowingspecifications: An Intel(R)Xeon(R)
W-1270CPUrunningat3.40GHzwith64GBofRAMand64-bitoperatingsysteminWindows11. Allalgorithms
were implemented in Python 3.8.8 using PyTorch 2.3.0+cpu.