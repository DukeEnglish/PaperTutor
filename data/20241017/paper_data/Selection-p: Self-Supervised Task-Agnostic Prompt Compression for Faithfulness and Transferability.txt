Selection-p: Self-Supervised Task-Agnostic Prompt Compression for
Faithfulness and Transferability
TszTingChung1 LeyangCui2 LemaoLiu2 XintingHuang2
ShumingShi2 Dit-YanYeung1
1TheHongKongUniversityofScienceandTechnology 2TencentAILab
{ttchungc, nealcly.nlp, lemaoliu, thisisjcykcd}@gmail.com
timxthuang@tencent.com dyyeung@cse.ust.hk
Abstract Transferable SingleRun ¬External
AutoCompressor ✗ ✓ ✓
LargeLanguageModels(LLMs)havedemon- LLMLingua ✓ ✗ ✓
stratedimpressivecapabilitiesinawiderange LLMLingua-2 ✓ ✓ ✗
of natural language processing tasks when Selection-p ✓ ✓ ✓
leveragingin-contextlearning. Tomitigatethe
additional computational and financial costs Table1:ComparisonbetweentheproposedSelection-
associated with in-context learning, several p and existing content compression approaches.
prompt compression methods have been pro- Selection-pexhibitsgreattransferability(Transferable),
posed to compress the in-context learning doesnotrequiremulti-rounditerativedecoding(Single
prompts. Despite their success, these meth- Run),anddoesnotrelyoncostlyexternalresourcesfor
odsfacechallengeswithtransferabilitydueto training(¬External).
model-specificcompression,orrelyonexter-
naltrainingdata,suchasGPT-4. Inthispaper,
weinvestigatetheabilityofLLMstodevelopa focusing on prompt compression, which aims to
unifiedcompressionmethodthatdiscretizesun- compress the original prompts while minimizing
informativetokens,utilizingaself-supervised
informationloss. Theyarecategorizedintodiscrete
pre-trainingtechnique. Byintroducingasmall
compression(Lietal.,2023;Jiangetal.,2023;Pan
numberofparametersduringthecontinualpre-
etal.,2024)andcontinuouscompression(Muetal.,
training, the proposed Selection-p produces
aprobabilityforeachinputtoken, indicating 2023;Chevalieretal.,2023;Geetal.,2024);the
whethertopreserveordiscardit. Experiments formercompressesthecontextintodiscretetokens,
showSelection-pachievesstate-of-the-artper- whilethelattercompressesitintoashortsequence
formanceacrossnumerousclassificationtasks, ofcontinuousvectors.
achievingcompressionratesofupto10times
Observing redundant and repetitive content in
while experiencing only a marginal 0.8% de-
a given input, discrete compression methods aim
crease in performance. Moreover, it exhibits
superiortransferabilitytodifferentmodelscom- to eliminate less informative context without sig-
paredtopriorwork. Additionally,wefurther nificantlycompromisingthemodel’sperformance.
analyzehowSelection-phelpsmaintainperfor- Forexample,LLMLingua(Jiangetal.,2023)pro-
mance on in-context learning with long con- posestoperformiterativetokentruncationbased
texts.
on the content perplexity, requiring multi-round
decoding(Jiangetal.,2023). LLMLingua-2(Pan
1 Introduction
etal.,2024),distilledfromGPT-4(OpenAI,2023),
addressesthepotentialmisalignmentbetweenen-
In-context learning has shown remarkable suc-
tropyandthecompressionobjective,aswellasthe
cess in various natural language processing
distributiongapbetweentheperplexityofthecom-
tasks (Brown et al., 2020), such as classification
pression model and the target model. High costs
task(Minetal.,2022),andmathematicalreasoning
arestillinvolvedinthetrainingdataconstruction.
task(Weietal.,2023),enablingLargeLanguage
Meanwhile,optimizingthedistributionforspecific
Models(LLMs)totacklecomplexanddiversetasks
LLMs(GPT-4)may,ontheotherhand,hinderthe
usingonlyfew-shotsamples. However,in-context
transferabilityofthecompressedcontenttoother
learning also significantly extends the length of
LLMs. MoredetailsarediscussedinSection4.4.
prompts,resultinginincreasedcomputationaland
financialcosts. Recently,alineofworkhasbeen Continuouscompression(Bulatovetal.,2022;
4202
tcO
51
]LC.sc[
1v68711.0142:viXraWingateetal.,2022)teachespre-trainedLMsthe 2 RelatedWork
ability to compress text into a short sequence of
continuousvectors. AutoCompressors(Chevalier 2.1 HardCompression
etal.,2023)usesanunsupervisedlearningobjec-
Somestudiesfocusontokenpruning(Goyaletal.,
tive, which motivates the model to cache crucial
2020;KimandCho,2021;Raoetal.,2021;Kim
informationwithinthesummaryvectors. Despite
et al., 2022; Modarressi et al., 2022) and token
their success, these methods have poor general-
merging(Bolyaetal.,2023)buttheyaredesigned
ization as they can only compress to the length
primarilyforsmallermodelslikeBERT.Morere-
specified during training. Additionally, since the
cently,SelectiveContext(Lietal.,2023)isthefirst
continuous vector cannot be transferred between
to propose to prune less important tokens based
models,aseparatecompressormustbetrainedfor
on information entropy. Subsequently, LLMLin-
eachmodel.
gua(Jiangetal.,2023)refinedtheapproachbyinte-
Thisraisesaninterestingresearchquestion:
gratingtheselectionofdemonstrationsandtheallo-
“Can LLMs learn to identify less informative
cationofcompressionbudgetsforvarioussegments
tokenswithinagivencontextwithoutexternalan-
oftheinputprompt. Notrainingisrequiredinthese
notatedsignals?”
modelsbuttheirefficacyindownstreamtaskswith
To answer this question, we propose a pre-
compressionappliedtoin-contextdemonstrations
training strategy with a self-supervised signal,
remainslimited. Panetal.(2024)extendedtheidea
which enables the model to autonomously learn
andaddressedthepotentialmisalignmentbetween
topredictthenexttokenbasedoncompressedcon-
entropyandthecompressionobjective,leveraging
text. Withasmalladditionalnumberofparameters,
fullbidirectionalcontextbytrainingontheirpro-
a forward pass on the proposed selection-p cre-
posed GPT-4 distilled compression dataset. Our
atestheprobabilityvectorpcorrespondingtoeach
simpleyeteffectiveapproachesoutperformprevi-
inputtoken,indicatingwhethertopreserveordis-
ousstudies.
card the token. During inference, we can apply
the detokenized compressed tokens to any down-
2.2 SoftCompression
streamLLMswithonlysingle-turndecodingand
without reliance on any costly external resources Gist (Mu et al., 2023) is first proposed to com-
asdepictedinTable1. presspromptwithsofttokens. Subsequently,Auto-
Themaincontributionsofthisworkarefourfold: compressor(Chevalieretal.,2023)andICAE(Ge
etal.,2024)extendtheideatohandlelongcontexts
• We present Selection-p which achieves only
withdifferentpretrainingapproaches. ICAEfurther
0.8% drops in performance under 10x com-
conductsinstructiontuningtoenhancemodelper-
pressionrateacrossninetraditionalclassifica-
formance. Thedownstreamperformanceofthese
tiontasks,surpassingtheperformanceofthe
models heavily relies on the tuned compression
existingcompressionmodels. Underthisset-
model, with a fixed compression rate. Addition-
ting,aspeedupof5.3xcanbeachievedduring
ally,retrainingisnecessaryfordifferentversions
inferencewithin-contextlearning.
ofLLMs. Comparedtotheseapproaches,ourwork
• Selection-p demonstrated great transferabil- offersgreaterflexibilityandtransferability,while
ity,whichsurpassestheperformanceofprior simultaneouslysurpassingtheperformanceofex-
workinperforminghardcompressionforboth istingcompressionmodels.
open-sourceandclose-sourcemodels.
3 Methodology
• WefurtheranalyzehowSelection-phelpsin
in-context learning in long-context settings,
Undertheintuitionthatredundanttextsoftenexist
presentingapotentialsolutiontoaddressthe
and their removal does not hinder human under-
performancedeclinationoflong-contextmod-
standingofthetext,weassumethatLLMsbehave
elsinICL.
in a similar manner. To efficiently identify less
• Weconnectin-domainpriorworksandmake informativetokenswithinagivencontext,wepro-
comparisonswiththesestate-of-the-artcom- pose a simple pre-training objective encouraging
pression models, providing a complete pic- the model to predict the same token both before
ture. andafterdiscardinglessinformativetokens.(W,b) lessinformativetokenswithinthecontextinadis-
hL criminativeway. Intheinferencestep,thetokens
selectedviatheselectionmodelarepassedtoour
targetedinferencemodel.
LLM
Selection. Assumep ∈ [0,1]denoteameasure
i
LoRA
oftheinformativenessoftokenx . Theoretically,
i
wecancustomizeadeepneuralmodeltoinstanti-
input_ids: x -1 -x 2 ... x -n-1 ate p i. In practice, we directly take a pre-trained
attn_mask: p 1 p 2 ... p n-1 language model and adopt the last layer of hid-
den representation hL from a pre-trained LM to
(W,b) thenewlinearprojectionlayer. Formally,suppose
hL
hL = {h ,h ,··· ,h } denotes the sequence of
1 2 n
hiddenstatesforalltokensx atthelastlayerofa
i
LLM pre-trainedlanguagemodel. Theselectionmodel
p = {p ,p ,··· ,p }isdefinedasfollows:
1 2 n
FREEZE
p = σ(WhL+b) (1)
input_ids: x 1 x 2 ... x n-1
whereσ isthesigmoidfunction,Wandbarepa-
Figure1:Illustrationwiththetrainingprocess.Areas rametersoftheprojectionmatrixandbiasvector,
inorangearelearnableparameters. Fortheinputcon- respectively.
text [x ,x ,...,x ], inference without parameters
1 2 n−1 By using the selection model p, it is straight-
update is performed first to create the attention mask
forwardtocompressthecontextforinference: we
p¯. ThesesubsequentlyformthemodelinputforLoRA
directlyprunethecorrespondingtokensaccording
trainingandupdatingtheparametersoftheadditional
linearlayer. to our desired compression rate in retaining the
topk%oftokensinthecontext. Toensuretheeffi-
3.1 Preliminary
ciencyinperformingcompression,asingleforward
Language Model. Given a context pass on our model creates the token preservation
[x ,x ,...,x ], the objective of the lan- probabilityforallinputtokensforsimplicity.
1 2 n−1
guage model is to predict the next token x ,
n
Training. Ourtrainingcriterionfortheselection
formed as P(x |x ,x ,...,x ). In case of
n 1 2 n−1
model aims to preserve the language modeling
trainingwiththecausallanguagemodeling(CLM)
abilityoftheLLMwhilealsolearningtodiscard
loss,wewillhave,
tokens effectively. To this end, we employ the
(cid:88) self-supervisedapproachtooptimizetheselection
L = − logP(x | x ,x ,...,x ;θ)
CLM i 1 2 i−1
model and therefore we do not need external re-
sourcestotraintheselectionmodelcomparedwith
Tokens Selection Models. LLMLingua (Jiang
Panetal.(2024).
et al., 2023) and its variant (Li et al., 2023; Pan
Tokeepthetrainingprocessconsistentwiththe
et al., 2024) select tokens according to the com-
inferenceprocess,wefirstdiscretizetheselection
puted distribution of the targeted LLM. Specifi-
model p. Let p¯ denote the discretized binary
cally, suppose x is a token in the prompt, if the i
i
model of p . In other words, p¯ is 1 if it ranks
probability of a token x is less than a threshold, i i
i
thetopk%oftokenswiththehighestpvaluesand
thenthetokenisselectedtobecompressed.
0otherwise. Thenweusethediscretizedmodelas
amasktodefinetheCLMlossfunctionasfollows:
3.2 Selection-p
(cid:88)
Following the tokens selection models (Li et al., Lˆ = − logP(x | p¯ x ,...,p¯ x ;θ)
CLM i 1 1 i−1 i−1
2023;Jiangetal.,2023;Panetal.,2024),Selection- (2)
pincludesthetwostepsfortesting,i.e. theselec- wherep¯x denoteswhetherthetokenx ismasked
i i i
tion(compression)stepandtheinferencestep. In or not depending on the value of p¯, where the
i
theselectionstep,unlikeLLMLinguaanditsvari- abovelanguagemodelP issetasthesamemodel
ant, weinsteaddefineaselectionmodeltoselect asthatusedintheselectionmodelinEq.1.Training Details for Transferability. One of To ensure a fair comparison among different
ourgoalsistoachieveatransferablecompression compressionmodels,acompressionrateof0.1is
method. Therefore, the parameters that achieve used. For each task, four sets of demonstrations
thebestlossonthelanguagemodelsinEq.1and are selected, and the average result across these
Eq.2intrainingmaynotbetransferredtothetar- fourtrialsispresentedinTable2. Additionally,the
geted language model in inference since the lan- averageaccuracyoftheninetasksispresentedfor
guagemodelsintrainingandinferencecanbedif- aclearerperformancecomparison.
ferent. Asaresult,toensurethebettertransferabil- Toachieveacompressionrateof0.1,asimpler
ityoftheoptimizedselectionmodel,wefreezethe approachistodirectlyretainone-tenthofthefull-
pre-trained language model in Eq. 1 and employ shot demonstration (e.g., using 1-shot instead of
LoRA(Huetal.,2021)totrainapartialparameter 10-shot for the RTE task) instead of performing
inthelanguagemodelinEq.2.1 Insummary,the selection at the token level. We also include this
CLM-basedtraininglossisillustratedinFigure1. method as a baseline to further demonstrate the
effectivenessofourapproach.
4 Experiment
LongContextClassificationTasks. Recentre-
4.1 Setting search by Li et al. (2024) shows the failure of
WefinetuneaLLaMA-2-7bmodel(Touvronetal., in-context learning tasks when applied to long-
2023)on100MtokensfromRedPajama(Togeth- context scenarios. To investigate whether com-
erAI,2023)onsplitsegmentsof1,024tokensvia pression models can serve as a viable solution
LoRA(Huetal.,2022). Toprovideacomprehen- inlong-contextsettings, wecompareSelection-p
siveanalysisofthemodelcapabilities,ourevalua- withlong-contextmodels,includingLLaMA-2-7B-
tionisconductedontraditionalclassificationtasks LongLora(Chenetal.,2023)andLong-LLaMA-
aswellasthelong-contextclassificationtask. code-7B(Tworkowskietal.,2023)ontheBANK-
For each task, we randomly sample from the ING77 dataset (Casanueva et al., 2020). The
trainingsettoconstructthedemonstrationsetfor dataset contains 77 classes where traversing all
In-Context Learning (ICL), which also serves as the instances with unique labels requires approx-
ourcompressiontargetfortokenselection. During imately two thousand tokens. Evaluation is con-
inference,thecompressionprocessonlyneedsto ducted at 2K, 4K, and 7K token levels, and we
becomputedonceforallsubsequentinferenceson adopt a compression rate of 10x for Selection-p
thetestinginstances. andLLMLingua-2amongalllevels. Sincethelong
in-contextdemonstrationisused,chunkingisper-
Traditional Classification Tasks. Following formedforevery2,048tokensinSelection-p. The
Chevalieretal.(2023),weevaluateandcompare compressedresultsareconcatenatedtogetherwith
differentcompressionmodelsonnineclassification a space token between each pair of chunks. We
tasks,includingsixtasksfromSuperGlue(Wang again follow the evaluation setting by Chevalier
et al., 2019). The predictions by LLMs are de- et al. (2023) on the models’ prediction, with the
terminedbyiteratingthroughallpossibleanswer resultpresentedinTable3.
options for the instance and selecting the option
with the minimum negative log-likelihood. The 4.2 Baselines
in-contextdemonstrationshavebeencarefullyse- We compare the Selection-p with the following
lected to approximate a size of 750 tokens, and state-of-the-artcompressionmodels.
thecompletedemonstrationisemployed. Thisis
• LLMLingua(Jiangetal.,2023)employsan
referred to as the “full-shot”. Since the average
iterativecompressionalgorithmtofilterless
tokenlengthforasingledemonstrationvariesfor
informative tokens based on the token-level
differenttasks(e.g.,asingledemonstrationinRTE
perplexity. Tofurtherboosttheperformance,
averages about 75 tokens, resulting in a 10-shot
Jiangetal.(2023)alsoconductsabudgetcon-
setupunderthefull-shotsetting),theexactnumber
trollertoallocatevaryingbudgetsacrossdif-
ofshotsdiffersdependingonthetask.
ferentdemonstrationsandquestions. Wefind
1In our preliminary experiments, we tried the Gumbel- thatthereisalsoasignificantdiscrepancyob-
softmax trick to optimize the loss in Eq. 2 but we did not
served between the prescribed compression
observegainsoverthedirectoptimizationimplementedinour
paper. rateandtheactualcompressionratethroughLLMLinguaAPIcalls. Forafaircomparison, Inaddition,agrowingtrendwithvariationsisob-
weexclusivelyutilizethetoken-levelprompt servedwithincreasingcompresseddemonstrations,
compressionalgorithmfromLLMLinguain indicating that our model can successfully learn
Table 2. We additionally compare LLMLin- from additional information after compression.
gua with Selection-p equipped with Budget Examplesofin-contextdemonstrationbeforeand
ControllerinSection5.5. aftercompressionarepresentedinAppendixA.
• LLMLingua-2 (Pan et al., 2024) is derived 4.4 Transferability
fromdatadistillationobtainedbyinstructing Compressionisfirstperformedonthedemonstra-
GPT-4 to perform compression. Similar to tionsetforICLwithSelection-p. Subsequently,the
ours, the model is trained as a binary classi- compressedtokensarepassedtoaseparatedown-
fieroneachtoken,determiningwhethereach streammodel(i.e. LLaMA-2-13Bortheblack-box
tokenshouldbepreserved. models)asthecompresseddemonstrationprompt
forevaluation.
• AutoCompressor (Chevalier et al., 2023)
is constructed based on the RMT architec- ToLLaMA-2-13B. Wefollowthesamesetting
ture (Bulatov et al., 2022). It compresses of evaluation across different classification tasks
textintosummaryvectorsthatcanbereused inSection4.3. Toassessthetransferabilityofthe
in subsequent segments. It is the only soft compressionmodels,wecompressdemonstrations
compressionmodeladoptedforcomparison, withSelection-pandinputthecompresseddemon-
consideringthatwefollowedtheexperiment strationtokensintoLLaMA-2-13B.Incomparing
setting for assessing the in-context learning different compression models, since retraining is
abilityofLLMs. requiredforsoftcompressionmethods,noresults
can be obtained for AutoCompressor (Chevalier
et al., 2023). In the case of LLMLingua, token-
4.3 EvaluationResult
levelperplexityiscalculatedwithLLaMA-2-13B
Traditional Classification Tasks. None of the insteadofLLaMaA-2-7Binthisexperiment.
compression models can achieve superior perfor- Ourapproachoutperformsallothercompression
mance compared to the full-shot demonstration models as shown in Table 4. In addition, a small
setting,whichisinlinewithourexpectationsgiven deviationisobservedbetweenthe10xcompression
theinformationlossduringcompression. However, rate and the full shot setting, demonstrating the
certain tasks show a notable improvement when greattransferabilityofourmodels. Notably,with
compared to both the zero-shot and full-shot ap- Selection-p,thetasksthatoutperformthefull-shot
proaches,e.g.,allthehardcompressionmodelssur- settinginLLaMA-2-7balsoexhibitsimilarpatterns
passzero-shotandfull-shotbyapproximately20% inLLaMA-2-13B.
intheWSCtask. Amongallcompressionmodels,
ToBlack-boxModels. Takingcostintoconsid-
Selection-pdemonstratesthehighestperformance
eration, we select ChatGPT (OpenAI, 2023) and
in conducting ICL, with an average accuracy of
Gemini(Team,2023)forevaluationtoexamineits
67.4% across all 10 tasks as presented in Table
transferabilitytoLLMs. Traditionalclassification
2. Examples of in-context demonstration before
tasksoftenhaveasimplenatureandthepotentialis-
and after compression are shown in Appendix A.
sueofdatacontamination,leadingtohighaccuracy
Todemonstratetheeffectivenessofourmodel,we
andcausinganinsignificantevaluation. Therefore,
haveincludedone-tenthoftheoriginaldemonstra-
weuseBANKING77(Casanuevaetal.,2020)for
tion set as the baseline. Our model significantly
evaluation. Followingasimilarsetupasdescribed
outperformsthebaselinewithacomparablenum-
in Section 4.3, we adopt a token size of 750 for
beroftokens,thereforehighlightingtheeffectof
examination. However, in case the compression
performingcompressionatthetokenlevel.
rate is too high, ChatGPT and Gemini are more
Long Context Classification Tasks. Our likelytodeviatefromtheinstructionsandprovide
model outperforms LLaMA-2-7B-LongLora, task-irrelevant responses. Therefore, we adopt a
Long-LLaMA-code-7B and LLMLingua-2 at all compressionrateof3xandusetheEMmetricfor
tokensizelevels,andachievessimilarresultsto Li thisexperimentgiventheirblack-boxnature. Note
etal.(2024)’sfindingsonthelong-contextmodels. thattheremaybevariationintheresultofGeminiSubj RTE WSC BoolQ MultiRC SST-2 WIC COPA AGNews AVG
Zero-shot 49.3 58.8 43.4 67.4 52.5 67.7 50.8 52.5 63.3 56.2
“One-tenth”-shot 48.6 65.3 52.6 68.3 49.2 84.0 53.6 83.9 55.5 62.3
Full-shot 80.7 70.7 41.8 62.8 46.8 92.5 56.4 85.6 76.3 68.2
AutoCompressor 57.9 56.1 39.4 66.5 51.8 92.8 53.0 84.4 80.9 64.7
LLMLingua 56.7 60.9 63.0 69.4 50.3 69.9 51.6 76.4 61.5 62.2
LLMLingua-2 57.3 67.8 63.7 69.8 52.8 66.2 50.3 71.4 61.9 62.3
Selection-p 68.5 68.5 61.1 69.7 54.4 90.7 50.3 76.9 66.8 67.4
Table2: Evaluationresultontraditionalclassificationtasks. Foursetsofrandomdemonstrationswereselected,
withtheaverageresultbeingpresented. Theaverageresultacrossdifferentclassificationtasksispresentedunder
AVG.
2K 4K 7K tasksincreasesalongwiththenumberofprovided
demonstrations. On the contrary, other compres-
LLaMA-2-7B-LongLora 0.0 0.0 0.0
Long-LLaMA-code-7B 0.0 0.0 0.0 sionmodelsdidn’tachieveanimprovementinac-
LLMLingua-2 41.2 31.6 35.2 curacy with more demonstrations. For instance,
Selection-p 46.9 50.9 51.6 thereisadropof2%recordedwithanadditional
500tokensofinformationforAutoCompressor.
Table3:EvaluationresultonBANKING77withincreas-
ingin-contentdemonstrationstokenslength. 5.2 LatencyAnalysis
Weanalyzeend-to-endlatencyonA100-80GGPU
since the discrete compressed tokens sometimes with the WSC task, illustrated in Table 7. Our
trigger the SAFETY error. The prompt used is method can achieve 5.3x speed up on 10x com-
presentedinAppendixB. pressedin-contextdemonstration. Comparedtothe
Thoughtheperformanceofourmodelstilldevi- inferencetime,negligibletimeisrequiredforcom-
atesfromthefull-shotsetting,itachievedthebest pression on the ICL task setting, demonstrating
performancecomparedtotheexistingworksaspre- high efficiency in adopting our models for com-
sentedinTable4,demonstratingfairtransferability pression. WealsocomparedLLMLinguawiththe
eveninLLMslikeChatGPT.Surprisingly,though disabledcontentBudgetController. Itrequiresit-
LLMLingua-2isdistilledfromGPT-4,itexhibits erative decoding on the segmented context while
poorgeneralizationcomparedtoothercompression Selection-ponlyrequiresasingleinferenceonall
models. tokensanddemonstratesagoodperformance.
5 Analysis 5.3 CorrelationwithAttentionandPerplexity
Withthep-valuerangingbetween0and1foreach
5.1 Flexibility
token, we further study whether any correlations
Performance withDifferentNumber of Initial existamongp,themeanattentionvalueduringthe
Tokens. Theresultinlongcontextclassification forwardpass,andthetokenslevelperplexity(i.e. a
tasks in Section 4.3 shows the effectiveness of corecomponentinLLMLingua(Jiangetal.,2023)).
chunk-wisecompressioninlongcontext. Wefur- Sincethevalueofpisderivedfromthelasthidden
ther analyze if compression models work well in stateofthemodel,weonlyconsiderthelastlayer
normalfew-shotsettingsinclassificationtasks. In meanattentionofourtunedmodel. Weemployed
thisexperiment,thein-contextdemonstrationsare Spearman’s Rank Correlation Coefficient (Spear-
selected with an approximate size of 250 tokens. man,1904)tocomputethecorrelationbetweenthe
Thecomparisontotheresultwiththetokensizeof threevariables. Itiscalculatedfordifferenttradi-
750inSection4.3ispresentedinTable6. tional classification tasks and the averaged value
Selection-p shows the best performance under across tasks. The result presented in Figure 2 in-
the constraint of 250 tokens when compared to dicatesonlyaweakcorrelationobservedbetween
other compression models. Additionally, it also the p value and the other two variables while the
followsthefull-shot(i.e. 750tokenslevel)trend, correlation between the last layer mean attention
the average performance across all classification andperplexityismoresignificant. Amongalltasks,LLaMA-2-13B
Subj RTE WSC BoolQ MultiRC SST-2 WIC COPA AGNews AVG
Full-shot 91.6 74.8 46.9 67.7 45.7 94.7 54.6 77.6 79.2 70.3
LLMLingua 53.6 61.0 63.2 70.6 51.5 64.3 50.0 78.1 58.0 61.4
LLMLingua-2 48.3 68.7 41.3 75.8 52.2 51.3 49.0 48.2 71.1 56.2
Selection-p 69.3 69.5 65.4 74.7 50.7 81.1 50.5 87.5 63.2 68.0
Table4: Analysisoftransferabilitytoopen-sourcemodelLLaMA-2-13B.Theexperimentisperformedon750
tokensin-contextdemonstrationswitha10xcompressionrate.
0.6
ChatGPT Gemini Subj 0.18 -0.013 0.32
GPT-3.5-Turbo Gemini-1.0-Pro
RTE 0.15 0.049 0.33
Full-shot 74.2 73.3 0.5
WSC 0.29 0.19 0.42
LLMLingua 58.6 40.2
LLMLingua-2 55.7 51.9
BoolQ 0.16 -0.039 0.18 0.4
Selection-p 62.9 58.9
MultiRC 0.11 0.022 0.2
Table5: Analysisoftransferabilitytoblackboxmod-
0.3
SST-2 0.18 -0.025 0.32
els(i.e. ChatGPTandGemini). Theexperimentisper-
formedon750tokensin-contextdemonstrationswitha
WIC 0.25 0.43 0.6
3xcompressionrate. 0.2
COPA 0.18 0.28 0.35
AG News 0.17 0.086 0.28 0.1
WICdemonstratesaprominentlyhighvaluecom-
paredtoothers,thismayexplainthesmallvariation AVG 0.19 0.11 0.33
in accuracy across different compression models BANKING77 -0.0073 0.063 0.33 0.0
anddifferentexperimentalsettings.
p_a p_ppl ppl_a
spearmanr
5.4 TokensLevelPart-of-SpeechAnalysis
Tofurtherinterprettherationalebehindourcom- Figure 2: Spearman’s Rank Correlation Coefficient
(spearmanr)betweenp(p)value,meanattention(a)and
pressionmodels,weanalyzewhatkindsofwords
token-levelperplexity(ppl)acrossdifferenttraditional
arelikelypreservedbySelection-p. Underthedis-
classificationtasks.
cretenessofourcompressionresult,welocatethe
correspondingwordsfromthecompressedtokens setting,itcanbeusefulhintsfortheanswerderiva-
andobtainthePart-of-Speech(PoS)tagswithan tion.
NLTKtagger. ForeachtypeofPoStag,wecom- Thehighpreservationratioofpunctuationmay
putethetokenpreservationpercentagewith indicatealargeredundancyinasentence,andtrun-
cating sentence separation tokens is undesirable.
|compressed_token | Additionally,ashighlightedbyWangetal.(2023),
tag
i
|total_token | formattinginformation(i.e. structureofthedemon-
tag
i
strations)mattersalotinin-contextlearning. How-
foreachPoStagtag . Theexperimentisconducted ever,formattingtokens(i.e. “:”) areunlikelytobe
i
between the compressed result and the original preserved with Selection-p compared to the orig-
demonstrationsamongtheninetraditionalclassifi- inal distribution in our case. In general, we also
cationtaskswithfourdemonstrationsetspertask. observe a higher degree of preservation of noun
We analyze tags with a frequency of appearance phrasescomparedtoverbs.
greaterthan1%.
5.5 OnFairComparisonwithLLMLingua
FromtheresultpresentedinFigure3,PRPand
punctuations (i.e. indicating the start of the next AsdescribedinSection4.2,LLMLinguaconducts
sentenceorphrase)aremorelikelypreserved. The demonstrationselectionpriortocompressionatthe
potentialreasonforthehighpreservationratioon tokenlevel,whileothermethodscompressdirectly
PRP(personalpronoun)likelycorrespondstothe onthetokenlevel. Sincethedemonstrationselec-
pronoun resolution task of WSC. Under the task tion process can also be incorporated into otherSubj RTE WSC BoolQ MultiRC SST-2 WIC COPA AGNews AVG
Zero-shot 49.3 58.8 43.4 67.4 52.5 67.7 50.8 52.5 63.3 56.2
Full-shot 81.3 69.9 51.2 62.7 46.8 89.3 51.8 85.1 67.9 67.3
∆ -0.6 +0.8 -9.4 +0.1 ±0 +3.2 +4.6 +0.5 +8.4 +0.9
Full-shot
AutoCompressor 56.2 61.5 44.2 68.3 52.7 93.0 51.5 83.6 76.1 65.2
∆ +1.7 -5.4 -4.8 -1.8 -0.9 -0.2 +1.5 +0.8 +4.8 -0.5
AutoCompressor
LLMLingua* 55.6 61.4 61.3 68.2 53.1 81.1 50.2 75.8 70.9 64.2
∆ +1.1 -0.5 +1.7 +1.2 -2.8 -11.2 +1.4 +0.6 -9.4 -2.0
LLMLingua
LLMLingua-2 52.4 65.3 63.9 66.1 50.9 82.9 50.8 77.8 56.3 62.9
∆ +4.9 +2.5 -0.2 +3.7 +1.9 -16.7 -0.5 -6.4 +5.6 -0.6
LLMLingua-2
Selection-p 65.7 65.5 58.7 67.5 54.3 81.3 50.4 77.9 68.3 65.5
∆ +2.8 +3.0 +2.4 +2.3 +0.1 +9.4 -0.1 -1.0 -1.5 +1.9
Selection-p
Table6: Performancewithdifferentnumberofdemonstrationsfromabout250tokenstoabout750tokens. ∆
referstotheperformanceenhancementthatcanbeachievedbyincreasingthedemonstrationtokenssizeto750.
1x 2x 5x 10x
End-to-Endwithoutcompression 298.6
End-to-EndwithSelection-p 167.0(1.8x) 81.6(3.7x) 55.6(5.3x)
LLMLinguaperdemonstrationsset - 0.82 0.82 0.81
Selection-pperdemonstrationsset - 0.68 0.67 0.67
Table 7: Latency(s) comparison on WSC in 750 tokens level with about 16 demonstrations. We present
the averaged complete end-to-end inference with and without Selection-p among four sets of demonstrations.
ComparisonisconductedwithLLMLinguawhichalsobuildsupontheLLaMA-2-7Bbackbone,withtheaveraged
compressiontimeofthein-contextdemonstrationsbeingpresented.
compressionmodels,weonlyutilizethemodified WSC rate
versionofLLMLinguainthepreviousexperiments
Selection-p 61.1 10x
toensureafaircomparison. LLMLingua 63.0 10x
In this section, we further analyze the per-
Selection-p(+BudgetController) 47.6 10x
Selection-p(+BudgetController) 57.0 38x
formance by comparing our proposed method, LLMLingua(whole) 44.7 10x
equipped with the Budget Controller, with the
whole LLMLingua to provide a comprehensive Table 8: Comparison with LLMLingua on Budget
Controller. Adoptingdifferentstrategiesinequipping
analysis. We follow the setting described in Sec-
Selection-p with Budget Controller, leads to the two
tion 4.3 and select the WSC task for our experi-
differentcompressionrates(rate)of10xand38x.
ment. To illustrate, in the original demonstration
setconsistingof16demonstrations,theLLMLin-
gua API retains only four demonstrations. This Furthermore, there is a disparity between the
leadstotwooptionswithSelection-p: (1)continu- instructed compression rate and the actual com-
ouslyapplyingthe10xcompressiondirectlytothe pression rate in LLMLingua. The target size for
filteredsetoffourdemonstrationsandresultingin compressedtokensis75,whileLLMLinguatypi-
afinalcompressionrateof38x,and(2)adjusting callyachievesanaveragecompressedtokensizeof
the compression rate of Selection-p to achieve a around192.1,whichismorethan1.5timeshigher
finalcompressionrateof10x. thanthedesiredrateacrossallclassificationtasks.
TheresultpresentedinTable8demonstratesthe
6 Conclusion
significantimpactoftheBudgetController. Simi-
lartrendsinperformanceforbothSelection-pand Weintroduceasimpleyeteffectiveself-supervised
LLMLinguaareobserved(i.e.,adecreaseinperfor- approachincontextcompressionandconducteval-
manceontheWSCtask). Notably,theperformance uation across 10 classification tasks in both few-
ofSelection-psurpassesLLMLinguaafterequip- shotandlong-contextsettings. Ourapproachalso
pingwiththeBudgetController. demonstratedgreattransferabilitytoboththeopen-0.3
0.2
0.1
0.0
Part of Speech
Figure3: AnalysisofthetokenpreservationpercentagewithrespecttodifferenttypesofPart-of-Speechtagsunder
10xcompressionrate.
source(i.e. LLaMA-2-13B)andblack-boxmodels EleventhInternationalConferenceonLearningRep-
(i.e. ChatGPTandGemini),withperformancesur- resentations.
passing the existing state-of-the-art compression
TomB.Brown,BenjaminMann,NickRyder,Melanie
models. Analysisisalsoconductedamongdiffer- Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
entcompressionratesanddemonstrationlengths. Neelakantan,PranavShyam,GirishSastry,Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Witha10xcompressionrate,ourmodelonlyshows
Gretchen Krueger, Tom Henighan, Rewon Child,
a 0.8-point drop in performance across different
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
traditionalclassificationtaskswitha5.3xspeedup. ClemensWinter,ChristopherHesse,MarkChen,Eric
Throughexperimentsinlong-contextsettings,our Sigler,MateuszLitwin,ScottGray,BenjaminChess,
Jack Clark, Christopher Berner, Sam McCandlish,
workalsopresentsthepossibilityofaddressingthe
Alec Radford, Ilya Sutskever, and Dario Amodei.
in-contextlearningissueoftherecentlong-context
2020. Languagemodelsarefew-shotlearners. InAd-
models. Both efficiency enhancement as well as vancesinNeuralInformationProcessingSystems33:
performancepreservationareshowninourmodel. AnnualConferenceonNeuralInformationProcess-
ing Systems 2020, NeurIPS 2020, December 6-12,
Acknowledgement Thisresearchhasbeenmade 2020,virtual.
possible by the Hong Kong PhD Fellowship pro-
AydarBulatov,YuriKuratov,andMikhailBurtsev.2022.
videdtoTszTingChungandtheResearchImpact Recurrentmemorytransformer. InAdvancesinNeu-
FundprojectR6003-21providedbytheResearch ralInformationProcessingSystems.
GrantsCouncilofHongKongtoDit-YanYeung.
Iñigo Casanueva, Tadas Temcˇinas, Daniela Gerz,
MatthewHenderson,andIvanVulic´.2020. Efficient
Limitations
intentdetectionwithdualsentenceencoders. InPro-
ceedingsofthe2ndWorkshoponNaturalLanguage
Undertheconsiderationofcost,wedidnotperform
ProcessingforConversationalAI,pages38–45,On-
further analysis on other LLMs apart from Chat- line.AssociationforComputationalLinguistics.
GPT and Gemini. In addition, our model which
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
builds up LLaMA-2-7B does not achieve better
ZhijianLiu,SongHan,andJiayaJia.2023. Longlora:
latencythanmodelslikeLLMLingua-2andAuto- Efficientfine-tuningoflong-contextlargelanguage
Compressor. UndertheICLsetting,minimaltime models. ArXivpreprint,abs/2309.12307.
is required for compression, leading to insignifi-
AlexisChevalier,AlexanderWettig,AnirudhAjith,and
cance in end-to-end inference time. While Auto- Danqi Chen. 2023. Adapting language models to
Compressoroffersbetterlatency,itssoftcompres- compresscontexts. InProceedingsofthe2023Con-
ferenceonEmpiricalMethodsinNaturalLanguage
sionnaturelimitsitsapplicabilitytootherLLMs.
Processing,pages3829–3846,Singapore.Associa-
Overall,ourexperimentsacrossvarioustasksand
tionforComputationalLinguistics.
settingsdemonstratebetterperformanceandtrans-
TaoGe,HuJing,LeiWang,XunWang,Si-QingChen,
ferability,withthebenefitsoutweighingthelatency
andFuruWei.2024. In-contextautoencoderforcon-
issue.
textcompressioninalargelanguagemodel. InThe
TwelfthInternationalConferenceonLearningRepre-
sentations.
References
Saurabh Goyal, Anamitra Roy Choudhury, Saurabh
DanielBolya,Cheng-YangFu,XiaoliangDai,Peizhao Raje,VenkatesanT.Chakaravarthy,YogishSabhar-
Zhang,ChristophFeichtenhofer,andJudyHoffman. wal, and Ashish Verma. 2020. Power-bert: Accel-
2023. Token merging: Your vit but faster. In The eratingBERTinferenceviaprogressiveword-vector
)%(
noitavreserP
, . PRP OT SNN SOP JJ BR NN PNN DC TD NBV DBV ZBV GBV BV PBV NI CC : $PRPelimination. InProceedingsofthe37thInternational JesseMu,XiangLi,andNoahGoodman.2023. Learn-
ConferenceonMachineLearning,ICML2020,13-18 ing to compress prompts with gist tokens. In Ad-
July2020,VirtualEvent,volume119ofProceedings vances in Neural Information Processing Systems,
of Machine Learning Research, pages 3690–3699. volume36,pages19327–19352.CurranAssociates,
PMLR. Inc.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
OpenAI.2023. GPT-4technicalreport. ArXivpreprint,
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
abs/2303.08774.
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin
arXiv:2106.09685.
Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor
Rühle,YuqingYang,Chin-YewLin,H.VickyZhao,
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Lili Qiu, and Dongmei Zhang. 2024. Llmlingua-
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
2: Data distillation for efficient and faithful task-
WeizhuChen.2022. Lora: Low-rankadaptationof
agnosticpromptcompression.
largelanguagemodels. InTheTenthInternational
ConferenceonLearningRepresentations,ICLR2022,
YongmingRao,WenliangZhao,BenlinLiu,JiwenLu,
VirtualEvent,April25-29,2022.OpenReview.net.
JieZhou,andCho-JuiHsieh.2021. Dynamicvit: Ef-
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing ficientvisiontransformerswithdynamictokenspar-
Yang,andLiliQiu.2023. LLMLingua:Compressing sification. InAdvancesinNeuralInformationPro-
promptsforacceleratedinferenceoflargelanguage cessingSystems34: AnnualConferenceonNeural
models. InProceedingsofthe2023Conferenceon InformationProcessingSystems2021,NeurIPS2021,
Empirical Methods in Natural Language Process- December6-14,2021,virtual,pages13937–13949.
ing,pages13358–13376,Singapore.Associationfor
ComputationalLinguistics. C. Spearman. 1904. The proof and measurement of
associationbetweentwothings. AmericanJournal
Gyuwan Kim and Kyunghyun Cho. 2021. Length-
ofPsychology,15:88–103.
adaptivetransformer: Trainoncewithlengthdrop,
useanytimewithsearch. InProceedingsofthe59th
GeminiTeam.2023. Gemini: Afamilyofhighlycapa-
AnnualMeetingoftheAssociationforComputational
blemultimodalmodels.
Linguisticsandthe11thInternationalJointConfer-
ence on Natural Language Processing (Volume 1:
TogetherAI. 2023. Redpajama: An open dataset for
LongPapers),pages6501–6511,Online.Association
traininglargelanguagemodels.
forComputationalLinguistics.
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
SehoonKim,ShengShen,DavidThorsley,AmirGho-
Martinet,Marie-AnneLachaux,TimothéeLacroix,
lami, Woosuk Kwon, Joseph Hassoun, and Kurt
BaptisteRozière,NamanGoyal,EricHambro,Faisal
Keutzer.2022. Learnedtokenpruningfortransform-
Azhar,AurelienRodriguez,ArmandJoulin,Edouard
ers.
Grave,andGuillaumeLample.2023. Llama: Open
Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and andefficientfoundationlanguagemodels.
WenhuChen.2024. Long-contextllmsstrugglewith
longin-contextlearning. Szymon Tworkowski, Konrad Staniszewski, Mikołaj
Pacek,YuhuaiWu,HenrykMichalewski,andPiotr
YuchengLi,BoDong,FrankGuerin,andChenghuaLin.
Miłos´.2023. Focusedtransformer: Contrastivetrain-
2023. Compressingcontexttoenhanceinferenceef-
ingforcontextscaling.
ficiencyoflargelanguagemodels. InProceedingsof
the2023ConferenceonEmpiricalMethodsinNatu-
AlexWang,YadaPruksachatkun,NikitaNangia,Aman-
ralLanguageProcessing,pages6342–6353,Singa-
preetSingh,JulianMichael,FelixHill,OmerLevy,
pore.AssociationforComputationalLinguistics.
andSamuelR.Bowman.2019. Superglue:Astickier
benchmarkforgeneral-purposelanguageunderstand-
Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and
ing systems. In Advances in Neural Information
Luke Zettlemoyer. 2022. Noisy channel language
ProcessingSystems32: AnnualConferenceonNeu-
modelpromptingforfew-shottextclassification. In
ralInformationProcessingSystems2019,NeurIPS
Proceedings of the60th Annual Meeting of the As-
2019,December8-14,2019,Vancouver,BC,Canada,
sociationforComputationalLinguistics(Volume1:
pages3261–3275.
LongPapers),pages5316–5330,Dublin,Ireland.As-
sociationforComputationalLinguistics.
LeanWang,LeiLi,DamaiDai,DeliChen,HaoZhou,
Ali Modarressi, Hosein Mohebbi, and Moham- FandongMeng,JieZhou,andXuSun.2023. Label
madTaherPilehvar.2022. AdapLeR:Speedingup wordsareanchors: Aninformationflowperspective
inferencebyadaptivelengthreduction. InProceed- for understanding in-context learning. In Proceed-
ingsofthe60thAnnualMeetingoftheAssociation ingsofthe2023ConferenceonEmpiricalMethods
forComputationalLinguistics(Volume1: LongPa- inNaturalLanguageProcessing,pages9840–9855,
pers),pages1–15,Dublin,Ireland.Associationfor Singapore.AssociationforComputationalLinguis-
ComputationalLinguistics. tics.JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Bosma,BrianIchter,FeiXia,EdChi,QuocLe,and
DennyZhou.2023. Chain-of-thoughtpromptingelic-
itsreasoninginlargelanguagemodels.
David Wingate, Mohammad Shoeybi, and Taylor
Sorensen.2022. Promptcompressionandcontrastive
conditioningforcontrollabilityandtoxicityreduction
inlanguagemodels. InFindingsoftheAssociation
forComputationalLinguistics: EMNLP2022,pages
5621–5634,AbuDhabi,UnitedArabEmirates.As-
sociationforComputationalLinguistics.A ExamplesIllustrationofCompression Consideringthatthetimetakenforcompression
Results onthedemonstrationsetisnegligiblewhencom-
pared to the time required for inference (approx-
Figure4showsexamplesoftwotraditionalclassifi-
imately at a ratio of 0.01), the end-to-end infer-
cationtasks(i.e. SubjandWSC)withSelection-p
ence time for thetoken-selection-based compres-
under10xcompression,displayingboththecom-
sionmodelsisroughlythesame. Theend-to-end
pressedresultsandtheoriginaldemonstrationsets
inferencetimeresultfortheWSCtaskonLLaMA-
before compression. Additionally, Figure 5 illus-
2-7BisshowninTable10. Thetableispresented
tratesanexampleforBANKING77under10xcom-
inorderoftheinferencetimeforclarity.
pression.
AutoCompressor LLMLingua-2 Selection-p LLMLingua
B PromptofEvaluationonBANKING77
22.73 55.6 55.6 55.6
InourprompttoChatGPT,wefirstlistoutall77
Table10: End-to-endinferencetimeontheWSCtask.
labels and then provide a list of demonstrations.
Thetemplateisdetailedbelow,
Answer in activate_my_card or age_limit
or apple_pay_or_google_pay or
atm_support or automatic_top_up or bal-
ance_not_updated_after_bank_transfer or
... or wrong_amount_of_cash_received or
wrong_exchange_rate_for_cash_withdrawal.
Context: <context>
Answer: <answer>
C TrainingDetails
WeuseLLaMA-2-7Bforcompression(i.e. token
selection). It takes roughly 50 hours on a single
A100GPUtotrainon100MtokensfromRedPa-
jama.
D DetailedLatencyAnalysis
Forthetoken-basedcompressionmodels,thetime
neededforcompressionperdemonstrationseton
theWSCtaskispresentedinTable9.
LLMLingua-2 Selection-p LLMLingua
0.15 0.67 0.81
Table9: Timeneededforcompressionperdemonstra-
tionsetontheWSCtask.
UndertheICLsetting,thesamedemonstration
set is used consistently, and compression on the
demonstrationsetonlyneedstobecomputedonce
forallsubsequentinferences. Thiscontributedto
theshortcompressiontimeinthecompleteend-to-
endprocess.[Subj]OriginalDemonstrations:
input: eachoftheprincipalshasaradicallydifferentwayofdealingwithit.
type: objective
input: well-intentioned though it may be , its soap-opera morality tales have the antiseptic ,
preprogrammedfeelofanafter-schoolspecial.
type: subjective
...
input: anastonishingfeatforamajorstarletalonea27yearoldfrompickum,southcarolinawho
onlytwoyearsagowassleepinginacardboardboxinthebackalleysofdetroitwithhermother,
connie,andheruncleclutch,whileplayingguitaronthestreetsforsparechange.
type: objective
input: mayisayoungstrangegirlwhohadaverydisturbedchildhoodanddoesnotstillknowthe
meaningoftruefriendshiporlove.
type: objective
[Subj]Compresseddemonstrations(10x):
input:-intitmay-morexclordtreasureplanet-thestemoments-itthrenarwithheandlcrowd
grua-ermown–intoimposiblyrisgar“irwhshdeprerepredeitmagmrophilosophvagrede
thesvdespcertwz-passerastonfetwowithclfordistandstill
[WSC]OriginalDemonstrations:
Question: Inthesentence"JamesaskedRobertforafavorbuthewasrefused.",doesthepronoun
’he’refertoRobert?
Answer: no
Question: Inthesentence"Whataboutthetimeyoucutuptulipbulbsinthehamburgersbecause
youthoughttheywereonions?",doesthepronoun’they’refertotulipbulbs?
Answer: yes
...
Question: Inthesentence"WhenMr. Bond,theveterinarian,cametolookattheblackhorsethat
laygroaningonthegrass,hefelthimallover,andshookhishead;oneofhislegswasbroken.",
doesthepronoun’his’refertotheblackhorse?
Answer: no
Question: Inthesentence"SamtookFrenchclassesfromAdam,becausehewaseagertospeakit
fluently.",doesthepronoun’he’refertoAdam?
Answer: no
[WSC]Compresseddemonstrations(10x):
"pron:: "tulpron’tul: hepron’toldPwhichP.Hehavepron’PathewouldonlyGruunepron
pronIputcfrItpronrefrigermanpronJohnwheng. HeveryimWainwsdFolhepronveteringro
hisprontherepapronwtwingro"hepron’
Figure4: IllustrationofthecompressionresultbySelection-pforSubjandWSCtasksunder10xcompressionrate.
Compressionisperformedwith19demonstrationsforSubjwhileitisperformedwith16demonstrationsforWSC
withtotalsumofabout750tokensrespectively.[BANKING77]OriginalDemonstrations:
Context: WhydidusinganATMcausemetobechargedanadditionalfee?
Answer: cash_withdrawal_charge
Context: Iaskedforarefundbutitsnothereyet
Answer: Refund_not_showing_up
Context: isthereareasonineedtoverifytopup
Answer: verify_top_up
...
Context: ThereisapaymentonmycardthatIdonotrecognize. I’veneverseenthenameonthe
transactionbefore.
Answer: card_payment_not_recognised
Context: Ihappenedtoforgetmypasscode
Answer: passcode_forgotten
Context: Imadeacashwithdrawalanditisstilllistedasapendingtransaction.
Answer: pending_cash_withdrawal
[BANKING77]Compresseddemonstrations(10x):
c_withaskedrefRefnoting_up__recogn_____cwrong_rece_transaction_chargtwunblockactivac-
tiv_not__fe_charg_tim: exshoPleaserevert__thecard__wr__: I:pending__: thecard_not_recogn:
passf:_c
Figure 5: Illustration of the compression result by Selection-p for BANKING77 under 10x compression rate.
Compressionisperformedwith27demonstrationswithtotalsumofabout750tokens.