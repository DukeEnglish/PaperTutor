Preprint.
MLLM CAN SEE? DYNAMIC CORRECTION DECODING
FOR HALLUCINATION MITIGATION
ChenxiWang♠∗,XiangChen♠∗,NingyuZhang♠†,BozhongTian♠,HaomingXu♠,
ShuminDeng♢,HuajunChen♠†
♠ZhejiangUniversity ♢NationalUniversityofSingapore,NUS-NCSJointLab,Singapore
{sunnywcx,xiang chen,zhangningyu}@zju.edu.cn
ABSTRACT
Multimodal Large Language Models (MLLMs) frequently exhibit hallucination
phenomena, but the underlying reasons remain poorly understood. In this paper,
wepresentanempiricalanalysisandfindthat,althoughMLLMsincorrectlygen-
erate the objects in the final output, they are actually able to recognize visual
objects in the preceding layers. We speculate that this may be due to the strong
knowledgepriorsofthelanguagemodelsuppressingthevisualinformation,lead-
ing to hallucinations. Motivated by this, we propose a novel dynamic correction
decoding method for MLLMs (DeCo), which adaptively selects the appropriate
preceding layers and proportionally integrates knowledge into the final layer to
adjust the output logits. Note that DeCo is model agnostic and can be seam-
lessly incorporated with various classic decoding strategies and applied to dif-
ferent MLLMs. We evaluate DeCo on widely-used benchmarks, demonstrating
that it can reduce hallucination rates by a large margin compared to baselines,
highlightingitspotentialtomitigatehallucinations1.
“Thefirstprincipleisthatyoumustnotfoolyourself—andyouaretheeasiestpersontofool.”
—RichardFeynman
1 INTRODUCTION
Recently, the rapid development of Multimodal Large Language Models (MLLMs) has demon-
strated a potential pathway towards achieving Artificial General Intelligence (AGI) (Wang et al.,
2024;Yaoetal.,2024;Luetal.,2024a;Team,2024;OpenAI,2023;Liuetal.,2023b;Chernetal.,
2024).However,inpractice,thedevelopmentofMLLMsishinderedbythephenomenonofhalluci-
nation,whichtypicallyresultsinthemodelgeneratingstatementsaboutnon-existentimageswhile
neglecting to mention certain visible objects, effectively causing it to fool itself (Bai et al., 2024;
Liuetal.,2024a;Lietal.,2023b;Liuetal.,2023a;Rawteetal.,2023).Thisissueposessignificant
risksinhigh-stakesfieldssuchasmedicalimaging(Chenetal.,2024a;Huetal.,2023;Wangetal.,
2023b),autonomousdriving(Cuietal.,2024;Wangetal.,2023c),andhuman-computerinteraction
systems(Brieetal.,2023),wheresucherrorscouldresultinirreparableconsequences.
The reasons behind hallucinations in MLLMs are complex. Unlike analyses focused on unimodal
LLMs (Chuang et al., 2024; Chen et al., 2024c; Orgad et al., 2024; Chen et al., 2024d; Lu et al.,
2024b), many current works assume that MLLM may indeed ‘see’ visual information. However,
duetofactorssuchasexcessivemodeldepth(Chenetal.,2024b;Zhangetal.,2024a),aggregation
patterns(Huangetal.,2024),orpriorsknowledgeinherentintheMLLMs(Lengetal.,2023;Zhang
etal.,2024b),thesemodelsultimatelystillexperiencehallucinations.Concretely,ourunderstanding
of the underlying mechanisms of hallucinations in MLLMs remains limited. It is still uncertain
whetherthevisualinformationisnevercorrectlyrecognizedorifitisrecognizedbutsubsequently
suppressedbylaterinformationstreams.
HallucinatedMLLMcansee(tosomeextent).Inspiredbytheaforementionedworks,weconduct
anempiricalanalysisandfindthatMLLMsarenotblind;theycanrecognizeobjectsinthepreceding
∗ EqualContribution.
† CorrespondingAuthor.
1Codeisavailableinhttps://github.com/zjunlp/DeCo.
1
4202
tcO
51
]LC.sc[
1v97711.0142:viXraPreprint.
100 In-Dist OOD 100 Adversarial Random Popular
90 90
80 80
70 70
60 60
all data exist non-exist all data exist non-exsit
AMBER POPE
(a) Objectprobingresults. (b) Differentresolutionresults.
Figure 1: Overall results of the probing experiment with MLLMs, indicating that they possess a
certain level of awareness regarding the presence of visual objects (Figure 1(a)), with prediction
accuracybeinghigherintheprecedinglayers(Figure1(b))butgraduallydeclineafterward.
layers, but this recognition is suppressed in later layers, leading to hallucinations. Specifically, we
focus on object hallucinations2 and conduct experiments with MLLMs, demonstrating that they
know to some extent whether an object exists (as shown in Figure 1 and Section 2.1). We further
observe that the confidence of generated tokens is influenced by the knowledge priors of MLLMs
(Section2.2),leadingtoareductionintheprobabilityofgroundtruthtokensinthedeeperlayers.
Dynamiccorrectiondecodingwithpreceding-layerknowledge.Basedonthosefindings,wepro-
pose Dynamic Correction Decoding with preCeding-Layer Knowledge (DeCo) to mitigate hallu-
cinations for MLLMs. Our core hypothesis is that preceding layers exhibit higher confidence for
groundtruthtokens,andthelogitsforthesetokensshouldrankprominentlyatthelastlayer’sout-
puts. To enhance the logits of ground truth tokens, DeCo dynamically selects preceding layer and
utilizesitspriorknowledgetocorrectthefinaloutputlogits.Additionally,weintroduceadynamic
softmodulationtopreservetheoriginalstyleofthegeneratedresponses.DeCoistraining-freeand
canbeintegratedwithanypopulardecodingstrategies,suchasgreedysearch,nucleussamplingas
wellasbeamsearch,andcanseamlesslyincorporateintoanyMLLMsforhallucinationmitigation.
Contributions.Ourprimarycontributionliesinexploringtheinternalmechanismsofhallucinations
inMLLMs.Wefindthattheconfidenceofgeneratedtokensisinfluencedbytheknowledgepriors
ofMLLMs,leadingtoareductionintheprobabilityofgroundtruthtokensinthedeeperlayers.We
further propose DeCo, a dynamic correction decoding method guided by preceding-layer knowl-
edge.DeCoisintegratedwithInstructBLIP,MiniGPT-4,LLaVA,andQwen-VLusingthreepopular
decodingstrategies:greedysearch,nucleussampling,andbeamsearch.Experimentalresultsshow
thatDeCoachievesanaveragehallucinationsuppressionrateof10.8%inimagecaptioningdataset,
demonstratingsuperiorsuppressioneffectiveness.Additionally,DeCooutperformsbaselinesonvi-
sualquestionansweringdatasetsincludingPOPE,andMME.Additionally,weanalyzethelatency
andthroughput,showingthatDeCointroducesanapproximate1.2xincreaseinlatencycomparedto
thebasicdecodingprocess,muchfasterthanpreviousbaselinessuchasVCDandOPERA.
2 WHY DO MLLMS GENERATE NON-EXIST OBJECTS?
Inthissection,weconductaseriesofempiricalanalysistoinvestigatetheinternalmechanismsof
MLLM and elucidate the underlying reasons for its generation of non-existent objects. To strike a
balancebetweentherealismandcomplexityoftheexperiments,weprimarilyfocusonthegenera-
tionofobjectsinimagedescriptionscenarios(imagecaptiontasks).
PreliminariesofMLLMgeneration. MLLMstypicallyconcatenatevisualtokens,processedby
thevisualencoderandprojectionlayer,withembeddedtextualtokensbeforefeedingthemintoan
autoregressive language model. We denote the visual tokens as XV = {x ,x ,...,x } and
v1 v2 vP
textual tokens as XC = {x ,x ,...,x }. Here P and Q are the lengths of the visual tokens
c1 c2 cQ
and textual tokens respectively. Finally, the input is X = concat{XV,XC}. Then X would be
passed into MLLM with N stacked transformer layer. The intermediate variable generated by the
2Thisapproachisapplicabletoothertypesofhallucinationsaswell.
2
)%(ycaruccA )%(ycaruccAPreprint.
Describe the image.
User
The image features a beach
with several people swimming
in the ocean. There are two
chairs visible on the sandy
beach, possibly for people to sit
and relax. The purple chair is on
MLLMs the left and the green chair is
on the right side. Additionally,
there is a small bird flying
around the area, adding to the
lively atmosphere of the beach.
Figure2:Illustrationoftokenprobabilitiesacrosstransformerlayers,whichrevealsdistincttrends
fortargethallucinated(orange)andnon-hallucinated(green)tokens.Intheprecedinglayers,non-
hallucinatedtokensexhibitahigherprobability.Inthefinallayers,hallucinatedtokensdemonstrate
increasedprobabilities,whiletheprobabilityofnon-hallucinatedtokensdropssharply.
i-thlayeriscalledhiddenstates,denotedashi = {hi,hi,...,hi },whereT = P +Q.During
0 1 T−1
thegenerationphase,weusethehiddenstateatthelastpositioninthefinallayer,whichismapped
tothevocabularydimensionthroughanaffinelayerϕ(·),topredicttheprobabilityofthenexttoken.
Formally,wehave:
p(x |x )=softmax(ϕ(hN )) ,x ∈V (1)
T <T T−1 xT T
whereweusex tosimplifythesequence{x }T−1andV referstothewholevocabularyset.
<T i i=0
2.1 FINDING1:MLLMKNOWSTOSOMEEXTENTWHETHERANOBJECTEXISTS
Inspiredby(Yeetal.,2024),weexplorehowMLLMscomprehendobjectsintheimagecaptioning
task. For simplicity, we abstract this process into a function called isexist(obj), which determines
whether an object is present in an image. To examine the application of this function within the
MLLM’simagecaptioningworkflow,weconductprobingexperimentsattheconclusionofobject
descriptionsineachlayeroftheMLLM’slanguagemodelcomponent,whichconsistsof32trans-
formerlayersina7-billion-parametermodel(DetailedsetupinAppendixA.1).
We employ the prompt template, “USER: <image>Describe the image. ASSISTANT: The image
contains obj.” Both the training and testing datasets are formatted accordingly before being input
into MLLMs. We train a probe classifier at the final position of the hidden state outputs for each
transformerlayer,resultinginatotalof32classifiers.(Fordetailsonthesubsetdivision,OODand
in-distributionsplits,andprompttemplates,pleaserefertoAppendixA.1.)Themodelisevaluated
usingthetestset,asshowninFigure1(a)(left).Furtherexperimentsareconductedonthreesplits
of the evaluation dataset proposed by POPE, with results reported in Figure 1(a) (right). These
evaluations provide a comprehensive understanding of the model’s object recognition capabilities
acrossdiversescenarios.
Weselectthebest-performingprobeclassifierfromthe32classifierstocompareaccuracyacrossall
objects,existingobjects,andnon-existingobjects.OurresultsshowthattheMLLMachieveshigh
accuracy for correctly generated objects in image captions. Despite generating many non-existent
objects,theMLLMstillmaintainsaround80%accuracyinourprobingexperiments.Thissuggests
thatMLLMspossessacertainlevelofunderstandingregardingobjectexistenceinimages.
Additionally,ourprobingexperimentsrevealhigheraccuracyintheprecedinglayers,asillustrated
inFigure1(b),whichalignswithpreviousfindings(Zhangetal.,2024b;Lengetal.,2023).Further-
more,weshowthatincreasingtheresolutionofthevisualencoder(from224pxto336px)enhances
accuracy for non-existing objects, indicating that token information at the last position in the
precedinglayersbetterrepresentsvisualinformation.(Foradetailedexplanationofthedifferent
3Preprint.
visual resolutions, please refer to Appendix A.1) These findings can deepen our understanding of
theobjectcomprehensionprocessesinMLLMsandhighlightpotentialavenuesforimprovement.
2.2 FINDING2:LANGUAGEMODELPRIORSSUPPRESSTHEVISUALINFORMATIONTHAT
MLLMALREADYSEE.
Wehypothesizethattherepresentationsintheprecedinglayerseffectivelycapture(tosomeextent)
visualinformation.However,thepriorknowledgeembeddedintheMLLMreducestheprobabilities
ofgroundtruthtokensindeeperlayers.Figure2illustratesthishypothesiswithrunningexamples.
WeanalyzetheTop-4tokensrankedbyprobabilityinthefinallayer’soutput.Non-hallucinatedto-
kenslike“people”,“left”,“blue”,and“umbrella”exhibithighprobabilitiesfromthe18thlayer.
In contrast, hallucinated tokens like “bird” and “green” only show comparatively high probabili-
ties around the 30-th layer. Interestingly, the probabilities of ground truth tokens “umbrella” and
“blue”sharplydeclinefromthe30-thlayeronwards,eventuallyfallingbelowthehallucinatedto-
kens’probabilitiesinthefinallayer.
To further investigate this phenomenon, we conduct an early
exitexperiment (Teerapittayanonetal.,2016;Elbayadetal., 160
threshold=0.05
2020; Schuster et al., 2022) to analyze the evolution of the 140 threshold=0.1
120 threshold=0.2
MLLM’s internal representations across transformer layers. 100
We randomly select 500 images from the MSOCO dataset 80
60
anduserandompromptstoelicitrawresponsesfromLLaVA- 40
20
1.5-7b. We then extract all non-existent objects along with
0
5 10 15 20 25 30
theircorrespondingprecedingtextandinputthisdataintothe Layer
MLLM.Weobservetheprobabilitiesofthenexttokenacross
thetransformerlayerstogaininsightsintothemodel’sbehav- Figure 3: Distribution of activated
ior (see Appendix A.2 for detailed experimental setup). The ground-truthtokensacrosslayers.
output of the i-th layer is denoted as hi, and the probability
distributionofthenexttokenisrepresentedasp(·|x )i =softmax(ϕ(hi )).Toreducetheobser-
<s s−1
vationtokensandsimulatetherealsamplingprocess,wetruncatethevocabulary,similartoTop-p
sampling, and obtain the candidate tokens, denoted as V with a default threshold of 0.9.
candidate
WethenlabelthetokensinV .Specifically,wefilteroutdatawhereV containsat
candidate candidate
least one ground truth token and observe whether an activated ground truth token exists among
thecandidatetokens,formallyexpressedas:
∃x ∈V ∧i∈(0,N], p(x |x )i−p(x |x )i ≥threshold, (2)
a candidate a <s h <s
wherex istheactivatedgroundtruthtoken,x isthetokenwiththehighestprobabilityofbeinga
a h
hallucinatedtokenintheprobabilitydistributionofthefinallayerandthreshold ∈ (0,1).Basedon
theexperimentalsetupdescribedabove,weconductedthefollowinginvestigation:
Whatsuppressestheexpressionofvisualfacts? Weanalyzetheoccurrenceofx ateachdecod-
a
inglayer,asshowninFigure3.Theresultsrevealthattheactivatedgroundtruthtokensareprimarily
presentbetweenlayers20and28,indicatingthatMLLMsaccuratelyrecognizetheimagecontentin
thelatterlayers.However,theactivatedgroundtruthtokensaresuppressedinthefinaloutputlayer.
Thissuppressionmaystemfromtheguidanceoftheinputimageortheinherentknowledgebiasof
theMLLM.Toinvestigatethis,wegeneratecandidatetokensV′ intheabsenceofaninput
candidate
image,representingtokensbasedontheMLLM’sinherentknowledge.Wecalculatethattheoverlap
rateofx existinginV′ reaches91.05%,suggestingthatevenwithoutexpressingimagein-
h candidate
formation,MLLMsstilltendtogeneratetheoriginalhallucinationtokens.Thisfindingrevealsthat
theinherentknowledgeinMLLMsmaydiminishtheprobabilityofthegroundtruthtokenin
thedeeperlayers.
3 PROPOSED APPROACH: DYNAMIC CORRECTION DECODING WITH
PRECEDING-LAYER KNOWLEDGE
After investigating the reasons why MLLMs generate non-existent objects, inspired by (Chuang
et al., 2024), we introduce Dynamic Correction Decoding with preCeding-Layer Knowledge
(DeCo), which can alleviate hallucinations during inference. The overall framework of Deco is
4
tnuoCPreprint.
Dynamic Preceding-Layer Selection
DDesesccrriibbee tthhee i mimagaeg.e.
Candidate
Tokens max_prob
car, tree ModulaD tiy on na Cm oic e fficient
park, bench
User
The image captures a young man Top-p
riding a skateboard, performing a Truncation
trick by jumping over a small white
pole. He appears to be skilled and
MLLMs e ox np te hr eie sn tc ree ed t, .showcasing his abilities Prob. Observation Anchor Layer
In the background, there is a ???
Preceding-layer Knowledge Guided Decoding
Greedy Search
... In the background, there is a car
parked, further emphasizing the urban
environment. A backpack can be seen
placed nearby, possibly belonging to the
skateboarder or someone else in the area.
Ours
Softmax add logits (last layer)
... In the background, there is a tree
visible near the edge of the scene, adding
to the urban atmosphere of the scene. × α × max_prob
Ours (wo/max_probs)
... In the background, there is a tree visible, calibrated logits
adding to the urban setting of the scene.
Additionally, there are two other people
present in the background, one near the
left side of the image and another person
further away towards the right side.. logits (anchor layer)
Figure4:FrameworkofDeCo.DeCofirstdynamicallyselectsanappropriateanchorlayerfromthe
precedinglayersandthencorrecttheknowledgeinthefinallayerwithdynamiccoefficient.
illustratedinFigure4,consistingofdynamicprecedinglayerselection(Section3.1)anddecoding
correctionwithpreceding-layerknowledge(Section3.2).
3.1 DYNAMICPRECEDING-LAYERSELECTION
Candidatetokenacquisition. Duetothevastvocabularyspace,wetrackonlythechangesinthe
top-ranked tokens as candidate tokens across different layers for computational convenience. This
isbasedonthehypothesisthatgroundtokensusuallyappearinthetoppositionoftheMLLM’slast
layeroutputlogits.Inspiredby(Lietal.,2023a),weuseatruncationstrategytoselectthecandidate
tokens,withthedefaulttruncationstrategybeingtop-ptruncation,formally:
 
 (cid:88) 
V (x |x )= x ∈V : P (x =v|x ,x ,...,x )≤p (3)
candidate T <T T τ T 0 1 T−1
 
v∈Vp
whereV isthewholevocabulary,andpreferstotheparameterusedintop-p.
Preceding-layer selection. Our findings in Section 2
Table 1: Hit Rate of layers across demonstrate that activated ground truth tokens typically ex-
differentintervals. hibithigherprobabilitiesinprecedinglayerscomparedtohal-
lucinated tokens. Based on this observation, we hypothesize
thatselectingthetokenx ,wherex ∈ V ,withthe
LayerRange 20-28 15-28 th th candidate
highestprobabilityfromtheintervallayerscorrespondstothe
HitRate(%) 61.69 71.14 ground truth token. We compute the accuracy of x as the
th
ground truth token and denote this metric as the hit rate, as
shown in Table 1. The results indicate that within a specific range of layers (e.g., 15-28), x in-
th
deed has a high universal probability of representing the ground truth token. Intuitively, we track
5
d
d
dPreprint.
candidate tokens and dynamically choose the layer in which the token with the highest probabil-
ity among the preceding layers resides to calibrate the final logit distribution of the MLLM. The
selectedprecedinglayerisreferredtoastheanchorlayer,formallydefinedas:
A=argmax (cid:8) x ∈V :softmax(ϕ(hi )) ,i∈[a,b](cid:9) , (4)
i T candidate T−1 xT
wherea≤b,a,b∈[1,N],and[a,b]representsthelayerintervalforMLLMs.Expandingtherange
of layers can improve the hit rate. To avoid increased search computation time, we assign default
valuesofa=20andb=28foroursubsequentexperiments.
3.2 DECODINGCORRECTIONWITHPRECEDING-LAYERKNOWLEDGE
Dynamicsoftmodulation. Weintroduceadynamicmodulationcoefficient,defaultingtothemax-
imumprobability.Formally,wehave:
max prob=max(softmax(ϕ(hA ))). (5)
T−1
Thiscoefficientcanhelppreventhardchangesinlogits,particularlywhentheprobabilitydifferences
betweencandidatetokensinprecedinglayersareinsignificant.FromtheexampleinFigure4,wecan
observe that the absence of the dynamic modulation coefficient may lead to semantic incoherence
orevenmoreseverehallucinations.
Preceding-layerknowledgeguideddecoding. Giventheselectedprecedinglayers,weintegrate
informationfromtheselayersintothefinallayertocorrectthelogitdistribution.Weutilzeahyperpa-
rameter,α,tocontroltheproportionofearly-layerinformationincorporated.Additionally,dynamic
softmodulationisemployedtopreservethegenerativestyleoftheoriginalmodel.Byutilizingthe
correction of preceding-layer representations, the probability of predicting the next token and the
logitsareupdatedasfollows:
(cid:0) (cid:1)
pˆ(x |x )=softmax logits , (6)
T <T xT
logits=ϕ(hN )+α×max prob×ϕ(hA ), (7)
T−1 T−1
whereN isthelastlayerofMLLMandAistheselectedprecedinglayer.
Comparisonofpreviousmethods. OurworksharesasimilarassumptionwithOPERA(Huang
et al., 2024) and VCD (Leng et al., 2023), positing that the knowledge priors inherent in MLLMs
maysuppressthemodel’sabilitytocomprehendvisualinformation.However,ourapproachiscom-
paratively simpler than that of OPERA (Huang et al., 2024) and VCD (Leng et al., 2023). Addi-
tionally,ourworkdiffersfromtheassumptioninunimodalLLMs,wherethesemanticinformation
present in the shallow layers interferes with factual recall in the final layer (Chuang et al., 2024;
Chen et al., 2024c). However, our method is actually parallel to previous approaches and can be
combinedtoachievebetterresults.
4 EXPERIMENT
4.1 SETUP
Baselines.WeintegrateDeCowithvariousdecodingmethods,includinggreedydecoding,nucleus
sampling,andbeamsearch,andcompareitagainstseveralbaselinesformitigatinghallucinations,
asoutlinedbelow:Dola(Chuangetal.,2024)isspecificallydesignedforalleviatinghallucinations
infactualtasksforLLMsbyreducingshallowsemanticinfluencestoimprovethefactualityofthe
finallayer’soutput.VCD(Lengetal.,2023)mitigatestheinfluenceoflanguagemodel’spriorsin
MLLMs by generating representations that enhance visual information through the subtraction of
interfering knowledge prior during each sampling step. OPERA (Huang et al., 2024) dynamically
penalizesoverconfidenttokensbasedontheemergenceofaggregationpatterns,whileproposinga
retrospectiveallocationstrategytoavoidcaseswherehallucinationshavealreadyoccurred.Forall
thebaselines,weusethedefaulthyperparametersfromthesourcecodeforafaircomparsion.
6Preprint.
Table 2: CHAIR hallucination evaluation results. Lower scores indicate fewer hallucinations.
OPERA utilizes beam search, VCD applies nucleus sampling, and DeCo is the proposed method
compatiblewithvariousdecodingapproaches.
InstructBLIP MiniGPT-4 LLaVA-1.5 Qwen-VL
Decoding Method
CHAIRS↓ CHAIRI↓ CHAIRS↓ CHAIRI↓ CHAIRS↓ CHAIRI↓ CHAIRS↓ CHAIRI↓
Vanilla 58.8 23.7 31.8 9.9 45.0 14.7 46.0 12.5
Greedy
DoLa 48.4 15.9 32.2 10.0 47.8 13.8 46.8 12.9
DeCo(Ours) 41.2↓17.6 14.4↓9.3 27.0↓4.8 8.8↓1.1 37.8↓7.2 11.1↓3.6 42.2↓3.8 10.7↓1.8
Vanilla 55.6 15.8 30.6 9.5 48.8 13.9 41.8 10.8
BeamSearch OPERA 46.4 14.2 26.2 9.5 44.6 12.8 34.6 9.5
DeCo(Ours) 43.8↓11.8 12.7↓3.1 24.8↓5.8 7.5↓2.0 33.0↓15.8 9.7↓4.2 32.0↓9.8 8.7↓2.1
Vanilla 54.6 24.8 32.6 10.7 48.8 14.2 49.2 13.1
Nucleus VCD 58.0 17.0 33.8 11.1 54.0 16.0 46.4 11.9
DeCo(Ours) 43.6↓11.0 12.9↓11.9 30.8↓1.8 9.5↓1.2 42.8↓6.0 13.2↓1.0 43.8↓5.4 11.8↓1.3
Table3:POPEhallucinationevaluationresults.Thebestresultsareinbold.
InstructBLIP MiniGPT-4 LLaVA-1.5 Qwen-VL
Decoding Method
F1↑ F1↑ F1↑ F1↑
Vanilla 80.0 58.5 82.2 85.2
Greedy DoLa 83.4 72.8 83.2 85.8
DeCo(Ours) 84.9↑4.9 77.4↑18.9 86.7↑4.5 86.3↑1.1
Vanilla 84.4 70.3 84.9 85.3
BeamSearch OPERA 84.8 73.3 85.4 86.1
DeCo(Ours) 84.9↑0.5 77.9↑7.6 86.7↑1.8 86.4↑1.1
Vanilla 79.8 52.8 83.1 84.5
Nucleus VCD 79.9 56.0 83.1 84.7
DeCo(Ours) 81.8↑2.0 63.8↑11.0 85.4↑2.3 85.2↑0.7
Model.WeselectfourofthemostrepresentativeMLLMmodelsforevaluation,includingInstruct-
BLIP(Daietal.,2023),MiniGPT-4(Zhuetal.,2024),LLaVA-1.5(Liuetal.,2023b)andQwen-VL
(Baietal.,2023).AlltheMLLMsweusedhavealanguagemodelsizeof7billionparameters(7B).
Implementation Details. To select the appropriate preceding layers for hallucination mitigation,
weconductablationexperiments,detailsofwhichcanbefoundintheSection4.4.Fora7B-sized,
32-layer decoder-only architecture language model, we choose layers 20-28 as candidates for the
preceding layers (according to the findings in Section 2.1). For the image captioning and VQA
tasks, α is set within the range of 0.1 to 0.6. In all experiments, we conduct inference on a single
A800GPU.Theinferenceof500image-captionpairstakeapproximately40minutes.
4.2 BENCHMARKANDMETRICS
CHAIR.CaptionHallucinationAssessmentwithImageRelevance(CHAIR)(Rohrbachetal.,2018)
metric,widelyusedinimagecaptioning,identifieshallucinatedobjectsbycomparingtheextracted
objects with ground truth labels and evaluates both at the instance level (CHAIR ) and sentence
I
level(CHAIR ),asshowninEq.8.Following(Huangetal.,2024),weconductexperimentsusing
S
thesamesettings,includingtheconsistent500imagesfromtheMSCOCO2014validationdataset
andtheidenticalprompt,“Please help me describe the image in detail.”.
|{hallucinatedobjects}| |{captionswithhallucinatedobjects}|
CHAIR = ,CHAIR = . (8)
I S
allmentionedobjects allcaptions
POPE. The Polling-based Object Probing Evaluation (POPE) (Li et al., 2023b) is a VQA-based
metricforassessingobjecthallucinationinMLLMs.Itevaluateshallucinationsbyaskingquestions
suchas“Is there a <object> in the image?”where<object>isderivedfromthree
types of splits: random (randomly selected objects), popular (frequently occurring objects), and
adversarial (objects closely related to those in the image). The evaluation includes 500 MSCOCO
images,withsixquestionsperimageforeachsplit.WeuseF1scoreforperformanceevaluation.
MME. The comprehensive MLLM Evaluation benchmark (MME) (Fu et al., 2023) assesses the
perceptualandcognitiveabilitiesofMLLMsacrossatotalof14subtasks,includingtaskssuchas
OCR,visualknowledge,attributerelationships,andobjectrecognition.
7Preprint.
GPT-4oassistedevaluation.Tofurtherassessthemodel’sperformanceinimagecaptioning,weex-
tendbeyondtheCHAIRmetric,whichtargetsobjecthallucination.Followingpriorstudies(Huang
et al., 2024; Leng et al., 2023), an open evaluation is conducted using GPT-4o on 100 randomly
sampledCOCOimages.GPT-4oassessestwoassistants’descriptionsintermsofAccuracy(C)and
Detailedness(D).WeintroducethepromptusedintheexperimentsinTable8.
4.3 EXPERIMENTALRESULTS
Results of hallucination in image captioning. Note that we use the baseline’s original decod-
ing settings for a fair comparison and run DeCo under the same settings. From Table 2, we no-
tice that DeCo consistently outperforms other approaches in mitigating hallucinations across four
MLLMs—InstructBLIP,MiniGPT-4,LLaVA-1.5,andQwen-VL—usingthreedecodingstrategies:
greedysearch,beamsearch,andnucleussampling.WefindthatDeCoslightlyoutperformsOPERA,
whileourmethoddemonstrateshigherefficiencyandsimplicityininference(seeSection4.4).Ad-
ditionally,VCDdoesnotperformaswell,likelyduetoproducinganincreasednumberofhalluci-
nateddescriptionsduringthegenerationprocess.Inconclusion,theproposedapproachDeCoeffec-
tivelyreduceshallucinationsinvisualdescriptiontaskssolelythroughdynamicdecodingcorrection,
achieving an average suppression rate of approximately 10.8% on image captioning datasets. Ad-
ditionally,wefurtherevaluatetheperformanceofDeCoontheAMBERimagecaptiondataset,as
detailedinTable7oftheAppendix.
ResultsofhallucinationinVQA.Incontrasttoimagecap-
tioning, POPE employs a simple polling approach to assess
1500 MME
hallucinationlevelsinMLLMswithrespecttoobjectrecog-
Nucleus
VCD nition.AsshowninTable3,DeCodemonstratessuperiorper-
Beam
1250 OPERA formance across all settings, further validating the effective-
Greedy
DoLa nessoftheproposedapproach.Additionally,Figure5reveals
DeCo
1000 thatDeCoalsoachievesbetterresultsonMME,whichevalu-
Nucleus VCD Beam OPERA Greedy DoLa DeCo
atesthemultifacetedVQAcapabilitiesofLLaVA-1.5.These
findings suggest that the underlying mechanism we identi-
Figure5:DeCogenerallyimproves
fied not only applies to object recognition but also extends
theMLLM’sperformance.
toattribute-relatedtasksandmorecomplexreasoningtasks.
ResultsofGPT-4o’sassistance.Following(Huangetal.,2024;Lengetal.,2023),wefurtheruse
GPT-4otoevaluateourmethodagainstgreedydecodingacrossfourdistinctmodels.FromTable4,
wenoticethatourapproachconsistentlyoutperformgreedydecodingintermsofaccuracy,demon-
stratingitsefficacyinhallucinationsuppression.Theimpactofdecodinginterventionisevidentin
thelevelofdetailproduced:forsomemodels,ourmethodyieldonlymarginallyhigheror,incertain
cases,slightlylowerlevelsofdetailcomparedtogreedydecoding.Nonetheless,ourmethodexhibit
aclearadvantageinmitigatinghallucinationsacrossallevaluatedmodels.
Table 4: GPT-4o assisted hallucination evaluation results on MSCOCO. Two aspects are verified,
correctness(C)anddetailedness(D).
InstructBLIP MiniGPT-4 LLaVA-1.5 Qwen-VL
Method
C D C D C D C D
GreedySearch 4.92 5.65 5.71 6.20 5.21 6.31 5.56 6.62
DeCo(Ours) 6.25 5.77 6.33 6.08 7.42 6.25 7.81 6.70
4.4 ANALYSIS Vanila Ours Baseline
Greedy
Latencyandthroughputanalysis. Toevaluatetheef-
Beam Search
ficiency of DeCo, we compare its latency and through-
Nucleus
put with several baselines, including DoLa, OPERA,
andVCDbasedonGreedy,BeamSearch,andNucleus 0 50 100 150 200 0 10 20 30 40 50
Latency (ms/token) Throughput (token/s)
Sampling,respectively.Figure6illustratestheresultsof
Figure 6: Comparison of latency and
this comparison. The findings indicate that DeCo oper-
throughputacrossdifferentbaselines.
ates within an acceptable efficiency cost, striking a bal-
8
erocSPreprint.
ancebetweeneffectivenessandcomputationaloverhead.Comparedtothebasicdecodingprocess,
the latency increase introduced by our method is approximately 1.2 times. In contrast, the latency
increasesforVCDandOPERAare1.8and5.1times,respectively.WhilebothVCDandOPERA
demonstratecomparableefficacyinmitigatinghallucinations,theircomputationaloverheadsremain
relatively high. This highlights the practical value of DeCo, as it can be integrated into real-world
applicationswithoutsignificantlycompromisingefficiency.
Perturbationintheselectedpreceding-layer. Toevaluatetheeffectivenessofthedynamiclayer
selectionmethod,weintroducearandomperturbationstrategy.Specifically,forthepredetermined
preceding layers, we add random values ranging from -5 to 5 to modify the selection of layers.
We randomly select 200 images from the MSCOCO dataset and prompt MLLMs to generate de-
scriptions. The results after incorporating the perturbations are presented in Table 5. Notably, the
perturbedresultsdemonstrateasignificantdegradationinperformance,furthervalidatingtheeffec-
tivenessofourproposedmethod.
Table5:ComparisonofresultsbetweenDeCoandperturbedDeCoinimagecaptioningtasks
InstructBLIP MiniGPT-4 LLaVA-1.5 Qwen-VL
Method
CHAIRS↓ CHAIRI↓ CHAIRS↓ CHAIRI↓ CHAIRS↓ CHAIRI↓ CHAIRS↓ CHAIRI↓
DeCo 39.3 12.6 32.4 9.6 38.8 11.1 44.5 11.1
DeCo+ϵ 45.6↑6.3 14.3↑1.7 33.3↑0.9 10.1↑0.5 42.2↑2.4 11.3↑0.2 47.0↑2.5 12.8↑1.7
Hyperparameter analysis. Our method incorporates two primary hyperparameters: α and the
selection of interval layers. In the experiments, we employ DeCo based on greedy decoding. On
theonehand,thehyperparameterαregulatestheintensityofearlyinformationenhancement.Fig-
ure7(a)illustratestheperformanceacrossvariousαvalues.Weobservethathallucinationsuppres-
sionismosteffectivewhenαapproximates0.6.Asαincreases,theefficacyofDeCoinmitigating
hallucinations improves. However, it is crucial to note that excessively high α values may lead to
thegenerationofatypicalimagedescriptions,characterizedbyrepetitivewordusage.Ontheother
hand, the layer interval hyperparameter [a,b] determines the candidate layers for inclusion in the
enhancementprocess.Weconductexperimentsusingintervalsoffourlayers,withresultspresented
inFigure7(b).OuranalysisrevealsthathallucinationsuppressionforMLLMisnegligibleinlayers
1-16,whilelayers20-28demonstratesubstantialmitigationofhallucinations.Notably,layers29-32
exhibitminimalhallucinationsuppression,aligningwithourfindingsdiscussedinSection2.2.
LLaLVLaAVA MinMiGinPiGTP-4T-4 QwQewn-eVnL-VL InsItnrsutcrutBcLtBIPLIP OurOsurs GreGerdeye Sdye aSrecahrch LLaLVLAaVA MinMiGinPiGT-P4T-4
5050 5050
4545 4545
4040 4040
3535 3535
3030 3030
2525 2525
0.10.10.20.20.30.30.40.40.50.50.60.60.70.70.80.80.90.91.01.0 1-41-4 5-85-8 9-192-12131-136-16171-270-20212-214-24252-258-28292-392-32
AlAphlpaha LaLyaeyrerInItnetrevravlal
(a) Ablationstudyofα. (b) Ablationstudyofintervallayers.
Figure7:Ablationexperimentresultsforhyperparameterαanddifferentintervallayers.
Mitigating snowballing hallucinations. Snowballing hallucinations are a prevalent issue in the
responsesgenerated byMLLMs.This phenomenonoccurswhen aninitialhallucinationtriggers a
sequenceofsubsequenterrors,leadingtoacompoundingeffectthatsignificantlydegradesthequal-
ityandcoherenceofthegeneratedtext.Figure8illustratesatypicalexampleofsnowballinghallu-
cinations,whereaninitialmisinterpretationofthevisualinputpropagatesthroughthedecodingpro-
cess,resultinginahighlyinconsistentanderroneousoutput.Ourapproachcanreducetheaccumu-
lationoferrorsandimprovestheoverallconsistencyandaccuracyofthegeneratedresponses.The
effectiveness of DeCo is further demonstrated through additional cases based on diverse MLLMs,
whichcanbefoundinFigures9,10,11,and12inAppendixC.
9
SRIAHC SRIAHC SRIAHC SRIAHCPreprint.
5 RELATED WORK
5.1 MLLMHALLUCINATIONMECHANISM
HallucinationinMLLMs,characterizedbycontradictionsbetweenimageinputandtextualoutput,
hasbeenaprevalentissue(Liuetal.,2024a;Chenetal.,2024f).Currentresearchonthemechanism
ofhallucinationinMLLMsfocusesontwokeyaspects:theinteractionbetweenimagesandtextat
differentlayers,andthepriorbiasoftheLLMduringdecoding.Severalstudieshaveinvestigatedthe
roleofimage-textinteractionatdifferentlayersinMLLMs.Grad-CAM(Zhangetal.,2024a)visu-
alizationsrevealthatimage-textinteractionexistsintheprecedinglayers(1-11)butnotinthedeep
layers.OPERA(Huangetal.,2024)furtherproposesthatthe“AggregationPattern”leadstohallu-
cination,wherevisualinformationfromprecedinglayersisgraduallyaggregatedtoanchortokens,
andfocusingsolelyonthesetokensduringpredictionwhileignoringvisualinformationleadstoa
high probability of hallucination in the generated sequence. However, other studies have revealed
thatMLLMsexhibitbiasestowardsLLMpriors,eveninthepresenceofnoisyorabsentvisualin-
formation.VCD(Lengetal.,2023)discoversthatMLLMsgeneratehigh-confidenceanswerseven
whentheimageisnoisyorabsent,indicatingabiastowardsLLMpriors.Similarly,PAI(Liuetal.,
2024b)describesthisphenomenonas“TextInertia”andpositsthatitstemsfromexistingparadigms
that map visual representations onto the text representations as tokens. This leads to an inference
processthatfailstoadequatelyaccountforimagetokens,resultinginhallucinations.
5.2 HALLUCINATIONMITIGATIONFORMLLMS
Onestraightforwardapproachtomitigatehallucinationistoreducetheknowledgegapsanddatabias
betweenvisionandlanguageduringmodeltraining.Finetuning-basedmethodshavebeenexplored,
focusingoncraftingspecificdatasets(Youetal.,2024;Gunjaletal.,2024;Chenetal.,2024e)and
alignmenttraining(Sunetal.,2023;Yuetal.,2023;Chenetal.,2023;Lietal.,2023c)toachieve
betterknowledgealignmentbetweenimagesandtext.Whilethesemethodshaveshownpromising
results,theyoftenrequireexpensiveannotatedpaireddataandsubstantialcomputationalresources.
Hallucinationcanalsobemitigatedbypost-processingmethods,whichusuallyinvolveusingaddi-
tionaltoolsorself-reflectionstrategiestorevisetheresponse.Forinstance,LURE(Zhouetal.,2024)
detects hallucinations using manually-crafted features and revises the generated text accordingly.
Woodpecker (Yin et al., 2023) combines MLLM outputs with an expert VQA model to post-edit
hallucinations.VOLCANO(Leeetal.,2023)trainsMLLMstoprovideself-feedbackandreflecton
theoriginalgeneratedtext.However,thesemethodsincuradditionalinferencecostsanddelays,and
requiretask-specificproceduresandpromptstobedesigned(Xuetal.,2024).Training-freedecod-
ing methods have been explored to mitigate hallucination. OPERA (Huang et al., 2024) identifies
an abnormal attention pattern that often accompanies hallucinated descriptions and proposes the
mitigationmethodbasedonthispattern.VCD(Lengetal.,2023)introducesthenotionthatvisual
uncertaintyincreaseshallucinationandproposesacontrastdecodingmethodtoalleviatetheissue.
VDD(Zhangetal.,2024b)proposesa“Post-Hocdebias”approachthatensuresuniformscoresfor
eachanswerintheabsenceofanimagetomitigatetheinfluenceofLLMpriors.
6 CONCLUSION AND LIMITATIONS
Inthispaper,wedemonstratethatMLLMsexhibitanawarenessofhallucinatedobjects,withearlier
layersshowinghigherconfidence,whiletokensshapedbypriorknowledgediminishthelikelihood
oftruetokensinthefinallayers.Basedonthisinsight,weintroduceDeCo,dynamiccorrectionde-
coding with preceding-layer knowledge to mitigate hallucinations. Extensive experiments demon-
stratetheefficacyofourapproach,whichalsoshowsadvantagesinlatencyandthroughput.
Limitations. (1) Lack of generalized research. Due to the GPU cost consideration, we conduct
experiments solely on limited MLLMs, without exploring additional MLLMs or those with larger
parametersizes.(2)Nofreelunch.TheresultsshowninTable4indicatethatourmethodhasalittle
negativeimpactonthelevelofdetailednessmetric.Infuturework,weaimtointegrateDeCowith
otherstrategiesandexploreapproachesthatcaneffectivelybalancetruthfulnessanddiversity.
10Preprint.
REPRODUCIBILITY STATEMENT
Wehavesubmittedtherelevantcodeinthesupplementarymaterials.Thenamesoftheexperimental
benchmarks,theprompttemplatesused,andthemodel’shyperparametersettingscanallbefound
inSection4.TheAppendixA.1andA.2providesadetaileddescriptionoftheexperimentalsetup
forthemechanismexperiments.
REFERENCES
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou,andJingrenZhou. Qwen-vl:Aversatilevision-languagemodelforunderstanding,local-
ization,textreading,andbeyond. arXivpreprintarXiv:2308.12966,2023.
Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng
Shou. Hallucination of multimodal large language models: A survey. CoRR, abs/2404.18930,
2024. doi: 10.48550/ARXIV.2404.18930. URL https://doi.org/10.48550/arXiv.
2404.18930.
Paul Brie, Nicolas Burny, Arthur Sluy¨ters, and Jean Vanderdonckt. Evaluating a large language
modelonsearchingforGUIlayouts. Proc.ACMHum.Comput.Interact.,7(EICS):1–37,2023.
doi:10.1145/3593230. URLhttps://doi.org/10.1145/3593230.
JunyingChen,RuyiOuyang,AnningzheGao,ShunianChen,GuimingHardyChen,XidongWang,
Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, and Benyou Wang. Huatuogpt-
vision, towards injecting medical visual knowledge into multimodal llms at scale. CoRR,
abs/2406.19280, 2024a. doi: 10.48550/ARXIV.2406.19280. URL https://doi.org/10.
48550/arXiv.2406.19280.
LiangChen,HaozheZhao,TianyuLiu,ShuaiBai,JunyangLin,ChangZhou,andBaobaoChang.
Animageisworth1/2tokensafterlayer2:Plug-and-playinferenceaccelerationforlargevision-
language models. CoRR, abs/2403.06764, 2024b. doi: 10.48550/ARXIV.2403.06764. URL
https://doi.org/10.48550/arXiv.2403.06764.
Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, and Junxian
He. In-contextsharpnessasalerts:Aninnerrepresentationperspectiveforhallucinationmitiga-
tion. InForty-firstInternationalConferenceonMachineLearning,ICML2024,Vienna,Austria,
July21-27,2024.OpenReview.net,2024c. URLhttps://openreview.net/forum?id=
s3e8poX3kb.
TianxiangChen, ZhentaoTan,TaoGong, YueWu,Qi Chu,BinLiu, JiepingYe,andNenghai Yu.
Llama slayer 8b: Shallow layers hold the key to knowledge injection, 2024d. URL https:
//arxiv.org/abs/2410.02330.
Xiang Chen, Duanzheng Song, Honghao Gui, Chenxi Wang, Ningyu Zhang, Yong Jiang, Fei
Huang, Chengfei Lyu, Dan Zhang, and Huajun Chen. Factchd: Benchmarking fact-conflicting
hallucination detection. In Kate Larson (ed.), Proceedings of the Thirty-Third International
Joint Conference on Artificial Intelligence, IJCAI-24, pp. 6216–6224. International Joint Con-
ferences on Artificial Intelligence Organization, 8 2024e. doi: 10.24963/ijcai.2024/687. URL
https://doi.org/10.24963/ijcai.2024/687. MainTrack.
Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Lei
Liang,JinjieGu,andHuajunChen.Unifiedhallucinationdetectionformultimodallargelanguage
models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),ACL
2024, Bangkok, Thailand, August 11-16, 2024, pp. 3235–3252. Association for Computational
Linguistics,2024f. doi:10.18653/V1/2024.ACL-LONG.178. URLhttps://doi.org/10.
18653/v1/2024.acl-long.178.
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong
Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl:
Scaling up vision foundation models and aligning for generic visual-linguistic tasks. CoRR,
11Preprint.
abs/2312.14238, 2023. doi: 10.48550/ARXIV.2312.14238. URL https://doi.org/10.
48550/arXiv.2312.14238.
Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. ANOLE: an open, autoregressive, native large
multimodalmodelsforinterleavedimage-textgeneration. CoRR,abs/2407.06135,2024. doi:10.
48550/ARXIV.2407.06135. URLhttps://doi.org/10.48550/arXiv.2407.06135.
Yung-SungChuang,YujiaXie,HongyinLuo,YoonKim,JamesR.Glass,andPengchengHe. Dola:
Decodingbycontrastinglayersimprovesfactualityinlargelanguagemodels.InTheTwelfthInter-
nationalConferenceonLearningRepresentations,ICLR2024,Vienna,Austria,May7-11,2024.
OpenReview.net,2024. URLhttps://openreview.net/forum?id=Th6NyL07na.
Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu
Lu,ZichongYang,Kuei-DaLiao,TianrenGao,ErlongLi,KunTang,ZhipengCao,TongZhou,
AoLiu,XinruiYan,ShuqiMei,JianguoCao,ZiranWang,andChaoZheng. Asurveyonmul-
timodal large language models for autonomous driving. In IEEE/CVF Winter Conference on
Applications of Computer Vision Workshops, WACVW 2024 - Workshops, Waikoloa, HI, USA,
January 1-6, 2024, pp. 958–979. IEEE, 2024. doi: 10.1109/WACVW60836.2024.00106. URL
https://doi.org/10.1109/WACVW60836.2024.00106.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning. In Alice Oh, Tristan Nau-
mann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances
in Neural Information Processing Systems 36: Annual Conference on Neural Information
Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
9a6a435e75419a836fe47ab6793623e6-Abstract-Conference.html.
Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April26-30,2020.OpenReview.net,2020. URLhttps://openreview.net/forum?id=
SJg7KhVKPH.
ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,Xiawu
Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal
largelanguagemodels. arXivpreprintarXiv:2306.13394,2023.
Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vi-
sion language models. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan
(eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Con-
ference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium
on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancou-
ver, Canada, pp. 18135–18143. AAAI Press, 2024. doi: 10.1609/AAAI.V38I16.29771. URL
https://doi.org/10.1609/aaai.v38i16.29771.
MingzheHu,ShaoyanPan,YuhengLi,andXiaofengYang. Advancingmedicalimagingwithlan-
guagemodels:Ajourneyfromn-gramstochatgpt. CoRR,abs/2304.04920,2023. doi:10.48550/
ARXIV.2304.04920. URLhttps://doi.org/10.48550/arXiv.2304.04920.
QidongHuang,XiaoyiDong,PanZhang,BinWang,ConghuiHe,JiaqiWang,DahuaLin,Weiming
Zhang,andNenghaiYu. OPERA:alleviatinghallucinationinmulti-modallargelanguagemodels
viaover-trustpenaltyandretrospection-allocation. CVPR,abs/2311.17911,2024.
Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa
Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language mod-
els. InForty-firstInternationalConferenceonMachineLearning,ICML2024,Vienna,Austria,
July21-27,2024.OpenReview.net,2024. URLhttps://openreview.net/forum?id=
6FXtu8clyp.
SeongyunLee,SueHyunPark,YongraeJo,andMinjoonSeo. Volcano:Mitigatingmultimodalhal-
lucinationthroughself-feedbackguidedrevision. CoRR,abs/2311.07362,2023. doi:10.48550/
ARXIV.2311.07362. URLhttps://doi.org/10.48550/arXiv.2311.07362.
12Preprint.
Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong
Bing. Mitigating object hallucinations in large vision-language models through visual con-
trastive decoding. CoRR, abs/2311.16922, 2023. doi: 10.48550/ARXIV.2311.16922. URL
https://doi.org/10.48550/arXiv.2311.16922.
Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto,
Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as op-
timization. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceed-
ings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 12286–12312. Associa-
tion for Computational Linguistics, 2023a. doi: 10.18653/V1/2023.ACL-LONG.687. URL
https://doi.org/10.18653/v1/2023.acl-long.687.
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evalu-
ating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino,
and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 292–305. Associa-
tion for Computational Linguistics, 2023b. doi: 10.18653/V1/2023.EMNLP-MAIN.20. URL
https://doi.org/10.18653/v1/2023.emnlp-main.20.
Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu,
and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-
modalmodels.CoRR,abs/2311.06607,2023c.doi:10.48550/ARXIV.2311.06607.URLhttps:
//doi.org/10.48550/arXiv.2311.06607.
FuxiaoLiu,KevinLin,LinjieLi,JianfengWang,YaserYacoob,andLijuanWang. Aligninglarge
multi-modalmodelwithrobustinstructiontuning. CoRR,abs/2306.14565,2023a. doi:10.48550/
ARXIV.2306.14565. URLhttps://doi.org/10.48550/arXiv.2306.14565.
Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou,
Rongjun Li, and Wei Peng. A survey on hallucination in large vision-language models. CoRR,
abs/2402.00253, 2024a. doi: 10.48550/ARXIV.2402.00253. URL https://doi.org/10.
48550/arXiv.2402.00253.
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.Improvedbaselineswithvisualinstruction
tuning. arXivpreprintarXiv:2310.03744,2023b.
ShiLiu,KechengZheng,andWeiChen.Payingmoreattentiontoimage:Atraining-freemethodfor
alleviatinghallucinationinlvlms,2024b. URLhttps://arxiv.org/abs/2407.21771.
HaoyuLu,WenLiu,BoZhang,BingxuanWang,KaiDong,BoLiu,JingxiangSun,TongzhengRen,
ZhuoshuLi,HaoYang,YaofengSun,ChengqiDeng,HanweiXu,ZhendaXie,andChongRuan.
Deepseek-vl:Towardsreal-worldvision-languageunderstanding. CoRR,abs/2403.05525,2024a.
doi: 10.48550/ARXIV.2403.05525. URL https://doi.org/10.48550/arXiv.2403.
05525.
Taiming Lu, Muhan Gao, Kuai Yu, Adam Byerly, and Daniel Khashabi. Insights into llm long-
contextfailures:Whentransformersknowbutdon’ttell,2024b. URLhttps://arxiv.org/
abs/2406.14673.
OpenAI. GPT-4technicalreport. CoRR,abs/2303.08774,2023. doi:10.48550/ARXIV.2303.08774.
URLhttps://doi.org/10.48550/arXiv.2303.08774.
Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, and
YonatanBelinkov. Llmsknowmorethantheyshow:Ontheintrinsicrepresentationofllmhallu-
cinations,2024. URLhttps://arxiv.org/abs/2410.02707.
Vipula Rawte, Amit P. Sheth, and Amitava Das. A survey of hallucination in large foundation
models. CoRR, abs/2309.05922, 2023. doi: 10.48550/ARXIV.2309.05922. URL https://
doi.org/10.48550/arXiv.2309.05922.
13Preprint.
Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object
hallucinationinimagecaptioning. InProceedingsofthe2018ConferenceonEmpiricalMethods
in Natural Language Processing, Jan 2018. doi: 10.18653/v1/d18-1437. URL http://dx.
doi.org/10.18653/v1/d18-1437.
Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay,
and Donald Metzler. Confident adaptive language modeling. In Sanmi Koyejo, S. Mo-
hamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neu-
ral Information Processing Systems 35: Annual Conference on Neural Information Process-
ing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,
2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
6fac9e316a4ae75ea244ddcef1982c71-Abstract-Conference.html.
Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan,
Liang-YanGui,Yu-XiongWang,YimingYang,KurtKeutzer,andTrevorDarrell. Aligninglarge
multimodal models with factually augmented RLHF. CoRR, abs/2309.14525, 2023. doi: 10.
48550/ARXIV.2309.14525. URLhttps://doi.org/10.48550/arXiv.2309.14525.
Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. CoRR,
abs/2405.09818, 2024. doi: 10.48550/ARXIV.2405.09818. URL https://doi.org/10.
48550/arXiv.2405.09818.
Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. Branchynet: Fast inference via early
exiting from deep neural networks. In 23rd International Conference on Pattern Recognition,
ICPR 2016, Cancu´n, Mexico, December 4-8, 2016, pp. 2464–2469. IEEE, 2016. doi: 10.1109/
ICPR.2016.7900006. URLhttps://doi.org/10.1109/ICPR.2016.7900006.
JunyangWang,YuhangWang,GuohaiXu,JingZhang,YukaiGu,HaitaoJia,MingYan,JiZhang,
and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation.
CoRR,abs/2311.07397,2023a.doi:10.48550/ARXIV.2311.07397.URLhttps://doi.org/
10.48550/arXiv.2311.07397.
PengWang,ShuaiBai,SinanTan,ShijieWang,ZhihaoFan,JinzeBai,KeqinChen,XuejingLiu,
JialinWang,WenbinGe,YangFan,KaiDang,MengfeiDu,XuanchengRen,RuiMen,Dayiheng
Liu,ChangZhou,JingrenZhou,andJunyangLin.Qwen2-vl:Enhancingvision-languagemodel’s
perception of the world at any resolution, 2024. URL https://arxiv.org/abs/2409.
12191.
Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive
computer-aideddiagnosisonmedicalimageusinglargelanguagemodels.CoRR,abs/2302.07257,
2023b. doi:10.48550/ARXIV.2302.07257. URLhttps://doi.org/10.48550/arXiv.
2302.07257.
WenhaiWang,JiangweiXie,ChuanyangHu,HaomingZou,JiananFan,WenwenTong,YangWen,
SileiWu,HanmingDeng,ZhiqiLi,HaoTian,LeweiLu,XizhouZhu,XiaogangWang,YuQiao,
and Jifeng Dai. Drivemlm: Aligning multi-modal large language models with behavioral plan-
ningstatesforautonomousdriving. CoRR,abs/2312.09245,2023c. doi:10.48550/ARXIV.2312.
09245. URLhttps://doi.org/10.48550/arXiv.2312.09245.
ZiweiXu,SanjayJain,andMohanS.Kankanhalli. Hallucinationisinevitable:Aninnatelimitation
of large language models. CoRR, abs/2401.11817, 2024. doi: 10.48550/ARXIV.2401.11817.
URLhttps://doi.org/10.48550/arXiv.2401.11817.
Yuan Yao, TianyuYu, Ao Zhang, ChongyiWang, Junbo Cui, Hongji Zhu,Tianchi Cai, Haoyu Li,
WeilinZhao,ZhihuiHe,etal. Minicpm-v:Agpt-4vlevelmllmonyourphone. arXivpreprint
arXiv:2408.01800,2024.
Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1,
grade-school math and the hidden reasoning process. CoRR, abs/2407.20311, 2024. doi: 10.
48550/ARXIV.2407.20311. URLhttps://doi.org/10.48550/arXiv.2407.20311.
14Preprint.
ShukangYin,ChaoyouFu,SiruiZhao,TongXu,HaoWang,DianboSui,YunhangShen,KeLi,Xing
Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language
models. CoRR, abs/2310.16045, 2023. doi: 10.48550/ARXIV.2310.16045. URL https://
doi.org/10.48550/arXiv.2310.16045.
HaoxuanYou,HaotianZhang,ZheGan,XianzhiDu,BowenZhang,ZiruiWang,LiangliangCao,
Shih-FuChang,andYinfeiYang. Ferret:Referandgroundanythinganywhereatanygranularity.
InTheTwelfthInternationalConferenceonLearningRepresentations,ICLR2024,Vienna,Aus-
tria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?
id=2msbbX3ydD.
Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu,
Hai-TaoZheng,MaosongSun,andTat-SengChua. RLHF-V:towardstrustworthymllmsviabe-
havioralignmentfromfine-grainedcorrectionalhumanfeedback. CoRR,abs/2312.00849,2023.
doi: 10.48550/ARXIV.2312.00849. URL https://doi.org/10.48550/arXiv.2312.
00849.
XiaofengZhang,ChenShen,XiaosongYuan,ShaotianYan,LiangXie,WenxiaoWang,Chaochen
Gu,HaoTang,andJiepingYe. Fromredundancytorelevance:Enhancingexplainabilityinmulti-
modallargelanguagemodels.CoRR,abs/2406.06579,2024a.doi:10.48550/ARXIV.2406.06579.
URLhttps://doi.org/10.48550/arXiv.2406.06579.
Yi-FanZhang,WeichenYu,QingsongWen,XueWang,ZhangZhang,LiangWang,RongJin,and
TieniuTan.Debiasingmultimodallargelanguagemodels.CoRR,abs/2403.05262,2024b.doi:10.
48550/ARXIV.2403.05262. URLhttps://doi.org/10.48550/arXiv.2403.05262.
Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit
Bansal,andHuaxiuYao. Analyzingandmitigatingobjecthallucinationinlargevision-language
models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vi-
enna,Austria,May7-11,2024.OpenReview.net,2024. URLhttps://openreview.net/
forum?id=oZDJKTlOUe.
DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4:Enhancing
vision-language understanding with advanced large language models. In The Twelfth Interna-
tional Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024.
OpenReview.net,2024. URLhttps://openreview.net/forum?id=1tZbq88f27.
15Preprint.
APPENDICES
A DETAILED EXPERIMENTAL SETUP
A.1 DETAILEDSETTINGSFORFINDINGS1
Intheprobingexperiment,weutilizethepipelineproposedinthePOPE(Lietal.,2023b)tocon-
struct1,200balancedpositiveandnegativesamplepairsfromtheMSCOCOdatasetastrainingdata
fortheprobeclassifier,whereeachsampleconsistsofanobjectaccompaniedbyalabelindicating
itsexistenceornon-existence.(Note:Thereisnooverlapbetweenthetrainingdataandtheevalu-
ation data for object hallucination proposed by the POPE). We select the AMBER dataset (Wang
et al., 2023a), which has a different distribution from the MSCOCO dataset, to test whether our
conclusions can generalize. The AMBER dataset contains 1,004 carefully annotated images, each
labeledwithexistentobjectsaswellasnon-existentobjects.Weusetheprompt“Describe the
image.” to generate raw responses from LLaVA-1.5 on the images and then extract all object
category tokens and label them with whether they exist. Given that the training set contains only
80 object categories, we denote the object tokens in test data belonging to these 80 categories as
in-distribution(in-dist),whiletheremainingtokensarecategorizedasout-of-distribution(OOD).
Previous work (Karamcheti et al., 2024) has demonstrated that increasing the resolution of the vi-
sionencoderenhancesthevisualcomprehensioncapabilitiesofMLLMs.Inourstudy,wecompare
LLaVA trained with a resolution of 224px against the original LLaVA with a resolution of 336px
inprobingexperiments.Notably,thelanguagemodel’sweightsdifferbetweenthetwoMLLMs,al-
thoughbothinitialmodelsarebasedonVicuna-1.5-7b.Ourresults,asillustratedintheFigure1(b),
furtheraffirmthescalinglawassociatedwithvisualresolution,whilealsoprovidingindirectvalida-
tionofthereliabilityoftheprobingexperiments.
A.2 DETAILEDSETTINGSFORFINDINGS2
In the early exit experiment, we randomly select 500 images from MSOCO and use random
prompts (shown in Table 6) to elicit raw responses from LLaVA-1.5-7b. We then extract all
non-existent objects along with their corresponding preceding text. Specifically, for the sentence
“Additionally, there is a car.”, we extract the hallucinated object token “car” and
the preceding text “Additionally, there is a”. We re-input the preceding text into the
MLLMandobservethechangesinitsinternalstatewhenpredictingthenexttoken.Wedenotethat
atotalofK precedingtextsareselected,withthej-thprecedingtextdenotedassj.
Table6:Randomlyprompts.
Prompts
Describetheimage.
Pleasedescribethisimageindetail.
Generateacaptionforthisimage.
16Preprint.
B EVALUATION RESULTS IN AMBER
The AMBER image caption dataset consists of 1,004 images, each accompanied by meticulously
annotated labels. These annotations include all objects present in the images, as well as some po-
tential hallucinated objects. AMBER employs four evaluation metrics: CHAIR (the proportion of
generatedhallucinatedobjectsamongallobjects),Cover(thecoverageofgeneratedobjectsagainst
all ground truth objects), Hal (the proportion of hallucinations among all generated captions), and
Cog(theoverlapratiowithpotentialhallucinatedobjects).LowervaluesofCHAIR,Hal,andCog
indicate higher truthfulness for the MLLMs, while a higher Cover value signifies better diversity.
WecompareDecowiththebaselinesontheLLaVA-1.5-7b.TheresultsareasshowninTable7.The
resultsrevealthatDecodemonstratesasignificantadvantageintruthfulness,althoughitsdiversity
issomewhatlacking,yetremainswithinanacceptablerange.
Table7:ResultsofusingDeCoontheAMBERimagecaptiondatasetwithLLaVA-1.5-7b.
LLaVA-1.5
Decoding Method
CHAIR↓ Cover↑ Hal↓ Cog↓
Vanilla 8.2 48.9 34.3 4.0
Greedy
DoLa 8.0 50.8 37.5 4.3
DeCo(Ours) 6.6↓1.6 47.5↓1.4 28.1↓6.2 2.8↓1.2
Vanilla 7.1 50.7 32.4 3.8
BeamSearch OPERA 6.4 49.0 27.5 2.9
DeCo(Ours) 6.3↓0.8 46.8↓3.9 25.1↓7.3 2.4↓1.4
Vanilla 10.2 50.2 43.3 4.5
Nucleus VCD 9.0 51.7 40.2 4.4
DeCo(Ours) 8.3↓1.9 48.0↓2.2 37.5↓5.8 3.4↓1.1
17Preprint.
C CASE ANALYSIS ACROSS DIVERSE MLLMS
DDeescsrcibrieb teh et himea igme.age. Greedy Search
there is a car parked on the street,
and a person can be seen standing
near the corner. The scene captures
the essence of a bustling urban
User environment with clear signage to
help people navigate their way
around the city.
The image showcases a street corner Ours
in a city, featuring street signs
there is a car parked near the corner,
mounted on a street light post. The
possibly parked at the parking meter
MLLMs signs provide directions to various
indicated by one of the street signs.
locations, including the local library
The scene captures the essence of
and a nearby market. The sky above
urban life and navigation, with clear
the street corner is blue, indicating a
directions for pedestrians and
clear day.
drivers alike.
In the background, there is a car ???
Figure8:ThecaseofmitigatingsnowballinghallucinationwithDeCo.
18Preprint.
Instruction: Please describe this image in detail.
InstructBlip (Greedy):
The image features a large red chair with the shape of an animal, specifically in this case
being represented by two horses. One horse is standing on top and another one appears to
be sitting atop it from behind its back legs towards front side viewing angle. There are
several people visible throughout different parts around or near these animals: three
individuals can been seen walking along either sides close together; there's also someone
further away who seems more distant but still within reachable distance for interaction if
needed. Additionally, some cars parked nearby add movement into scene as they come
closer while driving past each other during their daily activities outside.
InstructBlip (Ours):
The image depicts a large red chair sculpture with a horse statue placed on top of it. The
horse statue is positioned on top of the chair, which resembles a chair in itself. The horse
statue is placed on top of the chair, making it look like a realistic horse standing on a
chair. The chair itself is quite large, measuring around 10 feet tall. The horse statue is
positioned towards the center of the image, with its head facing towards the left side of
the image. The other side of the horse statue is positioned towards the right side of the
image. The chair and horse statue combination creates an interesting visual effect, making
Input Image
it appear like a realistic horse standing on a chair.
Instruction: Please describe this image in detail.
InstructBlip (Beam search):
The image depicts a group of three motorcyclists riding down a winding road
through a mountainous area. The road is surrounded by tall, rocky cliffs on
both sides, creating a dramatic and scenic setting. The motorcyclists are
wearing helmets and appear to be enjoying the ride, taking in the
breathtaking views around them. There are two motorcycles visible in the
scene, with one closer to the center of the image and the other slightly off
to the right side. In addition to the motorcycles, there are several cars
parked along the side of the road, providing additional transportation
options for those exploring the area.
InstructBlip (Ours):
The image depicts a group of motorcycles traveling down a roadway, passing
through a tunnel. The motorcycles are ridden by three people who are
enjoying the scenic route through the tunnel. The motorcycles are
positioned in various positions along the roadway, with one motorcycle being
closer to the center of the image, another motorcycle slightly behind it,
and another motorcycle further away from the center of the image. The
third motorcycle is positioned further away from the others, but still visible Input Image
in the background. The roadway itself is surrounded by trees and rocks,
adding to the scenic nature of the scene.
Instruction: Please describe this image in detail.
InstructBlip (Nucleus):
The image captures a skateboarder skillfully riding a rail in a blue skate park,
likely practicing his tricks. The skateboarder is wearing a helmet, providing safety
for the activity. There are multiple cars present in the background, including two
parked on either side of the skate park. Additionally, there are two benches
scattered around the scene, likely providing seating for the skateboarder and
other park visitors.
InstructBlip (Ours):
The image depicts a skateboarder skillfully skating on a rail near a blue railing at
a skateboarding park. The skateboarder is wearing a helmet and appears to be
engaging in a trick, performing a rail grind on the rail. The skateboarding park is
well-equipped with several skateboarding obstacles, including a skateboarding
ramp, a skateboarding rail, and a skateboarding ledge. These obstacles are
designed to challenge skateboarders and provide them with the opportunity to
Input Image practice their skills and showcase their skills in various ways.
Figure9:DeCo’sperformanceinreducinghallucinationsofInstructBlip-7Bonthreebasicdecoing
methods.
19Preprint.
Instruction: Please describe this image in detail.
LLaVA-1.5 (Greedy):
The image features a woman sitting on a motorcycle, which is parked in front of a white
building. She is wearing a leather jacket and appears to be posing for the camera. The
motorcycle is positioned in the foreground, with the woman sitting comfortably on the seat.
In the background, there is a car parked further away from the motorcycle. The scene
captures the woman's enthusiasm for her motorcycle and her outdoor adventures.
LLaVA-1.5 (Ours):
The image features a woman sitting on a motorcycle parked outside a building. She is
positioned in front of the motorcycle, occupying a majority of the frame. The motorcycle
itself occupies a large portion of the scene, extending from left to right.
The woman appears to be dressed in motorcycle gear, possibly preparing for a ride or
Input Image already having completed one. The motorcycle's positioning suggests that it might be
parked outside a garage or other similar location where motorcycle enthusiasts gather.
Instruction: Please describe this image in detail.
LLaVA-1.5 (Beam search):
The image features a delicious-looking pizza sitting in a box on a dining
table. The pizza is topped with tomatoes and cheese, making it an appetizing
sight. Next to the pizza, there is a bottle of beer, which complements the
meal perfectly.
In addition to the pizza and beer, there is a knife placed on the table,
likely for cutting and serving the pizza slices. The dining table occupies most
of the space in the image, emphasizing the focus on the pizza and beer.
LLaVA-1.5 (Ours):
The image features a delicious looking pizza sitting inside a box, ready to be
eaten. The pizza is topped with tomatoes and cheese, making it look
delicious and freshly prepared. Next to the pizza box, there is a bottle of
beer, adding to the overall enjoyment of the meal. The scene appears to be
ready for someone to enjoy a delicious combination of pizza and beer. Input Image
Instruction: Please describe this image in detail.
LLaVA-1.5 (Nucleus):
In the image, a snowboarder is performing a mid-air trick while jumping
over a snow-covered hill. The snowboarder is wearing a red jacket and is
captured in mid-flight. The jump appears to be part of a half-pipe course.
The scene also includes a couple of other people standing nearby, likely
watching the snowboarder's impressive stunt. There are two chairs placed
within the area, possibly for spectators to sit and enjoy the view. A
backpack is placed on the ground, close to one of the chairs, possibly
belonging to one of the people in the area.
LLaVA-1.5 (Ours):
The image depicts a snowboarder in mid-air performing an aerial trick over
a snowy hillside. The snowboarder is suspended above the snow-covered
ground, showcasing their skill and agility.
There are several other people visible in the background, likely spectators
Input Image or fellow snowboarders, scattered around the scene. Some are close to the
main snowboarder while others are further away, possibly admiring the
performance or waiting for their turn to snowboard.
Figure10:DeCo’sperformanceinreducinghallucinationsofLLaVA-1.5-7Bonthreebasicdecoing
methods.
20Preprint.
Instruction: Describe this image in detail.
Qwen-VL (Greedy):
The image features a young woman standing in a doorway of a bathroom. She is
holding a toothbrush in her hand and appears to be smiling as she poses for the
camera. The bathroom has a window, which allows natural light to enter the room.
In the background, there is a clock on the wall, located above a door handle. The
door handle is on the door leading into the bathroom. The woman is standing near
the door, creating a frame with her body that captures her and the doorway.
Qwen-VL (Ours):
The image features a young woman standing in a doorway of a bathroom. She is
holding a toothbrush in her hand and is smiling for the camera. The bathroom
appears to be an old-fashioned style, with a 1999 date stamp on the bottom right
corner of the image. The woman's hair is styled in a bun, and she is dressed in a
Input Image gray sweater and jeans.
Instruction: Describe this image in detail.
Qwen-VL (Beam search):
The image captures a man skiing down a snow-covered slope at high
speed. He is wearing ski gear and skiing equipment, including skis on his
feet. The man is fully immersed in the skiing experience, leaning
forward slightly as he navigates the snowy hill.
There are two ski poles visible in the scene, one on the left side and
the other on the right side of the man. Additionally, a backpack can be
spotted in the background, likely carrying the skier's belongings.
Qwen-VL (Ours):
The image features a man wearing a brown jacket and a hat skiing down
a snowy hill. He is equipped with two ski poles for balance and control
during his descent. The man is actively making his way through the
snow, enjoying the winter sport. The scene is set in a snow-covered
landscape, providing a fun and challenging environment for skiing. Input Image
Instruction: Describe this image in detail.
Qwen-VL (Nucleus):
The image features two zebras standing side by side in a dirt area, either in
a zoo or a wildlife park. They are both eating hay from a trough placed in
their enclosure. The zebras have distinctive black and white stripes that are
easily recognized.
In the background, there are several rocks scattered around the area, giving
the scene a natural feel. A person can be seen near the right side of the
image, possibly observing or maintaining the enclosure.
Qwen-VL (Ours):
The image features two zebras standing side by side in a dirt field, eating hay
from a trough. They both have a distinctive black and white striped pattern
on their coats. The first zebra is located more towards the left side of the
image, while the second zebra is slightly further to the right.
The field appears to be a part of a wildlife reserve or a zoo, as suggested by
Input Image the presence of a feeding station. The overall setting suggests a calm and
peaceful environment, allowing these two zebras to graze together in harmony.
Figure11:DeCo’sperformanceinreducinghallucinationsofQwen-VL-7Bonthreebasicdecoing
methods.
21Preprint.
Instruction: Please describe this image in detail.
Instruction: Please describe this image in detail.
MiniGPT4 (Greedy):
LLaVA-1.5(GreedTyh)i:s image shows two men standing in a living room. One man is holding a
The image featurwehsi tae woobmjeacnt siint thinisg roign hat mhaontdor, cwychliele, twhhei coht hise rp amrakne di si ns tfarnodnitn go fb eah winhdite
building. She is wheimar iwnigt ha hleisa tahremrs jcarcokseste da nidn afpropneta rosf thoi sb ec hpeossti.n gT hfoer rtohoem cias mdeercao.r aTtheed
motorcycle is poswitiitohn eyde llionw t hceu rftoarinesg raonudn da, bwliuteh ctohuec hw.o mThane sfiutrtninitgu rceo misf ocrlutattbelyr eodn wtihteh seat.
various objects, including a lamp, a vase, and a book. The walls are
In the backgrounadd,o trhneerde wiist ha ycealrlo pwa raknedd w fhuirtteh ewra lalpwaapye rf.r oTmh et hoev emraoltl oartcmycolsep.h Terhee osfc etnhee
captures the womimana'gse eins tchaussuiaasl ma nfdo rr ehlearx emdo.torcycle and her outdoor adventures.
LLaVA-1.5(DSREM):iniGPT4 (Ours):
The image featurTehsi sa imwoamgea nd espititcitnsg towno am meno tsotracnydcilneg pianr kae rdo oomut swiditeh ay ebluloilwd inwga.l lsS haen dis
positioned in fronctu rotfa itnhse. Omonteo mrcaync lies, hooclcduinpgy inag w ah imtea joobrjiteyc to fin thhies frrigahmte h. aTnhde w mhiolteo rtchyecle
itself occupies a olathrgeer pmoarnt ioisn sotfa ntdhien gs cbeenhein, de xhtimen dwiintgh fhriso ma rlmefst ctroo srsiegdht i.n front of him.
The furniture in the room includes a blue couch and a chair. The room
Input Image The woman appeaarpsp etao rbs et od rbees sweedl li nli tm wotitohr csyucnleli ggheta cro, mpionsgs itbhlyro purgehp athrien gw ifnodro wa .ride or
Input Image already having completed one. The motorcycle's positioning suggests that it might be
parked outside a garage or other similar location where motorcycle enthusiasts gather.
Instruction: Please describe this image in detail.
Instruction: Please describe this image in detail.
LLaVA-1.5(Beam search):
ThMei niimGaPgTe4 f(eBaetaumre sse aa rdcehl)i:cious-looking pizza sitting in a box on a dining
taTbhlee. imThaeg ep iszhzoaw sis at olaprpgeed cwloitchk ttoomwaetro eins tahned mchidedelsee ,o fm aa kciintgy istt raene at.p pTehtiez icnlgock
sigtohwt.e rN iesx mt atdoe t hoef mpieztzaal, atnhde rhea sis taw ob olatrtglee ocflo bcke efra,c wesh icohn icto.m Tphleem celnotcsk tthoewer is
surrounded by tall buildings on either side of it. There is a black car parked on
meal perfectly.
the sidewalk next to the clock tower. The car is facing the clock tower and has
In addition to the pizza and beer, there is a knife placed on the table,
its lights on. There are people walking on the sidewalk in front of the clock tower.
likSeolym ef oorf ctuhtetmin ga raen dl osoekrinvign gu pt haet ptihzez ac lsolcicke tso. wTehre w dhiinlein go tthaebrlse aorcec uwpaielksi nmgo pstast it.
ofT thhee b supiladcineg isn otnh ee itimhearg es,i deem opfh atshizei ncglo tchke t ofwoceurs hoanv et hlea rpgiez zwai nadnodw sb eaenrd. balconies.
The street is lined with trees and streetlights.
LLaVA-1.5(DSRE):
MiniGPT4 (Ours):
The image features a delicious looking pizza sitting inside a box, ready to be
This image depicts a large clock tower in the middle of a city street. The clock
eaten. The pizza is topped with tomatoes and cheese, making it look
tower is made of metal and has two clock faces on it. The clock tower is
delicious and freshly prepared. Next to the pizza box, there is a bottle of
surrounded by buildings and cars passing by on the street below it. There is a
bebelar,c ka dcadrin gp atrok etdh eo no vtehrea lsl iednejwoaylmk ennetx ot ft ot hteh em ecalol.c kT thoew secre.ne appears to be
ready for someone to enjoy a delicious combination of pizza and beer. Input Image
Instruction: Please describe this image in detail.
Input Image
LLaVA-1.5(Nucleus):
In the image, a snowboarder is performing a mid-air trick while jumping
over a snow-covered hill. The snowboarder is wearing a red jacket and is
Instrcuacpttiounr:e dP liena smei dd-efslicgrhibt.e Tthhies jiummapg ea pinp edaertsa itl.o be part of a half-pipe course.
MiniGTPhTe4 s(cNeuncel eaulsso): includes a couple of other people standing nearby, likely
The iwmaatgceh insgh otwhse as nwohwibteo aprldaetre' sw iitmhp rae pssieivcee sotfu nmte. aTt,h ear eb uanrceh towfo bcrhoacicrosl ip, laacnedd
somew citahsihne wthse. Tarheear,e piso sas ibklnyi ffeo ra nsdp efcotrakt oorns tthoe ssitid aen odf etnhjeo yp ltahtee ,v iaenwd. aA glass
of wabtaecrk poanc kth ies optlahceerd s iodne .t hTeh ger opulantde, icsl oosne at ob roonwen o tfa tbhleec lcohtahi rasn,d p othsseibly
backgbreolounngdi nigs ato b ornoew no fw athll.e people in the area.
LLaVA-1.5(DSRE):
MiniGTPhTe4 im(Oaugres )d:epicts a snowboarder in mid-air performing an aerial trick over
The iam sangoew syh ohwillss iad ep.l aTthee o sfn foowobdo acrodnesris tisin gsu osfp ebnrdoecdc oalbi,o vpeo ttahteo essn,o awn-dc omveeraetd.
The pglraotuen dis, wshhoitwec awsiitnhg at hbeliarc ksk rililm a nadn da gaiplipteya.rs to be on a table or countertop.
The food is arranged in rows with broccoli on one side of the plate and
potatToheesr oen a trhee s oetvheerar l soidthee. rA p epoiepclee voifs imbleea itn ist hien bthaeck cgernotuenrd ,o fli ktehlye spplaetcet.ators
InIpnuptu tI mImagaege
or fellow snowboarders, scattered around the scene. Some are close to the
main snowboarder while others are further away, possibly admiring the
performance or waiting for their turn to snowboard.
Figure12:DeCo’sperformanceinreducinghallucinationsofMiniGPT4-7Bonthreebasicdecoing
methods.
22Preprint.
Table 8: The prompt used for GPT-4o evaluation adopted from Leng et al. (2023); Huang et al.
(2024);Liuetal.(2024b)
GPT-4oPrompt
You are required to score the performance of two AI assistants in describing a given image. You
should pay extra attention to the hallucination, which refers to the part of descriptions that are in-
consistent with the image content, such as claiming the existence of something not present in the
imageordescribingincorrectlyintermsofthecounts,positions,orcolorsofobjectsintheimage.
Pleaseratetheresponsesoftheassistantsonascaleof1to10,whereahigherscoreindicatesbetter
performance,accordingtothefollowingcriteria:
1: Accuracy: whether the response is accurate with respect to the image content. Responses with
fewerhallucinationsshouldbegivenhigherscores.
2:Detailedness:whethertheresponseisrichinnecessarydetails.Notethathallucinateddescriptions
shouldnotcountasnecessarydetails.
Please output the scores for each criterion, containing only two values indicating the scores for
Assistant 1 and 2, respectively. The two scores are separated by a space. Following the scores,
pleaseprovideanexplanationofyourevaluation,avoidinganypotentialbiasandensuringthatthe
orderinwhichtheresponseswerepresenteddoesnotaffectyourjudgment.
[Assistant1]
{ResponseofAssistant1}
[EndofAssistant1]
[Assistant2]
{ResponseofAssistant2}
[EndofAssistant2]
Outputformat:
Accuracy:<Scoresofthetwoanswers>
Reason:
Detailedness:<Scoresofthetwoanswers>
Reason:
23