COTRACKER3: SIMPLER AND BETTER POINT
TRACKING BY PSEUDO-LABELLING REAL VIDEOS
NikitaKaraev1,2 IuriiMakarov1 JianyuanWang1,2 NataliaNeverova1
AndreaVedaldi1 ChristianRupprecht2
1MetaAI 2VisualGeometryGroup,UniversityofOxford
https://cotracker3.github.io
nikita@robots.ox.ac.uk
Figure1: Scalingpointtrackersusingunsupervisedvideos. Left: WecompareourCoTracker3,
LocoTrack,CoTracker,BootsTAPIRandTAPIR. Eachmodelispre-trainedonsyntheticdata(from
Kubric)andthenfine-tunedonrealvideosusingournew,simpleprotocolforunsupervisedtraining.
Our new model and training protocol outperform SoTA by a large margin using only 0.1% of the
trainingdata. Right: Thenewmodelisparticularlyrobusttoocclusions.
ABSTRACT
Most state-of-the-art point trackers are trained on synthetic data due to the diffi-
cultyofannotatingrealvideosforthistask.However,thiscanresultinsuboptimal
performance due to the statistical gap between synthetic and real videos. In or-
dertounderstandtheseissuesbetter,weintroduceCoTracker3,comprisinganew
trackingmodelandanewsemi-supervisedtrainingrecipe.Thisallowsrealvideos
withoutannotationstobeusedduringtrainingbygeneratingpseudo-labelsusing
off-the-shelfteachers. Thenewmodeleliminatesorsimplifiescomponentsfrom
previoustrackers,resultinginasimplerandoftensmallerarchitecture. Thistrain-
ingschemeismuchsimplerthanpriorworkandachievesbetterresultsusing1,000
timeslessdata. Wefurtherstudythescalingbehaviourtounderstandtheimpact
ofusingmorerealunsuperviseddatainpointtracking. Themodelisavailablein
onlineandofflinevariantsandreliablytracksvisibleandoccludedpoints.
1 INTRODUCTION
Tracking points is a key step in the analysis of videos, particularly for tasks like 3D recon-
struction and video editing that require precise recovery of correspondences. Point trackers have
evolvedsignificantlyinrecentyears,withdesignsbasedontransformerneuralnetworksinspiredby
PIPs(Harleyetal.,2022). NotableexamplesincludeTAP-Vid(Doerschetal.,2022),whichintro-
ducedanewbenchmarkforpointtracking,andTAPIR(Doerschetal.,2023),whichintroducedan
1
4202
tcO
51
]VC.sc[
1v13811.0142:viXraimprovedtrackerthatextendsPIPs’designwithaglobalmatchingstage. CoTracker(Karaevetal.,
2024) proposed a transformer architecture that tracks multiple points jointly, with further gains in
trackingquality,particularlyforpointspartiallyoccludedinthevideo.
In this paper, we introduce a new point tracking model, CoTracker3, that builds on the ideas of
recenttrackersbutissignificantlysimpler,moredataefficient,andmoreflexible. Ourarchitecture,
inparticular,removessomecomponentsthatrecenttrackersproposedasnecessaryforgoodperfor-
mance while still improving on the state-of-the-art. For the first time, we also investigate the data
scaling behaviour of a point tracker and show the advantages of different model architectures and
trainingprotocolsintermsoffinaltrackingqualityanddataefficiency.
Theexcellentperformanceofrecenttrackersisduetotheabilityofhigh-capacityneuralnetworks
tolearnarobustpriorfromalargenumberoftrainingvideosandusethispriorfortacklingcomplex
and ambiguous tracking cases, such as occlusions and fast motion. Therefore, the availability of
high-qualitytrainingdataisofcrucialimportanceinobtainingsolidtrackingresults.
While, in principle, there is no shortage of videos that could be used to train point trackers, it is
difficulttomanuallyannotatethemwithpointtracks(Doerschetal.,2022). Fortunately,synthetic
videos (Greff et al., 2022), which can be annotated automatically, have been found to be a good
substitute for real data for low-level tasks like point tracking (Harley et al., 2022). Still, a diverse
collectionofsyntheticvideosisexpensiveatscale,andthesim-to-realgapisnotentirelynegligible.
Hence,usingrealvideostotrainpointtrackersremainsanattractiveoption.
Recent works have thus explored utilizing large collections of real but unlabelled videos to train
pointtrackers. BootsTAPIR(Doerschetal.,2024),inparticular,hasrecentlyachievedstate-of-the-
artaccuracyontheTAP-Vidbenchmarkbytrainingamodelon15millionunlabelledvideos. While
thebenefitsofusingmoretrainingdatahavethusbeendemonstrated,thedatascalingbehaviourof
pointtrackersisnotwellunderstood. Inparticular,itisunclearifthemillionsofrealtrainingvideos
usedinBootsTAPIRarenecessarytotrainagoodtracker. Thesamecanbesaidaboutthebenefits
oftheirrelativelycomplexsemi-supervisedtrainingrecipe.
Another largely unexplored aspect is the competing designs of different trackers. Transformer ar-
chitectures like PIPs (Harley et al., 2022), TAPIR (Doersch et al., 2023), and CoTracker (Karaev
etal.,2024), aswellasmorerecentcontributionslikeLocoTrack(Choetal.,2024), proposeeach
significant changes, extensions, new components, and different design decisions. While these are
showntohelpintherespectivepapers,itislesscleariftheyareallessentialorwhetherthesedesigns
canbesimplifiedandmademoreefficient.
CoTracker3 contributes to answering these questions. Our model is based on a simpler architec-
tureandtrainingprotocolsthanrecenttrackerssuchasBootsTAPIRandLocoTrack. Itoutperforms
BootsTAPIR by a significant margin on the TAP-Vid and Dynamic Replica (Karaev et al., 2023)
benchmarks while using three orders of magnitude fewer unlabelled videos and a simpler training
protocolthanBootsTAPIR. Wealsostudythedatascalingbehaviourofthismodelunderincreas-
inglymorerealtrainingvideos. LocoTrackbenefitsinasimilarmannertoCoTracker3fromscaling
databutcannottrackoccludedpointswell.
CoTracker3borrowselementsfrompriormodels,includingiterativeupdatesandconvolutionalfea-
tures from PIPs, cross-track attention for joint tracking, virtual tracks for efficiency, and unrolled
trainingforwindowedoperationfromCoTracker,aswellasthe4DcorrelationfromLocoTrack. At
thesametime,itsignificantlysimplifiessomeofthesecomponentsandremovesothers,suchasthe
globalmatchingstageofBootsTAPIRandLocoTrack. Thishelpstoidentifywhichcomponentsare
reallyimportantforagoodtracker. CoTracker3’sarchitectureisalsoflexibleasitcanoperateboth
offline(i.e.,singlewindow)andonline(i.e.,slidingwindow)iftrainedinthesameway.
2 RELATED WORK
Tracking-Any-Point. ThetaskoftrackinganypointwasintroducedbyPIPs(Harleyetal.,2022),
who revisited the classic Particle Video (Sand & Teller, 2008) method and proposed to use deep
learning for point tracking. Inspired by RAFT (Teed & Deng, 2020), an optical flow algorithm,
PIPs extracts correlation maps between frames and feeds them into a network to refine the track
estimates. TAP-Vid(Doerschetal.,2022)improvedproblemframing,proposedthreebenchmarks,
2and TAP-Net, a model for point tracking. TAPIR (Doersch et al., 2023) combined TAP-Net-like
globalmatchingwithPIPs,resultinginamuch-improvedperformance. (Zhengetal.,2023)intro-
duced another synthetic benchmark, PointOdyssey, and PIPs++, an improved version of PIPs that
cantrackpointsoverextendeddurations. CoTracker(Karaevetal.,2024)notedastrongcorrelation
betweendifferenttracks,whichcanbeexploitedtoimprovetracking,particularlybehindocclusions
and out-of-frame. (Le Moing et al., 2024) further improved CoTracker by densifying its output.
VGGSfM (Wang et al., 2024) proposed a coarse-to-fine tracker design where tracks are validated
through3Dreconstruction,butitonlytargetsstaticscenes. InspiredbyDETR(Carionetal.,2020),
(Lietal.,2024)introducedTAPTR,anend-to-endtransformerarchitectureforpointtracking,rep-
resenting points as queries in the transformer decoder. LocoTrack (Cho et al., 2024) extended 2D
correlationfeaturesto4Dcorrelationvolumeswhilesimplifyingthepoint-trackingpipelineanden-
hancing efficiency. Our work proposes a further simplified framework that runs 27% faster than
LocoTrack,maintainingtheabilitytotrackoccludedpointsviajointtracking,likeCoTracker.
Annotating data for point tracking is particularly challenging due to the required precision: the
annotation should have (at least) pixel-level accuracy. The prevailing paradigm in point tracking
isthustotrainmodelsusingsyntheticdata, wheresuchannotationscanbeobtainedautomatically
and without errors, and show that the resulting models generalize to real data. All the methods
mentionedabovefollowthisparadigm,andmostaretrainedsolelyonKubric(Greffetal.,2022).
Semi-supervisedcorrespondence. Analternativetosyntheticdataisunlabelledrealdataincom-
bination with unsupervised or semi-supervised learning. For example, one can use photometric
consistencyasaproxyforcorrespondences. Suchtrainingiswellsuitedforopticalflowanddense
tracking but often leads to false matches due to occlusions, repeated textures, or lighting changes.
Therefore it usually requires multi-frame estimates (Janai et al., 2018), explicit reasoning about
occlusions (Wang et al., 2018), hand-crafted loss terms (Liu et al., 2019b; Meister et al., 2018),
or various data augmentation strategies (Liu et al., 2020). Alternatively, one can use an existing
tracker to train another in a process akin to distillation (Liu et al., 2019a). More robust unsuper-
visedlearningsignalsforlong-rangetrackingcanbeobtainedviasimplecolorizationofgray-scale
videosbycopyingcolorsfromthereferenceframe(Vondricketal.,2018)orutilizingrichervisual
patterns(Laietal.,2020).
Accounting for cycle consistency (Wang et al., 2019; Jabri et al., 2020) or temporal continu-
ity (Fo¨ldia´k, 1991; Wiskott & Sejnowski, 2002) in videos is another way to obtain a reliable
proxy signal to learn correspondences without full supervision, or even learn generic visual fea-
tures(Goroshinetal.,2015;Wang&Gupta,2015). Recently,(Sunetal.,2024)proposedrefining
PIPsandRAFTonapre-generateddatasetwithpseudo-labelsusingcolorconstancyandcyclecon-
sistencysignals. Thispipelineimprovestracking,butperformancequicklysaturates.
Mostrelevanttoourwork,BootsTAPIRDoerschetal.(2024)improvedTAPIRtrainedonKubricby
fine-tuningiton15millionrealvideosusingself-trainingwhileretainingasmallsyntheticdataset
with ground-truth supervision to avoid catastrophic forgetting. They proposed applying augmen-
tations to student predictions and trained the model with an exponential moving average (EMA)
whilecomputingthreedifferentlossmasksforrobustness. Incontrast,ourapproachusesasimpler
design which does not require augmentations, masks, or EMA for training. We also do not need
ground-truthsuperviseddataduringself-supervisedfinetuning. Instead,ourideaistotrainastudent
model by utilizing existing trackers with complementary qualities as teachers. We also show that
thisprotocolonlyrequiresasmallfractionoftherealvideosutilizedinBootsTAPIR.
3 METHOD
Inthissection, weformallyintroducethetaskofpointtrackingandthenoutlinetheproposedCo-
Tracker3architectureandthesemi-supervisedtrainingpipelineweusetotrainit.
Given a video (I )T , which is a sequence of T frames I ∈ R3×H×W, and a query point
t t=1 t
Q=(tq,xq,yq)∈R3 wheretq indicatesthequeryframeindexand(xq,yq)representstheinitial
locationofthequerypoint,ourgoalistopredictthecorrespondingpointtrackP =(x ,y )∈R2,
t t t
t=1,...,T,with(x ,y )=(xq,yq). Asiscommoninmodernpointtrackingmodels(Doersch
tq tq
et al., 2023; Karaev et al., 2024), CoTracker3 also estimates visibility V ∈ [0,1] and confidence
t
C ∈ [0,1]. Visibility shows whether the tracked point is visible (V = 1) or occluded (V = 0)
t t t
3in the current frame, while confidence measures whether the network is confident that the tracked
point iswithin a certaindistance fromthe ground truthin the currentframe (C = 1). The model
t
initializesalltrackswithquerycoordinatesP :=(x ,y ),t=1,...,T,confidenceandvisibility
t tq tq
withzerosC :=0,V :=0,thenupdatesallofthemiteratively.
t t
3.1 TRAININGUSINGUNLABELLEDVIDEOS
Recent trackers are trained primarily on synthetic data (Greff et al., 2022) due to the challenge of
annotating real data for this problem at scale. However, BootsTAPIR (Doersch et al., 2024) has
shownthatitispossibletotrainbettertrackersbyaddingtothemixunlabelledrealvideos. Inorder
todoso,theyproposeasophisticatedself-trainingprotocolthatusesalargenumberofunlabelled
videos(15M),self-training,dataaugmentations,andtransformationequivariance.
Here, we propose a much simpler protocol that allows us to surpass the performance of (Doersch
etal.,2024)with1,000×lessdata: weuseavarietyofexistingtrackerstolabelacollectionofreal
videos,usingthemasteachers,andthenusethepseudo-labelstotrainanewstudentmodel,which
wepre-trainutilizingsyntheticdata.
Importantly,theteachermodelsarealsotrainedusingthesamesyntheticdataonly. Onemaythus
wonderwhythisprotocolshouldresultinastudentbeingbetterthananyoftheteachers. Thereare
severalreasonsforthis:(1)thestudentbenefitsfromlearningfromamuchlarger(noisy)datasetthan
thesyntheticdataalone;(2)learningfromrealvideosmitigatesthedistributionshiftsbetweensyn-
theticandrealdata;(3)thereisanensembling/votingeffectwhichreducesthepseudo-annotations
noise;(4)thestudentmodelmayinheritthestrengthsofthedifferentteachers,whichmayexcelin
different aspects of the task (e.g., offline trackers track occluded points better and online trackers
tendtosticktothequerypointsmorecloselynearthetrack’sorigin).
Dataset. Inordertoenablesuchtraining,wecollectedalarge-scaledatasetofInternet-likevideos
(around100,000videosof30secondseach)featuringdiversescenesanddynamicobjects,primarily
humans and animals. We demonstrate that performance improves when training on increasingly
largersubsetsofthisdata,startingfromasfewas100videos(seeFigure1).
Teachermodels. Tocreateadiversesetofsupervisorysignals,weemploymultipleteachermod-
els trained only on synthetic data from Kubric (Greff et al., 2022). Our set of teachers consists of
our proposed models CoTracker3 online and CoTracker3 offline, CoTracker (Karaev et al., 2024),
and TAPIR (Doersch et al., 2023). During training, we randomly and uniformly sample a frozen
teacher model for every batch (meaning that it is likely that, over several epochs, the same video
willreceivepseudo-labelsfromdifferentteachers),whichhelpstopreventover-fittingandpromotes
generalization. Theteachermodelsarenotupdatedduringtraining.
Querypointsampling. Trackersrequireaquerypointtotrackinadditiontoavideo. Afterran-
domlychoosingateacherforthecurrentbatch,wesampleasetofquerypointsforeachvideo. To
selectsuchqueries,weusetheSIFTdetector(Lowe,1999)sampling,biasingtheselectionofpoints
tothosewhichare“goodtotrack”(Shi&Tomasi,1994). Specifically,werandomlyselectTˆframes
acrossavideoandapplySIFTtogeneratepointstostarttracksonthesekeyframes. Ourintuition
behind using a feature extractor is guided by its ability to detect descriptive image features when-
everpossiblewhilefailingtodosowhenmeetingambiguouscases. Wehypothesisethatthiswill
serve as a filter for hard-to-track points and will thus improve the stability of training. Following
thisintuition,ifSIFTfailstoproduceasufficientnumberofpointsforanyframe,weskipthevideo
completelyduringtrainingtomaintainthequalityofourtrainingdata.
Supervision. Wesupervisetrackspredictedbythestudentmodelwiththesamelossusedtopre-
trainthemodelonsyntheticdata,withonlyminormodificationsforhandlingocclusionandtracking
confidence. ThesedetailsaregivenlaterinSection3.3.
3.2 COTRACKER3MODEL
We provide two model versions of CoTracker3: offline and online. The online version operates
in a sliding window manner, processing the input video sequentially and tracking points forward-
only. Incontrast,theofflineversionprocessestheentirevideoasasingleslidingwindow,enabling
4Figure2: Architecture. Wecomputeconvolutionalfeaturesforeveryframeofthegivenvideo,and
then the correlations between the feature sampled around the query frame for the query point and
alltheotherframes. WetheniterativelyupdatetracksP(m) =P(m)+∆P(m+1),confidenceC(m),
andvisibilityV(m)withatransformerthattakesthepreviousestimatesP(m),C(m),V(m)asinput.
pointtrackinginbothforwardandbackwarddirections. Theofflineversiontracksoccludedpoints
betterandalsoimprovesthelong-termtrackingofvisiblepoints. However, themaximumnumber
oftrackedframesismemory-bound,whiletheonlineversioncantrackinreal-timeindefinitely.
Feature maps. We start by computing dense d-dimensional feature maps with a convolutional
neural network for each video frame, i.e., Φ = Φ(I ),t = 1,...,T. We downsample the input
t t
videobyafactorofk = 4forefficiencysothatΦ t ∈ Rd×H k×W k ,andcomputethefeaturemapsat
S =4differentscales,i.e.,Φs
t
∈Rd× k2H s−1× k2W s−1,s=1,...,S.
4Dcorrelationfeatures. InordertoallowthenetworktolocatethequerypointQ=(tq,xq,yq)
inframest = 1,...,T,wecomputethecorrelationbetweenthefeaturevectorsextractedfromthe
mapΦ atthequeryframetq aroundthequerycoordinates(xq,yq)andfeaturevectorsextracted
tq
frommapsΦ ,t=1,...,T aroundcurrenttrackestimatesP =(x ,y )attheotherframes.
t t t t
Morespecifically,everypointP isdescribedbyextractingasquareneighbourhoodoffeaturevec-
t
torsatdifferentscales. Wedenotethiscollectionoffeaturevectorsas:
(cid:104) (cid:16) x y (cid:17) (cid:105)
ϕs = Φs +δ, +δ : δ ∈Z, ∥δ∥ ≤∆ ∈Rd×(2∆+1)2 , s=1,...,S, (1)
t t ks ks ∞
wherethefeaturemapΦs issampledusingbilinearinterpolationaroundthepoint(x ,y ). There-
t t t
fore,foreachscales,ϕscontainsagridof(2∆+1)2pointwised-dimensionalfeatures.
t
Next,wedefinethe4Dcorrelation(Choetal.,2024)⟨ϕs ,ϕs⟩=stack((ϕs )⊤ϕs)∈R(2∆+1)4 for
tq t tq t
everyscales = 1,...,S. Intuitively,thisoperationcompareseachfeaturevectoraroundthequery
point (xq,yq) to each feature vector around the track point (x ,y ), which the network uses to
t t
predictthetrackupdate. Beforepassingthemtothetransformer,weprojectthesecorrelationswith
amulti-layerperceptron(MLP)toreducetheirdimensionality, definingthecorrelationfeaturesto
be:Corr =(cid:0) MLP(⟨ϕ1 ,ϕ1⟩),...,MLP(⟨ϕS ,ϕS⟩)(cid:1) ∈RpS,wherepistheprojectiondimension.
t tq t tq t
ThisMLParchitectureismuchsimplerthanthead-hocmoduleusedbyLocoTrack(Choetal.,2024)
forcomputingtheircorrelationfeatures.
Iterativeupdates. WeinitializetheconfidenceC andvisibilityV withzeros,andthetracksP
t t t
forallthetimest=1,...,T withtheinitialcoordinatesfromthequerypointQ.Wetheniteratively
updateallthesequantitieswithatransformer.
Ateveryiteration,weembedthetracksusingtheFourierEncodingoftheper-framedisplacements,
i.e., η = η(P − P )., Then, we concatenate the track embeddings (in both directions
t→t+1 t+1 t
η and η ), confidence C , visibility V , and the 4D correlations Corr for every query
t→t+1 t−1→t t t t
(cid:16) (cid:17)
point i = 1,...,N: Gi = ηi ,ηi ,Ci,Vi,Corri . Gi forms a grid of input tokens for
t t−1→t t→t+1 t t t t
thetransformerthatspantimeT andthenumberofquerypointsN. ThetransformerΨtakesthis
gridasinput,addsstandardFouriertimeembeddings,andappliesfactorizedtimeattentionwitht=
51,...,T andgroupattentionwithi = 1,...,N. Italsousesproxytokens(Karaevetal.,2024)for
efficiency. Thistransformerestimatestheupdatestotracks,confidence,andvisibilityincrementally
as (∆P,∆C,∆V) = Ψ(G). We update tracks P, confidence C and visibility V M times, where:
P(m+1) =P(m)+∆P(m+1);C(m+1) =C(m)+∆C(m+1);V(m+1) =V(m)+∆V(m+1).Notethat
weresamplethepointwisefeaturesϕaroundupdatedtracksP(m+1)andrecomputethecorrelations
Corraftereveryupdate.
3.3 MODELTRAINING
WesupervisebothvisibleandoccludedtracksusingtheHuberlosswithathresholdof6andexpo-
nentiallyincreasingweights. Weassignasmallerweighttothelosstermforoccludedpoints:
M
(cid:88)
L (P,P⋆)= γM−m(1 /5+1 )Huber(P(m),P⋆), (2)
track occ vis
m=1
whereγ =0.8isadiscountfactor. Thisprioritisestrackingwellthevisiblepoints.
ConfidenceandvisibilityaresupervisedwithaBinaryCrossEntropy(BCE)lossateveryiterative
update. Thegroundtruthforconfidenceisdefinedbyanindicatorfunctionthatcheckswhetherthe
predicted track is within 12 pixels of the ground truth track for the current update. We apply the
sigmoidfunctiontothepredictedconfidenceandvisibilitybeforecomputingtheloss:
M
L (C,P,P⋆)= (cid:88) γM−mCE(cid:0) σ(C(m)),1(cid:2) ∥P(m)−P⋆∥ <12(cid:3)(cid:1) , (3)
conf 2
m=1
M
(cid:88)
L (V,V⋆)= γM−mCE(σ(V(m)),V⋆). (4)
occl
m=1
Trainingusingpseudo-labels. Whenusingpseudo-labelledvideos,wesuperviseCoTracker3us-
ingthesameloss(2)usedforthesyntheticdata,butfounditmorestablenottosuperviseconfidence
and visibility. To avoid forgetting the latter predictions, we use a separate linear layer to estimate
confidenceandvisibilityandsimplyfreezeitatthistrainingstage.
Online model. Both online and offline versions of CoTracker3 have the same architecture. The
main difference between them is the way of training. The online version processes videos in a
windowed manner: it takes T′ frames as input, predicts tracks for them, then moves forward by
T′/2frames,andrepeatsthisprocess. Itusestheoverlappedpredictionsforthetracks,confidence,
andvisibilityfromthepreviousslidingwindowasinitializationforthecurrentwindow.
During training, we compute the same losses (2) to (4) for the online version separately for each
sliding window. Then, we take the mean across all the sliding windows. Since the online version
can track points only forward in time, we compute the losses only starting from the first window
withthequeryframetq onwards. Fortheofflineversion,however,wecomputethelossesforevery
framebecauseittrackspointsinbothdirections. Wetraintheonlineversiononvideosofthesame
length, while the offline version needs to see videos of different lengths during training to avoid
overfittingtoaspecificlength. Withthisintuitioninmind,fortheofflineversion,werandomlytrim
avideobetweenT/2andT framesandlinearlyinterpolatetimeembeddingsduringtraining.
3.4 DISCUSSION
Our model includes several simplifications and improvements compared to previous architectures
likePIPs,TAPIRandCoTracker. Inparticular: (1)Themodelusestheideaof4Dcorrelationfrom
LocoTrack but is further simplified by utilizing a simple MLP to process the correlation features
instead of their ad-hoc architecture; (2) It estimates confidence for every tracked point; (3) Com-
pared to CoTracker, the grid of tokens G is simplified, using only correlation features and Fourier
embeddingsofdisplacements;(4)Thevisibilityflagsareupdatedateachiterationalongwithother
quantities instead of using a separate network. (5) Compared to TAPIR, BootsTAPIR and Loco-
Track,CoTracker3doesnotuseaglobalmatchingmoduleaswefounditredundant.
AbenefitofthesesimplificationsisthatCoTracker3isconsiderablyleanerandfasterthanothersim-
ilartrackers. Specifically,CoTracker3has2×fewerparametersthanCoTracker,whiletheabsence
6Kinetics RGB-S DAVIS Mean
Method Train
AJ↑ δvis ↑ OA↑ AJ↑ δvis ↑ OA↑ AJ↑ δvis ↑ OA↑ δvis ↑
avg avg avg avg
PIPs++(Zhengetal.,2023) PO — 63.5 — — 58.5 — — 73.7 — 65.2
TAPIR(Doerschetal.,2023) Kub 49.6 64.2 85.0 55.5 69.7 88.0 56.2 70.0 86.5 68.0
CoTracker(Karaevetal.,2024) Kub 49.6 64.3 83.3 67.4 78.9 85.2 61.8 76.1 88.3 73.1
TAPTR(Lietal.,2024) Kub 49.0 64.4 85.2 60.8 76.2 87.0 63.0 76.1 91.1 72.2
LocoTrack(Choetal.,2024) Kub 52.9 66.8 85.3 69.7 83.2 89.5 62.9 75.3 87.2 75.1
CoTracker3(Ours,online) Kub 54.1 66.6 87.1 71.1 81.9 90.3 64.5 76.7 89.7 75.1
CoTracker3(Ours,offline) Kub 53.5 66.5 86.4 74.0 84.9 90.5 63.3 76.2 88.0 75.9
BootsTAPIR(Doerschetal.,2024) Kub+15M 54.6 68.4 86.5 70.8 83.0 89.9 61.4 73.6 88.7 75.0
CoTracker3(Ours,online) Kub+15k 55.8 68.5 88.3 71.7 83.6 91.1 63.8 76.3 90.2 76.1
CoTracker3(Ours,offline) Kub+15k 54.7 67.8 87.4 74.3 85.2 92.4 64.4 76.9 91.2 76.6
Table 1: TAP-Vid benchmarks CoTracker3 trained on synthetic Kubric shows strong perfor-
mancecomparedtoothermodels,whiletheonlineversionfine-tunedon15kadditionalrealvideos
(Kub+15k) outperforms all the other methods, even BootsTAPIR trained on 1,000× more real
videos. Trainingdata: (Kub)Kubric(Greffetal.,2022),(PO)PointOdyssey(Zhengetal.,2023).
of global matching and the use of an MLP to process correlations makes CoTracker3 27% faster
thanthefastesttracker(LocoTrack)despitecross-trackattention.
4 EXPERIMENTS
Inthissection,wedescribeourevaluationprotocol.Then,wecompareouronlineandofflinemodels
tostate-of-the-arttrackers(Section4.1),analysetheirperformanceforoccludedpoints(Section4.1),
show how different models scale with the proposed pseudo-labeling pipeline (Section 4.2), and
ablatethedesignchoicesofthearchitectureandthescalingpipeline(Section4.3).
Evaluation protocol. We conduct our evaluation on TAP-Vid (Doersch et al., 2022) compris-
ing TAP-Vid-Kinetics, TAP-Vid-DAVIS and RGB-Stacking. TAP-Vid-Kinetics consists of 1,144
YouTubevideosfromtheKinetics-700–2020validationset(Carreira&Zisserman,2017),featuring
complex cameramotion and cluttered backgrounds, with an average of26 tracks pervideo. TAP-
Vid-DAVIS comprises 30 real-world videos from the DAVIS 2017 validation set (Perazzi et al.,
2016),withanaverageof22trackspervideo. RGB-Stackingisasyntheticallygenerateddatasetof
roboticvideoswithmanytexture-lessregionsthataredifficulttotrack.
WeusethestandardTAP-Vidmetrics:OcclusionAccuracy(OA;accuracyofocclusionpredictionas
binaryclassification),δvis (fractionofvisiblepointstrackedwithin1,2,4,8and16pixels,averaged
avg
over thresholds) and Average Jaccard (AJ, measuring tracking and occlusion prediction accuracy
together). Allvideosareresizedto256×256pixelsbeforebeingprocessedbythemodel.
Similarly, we evaluate CoTracker3 on RoboTAP (Vecerik et al., 2023), which contains 265 real-
worldvideosofroboticmanipulationtasks,withanaveragedurationof272frames. Following(Do-
erschetal.,2022), weevaluateTAP-VidandRoboTAPinthe“firstquery”mode: samplingquery
pointsfromthefirstframewheretheybecomevisible. Additionally,wealsoevaluateonDynami-
cReplica(Karaevetal.,2023)following(Karaevetal.,2024). Becausethisdatasetissynthetic,the
trackercanbeevaluatedonoccludedpoints. TheevaluationsubsetofDynamicReplicaconsistsof
20 long (300 frames) sequences of articulated 3D models. We evaluate these benchmarks at their
nativeresolutionbutresizethepredictionstoaresolutionof256×256pixelsandreporttheaccuracy
ofvisible(δvis)andoccludedpoints(δocc)usingthesamethresholdsasinTAP-Vid.
avg avg
4.1 COMPARISONTOTHESTATE-OF-THE-ART
Forfairnesswithtrackersblindtothecorrelationbetweendifferenttracks,weevaluateCoTracker3
on TAP-Vid on one query point at a time and sample additional support points to leverage joint
tracking (Karaev et al., 2024). This ensures that no information about objects in the videos leaks
tothetrackerthroughtheselectionofbenchmarkpoints(whichgenerallycorrelatewithobjectsin
7DynamicReplica RoboTAP Mean
Method Train Size↓ Time↓
δvis ↑ δocc↑ AJ↑ δvis ↑ OA↑ δvis ↑
avg avg avg avg
PIPs++(Zhengetal.,2023) PO 25M - 64.0 28.5 — 63.0 — 63.5
TAPIR(Doerschetal.,2023) Kub 31M 293 66.1 27.2 59.6 73.4 87.0 69.8
CoTracker(Karaevetal.,2024) Kub 45M 472 68.9 37.6 58.6 70.6 87.0 69.8
TAPTR(Lietal.,2024) Kub - - 69.5 34.1 60.1 75.3 86.9 72.4
LocoTrack(Choetal.,2024) Kub 12M 290 71.4 29.8 62.3 76.2 87.1 73.8
CoTracker3(Ours,online) Kub 25M 405 72.9 41.0 60.8 73.7 87.1 73.3
CoTracker3(Ours,offline) Kub 25M 209 69.8 41.8 59.9 73.4 87.1 71.6
BootsTAPIR(Doerschetal.,2024) Kub+15M 78M 303 69.0 28.0 64.9 80.1 86.3 74.6
CoTracker3(Ours,online) Kub+15k 25M 405 73.3 40.1 66.4 78.8 90.8 76.1
CoTracker3(Ours,offline) Kub+15k 25M 209 72.2 42.3 64.7 78.0 89.4 75.1
Table 2: Results on Dynamic Replica and RoboTAP. Our approach consistently shows better
results. Only δvis on RoboTAP is better for BootsTAPIR, trained on 1,000× more data. Size in
avg
numberofparams;speedexpressedasµsperframeandpertrackedpoint. SeeFigure3forqualita-
tiveresultsonRoboTAP.
Figure3: Ours. Predictionsofonline(firstthreecolumns)andoffline(lastthreecolumns)models
on RoboTAP before (first row) and after (second row) scaling. We visualize the distance between
groundtruth(crosses)andmodelpredictions(points). Scalinghelpstoimprovepredictionsofboth
onlineandofflinemodels.
benchmarks). Wemultiplypredictedvisibilitybypredictedconfidenceandapplyathresholdtothe
resultingquantityasin(Doerschetal.,2023),improvingtheAJandOAmetrics.
As shown in Table 1, CoTracker3 is highly competitive with other trackers across various bench-
marks even when only trained using synthetic data (Kub). Adding unlabelled videos utilizing the
approachofSection4.2(+15k)booststheresultswellabovethestate-of-the-artforallmetricsfor
DAVIS, RGB-S, and Kinetics, and for two out of three metrics (AJ and OA) on RoboTAP (Ta-
ble2). The+15kofflineversionisevenbetterthantheonlineoneonDAVISandRGB-S,butworse
onKineticsandRoboTAP. Asfordataefficiency, despitebeingtrainedonjust15kadditionalreal
videos,ourmodelsoutperformBootsTAPIR,whichwastrainedusing15Mvideos(i.e.,1,000more).
Slightlybetterperformancecanbeobtainedbyincreasingthedatafurther(Section4.2). LocoTrack
alsobenefitssimilarlyfromourtrainingschemebutstrugglesduringocclusions,asshownnext.
Trackingoccludedpoints WecompareCoTracker3withothermethodsonDynamicReplicain
Table2(δocc andOAcolumns). Onthisbenchmark, CoTracker3onlineisbetterthanalltheother
avg
methodsevenwhentrainedsolelyonKubric;inparticular,itismuchbetterthanLocoTrack,which
justifiestheadditionalparametersinthecross-trackattentionmodules. Addingthe15krealvideos
improvesthetrackingofvisiblepointsfortheonlineandofflineversions,butonlytheofflinemodel
showsimprovementintrackingoccludedpoints. Inadditiontoimprovingmore,CoTracker3offline
tracksoccludedpointsbetterthantheonlineversion. Thisisbecauseaccessingallvideoframesat
oncehelpstointerpolatetrajectoriesbehindocclusions. SeeFigure4forqualitativeresults.
8DynamicReplica MeanonTAP-Vid
Cross-track
Self-training
attention δvis ↑ δocc↑ AJ↑ δ ↑ OA↑
avg avg avg
✗ 71.3 35.9 ✗ 62.2 74.5 88.2
✓ 72.9 41.0 ✓ 63.5 75.7 89.5
Table 3: Impact of cross-track attention on oc- Table 4: Self-training. Training Co-
cluded tracking. Cross-track attention improves the Tracker3 online on its own predictions im-
tracking of occluded points substantially. It also im- proves the model. We use 10k real videos
provesvisiblepoints,buttheeffectissmaller. andtraintoconvergence.
Table 5: Models used as teachers.
MeanonTAP-Vid
RTonl. RToffl. TAPIR CoTr. We use CoTracker3 online as a student
AJ↑ δ avg↑ OA↑ modelandablatedifferentcombinations
✗ ✗ ✗ ✗ 62.2 74.5 88.2 of teacher models. The first row corre-
✓ ✗ ✗ ✗ 63.5 75.7 89.5 spondstothemodeltrainedonlyonsyn-
✓ ✓ ✗ ✗ 64.5 76.4 89.9 theticdata.Thesecondrowcorresponds
✓ ✗ ✓ ✗ 63.6 76.2 89.7 toself-training. Generally,themoredi-
✓ ✗ ✗ ✓ 64.2 76.5 90.1 verseteacherswehave,thebetteristhe
✓ ✓ ✓ ✗ 64.0 76.6 89.9
trackingaccuracy(δ ).
✓ ✗ ✓ ✓ 64.2 76.6 90.1 avg
✓ ✓ ✗ ✓ 64.0 76.6 90.0
✓ ✓ ✓ ✓ 64.0 76.8 90.2
4.2 SCALINGEXPERIMENTS
In Figure 1, we show how CoTracker3, LocoTrack, and CoTracker (Karaev et al., 2024) improve
withourpseudo-labelingpipelineasthetrainingsetsizeincreases. Startingwithmodelspre-trained
on a synthetic dataset (Greff et al., 2022) (0 at x-axis), we train them on progressively larger real
data sets: 0.1k, 1k, 5k, 15k, 30k, and 100k videos. Models are trained to convergence on their
respective subsets. All models improve with just 0.1k real-world videos and continue improving
withmore. ImprovementsforCoTracker3online,offline,andLocoTracktendtoplateauafter30k
videos, likely because the student surpasses the teachers. This may also explain why CoTracker,
initiallymuchweakerthantwoofitsteachers(CoTracker3onlineandoffline),keepsimprovingup
toandpossiblybeyond100kvideos,whichisthemaximumwecanaffordtoexplore. Ourtraining
strategy is effective for all these models. We analyse the effect of using a scaled CoTracker3 as a
newteacherinthesupplement. Forcomparison,BootsTAPIR(Doerschetal.,2024)uses15million
realvideosandacomplexprotocolinvolvingaugmentations,lossmasks,andmore.
Interestingly, we found that training CoTracker3 with its own predictions as annotations without
other teachers (i.e., self-training) further improves the results on all the TAP-Vid benchmarks by
+1.2 points on average (see Table 4). Presumably, fine-tuning on real data, even with its own
annotations,helpsthemodelreducethedomaingapbetweenrealandsyntheticdata.
4.3 ABLATIONS
Cross-trackattention. Table3showsthatcross-trackattentionimprovesresults,particularlyfor
occluded points (+5.1 occluded vs. +1.6 visible on Dynamic Replica). This is because by using
cross-trackattention,themodelcanguessthepositionsoftheoccludedpointsbasedonthepositions
ofthevisibleones. Thiscannotbedoneifthepointsaretrackedindependently.
Teachermodels. Weassesstheimpactofusingmultipleteachersforgeneratingpseudo-labelsin
Table5. Westartbyremovingweakermodelsandalwayskeepthestudentmodelitselfasateacher.
We demonstrate that removing a teacher always leads to worse results compared to the last row,
wherewetrainwithallfourteachermodels. Thisshowsthateveryteacherisimportantandthatthe
studentmodelcanalwaysextractcomplementaryknowledge,evenfromweakerteachers.
Pointsampling. InTable6wehaveexploredalternativepointsamplingmethods,includingLight-
Glue(Lindenbergeretal.,2023), SuperPoint(DeToneetal.,2018), andDISK(Tyszkiewiczetal.,
9Figure4: Qualitativecomparison. Trackingagridof100×100pointsfromthefirstframeshould
maintain grid patterns in future frames when the motion is simple. LocoTrack and CoTracker3
are more consistent than BootsTAPIR, but neither LocoTrack nor BootsTAPIR can track through
occlusionsandalsolosemorebackground(1stcolumn)andobjectpoints(3rdand4thcolumns).
Sampling Kinetics DAVIS RoboTAP RGB-S AverageonTAP-Vid
Frozenhead
Uniform 67.9 76.9 78.4 84.0 AJ↑ δ ↑ OA↑
avg
SuperPoint 68.1 76.7 78.9 81.9
✗ 63.2 76.6 86.3
DISK 68.0 76.7 78.6 82.7
✓ 64.0 76.8 90.2
SIFT 68.2 77.0 78.8 83.3
Table 7: Average AJ, δ and OA on TAP-Vid,
Table 6: Point sampling strategies on δ on avg
avg
wherefreezingtheconfidenceandvisibilityheads
TAP-Vid. SIFT is overall best, but the method
improvesperformance,avoidingforgetting.
isrobustw.r.t. thischoice.
2020). Thechoiceofthesamplingmethoddoesnotsignificantlyaffecttheperformance. However,
SIFTsamplingresultsareconsistentlyhighacrossalltheTAP-Viddatasets.
Freezingtheconfidence andvisibilityhead. InTable7, we showthatsplittingthe transformer
headintoaseparateheadfortracksandaheadforconfidenceandvisibilityhelpstoavoidforgetting
when supervising only tracks while training on real data. We freeze the head for confidence and
visibilityatthisstage. ThisimprovesAJby+0.8andOAby+3.9onTAP-Vidonaverage.
5 CONCLUSION
We introduced CoTracker3, a new point tracker that outperforms the state-of-the-art on TAP-Vid
andotherbenchmarks. CoTracker3’sarchitecturecombinesseveralgoodideasfromrecenttrackers
buteliminatesunnecessarycomponentsandsignificantlysimplifiesothers. CoTracker3alsoshows
thepowerofasimplesemi-supervisedtrainingprotocol, whererealvideosareannotatedutilizing
severaloff-the-shelftrackersandthenusedtofine-tuneamodelthatoutperformsallteachers. With
thisprotocol,CoTracker3cansurpasstrackerstrainedon×1,000morevideos. Bytrackingpoints
jointly, CoTracker3 handles occlusions better than any other model, particularly when operated in
offlinemode. Ourmodelcanbeusedasabuildingblockfortasksrequiringmotionestimation,such
as3Dtracking,controlledvideogeneration,ordynamic3Dreconstruction.
10REFERENCES
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In Proc. ECCV. Springer,
2020.
Joa˜oCarreiraandAndrewZisserman. Quovadis,actionrecognition? anewmodelandthekinetics
dataset. In2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR),2017.
doi: 10.1109/CVPR.2017.502.
SeokjuCho,JiahuiHuang,JisuNam,HonggyuAn,SeungryongKim,andJoon-YoungLee. Local
all-paircorrespondenceforpointtracking. Proc.ECCV,2024.
DanielDeTone,TomaszMalisiewicz,andAndrewRabinovich. Superpoint:Self-supervisedinterest
pointdetectionanddescription,2018.
CarlDoersch, AnkushGupta, LarisaMarkeeva, Adria` Recasens, LucasSmaira, YusufAytar, Joa˜o
Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: A benchmark for tracking any point in a
video. arXiv,2022.
Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira,
and Andrew Zisserman. TAPIR: Tracking any point with per-frame initialization and temporal
refinement. arXiv,2306.08637,2023.
CarlDoersch, YiYang, DilaraGokay, PaulineLuc, SkandaKoppula, AnkushGupta, JosephHey-
ward,RossGoroshin,Joa˜oCarreira,andAndrewZisserman. Bootstap:Bootstrappedtrainingfor
tracking-any-point. arXivpreprintarXiv:2402.00847,2024.
William Falcon and The PyTorch Lightning team. PyTorch Lightning, 2019. URL https://
github.com/Lightning-AI/lightning.
PeterFo¨ldia´k.Learninginvariancefromtransformationsequences.Neuralcomputation,3(2),1991.
Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, and Yann LeCun. Unsupervised
learningofspatiotemporallycoherentmetrics. InProc.ICCV,2015.
Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J
Fleet,DanGnanapragasam,FlorianGolemo,CharlesHerrmann,etal. Kubric: Ascalabledataset
generator. InProc.CVPR,2022.
Adam W Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking
throughocclusionsusingpointtrajectories. InProc.ECCV,2022.
AllanJabri,AndrewOwens,andAlexeiEfros. Space-timecorrespondenceasacontrastiverandom
walk. Proc.NeurIPS,33,2020.
JoelJanai,FatmaGuney,AnuragRanjan,MichaelBlack,andAndreasGeiger. Unsupervisedlearn-
ingofmulti-frameopticalflowwithocclusions. InProc.ECCV,2018.
NikitaKaraev,IgnacioRocco,BenjaminGraham,NataliaNeverova,AndreaVedaldi,andChristian
Rupprecht. Dynamicstereo: Consistentdynamicdepthfromstereovideos. InProc.CVPR,2023.
NikitaKaraev,IgnacioRocco,BenjaminGraham,NataliaNeverova,AndreaVedaldi,andChristian
Rupprecht. Cotracker: Itisbettertotracktogether. Proc.ECCV,2024.
ZihangLai,ErikaLu,andWeidiXie. Mast:Amemory-augmentedself-supervisedtracker. InProc.
CVPR,2020.
Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. Dense optical tracking: Connecting the
dots. InCVPR,2024.
HongyangLi,HaoZhang,ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,andLeiZhang.Taptr:
Trackinganypointwithtransformersasdetection. arXivpreprintarXiv:2403.13042,2024.
11ShenLi,YanliZhao,RohanVarma,OmkarSalpekar,PieterNoordhuis,TengLi,AdamPaszke,Jeff
Smith,BrianVaughan,PritamDamania,andSoumithChintala. Pytorchdistributed: Experiences
onacceleratingdataparalleltraining,2020.
PhilippLindenberger,Paul-EdouardSarlin,andMarcPollefeys. Lightglue: Localfeaturematching
atlightspeed,2023.
LiangLiu,JiangningZhang,RuifeiHe,YongLiu,YabiaoWang,YingTai,DonghaoLuo,Chengjie
Wang,JilinLi,andFeiyueHuang. Learningbyanalogy: Reliablesupervisionfromtransforma-
tionsforunsupervisedopticalflowestimation. InProc.CVPR,2020.
Pengpeng Liu, Irwin King, Michael R Lyu, and Jia Xu. Ddflow: Learning optical flow with un-
labeled data distillation. In Proceedings of the AAAI Conference on Artificial Intelligence, vol-
ume33,2019a.
PengpengLiu, MichaelLyu, IrwinKing, andJiaXu. Selflow: Self-supervisedlearningofoptical
flow. InProc.CVPR,2019b.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
DavidGLowe. Objectrecognitionfromlocalscale-invariantfeatures. InProc.ICCV,1999.
SimonMeister,JunhwaHur,andStefanRoth. Unflow: Unsupervisedlearningofopticalflowwith
a bidirectional census loss. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume32,2018.
F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A
benchmark dataset and evaluation methodology for video object segmentation. In Proc. CVPR,
2016.
PeterSandandSethTeller. Particlevideo: Long-rangemotionestimationusingpointtrajectories.
IJCV,80,2008.
JianboShiandCarloTomasi. Goodfeaturestotrack. InProc.CVPR,1994.
XinglongSun,AdamWHarley,andLeonidasJGuibas. Refiningpre-trainedmotionmodels. arXiv
preprintarXiv:2401.00850,2024.
Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Proc.
ECCV,2020.
Michał J. Tyszkiewicz, Pascal Fua, and Eduard Trulls. Disk: Learning local features with policy
gradient,2020.
MelVecerik, CarlDoersch, YiYang, TodorDavchev, YusufAytar, GuangyaoZhou, RaiaHadsell,
LourdesAgapito,andJonScholz. Robotap: Trackingarbitrarypointsforfew-shotvisualimita-
tion,2023.
CarlVondrick,AbhinavShrivastava,AlirezaFathi,SergioGuadarrama,andKevinMurphy. Track-
ingemergesbycolorizingvideos. InProc.ECCV,2018.
JianyuanWang,NikitaKaraev,ChristianRupprecht,andDavidNovotny. Vggsfm:Visualgeometry
groundeddeepstructurefrommotion. InProc.CVPR,2024.
XiaolongWangandAbhinavGupta. Unsupervisedlearningofvisualrepresentationsusingvideos.
InProc.ICCV,2015.
Xiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-
consistencyoftime. InProc.CVPR,2019.
Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng Wang, and Wei Xu. Occlusion aware
unsupervisedlearningofopticalflow. InProc.CVPR,2018.
12LaurenzWiskottandTerrenceJSejnowski. Slowfeatureanalysis: Unsupervisedlearningofinvari-
ances. Neuralcomputation,14(4),2002.
Yang Zheng, Adam W Harley, Bokui Shen, Gordon Wetzstein, and Leonidas J Guibas.
Pointodyssey: A large-scale synthetic dataset for long-term point tracking. In Proceedings of
theIEEE/CVFInternationalConferenceonComputerVision,2023.
13APPENDIX
A IMPLEMENTATIONDETAILS
Wepre-trainbothonlineandofflinemodelversionsonsyntheticTAP-Vid-Kubric(Doerschetal.,
2022;Greffetal.,2022)for50,000iterationson32NVIDIAA10080GBGPUswithabatchsize
of 1 video. We train CoTracker3 online on videos of length T = 64 with a window size of 16,
andsample384querypointspervideowithabiastowardsobjects. Sincetheonlineversiontracks
onlyforwardintime,wesamplepointsprimarilyatthebeginningofthevideo. Wetraintheoffline
versiononvideosoflengthT ∈ {30,31,...,60}withtimeembeddingsofsize60. Weinterpolate
time embeddings to the current sequence length both at training and evaluation. We sample 512
querypointspervideouniformlyintime. Bothmodelsaretrainedinbfloat16withgradientnorm
clipping using PyTorch Lightning (Falcon & The PyTorch Lightning team, 2019) with PyTorch
distributed data parallel (Li et al., 2020). The optimizer is AdamW (Loshchilov & Hutter, 2017)
withβ =0.9,β =0.999,learningrate5e−4,andweightdecay1e−5. Theoptimizeradoptsa
1 2
linearwarm-upfor1000stepsfollowedbyacosinelearningratescheduler.
WescaleCoTracker3onadatasetofInternet-likevideosprimarilyfeaturinghumansandanimals.
We visualize the scaling pipeline in Figure 5. To ensure the quality and relevance of our training
data, we use caption-based filtering with specific keywords to select videos containing real-world
contentwhileexcludingthosewithcomputer-generatedimagery,animation,ornaturalphenomena
thatarechallengingtotrack,suchasfire,lights,andwater.
Whentrainingonrealdata, weuseasimilarsetupwhilereducingthelearningrateto5e−5with
the same cosine scheduler without warm-up. We train both online and offline versions for 15,000
iterations with 384 tracks per video sampled with SIFT on eight randomly selected frames with
framesamplingbiasedtowardsthebeginningofthevideo.
Following(Karaevetal.,2024),whenevaluatingCoTracker3onlineonTAP-Vid,weadd5×5points
sampledonaregulargridand8×8pointssampledonalocalgridaroundthequerypointtoprovide
contexttothetracker. Wedothesameforthescaledofflineversionduringinference. TheKubric-
trainedofflineversion, however, reliesonuniformpointsamplingduringtraining. Forthismodel,
during evaluation on TAP-Vid, we instead sample 1000 additional support points uniformly over
time.
Figure 5: Scaling pipeline. Given a video, we randomly choose 8 frames and sample 384 query
pointsacrosstheseframesusingSIFTLowe(1999). Then,wepredicttracksforthesequerypoints
withthestudentandrandomlyselectedteachermodels. Finally,wecomputethedifferencebetween
thepredictedtracksandupdatethestudentmodel.
B PERFORMANCE
InFigure6,wecomparethespeedofCoTracker3withotherpointtrackers.Wemeasuretheaverage
timeittakesforthemethodtoprocessoneframe,withthenumberoftrackedpointsvaryingbetween
1and10,000. Weaveragethisacross20videosofvaryinglengthsfromDAVIS. EventhoughCo-
TrackerandCoTracker3applygroupattentionbetweentrackedpoints,thetimecomplexityremains
linearthankstotheproxytokensintroducedby(Karaevetal.,2024). Whileallthetrackersexhibit
14Teacherselectionstrategy Kinetics DAVIS RoboTAP RGB-S
Random 68.2 77.0 78.8 83.3
Averaging 67.4 76.5 77.9 82.4
Median 67.3 76.3 77.3 81.1
Table 8: Supervision. Random sampling of teachers consistently leads to better δ on TAP-Vid
avg
comparedtosupervisionwitheitherthemeanorthemedianofallteachers’predictions.
AverageonTAP-Vid
Num. ofiterations
AverageonTAP-Vid
Model AJ↑δ ↑ OA↑
avg
AJ↑δ ↑ OA↑
avg 1k 63.3 75.6 87.5
Kub+15k 64.0 76.8 90.2 15k 64.0 76.8 90.2
Kub+15k+15k64.2 76.9 89.7 30k 64.4 76.8 89.7
60k 64.4 77.0 89.5
Table 9: Repeated scaling. We scale Co-
Tracker3offline,thenstartfromascaledmodel, Table10: Longertrainingon15kvideos. We
andscaleitagainwiththescaledmodelasone trainCoTracker3offlineforlongertodetermine
of the teachers. Repeated scaling slightly im- the optimal number of iterations for a given
provestrackingaccuracy. numberofvideos. Asatrade-offbetweentrain-
ing costs and the results obtained, we use the
same number of iterations as the number of
videos.
lineartimecomplexitydependingonthenumberoftracks,CoTracker3isapproximately30%faster
thanLocoTrack(Choetal.,2024),thefastestpointtrackertodate.
0.5
LocoTrack
BootsTAPIR
0.4
CoTracker
Ours offline
0.3 Ours online
0.2
0.1
0.0
0 2000 4000 6000 8000 10000
Number of tracked points
Figure 6: Efficiency. We evaluate the speed of different trackers on DAVIS depending on the
number of tracks and report the average time each tracker takes to process a frame. Our offline
architectureisthefastestamongallthesemodels,withLocoTrackbeingthefastesttrackertodate.
C ADDITIONALEXPERIMENTS
Trainingwiththeaverageofteachers’predictions. Interestingly,wefoundthataggregatingthe
predictions of multiple teachers instead of using a random teacher does not improve performance,
asshowninTable8, whereasincorporatingadditionalteachersintotrainingconsistentlyenhances
thequalityofourstudentmodel,demonstratedinTable5.
15
]s[
,emarf
rep
emiTRepeated scaling. We study the effect of iterative scaling to investigate the limits of our multi-
teacherscalingpipeline. Specifically,wescaleCoTracker3offlineusingourpipeline,whereoneof
theteachersisthemodelitself. Wethentakethistrainedstudentmodelandattempttoimproveit
furtherbyre-applyingthesamescalingpipelinebutwiththeoriginalstudentmodelreplacedbythe
newlytrainedstudentmodelasoneoftheteachers.
Wefindthatthissecondroundofscalingleadstoslightimprovementsinperformancemetrics. This
suggeststhatthestudentmodelhasalreadydistilledmostoftheknowledgefromtheotherteachers
duringtheinitialtrainingphase. WereporttheresultsinTable9.
Convergence behavior during scaling. We examine the convergence behavior of our scaling
pipeline by fixing the dataset and all the hyper-parameters, varying only the number of iterations
over the dataset. We show in Table 10 that increasing the number of iterations leads to improved
performanceonTAP-Vid,butwithdiminishingreturns. Specifically,weobserveasaturationpoint
beyondwhichfurtherincreasesinthenumberoftrainingiterationsdonotyieldsignificantimprove-
mentsinmodelquality. Wethususethesamenumberofiterationsasthenumberoftrainingvideos
withabatchsizeof32,iteratingovereachvideo32times.
D LIMITATIONS
Akeylimitationofourpseudo-labelingpipelineisitsrelianceonthequalityanddiversityofteacher
models. TheobservedsaturationinperformanceonTAP-Vidduringscalingsuggeststhatthestu-
dentmodelabsorbsknowledgefromalltheteachersand,afteracertainpoint,strugglestoimprove
further. Thus,weneedstrongerormorediverseteachermodelstoachieveadditionalgainsforthe
studentmodel.
16