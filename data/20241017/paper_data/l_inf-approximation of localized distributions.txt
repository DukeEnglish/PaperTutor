ℓ
-approximation of localized distributions
∞
Tiangang Cui∗ Shuigen Liu† Xin Tong†
October 16, 2024
Abstract
Distributions in spatial model often exhibit localized features. Intuitively, this locality implies
a low intrinsic dimensionality, which can be exploited for efficient approximation and computation
of complex distributions. However, existing approximation theory mainly considers the joint distri-
butions, which does not guarantee that the marginal errors are small. In this work, we establish
a dimension independent error bound for the marginals of approximate distributions. This ℓ ∞-
approximation error is obtained using Stein’s method, and we propose a δ-locality condition that
quantifies the degree of localization in a distribution. We also show how δ-locality can be derived
from different conditions that characterize the distribution’s locality. Our ℓ ∞ bound motivates the
localization ofexistingapproximationmethodstorespectthelocality. Asexamples,weshowhowto
uselocalizedlikelihood-informedsubspacemethodandlocalizedscorematching,whichnotonlyavoid
dimension dependence in the approximation error, but also significantly reduce the computational
cost dueto thelocal and parallel implementation based on thelocalized structure.
1 Introduction
Approximationofdistributions is a fundamentalprobleminappliedmathematics andcomputer science.
With the developmentofmodernmachine learning methods [15, 9, 21, 11], significantadvancements are
achievedinapproximationofcomplexdistributions. Comparedtovastempiricalsuccess,concretemath-
ematicalunderstandingofthese methods isstilllacking,especiallyinthe contextofhigh-dimensionality.
Inthispaper,wedevelopaℓ -approximationtheoryforalargeclassofhigh-dimensionaldistributions
∞
thatexhibitlocalized features,explaininghowsuchdistributionscanbeapproximatedefficientlywithout
suffering from the curse of dimensionality. These localized distributions are common in spatial models
and graphical models arising from many statistical physics and computer science problems. A typical
example is the Ising model where there are only local interactions.
Intuitively, such localized structure implies that the distribution is intrinsically low-dimensional,
which can be exploited for efficient approximation and computation. We formalize this intuition and
proves a dimension independent error bound for the marginals of approximate distributions. More
precisely, we show in Theorem 2.4 that under some locality assumption, the 1-Wasserstein marginal
approximation error is bounded by local perturbations of the log density:
maxW (π ,π′) δ max logπ(x) logπ′(x) π(x)dx.
i 1 i i ≤ · i |∇i −∇i |
Z
Note that the choice of the log density is motivated by its foundational role in many state-of-the-art
approximationand sampling methods, such as the score matching [14], variationalautoencoder [15] and
diffusion models [21, 11]. Approximation error in terms of the log density is well documented in the
literature, but most only consider the joint distribution, which in general is dimension dependent. One
typical example is the Otto-Villani inequality [19]:
W2(π,π′) c−2 logπ(x) logπ′(x)2 π(x)dx.
2 ≤ |∇ −∇ |
Z
These bounds does not guarantee that the marginal error is small, and becomes less informative when
one is only interested in the statistics of the marginals. Our ℓ -approximation result fills the gap by
∞
exploiting the locality of the distribution and avoid the dimension dependence here.
∗UniversityofSydney(tiangang.cui@sydney.edu.au)
†NationalUniversityofSingapore(shuigen@u.nus.edu, xin.t.tong@nus.edu.sg)
1
4202
tcO
51
]LM.tats[
1v17711.0142:viXraLocalityinthedistributionsiswell-knownandexploitedinthecommunitieslikespatialstatistics[25,
6,20],dataassimilation[13,10]andimageprocessing[27]. Thekeyobservationisthatthecorrelationof
parametersatlargedistancesisneglectableinmanyspatialmodels. Therefore,modelingandcomputing
suchdistributionscanbedoneinanapproximatemanner,byartificiallyremovinglong-rangecorrelations,
thusgreatlyimprovingthecomputationalefficiency. SuchtechniqueiscalledVecchia approximation [25]
in spatial statistics, and localization [13] in data assimilation. Despite the extensive application studies,
themathematicalcharacterizationofthisapproximationisrarelyexplored. Withthisinmind,wediscuss
inSection2.3the problemofapproximatinglocalizeddistributions by undirectedgraphicalmodels (also
known as the Markovrandom fields [16, 27]), i.e. one aims to find local potentials u that depends only
i
the neighboring sites N of the site i, such that
i
1
π(x) exp u (x ) .
≈ Z
i Ni
!
i
X
Note this is the prototype math formulation of the Vecchia approximation.
In the sampling literature, there are also recent work [17, 23, 8, 2] exploiting the locality for efficient
sampling. [17]proposedtoapplythelocalizationtechniqueinMCMCsamplingtosolveBayesianinverse
problems and obtained a dimension independent convergencerate. [23] extended this idea and proveda
dimension independent convergence rate of the Metropolis-Adjusted-Langevin-Algorithm-within-Gibbs
(MLwG) for distributions witha sparseconditionstructure(equivalentto a sparsegraphicalmodel). [8]
made a initial trial to obtain ℓ -approximation error for a image deblurring problem, based on which
∞
we further develop the theory to the general case in this paper. We also notice a concurrent work [2]
thatshowsthemarginalsofunadjustedLangevinalgorithmconvergestothetargetataconvergencerate
with logarithm dependence on the dimension. They introduced a ℓ -type Wasserstein distance, which
∞
follows a similar focus on the marginals as in our work.
In this paper, we propose the concept of δ-locality in Definition 2.1 to quantifies how a distribution
is localized. We will use the δ-locality to obtain ℓ -approximationerror by applying the Stein’s method
∞
[22]. We show how to obtain δ-locality given different characterization of the localized distribution,
includingthe sparsegraphicalmodel. Themaintechnicalcontributionistoshowthatδ isdimension in-
dependent fortypicallocalizeddistributions. Theℓ -approximationboundmotivatestolocalizeexisting
∞
approximation methods to respect the locality. As examples, we show how to localize the likelihood-
informed subspace (LIS) method [4, 3, 5] and the score matching method [14, 21, 11]. Such localized
approximationmethodnotonlyyieldsadimensionindependentℓ errorbound,butalsogreatlyreduces
∞
the computational cost due to the local and parallel implementation based on the localized structure.
The paper is organized as follows. The main results are presented in Section 2, where we define
the δ-locality, proves the main approximation theory, and then discuss some useful extensions. The
following three sections discuss three applications of the theory: the localized LIS method in Section 3,
the localized score matching in Section 4, and the localized neural network surrogate in Section 5. In
Section 6, we provide the main steps of the technical results for the δ-locality.
Notations
• Norms. For v Rd, briefly denote the vector 2-norm as v . Likewisely, for a matrix A Rm×n,
∈ | | ∈
A =sup |Av| denotesthematrix2-norm. A denotestheFrobeniusnorm A 2 = A2 .
k k v6=0 |v| k kF k kF i,j ij
Forafunctionf :Rd R,denote f astheweightedLp norm f p = f(x)Ppπ(x)dx.
→ k kLp(π) k kLp(π) Rd| |
Foravector-valuedfunctionf :Rd Rm,stilldenote f p = f(x)pπ(x)dxandnotehere
→ k kLp(π) Rd| | R
f(x) is the vector 2-norm without specification.
| | R
• Matrix. Denote A B if A B is positive semi-definite, and A B if B A.
(cid:23) − (cid:22) (cid:23)
• Probability. Denote (Rd) as the set of probability distributions in Rd with finite first moment.
1
The 1-Wasserstein diP stance between two distributions π,π′ (Rd) is defined as
1
∈P
W (π,π′)= inf x y dγ(x,y),
1
γ∈Π(π,π′) ZRd| − |
whereΠ(π,π′)isthesetofcouplingsofπ andπ′,i.e. if(X,Y) γ Π(π,π′),thenX π,Y π′.
∼ ∈ ∼ ∼
Denote conditional independence X Y Z if p(x,y z)=p(xz)p(y z).
⊥⊥ | | | |
2• Block decomposition. For the parameter x Rd, we assume there is a fixed block decomposition
∈
b
x=(x ,...,x ), i [b], x Rdi, d= d ,
1 b i i
∀ ∈ ∈
i=1
X
where we denote [b] := 1,...,b . Denote i := [b] i . For a distribution π, denote π as
i
{ } − \{ }
the marginal distribution on x , i.e. π (x ) = π(x ,x )dx . Denote the partial derivatives
i i i i −i −i
= , 2 = 2 .
∇i ∇xi ∇ij ∇xixj R
2 ℓ -approximation theory
∞
2.1 δ-locality
We begin by defining the δ-locality of a distribution, which quantifies the locality of the distribution.
Definition 2.1. A distribution π (Rd) is called δ-localized for some constant δ >0, independent
1
of d, if for any i [b] and 1-Lipsch∈ itzP function φ:Rdi R such that φ(x i)π i(x i)dx
i
=0, the solution
∈ →
u(x) to the Stein equation
R
∆u(x)+ logπ(x) u(x)=φ(x ) (2.1)
i
∇ ·∇
satisfies the gradient estimate
b
u := u δ. (2.2)
k∇ k∞,1 k∇j kL∞ ≤
j=1
X
Definition 2.1 is a technical description of the localized structure of the distribution motivated by
the Stein’s method [22]. We will show in Theorem2.4 that the δ-locality condition directly implies that
localperturbationofthedistributiononlyintroducesdimensionindependentmarginalerror. Wewillsee
from the proof there that
u(x) quantifies how modifications of logπ(x) affect the marginal π .
j j i
∇ ∇
We keep the technical definition here for the sake of generality. To make it more concrete, we introduce
below two common assumptions that imply the δ-locality.
Proposition 2.2. π (Rd) is δ-localized if it satisfies any of the following conditions:
1
∈P
(1) (ℓ -condition) H(x)= 2logπ(x) is c-unifomly diagonal block dominant, i.e. thereexists a matrix
1
M Rb×b s.t. i,j [b− ],∇ i=j,
∈ + ∀ ∈ 6
H (x) M I , H (x) M ; M +c M ,
ii
(cid:23)
ii di
k
ij
k≤
ij ij
≤
ii
j6=i
X
where H (x) denotes the (i,j)-th subblock of H(x). Here δ can be taken as δ =c−1.
ij
(2) (ℓ -condition) H(x) = 2logπ(x) has uniform block bandwidth B > 0, and uniform lower and
2
−∇
upper bounded m and M where 0<m M < , i.e.
≤ ∞
i j >B, H (x)=0; mI H(x) MI,
ij
∀| − | (cid:22) (cid:22)
Here δ can be taken as δ = 2BM.
m2
Proofs can be found in Section 6.1.
Remarks 2.3. (1) Note that the two conditions above both require that π is log-concave. To see that
the ℓ -condition implies log-concavity, denote M′ := 2M 1 M , then M′ is c-diagonal dominant.
1 ij ii i=j − ij
Gerˆsgorindiscs theorem [12] then implies the smallest eigenvalue of M′ is lower bounded by c, and thus
uTH(x)u= uTH (x)u M u 2 M u u
i ij j ≥ ii | i | − ij | i || j |
i,j i i6=j
X X X
= M′ u u c u 2 =c u2.
ij| i || j |≥ | i | | |
ij i
X X
(2) The two conditions characterize the locality of the distribution in different ways. The ℓ -condition
1
uses diagonaldominance ofthe Hessian,whichimplies that anyblockis mostly influencedby itself. The
ℓ -condition directly uses bandedness of Hessian to characterize the localized structure.
2
32.2 ℓ -approximation error
∞
Theorem 2.4. Consider two distributions π,π′ (Rd). Assume π′ is δ-localized, then the marginal
1
∈ P
W -distance of π,π′ satisfies
1
maxW (π ,π′) δ max logπ′ logπ , (2.3)
i∈[b] 1 i i ≤ ·j∈[b]k∇j −∇j kL1(π)
where π and π′ are the marginals of π and π′ on x respectively.
i i i
Proof. By Kantorovichduality [26],
W (π ,π′)= sup φ (x )(π (x ) π′(x ))dx ,
1 i i 0 i i i − i i i
φ0∈Lip 1Z
where Lip denotes the 1-Lipchitz function class. Fix any φ Lip . Denote
1 0 ∈ 1
φ(x )=φ (x ) φ (x )π′(x )dx .
i 0 i 0 i i i
−
Z
Then
φ (x )(π (x ) π′(x ))dx = φ(x )(π (x ) π′(x ))dx
0 i i i − i i i i i i − i i i
Z Z
= φ(x )π (x )dx = φ(x )π(x)dx.
i i i i i
Z Z
Here we use π(x )= π(x ,x )dx . Let u(x) solve the Stein equation
i i −i −i
R ∆u(x)+ logπ′(x) u(x)=φ(x ).
i
∇ ·∇
Now by integration by parts,
φ(x )π(x)dx= (∆u(x)+ logπ′(x) u(x))π(x)dx
i
∇ ·∇
Z Z
= ( u(x) logπ(x)+ logπ′(x) u(x))π(x)dx
−∇ ·∇ ∇ ·∇
Z
b
= u(x) ( logπ′(x) logπ(x))π(x)dx.
j j j
∇ · ∇ −∇
j=1Z
X
By assumption, the solution satisfies the gradient estimate (2.2). Therefore,
b
φ(x )π(x)dx u logπ′(x) logπ(x) π(x)dx
i ≤ k∇j kL∞ |∇j −∇j |
Z j=1 Z
X
δ max logπ′(x) logπ(x) π(x)dx.
j j
≤ · j |∇ −∇ |
Z
Therefore, we obtain
maxW (π ,π′) max max φ (x )(π (x ) π′(x ))dx
i 1 i i ≤ i φ0∈Lip
1Z
0 i i i − i i i
max max δ max logπ′(x) logπ(x) π(x)dx
j j
≤ i φ0∈Lip
1
· j
Z
|∇ −∇ |
= δ max logπ′ logπ .
· j k∇j −∇j kL1(π)
This completes the proof.
42.3 Approximation by sparse graphical models
In this section, we consider a special case of approximating a distribution π by an undirected graphical
model [16, 27]. The undirected graphical model, also called Markov random field, models the local
dependenceofthedistributionbyanundirectedgraph. Itrequiresthatanynonadjacenttwocomponents
X ,X are conditionally independent given the rest of the components.
i j
Consider an undirected graph G = (V,E) where V = b. We attach each vertex in V with a block
| |
index i uniquely. Denote N = k V : jk E as the neighboring vertices of j. Here we require E
j
{ ∈ ∈ }
includes all the self-loop in G, i.e. j G,jj E.
We say a distribution π(x)
∀ (Rd∈
) has
de∈
pendence graph G if
∈P
j,k V, k / N x x x , (2.4)
∀ ∈ ∈ j ⇒ j ⊥⊥ k | N j−
where we denote as the conditional independence, and N− =N v . If π(x) is twice differentiable,
⊥⊥ j j \{ }
then it is equivalent to require that
k / N 2 logπ(x)=0.
∈ j ⇒ ∇jk
Note itgeneralizesthe bandedcondition onthe Hessianin Proposition2.2 by using the sparsegraphical
structure. Parallelly, one can show that under certain sparsity condition on G, a distribution π with
dependence graph G is δ-localized.
To specify the sparsity condition, we denote the q-th neighbor Nq (q Z ) as
j ∈ +
Nq := k V :d (j,k) q ,
j { ∈ G ≤ }
where d (j,k) is the shortest path distance between j and k in G, i.e. the minimum number of edges
G
thatmust be traversedto go fromj to k. For convenience,we taked (j,j)=0,andd (j,k)= if j,k
G G
∞
are not connected.
Definition 2.5. A s-sparse graph G with ambient dimension r Z is such that
+
∈
j V, Nq 1+sqr.
∀ ∈ | j|≤
We also call G a s-sparse graph without specifying the ambient dimension.
Remark 2.6. Thedefinitionofsparsegraphisdifferentfromthatingraphtheory. Roughlyspeaking,the
sparsity s denotes the size of the immediate neighbor, and the dimension r controls the growth rate of
the neighbor size with respect to the distance. Note we require it grows at most polynomially to ensure
the locality. A typical example of such sparse graph is the lattice model Zr. A naive bound for Zr is
Nq (2q+1)r <1+(3q)r. So that Zr is 3r-sparse with ambient dimension r.
| j|≤
Proposition 2.7. Let G=(V,E) be a s-sparse graph with ambient dimension r. Suppose π (Rd) is
∈P
a distribution with dependence graph G, and satisfies
x Rd, mI 2logπ(x) MI.
∀ ∈ (cid:22)−∇ (cid:22)
Then π is δ-localized with δ = sr!κr where κ= M. As a consequence, for any π′ (Rd),
m m ∈P1
sr!κr
maxW (π ,π′) max logπ logπ′ ,
j∈V 1 j j ≤ m k∈V k∇k −∇k kL1(π′)
Proofs can be found in Section 6.2. Note this recovers the second result in Proposition 2.2, where
the bandedness condition corresponds to a 2B-sparse graph with ambient dimension r =1.
2.4 Marginal error of multiple blocks
In Theorem 2.4, we show that the marginal distance of π′ and a δ-localized distribution π on one block
x is dimension independent. Here we extend this result to the case of multiple blocks. This result
i
providesfurther controlonthe correlationbetween differentblocks in π′, which may notbe capturedby
Theorem 2.4.
5Theorem 2.8. Consider two distributions π,π′ (Rd). Assumeπ′ satisfies any one of the conditions
1
∈P
in Proposition 2.2 or Proposition 2.7, then for any index set I [b], the marginal W -distance of π,π′
1
⊂
on x :=(x ) satisfies
I i i∈I
W (π ,π′) δ I max logπ′ logπ . (2.5)
1 I I ≤ | |· j∈[b]k∇j −∇j kL1(π)
Here δ can be taken as the same constant in Proposition 2.2 or Proposition 2.7.
Proofs can be found in Section 6.3. The implications of the theorem is two-fold. First, for blocks
i,j at large distance, the correlation of x ,x under π is neglectable, and π are almost independent:
i j i,j
π π π . The above theorem says that π′ is close to π , so that x ,x are also almost independent
i,j ≈ i j i,j i,j i j
under π′. Conversely, for neighboring blocks i,j, π′ can capture the correlation between x ,x in π.
i,j i j
3 Application I: localized LIS method
Likelihood-informed subspace (LIS) method [4, 3, 5] is a dimension reduction technique to sample effi-
ciently from a high-dimensional distribution of the form
1
π(x)= l(x)π (x), (3.1)
0
Z
whereπ (x)isthepriordistribution,andl(x)isthelikelihoodfunction. Samplingfromsuchdistributions
0
is a central problem in Bayesian inference. The key in the LIS method is the low rank structure of the
likelihood function l(x), making it possible to be approximated by a rigde function
l(x) l (x ), x =P x,
r r r r
≈
whereP isaprojectiontoalow-dimensionalsubspace . TheLISmethodproposetoapproximate
r r
X ⊂X
the target distribution by
π(x) π (x) l (x )π (x),
r r r 0
≈ ∝
which, with properly chosen informed subspace and ridge approximation of l(x), provides certified
r
X
approximationerror bounds [5]. With such low-rank approximation,the sampling complexity is greatly
reduced, and the method has been successfully applied in many high-dimensional problems [4, 3].
3.1 Localized LIS method
In LIS, the informed subspace is constructed globally, which may not capture the spatial structure
in the target distribution. For distributions with sparse dependence graph, a locally defined informed
subspacecanbeabetterchoice. Basedonthisobservation,weproposetouselocalizedlikelihood-informed
subspace (LLIS), which incorporates domain decomposition in LIS method. In essence, we decompose
the parameters into small blocks, and apply LIS in each block to construct the LLIS.
Inthe following,weshowhowto apply LLISwhenthe prioris aGaussiandistribution,andestablish
a dimension independent error for LLIS approximation under the sparse graphical assumption.
LLIS construction WeproposetoapplytheLLISapproximationunderatransformedrepresentation
so that the prior is a standard Gaussian, i.e. consider the change of variables
x˜=C−1/2x.
Then x˜ π˜ = (0,I ) if x π . Under the transformed representation x˜, we aim to find the LLIS of
0 d 0
∼ N ∼
the product form
= ,
r r,j
X X
j∈V
M
e e
where
r,j
Rdj denotesthe subspaceofthe reducedparamaterinblockj. Todetermine r,j, consider
X ⊂ X
the following diagnostic matrix
e e
= E ˜ log˜l(x˜)(˜ log˜l(x˜))T+ ˜2log˜l(x˜) ˜2log˜l(x˜)
x˜∼π˜
M ∇ ∇ ∇ ·∇ (3.2)
= C1/2 hE logl(x)( logl(x))T+ 2logl(x) C i2logl(x) C1/2,
f · x∼π ∇ ∇ ∇ · ·∇ ·
(cid:2) (cid:3)
6where we denote the transformed quantities with a tilde, to be specific,
x˜=C−1/2x, π˜(x˜)=(detC)−1 2π(x), ˜l(x˜)=l(x), ∇˜ = ∇x˜, ∇˜2 = ∇2 x˜x˜.
Notice comparedto the existing LISmethods [5]where only firstorderinformationis used, here we also
incorporates the second order information in the diagnostic matrix.
With the diagnostic matrix, we define
r,j
to be the rank-r
j
principle subspace of
jj
Rdj×dj,
X M ∈
the diagonal block of for block j. To be specific,
M
e f
f Xr,j =span {u 1,...,u rj},
where u ,...,u are the eigenvectors oef with the largest r eigenvalues. Here r = rank( ) is
1 rj Mjj j j Xr,j
the number of informed directions we choose. It can be determined, for instance, by (see [5])
f e
r
r =min 1 r d : λ ( ) (1 ǫ)tr ,
j j k jj jj
( ≤ ≤ M ≥ − M )
k X=1 h i
f f
where λ ( ) denotes the k-th largest eigenvalue of , and ǫ (0,1) is some prescribed threshold.
k jj jj
M M ∈
Given , denote accordingly the complementary space
r
X
f f
e ⊥,j =( r,j)⊥, ⊥ = ⊥,j.
X X X X
j∈V
M
e e e e
Denotethereducedparameterx˜ ,itscomplementx˜ andtheircounterpartsintheoriginalrepresentation
r ⊥
as
x˜ =(x˜ ) , x˜ =P˜ x˜ , x :=C1/2x˜ :=C1/2 .
r r,j j∈V r,j r,j j r r r r
∈X X
x˜ =x˜ x˜ =(x˜ ) , x˜ =x˜ x˜ =P˜ x˜ , x :=C1/2x˜ :=C1/2 .
⊥
−
r ⊥,k j∈V ⊥,k j
−
r,j ⊥,k j ⊥ ⊥ ∈eX⊥ X⊥
Here P˜ is the orthogonalprojectoronto , and P˜ is its orthogonalcomplement. Note x and x
r,j Xr,j ⊥,k er ⊥
are not orthogonal in the usual sense, but it holds that
e
xTC−1x =x˜Tx˜ =0.
r ⊥ r ⊥
That is, x and x are independent under the prior π = (0,C).
r ⊥ 0
N
LLIS approximation As in the LIS method, we construct the approximate likelihood l (x ) by av-
r r
eraging l(x) in the logarithm regime over the uninformed subspace , and approximate the target
⊥
X
distribution by
π (x) l (x )π (x), x =P x,
r r r 0 r r
∝
(3.3)
where logl (x )= logl(x +x )π (x )dx .
r r r ⊥ 0 ⊥ ⊥
Z
Note x and x are independent under π , so that π (x x ) = π (x ). Also note in the transformed
⊥ r 0 0 ⊥ r 0 ⊥
|
representation, the approximation takes the same form,
log˜l (x˜ ):= logl (C1/2x˜ )= logl(C1/2x˜ +x )π (x )dx
r r r r r ⊥ 0 ⊥ ⊥
Z (3.4)
= log˜l(x˜ +C−1/2x )π (x )dx = log˜l(x˜ +x˜ )π˜ (x˜ )dx˜ .
r ⊥ 0 ⊥ ⊥ r ⊥ 0 ⊥ ⊥
Z Z
Here we choose to average in the logarithm regime for the sake of simplicity of the theoretical results.
In practice, one can also consider other averaging schemes, see [5].
3.2 Approximation error
We show the marginal approximation error of the LLIS method is dimension independent under the
sparse graphical assumption. To be specific, we assume
Assumption 3.1. The target distribution is of the form (3.1), and
7• π (x),π(x) have dependence graph G that is s-sparse with ambient dimension r.
0
• π (x)= (0,C), and there exists 0<m M < s.t.
0
N ≤ ∞
mI C−1 MI, mI 2logπ(x) MI.
(cid:22) (cid:22) (cid:22)−∇ (cid:22)
Asin[4,5],wewillboundtheapproximationerrorofLLISbytheeigenvalueresiduesofthemodified
diagnosticmatrices(3.2). Forsimplicity, denote the eigenvalue residuefor apositive semidefinite matrix
C Rd×d and a subspace V Rd as
r
∈ ⊂
(C,V ):=tr[P CP ], (3.5)
r ⊥ ⊥
R
where P is the orthogonalprojector onto V⊥.
⊥ r
Theorem 3.2. Assume the target distribution π satisfies Assumption 3.1. Consider its LLIS approxi-
mation π (3.3). Then there exists a dimension independent constant C s.t. the marginal error of the
r π
LLIS approximation satisfies
1/2
maxW (π ,π ) C max ( , )+ ( , ) , (3.6)
1 j r,j π k r,k j,k r,j
j ≤ k∈V R G X R H X 
j∈V
X
 e e e e 
where we denote as the residue defined in (3.5), and the diagnostic matrices
R
:= E ˜ log˜l(x˜)(˜ log˜l(x˜))T .
Gk x˜∼π˜r ∇k ∇k
(3.7)
h i
Hje,k := E x˜∼π˜r ∇˜ j ∇˜ klog˜l(x˜) ·∇˜ k ∇˜ jlog˜l(x˜) .
h i
Proofs can be found in Apependix B.
Remarks 3.3. (1)We wouldliketocomparethe rightrandsideof (3.6)withthe diagnosticmatrix(3.2):
• The expectation in (3.7) is taken w.r.t. the approximate distribution π˜ , while in (3.2) it is taken
r
w.r.t. the target distribution π˜. So that Theorem 3.2 is a posteriori error estimate instead of a
priori estimate.
• (3.2) can be written as
=E ˜ log˜l(x˜)(˜ log˜l(x˜))T+ ˜ ˜ log˜l(x˜) ˜ ˜ log˜l(x˜) .
jj x˜∼π˜ j j j k k j
M "∇ ∇ ∇ ∇ ·∇ ∇ #
k∈V
X
f
Forthe secondtermhere,the summationis takenonthe secondindex (k) in ˜ ˜ , while in(3.6),
j k
∇ ∇
it is taken on the first index (j) in ˜ ˜ .
j k
∇ ∇
(2) We would also like to comment that the summation in (3.6) is still a dimension independent
j∈V
term. Since the second order derivatives ˜ ˜ log˜l(x˜) decay fast with d (j,k), which is a result of the
∇j ∇k P G
sparsity graphical assumption.
4 Application II: localized score matching
The score function of a distribution p(x) is defined as s(x) := logp(x). In score matching [14, 21],
∇
one uses variationalmethod to approximate p(x) by fitting the score function. Given a hypothesis class
P = p (x) : θ Θ of distributions parameterized by θ, the score matching aims to minimize the
θ
{ ∈ }
Fisher divergence over θ:
D (p ,p):=E s (X) s(X)2, s (x):= logp (x).
Fisher θ X∼p θ θ θ
| − | ∇
Thoughthe Fisher divergence is not directly computable, a simple integrationby parts givesa tractable
loss function
J(θ)=E 2tr( s (X))+ s (X)2 .
X∼p θ θ
∇ | |
h i
8HereJ(θ)=D (p ,p) E logp(X)2,sothatminimizing J(θ) isequivalentto minimizingthe
Fisher θ X∼p
− |∇ |
Fisher divergence. In practice, J(θ) is approximated by its empirical version: given data (X ,...,X )
1 N
drawn from p, the empirical loss is
N
1
Jˆ(θ)= 2tr( s (X ))+ s (X )2 .
θ i θ i
N ∇ | |
Xi=1h i
ThehypothesisclassP isconstructedsuchthatthegradientofthescorefunctions (x)iseasytocompute
θ
andoptimizeover,sothatanapproximatedistributionpˆ canbeobtainedefficientlybyminimizingJˆ(θ).
θ
Despite its simple implementation, fitting a generic score function in high-dimensional space is still
computationally expensive. We propose to reduce the complexity by leveraging the localized structure,
i.e. the target distribution is or can be approximated by a sparse graphical model. The locality in the
modellargelyreducesthecomplexityofthescorefunction,basedonwhichweproposethelocalized score
matching method.
4.1 Localized score matching
Factorization of sparse graphical models The key motivation of localized score matching is the
existenceoflocalfactorizationofthelogdensityforthesprasegraphicalmodels. Consideradistribution
p(x) with dependence graph G=(V,E). By Hammersley-Clifford theorem [16], one can factorize
logp(x)= u (x ).
j Nj
j∈V
X
Note u
j
is a function of x
Nj
∈RdNj, where we denote the local dimension
d := d .
Nj k
k X∈Nj
Suppose G is s-sparse in the sense of Definition 2.5, then the maximal local dimension satisfies
d :=maxd (1+s)maxd . (4.1)
loc
j
Nj
≤ j
j
In a sparse graphical model, the local dimension d d, so that the complexity of logp(x) would be
loc
greatly reduced. This motivates to fit the target usin≪ g distributions from a hypothesis class P that
G
captures the localized structure. Specifically, we consider the hypothesis class
P = p (x)=exp u (x ) :u ,
G  θ  θ,j Nj  θ,j ∈Uθ,j
 j X∈V 
 
where
θ,j
issomeclassoffunctionsinRdNj tobedetermined. ThedistributionsinP
G
havedependence
U
graph G by definition. Notice the size of the hypothesis space, measured by its metric entropy [24], is
O(poly(d)exp(Cd )), thus avoiding the curse of dimensionality.
loc
Here we assume that the G is prefixed based on prior knowledge of the target distribution. One can
also learn the dependence graph from data, but we do not consider it here.
Localized loss function We define the local score function as s (x) := logp(x). To fit the local
j j
∇
score function, it suffices to minimize the local loss
J (θ):=E 2tr( s (X))+ s (X)2 , (4.2)
j X∼p j θ,j θ,j
∇ | |
h i
where s (x)= logp (x). Note for p P , the local score s can be computed locally by
θ,j j θ θ G θ,j
∇ ∈
s (x)= u (x ),
θ,j ∇j θ,k Nk
k X∈Nj
whichonlyconcernsasmallgroupofthe localscores. Tofitthe wholescorefunction, onecouldconsider
to minimize max J (θ). However, it is no longer equivalent to minimizing
v∈V j
maxE s (X) s (X)2 =max(J (θ)+C ), C :=E s (X)2.
X∼p θ,j j j j j X∼p j
j∈V | − | j∈V | |
9ThereasonisthatC variateswithj anddirectlyminimizingmax J (θ)mayignoresthosej withlarger
j j j
C . Nevertheless, one can relax the problem to be a saddle point problem:
j
minJ (θ):=min max max(J (θ)+λ ), (4.3)
loc j j
θ∈Θ θ∈Θ λj≤Cj j∈V
∀j∈V
Here Θ denotes the parameter space of the hypothesis class. Though C is not directly accessible, one
j
canreplaceitbysomeaccessiblelowerboundofitinimplementation. Onealsoneedsto replacethe loss
by the empirical version
Jˆ (θ)= max max Jˆ(θ)+λ ,
loc j j
λj≤Cj j∈V
∀j∈V (cid:16) (cid:17)
(4.4)
N
1
Jˆ(θ)= 2tr( s (X ))+ s (X )2 ,
j j θ,j i θ,j i
N ∇ | |
Xi=1h i
where X ,...,X are i.i.d. samples from p.
1 N
4.2 Statistical error
In this section, we aim to quantify rate the convergence of the minimizer θˆ of the empirical loss (4.4)
∗
to the minimizer θ of the population loss (4.3), i.e.
∗
θ =argminJ (θ), θˆ =argminJˆ (θ)
∗ loc ∗ loc
θ∈Θ θ∈Θ
in terms of the number of samples N and the full dimension d. Take the hypothesis class P larger to
include the true distribution, i.e. p P, so that p = p. Then such convergence rate quantifies the
∈
θ∗
sample complexity to learn the target distribution, i.e. how many samples are needed to approximate
the target distribution within a certain error.
Forsimplicity,weconsiderthetargetdistributionp(x)supportedintheunitintervalId =[0,1]d,and
take the following hypothesis class
P = p (x)=exp u (x ) :mI 2logp MI, u ,
θ

θ,j Nj
 (cid:22)−∇
θ
(cid:22)
θ,j ∈Uθ,j
(4.5)
n j X∈V o
 
Uθ,j
= u
θ,j
∈C 02(IdNj): ku
θ,j kC2
≤R .
n o
In practice, it can be implemented, for instance, by the Input Convex Neural Networks [1].
Theorem 4.1. Given a s-sparse graph G=(V,E) with ambient dimension r. Consider approximating
the target distribution p(x) (Id) using localized score matching with the hypothesis class P defined
1
∈ P
above. Then with probability at least 1 δ, it holds that
−
maxW (p ,pˆ ) CRN−1/4 logδ−1+logb 1/4 ,
1 j j
v∈V ≤
(cid:0) (cid:1)
where pˆ:=p , C is a constant depending only on s,r,m,M and the local dimension d , thus indepen-
θˆ
∗
loc
dent of the total dimension d; and b is the number of blocks, and is the only term of O(d).
Proofs can be found in Appendix C. The theorem implies that sample complexity of the localized
score mathching is bounded by N = O(ε−4log(d/δ)), with only logarithm dependence on the total
ε
dimension d, exactly characterizing that learning a localized distribution does not suffer from the curse
of dimensionality. We also comment that this result does not achieve a minimax rate for estimating the
target distribution p, as this is a corollary of the statistical estimation of the local score functions.
5 Application III: localized neural network surrogate
In this section, we consider the problem of constructing a surrogate for high dimensional distributions.
It is ubiquitous that the target distribution is accessible, but is computationally expensive to evaluate.
A common strategy is to construct a surrogate of the target distribution that is more computationally
10efficient, but still preserves the required accuracy. In this section, we investigate how the localized
structure in the distribution helps to reduce the complexity of finding the surrogate.
As in the previous section, we assume the target distribution p(x) has dependence graphG=(V,E)
that is s-sparse with ambient dimension r. We will construct a surrogate distribution p (x) that has
θ
the same dependence graph G, and is easier to compute and sample from. There are various ways to
construct the surrogate, e.g. vartional autoencoder [15], or the localized score matching in the previous
section, but we do not specify the training method here.
In this section, we focus on the approximation capacity of the surrogate, and specifically consider
using neural networks as the surrogate. We take the hypothesis class to be
P = p (x)=exp u (x ) :mI 2logp MI, u Φ (L,W) , (5.1)
NN θ

θ,j Nj
 (cid:22)−∇
θ
(cid:22)
θ,j
∈
dNj
n j X∈V o
 
where we denote the fully connected neural network function class [28]
Φ (L,W)= :Rd0 Rθ = (l),b(l) L Θ (L,W) .
d0 {Nθ → | {W }l=1 ∈ d0 }
Here L,W are the depth and width of the neural function
(x)=( (L)σ()+b(L)) ( (1)x+b(1)),
θ
N W · ◦···◦ W
where σ is the activation function. Θ (L,W) denotes the admissible parameter class
d0
Θ (L,W)= (l),b(l) L : (1) RW×d0, (l) RW×W (1<l<L),
d0 {W }l=1 W ∈ W ∈
n
(L) R1×W;b(l) RW (1 l <L), b(L) R .
W ∈ ∈ ≤ ∈
o
In this section, we take the activation function to be the ReLU function σ(x)=max 0,x .
{ }
Note the complexity of the neural network is greatly reduced due to the sparse graphical structure.
We show that
Theorem 5.1. Given a s-sparse graph G=(V,E) with ambient dimension r. Consider approximating
the target distribution p P (4.5) usingthe neural network surrogatep P (5.1). Then there exists
θ NN
∈ ∈
constants C ,C depending only on the local dimension d , such that if we take the hyperparameters
1 2 loc
L=C L logL and W =C W logW for the NN class P , then p P s.t.
1 0 0 2 0 0 NN θ NN
∃ ∈
maxW (p ,p ) CRL−2/dlocW−2/dloc.
j∈V 1 j θ,j ≤ 0 0
Here C is a constant depending on s,r,m,M and the local dimension d , thus independent of the total
loc
dimension d.
Proofs can be found in Appendix D. Similar to Theorem 4.1, the above theorem shows in the ap-
proximationperspective that learning the localizeddistribution does notsuffer fromthe curse of dimen-
sionality.
6 Proof on the gradient estimate of Stein equation
In this section, we illustrate the main steps to obtain gradient estimate of the Stein equation, which is
the core of Proposition 2.2 and Proposition 2.7.
6.1 Proof of Proposition 2.2
Recall the Stein equation for the distribution π, index i and 1-Lipschitz function φ is
∆u(x)+ logπ(x) u(x)=φ(x ).
i
∇ ·∇
The existence of its solution is standard in elliptic theory. We will use an explicit solution by Dynkin’s
formula to derive the gradient estimate. Here the main technical tool is the stochastic analysis, which
cancoverallthe three assumptionsinthis paper. This extends ourpreviousmethod in[8]whichmainly
uses the PDE method.
11Let Xx =Xx(ω) be the pathwise solution of the overdamped Langevin dynamics
t t
dXx(ω)= logπ(Xx(ω))dt+√2dW (ω), Xx(ω)=x. (6.1)
t ∇ t t 0
By Dynkin’s formula [18],
T T
Eu(Xx) u(x)=E ( logπ(Xx) u(Xx)+∆u(Xx))dt= Eφ(Xx )dt. (6.2)
T − ∇ t ·∇ t t t,i
Z0 Z0
Under any condition in Proposition 2.2, the distribution is strongly log-concave (see Remarks 2.3), and
it is well-known that Law(Xx) convergesto the equilibrium π exponentially. This implies that the limit
t
T exists for both sides in (6.2) (note φ(x )π(x)dx =0), and the limit is
i
→∞
R ∞
u(x)π(x)dx u(x)= Eφ(Xx )dt.
− t,i
Z Z0
Taking derivative w.r.t x gives
j
∞
u(x)= E Xx φ(Xx ) dt.
∇j − ∇j t,i·∇ t,i
Z0
(cid:2) (cid:3)
Here Xx is the sample path partialderivative w.r.t x andnote ω is fixed. Note taking derivative on
∇j t,i j
both sides is valid due to the exponential decay of Xx . Since φ is 1-Lipschitz, we obtain
∇j t,i
∞ ∞
u(x) E Xx φ(Xx ) dt E Xx dt. (6.3)
|∇j |≤ k∇j t,ik ∇ t,i ≤ k∇j t,ik
Z0 Z0
(cid:2) (cid:12) (cid:12)(cid:3)
So that it remains to control Xx . Taking der(cid:12)ivative w.(cid:12)r.t. x in (6.1) with ω fixed, we get
∇j t,i
d Xx = H Xxdt, H := 2logπ(Xx). (6.4)
∇ t − t ·∇ t t −∇ t
where we denote for brevity Xx := Xx(ω) Rd×d. We then apply different assumptions in Propo-
∇ t ∇x t ∈
sition 2.2 to get the gradient estimate.
ℓ 1-condition Consider the partial derivative ∇jX tx ∈Rd×dj in (6.4)
d Xx = H Xxdt.
∇j t − t ·∇j t
We aim to control the 2-norm of the subblock ∇jX tx
,k ∈
Rdk×dj (k = 1,...b). Consider fixing a test
vector v
j
∈Rdj s.t. |v
j
|=1, and denote g tx(k,j)= ∇jX tx ,k·v
j
∈Rdk, then
b b
d d
gx(k,j)= Xx v = H Xxv = H (k,l) Xx v = H (k,l)gx(l,j).
dt t dt∇j t,k j − t ·∇j t j − t ∇j t,l j − t t
l=1 l=1
X X
where we denote H t(k,l) Rdk×dl as the (k,l)-th subblock of H t. Then by assumption,
∈
b
1 d
gx(k,j)2 = (gx(k,j))TH (k,l)gx(l,j)
2dt| t | − t t t
l=1
X
M gx(k,j)2+ M gx(k,j) gx(l,j) .
≤ − kk | t | kl | t || t |
l6=k
X
1 d d
Notice gx(k,j)2 = gx(k,j) gx(k,j), we obtain
2dt| t | | t |dt| t |
d
gx(k,j) M gx(k,j) + M gx(l,j) .
dt| t |≤− kk | t | kl | t |
l6=k
X
Such inequality holds for any index j,k [b], test vector v and initial condition x. For different indices
j
j [b], consider different initial conditio∈ ns x(j), and denote the matrix G Rb×b where
t
∈ ∈
G (k,j)=
gx(j)
(k,j).
t | t |
12Then the above inequality can be written compactly in a matrix form
d
G M˜G , M˜ :=2M 1 M .
t t ij ii i=j ij
dt ≤− −
Here is in the entrywise sense. Note the initial condition is G =I , since
0 b
≤
G (k,j)= gx(j) (k,j) = Xx(j) v = x(j)v =δ v =δ .
0 | 0 | |∇x j(j) 0,k j | |∇x j(j) k j | jk | j | jk
By assumption, i,j [b],i=j, M˜ = M 0, and
ij ij
∀ ∈ 6 − ≤
M˜ +c= M +c M =M˜ .
ij ij ii ii
| | ≤
j6=i j6=i
X X
Thus we can apply Lemma A.1 and obtain
G e−ct G =e−ct max Xx(j) v e−ct.
k t k∞ ≤ k 0 k∞ ⇒ k |∇x j(j) t,k j |≤
j
X
Since v is arbitrary, we obtain that
j
max Xx(j) e−ct.
k k∇x j(j) t,k k≤
j
X
Recall (6.3), this implies
∞
u(x(j)) E Xx(j) dt
j
|∇j |≤
j Z0
k∇x j(j) t,i k
X X
∞ ∞
= E Xx(j) dt E e−ctdt=c−1.
Z0 j
k∇x j(j) t,i k ≤
Z0
X
Now as x(j) is arbitrary,we obtain the gradient estimate
u = u c−1.
k∇ k∞,1 k∇j kL∞ ≤
j
X
ℓ -condition Consider Xx in (6.4). Denote G =emt Xx and H˜ =H mI, then it holds that
2 ∇ t t ∇ t t t −
d
G =emt(m Xx H Xx)= H˜ G , G = Xx =I . (6.5)
dt t ∇ t − t ∇ t − t t 0 ∇ 0 d
By assumption, 0 H˜ (M m)I, and H˜ has bandwidth B. By Lemma A.2, it holds that
t t
(cid:22) (cid:22) −
∞ tk(M m)k
emt Xx = G (j,i) e−(M−m)t − .
k∇j t,ik k t k≤ k!
k=⌈|Xi−j|/B⌉
Note this estimate holds for anyinitial conditionx. For different indices j [b], considerdifferentinitial
∈
conditions x(j), and take summation over j, we obtain
∞ tk(M m)k
Xx(j) e−mte−(M−m)t −
k∇x j(j) t,i k≤ k!
Xj Xj k=⌈|Xi−j|/B⌉
∞ tk(M m)k ∞ ∞ tk(M m)k
e−mte−(M−m)t − + 2B −
≤ k! k! !
k=0 n=1 k=n
X X X
∞ tk(M m)k
= e−mte−(M−m)t et(M−m)+2B k −
· k! !
k=1
X
∞ tk−1(M m)k−1
= e−mt 1+e−(M−m)t 2Bt(M m) −
· − (k 1)! !
k=1 −
X
= e−mt 1+e−(M−m)t 2Bt(M m)et(M−m) =e−mt(1+2Bt(M m)).
· − −
(cid:16) (cid:17)
13Recall (6.3), this implies
∞ ∞
u(x(j)) E Xx(j) dt=E Xx(j) dt
j
|∇j |≤
j Z0
k∇x j(j) t,i k
Z0 j
k∇x j(j) t,i k
X X X
∞ 1 2B(M m) 2BM
e−mt(1+2Bt(M m))dt= + − .
≤ − m m2 ≤ m2
Z0
Taking supremum over x(j), we obtain the gradient estimate
2BM
u = u .
k∇ k∞,1 k∇j kL∞ ≤ m2
j
X
6.2 Proof of Proposition 2.7
The proof is similar to that of the ℓ -condition case in the above section. For brevity, we only represent
2
the different parts for the sparse graph treatment.
Firstnote(6.3)isstillvalid,andwewilluse(6.5)tocontrol Xx. ForG in(6.5),weneedtomodify
∇ t 1
the proof in Lemma A.2. Follow the notations therein, we can truncate the Dyson series as
n ∞
1 1
G =exp( M) Xˆ [X](1)+exp( M) Xˆ [X](1).
1 q q
− q! − q!
q=0 q=n+1
X X
Consider the off-diagonalentry G (j,k). Take n=d (j,k) 1, then since all the pathconnecting j and
1 G
−
k has length no less than d (j,k)>n, it must hold that
G
1 q n, (MI H ) (MI H ) (j,k)=0 Xˆ [X](1)(j,k)=0,
∀ ≤ ≤ −
t1
··· −
tq
⇒
q
(cid:2) (cid:3)
Therefore,
∞
1
G (j,k)=exp( M) Xˆ [X](1)(j,k).
1 q
− q!
q=n+1
X
Note Xˆ [X](1) Mq by (A.2), we obtain
q op
k k ≤
∞ Mq ∞ Mq
G (j,k) exp( M) =exp( M) .
1
k k≤ − q! − q!
q= Xn+1 q=dXG(j,k)
By the same scaling argument as in the proof in Lemma A.2, we obtain
∞ tq(M m)q
emt Xx = G (j,k) e−(M−m)t − .
k∇j t,kk k t k≤ q!
q=dXG(j,k)
The estimate holds for any initial condition x. For different vertices j V, consider different initial
∈
conditions x(j), and take summation over j V, we obtain
∈
∞ tq(M m)q
Xx(j) e−mte−(M−m)t −
k∇x j(j) t,k k≤ q!
j X∈V j X∈V q=dXG(j,k)
∞ tq(M m)q ∞ tq(M m)q
e−mte−(M−m)t − + −
≤  q! q! 
Xq=0 Xk=1j:0<dXG(j,k)≤q
∞ tq(M m)q 
e−mt+e−Mt sqr − .
≤ q!
q=1
X
14Here we use the sparsity condition Nq v sqr. From (6.3), we obtain
| j\{ }|≤
∞ ∞
u(x(j)) E Xx(j) dt=E Xx(j) dt
j∈V
|∇j |≤
j∈V Z0
k∇x j(j) t,k k
Z0 j∈V
k∇x j(j) t,k k
X X X
∞ ∞ tq(M m)q
e−mt+se−Mt qr − dt
≤ Z0 q=1 q! !
X
∞
1 s m q
= + qr 1 .
m M − M
Xq=1 (cid:16) (cid:17)
By Lemma A.4, it holds that (denote κ= M)
m
1 s m −r−1 m
u(x(j)) + r! 1
j
|∇ |≤ m M M − M
j X∈V (cid:16) (cid:17) (cid:16) (cid:17)
1 sr!κr
= 1+sr!κr(1 κ−1) .
m − ≤ m
(cid:0) (cid:1)
Taking supremum over x(j), we obtain the gradient estimate.
6.3 Proof for Theorem 2.8
TheproofisbasedontheproofofTheorem2.4andProposition2.2. Andweonlyrepresentthe different
part for the multiple block case.
For any index set I [b], denote d = d . Then by definition,
⊂ I i∈I i
P
W (π ,π′)= sup φ (x )(π (x) π′(x))dx .
1 I I 0 I I − I I
φ0∈Lip 1(RdI)Z
Denote φ(x )=φ (x ) φ (x )π′(x )dx , and let u(x) solves the Stein equation
I 0 I 0 I I I
−
R ∆u(x)+ logπ′(x) u(x)=φ(x ).
I
∇ ·∇
Then using the same method as in Theorem 2.4, we obtain
b
φ (x )(π (x) π′(x))dx = u(x) ( logπ′(x) logπ(x))π(x)dx.
0 I I − I I ∇j · ∇j −∇j
Z j=1Z
X
W (π ,π′) u max logπ′ logπ .
⇒ 1 I I ≤k∇ k∞,1· j k∇j −∇j kL1(π)
Itremainstocontrol u forthenewSteinequation. ThemethodisstillthesameasinSection6.1.
k∇ k∞,1
Note here we need to replace (6.3) by
∞ ∞
u(x) E Xx φ(Xx ) dt E Xx dt.
|∇j |≤ k∇j t,Ik ∇ t,I ≤ k∇j t,Ik
Z0 Z0
(cid:2) (cid:12) (cid:12)(cid:3)
Notice Xx Xx , we obtain th(cid:12) at (cid:12)
k∇j t,Ik≤ i∈Ik∇j t,ik
P ∞
u = u(x) E Xx dt δ =δ I .
k∇ k∞,1 |∇j |≤ k∇j t,ik ≤ | |
jX∈[b] jX∈[b] Z0 Xi∈I Xi∈I
This completes the proof.
7 Conclusion
In this paper, we investigate the localized structure in high-dimensional distributions, and show how it
can reduce the complexity of approximating and computing the localized distributions. We introduce
the δ-localityconditiontoquantifythedegreeoflocalization,anddevelopaℓ -approximationtheoryto
∞
15getafinercontrolofthe approximationerror. Thistheoryprovidesnewinsightsintothe localizedstruc-
ture in high-dimensional distributions, and establishes a theoretical foundation for localized methods.
It suggests to exploit the localized structure in algorithm design to reduce the complexity of accessing
these distributions. As examples, we apply the ℓ -approximation theory to the localized LIS and lo-
∞
calized score matching method, and show how the localized structure help to reduce the computational
complexity. We also show the surrogateapproximationof the localizeddistribution does not suffer from
the curse of dimensionality.
References
[1] B. Amos, L. Xu, and J. Z. Kolter, Input convex neural networks, in Proceedings of the 34th
InternationalConferenceonMachineLearning,vol.70ofProceedingsofMachineLearningResearch,
PMLR, 06–11 Aug 2017, pp. 146–155.
[2] Y. Chen, X. Cheng, J. Niles-Weed, and J. Weare, Convergence of unadjusted langevin in
high dimensions: Delocalization of bias, arXiv preprint arXiv:2408.13115,(2024).
[3] T.Cui, K.J.H.Law, andY.M.Marzouk,Dimension-independent likelihood-informed MCMC,
J. Comput. Phys., 304 (2016), pp. 109–137.
[4] T. Cui, J. Martin, Y. M. Marzouk, A. Solonen, and A. Spantini, Likelihood-informed
dimension reduction for nonlinear inverse problems, Inverse Problems, 30 (2014), pp. 114015,28.
[5] T. Cui and X. T. Tong, A unified performance analysis of likelihood-informed subspace methods,
Bernoulli, 28 (2022), pp. 2788–2815.
[6] A. Datta, S. Banerjee, A. O. Finley, and A. E. Gelfand, Hierarchical nearest-neighbor
Gaussian process models for large geostatistical datasets, J. Amer. Statist. Assoc., 111 (2016),
pp. 800–812.
[7] F. J. Dyson, The radiation theories of tomonaga, schwinger, and feynman, Physical Review, 75
(1949), p. 486.
[8] R. Flock, S. Liu, Y. Dong, and X. T. Tong, Local mala-within-gibbs for bayesian image
deblurring with total variation prior, arXiv preprint arXiv:2409.09810,(2024).
[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio, Generative adversarial nets, in Advances in Neural Information
Processing Systems, vol. 27, Curran Associates, Inc., 2014.
[10] T. M. Hamill, J. S. Whitaker, and C. Snyder, Distance-dependent filtering of background
error covariance estimates in an ensemble Kalman filter, Monthly Weather Review, 129 (2001),
pp. 2776 – 2790.
[11] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, in Advances in Neural
Information Processing Systems, vol. 33, Curran Associates, Inc., 2020, pp. 6840–6851.
[12] R. A. Horn and C. R. Johnson, Matrix analysis, Cambridge University Press, Cambridge,
second ed., 2013.
[13] P. L. Houtekamer and H. L. Mitchell,A SequentialEnsemble Kalman Filter for Atmospheric
Data Assimilation, Monthly Weather Review, 129 (2001), p. 123.
[14] A. Hyva¨rinen, Estimation of non-normalized statistical models by score matching, JournalofMa-
chine Learning Research, 6 (2005), pp. 695–709.
[15] D.P.KingmaandM.Welling,Auto-encodingvariationalbayes,arXivpreprintarXiv:1312.6114,
(2013).
[16] D.KollerandN.Friedman,Probabilistic graphical models,AdaptiveComputationandMachine
Learning, MIT Press, Cambridge, MA, 2009. Principles and techniques.
[17] M. Morzfeld, X. T. Tong, and Y. M. Marzouk, Localization for MCMC: sampling high-
dimensional posterior distributions with local structure, J. Comput. Phys., 380 (2019), pp. 1–28.
16[18] B. Ø ksendal, Stochastic differential equations, Universitext, Springer-Verlag, Berlin, sixth ed.,
2003. An introduction with applications.
[19] F. Otto and C. Villani, Generalization of an inequality by Talagrand and links with the loga-
rithmic Sobolev inequality, J. Funct. Anal., 173 (2000), pp. 361–400.
[20] M. Peruzzi, S. Banerjee, and A. O. Finley, Highly scalable Bayesian geostatistical modeling
viameshedGaussianprocessesonpartitioneddomains,J.Amer.Statist.Assoc.,117(2022),pp.969–
982.
[21] Y. Song and S. Ermon, Generative modeling by estimating gradients of the data distribution, in
Advances in Neural Information Processing Systems, vol. 32, Curran Associates, Inc., 2019.
[22] C. Stein, Approximate computation of expectations, vol. 7 of Institute of Mathematical Statistics
Lecture Notes—MonographSeries, Institute of Mathematical Statistics, Hayward, CA, 1986.
[23] X. T. Tong, M. Morzfeld, and Y. M. Marzouk, MALA-within-Gibbs samplers for high-
dimensional distributions with sparse conditional structure, SIAM J. Sci. Comput., 42 (2020),
pp. A1765–A1788.
[24] A. W. van der Vaart and J. A. Wellner, Weak convergence and empirical processes, Springer
Series in Statistics, Springer-Verlag,New York, 1996. With applications to statistics.
[25] A.V.Vecchia,Estimationandmodelidentificationforcontinuousspatialprocesses,J.Roy.Statist.
Soc. Ser. B, 50 (1988), pp. 297–312.
[26] C. Villani, Optimal transport, vol. 338 of Grundlehren der mathematischen Wissenschaften [Fun-
damental Principles of Mathematical Sciences], Springer-Verlag,Berlin, 2009. Old and new.
[27] G. Winkler, Image analysis, random fields and Markov chain Monte Carlo methods, vol. 27 of
ApplicationsofMathematics(NewYork),Springer-Verlag,Berlin,seconded.,2003.Amathematical
introduction, With 1 CD-ROM (Windows), Stochastic Modelling and Applied Probability.
[28] Y. Yang, H. Yang, and Y. Xiang, Nearly optimal vc-dimension and pseudo-dimension bounds
for deep neural network derivatives, inAdvances inNeuralInformationProcessingSystems,vol.36,
Curran Associates, Inc., 2023,pp. 21721–21756.
A Lemmas
Lemma A.1. Suppose G Rb×b is a time-dependent nonnegative matrix satisfying
t ∈ +
d
G MG ,
t t
dt ≤−
where is in the entrywise sense, and M Rb×b is δ-diagonal dominant with non-positive off-diagonal
≤ ∈
entries, i.e. i,j [b],i=j,
∀ ∈ 6
M +δ M ; M 0.
ij ii ij
| | ≤ ≤
j6=i
X
Then for any t 0, it holds
≥ G e−δt G .
t ∞ 0 ∞
k k ≤ k k
Proof. Denote Gδ :=eδtG , then
t t
d d
Gδ =eδt G +δG eδt( MG +δG )=( M +δI)Gδ.
dt t dt t t ≤ − t t − t
(cid:18) (cid:19)
Multiple both sides by 1=(1,...,1)T Rb from right,
∈
d
Gδ1 ( M +δI)Gδ1.
dt t ≤ − t
This operator preserves the inequality since it is equivalent to taking summation over row indices. We
claim that
t 0, Gδ1 Gδ1 , (A.1)
∀ ≥ | t |∞ ≤| 0 |∞
17which is a reformulation of the conclusion G G by noticing Gδ Rb and
k t k∞ ≤k 0 k∞ t ∈ +
eδt G = Gδ =max Gδ(i,j)= Gδ1 .
k t k∞ k tk∞ i t | t |∞
j
X
We prove (A.1) by contraction. Suppose (A.1) is false, then s 0 and i s.t.
∃ ≥
d
(Gδ1) >0; (Gδ1) = Gδ1 .
dt s i s i | s |∞
On the other hand, notice by assumption on M,
d
(Gδ1) ( M +δ)(Gδ1) + ( M )(Gδ1)
dt s i ≤ − ii s i − ij s j
j6=i
X
( M +δ)(Gδ1) + ( M )(Gδ1)
≤ − ii s i − ij s i
j6=i
X
= M +δ+ M (Gδ1) 0.
− ii | ij | s i ≤
j6=i
X
 
Contradiction. This proves our result.
Lemma A.2. Let H Rd×d be a sequence of positive definite matrices with uniform block bandwidth
t
∈
B >0 and 2-norm bound M, i.e.
i j >B, H (i,j)=0; 0 H MI .
t t d
∀| − | (cid:22) (cid:22)
Consider the matrix ODE
d
G = H G , G =I .
t t t 0 d
dt −
Then for any t 0, it holds that
≥
∞ (tM)k
G (i,j) exp( tM) .
t
k k≤ − k!
k=⌈|i−j|/B⌉
X
Proof. I. Scaling. Firstnoticeitsufficestoconsiderthecaset=1. Forthegeneralcaset=t ,consider
0
the following scaling argument. Let G be the solution, then G solves
t t0s
d
G = t H G , G =I .
ds
t0s
−
0 t0s t0s 0 d
If the theorem holds for t=1, we obtain that at s=1 (notice t H Mt ),
k
0 t0s
k≤
0
∞ (Mt )k
0
G (i,j) exp( Mt ) .
k
t0
k≤ −
0
k!
k=⌈|Xi−j|/B⌉
Next we consider the case t=1.
II. Dyson series [7]. By variation of constants, we have
t
G =I H G ds.
t d s s
−
Z0
Applying this identity recursively, we obtain
t s
G = I H I H G du ds
t d s d u u
− −
Z0 (cid:18) Z0 (cid:19)
t t s
= I H ds+ H H G duds=
d s s u u
− ···
Z0 Z0 Z0
N−1
= I + ( 1)n H H 1 dt dt
d
−
tn··· t1 t1≤t2≤···≤tn 1
···
n
n=1
Z[0,t]n
X
+( 1)N H H G 1 dt dt .
−
tN
···
t1 t1 t1≤t2≤···≤tN 1
···
N
Z[0,t]N
18For simplicity, denote
n!
X (t)=I , X (t):= H H 1 dt dt , n 1.
0 d n tn tn··· t1 t1≤t2≤···≤tn 1 ··· n ≥
Z[0,t]n
R (t)= H H G 1 dt dt .
N tN
···
t1 t1 t1≤t2≤···≤tN 1
···
N
Z[0,t]N
Notice X (t) is the average of “Hn” in [0,t]. Then
n
N−1 tn
G = ( 1)n X (t)+R (t).
t n N
− n!
n=0
X
Now we prove that lim R (t)=0. First notice G 1, since
N→∞ N k t k2 ≤
d t
GTG = 2GTH G GTG =I GTH G ds I .
dt t t − t t t ⇒ t t d − s s s (cid:22) d
Z0
So that as N ,
→∞
R (t) H H G 1 dt dt
k N k2 ≤ k tNk2···k t1k2k t1k2 t1≤t2≤···≤tN 1 ··· n
Z[0,t]N
MNtN
MN 1 dt dt = 0.
≤
t1≤t2≤···≤tN 1
···
N
N! →
Z[0,t]N
This proves that the Dyson series converges,and
∞ tn
G = ( 1)n X (t).
t n
− n!
n=0
X
III. Representation of polynomials. Denote the matrix process space
n
X =span X (t),n 0 = a X (t):a C .
n k n k
{ ≥ } ∈
nk X=0 o
We define the representation of any polynomial P in X as
n n
P[X](t)= a X (t), if P(x)= a xk.
k k k
k=0 k=0
X X
In Lemma A.3 we show that the representation has an equivalent definition:
a t t1 tn−1
n
P X (t)= (H x I) (H x I) (H x I)dt dt dt .
{ } tn t1 − σ1 t2 − σ2 ··· tn − σn n ··· 2 1
σ X∈SnZ0 Z0 Z0
n
if P(x)=a (x x ).
n k
−
k=1
Y
Here S is the set of all permutations of [n]. We use P X to distinguish different ways of definitions.
n
{ }
But they are in fact equivalent.
IV. Banded matrix approximation. By Taylor expansion,
∞ (M x)n
exp( x)=exp( M) − .
− − n!
n=0
X
Represent the series in X, we obtain (denote Xˆ (x)=(M x)n)
n
−
∞
1
G =exp( M) Xˆ [X](1).
1 n
− n!
n=0
X
19Consider its finite truncation
n ∞
1 1
G =exp( M) Xˆ [X](1)+exp( M) Xˆ [X](1).
1 k k
− k! − k!
k=0 k=n+1
X X
Notice by the alternative representation,
Xˆ [X](1)= (MI H ) (MI H )1 dt dt .
n
−
t1
··· −
tn t1≤t2≤···≤tn 1
···
n
σ
X∈SnZ[0,1]n
Since 0 H MI, it holds that MI H M, and thus
(cid:22)
ti
(cid:22) k −
tikop
≤
Xˆ [X](1) Mnn! 1 dt dt =Mn. (A.2)
k
n kop
≤
t1≤t2≤···≤tn 1
···
n
Z[0,1]n
SinceH hasblockbandwidthB,theblockbandwidthofXˆ [X]isatmostkB. Sothatif i j >nB 0,
t k
Xˆ [X](i,j)=0 if k n, and thus | − | ≥
k
≤
∞
1
G (i,j) = exp( M) Xˆ [X](1)(i,j)
1 k
k k (cid:13) − k! (cid:13)
(cid:13) k= Xn+1 (cid:13)
(cid:13) ∞ (cid:13)
(cid:13) 1 (cid:13)
(cid:13)exp( M) Xˆ [X](1) (cid:13)
k op
≤ − k!k k
k=n+1
X
∞ Mk
exp( M) .
≤ − k!
k=n+1
X
This verifies the case i=j. For i=j, the result follows from G (i,i) G 1.
1 1
6 k k≤k k≤
Lemma A.3. The two representations of polynomials in X are equivalent, i.e.
P[X](t)=P X (t), P P.
{ } ∀ ∈
Proof. We prove by induction. First note n=1 is obvious,
a t
1
(a (x x )) X (t)= (H x I)dt =a (X (t) x I )=(a (x x ))[X](t).
1
−
1
{ } t
t1
−
1 1 1 1
−
1 d 1
−
1
Z0
Now consider n 2. Notice P[X](t),P X (t) can both be viewed as mulitlinear maps on x ,...,x .
1 n
≥ { }
Compute the partial derivative w.r.t. x ,
n
(P X (t))
∇xn
{ }
a n t tk−1 tn−1
n
= (H x I) (H x I) (H x I)dt dt
tn t1 − σ1 ··· ∇xn tk − n ··· tn − σn n ··· 1
Xk=1σ∈S Xn,σk=nZ0 Z0 Z0
a n t tk−1 tn−1
n
= (H x I) I (H x I)dt dt
− tn t1 − σ1 ··· ··· tn − σn n ··· 1
k X=1σ∈S Xn,σk=nZ0 Z0 Z0
a n t tn−1
= n (H x I) (t t ) (H x I)dt dtˆ dt
− tn t1 − σ1 ··· k−1 − k+1 ··· tn − σn n ··· k ··· 1
k X=1σ∈S Xn,σk=nZ0 Z0
a n t tn−1
n
= (H x I) (t t ) (H x I)dt dt
− tn t1 − σ1 ··· k−1 − k ··· tn−1 − σn−1 n−1 ··· 1
k X=1σ∈ XSn−1Z0 Z0
a t tn−1
n
= t (H x I) (H x I)dt dt
− tn · t1 − σ1 ··· tn−1 − σn−1 n−1 ··· 1
σ∈ XSn−1Z0 Z0
n−1 n−1
= a (x x ) X (t)= a (x x ) [X](t).
n k n k
− − !{ } − − !
k=1 k=1
Y Y
20Herethefirstequalityfollowsbydiscussiononthepositionofx . Thethirdequalityfollowsbyintegrating
n
the variable t and notice the constraint t t t . The forth equality follows by relabeling the
k k+1 k k−1
≤ ≤
index. The last equality follows by induction hypothesis. By symmetry, the relation holds for any x :
i
(P X (t))= a (x x ) [X](t)=( P)[X](t),
∇xi
{ } −
n
−
j

∇xi
j6=i
Y
 
since P(x) = a (x x ). Now notice the representation P[X] is linear in the coefficients of
∇xi − n j6=i − j
P, it is direct to verify
Q
(P[X](t))=( P)[X](t)= (P X (t)).
∇xi ∇xi ∇xi
{ }
Finally notice when x = =x =0,
1 n
···
n! t t1 tn−1
xn X (t)= H H H dt dt =X (t)=xn X (t).
{ } tn t1 t2··· tn 1 ··· n n { }
Z0 Z0 Z0
Now the two multi-linear maps agree on one point, and also all the partial derivatives. So that they
must be identical. This shows the equivalence holds for n, and by induction, it holds for all n.
Lemma A.4. For any t 0 and x (0,1), it holds that
≥ ∈
kt(1 x)k <2Γ(t+1)x−t−1(1 x).
− −
k≥1
X
When t N, the factor 2 can be omitted, i.e. kt(1 x)k t!x−t−1(1 x).
∈ k≥1 − ≤ −
Proof. We prove by induction. For t=0, it hoPlds that
1 x
(1 x)k = − .
− x
k≥1
X
For t (0,1), first notice by Abel transformation,
∈
kt(1 x)k =x−1 kt (1 x)k (1 x)k+1 =x−1 kt (k 1)t (1 x)k.
− − − − − − −
k≥1 k≥1 k≥1
X X (cid:2) (cid:3) X(cid:0) (cid:1)
Therefore, since kt (k 1)t t(k 1)t−1 when t (0,1) and k 2,
− − ≤ − ∈ ≥
kt(1 x)k x−1 (1 x)+ t(k 1)t−1(1 x)k
− ≤  − − − 
k≥1 k≥2
X X
 
x−1(1 x) 1+t (k 1)t−1e−(k−1)x
≤ −  − 
k≥2
X
 ∞ 
x−1(1 x) 1+t yt−1e−yxdy
≤ −
(cid:20) Z0 (cid:21)
= x−1(1 x) 1+tΓ(t)x−t <2Γ(t+1)x−t−1(1 x).
− −
Here we use tΓ(t) = Γ(t+1) and
Γ(t+1)x−(cid:0)t
> 1 in
the(cid:1)
last step. This verifies the case t [0,1).
∈
Suppose the inequality holds for t 1 0, then using the same methods,
− ≥
kt(1 x)k = x−1 kt (k 1)t (1 x)k x−1 tkt−1(1 x)k
− − − − ≤ −
k≥1 k≥1 k≥1
X X(cid:0) (cid:1) X
< x−1t 2Γ(t)x−(t−1)−1(1 x)=2Γ(t+1)x−t−1(1 x).
· − −
Here the first inequality follows by the elementary inequality kt (k 1)t tkt−1 when t 1, and
the second inequality follows by induction hypothesis. The finer− inequ− ality f≤ or t N can be≥ obtained
∈
similarly. This completes the proof.
21B Proofs for Theorem 3.2
By Assumption 3.1 and Proposition 2.7, denote δ =
sr!κr
,
m
maxW (π ,π ) δ max logπ logπ .
j∈V 1 j r,j ≤ · j∈V k∇j −∇j r kL1(πr)
Denote x:=C−1/2x˜. By the chain rule,
∂x˜
= k ˜ = C1/2(j,k)˜ .
j k k
∇ ∂x ·∇ ∇
j
k∈V k∈V
X X
So that by Lemma B.1 (see below),
max logπ(x) logπ (x)π (x)dx
j j r r
j |∇ −∇ |
Z
= max C1/2(j,k) ˜ logπ˜(x˜) ˜ logπ˜ (x˜) π˜ (x˜)dx˜
k k r r
j | ∇ −∇ |
Z k X∈V (cid:16) (cid:17)
max C1/2(j,k) ˜ logπ˜(x˜) ˜ logπ˜ (x˜)π˜ (x˜)dx˜
k k r r
≤ j k k|∇ −∇ |
Z k∈V
X
m−1/2sr!κr max ˜ logπ˜(x˜) ˜ logπ˜ (x˜)π˜ (x˜)dx˜.
k k r r
≤ · k |∇ −∇ |
Z
So that one obtain (denote C =m−3/2(sr!κr)2)
π
maxW (π ,π ) C max ˜ logπ˜(x˜) ˜ logπ˜ (x˜)π˜ (x˜)dx˜. (B.1)
1 j r,j π k k r r
j ≤ k |∇ −∇ |
Z
Next we control the right hand side of (B.1). Note π˜ ˜lπ˜ and π˜ ˜l π˜ , it holds that
0 r r 0
∝ ∝
˜ logπ˜(x˜) ˜ logπ˜ (x˜)= ˜ log˜l(x˜) ˜ log˜l (x˜ ),
k k r k k r r
∇ −∇ ∇ −∇
Due to the orthogonality of x˜ and x˜ , one can decompose
r ⊥
˜ log˜l(x˜) ˜ log˜l (x˜ )2 = ˜ log˜l(x˜)2+ ˜ log˜l(x˜) ˜ log˜l (x˜ )2,
k k r r ⊥,k r,k r,k r r
|∇ −∇ | |∇ | |∇ −∇ |
where we denote ˜ = , ˜ = and use ˜ log˜l (x˜ ) = 0 as ˜l (x˜ ) does not depend on
∇r,k ∇x˜r,k ∇⊥,k ∇x˜⊥,k ∇⊥,k r r r r
x˜ . Therefore,
⊥
˜ logπ˜(x˜) ˜ logπ˜ (x˜)2π˜ (x˜)dx˜= ˜ log˜l(x˜)2π˜ (x˜)dx˜
k k r r ⊥,k r
|∇ −∇ | |∇ |
Z Z (B.2)
+ ˜ log˜l(x˜) ˜ log˜l (x˜ )2π˜ (x˜)dx˜=: I+II.
r,k r,k r r r
|∇ −∇ |
Z
The first term is straightforwardto control. Notice
I= tr ˜ log˜l(x˜)(˜ log˜l(x˜))T π˜ (x˜)dx˜
⊥,k ⊥,k r
∇ ∇
Z h i
= tr P˜ ˜ log˜l(x˜)(˜ log˜l(x˜))TP˜ π˜ (x˜)dx˜ (B.3)
⊥,k k k ⊥,k r
∇ ∇
(cid:20)Z (cid:21)
= tr P˜ P˜ = ( , ).
⊥,k k ⊥,k k r,k
G R G X
h i
For the second term, first notice π˜ caen be decompoesedeas
r
π˜ (x˜)=Z˜−1˜l (x˜ )π˜ (x˜)=Z˜−1˜l (x˜ )π˜ (x˜ )π˜ (x˜ )=π˜ (x˜ )π˜ (x˜ ),
r r r r 0 r r r 0 r 0 ⊥ r r 0 ⊥
where π˜ (x˜ )=Z˜−1˜l (x˜ )π˜ (x˜ ) denotes the marginal distribution of π˜ on x˜ . So that
r r r r r 0 r r r
II= ˜ log˜l(x˜ +x˜ ) ˜ log˜l (x˜ )2π˜ (x˜ )dx˜ π˜ (x˜ )dx˜ .
r,k r ⊥ r,k r r 0 ⊥ ⊥ r r r
|∇ −∇ |
Z (cid:18)Z (cid:19)
22Take ˜ in (3.4), we have
r,k
∇
˜ log˜l (x˜ )= ˜ log˜l(x˜ +x˜ )π˜ (x˜ )dx˜ .
r,k r r r,k r ⊥ 0 ⊥ ⊥
∇ ∇
Z
So that ˜ log˜l (x˜ ) is the average of ˜ log˜l(x˜ +x˜ ) with respect to π˜ (x˜ ). Therefore, we can
r,k r r r,k r ⊥ 0 ⊥
∇ ∇
apply the Poincar´e inequality for the function ˜ log˜l(x˜) with reference measure π˜ (x˜ ) (which is a
r,k 0 ⊥
∇
standard Gaussian) to obtain
˜ log˜l(x˜ +x˜ ) ˜ log˜l (x˜ )2π˜ (x˜ )dx˜
r,k r ⊥ r,k r r 0 ⊥ ⊥
|∇ −∇ |
Z
˜ ˜ log˜l(x˜) 2π˜ (x˜ )dx˜ = ˜ ˜ log˜l(x˜) 2π˜ (x˜ )dx˜ .
≤ k∇⊥ ∇r,k kF 0 ⊥ ⊥ k∇⊥,j ∇r,k kF 0 ⊥ ⊥
Z Z j∈V
X
Note one can relax
˜ ˜ log˜l(x˜) 2 = tr ˜ ˜ log˜l(x˜) ˜ ˜ log˜l(x˜)
k∇⊥,j ∇r,k kF ∇⊥,j ∇r,k ·∇r,k ∇⊥,j
h i
= tr P˜ ˜ ˜ log˜l(x˜) P˜ ˜ ˜ log˜l(x˜)P˜
⊥,j j k r,k k j ⊥,j
∇ ∇ · ·∇ ∇
h i
tr P˜ ˜ ˜ log˜l(x˜) ˜ ˜ log˜l(x˜)P˜ .
⊥,j j k k j ⊥,j
≤ ∇ ∇ ·∇ ∇
h i
Therefore,
II ˜ ˜ log˜l(x˜) 2π˜ (x˜ )dx˜ π˜ (x˜ )dx˜
≤  k∇⊥,j ∇r,k kF 0 ⊥ ⊥  r r r
Z Z j∈V
X
 
tr P˜ ˜ ˜ log˜l(x˜) ˜ ˜ log˜l(x˜)P˜ π˜ (x˜)dx˜ (B.4)
⊥,j j k k j ⊥,j r
≤ ∇ ∇ ·∇ ∇
Z j X∈V h i
= tr P˜ P˜ = ( , ).
⊥,j j,k ⊥,j j,k r,j
H R H X
j X∈V h i v X∈V
e e e
Combine (B.1)-(B.4), we obtain
1/2
maxW (π ,π ) C max ˜ logπ˜(x˜) ˜ logπ˜ (x˜)2π˜ (x˜)dx˜
1 j r,j π k k r r
j ≤ k |∇ −∇ |
(cid:18)Z (cid:19)
1/2
C max ( , )+ ( , ) .
π k r,k j,k r,j
≤ k R G X R H X 
j∈V
X
 e e e e 
where we use Cauchy’s inequality in the first step.
B.1 Lemma
Lemma B.1. Under Assumption 3.1, it holds that (denote κ= M)
m
max C1/2(j,k) m−1/2sr!κr. (B.5)
j∈V k k≤
k∈V
X
Proof. By Taylor expansion,
∞
(2q 1)!! x q
x−1/2 =M−1/2 1+ − 1 , x [m,M].
(2q)!! − M ! ∈
Xq=1 (cid:16) (cid:17)
For j =k, let n=d (j,k) 1, plug in x=C−1 and notice mI C−1 MI, we obtain
G
6 − (cid:22) (cid:22)
∞ (2q 1)!! C−1 q
C1/2 =P (C−1)+M−1/2 − I .
n
(2q)!! − M
q=n+1 (cid:18) (cid:19)
X
23where P P . By assumption, C−1(u,v)=0 if v / N . Since d (j,k)=n+1, it holds
n n u G
∈ ∈
P (C−1)(j,k)=0,
n
as all the path connecting j and k has length no less than d (j,k)>n. Therefore,
G
∞ (2q 1)!! C−1 k
C1/2(j,k) = M−1/2 − I (j,k)
k k (cid:13) (2q)!! − M (cid:13)
(cid:13)q= Xn+1 (cid:18) (cid:19) (cid:13)
(cid:13) (cid:13)∞ (2q 1)!! C−1 q (cid:13) (cid:13)
M−1/2(cid:13) − I . (cid:13)
≤ (2q)!! − M
q=n+1 (cid:13)(cid:18) (cid:19) (cid:13)op
X (cid:13) (cid:13)
(cid:13) (cid:13)
For the coefficients, we bound (cid:13) (cid:13)
q q q
(2q 1)!! 1 1 1 1 1 1 1
− = 1 exp = exp
(2q)!! 2 − 2j ≤ 2 −2j 2 −2 j
j=2(cid:18) (cid:19) j=2 (cid:18) (cid:19) j=2
Y Y X
1 1 q dx 1 1 1 
exp = exp logq = q−1/2.
≤ 2 −2 x 2 −2 2
(cid:18) Z1 (cid:19) (cid:18) (cid:19)
Since mI C−1 MI, it holds that
(cid:22) (cid:22)
C−1 q m q C−1 q
0 I 1 I I (1 κ−1)q.
(cid:22) − M (cid:22) − M ⇒ − M ≤ −
(cid:18) (cid:19) (cid:16) (cid:17) (cid:13) (cid:13)(cid:18) (cid:19) (cid:13) (cid:13)op
For diagonal entry, C1/2(j,j) C1/2 m−1/2, so(cid:13)that (cid:13)
(cid:13) (cid:13)
k k≤ ≤
C1/2(j,k) =(cid:13) C1/2(cid:13)(j,j) + C1/2(j,k)
(cid:13) (cid:13)
k k k k k k
k∈V k6=j
X X
∞
1
m−1/2+M−1/2 q−1/2(1 κ−1)q
≤ 2 −
Xk6=jq=dXG(j,k)
∞
1
m−1/2+ M−1/2 sqr−1/2(1 κ−1)q
≤ 2 −
q=1
X
1
m−1/2+sM−1/2Γ r+ κr+ 21 (1 κ−1)
≤ 2 −
(cid:18) (cid:19)
1
= m−1/2 1+sΓ r+ κr(1 κ−1) m−1/2sr!κr.
2 − ≤
(cid:20) (cid:18) (cid:19) (cid:21)
Here the secondinequality followsthe sparsityassumption,the thirdinequality followsLemmaA.4, and
the last inequality follows from Γ r+ 1 r! and sr!κr−1 1. This completes the proof.
2 ≤ ≥
(cid:0) (cid:1)
C Proof of Theorem 4.1
By Proposition 2.7, p P is δ-localized with δ = sr!κr , and
θ ∈ m
maxW (p ,pˆ ) δ max logp logpˆ
v∈V 1 j j ≤ · j∈V k∇j −∇j kL1(p)
1/2 (C.1)
δ maxE s(X) s (X)2 =δ J1/2(θˆ ).
≤ · j∈V X∼p | − θˆ ∗,v | · loc ∗
(cid:18) (cid:19)
Using optimality of θˆ and J (θ )=0,
∗ loc ∗
J (θˆ )= J (θˆ ) Jˆ (θˆ ) + Jˆ (θˆ ) Jˆ (θ ) + Jˆ (θ ) J (θ )
loc ∗ loc ∗ loc ∗ loc ∗ loc ∗ loc ∗ loc ∗
− − −
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
J (θˆ ) Jˆ (θˆ ) +0+ Jˆ (θ ) J (θ ) 2max Jˆ (θ) J (θ)
loc ∗ loc ∗ loc ∗ loc ∗ loc loc
≤ − − ≤ θ∈Θ | − |
(cid:16) (cid:17) (cid:16) (cid:17)
= 2max max max Jˆ(θ)+λ max max(J (θ)+λ )
θ∈Θ (cid:12) (cid:12)λj≤Cj j∈V j j − λj≤Cj j∈V j j (cid:12)
(cid:12)
(cid:12) ∀j∈V (cid:16) (cid:17) ∀j∈V (cid:12)
2max(cid:12) (cid:12)max Jˆ j(θ) J j(θ). (cid:12) (cid:12)
≤ θ∈Θ (cid:12)j∈V | − | (cid:12)
24Given data X ,...,X , denote for simplicity
1 N
N
1
P f = f(X ), f (x)=2tr( s (x))+ s (x)2.
N i θ,j j θ,j θ,j
N ∇ | |
i=1
X
Then Jˆ(θ) J (θ)=(P E )f . For any θ,φ Θ, it holds that
j j N p θ,j
− − ∈
f f 2 ∆ logp ∆ logp + logp 2 logp 2
θ,j φ,j jj θ jj φ j θ j φ
| − |≤ | − | |∇ | −|∇ |
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
2 ∆ (u u ) + (u +u ) (u u )
≤ (cid:12) jj θ,k − φ,k (cid:12) (cid:12)∇j θ,k φ,k (cid:12)(cid:12)∇j θ,k − φ,k (cid:12)
(cid:12)
(cid:12)
k X∈Nj (cid:12)
(cid:12)
(cid:12)
(cid:12)
k X∈Nj (cid:12) (cid:12)(cid:12)
(cid:12)
k X∈Nj (cid:12)
(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12)
2(cid:12) ∆ (u u )(cid:12)+ (cid:12) (u +u )(cid:12)(cid:12) (u u )(cid:12)
≤ (cid:12) | jj θ,k − φ,k |(cid:12) (cid:12) |∇j θ,k φ,k |(cid:12)·(cid:12) |∇j θ,k − φ,k(cid:12)|
k X∈Nj k X∈Nj k X∈Nj
2 [∆ (u u ) +(s+1)R (u u )]
jj θ,k φ,k j θ,k φ,k
≤ | − | ·|∇ − |
k X∈Nj
2(s+1)R u u .
≤ k θ,k − φ,k kC2
k X∈Nj
where we use N s+1 and u (x) R.
j j θ,k
| |≤ |∇ |≤
Consider the function space
:= u =(u ) :u .
θ θ θ,j j∈V θ,j θ,j
U { ∈U }
Equip with norm
θ
U
u :=max u .
k θ kU j∈V k θ,j kC2
By definition, sup u R. Consider a ε-net = u Nε of with respect to , i.e.
uθ∈Uθk θ kU ≤ Uε { i }n=1 ⊂Uθ Uθ k·kU
u , there exists some u s.t. u u ε. Then
∀ θ ∈Uθ i ∈Uε k θ − i kU ≤
f f 2(s+1)R u u 2(s+1)2Rε.
k θ,j − θi,j kL∞ ≤ k θ,k − θi,k kC2 ≤
k X∈Nj
Here we take θ to be the parameter corresponding to u . Therefore,
i i
Jˆ(θ) J (θ) = (P E )f (P E )f + (P E )(f f )
|
j
−
j
| |
N
−
p θ,j
|≤|
N
−
p θi,j
| |
N
−
p θ,j
−
θi,j
|
(P E )f +2(s+1)2Rε.
≤ |
N
−
p θi,j
|
This further implies that
maxmax Jˆ(θ) J (θ) max max (P E )f +2(s+1)2Rε.
θ∈Θ j∈V |
j
−
j
|≤1≤i≤Nε j∈V |
N
−
p θi,j
|
Next we give a bound on N . It is a classical result (see for instance [24]) that the covering number of
ε
the unit ball B 1(C2(IdNj)):= {u ∈C2(IdNj): ku
kC2
≤1
}
is bounded by
logN(ε,B 1(C2(IdNj)),L∞) ≤C dNjε−dNj/2.
Here
θ
is the product space of C2(IdNj) equipped with the ℓ ∞-norm, so that
U
logN
ε
≤
logN(ε,B 1(C2(IdNj)),L∞) ≤bC dlocε−dloc/2.
j∈V
X
Recall here d =max d . Notice f is uniformly bounded by
loc j Nj θ,j
f 2(s+1)R u 2(s+1)2R.
| θ,j |≤ k θ,k kC2 ≤
k X∈Nj
By Hoeffding’s inequality,
P[(P E )f t] e−Nt2/2(s+1)4R2 , t>0.
|
N
−
p θi,j
|≥ ≤ ∀
25So that by maximal inequality,
P max max (P E )f t N b e−Nt2/2(s+1)4R2 .
(cid:20)1≤i≤Nε j∈V |
N
−
p θi,j
|≥ (cid:21)≤
ε
·
Take t=(s+1)2Rε, then
P maxmax Jˆ(θ) J (θ) 3t P max max (P E )f t N b e−Nε2/2.
(cid:20)θ∈Θ j∈V |
j
−
j
|≥ (cid:21)≤ (cid:20)1≤i≤Nε j∈V |
N
−
p θi,j
|≥ (cid:21)≤
ε
·
Combine the above estimate, we obtain
P J loc(θˆ ∗) ≥6(s+1)2R2ε ≤b2C dlocε−dloc/2e−Nε2/2.
h i
In order to make the right hand side less than some δ >0, one can take
C
ε2 = logδ−1+logb ,
N
(cid:0) (cid:1)
where C is some constant depending only on d . So that with probability at least 1 δ,
loc
−
J (θˆ ) C′N−1/2 logδ−1+logb.
loc ∗
≤
p
The conclusion follows by plugging in the bound (C.1) on max W (p ,pˆ ).
j 1 j j
D Proof for Theorem 5.1
The proof is based on the following approximation result adapted from Theorem 3 in [28]:
Proposition D.1. For any u
∈
W2,∞(Id0) and ku
kW2,∞ ≤
1, there exists a neural network u
θ ∈
Φ (L,W) where L=C L logL and W =C W logW such that
d0 1 0 0 2 0 0
u u C L−2/d0W−2/d0.
k − θ kW1,∞(Id0) ≤ 3 0 0
Here C ,C ,C are constants depending only on d .
1 2 3 0
By Proposition 2.7, for p P , it holds that (denote δ = sr!κr )
θ ∈ NN m
maxW (p ,p ) δ max logp logp .
j∈V 1 j θ,j ≤ · k∈V k∇k −∇k θ kL1(p)
By the sprase graphicalstructure,
logp logp = u (x ) u (x ) .
∇k −∇k θ ∇k j Nj −∇k θ,j Nj
v X∈Nk(cid:0)
(cid:1)
Therefore, since G is s-sparse,
logp logp u u
k∇k −∇k θ kL1(p) ≤ k∇k j −∇k θ,j kL1(p)
v X∈Nk
(s+1)max u u (s+1)max u u .
≤ j∈V k∇k j −∇k θ,j kL∞ ≤ j∈V k j − θ,j kW1,∞
We obtain
maxW (p ,p ) δ(s+1) max u u .
j∈V 1 j θ,j ≤ · j∈V k j − θ,j kW1,∞
ByPropositionD.1,wecantakeL=C L logL andW =C W logW inP ,sothatforanyj V,
1 0 0 2 0 0 NN
∈
there exists u Φ (L,W) s.t. (notice we take u R)
θ,j ∈ dNj k j kW2,∞ ≤
u u C RL−2/dNjW−2/dNj C RL−2/dlocW−2/dloc.
k j − θ,j kW1,∞ ≤ 3 0 0 ≤ 3 0 0
Here d =max d , and the constant C ,C ,C depend only on d . This completes the proof.
loc j∈V Nj 1 2 3 loc
26