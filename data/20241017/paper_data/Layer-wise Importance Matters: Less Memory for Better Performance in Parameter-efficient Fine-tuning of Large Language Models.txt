Layer-wise Importance Matters: Less Memory for Better Performance in
Parameter-efficient Fine-tuning of Large Language Models
KaiYao1,2*,PenleiGao3*,LichunLi2,YuanZhao2,
XiaofengWang3,WeiWang2†,JiankeZhu1†,
1ZhejiangUniversity 2AntGroup 3ClevelandClinicLernerResearchInstitution
jiumo.yk@antgroup.com,gaop@ccf.org
Abstract 0.95
60 Weight Memory LoRA (32 Layers)
Activation Memory LoRA + Random (8 Layers) Parameter-EfficientFine-Tuning(PEFT)methodshavegained
50 Optimizer Memory LoRA + IST (8 Layers)
significantpopularityforadaptingpre-trainedLargeLanguage Gradient Memory 0.90
Models(LLMs)todownstreamtasks,primarilyduetotheir 40 Others
potentialtosignificantlyreducememoryandcomputational 30 24 GB
overheads.However,acommonlimitationinmostPEFTap- 20 0.85
proachesistheirapplicationofauniformarchitecturaldesign
10 acrossalllayers.Thisuniformityinvolvesidenticaltrainable
modulesandignoresthevaryingimportanceofeachlayer, 0 Fine-tuning LoRA LoRA + IST 0.80 0 1,000 2,000 3,000 4,000 5,000
leadingtosub-optimalfine-tuningresults.Toovercomethe Methods Training Steps
abovelimitationandobtainbetterperformance,wedevelop
anovelapproach,Importance-awareSparseTuning(IST),to Figure1:(Left)MemoryconsumptionoftuningaLLaMA
fullyutilizetheinherentsparsityandselectthemostimpor- 7Bmodelwithatokenbatchsizeof1024onasingledevice.
tantsubsetoffulllayerswitheffectivelayer-wiseimportance DetailsrefertoSec..(Right)Incomparisontothevanilla
scoring.TheproposedISTisaversatileandplug-and-play tuningofall32andrandomtuningof8LoRAlayers,IST
techniquecompatiblewithvariousPEFTmethodsthatoperate achievesabettervalidationloss.
onaper-layerbasis.Byleveragingtheestimatedimportance
scores,ISTdynamicallyupdatestheseselectedlayersinPEFT
modules,leadingtoreducedmemorydemands.Wefurtherpro-
videtheoreticalproofofconvergenceandempiricalevidence 2021;Liuetal.2022;Lester,Al-Rfou,andConstant2021),
ofsuperiorperformancetodemonstratetheadvantagesofIST havebeenproposedtoreducethenumberoftrainableparam-
overuniformupdatingstrategies.Extensiveexperimentson
etersinfine-tuningforthedownstreamtasks.However,most
arangeofLLMs,PEFTs,anddownstreamtaskssubstantiate
existingPEFTmethodsemployauniformapproachthatindis-
theeffectivenessofourproposedmethod,showcasingIST’s
criminatelyassignstrainableparameterstoidenticalpositions
capacitytoenhanceexistinglayer-basedPEFTmethods.Our
acrossalllayers,whichcouldbeunnecessary.Thisstrategy
codeisavailableathttps://github.com/Kaiseem/IST
reliesheavilyonhumanheuristicsandoverlookstask-specific
domaingapsandcharacteristics,limitingtheirperformance
Introduction acrossvariousdownstreamtasks.AlthoughsomePEFTmeth-
odshaveimprovedtheefficiencyoffine-tuningLLMs,such
Significant achievements in natural language processing
asdynamicrank(Zhangetal.2023b,2024),theyaretailored
(NLP)havebeenachievedthisyearfromtheuseoflargelan-
specificallyforLoRA-basedmodelsanddonotextendtheir
guagemodels(LLMs)thatarepre-trainedonextensivegen-
benefitstotheadditionallearnablemodule-basedmethods,
eraldatasets(Zhuangetal.2024;Brownetal.2020).These
i.e.,SeriesandParallelconfigurations.Thislimitationcreates
LLMstypicallyrequirefullfine-tuning(FFT)(Howardand
aclearnecessityforamoregeneralizedalgorithmtoenhance
Ruder2018)toadaptthemforspecializeddownstreamtasks,
modelperformanceacrossvariousdomains.
anapproachthatnecessitatesretrainingallmodelparameters.
InspiredbyLISA(Panetal.2024),weempiricallyfound
Nevertheless, as the size of these models and the volume
thattrainingasmallfractionofthefulllayersinPEFTcan
ofdataincrease,FFTbecomesincreasinglycostlyandim-
yieldcomparablypromisingresultstothoseachievedwith
practical.Aimingtoreducethecost,parameter-efficientfine-
FFT. The existing PEFT methods exhibit markedly redun-
tuning(PEFT)methods,involvingadapter-based(Houlsby
dancy in layer updating during the training process, lead-
etal.2019;Wangetal.2022;Leietal.2024;Heetal.2022a),
ing us to investigate the differences among the layers of
reparameterization-based(Huetal.2021;Edalatietal.2022;
varyingimportancefromtheperspectiveoflayer-wisespar-
Liuetal.2024),andprompt-basedmethods(LiandLiang
sity. Motivated by these inherent insights, we propose a
*Theseauthorscontributedequallytothiswork. novelPEFT-compatibleplug-and-playapproach,Importance-
†Correspondingauthors. awareSparseTuning(IST),thatestimatesthetask-specific
4202
tcO
51
]LC.sc[
1v27711.0142:viXra
)BG(
tsoC
yromeM
ssoL
noitadilaVimportancescoreofeachlayerandfine-tunesthemostimpor- Layer-wiseSparseTuning
tantones.AsshowninFigure1,ourmethodsubstantiallylow-
Many previous works have uncovered the phenomenon of
ersmemorydemandsduringtrainingbyreducingthenumber
layer redundancy in pre-trained models, as evidenced by
oflayersthatrequireupdates.Furthermore,byintegrating
methods such as LayerSkip (Elhoushi et al. 2024), Layer-
layer-wise sparsity into our methodology, we enhance the
Drop(Sajjadetal.2023),LayerSharing(Zhangetal.2023a;
convergenceoflayer-basedPEFTmethods,therebyachieving
Lan et al. 2020), and structured pruning (Fan et al. 2021;
improvedperformance.Theexperimentalresultsshowthat
ZhangandHe2020).Thisindicatesthattheimportanceof
ISTconsistentlyimprovesexistinglayer-wisePEFTmeth-
each layer could be different, and not all layers need fine-
odswithoutsacrificingperformanceandinferenceefficiency
tuning. However, selecting the appropriate layers for fine-
acrossawiderangeofmodels.
tuning downstream tasks remains a significant challenge.
Insummary,ourcontributionsareasfollows: Lee et al. (2023) suggests selectively fine-tuning a subset
oflayersdependingonthetypeofdomainshift.Similarly,
• Based on the empirical insight that sparse patterns
Kaplun et al. (2023) deploys a greedy search to find the
markedlyenhancetheconvergenceofPEFTmodels,we
mostsuitablelayersforfine-tuning,demandingconsiderable
propose an importance-aware sparse tuning method that
computational resources and time for initiation. Recently,
prioritizesthemostimportantlayersforupdating,making
layer-wisesparsetrainingforlargelanguagemodelshasbe-
PEFTmemoryefficientandmorepowerful.
comeapopulartopic.Forexample,LISA(Panetal.2024)
• WeprovidetheoreticalproofofconvergencefortheIST randomlyselectsasubsetoflayerstobeoptimizedduring
approachandpresentempiricalevidenceshowingthatit training, leading to promising faster convergence and bet-
outperformstraditionaluniformupdatestrategiesforPEFT. ter performance. LIFT (Zhu et al. 2024) selects one layer
• ExtensiveexperimentsinvariousLLMs,PEFTmethods, to fine-tune LLMs with different selection strategies such
anddownstreamtasksdemonstratetheeffectivenessandca- asfront-to-end,end-to-front,orrandom,obtainingcompa-
pacityofISTtoenhanceexistingPEFTwithoutsacrificing rable performance while reducing the computational load.
performance. Although effective, these methods require substantial stor-
age equivalent to the full model since all parameters are
beingupdated.Furthermore,theseapproachesdonotdeeply
RelatedWork
explorejointusewithPEFTandhaveemployedrelatively
Parameter-efficientFine-tuning simpleselectionstrategies,limitingtheirperformance.Un-
likethesepreviousmethods,wefocusonintegratingexisting
Asmodelsgrowinsizeandcomplexity,pre-trainedLarge layer-based PEFT and propose an importance-aware layer
Language Models (LLMs) have shown impressive perfor- selection strategy that significantly enhances performance
manceacrossarangeofnaturallanguageprocessing(NLP) whileincreasingefficiency.
tasks.However,efficientlyadaptingtheseLLMstospecific
downstream tasks poses increasing challenges. Parameter- Method
efficient fine-tuning (PEFT) addresses this dilemma by
Motivation
fine-tuning a few additional parameters or a subset of
thepre-trainedparameters.TheexistingPEFTapproaches ToshowcasetheexcessivelayerredundancyintrainingPEFT,
can be roughly categorized into three main types: adapter- weconductedempiricalevaluationsontheOPT1.3B(Zhang
based(Houlsbyetal.2019;Wangetal.2022;Leietal.2024; etal.2023c)modelfine-tunedontheWikiText(Merityetal.
Heetal.2022a),reparameterization-based(Huetal.2021; 2016)dataset.Initially,weadoptedLoRAonallmodel’slay-
Edalatietal.2022;Liuetal.2024),andprompt-basedmeth- ersandtraineditonthisdataset.Aftertraining,weemployed
ods (Li and Liang 2021; Liu et al. 2022; Lester, Al-Rfou, agreedyselectionstrategytoremovetheleastorthemost
andConstant2021).Adapter-basedmethodsfocusonadding importantlayersindividuallyaccordingtotheircontribution
extratunableparametersbyintroducingnewlayerswithin tothemodel’sperformance.Ontheonehand,asillustrated
theoriginalmodel.Forexample,SeriesAdapters(Houlsby inFigure2(a),removing50%oftheleastimportantLoRA
etal.2019)incorporatelinearmodulesinasequentialman- layersdidnotsubstantiallyelevateperplexity.Ontheother
ner,whereasParallelAdapters(Heetal.2022a)addlearn- hand,removingthemostimportantLoRAlayersresultedin
ablemodulesinparallelwiththemodel’sexistingsublayers. arapiddeclineinperformance.Thesepreliminaryfindings
Meanwhile,reparameterization-basedmethodsaimtoreduce indicateinherentlayer-wisesparsityduringPEFTtraining,
thetotalnumberoftrainableparametersbyemployinglow- leadingtothephenomenonthatnotalllayersareeffectively
rankrepresentations.LoRA(Huetal.2021),anotablyeffec- trainedwithPEFT.
tiveandpopularmethod,breaksdownthedeltaparameter Thisobservationpromptsustoquestion:whatcausesthe
matrixintotwolower-rankmatrices.Yet,mostcurrentPEFT layer-wisesparsity?Toanswerthisquestion,weutilizedthe
methodsapplyauniformarchitecturalapproachacrossall outcomeofthegreedysearchtorankthelayersaccording
layers,utilizingthesametrainablemodulesforeachlayer. totheircontributiontothemodel’sperformance.Next,we
Inthisstudy,wepresentanovelapproachthatdynamically performedPEFTfine-tuningonthemostandleastimportant
tunesasubsetoffulllayersthroughPEFT,significantlyen- layers. As shown in Figure 2(b), even when only a small
hancingbothtrainingefficiencyandtheperformanceofthe portion of the layers (or merely a single one) are trained
fine-tunedmodels. using LoRA, it is possible to attain comparable results to   / D \ H U V    / D \ H U V
                       
    ) X O O  ) L Q H  W X Q L Q J       ) X O O  ) L Q H  W X Q L Q J
    
  
    
    = H U R  6 K R W     
 5 H P R Y H  W K H  O H D V W  L P S R U W D Q W  O D \ H U V  7 X Q H  W K H  P R V W  L P S R U W D Q W  O D \ H U V
    
    5 H P R Y H  W K H  P R V W  L P S R U W D Q W  O D \ H U V  7 X Q H  W K H  O H D V W  L P S R U W D Q W  O D \ H U V
    
  D   5 H P R Y H  W U D L Q H G  / R 5 $  P R G X O H V  O D \ H U  E \  O D \ H U  J U H H G L O \   E   7 U D L Q  / R 5 $  P R G X O H V  Z L W K L Q  W K H  V H O H F W L Y H  O D \ H U V
Figure2:IllustrationoflayerredundancyinPEFTtrainingontheOPT-1.3B.(a)Agreedyselectionstrategyisemployedto
iterativelyremovethetrainedLoRAmodulesfromthemodel.(b)Specificlayersofthemodelareselectivelyfine-tunedusing
LoRA.Theimportanceoflayersdependsontheircontributiontotheperformance.
thoseobtainedthroughfullfine-tuning(FFT).Thissuggests functionL(θA,θA)aroundθA:
S S¯ S¯
thattheobservedsparsityisnotduetotheunimportantlayers
of the original network. Instead, it implies that the layer- L(θ SA,θ SA ¯)=L(θ SA,θ SA ¯0)
w nai ts ue rasp lla yrs ei mty ero gb is ne grv the rd ouin ghP oE uF tT thi es na en twin oh re kr ’e sn trt ac ih na inra gc pte rori cs eti sc s,
.
+∇ θ SA ¯L(θ SA,θ SA ¯0)⊤(θ SA ¯ −θ SA ¯0) (2)
Furthermore, training more important layers yields better +O((θA)2),
S
outcomes consistently than focusing on the less important
whereθA representsthefixedparametersbeforeanyfine-
ones,emphasizingthebeneficialroleofimportanceinlayer- S¯0
wisesparsity. tuning.Sinceθ SA ¯ doesnotchangeduringfine-tuningprocess,
wecansetθA = θA.Thefirst-ordertermofEq.2canbe
S¯ S¯0
ConvergenceofLayer-wiseSparseTuning eliminatedandwecanhavetheapproximateloss:
Inthefollowing,wewilldemonstratewhylayer-wisesparse L(θA,θA)≈L(θA,θA)∝L(θA). (3)
S S¯ S S¯0 S
tuningisefficientandeffectiveduringfine-tuning.Inparticu-
Thisestimationshowsthatthelossfunctionmainlydepends
lar,wedevelopproofthatifwerandomlyselectasubsetof
ontheupdatesofθA,supportingthedecisiontofocusupdates
fulllayersinthelayer-basedPEFTmethodandonlyupdate S
onthesubsetoffulllayers.
theseselectedparameters,theriskbondofthesubsetscanbe
In Vapnik–Chervonenkis (VC) theory (Devroye et al.
tighterthanupdatingthewholelayers.
1996),theVC-dimensiondenotedasVCdim(H)isamea-
GivenapretrainedLargeLanguageModel(LLM)M=
sureofthesize,i.e.,capacity,complexity,expressivepower,
{m ,m ,...,m }, comprising N layers and param-
1 2 NL L richness, or flexibility, of a class of sets H. For neural
eterized by Θ, alongside a downstream dataset D =
networks,includingLLMs,theVC-dimensiontypicallyin-
{(x ,y )} , full fine-tuning (FFT) this model on the
i i i∈[|D|] creaseswiththenumberoftrainableparameters.Letd be
downstream dataset achieve M → M , ∆ = S
Θ Θ+∆ theVC-dimensionofsubsetH anddbetheVC-dimension
argmin L(Θ+∆,D).PEFTintroducesalearnablemodule S
∆ offullsetH.ByupdatingonlyasubsetofparametersθA,
Awithasignificantlysmallernumberoftrainableparame- S
theeffectiveVC-dimensiond ofthehypothesisclasscor-
ters,denotedasM′ =[M ,A],where|θA|≪|∆|,aiming S
Θ respondingtotheseparametersisreduced,whichleadstoa
toachieveperformancecomparabletothefullyfine-tuned
tightergeneralizationbound:
model M . The empirical loss over the training set D
Θ+∆
isdefinedasL(θA;(x,y)) = 1 (cid:80) ℓ(y ,f(x ;θA)), Lemma0.2 Withaprobabilityatleast1−δoverthechoice
|D| i∈[|D|] i i of a training set of size n, the following bound holds for
where ℓ denotes a suitable loss function, such as cross-
H ⊆H:
entropy. S
ThelearnablemoduleAinmostexistingPEFTmethods |R(H)−Rˆ (H)|≈|R(H )−Rˆ (H )|
n S n S
canberepresentedasA={a 1,a 2,...,a NL}.Accordingto (cid:114)
Cd log(n/d )+log(1/δ)
(4)
thesparsetuningstrategy,thetotallayersofthegivenadapter ≤ S S ,
modulecanbedividedintotwogroups:S andS¯represent n
asetofrandomlyselectedlayersthatareupdatedandaset where R(H ) = E L(θA;x,y) denotes the ex-
ofremaininglayersthatarefrozenduringfine-tuningrespec- S (x,y)∼D S
tively.ThetotalparametervectorθAisthenpartitionedinto pected risk under the data distribution D and Rˆ n(H S) =
θ SA and θ SA ¯. The loss function can conceptually be decom- sn1 p(cid:80) ecin i fi= c1 dL a( tθ aSA se; tx .i C,y i is)d ae cn oo nte ss tath ne
t
rg ee ln ae tera dli tz oat ti ho en mri osk deo ln at nh de
posedasfollows:
data distribution. Since d ≤ d, the generalization bound
S
L(θA;(x,y))=L(θA,θA;(x,y)). (1) becomestighter,implyingthatmodelswithfewerupdating
S S¯
layersgeneralizebetterassumingthesamenumberoftrain-
Corollary0.1 Consider the Taylor expansion of the loss ingsamples.
 W [ H 7 L N L :  Q R  \ W L [ H O S U H 3  W [ H 7 L N L :  Q R  \ W L [ H O S U H 3scoreofeachlayer.Toestimatelayer-wiseimportancemore
Task loss ℒ Task loss ℒ
Update Update accurately,wedynamicallyselectthesubsetsofalllayersfor
Proj. Layer-wise Proj. PEFTresponsesuppressionduringtheimportanceupdating
Importance Score process. Drawing inspiration from reinforcement learning,
Layer Layer
which explores the best structure based on rewards (Zoph
Layer Sample Sample Layer andLe2017;Phametal.2018;Liuetal.2017),wetreatthe
… a subset a subset … layerselectionprocessasamulti-armedbanditproblemand
usereinforcementlearningtoobtaintheimportancescoreof
Layer PEFT Response Layer eachlayer.
Suppression
Layer PEFT Layer Fine-tuningLoop Formally,givenaPEFT-equippedLLM
×0.25 for downstream data fine-tuning, M′ = [M,A] =
Emb. Emb.
Trans. Modules {m ,a }NL,wherem isfrozenanda istrainable,ourgoal
i i i=1 i i
Fine-tuningLoop Importance Updating Loop
istogenerateasubsetSoffulllayerstoupdate,andkeepthe
remainingsetS¯unchanged.Tothisend,wefirstdefinethe
degreeofimportanceasI∈RNL,whichiszero-initialized
Figure 3: Workflow of Importance-aware Sparse Tuning
andupdatedthroughfine-tuningprocesssimultaneously.In
(IST): IST consists of two main loops: a fine-tuning loop,
eachtrainingiteration,wechooseN layerstoupdatebased
whichselectsasubsetoflayersforupdatingPEFTmodules, u
onI.Fort-thstep,theactionpolicyπ fori-thlayerfollows
andanimportanceupdatingloop,whichestimateslayer-wise i
theuniformdistribution:
importancebyassessingtheresponsesuppressionofthese-
lectedPEFTmodules. π ∼U(0,Sigmoid(I )). (6)
i i
Werandomlysampleprobabilityscorep foreachlayer,i.e.,
BasedonEq.3,thegeneralizationerrorofthemodelcan i
p ∼π .ThesubsetS canbedeterminedwiththescorep :
be formally estimated as |R(H)−Rˆ (H)| ≈ |R(H )− i i i
n S
Rˆ n(H S)|. S ={i|p i >p Nu},S¯={i|p i ≤p Nu}, (7)
When θA is updated and θA remains fixed, the model
effectivelyS
reduces the
dimensS¯
ionality of the optimization
where p Nu is the N u largest values in the sampled prob-
abilities. Then, the chosen PEFT modules are updated by
problem. This can potentially lead to a more focused and θ ←∇ L(θA)
efficientparametersearch:
ai,i∈S θA
Importance Updating Loop To update the importance
Corollary0.3 The derivative of L(θA) with respect to θA
S score,wesuppresstheresponseofa tomeasureitscontri-
canbeobtainedas: i
butiontotheresult.Ifa isrelativelyimportant,reducingits
i
∂L(θA)
=
1 (cid:88) ∂ℓ((y i,f(x i;θA)))
. (5)
responsewillsignificantlyincreasetheloss,andviceversa.
∂θ SA |D|
i∈[|D|]
∂θ SA W inge Nsam lp al ye erN s.c Fc oa rn td hi eda jt -e thse sats m{ pS lic1 n, g. ,. w., eS rc eN dc u} c, ee ta hc eh rc eo spn ota ni sn e-
v
The magnitude and direction of this gradient tell us how ofa iforthelayersthatwerenotselected:
sensitive the empirical risk is to changes in θ SA and hence (cid:26) m (oj)+a (oj) ifi∈Sj
guidetheupdatesduringtraining. oj = i i i i c , (8)
i+1 m (oj)+β∗a (oj) otherwise
i i i i
Fromtheaboveanalysis,wefoundthatnoticeablepatterns
ofsparsitycombinedwiththesmoothnessoftheobjective whereβ ∈[0,1]istheresponsesuppressionfactor.Then,for
function,canmarkedlyimprovetherateofconvergence,po- thej-thsampledsetS cj,wecalculatetherewardsaccording
tentiallyleadingtoalinearspeed-up.Toachieveimproved totheirloss:
errorboundsandconvergencerates,thecrucialstrategylies rj =e−Lj − 1 (cid:88)Nc e−Lk. (9)
inselectingthemostimportantlayersofthefullmodelthat N c k=1
areparticularlypertinenttothespecifictask.Thisselection
DuetothesmallercontributionsofPEFTcomparedtothe
processinvolvesidentifyingwhichlayerscontributethemost
originalnetwork,theresponsesuppressionofPEFTmaylead
totask-specificperformance,enablingamorefocusedand
torelativelysmallrewardvalues.Therefore,weemployeda
efficienttrainingregimen.
largeupdatingrateµtoacceleratetheconvergenceofimpor-
tance,ensuringitmatchesthefine-tuningprocess:
Importance-awareSparseTuning
Intheprevioussection,weprovedthatsparsetuningleads (cid:26) I +µ∗r ifi∈Sj
I = i j c , (10)
toabetterconvergencefordownstreamtaskfine-tuning.In i I i otherwise
thissection,wewillintroduceourmethod,Importance-aware
whereµcontrolstheconvergenceofimportance.
SparseTuning(IST),aimingtoenhancetheperformanceof
layer-wisesparsetuningmotivatedbyempiricalobservations. JointTraining WeproposejointlytrainingISTwithPEFT
ISTinvolvestwoloops:thefine-tuningloop,whichselects to avoid the costly greedy search observed in prior stud-
asubsetoffulllayerstoupdatethePEFTmodules,andthe ies(Kaplunetal.2023),asshowninFigure3.Specifically,
importance updating loop, which updates the importance toalignwiththetrainingdynamicsofPEFT,weexecutethePEFT PEFT+IST
Model WeightMem. FullFint-tuning
Series Parallel LoRA Series Parallel LoRA
GPT2-Small 0.4G 3.4G 2.5G 2.8G 2.9G 2.0G 2.1G 2.2G
120M
TinyLLaMA 2.2G 15.9G 9.6G 10.3G 10.5G 8.0G 8.3G 8.5G
1.1B
LLaMA 14G 60G 22G 23G 24G 19G 20G 20G
7B
LLaMA 27G OOM 38G 41G 42G 35G 36G 36G
13B
Table1:ComparisonofmemoryconsumptionforvariousLLMsandPEFTmethods.
importanceupdatingloopeveryT fine-tuningloop.While Settings Toobtainanaccurateestimationofthememory,
c
ourmethodreducesthetimerequiredforthefine-tuningloop werandomlysampledpromptsfromtheAlpaca(Pengetal.
slightly,itintroducesadditionalforwardtimewithintheim- 2023)datasetandrestrictedthemaximumoutputtokenlength
portanceupdatingloop.Consequently,wesetT =10and to 1024. We uniformly employed a mini-batch size of 1
c
N =3tokeepthetrainingtimeefficient. acrossfourLLMs,rangingfrom120Mto13Bparameters,
c
andthreetypesofPEFTmethods.Wepresentedtheoverall
memory consumption, consisting of weight memory, acti-
ExperimentalResults
vation memory, optimizer memory, and gradient memory.
Inthissection,weconductaseriesofexperimentstovalidate Additionally, we separately demonstrated weight memory
theeffectivenessofourproposedIST.WeintegrateISTinto tohighlightthesignificantroleofISTinreducingtraining
the Series Adapter, Parallel Adapter, and LoRA, and then memory.Toisolatetheimpactoftheevaluatedvariables,we
comparethemwiththeiroriginalcounterpartsacrossvarious excludedGPUmemory-savingtechniques,suchasgradient
tasks. checkpointing(Chenetal.2016),offloading(Renetal.2021),
andflashattention(Daoetal.2022).
Baselines We included the following widely used layer-
Results WelistthememoryconsumptionforvariousLLMs
basedfine-tuningmethods.
andPEFTmethodsinTable1.Theoverallresultsshowthat
• FullFine-tuning(HowardandRuder2018)-Allparam- trainingLLMswithourproposedISTstrategycouldsignif-
eters within the pre-trained model are optimized during icantlyreducememoryconsumptioninallthewidelyused
training. LLMscomparedtofullfine-tuningandstandalonePEFTcon-
figurations.CombiningPEFTmodulesandourproposedIST
• SeriesAdapter(Houlsbyetal.2019)-Additionallearn-
couldsavealotoftrainingmemoryincludingactivationmem-
ablemodulesareintroducedintoaspecificsublayerina
ory,optimizermemory,andgradientmemoryonthethree
sequentialmanner.
popularadapters.AsfortheLLaMA7Bmodel,trainingwith
• ParallelAdapter(Heetal.2022a)-Additionallearnable ISTcouldalmostreducetheaverage36%trainingmemory
modulesareintegratedinparallelwithdistinctsublayers for all the PEFT methods. This trend of reduced memory
withinthebackbonemodel. usagewithISTintegrationisconsistentacrossothermodels
aswell.TheseresultshighlighttheeffectivenessofISTinen-
• LoRA(Huetal.2021)-Parameterefficiencyisenhanced
hancingthememoryefficiencyoffine-tuningLLMs,which
bydecomposingthelearnabledeltaparametermatrixinto
makesISTavaluablestrategyindeployingmoreresource-
twolow-rankmatrices.
efficientfine-tuningpractices,especiallyimportantforsce-
For the optimal configuration and placement of PEFT narioswherecomputationalresourcesarealimitingfactor.
methods,weadheretothesettingsestablishedbyHuetal.
(2023).Specifically,SeriesandParallelAdaptersareseam- CommonsenseReasoning
lesslyintegratedintotheMLPlayerswithabottlenecksize
of256.Similarly,LoRAisseamlesslyincorporatedintoboth Settings TovalidatetheeffectivenessofIST,weevaluated
theMulti-headSelf-attentionlayersandtheMLPlayers,with threePEFTmethodsacrossfiveLLMsonthecommonsense
arankof32.AcrossallPEFTmethods,wemaintainthesame reasoningtasks.Specifically,theadaptabilityofPEFTwas
tunableparameterbudgets,adjustingonlythelearningrate. verifiedusingSeries,ParallelAdapter,andLoRAmethods
For IST, we consistently set N u to 25% of the layers for on the LLaMA 7/13B (Touvron et al. 2023) models, and
thefine-tuningloop,N v to50%ofthelayersfortheimpor- theadaptabilityofLLMwastestedonthreemodels:GPT-J
tance updating loop, and β to 0.25. Further details on the 6B (Wang and Komatsuzaki 2021), BLOOMZ 7B (Muen-
experimentalsettingsareavailableintheAppendix. nighoff et al. 2022), and LLaMA3 8B (AI@Meta 2024).
WealsoreportChatGPT’saccuracyobtainedwithgpt-3.5-
MemoryEfficiency turbo API using a zero-shot Chain of Thought (Wei et al.
2022). The commonsense reasoning tasks consisted of 8
WeconductedexperimentsonmaximumGPUmemoryto sub-tasks, each with a predefined training and testing set,
demonstratetheefficiencyofISTintermsofmemoryusage, includingBoolQ(Clarketal.2019),PIQA(Bisketal.2020),
revealingthatitrequireslessmemorycomparedtostandard SIQA (Sap et al. 2019), HellaSwag (Zellers et al. 2019),
PEFTmethods. WinoGrande(Sakaguchietal.2021),ARC(Clarketal.2018)Model PEFT BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg.
ChatGPT - 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0
Series 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8
Series+IST 66.2 78.3 74.9 72.2 75.9 75.8 59.0 72.2 71.8
Parallel 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.2
LLaMA
7B Parallel+IST 68.4 79.1 77.9 70.0 78.9 81.2 62.3 77.6 74.4
LoRA 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7
LoRA+IST 68.7 81.7 77.3 82.7 78.7 80.6 62.4 80.0 76.5
Series 71.8 83.0 79.2 88.1 82.4 82.5 67.3 81.8 79.5
Series+IST 72.9 82.2 81.4 87.9 84.0 82.7 69.1 81.1 80.2
Parallel 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.4
LLaMA
13B Parallel+IST 72.6 86.0 79.2 89.1 83.5 84.8 70.6 82.8 81.1
LoRA 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5
LoRA+IST 71.5 85.0 81.2 89.1 84.2 84.0 70.1 81.8 80.9
LoRA 62.4 68.6 49.5 43.1 57.3 43.4 31.0 46.6 50.2
GPT-J
6B LoRA+IST 63.0 63.2 62.9 35.8 39.1 56.8 39.1 51.2 51.4
LoRA 65.9 75.3 74.5 57.3 72.5 74.6 57.8 73.4 68.9
BLOOMz
7B LoRA+IST 67.0 74.4 74.4 51.4 68.7 77.9 58.9 74.4 68.4
LoRA 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8
LLaMA3
8B LoRA+IST 72.7 88.3 80.5 94.7 84.4 89.8 79.9 86.6 84.6
Table2:AccuracycomparisonofmultipleLLMswithvariousPEFTmethodsoneightcommonsensereasoningdatasets.Results
ofallthebaselinemethodsonGPT-J,BLOOMZandLLaMAaretakenfromHuetal.(2023).
andOBQA(Mihaylovetal.2018).Aligningwiththesetting Method GSM8K AQuA MAWPS SVAMP Avg.
of Hu et al. (2023), we aggregated the training data from ChatGPT 56.4 38.9 87.4 69.9 63.2
all eight tasks to form the training dataset and conducted LoRA 61.0 26.4 91.6 74.4 63.4
evaluationsontheindividualtestingdatasetforeachtask. LoRA+IST 62.8 31.5 89.9 76.3 64.7
Results ThequantitativeresultsinTable2offeracompre- Table3:AccuracycomparisonofLLaMA38Bonfourmath
hensiveviewoftheperformanceimprovementsbroughtby reasoningdatasets.
the proposed IST method across various LLMs and PEFT
configurations.WecanseethatISTconsistentlyshowsan Settings To further demonstrate IST’s scalability on dif-
enhancement in model performance on the commonsense ferenttasks,weconductadditionalfine-tuningexperiments
reasoningtask.AnalyzedfromtheLLaMA7Bmodel,IST onarithmeticreasoning.WeutilizedLoRAtofine-tunethe
shows significant performance gains across multiple tasks LLaMA38Bmodel.Similarly,weincludedtheresultsfrom
comparedtoitsPEFT-onlycounterpartsinallthreePEFTcon- ChatGPT3.5asareference,obtainedusingZero-shotChain-
figurations.Notably,intasksHellaSwagandQBQA,there’s of-Thought(Weietal.2022).Thefine-tuningprocesswas
anoticeableimprovement,demonstratinghowISTcanrefine conducted on the Math10K dataset, comprising math rea-
the model’s response to more complex queries. Moreover, soning samples collected by Hu et al. (2023). Following
theimpactofISTisnotlimitedtoonemodelorconfigura- thecompletionoftraining,weevaluatedthemodel’sperfor-
tion.Forexample,inGPT-J6BandBLOOMZ7B,theIST manceonpredefinedtestsetsfromseveraldatasets,includ-
enhancements lead to better outcomes in almost all tasks ingGSM8K(Cobbeetal.2021),AQuA(Lingetal.2017),
comparedtoLoRAconfigurationswithoutIST.Thisacross- MAWPS(Koncel-Kedziorskietal.2016),andSVAMP(Patel,
the-board improvement underscores IST’s robustness and Bhattamishra,andGoyal2021).
generalapplicability.IST’sabilitytofocusonthemostim-
pactfullayersmakesthefine-tuningprocessnotonlymore Results Table 3 shows the results of the arithmetic rea-
memoryefficientbutalsostrategicallyadaptabletovarious soningtask.TheaccuracyonGSM8KandSVAMPdatasets
reasoningtasks.Thisisparticularlybeneficialinscenarios shows a consistent improvement from ChatGPT to LoRA,
wheremodelresponsivenessandaccuracyarecritical.The and further enhancement when IST is applied alongside
aggregation of training data across different tasks and the LoRA, indicating the effectiveness of fine-tuning and IST
subsequentapplicationofISTlikelyhelpsindevelopinga inimprovingmodelperformanceforthesedatasets.There-
moregeneralizedunderstandingofcommonsensereasoning, sults of the AQuA dataset indicate a decrease in accuracy
makingISTavaluableadditiontothePEFTtechniques. forLoRAcomparedtoChatGPT,buttheapplicationofIST
helpstorecoversomeofthelostperformance.Thissuggests
ArithmeticReasoning that while LoRA alone may not be as effective for AQuA,
ISTcanmitigatesomeissues.Overall,combiningISTandMethod #Layers Results ComparisonwithAdaptiveMethods Wecomparedour
proposedISTmethodwithotheradaptivemethods,suchas
VanillaTuning 32 74.7
LISA(Panetal.2024)andAdaLoRA(Zhangetal.2023b),
RandomSparseTuning 4 67.1(-7.6)
with LLaMa 7B on the commonsense task to demonstrate
Importance-awareSparseTuning 4 73.7(-1.0)
RandomSparseTuning 8 75.8(+1.1) oureffectiveness.Notably,LISAisaPEFTmethodthatfo-
Importance-awareSparseTuning 8 76.5(+1.8) cuses on sparsely tuning a single transformer layer, while
AdaLoRA uses adaptive rank allocation and can widely
Table4:AblationstudiesonkeycomponentsofIST. adapt to reparameterization-based methods. As shown in
theTable5,comparedtoLoRA,LISAimprovedtheaverage
accuracy by 0.6, validating the concept of sparse training.
LoRA AdaLORA AdaLoRA improved accuracy by 1.5, highlighting the im-
Method LoRA LISA AdaLoRA
+IST +IST
portanceofrank-levelsparsity.Finally,ourmethodcanbe
Results 74.7 75.3 76.2 76.5 77.1 combinedwithLoRAandAdaLoRAtofurtherenhanceper-
formance,showcasingthebroadapplicabilityandpracticality
Table5:Comparisonwithotheradaptivemethods. ofIST.
Layer-wiseImportanceLearning Wevisualizethelayer-
wiseimportanceofthetwotaskswithISTinFigure4.The
Commonsense Reasoning
1.0 Layer 2 importancescoresconvergeasthetrainingiterationsincrease.
Layer 4 Theobservedvariationintheimportancescoresofeachlayer
Layer 6
0.5 acrossdifferenttasksindicatesdistinctlevelsofsignificance.
Layer 8
Layer 10 For instance, ‘Layer 2’ and ‘Layer 32’ significantly con-
Layer 12 tributetothecommonsensereasoningtask,whereastheyare
0.0
Layer 14
0 5000 10000 15000 20000 25000 30000 lessimportantforthearithmeticreasoningtask.Conversely,
Layer 16
# Iterations
Layer 18 ‘Layer6’and‘Layer18’exhibitcontrastingimportancelev-
Arithmetic Reasoning
1.0 Layer 20 elsacrossthesetasksaswell.Thislayer-wisedifferentiation
Layer 22
underscorestheeffectivenessofourmethod,similartocur-
Layer 24
0.5 Layer 26 riculumlearning,wherethemodelprogressivelyfocuseson
Layer 28 the most pertinent layers at each stage of training. By dy-
Layer 30
namicallyadjustingtheimportanceofdifferentlayers,our
0.0 Layer 32
0 250 500 750 1000 1250 1500 1750 approach allows for a more refined and task-specific tun-
# Iterations ingprocess,therebyenhancingthemodel’sadaptabilityand
Figure4:Layer-wiseimportanceondifferenttasks. performanceacrossdiversetasks.
Conclusion
existingPEFTmethodspresentsarobustapproachforfine-
tuningLLMs,leadingtobettergeneralizationandaccuracy Inthisstudy,weproposedanovelImportance-awareSparse
inarithmeticreasoningtasks. Tuning(IST)approachforPEFTofLLMs.Bydynamically
selectingthemostimportantlayersinthefine-tuningloop,
ISTachievesasignificantreductioninmemoryusageand
AnalyticalStudy
computationaloverhead.Theimportanceupdatingloopre-
Effect of Importance-aware Sparse Tuning We con- finestheselectionoflayersusingareinforcementlearning
ducted experiments to evaluate the effects of importance- approach,ensuringthatthemostimpactfullayersarepriori-
awaresparsetuningbytrainingtheLLaMA7Bmodelwith tizedduringtraining.Thisinnovativemethodleveragesthe
LoRAonacommonsensetask,reportingtheaverageaccu- inherentsparsityoflayer-wiseimportance,leadingtomore
racy.AsshowninTable4,usingsparsetuningwithrandomly efficientandeffectivefine-tuningthroughextensiveexperi-
selectedlayers,particularlywithonlyfourlayers,doesnot mentsacrossvariousLLMs,PEFTmethods,anddownstream
yieldsatisfactoryresults.Thisoutcomecontrastswithfind- tasks.Theproposedmethodholdspromiseforfutureapplica-
ingsfromLISA(Panetal.2024)andLIFT(Zhuetal.2024), tionswhereresourceconstraintsandperformancearecritical
wheretrainingveryfewlayers(1-2layers)resultedinagood considerations.
performance.Thediscrepancyarisesbecause,inLISAand
LIFT,trainingentiretransformerlayersencompassesasub- Acknowledgements
stantialnumberoftrainableparameters.Conversely,PEFT
This work was supported by Ant Group Postdoctoral Pro-
involvesrelativelyfewerparameters,necessitatingthefine-
gramme.
tuningofmorelayerstoachievebetterresults.Whenwein-
creasedthenumberofsparsetuninglayersto8,weobserved
Limitations
a considerable improvement of 1.1, aligning with theoret-
icalexpectationsthatsparsetuningenhancesconvergence. Therearethreelimitationsinthiswork.First,sinceISTem-
Finally,incorporatingimportance-awaretuningyieldedthe ploysreinforcementlearning,tuningsixrelatedhyperparam-
bestresults,underscoringtheeffectivenessofIST. etersisrequired.Evenafterfixingthreeoftheseparameters,
erocS
ecnatropmI
erocS
ecnatropmIthesearchspacefortheremainingthreeremainslarge,pos- inference and self-speculative decoding. arXiv preprint
siblyleadingtoincreasedtrial-and-errorcostsduringusage. arXiv:2404.16710.
Second,duetolimitedresources,wewereunabletovalidate
Fan,C.;Li,J.;Ao,X.;Wu,F.;Meng,Y.;andSun,X.2021.
larger language models such as the LLaMA3 70B. These
Layer-wise model pruning based on mutual information.
largermodelsexhibitstrongerlanguagecomprehensionca-
arXivpreprintarXiv:2108.12594.
pabilitiesand,consequently,yieldbetterperformance.Third,
wedidnotthoroughlyexplorethevariantsorcombinationsof Fu, C.; Huang, H.; Chen, X.; Tian, Y.; and Zhao, J. 2021.
eachPEFTmethod.Giventhesubstantialcomputationalde- Learn-to-Share: A Hardware-friendly Transfer Learning
mandsandextensivehyperparametersearchspace,weleave FrameworkExploitingComputationandParameterSharing.
thisasfuturework. InMeila,M.;andZhang,T.,eds.,Proceedingsofthe38th
InternationalConferenceonMachineLearning,volume139
References ofProceedingsofMachineLearningResearch,3469–3479.
PMLR.
AI@Meta.2024. Llama3ModelCard.
Han,Z.;Gao,C.;Liu,J.;Zhang,J.;andZhang,S.Q.2024.
Bisk,Y.;Zellers,R.;Bras,R.L.;Gao,J.;andChoi,Y.2020.
Parameter-efficientfine-tuningforlargemodels:Acompre-
PIQA:ReasoningaboutPhysicalCommonsenseinNatural
hensivesurvey. ArXiv,abs/2403.14608.
Language. InThirty-FourthAAAIConferenceonArtificial
Intelligence. He,J.;Zhou,C.;Ma,X.;Berg-Kirkpatrick,T.;andNeubig,G.
2021. Towardsaunifiedviewofparameter-efficienttransfer
Brown,T.;Mann,B.;Ryder,N.;Subbiah,M.;Kaplan,J.D.;
learning. arXivpreprintarXiv:2110.04366.
Dhariwal,P.;Neelakantan,A.;Shyam,P.;Sastry,G.;Askell,
A.; et al. 2020. Language models are few-shot learners. He,J.;Zhou,C.;Ma,X.;Berg-Kirkpatrick,T.;andNeubig,
Advancesinneuralinformationprocessingsystems,33:1877– G. 2022a. Towards a unified view of parameter-efficient
1901. transferlearning. InInternationalConferenceonLearning
Chen,J.;Zhang,A.;Shi,X.;Li,M.;Smola,A.;andYang,D. Representations.
2023. Parameter-EfficientFine-TuningDesignSpaces. arXiv He, S.; Ding, L.; Dong, D.; Zhang, J.; and Tao, D. 2022b.
preprintarXiv:2301.01821. SparseAdapter: An Easy Approach for Improving the
Chen,T.;Xu,B.;Zhang,C.;andGuestrin,C.2016. Train- Parameter-EfficiencyofAdapters. InFindingsoftheAsso-
ing deep nets with sublinear memory cost. arXiv preprint ciationforComputationalLinguistics:EMNLP2022,2184–
arXiv:1604.06174. 2190. Abu Dhabi, United Arab Emirates: Association for
ComputationalLinguistics.
Clark,C.;Lee,K.;Chang,M.-W.;Kwiatkowski,T.;Collins,
M.;andToutanova,K.2019. BoolQ:ExploringtheSurpris- Henderson,J.;Ruder,S.;etal.2021. Compacter:Efficient
ingDifficultyofNaturalYes/NoQuestions. InProceedings low-rankhypercomplexadapterlayers. InAdvancesinNeu-
ofthe2019ConferenceoftheNorthAmericanChapterofthe ralInformationProcessingSystems.
AssociationforComputationalLinguistics:HumanLanguage
Houlsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;
Technologies,Volume1(LongandShortPapers),2924–2936.
DeLaroussilhe,Q.;Gesmundo,A.;Attariyan,M.;andGelly,
Minneapolis,Minnesota:AssociationforComputationalLin-
S.2019. Parameter-efficienttransferlearningforNLP. In
guistics.
Internationalconferenceonmachinelearning,2790–2799.
Clark,P.;Cowhey,I.;Etzioni,O.;Khot,T.;Sabharwal,A.;
Howard,J.;andRuder,S.2018. Universallanguagemodel
Schoenick,C.;andTafjord,O.2018. ThinkyouhaveSolved
fine-tuning for text classification. In Proceedings of the
Question Answering? Try ARC, the AI2 Reasoning Chal-
56thAnnualMeetingoftheAssociationforComputational
lenge. arXiv:1803.05457v1.
Linguistics.
Cobbe,K.;Kosaraju,V.;Bavarian,M.;Hilton,J.;Nakano,R.;
Hesse,C.;andSchulman,J.2021. Trainingverifierstosolve Hu,E.J.;Shen,Y.;Wallis,P.;Allen-Zhu,Z.;Li,Y.;Wang,
mathwordproblems. arXivpreprintarXiv:2110.14168. S.;Wang,L.;andChen,W.2021. Lora:Low-rankadaptation
oflargelanguagemodels. arXivpreprintarXiv:2106.09685.
Dao, T.; Fu, D.; Ermon, S.; Rudra, A.; and Re´, C. 2022.
Flashattention: Fast and memory-efficient exact attention Hu,Z.;Lan,Y.;Wang,L.;Xu,W.;Lim,E.-P.;Lee,R.K.-W.;
withio-awareness. AdvancesinNeuralInformationProcess- Bing,L.;andPoria,S.2023. LLM-Adapters:AnAdapter
ingSystems,35:16344–16359. Family for Parameter-Efficient Fine-Tuning of Large Lan-
guageModels. InEmpiricalMethodsinNaturalLanguage
Devroye, L.; Gyo¨rfi, L.; Lugosi, G.; Devroye, L.; Gyo¨rfi,
Processing.
L.;andLugosi,G.1996. Vapnik-ChervonenkisTheory. A
probabilistictheoryofpatternrecognition,187–213. Kaplun, G.; Gurevich, A.; Swisa, T.; David, M.; Shalev-
Edalati,A.;Tahaei,M.S.;Kobyzev,I.;Nia,V.;Clark,J.J.; Shwartz, S.; and Malach, E. 2023. Less is More: Selec-
andRezagholizadeh,M.2022. KronA:ParameterEfficient tive Layer Finetuning with SubTuning. arXiv preprint
TuningwithKroneckerAdapter. ArXiv,abs/2212.10650. arXiv:2302.06354.
Elhoushi, M.; Shrivastava, A.; Liskovich, D.; Hosmer, B.; Koncel-Kedziorski, R.; Roy, S.; Amini, A.; Kushman, N.;
Wasti, B.; Lai, L.; Mahmoud, A.; Acun, B.; Agarwal, S.; andHajishirzi,H.2016. MAWPS:AMathWordProblem
Roman, A.; et al. 2024. Layer skip: Enabling early exit Repository. InProceedingsofNAACL,1152–1157.Lan,Z.;Chen,M.;Goodman,S.;Gimpel,K.;Sharma,P.;and Patel,A.;Bhattamishra,S.;andGoyal,N.2021. AreNLP
Soricut,R.2020. Albert:Alitebertforself-supervisedlearn- ModelsreallyabletoSolveSimpleMathWordProblems?
ingoflanguagerepresentations. InInternationalConference InProceedingsofNAACL,2080–2094.
onLearningRepresentation.
Peng,B.;Li,C.;He,P.;Galley,M.;andGao,J.2023. Instruc-
Lee,Y.;Chen,A.S.;Tajwar,F.;Kumar,A.;Yao,H.;Liang, tionTuningwithGPT-4. arXivpreprintarXiv:2304.03277.
P.;andFinn,C.2023. Surgicalfine-tuningimprovesadap- Pham, H.; Guan, M. Y.; Zoph, B.; Le, Q. V.; and Dean, J.
tationtodistributionshifts. InInternationalConferenceon 2018. Efficient Neural Architecture Search via Parameter
LearningRepresentation. Sharing. InInternationalConferenceonMachineLearning
Lei, T.; Bai, J.; Brahma, S.; Ainslie, J.; Lee, K.; Zhou, Y.; (ICML),4092–4101.
Du, N.; Zhao, V.; Wu, Y.; Li, B.; et al. 2024. Conditional Ren,J.;Rajbhandari,S.;Aminabadi,R.Y.;Ruwase,O.;Yang,
adapters:Parameter-efficienttransferlearningwithfastinfer- S.; Zhang, M.; Li, D.; and He, Y. 2021. ZeRO-Offload:
ence. AdvancesinNeuralInformationProcessingSystems, DemocratizingBillion-ScaleModelTraining. arXivpreprint
36. arXiv:2101.06840.
Lester,B.;Al-Rfou,R.;andConstant,N.2021. Thepower Sajjad,H.;Dalvi,F.;Durrani,N.;andNakov,P.2023. Onthe
ofscaleforparameter-efficientprompttuning. InEmpirical effectofdroppinglayersofpre-trainedtransformermodels.
MethodsinNaturalLanguageProcessing. ComputerSpeech&Language,77:101429.
Li, X. L.; and Liang, P. 2021. Prefix-tuning: Optimiz- Sakaguchi, K.; Bras, R. L.; Bhagavatula, C.; and Choi, Y.
ing continuous prompts for generation. arXiv preprint 2021. Winogrande:Anadversarialwinogradschemachal-
arXiv:2101.00190. lengeatscale. CommunicationsoftheACM,64(9):99–106.
Ling,W.;Yogatama,D.;Dyer,C.;andBlunsom,P.2017.Pro- Sap, M.; Rashkin, H.; Chen, D.; LeBras, R.; and Choi, Y.
gramInductionbyRationaleGeneration:LearningtoSolve 2019. Socialiqa:Commonsensereasoningaboutsocialinter-
andExplainAlgebraicWordProblems. InProceedingsofthe actions. arXivpreprintarXiv:1904.09728.
55thAnnualMeetingoftheAssociationforComputational
Sung, Y.-L.; Cho, J.; and Bansal, M. 2022. LST: Ladder
Linguistics(Volume1:LongPapers),158–167.
Side-TuningforParameterandMemoryEfficientTransfer
Liu, H.; Simonyan, K.; Vinyals, O.; Fernando, C.; and Learning. ArXiv,abs/2206.06522.
Kavukcuoglu,K.2017. Hierarchicalrepresentationsforeffi-
Touvron,H.;Lavril,T.;Izacard,G.;Martinet,X.;Lachaux,
cientarchitecturesearch. arXivpreprintarXiv:1711.00436.
M.-A.; Lacroix, T.; Rozie`re, B.; Goyal, N.; Hambro, E.;
Liu,S.-Y.;Wang,C.-Y.;Yin,H.;Molchanov,P.;Wang,Y.- Azhar, F.; et al. 2023. Llama: Open and efficient founda-
C.F.;Cheng,K.-T.;andChen,M.-H.2024. DoRA:Weight- tionlanguagemodels. arXivpreprintarXiv:2302.13971.
DecomposedLow-RankAdaptation. InInternationalCon- Wang,B.;andKomatsuzaki,A.2021. GPT-J-6B:A6Billion
ferenceonMachineLearning.
ParameterAutoregressiveLanguageModel. https://github.
Liu, X.; Ji, K.; Fu, Y.; Tam, W. L.; Du, Z.; Yang, Z.; and com/kingoflolz/mesh-transformer-jax.
Tang, J. 2022. P-tuning v2: Prompt tuning can be compa- Wang,Y.;Mukherjee,S.;Liu,X.;Gao,J.;Awadallah,A.H.;
rabletofine-tuninguniversallyacrossscalesandtasks. In andGao,J.2022. Adamix:Mixture-of-adapterforparameter-
Proceedingsofthe60thAnnualMeetingoftheAssociation efficient tuning of large language models. arXiv preprint
ofComputationalLinguistics. arXiv:2205.12410,1(2):4.
Mao,Y.;Mathias,L.;Hou,R.;Almahairi,A.;Ma,H.;Han, Wei,J.;Wang,X.;Schuurmans,D.;Bosma,M.;Xia,F.;Chi,
J.;tauYih,W.;andKhabsa,M.2021. UniPELT:AUnified E.;Le,Q.V.;Zhou,D.;etal.2022.Chain-of-thoughtprompt-
FrameworkforParameter-EfficientLanguageModelTuning. ingelicitsreasoninginlargelanguagemodels. Advancesin
ArXiv,abs/2110.07577. neuralinformationprocessingsystems,35:24824–24837.
Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. Zellers,R.;Holtzman,A.;Bisk,Y.;Farhadi,A.;andChoi,Y.
2016. Pointer sentinel mixture models. arXiv preprint 2019. Hellaswag:Canamachinereallyfinishyoursentence?
arXiv:1609.07843. arXivpreprintarXiv:1905.07830.
Mihaylov,T.;Clark,P.;Khot,T.;andSabharwal,A.2018. Zhang,K.;Ding,N.;Qi,B.;Zhu,X.;Long,X.;andZhou,
Can a suit of armor conduct electricity? a new dataset for B.2023a. CRaSh:Clustering,Removing,andSharingEn-
openbookquestionanswering. hanceFine-tuningwithoutFullLargeLanguageModel. In
Muennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.; EmpiricalMethodsinNaturalLanguageProcessing.
Biderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong, Zhang, M.; and He, Y. 2020. Accelerating training of
Z.-X.; Schoelkopf, H.; et al. 2022. Crosslingual gen- transformer-basedlanguagemodelswithprogressivelayer
eralization through multitask finetuning. arXiv preprint dropping. AdvancesinNeuralInformationProcessingSys-
arXiv:2211.01786. tems,33:14011–14023.
Pan, R.; Liu, X.; Diao, S.; Pi, R.; Zhang, J.; Han, C.; and Zhang,Q.;Chen,M.;Bukharin,A.;He,P.;Cheng,Y.;Chen,
Zhang, T. 2024. LISA: Layerwise Importance Sampling W.; and Zhao, T. 2023b. Adaptive budget allocation for
forMemory-EfficientLargeLanguageModelFine-Tuning. parameter-efficientfine-tuning. InInternationalConference
arXivpreprintarXiv:2403.17919. onLearningRepresentations.Openreview.Zhang,R.;Qiang,R.;Somayajula,S.A.;andXie,P.2024.
AutoLoRA: Automatically Tuning Matrix Ranks in Low-
RankAdaptationBasedonMetaLearning. arXivpreprint
arXiv:2403.09113.
Zhang,S.;Roller,S.;Goyal,N.;Artetxe,M.;Chen,M.;Chen,
S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; et al. 2023c.
Opt:Openpre-trainedtransformerlanguagemodels. URL
https://arxiv.org/abs/2205.01068,3:19–0.
Zhu,L.;Hu,L.;Lin,J.;andHan,S.2024. LIFT:Efficient
Layer-wiseFine-tuningforLargeModelModels.
Zhuang,Y.;Yu,Y.;Wang,K.;Sun,H.;andZhang,C.2024.
Toolqa:Adatasetforllmquestionansweringwithexternal
tools. AdvancesinNeuralInformationProcessingSystems,
36.
Zoph,B.;andLe,Q.V.2017. NeuralArchitectureSearch
withReinforcementLearning. InInternationalConference
onLearningRepresentations(ICLR).Appendix
AdapterLayer
CodeandReproducibility
Series Adapter
Algorithm1:IST,PyTorch-like
Parallel
Adapter
import peft, transformers LoRA Adapter
from ist import IST
Adapter
# initialize the pre-trained model and PEFT Transformer
𝑊
modules 𝑊 !" Modules
𝑊
peft model = get peft model() #$%&
# initialize IST as callback function
ist callback = IST()
# adopt IST in Trainer with one modification Figure 5: Most existing PEFT approaches employ a layer-
trainer = transformers.Trainer(model=peft model, baseddesign,consistentlyaddinglearnablemodulesorpa-
callbacks=[ist callback])) rameterstoeachlayerofthetransformermodules,including
trainer.fit() theMulti-HeadSelf-Attention(MHSA)andFeed-Forward
Network(FFN).
OurcodeisbasedontheLLM-Adapterlibrary1(Huetal.
2023),abenchmarklibraryforparameter-efficientfine-tuning ParallelAdapters. Paralleladaptersfocusonincorporat-
(PEFT).Tofacilitatereproducibility,wehaveincludedthe ing additional learnable modules in parallel with distinct
code,alongwithtrainingscriptsandinstructions,inthesup- sublayerswithinthebackbonemodel.TheParallelAdapter
plementarymaterial.Notably,ourISTmethodisorthogonal canbeformulatedasfollows:
tomostPEFTmethodsandcanbereadilyincorporatedinto
H →H +f(H W )W . (11)
thetrainingprocess.Asdemonstratedin1,ourmethodre- o o i down up
quiresonlyasinglelineofmodificationtothetrainerbased Reparametrization-basedmethod. Thistypeofmethod
ontheHuggingFaceTransformerslibrary2andPeftlibrary3. aims to transform network weights using a low-rank tech-
Pleaserefertothecodeformoredetails. nique. We take LoRA (Hu et al. 2021) as an example of
Reparametrization-basedlearning,whichcanbeformulated
PEFTOverview below:
H =H W +H ∆W =H W +H BA, (12)
o i 0 i i 0 i
M Pre ot mho pd tTuning(Lester,Al-Rfou,andConstant2021)Pro√ √mptReparaSeriesParallel
where H
i
and H
o
are the input and output of a sublayer
Prefix-Tuning(LiandLiang2021) √ module(e.g.,Linear),W 0 ∈Rd×dcanbeanylinearweight
LoRA(Huetal.2021) √ inthepre-trainedLLM,B ∈Rr×dandB ∈Rd×r arelower-
KronA(Edalatietal.2022) √
DoRA(Liuetal.2024) √ rank learnable matrix to approximate ∆W. r ≪ d is the
Adapters(Houlsbyetal.2019) √ pre-definedrankforLoRA.
AdaMix(Wangetal.2022) √
SparseAdapter(Heetal.2022b) √
LeTS(Fuetal.2021) √ Dataset #Train #Test Answer
ParallelAdapter(Heetal.2022a) √
LST(Sung,Cho,andBansal2022) √ √ √ Commonsense 170K - -
MAMAdapter(Heetal.2021) √ √ √ BoolQ 9.4K 3,270 Yes/No
U Con miP pE aL cT te( rM (Ha eo ne dt ea rl s. o2 n0 ,2 R1 u) deretal.2021) √ √√ √ PIQA 16.1K 1,830 Option
S4-model(Chenetal.2023) SIQA 33.4K 1,954 Option
HellaSwag 39.9K 10,042 Option
Table 6: The PEFT methods are categorized based on the
WinoGrande 63.2K 1,267 Option
fourcommonbasicmethods.”Prompt”representsprompt-
ARC-e 1.1K 2,376 Option
basedlearningmethods,”Repara”denotesreparametrization-
ARC-c 2.3K 1,172 Option
basedmethods,”Series”isSeriesAdapters,and”Parallel”
OBQA 5.0K 500 Option
representsParallelAdapters.
Math10K 10K - -
AccordingtoHuetal.(2023)andHanetal.(2024),exist-
GSM8K 8.8K 1,319 Number
ingparameter-efficientfine-tuning(PEFT)methodscanbe
AQuA 100K 254 Option
roughlycategorisedintofourtypesasshowninTable6.Inthe
MAWPS - 238 Number
following,weprovideabriefoverviewofthreelayer-based
SVAMP - 1,000 Number
PEFTmethodsusedinourstudy:reparametrization-based
methods,seriesadapters,andparalleladapters.
Table 7: The statistics of datasets for evaluation. # Train
and#Testdenotethenumberoftrainingandtestsamples
1https://github.com/AGI-Edgerunners/LLM-Adapters
respectively.
2https://github.com/huggingface/transformers
3https://github.com/huggingface/peft
𝑊#$%& ReLU 𝑊!"Hyperparameters CommonsenseReasoning ArithmeticReasoning
(LoRA+IST) LLaMA LLaMA GPT-J BLOOMz LLAMA3 LLAMA3
7B 13B 6B 7B 8B 8B
Rankr 32 32
α 64 64
Dropout 0.05 0.05
Optimizer AdamW AdamW
LR 2e-4 1e-4
LRScheduler WarmupSteps WarmupSteps
Batchsize 16 16
WarmupSteps 100 100
Epochs 3 3
Where {Q,K,V,Up,Down} {Q,K,V,Up,Down}
µ 10 100
β 0.25 0.25
N ,N ,N {32,8,16} {40,10,20} {28,7,14} {30,8,15} {32,8,16} {32,8,16}
L u v
Table8:HyperparameterconfigurationsofISTforLLaMA-7B/13B,GPT-J6B,BLOOMz7B,andLLaMA3-8BwithLoRA.
SeriesAdapter+IST ParallelAdapter+IST IST
Hyperparameters LLaMA7B LLaMA13B LLaMA7B LLaMA13B Random
µ=0.1 µ=1 µ=10 µ=100 µ=1000
BottleneckSize 256 75.8 75.7 75.9 76.5 74.8 73.9
Optimizer AdamW
LR 2e-4
LRScheduler WarmupSteps Table10:Sensitivityofimportanceupdatingrateµ.
Batchsize 16
WarmupSteps 100
Epochs 3
Where {Up,Down} {Up,Gate} thatisnotconditionedontheinput.
µ 10
β 0.25 ExperimentalDetails
NL,Nu,Nv {32,8,16} {40,10,20} {32,8,16} {40,10,20}
Dataset Statistics Detailed dataset statistics can be re-
Table9:HyperparameterconfigurationsofISTforLLaMA- ferred to Table 7. Note that we trained on Commonsense
7B/13B on commonsense reasoning tasks with series and and Math10K for commonsense reasoning and arithmetic
paralleladapters. reasoning,respectively.Duringtesting,weevaluatedthepre-
definedtestsetsofeachdataset.
Hyperparameters Detailed hyperparameter settings are
SeriesAdapters. Seriesadaptersinvolveincorporatingad-
provided in Table 8 and Table 9. For PEFT training, we
ditionallearnablemodulesinasequentialmannerwithina
adheretothesettingsoutlinedbyLLM-AdapterLibrary(Hu
specificsublayer.SeriesAdaptercanbeformulatedasfol-
etal.2023),withtheexceptionofthelearningrate.ForIST,
lows:
weconsistentlysetN to25%ofthelayersforthefine-tuning
H →H +f(H W )W , (13) u
o o o down up loop,N to50%ofthelayersfortheimportanceupdating
v
where H is the output of a specific layer like MLP layer, loop, and β to 0.25. Additionally, µ is set to 10 and 100
o
f(·) is a non-linear function like ReLU, W ∈ Rd×r for the Commonsense Reasoning task and the Arithmetic
down
andW ∈Rr×d formabottleneckMLPtosavelearnable Reasoningtask,respectively.
up
parameters.
AdditionalExperiments
AsshownintheFigure5,thethreePEFTmethodsmen-
tionedaboveallutilizethelayer-baseddesign,i.e.,adding Importance Updating Rate µ The updating rate of im-
identicallearnablemodulesorparameterstoeachlayerof portance is associated with several hyperparameters, such
thepre-trainedLLM. asT ,N ,N ,andµ.Tonarrowthehyperparametersearch
c c v
Itisimportanttonotethatwedidnotincludetheprompt- spaceandreducethecomplexityofusingIST,wefixedmost
based method in our comparison because original prompt hyperparameters,settingT to10,N to3,andN tohalf
c c v
tuning(Lester,Al-Rfou,andConstant2021)isnotalayer- thenumberoflayers.Wethenadjustedtheimportanceupdat-
basedmethod;rather,itaddslearnablesoftpromptsatthein- ingrateµtomatchwiththedynamicsofPEFTfine-tuning.
putlayer.Furthermore,whilesomeadvancementsinprompt Thisparameterislargelydependentonthemaximumnum-
tuningarelayer-based,suchasPrefixTuning(LiandLiang beroftrainingiterations.Ifµistoosmall,themethodwill
2021),whichindependentlyaddssoftpromptstothehidden approximatearandomstrategy.Conversely,ifµistoolarge,
statesatalllayers,theydonotalignwithourdesign.This themethodwilltendtotrainonlyafixedsetoflayers.As
misalignmentoccursbecauseourproposedresponsesuppres- showninTable10,weconsiderµvaluesof[0.1,1,10,100,
sionoperatesontheoutputofaPEFTmethodconditionedon 1000].µisaparameterthatexhibitsinsensitivity,indicating
theinput,whereasprompt-basedmethodsproduceanoutput therobustnessofourmethod.Method Results
Baseline 74.7
ISTwithβ =0 75.0
ISTwithβ =0.1 76.3
ISTwithβ =0.25 76.5
ISTwithβ =0.5 75.3
Table11:Effectofresponsesuppressionfactorβ withinIST
forLLaMA-7BoncommonsensereasoningtaskswithLoRA.
ResponseSuppressionFactorβ Table11illustratesthe
effectofvaryingtheresponsesuppressionfactorβ ontrain-
ing a LLaMA 7B model using the commonsense dataset.
Weevaluatedamongfourvalues:[0,0.1,0.25,0.5].When
the factor is set to 0, which is equivalent to dropping the
PEFTmoduleswithinthelayer,itdoesnotadequatelyreflect
theimportanceofPEFT.Increasingthefactorenhancesper-
formance,peakingat0.25.Thisindicatesthatcomparedto
removingthePEFTmodules,suppressingitsoutputbetter
capturesitsinfluenceontheloss.However,furtherincreasing
the factor to 0.5 results in diminished effectiveness, likely
duetoreducedvariationinloss.Thesefindingssuggestthat
arelativelysmall,non-zerofactorisoptimalforaccurately
estimatingthePEFTmodule’simpactonloss.
AdaptabilitytoSoTAPEFTmethod Todemonstratethe
versatility of the IST method, we integrated IST into a re-
cent LoRA variant called DORA (Liu et al. 2024), which
decouplesthelow-rankcomponentintodirectionandmag-
nitude,yieldingbetterperformance.AsshowninTable12,
ourmethodcanenhancetheDoRAmethodinCommonsense
Reasoningtaskswithoutanylossofperformance,whilealso
requiringlessmemoryandcomputationalresources.Thisef-
ficiencyisachievedbyexplicitlytrainingonlyasubsetofall
layers,highlightingthegeneralapplicabilityofourproposed
IST.
TimeConsumption
Toaccuratelyestimatethetrainingtime,werandomlysam-
pledpromptsfromtheAlpacadatasetandlimitedthemaxi-
mumoutputtokenlengthto1024.WeusedLoRAonLLaMA-
7Bwitharankof32asourbaseline.Additionally,weem-
ployedtheLISA(Panetal.2024)method,whichrandomly
selectstwotransformerlayersforupdating.Weconducted
140iterationsandaveragedtheforwardandbackwardtimes
ofthemiddle100iterationstoobtainastabletimeestimate
duringtraining.AsshowninTable13,LISAreducesthefor-
wardtimecomparedtoLoRAduetotheabsenceofadditional
parameters for inference, while it increases the backward
time.Conversely,ISTmaintainstheforwardpasstimebut
reducesthebackwardtimebyapproximately10%.Despite
this,ISTrequiresanadditionalthreeforwardpassesevery
10fine-tuningloopsforimportanceupdating.Consequently,
after100iterations,thetotaltimeconsumptionforISTbe-
comescomparabletothatofLISAandslightlyhigherthan
LoRA.Model PEFT BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg.
ChatGPT - 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0
DoRA 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2
LLaMA3 ICML2024
8B DoRA+IST 74.0 89.2 80.2 95.0 86.2 90.3 81.2 85.6 85.2
Table12:AdaptabilitytothelatestLoRA-variantmethodcalledDoRA(Liuetal.2024).Ourapproachcanreducememory
consumptionwithoutcompromisingaccuracy.
LoRA LISA LoRA+IST
Forwardtime
135 101 135
periter.(ms)
Backwardtime
184 225 150
periter.(ms)
Timeconsumption
31.9 32.6 32.6
per100iter.(s)
Table13:ComparisonofTrainingTimes.Allresultswere
obtainedusingoneNvidiaGTX4090GPU.