RClicks: Realistic Click Simulation
for Benchmarking Interactive Segmentation
AntonAntonov1∗# AndreyMoskalenko1,2∗ DenisShepelev1∗
AlexanderKrapukhin1 KonstantinSoshin1 AntonKonushin1 VladShakhuro1†
1AIRI,Moscow,Russia 2LomonosovMoscowStateUniversity
{lastname}@airi.net
∗Equalcontribution †Projectleader #Correspondingauthor
§https://github.com/emb-ai/rclicks
Abstract
TheemergenceofSegmentAnything(SAM)sparkedresearchinterestinthefield
ofinteractivesegmentation,especiallyinthecontextofimageeditingtasksand
speeding up data annotation. Unlike common semantic segmentation, interac-
tivesegmentationmethodsallowuserstodirectlyinfluencetheiroutputthrough
prompts(e.g. clicks). However,clickpatternsinreal-worldinteractivesegmenta-
tionscenariosremainlargelyunexplored. Mostmethodsrelyontheassumption
thatuserswouldclickinthecenterofthelargesterroneousarea. Nevertheless,
recentstudiesshowthatthisisnotalwaysthecase. Thus,methodsmayhavepoor
performanceinreal-worlddeploymentdespitehighmetricsinabaselinebench-
mark. Toaccuratelysimulatereal-userclicks,weconductedalargecrowdsourcing
studyofclickpatternsinaninteractivesegmentationscenarioandcollected475K
real-userclicks. Drawingonideasfromsaliencytasks,wedevelopaclickability
model that enables sampling clicks, which closely resemble actual user inputs.
Usingourmodelanddataset,weproposeRClicksbenchmarkforacomprehensive
comparisonofexistinginteractivesegmentationmethodsonrealisticclicks. Specif-
ically,weevaluatenotonlytheaveragequalityofmethods,butalsotherobustness
w.r.t. clickpatterns. Accordingtoourbenchmark,inreal-worldusageinteractive
segmentationmodelsmayperformworsethanithasbeenreportedinthebaseline
benchmark,andmostofthemethodsarenotrobust. WebelievethatRClicksisa
significantsteptowardscreatinginteractivesegmentationmethodsthatprovidethe
bestuserexperienceinreal-worldcases.
1 Introduction
Thetaskofinteractivesegmentationinvolvesprovidingadditionalhintsorpromptstothemethod,
allowingittoproducemorepreciseannotationscomparedtoconventionalsemanticsegmentation.
ThemostfamousmemberofinteractivesegmentationmethodsisSegmentAnything(SAM)[1,2].
Nowadays,SAM-likemethodsareappliedinvariousfields,includingthethinobjectsegmentation[3,
4],medicalsegmentation[5,6,7,8,9,10],3Dsegmentation[11,12],tracking[13]andvideo[2].
Typically,interactionsoccurinseveralrounds,whereineachroundtheusercorrectstheprediction
errorsofthepreviousone. Evaluationofsuchmethodsrequiresuserinputs. However,collecting
manyreal-userinputsformultipleroundsisimpracticalsincesuchadatasetneedstoberebuiltfor
everymethodandeveryinteractionroundduetoitsiterativenature. Thus,researchersoftenresortto
asimplestrategytosimulateuserinputs. Accordingtothisstrategy,asingleclickforeachinteraction
roundisgeneratedasfollows: (1)selectthelargesterrorregioninthepreviousinteractionround,and
(2)clickinthefurthestpointfromtheboundariesofthisregion(centerpoint). Hereinafter,werefer
Preprint.Underreview.
4202
tcO
51
]VC.sc[
1v22711.0142:viXra(a)GrabCut[15] (b)Berkeley[16] (c)COCO-MVal[17] (d)DAVIS[18] (e)TETRIS[14]
Figure1: Examplesofrealandpredictedusers’clicksofinteractivesegmentationtask. Theupper
rowdepictsreal-usersclicks(green)foragiventargetobject(whitecontour);themiddleandbottom
rowsvisualize, correspondingly, clicksandtheirdistributionpredictedbyourclickabilitymodel.
Purplepointsinthemiddleandbottomrowsrepresentclicksgeneratedbythebaselinestrategy[19].
Mostlybaselineclickisclosetoamodeofusers’distribution(see(b)and(e)),however,insome
casesitmaybefarfromthemode(e.g.(a),(d))ormaynotrepresentallmodesofthedistribution
(e.g.(c),(e)).
totheaboveclicksamplingstrategyasabaselinestrategy,following[14]. However,relyingsolely
onthisapproachmayresultinoverfittinganddegradedperformanceinreal-worldusagescenarios.
Ourgoalistocreateahighlyrealisticsimulatorofuserclickstoenableamoreaccurateevaluationof
interactivesegmentationmethods. Webeginourresearchwithasimpleobservation: whenauser
clicks,theirgazeisfocusedontheareawheretheyclick. Inturn,thetaskofpredictingsaliencyis
well-studied,withbenchmarkdatacollectedusingspecializeddevicessuchaseyetrackers. Saliency
predictionmodelsgeneratespatialattentionheatmaps,fromwhichfixationpointsofviewerscan
besampledandutilized. However,saliencymodelsassumeafree-viewingtask,whichdiffersfrom
interactivesegmentation,wheretheusershouldsegmentaspecificarea. Inotherwords,clicksshould
besampledfromaspatialdistribution,thatisconditionednotonlyonanimagebutalsoonatarget
segmentationarea.
Drawingfrombestpracticesininteractivesegmentationandsaliencypredictiontasks,wecollecta
datasetoftask-specificuserclicks,andproposeamodelthatfacilitatessamplingofthemostrealistic
clickpositionsininteractivesegmentation. Overall,ourmaincontributionsareasfollows:
• Wecuratealargemultiple-roundinteractiondatasetintheinteractivesegmentationtask(see
samplesfromthefirstroundinFigure1). Toachievethis,weintroduceaclickcollection
methodologyandconductanablationtoaddresspresentationbias,involvingusersonboth
PCsandmobiledevices.
• Weintroduceanovelclicksamplingstrategybasedonaclickabilitymodelthatcansample
morerealisticclicksthanthebaselinestrategyandestimateclickprobabilities.
• WepresentRClicks–anovelbenchmark,thatleveragestheclickabilitymodeltoestimate
thereal-worldperformanceofinteractivemethods. Weconductextensivecomparisonsand
benchmarkstate-of-the-artmethodsusingboththebaselinestrategyclicks,andrealistic
clickssimulatedbytheclickabilitymodel. Thiscomparisonrevealsthatbenchmarksemploy-
ingthebaselinestrategymayoverestimatemethods’real-worldperformance. Moreover,we
concludethatcurrentsegmentationmethodsareunabletoachievebothoptimalperformance
androbustnesssimultaneouslyonalldatasets.
• We utilize the collected first-round real-user clicks to evaluate the performance of seg-
mentationmethods. Furthermore, weproposeamethodologytoestimatethereal-world
segmentationdifficultyforstate-of-the-artmethodsforeachinstanceinadataset.
Webelievethattheproposedmethodologyenhancescomprehensionofreal-usersactionsandwill
facilitatethedevelopmentofinteractivemethodsthataremoreapplicableinreal-worldcases.
22 RelatedWork
2.1 UserInputTypesinInteractiveSegmentation
Varioustypesofuserinputshavebeenexploredintheliterature. In[15,20]aninitialselectionis
obtainedusingboundingboxes,andthenrefinedwithstrokes. In[21]objectselectionisdonewith
strokes. [22]considerscontoursforselectingsmallobjects,minorpartsofanobject,oragroupof
objectsofthesametype. [23]proposestousetrimap,scribblemaporclickmapasaninput. Segment
Anything,orSAM[1],processesmultipletypesofuserprompts,includingapoint,abox,amask,or
atext.
Clicks-basedapproachselectsobjectsofinterestaccordingtomultipleuserclicks(eitherpositive
or negative), and was first introduced in [24] and investigated in [19, 25, 26, 27, 1, 3, 28]. We
focussolelyontheclick-basedapproach,sinceitiswell-exploredandhasanestablishedevaluation
procedureinthefield.
2.2 BenchmarkingInteractiveSegmentation
GrabCut [15] is the first dataset proposed for interactive segmentation task. Then [16] adapted
Berkeley [29] segmentation dataset to evaluate interactive segmentation methods, but it required
manual testing. However, manual testing is a time-consuming and resource-intensive process.
Interactivesegmentationexpectsmultipleroundsofinteractions,wheneachinteractiondependson
previousones,anditisinfeasibletoapplymanualprocedureforlargerscales. Forthesereasons,in
practice,benchmarksgenerateuserinteractionsautomaticallybasedonpreviousinteractions.
In [24] authors proposed an automatic clicks generation strategy for evaluation on PAS-
CAL VOC 2012 [30] and COCO [17] segmentation datasets. The subsequent work [31] used
DAVIS[18]andSBD[32]datasetsforinteractivesegmentation,applyingthesamebaselinestrategy.
Mostoftheexistingclick-basedmethods[19,25,26,27,1,3,28]usethebaselinestrategy. However,
it has not been validated in real-world usage scenarios until recently. [14] introduced TETRIS
benchmarkandrevealedthatrealusersdonotalwaysclickinthecenterofanareawiththelargest
error,asassumedinthebaselinestrategy. Usingtheadversarialattacks,thepaperdemonstratedthat
methodshaveatendencytooverfittothebaselinestrategy. Specifically,whenthebaselineclicks
areused,thesegmentationqualitymaybehigh,butevenaslightchangeintheclickpositioncan
resultinasignificantdropinquality. Therefore,thebaselinestrategymaynotaccuratelyestimatethe
qualityofthemethodsinrealusage. Webelievethattoestimatetheactualquality,eachclickshould
begeneratedinaccordancewithhumanperception.
2.3 SaliencyPrediction
Thetaskofsaliencypredictionaimstomodelhumanperceptionbypredictingprobabilitymaps[33,
34]ofuserengagementinafree-viewobservationforagivenmediacontent. Referencedataforthis
taskusuallycomesfromaspecializeddevice–aneyetracker–whichrecordseyefixations[35,36,37].
Subsequently,fixationsfrommultipleviewersareaggregatedintoaprobabilitydistributionthrough
Gaussianateachfixationpoint,withsigmacorrespondingtotheretinalangleofahuman’sfieldof
view[35].Sincescalingexpensiveeyetrackerexperimentsistoocomplex,severalresearchers[38,39]
proposedtousemousemovementsasaproxyforsaliencywhentrainingsaliencymodels. However,
saliency fixations cannot be directly used in the interactive segmentation task because saliency
observersengageinfree-viewing,whileinourtask,theuser’sgoalistomakeaclicktohighlighta
specificobjectorapartofit. Thus,fortheinteractivesegmentationproblem,real-userclicksshould
becollected.
3 Users’ClicksDataset
Weproposeanoveldatasetofreal-usersclicksforinteractivesegmentation. Ourdatasetisbasedon
theexistingimagesegmentationdatasets. Intotal,wecollected475544userinputsforGrabCut[15],
Berkeley[29],DAVIS[18],COCO-MVal[40],TETRIS[14]. Togatherusers’clicks,wedeveloped
aspecializedpresentationtool. Specifically,ineachtask,weaskeduserstoclickonthetargetobjects
bydisplayingimagesandcorrespondingsegmentationmasks. Weconsideredseveraldisplaymodes
3toinstructuserswhatobjectsshouldbeselected. Asinterfacecancausebiasinclicksdistribution,
weconductedauserstudytoselecttheoptionthatbestmimicsnaturaluserobjectselections.
Thissectionisorganizedasfollows. First,wepresenttaskdisplaymodes(3.1). Thenwechooseone
thateliminatesbiasassociatedwithuserviewingmodebehavior(3.2). Finally,wedescribeclicks
collectioninthefirstandthesubsequentroundsofinteractions(3.3).
3.1 CollectionProcedure
Whencollectinguserclicks,
Free-view Free-view, then click
we executed the follow-
ing procedure (see Fig-
ure 2): (1) Show the en- Instruct the
participant
tire image for 1.5 seconds.
using one of the
(2)Showsegmentationtar-
Display Modes
get using one of the Dis-
play Modes. (3) The en-
tire image is shown again
for 1.5 seconds, during
which clicking is not al-
RED PANDA
lowed.(4)Theusermakesa
click. Steps(1)and(3)sim-
ulatetheuserbehaviordur- Text Descripton Object CutOut Shifted CutOut Silhouette Mask Highlighted Instance
ing real-world interactive
segmentation,whenindivid- Figure2:Illustrationofthetesteddisplaymodestoreducepresentation
ualsinitiallyviewtheimage bias. ThebestresultwasobtainedwiththeObjectCutOutmode,where
andtheninteractwithitby anobjectispresentedonagraybackgroundwithoutshifts.
clicking. Step(2)visualizes
theobjectthatshouldbeselectedbytheuser.
Weconsideredthefollowingtaskdisplayingmodes. (a)TextDescriptionmodeshowsthetextual
description of the target object for 2.5 seconds. (b) Object CutOut visualizes the target instance
initsoriginalpositiononagraybackgroundfor2seconds. (c)DuringShiftedCutOut,thetarget
instanceisshownfor2secondsonagraybackground,thenshiftedtothetop-leftcorner,whichaims
tomotivatetheassessortoindependentlylocatetheinstanceontheimageasitspositionisshifted.
(d)SilhouetteMaskshowsablack-and-whitemaskofthetargetinstanceintheoriginalpositionfor2
seconds. (e)HighlightedInstancedisplaystheoriginalimagewiththebackgroundwherethetarget
instanceishighlightedwithagreenborder. Rationaleforourchoiceofthesedisplayingmodesand
timeperiodscanbefoundinAppendixB.1.
3.2 SelectingUnbiasedTaskDisplayMode
TextDescriptionisconsideredtobeunbiasedbecauseusersdonotseethetargetsegmentationmask,
asinreal-worldinteractivesegmentation.However,textualdescriptionsmaybeambiguousforcertain
typesofinstancesorareas(seeFigure3). Inotherdisplaymodes,themaskispresented,whichcould
potentiallydistortthedistributionofuserclicks. Tochoosemask-baseddisplaymodewithminimal
bias,wecompareallmodeswithTextDescriptionmode.
(a)Originalimage (b)Instancesmasks (c)False-positiveerror(red (d) False-negative error
mask) (tealmask)
Figure3: Thedifficultyofutilizingtextdescriptionswheninstructingparticipantsonthefirstround
selectionofacertaininstance((a)-(b))andtoselectorunselectthecertainerrorareainthesubsequent
round((c)-(d)).
4Therefore, we conducted an ablation study on 100 randomly selected images from TETRIS
dataset [14]. We used images and segmentation masks from TETRIS, and additionally manu-
allyannotatedtextualdescriptions. Inthisstudy, wecomparetheclicksobtainedviaconsidered
displaymodeswithclickscollectedthroughTextDescription.
ClicksgatheringwasdoneonTolokaAI1 crowdsourcingplatform. Eachparticipantwasgivena
batchof10uniqueimages,ineachimagetheywererequiredtomakeoneclick. Eachparticipant
receivedonaverage3batchesofimages. Participantsdidnotreceivethesameimagemorethan
once. Foreverydisplaymode,differentpeoplewereinvolvedinlabeling. Aparticipant’sclicksare
consideredtobevalid,ifatleast7outofthe10clicksinabatchwerewithintheobjectmask. We
also considered a click to be valid whether it was within the object mask or not farther than one
clickradiusfromtheborder. Otherwise,clicksweredisregardedasinvalid. Toselectanunbiased
presentationstrategy,foreachimage,wecollected25clicksfromparticipantsusingcomputersand
25clicksfromthoseusingmobiledevices. Afterfiltering,intotalweobtained47725validclicks.
TocomparequantitativelydisplaymodeswithunbiasedTextDescriptionmode,weutilizedthefol-
lowingsample-basedmetrics: (a)PL –meanofallpairwiseL -distancesbetweenclickcoordinates,
1 1
normalizedbyobjectwidthandheight.(b)WD–Wassersteindistance[41]betweenclickcoordinates,
normalizedbyobjectwidthandheight. (c)KS–Kolmogorov-Smirnovtestin2Dcase[42,43,44].
Weconcludethatclicksarenotsignificantlydifferentifap-valueisgreaterthan0.05. Theindicator
functionisusedasametric,whichequals1whenclicksarenotsignificantlydifferent.
Theaveragevaluesw.r.t. imagesofthelistedmetricsforeach Table 1: Comparison of display
displaymodearepresentedinTable1. Thebestresultswere modeswithTextDescription.
obtainedwiththeObjectCutOutmethod. Notethatwedidnot
useprobabilitymapbasedmetricsusedforevaluatingsaliency Displaymode PL1↓ KS↑ WD↓
prediction, becausetheactualmodelofclicksdistributionis ObjectCutOut 0.242 0.58 0.042
unknown,andwedidnotwanttolimititintheablationstage.
ShiftedCutOut 0.246 0.56 0.046
Here, weablateddisplaymodesonlyforthefirstinteraction
SilhouetteMask 0.246 0.41 0.048
rounds. Wecannotexamineourdisplaymodesinthesubse-
HighlightedInstance 0.258 0.37 0.051
quentrounds,as,inadditiontothereferenceinstance,theuser
needstoknowwhicherrorshouldbecorrectedandwhereitis
located,whichisnotpossibletodescribetextually. Thus,weassumethatthebestdisplaymodefor
thefirstroundisalsobestforthesubsequentones.
Inthefollowing,weusedObjectCutOutmodetocollectclicksfortheremainingdatasets.
3.3 CollectedInteractions
Weannotatedeachinstanceinallcommoninteractivesegmen- Table 2: The number of collected
tationbenchmarkdatasets–DAVIS[18],GrabCut[15],COCO- clicksforeachdatasetininteraction
MVal[17],Berkeley[29],TETRIS[14]usingPCandmobile rounds.
clicks. Collectedclickswerevalidatedsimilarlytotheablation
stage. Whenannotatingsubsequentinteractionrounds,theuser Dataset First# Subseq.# Sum#
shouldclickintheareaofthesegmentationerror. Toobtainer-
rormasksforthesubsequentrounds,weappliedstate-of-the-art GrabCut 2395 3427 5822
interactivesegmentationmethods–SAM[1],SimpleClick[25], Berkeley 4859 6937 11796
andRITM[19]–toallimagesandallclickscorrespondingto DAVIS 16975 23687 40662
thoseimagesfromthefirstround. Then,foreachimage,we COCO-MV 38097 53926 92023
selectedthemaskwiththehighestqualityuptoathresholdof TETRIS 123023 202218 325241
0.95IoU.Wemotivateitbythefactthatatsuchahighlevelof All 185349 290195 475544
quality,theuserislikelytostopannotatingtheinstanceasthe
errorswouldbeminimal,andeventheradiusoftheclickmayexceedthesizeoftheerroneousarea.
Intotal,wecollected475thousandvalidclicksfromusers. Thenumberofvalidclicksannotatedfor
eachdatasetandinteractionroundispresentedinTable2.
1https://toloka.ai/
54 ClickSimulation
Inthissection,weexploremodelsforpredictinguserclicks. Firstly,thebaselinesaredescribed(4.1).
Secondly,weintroduceaclickabilitymodelusedforclickprediction(4.2)inourinteractivesegmen-
tationbenchmark. Thirdly,in(4.3)wedescribetheconstructionoftrainingdatasetforclickability
model. Finally,wecompareourclickabilitymodelwithbaselines(4.4).
4.1 BaselineModels
As baselines for comparison, we considered uniform, distance, and saliency distribution models.
Theuniformhypothesispostulatesthattheclickabilityofallpixelswithinthetargetareaisequally
distributed(seeFigure4(b)). Whentheareaofinterestisrelativelysmall,theuniformassumptionis
reasonable. However,accordingtothisassumption,theclickprobabilityofobjectboundariesand
theircentersisequal,whichisnotnecessarilytrue. Thedistancetransform[14,19]addressesthis
issuebyassigninggreaterweighttopixelsinthecenteroftheobjectthantothoseontheboundary
(seeFigure4(c)). Nevertheless, thistransformconsidersonlytheshapeoftheobject, neglecting
humanperception. Toaccountforhumanperception,saliencydistributioncanbeused. Thisisa
reasonablebaseline,asuserslookatthetargetareaofinteractionwhenclicking. Forthesaliency
baseline,weutilizedastate-of-the-artmodel,TranSalNet[45]. Theexampleofconstructedsaliency
distributionispresentedinFigure4(d),detailsofhowsuchmapisconstructedcanbefoundinthe
Appendix A.2. However, saliency models are trained for free-viewing task, and do not take into
considerationthesetupofourtask.
Table 3: Evaluation of various click-
ability models on real-user clicks of
TETRISvalidationpart. Ourapproach
outperformsexistingclickingstrategies
intermsoftheproximityofsamplesto
real-userclicks.
(a)Clicks (b)UD (c)DT (d)SM (e)Ours Model KS↑ PL1↓ WD↓ NSS↑ PDE↑
Figure4: Examplesofconsideredclickabilitymodels: (a) UD 0.10 0.57 0.17 3.99 1.36E-05
visualizes target object (white contour) and ground-truth DT 0.14 0.52 0.16 6.45 2.76E-05
clicks(greenpoints);(b)–(d)depictuniformdistribution SM 0.13 0.51 0.15 4.79 1.83E-05
(UD),distancetransform(DT),andsaliencymap(SM)re- Ours 0.55 0.40 0.08 9.11 4.69E-05
spectively;(e)–ourpredictedclickabilitymap.
4.2 ClickabilityPredictionModel
Similartothesaliencyprediction
task, we formulate the task of Input
simulatinguserclicksasaproba- image Predicted
clickability
bilisticproblem.Givenanimage,
agroundtruthobjectmask,and
Segmentation Clickability
a segmentation error mask (FP KLD loss
error model
orFN),themodelshouldpredict
at each pixel the probability of Ground-
being clicked. We refer to this Target truth
asaclickabilitymap(seedetails instance clickability
in Section 4.3). The proposed
pipelineisshowninFigure5. As Figure5: Proposedclickabilitypredictionpipeline.
abasearchitectureforourmodel,
weadaptedstate-of-the-artSegNeXtsegmentationnetwork[46]. Weinputtheoriginalimageintothe
networkandconcatenatethegroundtruthmaskwiththeerrormask,feedingtheresultingtensorasan
additionalinputtothenetwork,atechniqueinspiredbytheConv1S[19].WeusetheKullback-Leibler
divergence(KLD)lossfunctionbetweenthepredictedandgroundtruthdistributions.
64.3 ClickabilityMapsDataset
Weintroducetheconceptofaclickabilitymapasasingle-channelimage,suchthatthevalueofeach
pixelcorrespondstotheprobabilitythattheuserwillclickonitduringtheinteractionround. We
proposetousesuchmapstotrainclickabilitymodels.
Givenanimage,errormask,anduserclicks,theclickabilitymapisconstructedasfollows: (1)ini-
tialize the map as an image of zero values; (2) at each pixel position that was clicked, add one;
(3)smooththemapbyaGaussianwithsomesigma,wheresigmaisahyperparameter;(4)multiply
pixelvaluesofthemapbycorrespondingpixelvaluesfromasofterrormask,obtainedbysmoothing
theoriginalerrormaskbyaGaussian;(5)normalizepixelsbythemapsum. Theproposedmethodis
analogoustotheconstructionofsaliencymapsfromhumaneyefixations,exceptforstep(4).
Unlikesaliency,weneedtosomehowconstrainthemostlikelyclickpositionswithintheboundaries
ofthemask. Moreover,recallthatduringthecollectionofclicks,weconsideredclicksasvalidif
theywereinsidethemaskorclosetoitsborder. Forthesereasons,instep(4)theclickabilitymap
isconditionedbymultiplyingitonthesofterrormask–smoothingtheerrormaskweconsiderthe
allowedradiusofthebordervicinity. WesmooththeerrormaskbyGaussianblurringwithasigma
equaltotheradiusoftheclickusedintheuserinterface(i.e.,1%ofimagediagonal). Thesigmain
step(3)simulatestheprobabilitydensityofclicksinsidethemask.
Weconstructedtrainandvalidationdatasetsasfollows. WesplitimagesofTETRISdatasetinto
non-overlappingtrainandvalidationparts. Sincewedonotknowtherealclickdensity,formodel
trainingandvalidation,severalsetsofclickabilitymapswereconstructedwithvaryingmagnitudesof
sigmafromstep(3). Tochoosethebestsigma,weconductedanablationstudy,whichcanbefound
inAppendixA.3. Notethatweconstructedclickabilitymapsusingclicksfrombothsmartphonesand
PCs. Thiswasdonetoensurethatthemodelwouldpredictclicksregardlessofthedevicetype.
4.4 ModelsEvaluation
To choose the best clickability model, we evaluated considered models on the real-user clicks of
TETRISvalidationpart. Here,inadditiontosample-basedmetricsconsideredabove,wecalculated
additional metrics, that were computed based on the ground-truth clicks positions and predicted
clicksdistribution: PDE–likelihoodofground-truthclicks,andNSSfromsaliencybenchmarks[47].
EvaluationresultsarepresentedinTable3. Ourclickabilitymodelshowsthebestperformance. We
addressthequestionofmodelgeneralizabilityintheAppendixA.2.
5 BenchmarkingInteractiveSegmentation
Inthissection,weintroduceRClicksbenchmarkthatevaluatestheinteractive
G10
segmentation methods according to the proposed clickability model. Our G9
evaluation protocol aims to estimate not only the average annotating time G8
but also the spread w.r.t. clicking groups. Our model returns a probability G7
densityforaninstance. Foreverypossibleclick,wehave(x,y)coordinates G6
G5
andprobability. Wesortclicksaccordingtotheirprobabilitiesandsplitthem
G4
into10intervals(calledclickinggroups){G i}1 i=0 1s.t. everyintervalhas10% G3
oftotalprobabilitymass. Weinterpretthesegroupsasdifferentuserclicking G2
patterns, and evaluate methods for each group separately. Visualization of G1
clickinggroupsforaninstancemaybeseeninFigure6. Note,thateventhe
probabilitymassofeachgroupG isequal,theaverageprobabilityofclicks Figure 6: Spatial
i
ineachgroupincreaseswithincreaseofi. distribution of
clicking groups
Wemodifyacommonevaluationprotocol[19]byreplacingthebaselinesam-
for the instance in
plingstrategywithsamplingfromdifferentclickinggroups,obtainedthrough
Figure4(a).
theclickabilitymodel. Specifically,(1)interactivesegmentationmetrics(e.g.
NoC)arecalculatedforeveryinstanceinadatasetandgroupG bysamplingclickfromG (weighted
i i
bytheclickabilitymodel)foreveryinteractionround(inourexperiments–20rounds);(2)thenfor
eachinstancestatistics(e.g. meanandstandarddeviation)ofsampledmetricsoverclickinggroups
areestimated;(3)finally,thesestatisticsareaveragedoverallinstancesinthedataset.
7Ininteractivesegmentation,acommonlyusedmetricisNoC @90,whichestimatestheannotation
20
time(inclicks,notmorethan20)toachieve90%IoUusingaparticularmethod. Weusethismetric
with our sampling strategy to estimate the following averaged statistics: (i) Mean and standard
deviation,whicharedenotedasSampleNoC.(ii)RelativeincreaseofSampleNoCcomparedto
thebaselinestrategyNoC.Thisstatisticindicateshowmuchextraannotationtimeanaverageuser
spendscomparedtothebaselinestrategy. Wedenoteitas∆SB.(iii)Relativeincreaseofannotation
timeusingclicksfromgroupG overusingclicksfromgroupG . Thismetricisdenotedas∆GR.
1 10
Thismetricrepresentsadifferenceinannotationspeedbetweentwoclickinggroups, thathavea
maximumdifferenceofaverageclickingprobabilities.
Figure7showsplotsoftheaver-
aged IoU versus the number of
DAVIS COCO-MVal TETRIS
clicks for various segmentation 0.93 0.95 0.96
methods when sampling clicks 0.83 G1 (86.63) 0.87 G1 (91.48) 0.89 G1 (91.85)
according to the baseline strat- 0.74 G10 (89.97) 0.79 G10 (92.86) 0.81 G10 (93.10)
egy, G and G groups. Over- 0.64 Base (90.67)0.71 Base (93.90)0.73 Base (94.43)
1 10 0.94 0.96 0.97
all,clicksfromG outperform
10 0.88 0.90 0.92 clicksfromG intermsofIoU- G1 (89.87) G1 (92.49) G1 (94.41)
1 0.82 G10 (91.78) 0.83 G10 (94.07) 0.87 G10 (95.08)
AuC,whilethebaselinestrategy 0.75 Base (92.05)0.77 Base (94.30)0.82 Base (95.44)
mostlyoutperformsclicksfrom 0.94 0.97 0.97
both groups. With a sufficient 0.87 0.91 0.93 G1 (89.85) G1 (93.43) G1 (95.19)
number of interactions, clicks 0.79 G10 (91.45) 0.85 G10 (95.01) 0.88 G10 (95.72)
from both G and G achieve 0.72 Base (91.81)0.79 Base (94.98)0.84 Base (96.05)
10 1 1 4 8 12 16 20 1 4 8 12 16 20 1 4 8 12 16 20
highIoU.However,clicksfrom Number of clicks Number of clicks Number of clicks
G 1requiremoreinteractions. Figure 7: Mean IoU for varying number of clicks for baseline
strategy,G andG clickinggroups.IoU-AuCunderthesecurves
Estimated statistics for 1 10
providedinbrackets.
NoC @90 are presented
20
in Table 4. Additional eval-
uation results for NoF @90,
20
IoU-AuC are provided in
20
AppendixC.
1.0
In addition to the evaluation re-
sultsonsimulatedclicks,wepro-
vide evaluation results on the 0.8
firstroundrealclicks. Theeval-
uation on the subsequent real 0.6
clicks is infeasible, since inter-
active segmentation in a subse-
0.4
quentrounddependsonamodel Dataset (NSR)
TETRIS (21.51)
output from a previous round.
COCO (20.34)
However,thereisnosuchprob- 0.2 DAVIS (24.15)
leminthefirstround,andactual Berkeley (16.50)
GrabCut (16.42)
performancemetricscanbecom- 0.0
puted on the real clicks of the 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35
STD-IoU@1
first round. Therefore, we em-
Figure8: Ascatterplotofthemeanvs. standarddeviation(STD)
ployed the real clicks from the
of IoU for the first real-users clicks. Each point represents the
first round as follows: (1) com-
statisticsforeachinstance,averagedacrossallconsideredsegmen-
puted real-world accuracy (see
tationmethodsandrealclicks. AnaverageNSRforeachdataset
AppendixC);(2)comparedaccu-
isprovidedinbracketsinthelegend.
racyofinteractivesegmentation
methodsusingrealandsimulatedclicks(seeAppendixC);and(3)estimatedreal-worldrobustnessof
themethodsforeachinstanceinthedataset. WeestimatedthelatterthroughIoUnoise-to-signalratio
(NSR).ThegreaterthevalueofNSR,themoredifficultsuchaninstanceisforsegmentation. Figure8
plotsascatterofmeanvs. standarddeviationofIoUoverreal-usersfirstclicksandsegmentation
methodsfromTable4.
8
TI-23RH-MTIR
B-TiV-tpadA
H-TiV-elpmiS
1@UoI-naeM
UUUoooIII
UUUoooIII
UUUoooIIITable 4: Evaluation results of state-of-the-art interactive segmentation methods. Statistics of
NoC @90 on clicking groups {G }10 , averaged over datasets: Sample – mean and standard
20 i i=1
deviation(std);∆SB–relativeincreaseofSampleNoCcomparedtobaselinestrategyNoC;∆GR–
relativeNoCincreasebetweenG andG . InDatacolumn: C+LdenotesCOCO+LVIS[19];SBD,
10 1
SA-1BandSA-V–datasetsfrom[32],[1]and[2]respectively. Thebestresultsareinbold, the
secondbestareunderlined,andthethirdbestareinitalics.
DAVIS[18] COCO-MVal[17] TETRIS[14]
Method Backbone Data NoC20@90 NoC20@90 NoC20@90
Sample ∆SB ∆GR Sample ∆SB ∆GR Sample ∆SB ∆GR
(±std) (+%) (+%) (±std) (+%) (+%) (±std) (+%) (+%)
GPCIS[27] RN50 C+L 6.44±0.85 16.88 53.65 4.74±1.31 26.43 79.00 3.87±0.79 19.55 56.43
RN34 C+L 5.95±0.73 14.95 45.88 4.13±0.85 15.15 49.79 3.10±0.53 14.16 44.06
CDNet[48]
RN34 SBD 7.87±1.25 23.39 64.95 6.36±1.29 20.86 51.58 4.51±0.76 17.08 55.22
HR18 C+L 6.23±0.67 6.92 16.13 3.71±0.78 10.27 20.22 3.69±0.52 7.02 13.95
HR18s-IT C+L 6.71±0.99 20.88 54.15 3.65±0.92 16.81 33.89 3.80±0.67 15.79 32.66
RITM[19] HR18-IT C+L 6.15±0.83 11.37 31.14 3.22±0.83 15.84 37.01 3.48±0.60 11.59 23.99
HR32-IT C+L 5.90±0.89 18.34 51.07 3.24±0.83 15.50 37.31 3.44±0.65 17.47 30.69
HR18-IT SBD 7.42±1.03 17.85 38.39 4.81±1.24 17.17 43.23 4.80±0.74 11.96 25.31
ViT-B C+L 4.97±0.40 8.60 15.14 2.93±0.58 9.44 19.75 2.62±0.37 6.99 12.94
AdaptClick[49]
ViT-B SBD 5.37±0.49 8.69 17.94 4.33±1.06 14.59 35.07 3.49±0.50 8.40 16.44
ViT-B C+L 5.32±0.54 9.05 26.33 3.07±0.70 11.72 23.60 2.73±0.41 8.86 16.64
ViT-L C+L 5.03±0.42 8.71 16.67 2.67±0.56 8.05 20.88 2.46±0.35 7.11 10.01
ViT-H C+L 5.00±0.42 7.06 12.29 2.57±0.54 6.14 17.65 2.36±0.33 6.94 10.83
SimpleClick[25] ViT-XT SBD 8.35±1.36 18.67 51.05 5.86±1.65 26.28 61.63 5.49±1.22 28.57 35.40
ViT-B SBD 5.77±0.58 8.44 25.72 4.52±1.07 17.58 36.18 3.75±0.54 11.42 19.50
ViT-L SBD 5.56±0.53 7.26 15.97 3.83±0.88 10.02 33.06 3.40±0.43 7.32 16.15
ViT-H SBD 5.49±0.55 7.67 23.69 3.74±0.86 10.46 31.97 3.32±0.43 7.37 16.88
CFR-ICL[26] ViT-H C+L 4.53±0.46 9.32 18.47 2.70±0.63 9.58 24.13 2.12±0.34 8.76 14.33
MobileSAM[28] ViT-Tiny SA-1B 5.96±0.56 8.63 15.39 5.25±0.78 9.79 19.78 3.42±0.48 7.47 12.69
ViT-B SA-1B 5.30±0.53 8.26 11.27 4.91±0.79 9.88 15.73 3.04±0.51 11.17 10.06
SAM[1] ViT-L SA-1B 5.21±0.41 8.82 11.59 4.81±0.63 8.89 14.97 2.60±0.40 8.11 7.08
ViT-H SA-1B 5.42±0.49 8.00 15.02 5.14±0.68 7.63 15.61 2.66±0.38 5.95 8.50
ViT-B SA-1B 5.32±0.50 7.45 13.03 5.39±0.89 12.48 16.68 3.35±0.67 16.41 10.74
SAM-HQ[3] ViT-L SA-1B 5.19±0.48 8.58 15.69 5.05±0.74 9.64 13.50 2.81±0.51 11.02 7.69
ViT-H SA-1B 5.16±0.44 8.15 18.36 4.97±0.68 7.71 12.36 2.75±0.41 6.78 7.95
Hiera-T SA-V 4.65±0.28 4.86 7.46 3.86±0.64 7.79 13.14 3.11±0.50 9.45 3.57
Hiera-B+ SA-V 4.67±0.33 8.49 15.86 3.75±0.61 7.44 12.67 3.02±0.47 9.51 4.79
SAM2[2]
Hiera-L SA-V 4.61±0.29 9.51 13.28 3.84±0.62 9.12 12.35 2.83±0.41 7.46 4.10
Hiera-H SA-V 4.39±0.23 7.55 10.03 3.42±0.51 6.12 9.34 2.74±0.38 6.51 4.87
Hiera-T SA-V 4.67±0.32 7.08 8.99 3.91±0.68 8.45 11.88 3.11±0.50 9.75 3.35
Hiera-B+ SA-V 4.63±0.32 9.72 14.30 3.76±0.62 8.16 12.35 3.04±0.49 9.59 4.70
SAM2.1[2]
Hiera-L SA-V 4.67±0.32 11.75 15.39 3.88±0.62 7.47 11.95 2.87±0.43 8.35 4.51
Hiera-H SA-V 4.44±0.25 10.35 9.48 3.51±0.52 6.78 9.91 2.81±0.39 7.41 4.50
6 Discussion
AreviewofTable4,Figures7and8leadstothefollowingconclusions. First,accordingto∆SB,
baseline strategy underestimates the real-world annotation time from 5% up to 29%. This
impliesthatthebaselinebenchmarkmaysignificantlyunderestimatethereal-worldannotationcosts.
Consequently,ourbenchmarkmaybeemployedforamoreaccurateestimationofannotationcosts.
Then,accordingto∆GR,annotationtimeofusersfromdifferentclickinggroupsvariesfrom3%
upto79%. Theobservedvariationsin∆GRindicatethatsegmentationmethodsareunstablew.r.t.
clickpositionsintheimage.
9AccordingtoSampleand∆GRvalues,thebestannotationtimeisachievedbySAM2Hiera-H(on
DAVIS),CFR-ICL(onTETRIS)andSimpleClickViT-H(onCOCO-MVal). Thetwolattermethods
arelessrobustcomparedtoSAM-likemethods,whichperformmoreconsistentlyacrossclicking
groups. However,SAM2Hiera-HbackboneislessrobustthanHiera-TbackboneonDAVISdataset.
Thisindicatesthatthereiscurrentlynosegmentationmethodthatisoptimalintermsofboth
performanceandrobustnessonalldatasets. Consequently,developersshouldselectamethodin
accordancewiththeirrequirements.
Finally,pointsinthebottom-rightpartofFigure8correspondtoinstanceswithhighNSR.These
values may be utilized to identify and analyze hard instances in the datasets. Additionally,
accordingtoaveragedNSR,weidentifiedthehardestdatasetforannotation,itisDAVISwith
24.15NSR.
7 Conclusion
In this paper, we presented RClicks – a benchmark for interactive segmentation methods, that
evaluatesbothreal-worldqualityandrobustnesswithrespecttodifferentclickingpatterns. Usingthe
developedunbiasedpresentationstrategy,wecollectedthemulti-roundreal-userclickdataset. We
developedtheclickabilitymodelthatcanbeutilizedtoestimateclickprobabilitiesandsamplerealistic
userclicks. Byemployingthismodelinourbenchmark,wedemonstratedthatbaselinestrategymay
overestimatemethods’performanceintherealworld. Furthermore,ouranalysisshowedthatthere
iscurrentlynointeractivesegmentationmethodthatisoptimalintermsofbothperformanceand
robustnessonalldatasets. Additionally,weevaluatedsegmentationmethodsusingreal-userclicks
ofthefirstroundandproposedamethodologytoestimatetheinstancedifficultyforstate-of-the-art
methods. WehopeRClickswillfacilitatetheadvancementofinteractivesegmentationmethodsthat
provideoptimaluserexperiencesinreal-worldscenarios.
References
[1] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,
TeteXiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. arXiv
preprintarXiv:2304.02643,2023. 1,3,5,9
[2] NikhilaRavi,ValentinGabeur,Yuan-TingHu,RonghangHu,ChaitanyaRyali,TengyuMa,
HaithamKhedr,RomanRädle,ChloeRolland,LauraGustafson,etal.Sam2:Segmentanything
inimagesandvideos. arXivpreprintarXiv:2408.00714,2024. 1,9
[3] LeiKe,MingqiaoYe,MartinDanelljan,YifanLiu,Yu-WingTai,Chi-KeungTang,andFisher
Yu. Segmentanythinginhighquality. arXivpreprintarXiv:2306.01567,2023. 1,3,9
[4] ZhaozhiXie,BochenGuan,WeihaoJiang,MuyangYi,YueDing,HongtaoLu,andLeiZhang.
Pa-sam: Promptadaptersamforhigh-qualityimagesegmentation. 2024IEEEInternational
ConferenceonMultimediaandExpo(ICME),2024. 1
[5] ChuanfeiHu,TianyiXia,ShenghongJu,andXindeLi. Whensammeetsmedicalimages: An
investigationofsegmentanythingmodel(sam)onmulti-phaselivertumorsegmentation,2023.
1
[6] TaoZhou,YizheZhang,YiZhou,YeWu,andChenGong. Cansamsegmentpolyps?,2023. 1
[7] RisabBiswas. Polyp-sam++: Canatextguidedsamperformbetterforpolypsegmentation?,
2023. 1
[8] YichiZhangandRushiJiao. Howsegmentanythingmodel(sam)boostmedicalimagesegmen-
tation? arXivpreprintarXiv:2305.03678,2023. 1
[9] Yichi Zhang, Zhenrong Shen, and Rushi Jiao. Segment anything model for medical image
segmentation: Currentapplicationsandfuturedirections. ComputersinBiologyandMedicine,
171:108238,2024. 1
10[10] Maciej A Mazurowski, Haoyu Dong, Hanxue Gu, Jichen Yang, Nicholas Konz, and Yixin
Zhang. Segmentanythingmodelformedicalimageanalysis: anexperimentalstudy. Medical
ImageAnalysis,89:102918,2023. 1
[11] YunhanYang,XiaoyangWu,TongHe,HengshuangZhao,andXihuiLiu. Sam3d: Segment
anythingin3dscenes,2023. 1
[12] MutianXu,XingyilangYin,LingtengQiu,YangLiu,XinTong,andXiaoguangHan.Sampro3d:
Locatingsampromptsin3dforzero-shotscenesegmentation.arXivpreprintarXiv:2311.17707,
2023. 1
[13] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and
YiYang. Segmentandtrackanything. arXivpreprintarXiv:2305.06558,2023. 1
[14] Andrey Moskalenko, Vlad Shakhuro, Anna Vorontsova, Anton Konushin, Anton Antonov,
AlexanderKrapukhin,DenisShepelev,andKonstantinSoshin. Tetris: Towardsexploringthe
robustnessofinteractivesegmentation. arXivpreprintarXiv:2402.06132,2024. 2,3,5,6,9
[15] CarstenRother,VladimirKolmogorov,andAndrewBlake. Grabcut–interactiveforeground
extractionusingiteratedgraphcuts. ACMtransactionsongraphics(TOG),23(3):309–314,
2004. 2,3,5
[16] KevinMcGuinnessandNoelEO’connor. Acomparativeevaluationofinteractivesegmentation
algorithms. PatternRecognition,43(2):434–444,2010. 2,3
[17] Tsung-YiLin, MichaelMaire, SergeBelongie, JamesHays, PietroPerona, DevaRamanan,
PiotrDollár,andCLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InECCV,
2014. 2,3,5,9
[18] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and
AlexanderSorkine-Hornung. Abenchmarkdatasetandevaluationmethodologyforvideoobject
segmentation. InCVPR,2016. 2,3,5,9
[19] Konstantin Sofiiuk, Ilya A. Petrov, and Anton Konushin. Reviving iterative training with
maskguidanceforinteractivesegmentation. In2022IEEEInternationalConferenceonImage
Processing(ICIP),pages3141–3145,2022. doi: 10.1109/ICIP46576.2022.9897365. 2,3,5,6,
7,9
[20] EirikurAgustsson,JasperR.R.Uijlings,andVittorioFerrari. Interactivefullimagesegmenta-
tionbyconsideringallregionsjointly. InCVPR,2019. 3
[21] Houssem-EddineGueziri,MichaelMcGuffin,andCatherineLaporte. Latencymanagementin
scribble-basedinteractivesegmentationofmedicalimages. IEEETransactionsonBiomedical
Engineering,2017. 3
[22] PolinaPopenova,DanilGaleev,AnnaVorontsova,andAntonKonushin. Contour-basedinter-
activesegmentation. InProceedingsoftheThirty-SecondInternationalJointConferenceon
ArtificialIntelligence,pages1322–1330,2023. 3
[23] HangCheng, ShugongXu, XiufengJiang, andRongrongWang. Deepimagemattingwith
flexibleguidanceinput. InBMVC,2021. 3
[24] NingXu,BrianPrice,ScottCohen,JimeiYang,andThomasSHuang. Deepinteractiveobject
selection. InCVPR,2016. 3
[25] QinLiu,ZhenlinXu,GedasBertasius,andMarcNiethammer. Simpleclick: Interactiveimage
segmentationwithsimplevisiontransformers. arXivpreprintarXiv:2210.11006,2022. 3,5,9
[26] Shoukun Sun, Min Xian, Fei Xu, Tiankai Yao, and Luca Capriotti. Cfr-icl: Cascade-
forwardrefinementwithiterativeclicklossforinteractiveimagesegmentation. arXivpreprint
arXiv:2303.05620,2023. 3,9
11[27] MinghaoZhou,HongWang,QianZhao,YuexiangLi,YawenHuang,DeyuMeng,andYefeng
Zheng. Interactive segmentation as gaussion process classification. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages19488–19497,2023.
3,9
[28] ChaoningZhang,DongshenHan,YuQiao,JungUkKim,Sung-HoBae,SeungkyuLee,and
ChoongSeonHong. Fastersegmentanything: Towardslightweightsamformobileapplications.
arXivpreprintarXiv:2306.14289,2023. 3,9
[29] DavidMartin,CharlessFowlkes,DoronTal,andJitendraMalik. Adatabaseofhumanseg-
mentednaturalimagesanditsapplicationtoevaluatingsegmentationalgorithmsandmeasuring
ecologicalstatistics. InICCV,2001. 3,5
[30] M.Everingham,L.VanGool,C.K.I.Williams,J.Winn,andA.Zisserman. ThePASCAL
VisualObjectClassesChallenge2012(VOC2012)Results,2012. 3
[31] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactive image segmentation with latent
diversity. InCVPR,2018. 3
[32] BharathHariharan,PabloArbelaez,LubomirBourdev,SubhransuMaji,andJitendraMalik.
Semanticcontoursfrominversedetectors. InICCV,2011. 3,9
[33] JianxunLou,HanheLin,DavidMarshall,DietmarSaupe,andHantaoLiu. Transalnet: Towards
perceptuallyrelevantvisualsaliencyprediction. Neurocomputing,2022. ISSN0925-2312. doi:
https://doi.org/10.1016/j.neucom.2022.04.080. 3
[34] AlexanderKroner, MarioSenden, KurtDriessens, andRainerGoebel. Contextualencoder-
decoder network for visual saliency prediction. Neural Networks, 129:261–270, 2020.
ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2020.05.004. URL http://www.
sciencedirect.com/science/article/pii/S0893608020301660. 3
[35] TilkeJudd,KristaEhinger,FrédoDurand,andAntonioTorralba. Learningtopredictwhere
humanslook. In2009IEEE12thInternationalConferenceonComputerVision,pages2106–
2113,2009. doi: 10.1109/ICCV.2009.5459462. 3
[36] TilkeJudd,FrédoDurand,andAntonioTorralba. Abenchmarkofcomputationalmodelsof
saliencytopredicthumanfixations. InMITTechnicalReport,2012. 3
[37] AliBorjiandLaurentItti. Cat2000: Alargescalefixationdatasetforboostingsaliencyresearch.
CVPR2015workshopon"FutureofDatasets",2015. arXivpreprintarXiv:1505.03581. 3
[38] MingJiang,ShengshengHuang,JuanyongDuan,andQiZhao. Salicon: Saliencyincontext.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
1072–1080,2015. 3
[39] HamedRTavakoli,FawadAhmed,AliBorji,andJormaLaaksonen.Saliencyrevisited:Analysis
ofmousemovementsversusfixations. InProceedingsoftheieeeconferenceoncomputervision
andpatternrecognition,pages1774–1782,2017. 3
[40] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,Piotr
Dollár,andCLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InComputer
Vision–ECCV2014: 13thEuropeanConference,Zurich,Switzerland,September6-12,2014,
Proceedings,PartV13,pages740–755.Springer,2014. 3
[41] GabrielPeyréandMarcoCuturi. Computationaloptimaltransport,2020. 5
[42] Sam Christian. Re-examining the evidence of the Hercules–Corona Borealis Great Wall.
MonthlyNoticesoftheRoyalAstronomicalSociety,495(4):4291–4296,052020. ISSN0035-
8711. doi: 10.1093/mnras/staa1448. 5
[43] JohnAPeacock. Two-dimensionalgoodness-of-fittestinginastronomy. MonthlyNoticesofthe
RoyalAstronomicalSociety,202(3):615–627,1983. 5
12[44] GiovanniFasanoandAlbertoFranceschini. Amultidimensionalversionofthekolmogorov–
smirnov test. Monthly Notices of the Royal Astronomical Society, 225(1):155–170, 1987.
5
[45] JianxunLou,HanheLin,DavidMarshall,DietmarSaupe,andHantaoLiu. Transalnet: Towards
perceptuallyrelevantvisualsaliencyprediction. Neurocomputing,494:455–467,2022. 6
[46] Meng-HaoGuo,Cheng-ZeLu,QibinHou,ZhengningLiu,Ming-MingCheng,andShi-MinHu.
Segnext: Rethinkingconvolutionalattentiondesignforsemanticsegmentation. arXivpreprint
arXiv:2209.08575,2022. 6
[47] MatthiasKummerer,ThomasS.A.Wallis,andMatthiasBethge. Saliencybenchmarkingmade
easy: Separatingmodels,mapsandmetrics. InProceedingsoftheEuropeanConferenceon
ComputerVision(ECCV),September2018. 7
[48] Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion
forinteractivesegmentation. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages7345–7354,2021. 9
[49] Jiacheng Lin, Jiajun Chen, Kailun Yang, Alina Roitberg, Siyu Li, Zhiyong Li, and Shutao
Li. Adaptiveclick: Click-aware transformer with adaptive focal loss for interactive image
segmentation. IEEETransactionsonNeuralNetworksandLearningSystems,2024. 9
13Checklist
1. Forallauthors...
(a) Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthepaper’s
contributionsandscope? [Yes] SeeIntroduction1andConclusion7.
(b) Didyoudescribethelimitationsofyourwork? [Yes] SeeAppendixD.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] See
AppendixD.
(d) Haveyoureadtheethicsreviewguidelinesandensuredthatyourpaperconformsto
them? [Yes]
2. Ifyouareincludingtheoreticalresults...
(a) Didyoustatethefullsetofassumptionsofalltheoreticalresults? [N/A]
(b) Didyouincludecompleteproofsofalltheoreticalresults? [N/A]
3. Ifyouranexperiments(e.g. forbenchmarks)...
(a) Didyouincludethecode,data,andinstructionsneededtoreproducethemainexperi-
mentalresults(eitherinthesupplementalmaterialorasaURL)?[Yes] Weincludeit
insupplementalmaterial.
(b) Didyouspecifyallthetrainingdetails(e.g.,datasplits,hyperparameters,howthey
werechosen)? [Yes] SeeAppendixA.
(c) Didyoureporterrorbars(e.g.,withrespecttotherandomseedafterrunningexperi-
mentsmultipletimes)? [Yes] SeeTable4,AppendixC.
(d) Didyouincludethetotalamountofcomputeandthetypeofresourcesused(e.g.,type
ofGPUs,internalcluster,orcloudprovider)? [Yes] SeeAppendicesA,C.
4. Ifyouareusingexistingassets(e.g.,code,data,models)orcurating/releasingnewassets...
(a) Ifyourworkusesexistingassets,didyoucitethecreators? [Yes]
(b) Didyoumentionthelicenseoftheassets? [Yes] SeeAppendixD.
(c) DidyouincludeanynewassetseitherinthesupplementalmaterialorasaURL?[Yes]
Seethesupplementary
(d) Didyoudiscusswhetherandhowconsentwasobtainedfrompeoplewhosedatayou’re
using/curating? [Yes] SeeAppendixB.
(e) Didyoudiscusswhetherthedatayouareusing/curatingcontainspersonallyidentifiable
informationoroffensivecontent? [N/A]
5. Ifyouusedcrowdsourcingorconductedresearchwithhumansubjects...
(a) Didyouincludethefulltextofinstructionsgiventoparticipantsandscreenshots,if
applicable? [Yes] SeeAppendixBforanexampleoftextualinstruction.
(b) Did you describe any potential participant risks, with links to Institutional Review
Board(IRB)approvals,ifapplicable? [N/A]
(c) Didyouincludetheestimatedhourlywagepaidtoparticipantsandthetotalamount
spentonparticipantcompensation? [Yes] SeeAppendixB.
14RClicks: Realistic Click Simulation
for Benchmarking Interactive Segmentation
SupplementaryMaterials
AntonAntonov1∗# AndreyMoskalenko1,2∗ DenisShepelev1∗
AlexanderKrapukhin1 KonstantinSoshin1 AntonKonushin1 VladShakhuro1†
1AIRI,Moscow,Russia 2LomonosovMoscowStateUniversity
{lastname}@airi.net
∗Equalcontribution †Projectleader #Correspondingauthor
§https://github.com/emb-ai/rclicks
A ClickabilityModel
A.1 Trainingandmodeldetails
Inthissubsection,weprovideclickabilitymodelarchitecturedescriptionanditstrainingdetails.
Architecture details. As a predictor, we adapted the state-of-the-art SegNeXt-B segmentation
network [1] with the MSCAN-B backbone. We input the original image into the network and
concatenatethegroundtruthmaskwiththeerrormask,feedingtheresultingtensorasanadditional
inputtothenetwork.ThistechniqueisinspiredbyConv1S[2].However,weusedthreeconvolutional
layerswithnon-linearactivationfunctionsinsteadofone,asweneededtoencodemorecomplex
featuresofthegroundtruthmaskanderrormask,ratherthanjustclicks. Additionally,afteraforward
pass,weusedmin-maxnormalizationtotransformvaluestotheintervalfrom0to1.
To compare our model’s complexity with state-of-the-art interactive segmentation methods, we
measuredinferencespeedandresourceusageofthestate-of-the-artsegmentationmethodsandour
clickabilitymodelinTable5. AllevaluationsweredoneonasingleA100. Theclickabilitymodelis
abouttwiceasfastasRITMandSimpleClick,andaboutfivetimesfasterthanSAM.Italsousesa
smallorcomparableamountofGPUmemory. NotethatourmodelcaneasilyfitonaconsumerGPU
with8GBofVRAM.
Trainingdetails. Wetrainedourmodelfor20epochsonthetrainpartofTETRISdataset(TETRIS
splitscanbefoundintxt-fileofthebenchmarkcode). Duringtraining,sampleswereaugmented
usingHorizontalFlip. WeminimizedKullback-LeiblerDivergenceLossbyAdamoptimizerwith
CosineLRScheduler and initial 0.01 learning rate. The training process took 3 hours on a single
NvidiaTeslaA100GPU.
Table5: Comparisonofthecharacteristicsofdifferentinteractivesegmentationmethodswiththe
clickabilitymodel.
Model Params,M GFLOPs Mem,Gb Inference,ms Inputsize
RITMHRNet-32 31.95 83 0.217 42±2.0 400×400
SimpleClick-ViT-H 659.39 1461 2.785 41±2.3 448×448
SAM-ViT-H 641.09 5473 5.745 150±2.7 1024×1024
Clickabilitymodel 27.59 64 0.378 23±0.9 416×416
15A.2 Modelgeneralizability
Table6: Evaluationofvariousclickabilitymodelsonreal-userclicks. Ourmodelwastrainedonthe
trainpartofTETRISdataset. Ourapproachoutperformsbaselineclickingstrategiesintermsofthe
proximityofsamplestoreal-userclicksonallconsideredinteractivesegmentationdatasets.
Dataset Model KS↑ PL ↓ WD↓ NSS↑ PDE↑
1
UD 0.15 0.55 0.16 2.44 4.06E-05
DT 0.22 0.49 0.14 3.94 7.75E-05
GrabCut SI 0.10 0.52 0.17 2.27 4.64E-05
SM 0.14 0.52 0.16 2.75 5.30E-05
Ours 0.50 0.40 0.10 5.63 2.12E-04
UD 0.16 0.54 0.16 3.39 1.20E-04
DT 0.23 0.48 0.14 5.28 2.13E-04
Berkeley SI 0.11 0.51 0.17 3.08 1.38E-04
SM 0.17 0.51 0.15 3.69 1.50E-04
Ours 0.50 0.39 0.09 7.29 4.99E-04
UD 0.15 0.56 0.17 4.31 6.49E-05
DT 0.25 0.49 0.14 6.68 1.21E-04
DAVIS SI 0.11 0.53 0.17 3.96 7.22E-05
SM 0.16 0.53 0.16 4.50 7.84E-05
Ours 0.48 0.41 0.10 8.55 2.53E-04
UD 0.20 0.79 0.21 6.08 4.04E-04
DT 0.26 0.73 0.19 8.34 5.74E-04
COCO-MVal SI 0.16 0.73 0.22 6.26 5.12E-04
SM 0.21 0.74 0.20 6.80 5.14E-04
Ours 0.50 0.61 0.14 10.44 7.29E-04
UD 0.10 0.57 0.17 3.99 1.36E-05
DT 0.14 0.52 0.16 6.45 2.76E-05
TETRIS(Val) SI 0.09 0.51 0.16 4.13 1.66E-05
SM 0.13 0.51 0.15 4.79 1.83E-05
Ours 0.55 0.40 0.08 9.11 4.69E-05
Toassessthegeneralizabilityofourclickabilitymodel,wecomputedmetricsonclicksamplesfor
differentdatasetsinTable6.
Note, that in Tables 3, 6 and 7 KS, PL , WD were calculated using clicks bootstrapping from
1
clickabilitymodel(100timesperinstance).
Additionally,tocalculatesaliencyfairly,weassumethemodelshouldreceivenotthefullimage,but
onlyacroppedregionoftheimagewiththeobjectofinterest. Thisensuresthatthemodel’sattention
isdirectedtowardstherelevantarea,leadingtoamoreaccurateassessmentofitssaliency. Wehave
checkedtwovariantsofgivingimagewithobjectofinterest:
(1) Part of image with expand around the object of interest on 1.4 times (denoted as SI in
Table6).
(2) Partofimagewithexpandaroundtheobjectofintereston1.4times,buteverythingexcept
theobjectofinterestisgray(denotedasSMinTable6).
16AccordingtoTable6,ourclickabilitymodelsignificantlyoutperformsallbaselinesonTETRISand
otherdatasets. Onotherdatasetstheclicksamplingqualityofourmodelmostlyisslightlyworse
thanonTETRIS,neverthelesswefindtheseresultscomparable. Therefore,weconcludethatthe
clickabilitymodelisnotlimitedtoTETRISdatasetandgeneralizeswelltootherdatasetsofcommon
images.
However,datasetsfromspecificdomains,e.g. medical,areoutofthescopeofourwork. Forthese
datasets,additionaldatacollectionandevaluationsshouldbedone.
A.3 Clickabilitymapsablations
Figure9illustratesexamplesofclickabilitymapswithvarioussigmas(denotedCM ). Foreach
σ
sigma,wetrainedaseparatemodelandchosethebestoneaccordingtoourablationsonavalidation
setofclicksandimages,Table7containsevaluationresultsforourclickabilitymodelswithvarious
sigmas. SinceCM performedbestonalmostallmetrics,weuseitinourinteractivesegmentation
5
benchmark.
(a)Clicks (b)CM (c)CM (d)CM (e)CM
10 30 60 120
Figure 9: Example of ground-turth clickability maps (b) – (e) with various σ (denoted CM )
σ
constructedfromground-truthclicksandobjectmask(a)usedtotrainclickabilitymodel.
Table7: Evaluationofourclickabilitymodelwhentrainingonground-turthclickabilitymapswith
variousσ(denotedCM )onvalidationpartofTETRIS.
σ
Traindata KS↑ PL ↓ WD↓ NSS↑ PDE↑
1
CM 0.48 0.408 0.09 8.61 4.44E-05
0.01
CM 0.46 0.416 0.10 8.58 4.40E-05
0.25
CM 0.49 0.411 0.09 8.87 4.70E-05
0.5
CM 0.51 0.407 0.09 8.94 4.86E-05
1
CM 0.53 0.399 0.09 9.05 4.83E-05
2
CM 0.55 0.397 0.08 9.11 4.69E-05
5
CM 0.51 0.403 0.09 8.87 4.17E-05
10
CM 0.51 0.403 0.09 8.24 3.11E-05
20
CM 0.48 0.404 0.09 7.71 2.48E-05
30
CM 0.37 0.424 0.10 6.60 1.67E-05
60
CM 0.31 0.439 0.10 6.06 1.40E-05
90
CM 0.26 0.453 0.11 5.69 1.26E-05
120
A.4 Examplesofgeneratedclicks
Figures 10, 11, 12 present examples of the collected ground truth clicks, generated clicks and
predictedclickabilitymapsbyourclickabilitymodel.
17(a)GrabCut,KS-test–True (b)GrabCut,KS-test–False
(c)Berkeley,KS-test–True (d)Berkeley,KS-test–False
(e)DAVIS,KS-test–True (f)DAVIS,KS-test–False
(g)COCO-MVal,KS-test–True (h)COCO-MVal,KS-test–False
(i)TETRIS,KS-test–True (j)TETRIS,KS-test–False
Figure 10: Examples of ground truth clicks (left), clicks (middle) and clickability map (right),
generated by our clickability model for the first round. Green dots illustrate first round clicks,
teal masks represent target regions that should be segmented. In the subcaptions the results of
Kolmogorov-Smirnovtest(Trueifp-value>0.05,i.e. therearenosignificantdifferencesbetween
thedistributionsofclicks)areprovided.
18(a)GrabCut,KS-test–True (b)GrabCut,KS-test–False
(c)Berkeley,KS-test–True (d)Berkeley,KS-test–False
(e)DAVIS,KS-test–True (f)DAVIS,KS-test–False
(g)COCO-MVal,KS-test–True (h)COCO-MVal,KS-test–False
(i)TETRIS,KS-test–True (j)TETRIS,KS-test–False
Figure11: Examplesoffalse-positive(FP)groundtruthclicks(left),generatedclicks(middle)and
predicted clickability map (right) by our clickability model for the subsequent round. Red dots
illustrateFPclicks,redmasksrepresentregionsofsegmentationerrors. Inthesubcaptionstheresults
ofKolmogorov-Smirnovtest(Trueifp-value>0.05,i.e. therearenosignificantdifferencesbetween
thedistributionsofclicks)areprovided.
19(a)GrabCut,KS-test–True (b)GrabCut,KS-test–False
(c)Berkeley,KS-test–True (d)Berkeley,KS-test–False
(e)DAVIS,KS-test–True (f)DAVIS,KS-test–False
(g)COCO-MVal,KS-test–True (h)COCO-MVal,KS-test–False
(i)TETRIS,KS-test–True (j)TETRIS,KS-test–False
Figure12: Examplesoffalse-negative(FN)groundtruthclicks(left),generatedclicks(middle)and
predictedclickabilitymap(right)byourclickabilitymodelforthesubsequentround. Greendots
illustrateFNclicks,tealmasksrepresenttargetregionsthatshouldbesegmented. Inthesubcaptions
theresultsofKolmogorov-Smirnovtest(Trueifp-value>0.05,i.e.therearenosignificantdifferences
betweenthedistributionsofclicks)areprovided.
20B ClicksDataset
B.1 Collectionprocedure
Beforestartingdatacollection,weconsideredvariouswaystovisuallypresentthemaskandimageto
aperson. Theseareallthemethodswefoundreasonable.
Hereisabriefstorybehindchoosingdisplaymode:
1. ThemostunbiaseddisplaymodetoinstructaparticipantisconsideredtobeTextDescription.
Byavoidinganyvisualdisplayofthemask,weeliminatepotentialbiases. However,we
couldnotuseittoannotateallimagesbecausesuchdescriptionscanbeambiguous. Wealso
cannotusethismethodforsubsequentroundsbecauseweneedtoindicatethepartofthe
maskthatwasnotsegmentedinthepreviousround.
2. Thenextdisplaymodewetriedwastoshowthegroundtruthblack-and-whitemask(Sil-
houetteMask). Inthatcase,wesimplyshowthemaskasiswithoutanymodifications. We
collectedthefirstbatchofdataandnoticedthatparticipantstendtoclicktothegeometric
centeroftheobject. Therefore,thisstrategyseemedtobebiased. Westartedtolookforthe
leastunbiasedone.
3. Topreventpeoplefromselectingthegeometriccenteroftheobject,weaddedObjectCutOut
mode. Wehypothesizethatitishardertobebiasedonthegeometricformofanobjectwhen
itretainsitsappearance.
4. Afterthat,wetriedShiftedCutOutmode.Theideaofthismodewastoeliminatedependency
oninitialcursorposition. Inthatdisplaymode,apersonhastomovethecursortotheobject.
5. We also tried Highlighted Instance mode, since it’s a natural way to highlight an object
retainingits’background. Wehypothesizedthatobjectbackgroundmayinfluenceclick
position.
Indeterminingtheoptimaldisplaydurationforourdatasetcollection,weconductedathoroughreview
ofexistingliteratureonannotationtimerequirements. Anumberofstudieshavebeenconductedon
thissubject [3](Sec. 4), [4](Sec. E.1.2), [5](Sec. 2), [6](Sec. 3.4), [7](Sec. 4.1). Researchers
cametotheconclusionthattherequiredtimeplanformulaisthefollowing: 1sec. fortheannotator
tovisuallylocateanobjectand1.5sec. foraddingeachclick,whiletotalreportedtimetolocalize
andclickmayvarybetween1.87sec. and2.5sec. Since,wedidnothavethegoalofspeedingup
thelabelingofRClicks,buttocollecthigh-qualitydata,weincreasedtimesasfollows: showingan
imagefor1.5sec.,differentmaskdisplayingmodesfor2sec. (sincetimeisneededtolocalizeand
rememberthetargetobject)andTextDescriptionmodefor2.5sec. (sincemoretimeisneededto
readandunderstandthetext).
Toannotatedatasets,weusedthetoloka.aicrowdsourcingplatform. Eachcrowdworkerwaspaid
$0.02for10clicks. Onaverage,workersmade13clicksperminute,earningapproximately$1.5per
hour. Thislevelisaboveminimumhourwageforthecountriestheannotatorswerefrom. Overallwe
spentabout1300$onannotation.
According to the user agreement and privacy policy of Toloka, personal data typically includes
informationthatcanidentifyanindividual,suchasname,contactinformation,andotherpersonal
identifiers. Annotatorsclicksonimagesdonotfallunderthiscategory. Moreover,weprovidefully
anonymizeddata,thatcannotbelinkedwithpeoplewhoclicked. Toloka’spolicyallowsforthe
sharingofanonymizeddatawiththirdparties. Ifthecollectedclickdataisanonymizedandcannotbe
tracedbacktoindividualannotators,itmaybesharedwiththirdpartieswithoutviolatingtheprivacy
terms.
Hereisanexampleofaninstruction,whichannotatorssaw,forObjectCutOutmode:
21Instructions
1. Afteropeningthetask,therewillbeanimageloadingperiodofapproximately
30seconds.
2. Afterclickingthe“START”button,animagewillbeshownfor1.5seconds,
followedbyademonstrationoftheobjectofinterestonagraybackground
for2seconds. Notethatduringthistime,youcannotclickontheobject!
Example:
3. Then you will see the original image again, which will displayed for 1.5
second. Yourtaskistoselecttheobjectthatwasindicatedintheprevious
stepwithoneclick.
4. Tosuccessfullycompletethetask,itisnecessarytoprocess10imagesin
sequence.
22B.2 ComparisonPCandmobileclicks
Table8presenttheaveragecomparisonmetricsclicks Table 8: Comparison clicks collected from
collectedfromPCandmobiledevices. Weaveraged PCandmobiledevices
onlyclicksforonlyinstancesthathavemorethan10
clicksforbothdevices. AccordingtoKScolumn,for Dataset KS↑ WD↓ PL ↓
1
60%ofthecomparedinstances,therewerenosignif-
icantdifferencebetweenPCandmobiledistributions. GrabCut 0.63 0.26 0.54
InFigure13weprovideexamplesthatillustrateboth
Berkeley 0.64 0.41 0.79
significantdifferencesandcaseswithoutsignificant
DAVIS 0.62 0.58 1.08
differencesbetweenPCandmobile.
COCO-MV 0.68 1.15 2.22
TETRIS 0.70 0.41 0.70
(a)Firstround,KS-test–True (b)Firstround,KS-test–False
(c)Subsequentround(FP),KS-test–True (d)Subsequentround(FP),KS-test–False
(e)Subsequentround(FN),KS-test–True (f)Subsequentround(FN),KS-test–False
Figure13: Examples ofcollectedclicksfor PC(left)andmobile (right) devicesfor thefirstand
subsequentrounds. Greendotsillustratethefirstroundandsubsequentfalse-negative(FN)clicks,red
dots–thesubsequentfalse-positive(FP)clicks. Tealmasksrepresenttargetregionsthatshouldbe
segmented,redmasks–regionsofsegmentationerrors. InthesubcaptionstheresultsofKolmogorov-
Smirnovtest(Trueifp-value>0.05,i.e. therearenosignificantdifferencesbetweenthedistributions
ofclicks)areprovided.
23C AdditionalBenchmarkResults
C.1 Evaluationsetup
Webenchmarked11methodswith33checkpoints. Todothis,wespent2400GPUhours,whichis
equivalenttoapproximately6daysofcomputeusing16NVIDIATeslaA100GPUs.
C.2 Additionalevaluationresults
Tables9,10,11presentevaluationresultsonsimulateduserclicksofvariousinteractivesegmentation
algorithms on GrabCut, Berkeley, DAVIS, COCO-MVal and TETRIS datasets for NoC @90,
20
NoF @90andIoU-AuC respectively. Table12presentstheevaluationresultsonthefirstround
20 20
realandsimulateduserclicks.
Basestatisticsrepresenttheperformanceofthemethodsaccordingtobaselinebenchmark.Inaddition
tothestatisticspresentedinthemainpaper,wecalculated∆HHstatistic–thisstatisticisbasedon
∆GR,butfor∆HHwemergeintervals{G }5 and{G }10 s.t. everyintervalhas50%oftotal
i i=1 i i=6
probabilitymass.Moreover,wecalculatednoise-signal-ratio(NSR)ofIoUaveragedoverthedatasets
instances.
Spearman Correlation
GR NoC(%) 1.000.940.740.710.610.520.860.850.720.450.560.730.140.360.650.400.540.600.610.280.210.200.400.420.450.46 1.0
HH NoC(%) 0.881.000.760.680.620.510.780.820.710.510.590.780.160.370.650.450.570.660.660.140.070.060.260.280.310.31
SB NoC(%) 0.590.661.000.530.540.570.540.520.760.510.650.830.160.420.720.420.570.680.670.010.100.110.150.160.170.17
GR NoF(D) 0.690.630.421.000.790.580.780.770.680.480.540.620.180.440.780.440.560.550.560.160.110.090.370.370.390.38
HH NoF(D) 0.550.540.390.751.000.580.630.640.640.380.460.600.120.390.800.320.440.560.570.130.080.070.310.320.330.33
SB NoF(D) 0.420.380.420.440.391.000.470.480.590.570.650.600.220.720.730.520.590.510.540.120.090.080.190.210.210.21 0.8
GR IoU(D) 0.850.680.440.690.530.371.000.980.720.380.460.580.130.310.610.380.520.550.560.430.360.350.560.580.610.62
HH IoU(D) 0.810.760.410.680.580.340.911.000.720.400.470.590.130.310.620.400.540.570.580.390.330.320.520.530.570.57
SB IoU(D) 0.600.600.690.630.570.520.590.621.000.550.660.760.220.480.770.490.690.790.800.090.010.000.320.330.320.32
Base NoC@20 0.400.460.420.400.280.470.300.350.551.000.980.790.590.770.600.890.890.640.660.300.300.320.040.020.020.04
Sample NoC@20 0.460.510.540.430.320.530.360.400.640.971.000.860.530.770.680.860.900.690.720.250.270.290.080.070.040.02 0.6
Sample STD NoC@20 0.660.720.700.540.460.500.510.540.680.790.831.000.280.540.790.650.760.790.800.160.210.220.210.200.170.16
Base NoF@20 0.170.180.120.190.120.210.150.160.260.610.580.360.800.700.250.580.540.250.290.090.080.090.030.030.030.04
Sample NoF@20 0.260.290.330.270.220.630.220.200.430.740.740.530.711.000.620.730.730.470.510.030.030.050.030.040.040.04
Sample STD NoF@20 0.620.590.600.700.690.680.570.570.730.540.590.730.260.551.000.510.640.700.720.050.000.010.310.320.300.30
Base IoU@20 0.340.390.390.350.230.440.260.290.460.870.870.690.570.680.431.000.970.690.720.270.280.300.020.040.060.08 0.4
Sample IoU@20 0.430.490.510.430.320.480.370.400.630.890.910.770.550.670.540.961.000.800.820.200.240.250.070.060.040.02
Sample STD IoU@20 0.410.550.610.420.370.390.300.360.620.650.680.760.270.410.540.700.771.000.990.270.330.340.040.040.060.06
Sample NSR IoU@20 0.450.580.620.450.390.420.340.400.650.690.720.790.300.450.580.740.810.991.000.230.290.300.010.010.010.01
Base IoU@1 0.380.170.000.260.210.150.520.430.150.310.250.140.120.090.180.300.240.390.351.000.980.980.720.750.840.86
User IoU@1 0.300.080.140.180.160.120.460.360.020.320.290.210.090.080.110.340.310.490.450.951.001.000.720.750.840.85 0.2
Sample IoU@1 0.300.080.150.170.160.120.460.370.020.330.300.220.100.090.100.350.320.500.450.950.991.000.710.740.830.84
User STD IoU@1 0.450.210.030.450.400.180.640.530.310.020.010.110.020.030.380.090.000.230.160.710.720.711.000.990.960.95
Sample STD IoU@1 0.450.210.040.440.390.180.630.530.310.050.020.090.040.040.370.130.030.250.180.750.760.750.991.000.970.96
User NSR IoU@1 0.470.250.030.400.350.160.670.560.270.100.060.060.030.030.340.160.070.290.230.820.820.810.940.951.001.00
Sample NSR IoU@1 0.480.250.040.390.350.160.670.560.260.120.090.040.040.040.330.190.090.300.240.830.840.830.930.950.991.00
0.0
Metrics
Figure 14: Pearson\Spearman correlations of considered averaged performance and robustness
metricsstatisticsaveragedoverallinstancesinalldatasets.
Figure14showsthecorrelationsbetweendifferentmetrics. Fromthis,wecandrawseveralconclu-
sions:
1. ThereisastrongcorrelationbetweenSampleandUserperformancesonthefirstclick,as
seeninSampleIoU@1andUserIoU@1,aswellasSampleNSRIoU@1andUserNSR
IoU@1. Thisdemonstratesthatperformanceonrealandsimulatedclicksisverysimilar.
2. Thereisaweakcorrelationbetweenperformanceonthefirstandtwentiethclicks,asseenin
theSampleIoU@1withSampleIoU@20andBaseIoU@1withBaseIoU@20. Thislikely
indicatesthatallmodelsconvergebythe20thround.
3. Thereisaweakcorrelationbetweenrobustnessmetricsandaveragemodelperformance.
Thisisobservedin: ∆GRNoCwith(BaseNoC@20andSampleNoC@20),∆HHNoC
with(BaseNoC@20andSampleNoC@20),∆GRIoUwith(BaseIoU@20andSample
IoU@20),and∆HHIoUwith(BaseIoU@20andSampleIoU@20). Thissuggeststhat
high-performancemethodsmaynotberobust.
24
noitalerroC
nosraeP
)%(CoN
RG
)%(CoN
HH
)%(CoN
BS
)D(FoN
RG
)D(FoN
HH
)D(FoN
BS
)D(UoI
RG
)D(UoI
HH
)D(UoI
BS
02@CoN
esaB
02@CoN
elpmaS
02@CoN
DTS
elpmaS
02@FoN
esaB
02@FoN
elpmaS
02@FoN
DTS
elpmaS
02@UoI
esaB
02@UoI
elpmaS
02@UoI
DTS
elpmaS
02@UoI
RSN
elpmaS
1@UoI
esaB
1@UoI
resU
1@UoI
elpmaS
1@UoI
DTS
resU
1@UoI
DTS
elpmaS
1@UoI
RSN
resU
1@UoI
RSN
elpmaS
)sba(
noitalerroC4. Performance measures have high correlations with each other, as seen in the cross-
correlationsbetweenSampleNoC,SampleIoU,SampleNoFandBaseNoC,BaseIoU,
BaseNoF.
5. The same phenomenon is observed with robustness metrics, demonstrated by the cross-
correlationsbetween∆HHNoC,∆GRNoCand∆HHIoU,∆GRIoU.
2526
stlusernoitaulaveCoN
:9elbaT
]21[SIRTET
]11[laVM-OCOC
]01[SIVAD
]9[yelekreB
]8[tuCbarG
09@02CoN
09@02CoN
09@02CoN
09@02CoN
09@02CoN
ataD
ledoM
dohteM
HH∆RG∆BS∆
elpmaS
HH∆RG∆BS∆
elpmaS
HH∆RG∆BS∆
elpmaS
HH∆RG∆BS∆
elpmaS
HH∆RG∆BS∆
elpmaS
esaB
esaB
esaB
esaB
esaB
)%+(
)%+(
)%+(
)dts±(
)%+(
)%+(
)%+(
)dts±(
)%+(
)%+(
)%+(
)dts±(
)%+(
)%+(
)%+(
)dts±(
)%+(
)%+(
)%+(
)dts±(
09.22
34.6555.91
93.3
97.0±78.3
20.03
00.9734.62
50.4
13.1±47.4
13.91
56.3588.61
49.5
58.0±44.6
35.42
31.7531.22
16.2
38.0±60.3
83.02
95.1685.31
08.1
94.0±40.2
L+C
05NR
]31[SICPG
52.71
60.4461.41
38.2
35.0±01.3
65.22
97.9451.51
88.3
58.0±31.4
65.81
88.5459.41
65.5
37.0±59.5
30.91
71.9420.01
60.2
06.0±33.2
83.02
75.2598.21
25.1
83.0±27.1
L+C
43NR
]41[teNDC
72.22
22.5580.71
81.4
67.0±15.4
81.42
85.1568.02
88.5
92.1±63.6
90.62
59.4693.32
98.6
52.1±78.7
67.22
04.5584.02
72.3
79.0±28.3
98.12
37.6675.11
81.2
64.0±23.2
DBS
43NR
67.7
59.31
20.7
75.3
25.0±96.3
55.01
22.0272.01
85.3
87.0±17.3
95.6
31.61
29.6
00.6
76.0±32.6
66.21
50.1268.31
84.2
56.0±18.2
10.41
38.03
87.5
07.1
82.0±86.1
L+C
81RH
68.41
66.2397.51
64.3
76.0±08.3
64.61
98.3318.61
33.3
29.0±56.3
17.91
51.4588.02
89.5
99.0±17.6
54.61
57.1343.42
06.2
97.0±21.3
82.81
29.4489.21
86.1
04.0±28.1
L+C
TI-s81RH
42.11
99.3295.11
22.3
06.0±84.3
01.71
10.7348.51
89.2
38.0±22.3
83.31
41.1373.11
47.5
38.0±51.6
50.7
52.4267.41
62.2
35.0±16.2
54.9
04.1316.11
45.1
92.0±07.1
L+C
TI-81RH
]2[MTIR
80.31
96.0374.71
60.3
56.0±44.3
13.81
13.7305.51
79.2
38.0±42.3
69.12
70.1543.81
43.5
98.0±09.5
27.31
25.7229.51
01.2
55.0±53.2
01.41
76.74
38.9
65.1
63.0±46.1
L+C
TI-23RH
47.11
13.5269.11
83.4
47.0±08.4
74.91
32.3471.71
93.4
42.1±18.4
41.71
93.8358.71
17.6
30.1±24.7
99.11
95.1208.51
22.3
97.0±17.3
74.6
16.12
01.7
40.2
43.0±91.2
DBS
TI-81RH
28.6
49.21
99.6
15.2
73.0±26.2
18.9
57.91
44.9
78.2
85.0±39.2
55.8
41.51
06.8
18.4
04.0±79.4
33.7
12.2174.71
38.1
34.0±51.2
34.4
71.61
30.2-
84.1
61.0±04.1
L+C
B-TiV
]51[kcilCtpadA
82.8
44.61
04.8
13.3
05.0±94.3
59.51
70.5395.41
21.4
60.1±33.4
49.9
49.71
96.8
51.5
94.0±73.5
65.7
36.2174.01
81.2
73.0±93.2
60.3
08.21
78.3
44.1
22.0±24.1
DBS
B-TiV
58.8
46.61
68.8
75.2
14.0±37.2
19.11
06.3227.11
29.2
07.0±70.3
59.01
33.62
50.9
60.5
45.0±23.5
55.01
43.32
33.7
79.1
04.0±10.2
46.6
05.22
37.5
84.1
03.0±95.1
L+C
B-TiV
60.7
10.01
11.7
83.2
53.0±64.2
82.01
88.02
50.8
36.2
65.0±76.2
97.7
76.61
17.8
18.4
24.0±30.5
82.6
57.6
21.5
98.1
92.0±38.1
08.9
31.22
82.4
04.1
42.0±54.1
L+C
L-TiV
46.6
38.01
49.6
62.2
33.0±63.2
40.01
56.71
41.6
56.2
45.0±75.2
15.6
92.21
60.7
87.4
24.0±00.5
71.6
99.31
06.4
57.1
82.0±87.1
61.8
05.32
39.2
05.1
92.0±84.1
L+C
H-TiV
56.81
04.5375.82
66.4
22.1±94.5
06.42
36.1682.62
30.5
56.1±68.5
59.02
50.1576.81
45.7
63.1±53.8
43.91
32.1426.81
39.3
30.1±44.4
97.9
34.32
86.4
82.2
94.0±62.2
DBS
TX-TiV
]61[kcilCelpmiS
77.8
05.9124.11
24.3
45.0±57.3
31.51
81.6385.71
70.4
70.1±25.4
26.01
27.52
44.8
84.5
85.0±77.5
30.31
79.32
09.9
64.2
64.0±16.2
11.6
38.21
07.4
45.1
22.0±35.1
DBS
B-TiV
10.8
51.61
23.7
42.3
34.0±04.3
00.41
60.3320.01
57.3
88.0±38.3
50.8
79.51
62.7
83.5
35.0±65.5
18.5
55.31
07.6
33.2
24.0±63.2
53.5
47.81
19.8
64.1
22.0±55.1
DBS
L-TiV
55.7
88.61
73.7
31.3
34.0±23.3
71.31
79.1364.01
95.3
68.0±47.3
02.21
96.32
76.7
33.5
55.0±94.5
28.8
96.61
44.9
90.2
73.0±22.2
41.1
85.1
31.6
44.1
81.0±25.1
DBS
H-TiV
86.8
33.41
67.8
10.2
43.0±21.2
38.21
31.42
85.9
07.2
36.0±07.2
12.9
74.81
23.9
42.4
64.0±35.4
96.7
85.32
11.6
64.1
72.0±75.1
62.11
32.42
23.0
85.1
23.0±94.1
L+C
H-TiV
]71[LCI-RFC
91.8
96.21
74.7
83.3
84.0±24.3
34.9
87.91
97.9
42.5
87.0±52.5
72.9
93.51
36.8
29.5
65.0±69.5
88.11
40.8149.11
37.2
85.0±08.2
29.8
05.11
19.9
07.1
23.0±18.1
B1-AS
yniT-TiV
]81[MASeliboM
27.8
60.0171.11
79.2
15.0±40.3
96.9
37.51
88.9
68.4
97.0±19.4
71.5
72.11
62.8
51.5
35.0±03.5
23.9
53.1105.01
51.2
44.0±12.2
68.71
24.9
76.6
88.1
83.0±19.1
B1-AS
B-TiV
57.6
80.7
11.8
65.2
04.0±06.2
69.7
79.41
98.8
77.4
36.0±18.4
69.6
95.11
28.8
80.5
14.0±12.5
25.8
28.21
85.5
98.1
23.0±98.1
52.6
97.4
60.5
46.1
92.0±07.1
B1-AS
L-TiV
]91[MAS
67.6
05.8
59.5
66.2
83.0±66.2
10.9
16.51
36.7
41.5
86.0±41.5
98.7
20.51
00.8
23.5
94.0±24.5
85.8
06.01
81.5
80.2
34.0±10.2
83.7
52.5
29.3
87.1
82.0±28.1
B1-AS
H-TiV
64.8
47.0114.61
31.3
76.0±53.3
85.01
86.6184.21
21.5
98.0±93.5
80.6
30.31
54.7
62.5
05.0±23.5
16.21
87.61
39.5
12.2
54.0±02.2
21.6
09.4
99.61
69.1
65.0±01.2
B1-AS
B-TiV
78.6
96.7
20.11
27.2
15.0±18.2
15.7
05.31
46.9
98.4
47.0±50.5
50.9
96.51
85.8
80.5
84.0±91.5
04.7
68.5
87.6
11.2
83.0±30.2
50.9
39.5102.21
08.1
84.0±30.2
B1-AS
L-TiV
]02[QH-MAS
46.5
59.7
87.6
27.2
14.0±57.2
38.6
63.21
17.7
29.4
86.0±79.4
05.9
63.81
51.8
60.5
44.0±61.5
35.4
01.0-
06.4
01.2
13.0±99.1
19.01
07.71
97.4
68.1
23.0±29.1
B1-AS
H-TiV
74.4
75.3
54.9
00.3
05.0±11.3
38.7
41.31
97.7
67.3
46.0±68.3
33.4
64.7
68.4
95.4
82.0±56.4
86.8
34.31
84.6
29.1
72.0±88.1
64.8
59.11
16.7
25.1
32.0±06.1
V-AS
T-areiH
46.4
97.4
15.9
19.2
74.0±20.3
96.8
76.21
44.7
07.3
16.0±57.3
26.7
68.51
94.8
65.4
33.0±76.4
02.6
41.7
77.3
09.1
62.0±08.1
20.8
76.7
35.5
06.1
42.0±56.1
V-AS
S-areiH
]4[2MAS
28.4
01.4
64.7
67.2
14.0±38.2
50.7
53.21
21.9
57.3
26.0±48.3
42.7
82.31
15.9
94.4
92.0±16.4
37.4
09.9
31.5
07.1
22.0±96.1
07.31
92.02
72.8
05.1
42.0±95.1
V-AS
+B-areiH
86.4
78.4
15.6
96.2
83.0±47.2
16.5
43.9
21.6
04.3
15.0±24.3
43.4
30.01
55.7
03.4
32.0±93.4
17.6
85.8
77.2
06.1
71.0±06.1
51.6
36.41
60.4
63.1
71.0±04.1
V-AS
L-areiH
54.4
53.3
57.9
99.2
05.0±11.3
66.7
88.11
54.8
38.3
86.0±19.3
44.6
99.8
80.7
85.4
23.0±76.4
80.11
07.61
83.9
09.1
13.0±98.1
92.01
05.12
60.9
45.1
12.0±06.1
V-AS
T-areiH
67.4
07.4
95.9
19.2
94.0±40.3
34.6
53.21
61.8
96.3
26.0±67.3
28.6
03.41
27.9
15.4
23.0±36.4
59.5
74.7
95.8
17.1
82.0±67.1
72.3
02.9
37.11
05.1
22.0±46.1
V-AS
S-areiH
]4[1.2MAS
10.5
15.4
53.8
87.2
34.0±78.2
73.7
59.11
74.7
78.3
26.0±88.3
37.8
93.5157.11
35.4
23.0±76.4
35.8
42.0155.01
66.1
03.0±27.1
47.31
05.82
57.5
06.1
72.0±96.1
V-AS
+B-areiH
72.4
05.4
14.7
37.2
93.0±18.2
56.5
19.9
87.6
44.3
25.0±15.3
05.5
84.9
53.01
82.4
52.0±44.4
52.5
71.6
09.4
06.1
02.0±26.1
78.9
05.12
70.7
25.1
52.0±85.1
V-AS
L-areiH27
.stlusernoitaulaveFoN
:01elbaT
]21[SIRTET
]11[laVM-OCOC
]01[SIVAD
]9[yelekreB
]8[tuCbarG
09@02FoN
09@02FoN
09@02FoN
09@02FoN
09@02FoN
ataD
ledoM
dohteM
elpmaS
elpmaS
elpmaS
elpmaS
elpmaS
HH∆RG∆BS∆esaB
HH∆RG∆BS∆esaB
HH∆RG∆BS∆esaB
HH∆RG∆BS∆esaB
HH∆RG∆BS∆esaB
)dts±(
)dts±(
)dts±(
)dts±(
)dts±(
6.82
39
7.22
621
13.72±7.841
2.51
84
8.1-
84
98.51±2.64
8.4
11
0.2
95
28.3±0.16
6.0
1
5.0
1
18.0±5.1
0.0
0
0.0
0
00.0±0.0
L+C
05NR
]31[SICPG
6.12
84
6.81
621
58.41±6.441
4.6
51
0.0
85
58.5±0.85
0.6
61
8.5
65
01.5±8.16
4.1
3
9.0
0
49.0±9.0
0.0
0
0.0
0
00.0±0.0
L+C
43NR
]41[teNDC
2.83
59
3.43
591
81.82±3.922
0.31
53
3.31
96
18.21±3.28
2.32
95
6.51
26
11.81±6.77
8.2
5
4.2
1
19.1±4.3
0.0
1
6.0
0
94.0±6.0
DBS
43NR
2.51
24
8.32
671
33.11±8.991
6.11
92
8.0
04
47.7±8.04
8.3
01
9.3
45
50.3±9.75
8.0
4
4.1
2
63.1±4.3
0.0
0
0.0
0
00.0±0.0
L+C
81RH
8.71
84
7.12
051
36.41±7.171
2.9
72
8.0-
42
82.8±2.32
2.5
21
6.6
35
02.4±6.95
2.0
1
3.1
1
46.0±3.2
0.0
0
0.0
0
00.0±0.0
L+C
TI-s81RH
8.21
34
4.71
051
45.41±4.761
0.9
02
3.8-
92
10.7±7.02
6.1
8
4.2
25
26.2±4.45
6.0-
2
3.0
2
01.1±3.2
0.0
0
0.0
0
00.0±0.0
L+C
TI-81RH
]2[MTIR
6.1
61
4.52
531
69.7±4.061
8.3
91
9.3-
13
07.5±1.72
0.3
6
5.1
15
68.1±5.25
0.1
2
3.0-
2
00.1±7.1
0.0
0
0.0
0
00.0±0.0
L+C
TI-23RH
4.52
98
3.54
112
63.03±3.652
8.11
54
5.2-
63
41.41±5.33
8.8
32
0.9
75
42.7±0.66
4.0-
1
4.1
3
66.0±4.4
0.0
0
0.0
0
00.0±0.0
DBS
TI-81RH
2.5
91
4.21
79
85.6±4.901
2.3
41
4.5-
92
80.5±6.32
6.0
1
9.0
05
07.0±9.05
4.0-
2-
4.1
1
66.0±4.2
0.0
0
0.0
0
00.0±0.0
L+C
B-TiV
]51[kcilCtpadA
6.21
05
7.61
931
42.51±7.551
4.01
13
6.8-
24
90.11±4.33
6.2
7
3.2
15
09.1±3.35
2.0
1-
1.0
2
45.0±1.2
0.0
0
0.0
0
00.0±0.0
DBS
B-TiV
0.8
22
6.6
101
47.6±6.701
6.9
72
0.7-
92
99.8±0.22
8.0
2
6.0
94
29.0±6.94
4.0
0
8.0-
2
04.0±2.1
0.0
0
0.0
0
00.0±0.0
L+C
B-TiV
2.7
11
2.6
98
60.5±2.59
6.6
91
7.5-
52
72.5±3.91
2.1
3
6.0
94
11.1±6.94
2.0
0
9.0-
2
03.0±1.1
0.0
0
0.0
0
00.0±0.0
L+C
L-TiV
8.6
31
6.4
18
01.5±6.58
6.6
01
5.9-
42
36.4±5.41
6.1
3
6.4
44
20.1±6.84
8.0
0
4.0-
1
94.0±6.0
0.0
0
0.0
0
00.0±0.0
L+C
H-TiV
8.14
841
1.04
361
37.34±1.302
6.12
27
4.2
04
54.32±4.24
2.01
33
5.6
75
27.9±5.36
2.1
2
0.0
3
00.1±0.3
0.0
0
0.0
0
00.0±0.0
DBS
TX-TiV
]61[kcilCelpmiS
2.61
84
7.02
441
32.41±7.461
0.31
04
9.5-
93
56.41±1.33
2.3
9
2.2
45
28.2±2.65
2.1
3
2.0-
3
71.1±8.2
0.0
0
0.0
0
00.0±0.0
DBS
B-TiV
6.5
32
8.61
931
54.6±8.551
6.5
62
2.51-
83
65.7±8.22
6.2
6
7.1
45
01.2±7.55
6.0-
0
3.0
2
46.0±3.2
0.0
0
0.0
0
00.0±0.0
DBS
L-TiV
0.9
82
9.01
631
52.8±9.641
6.8
72
1.01-
13
08.6±9.02
6.0-
1
5.1
25
18.0±5.35
0.0
0
0.0
1
00.0±0.1
0.0
0
0.0
0
00.0±0.0
DBS
H-TiV
6.5
51
4.4
85
92.4±4.26
0.7
71
9.01-
62
26.6±1.51
0.3
6
1.6
53
12.2±1.14
4.0
1
2.0
0
04.0±2.0
0.0
0
0.0
0
00.0±0.0
L+C
H-TiV
]71[LCI-RFC
2.1
31
4.3
871
28.5±4.181
2.1
51
0.9-
101
21.5±0.29
0.1
1
7.1-
76
00.1±3.56
2.0
0
1.0-
3
07.0±9.2
2.0
0
1.0
0
03.0±1.0
B1-AS
yniT-TiV
]81[MASeliboM
4.0-
2
6.1-
311
19.1±4.111
4.1-
0
9.7-
88
15.2±1.08
8.0
4
0.2
15
84.1±0.35
2.0
1
1.0-
1
03.0±9.0
0.0
0
0.0
0
00.0±0.0
B1-AS
B-TiV
4.0-
5
2.0
99
63.2±2.99
4.3
21
1.2-
29
10.4±9.98
0.1
2
1.0
35
38.0±1.35
0.0
0
0.0
1
00.0±0.1
0.0
0
0.0
0
00.0±0.0
B1-AS
L-TiV
]91[MAS
4.0
3
0.2
111
16.1±0.311
4.1-
9
5.2-
901
80.4±5.601
6.2
5
7.0
55
97.1±7.55
2.0
0
1.1-
2
45.0±9.0
0.0
0
0.0
0
00.0±0.0
B1-AS
H-TiV
4.6
51
0.7
79
57.6±0.401
8.0
8
6.4
88
90.6±6.29
0.1
3
7.0
25
72.1±7.25
0.0
0
0.0
1
00.0±0.1
2.0-
1-
1.0
0
03.0±1.0
B1-AS
B-TiV
8.1-
7-
9.2
88
21.7±9.09
6.1
4
6.4
49
32.6±6.89
0.0
1-
6.0
25
66.0±6.25
2.0
1
9.0-
2
03.0±1.1
4.0
0
6.0
0
94.0±6.0
B1-AS
L-TiV
]02[QH-MAS
8.1
4
9.0
39
29.1±9.39
6.0-
5
9.6-
201
37.3±1.59
6.0
2
5.0
35
76.0±5.35
0.0
0
0.1-
2
00.0±0.1
0.0
0
0.0
0
00.0±0.0
B1-AS
H-TiV
6.3
21
8.1
29
98.3±8.39
6.0
7
3.5-
84
30.3±7.24
0.0
0
0.0
15
00.0±0.15
0.0
0
0.0
1
00.0±0.1
0.0
0
0.0
0
00.0±0.0
V-AS
T-areiH
8.2
41
6.1
98
14.3±6.09
2.0-
2
5.5-
54
81.4±5.93
0.0
0
0.0
05
00.0±0.05
0.0
0
0.0
1
00.0±0.1
0.0
0
0.0
0
00.0±0.0
V-AS
S-areiH
]4[2MAS
2.2
5
7.0
39
51.2±7.39
6.1
6
4.3-
45
08.2±6.05
0.0
0
0.0
05
00.0±0.05
0.0
0
0.0
1
00.0±0.1
0.0
0
0.0
0
00.0±0.0
V-AS
+B-areiH
4.1-
5
9.1
78
15.2±9.88
8.1
7
7.6-
74
39.2±3.04
0.0
0
0.0
05
00.0±0.05
0.0
0
0.0
1
00.0±0.1
0.0
0
0.0
0
00.0±0.0
V-AS
L-areiH
8.4
01
8.2
29
36.3±8.49
4.1
7
7.7-
94
01.3±3.14
2.0
1
1.0
15
03.0±1.15
0.0
0
0.0
1
00.0±0.1
0.0
0
0.0
0
00.0±0.0
V-AS
T-areiH
8.0
8
4.2
19
19.2±4.39
6.2
6
9.2-
14
48.2±1.83
0.0
0
0.0
05
00.0±0.05
0.0
0
0.0
1
00.0±0.1
0.0
0
0.0
0
00.0±0.0
V-AS
S-areiH
]4[1.2MAS
2.1
4
8.2
88
25.2±8.09
2.0
3
9.8-
75
12.2±1.84
0.0
0
0.0
05
00.0±0.05
2.0
0
1.0-
1
03.0±9.0
0.0
0
0.0
0
00.0±0.0
V-AS
+B-areiH
4.3
8
3.0-
98
67.2±7.88
2.2
8
7.3-
44
79.2±3.04
0.0
0
0.0
05
00.0±0.05
0.0
0
0.0
1
00.0±0.1
0.0
0
0.0
0
00.0±0.0
V-AS
L-areiH28
stlusernoitaulaveCuA-UoI
:11elbaT
]21[SIRTET
]11[laVM-OCOC
]01[SIVAD
]9[yelekreB
]8[tuCbarG
02CuA-UoI
02CuA-UoI
02CuA-UoI
02CuA-UoI
02CuA-UoI
ataD
ledoM
dohteM
elpmaS
elpmaS
elpmaS
elpmaS
elpmaS
HH∆RG∆BS∆
esaB
HH∆RG∆BS∆
esaB
HH∆RG∆BS∆
esaB
HH∆RG∆BS∆
esaB
HH∆RG∆BS∆
esaB
)dts±(
)dts±(
)dts±(
)dts±(
)dts±(
92.1-
96.3-
52.1-89.3938.1±37.29
03.2-
84.6-
37.1-26.2919.2±98.09
80.1-
68.2-
90.1-93.0972.1±03.98
16.0-
37.1-
55.0-71.5929.0±26.49
65.0-
17.1-
31.0-92.7938.0±61.79
L+C
05NR
]31[SICPG
45.1-
79.3-
22.1-91.4979.1±89.29
34.1-
75.3-
80.1-18.1971.2±47.09
29.1-
81.5-
56.1-17.9850.2±50.88
93.1-
07.3-
19.0-87.4908.1±78.39
70.1-
08.3-
65.0-41.7995.1±85.69
L+C
43NR
]41[teNDC
67.1-
66.4-
79.0-94.1921.2±25.09
29.1-
37.4-
50.1-41.8898.2±01.78
01.2-
79.5-
26.1-12.8870.2±95.68
92.1-
95.3-
28.0-77.3963.1±59.29
21.1-
99.2-
74.0-09.5952.1±44.59
DBS
43NR
07.0-
67.1-
86.0-54.2986.1±77.19
59.0-
35.2-
49.0-99.1940.2±50.19
18.0-
04.2-
78.0-26.8856.1±57.78
96.0-
37.1-
96.0-29.3983.1±22.39
54.0-
10.1-
51.0-23.6922.1±81.69
L+C
81RH
54.0-
04.1-
77.0-88.3962.1±11.39
95.0-
77.1-
47.0-47.3946.1±00.39
20.1-
50.3-
25.1-27.9869.1±91.88
33.0-
57.0-
82.0-09.4928.0±16.49
83.0-
77.0-
21.0
51.7967.0±72.79
L+C
TI-s81RH
34.0-
92.1-
17.0-02.4991.1±94.39
37.0-
90.2-
85.0-71.4945.1±06.39
09.0-
76.2-
69.0-99.9816.1±30.98
52.0-
50.1-
72.0-04.5917.0±21.59
23.0-
67.0-
90.0-33.7967.0±42.79
L+C
TI-81RH
]2[MTIR
12.0-
52.1-
01.1-34.4935.1±33.39
75.0-
83.1-
59.0-09.3946.1±59.29
41.1-
43.3-
63.1-76.0998.1±23.98
83.0-
69.0-
73.0-45.5938.0±61.59
42.0-
70.1-
21.0
62.7988.0±93.79
L+C
TI-23RH
28.0-
77.2-
85.1-72.1950.2±96.98
55.1-
57.4-
06.1-84.1979.2±78.98
89.0-
65.2-
80.1-27.8816.1±46.78
82.0-
50.1-
23.0-10.4958.0±96.39
41.0-
75.0-
10.0-83.6996.0±63.69
DBS
TI-81RH
22.0-
76.0-
73.0-44.5928.0±70.59
25.0-
85.1-
33.0-03.4981.1±79.39
27.0-
19.1-
47.0-50.2950.1±03.19
23.0-
59.0-
50.0-47.5926.0±96.59
52.0-
28.0-
51.0
11.8973.0±72.89
L+C
B-TiV
]51[kcilCtpadA
76.0-
01.2-
56.0-62.3925.1±16.29
45.1-
95.4-
28.0-34.1968.2±16.09
95.0-
54.1-
76.0-12.1988.0±45.09
83.0-
77.0-
72.0-42.5995.0±79.49
01.0-
54.0-
12.0
37.7943.0±49.79
DBS
B-TiV
03.0-
28.0-
14.0-36.5938.0±32.59
57.0-
89.1-
74.0-24.4924.1±59.39
87.0-
80.2-
25.0-15.1950.1±00.19
43.0-
39.0-
31.0-99.5965.0±68.59
12.0-
89.0-
50.0
21.8944.0±71.89
L+C
B-TiV
81.0-
24.0-
92.0-09.5967.0±26.59
85.0-
16.1-
12.0-97.4901.1±85.49
55.0-
27.1-
65.0-00.2949.0±44.19
41.0-
43.0-
60.0-80.6985.0±20.69
82.0-
87.0-
10.0
92.8925.0±92.89
L+C
L-TiV
81.0-
35.0-
23.0-50.6967.0±37.59
36.0-
85.1-
71.0-89.4980.1±18.49
95.0-
06.1-
56.0-18.1911.1±51.19
92.0-
46.0-
01.0-12.6945.0±11.69
02.0-
48.0-
80.0
32.8995.0±13.89
L+C
H-TiV
50.1-
71.3-
20.2-87.1977.2±67.98
78.1-
72.6-
82.2-61.1927.3±88.88
73.1-
83.3-
44.1-59.7852.2±15.68
19.0-
11.2-
48.0-59.3942.1±01.39
25.0-
92.1-
10.0-36.6967.0±26.69
DBS
TX-TiV
]61[kcilCelpmiS
43.0-
64.1-
01.1-03.3974.1±02.29
90.1-
95.3-
68.1-19.1908.2±50.09
56.0-
56.1-
96.0-18.0978.0±21.09
83.0-
09.0-
62.0-72.5965.0±10.59
01.0-
84.0-
51.0
95.7923.0±47.79
DBS
B-TiV
94.0-
74.1-
05.0-26.3901.1±21.39
10.1-
31.3-
36.0-56.2949.1±20.29
55.0-
05.1-
75.0-38.0909.0±52.09
03.0-
98.0-
52.0-52.5935.0±10.59
00.0-
74.0-
82.0-46.7926.0±63.79
DBS
L-TiV
75.0-
76.1-
66.0-86.3902.1±20.39
01.1-
24.3-
09.0-88.2921.2±89.19
96.0-
49.1-
35.0-96.0970.1±61.09
73.0-
08.0-
24.0-83.5916.0±79.49
00.0-
51.0-
80.0-07.7993.0±26.79
DBS
H-TiV
91.0-
25.0-
63.0-06.6948.0±52.69
06.0-
17.1-
92.0-48.4962.1±45.49
66.0-
16.1-
26.0-17.2990.1±90.29
13.0-
39.0-
31.0-07.6906.0±75.69
83.0-
08.0-
13.0
53.8975.0±66.89
L+C
H-TiV
]71[LCI-RFC
41.0-
94.0-
32.0-48.2983.1±16.29
91.0-
97.0-
16.0-62.9885.2±46.88
46.0-
96.1-
96.0-97.8865.1±01.88
00.0-
11.0-
71.0-65.3963.1±93.39
21.0-
71.0-
20.0-29.6989.0±09.69
B1-AS
yniT-TiV
]81[MASeliboM
40.0
90.0
62.0-81.4981.1±29.39
40.0-
32.0-
23.0-30.1935.1±07.09
41.0-
35.0-
03.0-67.9803.1±64.98
81.0-
20.0-
50.0-75.4968.0±25.49
91.0-
92.0
20.0-99.6939.0±79.69
B1-AS
B-TiV
90.0
53.0
02.0-58.4970.1±56.49
10.0
11.0-
63.0-90.1924.1±37.09
63.0-
98.0-
74.0-71.0972.1±17.98
50.0-
11.0-
21.0
99.4988.0±11.59
90.0-
00.0
40.0-85.7977.0±45.79
B1-AS
L-TiV
]91[MAS
30.0
11.0
81.0-05.4990.1±23.49
80.0-
35.0-
14.0-46.9803.2±42.98
16.0-
13.1-
07.0-58.9855.1±41.98
33.0-
90.0
71.0-28.4990.1±56.49
71.0-
81.0-
90.0
82.7997.0±73.79
B1-AS
H-TiV
63.0
98.0
14.0-41.4977.1±37.39
51.0
63.0
34.0-68.0926.1±44.09
40.0
60.0-
52.0-13.0913.1±70.09
50.0-
11.0
91.0-60.5999.0±78.49
75.0
03.2
03.0-51.7916.1±58.69
B1-AS
B-TiV
13.0
88.0
41.0-00.5963.1±58.49
13.0
85.0
93.0-71.1964.1±97.09
60.0-
72.0-02.0-14.0923.1±22.09
51.0
51.0
80.0-45.5958.0±74.59
73.0
39.1
53.0-46.7934.1±92.79
B1-AS
L-TiV
]02[QH-MAS
71.0
35.0
01.0-41.5922.1±40.59
21.0
21.0
63.0-71.1994.1±18.09
11.0-
91.0-
32.0-95.0922.1±63.09
60.0
61.0
30.0
15.5917.0±45.59
71.0-
10.0-
30.0-85.7927.0±55.79
B1-AS
H-TiV
03.0
48.0
14.0-83.4934.1±79.39
71.0
71.0
01.0-54.2992.1±43.29
71.0-
55.0-
04.0-07.1969.0±03.19
50.0-
81.0-
60.0
28.5976.0±88.59
81.0-
91.0-
22.0-12.8996.0±99.79
V-AS
T-areiH
32.0
07.0
04.0-74.4973.1±70.49
10.0-
91.0-
80.0-74.2962.1±83.29
13.0-
40.1-
33.0-94.1920.1±61.19
00.0
11.0
82.0
95.5946.0±78.59
04.0-
44.0-
61.0-89.7948.0±28.79
V-AS
S-areiH
]4[2MAS
71.0
06.0
33.0-85.4962.1±62.49
41.0
91.0
81.0-51.2933.1±79.19
24.0-
87.0-
54.0-87.1990.1±33.19
10.0
90.0-
41.0
69.5975.0±01.69
72.0-
34.0-
10.0-89.7916.0±79.79
V-AS
+B-areiH
61.0
65.0
42.0-47.4961.1±05.49
30.0
03.0-
40.0-80.3920.1±40.39
81.0-
45.0-
53.0-73.2998.0±20.29
40.0-
70.0-
62.0
69.5955.0±22.69
61.0-
52.0-
20.0-42.8955.0±22.89
V-AS
L-areiH
82.0
87.0
14.0-84.4904.1±70.49
11.0
30.0-
41.0-15.2933.1±83.29
73.0-
08.0-
93.0-65.1941.1±61.19
21.0-
43.0-
40.0
96.5977.0±37.59
51.0-
65.0-
80.0-31.8966.0±50.89
V-AS
T-areiH
72.0
57.0
84.0-14.4954.1±49.39
21.0
41.0
11.0-65.2952.1±64.29
53.0-
02.1-
45.0-65.1951.1±20.19
20.0
70.0-
10.0
18.5966.0±28.59
70.0-
32.0-
91.0-02.8996.0±10.89
V-AS
S-areiH
]4[1.2MAS
32.0
66.0
04.0-76.4913.1±72.49
51.0
02.0
22.0-82.2972.1±50.29
52.0-
17.0-
93.0-16.1911.1±22.19
20.0
30.0-
70.0-79.5986.0±09.59
22.0-
54.0-
50.0
47.7947.0±97.79
V-AS
+B-areiH
51.0
35.0
92.0-47.4991.1±44.49
70.0-
73.0-
10.0
69.2999.0±79.29
22.0-
75.0-
83.0-13.2910.1±29.19
50.0-
20.0
80.0
79.5946.0±50.69
51.0-
34.0-
51.0
58.7996.0±10.89
V-AS
L-areiH29
.skcilcdetalumisrof02@RSNdnaskcilcresu-laernostlusernoitaulavE
:21elbaT
]21[SIRTET
]11[laVM-OCOC
]01[SIVAD
]9[yelekreB
]8[tuCbarG
UoI
UoI
UoI
UoI
UoI
ataD
ledoM
dohteM
RSNRSN
esaB
1@elpmaS
1@resU
RSNRSN
esaB
1@elpmaS
1@resU
RSNRSN
esaB
1@elpmaS
1@resU
RSNRSN
esaB
1@elpmaS
1@resU
RSNRSN
esaB
1@elpmaS
1@resU
02@
1@
1@
)dts±(
)dts±(
02@
1@
1@
)dts±(
)dts±(
02@
1@
1@
)dts±(
)dts±(
02@
1@
1@
)dts±(
)dts±(
02@
1@
1@
)dts±(
)dts±(
63.242.7104.47
37.9±57.17
31.01±99.17
18.312.8103.66
59.7±34.46
60.9±40.46
26.103.6130.67
93.7±66.37
17.9±35.27
99.042.2116.97
40.8±51.97
03.8±97.87
98.036.1164.48
86.7±94.38
76.8±14.38
L+C
05NR
]31[SICPG
13.223.7174.2848.01±16.9783.11±65.97
56.223.6122.97
00.8±08.67
82.01±50.67
35.227.9107.67
42.8±14.37
05.11±79.17
69.196.4146.48
54.9±74.28
24.01±29.18
66.105.2108.09
91.8±72.88
22.01±58.78
L+C
43NR
]41[teNDC
68.215.8129.6796.01±22.3779.01±54.37
42.470.9111.86
32.8±27.56
47.9±02.56
76.269.9148.27
86.7±05.07
81.11±61.96
15.170.3143.38
57.7±85.18
60.9±40.18
73.193.3171.88
63.9±88.48
60.01±72.58
DBS
43NR
69.198.2132.67
90.7±99.67
11.7±99.67
15.244.7108.37
68.6±90.47
43.8±18.37
50.288.0281.86
99.7±08.76
36.8±03.76
05.1
73.9
61.38
57.5±33.28
90.6±55.28
92.135.0187.68
43.7±11.68
75.7±01.68
L+C
81RH
35.166.6169.57
91.9±92.37
88.9±52.37
89.100.9189.37
30.8±73.27
50.01±89.17
15.222.4268.07
66.8±68.76
07.11±05.66
88.075.3196.87
54.8±55.77
11.9±15.77
18.086.3102.68
38.9±21.38
51.01±06.38
L+C
TI-s81RH
84.169.1160.87
97.6±80.77
31.7±31.77
98.100.7176.67
32.7±40.67
20.9±55.57
10.268.1253.17
44.8±96.96
83.01±27.86
77.0
24.9
73.38
02.5±34.28
84.6±80.28
28.081.0133.88
82.7±52.58
95.7±58.58
L+C
TI-81RH
]2[MTIR
60.239.3108.87
66.7±59.67
81.8±89.67
82.204.8107.67
79.7±43.57
02.01±19.47
13.217.2235.27
32.8±53.07
69.01±31.96
98.066.0141.38
31.6±83.28
11.7±10.28
49.097.0186.78
61.7±21.68
99.7±02.68
L+C
TI-23RH
76.3
29.9
49.17
06.4±71.17
58.4±91.17
31.491.5140.26
62.6±27.16
20.7±95.16
41.246.1289.56
16.7±41.56
50.9±74.46
69.0
43.8
17.77
60.4±05.67
09.4±43.67
67.0
53.7
29.08
83.4±82.97
68.3±04.97
DBS
TI-81RH
89.028.0179.18
70.6±81.28
51.6±61.28
55.178.2112.87
62.5±35.77
94.6±71.77
72.154.3149.67
49.5±81.67
90.7±29.47
76.0
78.8
50.48
61.4±01.48
65.5±54.38
83.0
50.6
70.7878.4±91.98
74.4±62.98
L+C
B-TiV
]51[kcilCtpadA
02.2
89.8
80.77
23.4±34.67
63.4±94.67
78.301.6161.26
14.6±30.26
68.6±58.16
80.182.1139.8719.4±19.77
12.6±75.67
66.0
14.5
20.5849.2±82.48
65.3±37.38
53.0
16.3
11.1959.2±14.09
83.2±13.09
DBS
B-TiV
99.063.1188.08
24.6±69.08
85.6±30.18
08.132.3164.77
89.5±86.67
89.6±84.67
82.176.4119.27
82.6±11.37
43.7±80.27
06.0
05.8
03.48
22.4±20.58
37.5±31.48
64.0
98.4
99.88
55.4±21.98
87.3±02.98
L+C
B-TiV
09.061.1161.3838.6±84.38
89.6±74.38
83.147.1126.0871.5±26.08
61.6±43.08
51.143.4113.47
95.5±61.57
17.6±53.47
26.0
27.7
81.28
67.3±52.48
88.4±66.38
45.0
43.6
15.78
87.4±97.88
84.4±15.88
L+C
L-TiV
09.074.0176.38
87.5±97.38
10.6±88.38
23.158.0110.18
56.4±47.08
89.5±84.08
63.167.6134.37
91.6±22.37
65.7±44.27
85.0
94.7
51.38
25.3±57.38
17.4±61.38
16.0
03.7
92.68
52.4±94.88
27.4±19.78
L+C
H-TiV
06.385.1289.0610.01±17.7560.01±30.85
97.414.2226.05
47.8±12.05
31.9±34.05
58.257.4201.55
95.9±80.35
03.01±65.25
93.142.6189.07
88.8±62.96
39.8±74.96
28.051.2122.08
82.8±84.77
05.7±10.87
DBS
TX-TiV
]61[kcilCelpmiS
80.2
96.8
87.57
54.4±99.47
54.4±50.57
09.365.5130.16
80.6±25.06
95.6±65.06
80.102.1120.67
78.4±65.57
11.6±22.47
06.0
05.6
14.48
78.3±76.38
42.4±01.38
33.0
47.3
14.98
68.2±49.88
76.2±48.88
DBS
B-TiV
34.1
58.8
81.87
81.4±56.77
41.4±67.77
04.275.5101.56
69.5±69.46
17.6±79.46
11.195.1169.77
54.4±76.77
12.6±52.67
85.0
81.5
32.48
89.2±10.48
92.3±14.38
96.0
21.6
16.09
57.3±94.88
65.3±84.88
DBS
L-TiV
85.1
96.6
22.87
61.3±27.77
11.3±27.77
86.206.4196.56
10.6±17.56
93.6±25.56
13.141.3165.5746.5±44.67
01.7±69.47
76.0
29.4
04.38
24.2±30.38
80.3±64.28
24.0
10.3
83.98
74.2±23.88
90.2±45.88
DBS
H-TiV
89.074.1146.58
86.6±68.58
20.7±29.58
35.143.2146.1882.5±74.18
55.6±81.18
13.190.7148.67
17.6±34.67
32.8±84.57
46.0
73.9
75.4895.4±57.58
88.5±50.58
95.0
92.7
16.78
68.4±33.98
27.4±59.88
L+C
H-TiV
]71[LCI-RFC
87.112.6327.5606.81±21.7677.81±18.66
03.355.2335.5690.31±58.6610.51±56.66
80.259.0478.1655.41±84.9510.61±71.85
55.182.7238.6719.41±30.6779.51±03.57
40.154.4241.9794.31±20.7776.41±16.67
B1-ASyniT-TiV]81[MASeliboM
83.191.1588.5552.12±99.4595.12±50.55
48.161.4492.8588.51±02.1602.81±09.06
95.160.7555.8414.61±55.6484.71±72.54
29.066.1474.8685.81±18.6681.02±20.66
79.016.2434.2674.02±25.3670.12±56.46
B1-AS
B-TiV
92.123.1405.6687.91±81.7651.02±89.66
77.121.6364.4657.41±08.6658.61±09.66
36.101.5488.8576.51±57.8568.61±74.75
39.092.1359.5757.51±78.6733.71±84.57
08.061.1352.4731.81±72.4729.71±60.57
B1-AS
L-TiV
]91[MAS
43.155.2435.2630.02±43.4693.02±88.36
69.288.5313.5649.41±64.7600.71±62.76
70.207.7433.9543.51±51.6588.61±88.45
12.136.2327.3728.51±82.5703.71±31.47
28.064.8293.3713.71±49.2734.61±91.57
B1-AS
H-TiV
00.274.8521.9485.02±08.7488.02±30.84
99.163.8493.5553.61±52.8586.81±12.85
35.199.6538.5499.51±77.4467.61±99.24
50.190.9434.2622.02±64.2691.12±14.16
27.184.7450.3674.12±26.0655.22±40.16
B1-AS
B-TiV
16.186.9446.7540.12±84.8534.12±03.85
88.108.2406.8549.51±50.2683.81±30.26
95.168.0561.3528.61±10.2536.71±37.05
09.062.0439.9690.81±26.9667.81±02.86
55.113.8305.9611.02±62.9649.12±92.07
B1-AS
L-TiV
]02[QH-MAS
14.186.6408.4549.81±03.5562.91±91.55
79.198.0400.0695.51±00.3629.71±70.36
64.145.0515.1550.51±03.0582.61±86.84
67.086.9398.6642.71±47.7629.71±12.66
67.033.5347.6644.71±65.6636.91±79.76
B1-AS
H-TiV
36.115.5547.7423.81±88.5416.81±09.54
15.189.4336.3634.31±08.6678.51±84.66
11.193.3374.0707.41±76.7684.61±23.66
17.032.6279.7771.41±66.7751.51±31.67
27.078.0307.1809.71±42.7723.91±08.67
V-AS
T-areiH
65.131.6524.7412.81±25.5465.81±65.54
15.113.5357.3662.41±19.6683.61±75.66
02.194.9374.6609.61±48.3657.71±69.16
76.023.9231.6718.41±05.4702.61±95.37
78.037.9285.6729.51±26.2788.61±22.27
V-AS
S-areiH
]4[1.2MAS
05.119.2521.1536.81±13.9480.91±04.94
36.128.8333.3642.51±99.5687.71±29.56
82.168.9388.6655.61±96.3648.71±71.26
06.026.8256.6727.51±44.7702.71±83.67
36.007.4300.7741.91±20.5773.12±06.47
V-AS
+B-areiH
73.126.1527.1586.81±92.0500.91±65.05
91.139.5293.2733.11±38.4716.31±24.47
30.144.5222.7798.21±96.4726.41±52.37
75.042.3250.2850.31±54.1852.51±77.08
75.017.4262.5851.41±87.3885.61±80.38
V-AS
L-areiH
95.181.6590.8484.81±28.5438.81±89.54
65.164.7376.2670.41±41.5653.61±39.46
33.195.8335.7676.61±56.4601.81±25.36
18.054.0385.7702.51±91.6717.61±96.47
86.045.4329.7792.91±97.3706.02±31.47
V-AS
T-areiH
86.193.6574.7473.81±17.5407.81±08.54
74.120.6364.3631.41±78.6694.61±85.66
53.173.5363.1748.51±87.7631.71±51.66
07.049.8251.9751.51±30.6742.61±00.57
17.050.2320.0869.71±19.2732.91±58.27
V-AS
S-areiH
]4[1.2MAS
25.107.2525.0581.81±23.8495.81±34.84
45.141.0406.2634.51±61.5639.71±72.56
03.132.6497.3657.71±30.9501.91±96.75
27.028.1324.6758.61±03.5772.81±64.47
77.081.8324.3781.12±71.2775.22±91.27
V-AS
+B-areiH
83.115.3589.9414.81±71.8467.81±05.84
71.139.7269.0778.21±45.3757.41±32.37
61.150.1376.5719.41±04.2790.71±85.07
76.020.8239.9758.41±94.8787.61±44.77
17.001.2367.9754.81±70.8749.02±51.87
V-AS
L-areiHC.3 Findinghardinstancesforthefirstroundusingreal-userclicks.
UsingFigure8wedepictafirst-roundreal-worldrobustnessofthemethodsforeachinstanceinthe
datasets. Obtainedmeanandstandarddeviation(STD)ofIoUweestimatedNSRforeachsample.
Ideally,forrobustinteractivesegmentationmethods,allinstancesshouldhavelowNSR,e.g. high
meanandlowSTDofIoU.TheinstanceswithhighNSRcanbeconsideredashardcasesformethods
inthefirstround.
Figures15and16illustratethehard(highNSR)andsimple(lowNSR)casesforRITMHRNet32-IT
(C+L)[2],SimpleClickViT-H(C+L)[16],SAMViT-H(SA1-B)[19],SAM-HQViT-H(SA1-B)[20].
For each method, the uncertainty of prediction is depicted by the mean mask averaged over the
method’s predictions on the first-round clicks. In the averaged mask, gray pixels correspond to
variationsofthemethodpredictions(hardcases),blackandwhitepixelsillustratelowvariationin,
respectively,backgroundandforegroundpredictions(simplecases).
30Figure 15: Samples with high User-IoU@1-NSR. From left to right – the image with the target
instance and real-user clicks of the first round; masks, averaged over the clicks, obtained by the
methods: RITMHRNet32-IT(C+L)[2],SimpleClickViT-H(C+L)[16],SAMViT-H(SA1-B)[19],
SAM-HQViT-H(SA1-B)[20]. 31
54.71±10.72
:UoI
24.12±75.82
:UoI
15.51±33.96
:UoI
65.61±88.96
:UoI
87.22±81.53
:UoI
90.71±46.92
:UoI
73.01±54.3
:UoI
01.52±75.93
:UoIFigure 16: Samples with low User-IoU@1-NSR. From left to right – the image with the target
instance and real-user clicks of the first round; masks, averaged over the clicks, obtained by the
methods: RITMHRNet32-IT(C+L)[2],SimpleClickViT-H(C+L)[16],SAMViT-H(SA1-B)[19],
SAM-HQViT-H(SA1-B)[20].
32
00.1±52.19
:UoI
43.0±31.39
:UoI
03.1±13.78
:UoI
15.0±60.39
:UoI
42.2±53.09
:UoI
23.0±30.78
:UoI
41.0±32.29
:UoI
22.1±81.49
:UoID BenchmarkDiscussion
Access. Thebenchmarkispubliclyavailableathttps://github.com/emb-ai/rclicksreposi-
tory.
License. ClicksareunderCCBY-NC4.0,evaluationcodeandbaselinemodelsareunderMIT.
Ethical issues. The benchmark is created for testing interactive segmentation methods. To the
bestofourknowledge,interactivesegmentationhastwoapplications: imageeditingandassisted
imagelabeling. Sinceinteractivesegmentationmethodsstilldorequireuserinput,webelievetheir
emergencewillnotmakeimagelabelingandimageeditingjobsredundant.
Limitationsofwork. Weobservethefollowingmajorlimitationsofourbenchmark:
1. Toestimatethesamplestatistic,wesplitclicksinto10patterngroupsandsample1click
fromeachgroupperround(10samplesperround). Whileincreasingthenumberofsampled
clicksimprovesperformanceestimationsperimage,webelievethat,forasufficientlylarge
dataset,10sampledclicksperroundissufficienttoaccuratelyestimateaverageperformance
overthedataset.
2. WetrainedourclickabilitymodelontheTETRISsubset. Theclickpatternsobservedinour
studymaybedomain-specifictothetrainimages. Differenttypesofimages(e.g.,medical
images,satelliteimages)mightresultindifferentclickbehaviorsthatourmodeldoesnot
accountfor.
3. Ourdatasetofclickswascollectedfromanonlineannotationplatform,whichmeansour
modelmayreflectbiasesspecifictotheindividualswhoparticipatedinthisplatform. This
couldincludebiasesrelatedtotheirannotationhabits,demographicbackgrounds,orother
unmeasuredfactors,potentiallyaffectingthemodel’sperformancewhenappliedtoawider
ordifferentuserbase.
4. Our model and benchmark are based on click interactions, but interactive segmentation
caninvolveothermodalitiessuchasscribbles,contours,orvoicecommands. Wehavenot
consideredthesetypesofinputs.
ToovercomeLimitation1,wewouldneedtoevaluatethemodelforeverypossibleclickposition,
whichiscomputationallyimpossible.
WebelievethatLimitation2isinevitableduetothefactthatwecannotannotateallpossibleimage
scenarios.
Limitation3cannotbeavoidedbecauseitisnotfeasibletoobtainafullyrepresentativepopulationof
allpeoplewithdifferentpatterns.
ToovercomeLimitation4,itwouldbenecessarytoinitiateaseparateresearchprojectdedicatedto
studyingdifferentinputtypesforinteractivesegmentation. Unfortunately,thisisbeyondthescopeof
thispaper.
33E DatasheetfortheBenchmark
Datasheetsfordatasets[21]facilitatecommunicationbetweendatacreatorsandusersintheformof
thequestionnaireexplicatingmotivation,dataacquisitionprocess,andpotentialusecases. Inthis
document,weprovideadatasheetforthebenchmark.
E.1 Motivation
Q1. Forwhatpurposewasthedatasetcreated? Wasthereaspecifictaskinmind? Wastherea
specificgapthatneededtobefilled? Pleaseprovideadescription.
–Interactiveimagesegmentationaimsatsegmentingobjectsofinterestgivenanimageandsequential
userinput(clicks,strokes,contours),witheachroundallowingtheusertocorrectpredictionerrors
fromthepreviousround. Whilenumerousinteractivesegmentationmethodshavebeendeveloped,
accuratelyevaluatingthesemethodsiscrucialforidentifyingthebestone. Trueevaluationrequires
real-userinputs. However,collectingmanyreal-userinputsformultipleroundsisimpractical,assuch
adatasetwouldneedtoberebuiltforeverymethodandinteractionroundduetoitsiterativenature.
Toaddressthis,researchersoftenuseasimplestrategytosimulateuserinputs. Thisstrategyinvolves
generatingasingleclickforeachinteractionroundbyselectingthelargesterrorregionfromthe
previousroundandclickingatthefurthestpointfromtheboundariesofthisregion(centerpoint).
However,previousworks[2,12]haveshownthatassumingusersclickinthecenterofanobjectis
overlysimplisticandunrealistic.
To address the issue of unfair assessment algorithms, we started our work by collecting a large
datasetofreal-userclicksovermultipleroundsofinteractions. Wetrainedaclickabilitymodelto
samplerealisticuserclicks,ensuringfairerevaluation. Thismodelnotonlyallowsformoreaccurate
multi-roundevaluationbutalsoprovidesdataforfirst-roundassessments.
Q2. Whocreatedthisdataset(e.g.,whichteam,researchgroup)andonbehalfofwhichentity
(e.g.,company,institution,organization)?
–SevenresearchersatAIRI,Moscow(affiliatedasof2024)havecreatedRClicks: AntonAntonov,
AndreyMoscalenko,DenisShepelev,AlexanderKrapukhin,KonstantinSoshin,AntonKonushin,
VladShakhuro.
Q3. Whofundedthecreationofthedataset? Ifthereisanassociatedgrant,pleaseprovidethe
nameofthegrantorandthegrantnameandnumber.
–Thisresearchworkwasfullysupportedbytheauthors,includingfundingannotators’work.
Q4. Anyothercomments?
–No.
E.2 Composition
Q5. Whatdotheinstancesthatcomprisethedatasetrepresent(e.g.,documents,photos,people,
countries)? Aretheremultipletypesofinstances(e.g.,movies,users,andratings;peopleand
interactionsbetweenthem;nodesandedges)? Pleaseprovideadescription.
– RClicks dataset consists of csv-file with clicks and previous-round masks, obtained by some
interactionmethodsafterfirstroundforeachinstanceinasegmentationdataset. Eachrowinthe
csv-filecontainsthefollowinginformationinthecolumns:
• dataset–anameofasegmentationdataset;
• image_stem–anameofanimagewithoutsuffix;
• object_stem–anencodeofatargetinstance,ifamaskcontainedmultipleinstances;
• model_type–anameofinteractionmethod,thatwasusedinthepreviousround(incase
offirstroundinteraction–empty);
• click_type – a click type (first for the first round, or fp or fn for the subsequent
rounds);
• full_stem – a unique identifier of image_stem, object_stem, model_type and
click_type;
• device–atypeofdevicewhereitwasclicked(pcormobile);
34• x,y–coordinatesofaclick;
• w,h–awidthandaheightoftheimage.
Q6. Howmanyinstancesarethereintotal(ofeachtype,ifappropriate)?
–Thereare185349clicksfromthefirstroundofiteration,290195clicksformthesecond(tocollect
theseclicksweused8144masksfromthefirstround). Overall—475544.
Q7. Doesthedatasetcontainallpossibleinstancesorisitasample(notnecessarilyrandom)
ofinstancesfromalargerset? Ifthedatasetisasample,thenwhatisthelargerset? Isthe
samplerepresentativeofthelargerset(e.g.,geographiccoverage)? Ifso,pleasedescribehow
thisrepresentativenesswasvalidated/verified. Ifitisnotrepresentativeofthelargerset,please
describe why not (e.g., to cover a more diverse range of instances, because instances were
withheldorunavailable).
– RClicks contains all click-annotated instances from GrabCut, Berkeley, DAVIS, COCO-MVal,
TETRIS.
Q8. Whatdatadoeseachinstanceconsistof? “Raw”data(e.g.,unprocessedtextorimages)or
features? Ineithercase,pleaseprovideadescription.
–RClickscsv-filecolumnshaveafollowing“raw”types:
• strings: dataset,image_stem,object_stem,model_type,click_type,full_stem,
device;
• integers: x,y,w,h.
Eachprevious-roundmaskinRClicksdataset–isasingle-channelimage.
Q9. Istherealabelortargetassociatedwitheachinstance? Ifso,pleaseprovideadescription.
–Yes,eachinstancehasacorrespondingmaskofinstanceontheimage,whereuserhadtoclick.
Q10. Isanyinformationmissingfromindividualinstances? Ifso,pleaseprovideadescription,
explainingwhythisinformationismissing(e.g., becauseitwasunavailable). Thisdoesnot
includeintentionallyremovedinformation,butmightinclude,e.g.,redactedtext.
–No.
Q11. Arerelationshipsbetweenindividualinstancesmadeexplicit(e.g.,users’movieratings,
socialnetworklinks)? Ifso,pleasedescribehowtheserelationshipsaremadeexplicit.
–No,thereareneitherexplicitofimplicitrelationshipsbetweenindividualinstancesinRClicks.
Q12. Arethererecommendeddatasplits(e.g.,training,development/validation,testing)? Ifso,
pleaseprovideadescriptionofthesesplits,explainingtherationalebehindthem.
–Weintendourdatasettobeprimarilyusedforbenchmarkinginteractivesegmentationmethods.
Hence,allinstancesinourdatasetwouldbeusedfortesting. Forclickabilitymodeltrainingthereisa
train/testsplit.
Q13.Arethereanyerrors,sourcesofnoise,orredundanciesinthedataset?Ifso,pleaseprovide
adescription.
–ItcanbestatedwithcertaintythattherearenoerroneousvaluesinRClicks. Thesolesourceofnoise
isthefactthatduringtheannotationprocess,participantswerepermittedtomakeminor"mistakes."
Asmallsubsetofclickswascollectedthatweresituatedoutsidethechosenobjectmaskbutinclose
proximitytoitsboundaries.
Q14. Isthedatasetself-contained,ordoesitlinktoorotherwiserelyonexternalresources
(e.g.,websites,tweets,otherdatasets)? Ifitlinkstoorreliesonexternalresources,(a)Arethere
guaranteesthattheywillexist,andremainconstant,overtime?
(b) Are there official archival versions of the complete dataset (i.e., including the external
resourcesastheyexistedatthetimethedatasetwascreated)?
(c)Arethereanyrestrictions(e.g.,licenses,fees)associatedwithanyoftheexternalresources
thatmightapplytoafutureuser? Pleaseprovidedescriptionsofallexternalresourcesandany
restrictionsassociatedwiththem,aswellaslinksorotheraccesspoints,asappropriate.
–Thedatasetisself-contained.
Q15. Does the dataset contain data that might be considered confidential (e.g., data that is
protectedbylegalprivilegeorbydoctor-patientconfidentiality,datathatincludesthecontent
35ofindividualsnon-publiccommunications)? Ifso,pleaseprovideadescription.
–No,theclicksinRClicksdonotcoverscenariosthatmaybeconsideredconfidential.
Q16. Does the dataset contain data that, if viewed directly, might be offensive, insulting,
threatening,ormightotherwisecauseanxiety? Ifso,pleasedescribewhy.
–Thedatasetcontainsnodatathatmightbeoffensive,insulting,threatening,ormightcauseanxiety
bymanuallycuratingthesetofimages.
Q17. Doesthedatasetrelatetopeople? Ifnot,youmayskipremainingquestionsinthissection.
–No.
Q21. Anyothercomments?
–No.
E.3 CollectionProcess
Q22. Howwasthedataassociatedwitheachinstanceacquired? Wasthedatadirectlyobserv-
able(e.g.,rawtext,movieratings),reportedbysubjects(e.g.,surveyresponses),orindirectly
inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or
language)? Ifdatawasreportedbysubjectsorindirectlyinferred/derivedfromotherdata,was
thedatavalidated/verified? Ifso,pleasedescribehow.
– We downloaded images and mask annotation from GrabCut, Berkeley, DAVIS, COCO-MVal,
TETRISandannotatedinstancesfromthedatasetswithclicksoncrowdsourcingplatformtoloka.ai.
Accordingly, the “raw” image data was directly observable by annotators, and annotations were
createdmanually.
Q23.Whatmechanismsorprocedureswereusedtocollectthedata(e.g.,hardwareapparatusor
sensor,manualhumancuration,softwareprogram,softwareAPI)?Howwerethesemechanisms
orproceduresvalidated?
–Weselectedanddownloadedinteractivesegmentationdatasets. Thenwithtoloka.aicrowdsourcing
platformannotateddatasets’instanceswithclicks. Theannotatorssawimage,thenmaskofchosen
object,thentheyclickedatthisobjectonimage.
Q24. Ifthedatasetisasamplefromalargerset,whatwasthesamplingstrategy?
–Wereleasewholedataset.
Q25. Whowasinvolvedindatacollectionprocess(e.g.,students,crowd-workers,contractors)
andhowweretheycompensated(e.g.,howmuchwerecrowd-workerspaid)?
–Weusedtoloka.aicrowdsourcingplatform,everycrowd-workerwaspaid0.02$for10clicks.
Q26. Overwhattimeframewasthedatacollected? Doesthistimeframematchthecreation
timeframeofthedataassociatedwiththeinstances(e.g.,recentcrawlofoldnewsarticles)? If
not,pleaseprovideadescriptionofthetimeframe.
–DatawascollectedfromtheAprilof2024totheMayof2024.
Q27. Wereanyethicalreviewprocessesconducted(e.g.,byaninstitutionalreviewboard)? If
so,pleaseprovideadescriptionofthesereviewprocesses,includingtheoutcomes,aswellasa
linkorotheraccesspointtoanysupportingdocumentation.
–No,suchprocesseswereunnecessaryinourcase.
Q28. Doesthedatasetrelatetopeople? Ifnot,youmayskipremainingquestionsinthissection.
–No.
Q34. Anyothercomments?
–No.
E.4 Preprocessing,Cleaning,and/orLabeling
Q35. Wasanypreprocessing/cleaning/labelingofthedatadone(e.g.,discretizationorbucket-
ing,tokenization,part-of-speechtagging,SIFTfeatureextraction,removalofinstances,process-
ingofmissingvalues)? Ifso,pleaseprovideadescription. Ifnot,youmayskiptheremainder
ofthequestionsinthissection.
–Wecleandatafromtheclicksthatweredonenotintheobjectofinterest.
36Q36. Wasthe“raw”datasavedinadditiontothepreprocessed/cleaned/labeleddata(e.g.,to
supportunanticipatedfutureuses)? Ifso,pleaseprovidealinkorotheraccesspointtothe
“raw”data.
–No.
Q37. Isthesoftwareusedtopreprocess/clean/labeltheinstancesavailable? Ifso,pleaseprovide
alinkorotheraccesspoint.
–Weprovidecodescriptsinsupplementalmaterials.
Q38. Anyothercomments?
–No.
E.5 Uses
Q39. Hasthedatasetbeenusedforanytasksalready? Ifso,pleaseprovideadescription.
–Wehaveusedourdatasettoevaluatestate-of-the-artinteractivesegmentationmethodsandtrainour
clickabilitymodel.
Q40. Istherearepositorythatlinkstoanyorallpapersorsystemsthatusethedataset? Ifso,
pleaseprovidealinkorotheraccesspoint.
–Wedonotmaintainsucharepository. However,citationtrackerslikeGoogleScholarandSemantic
Scholarwouldlistallfutureworksthatciteourdataset.
Q41. What(other)taskscouldthedatasetbeusedfor?
–Weanticipatethatthedatasetcouldbeusedforbenchmarkinginteractivesegmentationmethodsand
traininginteractivesegmentationmethods.
Q42. Isthereanythingaboutthecompositionofthedatasetorthewayitwascollectedand
preprocessed/cleaned/labeledthatmightimpactfutureuses? Forexample,isthereanything
thatafutureusermightneedtoknowtoavoidusesthatcouldresultinunfairtreatmentof
individualsorgroups(e.g.,stereotyping,qualityofserviceissues)orotherundesirableharms
(e.g.,financialharms,legalrisks)Ifso,pleaseprovideadescription. Isthereanythingafuture
usercoulddotomitigatetheseundesirableharms?
–Thisisverydifficulttoanticipate. Futureusersshouldbeawarethatourdatasetwascollectedboth
fromPCandMobiledevices,chooseclicksneededtotheirplatform.
Q43. Arethereanytasksforwhichthedatasetshouldnotbeused? Ifso, pleaseprovidea
description.
–No.
Q44. Anyothercomments?
–No.
E.6 Distribution
Q45. Will the dataset be distributed to third parties outside of the entity (e.g., company,
institution,organization)onbehalfofwhichthedatasetwascreated? Ifso,pleaseprovidea
description.
–Yes,ourdatasetwillbepubliclyavailable.
Q46. Howwillthedatasetwillbedistributed(e.g.,tarballonwebsite,API,GitHub)Doesthe
datasethaveadigitalobjectidentifier(DOI)?
– We will distribute our dataset at github repository. All uses of RClicks should cite this paper,
explicatingwhichversionofthedatasetwasconsidered.
Q47. Whenwillthedatasetbedistributed?
–ThedatasetwillbepubliclyavailablestartingfromSeptember2024.
Q48. Will the dataset be distributed under a copyright or other intellectual property (IP)
license,and/orunderapplicabletermsofuse(ToU)?Ifso,pleasedescribethislicenseand/or
ToU,andprovidealinkorotheraccesspointto,orotherwisereproduce,anyrelevantlicensing
termsorToU,aswellasanyfeesassociatedwiththeserestrictions.
–ClicksareunderCCBY-NC4.0,evaluationcodeandbaselinemodelsareunderMIT.
37Q49. HaveanythirdpartiesimposedIP-basedorotherrestrictionsonthedataassociatedwith
theinstances? Ifso,pleasedescribetheserestrictions,andprovidealinkorotheraccesspoint
to,orotherwisereproduce,anyrelevantlicensingterms,aswellasanyfeesassociatedwith
theserestrictions.
–Nowadays,GrabCutandBerkeleydatasetsareunavailablefromofficialsites,theycanbedownloaded
throughWebArchiveMachineorthroughRITMrepository.
Q50.Doanyexportcontrolsorotherregulatoryrestrictionsapplytothedatasetortoindividual
instances? Ifso,pleasedescribetheserestrictions,andprovidealinkorotheraccesspointto,
orotherwisereproduce,anysupportingdocumentation.
–No.
Q51. Anyothercomments?
–No.
E.7 Maintenance
Q52. Whowillbesupporting/hosting/maintainingthedataset?
–Ourteamwillmaintainthedataset.
Q53. Howcantheowner/curator/managerofthedatasetbecontacted(e.g.,emailaddress)?
–antonov@airi.net
Q54. Isthereanerratum? Ifso,pleaseprovidealinkorotheraccesspoint.
–Thereisnoerratumforourinitialrelease.
Q55. Willthedatasetbeupdated(e.g.,tocorrectlabelingerrors,addnewinstances,delete
instances)? Ifso,pleasedescribehowoften,bywhom,andhowupdateswillbecommunicated
tousers(e.g.,mailinglist,GitHub)?
–Wewillprobablyupdateourdatasetonanon-regularbasis.
Q56. Ifthedatasetrelatestopeople,arethereapplicablelimitsontheretentionofthedata
associatedwiththeinstances(e.g.,wereindividualsinquestiontoldthattheirdatawouldbe
retainedforafixedperiodoftimeandthendeleted)? Ifso,pleasedescribetheselimitsand
explainhowtheywillbeenforced.
–No.
Q57. Will older versions of the dataset continue to be supported/hosted/maintained? If so,
pleasedescribehow. Ifnot,pleasedescribehowitsobsolescencewillbecommunicatedtousers.
–AnewversionreleaseofRClickswillautomaticallydeprecateitspreviousversion. Wewillonly
supportandmaintainthelatestversionatalltimes.
Q58. Ifotherswanttoextend/augment/buildon/contributetothedataset,isthereamechanism
forthemtodoso? Ifso,pleaseprovideadescription. Willthesecontributionsbeverified? Ifso,
pleasedescribehow. Ifnot,whynot? Isthereaprocessforcommunicating/distributingthese
contributionstootherusers? Ifso,pleaseprovideadescription.
–AnyonecanextendRClicksifprovidinghigh-resolutionimageswithproperlyannotatedmasksand
clicksforit.Weareopentoacceptextensionsviapersonalcommunicationwithpotentialcontributors.
Otherwise, our code and data licenses allow others to create independent derivative works (with
properattribution).
38References
[1] Meng-HaoGuo,Cheng-ZeLu,QibinHou,ZhengningLiu,Ming-MingCheng,andShi-MinHu.
Segnext: Rethinkingconvolutionalattentiondesignforsemanticsegmentation. arXivpreprint
arXiv:2209.08575,2022. 15
[2] Konstantin Sofiiuk, Ilya A. Petrov, and Anton Konushin. Reviving iterative training with
maskguidanceforinteractivesegmentation. In2022IEEEInternationalConferenceonImage
Processing(ICIP),pages3141–3145,2022. doi: 10.1109/ICIP46576.2022.9897365. 15,26,27,
28,29,30,31,32,34
[3] ThanosDelatolas,VickyKalogeiton,andDimPPapadopoulos. Learningthewhatandhowof
annotationinvideoobjectsegmentation. InProceedingsoftheIEEE/CVFWinterConference
onApplicationsofComputerVision,pages6951–6961,2024. 21
[4] NikhilaRavi,ValentinGabeur,Yuan-TingHu,RonghangHu,ChaitanyaRyali,TengyuMa,
HaithamKhedr,RomanRädle,ChloeRolland,LauraGustafson,etal.Sam2:Segmentanything
inimagesandvideos. arXivpreprintarXiv:2408.00714,2024. 21,26,27,28,29
[5] DimP.Papadopoulos,AlasdairD.F.Clarke,FrankKeller,andVittorioFerrari. Trainingobject
classdetectorsfromeyetrackingdata. InEuropeanConferenceonComputerVision, 2014.
URLhttps://api.semanticscholar.org/CorpusID:14119147. 21
[6] DimPPapadopoulos,JasperRRUijlings,FrankKeller,andVittorioFerrari. Trainingobject
classdetectorswithclicksupervision. InProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition,pages6374–6383,2017. 21
[7] AmyBearman,OlgaRussakovsky,VittorioFerrari,andLiFei-Fei. What’sthepoint: Semantic
segmentation with point supervision. In European conference on computer vision, pages
549–565.Springer,2016. 21
[8] CarstenRother,VladimirKolmogorov,andAndrewBlake. Grabcut–interactiveforeground
extractionusingiteratedgraphcuts. ACMtransactionsongraphics(TOG),23(3):309–314,
2004. 26,27,28,29
[9] DavidMartin,CharlessFowlkes,DoronTal,andJitendraMalik. Adatabaseofhumanseg-
mentednaturalimagesanditsapplicationtoevaluatingsegmentationalgorithmsandmeasuring
ecologicalstatistics. InICCV,2001. 26,27,28,29
[10] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and
AlexanderSorkine-Hornung. Abenchmarkdatasetandevaluationmethodologyforvideoobject
segmentation. InCVPR,2016. 26,27,28,29
[11] Tsung-YiLin, MichaelMaire, SergeBelongie, JamesHays, PietroPerona, DevaRamanan,
PiotrDollár,andCLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InECCV,
2014. 26,27,28,29
[12] Andrey Moskalenko, Vlad Shakhuro, Anna Vorontsova, Anton Konushin, Anton Antonov,
AlexanderKrapukhin,DenisShepelev,andKonstantinSoshin. Tetris: Towardsexploringthe
robustnessofinteractivesegmentation. arXivpreprintarXiv:2402.06132,2024. 26,27,28,29,
34
[13] MinghaoZhou,HongWang,QianZhao,YuexiangLi,YawenHuang,DeyuMeng,andYefeng
Zheng. Interactive segmentation as gaussion process classification. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages19488–19497,2023.
26,27,28,29
[14] Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion
forinteractivesegmentation. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages7345–7354,2021. 26,27,28,29
[15] Jiacheng Lin, Jiajun Chen, Kailun Yang, Alina Roitberg, Siyu Li, Zhiyong Li, and Shutao
Li. Adaptiveclick: Click-aware transformer with adaptive focal loss for interactive image
segmentation. IEEETransactionsonNeuralNetworksandLearningSystems,2024. 26,27,28,
29
39[16] QinLiu,ZhenlinXu,GedasBertasius,andMarcNiethammer. Simpleclick: Interactiveimage
segmentationwithsimplevisiontransformers. arXivpreprintarXiv:2210.11006,2022. 26,27,
28,29,30,31,32
[17] Shoukun Sun, Min Xian, Fei Xu, Tiankai Yao, and Luca Capriotti. Cfr-icl: Cascade-
forwardrefinementwithiterativeclicklossforinteractiveimagesegmentation. arXivpreprint
arXiv:2303.05620,2023. 26,27,28,29
[18] ChaoningZhang,DongshenHan,YuQiao,JungUkKim,Sung-HoBae,SeungkyuLee,and
ChoongSeonHong. Fastersegmentanything: Towardslightweightsamformobileapplications.
arXivpreprintarXiv:2306.14289,2023. 26,27,28,29
[19] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,
TeteXiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. arXiv
preprintarXiv:2304.02643,2023. 26,27,28,29,30,31,32
[20] LeiKe,MingqiaoYe,MartinDanelljan,YifanLiu,Yu-WingTai,Chi-KeungTang,andFisher
Yu. Segmentanythinginhighquality. arXivpreprintarXiv:2306.01567,2023. 26,27,28,29,
30,31,32
[21] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna
Wallach,HalDauméIII,andKateCrawford. Datasheetsfordatasets. volume64,pages86–92,
2021. 34
40