Preprintversion. WorkinProgress.
MOH: MULTI-HEAD ATTENTION AS MIXTURE-OF-
HEAD ATTENTION
PengJin1,2‚àó,BoZhu4,LiYuan1,2,3(cid:12),ShuichengYan4(cid:12)
1SchoolofElectronicandComputerEngineering,PekingUniversity,Shenzhen,China
2PengChengLaboratory,Shenzhen,China
3RabbitpreIntelligence,Shenzhen,China
4Kunlun2050Research&SkyworkAI,Singapore
jp21@stu.pku.edu.cn, yuanli-ece@pku.edu.cn
Code:https://github.com/SkyworkAI/MoH
ABSTRACT
In this work, we upgrade the multi-head attention mechanism, the core of the
Transformer model, to improve efficiency while maintaining or surpassing the
previousaccuracylevel. Weshowthatmulti-headattentioncanbeexpressedinthe
summationform. Drawingontheinsightthatnotallattentionheadsholdequal
significance, we propose Mixture-of-Head attention (MoH), a new architecture
thattreatsattentionheadsasexpertsintheMixture-of-Experts(MoE)mechanism.
MoHhastwosignificantadvantages: First,MoHenableseachtokentoselectthe
appropriateattentionheads,enhancinginferenceefficiencywithoutcompromising
accuracyorincreasingthenumberofparameters. Second,MoHreplacesthestan-
dardsummationinmulti-headattentionwithaweightedsummation,introducing
flexibilitytotheattentionmechanismandunlockingextraperformancepotential.
ExtensiveexperimentsonViT,DiT,andLLMsdemonstratethatMoHoutperforms
multi-headattentionbyusingonly50%‚àº90%oftheattentionheads. Moreover,
wedemonstratethatpre-trainedmulti-headattentionmodels,suchasLLaMA3-8B,
canbefurthercontinue-tunedintoourMoHmodels. Notably,MoH-LLaMA3-8B
achieves an average accuracy of 64.0% across 14 benchmarks, outperforming
LLaMA3-8Bby2.4%byutilizingonly75%oftheattentionheads. Webelievethe
proposedMoHisapromisingalternativetomulti-headattentionandprovidesa
strongfoundationfordevelopingadvancedandefficientattention-basedmodels.
1 INTRODUCTION
SinceattentionisintroducedandbecomesafundamentalcomponentofTransformers(Vaswanietal.,
2017),multi-headattentionhasbeenthestandardarchitecturefornaturallanguageprocessing(Kenton
& Toutanova, 2019) and computer vision tasks (Dosovitskiy et al., 2021). It is well known that
using multiple heads can improve model accuracy. However, not all attention heads hold equal
significance. Someworkshaveshownthatmanyattentionheadscanbeprunedwithoutaffecting
accuracy. Forexample,Voitaetal.(2019)introducesamethodtoquantifytheusefulnessofeach
attentionheadandprunethosethatareredundant. Similarly, Micheletal.(2019)challengesthe
necessityofmultipleheadsbyexaminingtheimpactofextensivepruningacrossvarioussettings.
Thesefindingsdemonstratethatvanillamulti-headattentioncontainsredundantattentionheads.
Besides,inmulti-headattention,eachattentionheadoperatesinparallel,andthefinaloutputisthe
sum of all attention heads (please refer to Section 3.1). Given that these attention heads operate
independentlyandsomemayberedundant,wearguethatitispossibletobuildadynamicattention-
head routing mechanism. Such a mechanism would enable each token to adaptively select the
appropriateattentionheads,enhancinginferenceefficiencywithoutcompromisingaccuracy.
Tothisend,weintroduceMixture-of-Headattention(MoH),anewarchitecturethatintegratesmulti-
headattentionwiththeMixture-of-Experts(MoE)mechanism(Jacobsetal.,1991;Jinetal.,2024b).
*ThisworkwasperformedwhenPengJinwasanInternatSkyworkAI.Correspondingauthor:LiYuan,ShuichengYan.
1
4202
tcO
51
]VC.sc[
1v24811.0142:viXraPreprintversion. WorkinProgress.
Specifically, we propose to treat attention heads as experts within the MoE framework. Similar
toMoE,MoHconsistsofmultipleattentionheadsandarouterthatactivatestheTop-Kheadsfor
eachtoken. Moreover,wereplacethestandardsummationinmulti-headattentionwithaweighted
summation. Thisdesignofferstwosignificantadvantages: First,MoHallowseachtokentoselect
themostrelevantattentionheads, improvinginferenceefficiencywithoutsacrificingaccuracyor
increasingtheparameters. Second,byreplacingthestandardsummationinmulti-headattentionwith
aweightedsummation,MoHenhancestheflexibilityoftheattentionmechanismandincreasesthe
performancepotential. Moreover,toefficientlycapturecommonknowledgeacrossdifferentcontexts,
wedesignateasubsetofattentionheadsassharedheadsthatremainalwaysactivated.
WeevaluateourproposedMoHacrossvariouspopularmodelframeworks,includingVisionTrans-
formers (ViT) (Dosovitskiy et al., 2021) for image classification, Diffusion models with Trans-
formers(DiT)(Peebles&Xie,2023)forclass-conditionalimagegeneration,andLargeLanguage
Models(LLMs)(Brownetal.,2020;OpenAI,2022;Ouyangetal.,2022)forlanguagetasks. We
showthatMoHachievescompetitiveperformance,orevenoutperformsmulti-headattentionwith
only 50%‚àº90% of the attention heads. For example, MoH-ViT-B achieves 84.9%/84.7% Top-1
accuracyontheImageNet-1K(Dengetal.,2009)classificationbenchmark,surpassingwell-tuned
multi-headattentionbaselineswithonly75%/50%oftheattentionheads.
Furthermore, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-
8B (Dubey et al., 2024), can be further continue-tuned into our MoH models. Specifically, us-
ing only about 3% (400B tokens) of the original LLaMA3 pre-training data for continue-tuning,
MoH-LLaMA3-8Bachievesanaverageaccuracyof64.0%across14benchmarks,outperforming
LLaMA3-8Bby2.4%byutilizingonly75%oftheattentionheads. TheseresultsshowthatMoH
isapromisingalternativetovanillamulti-headattention,layingasolidfoundationfordeveloping
advancedandefficientattention-basedmodels. Themaincontributionsaresummarizedasfollows:
‚Ä¢ Weproposeadynamicattention-headroutingmechanismthatallowseachtokentoadaptively
selecttheappropriateattentionheads,enhancingmodelperformanceandinferenceefficiency
withoutincreasingthenumberofparameters.
‚Ä¢ Inadditiontotrainingfromscratch,wedemonstratethatpre-trainedmulti-headattention
models,suchasLLaMA3-8B,canbefurthercontinue-tunedintoourMoHmodels,greatly
enhancingtheapplicabilityoftheproposedMoHmethod.
‚Ä¢ Awiderangeofexperimentsacrossvariouspopularmodelframeworks,includingViT,DiT,
and LLMs, confirm that MoH is a promising alternative to vanilla multi-head attention,
layingasolidfoundationfordevelopingadvancedandefficientattention-basedmodels.
2 RELATED WORK
Multi-HeadAttention. Transformers(Vaswanietal.,2017)havegarneredsignificantinterestand
successinbothnaturallanguageprocessingandcomputervision. Thesuccessoftransformershas
beenlongattributedtothemulti-headattentionmechanism(Cordonnieretal.,2020). Multi-head
attention mechanism is proposed by Vaswani et al. (2017) to enhance the representation power
ofanattentionlayerbyallowingmultipleattentionheadstooperateondifferentlow-dimensional
projections of the input. The outputs from these heads are then concatenated to form the final
result. Alternatively,bydecomposingtheoutputprojectionmatrixbyrows,multi-headattention
can be expressed in a summation form. In summation form, each head operates in parallel, and
thefinaloutputisthesumofallheads. Inspiredbythisobservation,weproposeMoH,adynamic
attention-headroutingmechanismthatallowseachtokentoadaptivelyselecttheappropriateheads.
Mixture-of-ExpertsModels. TheMixture-of-Experts(MoE)method(Duetal.,2022;Lewisetal.,
2021;Rajbhandarietal.,2022;Rolleretal.,2021;Zhouetal.,2022;Jinetal.,2024b)isintroducedto
expandthecapacityofdeepneuralnetworkswithoutincreasingcomputationalcosts. Inthisapproach,
onlyasubsetofparameters,knownasexperts,isactivatedforeachinput. Shazeeretal.(2017)first
introducesanMoElayerbetweenLSTMlayers. SwitchTransformer(Fedusetal.,2022)further
simplifiesthegatingmechanismbyselectingonlytheTop-1expertpertoken. Gshard(Lepikhinetal.,
2021)improvestheTop-2expertroutingstrategy. IncontrasttoMoE,whichemphasizesefficient
parameterscalingwhilemaintainingmanageablecomputationalcosts,theproposedMoHfocuseson
reducingtheactivationofredundantattentionheadswithoutincreasingthenumberofparameters.
2Preprintversion. WorkinProgress.
Head1 Head‚Ñéùë† Head‚Ñéùë†+1 Head‚Ñé Head1 Head‚Ñéùë† Head‚Ñéùë†+1 Head‚Ñéùë†+2 Head‚Ñéùë†+3 Head‚Ñé Shared Heads
Head
Attention Attention
RoutedHeads
Head1 Head‚Ñéùë† Head‚Ñéùë†+1 Head‚Ñé Head1 Head‚Ñéùë† Head‚Ñéùë†+1 Head‚Ñéùë†+2 Head‚Ñéùë†+3 Head‚Ñé Head
Input Input Router
(a) Multi-Head Attention (b) Our proposed Mixture-of-Head Attention
Figure1: Ahigh-levelcomparisonbetweenthemulti-headattentionandourproposedmixture-
of-headattention. Subfigure(a)illustratesastandardmulti-headattentionlayerwithhattention
heads, while subfigure (b) demonstrates the Mixture-of-Head attention (MoH) architecture. It is
importanttonotethatMoHdoesnotincreasethenumberofattentionheads,ensuringthatthetotal
parameterforMoHiscomparabletothatofthemulti-headattention.
3 METHODOLOGY
Inthiswork,weaimtoreducetheactivationofredundantattentionheadswithoutincreasingthe
numberofparameters. Ahigh-levelcomparisonbetweenthevanillamulti-headattentionandour
proposedMixture-of-Headattention(MoH)ispresentedinFig.1.
3.1 MULTI-HEADATTENTION
Webeginbyreviewingthestandardmulti-headattentionmechanismintroducedbyVaswanietal.
(2017). Themulti-headattentionmechanismisbasedonscaleddot-productattention. Specifically,
forT tokensX ‚ààRT√ódin ofd indimensionseachandT‚Ä≤tokensX‚Ä≤ ‚ààRT‚Ä≤√ódin ofd indimensions
each,thescaleddot-productattentioniscomputedasfollows:
(cid:16)QK‚ä§(cid:17)
Attention(Q,K,V)=Softmax ‚àö V,
d (1)
k
Q=XW ,K =X‚Ä≤W , V =X‚Ä≤W ,
Q K V
whereW
Q
‚ààRdin√ódk,W
K
‚ààRdin√ódk,andW
V
‚ààRdin√ódv representtheprojectionmatricesfor
thequery,key,andvalue,respectively. Inself-attention,theinputtokensarethesame,i.e.,X‚Ä≤ =X,
anditiscommonforthekeyandvaluedimensionstobeequal,i.e.,d =d .
v k
Concatenation Form. To enhance the representation power of the attention layer, Vaswani
et al. (2017) proposes to allow multiple attention heads to operate on different low-dimensional
projectionsoftheinputtokens.Specifically,themulti-headattentionmechanismcomputeshdifferent
low-dimensional projections of (Q,K,V), performs scaled dot-product attention for each head,
concatenatestheresults,andappliesafinalprojectiontotheconcatenatedoutput. Theconcatenation
formofthemulti-headattentioncanbeformulatedas:
MultiHead(X,X‚Ä≤)=Concat(H1,H2,...,Hh)W ,
O
(2)
Hi =Attention(XWi,X‚Ä≤Wi ,X‚Ä≤Wi),
Q K V
where W Qi ‚àà Rdin√ódk/h, W Ki ‚àà Rdin√ódk/h, and W Vi ‚àà Rdin√ódv/h represent the i
th
projection
matricesforthequery,key,andvalue,respectively. W
O
‚ààRdv√ódout isthefinalprojectionmatrix.
SummationForm. Themulti-headattentionmechanismistypicallyrepresentedinitsconcatena-
tionform. However,fromanotherperspective,ifwedecomposeW
O
‚ààRdv√ódout byrows,wecan
expressmulti-headattentioninasummationform. Specifically,W canbedividedintohmatrices
O
byrows,i.e.,[W O1,W O2,...,W Oh]=W O,whereW Oi ‚ààRdv/h√ódout. Finally,thesummationform
ofthemulti-headattentioncanthenbeformulatedas:
h
(cid:88)
MultiHead(X,X‚Ä≤)= HiWi. (3)
O
i=1
3Preprintversion. WorkinProgress.
Theconcatenationformcanbeviewedasavariantofthesummationform,wherethesumofthe
dimensionsofallattentionheadsisexactlyequaltothehiddensize. AsshowninEq.3,instandard
multi-head attention, each attention head operates in parallel, and the final output is the sum of
all attention heads. Since these attention heads function independently, we can build a dynamic
attention-headroutingmechanismallowingeachtokentoadaptivelyselectthemostrelevantattention
heads,improvinginferenceefficiencywithoutcompromisingaccuracy.
3.2 MIXTURE-OF-HEADATTENTION
Recently,theMixture-of-Experts(MoE)methodhasemergedasapopularapproachforscalingthe
parametersoflargelanguagemodels(Jiangetal.,2024). AtypicalMoElayerconsistsofmultiple
expertnetworksandarouterthatactivatestheTop-Kexperts. Generally,thenumberofactivated
expertsK issignificantlysmallerthanthetotalnumberofexpertstoensureinferenceefficiency.
Heads as Experts. Inspired by the great success of MoE, we propose Mixture-of-Head at-
tention (MoH), which treats attention heads as experts. Specifically, MoH consists of h heads
H ={H1,H2,...,Hh}andarouterthatactivatestheTop-Kheads. Formally,giveninputtokensX
andX‚Ä≤,theoutputofMoHistheweightedsumofoutputsfromtheK selectedheads:
h
(cid:88)
MoH(X,X‚Ä≤)= g HiWi, (4)
i O
i=1
whereg representstheroutingscore. g isnon-zeroonlywhenthei attentionheadisactivated.
i i th
Thisdesignprovidestwokeyadvantages: Ontheonehand,MoHenableseachtokentoselectthe
mostrelevantattentionheads,boostinginferenceefficiencywhilemaintainingaccuracy. Ontheother
hand,incontrasttothestandardsummationinmulti-headattention,theweightedsummationinMoH
enhancestheflexibilityoftheattentionmechanismandunlocksperformancepotential.
SharedHeads. Inattentionmechanism,someattentionheadsmaycapturecommonknowledge
acrossdifferentcontexts,suchasgrammaticalrulesinlanguage. InspiredbyDaietal.(2024),we
designateasubsetofheadsassharedheadsthatremainalwaysactivated. Byconsolidatingcommon
knowledgewithinsharedheads,wereduceredundancyamongtheotherdynamicallyroutedheads.
Two-StageRouting. Moreover,todynamicallybalancetheweightsbetweensharedandrouted
heads, we propose a two-stage routing strategy. In this routing strategy, the routing scores are
determinedbyboththescoreofeachindividualheadandthescoreassociatedwiththeheadtype.
Specifically,giventhet thinputtokenx
t
‚ààRdin inX ‚ààRT√ódin,theroutingscoreg iisdefinedas:
Ô£±
Œ± Softmax(W x ) , if1‚â§i‚â§h ,
Ô£≤ 1 s t i s
(cid:0) (cid:1)
g = Œ± Softmax(W x ) , if(W x ) ‚ààTop-K {(W x ) |h +1‚â§i‚â§h} , (5)
i 2 r t i r t i r t i s
Ô£≥0, otherwise,
whereh sdenotesthenumberofsharedheads. W
s
‚ààRhs√ódin andW
r
‚ààR(h‚àíhs)√ódin representthe
projectionmatricesforthesharedandroutedheads,respectively. ThecoefficientsŒ± andŒ± balance
1 2
thecontributionsofthesharedandroutedheads,andaredefinedas:
[Œ± ,Œ± ]=Softmax(W x ), (6)
1 2 h t
whereW
h
‚ààR2√ódin isthetrainableprojectionmatrix,andd inisthehiddensizeofx t.
LoadBalanceLoss DirectlytraininganMoElayeroftencausesthemajorityoftokenstoberouted
toasmallnumberofexperts,leavingtheremainingexpertsinsufficientlytrained(Shazeeretal.,2017).
ToavoidtheunbalancedloadintheproposedMoH,followingpreviousMoEmethods(Lepikhin
etal.,2021;Weietal.,2024), weapplyaloadbalanceloss. Specifically, forthe t inputtoken
th
x
t
‚ààRdin inX ‚ààRT√ódin,theloadbalancelossL bisformulatedas:
h T T
(cid:88) 1 (cid:88) 1 (cid:88)
L = f P , f = 1(Tokenx selectsHeadi), P = Softmax(W x ) , (7)
b i i i T t i T r t i
i=hs+1 t=1 t=1
whereT denotesthenumberoftokens. 1(‚àó)denotestheindicatorfunction.
TotalTrainingObjective. ItisworthnotingthattheMoHisageneralframework. Therefore,
weevaluateourproposedMoHacrossvariouspopularmodelframeworks,includingVisionTrans-
formers(ViT),DiffusionmodelswithTransformers(DiT),andLargeLanguageModels(LLMs).
4Preprintversion. WorkinProgress.
Table1: Comparisonstocurrentstate-of-the-artmethodsonImageNet-1Kclassification. All
models are trained exclusively on the ImageNet-1K training set. Our MoH-ViT models, based
onTransNeXt(Shi,2024), aretrainedfor300epochsusingaresolutionof224√ó224. Toensure
a fair comparison, we only replace the standard multi-head attention with our Mixture-of-Head
attention(MoH),keepingallothertrainingparametersidenticaltoTransNeXt.
#Params#ActivatedAcc #Params#ActivatedAcc
Methods Methods
(M) Heads(%)(%) (M) Heads(%)(%)
DeiT-S(Touvronetal.,2021) 22 100 79.8 DeiT-B(Touvronetal.,2021) 86 100 81.8
T2T-ViT-19(Yuanetal.,2021) 39 100 81.9 T2T-ViT-24(Yuanetal.,2021) 64 100 82.3
Swin-S(Liuetal.,2021) 50 100 83.1 Swin-B(Liuetal.,2021) 88 100 83.5
PVTv2-B3(Wangetal.,2022) 45 100 83.2 PVTv2-B5(Wangetal.,2022) 82 100 83.8
CoAtNet-1(Daietal.,2021) 42 100 83.3 Focal-B(Yangetal.,2021) 90 100 83.8
Focal-S(Yangetal.,2021) 51 100 83.5 FocalNet-B(Yangetal.,2022b) 89 100 83.9
FocalNet-S(Yangetal.,2022b) 50 100 83.5 CoAtNet-2(Daietal.,2021) 75 100 84.1
MViTv2-S(Lietal.,2022) 35 100 83.6 MViTv2-B(Lietal.,2022) 52 100 84.4
UniFormer-B(Lietal.,2023b) 50 100 83.9 MOAT-2(Yangetal.,2022a) 73 100 84.7
CAFormer-S36(Yuetal.,2023) 39 100 84.5 iFormer-L(Sietal.,2022) 87 100 84.8
TransNeXt-S(Shi,2024) 50 100 84.7 TransNeXt-B(Shi,2024) 90 100 84.8
MoH-ViT-S 50 80 84.7 MoH-ViT-B 90 75 84.9
MoH-ViT-S 50 75 84.6 MoH-ViT-B 90 50 84.7
Dependingonthespecifictask,werequirethetask-specificloss. Finally,thetotaltraininglossisthe
weightedsumofthetask-specificlossL andtheloadbalancelossL :
task b
L=L +Œ≤L , (8)
task b
whereŒ≤ isthetrade-offhyper-parametertomitigatetheriskofroutingcollapse. Bydefault, the
weightŒ≤ fortheloadbalancelossissetto0.01foralltasks.
4 EXPERIMENTS
4.1 VITFORIMAGECLASSIFICATION
ModelSettings. ForVisionTransformers(ViT)(Dosovitskiyetal.,2021),ourMoH-ViTmodels
areimplementedbasedontheTransNeXt(Shi,2024)frameworkandtrainedfromscratchonthe
ImageNet-1Kdataset(Dengetal.,2009),whichcontainsover1.2millionimagesin1,000categories.
Toensureafaircomparison,weonlyreplacethestandardmulti-headattentionwiththeproposed
MoH,whilekeepingallothertrainingparametersidenticaltoTransNeXt.
Training Details. Our MoH-ViT models are trained for 300 epochs using automatic mixed
precisionacross8GPUs. WefollowthetrainingstrategyofTransNeXt,whichincludesvariousdata
augmentationtechniques, includingRandomAugmentation(Cubuketal.,2020), Mixup(Zhang,
2017),CutMix(Yunetal.,2019),andRandomErasing(Zhongetal.,2020). WealsoapplyLabel
Smoothing(Szegedyetal.,2016)andDropPath(Huangetal.,2016)toregularizeourmodels. We
optimizeourmodelsusingAdamWoptimizer(Loshchilov&Hutter,2017)withagradientclipping
norm of 1.0 and a weight decay of 0.05. The initial learning rate is set to 1e-3, with a 5-epoch
warm-upstartingat1e-6. Acosinelearningratescheduler(Loshchilov&Hutter,2016)isemployed
todecaythelearningrate. Duringtraining,imagesarerandomlycroppedtoasizeof224√ó224. Itis
worthnotingthatwedonotuseExponentialMovingAverage(EMA)weights.
Results. AsshowninTab.1,despiteactivatingonlyasubsetofattentionheads,MoH-ViTachieves
highlycompetitiveperformancecomparedtocurrentstate-of-the-artmethods. Forexample,MoH-
ViT-Bachieves84.9%Top-1accuracyontheImageNet-1Kclassificationbenchmarkwithjust75%
oftheattentionhead. Incontrast,thewell-establishedViTbaseline,TransNeXt,attainsaslightly
loweraccuracyof84.8%whilerequiring100%oftheheadstobeactivated. Tab.1demonstratesthat
MoH-ViToutperformsothermodelswithfeweractivatedattentionheads. ThissuggeststhatMoHis
apromisingalternativetovanillamulti-headattentionforvisionmodeldesign,offeringthepotential
forcompetitiveperformancewithmoreefficientattentionheadusage.
5Preprintversion. WorkinProgress.
Table2: ComparisonstoDiTonthebenchmarkingofclass-conditionalimagegenerationon
ImageNet-1Kat256√ó256resolution. Toensureafaircomparison,weonlyreplacethestandard
multi-headattentionwiththeMoHinMoH-DiTmodels,whilekeepingallothertrainingparameters
identicaltoDiT.‚Äú400K‚Äùdenotesthetrainingbudgetis400Ktrainingsteps.
#Params #Activated
Methods FID‚Üì sFID‚Üì IS‚Üë Precision‚Üë Recall‚Üë
(M) Heads(%)
DiT-S/2400K(Peebles&Xie,2023) 33 100 68.40 - - - -
MoH-DiT-S/2400K 33 90 67.25 12.15 20.52 0.37 0.58
MoH-DiT-S/2400K 33 75 69.42 12.85 19.96 0.36 0.55
DiT-B/2400K(Peebles&Xie,2023) 130 100 43.47 - - - -
MoH-DiT-B/2400K 131 90 43.40 8.40 33.51 0.49 0.63
MoH-DiT-B/2400K 131 75 43.61 8.48 33.43 0.49 0.62
DiT-L/2400K(Peebles&Xie,2023) 458 100 23.33 - - - -
MoH-DiT-L/2400K 459 90 23.17 6.16 58.92 0.61 0.63
MoH-DiT-L/2400K 459 75 24.29 6.38 57.75 0.60 0.63
Table 3: Comparisons to current state-of-the-art methods on the benchmarking of class-
conditionalimagegenerationonImageNet-1Kat256√ó256resolution. ‚Äú‚Üë‚Äùdenotesthathigheris
better. ‚Äú‚Üì‚Äùdenotesthatlowerisbetter. ‚Äúcfg‚Äùdenotestheclassifier-freediffusionguidancescale. We
extendthetrainingbudgetofourMoH-DiT-XL/2to7,000Ktrainingsteps,aligningitwithDiT-XL/2.
#Activated
Methods FID‚Üì sFID‚Üì IS‚Üë Precision‚Üë Recall‚Üë
Heads(%)
ADM-G,ADM-U(Dhariwal&Nichol,2021) - 3.94 6.14 215.84 0.83 0.53
CDM(Hoetal.,2022) - 4.88 - 158.71 - -
LDM-8(Rombachetal.,2022) - 15.51 - 79.03 0.65 0.63
LDM-4(Rombachetal.,2022) - 10.56 - 103.49 0.71 0.62
LDM-4-G(cfg=1.25) - 3.95 - 178.22 0.81 0.55
DiT-XL/27,000K(Peebles&Xie,2023) 100 9.62 6.85 121.50 0.67 0.67
DiT-XL/27,000K(cfg=1.25) 100 3.22 5.28 201.77 0.76 0.62
MoH-DiT-XL/22,000K 75 10.95 6.19 106.69 0.67 0.66
MoH-DiT-XL/22,000K 90 10.67 6.15 107.80 0.67 0.65
MoH-DiT-XL/27,000K 90 8.56 6.61 129.54 0.68 0.67
MoH-DiT-XL/27,000K(cfg=1.25) 90 2.94 5.17 207.25 0.77 0.63
4.2 DITFORCLASS-CONDITIONALIMAGEGENERATION
ModelSettings. ForDiffusionmodelswithTransformers(DiT)(Peebles&Xie,2023),weonly
replace the standard multi-head attention with our MoH in MoH-DiT models, while keeping all
othertrainingparametersidenticaltoDiT.WeusetheImageNet-1Kdataset(Dengetal.,2009)for
class-conditionalimagegenerationataresolutionof256√ó256.
Training Details. Following DiT, the final linear layer is initialized with zeros, and all other
layers follow standard ViT weight initialization. We train all models using the AdamW opti-
mizer (Loshchilov & Hutter, 2017) with a constant learning rate of 1e-4, no weight decay, and
a batch size of 256, applying horizontal flips for data augmentation. Following DiT, we employ
theExponentialMovingAverage(EMA)ofMoH-DiTweightsduringtrainingwithadecayrate
of0.9999,generatingallimagesusingtheEMAmodel. Weuseanoff-the-shelfpre-trainedvaria-
tionalautoencoder(Kingma,2013)modelfromStableDiffusion(Rombachetal.,2022). Following
TransNeXt,ourattention-headactivationbudgetisunevenlydistributedacrosslayers,withfewer
attentionheadsactivatedintheshallowlayersandmoreinthedeeperlayers.
Evaluation Benchmarks. To evaluate generation performance, we use Frechet Inception
Distance (FID) (Heusel et al., 2017) to assess overall sample quality, Precision and Re-
call (Kynka¬®a¬®nniemi et al., 2019) to measure fidelity and diversity separately, and sFID (Nash
etal.,2021)asametricthatbettercapturesspatialrelationshipsthanFID.Moreover,weuseInception
Score(IS)(Salimansetal.,2016)asanothermetricforfidelity.
6Preprintversion. WorkinProgress.
Table4: ComparisonsbetweenMoH-LLMsandvanillaLLMs. ‚Äú100B‚Äùdenotesatrainingbudget
of100billiontokens,while‚Äú200B‚Äùdenotesabudgetof200billiontokens. Weobservethatlarger
models,e.g.,MoH-LLM-B,generallyperformworsethansmallermodels,e.g.,MoH-LLM-S,on
TruthfulQA,consistentwiththefindingsreportedbyLinetal.(2022).
#Activated LanguageTasks
Methods Avg.
Heads(%) SciQ PIQA WinoGrande OpenbookQA LogiQA TruthfulQA
LLM-S100B 100 63.0 63.1 51.1 27.4 26.9 31.6 43.9
MoH-LLM-S100B 75 64.7 62.0 50.6 28.8 26.4 35.2 44.6
MoH-LLM-S100B 50 67.0 62.2 51.5 29.2 26.7 35.6 45.4
LLM-B100B 100 73.1 69.7 52.0 31.8 28.4 29.5 47.4
MoH-LLM-B100B 75 74.7 69.2 52.8 30.0 28.1 32.2 47.8
MoH-LLM-B100B 50 75.2 67.0 52.0 29.0 26.9 32.8 47.2
LLM-B200B 100 73.1 70.3 53.3 32.4 29.0 29.5 47.9
MoH-LLM-B200B 75 76.0 69.2 52.7 30.4 29.8 32.6 48.5
MoH-LLM-B200B 50 75.6 66.9 53.5 29.4 26.7 32.7 47.5
Results. ToconductcomparativeevaluationsofourproposedMoH-DiTmodelsagainstvanilla
DiTmodels,westartwithSmallmodelsandexpandtoXLargemodels. AsshowninTab.2,MoH-
DiT models consistently outperform vanilla DiT models with 90% of attention heads activated.
However, when only 75% of the attention heads are activated, MoH-DiT models perform worse
thanDiTmodelswith100%ofattentionheadsactivated. Thismaybebecauseimagegeneration
tasksaredensepredictiontasksthatrequireattentionmechanismstocapturepixel-levelfine-grained
relationships,leavinglessredundancyintheattentionheadscomparedtoimageclassificationtasks.
Moreover,weextendthetrainingbudgetofourMoH-DiT-XL/2to7,000Ktrainingsteps,aligningit
withDiT-XL/2. AsshowninTab.3,despiteactivating90%attentionheads,MoH-DiT-XL/2achieves
highlycompetitiveperformancecomparedtocurrentstate-of-the-artmethods. Theseresultssuggest
thatMoHisapromisingalternativetomulti-headattentionfordiffusionmodels.
4.3 TRAININGLLMSFROMSCRATCH
Model Settings. For training LLMs from scratch, we use Megatron (Shoeybi et al., 2019), an
open-source training code, as the training framework. Please refer to the Appendix for detailed
hyper-parametersettings(Tab.A)ofvariousMoH-LLMs. AllmodelsaretrainedwiththeAdamW
optimizer(Loshchilov&Hutter,2017),usingabatchsizeof4milliontokenswithasequencelength
of2048. Thefinallearningrateissetto10%ofthemaximum. Duringtraining,aweightdecayof0.1
andgradientclippingof1.0areapplied. ForLLM-SandMoH-LLM-S,themaximumlearningrateis
setto3e-4. ForLLM-BandMoH-LLM-B,themaximumlearningrateissetto5e-4.
TrainingDetails. Weonlyusepublicdatasetsfortraining,ensuringaccessibilityforacademic
research. Specifically,wesamplefromtheRedPajama(Computer,2023),Dolma(Soldainietal.,
2024), and Pile (Gao et al., 2020) datasets according to different sampling probabilities. Please
refertotheAppendixfordetailedsampleratios(Tab.B).Followingpreviousworks,weutilizethe
tokenizerfromLLaMA2(Touvronetal.,2023),whichcontains65,536vocabularytokens.
EvaluationBenchmarks. TheevaluationisperformedonmultiplebenchmarksusingtheEleuther
AILanguageModelEvaluationHarness(Gaoetal.,2024),aunifiedframeworkfortestinggenerative
languagemodels. Sincetheparametersareonlyabout0.2Bforthesmallestmodel,weselect6simple
benchmarks as the metric. Specifically, we report 0-shot accuracy on SciQ (Welbl et al., 2017),
PIQA(Bisketal.,2020),WinoGrande(Sakaguchietal.,2021),OpenbookQA(Mihaylovetal.,
2018),LogiQA(Liuetal.,2020),andTruthfulQA(Linetal.,2022).
Results. As shown in Tab. 4, despite activating only a subset of attention heads, MoH-LLMs
achievehighlycompetitiveperformancecomparedtoourbaselinemodels. Forexample,MoH-LLM-
Sachievesanaverageaccuracyof45.4%withjust50%oftheattentionheadsactivated. Incontrast,
thebaselinemodelreachesaslightlyloweraccuracyof43.9%with100%oftheattentionheads
activated. TheseresultssuggestthatMoHisapromisingalternativetovanillamulti-headattention
fortrainingLLMsfromscratch. Surprisingly,wefindthatforMoH-LLM-S,activatingonly50%
oftheattentionheadsoutperformsactivating75%. Weconsideritmaybebecausewhenboththe
7Preprintversion. WorkinProgress.
Table5: ComparisonsbetweenMoH-LLaMA3-8BandLLaMA3-8B.PleaserefertotheAppendix
fortheperformanceofthemodelattheendofthefirststageoftraining.
#Activated
Methods MMLU(5) CEVAL(5) CMMLU(5)GSM8K(8)TruthfulQA
Heads(%)
LLaMA3-8B(Dubeyetal.,2024) 100 65.2 52.3 50.7 49.5 35.4
MoH-LLaMA3-8B 75 65.8 61.5 64.4 56.9 44.0
#Activated
Methods HellaSwag(10) LogiQA BoolQ(32) LAMBADA SciQ
Heads(%)
LLaMA3-8B(Dubeyetal.,2024) 100 81.9 30.0 83.9 75.5 94.0
MoH-LLaMA3-8B 75 80.1 30.3 84.0 76.4 92.2
#Activated
Methods PIQA WinoGrande NQ(32) ARC-C(25) Average
Heads(%)
LLaMA3-8B(Dubeyetal.,2024) 100 81.0 72.5 31.5 59.0 61.6
MoH-LLaMA3-8B 75 78.8 72.9 28.3 60.1 64.0
y y y
c c c
a a a
ru ru ru
c c c
c c c
A A A
MMLU (5) HellaSwag(10) TruthfulQA
Training Tokens (B) Training Tokens (B) Training Tokens (B)
Figure2: Performanceevolutionduringcontinue-tuning. TheMoHmodelquicklyrecoversto
over95%oftheperformanceoftheoriginalmodelwithinatrainingbudgetof10Btokens. Then,the
performancegraduallyimproveswiththeincreaseofthetrainingtokens.
modelanddatasetaresmall,activatingfewerheadseffectivelyregularizesthemodel. However,as
theamountofdataincreases,activatingmoreheadsoffersahigherpotentialforperformance.
4.4 CONTINUE-TUNINGLLAMA3-8B
Model Settings. To significantly enhance the applicability of the proposed MoH method, we
alsoattempttofurthercontinue-tunepre-trainedmulti-headattentionmodels,suchasLLaMA3-8B,
intoMoHmodels. However,thispresentsthreechallenges. (i)Determiningthesharedattention
heads: Wesimplyselectthefirst16attentionheadsofeachlayerassharedheads. (ii)Addinghead
routers: Integratingarandomlyinitializedrouterintothepre-trainedmodelwithoutcompromising
itsoriginalperformancerequirescarefultrainingtechniques. Toaddressthis,weproposeaparameter-
free router that determines routing scores using the ‚Ñì norm of the query of each attention head.
2
(iii)Weightingattentionheads: Weobservethatweightingtheattentionheadoutputssignificantly
altersthedistributionoftheoutputoftheattentionlayer,whichnecessitatesalargeamountoftraining
datatorestoretheoriginalperformance. Totacklethis,wequantizetheroutingscoreandusethe
straight-through estimator (Bengio et al., 2013; Liu et al., 2022) to back-propagate the gradients
through the sparsity function. Specifically, given the input token x, we employ a quantizer for
activationroutingscores,withitsforwardpassformulatedas:
gq =1(TokenxselectsHeadi), (9)
i
where1(‚àó)denotestheindicatorfunction. gq representsthequantizedroutingscore. Wethenadopt
i
astraight-throughestimator,whichassignstheincominggradientstoathresholdoperationtobethe
outgoinggradients,whichisformulatedas:
‚àÇL ‚àÇL
= , (10)
‚àÇgq ‚àÇg
i i
whereg denotesthereal-valuedroutingscore. Thissimpleapproximationfunctionsignificantly
i
mitigatestheissueofgradientvanishing(Wangetal.,2024). SimilartotrainingLLMsfromscratch,
wealsouseMegatron(Shoeybietal.,2019),anopen-sourcetrainingcode,asthetrainingframework.
8Preprintversion. WorkinProgress.
Table 6: Ablation study on the impact of each component of the proposed MoH. The image
classificationresultsarefromMoH-ViT-S,byutilizing75%oftheattentionheadswithatraining
budgetof100epochs.Theclass-conditionalimagegenerationresultscomefromMoH-DiT-S/2-400K,
alsobyusing75%oftheattentionheads,withatrainingbudgetof400Ktrainingsteps.
Shared Two-Stage ImageClassification Class-ConditionalImageGeneration
Heads Routing Acc(%)‚Üë FID‚Üì sFID‚Üì IS‚Üë Precision‚Üë Recall‚Üë
75.6 71.97 13.58 19.06 0.35 0.55
‚úì 78.3 69.54 12.80 19.67 0.36 0.55
‚úì ‚úì 78.6 69.42 12.85 19.96 0.36 0.55
Table7: Ablationstudyontheimpactofthesharedheadsratioamongactivatedheads. All
resultsarefromMoH-ViT-S,byusing75%oftheheadswithatrainingbudgetof100epochs.
RatioofSharedHeads 13.9% 27.6% 31.3% 35.9% 37.5% 40.5% 46.8% 60.4% 74.0%
Accuracy(%) 78.6 78.5 78.4 78.4 78.5 78.6 78.4 78.6 78.4
TrainingDetails. Wefindthatifthereisadiscrepancybetweenthecontinue-trainingdataandthe
originaltrainingdatadistributionofthemodel,theperformanceofthemodelmayfluctuatewildlyat
thebeginningofthetrainingprocess. Sinceweareunabletohaveaccesstotherawtrainingdataof
LLaMA3,weaddressthesepotentialperformancefluctuationsbydividingthetrainingprocessinto
twostages. Inthefirststage,wecontinue-tunetheoriginalLLaMA3-8Bmodelusing300Btokensto
adaptthemodeltoourdataset. Inthesecondstage,wecontinue-tunethisadaptedmodelintoour
proposedMoHmodelwith100Btokens. Duringthefirststage,themaximumlearningrateissetto
6e-5,andthefinallearningrateis6e-6. Inthesecondstage,themaximumlearningrateissetto2e-5,
andthefinallearningrateis1e-6. Forbothstages,weemploytheAdamWoptimizer(Loshchilov&
Hutter,2017),withabatchsizeof16milliontokenswithasequencelengthof8192. Duringtraining,
weuseaweightdecayof0.1andgradientclippingof1.0.
Evaluation Benchmarks. We use the Eleuther AI Language Model Evaluation Harness (Gao
et al., 2024) to evaluate models on multiple key benchmarks. Specifically, we utilize the lm-
evaluation-harnesspackagetoassessperformanceonacomprehensivesuiteofdownstreamtasks:
(i)FollowingPythia(Bidermanetal.,2023),wereport0-shotaccuracyonLAMBADA(Paperno
etal.,2016),LogiQA(Liuetal.,2020),PIQA(Bisketal.,2020),SciQ(Welbletal.,2017),and
WinoGrande(Sakaguchietal.,2021). (ii)WereporttheaccuracyofChinesetasks,including5-shot
CEVAL(Huangetal.,2023)and5-shotCMMLU(Lietal.,2023a). (iii)Wereporttheaccuracyof
tasksfromtheOpenLLMLeaderboard(Beechingetal.,2023),including10-shotHellaSwag(Zellers
etal.,2019),25-shotARCChallenge(ARC-C)(Clarketal.,2018),and5-shotMMLU(Hendrycks
etal.,2021). (iv)Wereporttheexactmatchscorefor32-shotNaturalQuestions(NQ)(Kwiatkowski
etal.,2019)andtheaccuracyfor32-shotBoolQ(Clarketal.,2019). (v)Wereporttheexactmatch
scorefor8-shotGSM8K(Cobbeetal.,2021)toevaluatethemathability. (vi)Moreover,wereport
0-shotaccuracyonTruthfulQA(Linetal.,2022)toassesstheabilitytogeneratetruthfulanswers.
Results. AsshowninFig.2,MoH-LLaMA3-8Bquicklyrecoverstoover95%oftheperformance
oftheoriginalmodelwithinatrainingbudgetof10Btokens. Aftercontinue-tuningwith100Btokens,
asshowninTab.5,MoH-LLaMA3-8Bachievesanaverageaccuracyof64.0%across14benchmarks,
outperformingLLaMA3-8Bby2.4%byutilizingonly75%oftheattentionheads. Theseresults
demonstratethatpre-trainedmulti-headattentionmodelscanbefurthercontinue-tunedintoourMoH
models,significantlyenhancingtheapplicabilityoftheMoHmethod.
4.5 ABLATIVEANALYSIS
EffectofEachComponentoftheProposedMoH. Toexploretheimpactofeachcomponentof
ourMoHmethod,weprovidetheablationresultsinTab.6. ‚ÄúSharedHeads‚Äùreferstoasubsetof
attentionheadsthatarealwaysactivated. ‚ÄúTwo-StageRouting‚Äùrepresentsthedynamiccoefficient
thatbalancestheweightsbetweensharedandroutedheadsovertheroutingscore,asdescribedin
Eq. 5 and Eq. 6. As shown in Tab. 6, shared heads significantly improve model performance by
effectivelycapturingcommonknowledge,allowingtheroutedheadstofocusmoreondomain-specific
information. Moreover, two-stage routing further enhances model performance by dynamically
9Preprintversion. WorkinProgress.
MoH-ViT-B MoH-DiT-XL/2 MoH-LLM-B
Desk Desk LogiQA
y y y
tisn tisn tisn
eD eD eD
Routed Heads ID Routed Heads ID Routed Heads ID
Goldfish Goldfish PIQA
y y y
tisn tisn tisn
eD eD eD
Routed Heads ID Routed Heads ID Routed Heads ID
Ice cream Ice cream WinoGrande
y y y
tisn tisn tisn
eD eD eD
Routed Heads ID Routed Heads ID Routed Heads ID
Figure3: VisualizationoftheheadloaddistributioninthefinalMoHlayer. ForViTandDiT,we
presenttheheadloaddistributionsforthecategories‚ÄúDesk‚Äù,‚ÄúGoldfish‚Äù,and‚ÄúIcecream‚Äù. ForLLM,
wedisplaytheheaddistributionsforthetasks‚ÄúLogiQA‚Äù,‚ÄúPIQA‚Äù,and‚ÄúWinoGrande‚Äù. MoH-ViT-B,
MoH-DiT-XL/2,andMoH-LLM-Bactivate75%,90%,and75%oftheattentionheads,respectively.
balancingtheweightsbetweensharedandroutedheads.Ourfullmodelachievesthebestperformance,
demonstratingthatbothcomponentssignificantlybenefittheattentionmechanism.
EffectoftheSharedHeadsRatioamongActivatedHeads. InTab.7,weprovidetheablation
studyonthesharedheadsratioamongactivatedheads. Wefindthatmodelperformanceremains
relativelyconsistentacrossawiderangeofsharedheadsratios(from13.9%to74.0%). Theseresults
indicatethattheperformanceofthemodelisstableaslongasthesharedheadsratioisnotextreme.
From another perspective, shared heads can be viewed as a form of Soft MoE (Puigcerver et al.,
2024). BasedonthefindingsfromtheSoftMoEpaper(Puigcerveretal.,2024),werecommend
usingahigherratioofsharedheadsamongtheactivatedheads(greaterthan40%).
5 DISCUSSION
VisualizationoftheHeadLoadDistribution. AsshowninFig.3,weobservesignificantvariation
inattentionheadassignmentsacrossdifferentcategoriesandtasktopics,indicatingthattheMoH
modeladaptstodiversetasksbyemployingdistinctheadassignmentpatterns. Thischaracteristicof
MoHallowsdifferentattentionheadstofocusondifferenttypesoftasks,makingparameterutilization
moreefficientthanmulti-headattention. ForadditionalvisualizationsofMoH-LLaMA3-8Banda
detailedanalysisoftheheadloaddistribution,pleaserefertoAppendixD.
The Difference between MoH and MoA. We clarify the differences between MoH and
MoA (Zhang et al., 2022) from the following three aspects. First, in terms of motivation, the
goal of MoH is to improve the efficiency and performance of the attention mechanism without
increasingthenumberofparameters. Incontrast,MoAsharesthemotivationofMoE,whichisto
expandmodelparameterswhilekeepinginferencecostslow. Therefore,themodelsettingsofMoH
are more stringent than those of MoA. Second, in terms of methodology, our MoH introduces
sharedheadsandtwo-stageroutingtoenhancethestandardMoEmethod. Moreimportantly,weshow
thatpre-trainedmulti-headattentionmodelscanbefurthercontinue-tunedintoourMoHmodels,
greatlyimprovingtheapplicabilityoftheproposedMoHmethod. Incontrast,MoAdirectlycombines
multi-headattentionwithMoE.Duetotheadoptionofsharedkeysandvalues,MoAmustbetrained
fromscratch,whichlimitsitsapplicability. Finally,intermsofmodelframeworks,ourMoHis
validatedacrossvariouspopularmodelframeworksandtasks,includingViT,DiT,anddecoder-only
LLMs,whileMoAisonlyvalidatedontheencoder-decoderarchitectureforlanguagetasks.
6 CONCLUSION
In this paper, we introduce MoH, a promising alternative to multi-head attention. MoH enables
eachtokentoadaptivelyselecttheappropriateattentionheads,improvingbothmodelperformance
and inference efficiency without increasing the number of parameters. Extensive experiments
10Preprintversion. WorkinProgress.
acrossvariouspopularmodelframeworks,includingViT,DiT,andLLMs,demonstratethatMoH
outperformsmulti-headattention,evenwhenusingonly50%‚àº90%oftheattentionheads. More
encouragingly,weshowthatpre-trainedmulti-headattentionmodels,suchasLLaMA3-8B,canbe
furthercontinue-tunedintoourMoHmodels,significantlyenhancingtheapplicabilityoftheproposed
MoHmethod. Thisworkrepresentsapromisingsteptowardadvancedandefficientattention-based
models,whichmaybemeaningfulandhelpfultoboththeresearchandindustrialcommunities.
REFERENCES
Edward Beeching, Cle¬¥mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen
Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard
(2023-2024). https://huggingface.co/spaces/open-llm-leaderboard-old/
open_llm_leaderboard,2023.
YoshuaBengio,NicholasLe¬¥onard,andAaronCourville. Estimatingorpropagatinggradientsthrough
stochasticneuronsforconditionalcomputation. arXivpreprintarXiv:1308.3432,2013.
StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO‚ÄôBrien,Eric
Hallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal.
Pythia: Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. InICML,pp.
2397‚Äì2430,2023.
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical
commonsenseinnaturallanguage. InAAAI,pp.7432‚Äì7439,2020.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. InNeurIPS,pp.1877‚Äì1901,2020.
ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,andKristina
Toutanova. Boolq: Exploringthesurprisingdifficultyofnaturalyes/noquestions. InNAACL,
2019.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,and
OyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge.
arXivpreprintarXiv:1803.05457,2018.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifierstosolve
mathwordproblems. arXivpreprintarXiv:2110.14168,2021.
TogetherComputer. Redpajama: anopendatasetfortraininglargelanguagemodels,2023. URL
https://github.com/togethercomputer/RedPajama-Data.
Jean-BaptisteCordonnier,AndreasLoukas,andMartinJaggi. Multi-headattention: Collaborate
insteadofconcatenate. arXivpreprintarXiv:2006.16362,2020.
EkinDCubuk,BarretZoph,JonathonShlens,andQuocVLe. Randaugment: Practicalautomated
dataaugmentationwithareducedsearchspace. InCVPRW,pp.702‚Äì703,2020.
DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,HuazuoGao,DeliChen,JiashiLi,Wangding
Zeng,XingkaiYu,YWu,etal. Deepseekmoe: Towardsultimateexpertspecializationinmixture-
of-expertslanguagemodels. arXivpreprintarXiv:2401.06066,2024.
Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and
attentionforalldatasizes. InNeurIPS,pp.3965‚Äì3977,2021.
JiaDeng, WeiDong, RichardSocher, Li-JiaLi, KaiLi, andLiFei-Fei. Imagenet: Alarge-scale
hierarchicalimagedatabase. InCVPR,pp.248‚Äì255,2009.
PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. InNeurIPS,
pp.8780‚Äì8794,2021.
11Preprintversion. WorkinProgress.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,
andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionatscale.
InICLR,2021.
NanDu,YanpingHuang,AndrewMDai,SimonTong,DmitryLepikhin,YuanzhongXu,Maxim
Krikun,YanqiZhou,AdamsWeiYu,OrhanFirat,etal.Glam:Efficientscalingoflanguagemodels
withmixture-of-experts. InICML,pp.5547‚Äì5569,2022.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024.
WilliamFedus,BarretZoph,andNoamShazeer. Switchtransformers: Scalingtotrillionparameter
modelswithsimpleandefficientsparsity. JournalofMachineLearningResearch,23(120):1‚Äì39,
2022.
LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,JasonPhang,
HoraceHe,AnishThite,NoaNabeshima,ShawnPresser,andConnorLeahy. ThePile: An800gb
datasetofdiversetextforlanguagemodeling. arXivpreprintarXiv:2101.00027,2020.
LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,CharlesFoster,
LaurenceGolding,JeffreyHsu,AlainLeNoac‚Äôh,HaonanLi,KyleMcDonell,NiklasMuennighoff,
ChrisOciepa,JasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,LintangSutawika,
Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot
languagemodelevaluation,072024. URLhttps://zenodo.org/records/12608602.
DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,andJacob
Steinhardt. Measuringmassivemultitasklanguageunderstanding. InICLR,2021.
MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter. Gans
trainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. InNeurIPS,2017.
JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet,MohammadNorouzi,andTimSalimans.
Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning
Research,23(47):1‚Äì33,2022.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with
stochasticdepth. InECCV,pp.646‚Äì661,2016.
YuzhenHuang,YuzhuoBai,ZhihaoZhu,JunleiZhang,JinghanZhang,TangjunSu,JuntengLiu,
Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-discipline chinese
evaluationsuiteforfoundationmodels. InNeurIPS,2023.
RobertAJacobs,MichaelIJordan,StevenJNowlan,andGeoffreyEHinton. Adaptivemixturesof
localexperts. Neuralcomputation,3(1):79‚Äì87,1991.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,etal.
Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024.
PengJin,JinfaHuang,FenglinLiu,XianWu,ShenGe,GuoliSong,DavidClifton,andJieChen.
Expectation-maximizationcontrastivelearningforcompactvideo-and-languagerepresentations.
InNeurIPS,pp.30291‚Äì30306,2022.
PengJin,JinfaHuang,PengfeiXiong,ShangxuanTian,ChangLiu,XiangyangJi,LiYuan,andJie
Chen. Video-textasgameplayers: Hierarchicalbanzhafinteractionforcross-modalrepresentation
learning. InCVPR,pp.2472‚Äì2482,2023a.
Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji, Chang Liu, Li Yuan, and Jie Chen.
Diffusionret: Generative text-video retrieval with diffusion model. In ICCV, pp. 2470‚Äì2481,
2023b.
12Preprintversion. WorkinProgress.
PengJin,RyuichiTakanobu,WancaiZhang,XiaochunCao,andLiYuan. Chat-univi: Unifiedvisual
representationempowerslargelanguagemodelswithimageandvideounderstanding. InCVPR,
pp.13700‚Äì13710,2024a.
PengJin,BoZhu,LiYuan,andShuichengYan. Moe++: Acceleratingmixture-of-expertsmethods
withzero-computationexperts. arXivpreprintarXiv:2410.07348,2024b.
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. InNAACL,volume1,pp. 2,2019.
DiederikPKingma. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,2013.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti,DanielleEpstein,IlliaPolosukhin,JacobDevlin,KentonLee,etal. Naturalquestions: a
benchmarkforquestionansweringresearch. TransactionsoftheAssociationforComputational
Linguistics,7:453‚Äì466,2019.
Tuomas Kynka¬®a¬®nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
precisionandrecallmetricforassessinggenerativemodels. InNeurIPS,2019.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
MaximKrikun,NoamShazeer,andZhifengChen. Gshard: Scalinggiantmodelswithconditional
computationandautomaticsharding. InICLR,2021.
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:
Simplifyingtrainingoflarge,sparsemodels. InICML,pp.6265‚Äì6274,2021.
HaonanLi,YixuanZhang,FajriKoto,YifeiYang,HaiZhao,YeyunGong,NanDuan,andTimothy
Baldwin. Cmmlu: Measuringmassivemultitasklanguageunderstandinginchinese. arXivpreprint
arXiv:2306.09212,2023a.
Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and
YuQiao. Uniformer: Unifyingconvolutionandself-attentionforvisualrecognition. TPAMI,45
(10):12581‚Äì12600,2023b.
Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and
ChristophFeichtenhofer. Mvitv2: Improvedmultiscalevisiontransformersforclassificationand
detection. InCVPR,pp.4804‚Äì4814,2022.
BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunitedvisual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and
Li Yuan. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint
arXiv:2401.15947,2024.
StephanieLin,JacobHilton,andOwainEvans. TruthfulQA:Measuringhowmodelsmimichuman
falsehoods. InACL,pp.3214‚Äì3252,2022.
DongyangLiu,RenruiZhang,LongtianQiu,SiyuanHuang,WeifengLin,ShitianZhao,ShijieGeng,
ZiyiLin,PengJin,KaipengZhang,etal. Sphinx-x: Scalingdataandparametersforafamilyof
multi-modallargelanguagemodels. InICML,2024.
JianLiu,LeyangCui,HanmengLiu,DandanHuang,YileWang,andYueZhang.Logiqa:Achallenge
dataset for machine reading comprehension with logical reasoning. In IJCAI, pp. 3622‚Äì3628,
2020.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical visiontransformer using shiftedwindows. In ICCV, pp.
10012‚Äì10022,2021.
ZechunLiu,Kwang-TingCheng,DongHuang,EricPXing,andZhiqiangShen. Nonuniform-to-
uniformquantization: Towardsaccuratequantizationviageneralizedstraight-throughestimation.
InCVPR,pp.4942‚Äì4952,2022.
13Preprintversion. WorkinProgress.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprintarXiv:1608.03983,2016.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
PaulMichel,OmerLevy,andGrahamNeubig. Aresixteenheadsreallybetterthanone? InNeurIPS,
pp.14014‚Äì14024,2019.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? anewdatasetforopenbookquestionanswering. InEMNLP,2018.
Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W Battaglia. Generating images with
sparserepresentations. arXivpreprintarXiv:2103.03841,2021.
OpenAI. Introducingchatgpt. CoRR,2022. URLhttps://openai.com/blog/chatgpt.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollow
instructionswithhumanfeedback. InNeurIPS,pp.27730‚Äì27744,2022.
Denis Paperno, Germa¬¥n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,
SandroPezzelle,MarcoBaroni,GemmaBoleda,andRaquelFerna¬¥ndez. Thelambadadataset:
Wordpredictionrequiringabroaddiscoursecontext. InACL,pp.1525‚Äì1534,2016.
William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pp.
4195‚Äì4205,2023.
Joan Puigcerver, Carlos Riquelme Ruiz, Basil Mustafa, and Neil Houlsby. From sparse to soft
mixturesofexperts. InICLR,2024.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. TheJournalofMachineLearningResearch,21(1):5485‚Äì5551,2020.
Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Am-
marAhmadAwan,JeffRasley,andYuxiongHe. Deepspeed-moe: Advancingmixture-of-experts
inferenceandtrainingtopowernext-generationaiscale. InICML,pp.18332‚Äì18346,2022.
StephenRoller,SainbayarSukhbaatar,JasonWeston,etal. Hashlayersforlargesparsemodels. In
NeurIPS,pp.17555‚Äì17566,2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¬®rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,pp.10684‚Äì10695,2022.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99‚Äì106,
2021.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improvedtechniquesfortraininggans. InNeurIPS,2016.
NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,and
JeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-expertslayer. arXiv
preprintarXiv:1701.06538,2017.
DaiShi. Transnext: Robustfovealvisualperceptionforvisiontransformers. InCVPR,pp.17773‚Äì
17783,2024.
MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,JaredCasper,andBryanCatan-
zaro. Megatron-lm: Trainingmulti-billionparameterlanguagemodelsusingmodelparallelism.
arXivpreprintarXiv:1909.08053,2019.
ChenyangSi,WeihaoYu,PanZhou,YichenZhou,XinchaoWang,andShuichengYan. Inception
transformer. InNeurIPS,pp.23495‚Äì23509,2022.
14Preprintversion. WorkinProgress.
LucaSoldaini,RodneyKinney,AkshitaBhagia,DustinSchwenk,DavidAtkinson,RussellAuthur,
BenBogin,KhyathiChandu,JenniferDumas,YanaiElazar,ValentinHofmann,AnanyaHarsh
Jha,SachinKumar,LiLucy,XinxiLyu,NathanLambert,IanMagnusson,JacobMorrison,Niklas
Muennighoff,AakankshaNaik,CrystalNam,MatthewE.Peters,AbhilashaRavichander,Kyle
Richardson,ZejiangShen,EmmaStrubell,NishantSubramani,OyvindTafjord,PeteWalsh,Luke
Zettlemoyer, NoahA.Smith, HannanehHajishirzi, IzBeltagy, DirkGroeneveld, JesseDodge,
andKyleLo. Dolma: AnOpenCorpusofThreeTrillionTokensforLanguageModelPretraining
Research. arXivpreprint,2024. URLhttps://arxiv.org/abs/2402.00159.
ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniewWojna. Rethinking
theinceptionarchitectureforcomputervision. InCVPR,pp.2818‚Äì2826,2016.
HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,andHerve¬¥
Je¬¥gou. Trainingdata-efficientimagetransformers&distillationthroughattention. InICML,pp.
10347‚Äì10357,2021.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,≈Åukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. InNeurIPS,2017.
ElenaVoita,DavidTalbot,FedorMoiseev,RicoSennrich,andIvanTitov. Analyzingmulti-headself-
attention: Specializedheadsdotheheavylifting,therestcanbepruned. InACL,pp.5797‚Äì5808,
2019.
Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, and
Li Yuan. Look-m: Look-once optimization in kv cache for efficient multimodal long-context
inference. arXivpreprintarXiv:2406.18139,2024.
HongyuWang,ShumingMa,RuipingWang,andFuruWei. Q-sparse: Alllargelanguagemodelscan
befullysparsely-activated. arXivpreprintarXiv:2407.10969,2024.
WenhaiWang,EnzeXie,XiangLi,Deng-PingFan,KaitaoSong,DingLiang,TongLu,PingLuo,
and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational
VisualMedia,8(3):415‚Äì424,2022.
Tianwen Wei, Bo Zhu, Liang Zhao, Cheng Cheng, Biye Li, Weiwei Lu¬®, Peng Cheng, Jianhao
Zhang,XiaoyuZhang,LiangZeng,etal. Skywork-moe: Adeepdiveintotrainingtechniquesfor
mixture-of-expertslanguagemodels. arXivpreprintarXiv:2406.06563,2024.
JohannesWelbl,NelsonFLiu,andMattGardner. Crowdsourcingmultiplechoicesciencequestions.
arXivpreprintarXiv:1707.06209,2017.
ChenglinYang,SiyuanQiao,QihangYu,XiaodingYuan,YukunZhu,AlanYuille,HartwigAdam,
andLiang-ChiehChen. Moat: Alternatingmobileconvolutionandattentionbringsstrongvision
models. InICLR,2022a.
Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng
Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint
arXiv:2107.00641,2021.
JianweiYang,ChunyuanLi,XiyangDai,andJianfengGao. Focalmodulationnetworks. InNeurIPS,
pp.4203‚Äì4217,2022b.
WeihaoYu,ChenyangSi,PanZhou,MiLuo,YichenZhou,JiashiFeng,ShuichengYan,andXinchao
Wang. Metaformerbaselinesforvision. TPAMI,2023.
LiYuan,YunpengChen,TaoWang,WeihaoYu,YujunShi,Zi-HangJiang,FrancisEHTay,Jiashi
Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on
imagenet. InICCV,pp.558‚Äì567,2021.
15Preprintversion. WorkinProgress.
SangdooYun,DongyoonHan,SeongJoonOh,SanghyukChun,JunsukChoe,andYoungjoonYoo.
Cutmix: Regularizationstrategytotrainstrongclassifierswithlocalizablefeatures. InICCV,pp.
6023‚Äì6032,2019.
RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Canamachine
reallyfinishyoursentence? InACL,pp.4791‚Äì4800,2019.
HongyiZhang. mixup: Beyondempiricalriskminimization. arXivpreprintarXiv:1710.09412,2017.
XiaofengZhang,YikangShen,ZeyuHuang,JieZhou,WengeRong,andZhangXiong. Mixtureof
attentionheads: Selectingattentionheadspertoken. arXivpreprintarXiv:2210.05144,2022.
ZhunZhong,LiangZheng,GuoliangKang,ShaoziLi,andYiYang. Randomerasingdataaugmenta-
tion. InAAAI,pp.13001‚Äì13008,2020.
YanqiZhou,TaoLei,HanxiaoLiu,NanDu,YanpingHuang,VincentZhao,AndrewMDai,QuocV
Le,JamesLaudon,etal.Mixture-of-expertswithexpertchoicerouting.InNeurIPS,pp.7103‚Äì7114,
2022.
16Preprintversion. WorkinProgress.
A APPENDIX
Abstract. Thisappendixprovidesadditionaldiscussions(AppendixA),implementationdetails(Ap-
pendixB),severaladditionalexperiments(AppendixC),morequalitativeanalysis(AppendixD),
anddetailsofquantitativeevaluationsforLLMs(AppendixE).
Code. Wehaveattachedthecodetothesupplementarymaterial. Inthiscode,wealsoprovidethe
evaluationprocessoftheproposedmethod. Wepromisetoreleaseamoredetailedandcleancode
versionuponpublication.
A ADDITIONAL DISCUSSIONS
A.1 LIMITATIONSANDFUTUREWORK
Inthissection,wedelineatethelimitationsofourworkandoutlineavenuesforfutureresearch.
HeterogeneousAttentionHeads. Wefindthatdifferentattentionheadsoperateinparallelwithin
theattentionmechanism,suggestingthatdifferentheadscanhavevaryinghiddensizes. Futurework
couldexploretheuseofheterogeneousattentionheadsbasedonourMoHframework.
Lower Activation Rate. Currently, MoH outperforms multi-head attention by utilizing only
50%‚àº90%oftheattentionheads. However,thisisstillarelativelyhighproportion. Futurework
couldaimtofurtheroptimizeMoH,reducingheadactivationtolessthan50%.
MultimodalInputs. Effectivelyprocessinginformationfrommultiplemodalitiesintheattention
mechanismremainsanopenquestion. Recentwork(Wanetal.,2024)hasshownthatvisualand
textualtokensexhibitdistinctattentionpatternsinmulti-headattention. Futureworkcouldexplore
the attention patterns of MoH with different modal inputs, for example within multimodal large
languagemodels(Jinetal.,2024a;Linetal.,2023;2024;Liuetal.,2024;Jinetal.,2023b).
MoreDownstreamTasks. WeevaluateourproposedMoHacrossvariouspopularmodelframe-
works,includingViTforimageclassification,DiTforclass-conditionalimagegeneration,andLLMs
forlanguagetasks. FutureworkcanexploretheapplicationofMoHinmoredownstreamtasks,such
asaudiotasksandmultimodaltasks(Jinetal.,2023a;2022).
More Parameters. Due to computational constraints, the maximum number of MoH model
parametersinourexperimentsislimitedto8B(MoH-LLaMA3-8B).However,ourMoHmethodis
highlygeneralizableandcanbescaledtolargermodelsinfutureresearch.
B IMPLEMENTATION DETAILS
B.1 TRAININGLLMSFROMSCRATCH
Model Settings. For training LLMs from scratch, we use Megatron (Shoeybi et al., 2019), an
open-source training code, as the training framework. The detailed hyper-parameter settings of
variousMoH-LLMsareshowninTab.A.
TableA:SizesandarchitecturesofMoH-LLMsandLLMs. ‚ÄúMoH-LLM-B‚Äùhasmoreparameters
than‚ÄúLLM-B‚Äùduetotheadditionalparametersintroducedbytherouternetwork.
Methods #Params #Layers #HiddenSize #IntermediateSize #Heads #HeadDim
LLM-S 186
12 768 2048 12 64
MoH-LLM-S 186
LLM-B 881
24 1536 4096 16 96
MoH-LLM-B 882
DataDetails. Consistentwithpreviousworks,weusethetokenizerofLLaMA2,whichcontains
65,536vocabularytokens. ItisworthnotingthatMoH-LLMistrainedexclusivelyonpublicdatasets,
making it accessible for academic research settings. Tab. B shows the detailed sample ratios of
17Preprintversion. WorkinProgress.
different open-sourcedatasets. Specifically, wesample from the followingdatasets accordingto
differentsamplingprobabilities:
‚Ä¢ TheRedPajama(Computer,2023)includestrainingdatafromsevendomains: Common-
Crawl,C4,Github,Wikipedia,Books,ArXiv,andStackExchange.
‚Ä¢ TheDolma(Soldainietal.,2024),alargeanddiverseopenEnglishtextcorpus,contains3
trilliontokenssampledfromsevensources,includingwebpagesfromCommonCrawl,code
fromTheStack,curatedwebdatafromC4(Raffeletal.,2020),socialmediaconversations
fromReddit,academicpapersfromPeS2o,publicdomainbooksfromProjectGutenberg,
andcomprehensivecontentfromWikipediaandWikibooks.
‚Ä¢ ThePile(Gaoetal.,2020),anopen-sourceEnglishtextcorpusfortraininglargelanguage
models,includes22diverse,publiclyavailabledatasetssuchasWikipedia,NIHExPorter,
ArXiv,Books3,BookCorpus2,OpenSubtitles,YoutubeSubtitles,andEnronEmails.
TableB:Samplingratioofdifferentopen-sourcedatasetsforMoH-LLMs. MoH-LLMistrained
exclusivelyonpublicdatasets,makingitaccessibleforacademicresearchsettings.
SamplingRatio
RedpajamaBooks 4.24%
RedpajamaWikipedia 3.50%
RedpajamaArXiv 4.37%
RedpajamaStackExchange 3.19%
RedpajamaC4 10.94%
Dolma 61.28%
Pile 12.48%
TrainingHyper-Parameters. Tab.Cshowsthedetailedtraininghyper-parametersofMoH-LLMs.
Specifically,allMoH-LLMsaretrainedwiththeAdamWoptimizer(Loshchilov&Hutter,2017),
usingabatchsizeof4milliontokenswithasequencelengthof2048. Thefinallearningrateisset
to10%ofthemaximum. Duringtraining,aweightdecayof0.1andgradientclippingof1.0are
applied. ForLLM-SandMoH-LLM-S,themaximumlearningrateissetto3e-4. ForLLM-Band
MoH-LLM-B,themaximumlearningrateissetto5e-4.
TableC:Traininghyper-parametersofMoH-LLMs.
MoH-LLM-S100B MoH-LLM-B100B MoH-LLM-B200B
(LLM-S100B) (LLM-B100B) (LLM-B200B)
Trainingbudget 100B 100B 200B
Maximumlearningrate 3e-4 5e-4 5e-4
Finallearningrate 3e-5 5e-5 5e-5
LRwarmupinit 1e-7 1e-7 1e-7
LRwarmupiters 2000 500 500
Sequencelength 2048 2048 2048
Batchsize(tokens) 4M 4M 4M
Œ≤forL 0.01 0.01 0.01
b
Tensorparallel 1 1 1
Pipelineparallel 1 1 1
B.2 CONTINUE-TUNINGLLAMA3-8B
Training Hyper-Parameters. Tab. D shows the detailed training hyper-parameters of MoH-
LLaMA3-8B. We find that if there is a discrepancy between the continue-training data and the
originaltrainingdatadistributionofthemodel,theperformanceofthemodelmayfluctuatewildly
atthebeginningofthetrainingprocess. Sincewedonothaveaccesstotherawtrainingdataof
LLaMA3,weaddressthesepotentialperformancefluctuationsbydividingthetrainingprocessinto
twostages. Inthefirststage,wecontinue-tunetheoriginalLLaMA3-8Bmodelusing300Btokensto
18Preprintversion. WorkinProgress.
adaptittoourdataset. Inaddition,duringthefirststage,toenhancetheChineseabilityofthemodel,
weexpandthevocabularysize. Specifically,weincreasetheoriginalLLaMA3-8Bvocabularysize
from128,256to160,896. Inthesecondstage,wecontinue-tunethisadaptedmodelintoourproposed
MoHmodelwith100Btokens. Duringthefirststage,themaximumlearningrateissetto6e-5,and
thefinallearningrateis6e-6. Inthesecondstage,themaximumlearningrateissetto2e-5,andthe
finallearningrateis1e-6. Forbothstages,weemploytheAdamWoptimizer(Loshchilov&Hutter,
2017),withabatchsizeof16milliontokenswithasequencelengthof8192. Duringtraining,weuse
aweightdecayof0.1andgradientclippingof1.0.
TableD:Traininghyper-parametersofMoH-LLaMA3-8B.Wedividethetrainingprocessinto
twostages. Inthefirststage,wecontinue-tunetheLLaMA3-8Bmodelusing300Btokens. Inthe
secondstage,wecontinue-tunethisadaptedmodelintoourproposedMoHmodelwith100Btokens.
TheFirstStage TheSecondStage
Trainingbudget 300B 100B
Maximumlearningrate 6e-5 2e-5
Finallearningrate 6e-6 1e-6
LRwarmupiters 50 50
Sequencelength 8192 8192
Batchsize(tokens) 16M 16M
Œ≤forL - 0.01
b
Tensorparallel 2 1
Pipelineparallel 1 8
TableE:ComparisonsbetweenMoH-LLaMA3-8BandLLaMA3-8B-stage1. MoH-LLaMA3-8B
outperformsLLaMA3-8B-stage1byutilizingonly75%oftheattentionheads.
#Activated
Methods MMLU(5) CMMLU(5) NQ(32) GSM8K(8) TruthfulQA
Heads(%)
LLaMA3-8B-stage1 100 66.2 66.0 28.1 58.6 41.9
MoH-LLaMA3-8B 75 65.8 64.4 28.3 56.9 44.0
#Activated
Methods HellaSwag(10) LogiQA BoolQ(32) LAMBADA SciQ
Heads(%)
LLaMA3-8B-stage1 100 79.4 30.4 85.1 75.8 92.2
MoH-LLaMA3-8B 75 80.1 30.3 84.0 76.4 92.2
#Activated
Methods PIQA WinoGrande ARC-E ARC-C(25) Average
Heads(%)
LLaMA3-8B-stage1 100 79.1 73.0 70.9 59.6 64.7
MoH-LLaMA3-8B 75 78.8 72.9 72.5 60.1 64.8
C ADDITIONAL EXPERIMENTS
Comparison between MoH-LLaMA3-8B and LLaMA3-8B-stage1. We divide the training
processintotwostages. Tab.EshowsthecomparisonbetweenMoH-LLaMA3-8Bandthemodel
attheendofthefirsttrainingstage(LLaMA3-8B-stage1). AsshowninTab.E,MoH-LLaMA3-8B
quicklyrecoverstheperformanceofLLaMA3-8B-stage1withinatrainingbudgetof100Btokens.
Notably,inEnglishlanguagetasks,MoH-LLaMA3-8BsurpassesLLaMA3-8B-stage1whileusing
only 75% of the attention heads. However, for Chinese language and math tasks, the recovery
performanceoftheMoHmodelisnotasstrongasforEnglish. Forexample,MoH-LLaMA3-8B
achievesanaccuracyof64.4onCMMLU,comparedto66.0forLLaMA3-8B-stage1. Weattribute
thistothefactthatthemodel‚ÄôsChineseandmathematicalcapabilitiesareprimarilyestablishedduring
thefirsttrainingstage. Sincethefirsttrainingstageusesonly300Btokens,significantlylessthanthe
15TtokensinLLaMA3-8B‚Äôspre-training,themodel‚Äôsabilitiesintheseareasarenotfullystable. In
thesecondtrainingstage,afterswitchingtotheMoHmodel,themodelexperiencesmoresignificant
19Preprintversion. WorkinProgress.
MoH-ViT-B MoH-DiT-XL/2
Basketball Basketball
y y
tis tis
n n
e e
D D
Routed Heads ID Routed Heads ID
Bookshop Bookshop
y y
tis tis
n n
e e
D D
Routed Heads ID Routed Heads ID
Cart Cart
y y
tis tis
n n
e e
D D
Routed Heads ID Routed Heads ID
Husky Husky
y y
tis tis
n n
e e
D D
Routed Heads ID Routed Heads ID
Jean Jean
y y
tis tis
n n
e e
D D
Routed Heads ID Routed Heads ID
FigureA:AdditionalvisualizationoftheheadloaddistributioninthefinalMoHlayer. ForMoH-
ViT-BandMoH-DiT-XL/2,wepresenttheheadloaddistributionsforthecategories‚ÄúBasketball‚Äù,
‚ÄúBookshop‚Äù,‚ÄúCart‚Äù,‚ÄúHusky‚Äù,and‚ÄúJean‚Äù. MoH-ViT-Bactivates75%oftheattentionheads. MoH-
DiT-XL/2activates90%oftheattentionheads.
forgetting in Chinese and mathematical tasks. Overall, as shown in Tab. E, MoH-LLaMA3-8B
achievesanaverageaccuracyof64.8%across14benchmarks,outperformingLLaMA3-8B-stage1by
utilizingonly75%oftheattentionheads.
D ADDITIONAL QUALITATIVE ANALYSIS
AdditionalVisualizationoftheHeadLoadDistribution. Weprovideadditionalvisualizationof
theheadloaddistributioninFig.A.AsillustratedinbothFig.3andFig.A,thereisnotablevariation
inattentionheadassignmentsacrossdifferentcategoriesandtasktopics. ThissuggeststhattheMoH
modeladaptstoawiderangeoftasksbyutilizingdistinctheadassignmentpatterns. Thisability
enables MoH to allocate attention heads more effectively to specific task types, leading to more
efficientparameterutilizationcomparedtostandardmulti-headattention.
AdditionalVisualizationoftheHeadLoadDistributioninMoH-LLaMA3-8B. Weprovide
additionalvisualizationoftheheadloaddistributioninFig.B.AsshowninFig.B,MoH-LLaMA3-8B
exhibitssimilarcharacteristicstoMoH-LLMstrainedfromscratch,withsignificantvariationinatten-
tionheadassignmentsacrossdifferentcategoriesandtasktopics. Thisindicatesthatcontinue-tuning
enablesthemodeltoadoptdifferentheadassignmentpatternsquickly. Theseresultsdemonstrate
20Preprintversion. WorkinProgress.
ARC Challenge MMLU
y y
tis tis
n n
e e
D D
Routed Heads ID Routed Heads ID
ARC Easy NQ
y y
tis tis
n n
e e
D D
Routed Heads ID Routed Heads ID
CEVAL PIQA
y y
tis tis
n n
e e
D D
Routed Heads ID Routed Heads ID
LAMBADA SciQ
y y
tis tis
n n
e e
D D
Routed Heads ID Routed Heads ID
LogiQA WinoGrande
y y
tis tis
n n
e e
D D
Routed Heads ID Routed Heads ID
FigureB:AdditionalvisualizationoftheheadloaddistributioninMoH-LLaMA3-8B.
thatpre-trainedmulti-headattentionmodelscanbeeffectivelycontinue-tunedintoMoHmodels,
significantlybroadeningtheapplicabilityoftheproposedMoHapproach.
ImagesGeneratedfromtheProposedMoH-DiT-XL/2Model. Fig.Cshowssamplesgenerated
byourclass-conditionalMoH-DiT-XL/2model. TheseresultsdemonstratetheabilityofMoH-DiT-
XL/2togeneratesemanticallycorrectcontentwithaccuratespatialrelationships.
E DETAILS OF QUANTITATIVE EVALUATIONS FOR LLMS
We conduct comparative comparisons of MoH-LLM (MoH-LLaMA3-8B) against vanilla
LLMs(LLaMA3-8B).TheevaluationisperformedonmultiplekeybenchmarksusingtheEleutherAI
LanguageModelEvaluationHarness¬ß(Gaoetal.,2024),aunifiedframeworkfortestinggenerative
languagemodelsacrossawiderangeoftasks. Thebenchmarksusedforevaluationinclude:
ARC(Clarketal.,2018)isamultiple-choicequestion-answeringresourcefeaturingquestionsfrom
scienceexamsforgrades3to9. Itisdividedintotwopartitions: EasyandChallenge,withthelatter
containing more difficult questions that necessitate reasoning. Most questions offer four answer
choices, while less than 1% feature either three or five choices. Additionally, ARC includes a
supportingknowledgebasewith14.3millionunstructuredtextpassages. Wereport0-shotaccuracy
onARCEasyand25-shotaccuracyonARCChallenge.
¬ßhttps://github.com/EleutherAI/lm-evaluation-harness
21Preprintversion. WorkinProgress.
FigureC:ImagesgeneratedfromtheproposedMoH-DiT-XL/2model. Weshowsamplesgener-
atedfromourclass-conditionalMoH-DiT-XL/2modeltrainedonImageNetat256√ó256resolution.
MoH-DiT-XL/2activates90%oftheattentionheads.
LAMBADA(Papernoetal.,2016)isanopen-endedclozetaskconsistingofapproximately10,000
passages from BooksCorpus, where the objective is to predict a missing target word in the last
sentenceofeachpassage. Themissingwordisalwaysthelastwordofthefinalsentence,withno
optionsprovided. Wereport0-shotaccuracyonLAMBADA.
LogiQA(Liuetal.,2020)comprises8,678question-and-answerinstancesthatencompassvarious
typesofdeductivereasoning. ThedatasetservesasabenchmarkforreexamininglogicalAIwithin
thecontextofdeeplearninginNLP.Wereport0-shotaccuracyonLogiQA.
PIQA(Bisketal.,2020)isadatasetdesignedforcommonsensereasoning,aimedatevaluatingthe
physicalknowledgeofcurrentmodels. Wereport0-shotaccuracyonPIQA.
SciQ(Welbletal.,2017)includes13,679crowdsourcedscienceexamquestionscoveringsubjects
suchasPhysics,Chemistry,andBiology. Eachquestionispresentedinamultiple-choiceformatwith
fouransweroptions,andformostquestions,anadditionalparagraphprovidessupportingevidence
forthecorrectanswer. Wereport0-shotaccuracyonSciQ.
WinoGrande(Sakaguchietal.,2021)isalarge-scaledatasetcomprising44,000problems,inspired
bytheoriginalWSCdesignbutenhancedtoincreasebothitsscaleanddifficulty. Wereport0-shot
accuracyonWinoGrande.
HellaSwag(Zellersetal.,2019)isachallengingdatasetdesignedtoevaluatecommonsensenatu-
rallanguageinference, whichprovesdifficultforstate-of-the-artmodelsbutposesnosignificant
challengeforhumans. Wereporttheaccuracyforthe10-shotHellaSwag.
MMLU(Hendrycksetal.,2021)isabenchmarkdesignedtoassessmodels‚Äôknowledgeacquired
duringpretraining,makingitmorechallengingandhuman-likeinevaluation. Itcovers57subjects
acrossSTEM,humanities,socialsciences,andmore,rangingfromelementarytoadvancedprofes-
sionallevels. Thebenchmarktestsbothworldknowledgeandproblem-solvingskills,withsubjects
22Preprintversion. WorkinProgress.
spanningtraditionalareaslikemathandhistorytospecializedfieldssuchaslawandethics,offeringa
comprehensivetoolforidentifyingmodelblindspots. Wereporttheaccuracyforthe5-shotMMLU.
NaturalQuestions(NQ)(Kwiatkowskietal.,2019)isaquestion-answeringdatasetbasedonreal,
anonymizedGooglequeries. Annotatorslabellongandshortanswers(ornullifnoanswerisfound)
fromWikipediapagesinthetop5searchresults. Thedatasetincludes307,373trainingexamples,
7,830developmentexamples,and7,842testexampleswith5-wayannotations. Wereporttheexact
matchscorefor32-shotNaturalQuestionstomeasurethefactualknowledgeinthemodel.
BoolQ(Clarketal.,2019)isaquestion-answeringdatasetconsistingof15,942yes/noquestions.
Thesequestionsarenaturallyoccurring,andgeneratedinunpromptedandunconstrainedcontexts.
Eachexampleisprovidedasatripletof(question,passage,andanswer),withthepagetitleoptionally
includedasadditionalcontext. Wereporttheaccuracyforthe32-shotBoolQ.
OpenbookQA (Mihaylov et al., 2018) is a question-answering dataset designed to assess under-
standingofelementary-levelscience,similartoopen-bookexams. Itcontains5,957multiple-choice
questionsbasedona‚Äúbook‚Äùof1,326coresciencefacts. Thedatasetrequiresnotonlyknowledgeof
thesefactsbutalsotheapplicationofbroadcommonknowledge. Itincludesmappingsfromeach
questiontothecorefactittargetsandadditionalcommonknowledgefacts. Thedatasetalsoprovides
scoresofhumanaccuracyandclarity,aswellascrowd-sourceddataforfurtheranalysis. Wereport
0-shotaccuracyonOpenbookQA.
TruthfulQA(Linetal.,2022)isabenchmarkdesignedtoevaluatethetruthfulnessofalanguage
model‚Äôsresponses. Itconsistsof817questionsacross38categories,suchashealth,law,finance,and
politics. Thequestionsarecraftedtoreflectcommonfalsebeliefsormisconceptionsthatmightlead
humanstoanswerinaccurately. Wereport0-shotaccuracyonTruthfulQA.
GSM8K(Cobbeetal.,2021)isadatasetcontaining8.5Khigh-quality,linguisticallydiversegrade
schoolmathwordproblems. Itisdividedinto7.5Ktrainingproblemsand1Ktestproblems. Each
problemrequires2to8stepstosolve, typicallyinvolvingasequenceofelementarycalculations
usingbasicarithmeticoperations. Acapablemiddleschoolstudentshouldbeabletosolveallthe
problems,makingthedatasetsuitableforevaluatingmulti-stepmathematicalreasoning. Wereport
theexactmatchscorefor8-shotGSM8K.
CEVAL(Huangetal.,2023)isacomprehensiveChineseevaluationsuitedesignedtoassessthe
advancedknowledgeandreasoningabilitiesofLLMsinaChinesecontext. Itincludesmultiple-
choicequestionsacrossfourdifficultylevels(middleschool,highschool,college,andprofessional)
andspans52diversedisciplines. Wereporttheaccuracyforthe5-shotCEVAL.
CMMLU(Lietal.,2023a)isacomprehensiveChinesebenchmarkdesignedtoevaluatetheknowl-
edge and reasoning abilities of LLMs across various subjects, including natural sciences, social
sciences,engineering,andhumanities. Wereporttheaccuracyforthe5-shotCMMLU.
23