Preprint.
MITIGATING SUBOPTIMALITY OF DETERMINISTIC
POLICY GRADIENTS IN COMPLEX Q-FUNCTIONS
AyushJain1∗, NorioKosaka2, XinhuLi1, Kyung-MinKim3, ErdemBıyık1, JosephJLim4
1UniversityofSouthernCalifornia 2LineYahooCorp 3NAVERCloud 4KAIST
ABSTRACT
Inreinforcementlearning,off-policyactor-criticapproacheslikeDDPGandTD3
arebasedonthedeterministicpolicygradient. Herein,theQ-functionistrained
fromoff-policyenvironmentdataandtheactor(policy)istrainedtomaximizethe
Q-functionviagradientascent. Weobservethatincomplextaskslikedexterous
manipulation and restricted locomotion, the Q-value is a complex function of
action,havingseverallocaloptimaordiscontinuities. Thisposesachallengefor
gradientascenttotraverseandmakestheactorpronetogetstuckatlocaloptima.
Toaddressthis,weintroduceanewactorarchitecturethatcombinestwosimple
insights:(i)usemultipleactorsandevaluatetheQ-valuemaximizingaction,and(ii)
learnsurrogatestotheQ-functionthataresimplertooptimizewithgradient-based
methods. Weevaluatetaskssuchasrestrictedlocomotion,dexterousmanipulation,
andlargediscrete-actionspacerecommendersystemsandshowthatouractorfinds
optimalactionsmorefrequentlyandoutperformsalternateactorarchitectures.
1 INTRODUCTION
In sequential decision-making, the goal is to
build an optimal agent that maximizes the
expected cumulative returns (Sondik, 1971;
Littman, 1996). Value-based reinforcement
learning(RL)approacheslearneachaction’sex-
pectedreturnswithaQ-functionandmaximize
it(Sutton&Barto,1998). However,incontin-
Figure1: Anactorµtrainedwithgradientascent
uous action spaces, evaluating the Q-value of
onachallengingQ-landscapegetsstuckinlocalop-
every possible action is impractical. This ne-
tima.Ourapproachlearnsasequenceofsurrogates
cessitatesanactortogloballymaximizetheQ-
Ψ oftheQ-functionthatsuccessivelypruneout
functionandefficientlynavigatethevastaction i
theQ-landscapebelowthecurrentbestQ-values,
space(Grondmanetal.,2012). Yet,thisispar-
resultinginfewerlocaloptima. Thus,theactors
ticularlychallengingintaskssuchasrestricted
ν trainedtoascendonthesesurrogatesproduce
locomotion,withanon-convexQ-functionland- i
actionswithamoreoptimalQ-value.
scapewithmanylocaloptima(Figure2).
CanwebuildanactorarchitecturetofindclosertooptimalactionsinsuchcomplexQ-landscapes?
PriormethodsperformasearchovertheactionspacewithevolutionaryalgorithmslikeCEM(DeBoer
et al., 2005; Kalashnikov et al., 2018; Shao et al., 2022), but this requires numerous costly re-
evaluationsoftheQ-function. Toavoidthis,deterministicpolicygradient(DPG)algorithms(Silver
etal.,2014),suchasDDPG(Lillicrapetal.,2015)andTD3(Fujimotoetal.,2018)trainaparameter-
izedactortooutputactionswiththeobjectiveofmaximizingtheQ-functionlocally.
AsignificantchallengearisesinenvironmentswheretheQ-functionhasmanylocaloptima,asshown
in Figure 2. An actor trained via gradient ascent may converge to a local optimum with a much
lowerQ-valuethantheglobalmaximum. Thisleadstosuboptimaldecisionsduringdeploymentand
sample-inefficienttraining,astheagentfailstoexplorehigh-rewardtrajectories(Kakade,2003).
Toimproveactors’abilitytoidentifyoptimalactionsincomplex,non-convexQ-functionlandscapes,
weproposetheSuccessiveActorsforValueOptimization(SAVO)algorithm. SAVOleveragestwo
∗Correspondenceto:ayushj@usc.edu.
1
4202
tcO
51
]GL.sc[
1v33811.0142:viXraPreprint.
Hopper-Restricted Recsim
3.4
315 3.3
Q -v
a lu
e3
3
31
20
00
95 0
5
Q -value3
3
3. .2
.1
0
290 2.9
285 2.8
A0.1 c0 t.2 io0. n3 0 . S4 p0.5 a0 c.6 e0.7 0.8 0.9 0.9 0.8 0.7Ac0.6tion0.5 Sp0.4ace0.3 0.2 0.1 0.1 A0.2 ct0 i. o3 n0 .4 Sp0.5 ac0. e6 0.7 0.8 0.9 0.8 0.7 A0.6ctio0n.5 Sp0.a4ce0.3 0.2 0.1
Figure2: ComplexQ-landscapes. WeplotQ-valueversusactionaforsomestate. Incontrolof
Inverted-Double-Pendulum-Restricted(left)andHopper-Restricted(middle),certainactionrangesare
unsafe,resultinginvariouslocallyoptimalactionpeaks. Inalargediscrete-actionrecommendation
system(right),therearelocalpeaksatactionsrepresentingrealitems(blackdots).
key insights: (1) combining multiple policies using an argmax on their Q-values to construct a
superiorpolicy(§4.1),and(2)simplifyingtheQ-landscapebyexcludinglowerQ-valueregionsbased
onhigh-performingactions,inspiredbytabusearch(Glover,1990),therebyreducinglocaloptima
andfacilitatinggradient-ascent(§4.2). Byiterativelyapplyingthesestrategiesthroughasequenceof
simplifiedQ-landscapesandcorrespondingactors,SAVOprogressivelyfindsmoreoptimalactions.
WeevaluateSAVOincomplexQ-landscapessuchas(i)continuouscontrolindexterousmanipu-
lation(Rajeswaranetal.,2017)andrestrictedlocomotion(Todorovetal.,2012),and(ii)discrete
decision-makinginthelargeactionspacesofsimulated(Ieetal.,2019)andreal-datarecommender
systems(Harper&Konstan,2015),andgridworldminingexpedition(Chevalier-Boisvertetal.,2018).
WeusethereframingoflargediscreteactionRLtocontinuousactionRLfollowingVanHasselt&
Wiering(2009)andDulac-Arnoldetal.(2015),whereapolicyactsincontinuousactions,suchasthe
featurespaceofrecommenderitems(Figure2),andthenearestdiscreteactionisexecuted.
Our key contribution is SAVO, an actor architecture to find better optimal actions in complex
non-convex Q-landscapes (§4). In experiments, we visualize how SAVO’s successively learned
Q-landscapeshavefewerlocaloptima(§6.2),makingitmorelikelytofindbetteractionoptimawith
gradientascent. ThisenablesSAVOtooutperformalternativeactorarchitectures,suchassampling
moreactioncandidates(Dulac-Arnoldetal.,2015)andlearninganensembleofactors(Osbandetal.,
2016)(§6.1)acrosscontinuousanddiscreteactionRL.
2 RELATED WORK
Q-learning(Watkins&Dayan,1992;Tesauroetal.,1995)isafundamentalvalue-basedRLalgorithm
thatiterativelyupdatesQ-valuestomakeoptimaldecisions. DeepQ-learning(Mnihetal.,2015)
hasbeenappliedtotaskswithmanageablediscreteactionspaces,suchasAtari(Mnihetal.,2013;
Espeholt et al., 2018; Hessel et al., 2018), traffic control (Abdoos et al., 2011), and small-scale
recommender systems (Chen et al., 2019). However, scaling Q-learning to continuous or large
discreteactionspacesrequiresspecializedtechniquestoefficientlymaximizetheQ-function.
Analytical Q-optimization. Analytical optimization of certain Q-functions, such as wire fitting
algorithm(Baird&Klopf,1993)andnormalizedadvantagefunctions(Guetal.,2016;Wangetal.,
2019), allows closed-form action maximization without an actor. Likewise, Amos et al. (2017)
assume that the Q-function is convex in actions and use a convex solver for action selection. In
contrast,theQ-functionsconsideredinthispaperareinherentlynon-convexinactionspace,making
such an assumption invalid. Generally, analytical Q-functions lack the expressiveness of deep
Q-networks(Horniketal.,1989),makingthemunsuitabletomodelcomplextaskslikeinFigure2.
Evolutionary Algorithms for Q-optimization. Evolutionary algorithms like simulated anneal-
ing(Kirkpatricketal.,1983),geneticalgorithms(Srinivas&Patnaik,1994),tabusearch(Glover,
1990),andthecross-entropymethod(CEM)(DeBoeretal.,2005)areemployedinRLforglobal
optimization(Huetal.,2007). ApproachessuchasQT-Opt(Kalashnikovetal.,2018;Leeetal.,
2023;Kalashnikovetal.,2021)utilizeCEMforactionsearch,whilehybridactor-criticmethods
likeCEM-RL(Pourchot&Sigaud,2018),GRAC(Shaoetal.,2022),andCross-EntropyGuided
Policies(Simmons-Edleretal.,2019)combineevolutionarytechniqueswithgradientdescent.Despite
theireffectiveness,CEM-basedmethodsrequirenumerousQ-functionevaluationsandstrugglewith
2Preprint.
high-dimensionalactions(Yanetal.,2019). Incontrast,SAVOachievessuperiorperformancewith
onlyafew(e.g.,three)Q-evaluations,asdemonstratedinexperiments(§6).
Actor-CriticMethodswithGradientAscent. Actor-criticmethodscanbeon-policy(Williams,
1992;Schulmanetal.,2015;2017)primarilyguidedbythepolicygradientofexpectedreturns,or
off-policy(Silveretal.,2014;Lillicrapetal.,2015;Fujimotoetal.,2018;Chenetal.,2020)primarily
guidedbytheBellmanerroronthecritic. DeterministicPolicyGradient(DPG)(Silveretal.,2014)
anditsextensionslikeDDPGLillicrapetal.(2015),TD3(Fujimotoetal.,2018)andREDQ(Chen
etal.,2020)optimizeactorsbyfollowingthecritic’sgradient. SoftActor-Critic(SAC)(Haarnoja
et al., 2018) extends DPG to stochastic actors. However, these methods can get trapped in local
optimawithintheQ-functionlandscape. SAVOaddressesthislimitationbyenhancinggradient-based
actor training. This issue also affects stochastic actors, where a local optimum means an action
distribution(insteadofasingleaction)thatfailstominimizetheKLdivergencefromtheQ-function
densityfully,andisapotentialareaforfutureresearch.
Sampling-AugmentedActor-Critic. SamplingmultipleactionsandevaluatingtheirQ-valuesis
a common strategy to find optimal actions. Greedy actor-critic (Neumann et al., 2018) samples
high-entropyactionsandtrainstheactortowardsthebestQ-valuedaction,yetremainssusceptibleto
localoptima. Inlargediscreteactionspaces,methodslikeWolpertinger(Dulac-Arnoldetal.,2015)
usek-nearestneighborstoproposeactions,requiringextensiveQ-evaluationsonupto10%oftotal
actions. Incontrast,SAVOefficientlygenerateshigh-qualityactionproposalsthroughsuccessive
actorimprovementswithoutbeingconfinedtolocalneighborhoods.
Ensemble-Augmented Actor-Critic. Ensembles of policies enhance exploration by providing
diverseactionproposalsthroughvariedinitializations(Osbandetal.,2016;Chen&Peng,2019;Song
etal.,2023;Zheng12etal.,2018;Huangetal.,2017). ThebestactionisselectedbasedonQ-value
evaluations. Unlikeensemblemethods, SAVOsystematicallyeliminateslocaloptima, offeringa
morereliableoptimizationprocessforcomplextasks(§6).
3 PROBLEM FORMULATION
OurworktacklestheeffectiveoptimizationoftheQ-valuelandscapeinoff-policyactor-criticmethods
forcontinuousandlarge-discreteactionRL.WemodelataskasaMarkovDecisionProcess(MDP),
definedbyatuple{S,A,T,R,γ}ofstates,actions,transitionprobabilities,rewardfunction,anda
discountfactor. TheactionspaceA⊆RD isaD-dimensionalcontinuousvectorspace. Ateverystep
tintheepisode,theagentreceivesastateobservations ∈S fromtheenvironmentandactswith
t
a ∈A.Then,itreceivesthenewstateaftertransitions andarewardr .Theobjectiveoftheagent
t t+1 t
istolearnapolicyπ(a|s)thatmaximizestheexpecteddiscountedreward,max E [(cid:80) γtr ].
π π t t
3.1 DETERMINISTICPOLICYGRADIENTS(DPG)
DPG(Silveretal.,2014)isanoff-policyactor-criticalgorithmthattrainsadeterministicactorµ to
ϕ
maximizetheQ-function. Thishappensviatwostepsofgeneralizedpolicyiteration,GPI(Sutton&
Barto,1998): policyevaluationestimatestheQ-function(Bellman,1966)andpolicyimprovement
greedilymaximizestheQ-function. ToapproximatetheargmaxovercontinuousactionsinEq.(2),
DPGproposesthepolicygradienttoupdatetheactorlocallyinthedirectionofincreasingQ-value,
Qµ(s,a)=r(s,a)+γE [Qµ(s′,µ(s′))], (1)
s′
µ(s)=argmaxQµ(s,a), (2)
a
∇ ϕJ(ϕ)=E s∼ρµ(cid:104) ∇ aQµ(s,a)(cid:12) (cid:12) a=µ(s)∇ ϕµ ϕ(s)(cid:105) . (3)
DDPG(Lillicrapetal.,2015)andTD3(Fujimotoetal.,2018)madeDPGcompatiblewithdeep
networksviatechniqueslikeexperiencereplayandtargetnetworkstoaddressnon-stationarityof
onlineRL,twincriticstomitigateoverestimationbias,targetpolicysmoothingtopreventexploitation
oferrorsintheQ-function,anddelayedpolicyupdatessocriticisreliabletoprovideactorgradients.
3.2 THECHALLENGEOFANACTORMAXIMIZINGACOMPLEXQ-LANDSCAPE
DPG-basedalgorithmstraintheactorfollowingthechainruleinEq.(3). Specifically,itsfirstterm,
∇ Qµ(s,a) involves gradient ascent in Q-versus-a landscape. This Q-landscape is often highly
a
non-convex(Figures2,3)andnon-stationarybecauseofitsowntraining. Thismakestheactor’s
3Preprint.
outputµ(s)getstuckatsuboptimalQ-values, thusleadingtoinsufficientpolicyimprovementin
Eq.(2). Wecandefinethesuboptimalityoftheµw.r.t. Qµatstatesas
∆(Qµ,µ,s)=argmaxQµ(s,a)−Qµ(s,µ(s))≥0. (4)
a
Suboptimalityinactorsisacrucialproblembecauseitleadsto(i)poorsampleefficiencybyslowing
down GPI, and (ii) poor inference performance even with an optimal Q-function, Q∗ as seen
inFigure3whereaTD3actorgetsstuckatalocallyoptimumactiona inthefinalQ-function.
0
Thischallengefundamentallydiffersfromthewell-studied
field of non-convex optimization, where non-convexity
arisesinthelossfunctionw.r.t. themodelparameters(Good-
fellow, 2016). In those cases, stochastic gradient-based
optimizationmethodslikeSGDandAdam(Kingma&Ba,
2014)areeffectiveatfindingacceptablelocalminimadue
to the smoothness and high dimensionality of the param-
eterspace,whichoftenallowsforescapefrompoorlocal
optima(Choromanskaetal.,2015). Moreover,overparame-
terizationindeepnetworkscanleadtolosslandscapeswith
numerousgoodminima(Neyshaburetal.,2017).
Incontrast,ourchallengeinvolvesnon-convexityintheQ-
functionw.r.t.theactionspace.Theactor’staskistofind,for
everystates,theactionathatmaximizesQµ(s,a). Since
theQ-functioncanbehighlynon-convexandmultimodalin Figure3: Non-convexQ-landscapein
a,thegradientascentstep∇ Qµ(s,a)usedinEq.(3)may Inverted-Pendulum-Restrictedleadsto
a
lead the actor to converge to suboptimal local maxima in asuboptimallyconvergedactor.
actionspace. Unlikeparameterspaceoptimization,theactor
cannotrelyonhighdimensionalityoroverparameterizationtosmoothouttheoptimizationlandscape
inactionspacebecausetheQ-landscapeisdeterminedbythetask’sreward. Furthermore,thenon-
stationarityoftheQ-functionduringtrainingcompoundsthischallenge. Thesepropertiesmakeour
non-convexchallengeunique,requiringaspecializedactortonavigatethecomplexQ-landscape.
Tasks with several local optima in the Q-function include restricted inverted pendulum shown
in Figure 3, where certain regions of the action space are invalid or unsafe, leading to a rugged
Q-landscape(Florenceetal.,2022). Dexterousmanipulationtasksexhibitdiscontinuousbehaviors
likeinsertingaprecisepeginplacewithasmallregionofhigh-valuedactions(Rajeswaranetal.,
2017)andsurgicalroboticshaveahighvarianceinQ-valuesofnearbymotions(Barnoyetal.,2021).
3.2.1 LARGEDISCRETEACTIONRLREFRAMEDASCONTINUOUSACTIONRL
Wediscussanotherpracticaldomainwherenon-convexQ-functionsarepresent. Inlargediscrete
action tasks like recommender systems (Zhao et al., 2018; Zou et al., 2019; Wu et al., 2017), a
commonapproach(VanHasselt&Wiering,2009;Dulac-Arnoldetal.,2015)istousecontinuousrep-
resentationsofactionsasamediumofdecision-making.Givenasetofactions,I ={I ,...,I },a
1 N
predefinedmoduleR:I →AassignseachI ∈I toitsrepresentationR(I),e.g.,textembedding
ofagivenmovie(Zhouetal.,2010). Acontinuousactionpolicyπ(a | s)islearnedintheaction
representationspace,witheacha∈AconvertedtoadiscreteactionI ∈I vianearestneighbor,
f (a)=arg min ∥R(I )−a∥ .
NN i 2
I i∈I
Importantly,thenearestneighboroperationcreatesachallengingpiece-wisecontinuousQ-function
withsuboptimaatvariousdiscretepointsasshowninFigure2(Jainetal.,2021;2020).
4 APPROACH: SUCCESSIVE ACTORS FOR VALUE OPTIMIZATION (SAVO)
Ourobjectiveistodesignanactorarchitecturethatefficientlydiscoversbetteractionsincomplex,
non-convexQ-functionlandscapes. Wefocusongradient-basedactorsandintroducetwokeyideas:
1. Maximizing Over Multiple Policies: By combining policies using an argmax over their Q-
values,wecanconstructapolicythatperformsatleastaswellasanyindividualpolicy(§4.1).
2. SimplifyingtheQ-Landscape: Drawinginspirationfromtabusearch(Glover,1990),wepropose
using actions with good Q-values to eliminate or “tabu” the Q-function regions with lower
Q-values,therebyreducinglocaloptimaandfacilitatinggradient-basedoptimization(§4.2).
4Preprint.
𝐀𝐜𝐭𝐨𝐫 Actors
𝜈
0
𝜈
1
𝜈
2
sn
o
itc FiLM
A
𝑎 𝑎 𝑎 𝑎 𝑎 𝑎 𝒂∗ Deep Set
0 0 1 0 1 2
𝐂𝐫𝐢𝐭𝐢𝐜 e ta Action
tS
𝑄 Ψ 0 Ψ 1 Ψ 2 𝜈
𝑖
Surrogates
Figure4: SAVOArchitecture. (left)Q-networkisunchanged. (center)Insteadofasingleactor,we
learnasequenceofactorsandsurrogatenetworksconnectedviaactionpredictions. (right)Condition-
ingonpreviousactionsisdonewiththehelpofadeep-setsummarizerandFiLMmodulation.
4.1 MAXIMIZERACTOROVERACTIONPROPOSALS
WefirstshowhowadditionalactorscanimproveDPG’spolicyimprovementstep. Givenapolicyµ
beingtrainedwithDPGoverQ,considerkadditionalarbitrarypoliciesν ,...,ν ,whereν :S →A
1 k i
andletν =µ. Wedefineamaximizeractorµ fora =ν (s)fori=0,1,...,k,
0 M i i
µ (s):= argmax Q(s,a), (5)
M
a∈{a0,a1,...,ak}
Here,µ isshowntobeabettermaximizerofQ(s,a)inEq.(2)thanµ∀s:
M
Q(s,µ (s))=maxQ(s,a )≥Q(s,a )=Q(s,µ(s)).
M i 0
ai
Therefore,bypolicyimprovementtheorem(Sutton&Barto,1998),VµM(s)≥Vµ(s),provingthat
µ isbetterthanasingleµforagivenQ. AppendixAprovesthefollowingtheorembyshowing
M
thatpolicyevaluationandimprovementwithµ converge.
M
Theorem4.1(ConvergenceofPolicyIterationwithMaximizerActor). Amodifiedpolicyiteration
algorithmwhereν =µisthecurrentpolicylearnedwithDPGandmaximizeractorµ definedin
0 M
Eq.(5),convergesinthetabularsettingtotheoptimalpolicy.
Thisalgorithmisvalidforarbitraryν ,...ν . Weexperimentwithν’sobtainedbysamplingfroma
1 k
Gaussiancenteredatµorensemblingonµtogetdiverseactions. However,inhigh-dimensionality,
randomnessaroundµisnotsufficienttogetactionproposalstosignificantlyimproveµ.
4.2 SUCCESSIVESURROGATESTOREDUCELOCALOPTIMA
Totrainadditionalpoliciesν thatcanimproveuponµ ,weintroducesurrogateQ-functionswith
i M
fewerlocaloptima,inspiredbytheprinciplesoftabusearch(Glover,1990),whichisanoptimization
technique that uses memory structures to avoid revisiting previously explored inferior solutions,
therebyenhancingthesearchforoptimalsolutions. Similarly,oursurrogatefunctionsactasmemory
mechanismsthat“tabu”certainregionsoftheQ-functionlandscapedeemedsuboptimalbasedon
previouslyidentifiedgoodactions. Givenaknownactiona†,wedefineasurrogatefunctionthat
elevatestheQ-valuesofallinferioractionstoQ(s,a†),whichservesasaconstantthreshold:
Ψ(s,a;a†)=max{Q(s,a),Q(s,a†)}. (6)
Extendingthisidea,wedefineasequenceofsurrogatefunctionsusingtheactionsfromprevious
policies. Leta ={a ,a ,...,a }. Thei-thsurrogatefunctionis:
<i 0 1 i−1
(cid:26) (cid:27)
Ψ (s,a;a )=max Q(s,a),maxQ(s,a ) . (7)
i <i j
j<i
Theorem4.2. Forastates ∈ S andsurrogatesΨ definedasabove,thenumberoflocaloptima
i
decreaseswitheachsuccessivesurrogate:
N (Q(s,·))≥N (Ψ (s,·;a ))≥···≥N (Ψ (s,·;a )),
opt opt 1 0 opt k <k
whereN (f)denotesthenumberoflocaloptimaoffunctionf overA.
opt
ProofSketch. AsΨ →Ψ ,theanchorQ-valueinEq.(7)weaklyincreases,max Q(s,a )≤
i i+1 j<i j
max Q(s,a ),thus,eliminatingmorelocalminimabelowit(proofinAppendixB.1).
j<(i+1) j
5Preprint.
4.3 SUCCESSIVEACTORSFORSURROGATEOPTIMIZATION
To effectively reduce local optima using the surrogates Ψ ,...,Ψ , we design the policies ν to
1 k i
optimize their respective surrogates Ψ (s,a;a ). Each ν focuses on regions where Q(s,a) ≥
i <i i
max Q(s,a ),allowingittofindbetteroptimathanpreviouspolicies. Theactorν isconditioned
j<i j i
onpreviousactions{a ,...,a },summarizedusingdeepsets(Zaheeretal.,2017)(Figure4). The
0 i−1
maximizeractorµ (Eq.(5))selectsthebestactionamongallproposals.
M
Wetraineachactorν usinggradientascentonitssurrogateΨ ,similartoDPG:
i i
∇ ϕiJ(ϕ i)=E s∼ρµM (cid:2) ∇ aΨ i(s,a;a <i)(cid:12) (cid:12) a∇ ϕiν i(s;a <i)(cid:3) . (8)
4.4 APPROXIMATESURROGATEFUNCTIONS
ThesurrogatesΨ havezerogradientswhenQ(s,a)<τ,whereτ =max Q(s,a ),
i j<i j
∇ Ψ (s,a;a
)=(cid:26) ∇ aQµM(s,a) ifQ(s,a)≥τ,
a i <i 0 ifQ(s,a)<τ.
Thismeansthepolicygradientonlyupdatesν whenQ(s,a)≥τ,whichmayslowdownlearning.
i
Toaddressthisissue,weeasethegradientflowbylearningasmoothlossyapproximationΨˆ ofΨ .
i i
We approximate each surrogate Ψ
i
with a neural network Ψˆ . This ap-
i
proachleveragestheuniversalapprox-
imationtheorem(Horniketal.,1989;
Cybenko,1989)andbenefitsfromem-
pirical evidence that deep networks
caneffectivelylearnnon-smoothfunc-
tions(Imaizumi&Fukumizu,2019).
ThesmoothsurrogateΨˆ enablescon-
i
tinuous gradient propagation, which
isessentialforoptimizingtheactors Figure5: Inrestrictedinvertedpendulum,givenananchor
ν . We train Ψˆ to approach Ψ by Q(a )value,Ψ(left)hassomezero-gradientsurfaceswhich
i i i 0
minimizingthemeansquarederrorat Ψˆ (right) approximately follows while allowing non-zero
twocriticalpoints: gradientstowardshighQ-valuestoflowintoitsactorν.
1. µ˜ (s)istheactionselectedbythecurrentmaximizeractorµ ,havingahighQ-value,
M M
2. ν (s;a )istheactionproposedbythei-thactorconditionedonpreviousactionsa ,
i <i <i
 
L approx =E s∼ρµM  (cid:88) (cid:13) (cid:13) (cid:13)Ψˆ i(s,a;a <i)−Ψ i(s,a;a <i)(cid:13) (cid:13) (cid:13)2 . (9)
2
a∈{µ˜M(s),νi(s;a<i)}
ThisdesignensuresΨˆ isupdatedonhighQ-valueactionsandthusthelandscapeisbiasedtowards
i
thosevalues. ThismakesthegradientflowtrendinthedirectionofhighQ-values. So,evenwhena
i
fromν fallsinaregionofzerogradientsforΨ ,inΨˆ wouldprovidepolicygradientinahigher
i i i
Q-valuedirection,ifitexists. Figure5showsΨ andΨˆ inrestrictedinvertedpendulumtask.
1 1
4.5 SAVO-TD3ALGORITHMANDDESIGNCHOICES
WhiletheSAVOarchitecture(Figure4)canbeintegratedwithanyoff-policyactor-criticalgorithm,
wechoosetoimplementitwithTD3(Fujimotoetal.,2018)duetoitscompatibilitywithcontinuous
andlarge-discreteactionRL(Dulac-Arnoldetal.,2015). UsingtheSAVOactorinTD3enhances
its ability to find better actions in complex Q-function landscapes. Algorithm 1 depicts SAVO
(highlighted)appliedtoTD3. WediscussdesignchoicesinSAVOandvalidatethemin§6.
1. Removing policy smoothing: We eliminate TD3’s policy smoothing, which adds noise
to the target action a˜ during critic updates. In non-convex landscapes, nearby actions may
have significantly different Q-values and noise addition might obscure important variations.
6Preprint.
2. ExplorationinAdditionalActors:
Algorithm1SAVO-TD3
Addedactorsν explorethesurrogateland-
i
scapesforhigh-rewardregionsbyadding InitializeQ,Q ,µ, Ψˆ ,...,Ψˆ , ν ,...,ν
2 1 k 1 k
OU(Lillicrapetal.,2015)orGaussian(Fu- InitializetargetnetworksQ′ ←Q,Q′ ←Q
2 twin
jimotoetal.,2018)noisetotheiractions. InitializereplacebufferB.
fortimestept=1toT do
3. TwinCriticsforSurrogates:
SelectAction:
To prevent overestimation bias in surro-
Evaluatea =µ(s),a =ν (s;a )
gatesΨˆ ,weusetwincriticstocomputethe 0 i i <i
i AddperturbationswithOUNoiseaˆ =a +ϵ
targetofeachsurrogate,mirroringTD3. i i i
Evaluateµ (s)=argmax Qµ(s,a)
M a∈{aˆ0,...,aˆk}
4. ConditioningonPreviousActions: Explorationactiona=µ˜ (s)=µ (s)+ϵ
M M
Actors ν and surrogates Ψˆ are condi- Observerewardrandnewstates′
i i
tionedonprecedingactionsviaFiLMlay- Store(s,a,{aˆ i}K i=0,r,s′)inB
ers(Perezetal.,2018)asinFigure4. Update:
SampleNtransitions(s,a,{aˆ }K ,r,s′)fromB
5. DiscreteActionSpaceTasks: Computetargetactiona˜=µi (i s= ′0
)
M
Weapply1-nearest-neighborf NNbeforeQ- UpdateQ,Q ←r+γmin{Q′(s′,a˜),Q′(s′,a˜)}
valueevaluationtoensuretheQ-function 2 2
isonlyqueriedatin-distributionactions. UpdateΨˆ withEq.9∀i=1,...k
i
SAVO-TD3 employs SAVO actor to sys- UpdateactorµwithEq.3
tematically reduce the local optima in its Updateactorν iwithEq.8∀i=1,...k
basealgorithmTD3. Weempiricallyvali- endfor
datetheproposeddesignimprovements.
5 ENVIRONMENTS
WeevaluateSAVOondiscreteandcontinuousactionspaceenvironmentswithchallengingQ-value
landscapes. MoreenvironmentdetailsarepresentedinAppendixCandFigure13.
LocomotioninMujoco. WeevaluateonMuJoCo(Todorovetal.,2012)environmentsofHopper-v4,
Walker2D-v4,InvertedPendulum-v4,andInvertedDoublePendulum-v4.
Locomotion in Restricted Mujoco. We create a restricted loco- ValidActionSpace
OriginalActionSpace
motion suite of the same environments as in MuJoCo. A hard
Q-landscapeisrealizedviahigh-dimensionaldiscontinuitiesthatre-
stricttheactionspace. Concretely,asetofpredefinedhyper-spheres
(asshowninFigure6)intheactionspacearesampledandsetto
bevalidactions,whiletheotherinvalidactionshaveanulleffectif
selected. ThecompletedetailscanbefoundinAppendixC.3.1.
AdroitDexterousManipulation. Rajeswaranetal.(2017)propose
manipulationtaskswithadexterousmulti-fingeredhand. Door: In
thistask,arobotichandisrequiredtoopenadoorwithalatch. The
Figure6: Hopper’s3Dvisual-
challenge lies in the precise manipulation needed to unlatch and
izationofActionSpace.
swingopenthedoorusingthefingers. Hammer: therobotichand
mustuseahammertodriveanailintoaboard. Thistaskteststhehand’sabilitytograspthehammer
correctlyandapplyforceaccuratelytoachievethegoal. Pen: Thistaskinvolvestherobotichand
manipulatingapentoreachaspecificgoalpositionandrotation. Theobjectiveistocontrolthepen’s
orientationandpositionusingfingers,whichdemandsfinemotorskillsandcoordination.
MiningExpeditioninGridWorld. Wedevelopa2DMininggridworldenvironment(Chevalier-
Boisvertetal.,2018)wheretheagent(Figure13)navigatesa2Dmazetoreachthegoal,removing
mineswithcorrectpick-axetoolstoreachthegoalintheshortestpath. Theactionspaceincludes
navigationandtool-choiceactions, withaprocedurally-definedactionrepresentationspace. The
Q-landscapeisnon-convexbecauseofthediverseeffectsofnearbyactionrepresentations.
SimulatedandReal-DataRecommenderSystems. RecSim(Ieetal.,2019)simulatessequential
user interactions in a recommender system with a large discrete action space. The agent must
recommendthemostrelevantitemfromasetof10,000itemsbasedonuserpreferenceinformation.
Theactionrepresentationsaresimulateditemcharacteristicvectorsinsimulatedandmoviereview
embeddingsinthereal-datataskbasedonMovieLens(Harper&Konstan,2015)foritems.
7Preprint.
Performance Profiles Performance Profiles
1.00 1.00
0.75 0.75
0.50 0.50
SAVO SAVO (Ours)
1-Actor (TD3) SAVO - Approximation
0.25 1-Actor, k-Samples (Wolpertinger) 0.25 SAVO - Previous Actions
Evolutionary Actor (CEM) SAVO + Action Smoothing
k-Actors (Ensemble) SAVO + Joint Action
0.00 0.00
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Normalized Score ( ) Normalized Score ( )
(a)SAVOversusbaselineactorarchitectures. (b)SAVOversusablationsofSAVO
Figure7: Aggregateperformanceprofilesusingnormalizedscoresover7tasksand10seedseach.
MineWorld RecSim RecSim-Data
0.98 6.3 55
5.5
0.78 46
4.7
0.58 3.9 37
0.38 3.1 28
0.18 2.3 19
0.02 1.5 10
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Env Steps 1e7 Env Steps 1e7 Env Steps 1e7
Hopper (Restricted) Inverted Pendulum (Restricted) Inv. Dbl Pend. (Restricted) Walker2D (Restricted)
3000 1000 10000 4000
2400 800 8000 3200
1800 600 6000 2400
1200 400 4000 1600
600 200 2000 800
0 0 0 0
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
SAVO (Ours) 1-Actor (TD3) 1-Actor, k-Samples (Wolpertinger) k-Actors (Ensemble)
Evolutionary Actor (CEM) Greedy-AC Greedy-TD3
Figure8: SAVOagainstbaselinesondiscreteandcontinuoustasks. Resultsaveragedover10seeds.
6 EXPERIMENTS
6.1 EFFECTIVENESSOFSAVOINCHALLENGINGQ-LANDSCAPES
WecompareSAVOagainstthefollowingbaselineactorarchitectures,wherek =3:
• 1-Actor(TD3): Conventionalsingleactorarchitecturewhichissusceptibletolocaloptima.
• 1-Actor,ksamples(Wolpertinger): Gaussiansamplingcenteredonactor’soutput. Fordiscrete
actions,weselectk-NNdiscreteactionsaroundthecontinuousaction(Dulac-Arnoldetal.,2015).
• k-Actors(Ensemble): BestQ-valueamongdiverseactionsfromensemble(Osbandetal.,2016).
• Evolutionaryactor(CEM):IterativeCEMsearchovertheactionspace(Kalashnikovetal.,2018).
• Greedy-AC:GreedyActorCritic(Neumannetal.,2018)trainsahigh-entropyproposalpolicyand
primaryactortrainedfrombestproposalswithgradientupdates.
• GreedyTD3: OurversionofGreedy-ACwithTD3explorationandupdateimprovements.
• SAVO:OurmethodwithksuccessiveactorsandsurrogateQ-landscapes.
WeablatethecrucialcomponentsanddesigndecisionsinSAVO:
• SAVO-Approximation: removestheapproximatesurrogates(§4.4),usingΨ insteadofΨˆ .
i i
• SAVO-PreviousActions: removesconditioningona inSAVO’sactorsandsurrogates.
<i
• SAVO+ActionSmoothing: TD3’spolicysmoothing(Fujimotoetal.,2018)computeQ-targets.
• SAVO+JointAction: trainsanactorwithajointactionspaceof3×D. Thekactionsamplesare
obtainedbysplittingthejointactionintoDdimensions. ValidatessuccessivenatureofSAVO.
Aggregateperformance. Weutilizeperformanceprofiles(Agarwaletal.,2021)toaggregateresults
acrossdifferentenvironmentsinFigure7a(evaluationmechanismdetailedinAppendixG.1). SAVO
8
nruteR
lavE
>
erocs
htiw
snur
fo
noitcarF
etaR
sseccuS
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
>
erocs
htiw
snur
fo
noitcarF
nruteR
lavE
nruteR
lavEPreprint.
consistentlyoutperformsbaselineactorarchitectureslikesingle-actor(TD3)andsampling-augmented
actor(Wolpertinger),showingwiderobustnessacrosschallengingQ-landscapes. InFigure7b,SAVO
outperformsitsablations,validatingeachproposedcomponentanddesigndecision.
Per-environmentresults. Indiscreteactiontasks,theQ-valuelandscapeisonlywell-definedat
exactactionrepresentationsandnearbydiscreteactionsmighthaveverydifferentvalues(§3.2.1).
ThismakestheQ-valuelandscapeuneven,withmultiplepeaksandvalleys(Figure2). Forexample,
actionsinMiningExpeditioninvolvebothnavigationandtool-selectionwhicharequitedifferent,
whileRecSimandRecSim-Datahavemanydiverseitemstochoosefrom. MethodslikeWolpertinger
thatsamplemanyactionsalocalneighborhoodperformbetterthanTD3whichconsidersasingle
action(Figure8). However,SAVOachievesthebestperformancebydirectlysimplifyingthenon-
convexQ-landscape. Inrestrictedlocomotion,thereareseveralgoodactionsthatarefarapart. SAVO
actorscansearchandexplorewidelytooptimizetheQ-landscapebetterthanonlynearbysampled
actions. Figure16ablatesSAVOinall7environmentsandshowsthatthemostcriticalfeaturesare
itssuccessivenature,removingpolicysmoothing,andapproximatesurrogates.
6.2 Q-LANDSCAPEANALYSIS: DOSUCCESSIVESURROGATESREDUCELOCALOPTIMA?
Figure9visualizessurrogatelandscapesinInverted-Pendulum-Restrictedforagivenstates. Succes-
sivepruningandapproximationsmooththeQ-landscapes,reducinglocaloptima. Asingleactorgets
stuckatalocaloptimuma (left),butsurrogateΨˆ usesa asananchortofindabetteroptimum
0 1 0
a . ThemaximizerpolicyfinallyselectsthehighestQ-valuedactionamonga ,a ,a . Figure24
1 0 1 2
extendsthisvisualizationtoInverted-Double-Pendulum-Restricted. Figure23showshowoneactoris
sufficientintheconvexQ-landscapeofunrestrictedInverted-Pendulum-v4. Figures25,26showhow
Hopper-v4Q-landscapeprovidesapathtoglobaloptimum,whileHopper-Restrictedisnon-convex.
(a)Q(s,a 0) (b)Ψˆ 1(s,a 1;a 0) (c)Ψˆ 2(s,a 2;{a 0,a 1})
Figure9:EachsuccessivesurrogatelearnsaQ-landscapewithfewerlocaloptimaandthusiseasierto
optimizebyitsactor. SAVOhelpsasingleactorescapethelocaloptimuma inInvertedPendulum.
0
6.3 CHALLENGINGDEXTEROUSMANIPULATION(ADROIT)
In Adroit dexterous manipulation tasks (Door, Pen, Hammer) (Rajeswaran et al., 2017), SAVO
improvesthesampleefficiencyofTD3(Figure10). Thenon-convexityinQ-landscapelikelyarises
fromnearbyactionshavinghighvarianceoutcomeslikegrasping,missing,dropping,ornoimpact.
Adriot Door Adriot Pen Adriot Hammer
1.0 0.6 1.0
TD3 + SAVO TD3 + SAVO TD3 + SAVO
TD3 TD3 TD3
0.8 0.8
0.4 0.6 0.6
0.4 0.4
0.2
0.2 0.2
0.0 0.0 0.0
0.00 0.75 1.50 2.25 3.00 0.00 0.75 1.50 2.25 3.00 0.00 0.75 1.50 2.25 3.00
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
Figure10: SAVOimprovesthesample-efficiencyofTD3onAdroitdexterousmanipulationtasks.
6.4 QUANTITATIVEANALYSIS: THEEFFECTOFSUCCESSIVEACTORSANDSURROGATES
Weinvestigatetheeffectofincreasingthenumberofsuccessiveactor-surrogatesinSAVOinPendulum
(Figure11a)andMineWorld(Figure11b). Additionalactor-surrogatessignificantlyhelptoreduce
severelocaloptimainitially. However,theimprovementsaturatesasthesuboptimalitygapreduces.
9
etaR
sseccuS
lavE
etaR
sseccuS
lavE
etaR
sseccuS
lavEPreprint.
Inv. Dbl Pend. (Restricted) MineWorld Inv. Dbl Pend. (Restricted)
10000 10000
1.0 1st-Actor
Subsequent-Actors
8000 0.8 8000
6000 0.6 6000
4000 0.4 4000
Length=10 Length=5
2000 L Le en ng gt th h= =1 50 0.2 L Le en ng gt th h= =4 3 2000
Length=3 Length=2
Length=1 Length=1
0 0.0 0
0 125000 250000 375000 500000 0.00 1.25 2.50 3.75 5.00 0.0 0.5 1.0 1.5 2.0
Env Steps Env Steps 1e6 Env Steps 1e6
(a)LengthAnalysisPendulum (b)LengthanalysisMineWorld (c)ReturnImprovement
Figure11: (L)Moresuccessiveactor-surrogatesarebetter,(R)SAVOv/ssingle-actoroninference.
Next,weshowthatsuccessiveactorsareneededbecauseasingleactorcangetstuckinlocaloptima
evenwithanoptimalQ-function. InFigure11c,weconsideraSAVOagenttrainedtooptimalitywith
3actors. Whenweremovetheadditionalactors,theremainingsingle-actoragentresemblesTD3
trainedtomaximizean“optimal”Q-function. However,thesignificantperformancegapindicates
thatthesingleactorcouldnotfindoptimalactionsforthegivenQ-function.
6.5 DOESRLWITHRESETSADDRESSTHEISSUEOFQ-FUNCTIONOPTIMIZATION?
Primacybias(Nikishinetal.,2022;Kimetal.,2024)occurs
whenanagentistrappedinsuboptimalbehaviorsfromearly
training. To mitigate this, methods like resetting parameters
andre-learningfromthereplaybufferaimtoreducerelianceon
initialsamples. WerunTD3inMineEnvwitheitherafull-reset
or last-layer reset every 200k, 500k, or 1 million iterations.
None of these versions outperformed the original TD3 algo-
rithmwithoutresets. Thisisbecauseresettingdoesnothelp
anactortonavigatetheQ-landscapebetterandcanevencause
anotherwiseoptimalactortogetstuckinasuboptimalsolu-
tionduringretraining. Incontrast,theSAVOactorarchitecture
specificallyaddresses thenon-convexQ-landscapes, beinga
morerobustmethodtofindingclosertooptimalactions. Figure 12: Reset (primacy bias)
doesnotimproveQ-optimization.
6.6 FURTHEREXPERIMENTSTOVALIDATESAVO
• Unrestrictedlocomotion. Figure15showsthatbothSAVOandbaselinesachieveoptimalper-
formanceinsimpleQ-landscapes,confirmingeffectivehyperparametertuning(§G.4,§G.3)and
indicatingthatthebaselinesunderperformduetothecomplexityintroducedinQ-landscapes.
• SAVOorthogonaltoSAC.Figure17showsthatSAVO+TD3>SAC>TD3,indicatingthatSAC’s
stochasticpolicydoesnotaddressnon-convexity,butcanitselfsufferfromlocaloptima(Figure18)
• DesignChoices.Figure20showsthatLSTM,DeepSet,andTransformersareallvalidchoicesas
summarizersofprecedingactionsa inSAVO.Figure21showsthatFiLMconditioningona
<i <i
especiallyhelpsfordiscreteactionspacetasksbuthasasmallereffectincontinuousactionspace.
InFigure22a,wefindOrnstein-Uhlenbeck(OU)noiseandGaussiannoisetobelargelyequivalent.
• MassiveDiscreteActions. SAVOoutperformsinRecSimwith100kand500kactions(Figure19).
7 LIMITATIONS AND CONCLUSION Method GPUMem. Return Time
TD3 619MB 1107.795 0.062s
IntroducingmoreactorsinSAVOhasnegligible
SAVOk=3 640MB 2927.149 0.088s
influenceonGPUmemory,butleadstolonger
SAVOk=5 681MB 3517.319 0.122s
inference time (Table 1). However, even for
3 actor-surrogates, SAVO achieves significant
Table1: Computev/sPerformanceGain(Mujoco)
improvementsinallourexperiments. Further,
fortaskswithasimpleconvexQ-landscape,asingleactordoesnotgetstuckinlocaloptima,making
thegainfromSAVOnegligible. Inconclusion,weimproveQ-landscapeoptimizationindeterministic
policygradientRLwithSuccessiveActorsforValueOptimization(SAVO)inbothcontinuousand
large discrete action spaces. We demonstrate with quantitative and qualitative analyses how the
improvedoptimizationofQ-landscapewithSAVOleadstobettersampleefficiencyandperformance.
10
nruteR
lavE
etaR
sseccuS
lavE
nruteR
lavEPreprint.
ACKNOWLEDGEMENTS
ThisworkwassupportedbyInstituteofInformation&communicationsTechnologyPlanning&
Evaluation(IITP)grant(No.RS-2019-II190075,ArtificialIntelligenceGraduateSchoolProgram,
KAIST)andNationalResearchFoundationofKorea(NRF)grant(NRF-2021H1D3A2A03103683,
BrainPoolResearchProgram),fundedbytheKoreagovernment(MSIT).AyushJainwassupported
partly as intern at Naver AI Lab during the initiation of the project. We appreciate the fruitful
discussionswithmembersofCLVRLabatKAISTandLiraLabatUSC.
BIBLIOGRAPHY
MonirehAbdoos, NasserMozayani, andAnaLCBazzan. Trafficlightcontrolinnon-stationary
environmentsbasedonmultiagentq-learning. In201114thInternationalIEEEconferenceon
intelligenttransportationsystems(ITSC),pp.1580–1585.IEEE,2011.
RishabhAgarwal,MaxSchwarzer,PabloSamuelCastro,AaronCCourville,andMarcBellemare.
Deepreinforcementlearningattheedgeofthestatisticalprecipice.Advancesinneuralinformation
processingsystems,34:29304–29320,2021.
BrandonAmos,LeiXu,andJZicoKolter. Inputconvexneuralnetworks. InInternationalconference
onmachinelearning,pp.146–155.PMLR,2017.
LeemonCBairdandAHarryKlopf. Reinforcementlearningwithhigh-dimensionalcontinuous
actions. WrightLaboratory,Wright-PattersonAirForceBase,Tech.Rep.WL-TR-93-1147,15,
1993.
YotamBarnoy,MollyO’Brien,WillWang,andGregoryHager. Roboticsurgerywithleanreinforce-
mentlearning. arXivpreprintarXiv:2105.01006,2021.
RichardBellman. Dynamicprogramming. Science,153(3731):34–37,1966.
LukasBiewald. Experimenttrackingwithweightsandbiases. Softwareavailablefromwandb.com,
2:233,2020.
GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,and
WojciechZaremba. Openaigym. arXivpreprintarXiv:1606.01540,2016.
GangChenandYimingPeng. Off-policyactor-criticinanensemble: Achievingmaximumgeneral
entropy and effective environment exploration in deep reinforcement learning. arXiv preprint
arXiv:1902.05551,2019.
XinshiChen,ShuangLi,HuiLi,ShaohuaJiang,YuanQi,andLeSong. Generativeadversarialuser
modelforreinforcementlearningbasedrecommendationsystem. InInternationalConferenceon
MachineLearning,pp.1052–1061.PMLR,2019.
XinyueChen,CheWang,ZijianZhou,andKeithWRoss. Randomizedensembleddoubleq-learning:
Learningfastwithoutamodel. InInternationalConferenceonLearningRepresentations,2020.
MaximeChevalier-Boisvert,LucasWillems,andSumanPal. Minimalisticgridworldenvironment
foropenaigym. https://github.com/maximecb/gym-minigrid,2018.
AnnaChoromanska,MIkaelHenaff,MichaelMathieu,GerardBenArous,andYannLeCun.TheLoss
SurfacesofMultilayerNetworks. InGuyLebanonandS.V.N.Vishwanathan(eds.),Proceedings
oftheEighteenthInternationalConferenceonArtificialIntelligenceandStatistics,volume38of
ProceedingsofMachineLearningResearch,pp.192–204,SanDiego,California,USA,09–12
May2015.PMLR. URLhttps://proceedings.mlr.press/v38/choromanska15.
html.
GeorgeCybenko. Approximationbysuperpositionsofasigmoidalfunction. Mathematicsofcontrol,
signalsandsystems,2(4):303–314,1989.
Pieter-TjerkDeBoer, DirkPKroese, ShieMannor, andReuvenYRubinstein. Atutorialonthe
cross-entropymethod. Annalsofoperationsresearch,134(1):19–67,2005.
11Preprint.
GabrielDulac-Arnold,RichardEvans,HadovanHasselt,PeterSunehag,TimothyLillicrap,Jonathan
Hunt,TimothyMann,TheophaneWeber,ThomasDegris,andBenCoppin. Deepreinforcement
learninginlargediscreteactionspaces. arXivpreprintarXiv:1512.07679,2015.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam
Doron,VladFiroiu,TimHarley,IainDunning,etal. Impala: Scalabledistributeddeep-rlwith
importanceweightedactor-learnerarchitectures. InInternationalconferenceonmachinelearning,
pp.1407–1416.PMLR,2018.
PeteFlorence,CoreyLynch,AndyZeng,OscarARamirez,AyzaanWahid,LauraDowns,Adrian
Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In
ConferenceonRobotLearning,pp.158–168.PMLR,2022.
ScottFujimoto,HerkeHoof,andDavidMeger. Addressingfunctionapproximationerrorinactor-
criticmethods. InInternationalconferenceonmachinelearning,pp.1587–1596.PMLR,2018.
FredGlover. Tabusearch: Atutorial. Interfaces,20(4):74–94,1990.
IanGoodfellow. Deeplearning,2016.
IvoGrondman,LucianBusoniu,GabrielADLopes,andRobertBabuska. Asurveyofactor-critic
reinforcementlearning: Standardandnaturalpolicygradients. IEEETransactionsonSystems,
Man,andCybernetics,partC(applicationsandreviews),42(6):1291–1307,2012.
ShixiangGu, TimothyLillicrap, IlyaSutskever, andSergeyLevine. Continuousdeepq-learning
withmodel-basedacceleration. InInternationalconferenceonmachinelearning,pp.2829–2838.
PMLR,2016.
TuomasHaarnoja, AurickZhou, Pieter Abbeel, andSergey Levine. Softactor-critic: Off-policy
maximumentropydeepreinforcementlearningwithastochasticactor. InInternationalconference
onmachinelearning,pp.1861–1870.PMLR,2018.
F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm
transactionsoninteractiveintelligentsystems(tiis),5(4):1–19,2015.
MatteoHessel,JosephModayil,HadoVanHasselt,TomSchaul,GeorgOstrovski,WillDabney,Dan
Horgan,BilalPiot,MohammadAzar,andDavidSilver. Rainbow: Combiningimprovementsin
deepreinforcementlearning. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume32,2018.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universalapproximators. Neuralnetworks,2(5):359–366,1989.
Jiaqiao Hu, Michael C Fu, and Steven I Marcus. A model reference adaptive search method for
globaloptimization. Operationsresearch,55(3):549–568,2007.
ZheweiHuang,ShuchangZhou,BoErZhuang,andXinyuZhou. Learningtorunwithactor-critic
ensemble. arXivpreprintarXiv:1712.08987,2017.
ZhihengHuang,WeiXu,andKaiYu. Bidirectionallstm-crfmodelsforsequencetagging,2015.
EugeneIe,Chih-weiHsu,MartinMladenov,VihanJain,SanmitNarvekar,JingWang,RuiWu,and
CraigBoutilier. Recsim: Aconfigurablesimulationplatformforrecommendersystems. arXiv
preprintarXiv:1909.04847,2019.
MasaakiImaizumiandKenjiFukumizu.Deepneuralnetworkslearnnon-smoothfunctionseffectively.
InThe22ndinternationalconferenceonartificialintelligenceandstatistics,pp.869–878.PMLR,
2019.
AyushJain,AndrewSzot,andJosephLim.Generalizationtonewactionsinreinforcementlearning.In
HalDauméIIIandAartiSingh(eds.),Proceedingsofthe37thInternationalConferenceonMachine
Learning,volume119ofProceedingsofMachineLearningResearch,pp.4661–4672.PMLR,
13–18Jul2020. URLhttp://proceedings.mlr.press/v119/jain20b.html.
12Preprint.
AyushJain,NorioKosaka,Kyung-MinKim,andJosephJLim.Knowyouractionset:Learningaction
relationsforreinforcementlearning. InInternationalConferenceonLearningRepresentations,
2021.
ShamMachandranathKakade. Onthesamplecomplexityofreinforcementlearning. Universityof
London,UniversityCollegeLondon(UnitedKingdom),2003.
DmitryKalashnikov,AlexIrpan,PeterPastor,JulianIbarz,AlexanderHerzog,EricJang,Deirdre
Quillen,EthanHolly,MrinalKalakrishnan,VincentVanhoucke,etal. Scalabledeepreinforcement
learningforvision-basedroboticmanipulation. InConferenceonRobotLearning,pp.651–673.
PMLR,2018.
Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski,
ChelseaFinn,SergeyLevine,andKarolHausman. Mt-opt: Continuousmulti-taskroboticrein-
forcementlearningatscale. arXivpreprintarXiv:2104.08212,2021.
WoojunKim,YongjaeShin,JongeuiPark,andYoungchulSung. Sample-efficientandsafedeep
reinforcementlearningviaresetdeepensembleagents.AdvancesinNeuralInformationProcessing
Systems,36,2024.
DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. arXivpreprint
arXiv:1412.6980,2014.
ScottKirkpatrick,CDanielGelattJr,andMarioPVecchi. Optimizationbysimulatedannealing.
science,220(4598):671–680,1983.
Kuang-HueiLee,TedXiao,AdrianLi,PaulWohlhart,IanFischer,andYaoLu. Pi-qt-opt: Predictive
informationimprovesmulti-taskroboticreinforcementlearningatscale. InConferenceonRobot
Learning,pp.1696–1707.PMLR,2023.
TimothyPLillicrap, JonathanJHunt, AlexanderPritzel, NicolasHeess, TomErez, YuvalTassa,
DavidSilver,andDaanWierstra. Continuouscontrolwithdeepreinforcementlearning. arXiv
preprintarXiv:1509.02971,2015.
MichaelLedermanLittman. Algorithmsforsequentialdecision-making. BrownUniversity,1996.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra,andMartinRiedmiller. Playingatariwithdeepreinforcementlearning. arXivpreprint
arXiv:1312.5602,2013.
VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,MarcGBellemare,
AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal. Human-levelcontrol
throughdeepreinforcementlearning. nature,518(7540):529–533,2015.
SamuelNeumann,SungsuLim,AjinJoseph,YangchenPan,AdamWhite,andMarthaWhite.Greedy
actor-critic: A new conditional cross-entropy method for policy improvement. arXiv preprint
arXiv:1810.09103,2018.
BehnamNeyshabur,SrinadhBhojanapalli,DavidMcAllester,andNatiSrebro. Exploringgeneraliza-
tionindeeplearning. Advancesinneuralinformationprocessingsystems,30,2017.
EvgeniiNikishin,MaxSchwarzer,PierlucaD’Oro,Pierre-LucBacon,andAaronCourville. The
primacybiasindeepreinforcementlearning. InInternationalconferenceonmachinelearning,pp.
16828–16847.PMLR,2022.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrappeddqn. Advancesinneuralinformationprocessingsystems,29,2016.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performancedeeplearninglibrary. Advancesinneuralinformationprocessingsystems,32,
2019.
13Preprint.
EthanPerez,FlorianStrub,HarmDeVries,VincentDumoulin,andAaronCourville. Film: Visual
reasoningwithageneralconditioninglayer. InProceedingsoftheAAAIconferenceonartificial
intelligence,volume32,2018.
AloïsPourchotandOlivierSigaud. Cem-rl: Combiningevolutionaryandgradient-basedmethodsfor
policysearch. arXivpreprintarXiv:1810.01222,2018.
AravindRajeswaran, VikashKumar, AbhishekGupta, GiuliaVezzani, JohnSchulman, Emanuel
Todorov,andSergeyLevine. Learningcomplexdexterousmanipulationwithdeepreinforcement
learninganddemonstrations. arXivpreprintarXiv:1709.10087,2017.
JohnSchulman,SergeyLevine,PieterAbbeel,MichaelJordan,andPhilippMoritz. Trustregion
policyoptimization. InInternationalconferenceonmachinelearning,pp.1889–1897.PMLR,
2015.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
LinShao,YifanYou,MengyuanYan,ShenliYuan,QingyunSun,andJeannetteBohg. Grac: Self-
guidedandself-regularizedactor-critic. InConferenceonRobotLearning,pp.267–276.PMLR,
2022.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministicpolicygradientalgorithms. InInternationalconferenceonmachinelearning,pp.
387–395.Pmlr,2014.
RileySimmons-Edler,BenEisner,EricMitchell,SebastianSeung,andDanielLee. Q-learningfor
continuousactionswithcross-entropyguidedpolicies. arXivpreprintarXiv:1903.10605,2019.
Edward Jay Sondik. The optimal control of partially observable Markov processes. Stanford
University,1971.
Yanjie Song, Ponnuthurai Nagaratnam Suganthan, Witold Pedrycz, Junwei Ou, Yongming He,
Yingwu Chen, and Yutong Wu. Ensemble reinforcement learning: A survey. Applied Soft
Computing,pp.110975,2023.
MandavilliSrinivasandLalitMPatnaik. Geneticalgorithms: Asurvey. computer,27(6):17–26,
1994.
RichardSSuttonandAndrewGBarto. ReinforcementLearning: AnIntroduction. TheMITPress,
1998.
GeraldTesauroetal. Temporaldifferencelearningandtd-gammon. CommunicationsoftheACM,38
(3):58–68,1995.
EmanuelTodorov,TomErez,andYuvalTassa. Mujoco: Aphysicsengineformodel-basedcontrol.
In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026–5033.
IEEE,2012.
HadoVanHasseltandMarcoAWiering. Usingcontinuousactionspacestosolvediscreteproblems.
In2009InternationalJointConferenceonNeuralNetworks,pp.1149–1156.IEEE,2009.
PinWang,HanhanLi,andChing-YaoChan. Quadraticq-networkforlearningcontinuouscontrolfor
autonomousvehicles. arXivpreprintarXiv:1912.00074,2019.
ChristopherJCHWatkinsandPeterDayan. Q-learning. Machinelearning,8(3):279–292,1992.
RonaldJWilliams. Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcement
learning. Machinelearning,8:229–256,1992.
QingyunWu,HongningWang,LiangjieHong,andYueShi. Returningisbelieving: Optimizinglong-
termuserengagementinrecommendersystems. InProceedingsofthe2017ACMonConference
onInformationandKnowledgeManagement,pp.1927–1936,2017.
14Preprint.
MengyuanYan,AdrianLi,MrinalKalakrishnan,andPeterPastor.Learningprobabilisticmulti-modal
actormodelsforvision-basedroboticgrasping. In2019InternationalConferenceonRoboticsand
Automation(ICRA),pp.4804–4810.IEEE,2019.
ManzilZaheer,SatwikKottur,SiamakRavanbakhsh,BarnabasPoczos,RussRSalakhutdinov,and
AlexanderJSmola. Deepsets. Advancesinneuralinformationprocessingsystems,30,2017.
Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. Deep re-
inforcement learning for page-wise recommendations. Proceedings of the 12th ACM Con-
ference on Recommender Systems, Sep 2018. doi: 10.1145/3240323.3240374. URL http:
//dx.doi.org/10.1145/3240323.3240374.
ZhuobinZheng12,ChunYuan,ZhihuiLin12,andYangyangCheng12. Self-adaptivedoubleboot-
strappedddpg. InInternationalJointConferenceonArtificialIntelligence,2018.
TaoZhou,ZoltánKuscsik,Jian-GuoLiu,MatúšMedo,JosephRushtonWakeling,andYi-Cheng
Zhang. Solvingtheapparentdiversity-accuracydilemmaofrecommendersystems. Proceedingsof
theNationalAcademyofSciences,107(10):4511–4515,2010.
LixinZou,LongXia,ZhuoyeDing,JiaxingSong,WeidongLiu,andDaweiYin. Reinforcement
learning to optimize long-term user engagement in recommender systems. In Proceedings of
the25thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,pp.
2810–2818,2019.
15Preprint.
APPENDIX
Table of Contents
A ProofofConvergenceofMaximizerActorinTabularSettings 17
B ProofofReducingNumberofLocalOptimainSuccessiveSurrogates 18
C EnvironmentDetails 19
C.1 MiningEnv . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C.2 RecSim . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.3 ContinuousControl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D AdditionalResults 22
D.1 ExperimentsonContinuousControl(UnrestrictedMujoco). . . . . . . . . . . . 22
D.2 Per-EnvironmentAblationResults . . . . . . . . . . . . . . . . . . . . . . . . 22
D.3 SACisOrthogonaltoSAVO. . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.4 IncreasingSizeofDiscreteActionSpaceinRecSim . . . . . . . . . . . . . . . 24
E ValidatingSAVODesignChoices 25
E.1 DesignChoices:Actionsummarizers . . . . . . . . . . . . . . . . . . . . . . . 25
E.2 ConditioningonPreviousActions:FiLMvs.MLP . . . . . . . . . . . . . . . . 25
E.3 ExplorationNoisecomparison:OUNoisevsGaussian . . . . . . . . . . . . . . 25
F NetworkArchitectures 26
F.1 SuccessiveActors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.2 SuccessiveSurrogates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.3 ListSummarizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.4 Feature-wiseLinearModulation(FiLM) . . . . . . . . . . . . . . . . . . . . . 27
G ExperimentandEvaluationSetup 27
G.1 AggregatedResults:PerformanceProfiles. . . . . . . . . . . . . . . . . . . . . 27
G.2 ImplementationDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
G.3 CommonHyperparameterTuning . . . . . . . . . . . . . . . . . . . . . . . . . 27
G.4 Hyperparameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
H Q-ValueLandscapeVisualizations 28
H.1 1-DimensionalActionSpaceEnvironments . . . . . . . . . . . . . . . . . . . . 28
H.2 High-DimensionalActionSpaceEnvironments . . . . . . . . . . . . . . . . . . 29
16Preprint.
A PROOF OF CONVERGENCE OF MAXIMIZER ACTOR IN TABULAR SETTINGS
TheoremA.1(ConvergenceofPolicyIterationwithMaximizerActor). InafiniteMarkovDecision
Process(MDP)withfinitestatespaceS,consideramodifiedpolicyiterationalgorithmwhere,at
eachiterationn,wehaveasetofk+1policies{ν ,ν ,...,ν },withν =µ beingthepolicyat
0 1 k 0 n
thecurrentiterationlearnedwithDPG.Wedefinethemaximizeractorµ as:
M
µ (s)=arg max Qµn(s,a), (10)
M
a∈{ν0(s),ν1(s),...,νk(s)}
where Qµn(s,a) is the action-value function for policy µ n. Then, the modified policy iteration
algorithmusingthemaximizeractorisguaranteedtoconvergetoafinalpolicyµ .
N
Proof. Toproveconvergence,wewillshowthatthesequenceofpoliciesµ yieldsmonotonically
n
non-decreasingvaluefunctionsthatconvergetoastablevaluefunctionVN.
POLICYEVALUATIONCONVERGES
Thus,iterativelyapplyingTπ startingfromanyinitialQ convergestotheuniquefixedpointQπ.
0
Giventhecurrentpolicyµ n,thepolicyevaluationcomputestheaction-valuefunctionQµn,satisfying:
(cid:88)
Qµn(s,a)=R(s,a)+γ P(s,a,s′)Vµn(s′),
s′
whereVµn(s′)=Qµn(s′,µ n(s′)).
Inthetabularsetting,theBellmanoperatorTµn definedby
(cid:88)
[TµnQ](s,a)=R(s,a)+γ P(s,a,s′)Q(s′,µ (s′))
n
s′
isacontractionmappingwithrespecttothemaxnorm∥·∥ withcontractionfactorγ,
∞
∥TµnQ−TµnQ′∥ ≤γ∥Q−Q′∥ .
∞ ∞
Therefore,iterativelyapplyingTµn convergestotheuniquefixedpointQµn.
POLICYIMPROVEMENTWITHDPGANDMAXIMIZERACTOR
Step1: DPGUpdate
Wedefineµ˜ astheDPGpolicythatlocallyupdatesµ towardsmaximizingtheexpectedreturn
n n
basedonQµn. Foreachstates,weperformagradientascentstepusingtheDeepPolicyGradient
(DPG)methodtoobtainanimprovedpolicyµ˜ :
n
µ˜ n(s)←µ n(s)+α∇
aQµn(s,a)(cid:12)
(cid:12) a=µn(s),
whereα>0isasuitablestepsize.
ThisDPGgradientstepleadstolocalpolicyimprovementfollowingoverµ (Silveretal.,2014):
n
Vµ˜n(s)≥Vµn(s), ∀s∈S.
(b)MaximizerActor
Givenadditionalpoliciesν ,...,ν ,definethemaximizeractorµ as:
1 k n+1
µ (s)=arg max Qµn(s,a).
n+1
a∈{µ˜n(s),ν1(s),...,νk(s)}
Sinceµ n+1(s)selectstheactionmaximizingQµn(s,a)amongcandidates,wehave:
Qµn(s,µ (s))= max Qµn(s,a)≥Qµn(s,µ˜ (s))≥Vµn(s).
n+1 n
a∈{µ˜n(s),ν1(s),...,νk(s)}
17Preprint.
BythePolicyImprovementTheorem,sinceQµn(s,µ n+1(s))≥Vµn(s)foralls,itfollowsthat:
Vµn+1(s)≥Vµn(s), ∀s∈S.
Thus,thesequence{Vµn}ismonotonicallynon-decreasing.
CONVERGENCEOFPOLICYITERATION
Sincethesequence{Vµn}ismonotonicallynon-decreasingandboundedabovebyV∗,itconverges
tosomeV∞ ≤V∗. Giventhefinitenumberofpossiblepolicies,thesequence{µ }musteventually
n
repeatapolicy. SupposethatatsomeiterationN,thepolicyrepeats,i.e.,µ =µ .
N+1 N
Atthispoint,sincethepolicyhasn’tchanged,wehave:
µ (s)=arg max QµN(s,a), ∀s∈S.
N
a∈{µ˜N(s),ν1(s),...,νk(s)}
Sinceµ˜ (s)isobtainedbyperformingaDPGupdateonµ (s),andwehavethatµ (s)maximizes
N N N
QµN(s,a)among{µ˜ N(s),ν 1(s),...,ν k(s)},itmustbethat:
QµN(s,µ (s))≥QµN(s,a), ∀a∈{µ˜ (s),ν (s),...,ν (s)}.
N N 1 k
Moreover,sinceµ˜ (s)isobtainedviagradientascentfromµ (s),andyetdoesnotyieldahigher
N N
Q-value,itimpliesthat:
∇
aQµN(s,a)(cid:12)
(cid:12)
a=µN(s)
=0.
Thissuggeststhatµ N(s)isalocalmaximumofQµN(s,a). Thisshowsthatthismodificationtothe
policyiterationalgorithmofDPGisguaranteedtoconverge.
Sincetheset{µ˜ (s),ν (s),...,ν (s)}includesmoreactionsfromA,µ (s)istheactionthatbetter
N 1 k N
maximizesQµN(s,a)thanµ˜ N. Therefore,µ
N
isagreedierpolicywithrespecttoQµN thanµ˜ N.
B PROOF OF REDUCING NUMBER OF LOCAL OPTIMA IN SUCCESSIVE
SURROGATES
TheoremB.1. Considerastates∈S,thefunctionQasdefinedinEq.1,andthesurrogatefunctions
Ψ asdefinedinEq.7. LetN (f)denotethenumberoflocaloptima(assumedcountable)ofa
i opt
functionf :A→R,whereAistheactionspace. Then,
N (Q(s,a))≥N (Ψ (s,a;{a }))≥N (Ψ (s,a;{a ,a }))≥···≥N (Ψ (s,a;{a ,...,a })).
opt opt 0 0 opt 1 0 1 opt k 0 k
Proof. Foreachi≥0,definethesurrogatefunctionΨ recursively:
i
Ψ (s,a;{a ,...,a })=max{Q(s,a),τ }, (11)
i 0 i i
where
τ = max Q(s,a ).
i j
0≤j≤i
Notethatτ isnon-decreasingwithrespecttoi,i.e.,τ ≥τ .
i i+1 i
Ourgoalistoshowthatforeachi≥0,
N (Ψ (s,a;{a ,...,a }))≥N (Ψ (s,a;{a ,...,a })).
opt i 0 i opt i+1 0 i+1
WeproceedbyconsideringhowthesetoflocaloptimachangesfromΨ toΨ .
i i+1
Consideranylocaloptimuma′ofΨ . Therearetwocases:
i
Case1: Q(s,a′)>τ
i
Inthiscase,Ψ (s,a′) = Q(s,a′)andΨ coincideswithQinaneighborhoodofa′. Sincea′ isa
i i
localoptimumofΨ ,itisalsoalocaloptimumofQ. Becauseτ ≥τ ,therearetwosubcases:
i i+1 i
18Preprint.
Subcase1a: Q(s,a′)>τ
i+1
Here,Ψ (s,a′)=Q(s,a′)and,inaneighborhoodofa′,Ψ coincideswithQ. Thus,a′remains
i+1 i+1
alocaloptimumofΨ .
i+1
Subcase1b: Q(s,a′)≤τ
i+1
SinceQ(s,a′)>τ andτ ≥τ ,thisimpliesτ >τ andQ(s,a′)=τ . Then,
i i+1 i i+1 i i+1
Ψ (s,a′)=τ ,
i+1 i+1
and in a neighborhood around a′, Ψ (s,a) ≥ τ . Thus, a′ is not a local optimum of Ψ
i+1 i+1 i+1
becausethereisnoneighborhoodwhereΨ (s,a)<Ψ (s,a′).
i+1 i+1
Case2: Q(s,a′)≤τ
i
In this case, Ψ (s,a′) = τ , and Ψ is constant at τ in a neighborhood of a′. Thus, a′ may be
i i i i
consideredalocaloptimuminΨ ifthefunctiondoesnotexceedτ nearby. WhenmovingtoΨ ,
i i i+1
sinceτ ≥τ ,wehave:
i+1 i
Ψ (s,a′)=τ ≥τ .
i+1 i+1 i
Intheneighborhoodofa′,Ψ remainsatleastτ ,soa′ isnotalocaloptimuminΨ unless
i+1 i+1 i+1
Q(s,a)<τ inaneighborhoodarounda′.
i+1
However,sinceΨ (s,a)≥τ foralla,thefunctiondoesnotdecreasebelowΨ (s,a′)inany
i+1 i+1 i+1
neighborhoodofa′. Therefore,a′isnotalocaloptimumofΨ .
i+1
Conclusion: Fromtheabovecases,weobservethat:
• Anylocaloptimuma′ofΨ whereQ(s,a′)>τ remainsalocaloptimuminΨ .
i i+1 i+1
• Anylocaloptimuma′ ofΨ whereQ(s,a′) ≤ τ doesnotremainalocaloptimumin
i i+1
Ψ .
i+1
Since Ψ does not introduce new local optima (because Ψ (s,a) ≥ Ψ (s,a) for all a and
i+1 i+1 i
coincideswithQonlywhereQ(s,a)>τ ),thenumberoflocaloptimadoesnotincreasefromΨ
i+1 i
toΨ .
i+1
BaseCase: Fori=0,wehave:
Ψ (s,a;{a })=max{Q(s,a),Q(s,a )}.
0 0 0
IfweconsiderQ(s,a )tobelessthantheminimumvalueofQ(s,a)(whichcanbearrangedby
0
choosinga appropriatelyorbydefiningτ tobelessthaninf Q(s,a)),thenΨ (s,a)=Q(s,a),
0 0 a 0
andthebasecaseholdstrivially.
InductiveStep: Assumingthat
N (Ψ (s,a;{a ,...,a }))≤N (Ψ (s,a;{a ,...,a })),
opt i 0 i opt i 0 i
wehaveshownthat
N (Ψ (s,a;{a ,...,a }))≤N (Ψ (s,a;{a ,...,a })).
opt i+1 0 i+1 opt i 0 i
Byinduction,itfollowsthat:
N (Q(s,a))≥N (Ψ (s,a;{a }))≥N (Ψ (s,a;{a ,a }))≥···≥N (Ψ (s,a;{a ,...,a })).
opt opt 0 0 opt 1 0 1 opt k 0 k
C ENVIRONMENT DETAILS
C.1 MININGENV
Thegridworldenvironment,introducedin §5,requiresanagenttoreachagoalbynavigatinga2D
mazeassoonaspossiblewhilebreakingtheminesblockingtheway.
State: Thestatespaceisan8+Kdimensionalvector,whereKequalstomine-category-size. This
vectorconsistsof4independentpiecesofinformation: AgentPosition,AgentDirection,Surrounding
Path,andFrontCellType.
19Preprint.
Movie
Click or Not
↓ ↓
Simulated
Movielens
x100 Data
(a) Mine World (b) Recommender Systems (c) Continuous Control
Figure 13: Benchmark Environments involve discrete action space tasks like Mine World and
recommendersystems(simulatedandMovieLens-Data)andrestrictedlocomotiontasks.
Agent Empty Basic Complex Empty A Tc yt pio en N Nav ui mga bt eio rn M Bein fe o- rI eD M Ain fte e- rID
Mines Mines (No mine)
Basic Tools 0 0 [7, 1 8 4, ] …, 15
Mine x8
Select All
Complex Tools 0 0 [0, 1, …, 6] 1[ 47 ], 8 o, r … 15,
x63
Select 42
Goal
Navigation actions 1 [0, 1, 2, 3] 0 0
x4
Mining World Select All
Overview Action Overview
Figure14: MiningExpedition. Theredagentmustreachthegreengoalbynavigatingthegridand
usingoneormorepick-axestocleareachmineblockingthepath.
1. AgentPosition: A2Dvectorrepresentingtheagent’sxandycoordinates.
2. AgentDirection: Onedimensionrepresentingdirections(0: right,1: down,2: left,3: up).
3. SurroundingPath: A4-dimensionalvectorindicatingiftheadjacentcellsareemptyoragoal(1:
empty/goal,0: otherwise).
4. FrontCellType: A(K+1)-dimensionalone-hotvectorwithfirstK dimensionsrepresentingthe
typeofmineandthelastdimensionrepresentingifthecellisempty(zero)orgoal(one).
Finally,wewillnormalizeeachdimensionto[0,1]witheachdimension’sminimum/maximumvalue.
Termination: Anepisodeterminateswhentheagentreachesthegoalorafter100timesteps. Upon
reset,thegridlayoutchangeswhilekeepingtheagent’sstartandgoalpositionsfixed.
Actions: Actions include navigation (up, down, left, right) and pick-axe categories. Navigation
changestheagent’sdirectionandattemptstomoveforward. Theagentcannotstepintoaminebut
willchangedirectionwhentryingtostepontoamineortheborderofthegrid. Thepick-axetool
actions(50types)haveapredefinedone-to-onemappingofhowtheyinteractwiththemines,which
meanstheycanbesuccessfullyappliedtoonlyonekindofmine,andeithertransformthatkindof
mineintoanothertypeofmineordirectlybreakit.
Reward: Theagent’srewardcomprisesagoal-reachingreward,adistance-basedstepreward,and
rewardsforsuccessfultooluseormine-breaking. Thegoalrewardisdiscountedbystepstakenover
theepisode,encouragingshorterpathstoreachthegoal.
(cid:18) (cid:19)
N
R(s,a) =1 ·R 1−λ currentsteps +
Goal Goal Goal N
maxsteps
R (D −D ) + (12)
Step distance before distance after
1 ·R +
correcttoolapplied Tool
1 ·R
successfullybreakmine Bonus
20
↓ ↓Preprint.
where R =10, R =0.1, R =0.1, R =0.1, λ =0.9, N =100
Goal Step Tool Bonus Goal maxsteps
ActionRepresentationsActionsarerepresentedasa4Dvectorwithnormalizedvalues[0,1]as
describedinFigure14. Dimensionsrepresentskillcategory(navigationorpick-axe), movement
direction (right, down, left, up), mine type where this action can be applied, and the outcome of
applyingthetooltothemine,respectively.
C.2 RECSIM
Inthesimulatedrecommendationsystem(RecSys)environment,theagentselectsanitemfromalarge
setthatalignswiththeuser’sinterests.Usersaremodeledwithdynamicallychangingpreferencesthat
evolvebasedontheirinteractions(clicks). Theagent’sobjectiveistoinfertheseevolvingpreferences
fromuserclicksandrecommendthemostrelevantitemstomaximizethetotalnumberofclicks.
State: Theuser’sinterestisrepresentedbyanembeddingvectore ∈Rn,wherenisthenumberof
u
itemcategories. Thisembeddingevolvesovertimeastheuserinteractswithdifferentitems. Whena
userclicksonanitemwithembeddinge ∈Rn,theuserinterestembeddinge isupdatedasfollows:
i u
e⊤e +1
e ←e +∆e , withprobability u i
u u u 2
1−e⊤e
e ←e −∆e , withprobability u i,
u u u 2
where∆e representsanadjustmentthatdependsonthealignmentbetweene ande . Thisupdate
u u i
mechanismadjuststheuser’spreferencetowardstheclickeditem,reinforcingtheconnectionbetween
thecurrentactiononfuturerecommendations.
Action: Theactionsetconsistsofallitemsthatcanberecommended,andtheagentmustselectthe
itemmostrelevanttotheuser’slong-termpreferencesovertheepisode.
Reward: Therewardisbasedonuserfeedback: eitheraclick(reward=1)orskip(reward=0). The
usermodelcomputesascoreforeachitemusingthedotproductoftheuseranditemembeddings:
score =⟨e ,e ⟩
item u i
Theclickprobabilityiscomputedwithasoftmaxovertheitemscoreandapredefinedskipscore:
escoreitem
p = , p =1−p
item escoreitem +escoreskip skip item
Theuserthenstochasticallychoosestoclickorskipbasedonthisdistribution.
ActionRepresentations: FollowingJainetal.(2021),itemsarerepresentedascontinuousvectors
sampledfromaGaussianMixtureModel(GMM),withcentersrepresentingitemcategories.
C.3 CONTINUOUSCONTROL
MuJoCo(Todorovetal.,2012)isaphysicsenginethatprovidesasuiteofstandardreinforcement
learningtaskswithcontinuousactionspaces,commonlyusedforbenchmarkingcontinuouscontrol
algorithms. Webrieflydescribesomeofthesetasksbelow:
Hopper: Theagentcontrolsaone-leggedrobotthatmustlearntohopforwardwhilemaintaining
balance. Theobjectiveistomaximizeforwardvelocitywithoutfalling.
Walker2d: Theagentcontrolsatwo-leggedbipedalrobotthatmustlearntowalkforwardefficiently
whilemaintainingbalance. Thegoalistoachievestablelocomotionathighspeeds.
HalfCheetah:Theagentcontrolsaplanar,cheetah-likerobotwithmultiplejointsina2Denvironment.
Thetaskrequireslearningacoordinatedgaittopropeltherobotforwardasquicklyaspossible.
Ant:Theagentcontrolsafour-legged,ant-likerobotwithmultipledegreesoffreedom.Thechallenge
istolearntowalkandnavigateefficientlywhilemaximizingforwardprogress.
C.3.1 RESTRICTEDLOCOMOTIONINMUJOCO
TherestrictedlocomotionMujocotasksareintroducedtodemonstratehowcommonDPG-based
approachesgetstuckinlocaloptimawhentheQ-landscapeiscomplexandnon-convex. Thissetting
21Preprint.
limitstherangeofactionstheagentcanperformineachdimension,simulatingrealisticscenarios
such as wear and tear of hardware. For example, action space may be affected as visualized in
Figure6. Amixture-of-hypersphereactionspaceisusedtosimulatesuchasymmetricrestrictions,
whichaffecttherangeoftorquesfortheHopperandWalkerjoints,aswellastheforcesappliedtothe
invertedpendulumanddoublependulum. Thehyperspheresaresampledrandomly,andtheirsizeand
radiusarecarefullytunedtoensurethattheactionspacehasenoughvalidactionstosolvethetask.
Definitionofrestriction.
• RestrictedHopper&Walker
Invalidactionvectorsarereplacedwith0bychangingtheenvironment’sstepfunctioncode:
1 def step(action):
2 ...
3 if check_valid(action):
4 self.do_simulation(action)
5 else:
6 self.do_simulation(np.zeros_like(action))
7 ...
The Hopper action space is 3-dimensional, with torque applied to [thigh,leg,foot],
while the Walker action space is 6-dimensional, with torque applied to
[rightthigh,rightleg,rightfoot,leftthigh,leftleg,leftfoot]. The physical implication of re-
strictedlocomotionisthatzerotorquesareexertedforthe∆tdurationbetweentwoactions,i.e.,
notorquesareappliedfor0.008seconds. Thiseffectivelyslowsdowntheagent’scurrentvelocities
andangularvelocitiesduetofrictionwhenevertheagentselectsaninvalidaction.
• InvertedPendulum&InvertedDoublePendulum
Invalidactionvectorsarereplacedwith-1bychangingtheenvironment’sstepfunctioncode:
1 def step(action):
2 ...
3 if not check_valid(action):
4 action[:] = -1.
5 self.do_simulation(action)
6 ...
Theactionspaceis1-dimensional,withforceappliedonthecart. Theimplicationisthatthecartis
pushedintheleftdirectionfor0.02(default)seconds. Notethattheactionvectorsarenotzeroed
because a 0-action is often the optimal action, particularly when the agent starts upright. This
wouldmaketheoptimalpolicytriviallybelearningtoselectinvalidactions.
D ADDITIONAL RESULTS
D.1 EXPERIMENTSONCONTINUOUSCONTROL(UNRESTRICTEDMUJOCO)
In standard MuJoCo tasks, the Q-landscape is likely easier to optimize compared to MuJoCo-
Restrictedtasks. InFigure15,wefindthatbaselinemodelsconsistentlyperformwellinallstandard
tasks,unlikeinMuJoCo-Restrictedtasks. Thus,wecaninferthefollowing:
1. Baselineshavesufficientcapacity,arewell-tuned,andcannavigatesimpleQ-landscapesoptimally.
2. SAVOperformsonparwithothermethodsinMuJoCotaskswheretheQ-landscapeiseasierto
optimize,showingthatSAVOisarobust,widelyapplicableactorarchitecture.
3. Baselinesperformingwellinunrestrictedlocomotionbutsuboptimallyinrestrictedlocomotion
delineatesthecauseofsuboptimalitytobethecomplexityoftheunderlyingQ-landscapes,such
asthoseshowninFigure2. SAVOisclosertooptimalinbothsettingsbecauseitcannavigate
bothsimpleandcomplexQ-functionsbetterthanalternateactorarchitectures.
D.2 PER-ENVIRONMENTABLATIONRESULTS
Figure 16 shows the per-environment performance of SAVO ablations, compiled into aggregate
performanceprofilesinFigure7b. TheSAVO-Approximationvariantunderperformssignificantly
22Preprint.
Ant-v4 HalfCheetah-v4 Hopper-v4 Walker2D-v4
12000 4000
6000
4800 9600 3200 4000
3600 7200 2400 3200
2400 2400 4800 1600
1600
1200 2400 800 800
0 0 0 0
0.000 0.375 0.750 1.125 1.500 0.000 0.375 0.750 1.125 1.500 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
SAVO (Ours) 1-Actor (TD3) 1-Actor, k-Samples (Wolpertinger) k-Actors (Ensemble)
Evolutionary Actor (CEM) Greedy-AC Greedy-TD3
Figure 15: Unrestricted Locomotion (§D.1). SAVO and most baselines perform optimally in
standardMuJoCocontinuouscontroltasks,wheretheQ-landscapeiseasytonavigate.
MineWorld RecSim RecSim-Data Hopper (Restricted)
0.98 6.3 3000
0.78 5.5 55 2400 0.58 4.7 46 1800
3.9 37
0.38 3.1 28 1200
0.18 2.3 19 600
0.02 1.5 10 0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.0 0.5 1.0 1.5 2.0
Env Steps 1e7 Env Steps 1e7 Env Steps 1e7 Env Steps 1e6
Walker2D (Restricted) Inverted Pendulum (Restricted) Inv. Dbl Pend. (Restricted)
4000 1000 10000
3200 800 8000
2400 600 6000
1600 400 4000
800 200 2000
0 0 0
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
SAVO (Ours) SAVO -Approximation SAVO -Action Smoothing SAVO + Previous Actions SAVO + Joint Action
Figure16: AblationsofSAVOvariations(§D.2)showstheimportanceof(i)theapproximation
ofsurrogates,(ii)removingTD3’sactionsmoothing,(iii)conditioningonprecedingactionsinthe
successiveactorandsurrogatenetworks,and(iv)individualactorsthatseparatetheactioncandidate
predictioninsteadofajointhigh-dimensionallearningtask.
indiscreteactionspacetasks,wheretraversingbetweenlocaloptimaiscomplexduetonearbyactions
having diverse Q-values (see the right panel of Figure 2). Similarly, adding TD3’s target action
smoothingtoSAVOresultsininaccuratelearnedQ-valueswhenseveraldifferentlyvaluedactions
existnearthetargetaction,asinthecomplexlandscapesofalltasksconsidered.
RemovinginformationaboutprecedingactionsdoesnotsignificantlydegradeSAVO’sperformance
sinceprecedingactions’Q-valuesareindirectlyincorporatedintothesurrogates’trainingobjective
(seeEq.(9)),exceptforMineWorldwherethisinformationhelpsimproveefficiency.
TheSAVO+Jointablationlearnsasingleactorthatoutputsajointactioncomposedofkconstituents,
aimingtocovertheactionspacesothatmultiplecoordinatedactionscanbettermaximizetheQ-
functioncomparedtoasingleaction. However,thisincreasesthecomplexityofthearchitectureand
onlyworksinlow-dimensionaltaskslikeInverted-PendulumandInverted-Double-Pendulum. SAVO
simplifiesactioncandidategenerationbyusingseveralsuccessiveactorswithspecializedobjectives,
enablingeasiertrainingwithoutexplodingtheactionspace.
D.3 SACISORTHOGONALTOSAVO
WecompareSoftActor-Critic(SAC),TD3,andTD3+SAVOacrossvariousMujoco-Restrictedtasks.
Figure17showsthatSACsometimesoutperformsandsometimesunderperformsTD3. Therefore,
SAC’sstochasticpolicydoesnotaddressthechallengeofnon-convexityintheQ-function.Incontrast,
SAVO+TD3consistentlyoutperformsTD3andSAC,demonstratingtheeffectivenessofSAVOin
complexQ-landscapes. WhileSACcanbebetterthanTD3incertainenvironments,itsalgorithmic
modificationsareorthogonaltothearchitecturalimprovementsduetotheSAVOactor.
23
nruteR
lavE
etaR sseccuS
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavEPreprint.
Hopper (Restricted) Walker2D (Restricted) Inverted Pendulum (Restricted) Inv. Dbl Pend. (Restricted)
3000 4000 1000 10000
2400 3200 800 8000
1800 2400 600 6000
1200 1600 400 4000
600 SAVO 800 SAVO 200 SAVO 2000 SAVO
TD3 TD3 TD3 TD3
0 SAC 0 SAC 0 SAC 0 SAC
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
Figure17: SACisorthogonaltotheeffectofSAVO(§D.3). SACisadifferentalgorithmthanTD3,
whereasSAVOisaplug-inactorarchitectureforTD3. Thus,taskswhereSACoutperformsTD3
differfromtaskswhereSAVOoutperformsTD3. Also,TD3outperformsSACinRestrictedHopper
andInverted-Double-Pendulum. However,SAVO+TD3guaranteesimprovementoverTD3.
Inv. Dbl Pend. (Restricted)
10000
IntheRestrictedInvertedDoublePendulumtask(Fig-
ure18),SACunderperformsevenTD3.Analogousto
8000
TD3,thesuboptimalityisduetothenon-convexityin
thesoftQ-functionlandscape,wheresmallchanges
6000
innearbyactionscanleadtosignificantlydifferent
environmentreturns. WecombineSACwithSAVO’s 4000
successiveactorsandsurrogatestobettermaximize
thesoftQ-function,naivelyconsideringactioncan- 2000 TD3 + SAVO
TD3
didatesforµ asthemeanactionofthestochastic SAC + SAVO
M SAC
actors. Weobservethatthispreliminaryversionof 0
0.0 0.5 1.0 1.5 2.0
SAC+SAVOshowssignificantimprovementsover Env Steps 1e6
SACincomplexQ-landscapes. Infuturework,we
Figure 18: SAC is suboptimal in complex
aim to formalize a SAVO-like objective that effec-
Q-landscape (§D.3) of Restricted Inverted
tivelyenablesSAC’sstochasticactorstonavigatethe
DoublePendulum,butSAVOhelps.
non-convexityofitssoftQ-function.
D.4 INCREASINGSIZEOFDISCRETEACTIONSPACEINRECSIM
WetesttherobustnessofourmethodtomorechallengingQ-valuelandscapesinFigure19,especially
indiscreteactionspacetaskswithmassiveactionspaces. InRecSim,weincreasethenumberof
actual discrete actions from 10,000 to 100,000 and 500,000. The experiments show that SAVO
outperformsthebest-performingbaselineofTD3+Sampling(Wolpertinger)andthebest-performing
ablationofSAVO+JointAction. ThisshowsthatSAVOmaintainsrobustperformanceevenasthe
actionspacesizeincreasesandtheQ-functionlandscapebecomesmoreintricate. Incontrast,the
baselinesexperiencedperformancedeteriorationasactionsizesgrewlarger.
RecSim RecSim
6.3 6.3
SAVO SAVO
TD3+Sampling TD3+Sampling
5.5 Joint 5.5 Joint
4.7 4.7
3.9 3.9
3.1 3.1
2.3 2.3
1.5 1.5
0.00 0.25 0.50 0.75 1.00 0 2 4 6 8
Env Steps 1e7 Env Steps 1e6
Figure 19: Increasing RecSim action set size (§D.4). (Left) 100,000 items, (Right) 500,000
items(6seeds)maintainstheperformancetrendsofSAVOandthebest-performingbaseline(TD3+
Sampling)andthebest-performingablation(SAVOwithJoint-Action).
24
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavEPreprint.
MineWorld RecSim RecSim-Data
6.3
0.98
0.78 5.5 55
4.7 46
0.58
3.9 37
0.38
3.1 28
0.18 SAVO-DEEPSET 2.3 SAVO-DEEPSET 19 SAVO-DEEPSET
SAVO-LSTM SAVO-LSTM SAVO-LSTM
SAVO-TRANSFORMER SAVO-TRANSFORMER SAVO-TRANSFORMER
0.02 1.5 10
0.000 1.875 3.750 5.625 7.500 0.00 0.25 0.50 0.75 1.00 0 2 4 6 8
Env Steps 1e6 Env Steps 1e7 Env Steps 1e6
Figure20: Actionsummarizercomparison(§E.1). TheeffectisnotsignificantTheresultsare
averagedover5randomseeds,andtheseedvarianceisshownwithshading.
RecSim RecSim-Data MineWorld
6.3 0.98
5.5 55
0.78 4.7 46
3.9 37 0.58
3.1 28 0.38
2.3 SAVO-FILM (Ours) 19 SAVO-FILM (Ours) 0.18 SAVO-FILM (Ours)
1.5 SAVO-No-FILM 10 SAVO-No-FILM 0.02 SAVO-No-FILM
0.00 0.25 0.50 0.75 1.00 0 2 4 6 8 0.000 1.875 3.750 5.625 7.500
Env Steps 1e7 Env Steps 1e6 Env Steps 1e6
Hopper (Restricted) Walker2D (Restricted) Inverted Pendulum (Restricted) Inverted Double Pendulum (Restricted)
3000 4000 1000 10000
2400 3200 800 8000
1800 2400 600 6000
1200 1600 400 4000
600 800 200 2000
SAVO-FILM (Ours) SAVO-FILM (Ours) SAVO-FILM (Ours) SAVO-FILM (Ours)
0 SAVO-No-FILM 0 SAVO-No-FILM 0 SAVO-No-FILM 0 SAVO-No-FILM
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
Figure21: FiLMtoconditiononprecedingactions(§E.2). FiLMensureslayerwisedependenceon
theprecedingactionsforactinginactorsν andforpredictingvalueinsurrogatesΨˆ ,whichgenerally
i i
resultsinbetterperformanceacrosstasks.
E VALIDATING SAVO DESIGN CHOICES
E.1 DESIGNCHOICES: ACTIONSUMMARIZERS
Threekeyarchitectureswereconsideredforthedesignoftheactionsummarizer: DeepSets,LSTM,
andTransformermodels,representedbySAVO,SAVO-LSTM,andSAVO-TransformerinFigure20,
respectively. Ingeneral,theeffectoftheactionsummarizerisnotsignificant,andwechooseDeepSet
foritssimplicityformostexperiments.
E.2 CONDITIONINGONPREVIOUSACTIONS: FILMVS. MLP
Weexaminedtwoapproachesforconditioningonthepreviousactionlistsummary: Feature-wiseLin-
earModulation(FiLM)andconcatenationwithinput,representedbytheFiLMandnon-filmvariants
inFigure21. Acrosstasks,FiLMoutperformedthenon-FiLMversion,showingtheeffectiveness
oflayerwiseconditioninginleveragingprioractioninformationforactionselectionandsurrogate
valueprediction. Thisshowsthatthesuccessiveactorsareappropriatelyutilizingtheactionsfrom
theprecedingactorstotailortheirsearchforoptimalactions,andthesuccessivesurrogatescanbetter
evaluateQ-values,knowingwheretheycouldbethresholdedbythelossfunction.
E.3 EXPLORATIONNOISECOMPARISON: OUNOISEVSGAUSSIAN
WecompareOrnstein-Uhlenbeck(OU)noisewithGaussiannoiseacrossourenvironmentsandfind
that OU noise was generally better, with the difference being minimal. We chose to use OU for
ourexperimentsandacomparisononHopper-RestrictedisshowninFigure22a. WenotethatTD3
(Fujimotoetal.,2018)alsosuggestsnosignificantdifferencebetweenOUandGaussiannoiseand
favoredGaussianforsimplicity. Allourbaselinesusethesameexplorationbackbone,andweconfirm
25
etaR
sseccuS
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
etaR
sseccuS
lavE
nruteR
lavE
nruteR
lavEPreprint.
Hopper (Restricted)
3000
OU-TD3
Gaussian-TD3
2400
1800
1200
600
0
0.0 0.5 1.0 1.5 2.0
Env Steps 1e6
(a)Noisetypes
Figure22: OUversusGaussianNoise(§E.3). Wedonotseeasignificantdifferenceduetothis
choice,andselectOUnoiseduetobetteroverallperformanceinexperiments.
thatusingOUnoiseisconsistentwiththeavailablestate-of-the-artresultswithTD3+Gaussiannoise
oncommonenvironmentslikeAnt,HalfCheetah,Hopper,andWalker2D.
F NETWORK ARCHITECTURES
F.1 SUCCESSIVEACTORS
The entire actor is built as a successive architecture (see Figure 4), where each successive actor
receives two pieces of information: the current state and the action list generated by preceding
actors. Eachactionisconcatenatedwiththestatetocontextualizeitandthensummarizedusinga
list-summarizer,describedin§F.3. Thislistsummaryisconcatenatedwiththestateagainandpassed
intoanMLPwithReLU(3layersforMuJoCotasksand4layersforMineWorld andRecSim)as
describedinTable2. ThisMLPgeneratesoneactionforeachsuccessiveactor,whichissubsequently
usedasaninputactiontothesucceedingactionlists. Fordiscreteactionspacetasks,thisgenerated
actionisprocessedwitha1-NNtofindthenearestexactdiscreteaction. Finally,theactionsgenerated
byeachindividualsuccessiveactorareaccumulated,andthemaximizeractorµ stepfromEq.(5)
M
selectsthehighest-valuedactionaccordingtotheCriticQ-network,describedin§F.2.
F.2 SUCCESSIVESURROGATES
AsFigure4illustrates,thereisasurrogatenetworkforeachactorinthesuccessiveactor-architecture.
Eachsuccessivecriticreceivesthreepiecesofinformation: thecurrentstate,theactionlistgenerated
byprecedingactors,andtheactiongeneratedbytheactorcorrespondingtothecurrentsurrogate.Each
actionisconcatenatedwiththestatetocontextualizeitandthensummarizedusingalist-summarizer,
describedin§F.3. Thislistsummaryisconcatenatedwiththestateandthecurrentaction,andpassed
intoa2-layerMLPwithReLU(SeeTable2). ThisMLPgeneratesthesurrogatevalueΨˆ (s,a;a )
i <i
andisusedasanobjectivetoascendoverbyitscorrespondingactorν .
i
F.3 LISTSUMMARIZERS
To extract meaningful information from the list of candidate actions, we employed several list
summarizationmethodsfollowingJainetal.(2021). Thesemethodsaredescribedbelow:
Bi-LSTM: The action representations of the preceding actors’ actions are first passed through a
two-layermultilayerperceptron(MLP)withReLUactivationfunctions. TheoutputofthisMLPis
thenprocessedbyatwo-layerbidirectionalLSTMnetwork(Huangetal.,2015). Theresultingoutput
isfedintoanothertwo-layerMLPtocreateanactionsetsummary,whichservesasaninputforthe
actor-network(§F.1)andthesurrogatenetwork(§F.2).
26
nruteR
lavEPreprint.
DeepSet: Theactionrepresentationsoftheprecedingactors’actionsareinitiallyprocessedbya
two-layerMLPwithReLUactivations. Theoutputsarethenaggregatedusingmeanpoolingoverall
candidateactionstocompresstheinformationintoafixed-sizesummary. Thissummaryispassed
throughanothertwo-layerMLPwithReLUactivationtoproducetheactionsetsummary, which
servesasaninputfortheactor-network(§F.1)andthesurrogatenetwork(§F.2).
Transformer:SimilartoBi-LSTM,theactionrepresentationsoftheprecedingactors’actionsarefirst
processedbyatwo-layerMLPwithReLUactivations. TheoutputsaretheninputintoaTransformer
networkwithself-attentionandfeed-forwardlayerstosummarizetheinformation. Theresulting
summaryisusedaspartoftheinputtotheactor-network(§F.1)andthesurrogatenetwork(§F.2).
F.4 FEATURE-WISELINEARMODULATION(FILM)
Feature-wise Linear Modulation (Perez et al., 2018) is a technique used in neural networks to
conditionintermediatefeaturerepresentationsbasedonexternalinformation,enhancingthenetwork’s
adaptabilityandperformanceacrossvarioustasks.FiLMmodulatesthefeaturesofalayerbyapplying
learned,feature-wiseaffinetransformations. Specifically,givenasetoffeaturesF,FiLMappliesa
scalingandshiftingoperation,
FiLM(F)=γ⊙F+β,
whereγ andβ aremodulationparameterslearnedfromanothersource(e.g.,aseparatenetworkor
input),and⊙denoteselement-wisemultiplication. Thisapproachallowsthenetworktoselectively
emphasizeorde-emphasizeaspectsoftheinputdata, effectivelycapturingcomplexandcontext-
specificrelationships. FiLMhasbeensuccessfullyappliedintaskssuchasvisualquestionanswering
andimagecaptioning, whereconditioningvisualfeaturesontextualinputisessential. Weapply
FiLMwhileconditioningtheactorandsurrogatenetworksonthesummaryofprecedingactions.
G EXPERIMENT AND EVALUATION SETUP
G.1 AGGREGATEDRESULTS: PERFORMANCEPROFILES
Torigorouslyvalidatetheaggregateefficacyofourapproach,weadopttherobustevaluationmethod-
ologyproposedbyAgarwaletal.(2021). Byincorporatingtheirsuggestedperformanceprofiles,
weconductacomprehensivecomparisonbetweenourmethodandbaselineapproaches,providing
a thorough understanding of the statistical uncertainties inherent in our results. Figure 7a shows
theperformanceprofilesacrossalltasks. Thex-axisrepresentsnormalizedscores,calculatedusing
min-max scaling based on the initial performance of untrained agents aggregated across random
seeds (i.e., Min) and the final performance from Figure 8 (i.e., Max). The results show that our
methodconsistentlyoutperformsthebaselinesacrossvariousrandomseedsandenvironments. Our
performancecurveremainsat thetopas thex-axis progresses, while thebaseline curvesdecline
earlier. ThishighlightsthereliabilityofSAVOoverdifferentenvironmentsand10seeds.
G.2 IMPLEMENTATIONDETAILS
WeusedPyTorch(Paszkeetal.,2019)forourimplementation,andtheexperimentswereprimarily
conductedonworkstationswitheitherNVIDIAGeForceRTX2080Ti,P40,orV32GPUson. Each
experimentseedtakesabout4-6hoursforMineWorld,12-72hoursforMujoco,and6-72hoursfor
RecSim,toconverge. WeusetheWeights&Biasestool(Biewald,2020)forplottingandlogging
experiments. AlltheenvironmentswereinterfacedusingOpenAIGymwrappers(Brockmanetal.,
2016). WeusetheAdamoptimizer(Kingma&Ba,2014)throughoutfortraining.
G.3 COMMONHYPERPARAMETERTUNING
Toensurefairnessacrossallbaselinesandourmethods,weperformedhyperparametertuningover
parametersthatarecommonacrossmethods:
• Learning Rates of Actor and Critic: (Actor) We searched over learning rates
{0.01,0.001,0.0001,0.0003} and found that 0.0003 was the most stable for the actor’s learn-
ingacrossalltasks. (Critic)Similartotheactor,wesearchedoverthesamesetoflearningrates
andfoundthesamevalueof0.0003wasthemoststableforthecritic’slearningacrossalltasks.
27Preprint.
• NetworkSizesofActorandCritic: Foreachtask,wesearchedoversimple3or4MLPlayersto
determinethenetworksizethatperformedbestbutdidnotobservemajordifferences. (Critic)To
ensureafaircomparison,weusedthesamenetworksizeforthecritic(Q-network)andsurrogates
acrossallmethodswithineachtask. (Actor)Similartothecritic,weusedthesamenetworksize
forthevariousactorsinallthebaselinesandsuccessiveactorsinSAVOwithinaparticulartask.
G.4 HYPERPARAMETERS
TheenvironmentandRLalgorithmhyperparametersaredescribedinTable2.
Hyperparameter MineWorld MuJoCo&Adroit RecSim
Environment
TotalTimesteps 107 3×106 107
Numberofepochs 5,000 8,000 10,000
#EnvsinParallel 20 10 16
EpisodeHorizon 100 1000 20
NumberofActions 104 N/A 10000
TrueActionDim 4 5 30
ExtraActionDim 5 N/A 15
RLTraining
Batchsize 256 256 256
Buffersize 5×105 5×105 106
Actor: LR 3×10−4 3×10−4 3×10−4
Actor: ϵ 1 1 1
start
Actor: ϵ 0.01 0.01 0.01
end
Actor: ϵdecaysteps 5×106 5×105 107
Actor: ϵinEval 0 0 0
Actor: MLPLayers 128_64_64_32 256_256 64_32_32_16
Critic: LR 3×10−4 3×10−4 3×10−4
Critic: γ 0.99 0.99 0.99
Critic: MLPLayers 128_128 256_256 64_32
#updatesperepoch 20 50 20
ListLength 3 3 3
TypeofListEncoder DeepSet DeepSet DeepSet
ListEncoderLR 3×10−4 3×10−4 3×10−4
Table2: Environment/Policy-specificHyperparameters
H Q-VALUE LANDSCAPE VISUALIZATIONS
H.1 1-DIMENSIONALACTIONSPACEENVIRONMENTS
WeanalyzedtheQ-valuelandscapesinMujocoenvironmentstoshowhowsuccessivecriticshelp
actors find better actions by reducing local optima. Figure 23 illustrates a typically smooth and
easy-to-optimizeQ-valuelandscapeinunrestrictedInverted-Pendulum. Figure24illustratesthat
in restricted locomotion, the Q-value landscape (leftmost and rightmost figures) is uneven with
manylocaloptima. However,theQ-valuelandscapeslearnedbysuccessivesurrogatesΨˆ become
i
successivelysmootherbyremovinglocalpeaksbelowtheQ-valuesofpreviouslyselectedactions.
Thishelpsactorsfindclosertooptimalactionsthanwithasinglecritic.
Finally,whenweplottheactionsa ,a ,a selectedbythelearnedsuccessiveactorsontheoriginal
0 1 2
Q-landscape(rightmostfigure),weseetheyoftenachievehigherQ-valuesthana ,theactionasingle
0
actorhaslearned. Thus,themaximizeractorµ oftenfindsclosertooptimalactionsthanasingle
M
actor,resultinginbetterperformanceasshowninthereturncomparisonbetweenµ andsingleactor
M
(Figure11c)andtheperformanceagainstbaselines(Figure8).
28Preprint.
(a)Q (s,a ) (b)Q (s,a |a ) (c)Q (s,a |{a ,a }) (d)Q(s,a )∀i=0,1,2
0 0 1 1 0 2 2 0 1 i
Figure23: SuccessivesurrogatelandscapesandtheQ-landscapeofInvertedPendulum-v4.
Figure24: SuccessivesurrogatelandscapesandQlandscapeforRestrictedInverted-Pendulumand
RestrictedInverted-Double-Pendulumenvironments.
H.2 HIGH-DIMENSIONALACTIONSPACEENVIRONMENTS
Figure25visualizeQ-valuelandscapesforaTD3agentinHopper-v4.Weprojectactionsfromthe3D
actionspaceofHopper-v4ontoa2DplaneusingUniformManifoldApproximationandProjection
(UMAP)andsample10,000actionsevenlytoensurethoroughcoverage. TheQ-valuesareplotted
using trisurf, which may introduce some artificial roughness but offers more reliable visuals
thangrid-surfaceplotting. Despitelimitationsofdimensionalityreduction—suchasdistortionof
distances—theQ-landscapeforHopper-v4revealsalargegloballyoptimalregion(showninyellow),
offeringacleargradientpaththatpreventsthegradient-basedactorfromgettingstuckinlocaloptima.
Incontrast,Hopper-Restricted(Figure26)hasmorecomplexQ-landscapesduetovalidactionsbeing
restrictedinoneofthehyperspheresshowninFigure6. Consequently,theseQ-landscapesappearto
havemorelocallyoptimalregionsthanHopper-v4. Thiscreatesmanypeakswheregradient-based
actorsmightgettrapped,degradingtheresultantagentperformance.
The curse of dimensionality limits conclusive analyses on higher dimensional environments like
Walker2D-v4(6D)andAnt-v4(8D)becauseprojectingto2Dcausessignificantinformationloss,
makingitdifficulttoassessconvexityintheirQ-landscapes.
29Preprint.
Figure25: Hopper-v4: Qlandscapevisualizationatdifferentstatesshowapathtooptimum.
Figure26: Hopper-restricted: Qlandscapevisualizationatdifferentstatesshowseverallocaloptima.
30