Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent Graph Attention
Reinforcement Learning
Jinjin Shen, Yan Lin, Member, IEEE, Yijin Zhang, Senior Member, IEEE, Weibin Zhang, Member, IEEE,
Feng Shu, Member, IEEE and Jun Li, Senior Member, IEEE
Abstract—In order to avoid repeated task offloading and TABLEI
realize the reuse of popular task computing results, we con- RELATEDCONTRIBUTIONS
structanovelcontentcaching-assistedvehicularedgecomputing
(VEC) framework. In the face of irregular network topology NetworkTopology Content Graphical Cooperative
and unknown environmental dynamics, we further propose a Dynamic Irregular Popularity Information Learning
multi-agent graph attention reinforcement learning (MGARL) [10] X ✗ ✗ ✗ X
based edge caching scheme, which utilizes the graph attention [11] X ✗ X ✗ ✗
convolutionkerneltointegratetheneighboringnodes’featuresof [12] X ✗ X ✗ X
each agent and further enhance the cooperation among agents. [16] ✗ ✗ X X X
Oursimulationresultsshowthatourproposedschemeiscapable
Ours X X X X X
of improving the utilization of caching resources while reducing
thelong-termtaskcomputinglatencycomparedtothebaselines.
for nearby amenities [7]. As such, multiple VUs may have
Index Terms—Vehicular Edge Computing, Edge Caching, similar computing tasks associated with the same computing
Multi-Agent, Graph Attention Reinforcement Learning results to access and utilize the shared services. In this case,
instead of duplicate task re-uploading and re-computing, the
I. INTRODUCTION
previoustaskcomputingresultscanbecachedbyESs,e.g.the
Recently, mobile edge computing (MEC) has evolved as a surrounding Roadside Units (RSUs) or VUs, and be further
promising technology for ultra-reliable and ultra-low latency utilized by other VUs to reduce the latency of subsequent
(URLLC)applications,suchasvirtualrealityandautonomous tasks [8]. Therefore, designing an optimal content caching
driving[1].Insteadofsendingtaskstothefar-endcloud,MEC policy for edge computing is of great significance for the
sinks computing and caching resources to the edge nodes vehicular edge computing (VEC) networks.
close to the users [2]. In vehicular networks, vehicles have Considering the unknown dynamics of time-varying net-
limited computing capability but can offload requested tasks work topology and channel conditions, the edge computing
to edge servers (ESs) for execution[3]–[5], thus relieving the contentcachingproblemisessentiallyamodel-freesequential
computing pressure of vehicles [6]. decision-making problem [9]. By combining the advantages
Nevertheless, in practical scenarios where vehicular users of deep learning in identifying data features and reinforce-
(VUs) are driving during rush hour traffic or in congested ment learning in dynamic programming, deep reinforcement
urban areas, the data they offload exhibits high spatial, tem- learning (DRL) has been widely employed in solving such
poral, and social correlation, which results in different VUs problems [10]. For example, H. Tian et al. of [11] used deep
requesting the same services simultaneously, such as naviga- deterministic policy gradient (DDPG) approach to orchestrate
tion assistance, real-time traffic updates, or recommendations thejointoffloading,caching,andresourceallocationdecision-
makingforVEC,takingintoaccountthetime-varyingcontent
Copyright (c) 20xx IEEE. Personal use of this material is permitted.
popularity. However, it relies on the centralized learning
However, permission to use this material for any other purposes must be
obtained fromtheIEEEbysendingarequesttopubs-permissions@ieee.org. framework without cooperative learning. Additionally, X. Ye
ThisworkwassupportedinpartbytheNationalNaturalScienceFoundation etal.of [12] designeda blockchain-basedcollaborativecom-
of China under Grants 62001225, 62071236, 62471204, and U22A2002,
puting and caching framework in VEC using the multi-agent
in part by Hainan Province Science and Technology Special Fund under
Grant ZDYF2024GXJS292, in part by the Scientific Research Fund Project asynchronous advantage actor-critic (A3C) approach, which
of Hainan University under Grant KYQD(ZR)-21008, in part by the Col- has verified the efficiency of cooperative learning compared
laborative Innovation Center of Information Technology, Hainan University,
to the non-cooperativelearning.
underGrantXTCX2022XXC,inpartbytheKeyTechnologiesR&DProgram
of Jiangsu (Prospective and Key Technologies for Industry) under Grants However, the randomness of VU mobility leads to the
BE2023022 and BE2023022-2, in part by Natural Science Foundation of dynamics and temporal-spatial irregularities of the network
JiangsuProvince underGrantBK2021022532, andinpartbyMajorNatural
topology, which results in the environmental uncertainty and
ScienceFoundation oftheHigherEducationInstitutions ofJiangsuProvince
underGrant24KJA510003.(Correspondingauthor:YanLin) makes environmental knowledge more difficult to learn. But
J.Shen,Y.Lin,W.ZhangandY.ZhangarewiththeSchoolofElectronic the aforementionedcommonlyusedDRL solutionsrelyingon
andOpticalEngineering,NanjingUniversityofScienceandTechnology,Nan-
convolutional neural networks exhibit weakness in extracting
jing210094,China(e-mail:{jinjin.shen, yanlin,weibin.zhang}@njust.edu.cn;
yijin.zhang@gmail.com). F. Shu is with the School of Information and the temporal and spatial relationships between nodes in the
Communication Engineering, Hainan University, Haikou 570228, China irregular topology. To solve this issue, graph convolutional
(email: shufeng0101@163.com). J. Li is with the School of Information
neural networks have been employed in DRL, which has
ScienceandEngineering,SoutheastUniversity,Nanjing210096,China(email:
jleesr80@gmail.com). the benefits of acting directly on graphs and fully utilizing
4202
tcO
41
]AM.sc[
1v17001.0142:viXrathe structural information such as the relationships among
nodes [13]. Consequently, it has been used for solving op-
timization problems by jointly considering the information
of agents close to an agent [14], and further enhances the
cooperation of agents [15]. Although D. Wang et al. of [16]
haveexploredthedeepgraphreinforcementlearningapproach
forsolvingthejointcaching,communication,andresourceal-
location problem,they did notcarefullyconsider the dynamic
graphicaltopologycharacteristicsresultedfromhighvehicular
mobility. In this context, this paper aims to investigate the
content caching-assisted VEC problem, where the irregular
and dynamic network topology and the cooperative learning
among multiple vehicles are considered. To the best of our
knowledge, at the time of writing, this is the first attempt
in the related literature to study the content caching prob-
Communication
lem in VEC networks relying on multi-agent graph attention RSU VU Range of RSU
reinforcement learning (MGARL) framework. Compared to
Task Offloading and Result Content
the existing literature, MGARL method has the promising Result Content Fetching Fetching Directly
potential to capture the dynamics and irregularities of time- Y Result Content Fetching Directly
Making
varying vehicular networks, as well as make better use of the
Cached? Caching
neighboring nodes’ information to facilitate the cooperative Local or Task Offloading Decisions
N Computing
content caching decision-making.
Fig.1. Illustration ofthecontent caching-assisted VECsystem.
Our main contributionsare boldly and explicitly contrasted
to the literature in Table I and are detailed as follows:
N = L∪M = {1,2,··· ,N} represent the index set of N
• Acontentcaching-assistedVECframeworkisestablished SNs.
whereeachVUneedstodecidewhethertocachethetask Without loss of generality, we assume that the system
computing result, with the aim of reducing computing operates in a time-slot (TS) mode and environmental pa-
latency and reusing the popular content. rameters (e.g. transmit power and channel gain) remain un-
• The edge caching decision-making problem is con- changedduringeachTS.Additionally,newtaskswithdifferent
structed as a decentralized partially observable Markov popularity characteristics arrive every certain TSs, and each
Decision Process (DEC-POMDP), where each VU agent task can be completed successfully during each TS. Let
observes its local information and takes action individu- K = {1,2,··· ,K} represent the index set of tasks, where
ally based on its learned policy. task k is represented by a 3-tuple hd ,s ,b i with the task
k k k
• An MGARL-based edge caching scheme is proposed, size d , the number of CPU cycles required to process the
k
where graph attention convolution kernels are utilized to task s and the size of the processed task content b . Note
k k
capturerelationshipsamongagentsbytakingintoaccount that, if the content of the task result has been cached on SNs
the neighboringagents’ features when making their own withinthecommunicationrangeoftheVU,thecontentcanbe
edge caching decisions. fetched directly from SNs to VUs. Otherwise, the VU has to
• Simulation results show that the proposed scheme out- computethetaskitself oroffloaditforexecution.Inthiscase,
performs other baselines in terms of both improving after completing the task, each VU needs to decide whether
caching resource utilization and reducing long-term task to cache the content and to which SN.
computing latency.
B. Communication Model
Let P and gup (t) denote the transmit power of VU m
II. SYSTEMMODELAND PROBLEM FORMULATION m m,n
andthe channelgainspanningfromVU m andSN n in TSt,
A. Network Model respectively. In this framework, we consider the small-scale
AsshowninFig.1,weconsideraManhattanvehicularnet- fading and the path loss of vehicle-to-vehicle(V2V) commu-
workmodelwhichconsistsofmultiplehorizontalandvertical nication and vehicle-to-infrastructure (V2I) communication,
two-lanetwo-way roads.L RSUs are deployeduniformlyand neglecting the effects of shadow fading [4]. We assume that
M VUs drive along the road with their velocity obeying the the inter-user interference has been eliminated by allocating
Markov-Gaussian stochastic process independently and may orthogonalresourceblocks.Thus,theuplinktransmissionrate
changetheirdirectionatintersectionswith a givenprobability from VU m to SN n in TS t is given by
η. Let L = {1,2,··· ,L} and M = {1,2,··· ,M} denote
ζup (t)=Blog (1+P gup (t)/σ2), (1)
the index sets of RSUs and VUs, respectively. Both RSUs m,n 2 m m,n
and VUs have the capability to compute tasks and cache where B is the bandwidth of each channel, and σ2 is the
contents, which are referred to as service nodes (SNs). Let poweroftheadditiveGaussianwhitenoise,bothofwhichareassumed to be the same for each TS. Similarly, let P and be expressed by T (t) = (1 − e (t))(Tl (t) + Toff(t)) +
n m m m m
gdown(t) denote the transmit power of SN n and the channel e (t)Tdown(t). Mathematically, the optimization problem is
n,m m m
gain spanning from SN n and VU m in TS t, respectively. formulated as
T M
Thus the downlink transmission rate from SN n to VU m in min E[X XT m(t)] (3a)
TS t is given by {aca(t)}
t=1m=1
ζ ndo ,mwn(t)=Blog 2(1+P ng ndo ,w mn(t)/σ2). (2) s.t. ac ma(t)∈{0,1,··· ,N},∀m,∀t, (3b)
C. Computing Model
Xhc na ,k(t)b
k
≤g n(t),∀n, (3c)
k∈K
Let k (t) denote the task requested by VU m in TS t and
m where constraint (3c) indicates that the occupied caching
w (t) ∈ {0,1} denote whether VU m has to offload k (t).
m m capacityat SN n can notexceedits maximumcapacityg (t).
If w (t)=0, VU m computesk (t) itself, otherwise VU m n
m m
offloads k m(t) to the nearest SN n m(t). III. THEDESIGN OF DEC-POMDP
• LocalComputing:Letf ml bethecomputationcapability Considering the fact that the vehicular environment is dy-
on VU m. Therefore, the latency of computing task namically changing and each VU is unable to obtain global
k (t) generated by user VU m is given by Tl (t) = environmental knowledge, we further formulate the above
m m
(1−w (t))s /fl . edge caching decision-making problem as a DEC-POMDP.
m km(t) m
• Task Offloading: Let f nof mf (t) be the computation capa- Explicitly, each VU agent gets the local observation and
bility onSN n (t). Then,the latencyin completingtask takes action individually.By interacting with the environment
m
k (t)isexpressedasToff(t)=w (t)(Tup(t)+Texe(t)+ iteratively, the agents can learn their strategies to maximize
m m m m m
Tdown(t)),whereTup(t)=d /ζup (t),Texe(t)= the system reward. Next, we will elaborate on the definitions
m m km(t) m,nm(t) m
s /foff and Tdown(t)=b /ζdown (t) denote of DEC-POMDP.
km(t) nm(t) m km(t) nm(t),m
the task offloading latency, task computing latency, and A. Observation
result content fetching latency, respectively. Due to limited sensing and positioning technology, we
assume that each VU agent can only observe its location, its
D. Caching Model
caching state, and the current remaining caching capacity of
Let H (t) = [hca (t)] represent whether computing
ca n,k ∀n,∀k all SNs. Accordingly, the observation of VU agent m can be
result contents are cached or not by all SNs in TS t. Specif-
defined as
ically, hca (t) = 1 if the content of task k is cached on
n,k ooo (t)=[x (t),y (t),hca (t),hca (t),··· ,
SN n in TS t, otherwise hc na ,k(t) = 0. Note that all SNs m m m m,1 m,2 (4)
have limited caching capacity with g n(t) for SN n, thus it hc ma ,K(t),g 1(t),g 2(t),··· ,g N(t)],
isimpossibletocacheallthetask contents.Letuscontinueto wherex m(t)andy m(t)are thecurrenthorizontalandvertical
denoteaca(t)∈{0,1,··· ,N}asthecachingdecisionvariable coordinates of VU agent m, respectively.
m
forVUminTSt.Tobespecific,ifaca(t)=n(n6=0),VUm B. Action
m
cachesthecomputingresultcontentto SN n andhca (t)=1,
n,k BasedonthelearnedpolicyanditsobservationinTSt,VU
otherwiseVU m doesnotcache the contentto anySN. Then, agent m selects an action aca(t) to decide whether to cache
m
the caching decision of all VUs in TS t can be denoted as
the content and to which SN.
a (t)=[aca(t),aca(t),··· ,aca(t)].
ca 1 2 M
When SNs cache the content of Z task computing results C. Reward Function
t
in TS t, we have Z = {1,2,··· ,Z t}. We assume that the In order to minimize the long-termtask computinglatency,
task content popularity follows the Zipf distribution, thus the the local reward of VU agent m can be designed as
probabilityoftask contentz ∈Z tobe requestedis expressed
as q (t) = I (t)−δ/ Z I (t)−δ , where I (t) denotes the r m(t)=e m(t)re m−T m(t)+pl(t), (5)
z z Pz=0 z z
rankingofthepopularityoftaskcontentz indescendingorder
where re is the reward for encouraging caching computing
m
in TS t, and δ is the Zipf distribution parameter. results. Moreover, pl(t) is negative if the caching decision
E. Problem Formulation made by VU agentm leadsto the excess of cachingcapacity,
otherwise pl(t) = 0. Accordingly, the system reward, which
Our aim is to design an edge caching scheme for the VEC
is defined as the sum of all local rewards, is given by r(t)=
system to minimize the long-term task computing latency by
M r (t).
utilizing limited edge caching resources. To formulate our Pm=1 m
problem, we introduce an auxiliary variable e m(t), where IV. MGARL-BASED SCHEME
e (t)=1whenmcanfetchtherequiredcontentdirectlyfrom To adapt to dynamically changing irregular topology and
m
SNs within its communication range, otherwise e (t) = 0. capture relationships among agents, we resort to MGARL to
m
Thus, the total computation latency is the local computing further utilize the cooperation of multiple agents for solving
latency or the task offloadinglatency when e (t)=0, and is the edge caching decision-making problem. As illustrated in
m
thelatencyforfetchingthecachedtaskresultwhene (t)=1. Fig.2, MGARL consists of three modules: graph modeling,
m
Then, the task computing latency for VU m in TS t can latent feature generating and parameter updating.Graph Modeling
Parameter Updating
ot ht h't h''t DRL
a (t)
ca
Q value
Latent Feature Generating
Adjacency MatrixCt i (cid:167) (cid:168)W Q1 W K1 W V1(cid:183) (cid:184)
(cid:170) (cid:171) (cid:171)10 1 0 00 00(cid:186) (cid:187) (cid:187) (cid:117) h it h it (cid:168) (cid:168) (cid:169)W(cid:35) QM W(cid:35) KM W(cid:35) VM(cid:184) (cid:184) (cid:185) 1|(cid:266)|M h't
(o it,a it,r it,o it+1,C it,C it+1) (cid:171)(cid:172)0 0 0 1(cid:187)(cid:188) Multi-Head Attention i
Feature MatrixFt
Replay Buffer
Fig.2. Illustration oftheproposedMGARL-basedcontent caching-assisted VECscheme.
A. Graph Modeling Algorithm 1 Training Process for MGARL-Based Content
The vehicularnetworktopologycan be modeledas a graph Caching-Assisted VEC Scheme
G = {V,E} where the node set V is the set of M VUs and 1: for each agent i∈V do
the edge set E is determined by the distance among vehicles. 2: Initialize the Q network parameter θ i of agent i;
To be specific, there exists an edge between two nodes when 3: end for
their distance does not exceed the maximum communication 4: for each episode q ∈{1,2,··· ,Q} do
range and such vehicles are neighbors of each other. Let At i 5: Reset simulation parameters;
denote the set of neighbors of node i in TS t. Each node in 6: for each TS t∈{1,2,··· ,T} do
thegraphhasitsnodefeatures,denotedbyht i fornodeiinTS 7: for each agent i∈V do
t, which are yielded from the observation ot i through a fully 8: VU i completes one task;
connected network. 9: Get local observation ot and construct the adja-
i
cency matrix Ct;
B. Latent Feature Generating i
10: Select at based on the learned policy;
Thelatentfeaturegeneratingstageutilizestheconvolutional i
11: Cache the content and calculate rt;
layer to integrate the node features in node i’s local region, i
12: Get ot+1 and Ct+1;
which includes i and At to generate the latent feature h′t in i i
TS t. Firstly, in TS t, ai ll nodes’ feature vectors are meri ged 13: Store the tuple (cid:10)ot i,at i,r it,ot i+1,Ct i,Ct i+1 (cid:11) in
shared replay buffer;
into a feature matrix Ft with size M × D in the order of
14: Encodeottohtthroughafullyconnectednetwork;
index where D is the length of the feature vector. Then, let i i
At represent the set of node i and At and we introduce
an+ ai djacency matrix Ct with size |At |i ×M to denote the 15: Integrate the features of At +i according to Ct i to
i +i generate h′t by adopting multi-head attention as
one-hot representations of At . To be specific, the first row i
+i the convolution kernel;
of Ct is the one-hot representation of node i, and the j ∈
i 16: Sample a random minibatch of S tuples from the
{2,3,··· ,|At |}th row is the representation of the (j−1)th
+i shared replay buffer;
neighborof i. Then, the features in the local region of node i
are obtained by Ct×Ft. 17: Update θ i to minimize the loss function;
i 18: end for
Tofurthercapturetherelationshipsamongnodes,themulti-
19: end for
headattentionisadoptedastheconvolutionkerneltointegrate
20: end for
the feature vectors in the local region of node i and generate
thelatentfeaturevectorh′t.LetWu,Wu andWu denotethe
i Q K V
parametermatricescorrespondingto the query,key,andvalue
ash′t =σ(con[ au Wuht,∀u]),wherecondenotes
representation in attention head u, respectively. For attention i Pj∈At +j i,j,t V j
the concatenation of the outputs of U attention heads. Note
head u, the relationship between node i and its neighbor j ∈
that, more graphical information can be extracted through
At in TS t can be formulated as
+i increasing the number of convolutionallayers.
exp(τ ·Wuht·(Wuht)T)
αu = Q i K j , (6) C. Parameter Updating
i,j,t exp(τ ·Wuht·(Wuht)T)
Pk∈At +i Q i K k Similar to the traditionaldeep Q network (DQN) algorithm
where τ is a scaling factor. Next, the outputs of U attention using Q values to estimate the long-term cumulative reward
heads for node i are concatenated and then fed into function of taking a certain action at the current state, the MGARL
σ to produce the output of the convolutionallayer, expressed algorithm utilizes both the training network and the target
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.network, where the training network parameters are updated
byoptimizingthelossfunctionofthetargetnetwork.Through
extractinggraphicalinformationtofacilitatecooperativelearn-
ing, the MGARL method can better capture the dynamics
and irregularities than the traditional DRL methods. In order
to utilize historical data for training and improve sample
utilization, the tuple ot,at,rt,ot+1,Ct,Ct+1 of vehicular
(cid:10) i i i i i i (cid:11)
agenti is storedin the sharedreplaybufferin eachTS. When
training, S sets of samples are randomly selected to update
theparametersofthetrainingnetwork.Particularly,inorderto
achievestableinteractionsbetweenagents,MGARLoptimizes
theattentionweightdistributionofthelastconvolutionallayer
to minimize the Kullback-Leibler divergence of the attention
weight distribution in the current TS over the weights in the
Fig.3. Comparisonoftheconvergence.
next TS. Then, the loss function of the target network is
expressed as respectively. The caching capacity of RSUs and VUs are
M 100MB and50MB, respectively.Thecomputationcapability
1 1
L(θ)=
S
X
M
X(y it−Q i(O i,Ct i,at i;θ))2+ of RSUs and VUs are 1 GHz and 0.8 GHz, respectively.
S i=1 (7) The communication range of RSUs and V2V communication
1 U are set as 400 m and 300 m, respectively. We adopt the
λ
U
XD KL(G uk ,t(O i,Ct i;θ)k(G uk ,t+1(O i,Ct i+1;θ)), channel model according to [4]. In terms of the VU mobility
u=1 model, we set η = 0.4 and the mean range of VUs’ velocity
where y it = r it+γmax a′Q′ i(O i,Ct+1,a′ i;θ′))2 is the Q value
as [36,54] km/h, and other parameters setting please refer
generated by the target network wi ith parameter θ′. γ is the
to [17]. With regard to the learning configurations, γ = 0.9,
discountfactor.O i,Ct denotesthesetofobservationsofnodes S = 128, the learning rate is 0.001 and the buffer capacity
i
in agent i’s local region determined by adjacency matrix
is 2000. For fair comparison, we choose the proposed w/o
Ct i. Q function parameterized by θ takes O i,Ct as input and attentionbasedscheme,multi-agentIndependentDoubleDeep
i
outputs Q value for agent i. λ is the coefficient for the loss
Q Network (IDDQN) based scheme and the random content
andG uk ,t(O i,Ct;θ)denotestheattentionweightdistributionof
caching scheme.
i
relation representations of attention head u in convolutional
layer k for agent i. The details of the training process for the B. Performance Evaluation
proposed MGARL-based algorithm are listed in Algorithm 1. Fig.3presentstheconvergenceperformanceofallschemes.
It can be observed that the cumulative reward of the pro-
D. Complexity Analysis
posed scheme is higher than the baselines. This is attributed
Let us now discuss the complexity of the graph attention to the fact that the proposed scheme, in conjunction with
convolutional layer in TS t including the feature mapping of the proposed w/o attention and IDDQN-based scheme, can
nodes and the calculation of attention weights. We assume learnpolicybycontinuouslyinteractingwiththeenvironment.
that the constructed graph in TS t consists of |E t| edges and Moreover, the proposed scheme utilizes graph attention con-
each node feature is mapped from dimension F to space volution kernels to capture the relationships among agents
in dimension F′. Then, the computational complexity for to further enhance cooperation, which is more adaptable to
mapping node features is represented as O(MFF′) while irregularnetworktopologiesand consequentlyhas the highest
the computationalcomplexity in calculating attention weights cumulative reward.
is related to the number of edges, which is expressed as The converged content hit ratio versus the number of VUs
O(|E t|F′).Therefore,thetotalcomputationalcomplexitywith is investigated in Fig. 4 when the number of content types
U attention heads is determined by the number of VUs, the (CTs) is 10 and 20. The first point to observe is that the
number of edges, and the node feature dimension, given by converged content hit ratio increases as the number of VUs
O(U(MFF′+|E t|F′)).
increases. This is because more users lead to the increasing
number of content requests, thus increasing the probability
V. SIMULATION RESULTS
of reusing the same cached content. Secondly, it is clear that
A. Simulation Settings
the content hit ratio is reduced when the number of CTs is
We consider a Manhattan vehicular network model, where increased from 10 to 20. The underlying reason is that due
thelengthandwidthoftheroadareboth1km.Bydefault,we to the limited caching capacity of SNs, they may not cache
setM =20,L=16,d ∈[50,80]MB,b ∈[6,16]MB,s = all the contents,which decreasesthe hit ratio of each content.
k k k
50 Megacycles (∀k), B = 10 MHz, σ2 = −174 dBm/Hz, Moreover, we can see that our proposed scheme outperforms
δ = 0.5, re = 2 (∀m), and pl ∈ {0,−0.5}. The transmit other schemes in improvingthe content hit ratio regardlessof
m
power of RSUs and VUs are set to 40 dBm and 23 dBm, boththenumberofCTsandthenumberofVUs,whichverifiesandtheenvironmentaluncertainty,wedevelopedanMGARL-
 3 U R S R V H G   & 7 V       3 U R S R V H G   & 7 V     
based edge caching scheme for VEC networks by utilizing
 3 U R S R V H G  Z  R  $ W W H Q W L R Q   & 7 V       3 U R S R V H G  Z  R  $ W W H Q W L R Q   & 7 V     
 , ' ' 4 1   & 7 V       , ' ' 4 1   & 7 V      thecooperationamongagentswiththeintegrationinformation
 5 D Q G R P   & 7 V       5 D Q G R P   & 7 V      of neighboring nodes in decision-making. Compared to the
baselines, the proposed scheme can better learn the irregu-
    lar topology dynamics, thus significantly reducing the task
computing latency and improving the utilization of densely
    deployed caching resources.
REFERENCES
   
[1] K.Jiang,H.Zhou,X.Chen,andH.Zhang,“Mobileedgecomputingfor
    ultra-reliable and low-latency communications,” IEEE Commun. Mag.,
vol.5,no.2,pp.68–75,2021.
[2] Z. Yao, S. Xia, Y. Li, and G. Wu, “Cooperative task offloading and
    servicecachingfordigitaltwinedgenetworks:Agraphattentionmulti-
agent reinforcement learning approach,” IEEE J. Sel. Areas Commun.,
vol.41,no.11,pp.3401–3413, 2023.
   
[3] H.Zhou,K.Jiang,S.He,G.Min,andJ.Wu,“Distributeddeepmulti-
              
 1 X P E H U  R I  9 8 V agentreinforcementlearningforcooperativeedgecachinginInternetof
Vehicles,” IEEE Trans. Wireless Commun., vol. 22, no. 12, pp. 9595–
Fig.4. Theconverged content hitratioversusthenumberofVUs. 9609,2023.
[4] Y.Lin,Y.Zhang,J.Li,F.Shu,andC.Li,“Popularity-awareonlinetask
offloadingforheterogeneousvehicularedgecomputingusingcontextual
 3 U R S R V H G  5 D Q G R P clusteringofbandits,”IEEEInternetofThingsJ.,vol.9,no.7,pp.5422–
 3 U R S R V H G  Z  R  $ W W H Q W L R Q  ( G J H  & R P S X W L Q J  Z  R  & D F K L Q J 5433,2022.
 , ' ' 4 1 [5] Y.Lin,J.Bao,Y.Zhang,J.Li,F.Shu,andL.Hanzo,“Privacy-preserving
jointedgeassociationandpoweroptimizationfortheInternetofVehicles
via federated multi-agent reinforcement learning,” IEEE Trans. Veh.
   
Technol.,vol.72,no.6,pp.8256–8261,2023.
[6] L.Liu,X.Yuan,N.Zhang,D.Chen,K.Yu,andA.Taherkordi, “Joint
    computationoffloadinganddatacachinginmulti-accessedgecomputing
enabledInternetofVehicles,”IEEETrans.Veh.Technol.,vol.72,no.11,
pp.14939–14954, 2023.
    [7] M. W. Al Azad and S. Mastorakis, “The promise and challenges
of computation deduplication and reuse at the network edge,” IEEE
    Wireless Commun.,vol.29,no.6,pp.112–118, 2022.
[8] F. Zeng, K. Zhang, L. Wu, and J. Wu, “Efficient caching in vehicular
edge computing based onedge-cloud collaboration,” IEEETrans. Veh.
              Technol.,vol.72,no.2,pp.2468–2481,2023.
 1 X P E H U  R I  5 6 8 V [9] G. Qiao, S. Leng, S. Maharjan, Y. Zhang, and N. Ansari, “Deep
reinforcementlearningforcooperativecontentcachinginvehicularedge
computing and networks,” IEEE Internet of Things J., vol. 7, no. 1,
Fig.5. Theconverged total systemlatency versusthenumberofRSUs.
pp.247–257, 2020.
theeffectivenessoftheproposedschemeinimprovingcaching [10] L. Geng, H. Zhao, J. Wang, A. Kaushik, S. Yuan, and W. Feng,
“Deep-reinforcement-learning-based distributed computation offloading
resource utilization.
in vehicular edge computing networks,” IEEE Internet of Things J.,
Fig. 5 compares the converged total system latency versus vol.10,no.14,pp.12416–12433, 2023.
thenumberofRSUs.Firstly,asthenumberofRSUsincreases, [11] H.Tian,X.Xu,L.Qi,X.Zhang,W.Dou,S.Yu,andQ.Ni,“Copace:
Edge computation offloading and caching for self-driving with deep
theconvergedtotalsystemlatencyexhibitsa downwardtrend.
reinforcement learning,” IEEE Trans. Veh. Technol., vol. 70, no. 12,
The underlying reason for this phenomenon is that with pp.13281–13293, 2021.
more RSUs having computation and caching capability, VUs [12] X.Ye,M.Li,P.Si,R.Yang,Z.Wang,andY.Zhang,“Collaborativeand
intelligentresourceoptimizationforcomputingandcachinginIoVwith
are more likely to fetch the cached result content from the
blockchainandMECusingA3Capproach,”IEEETrans.Veh.Technol.,
surrounding RSUs, thus avoiding the repeated uploading and vol.72,no.2,pp.1449–1463, 2023.
computingprocess.Secondly,it canbeseen thatthe proposed [13] R. Akmam Dziyauddin, D. Niyato, N. Cong Luong, M. A. M. Izhar,
M.Hadhari,andS.Daud,“Computationoffloadingandcontentcaching
scheme outperforms other schemes under different numbers
delivery in vehicular edge computing: A survey,” arXiv e-prints,
of RSUs, since multiple graph attention convolutional layers pp.arXiv–1912, 2019.
can extract more hidden structural information to facilitate [14] J.Jiang,C.Dun,T.Huang,andZ.Lu,“Graphconvolutional reinforce-
mentlearning,” arXivpreprintarXiv:1810.09202, 2018.
cooperative learning. This phenomenon implies that the pro-
[15] Z.Yao,Y.Li,S.Xia,andG.Wu,“Attentioncooperativetaskoffloading
posed scheme can make better use of densely deployedcache andservice caching in edgecomputing,” inGLOBECOM2022-2022
resources to further reduce the total system latency. IEEEGlobalCommunications Conference, pp.5189–5194, 2022.
[16] D.Wang,Y.Bai,G.Huang,B.Song,andF.R.Yu,“Cache-aidedMEC
forIoT:Resourceallocation usingdeepgraphreinforcement learning,”
VI. CONCLUSION
IEEEInternet ofThingsJ.,vol.10,no.13,pp.11486–11496, 2023.
[17] Y.Lin,Z.Zhang,Y.Huang,J.Li,F.Shu,andL.Hanzo,“Heterogeneous
In this paper, we proposed a content caching-assisted VEC
user-centricclustermigrationimprovestheconnectivity-handover trade-
framework by taking into account the reuse of popular task offinvehicular networks,” IEEETrans. Veh.Technol., vol. 69,no. 12,
computingresults. To adapt to the irregular network topology pp.16027–16043, 2020.
 R L W D 5  W L +  W Q H W Q R &  G H J U H Y Q R &
  V   \ F Q H W D /  P H W V \ 6  O D W R 7  G H J U H Y Q R &