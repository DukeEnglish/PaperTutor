1
Improve Value Estimation of Q Function and
Reshape Reward with Monte Carlo Tree Search
Jiamian Li
Abstract—Reinforcement learning has achieved remarkable feedback may be delayed for extended periods. This increases
success in perfect information games such as Go and Atari, the computational demand for sampling and renders much of
enabling agents to compete at the highest levels against hu-
thesampleddataineffective.Inpreviousresearchonimperfect
man players. However, research in reinforcement learning for
informationgameslikeStarCraft[22][21],Doudizhu[27][28]
imperfect information games has been relatively limited due to
the more complex game structures and randomness. Traditional [29]andTexasHoldem[5][1],DeepMonteCarlo(DMC)[18],
methods face challenges in training and improving performance Q-learning[24][20]andNeuralFictitiousSelfPlay(NFSP)[5]
in imperfect information games due to issues like inaccurate Q havebeenthedominantandcommonlyusedmethods,yielding
value estimation and reward sparsity. In this paper, we focus on
some results; however, they suffer from the issues such as
Uno, an imperfect information game, and aim to address these
overestimation of q values [20], inefficient for incomplete
problems by reducing Q value overestimation and reshaping
reward function. We propose a novel algorithm that utilizes episodes and large variance of expected returns(rewards) [3],
Monte Carlo Tree Search to improve the value estimation in and often require an extensive number of training episodes to
Q function. Even though we choose Double Deep Q Learning achieve satisfactory performance. These challenges highlight
as the foundational framework in this paper, our method can
the necessity for new methods to improve convergence speed,
be generalized and used in any algorithm which needs Q value
reduce training steps and increase sample efficiency. estimation, such as the Actor-Critic. Additionally, we employ
Monte Carlo Tree Search to reshape the reward structure in We choose an imperfect information game, Uno, as the
thegameenvironment.Wecomparedouralgorithmwithseveral training environment for our reinforcement learning agent in
traditional methods applied to games such as Double Deep Q this paper. Uno [19] is a popular game played worldwide,
Learning, Deep Monte Carlo and Neural Fictitious Self Play,
suitable for 2 to 10 players. The game consists of multiple
andtheexperimentsdemonstratethatouralgorithmconsistently
card types in four colors: red, yellow, blue, and green. The
outperforms these approaches, especially as the number of
players in Uno increases, indicating a higher level of difficulty. numbered cards range from 0 to 9, and there are special cards
such as Skip, Reverse, Draw 2, Wild, and Wild Draw 4. Each
Index Terms—Artificial Intelligence, Games, Monte Carlo,
Reinforcement Learning, User Interface color contains two of each number card from 1 to 9, and one
cardwiththenumber0.Additionally,eachcolorhastwoSkip,
Reverse, and Draw 2 cards. There are four Wild cards and
I. INTRODUCTION
four Wild Draw 4 cards, making a total of 108 cards. Table I
IN previous research, there has been considerable explo- providesdetailsaboutthefunctioncards,whileexamplecards
ration into the application of reinforcement learning in are shown in Figure I. The rules of Uno are as follows:
perfect information games, such as Go [14] [11], Chess [12]
1) Initialize: Shuffle the Uno deck, deal 7 cards to each
[13] and Atari [8] [20]. These studies have demonstrated
player, and place the remaining cards in the center as
significant success, with reinforcement learning agents con-
the draw pile.
sistently outperforming top human players through the deep
2) Start: Flip over the top card of the draw pile to form
neural networks to derive optimal policies. However, research
the target card. Players must match either the color or
in the domain of imperfect information games has been
number of this target card, or play a special card that
relatively scarce, and significant breakthroughs in this area
fits these rules, such as Wild or Wild Draw 4.
remain limited. Several factors contribute to this difference.
3) Play: If a player has a valid card, they must play it,
Imperfect information games are defined by the fact that
setting it as the new target card, and the old target card
players cannot access all the information within a given state,
is discarded. If they cannot play a valid card, they must
leading to varying levels of knowledge among participants,
draw a card from the draw pile. If the draw pile runs
unlike in perfect information games. For instance, in most
out, shuffle the discard pile to create a new draw pile.
card games, players can only view their own hands, while
4) Win: The first player to get rid of all their cards wins
crucial details, such as the cards held by opponents and those
the game. If a player is left with only one card in hand,
remaining in the deck, remain concealed. Additionally, these
they must call out ”Uno.” Failure to do so results in a
games are inherently non-deterministic and often include a
penalty, and the player must draw 2 additional cards.
high degree of randomness and luck, which presents signifi-
cant challenges for reinforcement learning algorithms during Uno, like other imperfect information games, suffers from
training, potentially leading to convergence issues or high the issue of reward sparsity, but to a more severe degree.
varianceinresults.Reinforcementlearningagentsinimperfect According to the rules, only the player who plays all their
information games also encounter the challenge of sparse cards first can win, which leads to situations where the deck
rewards,whereobtainingrewardscanbedifficult,andpositive isexhausted,requiringareshuffle.Thisresultsinmorerounds
4202
tcO
51
]GL.sc[
1v24611.0142:viXra2
TABLEI
FUNCTIONALDESCRIPTIONOFSPECIALCARDS
TypeofCard CardFunction
Skip Skipthenextplayer’sturn,allowingtheplayerafterthemtotaketheirturninstead.
Reverse Reversethedirectionofplay;ifitwasinitiallyclockwise,switchittocounterclockwise,andifitwascounterclockwise,changeittoclockwise.
Draw2 Makethenextplayerdrawtwocards.
Wild Canbeplayedasanycolor.
WildDraw4 Canbeplayedasanycolorandmakethenextplayerdraw4cards.
of inaccurate estimation of Q values. To achieve this, we
propose a new algorithm that improves value estimation of Q
functionsusingMonteCarloTreeSearch(MCTS)[2]tosolve
Uno. Additionally, we utilize MCTS to reshape the reward
structure within the Uno environment. We choose Double
Deep Q-Learning (DDQN) as the base algorithm for training
the agent and extend it to DDQN with MCTS; however, our
method can be applied to any algorithm that requires Q value
estimation. Our algorithm is trained for the same number
of steps as DMC, NFSP, and DDQN in environments with
three and four players. The evaluation results indicate that
DDQN with MCTS achieves a higher win rate than the other
algorithmsby4%-16%withthesametrainingepisodes.More
importantly, the integration of MCTS into DDQN exhibits a
marked acceleration in performance improvement during the
early and mid-training phases, whereas other algorithms may
either show no significant progress or improve at a much
slower rate.
II. RELATEDWORK
A. Partially Observable Markov Decision Process
Any traditional reinforcement learning problems can be
modelled as a mathematical framework called Markov De-
cision Process (MDP) [18], shown in Figure 3. In aca-
demic formalism, the MDP can be described as a five-tuple
(S,A,T,R,γ), where:
• S is the set of all possible states in the environment.
Fig. 1. The different kind of Uno cards including the Number card, Skip, • A is the set of all possible actions the agent can take in
Reverse,Draw2,WildandWildDraw4
the environment.
• T(s′|s,a) is the state transition probability function,
in Uno compared to other games(rounds shown in 2). which is the probability of transitioning to the next state
s′ when agents take action a in state s.
•
R(s,a,s′)istherewardfunction,whichistheimmediate
reward received when transitioning from state s to next
state s′ by taking action a.
• γ is the discount factor, which determines how future
rewards are valued compared to immediate rewards, typ-
ically ranging from 0 to 1.
MDP assumes that all information about a state in the
environment is accessible to agents. However, imperfect in-
formation games do not meet this condition, so they are
modeledasabroaderframework:PartiallyObservableMarkov
Fig. 2. The round distribution in the Uno game with 3 players. We record Decision Process (POMDP) [16]. A Partially Observable
rounds of one hundred thousand games in total. Most games typically end Markov Decision Process can be described as a seven-tuple
within around 40 - 80 rounds, but there are some games that can extend
(S,A,T,R,Ω,O,γ), where (S,A,T,R,γ) are the same as in
beyond200rounds.
MDP, while the additional two elements can be defined as:
In this paper, we aim to address the issues of reward • Ωisthesetofallpossibleobservationsthattheagentcan
sparsity and convergence challenges associated with problems perceive.3
wheresandaaretheoriginalstateandactiontaken,s′is
the next state after taking the action, r is the immediate
reward received, α is the learning rate, and γ is the
discount factor, which determines the decay rate of the
reward.
This iterative process allows the agent to learn an optimal
strategy over time by refining its estimate of the Q-values for
eachstate-actionpair,ultimatelyconvergingonthepolicythat
maximizes cumulative rewards.
Q-learning is well-suited for problems with small size of
Fig. 3. The simplified Markov Decision Process (MDP) consists of four
states, S0−S3, which represent the possible states. The actions, a1−a3, state and action spaces. However, in large-scale spaces, the
denotethedecisionstheagentmakestotransitionfromonestatetoanother. traditional Q-learning method becomes impractical, as the
Thetransitionfunction,p,representstheprobability(greaterthanorequalto
Q-table can grow excessively large, making it inefficient to
0) of the agent taking a particular action at a state. The reward function, r,
whichcanbeeitherpositiveornegative,definestherewardtheagentreceives learn and store. To address this, Q-learning was replaced by
upontransitioningtoanewstate. Deep Q-Network(DQN) [9], which employs neural networks
as Q value approximators to overcome the challenges of large
state spaces. In addition, DQN introduced the technique of
• O(ω|s′,a) is the observation probability function, which experience replay, where the agent’s experiences are stored
specifies the probability of observing ω when transition-
in memory and used for batch updates rather than updating
ing to state s′ after taking action a.
immediately after each transition. This approach increases
The agent, based on the current state s and observations Ω, sample efficiency by allowing the agent to learn from past
selects an action a, transitions to a new state s′ according to experiences multiple times, improving the use of available
the state transition probability P, receives a new observation data. Although DQN has become a benchmark algorithm in
basedonO(ω|s′,a),andobtainsarewardR(s,a,s′).Tosolve
reinforcement learning, it has some drawbacks, one of the
POMDP, agents need to find an optimal policy π∗ that maxi- main issues being its tendency to overestimate Q-values. To
mizes the expected cumulative reward over time. The optimal address this, Double Deep Q-Network (DDQN) [20] was
policy π∗ can be either deterministic or stochastic, selecting introduced. DDQN tackles the overestimation by using two
the best action given the current state and observations. separate neural networks: one network (estimator network) is
usedtopredictthecurrentQ-valuesforactionselection,while
the other network (target network), which is a past version
B. Q-learning and Deep Q-network
of the estimator network, is used to calculate the target Q-
Q-learning[18]isoneofthefoundationalandmostwidely-
values for updating. By separating these two processes and
used algorithms in reinforcement learning, playing a critical
using a target network that lags behind the estimator, DDQN
role as both a core component and an essential building block
reduces the bias in estimating the target values, leading to
for more advanced algorithms across various domains. One
more accurate and stable learning. It tries to minimize the
of the key characteristics of Q-learning is its reliance on
loss between target value and estimated Q value to train the
the Bellman optimal equation for updates. With sufficient
agent:
training data, Q-learning can guarantee that the agent con-
verges to the optimal policy. As a model-free, bootstrapping
algorithm, Q-learning learns to estimate the optimal action L =(Q (s,a;θ′)−Q (s,a;θ))2 (1)
ddqn target estimator
to take in any given state by calculating and updating Q-
where
values—representingtheexpectedutilityoftakingaparticular
action from a specific state. Q-learning operates on a Q-table Q (s,a;θ′)=r+γQ (cid:16) s′,argmaxQ (s′,a′;θ);θ′(cid:17)
target target estimator
where each state-action pair is associated with a Q-value. The a′
(2)
algorithm updates its Q-values through the following steps in
each iteration: Q and Q are the target network and the estimator
target estimator
1) Select andexecute an action: Select anaction according network, respectively.
to the current policy, such as the ϵ-greedy strategy,
which typically selects the best current action (in most
C. Deep Monte Carlo and Monte Carlo Tree Search
cases)butwithaverysmallprobabilityrandomlyselects
another action to explore unknown state-actions. Deep Monte Carlo (DMC) is similar to DQN; however,
2) Observe the outcome and reward: After executing the while DQN is based on bootstrapping, DMC relies on com-
action, agents observe the new state and the reward plete trajectories. In DQN, updates are made based on esti-
received. mated future returns, whereas DMC calculates the true return
3) Update the Q-value: Update the Q-table using the fol- from the entire episode, based on Monte Carlo sampling.
lowing update rule: Although the DMC method is known for its high variance
[18], it can be effective in certain episodic games, such as
(cid:16) (cid:17)
Q(s,a)←Q(s,a)+α r+γmaxQ(s′,a′)−Q(s,a) the card game DouDiZhu [27]. DMC agents usually selects
a′4
a random policy π at the start and optimizes π through the by utilizing two parallel learning processes: one focuses
following steps: on learning the average policy across all agents, while the
1) Generate an episode using π and store the tuple other estimates the value of the policy, typically implemented
(s ,a ,R(s )) for each step. through a Q-learning-based approach. NFSP is composed of
t t t
2) Initialize the return R(s) of each state s at time t to two core networks: a value network, the Deep Q-Network
0. The average return for each s is calculated using the (Q(s,a|θ Q)), which estimates action values, and a policy net-
formula R(s t) = R(s t) + λR(s t+1), where λ is the work Π(s,a|θ Π), responsible for mimicking the agent’s past
reward discount factor. bestresponses.Bothnetworksaresupportedbycorresponding
3) Minimize the mean squared error between R(s ) and memory buffers: Memory for Reinforcement Learning(MRL)
t
Q(s ,a ), where Q(s ,a ) is predicted by the deep stores experiences for Q-learning, while Memory for Super-
t t t t
neural network. Repeat steps from 1 - 3 and finally, the vised Learning(MSL) stores the best-response behaviors. The
optimal policy is generated by updating: For each s in agent’s learning process is a combination of data drawn from
the episode, π(s)←argmax Q(s,a). both networks. Experiences in MRL are used to train the
a
valuenetworkbyminimizingthemeansquarederrorbetween
Monte Carlo Tree Search (MCTS) and DMC are both based
predicted values and the stored experiences in the replay
onMonteCarlosampling,buttheyarefundamentallydifferent
buffer. At the same time, data from MSL is used to train
methods. MCTS builds a search tree by iterating over sim-
the policy network by minimizing the negative log-likelihood
ulations of possible future moves and backpropagating the
between the stored actions and those predicted by the policy
results, which makes it a key component of the renowned
network. NFSP has proven to be highly effective, particularly
Go agent AlphaGo’s [11] algorithm. The MCTS process in
in complex imperfect-information games like Leduc Hold’em
reinforcement learning includes four principal stages:
andLimitHold’em.Atthetimeofitsdevelopment,itdemon-
1) Selection:Startingfromtherootnode,repeatedlyselect
strated near-superhuman performance in these environments,
child nodes based on a combination of predicted state
outperforming state-of-the-art algorithms and setting a new
values from the neural network and the Upper Confi-
benchmark in imperfect information game strategies.
dence Bound for Trees (UCT) formula.
2) Expansion: If the selected node represents a non-
III. METHODOLOGY
terminal state and has not been fully expanded, add one
ofitsunvisitedchildnodestothetreeandusetheneural A. Uno Environment and Representations in Reinforcement
network to predict the value of this new state. Learning
3) Simulation: From the newly expanded node, simulate
We use RLCard [26], which is the Python game toolkit, as
the game by following a policy until a terminal state is
our Uno framework and reinforcement learning environment.
reached, where the outcome of the game is determined.
One problem for our games is how to represent the game into
4) Backpropagation: Once the simulation is complete,
a reinforcement learning environment. We have to define the
propagate the result back through the nodes along the
elements of the MDP in the context of UNO:
pathtotheroot.Updatethevisitcountsandtheaverage
• State (S): The state represents the current situation of a
value estimates at each node based on the outcome of
single player, which only includes information available
the simulation.
for that player. In our state encoding of UNO, this is
characterized by the player’s hand and the target card.
D. Neural Fictitious Self Play Differentplayerswouldhavedifferentstatesastheyhave
Many reinforcement learning algorithms on multi-players’ different hands.
games are based on Nash equilibrium [6], with Neural Fic- • Actions (A): Actions are the legal moves that a player
titious Self-Play (NFSP) [5] being a notable example. Nash can make during their turn in a round of UNO.
equilibrium is a crucial concept in game theory, especially in • Transition Function (T):Thetransitionfunctiondefines
non-cooperativegames.Itdescribesasituationwheremultiple theprobabilitydistributionoflegalmovesgivenaplayer’s
participants select their optimal strategies, assuming that all currentstate,whichcanbepredictedbyneuralnetworks,
other players will keep their strategies unchanged. In this which is π(a|s,θ).
equilibrium,noplayercanimprovetheirpayoffbyunilaterally • Reward (R): The reward is what a player gains after
altering their strategy. In the mathematical form, a Nash completing a round. In the basic rule of UNO, a reward
equilibrium of a game with two players can be expressed as: of +1 is granted exclusively to the winner who has no
cardsleft,whileallothersreceive-1.Duringtheongoing
u (s)≥ max u (s ,s )
1 1 1 2 game, the reward following all actions is 0. But our
s1∈S1
algorithm will reshape the reward structure based on
u (s)≥ max u (s ,s ), MCTS, allowing the agent to receive rewards for certain
2 2 1 2
s2∈S2 actions taken.
where u 1, u 2 are the payoffs (reward) of player 1 and player • DiscountFactor(λ):Thisisahyperparametersignifying
2, and s is the strategy. NFSP employs neural networks to thedegreetowhichfuturerewardsareconsideredrelative
approximate Nash equilibrium by responding to opponents’ to immediate ones. For the purpose of our analysis, we
average strategies. It builds on game-theoretical principles have set this value at 0.99.5
It is essential to abstract states and actions into a format plane 2 indicates that the player has two of such cards. For
suitable for neural networks based on our definitions. In Uno, example, if the player has no Red 8 cards, the entry for Red
there are 61 distinct types of actions, which defines the action 8 in the plane 0 is 1, while the entries for Red 8 in all other
space size as 61. Each action can be represented by a unique planes are 0. If the player has two Red 8 cards, the entry for
integer ranging from 0 to 60, the same encoding approach Red 8 in the plane 2 is 1, and the entries for Red 8 in all
used in RLCard. Our neural networks will produce an output other planes are 0. We encode the target card into a single
vector of size 61, with each element representing the Q- plane where the matrix entry corresponding to the target card
valueassociatedwithitscorrespondingaction.Detailedaction is 1, and the values of all other entries in the matrix are 0.
encodings are shown in Table II.
TABLEII
ACTIONENCODINGOFUNO
ActionID CardInfo
0-9 RedCardswithNumbers
10 RedSkip
11 RedReverse
12 RedDraw2
13 RedWild
14 RedWild4
15-24 GreenCardswithNumbers
25 GreenSkip
26 GreenReverse
27 GreenDraw2
28 GreenWild
29 GreenWild4
30-39 BlueCardswithNumbers
40 BlueSkip
41 BlueReverse
42 BlueDraw2
43 BlueWild
44 BlueWild4
45-54 YellowCardswithNumbers
55 YellowSkip
56 YellowReverse
57 YellowDraw2
58 YellowWild
59 YellowWild4
60 Draw
We have adopted the state encoding approach described in
RLCard, but our method differs by reducing the state size
andomittingunnecessaryinformation.WhileRLCardincludes
the player’s hand, the target card, and additional cards as
part of the state, we argue that incorporating additional cards
is redundant. Cards outside a player’s hand and target card
include opponents’ cards and the deck, which is reshuffled Fig. 4. The exampled hand encodings. In each plane, the first four rows
representthecolorsyellow,green,blue,andred,whilethecolumnscorrespond
in UNO. Since the agent can’t distinguish them with neural
to the number cards from 0 to 9, as well as the action cards: skip, reverse,
networks, this adds unnecessary complexity and may hinder draw2,wild,andwild4.
training. Including additional cards would vastly expand the
state space. To simplify and speed up training, we use only
the player’s hand and target card. The state size of RLCard’s B. ImproveQvalueestimationinDDQNandreshaperewards
state encoding is 10126, while ours is 1072. with Monte Carlo Tree Search
We encode hands and the target card into 4 planes. Every Some former research [23] [4] has explored the idea of
plane is a matrix of size 4x15, with each entry being 0 or 1 combinations of Q-learning and MCTS, but mostly in simple
forone-hotencodings.Thenumber15representsthedifferent perfect information games and not applicable to imperfect
typesofcards,disregardingcolor,whicharethenumbercards information games. Our algorithm introduces a more complex
from 0-9 and five kinds of special cards: skip, reverse, draw MCTSvariantwithmodificationssuchasadifferentexpansion
2, wild, and wild 4. The number 4 represents the four colors: procedure and alternative backpropagation methods. We have
red, green, blue and yellow. alsomodifylossfunctionsduringagenttraininganddeveloped
We encode the agent’s hand into three planes(shown in an MCTS-based reward shaping structure.
Figure 4), as in Uno, a player can have either 0, 1, or at After expanding a new state, the simulation continues
most 2 cards of any given type. The plane 0 indicates that until the game ends in the traditional MCTS. We limit each
the player has zero of such cards in their hand. The plane 1 simulationtoonlyonestateexpansionsincethisismorecom-
indicates that the player has exactly one of such cards. The putationally efficient. Unlike standard MCTS, a single agent6
typically plays through the entire game via self-play, with 3) Backpropagation: This step involves updating the Q-
anynewstatebeingexpanded,evaluated,andbackpropagated. values based on Equation 4 for all states of player
However, due to the varying information available to different ID along the path from the newest found state or
root
players in Uno, this approach is not feasible. The player in endstatetotheroot.Finally,repeattheabovethreesteps
the new state may be different, and it’s impossible for the for a new round of simulation.
nextplayertopassinformationbacktothepreviousoneifthe InsamplingofstandardDDQN,Whentheagentisatcurrent
two players are not the same. state s, actions are selected via ϵ-greedy: with probability ϵ,
In our approach, if the simulation starts with player 1’s a random action is taken, and with 1 − ϵ, the action with
turn and transitions to player 2’s, we skip evaluating or the highest Q-value (predicted by neural networks) is chosen.
backpropagatingfromallstatesofplayer2.Player2continues After agent transitioning to the next state s′ and receiving re-
with their own strategy until it’s player 1’s turn again. If wardr,thetuple(s,a,s′,r)isstoredforfurthertraining.The
the game ends, we backpropagate the results from the last sampling process in our algorithm is divided into two parts:
state where it was player 1’s turn along the path to the start MCTS simulation and interaction with the real environment.
state. Our algorithm ensures that all states along the MCTS Assume the current state s in the real environment, which is
simulationpathbelongtotheplayerwhostartsthesimulation. also the start state of MCTS, is simulated by MCTS. After
Theactiontakenbytheagentinexpansionisthelegalaction the simulation, each legal action a under state s is assigned a
withhighestsumofQ-valueandtheUpperConfidencebounds corresponding Q-value, Q (s,a), based on our MCTS rules.
m
applied to Trees (UCT): We then use an epsilon-greedy strategy to select the action
based on Q (s,a). Unlike Q-values Q(s,a) are derived from
π(a|s)=argmax(Q(s,a)+UCT(s,a)), (3) m
a a single neural network in DDQN, our Q (s,a) are obtained
m
where Q(s,a) is the Q-value of the legal action a under a from repeated MCTS simulations, reducing overestimations
state s, and UCT(s,a) can be defined as: of Q values and making them more accurate. During the
simulation, whenever an end state is reached and a reward
(cid:115)
N(s) r (-1 or 1) is obtained, we accumulate and average these
k
UCT(s,a)=c ,
puct 1+N(s,a) rewards:
(cid:80)n
r
whereN(s)isthetotalnumberofvisitsofstates,andN(s,a) r = k=1 k (5)
m N
isthetotalnumberofusesoflegalactionaatthestates,while s
c puct is a constant representing the exploration term. When a ,where n is the number of times the reward received at
new state is discovered, if it is not an end state, the Q-value end state, N is the number of simulation and r is the total
s m
willbepredictedforeachlegalactionunderthatstatethrough average reward from MCTS. After the agent chooses action
neuralnetworks.Q-valueispassedfromthechildtotheparent a based on Q (s,a) , it transitions to the next state s′ and
m
during backpropagation and it is updated based on: receives a reward r from real environment. We then add r
Q new(s,a)=
Q old(s,a)·N N(s (, sa ,) a+ )+Q 1(s′,a′)+r(s′)
, (4)
a stn ad ndr am rda es nvth ire onto mta el ntr ,e tw hear ad ger nt ta tf yt per icaa lg le yn rt ecta ek ive es sa rc et wio an rda o. fIn
0
aftertakinganactionduringtheongoinggame,butbecauseof
where r(s′) is the immediate reward after taking a to next rules MCTS, our agent can receive proper rewards at certain
state s′, always 0 unless the game ends. If s′ is the end state, points, preventing long periods without positive feedback. We
Q(s′,a′) will be 0, r(s′) will be 1. The whole procedure of then keep tuple (Q (s,a),s,a,s′,r ) as training data.
m t
ourMCTSinUnofollows(Graphicalrepresentationshownin
We also modified the loss function for training the agent,
Figure 5):
splitting it into two components:
1) Selection: This process starts from the root (start state)
to find a state that has not been expanded. Record the L=L ddqn+L mcts (6)
playerIDoftherootnode,ID .Ifthecurrentstateis
root , where L is the same as in Equation 1, except that the r
duringplayerID ’sturn,weselecttheactionwiththe ddqn
root in Equation 2 is replaced with the total reward r , and L
highest(Q+UCT)basedonEquation3forsimulation. t mcts
is:
If it is another player’s turn, that player will choose an
action based on their own policy until they transition to
L =(Q (s,a)−Q (s,a;θ))2
a state belonging to player ID . mcts m estimator
root
2) Expansion: This step unfolds on the previously unex- Q (s,a) is Q values from MCTS.
m
panded player ID ’s state, selecting the action with
root Algorithm 1 and 2 shows the pseudocode of our whole
the highest (Q+UCT) value. We try to find a child,
algorithm, including the search and training.
whichisalsoastateofID ,ofthisunexpandedstate.
root
If a game-ending state occurs during another player’s
IV. EXPERIMENTSANDEVALUATIONS
turn, we immediately proceed to step 3. If not, the
new state of player ID is found, and this state is We conducted experiments and trained our algorithm,
root
initialized by Q-value prediction for every legal action, DDQN with MCTS, alongside three traditional algo-
after which we move to step 3. rithms—DDQN, DMC, and NFSP—with the same number7
Algorithm 1 MCTS in Uno
1: function INITIALIZE
2: Initialize data structures of Q(s,a), N(s,a), N(s),
V(s)
3: Q(s,a):setofQ-valuesbytakingactionaunderstate
s
4: N(s,a): set of numbers of executing action a under
state s
5: N(s): set of numbers of visits for state s
6: V(s): set of legal actions for state s
7: r k: rewards get at the end state
8: Record the player ID of root node as ID root
9: end function
10: function MCTS(s)
11: INITIALIZE
12: for i∈{1,...,simulate num} do
13: SIMULATE(s)
14: end for
15: a best ←ϵ−greedy(Q(s,a))
16: Calculate r m based on Equation 5
17: return (Q(s,a best),r m,a best)
18: end function
19: function SIMULATE(s)
20: if ISEND(s) then
21: Record REWARD(s) as r k ▷ REWARD(s) is the
reward function based on the game rules
22: return 0
23: end if
24: if ISNEW(s) then
25: V(s),Q(s,a)← PREDICT(s) ▷ Predict Q value
of every action under state s
26: N(s,a)←0
27: return max(Q(s,a))
28: end if
29: a←argmax a(Q(s,a)+UCT(s,a))
30: s′,r ← STEP(a)
31: q ← SIMULATE(s′)
32: UPDATE(s, q, a, r)
33: end function
34: function STEP(a)
35: s g ← GAMESTEP(a) ▷ The game environment
steps based on rules of game
36: if s g is state of ID root then
37: return s g, r
38: end if
39: a′ ← POLICYOTHER(s g)
40: return STEP(a′)
41: end function
42: function UPDATE(s, q, a, r)
43: N(s,a),N(s)+=1 Fig.5. Thestartingplayerisplayer1,soweonlyexpandandbackpropagate
44: Update Q(s,a) ▷ Update Q-values with formula valuesforplayer1’sstates.
based on Equation 4
45: end function8
Algorithm 2 DDQN with MCTS written in Python 3.10, and agents were trained on a single
1: Initialize experience replay buffer B to keep the training NVIDIA RTX 4080 GPU. All agents shared the same neural
data, the network Q current, and target network Q target network architecture and hyperparameters, consisting of fully
2: function DDQN connected layers with sizes 240x64, 64x64, and 64x61. The
3: for i∈{1,...,training num} do batch size was set to 32, the learning rate to 0.00005, and the
4: Initializethegameenvironmentandgetthestarting reward discount factor to 0.99. During the sampling process
state s for DDQN with MCTS, each state was simulated 50 times
5: B ← GENERATEDATA(s) using MCTS. A higher number of simulations could lead to
6: TRAINING(B) more backpropagation updates and potentially more accurate
7: end for Q-value predictions, but it would also increase the sampling
8: end function time. We selected 50 simulations as it provides a balance
9: function GENERATEDATA(s) between having rewards r based on Equation 5 in some
m
10: while s is not the end state do simulations and avoiding a substantial increase in sampling
11: (Q m(s,a),r m,a)← MCTSINUNO(s) time.
12: (s′,r)← GAMESTEP(a) We trained the four algorithms separately until their perfor-
13: r t ←r m+r mance began to converge. The raw training graph and com-
14: Store the tuple (Q m(s,a),r t,s,a,s′) into B parisons between our algorithm and the other three traditional
15: s←s′ methodsin3-playerand4-playergamesareshowninFigure6.
16: end while DDQN with MCTS achieves the highest average total reward
17: end function greater than -0.05 in the 3-player game and greater than -0.25
18: function TRAINING(B) inthe4-playergame,indicatingawinratecloseto50%inthe
19: Update the parameters of Q current using the training 3-player game against random-playing agents, and a win rate
data in the buffer B based on Loss Function 6. higher than 37.5% in the 4-player game. In general, DDQN
20: if ISUPDATETARGET then with MCTS consistently outperformed the other algorithms in
21: Synchronize parameters of Q target with Q current both environments. As the number of players increases, the
22: end if training difficulty for the agent also grows. While other algo-
23: end function rithms tend to perform progressively worse in environments
with more players, DDQN with MCTS consistently maintains
a stable rate of improvement and learning. More importantly,
of training episodes in two Uno environments: a three- DDQNwithMCTSimprovesveryquicklyintheearlystages,
playergameandafour-playergame,configurationscommonly whereas other algorithms only begin to improve slowly in the
played by human players. The performance of DDQN with middle and later stages. From the raw graph, it is evident
MCTS was then compared to the three traditional algorithms. that the data exhibits a large variance due to the randomness
To ensure a fair comparison of the performance and learning inherent in the game. We focus more on the overall trends in
capabilities of the four algorithms, during training of every the data when comparing these algorithms. Figure 7 shows a
reinforcement learning agent, there was only one correspond- comparison of the mean and variance of the training data for
ing RL agent in each environment, while the others were thealgorithms.Intermsofthemeancomparison,DDQNwith
random-playing agents. The evaluation and comparison are MCTSachievesatotalaveragerewardthatis0.6to1.3higher
based on two main metrics: Total Average Rewards and Win than the other algorithms in the 3-player game, meaning its
Rate. Rewards were given to the agents at the end of each winrate intestsagainstrandom-playing agentsis3% to6.5%
game by the environment, with the agent receiving either - higherthanthatoftheotheralgorithms.Inthe4-playergame,
1 or 1. The total average reward was the sum of rewards DDQNwithMCTSachievesatotalaveragerewardthatis0.7
accumulated across all games divided by number of games. to1.3higher,correspondingtoawinrate3.5%to6.5%higher
The win rate was calculated as the number of games won by than that of the other algorithms in tests against random-
an agent divided by the total number of games played. Given playingagents.WealsotestedDDQNwithMCTSagainstthe
thehighrandomnessoftheUnogame,therewarddataexhibits otheralgorithmsin10,000gamestoevaluateitswinratewhen
significantvariance;hence,wedecidedtomainlyfocusonthe competing directly with them. Regardless of which algorithm
overall trends in the data. To reduce the effect of variance of it was tested against, DDQN with MCTS always achieved a
data, after every 1,000 training episodes, the agent was tested higher win rate. In the 3-player game, its win rate was 4%
by playing 1,000 games with agents playing cards randomly, to 16% higher than that of the other algorithms, and in the
andthetotalaveragerewardswererecordedandplottedinthe 4-player game, it was 5% to 11% higher, shown in Table III
training graph. and Table IV.
In the evaluation, each algorithm relied solely on its esti-
mator network (DMC, DDQN, DDQN with MCTS) or policy
A. Test with Human Players and Knowledge learned by
network (NFSP) to select actions. Although DDQN with
DDQN with MCTS
MCTS used MCTS during the sampling process, for fair
comparison, it only used the trained estimator network to In order to provide human players with an opportunity to
select the action with the highest Q-value. The code was compete against our agents and to assess the performance9
(a) (b) (c) (d) (e) (f)
Fig. 6. Comparison of the raw training graphs between DDQN with MCTS and traditional agents in the 3-player(a, b, c) and 4-player(d, e, f) Uno game.
Eachdatapointrepresentsthetotalaveragerewardsover1,000testgameswheretheagentcompetedagainstrandom-playingagents.DDQNwithMCTSvs
DDQN(a,d).DDQNwithMCTSvsDMC(b,e).DDQNwithMCTSvsNFSP(c,f).
(a) (b) (c) (d) (e) (f)
Fig.7. Comparisonofthetraininggraphs(meanandvariance)betweenDDQNwithMCTSandtraditionalagentsinthe3-player(a,b,c)and4-player(d,e,
f)Unogame.Eachdatapointrepresentsthetotalaveragerewardsover1,000testgameswheretheagentcompetedagainstrandom-playingagents.DDQN
withMCTSvsDDQN(a,d).DDQNwithMCTSvsDMC(b,e).DDQNwithMCTSvsNFSP(c,f).
TABLEIII
WINRATEOFFOURALGORITHMSIN3-PLAYERGAMESTESTIN10000
GAMES
Algorithms WinRateofAlgorithms
DDQNwithMCTS 40%±0.015
DDQN 35%±0.048
DMC 25%±0.1
DDQNwithMCTS 40%±0.05
DDQN 36%±0.052
NFSP 24%±0.005
DDQNwithMCTS 44%±0.048
NFSP 28%±0.018
DMC 28%±0.05
Fig.8. ExampledthreeplayersGUI
TABLEIV
WINRATEOFFOURALGORITHMSIN4-PLAYERGAMESTESTIN10000
GAMES algorithms indeed learned some patterns of the game, which
typically reflect strategies also exhibited by human players.
Algorithms WinRateofAlgorithms
DDQNwithMCTS 32%±0.05 For example, when the agents possess many cards along with
DDQN 27%±0.001 a’wilddraw4’,theytendtoplayitimmediately.Thisstrategy
DMC 20%±0.1
increasesthenumberofcardsthenextplayerholds,makingit
NFSP 21%±0.015
more challenging for them to win the round. When the agents
are down to only two cards, and one of them is a ’wild’ or
of our algorithms in competition with human players, we ’wild draw 4’, they will save these for the final play. This is
havedevelopedafullyfunctionalgraphicaluserinterface.Our because, regardless of the target card’s color or number, the
interface is implemented using the built-in Python module game rules allow a player with a wild card to play it directly,
Tkinter [7]. We are also implementing online multiplayer thus creating a guaranteed winning situation by holding onto
functionality using the User Datagram Protocol (UDP) [10], this card until the end.
allowing human players from different locations to remotely
compete against our algorithms in 2-players, 3-players and V. CONCLUSION
4-players environments, exampled GUI shown in Figure 8. We introduced the complexity of imperfect information
We conducted a series of 100 games for each algorithm in games and discussed their research value. We selected Uno,
one-on-one competitions with average human players. We a imperfect information game, as the basis for our research.
observed DDQN with MCTS have surpassed human per- We represented Uno as a reinforcement learning problem and
formance, achieving approximately a 54% win rate against presented a novel algorithm, Double Deep Q-Learning with
average human players. In both the 3-player and 4-player Monte Carlo Tree Search (DDQN with MCTS), to address
games, the win rate of the agent is comparable to that of challenges encountered in prior work with imperfect infor-
human players. When we tested the DDQN with MCTS mation games, such as reward sparsity and Q-value overesti-
algorithms against human players, we observed that these mation. Additionally, we developed a graphical user interface10
(GUI) to allow human players to compete against our agents, [16] Matthijs TJ Spaan. Partially observable markov decision processes.
where DDQN with MCTS outperformed the average human In Reinforcement learning: State-of-the-art, pages 387–414. Springer,
2012.
playerintermsofwinratesonone-to-onecompetition.DDQN
[17] Jianyu Su, Stephen Adams, and Peter Beling. Value-decomposition
with MCTS also demonstrated superior performance com- multi-agent actor-critics. In Proceedings of the AAAI conference on
paredtothreetraditionalmethods—DoubleDeepQ-Learning, artificialintelligence,volume35,pages11352–11360,2021.
[18] RichardSSutton. Reinforcementlearning:Anintroduction. ABradford
DeepMonteCarlo,andNeuralFictitiousSelf-Play—achieving
Book,2018.
higher total average rewards with fewer training steps and [19] Uno. Unorules. https://www.unorules.com/. [Accessed:07-Oct-2024].
higher win rates during testing. [20] HadoVanHasselt,ArthurGuez,andDavidSilver. Deepreinforcement
learningwithdoubleq-learning. InProceedingsoftheAAAIconference
Inoursubsequentwork,weplantoexperimentwithdeeper
onartificialintelligence,volume30,2016.
neuralnetworksandmorecomplexnetworkarchitectures.Due [21] OriolVinyals,IgorBabuschkin,WojciechMCzarnecki,Michae¨lMath-
to the long computation time of MCTS, we will also explore ieu,AndrewDudzik,JunyoungChung,DavidHChoi,RichardPowell,
Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii
optimizing its efficiency [15]. Furthermore, since our DDQN
using multi-agent reinforcement learning. nature, 575(7782):350–354,
withMCTScanbegeneralizedandappliedtoanyalgorithm’s 2019.
valueestimationfunction,weplantoextendthisimprovement [22] OriolVinyals,TimoEwalds,SergeyBartunov,PetkoGeorgiev,Alexan-
der Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich
to state-of-the-art Actor-Critic algorithms [17] [25].
Ku¨ttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new
challengeforreinforcementlearning. arXivpreprintarXiv:1708.04782,
2017.
REFERENCES [23] HuiWang,MichaelEmmerich,andAskePlaat. Montecarloq-learning
forgeneralgameplaying. arXivpreprintarXiv:1802.05944,2018.
[24] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine
[1] Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep
learning,8:279–292,1992.
counterfactual regret minimization. In International conference on
[25] Yuchen Xiao, Weihao Tan, and Christopher Amato. Asynchronous
machinelearning,pages793–802.PMLR,2019.
actor-criticformulti-agentreinforcementlearning. AdvancesinNeural
[2] Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M
InformationProcessingSystems,35:4385–4400,2022.
Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego
[26] DaochenZha,Kwei-HerngLai,YuanpuCao,SongyiHuang,RuzheWei,
Perez, Spyridon Samothrakis, and Simon Colton. A survey of monte
Junyu Guo, and Xia Hu. Rlcard: A toolkit for reinforcement learning
carlo tree search methods. IEEE Transactions on Computational
incardgames. arXivpreprintarXiv:1910.04376,2019.
IntelligenceandAIingames,4(1):1–43,2012.
[27] Daochen Zha, Jingru Xie, Wenye Ma, Sheng Zhang, Xiangru Lian,
[3] Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance
XiaHu,andJiLiu. Douzero:Masteringdoudizhuwithself-playdeep
reduction techniques for gradient estimates in reinforcement learning.
reinforcementlearning.Ininternationalconferenceonmachinelearning,
JournalofMachineLearningResearch,5(9),2004.
pages12333–12344.PMLR,2021.
[4] Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Tobias
[28] YoupengZhao,JianZhao,XunhanHu,WengangZhou,andHouqiang
Pfaff,TheophaneWeber,LarsBuesing,andPeterWBattaglia.Combin-
Li.Douzero+:Improvingdoudizhuaibyopponentmodelingandcoach-
ingq-learningandsearchwithamortizedvalueestimates.arXivpreprint
guidedlearning.In2022IEEEconferenceongames(CoG),pages127–
arXiv:1912.02807,2019.
134.IEEE,2022.
[5] Johannes Heinrich and David Silver. Deep reinforcement learn-
[29] YoupengZhao,JianZhao,XunhanHu,WengangZhou,andHouqiang
ing from self-play in imperfect-information games. arXiv preprint
Li.Fulldouzero+:Improvingdoudizhuaibyopponentmodeling,coach-
arXiv:1603.01121,2016.
guided training and bidding learning. IEEE Transactions on Games,
[6] David M Kreps. Nash equilibrium. In Game theory, pages 167–177.
2023.
Springer,1989.
[7] Fredrik Lundh. An introduction to tkinter. URL: www. pythonware.
com/library/tkinter/introduction/index.htm,539:540,1999.
[8] VolodymyrMnih.Playingatariwithdeepreinforcementlearning.arXiv
preprintarXiv:1312.5602,2013.
[9] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,
Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, An-
dreasKFidjeland,GeorgOstrovski,etal. Human-levelcontrolthrough
deepreinforcementlearning. nature,518(7540):529–533,2015.
[10] JonPostel. Userdatagramprotocol. Technicalreport,1980.
[11] DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,
George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
withdeepneuralnetworksandtreesearch. nature,529(7587):484–489,
2016.
[12] DavidSilver,ThomasHubert,JulianSchrittwieser,IoannisAntonoglou,
Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan
Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-
play with a general reinforcement learning algorithm. arXiv preprint
arXiv:1712.01815,2017.
[13] DavidSilver,ThomasHubert,JulianSchrittwieser,IoannisAntonoglou,
Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan
Kumaran, Thore Graepel, et al. A general reinforcement learning
algorithmthatmasterschess,shogi,andgothroughself-play. Science,
362(6419):1140–1144,2018.
[14] DavidSilver,JulianSchrittwieser,KarenSimonyan,IoannisAntonoglou,
Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai,
Adrian Bolton, et al. Mastering the game of go without human
knowledge. nature,550(7676):354–359,2017.
[15] Lei Song, Ke Xue, Xiaobin Huang, and Chao Qian. Monte carlo
tree search based variable selection for high dimensional bayesian
optimization. Advances in Neural Information Processing Systems,
35:28488–28501,2022.