Grounded Answers for Multi-agent Decision-making
Problem through Generative World Model
ZeyangLiu,XinruiYang,ShiguangSun,LongQian,LipengWan,XingyuChen
XuguangLan‚àó
NationalKeyLaboratoryofHuman-MachineHybridAugmentedIntelligence
NationalEngineeringResearchCenterforVisualInformationandApplication
InstituteofArtificialIntelligenceandRobotics
Xi‚ÄôanJiaotongUniversity,Xi‚Äôan,China,710049
zeyang.liu@stu.xjtu.edu.cn, xglan@mail.xjtu.edu.cn
Abstract
Recentprogressingenerativemodelshasstimulatedsignificantinnovationsinmany
fields,suchasimagegenerationandchatbots. Despitetheirsuccess,thesemodels
oftenproducesketchyandmisleadingsolutionsforcomplexmulti-agentdecision-
makingproblemsbecausetheymissthetrial-and-errorexperienceandreasoning
as humans. To address this limitation, we explore a paradigm that integrates a
language-guidedsimulatorintothemulti-agentreinforcementlearningpipelineto
enhancethegeneratedanswer.Thesimulatorisaworldmodelthatseparatelylearns
dynamicsandreward,wherethedynamicsmodelcomprisesanimagetokenizer
aswellasacausaltransformertogenerateinteractiontransitionsautoregressively,
andtherewardmodelisabidirectionaltransformerlearnedbymaximizingthe
likelihoodoftrajectoriesintheexpertdemonstrationsunderlanguageguidance.
Givenanimageofthecurrentstateandthetaskdescription,weusetheworldmodel
totrainthejointpolicyandproducetheimagesequenceastheanswerbyrunning
theconvergedpolicyonthedynamicsmodel. Theempiricalresultsdemonstrate
that this framework can improve the answers for multi-agent decision-making
problemsbyshowingsuperiorperformanceonthetrainingandunseentasksof
the StarCraft Multi-Agent Challenge benchmark. In particular, it can generate
consistentinteractionsequencesandexplainablerewardfunctionsatinteraction
states,openingthepathfortraininggenerativemodelsofthefuture.
1 Introduction
Recent progress in generative artificial intelligence with models capable of generating creative
contenthasshownattractiveprospectsforreal-worldapplications,suchasimagegeneration(Takagi
&Nishimoto,2023),embodiedagents(Brohanetal.,2023b),andchatbots(K√∂pfetal.,2024). Most
generativemodelsattempttodirectlyobtaintheanswerbytrainingonnaturallanguageorimage
datasets and inserting decomposed reasoning steps in few-shot demonstrations. However, these
methodsdonotexperiencefirsthandthesituationsdescribedbythelanguageandtheimage. They
cannotfindthecorrectanswersthroughtrialanderrorlikehumans,whichisnecessarytoground
reasoning on complicated problems and transfer learned knowledge to unfamiliar domains. For
example,asshowninFigure1,whenaskedacomplexmulti-agentdecisionproblem,oneofthemost
widely-usedlargelanguagemodels,GPT4-thoughachievingsuperhumanperformanceinmany
reasoningtasks-willgeneratesketchyandmisleadinganswers.
‚àóCorrespondingauthor.
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
tcO
3
]IA.sc[
1v46620.0142:viXraVision-Language Image-Text-State (Learned) LBI Augmented
Model Transformation World Model Policy
Environment Update
LBI Output
GPT-4 Output Question
1 2 3 4 5 6. . . . . . A P F U A Roo dn t eic ls aa ti rzuil p ety ei s to az tAn te F o i abi n rt E nh ieg li n de t ia e e HE n msn ed ySe a Fm t lMro ay or tm e vC egao mit cm io a ep n l nlo y tssition L m s t rh it ge ra eat hr' s t s tin e ? is xe ga s y ey no s w ehn moe t u' h yre le d m c l we ao f e rn t i. nt ur W eo s sl ehl i an ototg n df ti hv e ee fe at ‚Ä¶ ‚Ä¶ ‚Ä¶
Figure 1: Complex decision problems that require a good understanding of the environment‚Äôs
dynamicsandtheobjectivearestillchallengingforcurrentvision-languagemodels,e.g.,theanswer
elicitedbyGPT-4issketchyandmisleading. Instead, LearningbeforeInteraction(LBI)enables
groundedreasoningbysimulatingthetaskinthegivenquestion. LBIutilizesthesimulatortotraina
MARLpolicyandgeneratetheanswerbyrunningtheconvergedpolicyonthesimulator.
Totacklethisproblem,wecanutilizethegenerativemodelstounderstandthepropertiesofthetask
thattheuserdescribesandsimulatetheeffectsoftheactions. Wecanderivetheanswerwithahighly
realistic simulator by experiment-reasoning or training any machine intelligence from simulated
experience. TheoriginofthisideacanbetracedbacktoDynaarchitecture(Sutton,1990)andhas
spawned a series of model-based reinforcement learning (MBRL) theories and methods (Janner
etal.,2019;Kaiseretal.,2019;Laietal.,2020). Inspiredbythis, Mind‚ÄôsEye(Liuetal.,2022)
enableslanguagemodelstoperformreasoningconditionedonthesimulationresultsbyrunningthe
correspondingexperimentonacomputationalphysicsenginenamedMuJoCo(Todorovetal.,2012).
Mind‚ÄôsEyecanboostreasoningperformanceinzero-shotandfew-shotsettingsbyinfusingsuch
physicalknowledgeintolanguagemodels. However,itisparticularlydesignedforphysicalreasoning
ratherthandecision-makingproblems.
Incontrast,UniSim(Yangetal.,2024)formulatestheaction-in-video-outframeworkasanobservation
predictiondiffusionmodelconditionedonfinitehistory. Itshowsthatthesimulatorlearnedfrom
broaddatacangeneralizetotherealworldandbridgethesim-to-realgap. Genie(Bruceetal.,2024)
enablesuserstoactinthegeneratedenvironmentsonaframe-by-framebasis,openingthepathfor
traininggeneralistagentsofthefuture. Notably,mostoftheexistingbreakthroughsonlearningin
theimaginedexperiencehavebeenfocusingonsingle-agentscenariosandleavetheworldmodel
largelyunstudiedformulti-agentreinforcementlearning(MARL)tasks-itiscommoninreal-world
applicationsthatmultipleagentsarerequiredtosolveataskinacoordinatedfashion.
TheroadblockstobuildingasimulatorforMARLtasksarethreefold. First,MARLtasksinvolve
multiple entities‚Äô attributes, e.g., positions and roles, making it difficult to describe a state using
only text. The text and image information can be brought together to enrich the inputs for the
simulator,butsuchadatasetislimitedinquantity. Second,thedynamicsandrewardmodelsofthe
MARLenvironmentaremoreintricatethanthesingle-agentsetting. Currentapproachesassume
thesingle-rewardisknowninthedatasetMengetal.(2023)orcanbeeasilydeducedbytheframe
information(Yangetal.,2024),whichcouldbechallengingforMARLmethodsduetotheabundance
ofagents‚Äôtacticsandthecompositionalnatureoftheirfunctionalities.
Thisworkexploresaparadigmthataddslanguage-guidedsimulationtotheMARLpipelinetomake
policy learning grounded within the learned world model. First, we propose new offline MARL
datasetstoprovidepairedstate-imageinformationfortheStarCraftMulti-AgentChallenge(SMAC)
environmentbytransformingthestateinthetrajectorytothecorrespondingimage. Wealsodesigned
aparsertoconverteachtrajectorytoataskdescriptionusingnaturallanguage. Then,wepre-traina
vectorquantizedvariationalautoencoder(VQ-VAE)(VanDenOordetal.,2017)togeneratediscrete
representationsforeachframe.Theworldmodelisformulatedasaninteractivesimulatorthatconsists
ofadynamicsandarewardmodel. Thedynamicsmodelcomprisesanimagetokenizerandacausal
transformertogenerateinteractiontransitionsautoregressively. Therewardmodelisabidirectional
transformerlearnedbymaximizingthelikelihoodoftrajectoriesintheexpertdemonstrationsunder
thetaskdescription.
2State-based
Trajectories ‚ë† VisionSMACDataset Images ‚ë° VQ-VAE Learning
Parser ùëì
Image
Tokenizer
Encoder
Task Description Codebook
Offline We have 7 Marines, 2 Marauders, and 1
Buffer Medivac. What is the best strategy for us to Image
completely eliminate the enemy's 8 Marines, 3 Tokenizer
Marauders, and 1 Medivac? Decoder
Materials
Figure2: DatasetsconstructionandVQ-VAEtraining.
Given a decision-making problem by the user and an image from the environment, we store the
simulatedinteractiontrajectoriesintoareplaybufferbyrunninganoff-policyMARLalgorithmon
thegenerateddynamicsmodel. Then,weutilizethegeneratedrewardmodeltolabeltherewardfor
eachstate-actionpairbasedonthewholetrajectory. Weupdatethepolicynetworkaccordingtothe
rewardwithabehavior-regularizationterm,whichservesastheconservatismforout-of-distribution
state-actionpairs. Weusetheimagesequencegeneratedbytheinteractionofthedynamicsmodel
andtheconvergedpolicymodelastheanswertothedecision-makingproblem.
We summarize the main contributions of this paper in three folds: (1) It proposes novel MARL
datasetsforSMAC,whereaparserautomaticallygeneratestheground-truthimageofagivenstate
andtaskdescription. (2)ItintroducesLearningbeforeInteraction(LBI),aninteractivesimulator
thatgeneratestrial-and-errorexperiencesandimprovestheanswersformulti-agentdecision-making
problems. (3)TheempiricalresultsshowthatLBIoutperformsvariousofflinelearningmethodson
trainingandunseenMARLtasks. ThevisualizationalsoindicatesthatLBIcanproduceconsistent
imaginedtrajectoriesandexplainablerewardsforinteractionstates.
2 Methodology
Weformulateaninteractionsimulatorasatransitionpredictionmodelthat,givensomestateofthe
world and descriptions of the task, can take some actions as input and produce the consequence
oftheactionsintheformofimages,states,andrewards. Inthispaper,weconsiderbuildingsuch
simulatorsforamulti-agentdecision-makingenvironmentnamedStarCraftMulti-AgentChallenge
(SMAC)(Samvelyanetal.,2019),knownforitsrichenvironmentsandhighcontrolcomplexity. See
AppendixCformoreinformationaboutSMAC.
2.1 VisionSMAC
TheSMACbenchmarksavesareplayofanepisodeasaSC2REPLAYfileratherthanprovidingthe
imagefeatureduringexploration. Itiscomputationallyexpensivetoconstructdatasetsofimages
bywatchingsuchreplaywithintheStarCraftIIclientandthensubsamplingaframethatcaptures
meaningfulactions.Tosolvethisproblem,weintroduceVisionSMACtoconvertthestateintoimages
andlanguagesthroughaparserf,decoupledfromStarCraft,makingiteasytocreatenewcontent.
First,wecollectofflinedatasetsacrosstentrainingmapsintheSMACbenchmarkbyrunningmulti-
agentexplorationmethodsnamedEMC(Zhengetal.,2021)andIIE(Liuetal.,2024). Eachdataset
containsalargenumberofinteractiontrajectories: œÑ :={s ,{oi}n ,{ui}n ,{di}n }T ,where
t t i=1 t i=1 t i=1 t=0
s denotesthestate,{oi}n istheobservationofeachagent,{ui}n isthejointaction,andthe
t t i=1 t i=1
donesignaldi =1whentheagentiiskilled,tisthetimestep,nandT denotethenumberofagents
t
andthelengthoftheepisode,respectively. Wefurthercollecttheelementimagesthatappearinthe
gameandaffectthestate,suchastheunitsandthebackgroundterrainoftrainingmaps.
Weconstructthepairedstate-imagedatasetbyplacingeachunitimageanditshealthbarattheir
positionswiththecorrespondingbackgroundterrain. Thisreconstructedimagecancloselyresemble
aspecificstateintheoriginalreplay. Wealsoperformdataaugmentationtoenablebetterfeature
abstractionbychangingthebackgroundtodifferentterrains.
3<task description> <image> ‚ë† Reward-free Data Collection
Language Image Supervised Learning
Tokenizer Tokenizer dataset
‚Ä¶
<ùë†ùë°> <ùëúùë°> <ùë¢ùë°>
‚Ä¶
<ùë†ùë°+1><ùëúùë°+1>
‚ë¢ Update
Interaction
Dynamics Model (ùúì) Policy Model (ùúÇ)
‚Ä¶ ‚Ä¶ ‚Ä¶
Reward-free Trajectory
‚ë° Language-guided Reward Labeling
<task> <ùë†0> <ùëú0> <ùë¢0> <ùëü0>
‚Ä¶
<ùëü1>
‚Ä¶
<ùëüùë°>
Behavior-regularized RL
Reward model (ùúô)
Interaction
Auxiliary
‚Ä¶ ‚Ä¶ Environment
Module (ùúë)
Inverse RL
Figure3: TheoverviewofLearningbeforeInteraction.
WealsodefineataskdescriptionLtospecifytheenvironmentandthetask. Thetaskdescription
canbetheterminatedstate,asliceofatrajectory,oranyotherrepresentationoftheepisode. Inthis
paper, we use the terrain information, the number and unit types of agents and enemies, and the
sumofenemies‚Äôremaininghealthpointsattheterminatedstateasthetaskdescription. Thedetailed
descriptionoftheparserf canbefoundinAppendixB.
2.2 TrainingAnInteractiveSimulator
Withtrajectorieswithcorrespondingimagesandlanguageguidancefromdifferentscenarios,wecan
formulatetheinteractionswithStarCraftIIasinteractingwithaninteractivesimulator. Thesimulator
containsthreekeycomponents: (1)animagetokenizerthatconvertsrawvideoframeintodiscrete
tokens,(2)adynamicsmodelthatpredictsthenextframeandstategivenpastframeandstatetokens,
(3)arewardmodelthatinferstherewardofastate-actionpairgivenatrajectory. Theideabehind
decomposingtheworldmodelintoadynamicsmodelandarewardmodelistoreusethedynamics
modelfordifferenttasksbycombiningitwithanyrewardfunction.
ImageTokenizer Wecompressimagesintodiscretetokenstoreducedimensionalityandenable
higher-qualityimagegeneration. Wemakeuseofvectorquantizedvariationalautoencoder(VQ-
VAE)(VanDenOordetal.,2017),whichtakeseverysingleimageI ‚àà RH√óW√óC ofthestateas
input,generatingdiscreterepresentationsz ‚àà IB,whereB isthesizeofthediscretelatentspace.
ThetokenizeristrainedusingastandardVQ-VAEobjective.
Dynamics Model The dynamics model is a causal transformer q parameterized by œà, where
thetargetsequencehasthefollowingformx={...,L ,z ,s ,o1,...,on,u1,...,un,z ,s ,...},
t t t t t t t t+1 t+1
where z is the image representation generated by the fixed image tokenizer. We utilize the task
t
descriptionL(s )tospecifythedynamicsoftheenvironment,remainingconsistentinonesequence.
t
Anembeddingforeachtimestepislearnedandaddedtoeachtoken. Thedynamicsmodelprocesses
allpasttokensandpredictsfuturetokensviaautoregressivemodeling.
Then,weusethepredictionheadstodecodethepredictedtokenstothecorrespondingelementinthe
sequenceandtrainthembyminimizingthecross-entropylossforactionsandmean-squarederrorfor
others. Theactionswouldserveasthereferencepolicyforthelearningwiththesimulatedtrajectories
describedinSection2.3. Inparticular,weuseadynamicsresidualtermtoimprovetheaccuracy
andthestabilityofthegenerationbychangingthetargetfroms to‚àÜs = s ‚àís forthe
t+1 t+1 t+1 t
statepredictionhead. Wealsoapplythistermtopredictimagerepresentations. Inaddition,sincethe
observationisonlyrelatedtothecurrentstateandthevisionrangeoftheagents,wefilteroutthe
historicalmemoriesanduses astheinputfortheobservationprediction.
t
RewardModel Weresemblethetrainingpipelineofinversereinforcementlearning-maximizing
thelikelihoodoftrajectoriesintheexpertdemonstrationswhileminimizingthelikelihoodoftrajecto-
4riescollectedfromaninner-looppolicy. WeintroducearewardfunctionrÀÜ,whichreceivesthefull
trajectoryasinputstoperformcreditassignmentunderdeterministicdynamicswithinthetrajectory.
Weremakethisformulationasageneralizedversionoftheconventionalrewarddesign;ifthereward
functionisMarkovian,thetemporaldependenciesonotherstate-actionpairsshouldalwaysbezero.
Tothisend,wemodeltherewardfunctionasabidirectionaltransformermodelparameterizedbyœï,
wherethesequenceisxÀú={...,s ,L ,u1,...,un,rÀÜ,s ,...},andrÀÜ ={rÀÜi}N isasetofindividual
t t t t t t+1 t t i=1
rewardsfortheagents. Again,weutilizethetaskdescriptionL(s )toperformhindsightrelabeling,
t
whichconvertsanimperfecttrajectoryintoapossiblesolutionforreachingthelaststates ofthe
T
episode. Weoptimizetherewardfunctionbyminimizingthefollowingloss:
(cid:34) N (cid:35) (cid:34) N (cid:35)
(cid:88)(cid:88) (cid:88)(cid:88)
‚àá L=‚àíE Œ≥t‚àá rÀÜi(œÑ;œï) +E Œ≥t‚àá rÀÜi(œÑ;œï) , (1)
œï œÑ‚àºD œï t œÑ‚àºœÄŒ∏ œï t
i t i t
where œÄŒ∏ = {œÄi(ui|s;Œ∏)}N is the MA-SACpolicy parameterized by Œ∏, and we use the reward
i=1
constraintbyimposingaL2penalizationofthepredictedrewardsoverallpossibleactions,whichcan
beviewedasaconservativeupdateforout-of-distribution(OOD)state-actionpairs. Inpractice,we
alternatebetweenk-stepofpolicyupdateandrewardupdatetoavoidcompletelysolvingthepolicy
optimizationsubproblembeforeupdatingtherewardparameters.
2.3 Inference: LearningPolicyintheSimulator
Wenowdescribehowtogenerategroundedanswersformulti-agentdecision-makingproblemsvia
LBI. Given an image of the initial state and a task description from the user, the agents using a
randomlyinitializedoff-policyMARLalgorithm(e.g.,independentQ-learning)interactwiththe
dynamicsmodeltocollectreward-freetrajectoriesinanautoregressivemanner. Then,thereward
modelpredictstheimmediaterewardforeachtransitionpairinthesimulationtrajectories. These
relabeledtrajectoriesareaddedtothereplaybuffer,servingasthetrainingdataforthepolicynetwork.
Inpractice,weconstructtheMARLprobleminthesimulatorasabehavior-regularizedMDPby
imposingabehavior-regularizationterm:
maxE(cid:20) (cid:88)‚àû Œ≥(cid:32) (cid:88)N (cid:32) ri(œÑ;œï)‚àíŒ±¬∑log(cid:32) œÄ¬Ø i(ui t|oi t;Œ∑) (cid:33)(cid:33)(cid:33)(cid:21)
, (2)
œÄ¬Ø
t=0 i=1
t q(ui t|x <ui t;œà)
whereœÄ¬Ø = {œÄ¬Ø }N isthejointpolicy,andq(ui|x ;œà)isthereferencepolicyprovidedbythe
i i=1 t <ui
t
dynamicsmodel. Thelasttermwilltransferthegreedymaxfromthejointpolicytoasoftenedmax
overthereferencepolicy,enablingin-samplelearningandfurthermitigatingtheimpactofexploring
OODregionsinthestate-actionspace.
Sinceitispossibleforspecificagentstobecomeinactivebeforethegameterminates,wemarkthe
terminatedtimestepforeachagentandenemyonceitspredictedhealthislessthanzeroandthenuse
zerovectorsasthesubsequentactionsandobservations. Itcanmitigatethehallucinatingunrealistic
outcomes-adeadagentperformsa‚Äúmoving‚Äùaction. Wealsomaskthepredictedrewardafterthe
terminatedtimestepfortheinactiveagenttogetamoreaccuratevalueestimate.
3 RelatedWork
Thissectionbrieflyintroducestherecentworkoflearningworldmodelsandimitationlearning. See
AppendixFformorerelatedwork.
3.1 WorldModels
Thereisalong-standinghistoryoflearningpredictivemodelsoftheworld. Welistthreecategories
ofmodel-basedreinforcementlearning(MBRL)accordingtothetypeofmodelusage,including
planning,analyticgradientgeneration,anddataaugmentation.
Thefirstcategoryappliesplanningmethodswithworldmodelsimulation. AlphaGo(Silveretal.,
2016) and MuZero (Schrittwieser et al., 2020) learn a transition model and apply Monte Carlo
TreeSearchtosearchforanactionsequencewiththehighestaccumulatedrewards. Bycontrast,
5MBMF (Nagabandi et al., 2018), PETS (Chua et al., 2018), and PlaNet (Hafner et al., 2019b)
integrate model predictive control (MPC) into the learned world model and sample high-reward
actionsequences. TD-MPC(Hansenetal.,2022)andTD-MPC2(Hansenetal.,2024)utilizevalue
functionstobootstrapthetrajectoriesusedforMPCplanning.
Thesecondcategorymodelsadifferentiableworldmodelandutilizestheinternalstructureofthe
modeltofacilitatepolicylearning. GPS(Levine&Koltun,2013)andGDP(Srinivasetal.,2018)
performdifferentialplanningandobtaintheanalyticformoftheoptimalpolicy. SVG(Heessetal.,
2015)re-parameterizesthepolicyandtheworldmodel,thencomputesthepolicygradientestimateby
backpropagationviatheworldmodel. MAAC(Claveraetal.,2019),Dreamer(Hafneretal.,2019a)
anditssubsequentvariants(Hafneretal.,2020,2023)usetherecurrentstate-spacemodelinPlaNet
tolearntheworldmodelinacompactlatentspaceandlearnthepolicyentirelywithinthisspace.
Thelastoneutilizesthelearnedworldmodeltogeneratemoreexperiencesandthentrainsapolicyon
thedatasetaugmentedbythemodel,alsoknownasDyna-stylemethods(Sutton,1990). MVE(Fein-
berg et al., 2018) and STEVE (Buckman et al., 2018) depict a learned world model to calculate
multi-steptemporal-differencepredictionforbettervalueestimation. Incontrast,SLBO(Luoetal.,
2018),MBPO(Janneretal.,2019),andBMPO(Laietal.,2020)theoreticallyanalyzethislearning
frameworkandprovethatthepolicyperformancewillimprovemonotonicallyinaworldmodelwith
certainmodelbiasandrolloutlength. Tofurtherincreasetherolloutlengthandavoidcompounding
errors,M2AC(Panetal.,2020)andCOPlanner(Wangetal.,2023)computetheuncertaintyofeach
rolloutstepandperformconservativemodelrolloutsbydiscardingthesampleswithhighuncertainty
oraddingapenaltytermintototalreward. Inpractice,GAIA-1(Huetal.,2023),UniSim(Yang
etal.,2024),andGenie(Bruceetal.,2024)showthatthelearnedworldmodelcanenablethecontrol
policytogeneralizetotherealworldwhentrainedpurelyinsimulationandbridgethesim-to-real
gap. Thesemethodshaveimpressiveperformanceandtheoreticalbounds,attractingmuchresearch
interest in the MBRL community. However, they focus on generating videos or trajectories that
onlyinvolveonesingleagentinsteadofbuildingamulti-agentsimulatorthatcanbeusedtofurther
improvedecision-makingperformanceoncoordinationtasksinourwork.
3.2 ImitationLearning
ImitationLearning(Bain&Sammut,1995)formulatesimitatinganexpertasasupervisedlearning
problem, which has been widely adopted in various domains due to its simplicity and effective-
ness(Silveretal.,2016;Swamyetal.,2020). GAIL(Ho&Ermon,2016)anditsextensions(Song
etal.,2018;Ghasemipouretal.,2020)standasacornerstoneapproach,whichtrainsagenerator
policy to imitate expert behaviors and a discriminator to distinguish between the expert and the
learner‚Äôsstate-actionpairdistributions. Inlightoftherecentinterestinfoundationalmodels,the
conditionaldiffusionmodelisusedtorepresentandlearnanimitationlearningpolicy,whichproduces
apredictedactionconditioningonastateandasamplednoisevectorPearceetal.(2022);Chietal.
(2023). Thesemethodsachieveencouragingresultsinmodelingstochasticandmultimodalbehaviors
fromhumanexpertsorplaydata. DT-stylemethods(Chenetal.,2021;Wuetal.,2024)formulatethe
trajectorygenerationasasequencemodelingproblem,whichgeneratesstates,actions,andrewards
byconditioningonareturn-to-gotokeninanautoregressivemanner.
In contrast, inverse reinforcement learning (IRL) is designed to infer the reward function that
underliestheexpertdemonstrations,takingintoaccountthetemporalstructureandshowingbetter
generalizationthandirectBehavioralCloning(Ng&Russell,2000;Rossetal.,2011;Bardeetal.,
2020). Amainclassofalgorithms,Maximumentropy(MaxEnt)IRL(Haarnojaetal.,2017)and
its extensions (Liu et al., 2021; Rolland et al., 2022), learn a stationary reward by minimizing
divergencebetweentheagentandexpertdistribution. Sincethelearnedrewardfunctioncansolve
downstreamtasksandtransferbehavioracrossdifferentdynamics, IRLisalsohelpfulinseveral
broaderapplications,e.g.,IRLwithnaturallanguagegoals(Fuetal.,2018a;Zhou&Small,2021;
Xu et al., 2022), and RL with human feedback (Ziegler et al., 2019; Zhu et al., 2023; Wu et al.,
2023),anddynamicslearning(Luoetal.,2023). Furthermore,aseriesofsample-efficientalgorithms
are proposed to solve the MaxEnt IRL formulation (Fu et al., 2018b; Zeng et al., 2022, 2024).
To side-step the expensive online environmental interactions in classic IRL, some work aims to
learnarewardfunctionfromastaticdatasetbyavariationalBayesianframework(Chan&vander
Schaar, 2021), representing reward function via a learned soft Q-function (Garg et al., 2021), or
incorporatingconservatismintotheestimatedrewardlikeofflineQ-learning(Yueetal.,2022). The
6Table1: Testwinrates(%)andstandarddeviationscomparedwithreward-freeimitationlearning
methods.
MapName BC MA-AIRL MADT MAPT MA-TREX LBI
1c3s5z 16.44¬±1.35 7.88¬±2.49 61.35¬±7.26 74.77¬±5.15 64.76¬±11.62 94.59¬±3.41
10m_vs_11m 26.19¬±4.42 41.69¬±7.12 82.76¬±4.41 66.85¬±9.28 48.78¬±11.28 90.45¬±6.99
2c_vs_64zg 17.37¬±10.12 24.75¬±10.83 61.90¬±5.74 58.28¬±7.84 22.45¬±7.74 71.44¬±8.83
3s_vs_5z 0.00¬±0.00 0.05¬±0.03 80.90¬±0.45 72.33¬±3.93 55.38¬±18.03 92.82¬±6.25
5m_vs_6m 13.78¬±2.15 11.59¬±6.75 79.78¬±4.98 56.01¬±3.17 50.01¬±14.87 87.98¬±5.10
6h_vs_8z 9.28¬±5.06 16.47¬±8.08 30.94¬±25.54 37.16¬±6.27 28.38¬±5.31 66.61¬±4.57
3s5z_vs_3s6z 0.00¬±0.00 0.00¬±0.00 27.44¬±9.49 34.90¬±6.84 36.16¬±3.68 83.34¬±4.27
corridor 0.00¬±0.00 0.76¬±0.15 69.85¬±1.54 45.91¬±15.47 30.59¬±9.86 87.45¬±2.94
MMM2 0.00¬±0.00 0.00¬±0.00 54.34¬±12.83 19.21¬±5.59 21.52¬±6.58 95.96¬±4.65
Table2: Testreturnandstandarddeviationscomparedwithofflinereinforcementlearningmethods.
MapName BCQ-MA CQL-MA ICQ OMAR OMIGA LBI
5m_vs_6m 9.13¬±0.21 10.15¬±0.15 9.47¬±0.45 8.76¬±0.52 10.38¬±0.50 18.96¬±0.56
2c_vs_64zg 18.86¬±0.35 19.20¬±1.25 18.47¬±0.25 17.10¬±0.94 19.25¬±0.38 20.45¬±0.25
6h_vs_8z 11.91¬±0.44 9.95¬±0.32 11.55¬±0.15 9.74¬±0.28 12.74¬±0.21 18.97¬±0.28
corridor 16.42¬±1.55 6.64¬±0.90 16.74¬±1.78 8.15¬±0.89 17.10¬±1.33 19.50¬±0.73
majorbottleneckforthesemethodsincludesalackofknowledgeofthedynamicsinformationand
therewardoverestimationforout-of-distributionstate-actionpairs. Weformulatetherewardmodel
as a bidirectional transformer to receive the whole trajectory as the input, making it possible to
solve non-Markovian rewards. In addition, we leverage the reward constraint and the behavior
regularizationtoperformin-samplelearningtoavoidrewardoverestimation. Theamountofexpert
demonstrationsintheseexistingstudiesisalsolimited,astheydonottreathindsightrelabelingvia
thetextualdescriptionasanexperttrajectorylikeinourwork.
4 Experiments
Inthissection,weconductempiricalexperimentstoanswerthefollowingquestions: (1)IsLearning
beforeInteraction(LBI)betterthantheexistingmulti-agentreinforcementlearning(MARL)methods
incomplexcooperativescenarios? (2)CanLBIgeneratelong-horizontrajectoriesandreasonable
rewardfunctionsatcriticalstates? (3)DoesLBIhavethezero-shotabilitytogeneralizetounseen
tasks? Then, we investigate the contribution of each component in the dynamics and the reward
model. WeprovidetheinformationoftrainingdatasetsandexperimentalsettingsinAppendixBand
D.Wealsodiscussthispaper‚ÄôsbroaderimpactsandlimitationsinAppendixA.1and A.2.
4.1 PerformanceComparison
Reward-freeOfflineLearning WecompareLBIwiththefollowingimitationlearningbaselines:
(1)BC:behaviorcloningthatimitatesthewholedatasets,(2)MA-AIRL(Yuetal.,2019): using
adversariallearningtoperformpolicyimitation,(3)MADT(Mengetal.,2023):utilizingtheDecision
Transformer(Chenetal.,2021)toperformsequencemodeling,(4)MA-TREX:inferingthereward
according to ranked demonstrations, the multi-agent version of TREX (Brown et al., 2019), (5)
MAPT (Zhu et al., 2024): infering the team rewards according to the preference return from a
well-trainedscriptedteacher.
AsshowninTable1,LBIoutperformsthebaselinesbyasignificantmarginonvariousmapswith
differentdifficultylevels,indicatingtheimportanceandeffectivenessoflearningrewardfunctions
viatheproposedworldmodel. Incontrast,BCandMA-AIRLfailtoachievesuccessratesinmost
tasksbecausetheyimitateallpastinteractionsequencesandcannotgeneralizeandavoidsub-optimal
solutions. MA-TREXandMAPThaveplateauedinperformancebecausetheyusetheaccumulated
rewards and the preference deduced by the scripted teacher to specify the quality of the training
data,respectively. MADTperformsbetterthanotherbaselinesbecauseDecisionTransformercanbe
thoughtofasperformingimitationlearningonasubsetofthedatawithacertainreturn.
OfflineMARL WealsocompareLBIwiththeexistingofflineMARLmethodswithground-truth
rewardsfromtheStarCraftMulti-AgentChallenge(SMAC),includingthemulti-agentversionof
7Table3: Testwinrates(%)andstandarddeviationsonunseentasks.
UnseenTask MADT MA-TREX LBI UnseenTask MADT MA-TREX LBI
1c3s 16.21¬±5.38 23.53¬±8.83 56.47¬±5.63 1c2s7z 6.16¬±3.09 5.69¬±3.81 28.26¬±6.41
6m 49.28¬±4.06 37.12¬±2.59 97.85¬±2.15 6m_vs_7m 73.45¬±7.22 32.88¬±4.47 81.07¬±5.17
1c_vs_32zg 2.08¬±1.51 11.41¬±3.41 58.33¬±6.44 3s4z 90.21¬±1.82 79.71¬±3.56 87.55¬±1.76
3s2z_vs_2s3z 0.00¬±0.00 9.16¬±5.62 18.22¬±2.46 3s5z_vs_3s7z 10.21¬±3.66 15.88¬±4.34 22.08¬±7.63
1c3s6z 16.41¬±6.44 58.09¬±3.41 65.38¬±5.12 9m_vs_11m 76.44¬±4.17 70.91¬±6.95 75.05¬±2.16
Table4:Theablationresultsforthedynamics Table5: Theablationresultsfortherewardmodel
modelwithoutresidualterm(wo-RT),image withoutrewardconstraint(wo-RC),behaviorregu-
reference(wo-IR),andusingground-truthim- larization(wo-BR),andusingground-truthrewards
age(GTI)asthereferenceforstateprediction. (w-GTR)providedbytheSMACbenchmark.
Algorithm Predictionerror Return(all) Algorithm Return(training) Return(unseen)
LBI 0.016¬±0.023 18.91¬±1.33 LBI 19.47¬±0.77 18.54¬±1.49
LBI-GTI 0.014¬±0.018 18.98¬±0.89 LBI-GTR 16.68¬±1.55 14.07¬±2.79
LBI-wo-RT 0.434¬±0.351 14.25¬±1.84 LBI-wo-RC 17.85¬±0.59 14.75¬±1.67
LBI-wo-IR 0.029¬±0.041 18.63¬±1.01 LBI-wo-BR 18.82¬±1.28 17.46¬±2.01
LBI-wo-RT&IR 0.744¬±1.164 12.13¬±2.33 LBI-wo-RC&BR 12.35¬±2.38 9.83¬±1.46
BCQ (Fujimoto et al., 2019) and CQL (Kumar et al., 2020) (namely BCQ-MA and CQL-MA),
ICQ(Yangetal.,2021),OMAR(Panetal.,2022),andOMIGA(Wangetal.,2024). Table2shows
that the performance of these offline MARL methods degrades dramatically with an increasing
numberofagentsandismuchlowerthanthatofLBI.Wehypothesizethatthereasonsforthisgap
are: (1) It is challenging and unnecessary to recover the Q-value based on the reward functions
providedbySMAC(thehit-pointdamagedealt)becausesuchrewarddesignisinefficientforlearning
optimalpolicy. (2)Thesemethodsmayintroducetoomuchconservatismandaffectthelearningof
theoptimalpolicy,astheconservativeupdateoftheout-of-distribution(OOD)suboptimalpolicythat
consistsofsomeagentstakingnon-optimalactionsandotherstakingoptimalwillinhibitthelearning
oftheagentsthattaketheoptimalactions.
OnlineMARL UsingaText-to-CodeConvertercangeneratescenarioswiththeoriginalgame
engineandthenlearnthejointpolicy. Therefore,wealsoconsiderthecomparisonwithonlineMARL
methodsincludingCW-QMIX(Rashidetal.,2020),QPLEX(Wangetal.,2020a),MAVEN(Mahajan
etal.,2019),EMC(Zhengetal.,2021),RODE(Wangetal.,2020c),QMIX(Rashidetal.,2018),
MAPPO(Yuetal.,2022). TheresultsinAppendixE.2showasignificantimprovementinthesample
efficiencyofLBIcomparedtotheonlineMARLmethod,suggestingthatthepre-trainedworldmodel
isnecessarytoreducethewaitingtimeforusesingeneratingresponses.
4.2 GeneralizationonUnseenTasks
Since zero-shot generalization ability is crucial forgenerating grounded answers for multi-agent
decision-makingproblems, wealsotestLBI‚Äôsabilitytogeneralizetoextensiveunseenscenarios
withoutretraining. Specifically,weevaluateourLBIandMADTonthetenunseentestingmaps,
varyingagentnumbers,actionspaces,andlevelsofenvironmentcomplexity. Table3showsthatLBI
consistentlyoutperformsMADTinunseenscenariosbyalargemargin,successfullytransferring
knowledgetonewtaskswithoutrequiringadditionalfine-tuning. Ithighlightsthatlearningareward
functionhasbetterzero-shotgeneralizationperformancethansimplepolicyadaptation.
4.3 Visualization
Thissectionevaluatesthedynamicsmodelasalong-horizonpolicy-conditionedpredictivemodel.
Figure 4 showcases examples of length-40 image trajectories generated by the dynamics model,
includingMMM2,3s_vs_5z,and5m_vs_6m. Wedonotobserveconspicuouscompoundingerrors
asthesingle-steppredictionmodeldoes,highlightingthatLBIhasconsistencyandlong-horizon
generationability. Inthecaseof5m_vs_6m,wepresentthefollowingframesaftertakingoneofthe
possibleactions,showingthatLBIcanalsoperformaction-controllablegeneration.
Wealsoinvestigatetherewardpredictionatacriticaljunctioninthestate-actionspacethatcantransit
tovariousstatesandsignificantlyinfluencethesuccessrateonthe5m_vs_6mtask. Atthemoment,
8Generated (MMM2)
t=0 t=5 t=10 t=15 t=20 t=25 t=30 t=35 t=40
Generated (3s_vs_5z)
t=0 t=5 t=10 t=15 t=20 t=25 t=30 t=35 t=40
Generated (5m_vs_6m)
Agent‚ë†move ‚Üê
‚ë†
Agent ‚ë† : low-health
Expert policy: leapfrog
‚ë°
‚ë† ‚ë° ‚ë¢ ‚ë£ ‚ë§ ‚ë• ‚ë¢ ‚ë§
‚ë†
‚ë† ‚ë† ‚ë° Agents ‚ë†-‚ë§attack ‚ë†
‚ë¢
‚ë£ ‚ë£
‚ë§
‚ë† ‚ë° ‚ë¢ ‚ë£ ‚ë§ ‚ë•
Figure4: Visualizationofthepredictionfromdynamicsandrewardmodel,where‚Äúnp-op‚Äùand‚Äús‚Äù
denoteno-operationandstopping,respectively.
theagentshavetolearntomicromanageleapfroggingtoachievegoodcoordination. Specifically,
Agent1hasalowhealthpointandmustmovebackwardtoavoidtheenemiesfocusingfireonit;
otherwise,theenemieswilleliminateAgent1immediatelyandweakenourscarceforces. InFigure4,
wevisualizethelearnedrewardfunctionofAgent1,wheretheactionspaceisno-operation,stopping,
movingincardinaldirections,andselectinganenemy‚Äôsidentitytoattack. Thelearnedrewardfor
movingtotheleftismuchhigherthantheotheractions,allowingonetolearntheoptimaljointpolicy
quickly. TherewardsprovidedbytheSMACbenchmarkdonotshowthisproperty,wheremultiple
MonteCarlosamplesarerequiredtofindthecorrectpolicybyestimatingtheexpectedreturn.
4.4 AblationStudy
In this section, we conduct ablation studies to analyze the contributions of each component in
the dynamics model and the reward model across five evaluation runs on four training maps
(6h_vs_8z, 3s5z_vs_3s6z, corridor, and MMM2) and four unseen maps (3s5z_vs_3s7z, 1c3s7z,
3s4z, 1c_vs_32zg). WeshowtheresultsofthedynamicsmodelinTable4. Usingthedynamics
residualtermisnecessarytoreducethepredictionerrorofthesubsequentstatesandobtaingood
performanceacrossalltrainingandunseentasks. Theimagereferenceisnotsoeffective,evenifwe
useground-truthimagesasthereference. However,sinceimagesaremorepowerfulinrepresenting
some situations than language or state information, we believe that the image serves as another
modalitytocorrectthepredictionofthestate. Wewouldleaveitforfuturework.
WedemonstratetheablationresultsoftherewardmodelinTable5. ComparedwithLBI-wo-RC&BR,
therewardconstraintandbehaviorregularizationtermcanimprovetheoverallperformanceonthe
trainingtasks. However,LBI-wo-BRperformsbetterthanLBI-wo-RConunseentasks,suggesting
that the conservatism for reward is more important than the policy when OOD state-action pairs
exist. The poor performance of LBI-w-GTR indicates that learning rewards from conditioned
demonstrations may be more accessible and valuable for policy updates than reconstructing the
pre-definedrewardsbyhumanexperts.
95 ConclusionandFutureWork
WeproposedLearningbeforeInteraction(LBI),anovelparadigmthatenablesgenerativemodelsto
groundtheiranswersformulti-agentdecision-makingproblemswithsimulationsbetweentheworld
andthemulti-agentsystem. Weformulateaninteractivesimulatorconsistingofdynamicsandreward
models,givensomestatesoftheworldandthetaskdescriptions,generatingtheconsequenceofthe
actionsintheformofimages,states,andrewards. Wehopetheideaofincludingsimulationsinthe
reasoningwillinstigatebroadinterestinapplyinggenerativemodelstoaidmachineintelligenceand
decision-making.
10References
GaonAn,SeungyongMoon,Jang-HyunKim,andHyunOhSong. Uncertainty-basedofflinereinforcement
learningwithdiversifiedq-ensemble. Advancesinneuralinformationprocessingsystems,34:7436‚Äì7447,
2021.
ChenjiaBai,LingxiaoWang,ZhuoranYang,Zhi-HongDeng,AnimeshGarg,PengLiu,andZhaoranWang.
Pessimisticbootstrappingforuncertainty-drivenofflinereinforcementlearning. InInternationalConference
onLearningRepresentations,2021.
MichaelBainandClaudeSammut. Aframeworkforbehaviouralcloning. InMachineIntelligence15,pp.
103‚Äì129,1995.
PaulBarde,JulienRoy,WonseokJeon,JoellePineau,ChrisPal,andDerekNowrouzezahrai. Adversarialsoft
advantagefitting:Imitationlearningwithoutpolicyoptimization. AdvancesinNeuralInformationProcessing
Systems,33:12334‚Äì12344,2020.
WendelinB√∂hmer,VitalyKurin,andShimonWhiteson. Deepcoordinationgraphs. InInternationalConference
onMachineLearning,pp.980‚Äì991.PMLR,2020.
AnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,XiChen,KrzysztofChoromanski,Tianli
Ding,DannyDriess,AvinavaDubey,ChelseaFinn,etal. Rt-2:Vision-language-actionmodelstransferweb
knowledgetoroboticcontrol. arXivpreprintarXiv:2307.15818,2023a.
AnthonyBrohan,YevgenChebotar,ChelseaFinn,KarolHausman,AlexanderHerzog,DanielHo,JulianIbarz,
AlexIrpan,EricJang,RyanJulian,etal. Doasican,notasisay:Groundinglanguageinroboticaffordances.
InConferenceonrobotlearning,pp.287‚Äì318.PMLR,2023b.
DanielBrown,WonjoonGoo,PrabhatNagarajan,andScottNiekum. Extrapolatingbeyondsuboptimaldemon-
strationsviainversereinforcementlearningfromobservations. InInternationalconferenceonmachine
learning,pp.783‚Äì792.PMLR,2019.
JakeBruce,MichaelDennis,AshleyEdwards,JackParker-Holder,YugeShi,EdwardHughes,MatthewLai,
AditiMavalankar,RichieSteigerwald,ChrisApps,etal. Genie:Generativeinteractiveenvironments. arXiv
preprintarXiv:2402.15391,2024.
JacobBuckman,DanijarHafner,GeorgeTucker,EugeneBrevdo,andHonglakLee. Sample-efficientreinforce-
mentlearningwithstochasticensemblevalueexpansion. Advancesinneuralinformationprocessingsystems,
31,2018.
AlexJChanandMvanderSchaar.Scalablebayesianinversereinforcementlearning.InInternationalConference
onLearningRepresentations,2021.
LiliChen,KevinLu,AravindRajeswaran,KiminLee,AdityaGrover,MishaLaskin,PieterAbbeel,Aravind
Srinivas,andIgorMordatch.Decisiontransformer:Reinforcementlearningviasequencemodeling.Advances
inneuralinformationprocessingsystems,34:15084‚Äì15097,2021.
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song.
Diffusionpolicy:Visuomotorpolicylearningviaactiondiffusion. arXivpreprintarXiv:2303.04137,2023.
KurtlandChua,RobertoCalandra,RowanMcAllister,andSergeyLevine. Deepreinforcementlearningina
handfuloftrialsusingprobabilisticdynamicsmodels. Advancesinneuralinformationprocessingsystems,
31,2018.
IgnasiClavera,YaoFu,andPieterAbbeel. Model-augmentedactor-critic:Backpropagatingthroughpaths. In
InternationalConferenceonLearningRepresentations,2019.
AbhishekDas,Th√©ophileGervet,JoshuaRomoff,DhruvBatra,DeviParikh,MikeRabbat,andJoellePineau.
Tarmac: Targeted multi-agent communication. In International Conference on Machine Learning, pp.
1538‚Äì1546.PMLR,2019.
DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,AyzaanWahid,
JonathanTompson,QuanVuong,TianheYu,etal. Palm-e:Anembodiedmultimodallanguagemodel. In
InternationalConferenceonMachineLearning,pp.8469‚Äì8488.PMLR,2023.
Rasool Fakoor, Jonas W Mueller, Kavosh Asadi, Pratik Chaudhari, and Alexander J Smola. Continuous
doublyconstrainedbatchreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,34:
11260‚Äì11273,2021.
11VladimirFeinberg,AlvinWan,IonStoica,MichaelIJordan,JosephEGonzalez,andSergeyLevine. Model-
basedvalueestimationforefficientmodel-freereinforcementlearning. arXivpreprintarXiv:1803.00101,
2018.
Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From language to goals: Inverse
reinforcementlearningforvision-basedinstructionfollowing. InInternationalConferenceonLearning
Representations,2018a.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse reinforcement
learning. InInternationalConferenceonLearningRepresentations,2018b.
ScottFujimotoandShixiangShaneGu. Aminimalistapproachtoofflinereinforcementlearning. Advancesin
neuralinformationprocessingsystems,34:20132‚Äì20145,2021.
ScottFujimoto,DavidMeger,andDoinaPrecup. Off-policydeepreinforcementlearningwithoutexploration.
InInternationalconferenceonmachinelearning,pp.2052‚Äì2062.PMLR,2019.
DivyanshGarg,ShuvamChakraborty,ChrisCundy,JiamingSong,andStefanoErmon. Iq-learn:Inversesoft-q
learningforimitation. AdvancesinNeuralInformationProcessingSystems,34:4028‚Äì4039,2021.
SeyedKamyarSeyedGhasemipour,RichardZemel,andShixiangGu. Adivergenceminimizationperspective
onimitationlearningmethods. InConferenceonrobotlearning,pp.1259‚Äì1277.PMLR,2020.
TuomasHaarnoja,HaoranTang,PieterAbbeel,andSergeyLevine. Reinforcementlearningwithdeepenergy-
basedpolicies. InInternationalconferenceonmachinelearning,pp.1352‚Äì1361.PMLR,2017.
DanijarHafner,TimothyLillicrap,JimmyBa,andMohammadNorouzi. Dreamtocontrol:Learningbehaviors
bylatentimagination. InInternationalConferenceonLearningRepresentations,2019a.
DanijarHafner,TimothyLillicrap,IanFischer,RubenVillegas,DavidHa,HonglakLee,andJamesDavidson.
Learninglatentdynamicsforplanningfrompixels. InInternationalconferenceonmachinelearning,pp.
2555‚Äì2565.PMLR,2019b.
DanijarHafner,TimothyPLillicrap,MohammadNorouzi,andJimmyBa. Masteringatariwithdiscreteworld
models. InInternationalConferenceonLearningRepresentations,2020.
DanijarHafner,JurgisPasukonis,JimmyBa,andTimothyLillicrap. Masteringdiversedomainsthroughworld
models. arXivpreprintarXiv:2301.04104,2023.
NicklasHansen,HaoSu,andXiaolongWang. TD-MPC2:Scalable,robustworldmodelsforcontinuouscontrol.
InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
NicklasAHansen,HaoSu,andXiaolongWang. Temporaldifferencelearningformodelpredictivecontrol. In
InternationalConferenceonMachineLearning,pp.8387‚Äì8406.PMLR,2022.
Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning
continuouscontrolpoliciesbystochasticvaluegradients. Advancesinneuralinformationprocessingsystems,
28,2015.
JonathanHoandStefanoErmon. Generativeadversarialimitationlearning. Advancesinneuralinformation
processingsystems,29,2016.
AnthonyHu,LloydRussell,HudsonYeo,ZakMurez,GeorgeFedoseev,AlexKendall,JamieShotton,andGian-
lucaCorrado. Gaia-1:Agenerativeworldmodelforautonomousdriving. arXivpreprintarXiv:2309.17080,
2023.
MichaelJanner,JustinFu,MarvinZhang,andSergeyLevine. Whentotrustyourmodel:Model-basedpolicy
optimization. Advancesinneuralinformationprocessingsystems,32,2019.
LukaszKaiser,MohammadBabaeizadeh,PiotrMilos,BlazejOsinski,RoyHCampbell,KonradCzechowski,
DumitruErhan,ChelseaFinn,PiotrKozakowski,SergeyLevine,etal. Modelbasedreinforcementlearning
foratari. InInternationalConferenceonLearningRepresentations,2019.
Woojun Kim, Jongeui Park, and Youngchul Sung. Communication in multi-agent reinforcement learning:
Intentionsharing. InInternationalConferenceonLearningRepresentations,2020.
AndreasK√∂pf,YannicKilcher,DimitrivonR√ºtte,SotirisAnagnostidis,ZhiRuiTam,KeithStevens,Abdullah
Barhoum,DucNguyen,OliverStanley,Rich√°rdNagyfi,etal. Openassistantconversations-democratizing
largelanguagemodelalignment. AdvancesinNeuralInformationProcessingSystems,36,2024.
12IlyaKostrikov,AshvinNair,andSergeyLevine. Offlinereinforcementlearningwithimplicitq-learning. In
InternationalConferenceonLearningRepresentations,2021.
LandonKraemerandBikramjitBanerjee. Multi-agentreinforcementlearningasarehearsalfordecentralized
planning. Neurocomputing,190:82‚Äì94,2016.
JakubGrudzienKuba,RuiqingChen,MunningWen,YingWen,FangleiSun,JunWang,andYaodongYang.
Trustregionpolicyoptimisationinmulti-agentreinforcementlearning. arXivpreprintarXiv:2109.11251,
2021.
AviralKumar,AurickZhou,GeorgeTucker,andSergeyLevine.Conservativeq-learningforofflinereinforcement
learning. AdvancesinNeuralInformationProcessingSystems,33:1179‚Äì1191,2020.
Hang Lai, Jian Shen, Weinan Zhang, and Yong Yu. Bidirectional model-based policy optimization. In
Internationalconferenceonmachinelearning,pp.5618‚Äì5627.PMLR,2020.
MartinLauer. Analgorithmfordistributedreinforcementlearningincooperativemultiagentsystems. InProc.
17thInternationalConf.onMachineLearning,2000.
Kuang-HueiLee,OfirNachum,MengjiaoSherryYang,LisaLee,DanielFreeman,SergioGuadarrama,Ian
Fischer,WinnieXu,EricJang,HenrykMichalewski,etal. Multi-gamedecisiontransformers. Advancesin
NeuralInformationProcessingSystems,35:27921‚Äì27936,2022.
SergeyLevineandVladlenKoltun. Guidedpolicysearch. InInternationalconferenceonmachinelearning,pp.
1‚Äì9.PMLR,2013.
MinghuanLiu,TairanHe,MinkaiXu,andWeinanZhang. Energy-basedimitationlearning. InProceedingsof
the20thInternationalConferenceonAutonomousAgentsandMultiAgentSystems,pp.809‚Äì817,2021.
Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and
AndrewMDai. Mind‚Äôseye: Groundedlanguagemodelreasoningthroughsimulation. InTheEleventh
InternationalConferenceonLearningRepresentations,2022.
YongLiu,WeixunWang,YujingHu,JianyeHao,XingguoChen,andYangGao. Multi-agentgameabstraction
viagraphattentionneuralnetwork. InProceedingsoftheAAAIConferenceonArtificialIntelligence,2020.
ZeyangLiu,LipengWan,XueSui,ZhuoranChen,KewuSun,andXuguangLan. Deephierarchicalcommunica-
tiongraphinmulti-agentreinforcementlearning. InProceedingsoftheThirty-SecondInternationalJoint
ConferenceonArtificialIntelligence,pp.208‚Äì216,2023.
ZeyangLiu,LipengWan,XinruiYang,ZhuoranChen,XingyuChen,andXuguangLan. Imagine,initialize,and
explore:Aneffectiveexplorationmethodinmulti-agentreinforcementlearning. ProceedingsoftheAAAI
ConferenceonArtificialIntelligence,38(16):17487‚Äì17495,Mar.2024. doi:10.1609/aaai.v38i16.29698.
Fan-Ming Luo, Tian Xu, Xingchen Cao, and Yang Yu. Reward-consistent dynamics models are strongly
generalizable for offline reinforcement learning. In The Twelfth International Conference on Learning
Representations,2023.
YupingLuo,HuazheXu,YuanzhiLi,YuandongTian,TrevorDarrell,andTengyuMa. Algorithmicframework
formodel-baseddeepreinforcementlearningwiththeoreticalguarantees. InInternationalConferenceon
LearningRepresentations,2018.
JiafeiLyu,XiaotengMa,XiuLi,andZongqingLu. Mildlyconservativeq-learningforofflinereinforcement
learning. AdvancesinNeuralInformationProcessingSystems,35:1711‚Äì1724,2022.
AnujMahajan,TabishRashid,MikayelSamvelyan,andShimonWhiteson. Maven: Multi-agentvariational
exploration. AdvancesinNeuralInformationProcessingSystems,32,2019.
AnujMahajan,MikayelSamvelyan,LeiMao,ViktorMakoviychuk,AnimeshGarg,JeanKossaifi,ShimonWhite-
son,YukeZhu,andAnimashreeAnandkumar. Tesseract:Tensorisedactorsformulti-agentreinforcement
learning. InInternationalConferenceonMachineLearning,pp.7301‚Äì7312.PMLR,2021.
La√´titiaMatignon, GuillaumeJLaurent, andNadineLeFort-Piat. Hystereticq-learning: analgorithmfor
decentralizedreinforcementlearningincooperativemulti-agentteams.InIEEE/RSJInternationalConference
onIntelligentRobotsandSystems,pp.64‚Äì69.IEEE,2007.
LinghuiMeng,MuningWen,ChenyangLe,XiyunLi,DengpengXing,WeinanZhang,YingWen,Haifeng
Zhang, JunWang, YaodongYang, etal. Offlinepre-trainedmulti-agentdecisiontransformer. Machine
IntelligenceResearch,20(2):233‚Äì248,2023.
13OfirNachum,YinlamChow,BoDai,andLihongLi. Dualdice: Behavior-agnosticestimationofdiscounted
stationarydistributioncorrections. Advancesinneuralinformationprocessingsystems,32,2019.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for
model-baseddeepreinforcementlearningwithmodel-freefine-tuning.In2018IEEEinternationalconference
onroboticsandautomation(ICRA),pp.7559‚Äì7566.IEEE,2018.
AndrewYNgandStuartJRussell. Algorithmsforinversereinforcementlearning. InProceedingsofthe
SeventeenthInternationalConferenceonMachineLearning,pp.663‚Äì670,2000.
FransAOliehoekandChristopherAmato. AconciseintroductiontodecentralizedPOMDPs. Springer,2016.
FransAOliehoek, MatthijsTJSpaan, andNikosVlassis. Optimalandapproximateq-valuefunctionsfor
decentralizedpomdps. JournalofArtificialIntelligenceResearch,32:289‚Äì353,2008.
AbhishekPadalkar,AcornPooley,AjinkyaJain,AlexBewley,AlexHerzog,AlexIrpan,AlexanderKhazatsky,
AnantRai,AnikaitSingh,AnthonyBrohan,etal. Openx-embodiment:Roboticlearningdatasetsandrt-x
models. arXivpreprintarXiv:2310.08864,2023.
FeiyangPan,JiaHe,DandanTu,andQingHe. Trustthemodelwhenitisconfident: Maskedmodel-based
actor-critic. Advancesinneuralinformationprocessingsystems,33:10537‚Äì10546,2020.
LingPan,LongboHuang,TengyuMa,andHuazheXu. Planbetteramidconservatism: Offlinemulti-agent
reinforcementlearningwithactorrectification. InInternationalconferenceonmachinelearning,pp.17221‚Äì
17237.PMLR,2022.
EmilioParisottoandRussSalakhutdinov. Efficienttransformersinreinforcementlearningusingactor-learner
distillation. InInternationalConferenceonLearningRepresentations,2021.
EmilioParisotto,FrancisSong,JackRae,RazvanPascanu,CaglarGulcehre,SiddhantJayakumar,MaxJaderberg,
RaphaelLopezKaufman,AidanClark,SebNoury,etal. Stabilizingtransformersforreinforcementlearning.
InInternationalconferenceonmachinelearning,pp.7487‚Äì7498.PMLR,2020.
TimPearce,TabishRashid,AnssiKanervisto,DaveBignell,MingfeiSun,RalucaGeorgescu,SergioValcarcel
Macua,ShanZhengTan,IdaMomennejad,KatjaHofmann,etal. Imitatinghumanbehaviourwithdiffusion
models. InTheEleventhInternationalConferenceonLearningRepresentations,2022.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon
Whiteson. Qmix:Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning. In
InternationalConferenceonMachineLearning,pp.4295‚Äì4304.PMLR,2018.
TabishRashid,GregoryFarquhar,BeiPeng,andShimonWhiteson. Weightedqmix: Expandingmonotonic
valuefunctionfactorisationfordeepmulti-agentreinforcementlearning. Advancesinneuralinformation
processingsystems,33:10199‚Äì10210,2020.
ScottReed,KonradZolna,EmilioParisotto,SergioGomezColmenarejo,AlexanderNovikov,GabrielBarth-
Maron,MaiGimenez,YurySulsky,JackieKay,JostTobiasSpringenberg,etal. Ageneralistagent. arXiv
preprintarXiv:2205.06175,2022.
PaulRolland,LucaViano,NormanSch√ºrhoff,BorisNikolov,andVolkanCevher. Identifiabilityandgeneraliz-
abilityfrommultipleexpertsininversereinforcementlearning. AdvancesinNeuralInformationProcessing
Systems,35:550‚Äì564,2022.
St√©phaneRoss,GeoffreyGordon,andDrewBagnell. Areductionofimitationlearningandstructuredprediction
tono-regretonlinelearning.InProceedingsofthefourteenthinternationalconferenceonartificialintelligence
andstatistics,pp.627‚Äì635.JMLRWorkshopandConferenceProceedings,2011.
MikayelSamvelyan,TabishRashid,ChristianSchroederDeWitt,GregoryFarquhar,NantasNardelli,TimGJ
Rudner,Chia-ManHung,PhilipHSTorr,JakobFoerster,andShimonWhiteson. Thestarcraftmulti-agent
challenge. arXivpreprintarXiv:1902.04043,2019.
JulianSchrittwieser,IoannisAntonoglou,ThomasHubert,KarenSimonyan,LaurentSifre,SimonSchmitt,
ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,etal. Masteringatari,go,chessandshogi
byplanningwithalearnedmodel. Nature,588(7839):604‚Äì609,2020.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicyoptimization
algorithms. arXivpreprintarXiv:1707.06347,2017.
14JianzhunShao,YunQu,ChenChen,HongchangZhang,andXiangyangJi. Counterfactualconservativeq
learningforofflinemulti-agentreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,
36,2024.
DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriessche,Julian
Schrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal. Masteringthegameofgowith
deepneuralnetworksandtreesearch. nature,529(7587):484‚Äì489,2016.
KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYungYi. Qtran:Learningtofactorize
withtransformationforcooperativemulti-agentreinforcementlearning. InInternationalConferenceon
MachineLearning,pp.5887‚Äì5896.PMLR,2019.
JiamingSong,HongyuRen,DorsaSadigh,andStefanoErmon. Multi-agentgenerativeadversarialimitation
learning. Advancesinneuralinformationprocessingsystems,31,2018.
AravindSrinivas,AllanJabri,PieterAbbeel,SergeyLevine,andChelseaFinn. Universalplanningnetworks:
Learninggeneralizable representationsforvisuomotor control. In Internationalconferenceon machine
learning,pp.4732‚Äì4741.PMLR,2018.
PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,MaxJaderberg,
MarcLanctot,NicolasSonnerat,JoelZLeibo,KarlTuyls,etal. Value-decompositionnetworksforcooper-
ativemulti-agentlearningbasedonteamreward. InProceedingsofthe17thInternationalConferenceon
AutonomousAgentsandMultiAgentSystems,pp.2085‚Äì2087,2018.
RichardSSutton. Integratedarchitecturesforlearning,planning,andreactingbasedonapproximatingdynamic
programming. InMachinelearningproceedings1990,pp.216‚Äì224.Elsevier,1990.
RichardSSutton,ARupamMahmood,andMarthaWhite. Anemphaticapproachtotheproblemofoff-policy
temporal-differencelearning. JournalofMachineLearningResearch,17(73):1‚Äì29,2016.
GokulSwamy,SiddharthReddy,SergeyLevine,andAncaDDragan. Scaledautonomy: Enablinghuman
operatorstocontrolrobotfleets. In2020IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
pp.5942‚Äì5948.IEEE,2020.
YuTakagiandShinjiNishimoto. High-resolutionimagereconstructionwithlatentdiffusionmodelsfromhuman
brainactivity. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.
14453‚Äì14463,2023.
DenisTarasov,VladislavKurenkov,AlexanderNikulin,andSergeyKolesnikov. Revisitingtheminimalist
approachtoofflinereinforcementlearning. AdvancesinNeuralInformationProcessingSystems,36,2024.
EmanuelTodorov,TomErez,andYuvalTassa. Mujoco:Aphysicsengineformodel-basedcontrol. In2012
IEEE/RSJinternationalconferenceonintelligentrobotsandsystems,pp.5026‚Äì5033.IEEE,2012.
Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural
informationprocessingsystems,30,2017.
JianhaoWang,ZhizhouRen,TerryLiu,YangYu,andChongjieZhang. Qplex: Duplexduelingmulti-agent
q-learning. InInternationalConferenceonLearningRepresentations,2020a.
TonghanWang,JianhaoWang,ChongyiZheng,andChongjieZhang. Learningnearlydecomposablevalue
functionsviacommunicationminimization. InInternationalConferenceonLearningRepresentations,2019.
TonghanWang,HengDong,VictorLesser,andChongjieZhang. Roma:Multi-agentreinforcementlearning
withemergentroles. arXivpreprintarXiv:2003.08039,2020b.
TonghanWang,TarunGupta,AnujMahajan,BeiPeng,ShimonWhiteson,andChongjieZhang.Rode:Learning
rolestodecomposemulti-agenttasks. arXivpreprintarXiv:2010.01523,2020c.
XiangsenWang,HaoranXu,YinanZheng,andXianyuanZhan. Offlinemulti-agentreinforcementlearningwith
implicitglobal-to-localvalueregularization. AdvancesinNeuralInformationProcessingSystems,36,2024.
XiyaoWang,RuijieZheng,YanchaoSun,RuonanJia,WichayapornWongkamjan,HuazheXu,andFurong
Huang. Coplanner:Plantorolloutconservativelybuttoexploreoptimisticallyformodel-basedrl. InThe
TwelfthInternationalConferenceonLearningRepresentations,2023.
ErmoWeiandSeanLuke. Lenientlearninginindependent-learnerstochasticcooperativegames. TheJournalof
MachineLearningResearch,17(1):2914‚Äì2955,2016.
15MuningWen, JakubGrudzienKuba, RunjiLin, WeinanZhang, YingWen, JunWang, andYaodongYang.
Multi-agentreinforcementlearningisasequencemodelingproblem. arXivpreprintarXiv:2205.14953,2022.
YueWu,ShuangfeiZhai,NitishSrivastava,JoshuaMSusskind,JianZhang,RuslanSalakhutdinov,andHanlin
Goh. Uncertaintyweightedactor-criticforofflinereinforcementlearning. InInternationalConferenceon
MachineLearning,pp.11319‚Äì11328.PMLR,2021.
Yueh-Hua Wu, Xiaolong Wang, and Masashi Hamaya. Elastic decision transformer. Advances in Neural
InformationProcessingSystems,36,2024.
ZeqiuWu,YushiHu,WeijiaShi,NouhaDziri,AlaneSuhr,PrithvirajAmmanabrolu,NoahASmith,Mari
Ostendorf,andHannanehHajishirzi. Fine-grainedhumanfeedbackgivesbetterrewardsforlanguagemodel
training. InA.Oh,T.Neumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine(eds.),Advancesin
NeuralInformationProcessingSystems,volume36,pp.59008‚Äì59033.CurranAssociates,Inc.,2023.
ShushengXu,HuaijieWang,andYiWu. Groundedreinforcementlearning:Learningtowinthegameunder
humancommands. AdvancesinNeuralInformationProcessingSystems,35:7504‚Äì7519,2022.
SherryYang,YilunDu,SeyedKamyarSeyedGhasemipour,JonathanTompson,LesliePackKaelbling,Dale
Schuurmans,andPieterAbbeel. Learninginteractivereal-worldsimulators. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024.
YaodongYang,JianyeHao,GuangyongChen,HongyaoTang,YingfengChen,YujingHu,ChangjieFan,and
ZhongyuWei. Q-valuepathdecompositionfordeepmultiagentreinforcementlearning. InInternational
ConferenceonMachineLearning,pp.10706‚Äì10715.PMLR,2020a.
YaodongYang,JianyeHao,BenLiao,KunShao,GuangyongChen,WulongLiu,andHongyaoTang. Qatten:
Ageneralframeworkforcooperativemultiagentreinforcementlearning. arXivpreprintarXiv:2002.03939,
2020b.
YiqinYang,XiaotengMa,ChenghaoLi,ZewuZheng,QiyuanZhang,GaoHuang,JunYang,andQianchuan
Zhao. Believewhatyousee: Implicitconstraintapproachforofflinemulti-agentreinforcementlearning.
AdvancesinNeuralInformationProcessingSystems,34:10299‚Äì10312,2021.
ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu. Thesurprising
effectivenessofppoincooperativemulti-agentgames. AdvancesinNeuralInformationProcessingSystems,
35:24611‚Äì24624,2022.
LantaoYu, JiamingSong, andStefanoErmon. Multi-agentadversarialinversereinforcementlearning. In
InternationalConferenceonMachineLearning,pp.7194‚Äì7201.PMLR,2019.
ShengYue,GuanboWang,WeiShao,ZhaofengZhang,SenLin,JuRen,andJunshanZhang.Clare:Conservative
model-based reward learning for offline inverse reinforcement learning. In The Eleventh International
ConferenceonLearningRepresentations,2022.
AndreaZanette,MartinJWainwright,andEmmaBrunskill. Provablebenefitsofactor-criticmethodsforoffline
reinforcementlearning. Advancesinneuralinformationprocessingsystems,34:13626‚Äì13640,2021.
SiliangZeng,ChenliangLi,AlfredoGarcia,andMingyiHong. Maximum-likelihoodinversereinforcement
learningwithfinite-timeguarantees. AdvancesinNeuralInformationProcessingSystems,35:10122‚Äì10135,
2022.
SiliangZeng,ChenliangLi,AlfredoGarcia,andMingyiHong. Whendemonstrationsmeetgenerativeworld
models:Amaximumlikelihoodframeworkforofflineinversereinforcementlearning. AdvancesinNeural
InformationProcessingSystems,36,2024.
LuluZheng,JiaruiChen,JianhaoWang,JiaminHe,YujingHu,YingfengChen,ChangjieFan,YangGao,and
ChongjieZhang. Episodicmulti-agentreinforcementlearningwithcuriosity-drivenexploration. Advancesin
NeuralInformationProcessingSystems,34,2021.
LiZhouandKevinSmall. Inversereinforcementlearningwithnaturallanguagegoals. InProceedingsofthe
AAAIConferenceonArtificialIntelligence,volume35-12,pp.11116‚Äì11124,2021.
BanghuaZhu,MichaelJordan,andJiantaoJiao. Principledreinforcementlearningwithhumanfeedbackfrom
pairwiseork-wisecomparisons.InInternationalConferenceonMachineLearning,pp.43037‚Äì43067.PMLR,
2023.
TianchenZhu,YueQiu,HaoyiZhou,andJianxinLi. Decodingglobalpreferences:Temporalandcooperative
dependencymodelinginmulti-agentpreference-basedreinforcementlearning. InProceedingsoftheAAAI
ConferenceonArtificialIntelligence,volume38-15,pp.17202‚Äì17210,2024.
16DanielMZiegler,NisanStiennon,JeffreyWu,TomBBrown,AlecRadford,DarioAmodei,PaulChristiano,and
GeoffreyIrving. Fine-tuninglanguagemodelsfromhumanpreferences. arXivpreprintarXiv:1909.08593,
2019.
17A BroaderImpactsandLimitations
A.1 BroaderImpacts
LearningbeforeInteractionprovidesgroundedanswerstocomplexmulti-agentdecision-making
problemsthroughthegenerationofsimulatorsandtrial-and-errorlearning. Thiscanbenefitthose
seekingtomakedecisionsthroughlong-termplanning. Withsignificanttechnologicaladvancements,
exploringtheuseofthistechnologymaybecrucialforenhancingexistinghumandecision-making
capabilities. For instance, negotiators could describe the opponent‚Äôs personality traits and their
decision-makinglimitstogeneratebetternegotiationstrategies.
Atthesametime,werecognizethatcurrentgenerativesimulatorsstillcannotreliablygeneratestate
transitionsacrossmultipledomains,andlearningjointmulti-agentstrategiesstillfacesconvergence
difficulties. Therefore,LearningbeforeInteractionmayleadtoincorrectdecisionsinspecificfields.
Ifhumansintentionallyfollowthegeneratedanswersinsteadofusingthemasreferences,itcould
leadtounsafeorworseconsequences. Ontheotherhand,itcouldalsohavenegativeimpactswhen
LearningbeforeInteractionismisusedinharmfulapplicationsifthegeneratedenvironmentsand
answersaresufficientlyaccurate.
A.2 Limitations
Althoughwehavealreadyseensignificantimprovementsinreasoningcapabilitiesforcomplexmulti-
agenttaskswithLearningbeforeInteraction,performancemaybeaffectedbythesimulator‚Äôsaccuracy
andthemulti-agentpolicylearningperformance. Unqualifiedsimulatorsanddifficult-to-converge
multi-agentpoliciesmayleadtoerroneoussimulationresults,whichcouldbemoremisleadingthan
thevagueanswersgeneratedbyexistingvisuallanguagemodels. Forexample, theworldmodel
haslimitedout-of-domaingeneralizationfordomainsthatarenotrepresentedinthetrainingdata,
e.g.,unseenunittypes. Furtherscalinguptrainingdatacouldhelp,astheparsercanquicklyand
automaticallygenerateimagesbasedonagivenstate.
Whilethelearnedrewardfunctionscanenhancethespeedofmulti-agentpolicylearningcompared
to other inverse reinforcement learning and online interaction learning methods, it still requires
considerablewaitingtimetoobtainaconvergedpolicyandthefinalanswer. Suchlongwaitingtime
isunacceptableinapplicationsrequiringreal-timefeedback,suchaschatbots. Onepossiblesolution
istoreplacemulti-agentreinforcementlearningwithplanningmethodsbasedonthelearnedrewards
anddynamicsmodels,therebyacceleratingthereasoningprocess. Wewillleavethisissueinfuture
work.
Inaddition,thispaperisconfinedtoscenarioswithinthegameStarCraftII.Thisisanenvironment
that,whilecomplex,cannotrepresentthedynamicsofallmulti-agenttasks. Evaluationofmulti-agent
reinforcementlearningalgorithms,therefore,shouldnotbelimitedtoonebenchmarkbutshould
targetavarietywitharangeoftasks.
B DatasetPreparation
Thetrainingmapsinclude3s5z,1c3s5z,10m_vs_11m,2c_vs_64zg,3s_vs_5z,5m_vs_6m,6h_vs_8z,
3s5z_vs_3s6z, corridor, MMM2 in StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al.,
2019). We use EMC (Zheng et al., 2021) and IIE (Liu et al., 2024) to collect 50000 trajectories
foreachmapandsavethesedataasNPYfiles. Thedataincludesthestates,theobservations,the
terminatedsignals, theactions, theavailableactions, andtherewards. Thereturndistributionon
trainingmapsisshowninTable6. Theaveragereturnis19.64¬±1.63acrosstentrainingmaps.
InFigure6,wehavepresentedthewholeprocedureofconvertingastatevectorintoanimagefor
simulationandparsingatrajectorytoproduceatextualtaskdescription. First,asshowninFigure5,
we collect the element images that appear in the game and affect the state, including units and
backgroundterrainsoftrainingmaps.
Givenamulti-agentsystemanditsinteractiontrajectory,theparserreadspredefinedmapinformation,
suchasthenumberandracesofagentsandenemies. Then,theparserconvertstheoriginalstate
informationintostructuredinformation,readingagents‚Äôandenemies‚Äôpositionsandhealthpoints. It
18MapName ReturnDistribution MapName ReturnDistribution
3s5z 19.43¬±1.86 5m_vs_6m 19.83¬±2.16
1c3s5z 19.66¬±1.25 6h_vs_8z 18.84¬±2.09
10m_vs_11m 19.75¬±1.03 3s5z_vs_3s6z 19.76¬±1.26
2c_vs_64zg 19.98¬±0.71 corridor 19.69¬±1.48
3s_vs_5z 19.88¬±1.40 MMM2 19.63¬±2.07
Table6: Returndistributionontrainingmaps.
Space Square Jungle Space Octagon Redstone Space Bridge Space Terrain
Unit name Ally Enemy Unit name Ally Enemy
C(olossus) S(talker)
H(ydralisk) Z(ealot)
M(aine) Zerg(ling)
Med(ivac) Mar(auder)
Figure5: Imagesofunitsandterrains.
willgeneratethecorrespondinginteractionscenesbasedonpre-collectedunitimages. InFigure6,
wecanseethattheimagegeneratedbytheparserresemblesthatintheoriginalreplay(subsample
aframebyrunningaSC2REPLAYfilewithintheStarCraftIIclient). Finally,theparserreadsthe
laststateofthetrajectoryandextractstheremaininghealthpointsofbothsides. Wecanobtainthe
finaltaskdescriptionbyfillinginpredefineddescriptiontemplates(e.g.,‚ÄúConsiderthatwecontrol
{numberofagents}{agentraces}ontheleft.‚Äù) andaddingconnectingwords(e.g.,‚ÄúWhatplanshould
weuse‚Äù).
C StarCraftMulti-agentChallenge
StarCraftIIisareal-timestrategygamefeaturingthreedifferentraces,Protoss,Terran,andZerg,
withdifferentpropertiesandassociatedstrategies. Theobjectiveistobuildanarmypowerfulenough
todestroytheenemy‚Äôsbase. Whenbattlingtwoarmies,playersmustensurearmyunitsareacting
optimally.StarCraftMulti-AgentChallenge(SMAC)(Samvelyanetal.,2019)isapartiallyobservable
reinforcementlearningbenchmarkbuiltinStarCraftII.Anindividualagentwithparametersharing
controlseachalliedunit,andahand-codedbuilt-inStarCraftIIAIcontrolsenemyunits.Thedifficulty
ofthegameAIissettothe‚Äúverydifficult‚Äùlevel.
On the SMAC benchmark, agents can access their local observations within the field of view at
eachtimestep. Thefeaturevectorcontainsattributesofbothalliedandenemyunits: distance,
relative x,relative y,health,shield,andunit_type. Inaddition,agentscanobservethe
lastactionsofalliedunitsandtheterrainfeaturessurroundingthem. Theglobalstatevectorincludes
thecoordinatesofallagentsrelativetothecenterofthemapandotherfeaturespresentinthelocal
observation of agents. The state stores the energy of Medivacs, the cooldown of the rest of the
alliedunits,andthelastactionsofallagents. Notethattheglobalstateinformationisonlyavailable
to agents during centralized training. All features in state and local observations are normalized
bytheir maximumvalues. Afterreceivingtheobservations, eachagentis allowedto takeaction
from a discrete set which consists of move[direction], attack[enemy_id], stop and no-op.
19‚ë¢Place agents at corresponding positions
‚ë£Draw agents‚Äô health bars
‚ë†Load the map and unit races Image in the original replay (SC2REPLAY) Generated image by the parser
Task description
(Win) Consider that we control {number of agents} {agent races} on the left. What plan
should we use to completely destroy {number of enemies} {enemy race} on the right?
(Win) Let's assume we are managing {number of agents} {agent races} on the left. What
method should we take to fully eliminate the {number of enemies} {enemy race} on the
right?
(Loss) Imagine we are leading {number of agents} {agent races} on the left to against
{number of enemies} {enemy race}. What approach should we take to achieve a total
remaining enemy health of {the sum of the remaining health points of enemies} and ours
is {the sum of the remaining health points of agents}?
‚ë°Read the state information ‚ë§Generate the task description
Positions and remaining health points The remaining health points at the last state
Figure6: Thewholepipelineofhowtheparsergeneratestheimageandthetaskdescriptionfora
givenstate. Here,weonlyshowthreetaskdescriptionstheparserproducesfordemopurposes.
Movedirectionincludesnorth,south,east,andwest. Notethatthedeadagentscanonlytakeno-op
actionwhileliveagentscannot. Forhealthunits,Medivacsuseheal[agent_id]actionsinsteadof
attack[enemy_id].
Dependingondifferentscenarios,themaximumnumberofactionsvariesbetween7and70. Note
thatagentscanonlyperformtheattack[enemy_id]actionwhentheenemyiswithinitsshooting
range. Ateachtimestep,agentstakejointactionandreceiveapositiveglobalrewardbasedonthe
totaldamagedealttotheenemyunits. Inaddition,theycanreceiveanextrarewardof10pointsafter
killingeachenemyunitand200pointsafterkillingallenemyunits. Therewardsarescaledtoaround
20,sothemaximumcumulativerewardisachievableineachscenario.
D ExperimentSetting
In this section, we describe the ground-truth environment that agents interact, the implementa-
tiondetailsofonlinelearningmethods,offlinelearningmethods,andourmodelLearningbefore
Interaction.
D.1 OnlineLearning
WeadoptthesamearchitecturesforQMIX1,QPLEX1,CW-QMIX2,RODE3,MAVEN4,EMC5as
theirofficialimplementations(Samvelyanetal.,2019;Wangetal.,2020a;Rashidetal.,2020;Wang
etal.,2020c;Mahajanetal.,2019;Zhengetal.,2021). Eachagentindependentlylearnsapolicy
withfullysharedparametersbetweenallpolicies. WeusedRMSPropwithalearningrateof5e-4and
2https://github.com/oxwhirl/wqmix
3https://github.com/TonghanWang/RODE
4https://github.com/AnujMahajanOxf/MAVEN
5https://github.com/kikojay/EMC
20Œ≥ =0.99,buffersize5000,andmini-batchsize32forallalgorithms. Thedimensionofeachagent‚Äôs
GRUhiddenstateissetto64.
Forourexperiments,weemployanœµ-greedyexplorationschemeforthejointpolicy,whereœµdecreases
from1to0.05over1milliontimestepsin6h_vs_8z,3s5z_vs_3s6zandcorridor,andover50
thousandtimestepsinothermaps. TheimplementationofMAPPOisconsistentwiththeirofficial
repositories6 (Yuetal.,2022). AsshowninTable7,allhyperparametersareleftunchangedatthe
originbest-performingstatus. ForCW-QMIX,theweightfornegativesamplesissettoŒ±=0.5for
allscenarios.
Hyperparameter Value Hyperparameter Value
criticlr 5e-4 actorlr 5e-4
ppoepoch 5 ppo-clip 0.2
optimizer Adam batchsize 3200
optimeps 1e-5 hiddenlayer 1
gain 0.01 trainingthreads 32
rolloutthreads 8 Œ≥ 0.99
hiddenlayerdim 64 activation ReLU
Table7: Hyper-parametersinMAPPO.
All figures in online learning experiments are plotted using mean and standard deviation with
confidenceinternal95%. Weconductfiveindependentrunswithdifferentrandomseedsforeach
learningcurve.
D.2 OfflineLearning
WeadoptthesamearchitecturesforMA-AIRL7,MADT8,MAPT9,ICQ10,OMAR11,andOMIGA12
astheirofficialimplementations(Yuetal.,2019;Mengetal.,2023;Zhuetal.,2024;Fujimotoetal.,
2019;Kumaretal.,2020;Yangetal.,2021;Panetal.,2022;Wangetal.,2024). Weimplement
MA-TREX,BCQ-MAandCQL-MAbasedonTREX(Brownetal.,2019),BCQ(Fujimotoetal.,
2019),andCQL(Kumaretal.,2020),respectively. Inparticular,weaddthetaskdescriptioninto
MADT‚Äôstargetsequencebecauseitdeprecatesthereward-to-goterm.
D.3 LearningbeforeInteraction
Wetrainourimagetokenizerfor100kstepsusingtheAdamWoptimizer,withcosinedecay,using
thehyperparametersinTable8. Thebatchsizeis32,andthelearningrateis1e-4.
WebuildourdynamicsmodelimplementationbasedonDecisionTransformer13(Chenetal.,2021).
ThecompletelistofhyperparameterscanbefoundinTable9. Thedynamicsmodelsweretrained
usingtheAdamWoptimizer.
The reward shares the same architecture as the dynamics model, but the attention mask in the
transformermodelismodifiedinordertoreceivethewholetrajectoryasinputratherthanthetokens
thathavecomebeforethecurrentone. Herearesometricksforrewardlearning: (1)wecontrolthe
gapbetweentherewardsoftheexpertbehaviorandthepolicyaction-westopthegradientforthe
rewardoftheexpertbehavioratagivenstateifitisgreaterthantheoneofthepolicyaction,where
betaisthemarginandsetto2;(2)wealsosetthetargetofunavailableactions‚Äôrewardsto0;(3)we
alternatebetweenk-stepofpolicyupdateandrewardupdatetoavoidcompletelysolvingthepolicy
optimizationsubproblembeforeupdatingtherewardparameters,wherek =5.
6https://github.com/zoeyuchao/mappo
7https://github.com/ermongroup/MA-AIRL
8https://github.com/ReinholdM/Offline-Pre-trained-Multi-Agent-Decision-Transformer
9https://github.com/catezi/MAPT
10https://github.com/YiqinYang/ICQ
11https://github.com/ling-pan/OMAR
12https://github.com/ZhengYinan-AIR/OMIGA
13https://github.com/kzl/decision-transformer
21Component Hyperparameter Value
Encoder num_layers 5e-4
num_res_layers 2
num_channels (256,256)
num_res_channels (256,256)
downsample (2,4,1,1)
Decoder num_layers 5e-4
num_res_layers 2
num_channels (256,256)
num_res_channels (256,256)
upsample (2,4,1,1,0)
Codebook num_codes 256
latent_dim 32
commitment_cost 0.25
Table8: Hyper-parametersinVQ-VAE.
Hyperparameter Value Hyperparameter Value
numberoflayers 6 gradnormclip 1.0
attentionheads 8 weightdecay 0.1
embeddingdims 64 Adambetas (0.9,0.95)
Table9: Hyperparametersinthetransformermodel.
Inthetrainingphaseoftherewardmodel,wetraintheinnerpolicyofeachagentœÄi(ui|s;Œ∏)as:
L =E (cid:2) Qi(s ,ui;œÜ)‚àíyi(cid:3)2
œÜ œÑ‚àºB t t t t (3)
L =E (cid:2) ‚àíQi(s ,uÀÜi(s ;Œ∏))+Œ±logœÄi(uÀÜi(s;Œ∏)|s;Œ∏)(cid:3)
Œ∏ œÑ‚àºB t t t t t
(cid:104) (cid:105)
where yi(œÑ) = ri(œÑ;œï)+Œ≥E QÀÜi (s ,uÀúi ;œÜÀÜ)‚àíŒ±logœÄi(uÀúi |s ;Œ∏) is the
t t st+1‚àºP(¬∑|s,œÄ) t+1 t+1 t+1 t+1 t+1
targetfortheQ-functionofagentiattimestept,Qi(s ,ui;œÜ)isacriticparameterizedbyœÜ,uÀÜi(s ;Œ∏)
t t t t t
isasamplefromœÄi(¬∑|s;Œ∏)whichisdifferentiablewrtŒ∏viareparametrizationtrick,uÀÜi ‚àºœÄi(¬∑|s;Œ∏),
t+1
andŒ±isanentropyregularizationcoefficient.
Inthispaper,allexperimentsareimplementedwithPytorchandexecutedoneightNVIDIAA800
GPUs.
E AdditionalResults
E.1 AdditionalVisualizationResults
Figure7showsthequalitativecomparisonbetweenthetargetandthegeneratedsequences. Both
trajectories are collected by running the same policy. We can see that the generated sequence
can resemble the target one in most frames, but some differences exist in positions and health
bars. However,compoundingerrorsinthesingle-stepmodel,whichleadtophysicallyimplausible
predictions,arenotobservedinthedynamicsmodelgeneratedbythecausaltransformer.Forexample,
atthetimestepof10intheMMM2scenario,thegeneratedframedoesnotcontaintheally‚ÄôsMedivac,
butwecanseeitinthefollowingframes.
E.2 ComparisonswithOnlineLearningMethods
AText-to-CodeConvertercangeneratescenariosusingtheoriginalgameengineandthenlearnthe
jointpolicy. Consequently,wealsoconsidercomparingthisapproachwithonlineMARLmethods,
includingCW-QMIX(Rashidetal.,2020),QPLEX(Wangetal.,2020a),MAVEN(Mahajanetal.,
2019), EMC(Zhengetal.,2021), RODE(Wangetal.,2020c), QMIX(Rashidetal.,2018), and
22Target (3s_vs_5z)
t=0 t=5 t=10 t=15 t=20 t=25 t=30 t=35 t=40
Generated (3s_vs_5z)
t=0 t=5 t=10 t=15 t=20 t=25 t=30 t=35 t=40
Target (MMM2)
t=0 t=5 t=10 t=15 t=20 t=25 t=30 t=35 t=40
Generated (MMM2)
t=0 t=5 t=10 t=15 t=20 t=25 t=30 t=35 t=40
Target (5m_vs_6m)
t=0 t=3 t=6 t=9 t=12 t=15 t=18 t=21 t=24
Generated (5m_vs_6m)
t=0 t=3 t=6 t=9 t=12 t=15 t=18 t=21 t=24
Figure7: Comparisonsofthetargetandthegeneratedsequencesacrossthreedifferentmaps.
MAPPO(Yuetal.,2022).Figure8demonstratesasignificantimprovementinthesampleefficiencyof
LBIcomparedtotheonlineMARLmethods,suggestingthatapre-trainedworldmodelisnecessaryto
reducethewaitingtimeforgeneratinggroundedanswersformulti-agentdecision-makingproblems.
F BackgroundandAdditionalRelatedWork
F.1 DecentralizedPartiallyObservableMarkovDecisionProcess.
A fully cooperative multi-agent task in the partially observable setting can be formulated as a
DecentralizedPartiallyObservableMarkovDecisionProcess(Dec-POMDP)Oliehoek&Amato
(2016), consisting of a tuple G = ‚ü®A,S,‚Ñ¶,O,U,P,r,Œ≥‚ü©, where a ‚àà A ‚â° {1,...,n} is a set of
agents,S isasetofstates,and‚Ñ¶isasetofjointobservations. Ateachtimestep,eachagentobtains
its observation o ‚àà ‚Ñ¶ based on the observation function O(s,a) : S √óA ‚Üí ‚Ñ¶, and an action-
observationhistoryœÑ ‚àà T ‚â° (‚Ñ¶√óU)‚àó. Eachagentachoosesanactionu ‚àà U byastochastic
a a
policyœÄ (u |œÑ ) : T √óU ‚Üí [0,1],whichformsajointactionu ‚àà U. Itresultsinajointreward
a a a
r(s,u)andatransittothenextstates‚Ä≤ ‚àºP(¬∑|s,u). Theformalobjectivefunctionistofindthejoint
policyœÄthatmaximizesajointaction-valuefunctionQœÄ(s ,u )=r(s ,u )+Œ≥E [VœÄ(s‚Ä≤)],where
t t t t s‚Ä≤
VœÄ(s)=E[(cid:80)‚àû
Œ≥ r |s =s,œÄ],andŒ≥ ‚àà[0,1)isadiscountedfactor.
t=0 t t 0
23 / / % % , ,  & & : :   4 4 0 0 , , ; ;  4 4 3 3 / / ( ( ; ;  ( ( 0 0 & &  0 0 $ $ 9 9 ( ( 1 1  5 5 2 2 ' ' ( (  4 4 0 0 , , ; ;  0 0 $ $ 3 3 3 3 2 2
  D   0 0 0    E    K B Y V B  ]   F    F B Y V B   ] J   G    V  ] B Y V B  V  ]
               
           
           
           
           
       
      0   0     0   0     0   0   0   0   0       0   0     0   0     0   0   0   0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
  H    P B Y V B  P   I    V B Y V B  ]   J     P B Y V B   P   K   F R U U L G R U
               
           
           
           
           
       
      0   0     0   0       0   0     0   0       0   0     0   0     0   0   0   0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
Figure8: Performancecomparisonsbetweenonlinelearningmethodsusingground-truthrewardson
theSMACbenchmarkandLBIusingthelearnedrewardfunctionsontheimaginedworldmodel.
F.2 InverseReinforcementLearning
Suppose we do not have access to the ground truth reward function but have demonstrations D
providedbyanexpertpolicyœÄ ,whereDisasetofM trajectories{œÑi}M ={{(si,ui)}T }M
E i=1 t t t=1 i=1
collected by sampling s1 ‚àº Œ∑(s), ui ‚àº œÄ (u |s ), s ‚àº P(s |s ,u ). Given D, imitation
t E t t t+1 t+1 t t
learningaimstodirectlylearnpoliciesthatbehavesimilarlytothesedemonstrations,whereasinverse
reinforcementlearning(IRL)seekstoinfertheunderlyingrewardfunctionswhichinducetheexpert
policies. TheMaxEntIRLframeworkaimstorecoverarewardfunctionthatretionalizestheexpert
behaviorswiththeleastcommitment,denotedasIRL(œÄ ):
E
IRL(œÄ )=argmaxE [r(s,u)]‚àíRL(r)
E
r‚ààR
œÄE
(4)
RL(r)=maxH(œÄ)+E [r(s,u)]
œÄ
œÄ‚ààŒ†
whereH(œÄ) = E [‚àílogœÄ(u|s)]isthepolicyentropy. Itlooksforarewardfunctionthatassigns
œÄ
highrewardtotheexpertpolicyandalowrewardtootherpolicies,whilesearchingforthebestpolicy
fortherewardfunctioninaninnerloop.
F.3 AdditionalRelatedWork
OfflineQ-Learning OfflineQ-learninglearnsapolicyfromafixeddatasetwheretherewardis
providedforeachtransitionsample. Mostoff-policyreinforcementlearning(RL)algorithmsare
applicableinofflineQ-learning. However,theytypicallysufferfromtheoverestimationproblem
of out-of-distribution (OOD) actions due to the distribution shift between the action distribution
in the training dataset and that induced by the learned policy (Fujimoto et al., 2019). Several
constraint methods are proposed to restrict the learned policy from producing OOD actions by
leveragingimportancesampling(Suttonetal.,2016;Nachumetal.,2019),incorporatingexplicit
policyconstraints(Kostrikovetal.,2021;Fakooretal.,2021;Fujimoto&Gu,2021;Tarasovetal.,
2024), penalizing value estimates (Kumar et al., 2020; An et al., 2021; Shao et al., 2024), and
uncertaintyquantification(Wuetal.,2021;Zanetteetal.,2021). Anotherbranchresortstolearning
without querying OOD actions and thus constrain the learning process within the support of the
dataset(Baietal.,2021;Lyuetal.,2022).
Transformer Model Several works have explored the integration of transformer models into
reinforcement learning (RL) settings. We classify them into two major categories depending on
theusagepattern. ThefirstcategoryfocusesonrepresentingcomponentsinRLalgorithms,such
as policies and value functions (Parisotto et al., 2020; Parisotto & Salakhutdinov, 2021). These
methodsrelyonstandardRLalgorithmstoupdatepolicy,wherethetransformeronlyprovidesalarge
representationcapacityandimprovesfeatureextraction. Conversely,thesecondcategoryaimsto
replacetheRLpipelinewithsequencemodeling. Theyautoregressivelygeneratestates,actions,and
rewardsbyconditioningonthedesiredreturn-to-goduringinference(Chenetal.,2021;Leeetal.,
24
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 72022;Reedetal.,2022). Duetoitssimplicityandpotentialgeneralizationability,thiscategoryis
widelyusedinvariousdomains,suchasroboticscontrol(Brohanetal.,2023a;Padalkaretal.,2023;
Driessetal.,2023)andmulti-agentreinforcementlearning(Mengetal.,2023;Liuetal.,2024).
Multi-agent Reinforcement Learning This section briefly introduces recent related work on
cooperativemulti-agentreinforcementlearning(MARL).Intheparadigmofcentralizedtrainingwith
decentralizedexecution(CTDE),agents‚Äôpoliciesaretrainedwithaccesstoglobalinformationina
centralizedwayandexecutedonlybasedonlocalhistoriesinadecentralizedway(Oliehoeketal.,
2008;Kraemer&Banerjee,2016). OneofthemostsignificantchallengesinCTDEistoensure
thecorrespondencebetweentheindividualQ-valuefunctionsandthejointQ-valuefunctionQ ,
tot
i.e., the Individual-Global Max (IGM) principle (Son et al., 2019). VDN (Sunehag et al., 2018)
andQMIX(Rashidetal.,2018)learnthejointQ-valuesandfactorizethemintoindividualQ-value
functionsinanadditiveandamonotonicfashion,respectively. Severalworks(Yangetal.,2020b,a;
Wangetal.,2020b,c)havebeenproposedtoimprovetheperformanceofQMIX,butasmanyprevious
studiespointedout,monotonicvaluefunctionfactorizationlimitstherepresentationalcapacityofQ
tot
andfailstolearntheoptimalpolicywhenthetargetQ-valuefunctionsarenon-monotonic(Mahajan
etal.,2019;Sonetal.,2019;Rashidetal.,2020). Tosolvethisproblem,somerecentworks(Wang
etal.,2020a;Mahajanetal.,2021)trytoachievethefullrepresentationalcapacityofQ ,while
tot
othersprioritizethepotentialoptimaljointactionandlearnabiasedQ .
tot
Someindependentlearningalgorithmshavealsoprovenrobustinsolvingmulti-agentcooperative
tasks. Distributed Q-learning (Lauer, 2000) and Hysteretic Q-learning (Matignon et al., 2007)
place more importance on positive updates that increase a Q-value estimate, which is similar to
the weighting function in WQMIX. However, Wei & Luke (2016) prove that these methods are
vulnerabletowardsmisleadingstochasticityandproposeLMRL2,whereagentsforgivetheother‚Äôs
miscoordinationintheinitialexplorationphasebutbecomelesslenientwhenthevisitationofstate-
actionpairincreases. MAPPO(Yuetal.,2022)appliesPPO(Schulmanetal.,2017)intoMARLand
showsstrongempiricalperformance. However,Kubaetal.(2021)pointsoutMAPPOsuffersfrom
instabilityarisingfromthenon-stationarityinducedbysimultaneouslylearningandexploringagents.
Therefore,theyintroducethesequentialpolicyupdateschemetoachievemonotonicimprovementon
thejointpolicy.
Learningcommunicationprotocolstosolvecooperativetasksisoneofthedesiredemergentbehaviors
of agent interactions. It has recently become an active area in MARL, such as learning to share
observations(Dasetal.,2019;Wangetal.,2019;Liuetal.,2020)andintentions(Kimetal.,2020;
B√∂hmeretal.,2020;Wenetal.,2022;Liuetal.,2023).
25NeurIPSPaperChecklist
1. Claims
Question: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe
paper‚Äôscontributionsandscope?
Answer: [Yes]
Justification: Theabstractandintroductionincludethecontributionsmadeinthepaper. See
Section1formoreinformation.
Guidelines:
‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
madeinthepaper.
‚Ä¢ Theabstractand/orintroductionshouldclearlystatetheclaimsmade,includingthe
contributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor
NAanswertothisquestionwillnotbeperceivedwellbythereviewers.
‚Ä¢ Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflecthow
muchtheresultscanbeexpectedtogeneralizetoothersettings.
‚Ä¢ Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals
arenotattainedbythepaper.
2. Limitations
Question: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?
Answer: [Yes]
Justification: We discuss the limitations of this work in Appendix A.2, such as limited
out-of-domaingeneralizationandconsiderablecosttime.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat
thepaperhaslimitations,butthosearenotdiscussedinthepaper.
‚Ä¢ Theauthorsareencouragedtocreateaseparate"Limitations"sectionintheirpaper.
‚Ä¢ Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto
violationsoftheseassumptions(e.g.,independenceassumptions,noiselesssettings,
modelwell-specification,asymptoticapproximationsonlyholdinglocally).Theauthors
shouldreflectonhowtheseassumptionsmightbeviolatedinpracticeandwhatthe
implicationswouldbe.
‚Ä¢ Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas
onlytestedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoften
dependonimplicitassumptions,whichshouldbearticulated.
‚Ä¢ Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.
Forexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution
isloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbe
usedreliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandle
technicaljargon.
‚Ä¢ Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithms
andhowtheyscalewithdatasetsize.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
addressproblemsofprivacyandfairness.
‚Ä¢ Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby
reviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover
limitationsthataren‚Äôtacknowledgedinthepaper. Theauthorsshouldusetheirbest
judgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimpor-
tantroleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewers
willbespecificallyinstructedtonotpenalizehonestyconcerninglimitations.
3. TheoryAssumptionsandProofs
Question: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand
acomplete(andcorrect)proof?
26Answer: [NA]
Justification: NA.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.
‚Ä¢ Allthetheorems, formulas, andproofsinthepapershouldbenumberedandcross-
referenced.
‚Ä¢ Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.
‚Ä¢ Theproofscaneitherappearinthemainpaperorthesupplementalmaterial, butif
theyappearinthesupplementalmaterial,theauthorsareencouragedtoprovideashort
proofsketchtoprovideintuition.
‚Ä¢ Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented
byformalproofsprovidedinappendixorsupplementalmaterial.
‚Ä¢ TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.
4. ExperimentalResultReproducibility
Question: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-
perimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions
ofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?
Answer: [Yes]
Justification: WedescribethestepstakentoconstructthedatasetinAppendixB,andthe
implementationdetailsofourmodelandbaselinesinAppendixD.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
‚Ä¢ Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived
well by the reviewers: Making the paper reproducible is important, regardless of
whetherthecodeanddataareprovidedornot.
‚Ä¢ Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken
tomaketheirresultsreproducibleorverifiable.
‚Ä¢ Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.
Forexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully
mightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay
benecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame
dataset,orprovideaccesstothemodel. Ingeneral. releasingcodeanddataisoften
onegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed
instructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase
ofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare
appropriatetotheresearchperformed.
‚Ä¢ WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-
sionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe
natureofthecontribution. Forexample
(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow
toreproducethatalgorithm.
(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe
thearchitectureclearlyandfully.
(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould
eitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce
themodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstruct
thedataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authorsarewelcometodescribetheparticularwaytheyprovideforreproducibility.
Inthecaseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedin
someway(e.g.,toregisteredusers),butitshouldbepossibleforotherresearchers
tohavesomepathtoreproducingorverifyingtheresults.
5. Openaccesstodataandcode
27Question: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-
tionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental
material?
Answer: [No]
Justification: Wechoosenottoreleasethedataandcodeatpresent. Wewouldliketohave
theopportunitytofurtherengagewiththeresearchcommunityandtoensurethatanyfuture
suchreleasesarerespectful,safe,andresponsible.
Guidelines:
‚Ä¢ TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy)formoredetails.
‚Ä¢ Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe
possible,so‚ÄúNo‚Äùisanacceptableanswer. Paperscannotberejectedsimplyfornot
includingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source
benchmark).
‚Ä¢ Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto
reproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:
//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.
‚Ä¢ Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow
toaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.
‚Ä¢ Theauthorsshouldprovidescriptstoreproduceallexperimentalresultsforthenew
proposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they
shouldstatewhichonesareomittedfromthescriptandwhy.
‚Ä¢ Atsubmissiontime, topreserveanonymity, theauthorsshouldreleaseanonymized
versions(ifapplicable).
‚Ä¢ Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe
paper)isrecommended,butincludingURLstodataandcodeispermitted.
6. ExperimentalSetting/Details
Question: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: WeprovidethetrainingandtestdetailsinAppendixD.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
‚Ä¢ Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail
thatisnecessarytoappreciatetheresultsandmakesenseofthem.
‚Ä¢ Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental
material.
7. ExperimentStatisticalSignificance
Question:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate
informationaboutthestatisticalsignificanceoftheexperiments?
Answer: [Yes]
Justification: Weconductfiveindependentrunswithdifferentrandomseedsforeachresult.
Theresultsareaccompaniedbystandarddeviations.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
‚Ä¢ Theauthorsshouldanswer"Yes"iftheresultsareaccompaniedbyerrorbars,confi-
denceintervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupport
themainclaimsofthepaper.
28‚Ä¢ Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for
example,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall
runwithgivenexperimentalconditions).
‚Ä¢ Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,
calltoalibraryfunction,bootstrap,etc.)
‚Ä¢ Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).
‚Ä¢ Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderror
ofthemean.
‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis
ofNormalityoferrorsisnotverified.
‚Ä¢ Forasymmetricdistributions,theauthorsshouldbecarefulnottoshowintablesor
figuressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative
errorrates).
‚Ä¢ Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow
theywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.
8. ExperimentsComputeResources
Question: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-
puterresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce
theexperiments?
Answer: [Yes]
Justification: WeprovidetheinformationonthecomputerresourcesinAppendixD.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
‚Ä¢ ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,
orcloudprovider,includingrelevantmemoryandstorage.
‚Ä¢ Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual
experimentalrunsaswellasestimatethetotalcompute.
‚Ä¢ Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecompute
thantheexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthat
didn‚Äôtmakeitintothepaper).
9. CodeOfEthics
Question: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe
NeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: WeconducttheresearchwiththeNeurIPSCodeofEthics.
Guidelines:
‚Ä¢ TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.
‚Ä¢ IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea
deviationfromtheCodeofEthics.
‚Ä¢ Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsid-
erationduetolawsorregulationsintheirjurisdiction).
10. BroaderImpacts
Question: Does the paper discuss both potential positive societal impacts and negative
societalimpactsoftheworkperformed?
Answer: [Yes]
Justification: We discuss both potential positive societal impacts and negative societal
impactsinAppendixA.1.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.
29‚Ä¢ IftheauthorsanswerNAorNo,theyshouldexplainwhytheirworkhasnosocietal
impactorwhythepaperdoesnotaddresssocietalimpact.
‚Ä¢ Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses
(e.g.,disinformation,generatingfakeprofiles,surveillance),fairnessconsiderations
(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific
groups),privacyconsiderations,andsecurityconsiderations.
‚Ä¢ Theconferenceexpectsthatmanypaperswillbefoundationalresearchandnottied
toparticularapplications,letalonedeployments. However,ifthereisadirectpathto
anynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate
topointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto
generatedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout
thatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain
modelsthatgenerateDeepfakesfaster.
‚Ä¢ Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyis
being used as intended and functioning correctly, harms that could arise when the
technologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing
from(intentionalorunintentional)misuseofthetechnology.
‚Ä¢ Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom
feedbackovertime,improvingtheefficiencyandaccessibilityofML).
11. Safeguards
Question: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible
releaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,
imagegenerators,orscrapeddatasets)?
Answer: [Yes]
Justification: Wedoesnotuseapre-trainedmodel(whichmaygenerateunsafeimages),and
weconstructtheimagedatasetthroughaparser. SeeAppendixBformoreinformation.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperposesnosuchrisks.
‚Ä¢ Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith
necessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring
thatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing
safetyfilters.
‚Ä¢ DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors
shoulddescribehowtheyavoidedreleasingunsafeimages.
‚Ä¢ Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo
notrequirethis,butweencourageauthorstotakethisintoaccountandmakeabest
faitheffort.
12. Licensesforexistingassets
Question: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin
thepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand
properlyrespected?
Answer: [Yes]
Justification: WecitetheoriginalpaperandprovidetheURLsfortheassetsinAppendixD.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotuseexistingassets.
‚Ä¢ Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.
‚Ä¢ Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea
URL.
‚Ä¢ Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.
‚Ä¢ Forscrapeddatafromaparticularsource(e.g.,website),thecopyrightandtermsof
serviceofthatsourceshouldbeprovided.
30‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
packageshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasets
hascuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethe
licenseofadataset.
‚Ä¢ Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof
thederivedasset(ifithaschanged)shouldbeprovided.
‚Ä¢ Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachoutto
theasset‚Äôscreators.
13. NewAssets
Question:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation
providedalongsidetheassets?
Answer: [NA]
Justification: WedescribethestepstakentoconstructthedatasetinAppendixB,andthe
implementationdetailsofourmodelandbaselinesinAppendixD.However,Wechoose
not to release the data and code at present. We would like to have the opportunity to
furtherengagewiththeresearchcommunityandtoensurethatanyfuturesuchreleasesare
respectful,safeandresponsible.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotreleasenewassets.
‚Ä¢ Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir
submissions via structured templates. This includes details about training, license,
limitations,etc.
‚Ä¢ Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose
assetisused.
‚Ä¢ Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither
createananonymizedURLorincludeananonymizedzipfile.
14. CrowdsourcingandResearchwithHumanSubjects
Question: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper
includethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as
wellasdetailsaboutcompensation(ifany)?
Answer: [NA]
Justification: NA.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
‚Ä¢ Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontribu-
tionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe
includedinthemainpaper.
‚Ä¢ AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,
orotherlaborshouldbepaidatleasttheminimumwageinthecountryofthedata
collector.
15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman
Subjects
Question: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether
suchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)
approvals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor
institution)wereobtained?
Answer: [NA]
Justification: NA.
Guidelines:
‚Ä¢ TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
31‚Ä¢ Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)
mayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you
shouldclearlystatethisinthepaper.
‚Ä¢ Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions
andlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe
guidelinesfortheirinstitution.
‚Ä¢ Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if
applicable),suchastheinstitutionconductingthereview.
32