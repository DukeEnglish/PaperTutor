Highly Adaptive Ridge
Alejandro Schuler
Division of Biostatistics, University of California, Berkeley
Alexander Hagemeister
EECS, University of California, Berkeley
Mark van der Laan
Division of Biostatistics, University of California, Berkeley
October 4, 2024
Abstract
In this paper we propose the Highly Adaptive Ridge (HAR): a regression method
thatachievesan−1/3 dimension-freeL convergencerateintheclassofright-continuous
2
functions with square-integrable sectional derivatives. This is a large nonparametric
function class that is particularly appropriate for tabular data. HAR is exactly kernel
ridge regression with a specific data-adaptive kernel based on a saturated zero-order
tensor-product spline basis expansion. We use simulation and real data to confirm our
theory. We demonstrate empirical performance better than state-of-the-art algorithms
for small datasets in particular.
Keywords: nonparametric regression, high-dimensional regression, convergence rate
1
4202
tcO
3
]LM.tats[
1v08620.0142:viXra1 Introduction
In regression our task is to find a function that maps features to an outcome such that the
expected loss is minimized [14]. In the past decades a huge number of flexible regression
methods have been developed that effectively search over high- or infinite-dimensional func-
tion spaces. These are often collectively called “machine learning” methods for regression.
L convergence is a well-studied property of regression algorithms that measures how
2
quickly generalization MSE decreases as the size of the training sample increases.1 Faster
convergence rates (asymptotically) guarantee more efficient use of limited data. In many
causal inference settings fast rates must also be assumed to build valid confidence intervals
[23, 29].
Here we present a new machine learning method (highly adaptive ridge; HAR) that
achieves a O (n−1/3(logn)2(p−1)/3) L convergence rate under mild conditions on the true
P 2
data-generating process. This rate is remarkable because it is dimension-free (up to log
factors) as a consequence of the assumed function class. The proposed method has close ties
with the highly adaptive lasso [4, 27], lassoed tree boosting [19], and kernel ridge regression
[31].
2 Notation and Preliminaries
Throughout the paper we adopt the empirical process notation Pf = (cid:82) f(Z)dP and P f =
n
1 (cid:80) f(Z ). In this notation these operators do not average over any potential randomness
n i i
in f so Pf is a random variable if f is a random function learned from data. We use ∥f∥
n n
(cid:112)
to indicate an L (P) norm Pf2 unless otherwise noted.
2
Let X ,Y ∈ X × R be IID across i and with a generic X,Y that have some joint
i i
distribution P and let X denote the distribution of the covariates X. We will take X = [0,1]p
without loss of generality for applications with bounded covariates. Throughout let µ refer
to the Lebesgue measure on [0,1]p.
Let L be some loss (e.g. mean-squared error), which we construct such as to take a
prediction function f as an input and return a function of X,Y as output. For example,
if we want L to be squared error loss, we let L(f)(X,Y) = (f(X) − Y)2. Throughout we
abbreviate Lf = L(f). Let f = argmin PLf. This is the standard regression setup
f:X→R
where our goal is to estimate the function f from n samples of the vector of predictors X
and the outcome Y.
Sectional Variation Let s ⊆ {1...p} be some set of coordinates that we call a section.
When we write x we mean a vector identical to x except for that the elements of x not in
s
s have been set to 0. By x we mean the same except for the complement of s. We write
−s
f(x ,x ) to mean f(x + x ). The latter is technically correct but the former makes it
s −s s −s
easier to understand the intuition.
1In this paper we are speficically concerned with convergence in L (X) where X is the distribution of the
2
covariates. ThisisthegeneralizationMSE.OtherauthorssometimesstudyL (µ)convergencewhichisonly
2
related if µ≪X.
2Let [a,b] be a cube in Rp and define the generalized difference (also called “quasi-volume”
or “alternating sum”) ∆([a,b]) = (cid:80) (−1)|s|f(a ,b ). The vectors a +b are nothing
s⊆{1...p} s −s s −s
other than the corners of the cube [a,b]. This should be familiar: if f is taken to be a
multivariateCDFofarandomvariableZ, thegeneralizeddifferenceexpressestheprobability
that Z ∈ (a,b].
Let P represent a partition of the domain of X into a grid of cubes [a ,b ]. The Vitali
i i
variation of a function f is V(f) = sup (cid:80) ∆([a ,b ]). The Vitali variation captures
a nice notion of global variation but
iP
t
ha[ sai, sbio]∈ mP
e
unini tui
itive behavior. For example, if
f(x ,x ) = 1/x , which is unbounded, then the Vitali variation is zero. That motivates
1 2 1
the notion of sectional variation (also called Hardy-Krause variation).2 For any section s
let f : x → f(x ) defined over the domain X = (cid:81) (0,1] (cid:81) {0} be the value of the
s s s s j∈s j j̸∈s
function f along the s-“face” of [0,1]p. The sectional variation of f is defined to be (cid:80) V(f )
s s
where V(f ) = f(0) by convention. The sectional variation defined this way is a norm and
{∅}
we use the notation ∥·∥ to distinguish sectional variation norm from a standard L norm.3
v 2
Given some constant M, we use K (M) (K for Hardy-Krause) to denote functions with
∥f∥ ≤ M. Owen 2005 [16] provides an excellent summary of these topics including didactic
v
proofs.
In higher dimensions we say that a function is right-continuous if the univariate func-
tions x (cid:55)→ f(x ,x ) are continuous for all j and all values in X . For right-
j {j} −{j} −{j}
continuous4 functions the sectional variation norm of a function f on [0,1]p is given by
(cid:80) (cid:82)1 |df (x)|. Let K˜ denote the subset of functions of bounded HK variation K
s⊆{1...p} 0 s
thatarealsoright-continuousandletK˜ (M)betheright-continuousfunctionswithsectional
variation norm bounded by M.5 The utility of having right-continuity is that f ∈ K˜ corre-
spond1-to-1withfinitesignedmeasuresthesamewaythatcumulativedistributionfunctions
correspond with probability measures [1]. This makes it possible to define Lebesgue-Stieltjes
integrals (cid:82) ·df(x) the same way that one speaks about integration with respect to a CDF
(cid:82) ·dX(x).
In this paper we further assume that the Radon-Nikodym derivatives df /dX are defined
s s
andinL (X )foralls(X (x ) = P(X ≤ x )beingthemarginalCDFofX ). Thederivatives
2 s s s s s s
df /dX can be thought of as mixed derivatives. For example if X is uniform then df {1,2} =
s s dX
{1,2}
2This is the variation “anchored at 0”. Other authors sometimes use different conventions [16, 9, 1].
3In our definition we penalize the value f(0) which makes this a norm. In other definitions the section
over the empty set is not penalized and the sectional variation is a semi-norm [16].
4Acadlag functionisaright-continuousfunctionwithleft-handlimits. Insomerecentwork[4,27,28,19]
the authors consider cadlag functions of bounded sectional variation instead of right-continuous functions
of bounded sectional variation. The two classes are the same, however, because the left-hand limits are
redundant. Right-continuous functions of bounded variation can be decomposed into two bounded entirely
monotone functions which have both right and left limits [1].
5In many cases the right-continuity assumption is also redundant. Functions of bounded sectional varia-
tionhaveaJordandecompositionintothesumoftwoentirelymonotone functions[33,1]. Anydiscontinuities
in these functions exist on a set E of µ-measure zero [33, 2] and thus we lose nothing in an L (P) sense by
2
assuming right-continuity because estimators will simply converge to a representative function f˜=f P-a.e.
(as long as P does not have positive mass on E). But in many cases that assumption will not hold: for
example, by US law there is a discontinuity in pay after working more than 40 hours/week and precisely
as a result of this there is a positive mass of employees who work exactly 40 hours because employers cap
overtime.
3∂2f which exists almost everywhere. The mean-squared integrability of this derivative
∂x1∂x2
actually implies the bounded sectional variation by itself because (cid:80) (cid:82)1 |df (x)| =
s⊆{1...p} 0 s
(cid:12) (cid:12)
(cid:80) (cid:82)1(cid:12)dfs(x)(cid:12)dX which must be finite by our assumption because L ⊂ L . We will
s⊆{1...p} 0 (cid:12) dX s (cid:12) s 2 1
use K˜(1) to refer to right-continuous functions with these first-order (mixed) derivatives in
2
L . In the appendix we generalize this notation and show how K˜(1) is closely related to
2 2
the first-order Sobolev class with dominating mixed derivatives S(1) [34]. However, to keep
2
notation simple, we will use the alias F = K˜(1) and F(M) = K˜(1)(M) for the remainder
2 2
of the main body and in our proofs.
Motivation for This Function Class The utility of assuming bounding sectional varia-
tion is that we can assure a faster L convergence than is possible in traditional smoothness
2
classes without being as restrictive as assuming additive structure. The minimax rate in a
Hölder class with smoothness β is well-known to be n−β/(2β+p) [22]. This rate suffers from
the curse of dimensionality due to the strong dependence on p. In contrast, the minimax rate
for additive functions f(x) = (cid:80) f (x ) with f Lipschitz is n−1/3. This entirely dimension-
j j j
free rate is bought at the cost of a very strong assumption, however. For right-continuous
functions of bounded sectional variation the minimax rate is n−1/3(logn)2(p−1)/3 (up to log
factors) [9]. This looks like the rate for additive functions except for the fact that the
dimension incurs a cost in the log factor.
One way to understand this is that bounding the sectional variation (Vitali variation,
actually) limits the amount of multi-variable “interactivity” that is allowed. This is easiest
to see for continuous differentiable functions of two dimensions for which the Vitali variation
(cid:12) (cid:12)
takes the simple form (cid:82) (cid:12) ∂2f (cid:12)d(x ,x ). It is clear how this penalizes the amount of sub- or
(cid:12)∂x1∂x2(cid:12) 1 2
super-additivity: an additive function has zero mixed derivative everywhere. Bounding this
variation therefore results in a function class with members that behave more like sums of
univariate functions of each of their inputs. The larger the variation norm is allowed to be,
the more “interactivity” is allowed between variables. Assuming bounded sectional variation
therefore strikes a nice middle ground between assuming general smoothness and assuming
exact additive structure [15, 34, 1].
In practice this is an excellent model to use when it is known that most of the variation in
the outcome is due to variation along the x axes, as it often is for tabular data (e.g. econo-
j
metric, healthcare, business). It is not sensible when the individual features by themselves
give very little information about the outcome, as is the case for example when classifying
images from pixel values. Variation in each pixel is irrelevant; what matters is precisely the
local “interactions” at different scales, which is why algorithms (e.g. CNNs) that assume the
regression function is a composition of low-dimensional functions are so successful in those
settings [18].
Our purposes require the slightly stronger assumption that the mixed derivatives exist
(in terms of Radon-Nikodym derivatives) and are square-integrable in order to arrive at an
estimator with good computational properties.
43 Method
Highly adaptive ridge performs a ridge regression in a (data-adaptive) high-dimensional
expansion H(x) of the covariates. The estimated function fˆ is the empirical minimizer of a
n
loss function L in the parametric model {H(x)⊤β : ∥β∥2 ≤ M}. The bound M is chosen by
cross-validation (suppressed in the notation).
The high dimensional basis expansion H is constructed as follows. As before, let s ⊆
{1...p} denote a “section”, i.e. some subset of the dimensions of [0,1]p. Let
(cid:89)
h (x) = 1(X ≤ x )
i,s i,j j
j∈s
be a single, scalar-valued basis function indexed by i and s. Here and in what follows, we
use the convention that Π u = 1 so h (x) = 1 all give an “intercept” term. The bases
j∈∅ j i,∅
h are standard tensor-product zero-order splines each with “knot point” c ∈ [0,1]p where
i,s i,s
each element of the knot is c = X if j ∈ s and c = 0 if j ̸∈ s. In other words,
i,s,j i,j i,s,j
the knot c is the vector X with the non-s elements set to 0. With this notation we can
i,s i
write h (x) = 1(c ≤ x) where the inequality must hold in all dimensions. The bases h
i,s i,s i,s
are data-dependent (random) because X is an observed data point. Our full list of basis
i
functions is
H = [h : i ∈ {1...n},s ⊆ {1...p}].
i,s
We use d to refer to the number of basis functions |H|, which is n2p (there are 2p sections
and n “knots” per section). Technically the number of bases can be smaller if there are
“ties” in the data and certainly we have already over-counted the intercept term n−1 times.
To keep the notation clean, however, we consider H to be a multiset that allows repeated
elements and we can proceed with the exact equality d = n2p. This will make no difference
in the computations and theory that follows.
Formally, the HAR estimator is
fˆ = argminP Lf
n n
f∈F n(M)
(cid:26) (cid:27)
H(x)⊤β
F (M) =
n s.t. ∥β∥2 ≤ M
n
This is identical to the highly adaptive lasso estimator except for that the constraint on the
coefficients is on a 2-norm, not a 1-norm [27, 4].
3.1 Convergence Rate
Our main theoretical contribution is to show that the described algorithm converges quickly
in L norm to the truth under mild assumptions on the data generating process.
2
Theorem 1. Define the “truth” f = argmin PLg for a loss function L. Let our
{g:[0,1]p→R}
modelbeF (M ) = {H(x)⊤β : ∥β∥2 ≤ M }andourestimatebefˆ = argmin P Lg.
n n n n g∈F n(Mn) n
If (1) the loss function is Lipschitz and ∥g − f∥2 ≲ P(Lg − Lf) ≲ ∥g − f∥2, (2) f
is right-continuous with df /dX ∈ L for all s, i.e. f ∈ F, and (3) M is chosen via
s s 2 n
5¯
cross-validation from a grid of values M = [M ...M ] such that ∃k ,M,N : M ≤
n n,1 n,K n
n−1M ≤ M¯ for M ≥ (cid:112) ∥f∥ 2p and n > N, then ∥fˆ −f∥ = O (n−1/3(logn)2(p−1)/3).
n,kn v n P
The proof is given in the appendix. In brief, we use an oracle approximation f =
n
(cid:80) (cid:82)x dfs dX (X being the empirical CDF of X ) and show that f → f suitably
qus ick0 lyd wX shiles t, hn
e
sqs u,n
ared L norm of the
“coefficients”s
of this function
shrinn
k quickly. That
2
ensures ∥f ∥ remains bounded. We then use empirical process theory to show that the
n v
discrepancy between the estimate fˆ and approximation f also disappears quickly.
n n
The required conditions on the loss L are mild and satisfied by mean-squared error and
binary log-loss (see appendix). Our assumed function class is large and diverse so this
restriction is also mild. The condition on the grid of 2-norm bounds for β can be satisfied
in practice by choosing a large and fine-enough grid.
The theorem above extends trivially to cover mixtures of 1- and 2-norm penalties on the
coefficients of the bases (a “highly adaptive elastic net”). However the pure 2-norm penalty
comes with unique computational benefits for the squared-error loss function.
3.2 Computation
The constrained minimization problem in β described above is most often solved using the
Lagrangian formulation
βˆ = argmin P L(H(X)⊤β)+λ∥β∥2
n
β
which for mean-squared error loss has the closed-form solution βˆ = (HH⊤ +λI )−1H⊤Y
d
where H⊤ = [H(X ),...H(X )] and Y ⊤ = [Y ...Y ].
1 n 1 n
Becausethereared = n2p columnsinthe“predictor” matrixH thisproblemisimpossible
to solve computationally for even moderate values of p. Even instantiating the array in
memorycanbeprohibitive. However, anapplicationoftheWoodburymatrixidentityreveals
the equivalent expression βˆ = H(H⊤H +λI )−1Y , meaning that a prediction at x can be
n
computedasfˆ (x) = H(x)⊤H(H⊤H+λI )−1Y . Theadvantageofthisisthatpredictionat
n n
a point depends only on inner products of the form H(x)⊤H(x′). We can analytically work
out the kernel function that computes this inner product directly from the lower-dimensional
x,x′ and avoid ever having to instantiate H or invert a d×d matrix:
K(x,x′) = H(x)⊤H(x′)
(cid:32) (cid:33)(cid:32) (cid:33)
(cid:88)(cid:88) (cid:89) (cid:89)
= 1(X ≤ x ) 1(X ≤ x′)
i,j j i,j j
i s j∈s j∈s
(cid:88)(cid:88)(cid:89)
= 1(X ≤ (x∧x′) )
i,j j
i s j∈s
(cid:88) (cid:88)
= 1
i s⊆si(x,x′)
(cid:88)
= 2|si(x,x′)|
i
6wherex∧x′ denotestheelementwiseminimumands (x,x′)={j : 1(X ≤ (x∧x′) }. The
i i,j j
middle equality follows because the product term is 1 only if s ⊆ s (x,x′) and 0 otherwise.
i
This is a simple computation: we compare the point x∧x′ to each X and count the number
i
of dimensions in which the former is greater than or equal to the latter. This does not require
us to compute the basis expansions H(x) or the values of βˆ and is thus more scalable to high
dimensions than explicitly computing the design matrix with n2p columns, which is almost
impossible for e.g. p > 25. Unfortunately, however, due to the dependence on both test
points (x,x′) and each knot point the construction of the overall kernel matrix is an O(n3)
operation. This operation is the bottleneck in terms of runtime but it is trivially parallelized.
Because the kernel is constructed data-adaptively there are no additional tuning param-
eters. This saves time in cross-validation relative to other choices of kernel. As with other
kernel methods, cross-validation over the regularization parameter in HAR can be done effi-
ciently using an exact closed-form expression for leave-one-out CV [7]. Technically this is a
slight abuse because the kernel is data-adaptive so it should be recomputed without the ith
knot point but this does not make a meaningful difference. In appendix D we describe the
simple method we use to set the values of the regularization λ to cross-validate over.
3.3 Higher-Order HAR
Under additional smoothness assumptions we are able to improve the HAR rate towards the
parametric n−1/2 rate. Instead of assuming square integrable first sectional derivatives we
can assume square-integrable sectional derivatives of higher orders. This naturally motivates
an estimator identical to the above but with different set of data-adaptive basis functions.
The details of these function classes are given in the appendix and are largely reproduced
from [28]. Our novelty, also presented in the appendix, is in showing how these function
classes can be used for HAR. The upshot is that we obtain n−(t+1)/(2t+3) L rates up to logn
2
factors by assuming t orders of “smoothness”. We also show how the resulting estimators
can be kernelized.
3.4 Related Work
HAR is closely related to HAL, the highly adaptive lasso [4, 27, 9, 28]. In HAL the estimator
is the empirical minimizer of a loss function L in the parametric model {H(x)⊤β : ∥β∥ ≤
1
M}. As implied by the names, HAL penalizes the 1-norm of the coefficients while HAR
penalizes the 2-norm. HAL achieves the same fast convergence rate as HAR but HAL suffers
from a computational curse of dimensionality because the basis matrix H must be explicitly
computed. Moreover lasso problems are generally much slower to solve than ridge problems,
even absent the use of the kernel trick.
The extension of the rate result from HAL to HAR is not trivial. Changing from a
lasso to a ridge penalty fundamentally changes the function class being considered. HAR
with a fixed 2-norm bound on the coefficients does not work: as the size of the dataset
increases, the number of bases expand as well and the HAR function class quickly becomes
much bigger than any class of right-continuous functions of bounded sectional variation. It
is therefore essential to shrink the bound at a certain rate to keep the model inside this
Donsker class and it must be proved that this does not then eliminate any relevant functions
7from consideration. It is critical that cross-validation is able to do this (as we show it is)
because manual control over the 2-norm bound is difficult to achieve using the Lagrangian
formulation of the optimization problem. This is why HAR requires cross-validation and
the first-order smoothness assumption df /dX ∈ L to prove the rate result. This is not
s s 2
required by (0th-order) HAL.
Previous work demonstrated a close connection between HAL and gradient boosted trees
and exploited this to construct a rate-preserving boosting algorithm called Lassoed Tree
Boosting (LTB) [19]. LTB is more general in the sense that the computational benefit is not
limited to squared error loss as is the case for HAR. The conceptual advantage of HAR over
LTB is that HAR provides direct empirical minimization over H(x)⊤β whereas LTB must
iteratively “boost” a sequence of bases and repeatedly find the optimal linear combination.
Like HAL, however, LTB does not require the first-order smoothness condition required by
HAR to prove the fast rate.
HAR is very closely related to previous work on estimation in tensor product Sobolev
spaces [15, 34, 20]. These papers exploit the same idea (penalizing interactions) to achieve
similar dimension-free rates. In particular [34] proposes kernel ridge regression with the
following “mixed Sobolev” kernel:
p
1 (cid:89)
K(x,x′) = cosh(x ∧x′)cosh(1−x ∨x′).
sinh(1)p j j j j
j
Our theoretical results are distinct in that we work with right-continuous functions that lend
themselves to extremely concise proofs because of the connection between right-continuous
functions of bounded sectional variation and finite signed measures. We are also more easily
able to derive the more-relevant L (P) rates instead of L (µ) rates without imposing ab-
2 2
solute continuity on the distribution of X. In addition, we provide results for higher-order
smoothness conditions. In appendix C we compare and contrast various function spaces.
Computationally, HAR is also a form of kernel ridge regression (KRR) [31]. The twist
is that the kernel function for HAR is constructed automatically by the algorithm based on
the data instead of being chosen by the user. Therefore, unlike typical kernel ridge, HAR
is not doing exact empirical risk minimization so the standard theory does not apply. In
this sense HAR is somewhat related to “kernel learning” methods [11]. Previous results on
convergence rates for kernel ridge regression are found in [25, 35, 6].
4 Demonstration
First we give a qualitative visualization of HAR in action, compared to some other methods.
We drew 50 points from the data-generating process:
X ∼ Unif([−1,1])
(cid:40)
−x x ≤ 0
Y = N(0,0.32)+
sin(2πx) x > 0
We evaluated HAR, HAL [4], radial basis kernel ridge, kernel ridge with a mixed Sobolev
kernel [34], random forest, and ridge regression. We included HAL as a baseline because
8the function class that HAR minimizes over is closely related to that of HAL. Since HAR is
computationally a kernel ridge method, we also compare to kernel ridge methods with other
kernels (included the closely related mixed Sobolev kernel; see appendix C). Standard ridge
and random forest are also included as baselines since they are extremely commonly used
and easy to use out-of-the-box without much tuning.
We implemented the three kernel ridge methods (HAR, radial basis, and mixed Sobolev)
and did model selection over regression strength over a grid of 50 points logarithmically
spaced in [0,λ ] (see appendix D) using the closed-form leave-one-out CV error expression
0
[7]. We also tuned the bandwidth for radial basis, testing values from 0.001 to 10. For HAL
and ridge we tuned the regularization using internal 5-fold cross-validation. For the random
forest we used 2,000 trees and all other parameters at their defaults in sklearn [17].
We repeated the entire process three times and visualized the truth and predictions on a
test set, shown in figure 1. It is evident that, like HAL and random forests, HAR produces
piecewise constant fits. As expected, the jumps are more numerous and smaller than the
jumps for HAL because of the ridge vs. lasso regularization. The fits from HAR also look
quite similar to those from the mixed Sobolev KRR.
Method
HAL HAR Mixed Sobolev KRR
1.0
uth 0.5
Tr
n,
o 0.0
cti
di
e
Pr−0.5
−1.0
Radial Basis KRR Random Forest Ridge Regression
1.0
uth 0.5
Tr
n,
o 0.0
cti
di
e
Pr−0.5
−1.0
−1.−00.8−0.6−0.4−0.20.0 0.2 0.4 0.6 0.81.0 −1.−00.8−0.6−0.4−0.20.0 0.2 0.4 0.6 0.81.0 −1.−00.8−0.6−0.4−0.20.0 0.2 0.4 0.6 0.81.0
x x x
Figure 1: Fits of HAR and other methods on simple one-dimensional data.
94.1 Convergence Rate in Simulation
Our primary theoretical result concerns the convergence rate of HAR in mean-squared error
so we test this using a simple simulation. Our data-generating process is as follows:
X ∼ Unif([0,1]10)
5 10 (cid:18) (cid:19)
(cid:89) (cid:89) X −x
Y = X − 0∨ j 0 ∧1 +N(0,0.12).
j
ϵ
j=1 j=6
The true outcome regression is a sum of two 5-way interactions, one of which is smooth and
one of which is (almost) a jump. We set ϵ = 0.05 to make the cliff relatively sharp and let
and x = 1−(1/2)1/5−ϵ so that approximately half the data would be on either side of the
0
jump. This scenario is meant to be challenging because of the high-dimensional interactions
and the different scales at which they operate.
We drew datasets of increasing n from this data-generating process, trained HAR (tuning
regularization as described above), and evaluated RMSE on a large test set. This process
was repeated 10 times and we took an average of the test set errors for each n. We divided
these RMSEs by the theoretical rate n−1/3(logn)2(p−1)/3. The result (figure 2) confirms that
we have convergence faster than the advertised rate for this data-generating process.
0.00045
0.00040
0.00035
E
S0.00030
M
R
d 0.00025
e
al
c0.00020
S
e­
at0.00015
R
0.00010
0.00005
0.00000
60 80 100120140160180200220240260280300320340360380400420440460480500520540560580600
n
Figure 2: Convergence of HAR relative to theorized rate.
4.2 Empirical Performance
Lastly, we tested HAR against the baselines described above on several real regression
datasets from the UCI Machine Learning Repository [8]. We took the first 2000 rows of
each dataset and split them up randomly into 80% train and 20% test and computed test-
set RMSE. We used just 2000 rows to speed up the evaluations because the kernel methods
we use run slowly for large n (as does HAL), which is a known drawback. We then repeated
the process in its entirety 5 times and took the average of the test-set RMSEs. The results
are shown in table 1.
10Mixed Radial
Random Ridge
data n p HAR HAL Sobolev Basis
Forest Regression
KRR KRR
power 2000 4 4.05 — 4.11 4.28 4.11 4.56
yacht 308 6 8.74e-1 6.79e-1 4.18e-1 5.63e-1 1.01 8.66
concrete 1030 8 3.65 3.74 3.80 9.23 4.71 1.05e+1
energy 768 8 3.65e-1 4.39e-1 3.82e-1 4.72e-1 4.76e-1 2.85
kin8nm 2000 8 1.40e-1 — 1.29e-1 9.22e-2 1.67e-1 2.04e-1
protein 2000 9 1.88 — 1.91 5.85 1.86 2.64
wine 1599 11 6.07e-1 — 6.11e-1 6.36e-1 5.79e-1 6.60e-1
boston 506 13 3.33 3.36 2.54 4.65 3.03 4.51
naval 2000 17 7.66e-4 — 4.16e-4 1.89e-3 8.86e-4 1.32e-3
yearmsd 2000 90 1.15e+1 — 9.07 1.15e+1 9.46 9.88
slice 2000 384 9.00 — 7.96 1.31e-1 3.70e-1 6.35e-1
Table 1: RMSE of different methods across the UCI datasets. Lowest RMSE for each dataset
in bold.
The results in 1 show that HAR and mixed Sobolev KRR perform similarly and both
do very well in general. HAL also performs well but it was computationally impractical to
run the algorithm for the larger datasets. The only substantial degradation in performance
from HAR and mixed Sobolev KRR relative to baseline methods occurs for the very high-
dimensional slice dataset.
5 Discussion
HAR provides a conceptually simple and often performant and tractable algorithm with
fast convergence in a meaningfully large nonparametric class of functions. The fast rate
means, for example, that many efficient estimators of causal quantities can be shown to be
asymptoticallylinearusingHARunderweakerassumptionsthatwouldotherwiseberequired
[27, 29, 23].
For squared error loss HAR is more practical than HAL (which provides the same rate
guarantee for a larger function class) because HAR does not require explicit computation of
the n2p-column design matrix. However, when kernelized, HAR does suffer from the well-
known drawbacks of kernel methods in general. At training time kernel methods require
the inversion of an n × n matrix which is roughly an O(n3) operation. This is not ideal
but completely feasible with modern compute even for relatively large n. For truly massive
internet-scale data this can be a problem but there are existing methods that mitigate these
issues and which are likely rate-preserving with HAR (e.g. matrix sketching [32], divide-and-
conquer [35]). Recent work [3] suggests that it may also be possible to modify HAR so that
multiple solutions along the regularization path can be computed together with warm-start
optimization as is done in elastic net algorithms [10].
Nonetheless, the good empirical performance of HAR (and the mixed Sobolev kernel
11ridge) for small data increases the evidence that near-additive function classes are often a
good model when learning from tabular data. The theoretical results established here can
likely be combined with the early-stopping results from [19] to construct efficient boosting
algorithms with rate guarantees that are not slowed down by a lasso step.
References
[1] Christoph Aistleitner and Josef Dick. Functions of bounded variation, signed measures,
and a general Koksma-Hlawka inequality. June 2014.
[2] Christoph Aistleitner, Florian Pausinger, Anne Marie Svane, and Robert F Tichy. On
functions of bounded variation†. Math. Proc. Cambridge Philos. Soc., 162(3):405–418,
May 2017.
[3] Oskar Allerbo. Solving kernel ridge regression with Gradient-Based optimization meth-
ods. June 2023.
[4] David Benkeser and Mark van der Laan. The highly adaptive lasso estimator. Proc Int
Conf Data Sci Adv Anal, 2016:689–696, December 2016.
[5] Aurélien F Bibaut and Mark J van der Laan. Fast rates for empirical risk minimization
over càdlàg functions with bounded sectional variation norm. July 2019.
[6] AndreaCaponnettoandErnestoDeVito. Optimalratesfortheregularizedleast-squares
algorithm. Foundations of Computational Mathematics, 7:331–368, 2007.
[7] Gavin C Cawley, Nicola LC Talbot, and Olivier Chapelle. Estimating predictive vari-
ances with kernel ridge regression. In Machine Learning Challenges Workshop, pages
56–77. Springer, 2005.
[8] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
[9] Billy Fang, Adityanand Guntuboyina, and Bodhisattva Sen. Multivariate extensions
of isotonic regression and total variation denoising via entire monotonicity and Hardy-
Krause variation. March 2019.
[10] Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for gener-
alized linear models via coordinate descent. J. Stat. Softw., 33(1):1–22, 2010.
[11] Mehmet Gönen and Ethem Alpaydın. Multiple kernel learning algorithms. The Journal
of Machine Learning Research, 12:2211–2268, 2011.
[12] Laszlo Gyorfi. A distribution-free theory of nonparametric regression. Springer series in
statistics. Springer, New York, NY, 2002 edition, April 2006.
[13] Markus Hansen and Winfried Sickel. Best m-term approximation and Sobolev–Besov
spaces of dominating mixed smoothness—the case of compact embeddings. Constr.
Approx., 36(1):1–51, August 2012.
12[14] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2. Overview of Supervised
Learning. In The Elements of Statistical Learning, page 1 34. Springer New York, New
York, NY, January 2009.
[15] Yi Lin. Tensor product space anova models. The Annals of Statistics, 28(3):734–755,
2000.
[16] Art B Owen. Multidimensional variation for Quasi-Monte carlo. In Contemporary
Multivariate Analysis and Design of Experiments, volume 2 of Series in Biostatistics,
pages 49–74. WORLD SCIENTIFIC, March 2005.
[17] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830, 2011.
[18] Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli
Liao. Why and when can deep – but not shallow – networks avoid the curse of dimen-
sionality: a review. November 2016.
[19] Alejandro Schuler, Yi Li, and Mark van der Laan. Lassoed tree boosting. arXiv preprint
arXiv:2205.10697, 2022.
[20] Zhongjie Shi, Jun Fan, Linhao Song, Ding-Xuan Zhou, and Johan A K Suykens. Non-
linear functional regression by functional deep neural network with kernel embedding.
January 2024.
[21] Winfried Sickel and Tino Ullrich. Tensor products of Sobolev–Besov spaces and appli-
cations to approximation from the hyperbolic cross. J. Approx. Theory, 161(2):748–786,
December 2009.
[22] Charles J Stone. Optimal Global Rates of Convergence for Nonparametric Regression.
Ann. Stat., 10(4):1040 1053, December 1982.
[23] A Tsiatis. Semiparametric theory and missing data, 2007.
[24] Alexandre B Tsybakov. Introduction to Nonparametric Estimation. Springer, November
2008.
[25] Rui Tuo, Yan Wang, and CF Jeff Wu. On the improved rates of convergence for
matérn-type kernel ridge regression with application to calibration of computer models.
SIAM/ASA Journal on Uncertainty Quantification, 8(4):1522–1547, 2020.
[26] Aad W van der Vaart, Sandrine Dudoit, and Mark J van der Laan. Oracle inequalities
for multi-fold cross validation. Statistics & Decisions, 24(3):351–371, 2006.
[27] Mark van der Laan. A generally efficient targeted minimum loss based estimator based
on the highly adaptive lasso. Int. J. Biostat., 13(2), October 2017.
13[28] Mark van der Laan. Higher order spline highly adaptive lasso estimators of functional
parameters: Pointwise asymptotic normality and uniform convergence rates. 2023.
[29] Mark J van der Laan, M J Laan, and James M Robins. Unified Methods for Censored
Longitudinal Data and Causality. Springer Science & Business Media, January 2003.
[30] A W van der Vaart. Asymptotic Statistics. Cambridge University Press, June 2000.
[31] Vladimir Vovk. Kernel ridge regression. In Empirical Inference: Festschrift in Honor
of Vladimir N. Vapnik, pages 105–116. Springer, 2013.
[32] Rong Yin, Yong Liu, Weiping Wang, and Dan Meng. Sketch kernel ridge regression
using circulant matrix: Algorithm and theory. IEEE transactions on neural networks
and learning systems, 31(9):3512–3524, 2019.
[33] W H Young and Grace Chisholm Young. On the discontinuties of monotone functions
of several variables. Proc. Lond. Math. Soc., s2-22(1):124–142, 1924.
[34] Tianyu Zhang and Noah Simon. Regression in tensor product spaces by the method of
sieves. Electronic Journal of Statistics, 17(2):3660–3727, 2023.
[35] Yuchen Zhang, John Duchi, and Martin Wainwright. Divide and conquer kernel ridge
regression. In Conference on learning theory, pages 592–617. PMLR, 2013.
A Proof of Rate Result
Here we provide a proof of the rate result in theorem 1. The proof here is decomposed into a
main result and some corollaries that when combined give the result given in the main text.
We first construct an oracle approximation that converges quickly to the target function but
which is always in the HAR model with shrinking L norm. Standard empirical process
2
arguments then give the rate for the empirical minimizer in that HAR model. We assume
familiarity with empirical process arguments at the basic level of [30].
A.1 Loss Assumptions
Let f = argmin PLg. Throughout our proofs we assume that the following conditions on
F
the loss function L hold at any g ∈ F:
Assumption 1 (Lipschitz loss). ∥Lg −Lf∥2 ≲ c ∥g −f∥.
1
Assumption 2 (Divergence and L -norm equivalence). ∥Lg − Lf∥2 ≲ P(Lg − Lf) ≲
2
∥g −f∥2 .
∞
Instead of assumption 2 it is often more parsimonious and intuitive to assume the
divergence P(Lg − Lf) is “equivalent” in a norm sense with the squared L norm, i.e.
2
∥g − f∥2 ≲ P(Lg − Lf) ≲ ∥g − f∥2. This equivalence immediately implies the weaker
upper bound above. The lower bound above is also implied by combining this with the Lips-
chitz assumption (1). These conditions typically hold under some further weak assumptions
on the data distribution P as can be seen in the following examples:
14Example 1 (Squared Error). If Y is bounded, assumptions the above assumptions hold for
squared error loss L(g) = (g(X)−Y)2.
Proof. Equivalence of the divergence and squared error is immediate because P(Lg−Lf) =
∥g − f∥2 using total expectation. To prove the Lipschitz condition note (Lg − Lf) =
(g − Y)2 − (f − Y)2 = [(g − Y) − (f − Y)][(g − Y) + (f − Y)] = (g − f)(g + f − 2Y).
Squaring and taking the expectation,
(cid:90)
∥Lg −Lf∥2 = (g −f)2(g +f −2Y)2dP
(cid:90)
≤ (sup|g +f −2Y|)2 (g −f)2dP
which gives the result since f,g ∈ F ensures these functions are bounded and our additional
assumption bounds Y.
Now we consider logistic regression where we take the function of interest f to be the
conditional log-odds of the outcome:
Example 2 (Logistic Regression). For binary Y, assumptions 2 holds for log loss L(g) =
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)y(cid:16) (cid:17)1−y
−Y log 1 −(1−Y)log 1 ifthedensitydP /dP = p (y|x) ≡ 1 1
1+e−g(X) 1+eg(X) g g 1+e−g(x) 1+eg(x)
is uniformly bounded away from 0 for all g in F.
Proof. Although a direct proof of the assumptions would be slightly quicker, we will instead
prove the stronger L equivalence of the divergence and Lipschitz condition on the loss
2
because this argument is more easily generalized.
With the above we can write L(g) = −log(p ), simply the log-likelihood. In this case
g
P(Lg − Lf) is exactly the KL divergence (cid:82) log(p /p )dP = −(cid:82) log(p /p ) dP . Re-
f g f g f f
producing known arguments (see [30] pg. 62), an application of the pointwise inequality
√
−log(z) ≥ 2(1− z) for z ≥ 0 gives
(cid:90) (cid:18) (cid:19) (cid:18) (cid:90) (cid:19) (cid:90)
− log p g dP ≥ 2 1− √ p p dP ≥ (cid:0)√ p −√ p (cid:1)2 dP.
f g f g f
p
f
√ √
Bounding p ,p away from 0 also implies the bound away from 1 so | p − p | ≥
g f g f
c∥p − p ∥ for some c. Lastly, for ψ−1 : g (cid:55)→ p as defined above we have ∥p − p ∥ =
g f g g f
∥ψ−1(g)−ψ−1(f)∥ ≥ c∥g−f∥ for some other c because the mapping ψ is Lipschitz as long
as the inputs are such that the outputs are bounded away from 0. Thus we have established
the lower bound c ∥g −f∥2 ≤ P(Lg −Lf).
1
The upper bound comes from a typical Taylor expansion with the mean value theorem
applied to the second derivative (at each x, p˜takes value between p and p ):
g f
(cid:20) (cid:21)
1
P(Lg −Lf) = −P(log(p )−log(p )) = −P (p −p )p−1 − (p −p )2p˜−2 .
g f g f f 2 g f
The first term is zero and the second is bounded above by (sup|p˜−2|)∥p −p ∥2 where the
g f
supremum is finite because our densities are bounded away from 0. The norm in terms of
15the densities is itself upper bounded by a constant times the norm in terms of f,g because
ψ−1 is Lipschitz. This gives the upper bound P(Lg−Lf) ≤ c ∥g−f∥2 which completes the
2
proof of equivalence with squared error loss.
Lastly, the Lipschitz condition on L(g) = log(ϕ−1(g)) is satisfied because log(·) is Lip-
schitz for inputs bounded away from 0 and ψ−1 is also Lipschitz and produces outputs
uniformly bounded away from zero under our assumptions.
The proof given above for log loss is easily generalized to other likelihood-based losses, i.e.
losses that have the form L(g) = −log(ϕ−1(g)) with ϕ−1 : g (cid:55)→ p some invertible mapping
g
from a function-valued parameter to a density. The generalized conditions require upper and
lower bounded densities and Lipschitz continuity of both ψ and ψ−1.
A.2 Oracle Approximation
Let X be the empirical CDF of X and, assuming the densities dfs exist, define the
approxs i,n
mation
s dX
s
(cid:90)
(cid:88) df
f (x) = s dX (1)
n dX s,n
s (0,x] s
(cid:32) (cid:33)
(cid:88) 1 (cid:88) df
= 1(X ≤ x) s (X ) (2)
n i,s dX i,s
s
s i
= H(x)⊤γ (3)
where γ = n−1 dfs (X ) are collapsed into a vector γ.
Nowi, ws e consid dX esr hoi, ws well f approximates f in loss-based divergence.
n
Lemma 1. Let X, f, and f be as above. If assumption 2 holds then P(Lf −Lf) = O (n−1).
n n P
Proof. First we show ∥f − f∥ = O (n−1/2). We follow the same strategy employed in
n ∞ P
lemma 23 of [28]. Write f(x) = (cid:80) (cid:82) df . The difference is
s (0,x] s
(cid:90)
(cid:88) df
(f −f)(x) = s (dX −dX ) (4)
n dX s s,n
s (0,x] s
(cid:18) (cid:19)
(cid:88) df
= (X −X ) 1(· ≤ x) s (·) (5)
s s,n dX
s
s
This is an empirical process indexed by x and the functions g (u) = 1(u ≤ x) dfs (u) fall
in a Donsker class [30] (the density is a fixed function). Therefx ore the empiricald pX rsocess is
uniformly bounded in probability at the rate n−1/2 giving the desired supremum norm bound
on f −f. The final result follows immediately from assumption 2.
n
Now we show that this fast-converging approximation has a quickly shrinking L2 norm
for the coefficients.
16Lemma 2. Let X, f and f be as above and assume that df /dX exists and is in L for
n s s 2
each section. Then ∥γ∥2 = O (n−1).
p
(cid:16) (cid:17)2 (cid:16) (cid:17)2
Proof. (cid:80) iγ i2
,s
= n−1P
n
ddf Xs
s
. Of course (P n−P) ddf Xs
s
= O P(n−1/2) by the central limit
theorem and the result follows.
A.3 Highly Adaptive Ridge
Let F(M) be the set of right-continuous functions of sectional variation bounded by M with
sectional Radon-Nikodym derivatives w.r.t. the distribution of X in L . Define the highly
2
adaptive ridge (HAR) model
F (Mn−1) = {H⊤β : ∥β∥2 ≤ Mn−1} (6)
n
and the empirical minimizer (HAR estimator) fˆ = argmin P Lf.
n f∈F n(Mn−1) n
Theorem 2. Let X, f, f and fˆ be as above. If assumption 2 holds and df /dX exists and
n n s s
is in L for each section, then there is an M such that P(Lfˆ −Lf) = O (n−1/2).
2 n P
Proof. Lemma 2 directly implies that there exists an M > 0 for which f ∈ F (Mn−1) with
n n
highprobabilityforlarge-enoughn. UsethisM todefinetheestimatefˆ = argmin P Lf.
n F n(Mn−1) n
The term P (Lfˆ −Lf ) is thus less than or equal to zero because both fˆ ,f ∈ F (Mn−1)
n n n n n n
for every n and fˆ is defined as the empirical minimizer in each class.
n
(cid:112)
ForallfunctionsinthemodelF (Mn−1)wehave∥β∥2 ≤ Mn−1 =⇒ ∥β∥ ≤ Md/n =
√ n 1
M2p by an application of Cauchy-Schwarz and recalling d = n2p. For these functions the
sectional variation norm is given by ∥f∥ = ∥β∥ [4, 27, 9]. Thus fˆ ,f are of bounded
v 1 n n
sectional variation (and of course right-continuous), guaranteeing that Lfˆ − Lf falls in
√ n n
the class {Lf : f ∈ F( M2p)}. Assumption 1 guarantees that this class is Donsker [30].
Because of this, (P−P )(Lfˆ −Lf ) = O (n−1/2) [30]. Thus
n n n P
P(Lfˆ −Lf ) = (P−P )(Lfˆ −Lf )+P (Lfˆ −Lf ) (7)
n n n n n n n n
= O (n−1/2) (8)
P
Lastly, P(Lfˆ −Lf) = P(Lfˆ −Lf )+P(Lf −Lf) where the latter term is O (n−1) by
n n n n P
lemma 1 and thus negligible.
Corollary 1. If assumption 2 also holds then ∥f −f∥ = O (n−1/3(logn)2(p−1)/3).
n P
Proof. Here we give a sketch of the proof, which follows similar arguments in section 7.1 of
√
[28]. Let G = { n(P−P )l : l ∈ L} be the empirical process indexed by functions in
n √ n
G = {Lf : f ∈ F( M2p)}. We know
P(Lfˆ −Lf) ≤ (P−P )(Lfˆ −Lf) ≤ n−1/2supG (g). (9)
n n n n
g∈G
By assumption 2 and theorem 2 we get ∥Lfˆ −Lf∥2 = O (n−1/2). Therefore the above still
n P
holds if we instead take the supremum of G over {g ∈ G : ∥g − Lf∥2 ≤ n−1/2} instead
n
17of over all of L because we know fˆ is in this set (asymptotically, with high probability).
n
Using a bound on the entropy integral of {g ∈ G : ∥g −Lf∥2 ≤ n−1/2} [5, 28] we obtain
sup G g = O (n−1/8(logn)p−1) (10)
n P
{g∈G:∥g−Lf∥2≤n−1/2}
and thus we have improved the rate to P(Lfˆ −Lf) = O (n−5/8(logn)p−1). Now we again
n P
use the smoothness of L to bound ∥Lfˆ − Lf∥2 = O (n−5/8(logn)p−1) and again we can
n P
iterate using a bound on the entropy integral of the smaller class {g ∈ G : ∥g − Lf∥2 ≤
n−5/8(logn)p−1}, giving an even faster rate. This process iterates and the rate approaches a
fixed point which is O (n−2/3(logn)4(p−1)/3) A final application of the smoothness inequality
P
and taking the square root gives the result.
Corollary 2. Define a data-adaptive HAR model F (M) = {H⊤β : ∥β∥2 ≤ M } where
n n,k∗
n
M is chosen data-adaptively from a grid of values M = [M < M < ...M ] by
n n n,1 n,2 n,K
¯
minimizing cross-validation loss. If there is a constant M and sequence k such that M ≤
n
¯
nM ≤ M for M as defined in theorem 2 for all n large enough then the cv-HAR estimator
n,kn
fˆ = argmin P Lf attains the above convergence rate.
n f∈F n(M n,kn∗) n
Proof. This is a direct consequence of theorem 2 and the cross-validation oracle inequality
[26].
B Higher-Order HAR
InthissectionwepresentextensionsofHARthatachieveevenfasterconvergenceratesunder
more stringent smoothness assumptions. First we set up some function classes, reproducing
the exposition in [28].
B.1 Background
Recall that right-continuous functions of bounded sectional variation can be represented as
(cid:90)
(cid:88)
f(x) = df (u )
s0 s0
s0⊆{1...p}
(0,xs0]
using the convention that the term for s = ∅ above evaluates to f(0) (and (cid:81) u = 1).
0 j∈∅ j
The reason for the subscript on s will become evident shortly.
0
Presume now that the Radon-Nikodym derivatives f(s0) = df /dµ exist and are them-
s0 s0
18selves right-continuous functions of bounded sectional variation (df /dµ ∈ K˜ ). Then
s0 s0
(cid:90)
(cid:88)
f(x) = 1(u ≤ x ) f(s0)(u ) dµ (u )
s0 s0 s0 s0 s0
s0 u
(cid:32) (cid:33)
(cid:90) (cid:90)
(cid:88) (cid:88)
= 1(u ≤ x ) 1(v ≤ u ) df(s0)(v ) dµ (u )
s0 s0 s1 s1 s1 s1 s0 s0
s0 u s1⊆s0 v
(cid:34) (cid:35)
(cid:90) (cid:90) (cid:90)
(cid:88)
= 1(u ≤ x ) dµ(u ) 1(v ≤ u ≤ x ) dµ (u ) df(s0)(v )
s0/s1 s0/s1 s0/s1 s1 s1 s1 s1 s1 s1 s1
s1⊆s0⊆{1...p}
u
s0/s1
v us1
(cid:90)
(cid:88) (cid:89) (cid:89)
= x (x −v )1(v ≤ x )df(s0)(v )
j j j j j s1 s1
v
s1⊆s0⊆{1...p} j∈s0/s1 j∈s1
(cid:124) (cid:123)(cid:122) (cid:125)
hs0,s1(v,x)
(cid:90)
(cid:88)
= h (v,x) df(s0)(v )
s0,s1 s1 s1
v
s1⊆s0⊆{1...p}
TheD “e 0fi tn he or∥ df e∥ r( v ”1) no= rm(cid:80) cs o1r⊆ rs e0s⊆ p{ o1. n..p d} s∥ tf o(s t0 h)∥ ev stt ao nb dae rdth se ec“ t1 is ot nao lrd ve ar r” iats ie oc nti .on Wa el dva efiri na etio an cln ao ssrm of.
functions F(1)(M) to be those satisfying the above representation and which have ∥f∥(1) ≤
v
M. This class is smaller than our class F(M) and “smoother” in the sense that we have
required the existence and variational boundedness of certain derivatives.
We can now repeat this construction, taking s ⊆ s and assuming df(s0) = f(s0,s1)dµ
2 1 s1 s1
sw ei pth araf t( is n0, gs1 t)( hx e) in= teg(cid:80)
ras l2s⊆
as1n(cid:82)
d(0 e,x v]
ad lf us( a2s0 t, is n1 g) a thss eu mm ge id vet so be in F. Plugging everything in and
 
(cid:88) (cid:90) (cid:89) (cid:89) x2 (cid:89) (x −v )2
f(x) =  x j 2j 1(v j ≤ x j) j
2
j df s( 2s0,s1)(v s2)
v
s2⊆s1⊆s0⊆{1...p} j∈s0/s1 j∈s1/s2 j∈s2
and at this point the pattern is clear and we can generalize and condense notation.
Let s¯ denote a sequence of t+1 sets such that {s ⊆ ...s ⊆ s ⊆ {1...p}}. There are
t 1 0
(2+t)p of these sequences s¯. Now we can write
  
(cid:32) (cid:33)
(cid:89) (x −v )t (cid:89)t (cid:89) xτ
h s¯(v,x) = j j +   j 
t! τ!
j∈st τ=1 j∈sτ−1/sτ
where (x −v )t = 1(v ≤ x )(x −v )t. And we define
j j + j j j j
(cid:32) (cid:32) (cid:33) (cid:33)
(cid:18) (cid:19)
d d d
f(s¯) = ... f ...
st dµ dµ dµ s0
st−1 s1 s0 s1
s2 st
to arrive at the general t-th order representation
(cid:90)
(cid:88)
f(x) = h (v,x)df(s¯)(v )
s¯ st st
s¯
19under the condition that f(s¯) are right-continuous functions of bounded sectional variation
for all s¯. To each such function f we assign the t-th order sectional variation norm ∥f∥(t) =
v
(cid:80) ∥f(s¯)∥ . Generalizing our notation from the introduction of the article we can call the
s¯ v
class of functions that satisfy this representation and have finite sectional variation K˜(t): the
0
t indicates the maximum order of the derivatives f(s¯) and the subscript indicates smoothness
restrictions on first-order derivatives of each f(s¯). K˜(t) indicates that f(s¯) ∈ K˜, which is
0
what we have discussed (no condition on the derivatives). In what follows we will discuss
classes K˜(t) where the q > 0 subscript indicates that the first-order sectional derivatives of
q
each f(s¯) are in L (P). Note that the derivatives of f(s¯) are taken with respect to sections
q
of µ and must exist, but it is integrability with respect to P that needs to be satisfied at the
end.
B.2 Estimator
Let H denote the set of t-th order spline basis functions of the form h (x) = h (X ,x)
t i,s¯ s¯ i
indexed by i ∈ 1...n and s ⊆ ...s ⊆ s ⊆ {1...p}. There are d = n(2+t)p of these bases
t 1 0
(again double counting intercepts, etc.). Our t-th order HAR estimator for a fixed M is
fˆ = argmin P Lf
n n
f∈F n(t)(M)
(cid:26) (cid:27)
H (x)⊤β
F(t)(M) = t
n s.t. ∥β∥2 ≤ M
which is completely analogous to the “0th order” HAR presented previously (H = H ) except
0
with a different (larger) set of basis functions.
The basis functions h take an interesting form which may not be immediately apparent
i,s¯
from the notation above. For example, in the first-order basis functions
(cid:89) (cid:89)
h (x) = x (x −X )
i,s¯ j j j +
j∈s0/s1 j∈s1
we can think of X and s together defining a knot point X where as usual the non-s
i 1 i,s1 1
entries of X are set to zero (i.e. the point X is projected onto the face defined by s ).
i i 1
Varying over all s¯, we get one or more splines anchored at each knot point. For example,
when p = 2, we have the bases given in table 2 (suppressing the i subscripts).
At all knot points we place a first-order tensor-product spline (x − X ) where the
s1 +
positive part is taken elementwise. This may not be obvious but notice that, for example,
(x −X ) x = (x −X ) (x −0) on [0,1]2 and [X ,0] is precisely X . At knot points
1 1 + 2 1 1 + 2 + 1 {1}
that lie on the faces of [0,1]p we additionally have products of first- and zero-order splines
(indicators). Again, notice that (x −X ) = (x −X ) 1(x ≥ 0). Lastly, on the corner,
1 1 + 1 1 + 2
we additionally place a product of purely zero-order splines 1 = 1(x ≥ 0)1(x ≥ 0). This
1 2
is instructive because the pattern is the same in higher dimensions. Knot points that fall
on lower-dimensional sections have more bases placed at them which are products of higher-
and lower-order 1-dimensional splines.
20s s X h
1 0 s1 i,s¯
{1,2} {1,2} [X ,X ] (x −X ) (x −X )
1 2 1 1 + 2 2 +
{1} {1,2} [X ,0] (x −X ) x
1 1 1 + 2
{1} {1} [X ,0] (x −X )
1 1 1 +
{2} {1,2} [0,X ] x (x −X )
2 1 2 2 +
{2} {2} [0,X ] (x −X )
2 2 2 +
{} {1,2} [0,0] x x
1 2
{} {1} [0,0] x
1
{} {2} [0,0] x
2
{} {} [0,0] 1
Table 2: All 1st order basis functions generated by a point X for p = 2.
B.3 Convergence Rate
Theorem 3. Let our model be F(t)(M ) = {H (x)⊤β : ∥β∥2 ≤ M } and our estimate be
n n t n
fˆ = argmin P Lg.
n g∈F n(t)(Mn) n
If (1) the loss function obeys assumption 2, (2) f ∈ F(t)(M∗), and (3) M is cho-
n
¯
sen via cross-validation from a grid of values M = [M ...M ] such that ∃k ,M : M ≤
n n,1 n,K n
nM ≤ M¯ forM chosensuitablylargeenough, then∥fˆ −f∥ = O (n−(t+1)/(2t+3)(logn)r(p,t))
n,kn n P
for some fixed function r depending only on the dimension p and order t.
Proof. The convergence rate proofs given above carry over exactly to higher-order HAR. The
only difference is that the additional smoothness condition df /dX ∈ L must be replaced
s s 2
by the higher-order smoothness condition df(s¯)/dX ∈ L . Since they are smaller, the
st st 2
function classes F(t)(M) also have better entropy integrals and correspondingly better rates
[28], but the structure of the arguments is otherwise identical to what is given above for 0th
order HAR.
B.4 Computation
Higher-order HAR can be “kernelized” in the same way as 0th order HAR. The inner product
of the basis expansions of two points is
  
(cid:32) (cid:33)
H t(x)⊤H t(x′) = (cid:88)(cid:88) (cid:89) (x j −X i,j)t + t!( 2x′ j −X i,j)t + (cid:89)t  (cid:89) (x τjx !2′ j)τ .
i s¯ j∈st τ=1 j∈sτ−1/sτ
The “shells” {1...p}/s ,s /s ,s /s ...s /s ,s = S ,S ...S form a partition of
0 0 1 1 2 t−1 t t 0 1 t+1
{1...p}. The set of all sequences s¯ is one-to-one with the set of all such partitions S¯ so
we can instead think of the outer sum as looping over the set of all partitions. Moreover
for a fixed i let u = (x ,x′,X ) and let w (u ) =
(xj−Xi,j)t +(x′ j−Xi,j)t
+, w (u ) = 1, and
j j j i,j t+1 j t!2 0 j
21w (u ) =
(xjx′ j)τ
for all 0 < τ < t+1. Then
τ j τ!2
 
(cid:88)(cid:88) (cid:89) (cid:89) (cid:89) (cid:89) (cid:89)
H t(x)⊤H t(x′) =  w t+1(u j) w t(u j) w t−1(u j)··· w 1(u j) w 0(u j).
i S¯ j∈St+1 j∈St j∈St−1 j∈S1 j∈S0
Notice that any given j must fall into exactly one of the t+2 sets S . Thus the product
τ
inside the sum can be written as a product over j that looks like (cid:81) w (u ) where τ(j)
j τ(j) j
gives the index τ of the unique set S that contains j. Therefore each term in the sum is
τ
constructed by choosing one element from {w (u ) : t ∈ {0...t + 1}}, one element from
t 1
{w (u ) : t ∈ {0...t+1}}, and so on and then finally multiplying them all together. This is
t 2
precisely what we would get if we took any of the terms from the expansion of
(cid:18) (cid:19)(cid:18) (cid:19) (cid:18) (cid:19)
w (u )+...w (u ) w (u )+...w (u ) ··· w (u )+...w (u )
0 1 t+1 1 0 2 t+1 2 0 p t+1 p
and, indeed, since we consider a sum over all possible partitions, we actually obtain every
term in this expression in the sum and therefore
t+1
(cid:88)(cid:89)(cid:88)
H (x)⊤H (x′) = w (u )
t t t j
i j τ=0
(cid:32) (cid:33)
(cid:88)(cid:89) (x j −X i,j)t +(x′ j −X i,j)t + (cid:88)t (x jx′ j)τ
= + +1
t!2 τ!2
i j τ=1
which generalizes the formula given for 0th-order HAR.
C Comparison of Function Classes
There is extensive literature on L convergence rates in Sobolev and Hölder classes [24, 12].
2
In this section we briefly review some of these concepts and compare and contrast our work.
Inwhatfollowsletα ∈ Np beamulti-indexofintegersanddefinethegeneralmixedderivative
0
as
∂∥α∥1f
Dαf = .
(cid:81)
j∂αjx
j
Hölder Classes. A Hölder class H (β) on an open domain Ω ⊆ Rp with smoothness β is
the set of functions f : Ω → R for which there is a global constant C satisfying
|(Dαf)(x)−(Dαf)(x′)| ≤ C∥x−x′∥(β−⌊β⌋) ∀α : ∥α∥ ≤ ⌊β⌋,
1
implying the existence of the required derivatives. For integer β, this simply means that the
function is β − 1 times differentiable (in any directions) and that the resulting derivative
is Lipschitz continuous but not necessarily itself differentiable. When β is not an integer,
the continuity condition on the resulting derivative is weaker than the typical Lipschitz
condition (but still stronger than uniform continuity) because, e.g. |g(x)−g(x′)| ≤ (x−x′)1/2
allows g to be increasing arbitrarily quickly the closer we take x′ → x. Hölder classes are
usually understood to formalize the notion of functions having a certain amount of “local”
or pointwise smoothness.
22Sobolev Classes. A Sobolev class W (γ) on an open domain Ω ⊆ Rp with smoothness γ is
q
the set of functions f : Ω → R for which
Dαf ∈ L (µ) ∀α : ∥α∥ ≤ γ
q 1
where g ∈ L (µ) ⇐⇒ (cid:0)(cid:82) gqdµ(cid:1)1/q < ∞ and g is µ-measurable together define the typical
q Ω
L (µ)space. Sobolevclassesareusuallyunderstoodtoformalizeanotionoffunctionshaving
q
a certain amount of “global” smoothness.
For Sobolev spaces, the derivative Dαf should be read as a “weak” derivative. A function
we call Dαf is considered to be a derivative of f as long as it satisfies the multidimensional
integration by parts formula (cid:82) ϕ(Dαf)dµ = (−1)∥α∥1(cid:82) (Dαϕ)f dµ for any compactly sup-
Ω Ω
ported, infinitely differentiable function ϕ.
It’s not too difficult to see that H (β) ⊂ W (⌊β⌋) when the functions are defined on a
1
compact domain: local boundedness of the derivatives implies boundedness which implies
integrability. So in this sense some local smoothness implies a certain amount of global
smoothness. Interestingly, the inclusion can also go the other way: the Sobolev embedding
theorems show, for example, that W (p+1) ⊂ H (1). So with a lot of global smoothness
1
(depending on dimension p) one can also guarantee some local smoothness.
Mixed Sobolev Classes. Following [34, 20, 13], we define the Sobolev class with dominat-
ing mixed derivatives (mixed Sobolev class) S(γ) onanopendomainΩ ⊆ Rp withsmoothness
q
γ to be the set of functions f : Ω → R for which
Dαf ∈ L (µ) ∀α : ∥α∥ ≤ γ.
q ∞
Where again the derivatives need only exist in the weak sense described above. The only
difference between this definition and the one above is that we have replaced ∥α∥ by ∥α∥ ,
1 ∞
but this changes things substantially. For example, consider p = 2 so α = [α ,α ] and let
1 2
γ = 1. The Sobolev class W (1) constraint on α is just concerned with α : α +α ≤ 1, so we
1 1 2
only require integrability of f itself and its first derivatives in the x or x direction. There
1 2
is no constraint on the mixed derivative. However for S(1) the case α = [1,1] satisfies the
1
condition ∥α∥ = max α ≤ 1 and therefore the mixed derivative must also be integrable
∞ j j
in order for f to be in S(1). Indeed, we have the following tight inclusions: W (p) ⊂ S(1) ⊂
1 q q
W (1). So a mixed Sobolev class can be thought of as more “smooth” than the equivalent
q
Sobolev class, but not as smooth as the Sobolev class with γ = p.
A mixed Sobolev class of functions of dimension p can also be thought of as a tensor
product of equivalent Sobolev classes of one-dimensional functions. That is, for some set
Ω ⊆ R if we let S(γ) be the q-smooth, p-dimensional mixed Sobolev class of order γ, we
q,p
have S(γ)(Ωp) = (cid:78)pW (γ)(Ω) [34, 21]. Concretely, f ∈ S(γ) can be written as a limit of
q 1 q q,p
functions of the form (cid:80)R(cid:81) h (x ) where R is a finite positive integer and h ∈ W (γ)(Ω).
r j r,j j r,j q
For these functions, any mixed derivative is a sum of products of univariate derivatives so
it is easy to see why integrability of the latter implies integrability of the former, i.e. why
S(γ)(Ωp) ⊆ (cid:78)pW (γ)(Ω) at a minimum.
q 1 q
Appendix B of [34] provides an excellent summary of these results.
23Our Function Class F. InourpaperwemainlyconsiderthefunctionclassF = K˜(1)(P)
2
which are right-continuous functions where sectional Radon-Nikodym derivatives with re-
spect to marginal distributions of X are square-integrable. Here we show how this class is
related to the above classes, in particular the mixed Sobolev class S(1).
2
There are two ways in which our function class K˜(1) differs from the mixed Sobolev class
2
S(1). The first is the sense in which the derivative is defined. For our class we are interested
2
in Radon-Nikodym derivatives with respect to marginals of P, whereas in the mixed Sobolev
class we are interested in weak derivatives in the distributional sense. The second difference
is in where we care about the behavior of the derivative: for our class we care about the
behavior of these derivatives on the sections (edges, faces) of the domain, whereas for the
mixed Sobolev class we only care about the interior of the domain (the boundaries having
Lebesgue measure zero).
For the rest of this discussion, suppose that X is uniformly distributed on [0,1]p so
that df /dX = ∂|s|f /(cid:81) ∂x ≡ Dα(s)f .6 That effectively eliminates the first difference
s s s j∈s j s
between the two classes because Dα(s)f is a weak derivative of f on its domain X = (cid:81) =
s s s j∈s
(0,1] (cid:81) {0}. Moreoverifanyweakderivativeexistsitisnecessarilyalmosteverywherethe
j j̸∈s
Radon-Nikodym derivative df /dµ . Our class thus represents the right-continuous functions
s s
where
Dαf ∈ L (µ ) ∀α : ∥α∥ ≤ 1.
s(α) 2 s ∞
This looks quite similar to the 1st-order mixed Sobolev class S(1). However, K˜(1) is
2 2
only concerned with sectional weak derivatives: each Dαf is defined on the domain X
s(α) s
and not on all of [0,1]p as a typical weak derivative Dαf would be. However, we will show
that such a weak derivative can indeed be defined everywhere.
Theorem 4. Let the first-order mixed Sobolev class S(1) be defined as above and let K˜(1)(P)
2 2
be the set of right-continuous functions of X ∈ [0,1]p where sectional Radon-Nikodym deriva-
tives with respect to marginal distributions of X are square-integrable. If the distribution of
X is uniform, then K˜(1) ⊆ S(1).
2 2
Proof. Pick any function f ∈ K˜(1). If the distribution of X is uniform, then df /dX =
2 s s
∂|s|f /(cid:81) ∂x ≡ Dα(s)f , defined only on the section X = (cid:81) j ∈ s(0,1] (cid:81) {0} . Our
s j∈s j s s j j∈/s j
task is to extend these weak derivatives to the entire domain and show they are all in L .
2
Note that the “fully-saturated” mixed derivative D1f is already the weak derivative
{1...p}
of f over the whole domain since for weak derivatives we are not concerned with the faces
because they have measure 0 for the uniform distribution. Now we extend the other weak
derivatives to cover (0,1]p. For any section s (including s = {0}, for which Dα(s)f (x ) =
s s
f(0)), let
(cid:90) x−s
g (x ,x ) = Dα(s)f (x )+ D1f(x ,u )du .
s s −s s s s −s −s
0−s
6If X is absolutely continuous and has positive density everywhere then the proceeding discussion also
applies with Sobolev classes replaced with certain “weighted” equivalents. We are unaware of Sobolev-like
classes that are defined in terms of any kind of weak derivatives where integration is with respect to general
measures, but that could also generalize the following discussion.
24By construction, x (cid:55)→ D1f(x ,x ) is a weak derivative of g s in the x direction and
−s s −s s −s
therefore, leveraging the general integration by parts formula for weak derivatives,
(cid:90) (cid:90)
g (Dα(−s)ϕ)dx = (−1)∥α(−s)∥1 (Dα(−s)g)ϕdx
s
(cid:124) (cid:123)(cid:122) (cid:125)
ϕ˜
(cid:90)
= (−1)∥α(−s)∥1 (D1f)ϕdx
(cid:90)
= (−1)∥α(−s)∥1(−1)p f(D1ϕ)dx
(cid:90)
= (−1)∥α(s)∥1 f(Dα(s)Dα(−s)ϕ)dx
(cid:124) (cid:123)(cid:122) (cid:125)
ϕ˜
for any test function ϕ. In the last equation above we used the fact that the parity of
p+∥α(−s)∥ is equal to that of ∥α(s)∥ (sum of two odd numbers is even, etc.). This
1 1
shows that the constructed function g defined over (almost) the entire domain is a weak
s
derivative Dα(s)f because every test function ϕ˜ can be represented as a derivative Dα(−s)ϕ
with ϕ another test function (this is a property of the smooth functions).
Finally, we show all of these weak derivatives are square-integrable over the domain.
Using the above definition of g = Dα(s)f and expanding the square under the integral, the
s
squared L norm (cid:82) (Dα(s)f)2dx is
2
(cid:90) (cid:18)
(cid:2)
Dα(s)f (x
)(cid:3)2
+2Dα(s)f (x
)(cid:90) x−s
D1f(x ,u )du
+(cid:20)(cid:90) x−s
D1f(x ,u )du
(cid:21)2(cid:19)
dx
s s s s s −s −s s −s −s
0−s 0−s
The first term is bounded by assumption. The last term is
(cid:90) (cid:20)(cid:90) x−s
D1f(x ,u )du
(cid:21)2
dx ≤
(cid:90) (cid:90) (cid:20)(cid:90) 1−s(cid:2)
D1f(x ,u
)(cid:3)2
du
(cid:21)
dx dx
s −s −s s −s −s −s s
0−s xs x−s 0−s
by first applying Jensen’s inequality (D and then extending the innermost upper limit of
integration (the integrand is non-negative). The result is then bounded by assumption after
changing the order of integration. Given the bounded first and third terms, the second term
is bounded by Cauchy-Schwarz.
We have thus shown that f has all of the required weak derivatives and that they are
square-integrable. Thus f ∈ S(1) and since f was arbitrary this shows K˜(1) ⊆ S(1).
2 2 2
TheseargumentsshowthatthemixedSobolevclassincludesourclasswhenX isuniform.
However there is a sense in which the inclusion goes the other way as well. It is known that
everyfunctioninS(1) hasaL representativethatisabsolutelycontinuous: simplyintegrate
2 2
the given weak derivatives and the result is guaranteed to be almost everywhere equal to
the original function. The absolute continuity of course implies right-continuity. Moreover
it is known that sectional variation is bounded by the sum of the L norms over all the
2
first order mixed derivatives, which we know are bounded if we assume our function is in
S(1) [16]. Therefore every function in f ∈ S(1) is almost-everywhere equal to a function
2 2
in K˜. Clearly these functions have weak mixed derivatives everywhere, but it remains to
25be shown that they are square-integrable along the sections. However here again we are
saved by picking a good L representative because the all the sections except s = {1...p}
2
have measure zero. Since weak derivatives are only unique up to a set of measure zero, we
can change the values on the faces of [0,1]p arbitrarily so that the required sectional weak
derivatives are square-integrable. Therefore, while K˜(1) ̸⊃ S(1), from the perspective of
2 2
the L (µ) norm the spaces K˜(1) and S(1) are indeed the same when X is uniform. This
2 2 2
demonstrates the very close connection between our work and that of [34, 15].
The K˜(t) spaces may be more natural to work with because by being defined in terms
q
of the distribution of X we obtain L (P) convergence results without assuming absolute
2
continuity, etc. (technically for q ≥ 1 we should write K˜(t) to denote the dependence on P).
q,P
Moreover the proofs are quite simple. We also have the clear generalization to K˜(t) where
0
there is no condition on the integrability of any derivatives.
On the other hand, we must pay for this by assuming right-continuity, except in cases
where we think X does not have point masses at these discontinuities. There is also
a huge amount of additional theory for Sobolev classes and generalizations (Besov and
Triebel–Lizorkin) which might be leveraged to better understand different combinations of
relatively weak assumptions that give dimension-free rates for regression. Lastly, our al-
gorithm does not actually return the empirical minimizer in our function class (subject to
some norm constraint): the HAR “kernel” is data-adaptive and the returned function is an
empirical minimizer in an approximation of the function class.
Certainly more work is required to investigate the relationship between K˜(t) and S(t)
q q
spaces.
D Regularization Search for Kernel Ridge Regression
Here we describe in more detail how we search and set the regularization hyperparameter λ
for all of the kernel methods in our experiments. We have not encountered this strategy in
the wild so it may be of some independent interest. This method is not specific to HAR, it
works for any kernel.
Our strategy is to find a maximum value λ for which the predictions from a model
0
trained on the data we have will be so regularized that they are all very close to zero relative
tothemaximumabsolutevaluetheoutcometakes. Anyfurtherregularizationpastthispoint
will not substantially change the out-of-sample predictions so we do not need to evaluate
validation error for λ > λ . Once we have this value we can generate a (log-scale) grid of
0
values in [0,λ ] to validate over or perform a (bounded) adaptive search.
0
Since we only care about the prediction function we have in hand we need to consider
the training set fixed and we denote it x ...x and [y ...y ]⊤ = Y . Let fˆ be the kernel
1 n 1 n λ
ridge regression learned from these data when using regularization λ. Given a small ϵ > 0
our formal task is to find λ such that ∥fˆ (X)∥ < ϵ∥Y∥ for all λ ≥ λ .
0 λ ∞ ∞ 0
We start with the following expression for fˆ (X):
λ
fˆ (X) = k(X)(K +λI)−1Y
λ
(cid:124) (cid:123)(cid:122) (cid:125)
α
26where we have the fixed matrix K = K(x ,x ) and random vector k(X) = K(X,x ) with
ij i j i i
X a random test point. We have also denoted the fixed “coefficients” with α.
From Cauchy-Schwarz we know |fˆ (X)| = |kα| ≤ ∥k∥ ∥α∥ . Now we can take the supre-
λ 2 2
mum on both sides, giving ∥fˆ (X)∥ ≤ sup ∥k(X)∥ ∥α∥ . To bound the norm of the
λ ∞ X 2 2
coefficients we express it in terms of the product of the operator norm and norm of the
argument ∥α∥ ≤ ∥(K +λI)−1∥∥Y ∥ . For the operator norm of the positive-definite matrix
2 2
(K +λI)−1 we have
∥(K +λI)−1∥ = eig (cid:0) (K +λI)−1(cid:1) = (eig (K +λI))−1 = (eig K +λ)−1
1 n n
where eig A denotes the kth-largest eigenvalue of a matrix A.
k
Putting it all together, we have
∥fˆ (X)∥ ≤ sup∥k(X)∥ (eig K +λ)−1∥Y ∥
λ 2 n 2
X
which we would like to be to be smaller than ϵ∥Y∥ to achieve our goal. Solving for λ we
∞
get
sup ∥k(X)∥ ∥Y ∥
λ ≥ X 2 2 −eig K =⇒ ∥fˆ (X)∥ < ϵ∥Y∥ .
ϵ∥Y∥ n λ ∞ ∞
∞
In practice we cannot take the suprema so we replace these with the empirical maxima
and set
max ∥K ∥ ∥Y ∥
λ = i i 2 2 −eig K
0 ϵmax |y | n
i i
where K is the ith row of the kernel matrix K. Everything in this expression can be
i
calculated directly from the training data.
27