GUD: GENERATION WITH UNIFIED DIFFUSION
MathisGerdes∗ MaxWelling
InstituteofPhysics AmsterdamMachineLearningLab
UniversityofAmsterdam,NL UniversityofAmsterdam,NL
MirandaC.N.Cheng†
InstituteofPhysics,UniversityofAmsterdam,NL
InstituteforMathematics,AcademiaSinica,Taiwan
Korteweg-deVriesInstituteforMathematics,UniversityofAmsterdam,NL
ABSTRACT
Diffusiongenerativemodelstransformnoiseintodatabyinvertingaprocessthat
progressively adds noise to data samples. Inspired by concepts from the renor-
malization group in physics, which analyzes systems across different scales, we
revisit diffusion models by exploring three key design aspects: 1) the choice of
representationinwhichthediffusionprocessoperates(e.g.pixel-,PCA-,Fourier-,
orwavelet-basis),2)thepriordistributionthatdataistransformedintoduringdif-
fusion (e.g. Gaussian with covariance Σ), and 3) the scheduling of noise levels
applied separately to different parts of the data, captured by a component-wise
noiseschedule.Incorporatingtheflexibilityinthesechoices,wedevelopaunified
frameworkfordiffusiongenerativemodelswithgreatlyenhanceddesignfreedom.
Inparticular,weintroducesoft-conditioningmodelsthatsmoothlyinterpolatebe-
tween standard diffusion models and autoregressive models (in any basis), con-
ceptuallybridgingthesetwoapproaches. Ourframeworkopensupawidedesign
spacewhichmayleadtomoreefficienttraininganddatageneration,andpavesthe
waytonovelarchitecturesintegratingdifferentgenerativeapproachesandgener-
ationtasks.
1 INTRODUCTION
Diffusion-basedgenerativemodels,firstintroducedinSohl-Dicksteinetal.(2015),haveseengreat
successesinrecentyearssincetheworksofSong&Ermon(2019);Hoetal.(2020). Inthesemod-
els, dataaretransformedintonoisefollowingadiffusionprocess, andatransformationsimulating
thereverseprocessislearnedwhichisthenusedtomapnoiseintogeneratedsamples. Inphysics,
thetheoryofrenormalizationgroup(RG)flowshasbeenabasictoolinthestudyofawiderange
ofphysicalphenomena,includingphasetransitionsandfundamentalphysics,bothintheoreticalas
wellasnumericalapproaches. Inshort,anRGflowprescribesawayoferasingthehigh-frequency
information of a physical theory, while retaining the information relevant for the long-wavelength
physics. Assuch,thereareclearanalogsbetweenscore-basedgenerativemodelsandRGflows,at
least at a conceptual level. Indeed, it has been known for a long time that RG flows in quantum
field theories can also be described as a diffusive process (Zinn-Justin, 2002; Gaite, 2001; Cotler
& Rezchikov, 2023; Berman et al., 2023). In both cases, information gets erased along the flow
and many different initial distributions get mapped into the same final distribution – a feature of-
ten referred to as “universality” in the physics literature. In the diffusion context, the “universal”
distributionisgivenbythechosennoisedistribution,independentofthedatadistribution.
However,therearesalientdifferencesbetweenthewaysdiffusionmodelsandRGeraseinformation.
First, the basis: the diffusive RG process is diagonal in the frequency-basis while the standard
diffusionmodelstypicallydiffusediagonallyinthepixel-basis. Second,thepriordistribution: the
endpoint of RG is a scale-invariant distribution, often with the same second-order statistics as the
∗Correspondence:m.gerdes@uva.nl
†OnleavefromCNRS,France.
1
4202
tcO
3
]GL.sc[
1v76620.0142:viXradistributiononestartswithatthebeginningoftheRGflow. Thestandarddiffusionmodelsonthe
otherhandindiscriminatelymapalldatadistributionstothatofwhitenoise. Third,thecomponent-
wisenoisingschedule:RGflowseraseinformationsequentiallyfromhightolowfrequencies,while
theoriginaldiffusionmodelhasthesamenoisescheduleforallpixels.Inourchosenbasis,weallow
eachcomponenttohaveitsownnoisingschedule. Theseconsiderationsleadustotheframeworkof
generativeunifieddiffusion(GUD)modelswhichincorporatethefreedomindesignchoicesinthe
abovethreeaspects.
Autoregressive models, such as next-token prediction models, play an increasingly dominant role
inmodern-daymachine-learningapplicationssuchasLLMs,andseemtobedistinctfromdiffusion
modelsatfirstglance. Inautoregressivemodels,tokensaregeneratedoneatatime,conditionalon
previously generated ones, while diffusion models generate information in all components simul-
taneously. We will show that the two can in fact be unified in our framework, which in particular
allows for soft-conditioning generative processes. Intuitively, this means that we can condition on
partial informationfrom other components aslong as that informationhas already been generated
inthediffusionprocess.
2 RELATED WORK
Theideathatdiffusionmodelscanincorporatethemulti-scalenatureofthedatasetthroughaspecific
choiceofdatarepresentationhasbeenexploitedininterestingworksincludingWaveletScore-Based
Generative Models (Guth et al., 2022), Pyramidal Diffusion Models (Ryu & Ye, 2022), Cascaded
Diffusion Models (Ho et al., 2022), and more. In these works, the generation is sharply autore-
gressivebetweendifferenthierarchiesofgeneration(betweendifferentresolutions,forinstance). In
BlurringDiffusionModels(Hoogeboom&Salimans,2024),diffusionmodelsinthefrequencybasis
wasproposed. Ourframework,besidesnotbeingrestrictedtothesespecificchoicesofbasis,allows
foranextensionoftheseworkbyintroducingthefreedomtosoft-conditionthegenerativeprocess.
Thepossibilityofinterpolatingbetweenthestandarddiffusionmodelsandtoken-wiseautoregressive
modelshasrecentlybeenexploredinChenetal.(2024)inthecontextofcausalsequencegenera-
tion. In this work, a training paradigm where diffusion models, with a chosen token-wise noising
schedule,aretrainedtodenoisetokensbasedontheinformationofonlythe(partiallynoised)pre-
vious tokens, captured in latent variables. Our work integrates the choices of data representation,
thepriordistribution,andthecomponent-wisenoisingscheduleintooneframework,andtherefore
includesthisparticularparadigmasaspecialcase. Inparticular,aswedemonstrateinexperiments,
itispossibletointegratemulti-scaleandspatiallysequentialgenerationprocessesinourframework.
Inpractice, forapplicationswithhigh-dimensionaldata, thediffusiongenerationoftentakesplace
inalower-dimensionallatentspace(Rombachetal.,2021;Sinhaetal.,2021;Vahdatetal.,2021).
The freedom to choose the basis proposed in our work is not to be understood as replacing latent
spacediffusion. Rather,ourframeworkcanstraightforwardlybeusedinthelatentspace,leadingto
alatentGUDmodel.
3 PRELIMINARIES
3.1 STOCHASTICDIFFERENTIALEQUATIONS
In continuous time, the general diffusion setup can be described by the following Itoˆ stochastic
differentialequation(SDE):
dϕ=f(ϕ,t)dt+G(ϕ,t)dw , (1)
where dw represents a white noise Wiener process. We use ϕ ∈ Rd to denote a vector. In the
above,wehavef(·,t) : Rd → Rd andG(·,t) : Rd → Rd×d. Thereverse-timeSDEisgivenby
(Anderson,1982)
dϕ=(cid:0) f(ϕ,t)−∇·(GGT)(ϕ,t)−GGT∇ logp (ϕ)(cid:1) dt+G(ϕ,t)dw¯ (2)
ϕ t
wherew¯ istheinverseWienerprocess.
2Theprobabilitydensityp(ϕ,t)correspondingtotheSDEequation1solvesthefollowingFokker-
Planckequation(orKolgomorov’sforwardequation)(Oksendal,1992)
∂ (cid:88)d ∂ 1(cid:88)d (cid:88)d ∂2 (cid:32) (cid:88)d (cid:33)
p(ϕ(t))=− (f (ϕ,t)p(ϕ(t)))+ G G p(ϕ(t)) , (3)
∂t ∂ϕ i 2 ∂ϕ ∂ϕ ik jk
i i j
i=1 i=1j=1 k=1
whereϕ denotesthecomponentofϕinagivenbasis.
i
3.2 RENORMALIZATIONGROUP(RG)FLOWS
As mentioned in the introduction, the renormalization group refers to a collection of methods in
physics that aim to progressively remove the high-frequency degrees of freedom while retaining
the relevant low-frequency ones. In other words, one aims to remove the irrelevant details of the
physicalsystemwithoutalteringthephysicsatthelargerscaleoneisinterestedin. Bydoingso,one
hopestobeabletorobustlycalculatetheuniversalmacroscopicfeaturesofthephysicalsystems.
Therearemanywaysphysicistshaveproposedtoachievethisgoal,startingwiththeseminalworkof
Kadanoff(1966)andWilson(1971a;b). Howtoimprovetheunderstandingandtheimplementation
ofRGflows,includingeffortsinvolvingmachinelearningmethods,remainsanactivetopicofinves-
tigationinphysics(Koch-Janusz&Ringel,2018).HereweconsidertheexactRG(ERG)formalism,
a non-perturbative method pioneered by Polchinski (1984) for quantum field theories. In this RG
method, one implements Wilson’s idea of RG by specifying a cutoff kernel K (Λ) := K(k2/Λ2)
k
foragivencutoffscaleforeachfrequencyk,withthepropertythatK (Λ) → 1whenk ≪ Λand
k
K (Λ) → 0 when k ≫ Λ. With this, one erases information on frequencies much larger than Λ.
k
Oneexampleofsuchcutoffkernelsisthesigmoidfunction.
Givenaphysicaltheoryandachoiceofcutoffkernel,onecandefinephysicalprobabilitydistribution
p [ϕ] that satisfies a differential equation which is an infinite-dimensional version of the Fokker-
Λ
Planckequation3,wheretheroleofdiffusiontimeisplayedbyt=−log(Λ/Λ )forsomereference
0
scaleΛ .
0
3.3 STANDARDDIFFUSIONMODELS
In diffusion-based generative models, a forward diffusion process that gradually transforms data
samplesintonoisefollowingaparticularlysimpleSDEisinvertedtotransformnoiseintoimages.
AcommonlyusedforwardSDEisthefinite-variance1SDE(Songetal.,2021)definedas:
(cid:112)
dϕ=−1β(t)ϕdt+ β(t)dw, (4)
2
wheretheinitialvectorϕ(0)∈Rd representsthedatasampleanddwdenotesthestandardWiener
process. Thefunctionβ :[0,T]→R whichdeterminestheSDEisthepredefinednoiseschedule.
+
Thereverse-timeSDEfollowsfromspecializingequation2andreads
dϕ=(cid:2) −1β(t)ϕ−β(t)∇ logp (ϕ)(cid:3) dt+(cid:112) β(t)dw¯ , (5)
2 ϕ t
wheredw¯ isareverse-timeWienerprocess,and∇ logp (ϕ)isthescorefunctionofthemarginal
ϕ t
distributionattimet.Thetaskformachinelearningisthustoapproximatethescorefunction,which
canbeachievedbydenoisingscorematching(Vincent,2011)withtheobjectivefunction
L
DSM
=E t,ϕ(0),ϵ(cid:104) λ(t)(cid:13) (cid:13)s θ(ϕ(t),t)−∇ ϕ(t)logp t(ϕ(t)|ϕ(0))(cid:13) (cid:13)2(cid:105) , (6)
where ϵ ∼ N(0,I) is Gaussian white noise, ϕ(t) = α(t)ϕ(0)+σ(t)ϵ is the noised data at time
t, λ : [0,T] → R is a weighting function, and α(t),σ(t), and β(t) are functions capturing the
+
equivalentinformationaboutthenoisingschedule. Theyaredefinedinequation16byspecializing
β = β etc. Importantly, thechoiceofSDE(4)leadstoanOrnstein-Uhlenbeck(OU)processand
i
theconditionalscore∇ logp (ϕ(t)|ϕ(0))canbecomputedanalytically(Songetal.,2021).
ϕ(t) t
1Thisisreferredtoasthe“variance-preserving”(VP)diffusioninsomeliterature.Wewillreservetheterm
tocaseswhenthevarianceisactuallystrictlyconstantthroughoutthediffusionprocess,whichwewilldiscuss
in§4.4.
34 METHODS
4.1 DIAGONALIZABLEORNSTEINUHLENBECKPROCESS
Returning to the general diffusion SDE (1), we now consider the special case in which f = Fϕ
and F is ϕ-independent . This guarantees that the SDE describes a Ornstein-Uhlenbeck process
admittinganalyticalsolutionsfortheconditionaldistributionp (ϕ(t)|ϕ(0))requiredfordenoising
t
scorematching. Moreover,weconsiderachoiceofsimultaneouslydiagonalizableFandG:
F =M−1F˜M, G=M−1(cid:112) β (7)
withsomeconstantmatrixM anddiagonalβ =diag(β )andF˜ =diag(F˜).
i i
Intermsoftheparameterization
χ:=Mϕ, (8)
theSDEequation2isequivalenttoddecoupledSDEsoftheform
dχ =F˜(t)χ dt+(cid:112) β (t)dw (9)
i i i i
withthereverseSDEgivenby
dχ =(cid:16) F˜(t)χ −β (t)∇ logp (χ)(cid:17) dt+(cid:112) β (t)dw¯. (10)
i i i i χi t i
ThechoiceofthetransformationmatrixM isachoiceofdatarepresentationinwhichthediagonal
score-baseddiffusionbasedontheSDEequation9andequation10canbeefficientlyperformed.
Inparticular,astheWienerprocessisinvariantunderorthogonaltransformations,itisconvenientto
viewthechangeofbasis(givenbyM)asthecompositionofanorthogonal(U)andascaling(S)
transformation: M =S−1U. Intermsoftheoriginalϕvariables,theforwardSDEthenreads
dϕ=Fϕdt+U−1S(cid:112) βdw=U−1F˜Uϕdt+(cid:112)
β′dw′ (11)
√ √
where β′ = U−1 βU and dw′ = (cid:112) Σ dw is a Wiener process with covariance matrix
prior
Σ =U−1S2U.
prior
The choice of M, particularly the orthogonal part U, captures the freedom in our unified frame-
worktochoosethebasisinwhichthediffusionprocessisdiagonal. Moreover, thescalingS then
determines the choice of the noise (prior) distribution p = N(0,Σ ), which the forward
prior prior
processapproachesatlatetimes. Finally,notethattheβ (t)canaprioribeindependentfunctionsof
i
tforeachcomponenti. Thechoiceofβ (t)thuscapturesthechoiceofacomponent-wisenoising
i
schedule.
DuetothediagonalpropertyoftheSDE,thedenoisingscorematchinglossfunctionequation6for
learningtheSteinscore∇ logp (χ)canbestraightforwardlygeneralizedtotheGUDmodels:
χi t
(cid:88) (cid:12) (cid:12)2
L =E λ (t)(cid:12)s (χ(t),t)−∇ logp (χ(t)|χ(0))(cid:12) (12)
GUD t,χ(0),ϵ i (cid:12) i,θ χi(t) t (cid:12)
i=1,...,d
where the λ = (λ ,...,λ ) : [0,T] → Rd is the weighting vector. In our experiments, we let
1 d +
λ (t) = σ2(t), with the aim to scale the loss to be an order-one quantity and generalizing the
i i
common weighting factor λ(t) = σ2(t) in the standard diffusion loss (6) (Song & Ermon, 2019).
TheSDE(10)withthelearnedscore,whendiscretized,leadstoahierarchicalgenerativemodelwith
modeldensity
(cid:90) (cid:32) T (cid:33) (cid:32)T−1 (cid:33)
(cid:89) (cid:89)
p˜(χ(0))= ddχ(k) p(χ(ℓ)|χ(ℓ+1)) p˜(χ(1)) (13)
T T T
k=1 ℓ=0
whereT isthenumberofstepsinthediscretization,andthepriordistributionisgivenbythenoise
distributionp˜(χ(1))=N(0,I).
44.2 FINITE-VARIANCEDIFFUSIONANDTHESIGNALTONOISERATIO
Inthecoordinategivenbyχ,wenowfurtherspecializeequation9tothefollowingfinite-variance
diffusionprocess: withF˜(t)=−1β(t),thecorrespondingSDEreads
2
1 (cid:112)
dχ =− β (t)χ dt+ β (t)dw. (14)
i 2 i i i
Integratingtheabovegives
χ (t)=α (t)χ (0)+σ (t)ϵ, ϵ∼N(0,1) (15)
i i i i
where
(cid:18) 1(cid:90) t (cid:19)
α (t)=exp − β (s)ds , and σ (t)2 =1−α (t)2. (16)
i 2 i i i
0
Itfollowsthatthevariance
Var(χ (t))=α (t)2(Σ(χ)(0)) +σ (t)2 (17)
i i ii i
interpolates between 1 and the data variance (Σ(χ)(0)) , and is in particular finite at all stages of
ii
diffusion. Intheabove,wehaveusedthefollowingnotationforthedatacovariancematrix
(cid:104) (cid:105)
(Σ(χ)(0)) :=E (χ (0)−χ (0))(χ (0)−χ (0)) , where χ (0):=E [χ ],
ij pdata(χ(0)) i i j j i pdata(χ0) 0,i
Animportantquantitysignifyingthestageofthediffusionprocess(foreachcomponent)isthetime
evolutionoftheratiobetweenthesignalandthenoise,capturedbythesignal-to-noiseratio,
(cid:18) (α (t)χ (0))2(cid:19) α2(t)
SNR (t):=E i i =(Σ(χ)(0)) i , (18)
i σ (t)2 iiσ2(t)
i i
where the expectation is with respect to the data and the noise distribution, and we have assumed
that the data mean vanishes (which can always be made to be the case by subtracting the mean).
Notethatthisisdifferentfromthesignal-to-noiseratioquotedinsomediffusionmodelcontexts,
snr (t):=α2(t)/σ2(t)=e−γi(t), (19)
i i i
as this version does not take into accountthe magnitude of the signal in the data. As they depend
onlyontheschedule,wenotethatthefunctionsβ ,α ,σ andγ allcontainthesameinformation.
i i i i
Atagiventimet ∈ [0,T]inthediffusionprocess,theinformation
of γ(t) = (γ ,...,γ )(t) ∈ Rd is what we call the noising state, p data;α=1,γ →−∞
1 d
indicatingtheextenttowhichinformationinthedatahasbeenre-
placed by noise at that time. As a function of t, the evolution of
the noising state traces out a path connecting the data distribution
p ,correspondingto
data
α(γ)=(α 1,...,α d)=(1,...,1), p prior;α=0,γ →∞
andthepriordistributionp , correspondingtoα = (0,...,0), Figure 1: Different noising
prior
whereα =sigmoid(−γ )1/2asinequation19.Thedifferentpaths schedulesγ(t).
i i
correspondtodifferentOrnstein-Uhlenbeckprocesses,asdefinedin
equation14,withdifferentdiffusiondynamics.
Thishighlightsthefactthatthefreedomincomponent-wisenoisingschedulesintheGUDmodelis
fundamentallylargerthanthefreedominthenoisingscheduleinstandarddiffusionmodels,which
isgivenbydifferentchoicesofthefunctionγ(t)withγ (t)=γ (t)=γ(t). Inthiscase,allchoices
i j
ofγ(t)traceoutthesamediagonalpath(aslongastheboundaryvaluesγ(0),γ(T)areheldfixed)
andmerelyamounttodifferenttimeparameterizations(Kingmaetal.,2023). Incontrast,different
component-wiseschedulesgenericallycorrespondtogenuinelydifferentpaths,asillustratedinthe
schematicFigure1wherethecontinuouslinecorrespondstothestandarddiffusionschedule(with
anytimeparametrization)andthedashedlinesrepresentotherpossiblecomponent-wiseschedules.
54.3 UNIFICATIONVIASOFT-CONDITIONING
TheaboveformofSNRclarifiesanimplicithierarchicalstructureofthestandarddiffusionmodels:
even when γ (t) = γ(t) is identical for all components i, the components with larger amplitudes
i
have larger signal-to-noise ratio SNR (t) = (Σ(χ)(0)) e−γ(t), and are in this sense less “noised”
i ii
throughoutthediffusionprocess.Asaresult,thegenerationprocessequation13andinparticularthe
modelingoftheprobabilityp(χ(ℓ)|χ(ℓ+1))conditionalonthepreviousstateisimplicitlyaprocess
T T
ofgeneratingthelessimportantfeatures(withsmalleramplitude)conditionalonthemoreimportant
features(withlargeramplitude)thathavealreadybeenpartiallygenerated. Itisclearthatbymaking
more general choices of component-dependent noising schedules γ (t) one can tune the degree of
i
this soft-conditioning property, as we will explore in the experiments below. In the extreme case
when the support of β (t) and β (t), namely the “active time” for the ith resp. jth component,
i i̸=j
donotoverlap,wearriveatautoregressivegeneration,inwhichonefeature/token(oronegroupof
features/tokens)isgeneratedateachtime,conditionalonthosethathavebeengeneratedalready.See
Figure6forthevisualizationofaspecificexample.Inthisway,thefreedomtochooseacomponent-
dependentnoisingscheduleinourGUDmodelenablesustointerpolatebetweenstandarddiffusion
andautoregressivegeneration.
4.4 WHITENING
A particularly interesting choice for the matrix M = S−1U is the orthogonal transformation U
that diagonalizes the data covariance matrix Σ(ϕ)(0), and the diagonal matrix S−1 that performs
a whitening transformation. In other words, we choose S and U such that the data covariance
matrix matches Σ(ϕ)(0) = U−1S2U. Note that M is then precisely the familiar PCA transfor-
mationfollowed byawhitening transformationwhichmakesthe varianceuniform. Inthe context
of diffusive generation, such a basis has the following appealing features. First, the softness of
thesoft-conditioning, manifestedviatheevolutionofthesignal-to-noiseratioequation18, isnow
completely controlled by the component-wise schedule γ (t), which can make the design process
i
ofthediffusionmodeledmorestreamlinedanduniformacrossdifferentapplicationswithdifferent
datasets. Second, with such a choice the covariance matrix actually remains constant throughout
thediffusionprocessasthedatacovarianceΣ(χ)(0) = Iisnowthesameasthenoisecovariance,
and the finite-diffusion equation 14 is variance preserving in the strict sense. In other words, the
conditionalnumberofthecovariancematrixisalwaysoneandthegenerativeprocessdoesnotneed
to alter the second-order statistics. We expect this property to be beneficial in some situations for
learninganddiscretization.
4.5 NOISING-STATECONDITIONALNETWORKARCHITECTURE
Forthescorenetworkarchitecture,wefollowtheapproachofpredictingthenoiseϵgivenanoised
image(Hoetal.,2020),trainedviadenoisingscorematching(Vincent,2011). Instandarddiffusion
models,thisscorenetworkistypicallyconditionedonthetimevariableoranequivalentobjectsuch
as γ(t) (Kingma et al., 2023). The introduction of a component-wise schedule in our framework
suggestsgeneralizingthisbyconditioningthemodelonthemoreinformativecomponent-wisenois-
ing state, represented by the component-wise noise state γ(t) = (γ ,...,γ )(t). Since this is a
1 d
vectorofthesamedimensionasthedataandnotascalar,amodificationofthenetworkarchitecture
is required. We have implemented this by incorporating cross-attention between the data and the
noisingstate,furtherdetailscanbefoundinsectionAoftheappendix.
Sinceanychoiceofthenoisingscheduleγ (t) = γ(t)instandarddiffusionmodelscanbethought
i
of as just a reparametrization of time (Kingma et al., 2023) (cf. §4.2), the diffusion time t itself
sufficesasafeatureforthenetworktoindicatethenoisingstate. ForourGUDmodels,thisistrue
only for a fixed schedule choice. By conditioning directly on γ instead of t, our score network is
directlyconditionedontheinstantaneousnoisingstate,andnotonthetotalityofitspath,namelythe
scheduleγ(t). Thisenablesustotrainasinglenetworkforarangeofschedules, aswewilldoin
theexperimentsdescribedinthenextsection. Thesetofvaluesγ ∈Rd usedduringtrainingbound
aregion,visualizedschematicallybytheshadedareainFig. 1. Thisisimplicitlytheregionofthe
valuesofγ ∈ Rd wherethescorefunctionhasbeenlearned. Thissuggeststhepossibilityofusing
anyparticularpathwithintheshadedregionforgeneration,whichmightdifferfromthepathused
6fortraining(indicatedbythedashedlinesinFig. 1). ThisfeatureoftheGUDmodelmayfacilitate
thenumericaloptimizationofcomponent-wiseschedulesinfuturework.
5 EXPERIMENTS
WewillnowshowcasetheflexibilityoftheGUDmodelwithsomeexamples,andconductprelim-
inary investigations into the effects of these different design choices on the behavior of diffusion
modelsandtheirresultingsamplingquality. Anoverviewoftheexperiments,highlightingtherele-
vantdesignchoices,isgiveninthefollowingtable.
§5.1 §5.2 §5.3
basis pixel,PCA,FFT column wavelet⊗column
isotropicGaussianand
prior isotropicGaussian isotropicGaussian
variance-matchingGaussian
varyingsoftness
noisingschedule varyingsoftness varyingsoftness
andorderingvariables
otherapplications imageextension
5.1 SOFT-CONDITIONINGSCHEDULES
unwhitened, a=1 unwhitened, a=0 whitened, a=1 whitened, a=0
1.0
0.5
0.0
15
0
-15
20
denoise
0
noise
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Time t Time t Time t Time t
Figure 2: For eight of the PCA components χ of CIFAR-10, we visualize the OU noise level σ (t), the
i i
correspondingnoisingpathγ (t) = logit(σ2(t))forthelinearscheduleofequation20,andthecorrespond-
i i
ingsignal-to-noiseratio. Bluedashedlinesindicatechosenminimalnoising/reconstructionlevels. Fromleft
to right: (a) Standard diffusion where γ = γ . (b) The schedule γ is chosen such that logSNR (t) =
i j i
logSNR (t),correspondingtoagenerativeprocesswithnohierarchy.(c)Withwhiteneddata,withthesched-
j
uleγchosensuchthatlogSNR (t)isidenticaltothatinthestandarddiffusioncaseshownincolumn(a). (d)
i
Hierarchy-lessgenerationwithwhiteneddata.
First, we investigate the effect of choosing different bases, priors, and schedules by conducting
experiments on the unconditional generation of CIFAR-10 images. As an example of a simple
setup, we choose linear noise schedules, which means that all γ (t) we consider here are linear
i
functions of the diffusion time t. In the first setup, we choose the basis, given by the orthogonal
transformationU describedin§4.1,tobethePCAbasis. Intermsofpriordistributions,wechoose
ournoisetobeeithergivenbyisotropicGaussianorGaussianwithcovariancematchingthatofthe
data. Alternatively,asexplainedin§4.1,thisisequivalenttowhiteningthedata(inthePCAbasis)
while using the isotropic Gaussian noise. We will therefore refer to the two choices of priors as
whitenedand unwhitenedinwhatfollows.
In the second setup, we use the Fourier basis and consider a two-parameter family of component-
wise noising schedules, where we vary the precise ordering of the different Fourier components
7
)t(
)t(
)t(RNS
gol(givenbythe“orderingvariables”)aswellasthesoftnessparameterofthesoft-conditioningsched-
ule.Itisimportanttonotethattheseexperimentsonlyconstituteaone-ortwo-dimensionalsubspace
ofamuchlargerdesignspace. Wemaketheabovechoiceforthesakeofconcretenessandduetoa
limitationincomputationalresources.
Linearcomponent-wiseschedules. Foragivenchoiceofbasisχ,letΣ := (Σ(χ)(0)) denote
i ii
thevarianceofeachcomponentχ . Asdiscussedin§4.2,achoiceofthescheduleγ(t)thenleads
i
to the signal-to-noise ratio logSNR (t) = −γ (t)+logΣ . For the diffusion generative models,
i i i
eachcomponentmustbesufficientlynoisedintheforwardprocessandsufficientlydenoisedinthe
reverseprocess.Itwillthereforebeconvenienttointroducetheparametersγ˜ andγ˜ which
denoise noise
specifytheminimallevelsofdenoisingandnoisingattheinitialandfinaltime,respectively:
γ˜ =−minlogSNR (t=0)
denoise i
i
γ˜ =−maxlogSNR (t=1).
noise i
i
Hereandinwhatfollowsweparametrizethediffusiontimetobet∈[0,T]=[0,1].Theparameters
γ˜ and γ˜ have to be chosen carefully. In particular, to employ the inverse process as a
denoise noise
generativemodel,thedistributionatthefinaltimet=1mustbesufficientlyclosetothepriornormal
distributionfromwhichwedrawinitialsamples.Thisisnotguaranteedbydemandingendpointsfor
theSNRalone, asalowSNRcanbeachievedwithlittlenoiseifthedatamagnitudeissmall. We
thereforealsorequireσ (t=1)≥σ ,whichtranslatestoγ˜ ≥logit(σ2 )−min logΣ .
i min noise min i i
Next,weassociateanorderingvariablel toeachcomponent,whichwilldeterminethehierarchical
i
structureamongthecomponentsbyshiftingtheonsetofnoiseintheforwardprocess. CIFAR-10,
justlikemanyother(natural)imagedatasets,isanexampleofwhatmightbecalledfrequency-based
datasets. Weusethistermtodescribethedatasetswithanaturalmeaningoflocality,whosecovari-
anceisapproximatelydiagonalizedintheFourierbasisandwhosevarianceisgenerallydecreasing
withincreasingfrequencies(Tolhurstetal.,1992;Field,1987). Forthesedatasets,thehierarchical
structurecannaturallybespecifiedintermsoftherelatednotionsofvariance,frequencies,andreso-
lution,asfamiliarfromimageprocessing. Wechooseourorderingvariablel tocapturethisnotion
i
ofhierarchy.
Tocontrolthelevelofautoregressiveness,orsoftness,ofthesoft-conditioninglinearschedule,we
introduceaparametera>0anddefinethelinearlyinterpolatingschedule
γ (t)=γ +(γ −γ )t (20)
i min,i max,i min,i
withtheendpointsgivenby
γ :=γ˜ +logΣ +a(l −l )
min,i denoise i i max
(21)
γ :=γ˜ +logΣ +a(l −l ).
max,i noise i i min
Here l = max l and similarly for l . The
max i i min
larger a is, i.e. the smaller the softness 1/a, the unwhitened whitened
moreautoregressivetheschedulebecomes.Onthe NLL (bits/dim) FID
otherhand,inthelimitofextremesoftness(small
a)thehierarchicalnatureofthegenerativemodel 3.15 30
disappears. 3.10
Theparametersofourlinearschedulesarethusthe 3.05 20
ordering variables l , the softness parameter a−1, 0.75 1.00 1.25 0.75 1.00 1.25
i
and the SNR endpoints given by γ˜ , γ˜ . Inverse softness a Inverse softness a
denoise noise
In our experiments, we fix γ˜ = −7 and
denoise
γ˜ = max[3,logit(σ2 )−min logΣ ] with Figure3: Dependenceofmodelqualityintermsof
noise min i i
σ =0.99. negativelog-likelihood(left)andFID(right)onthe
min
softness parameter for the linear schedule in §5.1.
ThescheduleisdefinedinPCAcomponentsandre-
SoftnessinPCAspace. Inthefirstexperiment,
sults are shown both for unwhitened and whitened
weapplytheabovelinearscheduleinthewhitened data scaling (i.e. white and data-matching priors).
andunwhitenedPCAbases. Wechoosetheorder- TrainingonCIFAR-10usingasinglescore-network
ing variable to be given by l = −logΣ , which foreachchoiceofscaling.Standarddiffusioncorre-
i i
allowsforthetrajectoryofthesignal-to-noiseratio spondstoa=1intheunwhitenedcase.
8(a) (c)
(b) (d)
Figure 4: Diffusion forward process for a single image of CIFAR-10: (a) standard diffusion, (b) variance-
matchingGaussiannoisewithsameSNRasstandarddiffusion,(c)column-wisesequentialscheduleof§5.2
withb=0.5,(d)combinationofHaarwaveletandcolumn-sequentialscheduleof§5.3witha=0.5,andwith
variance-matchingGaussiannoise.
b = 0.4 b = 0.7
50
3.2 zoomed in region
0.2
0.2
0
0.0
1
0.2 0.0
3.1
0.9 1.0 1.1
0
0.75 1.00 1.25 0 1 0 1
Inverse softness a Time t Time t
Figure 5: The model quality with a two-parameter Figure6: Anillustrationofcolumn-wiseschedule
familyofschedulescontrollingthesoftnessandtheor- of§5.2for5columnsanddifferentsoftnessparam-
dering parameters. The right figure is the region in eterb−1. Thelargerb,themoreautoregressivethe
the box on the left and the same color map is shared. model is, as the overlap of the “active” times with
The black dots indicate the parameters corresponding noisingrateβ >0decreases,andsimilarlyforthe
i
tostandarddiffusionmodels. suppressionfactorsα .
i
ofthestandarddiffusionmodeltobereproducedat
a=1,alsowhenthepriorhasbeenchangedtohavethesamecovarianceasthedata(seeFig.2(c)).
We trained a single score network for the (inverse) softness parameter in the range a ∈ [0.4,1.6]
by randomly sampling a at each training step. Figure 3 shows the negative log-likelihood (NLL)
andFIDevaluatedfordifferentvaluesofa2. Interestingly, theunwhitenedconfigurationperforms
betterwhenmeasuredbyNLL,butworseintermsofFID,withthestandarddiffusionsetup(a=1)
appearingclosetooptimal. SeesectionBoftheappendixforfurtherexperimentaldetails.
Orderingvariables. WealsoperformexperimentsintheFourier(FFT)basis, forwhichtheRG
physics reviewed in §3.2 naturally suggests an ordering of noising based on the frequency |k |.
i
To test the dependence of the quality of the model on ordering parameters, we consider ordering
variables l = (1 − r)(−logΣ ) + r(|k | + δ)/κ, parametrized by r ∈ [0,1] and interpolating
i i i
between l = −logΣ as in the PCA experiments and the frequencies |k |. The slope and offset
i i i
parameters, κ and δ, are chosen such that the range of l are the same at r = 1 as at r = 0. We
i
trained a score network for a range of values of r in addition to the softness parameter a−1 on
CIFAR-10,withevaluationresultsshowninFigure5. Wefindtheoptimalperformanceintermsof
NLLislocatedslightlyawaybutclosetostandarddiffusion. Inthisexperiment,wechoosetheprior
tobetheisotropicGaussian(i.e.unwhitened).
5.2 SEQUENTIALGENERATIONINREALSPACE
While the previous experiment explores the GUD model in the context of multi-scale hierarchical
generation,itcanequallybeappliedtoperformsequentialgenerationinpixelspace,aswewillnow
demonstrate with a soft-conditioning column-wise generation model. Grouping the components
accordingtotheircolumninthepixelspaceofsizeL×L,weindextheschedulesaccordingtothe
columnslabeledbyi=1,...L.
Asbefore,weperformexperimentswithalinearschedule
(cid:18) (cid:19)
γ −γ L−i
γ (t)=clip γ +(t−t ) max min , with t =b (22)
i γmin,γmax min i 1−b i L−1
2TheFIDhasnotconvergedandwasstilldroppingatasignificantratewhenweterminatedtrainingdue
tolimitedcomputationalresources. Nevertheless,thequalitativefeaturesdisplayedinFigure3appeartohave
stabilized.
9
r
noitalopretni
gniredrO
)t(
)t(withavaryingdegreeofsoftness/autoregressivity,nowparametrizedbyb. Theclipping,definedby
theclippingfunctionclip (x):=max(y,min(z,x)),hastheeffectoffreezingthecolumnswhen
y,z
thedesignatednoising(γ )orreconstructionlevel(γ )isreached.
max min
Training on PCAM dataset. We trained separate score networks at b = 0.3 and b = 0.5 on
the PCAM dataset (Veeling et al., 2018), downscaled to 32 × 32 pixels, obtaining negative log-
likelihoodsof3.90and3.94bits/dim,respectively.
Image extension. Since images are generated column-by-column from left to right, conditioned
onthepartontheleftthathasalreadybeengenerated,thissuggeststhatthegenerativeprocesscan
be repeated. See section C of the appendix for further detail. Figure 7 shows three examples of
imagestripsgeneratedinthismanner,usingthescorenetworktrainedon32×32imagesatb=0.5.
Figure7: Imagesgeneratedusingthecolumn-wiseschedulebyrepeatedlyaddingnoisetotheright,usinga
scorenetworkthatwasonlytrainedonsquaretrainingdata.
5.3 HAARWAVELETS
Tofurthershowcasetheversatilityofourunifiedframework,weintegrateHaarwaveletdecompo-
sitionwithacolumn-wisenoisescheduleamongthewaveletcomponentsateachhierarchicallevel.
Thisextendsthewavelet-conditionedscorematchingofGuthetal.(2022)byincludingaparameter
allowingforsoft-conditioning,andincorporatingcolumn-wisesequentialnoisingateachlevel.
Concretely,weusetwoparametersaandbtoparametrizethe(inverse)softnessamongthedifferent
levelsofwaveletcomponentsandthecolumnswithineachlevel,respectively. SupposethereareN
hierarchicallevelsofwaveletdecompositions, labeledbyi = 1,...,N, andthereareL columns
i
inthei-thlevel,indexedbyj =1,...,L ,wedefinetheoffsets
i
N −i L −j
c =a , c =b i .
i N −1 ij L −1
i
Withthis,wespecifythelinearschedulefora,b∈[0,1]tobe
(cid:18) (cid:19)
t −c
γ (t)=clip γ +(γ −γ ) i ij ,
ij γmin,γmax min max min 1−b
wheret i =clip 0,1(t− 1c −i a).SeeFigure4(d)foravisualizationofthisschedule.
We trained a score network for a ∈ [0.3,0.7] and b ∈ [0.3,0.7] on CIFAR-10 for 300k steps and
usingN = 3levels. Similartotheresultsin§5.1,themodelqualityagaindependsonthesoftness
parameters,withthelowestNLLvaluereachedbeing3.17bits/dim.
6 CONCLUSIONS
Inthiswork,weproposedtheGUDframework,whichnaturallyintegratesnoveldesignfreedomsin
diffusion-basedgeneration.Notably,theframeworkeliminatestherigidboundarybetweendiffusive
and autoregressive generation methods and instead offers a continuous interpolation between the
two. Thisflexibilitypavesthewayforabroadrangeofpotentialapplications.
First,ourexperimentsindicatethatthechoicesinallthreeaspectsweinvestigateinthepresentwork
–thediagonalbasis,thepriordistribution,thecomponent-wiseschedule–dohaveaninfluenceon
thefinalqualityofthemodel. Asaresult, thereispotentiallyvastroomtoimprovethequalityof
10diffusionmodels. Infuturework, wewilladdressthequestionoftheoptimizationofthesedesign
choices.
Second,theflexibilityofourframeworkenablesseamlessintegrationofvariousapproachestogen-
erativemodels.Forinstance,weillustratedin§5.3thepossibilitytocombinehierarchicalgeneration
(inthewaveletbasis)withsequentialgeneration,andin§5.2howourframeworkcanreadilybeused
toextendimages.Similarly,theinpainting,coloring,upscaling,andconditionalgenerationtaskscan
allberealizedandgeneralizedwithintheGUDframework, viaanappropriatechoiceofbasisand
component-wiseschedules.
While the scope of our numerical experimentations and our ability to optimize important hyper-
parameters has been limited by the compute resources available to us, we believe our theoretical
frameworkhasthepotentialtoleadtomoreefficientdiffusionmodels,awiderangeofapplications,
andnovelarchitecturedesigns.
ACKNOWLEDGEMENTS
ThisworkhasbeensupportedbytheVidigrant(number016.Vidi.189.182)fromtheDutchResearch
Council(NWO).MGwaspartiallysupportedbyaprojectthathasreceivedfundingfromtheEuro-
pean Research Council (ERC) under the European Union’s Horizon 2020 research and innovation
programme(GrantagreementNo. 864035).
REFERENCES
Brian D. O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their
Applications,12(3):313–326,May1982.ISSN0304-4149.doi:10.1016/0304-4149(82)90051-5.
DavidSBerman,MarcSKlinger,andAlexanderGStapleton. Bayesianrenormalization. Machine
Learning: ScienceandTechnology,4(4):045011,2023.
Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitz-
mann. DiffusionForcing: Next-tokenPredictionMeetsFull-SequenceDiffusion,July2024.
Jordan Cotler and Semon Rezchikov. Renormalizing diffusion models, 2023. URL https://
arxiv.org/abs/2308.12355.
David J. Field. Relations between the statistics of natural images and the response properties of
corticalcells. J.Opt.Soc.Am.A,4(12):2379–2394,Dec1987. doi: 10.1364/JOSAA.4.002379.
Jose´ Gaite. The exact renormalization group in astrophysics. International Journal of Modern
PhysicsA,16(11):2041–2046,2001.doi:10.1142/S0217751X01004670.URLhttps://doi.
org/10.1142/S0217751X01004670.
Florentin Guth, Simon Coste, Valentin De Bortoli, and Stephane Mallat. Wavelet Score-Based
GenerativeModeling,August2022.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In
H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-
ral Information Processing Systems, volume 33, pp. 6840–6851. Curran Associates, Inc.,
2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/
file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.
JonathanHo, ChitwanSaharia, WilliamChan, DavidJ.Fleet, MohammadNorouzi, andTimSali-
mans. Cascadeddiffusionmodelsforhighfidelityimagegeneration. J.Mach.Learn.Res.,23(1),
January2022. ISSN1532-4435.
EmielHoogeboomandTimSalimans. Blurringdiffusionmodels,2024. URLhttps://arxiv.
org/abs/2209.05557.
LeoP.Kadanoff. ScalinglawsforisingmodelsnearT . PhysicsPhysiqueFizika,2:263–272,Jun
c
1966. doi: 10.1103/PhysicsPhysiqueFizika.2.263. URL https://link.aps.org/doi/
10.1103/PhysicsPhysiqueFizika.2.263.
11PatrickKidger. OnNeuralDifferentialEquations. PhDthesis,UniversityofOxford,2021.
Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational Diffusion Models,
April2023.
Maciej Koch-Janusz and Zohar Ringel. Mutual information, neural networks and the renormal-
ization group. Nature Phys., 14(6):578–582, March 2018. ISSN 1745-2481. doi: 10.1038/
s41567-018-0081-4.
Bernt Oksendal. Stochastic Differential Equations (3rd Ed.): An Introduction with Applications.
Springer-Verlag,Berlin,Heidelberg,1992. ISBN3387533354.
JosephPolchinski. RenormalizationandEffectiveLagrangians. Nucl.Phys.B,231:269–295,1984.
doi: 10.1016/0550-3213(84)90287-6.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolution image synthesis with latent diffusion models. CoRR, abs/2112.10752, 2021. URL
https://arxiv.org/abs/2112.10752.
Dohoon Ryu and Jong Chul Ye. Pyramidal denoising diffusion probabilistic models, 2022. URL
https://arxiv.org/abs/2208.01864.
Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano Ermon. D2C: diffusion-denoising
models for few-shot conditional generation. CoRR, abs/2106.06819, 2021. URL https:
//arxiv.org/abs/2106.06819.
JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Deepunsupervised
learning using nonequilibrium thermodynamics. In Francis Bach and David Blei (eds.), Pro-
ceedingsofthe32ndInternationalConferenceonMachineLearning,volume37ofProceedings
of Machine Learning Research, pp. 2256–2265, Lille, France, 07–09 Jul 2015. PMLR. URL
https://proceedings.mlr.press/v37/sohl-dickstein15.html.
YangSongandStefanoErmon.Generativemodelingbyestimatinggradientsofthedatadistribution.
CoRR,abs/1907.05600,2019. URLhttp://arxiv.org/abs/1907.05600.
YangSong,JaschaSohl-Dickstein,DiederikP.Kingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-BasedGenerativeModelingthroughStochasticDifferentialEquations,2021. URL
http://arxiv.org/abs/2011.13456.
D. J. Tolhurst, Y. Tadmor, and Tang Chao. Amplitude spectra of natural images. Ophthalmic
andPhysiologicalOptics,12(2):229–232,1992. doi: https://doi.org/10.1111/j.1475-1313.1992.
tb00296.x.
ArashVahdat,KarstenKreis,andJanKautz.Score-basedgenerativemodelinginlatentspace,2021.
URLhttps://arxiv.org/abs/2106.05931.
BastiaanSVeeling,JasperLinmans,JimWinkens,TacoCohen,andMaxWelling. Rotationequiv-
ariantCNNsfordigitalpathology,June2018.
Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural
Computation,23(7):1661–1674,July2011. ISSN0899-7667. doi: 10.1162/NECO a 00142.
KennethG.Wilson.Renormalizationgroupandcriticalphenomena.i.renormalizationgroupandthe
kadanoffscalingpicture.Phys.Rev.B,4:3174–3183,Nov1971a.doi:10.1103/PhysRevB.4.3174.
URLhttps://link.aps.org/doi/10.1103/PhysRevB.4.3174.
KennethG.Wilson. Renormalizationgroupandcriticalphenomena.ii.phase-spacecellanalysisof
criticalbehavior. Phys.Rev.B,4:3184–3205,Nov1971b. doi: 10.1103/PhysRevB.4.3184. URL
https://link.aps.org/doi/10.1103/PhysRevB.4.3184.
Jean Zinn-Justin. Quantum field theory and critical phenomena. Int. Ser. Monogr. Phys., 113:1–
1054,2002.
12A NOISING STATE CONDITIONAL SCORE NETWORK ARCHITECTURE
Inspiredbytechniquesinconditionalgenerationtasks(Rombachetal.,2021),weintroducedacross-
attention mechanism between the intermediate embeddings of the image and the component-wise
noising state γ, allowing the network to effectively modulate its predictions based on the noising
state at each stage of the diffusion process. Otherwise, we follow a U-Net architecture similar to
Songetal.(2021),
Toincorporateadditionalinformationonthestructureofthedata, wefirstconcatenatethenoising
stateγ withpositionlabelsspecifictotheapplicationandthechoiceofbasis. Forinstance,forthe
PCA example in §5.1 this is the negative logarithm of the variance of each component, which is
alsousedastheorderingvariable. FortheexperimentinFourierspace,weusedtheFFTfrequency
label|k|. In§5.2-5.3weusedasequenceofintegerswhichincrementsbyoneforeachsubsequent
columnandadjacentgroupofHaarwaveletcomponents,respectively. Theconcatenatedinputsare
thenprocessedthroughMLP-Mixerlayerstofacilitatelearnedembeddingsofγ. Alongthedepth
oftheU-Net,asingledenselayerisusedtoreducethespatialextenttothatofthecoarseimagesat
thatlevel,beforetheyareinputintothecross-attention.
B EXPERIMENTAL DETAILS
PCA and Fourier bases. In our experiments we make use of datasets of colored images, which
havepixelandcolorchannelindices. AmongourchoicesofbasisistheFourierbasis. TheFourier
transform (specifically the fast Fourier transform) is applied independently in each color channel.
TohaveananalogousPCAbasis,wehavedecidedtoperformthesameorthogonaltransformation
–theonecorrespondingthePCAbasisofthecolor-averageddata–ineachcolorchannel. Itcould
beinterestingtoinvestigatefurtherchoices,includingthePCAtransformationthatmixesthecolor
channels.
Training. Unlessspecifiedotherwise,trainingwasdonewithabatchsizeof128usingtheAdam
optimizerwithalearningrateof5×10−4.Thevalidationparametersusedtoevaluatesamplequality
are exponentially moving averages updated at a rate of 0.999. The diffusion times for denoising
score matching are sampled uniformly in [0,1], and schedule parameters (where applicable) were
drawnuniformlyfromthespecifiedrangeforeachtrainingbatchofsamples.
ThescorenetworksforbothCIFAR-10andPCAMweretrainedfor300ktrainingstepsonNVIDIA
A100 and H100s. After this time the NLL appears to have converged. However, the FID in the
caseofCIFAR-10keepsdecreasingwithfurthertraining. Fortheexperimentsof§5.1trainingwas
thereforeresumedandcontinuedforanother100kstepsandwithabatchsizeof512samples. The
FIDhasapparentlynotyetreacheditsminimumfortheshownvalues,andweintendtoupdatethese
oncewehavegainedaccesstofurthercomputationalresources.
Datasetprocessing. Wehaveusedauniformlydequantizedversionofthedataset,bothfortrain-
ingandevaluation,byfirstaddinguniformnoisetoeachquantizedpixelvalueandthenrescalingit
to[−1,1]. Wehaveadditionallyremovedtheempiricalmeanofthedataset,computedonalltrain-
ing data. Otherwise, the meanwould have tobe takeninto accountwhen definingthe magnitude-
sensitive SNR, instead of just the variances as discussed in §4.2, and dividing by the mean when
“whitening”couldleadtoextremelylargevalueswhenthevarianceofacomponentismuchsmaller
thanitsmean.
Scoresrepresentations. Thescoresindifferentdatarepresentations,e.g. theoriginaldataϕand
thechosencomponentsχinthenotationof§4.1,arerelatedby
∇ logp (χ)=SU†∇ logp (ϕ), (23)
χ t ϕ t
andwecanalwaysgoback-and-forthbetweenthescoresinbothbasis.
As we base our architecture on the commonly used convolutional U-net architecture as in (Song
et al., 2021), which implicitly assumes the locality and approximately shift-symmetric properties,
we let the inputs and outputs of the score network to be always represented in the original image
spaceandnotthechosenPCA,Fourier,orotherbasis.
13Evaluation and sampling. Evaluation of the negative log-likelihood was done using the ODE
correspondingtothereverseSDEequation10. Specifically,theSDEofequation14,i.e.
1 (cid:112)
dχ =− β (t)χ dt+ β (t)dw (24)
i 2 i i i
hasacorrespondingdeterministicODEthatproducesthesamemarginalprobabilities,givenby
1
dχ =− β (t)(χ +∇ logp (χ))dt . (25)
i 2 i i χi t
WeusetheaboveODEtocomputethelog-likelihoodsofthedataunderthetrainedmodels,withthe
scoreabovereplacedbyitslearnedapproximation(formoredetailswerefertoSongetal.(2021)).
We used 6144 samples in each computation, averaging over 3 different slices of the Hessian in
Hutchinson’straceestimatorforeachsample. Fortheintegrator,weusedTsitouras’5/4methodas
implemented in Kidger (2021) with adaptive step size and both relative and absolute tolerance of
1×10−4.
C REPEATED COLUMN-WISE GENERATION
Thereal-spacesequentialcolumnof§5.2generatesimagesconditionalontheleftpartoftheimage
thathasalreadybeendenoised, afteraninitialstageinwhichthefirstcolumnsgetdenoised. This
immediatelysuggestsanapplicationinreconstructinganimagethatisonlypartiallyavailable. Fig-
ure8showshowpartiallynoisedimagescanbereconstructedbyfillingintherighthandsideofthe
image. Differentchoicesofrandomkeythengenerateslightlydifferentcompletions.
The linear column-wise schedule in equation 22 is defined such that by integrating the diffusion
processbyatime∆t = b/(L−1),the“noisingfront”iseffectivelymovedbyonepixel. Inother
words,intheforwardprocess,aparticularcolumnafterthistimehasreachedthesameSNRitsleft
neighborhadattheprevioustime. Startingfromanimageatnoisingtimet>0,wecangeneratein
principalinfinitelylongstripsoftexture. First,wedenoiseusingthelearnedscorefromttot+k∆t
with k < L a positive integer. Then, we cut off the first k columns and store them for the final
image. Next, we append k columns of noise drawn from the prior to the right of the image. As
longasthesoftnessparameterbwaschosensufficientlylargegiventheparticulark,thiseffectively
restorestheimagetothenoisingstateattheoriginaltimet. Thisprocesscanthusberepeated,and
byconcatenatingthepreviouslygeneratedleft-sidecolumns,aconnectedrectangularstripeofimage
is constructed. As an example, in Figure 7 we show the results with b = 0.5 and k = 9. Finally,
notethatthisprocedureonlyworksifthetrainingdataisapproximatelytranslationallyinvariant.
Figure8:Reconstructionofimagesfromthetestset(leftcolumn)partiallynoisedtot=0.5(secondcolumn)
usingasequentialscheduleinrealspaceasdescribedin§5.2.Thedifferentreconstructionsshownontheright
differbyrandomkeyused.
D HAAR WAVELETS
The2DHaarwavelettransformdecomposesanimageX ∈RN×N×C intolow-andhigh-frequency
components across multiple scales. We apply the same wavelet transform to each color channel
14c = 1,...,C (forusC = 3)separately. Therefore,toeasethenotationwewillsuppressthecolor
indexinwhatfollows. Thewavelettransformatlevelnisdefinedrecursivelyasfollows:
1. RowTransformation Applythe1DHaartransformalongtherows:
(cid:16) (cid:17)
L(n) = √1 X(n−1)+X(n−1) ,
i,k 2 2i,k 2i+1,k
(cid:16) (cid:17)
H(n) = √1 X(n−1)−X(n−1) ,
i,k 2 2i,k 2i+1,k
wherei=0,..., N −1andk =0,...,N −1.
2n
2. ColumnTransforms Applythe1DHaartransformalongthecolumnstotheresultsoftherow
transformation:
(cid:16) (cid:17)
LL(n) = √1 L(n) +L(n) ,
i,j 2 i,2j i,2j+1
(cid:16) (cid:17)
LH(n) = √1 L(n) −L(n) ,
i,j 2 i,2j i,2j+1
(26)
(cid:16) (cid:17)
HL(n) = √1 H(n) +H(n) ,
i,j 2 i,2j i,2j+1
(cid:16) (cid:17)
HH(n) = √1 H(n) −H(n) ,
i,j 2 i,2j i,2j+1
wherej =0,..., N −1.
2n
3. High-Frequency Component: Stack the high-frequency sub-bands into a single high-
frequencyarrayatleveln:
(cid:16) (cid:17)
HF(n) =concat LH(n), HL(n), HH(n) . (27)
4. Recursive Decomposition The low-frequency component LL(n) becomes the input for the
nextlevel:
X(n) =LL(n). (28)
Ateachlevel,thetransformproducesonearrayoflow-frequencycomponentsLL(n) andonearray
ofhigh-frequencycomponentsHF(n).Thisprocesscanberecursivelyapplieduptoadesireddepth
N,resultinginahierarchicaldecompositionoftheimage.
After level N, the original image is represented by one lowest-frequency array and N higher-
frequencyarrays.
For example, for level 3 one obtains three high-frequency arrays HF(1), HF(2), HF(3), and one
coarse array LL(3). To accommodate images with multiple color channels C, the transform is
applied independently to each channel, and the resulting components are concatenated along the
√
channeldimension. Thefactorsof 2makesurethatthetransformisanorthogonaltransformation,
whoseinversecanbecomputedanalogously.
15