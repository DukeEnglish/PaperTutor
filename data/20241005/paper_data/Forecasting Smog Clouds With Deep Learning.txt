Forecasting Smog Clouds With Deep Learning: A Proof-Of-Concept
ValentijnOldenburg1 JuanCardenas-Cartagena12 MatiasValdenegro-Toro1
Abstract tionanddifficultyinbreathingtoanincreasedriskofcar-
diacandrespiratoryillnesses,cancer,andmortalityoverall
Inthisproof-of-conceptstudy,weconductmul-
(Brunekreef & Holgate, 2002; Kampa & Castanas, 2008;
tivariatetimeseriesforecastingfortheconcentra-
Wongetal.,2008;Orellanoetal.,2020). Arecentaddition
tionsofnitrogendioxide(NO ),ozone(O ),and
2 3
isitsdirectlinktoCOVID-19morbidityandseverity(Zorn
(fine)particulatematter(PM &PM )withme-
10 2.5
etal.,2024). AsstatedinLimetal.(2012), airpollution
teorologicalcovariatesbetweentwolocationsus-
rankshighinthegeneraldiseaseburdenattributabletoenvi-
ing various deep learning models, with a focus
ronmentalfactors,with3.1milliondeathsin2012and3.1% on long short-term memory (LSTM) and gated
ofdisability-adjustedlifeyearsworldwide.
recurrent unit (GRU) architectures. In particu-
lar,weproposeanintegrated,hierarchicalmodel Fundamentally,theairpollutionproblemandtheextentto
architecture inspired by air pollution dynamics whichitspreadsisevident. Indeed,theairanditscontam-
andatmosphericsciencethatemploysmulti-task inantsareeverywhereandwillremaininevitablyinherent
learning and is benchmarked by unidirectional tohuman-natureinteractioninthefuture. Positivenotions
andfully-connectedmodels. Resultsdemonstrate present themselves, nonetheless: (1) humans, being the
that,aboveall,thehierarchicalGRUprovesitself primary polluters, also possess the opportunity to act as
asacompetitiveandefficientmethodforforecast- the“primarypurifiers”;and(2)comprehensivefundamen-
ingtheconcentrationofsmog-relatedpollutants. talproblemknowledgeofferspositiveprospectsforfurther
advancingresearch(Vallero,2014).
This problem motivates the development and application
1.Introduction
of data-driven forecasting models based on multiple neu-
Airpollutionandsmogcarrycorrelationstonumerousper- ral network architectures, using contaminant and meteo-
vasivehealtheffects. Giventherisks,foreseeingtoxicpol- rological data to simulate and predict air pollution and
lutantlevelsposesavitalchallengethat, uponresolution, smog. In particular, we consider the modelling of nitro-
enactsaframeworkforlife-savingdecisions. Data-driven gendioxide(NO ),ozone(O ),and(fine)particulatematter
2 3
deeplearning(DL)methodsofferanovelapproachtoair (PM &PM )withvariousmeteorologicalcovariatesasa
10 2.5
qualityprediction,yettheirpotentialformodellingacom- firstproof-of-concept(PoC).Byemployingtheseweather-
binedsetofsmog-relatedpollutantswithrecurrentneural predictivemethods,thisstudyaimstocontributeincremen-
nets(RNNs)remainsunexplored. tallytounderstandingairpollutiondynamicsandenhance
environmentalconditionsforimprovedpublichealth.
The phenomenon of air pollution is characterised by the
presence of hazardous atmospheric chemicals. Although
1.1.State-of-the-art
anumberofphysicalactivities(volcanoes,fire,etc.) may
releasedifferentpollutants,anthropogenicactivitiesarethe Traditionalweathersystemshaveevolvedintosophisticated
headcauseofenvironmentalairpollution(Kampa&Cas- modelsthatapproximatereal-worldweatherdynamicswith
tanas,2008). increasingprecision(Alleyetal.,2019). Thesystemsap-
plynumericalweatherprediction(NWP),anowubiquitous,
Adverse air pollution effects can range from skin irrita-
thoughcomputationallycostly,numericalmethodgrounded
1Bernoulli Institute, University of Groningen, Gronin- onphysicalfirst-principles(Baueretal.,2015). Whileap-
gen, The Netherlands 2Wisenet Center, University of Agder, plyingpurelynaturallawsasboundaryconditionsforpre-
Grimstad, Norway. Correspondence to: Valentijn Olden-
dictionsistheoreticallypossible,itpresentschallengesin
burg<v.w.oldenburg@student.rug.nl>,JuanCardenas-Cartagena
practice: the weather system is everywhere and contains
<j.d.cardenas.cartagena@rug.nl>.
numerouscomplexprocessesthatmakeitcomputationally
AI for Science Workshop of the 41st International Conference infeasible to provide these predictions with more than a
onMachineLearning,Vienna,Austria. Copyright2024bythe highlysimplified,parameterisedvalue. Moreover,thenon-
author(s).
1
4202
tcO
3
]GL.sc[
1v95720.0142:viXraForecastingSmogCloudsWithDeepLearning
lineardynamics,exemplifiedbythechaoticbehaviouroftur- extent are models with the LSTM and GRU architecture
bulentflow,makepredictionsathighresolution—spatially, capableofthemultivariatetimeseriesforecastingofsmog-
temporally,and/oracrossvariables—alastingchallenge. related air components?” It is found that the LSTM and
GRUcanindeedaccuratelyforecastsmog-relatedaircom-
Theemergenceofdata-drivenmethodspresentsanovelap-
ponents,thusprovidinganeffectivepollutantmodellingand
proachtoabstractingphysicalprocessesembeddedinthe
forecastingmethod.
weathersystem. Machinelearning(ML)modelsareadept
atrecognisingcomplexpatternswithinlargedatasetswith
unparalleledefficiency—patternsthatmayrepresentrela- 2.Background
tionshipsandcorrelationsbetweenatmosphericvariables
Thissectionbrieflyexploresatmosphericinteractionsinair
andinfluencesnotyetunderstoodbytraditionalphysics.
pollutionandrecurrentneuralnets.
Arecentlyundertakenapplicationoflarge-scaleDLweather
forecastingisFourCastNetbyPathaketal.(2022). Four- 2.1.Atmosphericinteractions
CastNet generates global forecasts orders of magnitude
The very reality of contaminant concentrations changing
fasterthantraditionalNWPwithcomparableorbetteraccu-
overtimenaturallyfocusesattentiononthequestionofhow
racy. Itherewithdemonstratesthepotentialofdata-driven
thesechangesoriginateandevolve.Whereastheoriginsand
methodstomakesignificantprogressinweatherforecasting
sourcesofpollutionarereasonablywellunderstood(Vallero,
withoutexplicitlyconsideringtheunderlying(known)physi-
2014;Saxena&Naik,2018),muchisstillunknownabout
calprocessesandequations. Implicationsarereducingcosts
itsdynamicsandhowitevolves—hence,thesubjectofthis
ofthetraditionalNWPand,moreimportantly,reducingthe
research. EspeciallyfromaMLperspective,understanding
opportunitycostofinaccurateforecasts. Itsscopedoesnot,
thespecificrelationshipsbetweenvariables,orfeatures,is
however,encompasspredictingcomponentsdirectlyrelated
criticalforefficientlearning(Lietal.,2017).
toairpollutionorsmog.
How pollutants evolve is partly explicable by their inter-
More closely related state-of-the-art studies, see Masood
action with their environment. As a result, a combined
& Ahmad (2021) for a review, that distinctly forecast air
modelling of atmospheric variables can be justified. The
pollution are Freeman et al. (2018) and Tao et al. (2019).
followingparagraphsbrieflyintroducethepollutants’inter-
TheformerperformsaforecastofsurfaceO levelsusinga
3
connectivityandatmosphericinteraction.
recurrentneuralnetwork(RNN)withlongshort-termmem-
ory (LSTM); its approach takes as input hourly-sampled Foremost,thechemicalinterrelationofNO andO . Nitro-
2 3
meteorologicaldataandO 3itself,outputtingamultivariate genoxides(NO x)are(mostlyanthropogenically-generated)
72-hourhorizonforecast. Thelatter(Taoetal.,2019)high- primarypollutants. Wheninthepresenceofcertainvolatile
lightsacompositionof1Dconvnetsandthebidirectional organiccompounds(VOCs)oranotherinitiatororcatalyst,
gated recurrent unit (GRU) for a multivariate short-term NOcanoxidateintoNO (Atkinson,2000). Thephotode-
2
predictionofPM 2.5.Bothstudiesareconsistentandrelevant compositionofNO 2will,inturn,initiatethesequentialfor-
tothepurposeofthisPoCinthattheyuseRNNs,takeme- mationofsecondarypollutantO (Finlayson-Pitts&PittsJr,
3
teorologicalcovariatesasinputs,andconsequentlypredict 1993). It follows that, by exclusive consideration and as-
airpollution. Nonetheless,asmuchasO 3andPM 2.5arein- sumptionofthepresenceofphotonswithadequateenergy,
fluentialelements,amorecompleteairpollutionandsmog NO affectstheatmosphericquantityofO positively.
2 3
predictionrequiresconsiderationofabroaderandcombined
When no (or less) solar energy is present (e.g., at night),
setofaircontaminants.
NO remainsstableandreactswithO toformnitrateradi-
2 3
cal(NO )(Finlayson-Pitts&PittsJr,1993),loweringthe
1.2.Contributions 3
concentrations of NO and O (at least for now). There-
2 3
Acknowledging the recent developments (Masood & Ah- fore,owingtothechemicalinterdependenceofO andNO ,
3 x
mad,2021)andstate-of-the-art,theLSTMandGRUestab- the levels of O and NO are inextricably linked (Clapp,
3 2
lishthemselvesastheappropriatechoiceformodellingthe 2001). Besides,wealreadyobservethatonecannotdissoci-
sequentialseriesofcomponentsinambient(polluted)air. atepollutantconcentrationsfromatmospheric(andcosmic)
Thisinsightsteersustowardscontributinganattemptatget- influences.
tingfurthercommandoftheairpollutionproblemthrough
Continuing, PM is either emitted directly into the atmo-
a PoC of smog modelling with LSTM and GRU models.
sphere(primary)orformedlater(secondary)andissubject
Specifically,thecombinedmodellingofcontaminantsNO ,
2 to air transport, cloud processing, and removal from the
O ,PM ,andPM isconsidered.
3 2.5 10 atmosphere (Seinfeld & Pandis, 2016). PM and PM
10 2.5
Ultimately,thisresearchaddressesthequestion: “Towhat areinterconnectedasseenempirically(Veldersetal.,2015)
2ForecastingSmogCloudsWithDeepLearning
and naturally (Rhodes & Seville, 2024), given that their aconstanterrorflow: intheabsenceofnewinputorerror
distinctionistheirsize. Withitsrelativelylargesize,PM signalstothecell,thelocalerrorbackflowremainsconstant,
itselfexperiencesnegligiblechemicalreactivitywiththeat- neithergrowingnordecaying(Gersetal.,2000). Thus,with
mospherecomparedtominorcompoundssuchasNO and theLSTM,thegradientisindependentoftime.
2
O . Nonetheless,notingitssusceptibilitytotransport,PM
3 Amorerecentlyproposedrecurrentunitisthegatedrecur-
andotherpollutantsalikeareresponsivetoairflowanddis-
rent unit (GRU) by Cho et al. (2014). The GRU uses a
persionintheirambientairenvironment—anenvironment
similarapproachtosolvingthevanishinggradientproblem
amidstallmeteorologicalinfluences,withoutyetconsider-
butsimpler. Itcontainsonlytwogates,theresetgateandup-
ingfactorssuchasgeologyandtopology. Therefore,many,
dategate,makingiteasiertocompute. Theformercontrols
atleastimplicit,parametersarerequiredtomodelPMand
thedegreetowhichtheprevioushiddenstateinfluencesthe
otheraircomponentsreliably.
current,andthelattercombinestheLSTMinputandforget
Inshort,NO ,O ,PM ,andPM aresubjecttoinfluences gateintoone. Itsperformancehasshowntobeonparwith
2 3 10 2.5
fromalldimensionsandthuscanbebroadlymodelled: for theLSTM,and,insomecases,canoutperformitintermsof
modellingpollutionmovements,pollutioncanbeassumed convergenceinCPUtimeandintermsofparameterupdates
to behave as air. Furthermore, pollutants are “internally” andgeneralisation(Chungetal.,2014).
affected by each other and externally by the atmosphere,
AsseeninSection1.1,thegatingmechanismalsoproves
warrantingamultivariate,integratedmodellingapproach.
itselfinairpollution-relatedapplications(Freemanetal.,
2018;Taoet al.,2019). Still, these studiespredictedone
2.2.RecurrentNeuralNetworks
contaminantonly,whileLSTMsareproventobeadequate
Recurrent neural networks (RNNs) host cyclical connec- formultivariatedata(Cheetal.,2018).
tionpathwaysthat,mathematicallyspeaking,representnot
Atahigherlevel,beyondtheconfigurationsofindividual
functionsbutdynamicalsystems(DSs). RNNshaveanet-
gatesorneurons,iswherediscussionofmultivariatedata
workstatex(n)allowingsomeearlierinputu(n′)toleave
canbegin. Awaytostrikeabalancebetweenamultivariate
itstracesonoutputy(n),and,therewith,thecurrentstate
forecastandanindividualoneisthroughnon-homogeneous
is influenced by past states and input. In practical terms,
hierarchical neural circuits and architectures, also called
RNNsaretailoredforsequential,fixedorderdata,suchas
hierarchicalneuralnets(HNNs). HNNsconsistofanumber
timeseries. Illustratively,whendataisfednon-sequentially,
ofloosely-coupledsubnets,arrangedinlayers,whereeach
e.g. today’sweatherpriortoyesterday’s,theinternalstate
subnetisintendedtocapturespecificaspectsoftheinput
becomesconfounded.
data (Mavrovouniotis & Chang, 1992). Such a balance
Formally,thetransitionofanRNNnetworkstateisgiven lendsitselfparticularlywelltoairpollutiondata,aswillbe
bytheupdateequations: discussedinSection3.3.
x(n)=σ(Wx(n−1)+Winu(n), (1)
y(n)=f(Woutx(n)), (2) 3.Methods
wheren=0,1,2,...,n arethetimesteps,W isamatrix
max The ensuing sections describe the data, preprocessing,
containing the connections weights, Win and Wout con-
modeltypes,trainingprocess,andevaluationmetrics.
tain the weights from/to the input/output neurons, σ is a
sigmoid function, and f a function wrapping the readout
3.1.Data
Woutx(n)(Jaeger,2023b). Inparticulartheactivationfunc-
tionσ,whichintroducesnon-linearitytotheevolutionof Theproposedforecastingexperimentuseshourly-sampled
theinternalstate(1),enablesRNNstocapture(long-term) datafrom2016to2023(RIVM,2024;KNMI,2024),which
non-lineardependenciesinthedata. RNNsaretrainedwith is available under an initiative of the Dutch government
atechniquecalledbackpropagationthroughtime(BPTT), andtheDutchnationalmeteorologicalservice, theRoyal
whichisadaptedtotheirsequentialnature. However,BPTT NetherlandsMeteorologicalInstitute(KNMI).Thedatais
suffersfromvanishinggradients(Hochreiter,1998),making accreditatedunderNEN-EN-ISO/IEC17025standardsand
itchallengingtolearnlong-termdependencieseffectively istechnicallyandsubstantivelyvalidated(andpossiblyre-
(Bengioetal.,1994). jected)beforerelease(KNMI,2023).
Long short-term memory (LSTM) networks were intro-
3.1.1.SPATIOTEMPORALCONTEXT
ducedbyHochreiter, Schmidhuber, andGerswiththein-
tentiontosolvethisproblem(Hochreiter,1991;Hochreiter This experiment involves forecasting with data from two
&Schmidhuber,1997;Gersetal.,2000). Theyproposed locations, asourcelocation(A)andatargetlocation(B).
aself-connectedlinearunit,theLSTMmemorycell,with ThesourcelocationisinUtrecht,theNetherlands,andhere,
3ForecastingSmogCloudsWithDeepLearning
Table1.Predictivevariablesandinitiallyconsideredmeteorologi-
calvariables(inalphabeticorder). Someunitsaremultipliedby
0.1forsimplificationwithoutlosingsignificance.
Variable Unit
Nitrogendioxide(NO ) µgm−3
2
Ozone(O ) µgm−3
3
PM≤10µm(PM ) µgm−3
10
PM≤2.5µm(PM ) µgm−3
2.5
Airpressure(AP) 0.1hPa
Dewpointtemperature(DPT) 0.1◦C
Globalradiation(GR) Jcm−2
Maximumwindgust(MWG) 0.1ms−1
Meanwinddirection(MWD) 0−360◦
Meanwindspeed(MWS) 0.1ms−1
Figure1.UtrechtareawithmarkersindicatingtheAWSlocations. Precipitationamount(PA) 0.1mm
Precipitationduration(PD) 0.1h
pollutant data is combined with meteorologically related Sunshineduration(SD) 0.1h
covariates from a sensor located in De Bilt, to forecast Temperature(T) 0.1◦C
pollutantdataattherelativelynorthwesterntargetlocation
inBreukelen. Theirrelativepositionsarebestillustratedin
Figure1. where,givenpaireddata(x ,y )n ,nisthesamplesize,
i i i=1
and x¯, y¯are their sample means. It must be noted, how-
3.1.2.INSPECTION
ever,thatthecalculationassumeslinearrelationships,het-
Table1detailsthepredictivevariablesusedinthisexper- eroskedasticity, and a Gaussian distribution. The correla-
iment, along with the initially considered meteorological tionsareplottedinFigure2.
parameters. Therationaleandrelevanceofthemeteorolog-
Upon filtering, the features precipitation amount and du-
ical parameters in the context of pollution prediction are
ration, global radiation, and maximum wind gust are dis-
discussedinAppendixA.1,coupledwithaninspection.
carded,leaving10availablefeatures. Fordetailsonfactors
affecting the data split and filtering choices, refer to the
3.2.Preprocessing sourcedocument(Oldenburg,2024).
Preprocessingstartswithtidyingtherawdata,followedby Next,normalisation. Normalisingpromotesgeneralisation,
atrain-validation-testsplit,featureengineering,normalisa- stabilisesgradientsandthelearningprocess,andcanpro-
tion,andendswithgenerating(input,output)-pairs. duce faster convergence (Ioffe & Szegedy, 2015). The
selected features are normalised to a range of [0,1] with
Firstly,therawdatawascleanedtomakeitutilisable,for
examplebysolvingerronous(split)rowsandcolumns,con-
vertingencodings,extractingmetadata,andexcludingdata 1.0
NO2
disqualifiedduetooutliers. Next,thereweremissingvalues
O3
(Table3). Thesewereassumedtobemissingcompletelyat PM10
0.8
random(MCAR)andimputedbylinearinterpolation. PM2.5
AP
Secondly, the tidy data is split into a training, valida-
DPT 0.6
tion, and testing set. This resulted in a balance of GR
76.3%, 11.9%, 11.9%. Subsequently,thenewlyacquired MWG
training set undergoes feature selection. As described in MWD 0.4
MWS
Hall (1999), good feature sets contain features that are
PA
highlycorrelatedwiththeclass,yetuncorrelatedwitheach PD 0.2
other. Thus,toassessthefeaturestheirintercorrelationsare SD
assessedandcomparedtoathresholdr thusingtheabsolute T
0.0
valuePearsoncorrelationcoefficientr :
xy
(cid:12) (cid:12)
r
=(cid:12)
(cid:12)
(cid:80)n i=1(x i−x¯)(y i−y¯) (cid:12)
(cid:12), (3)
Figure2.Coefficientmatrixfortheinitiallyconsideredfeatures.A
xy (cid:12) (cid:12)(cid:112)(cid:80)n i=1(x i−x¯)2(cid:112)(cid:80)n i=1(y i−y¯)2(cid:12) (cid:12) t 0h .r 1e 5s .h Wold her nth nofo tr mt eh te ,ta hb eso enlu trte yP ree mar as io nn swco he if tfi e.cientissetatr th =
4
2ON 3O 01MP 5.2MP PA TPD RG GWM DWM SWM AP DP DS TForecastingSmogCloudsWithDeepLearning
min-maxscaling x(n) allowing some earlier input u(n′) to leave its traces
onoutputy(n), MLPslearntoapproximatea(nonlinear)
x−x
x′ = min , (4) functionf : RL0 → RLk,whereL0andLk representthe
x −x
max min
neuronsintheinput-andoutputlayer,andlackanexplicit
where x and x are the minimum and maximum
min max mechanismforretainingsequencesoverextendedperiods.
valueforeachfeatureinthetrainingset.
Inpractice,theycannotutilisethesequence-spanningBPTT;
Lastlyinpreparingthedataforthemodels,pairgeneration. theypropagateerrorssolelythroughthenetwork. Hence,
Inordertoobtainstaticinput-outputpairsfromadiscretised theMLPandHMLParelesssuitedforthistaskthanRNNs
hourly-sampledtemporaltrainingsequence,onesegments andserveasbenchmarks.
theinputtimeseries(u(n)) forUtrecht,andinput
n∈[0,Nu] Intermsoftheirspecificarchitecture,theinputandoutput
timeseries (y(n)) n∈[0,Ny] for Breukelen with N u = N y, layerareofsizeL0 = 10(tenfeatures)andLk = 4(four
intoslidingwindowsoflengthl =72andh=24,respec-
in predictive variables). Unidirectional layers knit these to-
tively,obtaininginput-outputpairs(u ,y ) consist-
i i i=1,...,P gether. FortheMLP,thesearestandardfully-connectedlay-
ingofinput
ers. Itscounterpart,theHMLP,isofthetypeofhierarchical
models—atermintroducedinSection2.2whichdescribes
u =(u(n ),...,(u(n +l )), (5)
i i i in
non-homogeneous,modularneuralcircuits. HNNscan,de-
andoutput pendingonthetask,performmulti-tasklearning(MTL),a
methodwhoseprinciplegoalistoimprovegeneralisation
y =(y(n +δ),...,(y(n +δ+h)), (6)
i i i performance(Caruana,1997).
where n represents the starting index of the i-th pair, P
denotesti
henumberofpairsasdefinedbyP
=(cid:4)Nu+1(cid:5)
with
Furthermore, with HNNs, the hierarchical organisational
∆n structure is in hands of the model designer and offers an
samplingstepsize∆n,andδisdefinedasδ =l −24+1,
in openingforaprioriknowledgetobeembodiedintheneu-
meaningy ’soutputisconsideredfromthe48thhouron,
i ronalarrangementasinductivebiasorregularisingfactor,
plusa1-hourwindowforthespatialpredictionfromu to
i guidingthemodelinapreferreddirection. Inthecontext
y . Toexpandonthelatter,andasseenin(1)and(2),RNNs
i ofthisstudy,weaimtopredictthefourpollutants,eachof
processvaluesone-by-one,whichforthiscasemeansfor
whichcanberegardedasadistinctsubtask. Recognising
l iterations—δ,however,selectsonlythelast24readouts
in boththeintercorrelationsofthepollutants(asdepicted,for
for predicting and, thus, learning (facilitated by the loss
example, in Figure 2) and the fact that they all live a life
function(7)discussedinSection3.4). Stepsize∆nisset
oftheirown,itseemsreasonabletomirrorthisrealityina
at∆n=24forcomputationalefficiency,andbecausetrial-
model’sarchitecture.Toachievethis,weemployoneshared
and-errortestingshowednoorminorupsidetoasmaller
layer to establish shared representation and subsequently
∆n.
partitionthenetworkflowintoamodularbranchpersub-
Essentially,thismeansthatforeachpair,anhourofpollutant tasktoreducetheinterferencebetweentasks. Thisdesign,
concentrationsatBwillbepredicted24timesinsequence, includingthisnuancedregularisingfactor,confersHNNs
withtheprecedinghoursofAavailableasthegroundfor ahypotheticaladvantageoverfully-connectednets,which
prediction. Thus, the data sets the models up to learn to neglectanexplicitinternal-externalbalance.
modelthepollutantsusingtheircovariatesforthespatial
Next are the RNNs. The RNNs use the PyTorch imple-
predictiontaskfromUtrechttoBreukelen.
mentation of LSTM and GRU memory cells introduced
in Section 2.2. The fully-connected RNNs are similar to
3.3.Modelarchitecture
theMLP,exceptfortheirgatingmechanismsandrecurrent
Themultivariateone-dimensionalforecastingtaskofsmog synaptic connections, and vice versa for the hierarchical
clouds,i.e.,modellingthefourpollutantsfromUtrechtto RNNsinrelationtotheHMLP.
Breukelen,istakenonusingsixmodels: anordinarymulti-
Followinguponidentifyingmodeltypes,hyperparameters
layerperceptron(MLP),ahierarchicalMLP(HMLP),an
revealinmoredetailhowthesetypesareshapedintocom-
LSTMandGRU,and, asmaincontenders, ahierarchical
pletearchitectures. Thefollowingsectionsdiscusshowthey
LSTM(HLSTM)andGRU(HGRU).Thissectionwillout-
areestablished.
linethemodellingtypesandset-ups,followedbytheirhy-
perparameteroptimizationprocedure.
3.3.2.HYPERPARAMETEROPTIMIZATION
3.3.1.TYPES
Hyperparameterscanbeusedtocontrolthebehaviourof
AstoucheduponinSection2.2,theMLPmodelsapprox- alearningalgorithmandarenotadaptedbythealgorithm
imate not DSs but functions. Where RNNs have a state itself(Goodfellowetal.,2016). FortheDLmodelsathand,
5ForecastingSmogCloudsWithDeepLearning
examplesarethenumberofhiddenlayersandhiddenunits, pairs (u ,y ) ; and the mean squared error (MSE)
i i i=1,...,P
the learning rate, choice of optimizer, and the regularisa- lossfunctiondefinedas
tion term. An overview of the used hyperparameters per n
1 (cid:88)
modelcanbefoundinAppendixC—thissectionwillchiefly MSE= (y −yˆ)2+λ||θ ||2, (7)
n i i 2
explorethemethodologybehindtheirselection. i=1
Todeterminetheirvalues,ahyperparametersearchproce- wherendenotessampleamount,ythegroundtruth,andyˆ
dureconsistingofagridsearchandcross-validation(CV) theprediction—onehastosolvetheoptimizationproblem
schemes is used. This procedure aims to find a hyperpa-
1 (cid:88)
rameterconfigurationcwithaminimisedloss,whilealso θ opt =argmin P MSE(m θ(x i),y i), (8)
testing c’s generalisation capabilities. The loss is calcu- θ∈Θ i=1,...,P
latedondistinctvalidationsetsgeneratedbyCVfromall
whereθ denotesamodelwithaminimisedempiricalrisk
opt
availabletrainingdata,i.e. aconcatenationofthetraining
(Jaeger,2023a;b). MSEpunishesextremevaluesquadrat-
andvalidationset,totestthisgeneralisationperformance
icallymore, suitingthecontextofairpollutionwhereex-
andpreventoverfitting. Nestedwithinthehyperparameter
tremesareofgreaterconcern.
searchandwithintheCVscheme,isthemodels’training
algorithm,which,togetherwiththeloss,isspecifiedinthe Withaninitialmodelθ(0),initialisedbythePyTorch-default
nextsubsection,Section3.4. Kaiminginitialisation(Heetal.,2015),optimizationofθ(n)
is performed by the Adam (ADAptive Momentum) opti-
Continuing,thetraditionalgridsearchwasusedbecauseof
mizer (Kingma & Ba, 2014). Adam differentiates itself
its ease suiting this PoC; it essentially does a brute-force
from,e.g.,stochasticgradientdescent(SGD)byusingmo-
searchthroughtheparameterspaceH.Here,Hisdefinedas
mentum(Sutskeveretal.,2013)andadaptivelearningrates
theCartesianproductofthefinitesetsScontainingpossible
perparameterwhileonlyrequiringfirst-ordergradientsand
valuesforeachparameter. BecauseH growsexponentially,
little memory (Kingma & Ba, 2014). This study uses its
alargeS isnotfeasible,andsmallerS arealreadycompu-
implementationinPyTorch.
tationallyexpensive. Hence,asmeasures,someinitialtrial
runswereexecutedtogetafeelofwhichoptionstoinclude AdamdoesitsworkeverytimeabatchBof16(u,y)-pairs
andthemanymodelswerecomputedonanHPCcluster. ispassed.Batchesarerandomlysampled(whileinsequence
order) from the available pairs, introducing stochasticity
Then, CV is run for each c, where c is a unique configu-
(and efficiency over, e.g., one-by-one calculation). Inter-
ration within H. For the stateless MLPs, regular k-fold
nally, thisaddsthebatchdimensiontothepairs, creating
cross-validation, with k = 5, is used—with the perk of thetensors[B,l ,L0]foruand[B,h,Lk]fory. Whenu
in
maximaldatausage. RNNs,conversely,dohaveastateand isfed,themodelsspitoutforecastsy′ intheformofsuch
allowmemorytraceofpastsequences,asaforementionedin
tensor,whichissubsequentlycomparedtothegroundtruth
Section2.2.Avariationofk-foldCV,calledslidingwindow
yyieldingthelosswithwhichθcanbeupdated.
CV,accommodatesthis: itsamplestrainingandvalidation
sets—with,incontrasttopairgeneration,superpositionof Updatingθ,however,proceedsquitedifferentlyforthetwo
intervals—usingaslidingwindowapproach,thusnotallow- maintypesofmodels. Whilstforthefully-connectedmod-
ing validation of trained models with out-of-sample data els,thisproceedsasusualwithoneoptimizerupdatingθ,the
directlyfromthepast. modularmodelsrequireadifferentapproach. Astheyessen-
tiallyconsistofmultiplecorecomponents(onesharedlayer,
With these schemes, grid search with k-fold CV for the
fourbranches)withdifferentsearchspacesandconvergence
MLPsandsliding-windowCVfortheRNNs,valuesforthe
qualities,theprocesscapitalisesonthis:allfivecomponents
hyperparametersweredetermined—forgingthemodeltypes
havetheirownoptimizerandmatchedscheduler. Inaddi-
intoarchitectures.
tion,theyhavetwoseparate(initial)learningrates,asseen
inTable6andTable7. Distributingthelearningtaskshelps
3.4.Training eachmodelpartstablyreachanoptimum.
Inthissection,weexplainthetrainingprocedureuseddur- Lighting this in terms of implementation, the shared and
inghyperparameteroptimizationandthefinaltrainingitself branched parts do epochs in turns, seeing all the batches
bydefiningtheoptimizationgoalandmethod,followedby separatelywhiletheotherisfrozen.Frozen,asin,theparam-
someanti-overfittingmeasures. Thefinalmodelsaretrained eterscannotupdatebutcaninfer. Aconhereisefficiency:
usingthetrainingandvalidationsetcreatedinSection3.2. the batches are passed through the model once more for
everyepoch.
Toapproximateamodelm parametrisedbytuneablepa-
θ
rameterscollectedinavectorθ,givenasearchspaceθ ∈Θ When zooming out and looking at when learning should
oftargetmodelsΘwithinthesamearchitecture; training finalise,earlystoppingcomesin: itfinishestrainingwhen
6ForecastingSmogCloudsWithDeepLearning
for some number of epochs (defined in Table 8) the val- 4.Results
idation loss does not decrease by ≥ 1×10−5. Another
Following training, the models are evaluated on out-of-
anti-overfittingmeasureistheL2-normregularisationadded
sampledata. Itisfoundthatthemodelsprovideaneffective
to(7). Aseffect,largerweightsarepenalisedandsmaller
methodforthemodellingandforecastingofthepollutants.
weights are encouraged, preventing some set of weights
Quantitative results by RMSE and sMAPE are listed in
dominatingthemodel.
Table2.
Insummary,thetrainingprocessseekstofindanoptimalset
Considering the subtask-specific lowest sMAPE values,
ofmodelweightsθ ,andtoregularisethelearningprocess
opt
NO is predicted most accurately. Following NO is O ,
earlystops,thebatchesintroducestochasticity,theregulari- 2 2 3
then PM , and the models were least successful in pre-
sationtermbalancesweightvalues,andthehierarchicalnets 2.5
dicting PM . Nonetheless, the lowest sMAPEs, as well
incorporateMTL.Withtheseregularisationsteps,thetrain- 10
as the RMSEs—which are primarily generated by the
ing procedure should yield models fulfilling the ultimate
HGRU—confirmthemodels’suitabilityforforecastingthe
objectiveofgeneralisation,testedinnextsection.
pollutantsatBusingdataatA. Meanwhile,themodelsalso
differedinperformance.
3.5.Evaluation
MeasuredbyRMSE,thenon-hierarchicalfully-connected
Forevaluationofthemodels,theheld-outtestsetisused.
RNNs perform predominantly better than the MLPs, but
The predictions were first post-processed using inverse
alsoutilisemanyparameterstodoso. MeasuredbysMAPE,
minmax-scaling, sampled in batches without shuffling to
theydotoo,despiteHMLP’ssMAPE(M =46.274,SD =
eliminate any randomness, and then evaluated using the
45.344) being slightly inferior to the LSTM’s (M =
rootmeansquarederror(RMSE)andsymmetricmeanper-
46.321,SD = 46.515): apairedt-testwithα < .05sug-
centageerror(sMAPE)metric,whichbothserveadifferent
geststhereisnosufficientevidencetorejectthenullhypoth-
interpretation of model performance. In addition, the in-
esisofnodifference,t(8927)=1.011,p=3.12×10−1.
ferencespeedofeachmodelisevaluated,asthisisoneof
the unique advantages of data-driven methods over first- Furthermore,theGRUyieldsthelowesterrorsofthenon-
principlemethodslikeNWP. hierarchical RNN models. The HLSTM ranks second,
and,asperRMSEandsMAPE,theHGRUperformsbest,
TheRMSE,definedas
establishing the hierarchical models as the top perform-
(cid:118) (cid:117) (cid:117)1 (cid:88)n ers. A paired t-test confirms the HGRU’s (M RMSE =
RMSE=(cid:116)
n
(y i−yˆ i)2, (9) 5.468,SD RMSE = 4.906,M sMAPE = 44.519,SD sMAPE =
44.519) significant predictive ability on the testing
i=1
set over the HLSTM (M = 5.633,SD =
RMSE RMSE
providesameasureoftheaveragemagnitudeoftheerror
4.935,M = 44.981,SD = 45.850) both by
sMAPE sMAPE
with, due to its squaring operation, larger errors getting RMSE(t(8927)=−5.922,p=3.30×10−9)andsMAPE
penalised more. This fits the context of smog modelling, (t(8927) = −2.855,p = 4.32×10−3), aswell as on the
wherehighervaluesareespeciallyharmful.
other models. Moreover, for all individual pollutant sub-
Theothermetric,thesMAPE: tasksbyRMSE,andmostbysMAPE,theHGRUexhibits
thehighestpredictiveprecision,whereitisonlysurpassed
n
sMAPE=
2 (cid:88) |y i−yˆ i|
×100, (10)
repeatedlywiththesMAPEofthePM 2.5-subtask.
n (|y |+|yˆ|)
i=1 i i A line plot, shown in Figure 3, visually represents the
is an accuracy measure based on percentage (or relative) HGRU’s forecasts. Consistent with the numerical inter-
errors, providing a scale-independent, well-interpretable pretation of the RMSE and sMAPE, the patterns of NO 2
metric. The sMAPE complements the RMSE by taking andO 3 seemtobemostcloselycaptured. Incontrast,the
intoaccounttheindividualanddifferentdistributionsofthe PMsexhibitmoreshort-termfluctuationsthatarelessfre-
pollutants. Therefore,themetricallowsforafairrelative quentlycaptured,withPM 10provingthemostchallenging.
comparisonofthemodels’performance. Altogether,itcanbestatedthattheHGRUiswellequipped
tousedataatAforforecastingatB.
Finally, it is worth emphasising that, generally speaking,
theRMSEservesapracticalpurposebecauseittellsabout Intermsofefficiency,theinferencespeedt infofthemodels,
thedeviationinµgm−3 andhasaquadraticallyprogressive as also seen in Table 2, shows that efficiency is high: a
penalty. ThesMAPEmainlyfulfilsa“scientific”purpose 24-hour prediction is generated with negligible delay on
duetothepossibilityofcomparingmodels,thoughthetwo a relatively inefficient processor. Counterintuitively, the
metricsarenotmutuallyexclusive. Thisideaactsasaguide modelwiththemostparametersisthequickest,thoughthe
tointerprettheresultsmeaningfully. marginsaresmall. AsalsodiscussedinPathaketal.(2022)
7ForecastingSmogCloudsWithDeepLearning
Table2.Resultsofeachmodel,evaluatedandcomparedonperformance(RMSEandsMAPE)andefficiency(inferencespeedt and
inf
parametercount).Theerrormetricsarecalculatedperpollutantandcombined,withthelowesterrorinbold.t isthetimeinmilliseconds
inf
foroneinferenceofone24-hourleadtimeprediction(processedonanIntelCorei7-8565UCPU,8GBRAM,64-bitOS).
Models Performance Efficiency
RMSE(µgm−3) sMAPE(%) t (ms) Param#
inf
NO O PM PM Total NO O PM PM Total
2 3 10 2.5 2 3 10 2.5
MLP 6.63 7.53 7.82 4.85 6.71 35.89 41.90 65.24 53.15 49.04 27.2 17604
HMLP 5.99 6.83 7.95 4.62 6.35 31.84 39.44 65.42 48.00 46.27 135.2 15620
LSTM 5.97 6.39 7.48 4.32 6.04 32.09 38.09 63.40 51.70 46.32 14.4 572640
HLSTM 5.36 6.57 6.60 4.00 5.63 28.53 38.83 60.80 51.76 44.98 18.7 72244
GRU 6.01 6.18 6.84 3.94 5.74 32.62 38.46 61.15 49.67 45.47 47.9 363360
HGRU 5.35 6.01 6.59 3.92 5.47 28.78 36.97 59.92 52.40 44.52 77.4 74948
48 NO2 isansweredbythefactthatthemodelsareindeedhighly
24 adequate. Resultsdemonstratethat,aboveall,theHGRUis
0 suitableandcompetitiveatthistask. Reasonsincludethe
64 O3 sequence-processingprowessofRNNs,aGRU’ssimplicity,
32 andanintegrateddesignstreamlinedtotheverynatureof
0 thepollutants.
56 PM10
22 To sum up, our study contributes a PoC of smog cloud
modellingusingRNNswithahierarchicalarchitecture,pro-
−12
42 PM2.5 vidingabasisforadvancementsinpollutionandweather
20 forecastingtoimprovefuturepublichealth.
−2
2021-12-20 2021-12-27
Acknowledgements
Figure3.HGRUforecastsforNO ,O ,PM ,andPM takenfor
2 3 10 2.5
aweekfromtheevaluationset.Blackindicatesthegroundtruth Theauthorsthankthereviewersfortheirusefulcomments.
andmaroontheforecasts.Dashedlinesindicatezero. ThanksshouldalsogototheCenterforInformationTech-
nologyoftheUniversityofGroningenforprovidingaccess
andSection1.1(or,e.g.,intherecentBodnaretal.(2024)), totheHa´bro´kHPCcluster.
the speeds, apart from the initial training cost, highlight
the operational advantage of DL models over traditional BroaderImpactStatement
first-principlemethods: theyareordersofmagnitudefaster
and more efficient. Last to note on efficiency is that the Air pollution stands as a critical global challenge to hu-
best-performingmodels,theHLSTMandHGRU,require manity (UN, 2015). The rise of large-scale combustion
significantlyfewerparameters(duetoreducedparameter andanthropogenicpollutingactivitieshasledtosignificant
sharing)thanthenon-hierarchicalRNNsaswell. increasesinairpollutantconcentrationsoverthelastcen-
tury, leavingaheavyburdenonhumanhealth(Kampa&
Castanas,2008). Anunmistakablemanifestationofthese
5.Conclusions
developmentsistheoccurrenceofsmog: anoxiousmixture
Inthispaper, multivariatetimeseriesforecastingofsmog of air pollutants that obstructs visibility and severely im-
clouds,representedbyNO ,O ,PM ,andPM concen- pairshumanhealthinvariousways(Brunekreef&Holgate,
2 3 10 2.5
trations,usingRNNsisconducted.Specifically,meteorolog- 2002). Giventhedetrimentaleffects,itisimperativetobe
icalandpollutiondataatAisusedtoforecastairpollution abletopredictwhenharmfulpollutantlevelsmightoccur.
levelsatB. Themostsophisticatedmodels, theHLSTM Thisresearchproposesdifferentmethodstogaininsightinto
andHGRU,arebenchmarkedwithunidirectionalandfully- airpollutantlevelsthroughtimeseriesforecastingandthe
connectedDLarchitectures. applicationofmultipledeepneuralarchitectures,notably
includingRNNs.
Theresearchquestion,“Towhatextentaremodelswiththe
LSTM and GRU architecture capable of the multivariate Thisresearch’slimitationsaresummarisedbysimplifying
timeseries forecasting of smog-related air components?” measurestokeepitwithinascopeappropriateforaPoC
8ForecastingSmogCloudsWithDeepLearning
andbyconceptuallyinherentlimitations. Notableinherent Chung,J.,Gulcehre,C.,Cho,K.,andBengio,Y. Empirical
limitationsofthisstudyinclude: thedatabeinglimitedto evaluationofgatedrecurrentneuralnetworksonsequence
merelytwosensors,whichfailstohonourthecomplexityof modeling. arXivpreprintarXiv:1412.3555,2014.
themodelledsystem,e.g. themultidimensionality,emission
Clapp, L. Analysis of the relationship between ambient
sources,geographicalfeatures;thenon-stationarityofthe
levelsofo3,no2andnoasafunctionofnoxintheuk.
datanotbeingexplicitlytakenintoaccountneitherinpre-
AtmosphericEnvironment,35(36):6391–6405,December
processingnorinmodeldesign;andmodellingatalocation
2001. ISSN1352-2310. doi: 10.1016/s1352-2310(01)
wheretheairpollutionandsmogcloudsproblemisalmost
00378-8.
absent,thuslimitingthedirectimpact.
Finlayson-Pitts,B.andPittsJr,J. Atmosphericchemistryof
Despitetheselimitations,recurrentdeeparchitecturesoffer
troposphericozoneformation: scientificandregulatory
a promising addition to NWP, given their adequacy and
implications. Air&Waste,43(8):1091–1100,1993.
efficiency. Additionally, theminimaldatatransformation
requiredenablesreal-timeandcontinuouspredictions. Freeman, B. S., Taylor, G., Gharabaghi, B., and The´, J.
Forecasting air quality time series using deep learning.
References JournaloftheAirandWasteManagementAssociation,
68(8):866–886, May 2018. ISSN 2162-2906. doi: 10.
Alley,R.B.,Emanuel,K.A.,andZhang,F. Advancesin
1080/10962247.2018.1459956.
weatherprediction.Science,363(6425):342–344,January
2019. ISSN1095-9203. doi: 10.1126/science.aav7274. Gelbard,F.andSeinfeld,J.H. Thegeneraldynamicequa-
tion for aerosols. theory and application to aerosol for-
Atkinson,R.Atmosphericchemistryofvocsandnox.Atmo- mation and growth. Journal of Colloid and Interface
sphericEnvironment,34(12–14):2063–2101,2000. ISSN Science,68(2):363–382,1979.
1352-2310. doi: 10.1016/s1352-2310(99)00460-4.
Gers,F.A.,Schmidhuber,J.,andCummins,F. Learningto
forget: Continualpredictionwithlstm. Neuralcomputa-
Bauer, P., Thorpe, A., and Brunet, G. The quiet revo-
tion,12(10):2451–2471,2000.
lution of numerical weather prediction. Nature, 525
(7567):47–55,September2015. ISSN1476-4687. doi: Goodfellow,I.,Bengio,Y.,andCourville,A. Deeplearning.
10.1038/nature14956.
MITpress,2016.
Bengio,Y.,Simard,P.,andFrasconi,P. Learninglong-term Hall,M.A.Correlation-basedfeatureselectionformachine
dependencies with gradient descent is difficult. IEEE learning. PhDthesis,TheUniversityofWaikato,1999.
TransactionsonNeuralNetworks,5(2):157–166,March
He, K., Zhang, X., Ren, S., and Sun, J. Delving deep
1994. ISSN1941-0093. doi: 10.1109/72.279181.
intorectifiers: Surpassinghuman-levelperformanceon
imagenetclassification. InProceedingsoftheIEEEinter-
Bodnar,C.,Bruinsma,W.P.,Lucic,A.,Stanley,M.,Brand-
nationalconferenceoncomputervision,pp.1026–1034,
stetter,J.,Garvan,P.,Riechert,M.,Weyn,J.,Dong,H.,
2015.
Vaughan,A.,etal. Aurora: Afoundationmodelofthe
atmosphere. arXivpreprintarXiv:2405.13063,2024. Hochreiter,S. Untersuchungenzudynamischenneuronalen
netzen. Diploma, TechnischeUniversita¨tMu¨nchen, 91
Brunekreef,B.andHolgate,S.T. Airpollutionandhealth.
(1):31,1991.
TheLancet,360(9341):1233–1242,October2002. ISSN
0140-6736. doi: 10.1016/s0140-6736(02)11274-8. Hochreiter,S. Thevanishinggradientproblemduringlearn-
ingrecurrentneuralnetsandproblemsolutions. Interna-
Caruana, R. Multitask learning. Machine learning, 28: tionalJournalofUncertainty,FuzzinessandKnowledge-
41–75,1997. BasedSystems,6(02):107–116,1998.
Hochreiter,S.andSchmidhuber,J.Longshort-termmemory.
Che,Z.,Purushotham,S.,Cho,K.,Sontag,D.,andLiu,Y.
Neuralcomputation,9(8):1735–1780,1997.
Recurrent neural networks for multivariate time series
withmissingvalues. ScientificReports,8(1),April2018.
Holton,J.R.andHakim,G.J. Anintroductiontodynamic
ISSN2045-2322. doi: 10.1038/s41598-018-24271-9.
meteorology. Academicpress,2012.
Cho,K.,VanMerrie¨nboer,B.,Bahdanau,D.,andBengio,Y. Ioffe,S.andSzegedy,C. Batchnormalization:Accelerating
Onthepropertiesofneuralmachinetranslation: Encoder- deepnetworktrainingbyreducinginternalcovariateshift.
decoder approaches. arXiv preprint arXiv:1409.1259, InInternationalconferenceonmachinelearning,pp.448–
2014. 456.pmlr,2015.
9ForecastingSmogCloudsWithDeepLearning
Irwin,J.andWilliams,M. Acidrain: Chemistryandtrans- Orellano,P.,Reynoso,J.,Quaranta,N.,Bardach,A.,and
port. Environmental Pollution, 50(1–2):29–59, 1988. Ciapponi,A. Short-termexposuretoparticulatematter
ISSN0269-7491. doi: 10.1016/0269-7491(88)90184-4. (pm10andpm2.5),nitrogendioxide(no2),andozone(o3)
and all-cause and cause-specific mortality: Systematic
Jaeger, H. Lecture Notes: Machine Learning. www.
review and meta-analysis. Environment International,
ai.rug.nl/minds/uploads/LN_ML_RUG.pdf,
142:105876, September 2020. ISSN 0160-4120. doi:
2023a. Accessedon18-04-2023.
10.1016/j.envint.2020.105876.
Jaeger, H. Lecture Notes: Neural Networks. www.ai.
Pathak, J., Subramanian, S., Harrington, P., Raja, S.,
rug.nl/minds/uploads/LN_NN_RUG.pdf,
Chattopadhyay, A., Mardani, M., Kurth, T., Hall, D.,
2023b. Accessedon02-07-2023.
Li, Z., Azizzadenesheli, K., et al. Fourcastnet: A
global data-driven high-resolution weather model us-
Kampa,M.andCastanas,E.Humanhealtheffectsofairpol-
ing adaptive fourier neural operators. arXiv preprint
lution. Environmentalpollution,151(2):362–367,2008.
arXiv:2202.11214,2022.
Kingma,D.P.andBa,J. Adam: Amethodforstochastic
Rhodes,M.J.andSeville,J. Introductiontoparticletech-
optimization. arXivpreprintarXiv:1412.6980,2014.
nology. JohnWiley&Sons,2024.
KNMI. Luchtkwaliteitsdata als csv bestanden, ver-
RIVM. RIVMluchtmeetnetdatasets, 2024. URLdata.
sie 7, 2023. URL data.rivm.nl/data/
rivm.nl/data/. Accessedon15/03/2024.
luchtmeetnet/readme.pdf. Accessed on
14/10/2023. Saxena,P.andNaik,V. Airpollution: sources,impactsand
controls. Cabi,2018.
KNMI.KNMIdataplatform,2024.URLdataplatform.
knmi.nl/. Accessedon15/03/2024. Seinfeld, J. H. and Pandis, S. N. Atmospheric chemistry
andphysics: fromairpollutiontoclimatechange. John
Li,J.,Cheng,K.,Wang,S.,Morstatter,F.,Trevino,R.P.,
Wiley&Sons,2016.
Tang,J.,andLiu,H.Featureselection:Adataperspective.
ACMcomputingsurveys(CSUR),50(6):1–45,2017. Sutskever,I.,Martens,J.,Dahl,G.,andHinton,G. Onthe
importanceofinitializationandmomentumindeeplearn-
Lim,S.S.,Vos,T.,Flaxman,A.D.,Danaei,G.,Shibuya,
ing. InInternationalconferenceonmachinelearning,pp.
K.,Adair-Rohani,H.,AlMazroa,M.A.,Amann,M.,An-
1139–1147.PMLR,2013.
derson,H.R.,Andrews,K.G.,etal. Acomparativerisk
assessmentofburdenofdiseaseandinjuryattributable Tao,Q.,Liu,F.,Li,Y.,andSidorov,D. Airpollutionfore-
to67riskfactorsandriskfactorclustersin21regions, castingusingadeeplearningmodelbasedon1dconvnets
1990–2010: asystematicanalysisfortheglobalburden and bidirectional gru. IEEE Access, 7:76690–76698,
ofdiseasestudy2010. Thelancet,380(9859):2224–2260, 2019. ISSN 2169-3536. doi: 10.1109/access.2019.
2012. 2921578.
Masood,A.andAhmad,K. Areviewonemergingartificial UN. Transformingourworld: the2030agendaforsustain-
intelligence(ai)techniquesforairpollutionforecasting: able development, 2015. URL https://sdgs.un.
Fundamentals,applicationandperformance. Journalof org/2030agenda.
CleanerProduction,322:129072,November2021. ISSN
0959-6526. doi: 10.1016/j.jclepro.2021.129072. Vallero,D.A. Fundamentalsofairpollution. Academic
press,2014.
Masters, D. and Luschi, C. Revisiting small batch
Velders,G.,Aben,J.,Geilenkirchen,G.,DenHollander,H.,
training for deep neural networks. arXiv preprint
van der Swaluw, E., de Vries, W., and van Zanten, M.
arXiv:1804.07612,2018.
Grootschalige concentratie-en depositiekaarten Neder-
Mavrovouniotis,M.L.andChang,S. Hierarchicalneural land: Rapportage2015. RijksinstituutvoorVolksgezond-
networks. Computers & chemical engineering, 16(4): heidenMilieuRIVM,2015.
347–369,1992.
Wong, C.-M., Vichit-Vadakan, N., Kan, H., and Qian, Z.
Oldenburg, V. W. Forecasting smog clouds with deep Publichealthandairpollutioninasia(papa): Amulticity
learning: A proof-of-concept. Bachelor’s thesis, Rijk- studyofshort-termeffectsofairpollutiononmortality.
suniversiteit Groningen, 2024. URL https://fse. EnvironmentalHealthPerspectives,116(9):1195–1202,
studenttheses.ub.rug.nl/32424/. IfURLis September 2008. ISSN 1552-9924. doi: 10.1289/ehp.
non-accessible,contactcorrespondingauthorsforaccess. 11257.
10ForecastingSmogCloudsWithDeepLearning
Zorn, J., Simo˜es, M., Velders, G.J., Gerlofs-Nijland, M.,
Strak,M.,Jacobs,J.,Dijkema,M.B.,Hagenaars,T.J.,
Smit,L.A.,Vermeulen,R.,Mughini-Gras,L.,Hogerw-
erf,L.,andKlinkenberg,D. Effectsoflong-termexpo-
sure to outdoor air pollution on covid-19 incidence: A
population-basedcohortstudyaccountingforsars-cov-2
exposure levels in the netherlands. Environmental Re-
search,252:118812,July2024. ISSN0013-9351. doi:
10.1016/j.envres.2024.118812.
11ForecastingSmogCloudsWithDeepLearning
A.Datainsights
ThisAppendixsectionprovidessomeadditionalinsightintothedatabyexploringthemeteorogicalvariables,displayingan
overviewofthedataavailabilities,andpresentingsomedatastatistics.
A.1.Explorationofmeteorologicaldata
Thissubsectionprovidesanoverviewofalltheinitiallyconsideredmeteorologicalvariablesaccompaniedbyanexplanation
ofwhythesevariablesmightbehelpfulformodellingsmogclouds,i.e. thepollutantsNO ,O ,PM ,PM . Importantto
2 3 10 2.5
emphasiseisthatnotalloftheserationaleshaveheldupinfeatureselection(or,wordeddifferently,showedinthedata).
Itisimportanttostressthatbesidestheindividualcharacteristicsofthepollutants,theyarelocatedinthetroposphericsky,
andhavesuchalowmasstheycanbeassumedtobehaveasairintermsoftheirinteractionwithlarge-scalemeteorological
processes. Wewillgothroughthevariablesfromtoptobottomtoexplaintheirrelevance:
• Airpressurecanindicatedispersionandtransportoflargeaircurrents,forexamplebylowandhigh-pressureareas,
andthusalsoinfluencetheaircurrentsofpollutants(Holton&Hakim,2012).
• Dew point temperature, precipitation sum, and precipitation amount are indicators of atmospheric moisture
contentlevels. Theselevelscansaysomethingabout,forexample,therateofcondensation(which,vianucleation,
canleadtotheformationoffineaerosols(PM)(Gelbard&Seinfeld,1979)),anyscavengingandcleansingoftheair
byrainfall(loweringthepollutantconcentrations)(Vallero,2014),andtheformationofacidrain(whereacidicgases
SO andNO thatare(relatedto)thepredictivevariables,getwashedout,thusloweringtheconcentrations(Irwin&
2 x
Williams,1988)).
• Globalradiationandsunshine,whichsignifythepresenceofsolarenergyintheformofphotons,serveasfundamental
driversoflow-entropyenergyinputonEarth. Section2.1discussedanexampleofaphotoinducedprocess.
• Temperatureisanessentialfactorinchemicalprocessesseenbyitsroleasacceleratorintheformationofsecondary
pollutants.Inaddition,temperatureplaysaroleinatmosphericstability,with,forexample,(suddenly)hightemperatures
signifying increased convective activity. Furthermore, temperature influences state changes, and is also tightly
connectedwithglobalradiationandsunshine,therewithalsoindirectlycontributingtotheireffects. Formorecontext
onatmosphericchemistryandphysics,refertotheextensiveSeinfeld&Pandis(2016).
• Meanwinddirection,meanwindspeed,andmaximumwindgustalltellaboutthewind’sproperties,whichinturn
carriesthepollutantsthroughtheatmosphere. Incontextoftheexperiment,winddirection,forexample,tellsaboutthe
relativedirectionalrelationshipbetweenAandB. Outofthepollutants,thewindespeciallyplaysaroleforthePMs,
astheyhaveabiggersurface.
A.2.Dataavailability
Hereisashortsummaryoftheavailabilityofthedatausedintheexperiment. Missingdatawasinterpolatedwithlinear
interpolation.
Table3.Dataavailabilitypercentageforthemodelledpollutantsforeachyear.ThemeteorologicalabbrevationsaredefinedinTable1.
Themeteorologicaldatawascompletelyavailableforallyears—forthepollutants,itwasnot.GiventhestrictproceduresbytheKNMI
(KNMI,2023),thisisnosurprise.
NO O PM PM AP DP MWD MWS SD T
2 3 10 2.5
2017 97.64% 96.85% 97.62% 99.10% 100% 100% 100% 100% 100% 100%
2018 99.67% 98.52% 97.70% 99.29% 100% 100% 100% 100% 100% 100%
2020 98.60% 98.05% 99.34% 99.31% 100% 100% 100% 100% 100% 100%
2021 99.67% 98.55% 98.88% 99.29% 100% 100% 100% 100% 100% 100%
2022 96.90% 97.62% 95.56% 99.62% 100% 100% 100% 100% 100% 100%
2023 98.60% 97.15% 98.46% 97.97% 100% 100% 100% 100% 100% 100%
12ForecastingSmogCloudsWithDeepLearning
A.3.Dataallocationandquantity
Thissectionprovidestransparencyonhowmuchdatawasusedandinwhatproportions. Table4showsthenumberofhours
ofdatabeforepairgeneration,andTable5thedataafterpairgeneration.
Table4.Hoursofdataforeachfeatureperyearinthetraining,validation,andtestingsetsbeforepairgeneration,illustratingthedata
balancebetweenthedifferentsets.Dividetheseby24fortheamountofdays.AbbrevationsaredefinedinTable1.Forfurtherdetailson
theestablishmentofthesebalances,refertothesourcedocument(Oldenburg,2024).
NO O PM PM AP DP MWD MWS SD T
2 3 10 2.5
Train’17 3648 3648 3648 3648 3648 3648 3648 3648 3648 3648
Train’18 3648 3648 3648 3648 3648 3648 3648 3648 3648 3648
Train’20 3648 3648 3648 3648 3648 3648 3648 3648 3648 3648
Train’21 2640 2640 2640 2640 2640 2640 2640 2640 2640 2640
Train’22 2640 2640 2640 2640 2640 2640 2640 2640 2640 2640
Validation’21 504 504 504 504 504 504 504 504 504 504
Validation’22 504 504 504 504 504 504 504 504 504 504
Validation’23 1512 1512 1512 1512 1512 1512 1512 1512 1512 1512
Test’21 504 504 504 504 504 504 504 504 504 504
Test’22 504 504 504 504 504 504 504 504 504 504
Test’23 1512 1512 1512 1512 1512 1512 1512 1512 1512 1512
Table5.Tablewithnumericaldescriptionsoftheuseddatasets,afterpairgenerationperformedinSection3.2(with∆n=24hours).Due
totheoverlappingnatureofthepairgenerationalgorithm,“more”usabledatawasgeneratedcomparedtotheoriginaldata.Theamount
ofpairsP isdisplayed,thetotalamountofhours,totaldatapoints,datapointspassedthroughthemodelasinputu,andthegroundtruthy
datapointsusedforthelossfunctionduringtraining,givinganindicationoftheamountofcomputationsneededforonetrainingepoch.
(Withthe“optimal”∆n=1,thetrainingsetwouldgrowtotheimpracticalamountof12847104datapoints.)
P hrs n n n
total total u y
Trainingset 656 47232 535296 472320 62976
Validationset 93 6696 75888 66960 8928
Testingset 93 6696 75888 66960 8928
13ForecastingSmogCloudsWithDeepLearning
B.Traininginsights
ThesubplotsinFigure4showthetrainingandvalidationlossdevelopmentduringfinaltrainingofthesixmodels. Figure5
showshowboththesharedandbranchedpartoftheHLSTMcontributedtoitstrainingloss.
Trainingthemodelstookanhourmaximum,usingthehyperparameterslistedinTable7and8andprocessedlocallyonan
IntelCorei7-8565UCPU,8GBRAM,64-bitOS.
MLP LSTM GRU
0.016
0.045 Ltrain Ltrain 0.011 Ltrain
0.040 Lvalidation 0.014 Lvalidation 0.010 Lvalidation
0.035 0.012 0.009
0.008
0.030 0.010
0.007
0.025
0.008
0.006
0.020
0.006 0.005
0.015
0.004
0.004
0.010 0.003
0 10 20 30 0 10 20 30 40 0 10 20
Epoch Epoch Epoch
−→ −→ −→
HMLP HLSTM HGRU
0.175 Ltrain 0.014 Ltrain Ltrain
0.012
0.150
Lvalidation Lvalidation Lvalidation
0.012
0.125 0.010
0.010
0.100
0.008
0.008
0.075
0.006
0.050 0.006
0.025 0.004 0.004
0.000
0 20 40 60 0 50 100 150 200 0 20 40 60
Epoch Epoch Epoch
−→ −→ −→
Figure4. Lossplotsforallmodels,showingthetrainingversusvalidationlossesoverepochs.
2.0 Lshared
1.5 Lbranches
1.0
0.5
0 25 50 75 100 125 150 175 200
Epoch
−→
Figure5.TraininglossplottedofthesharedandbranchedpartoftheHLSTM.Forillustrativepurposes,thefirstepochisleftoutfromthe
plot.Bothmodelpartshavedifferentcomplexities(seeSection3.3),causingtheirlearningprocesstobedifferentaswell.Thebranches
weremorecomplex,causingitslearningprocesstobelessstable,visiblebythesmall“bumps”initsdescend.
14
ssoL
ssoL
ssoL
→−
→−
→−
ssoL
ssoL
→−
→−
ssoL
ssoL
→−
→−ForecastingSmogCloudsWithDeepLearning
C.Architectures
ThisAppendixsectionprovidessomeadditionaldetailontheemployedarchitecturesbyspecifyingtheusedhyperparameters
andothermodelparameters. TheusedhyperparameteroptimizationprocedurecanbefoundinSection3.3.2.
Table6showsthehyperparametersincludedinthegridsearch,includingtheirsymbols,andTable7showsthedetermined
valuesasaresultofthesearch.
Some things are worth pointing out here. Note that the fully-connected models have just one learning rate, while the
hierarchicalmodelshavetwo: onefortheirsharedlayerandonefortheoptimizersofeachoftheirbranches. Theratio
betweenthesetwo,µ andµ ,isequivalenttok. Thiswasdone,afterlotsoftestruns,withtheideaofa“power
shared branch
ratio”betweenthetwo: thebranchesneededahigherµtoletthemconvergeinharmonywiththesharedlayer. Another
reasonwasthatbyinterlinkingthetwo,H wassignificantlyreduced.
AlastthingtonoteisλoftheHLSTMbeingzero. Duringtraining,theHLSTMstruggledtogetmomentumandtostart
learning,resultinginthehyperparametersearchchoosingamodelwithoptimalflexibility—aregularisationtermofzero.
Illustratively,Figure4showsrelativelyslowHLSTMconvergence.
Table8showsothertrainingsettings(or“hyperparameters”)thatweredeterminedthroughtrial-and-error(andnotthrough
exhaustivesearch). Allmodelsreducedtheirlearningratesbyafactorof0.1whenthevalidationlossreachedaplateau,had
abatchsize(B)of16,andusedk=5intheirk-foldcross-validationschemes. B =16wasplainlyadoptedfromMasters
&Luschi(2018),whofoundsmallerbatchsizes(2≤B ≤32)toprovidebenefitsintermsofconvergencestabilityand
overalltestperformanceforagivennumberofepochs. TheMLPshadapatienceof6andtheRNNsof15toaccomodate
fortheirdifferencesinconvergencespeed. AllactivationfunctionsemployedareReLU,includingthereadout.
Table6. Listofthehyperparametersincludedinthegridsearch.
Hyperparameter Symbol
Hiddenlayers k
Hiddenunits Lκ
Learningrate µ
Learningrate,shared µ
shared
Learningrate,branches µ
branch
Weightdecay λ
Table7. Overviewoftheusedhyperparametersdeterminedthroughgridsearch.
k Lκ µ µ µ λ
shared branch
MLP 4 64 1×10−5 1×10−5
HMLP 7 64 1×10−4 7×10−4 1×10−5
LSTM 6 112 1×10−3 1×10−6
HLSTM 7 48 1×10−4 7×10−4 0
GRU 4 128 1×10−3 1×10−5
HGRU 4 64 1×10−3 4×10−3 1×10−7
Table8. Overviewofotherusedtrainingsettingsdeterminedthroughtrial-and-error.
optimizer µ patience B k
scheduler folds
MLP Adam ReduceLROnPlateau 6 16 5
HMLP Adam ReduceLROnPlateau 6 16 5
LSTM Adam ReduceLROnPlateau 15 16 5
HLSTM Adam ReduceLROnPlateau 15 16 5
GRU Adam ReduceLROnPlateau 15 16 5
HGRU Adam ReduceLROnPlateau 15 16 5
15