[
    {
        "title": "Large Language Models as Markov Chains",
        "authors": "Oussama ZekriAmbroise OdonnatAbdelhakim BenechehabLinus BleisteinNicolas BoulléIevgen Redko",
        "links": "http://arxiv.org/abs/2410.02724v1",
        "entry_id": "http://arxiv.org/abs/2410.02724v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02724v1",
        "summary": "Large language models (LLMs) have proven to be remarkably efficient, both\nacross a wide range of natural language processing tasks and well beyond them.\nHowever, a comprehensive theoretical analysis of the origins of their\nimpressive performance remains elusive. In this paper, we approach this\nchallenging task by drawing an equivalence between generic autoregressive\nlanguage models with vocabulary of size $T$ and context window of size $K$ and\nMarkov chains defined on a finite state space of size $\\mathcal{O}(T^K)$. We\nderive several surprising findings related to the existence of a stationary\ndistribution of Markov chains that capture the inference power of LLMs, their\nspeed of convergence to it, and the influence of the temperature on the latter.\nWe then prove pre-training and in-context generalization bounds and show how\nthe drawn equivalence allows us to enrich their interpretation. Finally, we\nillustrate our theoretical guarantees with experiments on several recent LLMs\nto highlight how they capture the behavior observed in practice.",
        "updated": "2024-10-03 17:45:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02724v1"
    },
    {
        "title": "Highly Adaptive Ridge",
        "authors": "Alejandro SchulerAlexander HagemeisterMark van der Laan",
        "links": "http://arxiv.org/abs/2410.02680v1",
        "entry_id": "http://arxiv.org/abs/2410.02680v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02680v1",
        "summary": "In this paper we propose the Highly Adaptive Ridge (HAR): a regression method\nthat achieves a $n^{-1/3}$ dimension-free L2 convergence rate in the class of\nright-continuous functions with square-integrable sectional derivatives. This\nis a large nonparametric function class that is particularly appropriate for\ntabular data. HAR is exactly kernel ridge regression with a specific\ndata-adaptive kernel based on a saturated zero-order tensor-product spline\nbasis expansion. We use simulation and real data to confirm our theory. We\ndemonstrate empirical performance better than state-of-the-art algorithms for\nsmall datasets in particular.",
        "updated": "2024-10-03 17:06:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02680v1"
    },
    {
        "title": "GUD: Generation with Unified Diffusion",
        "authors": "Mathis GerdesMax WellingMiranda C. N. Cheng",
        "links": "http://arxiv.org/abs/2410.02667v1",
        "entry_id": "http://arxiv.org/abs/2410.02667v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02667v1",
        "summary": "Diffusion generative models transform noise into data by inverting a process\nthat progressively adds noise to data samples. Inspired by concepts from the\nrenormalization group in physics, which analyzes systems across different\nscales, we revisit diffusion models by exploring three key design aspects: 1)\nthe choice of representation in which the diffusion process operates (e.g.\npixel-, PCA-, Fourier-, or wavelet-basis), 2) the prior distribution that data\nis transformed into during diffusion (e.g. Gaussian with covariance $\\Sigma$),\nand 3) the scheduling of noise levels applied separately to different parts of\nthe data, captured by a component-wise noise schedule. Incorporating the\nflexibility in these choices, we develop a unified framework for diffusion\ngenerative models with greatly enhanced design freedom. In particular, we\nintroduce soft-conditioning models that smoothly interpolate between standard\ndiffusion models and autoregressive models (in any basis), conceptually\nbridging these two approaches. Our framework opens up a wide design space which\nmay lead to more efficient training and data generation, and paves the way to\nnovel architectures integrating different generative approaches and generation\ntasks.",
        "updated": "2024-10-03 16:51:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02667v1"
    },
    {
        "title": "Online Learning Guided Quasi-Newton Methods with Global Non-Asymptotic Convergence",
        "authors": "Ruichen JiangAryan Mokhtari",
        "links": "http://arxiv.org/abs/2410.02626v1",
        "entry_id": "http://arxiv.org/abs/2410.02626v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02626v1",
        "summary": "In this paper, we propose a quasi-Newton method for solving smooth and\nmonotone nonlinear equations, including unconstrained minimization and minimax\noptimization as special cases. For the strongly monotone setting, we establish\ntwo global convergence bounds: (i) a linear convergence rate that matches the\nrate of the celebrated extragradient method, and (ii) an explicit global\nsuperlinear convergence rate that provably surpasses the linear convergence\nrate after at most ${O}(d)$ iterations, where $d$ is the problem's dimension.\nIn addition, for the case where the operator is only monotone, we prove a\nglobal convergence rate of ${O}(\\min\\{{1}/{k},{\\sqrt{d}}/{k^{1.25}}\\})$ in\nterms of the duality gap. This matches the rate of the extragradient method\nwhen $k = {O}(d^2)$ and is faster when $k = \\Omega(d^2)$. These results are the\nfirst global convergence results to demonstrate a provable advantage of a\nquasi-Newton method over the extragradient method, without querying the\nJacobian of the operator. Unlike classical quasi-Newton methods, we achieve\nthis by using the hybrid proximal extragradient framework and a novel online\nlearning approach for updating the Jacobian approximation matrices.\nSpecifically, guided by the convergence analysis, we formulate the Jacobian\napproximation update as an online convex optimization problem over\nnon-symmetric matrices, relating the regret of the online problem to the\nconvergence rate of our method. To facilitate efficient implementation, we\nfurther develop a tailored online learning algorithm based on an approximate\nseparation oracle, which preserves structures such as symmetry and sparsity in\nthe Jacobian matrices.",
        "updated": "2024-10-03 16:08:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02626v1"
    },
    {
        "title": "Ranking Perspective for Tree-based Methods with Applications to Symbolic Feature Selection",
        "authors": "Hengrui LuoMeng Li",
        "links": "http://arxiv.org/abs/2410.02623v1",
        "entry_id": "http://arxiv.org/abs/2410.02623v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02623v1",
        "summary": "Tree-based methods are powerful nonparametric techniques in statistics and\nmachine learning. However, their effectiveness, particularly in finite-sample\nsettings, is not fully understood. Recent applications have revealed their\nsurprising ability to distinguish transformations (which we call symbolic\nfeature selection) that remain obscure under current theoretical understanding.\nThis work provides a finite-sample analysis of tree-based methods from a\nranking perspective. We link oracle partitions in tree methods to response\nrankings at local splits, offering new insights into their finite-sample\nbehavior in regression and feature selection tasks. Building on this local\nranking perspective, we extend our analysis in two ways: (i) We examine the\nglobal ranking performance of individual trees and ensembles, including\nClassification and Regression Trees (CART) and Bayesian Additive Regression\nTrees (BART), providing finite-sample oracle bounds, ranking consistency, and\nposterior contraction results. (ii) Inspired by the ranking perspective, we\npropose concordant divergence statistics $\\mathcal{T}_0$ to evaluate symbolic\nfeature mappings and establish their properties. Numerical experiments\ndemonstrate the competitive performance of these statistics in symbolic feature\nselection tasks compared to existing methods.",
        "updated": "2024-10-03 16:03:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02623v1"
    }
]