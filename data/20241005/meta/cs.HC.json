[
    {
        "title": "Aggregation of Constrained Crowd Opinions for Urban Planning",
        "authors": "Akanksha DasJyoti PatelMalay Bhattacharyya",
        "links": "http://arxiv.org/abs/2410.02454v1",
        "entry_id": "http://arxiv.org/abs/2410.02454v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02454v1",
        "summary": "Collective decision making is often a customary action taken in government\ncrowdsourcing. Through ensemble of opinions (popularly known as judgment\nanalysis), governments can satisfy majority of the people who provided\nopinions. This has various real-world applications like urban planning or\nparticipatory budgeting that require setting up {\\em facilities} based on the\nopinions of citizens. Recently, there is an emerging interest in performing\njudgment analysis on opinions that are constrained. We consider a new dimension\nof this problem that accommodate background constraints in the problem of\njudgment analysis, which ensures the collection of more responsible opinions.\nThe background constraints refer to the restrictions (with respect to the\nexisting infrastructure) to be taken care of while performing the consensus of\nopinions. In this paper, we address the said kind of problems with efficient\nunsupervised approaches of learning suitably modified to cater to the\nconstraints of urban planning. We demonstrate the effectiveness of this\napproach in various scenarios where the opinions are taken for setting up ATM\ncounters and sewage lines. Our main contributions encompass a novel approach of\ncollecting data for smart city planning (in the presence of constraints),\ndevelopment of methods for opinion aggregation in various formats. As a whole,\nwe present a new dimension of judgment analysis by adding background\nconstraints to the problem.",
        "updated": "2024-10-03 13:02:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02454v1"
    },
    {
        "title": "ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR",
        "authors": "Mengxu PanAlexandra KitsonHongyu WanMirjana Prpa",
        "links": "http://arxiv.org/abs/2410.02406v1",
        "entry_id": "http://arxiv.org/abs/2410.02406v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02406v1",
        "summary": "Many people struggle with learning a new language, with traditional tools\nfalling short in providing contextualized learning tailored to each learner's\nneeds. The recent development of large language models (LLMs) and embodied\nconversational agents (ECAs) in social virtual reality (VR) provide new\nopportunities to practice language learning in a contextualized and\nnaturalistic way that takes into account the learner's language level and\nneeds. To explore this opportunity, we developed ELLMA-T, an ECA that leverages\nan LLM (GPT-4) and situated learning framework for supporting learning English\nlanguage in social VR (VRChat). Drawing on qualitative interviews (N=12), we\nreveal the potential of ELLMA-T to generate realistic, believable and\ncontext-specific role plays for agent-learner interaction in VR, and LLM's\ncapability to provide initial language assessment and continuous feedback to\nlearners. We provide five design implications for the future development of\nLLM-based language agents in social VR.",
        "updated": "2024-10-03 11:32:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02406v1"
    },
    {
        "title": "Source Data Selection for Brain-Computer Interfaces based on Simple Features",
        "authors": "Frida HeskebeckCarolina BergelingBo Bernhardsson",
        "links": "http://arxiv.org/abs/2410.02360v1",
        "entry_id": "http://arxiv.org/abs/2410.02360v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02360v1",
        "summary": "This paper demonstrates that simple features available during the calibration\nof a brain-computer interface can be utilized for source data selection to\nimprove the performance of the brain-computer interface for a new target user\nthrough transfer learning. To support this, a public motor imagery dataset is\nused for analysis, and a method called the Transfer Performance Predictor\nmethod is presented. The simple features are based on the covariance matrices\nof the data and the Riemannian distance between them. The Transfer Performance\nPredictor method outperforms other source data selection methods as it selects\nsource data that gives a better transfer learning performance for the target\nusers.",
        "updated": "2024-10-03 10:17:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02360v1"
    },
    {
        "title": "Can Capacitive Touch Images Enhance Mobile Keyboard Decoding?",
        "authors": "Piyawat LertvittayakumjornShanqing CaiBilly DouCedric HoShumin Zhai",
        "links": "http://dx.doi.org/10.1145/3654777.3676420",
        "entry_id": "http://arxiv.org/abs/2410.02264v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02264v1",
        "summary": "Capacitive touch sensors capture the two-dimensional spatial profile\n(referred to as a touch heatmap) of a finger's contact with a mobile\ntouchscreen. However, the research and design of touchscreen mobile keyboards\n-- one of the most speed and accuracy demanding touch interfaces -- has focused\non the location of the touch centroid derived from the touch image heatmap as\nthe input, discarding the rest of the raw spatial signals. In this paper, we\ninvestigate whether touch heatmaps can be leveraged to further improve the tap\ndecoding accuracy for mobile touchscreen keyboards. Specifically, we developed\nand evaluated machine-learning models that interpret user taps by using the\ncentroids and/or the heatmaps as their input and studied the contribution of\nthe heatmaps to model performance. The results show that adding the heatmap\ninto the input feature set led to 21.4% relative reduction of character error\nrates on average, compared to using the centroid alone. Furthermore, we\nconducted a live user study with the centroid-based and heatmap-based decoders\nbuilt into Pixel 6 Pro devices and observed lower error rate, faster typing\nspeed, and higher self-reported satisfaction score based on the heatmap-based\ndecoder than the centroid-based decoder. These findings underline the promise\nof utilizing touch heatmaps for improving typing experience in mobile\nkeyboards.",
        "updated": "2024-10-03 07:29:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02264v1"
    },
    {
        "title": "Capturing complex hand movements and object interactions using machine learning-powered stretchable smart textile gloves",
        "authors": "Arvin TashakoriZenan JiangAmir ServatiSaeid SoltanianHarishkumar NarayanaKatherine LeCaroline NakayamaChieh-ling YangZ. Jane WangJanice J. EngPeyman Servati",
        "links": "http://dx.doi.org/10.1038/s42256-023-00780-9",
        "entry_id": "http://arxiv.org/abs/2410.02221v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02221v1",
        "summary": "Accurate real-time tracking of dexterous hand movements and interactions has\nnumerous applications in human-computer interaction, metaverse, robotics, and\ntele-health. Capturing realistic hand movements is challenging because of the\nlarge number of articulations and degrees of freedom. Here, we report accurate\nand dynamic tracking of articulated hand and finger movements using\nstretchable, washable smart gloves with embedded helical sensor yarns and\ninertial measurement units. The sensor yarns have a high dynamic range,\nresponding to low 0.005 % to high 155 % strains, and show stability during\nextensive use and washing cycles. We use multi-stage machine learning to report\naverage joint angle estimation root mean square errors of 1.21 and 1.45 degrees\nfor intra- and inter-subjects cross-validation, respectively, matching accuracy\nof costly motion capture cameras without occlusion or field of view\nlimitations. We report a data augmentation technique that enhances robustness\nto noise and variations of sensors. We demonstrate accurate tracking of\ndexterous hand movements during object interactions, opening new avenues of\napplications including accurate typing on a mock paper keyboard, recognition of\ncomplex dynamic and static gestures adapted from American Sign Language and\nobject identification.",
        "updated": "2024-10-03 05:32:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02221v1"
    }
]