Vegetable Peeling: A Case Study in Constrained
Dexterous Manipulation
TaoChen1,EricCousineau2,NaveenKuppuswamy2,PulkitAgrawal1
1MassachusettsInstituteofTechnology,2ToyotaResearchInstitute
{taochen, pulkitag}@mit.edu
Figure1: WepresentadexterousmanipulationsystemthatutilizesanAllegrohandmountedona
Frankarobotarmtoreorientfooditemsfordownstreampeeling. TheotherFrankarobotarm(the
rightarminthefigure)usesitsgrippertograspapeelerforpeeling. Thereorientationcontroller
fortheAllegrohandislearnedthroughreinforcementlearning,whilethepeelingisperformedvia
teleoperation. Inthefigure,wedemonstratetheprocessofreorientingandpeelingamelon,asweet
potato,andasquashfromtoptobottomrow.
Abstract: Recentstudieshavemadesignificantprogressinaddressingdexterous
manipulation problems, particularly in in-hand object reorientation. However,
there are few existing works that explore the potential utilization of developed
dexterousmanipulationcontrollersfordownstreamtasks. Inthisstudy,wefocus
onconstraineddexterousmanipulationforfoodpeeling. Foodpeelingpresents
variousconstraintsonthereorientationcontroller,suchastherequirementforthe
hand to securely hold the object after reorientation for peeling. We propose a
simplesystemforlearningareorientationcontrollerthatfacilitatesthesubsequent
peelingtask. Videosareavailableat: https://taochenshh.github.io/
projects/veg-peeling.
Keywords: In-handobjectreorientation,vegetablepeeling
1 Introduction
Havingrobotsperformfoodpreparationtaskshasbeenofgreatinterestinrobotics. Imaginethe
scenarioofmakingmashedpotatoes,whereacriticalstepistopeelpotatoes. Humanspeelpotatoes
by grasping the potato in one hand and using the second hand to actuate a peeler to remove the
potato’s skin. After a part of the potato is peeled, it is rotated while being held in the hand (i.e.,
in-handmanipulation)andpeeledagain. Thesequenceofrotatingandpeelingcontinuesuntilallof
thepotato’sskinisremoved. Inthiswork,wepresentaroboticsystemthatcanre-orientdifferent
vegetablesusinganAllegrohandinawaythattheirskincanbepeeledusinganothermanipulator.
OursetupisshowninFigure1andFigure2.
In-hand rotation of vegetables is an instance of dexterous manipulation problem [1], a family of
tasksthatinvolvescontinuouslycontrollingtheforceonanobjectwhileitismovingwithrespect
tothefingertips[2,3]. Thechallengesindexterousmanipulationstemfromthefrequentmaking
and breaking of contact, issues in contact modeling, high-dimensional control space, perception
challengesduetosevereocclusions,etc. Abodyofworkmadesimplifyingassumptionssuchas
manipulatingconvexobjects[4,5,1,6],smallfingermotions[7,8,9],sloworquasi-staticmotionor
4202
luJ
01
]OR.sc[
1v48870.7042:viXramanipulatingafewspecificobjects[10,7,8]toleveragetrajectoryoptimizationorplanning-based
methodstoachievein-handobjectre-orientation[1,7,8,9,6,4,5,10]. Anotherlineofworkhas
used reinforcementlearning for in-handre-orientation[11, 12, 13, 14, 15] andrecent works have
leveragedsimulationtotrainpoliciescapableofdynamicallyre-orientingadiversesetofnewobjects
inreal-timeandintherealworld[11,12].
Thereareseveralchallengesinadaptingre-orientationcontrollers
foradownstreamtasksuchaspeelingvegetables. Thesechallenges
stemfromthefactthatcontrollersoptimizedforre-orientation[16,
13,14,15,12]areonlyoptimizedtocontinuouslyreorienttheobject
andnottosatisfynumerousconstraintsarisingfromtask-specific
requirements. Forinstance,peelingvegetablesrequiresthehandto
firststopre-orientingtheobjectandthenforthepeelertopeelthe
vegetable. Manypriorworkssolveaversionofthere-orientation
problem where the object is continuously rotated [17, 16, 13] or
otherwiseperformquasistaticre-orientation[8]. Stoppingandre-
startingdynamicre-orientationisdifficultduetothechallengeof Figure 2: Robot setup for re-
dealing with the object’s inertia. Second, thehand needs to hold orientationandpeeling.
theobjectfirmlyenoughtoresistforcesappliedbythepeeler. The
closestworkthatattemptstoholdtheobjectatatargetconfiguration[12]isonlyabletolooselyhold
theobjectwhichisinsufficientforresistingforces. Third,thehandneedstoreorientthevegetable
alongaspecificaxisinplace. Here,thespecificaxisreferstotherotationalaxisontheobjectthatis
paralleltothepeelingdirection. Similartohowhumansreorientvegetablesforpeeling,itisdesirable
forthehandtoreorienttheobjectinplacesothatmultipleconsecutivecyclesofreorientationand
peeling can be performed. If the object substantially shifts its position during reorientation, the
controllerwillstruggletoreorientandholdtheobjectatfuturetimesteps.Fourth,whenthevegetable
isheldstationarythefingersshouldnotobstructthetopsurfaceofthevegetabletoensurethatthe
peelercanpeelthevegetable.
Whilein-handobjectreorientationhasbeenwidelystudied[11,12,16,18,13,17],nopriorworks
cansatisfytheconstraintsmentionedabove. Yet,theseconstraintsbecomecriticalfordownstream
dexterousmanipulationbeyondobjectre-orientation. Weusevegetablepeelingasacasestudyto
investigatethechallengesandsolutionsforbuildingadexterousmanipulationsystemthatcanoperate
underconstraints. Wedevelopaframeworkwhereweleveragereinforcementlearninginsimulation
totrainapolicythatcanperformobjectre-orientationunderconstraints. Forthepeelingtask,we
exploredtwoapproaches-ateleoperation-basedmethodleveraginghumanguidanceaswellasan
autonomousvision-basedtechnique. Ourcontributionsareasfollows:
1. Aframeworkforsolvingdexterousmanipulationproblemsundertheaforementionedcon-
straints.
2. WeproposeamethodthatcanmakeRLpolicylearntostopitsmotionandholdobjects
firmlyinhand–acriticalbehaviorformanydownstreamdexterousmanipulationproblems.
3. Wepresentasteptowardsaroboticsystemcapableofpeelingdiversevegetableswithdiffer-
entshapes,masses,andmaterialpropertieswhileholdingandmanipulatingthevegetables
inhand.
2 RelatedWork
In-handObjectReorientation:Dexterousmanipulationinvolvestheuseofhighdegrees-of-freedom
(DoF)manipulatorsforobjectmanipulation[19]. Itsrequirementforhigh-dimensionalreal-time
controlanditsnatureoffrequentcontact-makingandbreakingpresentgrandchallengestoroboticists.
Recently, there has been a growth of interest in a particular instance of dexterous manipulation
problems: in-handobjectreorientation. Thisproblemisofparticularinterestasitisanecessary
stepinmanytool-usescenarios. Forexample,touseascrewdriverfortighteningascrew,onehas
toreorientthescrewdrivertoalignitwiththescrew. Wecanclustertheworksinin-handobject
2reorientationfrommanyaspects. Forexample,fromtheperspectiveofsensoryinformation,[20]
studiesopen-loopcubereorientationwithoutusinganysensors,[21,5,16,10,22]usemotioncapture
systemorspecialtrackingmarkersforobjectreorientation,[17]usesproprioceptivesensorssuchas
jointencoders,[23,24,15,14]usetactilesensorsand[25,16,12,18]utilizevisionsensors. Interms
ofthedynamicsofthesystem,[7,8,9]achievedobjectreorientationundertheassumptionofquasi-
staticmotionwhereobjectmovesslowlyanditsinertiaeffectcanbeignored,while[15,16,12,14,26]
focusesondynamicobjectreorientationwhereobjectismanipulatedinafastanddynamicway. To
makein-handobjectmanipulationusefulfordownstreamtoolusetasks,oneimportantaspectofthe
skillistheabilityofstablyandfirmlyholdingtheobjectinendofthepolicyrollout. Whilemany
priorworksondynamicmanipulationsuchas[16,10,14,15,17]onlyconsiderendlesslyrotating
theobjectinhandandcannotstoptheobjectstablywhentheobjectreachesthegoalorientation,
someworkssuchas[12,26]trytodevelopcontrollersthatcanreorientobjectsinhandandalsohold
theobjectinthegoalorientation. Ourworkstudiesdynamicin-handobjectmanipulationwiththe
capabilityofstoppingobjectsstablyinhand.
ReinforcementLearningforContact-richTasks: Contact-richtasksareparticularlychallenging
duetothedifficultyinmodelingthesystemdynamics,especiallywhenthetasksareperformedinthe
wild,outsideofaconstrainedandcontrolledsetting.Examplesofsuchtasksincludequadrupedrobots
hikinginmountainsandrobothandsreorientingvariouseverydayobjects. Therehavebeenmany
worksusingreinforcementlearningtolearncontrollersforsolvingcontact-richtasks[27,16,13,28,
29,30,31].Intherealworld,robotstypicallyonlyhaveaccesstoalimitedamountofstateinformation
ofthesystemduetothelackofsensorsorthechallengesinsettingupthesensors.Usingreinforcement
learningtolearncontrollersfromscratchwithlimitedsensoryinformationtendstobedata-inefficient.
Onewaytospeeduppolicylearningistoprovideasymmetricinformationtothepolicyandvalue
function, where the value function observes much more privileged information [16, 13, 27, 32].
Anothermethodistodecouplepolicylearningintotwostages: areinforcementlearningstagewhere
agents(teacher)observeprivilegedfully-observablestateinformation,andanimitationlearningstage
wherethepolicywithlimitedsensoryobservationinput(student)learnstoimitatethepolicywith
fully-observablestateinformation. Thisapproachhasbeensuccessfullyappliedtovariouscontact-
rich problems such as locomotion [33, 34, 30, 35, 36] and dexterous manipulation [11, 12, 17].
Our pipeline is built upon the idea of teacher-student policy learning and has made several key
improvements,whichwewilldetailbelow.
3 Method
Peeling requires a reorientation controller that can stop its motion and firmly hold objects after
reorientation. Thefirststepinstoppingistodecidewhenre-orientationshouldbestopped. One
possibilityistohaveaperceptionsystempredictthedesiredrotationangleafterwhichthenextround
ofpeelingwouldbeperformed. Toaccomplishthegoal,therobotwouldneedtotrackchangesin
objectposeandcompareitwiththetargetrotationangle. However,accuratelyestimatingobjectpose
ischallenging,especiallywhengeneralizationtonewobjectsisnecessary[37,16,13,31].
Oneofourinsightsisthatinsteadoftrainingapredictorfordesiredrotationangleandobjectpose
estimation,itcanbeeasierandsufficienttotrainabinaryvisionclassifierthatdetectsinreal-time
whenthepeeledparthasbeenturnedover. Withsuchaclassifier,thereorientationcontroller’sjobis
simplytokeepreorientingtheobjectuntilitreceivesastopsignal. Inthisformulation,unlikeprior
works[11,12],thereorientationcontrollerisnotconditionedontargetorientationbutratherona
stopsignal. Formally,thepolicytakesasinputabinaryvariableIstop ∈{0,1}representingthestop
t
signal. IfIstop =1,thepolicyshouldstopimmediatelyandensurethefingersstablyandfirmlyhold
t
theobject. Otherwise,thepolicyshouldcontinuereorientingtheobject. Notethatinthiswork,we
focusonlearningthereorientationcontroller,leavingintegrationofavisionclassifiertofuturework.
Thenextquestionishowtotrainsuchapolicy. UsingRLtotrainthepolicyfromscratchcanbe
challengingandrequiresextensiverewardshapingbecauseIstop =1isarareeventinanepisode,
t
3andwhentheIstopisflippedtoonefromzero,thepolicyneedstoquicklystopthemotionposinga
t
hard-explorationchallenge.
Priorworks[11,12]showsuccessintrainingagoal-conditionedobjectreorientationcontroller. Can
weleverageagoal-conditionedreorientationcontrollertotrainacontrollerthatreactstoastopsignal?
Itturnsoutwecanformulatethisusingtheteacher-studentlearningframework[11,12,38,35,34].
Specifically,wecanuseRLtotrainagoal-conditionedcontrollerthatreorientsanobjectbyrandom
goalanglesalongitsrotationalaxis. Thisactsastheteacher. Next,wecanuseimitationlearning
(specificallyDAGGER[39])totrainacontrollerconditionedonthestopsignaltoimitatetheteacher.
Thestopsignalcanbegeneratedduringtrainingbycheckingiftheorientationdistancetothegoalis
belowathreshold. Usingimitationlearningbypassesthehardexplorationchallenge.
3.1 TeacherPolicyLearning: ReorientandStop
Wetraintheteacherpolicytore-orienttheobjectalongapre-definedaxisandstop(seeFigure3a).
Theteacherisformulatedasagoal-conditionedpolicyaE =πE(oE,a ,g),whereE represents
t t t−1
variables for the teacher policy, o is the observation, a is the action command, g is the goal
t t
representingtheamountbywhichtheobjectneedstobere-oriented. gisrandomlyanduniformly
sampledfrom[1.57,4.0]radduringtraining.
Whiletheteacherpolicy’sformulationissimilartothatinpriorworks[11,12],wepropose(i)a
muchsimplerrewardfunction,(ii)newsuccesscriteriathateffectivelyencouragesthepolicytostop
theobjectandfirmlyholdit,and(iii)aninterpolationschemethatenablessmootherpolicyactionsin
therealworld.
3.1.1 RewardFunction
Acommonapproachtodesigningtherewardfunctionistocreatemultipletermsthatmakeiteasier
forthemanipulatortodiscoverthedesiredbehavior(i.e.,rewardshaping). Forinstance,tofacilitate
exploration,wecandevisearewardtermthatreducesthedistancebetweenthefingertipsandthe
centerofmass(CoM)oftheobject. Todiscourageexcessivetranslationalmotionoftheobjectduring
rotation,wecancreatearewardtermthatpenalizesthedisplacementoftheCoM.Todiscouragethe
objectfromrotatingwithundesiredmotionalongotheraxes,wecanaddanotherrewardtermthat
reducesthedistancebetweenthetipofthethumbandthecenterlineofthepalm. Thisensuresthat
thethumbappliesforceclosetotheobject’sCoM,ratherthantoonesideoftheobject. Additionally,
weneedtodesignarewardtermthatdiscouragesthefingersfromcoveringthetopsurfaceofthe
object,whichaffectspeeling. Hence,designingmultiplerewardtermsisnecessarytoregulatethe
behaviorunderspecificconstraints. Balancingthesetermsrequiresextensivehyper-parametertuning.
For the task of in-hand re-orientation, we found that the reward function can be substantially
simplified by using a task demonstration. However, unlike prior works that rely on trajectory-
level demonstrations [40, 41], our method only requires a one-step demonstration (a keyframe),
whichismucheasiertocollect. Specifically,wemanuallymovetherealAllegrohandtoagoodpose
wheretheconstraintsmentionedabovearesatisfied(e.g.,thefingersdonotcoverthefooditem),
andthefingerstouchtheobjectandarereadytoreorientit. Werecordthejointpositionsasqdemo.
Duringtraininginsimulation,weencouragethejointpositionsatanytimesteptobeclosetoqdemo.
Overall,ourrewardfunctionisasfollows:
r
t
=c 11(Tasksuccessful)+c
2|∆θ
1
|+ϵ
+c 3(cid:13) (cid:13)q t−qdemo(cid:13) (cid:13)2
2
(1)
t θ
where c = 800,c = 1.5,c = −0.6 are coefficients. 1(Tasksuccessful) is 1 when the task is
1 2 3
successfullycompleted,and0otherwise. ∆θ isthedistancebetweentheobject’scurrentandgoal
t
orientation. Thefirsttwotermsaretaskrewardsforobjectreorientation. Thelasttermistoregulate
handbehavior.
4Teacher Student
MLP Transformer
...
(a) (b) (c)
Figure3: (a)showsanexampleoftherotationalaxisofamelon. (b)showsanexamplewherethe
object’sorientation(theblueline)hasalargedeviationfromthedesiredrotationalaxis(thegreen
line). Weresettheepisodewhenthisoccurs. (c)showsthepolicyArchitecturefortheteacherandthe
student. Inthisfigure,weuseo torepresentallthepolicyinputateachtimestep.
t
3.1.2 SuccessCriteria
Inagoal-conditionedobjectreorientation,acommonwaytoclaimthetasksuccessfulisbychecking
ifthedistancebetweentheobject’scurrentandthegoalorientationissmallerthanathresholdvalue
(orientationcriterionC = ∆θ < θ¯)[16,13]. Anothercriterionisthatallthefingertipsshould
ori
makecontactwiththeobject(contactcriterionC ),apre-requisiteforfirmlyholdingtheobject
contact
afterreorientation. However, onlycheckingthesetwocriteriaisinsufficienttoensurethepolicy
learnstostopthemotionandholdtheobjectfirmlyaroundthegoalorientation,asdiscussedin[12].
Thepolicycanoscillatearoundthegoalstateduetoobservationandcontroldelayandnoise.
Tofurtherencouragethepolicytostoprobotmotionwhenthegoalisreachedandfirmlyholdthe
object,weproposeaddingtimeconstraintstothesuccesscriteria: bothC andC shouldbe
ori contact
continuouslysatisfiedforT¯succtimesteps. AddingthiscriterionmakestheMDPpartiallyobservable
sincethepolicy’sobservationlackstheknowledgeoftime. Therefore,tofacilitatepolicylearning,
we augment the observation space with a scalar indicator variable Isucc = tsucc/T¯succ ∈ [0,1],
wheretsuccisthenumberofconsecutivestepssatisfyingC andC . Theobservationspace
ori contact
becomesoE :=oE ⊕Isucc. Inthiswork,θ¯=0.2rad,T¯succ =8.
3.1.3 ResetConstraints
Asmentionedearlier,areorientationpolicyforpeelingneedstomeetseveralconstraints,suchas
in-placeandfixed-axisreorientation(Figure3b). Whileonecoulddesignindividualrewardterms
tosatisfytheseconstraints,tuningtheserewardtermstoachievethedesiredresultcanbedifficult.
Instead,itismuchsimplertoformulatetheconstraintsasresetconditions. Inotherwords,ifthe
constraintsareviolated, theepisodeisresetimmediately. Thisincentivizesthepolicytoexplore
onlyinspacewheretheconstraintsaresatisfied. Similartechniqueswerealsousedinsomeprior
works[11,12,14].
3.1.4 InterpolationandReferenceforActionCommands
Ourneuralnetworkcontrolleroperatesatarelativelylowcontrolfrequencyof12Hz.Totrackthejoint
positioncommand,alow-levelPDcontrollerrunsat300Hz. Toensuresmootherjointmotion,we
interpolatethelow-frequencyjointpositioncommands. Whilemorecomplexinterpolationschemes
suchassplineinterpolationarepossible,wefoundthatsimplelinearinterpolationissufficientto
generatesmoothhigher-frequency(60Hz)jointpositioncommands.Todothis,welinearlyinterpolate
betweenthecurrentreferencejointpositions(qref)andthedesiredjointpositions(qcmd)forthenext
t t+1
policycontroltimestep. WethensendtheinterpolatedjointpositioncommandstothePDcontrollers.
Mathematically,qcmd,n =qref + na ,wheren∈[1,N](N =5)andqcmd,n representsthenth
t+1 t N t t+1
interpolatedjointpositioncommandforthenextpolicycontroltimestep.
Whentheactionspaceischosenasthechangeinjointposition,thetargetjointpositionforthePD
controlleriscalculatedasfollows: qcmd =q +a [12,11,16]. Here,q isthecurrentjointposition,
t+1 t t t
anda =∆q isthedesiredchangeinjointpositions,asdescribedearlier. Inthiscase,thereference
t t
5ischosentobethecurrentjointpositions,i.e.,qref =q . However,wefoundthatthisschemeresults
t t
insignificantjerkymotionwhencombinedwithactioninterpolation. Toillustratethis,considera
simplifiedexampleofonejoint,asshowninFigure4a. SinceweareusingaPDcontrolleronlyto
controlthejointposition,thereisusuallyanerrorintrackingthejointpositioncommand,asshownby
thedifferencebetweenqcmdandq . Ifwesetqref =q ,whenweinterpolatebetweenqref andqcmd,
t t t t t t+1
ittendstocauseasuddenchangeinthePDcontroller’ssetpoint,asshownin Figure4a. Asudden
changeinthesetpointcancauseasuddenchangeinthejointtorquecommandandhencecausejerky
motion. Toresolvethisissue,weusethepreviousjointpositioncommandasthereference,asshown
inFigure4b. Inotherwords,qref =qcmd,andqcmd =qcmd+a .
t t t+1 t t
Joint position Joint position
Time Time
(a) (b)
Figure4: Examplesofjointpositioncommandsafterinterpolationsenttoalow-levelPDcontroller.
representstheactualjointpositionofthemotor. isthecomputeddesiredjointposition.
onthegreenlineshowstheinterpolatedjointpositioncommandsthataresenttothelow-levelPD
controller. (a)showsthecaseofqcmd =q +a ,while(b)showsthecaseofqcmd =qcmd+a .
t+1 t t t+1 t t
Wecanseethat(b)generatesmuchsmootherjointcommands.
3.2 StudentPolicyLearning: ImitateandStop
After learning a goal-conditional teacher policy aE = πE(oE,a ,g), the next question is how
t t t−1
totrainareal-worlddeployablestudentpolicythatcanrotatetheobjectinhandandholditstably
after reorientation. We propose conditioning the student policy on a stop signal Istop ∈ {0,1}:
t
aS =πS(oS,a ,Istop). Inotherwords,thestudentpolicyshouldcontinuereorientingtheobject
t t t−1 t
whenIstop =0,butstablyholdtheobjectwhenIstop =1. Thisdesignchoiceprovidesflexibilityin
t t
howwecontrolthepolicytostopthereorientation. Forexample,thepolicycouldrotatetheobject
forapre-specifiedamountoftime(i.e., setIstop = 1aftertseconds). Alternatively, anexternal
t
perceptionmodulecoulddetectwhenthepeeledparthasfullyturnedover,triggeringIstop =1and
t
thepolicytostopthemotionandholdtheobjectimmediately.
Howcanweusethelearnedgoal-conditionedteacherpolicytotrainastudentpolicythatisconditioned
onthestopsignal? WecansetthevalueforIstopautomaticallyduringpolicyrolloutbasedonthe
t
orientationdistance∆θ .
t
(cid:26) 0 if∆θ >θ¯
Istop = t
t 1 otherwise
DetailsabouttheobservationspaceandthepolicyarchitectureareinSectionA.3intheappendix.
3.3 Peeling
Inthissection,wedemonstratethatourreorientationcontrollercanbeusedfordownstreampeeling
tasks. WeusethedexterousrobothandtodothereorientationandthencontrolanotherFrankaPanda
robotarmtodothepeelingasshownin Figure2. Tocontroltherobotarm,weexperimentedwith
bothusingateleoperationsystemandanautomaticvision-basedpeelingsystem.
3.3.1 Teleoperation-basedpeeling
Weusedaleader-followerteleoperationsysteminwhichahumanoperatorcontrolsaleadersystem,
and the Franka arm follows the motion of the leader in real-time. A 200 Hz operational space
6(a) (b) (c) (d) (e) (f)
Figure 5: (a): the Allegro hand holds a papaya to be peeled. (b): we utilize Grounded SAM to
segmentthepapaya. (c): the3Dpointcloudrepresentingthesegmentedpapaya’sexposedsurface.
(d): wetakeasliceofthispointcloudatthecenterregionalongthepapaya’slongestaxis. (e): the
pointswithinthiscentersliceareprojectedontothecentralplanealignedwiththeaxis. (f): wefita
splinecurvetotheprojectedpointstoobtainthedesiredtrajectoryforthepeelertiptofollow.
impedancecontroller[42]runsonthePandaarm,controllingforposeviatorque,andanoperator
interactswithaHaptionVirtuose™6DHFTAO1device. Bilateralposition-positionhapticcouplingis
donebetweenthetwodevices. ThecontrollersandhapticcouplingareimplementedusingDrake[43].
3.3.2 Vision-basedpeeling
While teleoperation provides effective peeling commands for the Franka arm and demonstrates
thatourreorientationcontrollercanfirmlygraspobjectsafterreorientation,automatingthepeeling
processwouldbeideal. Oneapproachtoachievethisisbycomputingthepeeler’smotiontrajectory
basedonRGBanddepthvisiondata. Thetrajectorycanbedeterminedthroughthefollowingsteps
(seeFigure5): (1)WeutilizeGroundedSAM[44]tosegmentthetargetvegetablegivenanimage
andvegetablenameinput. (2)Usingthesegmentationmaskanddepthdata,wereconstructthe3D
pointcloudrepresentingthevegetable’stopsurface. (3)Weidentifythevegetable’slongestaxis(the
peelingdirection)byapplyingprincipalcomponentanalysis. (4)Weslicethepointcloudintoa2cm
thicksegmentalongthecentralplanethatcrossesthecenterpointandalignswiththelongestaxis.
Wethenprojectallthepointswithinthesliceontotheplane. (5)Wefitasplinecurvetotheprojected
pointstoobtainasmoothtrajectoryforthepeelertip. Finally,cartesian-spacepositioncontrolmoves
thepeeleralongthistrajectorywhilekeepingthepeelerorientationfixed.
4 Results
Toquantitativelyevaluatethereal-worldpolicytransferperformance,wetestedthecontrolleronfour
vegetables(FigureB.2a): apumpkin(mass: 827g),amelon(623g),aradish(727g),apapaya(848g).
4.1 Travelingdistanceforafixedamountofcommandedmotiontime
Thefirstquestionwewanttoansweriswhetherthelearnedpolicycansuccessfullyreorientvegetables
intherealworld. Inpeeling,thewidthofthepeeledpartdependsonthepeeler’swidth. Thus,itis
moreinformativetomeasurehowmuchthereorientationcontrollerrotatesanobjectbythetraveling
distanceofasurfacepoint,ratherthantheabsoluterotationangle. Specifically,wemarkareference
pointPref ontheobjectsurfacenearthemid-pointofitsrotationalaxis. Atthestart,weensurePref
iscenteredandfacingupwardwhenheld. Afterreorientation,werecordthenewpointPnew thatis
nowcenteredandfacingupward. WethenmeasurethecontourlengthfromPnew toPref alongthe
surface(FigureB.2b).
Todemonstratethecapabilityofourcontrollertoreorientrealobjects,weconductedtworoundsof
testing. Ourcontrolleristrainedtostopmotionwhenitreceivesastopsignal. Inthefirstround,we
sentthestopsignal3.5secondsafterthecontrollerstartedrotating. Inthesecondround,wesentthe
stopsignal7secondsafterstart. Werepeatedeachtest10times. AsshowninFigure6a,thecontroller
successfullyreorientedallfourfooditemsbyasufficientamountforpeeling. Whencommandedto
1https://www.haption.com/en/products-en/virtuose-6d-tao-en.html#
fa-download-downloads
716 1.4
Distance
(cm) 111 2468024
3.5s
Time
to
stop
(s) 00001 .....
1
24682
0 7.0s 0
Pumpkin Melon Papaya Raddish 3.0 4.5 6.0 7.0
Object Commanded motion time (s)
(a) (b)
Figure6: (a): Violinplotsshowingthedistributionofthetravelingdistanceofapointontheobject
surfaceafterthecontrolleriscommandedtorotatetheobjectfor3.5sand7s, respectively. (b):
Violinplotshowingthedistributionoftimetakenbythecontrollertotransitionfromrotatingthe
objectinhandtofirmlyholdingtheobjectafterreceivingthestopsignal. Thex-axisrepresentsthe
timingofthestopsignalsenttothecontrollerafteritstarts.
reorientfor3.5s,90%oftestsreorientedtheobjectsbyatleast4cm. With7s,90%oftestsreoriented
objectsbyatleast7.3cm. Givenmoretime,thecontrollerreorientedobjectsbyalargeramount.
4.2 Howwelldoesthecontrollertrackthecommandedmotiontime?
AsdiscussedinSection3,ifourcontrollercanquicklyrespondtoastopsignalatanytimestep,it
canbecombinedwithaperceptionsystemthattrackspeelingprogress. Hence,wemeasuredhow
longittakestostopthehandandobjectmotionafterreceivingthestopsignal. AsshowninFigure6b,
themotionstopsafter0.4sonaverageafterthecontrollerreceivesthestopsignal.
4.3 Firmgraspafterreorientation
Toenabledownstreampeeling,thereorientationcontrollermustlearntofirmlygrasptheobjectafter
stoppingfingermotion. WetestedthisbycheckingiftheAllegrohandandobjectcouldbelifted
in the air for 3s by only lifting the object with a single human hand. Table B.1 in the appendix
showsthatacrossobjectsandcommandedtimes,thecontrollerfirmlygraspedobjectsin90%oftests.
Moreover,ourcontrollerpossessesthecapabilityofperformingconsecutivereorientations. Itcan
repetitivelyexecutethesequenceofpeelingandreorientationmultipletimesinsuccession.
4.4 Real-worldPeeling
We evaluated whether the reorientation controller could reorient food items to facilitate peeling
(Figure 1). We tested using an Allegro hand and a Leap hand [45]. Testing showed that peeling
appliedsubstantialpullingforcesonobjects. However,inmostcases,bothhandsmaintainedafirm
enoughgrasptoenablesuccessfulpeeling. Failuresoftenoccurwhenholdingsmallobjects,assome
fingertipsmayfailtoestablishsecurecontactwiththesurface.
5 Discussions
Thereorientationcontrollerpresentedinthisstudyisablindcontrollerthatreliessolelyonpropri-
oceptivesensoryinformation. Whileithasdemonstratedtheabilitytosuccessfullyreorientheavy
objectsandsecurelyholdtheminplace,itsperformancecouldpotentiallybeenhancedbyincorpo-
ratingvisualandtactilefeedback. Thecurrentsystemhasafewfailuremodes. Firstly,theobject
mightslipoutofthehandsincethecontrollerdoesnotutilizeanyvisioninformation. Secondly,the
controllermightfailifthevegetablesaresmall,asthefingerscannoteffectivelymakecontactwiththe
object. Whenusingavision-basedpeelingapproachtopeelthevegetables,thesegmentationnetwork
(Grounded SAM) might fail to correctly identify and segment the target vegetable in the image.
Sometimes,thesegmentationmaskwouldincorrectlyincludetherobothand. Somefine-tuningof
the pre-trained Grounded SAM model would be necessary to mitigate such issues. Future work
couldinvolvelearningapeelingpolicyviabehaviorcloningondatacollectedviateleoperationto
achievebetterautonomyofthesystem. Additionally,incorporatingvisualandtactilefeedbackinto
thereorientationcontrollercouldpotentiallyenhanceitsperformance
8Acknowledgments
Wethanktheanonymousreviewersfortheirhelpfulcommentsinrevisingthepaper. Wealsoextend
our appreciation to the members of Toyota Research Institute for their valuable feedback on the
formulationofourresearchideaandtheirengagingdiscussionsaboutrelatedresearchproblems.
References
[1] D.Rus. In-handdexterousmanipulationofpiecewise-smooth3-dobjects. TheInternational
JournalofRoboticsResearch,18(4):355–381,1999.
[2] M.T.Mason,J.K.Salisbury,andJ.K.Parker. Robothandsandthemechanicsofmanipulation.
TheMITPress,1989.
[3] N.C.Dafle, A.Rodriguez, R.Paolini, B.Tang, S.S.Srinivasa, M.Erdmann, M.T.Mason,
I. Lundberg, H. Staab, and T. Fuhlbrigge. Extrinsic dexterity: In-hand manipulation with
externalforces. In2014IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
pages1578–1585.IEEE,2014.
[4] Y. Bai and C. K. Liu. Dexterous manipulation using both palm and fingers. In 2014 IEEE
InternationalConferenceonRoboticsandAutomation(ICRA),pages1560–1565.IEEE,2014.
[5] B.SundaralingamandT.Hermans. Relaxed-rigidityconstraints: kinematictrajectoryoptimiza-
tionandcollisionavoidanceforin-graspmanipulation. AutonomousRobots,43(2):469–483,
2019.
[6] I.Mordatch,Z.Popovic´,andE.Todorov. Contact-invariantoptimizationforhandmanipulation.
In Proceedings of the ACM SIGGRAPH/Eurographics symposium on computer animation,
pages137–144,2012.
[7] T.Pang,H.Suh,L.Yang,andR.Tedrake. Globalplanningforcontact-richmanipulationvia
localsmoothingofquasi-dynamiccontactmodels. arXivpreprintarXiv:2206.10787,2022.
[8] A.S.Morgan,K.Hang,B.Wen,K.Bekris,andA.M.Dollar.Complexin-handmanipulationvia
compliance-enabledfingergaitingandmulti-modalplanning. IEEERoboticsandAutomation
Letters,7(2):4821–4828,2022.
[9] S.Abondance,C.B.Teeple,andR.J.Wood. Adexteroussoftrobotichandfordelicatein-hand
manipulation. IEEERoboticsandAutomationLetters,5(4):5502–5509,2020.
[10] A.Nagabandi, K.Konolige, S.Levine, andV.Kumar. Deepdynamicsmodelsforlearning
dexterousmanipulation. InConferenceonRobotLearning,pages1101–1112.PMLR,2020.
[11] T. Chen, J. Xu, and P. Agrawal. A system for general in-hand object re-orientation. In
ConferenceonRobotLearning,pages297–307.PMLR,2022.
[12] T.Chen,M.Tippur,S.Wu,V.Kumar,E.Adelson,andP.Agrawal. Visualdexterity: In-hand
dexterousmanipulationfromdepth. arXive-prints,pagesarXiv–2211,2022.
[13] A. Handa, A. Allshire, V. Makoviychuk, A. Petrenko, R. Singh, J. Liu, D. Makoviichuk,
K.VanWyk, A.Zhurkevich, B.Sundaralingam, etal. Dextreme: Transferofagilein-hand
manipulationfromsimulationtoreality. arXivpreprintarXiv:2210.13702,2022.
[14] Z.-H.Yin,B.Huang,Y.Qin,Q.Chen,andX.Wang. Rotatingwithoutseeing: Towardsin-hand
dexteritythroughtouch. arXivpreprintarXiv:2303.10880,2023.
[15] G.Khandate,M.Haas-Heger,andM.Ciocarlie. Onthefeasibilityoflearningfinger-gaiting
in-handmanipulationwithintrinsicsensing. In2022InternationalConferenceonRoboticsand
Automation(ICRA),pages2752–2758.IEEE,2022.
9[16] O.M.Andrychowicz,B.Baker,M.Chociej,R.Jo´zefowicz,B.McGrew,J.Pachocki,A.Petron,
M.Plappert, G.Powell, A.Ray, J.Schneider, S.Sidor, J.Tobin, P.Welinder, L.Weng, and
W.Zaremba. Learningdexterousin-handmanipulation. TheInternationalJournalofRobotics
Research,39(1):3–20,2020.
[17] H.Qi,A.Kumar,R.Calandra,Y.Ma,andJ.Malik. In-handobjectrotationviarapidmotor
adaptation. InConferenceonRobotLearning,pages1722–1732.PMLR,2023.
[18] A.Allshire,M.MittaI,V.Lodaya,V.Makoviychuk,D.Makoviichuk,F.Widmaier,M.Wu¨thrich,
S.Bauer,A.Handa,andA.Garg. Transferringdexterousmanipulationfromgpusimulationto
aremotereal-worldtrifinger. In2022IEEE/RSJInternationalConferenceonIntelligentRobots
andSystems(IROS),pages11802–11809.IEEE,2022.
[19] A.M.Okamura,N.Smaby,andM.R.Cutkosky. Anoverviewofdexterousmanipulation. In
Proceedings2000ICRA.MillenniumConference.IEEEInternationalConferenceonRobotics
and Automation. Symposia Proceedings (Cat. No. 00CH37065), volume 1, pages 255–262.
IEEE,2000.
[20] A.Bhatt,A.Sieler,S.Puhlmann,andO.Brock. Surprisinglyrobustin-handmanipulation: An
empiricalstudy. Robotics: ScienceandSystems(RSS),2021.
[21] V.Kumar,A.Gupta,E.Todorov,andS.Levine. Learningdexterousmanipulationpoliciesfrom
experienceandimitation. arXivpreprintarXiv:1611.05095,2016.
[22] B.Calli,K.Srinivasan,A.Morgan,andA.M.Dollar. Learningmodesofwithin-handmanip-
ulation. In2018IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages
3145–3151.IEEE,2018.
[23] T.Ishihara,A.Namiki,M.Ishikawa,andM.Shimojo.Dynamicpenspinningusingahigh-speed
multifingeredhandwithhigh-speedtactilesensor. In6thIEEE-RASInternationalConference
onHumanoidRobots,pages258–263.IEEE,2006.
[24] H.VanHoof,T.Hermans,G.Neumann,andJ.Peters. Learningrobotin-handmanipulation
withtactilefeatures. In2015IEEE-RAS15thInternationalConferenceonHumanoidRobots
(Humanoids),pages121–127.IEEE,2015.
[25] B.CalliandA.M.Dollar. Vision-basedmodelpredictivecontrolforwithin-handprecision
manipulationwithunderactuatedgrippers. In2017IEEEInternationalConferenceonRobotics
andAutomation(ICRA),pages2839–2845.IEEE,2017.
[26] N.Furukawa,A.Namiki,S.Taku,andM.Ishikawa. Dynamicregraspingusingahigh-speed
multifingeredhandandahigh-speedvisionsystem. InProceedings2006IEEEInternational
ConferenceonRoboticsandAutomation,2006.ICRA2006.,pages181–187.IEEE,2006.
[27] OpenAI,I.Akkaya,M.Andrychowicz,M.Chociej,M.Litwin,B.McGrew,A.Petron,A.Paino,
M.Plappert,G.Powell,R.Ribas,etal. Solvingrubik’scubewitharobothand. arXivpreprint
arXiv:1910.07113,2019.
[28] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke.
Sim-to-real: Learningagilelocomotionforquadrupedrobots. Robotics: ScienceandSystems
(RSS),2018.
[29] X.Da,Z.Xie,D.Hoeller,B.Boots,A.Anandkumar,Y.Zhu,B.Babich,andA.Garg. Learning
acontact-adaptivecontrollerforrobust,efficientleggedlocomotion. InConferenceonRobot
Learning,pages883–894.PMLR,2021.
[30] Z.Li,X.Cheng,X.B.Peng,P.Abbeel,S.Levine,G.Berseth,andK.Sreenath. Reinforce-
mentlearningforrobustparameterizedlocomotioncontrolofbipedalrobots. In2021IEEE
InternationalConferenceonRoboticsandAutomation(ICRA),pages2811–2817.IEEE,2021.
10[31] J. Pitz, L.Ro¨stel, L.Sievers, and B. Ba¨uml. Dextrous tactile in-hand manipulationusing a
modularreinforcementlearningarchitecture. arXivpreprintarXiv:2303.04705,2023.
[32] L.Pinto,M.Andrychowicz,P.Welinder,W.Zaremba,andP.Abbeel. Asymmetricactorcritic
forimage-basedrobotlearning. arXivpreprintarXiv:1710.06542,2017.
[33] G.B.Margolis,G.Yang,K.Paigwar,T.Chen,andP.Agrawal. Rapidlocomotionviareinforce-
mentlearning. Robotics: ScienceandSystems(RSS),2022.
[34] G.B.Margolis,T.Chen,K.Paigwar,X.Fu,D.Kim,S.Kim,andP.Agrawal. Learningtojump
frompixels. InConferenceonRobotLearning,pages1025–1034.PMLR,2022.
[35] J.Lee,J.Hwangbo,L.Wellhausen,V.Koltun,andM.Hutter. Learningquadrupedallocomotion
overchallengingterrain. Sciencerobotics,5(47):eabc5986,2020.
[36] A.Kumar,Z.Fu,D.Pathak,andJ.Malik. Rma: Rapidmotoradaptationforleggedrobots.
Robotics: ScienceandSystems(RSS),2021.
[37] J.Tremblay,T.To,B.Sundaralingam,Y.Xiang,D.Fox,andS.Birchfield. Deepobjectpose
estimationforsemanticroboticgraspingofhouseholdobjects.arXivpreprintarXiv:1809.10790,
2018.
[38] D.Chen,B.Zhou,V.Koltun,andP.Kra¨henbu¨hl. Learningbycheating. InConferenceonRobot
Learning,pages66–75.PMLR,2020.
[39] S.Ross,G.Gordon,andD.Bagnell. Areductionofimitationlearningandstructuredprediction
tono-regretonlinelearning. InProceedingsofthefourteenthinternationalconferenceonartifi-
cialintelligenceandstatistics,pages627–635.JMLRWorkshopandConferenceProceedings,
2011.
[40] J.HoandS.Ermon. Generativeadversarialimitationlearning. Advancesinneuralinformation
processingsystems,29,2016.
[41] X.B.Peng,Z.Ma,P.Abbeel,S.Levine,andA.Kanazawa. Amp: Adversarialmotionpriors
forstylizedphysics-basedcharactercontrol. ACMTransactionsonGraphics(ToG),40(4):1–20,
2021.
[42] O. Khatib. A unified approach for motion and force control of robot manipulators: The
operationalspaceformulation. IEEEJournalonRoboticsandAutomation,3(1):43–53,Feb.
1987. ISSN2374-8710. doi:10.1109/JRA.1987.1087068. ConferenceName: IEEEJournalon
RoboticsandAutomation.
[43] R.TedrakeandtheDrakeDevelopmentTeam. Drake: Model-baseddesignandverificationfor
robotics,2019. URLhttps://drake.mit.edu.
[44] T.Ren,S.Liu,A.Zeng,J.Lin,K.Li,H.Cao,J.Chen,X.Huang,Y.Chen,F.Yan,etal.Grounded
sam: Assemblingopen-worldmodelsfordiversevisualtasks. arXivpreprintarXiv:2401.14159,
2024.
[45] K.Shaw,A.Agarwal,andD.Pathak. Leaphand: Low-cost,efficient,andanthropomorphic
handforrobotlearning. arXivpreprintarXiv:2309.06440,2023.
[46] V.Makoviychuk,L.Wawrzyniak,Y.Guo,M.Lu,K.Storey,M.Macklin,D.Hoeller,N.Rudin,
A. Allshire, A. Handa, and G. State. Isaac gym: High performance GPU based physics
simulationforrobotlearning. InThirty-fifthConferenceonNeuralInformationProcessing
SystemsDatasetsandBenchmarksTrack,2021.
[47] M.Deitke,D.Schwenk,J.Salvador,L.Weihs,O.Michel,E.VanderBilt,L.Schmidt,K.Ehsani,
A.Kembhavi,andA.Farhadi. Objaverse: Auniverseofannotated3dobjects. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages13142–13153,
2023.
11[48] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I.Polosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,
30,2017.
12AppendixA Training
A.1 Trainingsetup
Robot: WeuseanAllegroHandthatiscontrolledviaaPDcontrollerat300Hz. Ourcontrolpolicy
setsjointpositioncommandsandrunsatalowerfrequencyat12Hz.
Simulation: WetrainedthepoliciesinIsaacGymsimulation[46]. Tosetdynamics-relatedrobot
parametersinthesimulation,wefollowedapriorapproach[12],whichusesagradient-freesearch
method to find the dynamics parameters for each joint (joint friction, damping, maximum joint
velocity,andmaximumeffort)insimulationthatgeneratesthemotorresponsethatisclosesttothe
realmotors.
Object Dataset: We collected 23 object meshes (potatoes, squash, cucumber, etc.) from Obja-
verse[47]. 10variantsforeachmeshwerecreatedbyvaryingthesize. Themassoftheobjectwas
randomlysampledintherangeof[80,960]g. Notethatweaimtoreorientmuchheavierobjectsthan
priorworks[16,12,11,13].
FigureA.1: Objectdatasetusedinthiswork. Wecollectedmeshesofcarrot,sweetpotato,potato,
squash,pumpkin,etc.
A.2 TeacherPolicyLearning
A.2.1 ObservationandActionSpace
oE includesjointpositionsandvelocities,thefingertipposesandvelocities,objectposeandvelocity,
t
thedistancebetweenthecurrentobjectorientationandthegoalorientation,andwhetheranyofthe
fingertipstouchtheobject. a isthedeltajointpositioncommand. Theneuralnetworkpolicyrunsat
t
12Hz.
A.2.2 DomainrandomizationandPerturbationduringtraining
Duringtraining,weapplydomainrandomizationonthejointstiffnessanddamping,friction,and
restitution. Additionally,werandomlyapplyaperturbationforceontheobject’sCoM.Werandomly
samplethedirectionoftheperturbationforceandsetitsmagnitudeto10m ,wherem istheobject
o o
mass.
A.3 StudentPolicyLearning
A.3.1 ObservationSpace
Inthiswork,weonlyuseproprioceptivesensoryinformation(jointpositionsq andvelocitiesq˙ )
t t
astheobservationinput(oS). Ourfindingsindicatethatrelyingsolelyonproprioceptivesensory
t
informationresultsinstrongperformance. Futureresearchcouldinvestigateincorporatingvisualdata
tofurtherenhancethesystem’scapabilities,suchaspreventingobjectsfromslippingoutofthegrasp.
A.3.2 PolicyArchitecture
Asthestudentpolicyonlyhasaccesstoalimitedamountofsensoryinformation(aPOMDPsetting),
itisimportanttoincorporatehistoryinformation,ashasbeendoneinpreviousworks[16,13,12].
While [16, 13, 12] utilized RNNs to process history information, Transformers [48] have gained
significant attention due to their improved performance and faster training in domains such as
13A B
A
(a) (b)
FigureB.2: (a)showstheobjectsforevaluation: melon,radish,pumpkin,papaya. (b)showsthe
travelingdistance.Beforereorientationbegins,weensureareferencepoint(pointA)isfacingupward.
Afterreorientation,weidentifythepoint(pointB)nowfacingupward. Wethenmeasurethedistance
frompointAtopointBalongthecontour.
TableB.1: Successfulliftingrate(10testseach)
Commandedmotiontime Pumpkin Melon Papaya Radish
3.5s 80% 90% 80% 90%
7s 100% 90% 100% 90%
natural language processing. Therefore, in this work, we employ a Transformer-based policy
architecture. aS = πS(oS,a ,Istop,...,oS,a ,Istop). The policy is a decoder-only attention
t 1 0 1 t t−1 t
network(Figure3c)withthreeself-attentionlayers. Thehiddensizeis256,theintermediatesizeis
512,andthenumberofattentionheadsis8. ThepolicyistrainedusingDAGGER[39].
AppendixB Testing
B.1 Testingsetup
Figure B.2a show the objects used for evaluation. Figure B.2b illustrates how we measure the
travelingdistanceoftherotationmotion.
B.2 Firmgraspafterreorientation
Table B.1 shows the success rate of the lifting action after the reorientation. It shows that our
reorientationcontrollercancontrolthefingerstofirmlyholdtheobjectafterthereorientation.
B.3 Ablationstudy
DemoterminRewardfunction Weproposedusingakeyframedemonstrationtoeasereward
shaping. Toevaluateitseffectiveness,wecomparedlearningcurvesoftheteacherpoliciestrained
withandwithoutthec
3(cid:13)
(cid:13)q
t−qdemo(cid:13) (cid:13)2
2rewardterm. AsshowninFigureB.3a,addingthekeyframe
substantiallyimprovedlearning. Additionally,itdemonstratesthatmimickingthekeyframeposevia
asinglerewardtermeffectivelyreducesthereward-shapingburden.
NecessityofhavingjointvelocityinformationinπS Thestudentpolicy’ssensoryinputincluded
jointpositionsandvelocities. Weinvestigatedwhetherincludingjointvelocityinformationinthe
inputisbeneficial. FigureB.3bshowsthataddingjointvelocitiestotheinputimprovedperformance.
Transformer vs RNN Different from prior works [16, 13, 11, 12], our student policy uses a
TransformerarchitectureinsteadofanRNNarchitecture. Wecomparedthelearningperformance
ofaTransformer-basedpolicyandanRNN-basedpolicy. FigureB.4aandFigureB.4bshowthat
aTransformer-basedpolicylearnsmuchfasterandgetsbetterperformanceatconvergencethanan
RNN-basedpolicy.
141 1
e w/o demo w/ demo e
at0.8 at0.8
r r
s 0.6 s 0.6
s s
ce0.4 ce0.4
Pos
c c
u0.2 u0.2 Pos+Vel
S S
0 0
100M 200M 300M 400M 10M 20M 30M
Steps Steps
(a) (b)
FigureB.3: (a)showslearningcurvesoftheteacherpolicieswithorwithoutc
3(cid:13)
(cid:13)q
t−qdemo(cid:13) (cid:13)2
2
in
therewardfunction. (b)showsthedifferencesbetweenstudentpoliciestrainedwithdifferentsensory
information(jointpositionsandvelocitiesvs. jointpositionsonly).
1 1
e e
at0.8 at0.8
r r
s 0.6 s 0.6
s s
ce0.4
RNN
ce0.4
RNN
c c
u0.2 Transformer u0.2 Transformer
S S
0 0
20M 40M 60M 80M 0.5 1 1.5
Steps Wall clock time (h)
(a) (b)
FigureB.4: LearningcurvesofstudentpolicieswithaTransformerorRNNarchitecturewithrespect
tothenumberofsamplesandwall-clocktime,respectively.
15