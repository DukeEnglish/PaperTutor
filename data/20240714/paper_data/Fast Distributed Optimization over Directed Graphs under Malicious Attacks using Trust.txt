Fast Distributed Optimization over Directed Graphs under
Malicious Attacks using Trust
Arif Kerem Dayı∗, Orhan Eren Akgu¨n∗, Stephanie Gil, Michal Yemini, Angelia Nedi´c †
Abstract
Inthiswork,weintroducetheResilientProjectedPush-Pull(RP3)algorithmdesignedfordistributed
optimizationinmulti-agentcyber-physicalsystemswithdirectedcommunicationgraphsandthepresence
of malicious agents. Our algorithm leverages stochastic inter-agent trust values and gradient tracking
to achieve geometric convergence rates in expectation even in adversarial environments. We introduce
growing constraint setsto limittheimpact ofthe maliciousagentswithoutcompromising thegeometric
convergencerateofthealgorithm. WeprovethatRP3convergestothenominaloptimalsolutionalmost
surelyandinther-thmeanforanyr≥1,providedthestepsizesaresufficientlysmallandtheconstraint
sets are appropriately chosen. We validate our approach with numerical studies on average consensus
and multi-robot target tracking problems, demonstrating that RP3 effectively mitigates the impact of
malicious agents and achieves the desired geometric convergence.
1 Introduction
Inthiswork, weareinterestedindistributedoptimizationproblemsinvolvingminimizingthesumofagents’
individualstronglyconvexlossfunctions,potentiallyoverclosedandconvexconstraintsetsinthepresenceof
malicious agents. Distributed optimization lays the foundation for many algorithms in multi-robot systems
[1] and sensor networks [2] such as collaborative manipulation [3], distributed control [2,4], localization
[5,6], and estimation [7,8]. In this work, we focus on two important challenges in the analysis of such
distributed optimization problems and their compounding impact: 1) having a directed communication
graph, and 2) the presence of malicious agents in the system. The study of directed communication graphs
is crucial for applications where the communication capabilities of the agents are heterogeneous. However,
havingasymmetricinformationflowrequirescarefuldesignofthedistributedoptimizationalgorithms[9–11].
The other great challenge is the presence of malicious agents, which may cause catastrophic effects on the
performance of multi-agent systems when there are no precautions taken [12–16]. Despite considerable
advancements in distributed optimization research, the combined impact of directed communication graphs
and malicious agents remains an under-explored area. Our goal in this paper is to develop and analyze a
fast resilient distributed optimization algorithm for directed communication graphs. Specifically, we want
the algorithm to have geometric convergence in directed graphs even in the presence of malicious agents.
Tomitigatetheimpactofmaliciousagents,agrowingbodyofliteratureinvestigatestheuseofthephysical
channels of information in cyber-physical systems. Agents can use many methods such as camera inputs,
sensorobservations,andwirelessfingerprintstoderiveinter-agenttrustvaluesandassessthetrustworthiness
of their neighbors [17–19] (see [20] for a survey). These trust values are stochastic and imperfect as they
come from noisyphysicalinformation [17]. However, agents can accumulate more values over time to have a
better estimate of trustworthiness of the agents they interact with. It has been shown that these inter-agent
trust values in such cases can lead to strong theoretical guarantees for multi-agent systems [14–16,21,22].
Moreover, methods that exploit inter-agent trust values do not require additional assumptions limiting the
number of tolerable malicious agents, or their strategies [14,15,22], unlike methods that rely solely on
transmitted data to eliminate malicious information [12,13,23]. Recent work [15] shows that it is possible
∗Co-primaryAuthors
†A. K. Dayı, O. E. Akgu¨n, and S. Gil are with the School of Engineering and Applied Sciences, Harvard University, USA.
M.YeminiiswiththeFacultyofEngineering,Bar-IlanUniversity,Israel. A.Nedi´ciswiththeSchoolofElectrical,Computer
andEnergyEngineering,ArizonaStateUniversity,USA.
1
4202
luJ
9
]YS.ssee[
1v14560.7042:viXrato retain the global optimal value in distributed optimization by leveraging the inter-agent trust values.
However, the existing results are limited in three ways: 1) the results are only applicable to undirected
graphs, 2) the algorithm only works for constrained optimization problems with a compact constraint set,
and 3) the algorithm requires decreasing step-sizes, resulting in slow convergence. On the other hand,
and for the case without malicious agents, it is possible to obtain a geometric convergence rate with fixed
step sizes over directed graphs using gradient tracking methods [24–26]. In these methods, agents store
an additional variable, called the gradient tracking variable, to estimate the global gradient and speed up
the convergence [27]. However, these algorithms work under the assumption that all the agents are fully
cooperative, and therefore, cannot handle malicious agents. Our aim in this work is to develop a more
general distributed optimization algorithm that achieves geometric convergence rate with a fixed step size
in the directed graph case with malicious agents.
In our previous work [28,29], we developed a learning protocol that enables agents to develop opinions
of trust about both their in and out neighbors leveraging trust values. In this work, we leverage this
learningprotocoltodeveloparesilientdistributedoptimizationalgorithm, referredtoasResilientProjected
Push-Pull (RP3). The RP3 algorithm uses gradient tracking to achieve geometric convergence over directed
graphs. IntheRP3,agentsusetrustopinionstoformatrustedneighborhoodandconsideronlytheagentsin
this neighborhood when performing their updates. Integrating the learning protocol with gradient tracking
presents two primary challenges. First, agents’ trust opinions improve over time, which initially allows
malicious agents opportunities to influence their neighbors. Moreover, at any time, legitimate agents do not
know if their opinions are perfectly accurate or not. Second, the gradient tracking method introduces new
attack surfaces as agents need to share gradient tracking variables with each other, which can be influenced
by the malicious agents arbitrarily. To limit the malicious influence until agents’ trust opinions improve, we
introducegrowingconstraintsetsthatagentsprojectboththeirdecisionandgradienttrackingvariablesonto.
We demonstrate that appropriately chosen constraint sets can restore the algorithm’s nominal performance,
stillachievinggeometricconvergencewhilecontainingtheinfluenceofmaliciousagents. Thesesetsalsoallow
us to extend our results to the optimization problems over unbounded constraint sets. Our contributions in
this paper are as follows
1. We introduce the Resilient Projected Push-Pull algorithm for constrained distributed optimization
problems over directed graphs with malicious agents. We show that the algorithm converges to the
nominal optimal solution almost surely and in the r-th mean for any r ≥1.
2. We demonstrate that, with sufficiently small step sizes and appropriately chosen constraint sets (as
characterized in this paper), the algorithm achieves a geometric convergence rate in expectation.
Finally, we apply our algorithm to average consensus and multi-robot target tracking problems in numerical
studies to validate our theoretical results.
2 Related Works
Achieving fast convergence in distributed optimization problem with fully cooperative agents is well studied
intheliterature. Thegradienttrackingmethodwasintroducedin[30–32]toachieveageometricconvergence
rate over unconstrained distributed optimization problems over undirected graphs. The algorithm was
extendedtodirectedgraphsin[32]byemployingcolumnstochasticmixingmatricesandin[33]byusingrow
stochastic mixing matrices. Both methods require distributed estimation of the non-one Perron vector of
the mixing matrix. The Push-Pull algorithm introduced in [24,25] utilizes both row and column stochastic
matricestoattaingeometricconvergenceindirectedgraphswithoutneedingtoestimatethenon-onePerron
vector. Further developments include extensions of row stochastic mixing matrix-based gradient tracking
methods to constrained optimization problems in [34–36]. Moreover, [37] extends the Push-Pull algorithm
to constrained optimization problems. Our method is closely related to the [37] algorithm, chosen due to
the sensitivity of methods in [32–36] to the initialization of the variables. Such dependency on initialization
is undesirable in the presence of malicious agents. Notably, none of these studies account for malicious
agents; they assume that all agents are fully cooperative, and share accurate information. Studies on noisy
information sharing in gradient tracking [38–43] have relaxed the assumption that the information sharing
between the agents is perfect. These works and works that consider stochastic gradient information [44–46]
2typically assume that the noise affecting the system is independent and unbiased. In contrast, our research
considers adversarial inputs that can be arbitrary and strategically chosen by adversaries. Additionally, we
explore the dynamics where agents accumulate and update trust values over time. This process introduces
correlated noise across the agents’ weights, making the previous analysis that assumes independent noise
inapplicable to our problem.
Theimpactofmaliciousagentsdiffersignificantlyfromunbiasedstatisticalnoise. Previousresultsin[13]
show that even a single malicious agent can force consensus based distributed optimization algorithms
to converge to an arbitrary value. To mitigate the impact of the malicious agents, a variety of resilient
distributedoptimizationalgorithmshavebeendeveloped[13,23,47–57]. Thesemethodsoftenutilizefiltering
basedonthevaluesreceivedfromotheragentstoachieveresilience[13,47,50–52,56]. Lateron,thesefiltering-
basedapproachesareextendedtomultidimensionalfunctionsin[48],andconstrainedoptimizationproblems
in [57]. The work in [49] introduces gradient tracking to these methods but still requires a decreasing step
size. Commonlimitationsoftheseapproachesincluderestrictionsonthenumberoftolerablemaliciousagents
and the network topology, and convergence typically only to the convex hull of the agents’ local minimizers,
rather than to a true optimal point. Works [50–52,57] demonstrate that exact convergence to the true
optima is possible with redundancy in the cost functions of legitimate agents. Techniques involving agents
estimating and cross-checking their neighbors’ gradients to perform detection and filtering are discussed
in [23,53]. However, these methods only consider specific attack types and do not guarantee convergence to
theoptimalpoint. Studies[54,55]proposetheuseoftrustedagentsthatareknowntoeveryonetoovercome
limitationsregardingthenumberoftolerablemaliciousagents. However,thesemethodsintroduceadditional
assumptions about connectivity between trusted and regular nodes, and guarantee only convergence to the
convexhullofthetrustedagents’localminimizers. Inourwork,weguaranteegeometricallyfastconvergence
to a true optimal point without any limitation on the number of tolerable malicious agents. Moreover, our
algorithm works for both constrained and unconstrained optimization problems, and in directed graphs. In
contrast to some of the existing work, we do not require a set of pre-existing trusted agents or redundancy
in the cost functions.
In our approach, we use inter-agent trust values that can be derived from physical properties of the
cyberphysical systems as explored in previous research [17–19,58–61]. A comprehensive survey of these
inter-agent trust values is available in [20]. Examples of such trust values include observations of other
robots [59,61] or vehicles [60], and using wireless finger profiles from incoming transmissions [17,19,58].
Furthermore, the study in [14] demonstrated that since these approaches leverage physical information
independentofthedatatransmittedbytheagentstoassesstrustworthiness,itispossibletoachieveresilience
even when a majority of the agents in the network are malicious in undirected graphs. In these works,
agents can develop more accurate trust estimations over time by aggregating more observations about their
neighbors. The learning protocol introduced in [28,29] further enhances this by enabling agents to learn
aboutthetrustworthinessoftheentirenetworkthroughpropagatedtrustopinions. Inourwork,weleverage
this protocol to enable agents to develop trust estimations about their in and out neighbors in directed
graphs.
The paper [15] has developed a resilient distributed optimization utilizing inter-agent trust values for
constrained problems with compact and convex constraint sets in undirected graphs. Our method diverges
from[15]byemployinggradienttrackingtoachieveageometricconvergencerateinexpectation,ratherthan
theslowerratefromthedecreasingstepsizeusedin[15]. Moreover,asopposedto[15]wheremaliciousagents
can only influence the decision variables, malicious agents can manipulate both the decision variables and
gradient tracking variables in our setup. For example, [62] proposes an attack model where malicious agents
can manipulate the gradient tracking variable to achieve convergence to an arbitrary value. This creates
additionalchallengesforachievingresilienceinoursetupandpreventusfromrelyingontheanalysisof[15].
Finally, unlike [15], our algorithm works for directed graphs and for constrained optimization problems
with closed and convex constraint sets. The closedness assumption is less restrictive than the compactness
assumption in [15] and makes our method applicable to unconstrained optimization problems. To achieve
this,weintroduceastrategyofprojectingdecisionandgradienttrackingvariablesontoanincreasingsequence
ofsets,mitigatingtheimpactofmaliciousagentsuntilmoreprecisetrustestimationsaredeveloped. Notably,
to our knowledge, this is the first analysis that considers projection of the gradient tracking variables.
33 Problem Formulation
3.1 Notation
We use ∥·∥ to denote the Euclidean norm. We define the u-weighted norm of x ∈ Rd×···×Rd (n copies
(cid:113)
of Rd) as ∥x∥ = (cid:80)n u ∥x ∥2 where x ∈ Rd for any vector u ∈ Rn with u > 0 ∀i. We use E[Z] and
u i=1 i i i i
E[Z|A] to denote the expectation of a random variable Z and the conditional expectation of Z conditioning
on the event A, respectively. When A is empty, we define1 E[Z|A] as 0. We will use the following definition
of the growth of the set sequences in this work.
Definition 1 (Growth of the set sequence {X }). For a non-empty set X, we define its size as ∥X∥ ≜
k
sup{∥x∥ : x ∈ X}. We let ∥X∥ = ∞ if X is unbounded. Moreover, when discussing the growth of a set
sequence {X }, we are specifically referring to the growth of the sequence {∥X ∥}.
k k
Finally, we define the projection operator as follows.
Definition 2 (Projection onto X). Let X ⊆ Rd be nonempty, closed, and convex. Then, the projection
operator Π (·):Rd →Rd is defined as follows
X
Π (x)=argmin∥x−z∥.
X
z∈X
3.2 Problem Setup
We consider a multi-agent system with a directed communication graph G = (V,E), where V with |V| = n
denotesthesetofagentsandE denotesthedirectedcommunicationlinks. Ifagenticansendinformationto
agentj, thenthereisanedge(i,j)∈E andwesaythatj isanout-neighborofiandiisanin-neighborofj.
We assume that every agent has a self-loop, i.e., (i,i)∈E for all i∈V. We are interested in the case where
an unknown subset of agents in the system are malicious We denote the set of malicious agents by M⊂V.
Malicious agents are non-cooperative and can act arbitrarily. The set of cooperative agents is denoted by L
andreferredtoaslegitimateagents. WeassumethatL∩M=∅andL∪M=V,i.e.,anagentinthesystem
is either legitimate or malicious. We denote the number of malicious agents by n ≜|M| and the number
M
of legitimate agents by n ≜|L|. The sets M and L are defined for analytical purposes, and the legitimate
L
agents do not know which agents in the system are legitimate or malicious. We say that malicious agents
are untrustworthy and legitimate agents are trustworthy.
Each legitimate agent i has a private local cost function, denoted by f (x), that is only known to agent
i
i. Thelegitimateagents’goalistosolvethefollowingminimizationproblem, withoutrevealingtheirprivate
cost functions while exchanging information over the links E of G,
1 (cid:88)
min f(x) , where f(x)≜ f (x). (1)
x∈X⊆Rd n L i
i∈L
The following assumption on the agents’ cost functions is used.
Assumption 1. For all legitimate agents i ∈ L, f (x) is µ-strongly convex, i.e, for some µ > 0, we have
i
⟨∇f (x)−∇f (y),x−y⟩≥µ∥x−y∥2, for all x,y ∈Rd. Moreover, for all legitimate agents i∈L, ∇f (x) is
i i i
L-Lipschitz continuous, i.e, for some L>0, ∥∇f (x)−∇f (y)∥≤L∥x−y∥, for all x,y ∈Rd.
i i
We consider two different assumptions on the constraint set X.
Assumption 2 (Compact and Convex Constraint Set). The constraint set X ⊆Rd is nonempty, compact,
and convex. Thus, there exist a scalar value B >0 such that
∥x∥≤B, ∀x∈X. (2)
1Thisnotationisneededforthecasesweusethetotallawofprobabilityinouranalysiswiththeemptyevents. Forthesake
ofsimplicityofpresentation,suchanexpectationwillbeusedintheformE[Z|A]Pr(A)wherePr(A)equals0.
4Having a compact set imposes a bound on the impact that malicious agents can have. For example,
malicious agents cannot send values with ∥x∥ > B when Assumption 2 is known to be true since sending
values with a norm B would reveal their identities. To make our method applicable to a broader set of
problems, we also consider a less restrictive assumption.
Assumption 3 (Closed and Convex Constraint Set). The constraint set X ⊆Rd is nonempty, closed, and
convex.
Assumption 3 on the constraint set require different treatment than Assumption 2 in our analysis since
there is no natural bound on the values malicious agents can send. In Section 6, we will introduce a method
toboundtheeffectofmaliciousagentswhenAssumption3holdstrue. Thiswaywegeneralizeourresultsfor
more general constrained optimization problems, including unconstrained optimization problems. Note that
under Assumption 1 and either Assumption 2 or Assumption 3, the problem in (1) has a unique solution,
denoted by x∗.
Inthiswork, weareinterestedintheproblemswhereagentsreceivestochasticobservationsoftrustfrom
other agents that send information to them. In practice, this information can be obtained in various ways,
includingsensorsonboardtheagents,wirelessfingerprintsofthecommunicationsignals,andagentbehaviors
(see [20] for a survey of such methods). In our previous work [28,29], we presented a learning protocol that
enables agents to develop opinions about the trustworthiness of the other agents in the system in directed
graphs. Next, we provide the necessary definitions from these works that will be referenced throughout.
3.3 Trust Opinions
Following previous works [14,15,17,20,28,29] that use and develop the stochastic observation of trust, we
give the following definition:
Definition 3 (Stochastic Observation of Trust α ). If agent j ∈V sends information to a legitimate agent
ij
i∈L at time k, meaning that we have j ∈Nin, agent i receives a stochastic observation of trust α [k] which
i ij
is the likelihood that agent j is trustworthy. We assume that α [k]∈[0,1] for all k.
ij
In our previous work [28,29], we developed a learning protocol where agents can develop opinions about
the trustworthiness of all the agents in the system using the trust observations and the opinions of their
trusted neighbors. Moreover, we showed that these opinions converge to the true trustworthiness of agents
over time. In this work, we employ the same protocol.
Definition 4 (Opinion of Trust). Let o [k] ∈ [0,1] denote agent i’s opinion of its trust about agent j at
ij
time k. A legitimate agent i∈L trusts agent j at time k if o [k]≥1/2 and does not trust agent j otherwise.
ij
Moreover, we stack these opinions to define a vector o [k] that stores agent i opinions about all the other
i
agents.
Wedefinetheaggregatetrustvalueofagenti∈Laboutagentj ∈Ninattimekasβ [k]≜(cid:80)k−1(α [t]−
i ij t=0 ij
0.5) for k ≥1 and define β [0]=0. Also, we define β [k]=1 for all k. During every communication round,
ij ij
agents share their opinion vectors with each other. A legitimate agent i determines its opinion about an
in-neighbor j ∈ Nin as o [k] = 1 , where 1 is the indicator function. Using this, we define the
i ij {βij[k]≥0}
trusted in-neighborhood of agent i at time k as Nin[k]={j ∈Nin |o (t)≥1/2}. For an arbitrary agent q
i i ij
that is not an in-neighbor, agent i uses the opinions of its trusted in-neighbors to update its opinion as
o [k]=
(cid:88) o jq[k−1]
.
iq |Nin[k]|
j∈Nin[k] i
i
Using these opinions, we also define the trusted out-neighborhood of an agent i at time k as Nout[k]={j ∈
i
Nout |o (t)≥1/2}.
i ij
We make the following assumptions about the trust observations.
Assumption 4 (Trust Observations). Assume that
5i) [Difference of trust observations in expectation]. The expectation of the variables α [k] are constant
ij
for malicious transmissions and legitimate transmissions, respectively, i.e., for some scalars E , E
M L
with E <0 and E >0, E =E[α [k]]−1/2 for all i∈L, j ∈Nin∩M, and E =E[α [k]]−1/2
M L M ij i L ij
for all i∈L, j ∈Nin∩L.
i
ii) [Independence of trust observations.] The observations α [k] are independent for all k. Moreover, for
ij
any i∈L and j ∈N iin, the observation sequence {α ij[k]} k∈N is identically distributed.
We note that the homogeneity of the trust variables (Assumption 4(i)) and identically distributed obser-
vation sequence {α ij[k]} k∈N assumptions (Assumption 4(ii)) are introduced for the sake of simplicity of the
presentation. Theseassumptionscanberelaxedtocovercaseswherethetrustobservationsareheterogeneous
over time and for different pairs of agents (i,j).
Finally, we make the following assumptions on the connectivity of the communication network.
Assumption 5 (Connectivity of Network).
i) [Sufficiently connected graph]. The subgraph G induced by the legitimate agents is strongly connected.
L
ii) [Observation of malicious agents]. For any malicious agent j ∈M, there exists some legitimate agent
i∈L that observes j, i.e., j ∈Nin for some i∈L.
i
The sufficient connectivity assumption (Assumption 5(i)) is common in the literature of resilient dis-
tributed optimization. The observation of malicious agents assumption (Assumption 5(ii)) is required for
learning the trustworthy out-neighbors of the legitimate agents as noted in [28,29].
3.4 Problem Definition
Our goal in this work is to develop a distributed optimization algorithm to solve the problem given in (1) in
the presence of malicious agents. More specifically, we want to solve the following problems:
Problem 1. Let x [k] denote the estimate of agent i∈L for the solution to the optimization problem given
i
in (1). We aim to develop a distributed optimization algorithm such that the iterates x [t] generated by the
i
algorithm converge to the optimal point x∗ for all legitimate agents i∈L almost surely.
Problem 2. We want to characterize the convergence rate of the expected error ∥x [k]−x∗∥2 for all legiti-
i
mate agents i∈L.
4 Resilient Projected Push-Pull (RP3) Algorithm
4.1 Background
As we are dealing with solving problem (1) over directed graphs, we base our algorithm on the Projected
Push-Pull algorithm [37] developed for constrained distributed optimization problems over time-varying
graphs. In the Projected Push-Pull algorithm, agents store two decision variables x [k] and z [k] and a
i i
gradient tracking variable y [k]. Agents initialize x [0] = z [0] ∈ X arbitrarily, and y [0] = ∇f (x [0]).
i i i i i i
Agentssharetheirvariablesz [k]andthescaledgradienttrackingvariablesC y [k]withtheirout-neighbors
i ij i
at every communication round k and do the following updates:
n
(cid:88)
x [k+1]= R z [k], (3a)
i ij j
j=1
n
(cid:88)
y [k+1]= C y [k]+∇f (x [k+1])−∇f (x [k]), (3b)
i ij j i i i i
j=1
z [k+1]=(1−λ)x [k+1]+λΠ (x [k+1]−ηy [k+1]), (3c)
i i X i i
where η and λ are the step-sizes for the algorithm. Agents choose the weights R such that R > 0 if
ij ij
and only if j ∈ Nin and (cid:80) R = 1. Similarly, C > 0 if and only if j ∈ Nout and (cid:80) C = 1.
i j∈Nin ij ji i j∈Nout ji
i i
6These choices of weights results in a row stochastic weight matrix R with ijth element R and column
ij
stochastic matrix C with ijth element C . The Projected Push-Pull satisfies
(cid:80)n
y
[k]=(cid:80)n
∇f (x [k]),
ij i=1 i i=1 i i
at each time step k, which is called the gradient tracking property. This property depends on the correct
initialization of y variables and it is crucial for the convergence of the algorithm in all gradient tracking
i
methods [24,25,27].
The Projected Push-Pull algorithm is especially suitable for our problem for the following reasons 1) it
is compatible with directed communication graphs, 2) it achieves a geometric convergence rate, and 3) the
z [k] variables shared among the agents stay within the feasible region X, limiting the effect of malicious
i
agents on the system. However, the algorithm is not designed for handling malicious agents and we will
describe the necessary modifications in the next section. Still, for completeness we discuss the analysis and
convergence of this algorithm without the malicious agents in this section.
Denote the left eigenvector of the row stochastic matrix R corresponding to the eigenvalue 1 by ϕ,
meaning that we have ϕ⊺R = ϕ. Similarly, denote the right eigenvector of the column stochastic matrix C
corresponding to the eigenvalue 1 by π, meaning that we have Cπ = π. Here, both ϕ and π are stochastic
vectors with positive entries. In the analysis of this algorithm, there are three different error terms that we
keeptrackof: 1)optimalityerror,2)theconsensuserror,and3)thegradienttrackingerror. Wedefinethese
respective error terms mathematically as follows:
Optimality error:
(cid:118)
(cid:117) n
∥x[k]−x∗∥
ϕ
≜(cid:117) (cid:116)(cid:88) ϕ i∥x i[k]−x∗∥2, (4)
i=1
where x[k]=(x [k],...,x [k]), x∗ =(x∗,...,x∗).
1 n
Consensus error:
(cid:118)
(cid:117) n n
D(x[k],ϕ)≜(cid:117) (cid:116)(cid:88)(cid:88) ϕ iϕ j∥x i[k]−x j[k]∥2. (5)
j=1i=1
Gradient tracking error:
(cid:118)
S(y[k],π)≜(cid:117) (cid:117) (cid:116)(cid:88)n
π
(cid:13) (cid:13) (cid:13)y i[k] −(cid:88)n
y
[k](cid:13) (cid:13) (cid:13)2
, (6)
i(cid:13) π l (cid:13)
(cid:13) i (cid:13)
i=1 l=1
where y[k]=(y [k],...,y [k]).
1 n
Convergence of the Projected Push-Pull is characterized using several parameters that depend on the
properties of the communication graph G, and the matrices R and C. We define additional notation to
introducetheseparameters. Letmin(v)andmax(v)denotetheminimumandmaximumvaluesofavectorv,
respectively. Also,letminM+denotetheminimumnon-zerovalueofanon-negativematrixM. Werepresent
the diameter and the maximum edge utility of graph G with D(G) and K(G), respectively (see [63, Lemma
6.1] for a detailed definition). We define
(cid:115)
min(ϕ)(minR+)2
σ ≜ 1− ∈(0,1),
max2(ϕ)D(G)K(G)
(cid:115)
min2(π)(minC+)2
τ ≜ 1− ∈(0,1),
max3(π D(G)K(G)
)
(cid:115) (cid:115)
1 √ 1 √
r ≜ + n, and φ≜ + n.
min(π) min(ϕ)
7Here, σ and τ are the contraction coefficients we get from the matrices R and C, respectively. Typical
analyses of gradient tracking methods involve upper bounding the error terms at the k+1th time step in
terms of the errors at the kth time step. The relationship between the error terms depends on the system
parameters we defined, as well as the strong convexity and L-smoothness of the cost functions and the
step sizes involved in the algorithm. By expressing these relationships in a system of inequalities, it can be
demonstrated that the algorithm converges with carefully chosen step sizes. For more details on this type
of analysis, see [25,37,38]. We state the following result characterizing the convergence of the Projected
Push-Pull algorithm.
Theorem 1 (Theorem 1, [37]). Define the error vector e[k] = (∥x[k]−x∗∥ ,D(x[k],ϕ),S(y[k],π))⊺. Let
ϕ
Assumption 1, Assumption 3, and Assumption 5 hold. Let 0<η < 1 and
nL
(cid:26) (cid:27)
1−σ 1−τ ηnmin(π)µ(1−σ)(1−τ)
λ<min √ , , ,
2φ n rφ K
where
√
K =(1+ηnmin(π)µ)φ·[2 n(1−τ)+r(1−σ)+2r(1+σ)].
Then, we have
e[k+1]≤M(η,λ)e[k], (7)
where the inequality is elementwise and M(η,λ)∈R3×3 is equal to
√
 1−ηλnmin(π)µ λφ n λL−1 
√
 2λ σ+2λ nφ √ 2λL−1  (8)
2λLrφ Lrφ(1+σ+λφ n) τ +λrφ
The spectral radius of M(η,λ) is less than 1, i.e., ρ(M(η,λ)) < 1, where ρ(·) denotes the spectral radius of
a matrix. Moreover, the errors ∥x[k]−x∗∥ ,D(x[k],ϕ), and S(y[k],π) all converge to 0 geometrically fast
ϕ
with rate ρ(M(η,λ)).
We note that since Theorem 1 requires Assumption 3 (i.e., closed and convex constraint set) and not
the more restrictive Assumption 2 which further requires that the constraint set is bounded. Nevertheless,
since compactness introduces a natural bound on the impact that malicious agents can have on the decision
variables,itiseasiertoanalyzeandwewillfirstconsiderthecaseAssumption2holdstrue. Then,inSection
6 we will extend our results to the case where Assumption 3 holds true.
Note that the Projected Push-Pull algorithm only converges when there are no malicious agents in the
system. This is because the analysis of the algorithm in [37] is based on several key assumptions: all the
agents adhere to the update rule in (3), the mixing matrix R is row stochastic, C is column stochastic, and
the gradient tracking variables y are correctly initialized. However, these assumption are violated in the
presence of malicious agents. Such agents can transmit arbitrary data to their neighbors, thereby violating
therowandcolumnstochasticityofthematrices. Moreover,eveniflegitimateagentseventuallyidentifyand
exclude all malicious agents, they would need to restart the algorithm to re-establish the gradient tracking
property. Furthermore, as we will elaborate later on, agents do not know when their trust estimations are
accurate, making it impossible to determine an appropriate restart time to guarantee convergence. In the
next section, we present the resilient version of this algorithm that resolves these issues.
4.2 Algorithm
In this section, we present the Resilient Projected Push-Pull (RP3) Algorithm given in Algorithm 1. The
legitimate agents keep track of two decision variables x [k] and z [k], and a gradient tracking variable s [k].
i i i
Agents initialize x [0] = z [0] ∈ X arbitrarily and choose s [0] = 0. Agents share their variables z [k] and
i i i i
C [k]s [k] with their trusted out-neighbors at every communication round k and do the following updates,
ji i
8Algorithm 1 Resilient Projected Push-Pull (RP3)
Input: Optimization parameters η,λ, chosen according to Theorem 1.
1: Each legitimate agent i does the following:
2: Initialize x i[0]=z i[0]∈X arbitrarily, and set s i[0]=0.
3: while k =0,1,... do
4: Update trust opinions o ij(t) using the learning protocol and stochastic trust observations as shown in
Definition 4.
5: Determine the trusted in and out neighborhoods using the rule N iin[k]={j ∈N iin |o ij(t)≥1/2} and
Nout[k]={j ∈Nout |o (t)≥1/2}.
i i ij
6: Determine coefficients C ji[k] and R ij[k] for in and out neighbors based on N iin[k] and N iout[k].
7: Send z i[k], C ji[k]y i[k] to out-neighbors j ∈N iout.
8: Receive z j[k], C ij[k]y j[k] from in-neighbors j ∈ N iin.
9: Perform the gradient tracking update using (9a):
 
n
(cid:88)
s i[k+1]←Π Sk C ij[k]s j[k]+∇f i(x i[k]).
j=1
10: Perform the consensus update using (9b):
n
(cid:88)
x [k+1]← R [k]z [k].
i ij j
j=1
11: Perform the lazy update using (9c):
z [k+1]←(1−λ)x [k+1]+λΠ (x [k+1]−ηs [k+1]).
i i X i i
12: end while
9for a predefined sequence of sets S ,k =0,1,... which we will strategically choose:
k
 
n
(cid:88)
s i[k+1]=Π Sk C ij[k]s j[k]+∇f i(x i[k]), (9a)
j=1
n
(cid:88)
x [k+1]= R [k]z [k], (9b)
i ij j
j=1
z [k+1]=(1−λ)x [k+1]+λΠ (x [k+1]−η(s [k+1]−s [k])). (9c)
i i X i i i
Here,η >0andλ∈(0,1]aretwodifferentstepsizes. Recallthatwedefinethetrustedin-neighborhoodand
trusted out-neighborhood as Nin[k] = {j ∈ Nin | o (t) ≥ 1/2} and Nout[k] = {j ∈ Nout | o (t) ≥ 1/2},
i i ij i i ij
respectively. Legitimate agents choose the weights R [k] such that R [k] > 0 if and only if j ∈ Nin[k]
ij ij i
and (cid:80) R [k] = 1. Similarly, C [k] > 0 if and only if j ∈ Nout[k] and (cid:80) C [k] = 1. The
j∈Nin[k] ij ji i j∈Nout[k] ji
i i
sequence of sets S is not defined yet but we will elaborate on the choice of this set sequence later on. A
k
malicious agent m ∈ M can send any z [k] ∈ X at all time k. However, since sending a value outside the
m
set would reveal their maliciousness immediately, we restrict the z values malicious agents send to the set
X. Similarly, a malicious agent m can only send s [k+1] values that lie within the set S at time k.
m k
The proposed RP3 algorithm has three important modifications over the Projected Push-Pull algorithm
(3), in the remaining of this section we will discuss these algorithmic choices.
4.2.1 Trust-based weights
Agents assign positive weights to their trusted neighbors only. Using our results from [28,29], we will
show that these weights will eventually stabilize and agents will assign positive weights to their legitimate
neighbors only.
4.2.2 Preserving the gradient tracking property in the presence of malicious agents
AswediscussinSection4.1,theconvergenceoftheProjectedPush-Pullalgorithmdependsontheinitializa-
tion of the gradient tracking variables y . If these variables are not initialized such that y [0] ̸= ∇f (x [0]),
i i i i
then the gradient tracking property no longer holds. Consider the scenario where the malicious agents are
removed from the system eventually, but they can affect the system for some time. In this case, there
is no guarantee that the gradient tracking property will hold with the update rule (3b). Therefore, we
replace the gradient tracking update rule (3b) in Projected Push-Pull with (9a). This change is adapted
from the Robust Push-Pull algorithm for unconstrained problems with noisy communication links [64]. Let
us assume that the legitimate agents have some arbitrary s [k] ∈ Rd. In the next communication step, if
i
there are no malicious agents in the system and the legitimate agents follow the update rule (9a), we have
(cid:80) (cid:80) (cid:80)
s [k+1]− s [k]= ∇f (x[k]),whichmeansthatthesystemwillrestorethegradienttracking
i∈L i i∈L i i∈L i
property.
4.2.3 Projecting the gradient tracking variables onto the set S
k
An important design consideration in our problem is limiting the effect of the malicious agents until the
legitimate agents cut them off from the system. The effect of the malicious agents on the z variables is
i
limited since all z [k]∈X for all i∈V and for all k, when X is compact. However, that is not the case for
i
the gradient tracking variables s [k]. Thus, we employ the projection onto the set S at every time k. The
i k
main concern with this change is to be able to preserve the gradient tracking property. In Section 5.2 we
show that by choosing a growing sequence of {S } with an appropriate growth rate, the gradient tracking
k
property will be restored.
5 Analysis
In this section, we introduce the theoretical foundations and convergence results of the RP3 algorithm. Our
analysishingesontheselectionofappropriatemixingweightsandasetsequence{S },whichtogetherensure
k
10thattheRP3methodeventuallybecomesequivalenttotheProjectedPush-Pullalgorithmwithoutmalicious
agents initialized at some random point. This set sequence is critical as it limits the influence of malicious
agents until agents’ opinions of trust become reliable. Our primary challenge in designing the algorithm is
that the agents cannot definitively know when their trust opinions are accurate, so they cannot simply wait
until their trust opinions become accurate to start the algorithm at that time. Nonetheless, we will prove
the existence of such time and use this concept in our analysis. We begin by formally defining the nominal
behaviour of the RP3 method when it becomes equivalent to the Projected Push-Pull algorithm.
Definition 5 (ThenominalbehavioroftheRP3). Assume that there exist a time k′ such that for all k ≥k′,
R [k] = R¯ where R¯ > 0 only if j ∈ Nin∩L and (cid:80) R¯ = 1. Similarly, for all k ≥ k′, we have
ij ij ij i j∈Nin∩L ij
C [k]=C¯ where C¯ >0 only if j ∈Nout∩L and (cid:80) i C¯ =1. This corresponds to the ideal case
ij ij ji i j∈Nout∩L ji
i
where legitimate agents assign positive weights to their legitimate neighbors only and malicious agents are
excluded from the system. Also, assume that for all k ≥k′, we have
 
n n
Π Sk(cid:88) C¯ ijs j[k]+∇f i(x i[k])=(cid:88) C¯ ijs j[k]+∇f i(x i[k]),
j=1 j=1
i.e., the projector operator onto the set S becomes the identity operator. We call the behavior of Algorithm 1
k
after such time k′ the nominal behavior of the algorithm. In this case, Algorithm 1 becomes equivalent to the
Projected Push-Pull algorithm given in (3) if we define y [k]=s [k]−s [k−1].
i i i
Our first goal in the analysis is to show that Algorithm 1 reaches nominal behavior and converges from
there onwards. Then, we will show that the algorithm reaches this behavior quickly while the effects of the
malicious agents until reaching this behavior is bounded.
5.1 Preliminary Results: Learning the Trustworthiness of the Agents
In this part, we present the following results from [28,29] that will be used in our analysis.
Lemma 1 ( [28], Corollary 1). Let Assumption 4 and Assumption 5 hold. Then, all legitimate agents i∈L
can learn the trustworthiness of all agents in the network correctly almost surely. That is, there exists a
finite random time T such that for all k ≥T and for all q ∈V, o (t)≥1/2 if q ∈L and o (t)<1/2
max max iq iq
if q ∈M almost surely.
WenotethatthetimeT isstochastic,butfinitealmostsurely. Thefollowingcorollaryisaconsequence
max
of the choice of the weight matrices and Lemma 1.
Corollary 1. Let Assumption 4 and Assumption 5 hold. Then, we have R[k] = R¯ for all k ≥ T with
max
weights R¯ such that R¯ >0 if and only if j ∈Nin∩L and (cid:80) R¯ =1. Similarly, for all k ≥T ,
ij ij i j∈Nin∩L ij max
we have C[k]=C¯ with weights C¯ >0 if and only if j ∈Nout∩Li and (cid:80) C¯ =1.
ji i j∈Nout∩L ji
i
Corollary 1 shows that after reaching the time T , legitimate agents will assign weights to their legiti-
max
mateneighborsonly, whichisnecessaryforreachingthenominalbehavior. Itisnotknownwhenthesystem
will reach this time since T is stochastic; its probability is characterized in [29]. First, we let N be
max L
the total number of legitimate in-neighbors in the system, i.e., N ≜(cid:80) |Nin∩L|. Similarly, we let N
L i∈L i M
be the total number of malicious in-neighbors in the system, i.e., N ≜ (cid:80) |Nin ∩M|. The following
M i∈M i
proposition characterizes some probabilities related to T .
max
Proposition 1 (Proposition 1, [29]). Define
p (k)≜N exp(−2kE2)+N exp(−2kE2 ), (10)
c L L L M
where E ≜E[α (t)]−1/2 for i∈L and j ∈L and E ≜E[α (t)]−1/2 for i∈L and j ∈M as defined
L ij M ij
in Assumption 4. Also, define
exp(−2kE2) exp(−2kE2 )
p (k)≜N L +N M . (11)
e L1−exp(−2E2) M1−exp(−2E2 )
L M
11Let d denote the maximum in-degree of any legitimate node in graph G, i.e., d ≜ max |Nin|. Let
max max i∈L i
D(G) denote the diameter of the graph G. Define ∆ ≜ h·D(G)+1, where h = 1/log 1 . Then,
2 1−(1/dmax)D(G)
we have for all k ≥0,
Pr(T =k)≤min{p (k−∆),1}, and (12)
max c
Pr(T >k−1)≤min{p (k−∆),1}. (13)
max e
Proposition 1 will be particularly useful when deriving the expected convergence rate of the algorithm.
Next, we will examine how the choices of the set sequence S affect the algorithm.
k
5.2 Bounding the s-variables
In the preceding section, we showed that the system will reach a time T after which the malicious agents
max
willbeeffectivelyexcludedfromthedynamics. Therefore,ournextgoalistoshowthattheeffectofincluding
malicious agents or excluding legitimate agents before reaching this time is limited. In this section, we focus
on bounding the effect of the malicious agents with a strategic choice of the set sequence {S }. However,
k
projecting the s [k] variables onto the set sequence {S } breaks the gradient tracking property. Therefore,
i k
we should choose a set sequence {S } that grows fast enough such that the projections onto this set do not
k
change s [k], i.e., the projection becomes the identity operator after some time so as to restore the gradient
i
tracking property. We start by estimating the growth of s [k] variables when the system is in the nominal
i
behaviour,i.e.,whenthereisnoprojectionontothesetS andnomaliciousagentsareinthesystem. Then,
k
we show that the rate of the growth of of s [k] is bounded.
i
5.2.1 s-variables growth
We start by expressing the update rule (9a) in the nominal case
n
s [k+1]=(cid:88) C¯ s [k]+∇f (x [k]). (14)
i ij j i i
j=1
Generally, the s [k]-values might not be bounded even when there are no malicious agents. In fact, they are
i
not bounded if ∇f(x∗) ̸= 0. To see why, let us evaluate the following identity which follows from (14) and
the column stochasticity of C¯:
1Ts[k+1]=1Ts[k]+1T∇F(x[k]),
where s[k] = (s [k],...,s [k]) and ∇F(x[k]) = (∇f (x [k]),...,∇f (x [k]))T. If all agents converge to x∗,
1 n 1 1 n n
then we have
1Ts[k+1]=1Ts[k]+1T∇F(x∗),
whose norm may grow since 1T∇F(x∗) is added to the sum. Next, we show that the growth is bounded.
5.2.2 Finding a growing linear bound on s-values
Weshowthatthereisa(linearly-growing)upperboundons [k]inthenominalcasewhentheconstraintset
i
X is bounded. This bound will be optimal as we have shown that the s [k] variables grow at a linear rate
i
when agents converge to x∗ and ∇f(x∗)̸=0.
The key to obtaining a bound for s [k+1] is using the fact that the gradients of f are bounded. We
i i
first state the following corollary of the L-smoothness of f and compactness of X.
Corollary 2 (Boundedness of Gradients). Let Assumption 1 and Assumption 2 hold. Then, there exists
G≥0 such that ∥∇f (x)∥≤G for all x∈X and i∈L.
i
12Proof. Because ∇f are continuous and X is compact, we can apply the extreme value theorem to upper
i
bound ∥∇f (x)∥ over X. Then, we choose G as the largest bound among all agents i∈L.
i
The following result establishes the nominal growth rate of the s [k] variables.
i
Proposition 2. Assume the RP3 algorithm has the nominal behavior since the beginning, i.e., legitimate
agents assign non-zero weights to their legitimate neighbors only and there is no projection of the s [k]
i
variables. Let s [0]=0 for all i∈L. Then, ∥s [k]∥≤kn G for all k ≥0 and i∈L.
i i L
Proof. We will first establish a recursion and then use the induction. For a legitimate agent i∈L, we have
(cid:13) (cid:13)
∥s
i[k+1]∥=(cid:13)
(cid:13)
(cid:13)(cid:88)nL
C¯ ijs j[k]+∇f i(x
i[k])(cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)j=1 (cid:13)
≤(cid:88)nL
C¯ ∥s [k]∥+∥∇f (x [k])∥
ij j i i
j=1
≤(cid:88)nL
C¯ ∥s [k]∥+G.
ij j
j=1
Then, by summing over all legitimate agents, we get
(cid:88)nL
∥s [k+1]∥≤n
G+(cid:88)nL (cid:88)nL
C¯ ∥s [k]∥
i L ij j
i=1 i=1j=1
=n
G+(cid:88)nL (cid:32) (cid:88)nL
C¯
(cid:33)
∥s [k]∥
L ij j
j=1 i=1
(a)
(cid:88)nL
= n G+ ∥s [k]∥,
L j
j=1
where (a) follows from the fact that C¯ is a column stochastic matrix. Since (cid:80)nL ∥s [0]∥ = 0 due to
i=1 i
the initialization, by induction and the definition of s [k] in (14) for the nominal case, it follows that
i
(cid:80)nL
∥s [k]∥≤kn G. Thus, we have
i=1 i L
(cid:88)nL
max∥s [k]∥≤ ∥s [k]∥≤kn G,
i i L
i∈L
i=1
implying that all the norms ∥s [k]∥ are bounded by kn G in the nominal case.
i L
Thisresultsaysthatintheabsenceofmaliciousinfluence,thes [k]variableswouldgrowatmostlinearly
i
withk. Onemightassumethatlegitimateagentscandetectifanagentismaliciousifitsendsvaluesgreater
than kn G. However, this growth rate is derived under the assumption that all the agents are legitimate at
L
alltimesteps. Inthepresenceofmaliciousagents,welosetherecursionweusedintheproofofProposition2.
Therefore, we follow a different approach. Since the legitimate agents’ s [k] values cannot grow faster than
i
linear in the nominal case, we can project the s [k] values onto a set that expands at a rate surpassing the
i
nominal growth. After some point, the projection will lose its effect and the algorithm will behave normally,
restoring the gradient tracking property. Next, we formalize this intuition.
5.2.3 Choosing the set sequence {S }
k
In Proposition 2, we considered scenarios where both legitimate and malicious agents are always classified
accurately. We now return to the more general case of the RP3 algorithm, where the presence of malicious
agents influences the system, and agents adjust their mixing weights based on trust opinions. The following
result shows that the s [k] values will stay in an invariant set with the correct choice of {S }, even when we
i k
have malicious agents in the system.
13Proposition 3. Let θ > n G and S = {s ∈ Rd | ∥s∥ ≤ θk} for all k ≥ 0. For all legitimate agents i ∈ L
L k
and all k ≥0, let
n
(cid:88)
d [k+1]= C [k]s [k]+∇f (x [k]),
i ij j i i
j=1
s [k+1]=Π [d [k+1]].
i Sk i
Define T ≜T n θ−G . Then, for all k >T and for all i∈L, we have s [k]=d [k].
nom max Lθ−nLG nom i i
Proof. Notice that we have ∥s [k]∥≤∥d [k]∥ for all k ≥0. Then, for k >T , we can write
i i max
(cid:88) (cid:88)
∥d [k]∥≤ ∥d [k]∥≤ ∥s [k]∥
i i i
i∈L i∈L
(cid:88)
≤n G+ ∥s [k−1]∥
L i
i∈L
(cid:88)
≤n G+ ∥d [k−1]∥
L i
i∈L
(cid:88)
≤···≤n G(k−T )+ ∥s [T ]∥
L max i max
i∈L
≤n G(k−T )+n θT ,
L max L max
where in the last step we used the fact that ∥s [T ]∥ ≤ θT due to the projection. When we have
i max max
k >n θ−G T , we get n G(k−T )+n θT <θk. Therefore, for all k >T =T n θ−G ,
Lθ−nLG max L max L max nom max Lθ−nLG
we have that
∥d [k]∥≤nθT +n G(k−T )<θk.
i max L max
Hence, d [k]∈S and s [k]=Π [d [k]]=d [k].
i k i Sk i i
Remark 1. Proposition 3 states that the projection operator will be the identity operator after k >T ≜
nom
T n θ−G . Hence, after this point, the protocol will have the gradient tracking property.
max Lθ−nLG
Corollary 3. Let p (t) and p (t) as defined in (10) and (11), respectively. Then, we have
c e
(cid:26) (cid:18) (cid:19) (cid:27)
θ−n G
Pr(T =t)≤min p L t−∆ ,1 , (15)
nom c n (θ−G)
L
and
(cid:26) (cid:18) (cid:19) (cid:27)
θ−n G
Pr(T >t−1)≤min p L t−∆ ,1 . (16)
nom e n (θ−G)
L
Proof. The result follows directly from T =T n θ−G and Proposition 1.
nom max Lθ−nLG
NoticethatthelegitimateagentsdonotneedtoknowthevalueofT fortheseresultstohold. However,
max
we implicitly assumed that they know n and G or an upper bound on them while choosing θ. However, as
L
seen from the proof of Proposition 3, we only need to ensure a faster growth than the nominal growth of the
s-variables. Therefore, this assumption can be removed by choosing a set sequence that grows faster than
linear.
Remark 2. Let g(k) denote the maximum norms of the vectors in S , i.e., g(k)≜∥S ∥. In Proposition 3,
k k
g(k) corresponds to θk. Agents can choose g(k) that grows faster than linear, for example, g(k)=k2. This
way, agents do not need to know n and G, and Proposition 3 will hold with a different T .
L nom
For the clarity of the presentation, we will adhere to the choice of S = {s ∈ Rd | ∥s∥ ≤ θk} in our
k
analysis. WewillshowresultsforexponentiallygrowingsetsinSection6andwilldiscusstheimpactofthese
different growth rates on the convergence of the algorithm.
Remark 3. The norm we chose to define S was Euclidean. Other norms can be used if they are more
k
suitable for computations. However, all agents need to agree on the norm they use.
In the next section, we provide our main results.
145.3 Asymptotic Results
Here, we present two of our main theorems, addressing Problem 1. First, we establish the almost sure
convergence of Algorithm 1 to the optimal point. We then show the convergence of Algorithm 1 to the
optimal point in the rth mean. To prove this convergence we present an auxiliary result which bounds the
worst case error.
Theorem 2 (Almost Sure Convergence). Let S = {s ∈ Rd | ∥s∥ ≤ θk} with θ > n G, and let each
k L
legitimate agent i ∈ L initialize x [0],z [0] ∈ X arbitrarily and set s [0] = 0. Choose the stepsizes η and
i i i
λ such that they satisfy the conditions defined in Theorem 1. For each legitimate agent i ∈ L, denote the
sequence generated by the dynamic (9) by {x [k]}. Define the error vector
i
e[k]=(∥x[k]−x∗∥ ,D(x[k],ϕ),S(y[k],π))⊺ ,
ϕ
and the random time T ≜T n θ−G . Let Assumptions 1, 2, 4, and 5 hold true. Then, we have
nom max Lθ−nLG
e[k]≤M(η,λ)k−Tnome[T ], (17)
nom
for all k > T almost surely. Moreover, the sequence {x [k]} converges to the optimal point x∗ for all
nom i
i∈L almost surely.
Proof. We start by showing that the algorithm reaches the nominal behavior at some finite time almost
surely. By Lemma 1 the finite (random) time T exists. Moreover, the weights almost surely converge to
max
the correct weights defined in the nominal behavior by Corollary 1.
Define T ≜ T n θ−G . The agents will stop projecting their s [k] values after reaching T
nom max Lθ−nLG i nom
as shown in Proposition 3. Denote the variables of the Projected Push-Pull algorithm given in (3) with
x′[k], z′[k], and y′[k]. Then, after time T , Algorithm 1 will be equivalent to running the Projected
i i i nom
Push-Pull algorithm given in (3) with the initialization x′[0] = x [T ], z′[0] = z [T ], and y′[0] =
i i nom i i nom i
s [T ]−s [T −1]. Therefore, by Theorem 1, we have
i nom i nom
e[k]≤M(η,λ)k−Tnome[T ].
nom
Moreover, the sequence {x [k]} generated by this dynamics converges to x∗ for all initial points x′[0] ∈ X
i i
and y′[0]=∇f (x′[0]), which concludes the proof.
i i i
Next, we will show the convergence of the algorithm to the optimal point in the r-th mean. We will use
the Dominated Convergence Theorem [65] in our proof. Before doing that, we will first bound each error
termatthetimethesystemreachesthenominalbehavior. Sincetheinfluenceofthemaliciousagentsisstill
in the system before reaching the nominal behavior, these bounds reflect “the worst case” scenario for the
error terms. The following lemma provides these bounds.
Lemma 2 (The Worst Case Error Bounds). Let S ={s∈Rd |∥s∥≤θk} and B denote the bound on the
k
vectors in X as defined in Assumption 2. Then, we have
∥x[k]−x∗∥ ≤2B, (18a)
ϕ
D(x[k],ϕ)≤2B, (18b)
2(n +1)
S(y[k],π)≤ L θk, (18c)
min(π)
where min(π) denotes the minimum element of the stochastic vector π.
Proof. Using the compactness of set X and the triangular inequality, we obtain ∥x [k]−x∗∥ ≤ 2B. Then,
i
using the definition of ∥x[k]−x∗∥ we obtain
ϕ
(cid:115) (cid:115)
∥x[k]−x∗∥ = (cid:88) ϕ ∥x [k]−x∗∥2 ≤2B (cid:88) ϕ =2B,
ϕ i i i
i∈L i∈L
15where in the last step, we used the stochasticity of the vector ϕ . The bound on D(x[k],ϕ) is obtained
i
similarly. Next, we bound the gradient tracking error S(y[k],π). By definition, we have
(cid:118)
(cid:117) (cid:13) (cid:13)2
S(y[k],π)=(cid:117) (cid:116)(cid:88)
π
(cid:13) (cid:13)y i[k] −(cid:88)
y
[k](cid:13)
(cid:13) .
i(cid:13) π l (cid:13)
(cid:13) i (cid:13)
i∈L l∈L
First, notice that
∥y [k]∥=∥s [k]−s [k−1]∥≤2θk,
i i i
for any i∈L due to the projection onto the set S . Using the triangular inequality, we get
k
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)y i π[ ik] −(cid:88) y l[k](cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 ≤(cid:32)(cid:13) (cid:13) (cid:13) (cid:13)y i π[ ik](cid:13) (cid:13) (cid:13) (cid:13)+(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(cid:88) y l[k](cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(cid:33)2
l∈L l∈L
(cid:18)
2(n +1)
(cid:19)2
≤ L θk .
min(π)
Using this bound and the stochasticity of π, we obtain S(y[k],π)≤ 2(nL+1)θk.
min(π)
Now, we give the convergence in mean result which relies on the bounds (18a)-(18b). We note that the
additional upper bound (18c) will be utilized in the finite time analysis of the convergence rate of the RP3
algorithm which we present later on in Theorem 4.
Theorem 3 (Convergence in Mean). Let S ={s∈Rd |∥s∥≤θk} with θ >n G. Let Assumptions 1, 2, 4,
k L
and 5 hold true. Let each legitimate agent i∈L initialize the algorithm such that x [0],z [0]∈X arbitrarily
i i
and s [0]=0. Choose the stepsizes η and λ such that they satisfy the conditions defined in Theorem 1. Then,
i
the sequence generated by the dynamic (9) converges in the r-th mean to x∗ for any r ≥1, that is
lim E[∥x[k]−x∗∥r]=0.
ϕ
k→∞
Proof. We will prove this theorem using the Dominated Convergence Theorem [65]. Using the bounds given
in Lemma 2 we get
∥x[k]−x∗∥ ≤2B,
ϕ
∥x[k]−x∗∥r ≤(2B)r,
ϕ
where we take the r-th power of both sides. The error ∥x[k]−x∗∥r is bounded by a constant value. Recall
ϕ
that we have almost surely convergence by Theorem 2. Therefore, the desired result follows directly from
the Dominated Convergence Theorem.
5.4 Finite Time Analysis
In this part, we derive the expected convergence rate of the algorithm.
Theorem4(ExpectedConvergenceRate). LetS ={s∈Rd |∥s∥≤θk}withθ >n G.LetAssumptions1,
k L
2, 4, and 5 hold. Let each legitimate agent i∈L initialize the algorithm such that x [0],z [0]∈X arbitrarily
i i
and s [0] = 0. Choose the stepsizes η and λ such that they satisfy the conditions in Theorem 1. Define the
i
error vector e[k]=(∥x[k]−x∗∥ ,D(x[k],ϕ),S(y[k],π))⊺. Then, for all k ≥0, we have
ϕ
 2B 
E[e[k]]≤M(η,λ)k−⌊k/2⌋(I−M(η,λ))−1

2B

2(nL+1)θ⌊k/2⌋
min(π)
(19)
 2B 
(cid:26) (cid:18) (cid:19) (cid:27)
θ−n G
+min p
e n
L(θ−L G)(⌊k/2⌋+1)−∆ ,1 · 2(nL2 +B 1)θk,
min(π)
16where B denote the bound on the vectors in X as defined in Assumption 2, ∆ and p (·) are as given in
e
Proposition 1, and ⌊·⌋ denotes the floor function.
Proof. We know that after time T , the system will reach nominal behavior as shown in the proof of
nom
Theorem 2. Since we can analyze the system after reaching the nominal behavior, our strategy in the proof
istousethelawofiteratedexpectationsbyconditioningonT .Themainideaoftheproofistwofold: For
nom
small realizations of T , error reduction occurs as a result of contraction in the nominal case, facilitated
nom
by the early achievement of nominal behavior. Conversely, with a large T , the error terms may increase
nom
accordingtotheirupperlimits. Yet, theexponentialdecreaseintheprobabilityofT =k withincreasing
nom
k allows us to bound the expected error in this case. Hence, by the law of total expectation we have
E[e[k]]=E[E[e[k]|T ]]
nom
⌊k/2⌋
(cid:88)
= Pr(T =t)E[e[k]|T =t]
nom nom
t=0
+Pr(T >⌊k/2⌋)E[e[k]|T >⌊k/2⌋]. (20)
nom nom
We bound the first term as follows. Note that inequalities we use with respect to vectors and matrices hold
entry-wise.
⌊k/2⌋
(cid:88)
Pr(T =t)E[e[k]|T =k]
nom nom
t=0
⌊k/2⌋
(a) (cid:88)
≤ M(η,λ)k−te[t]
t=0
 ⌊k/2⌋  2B 
(b) (cid:88)
≤  M(η,λ)k−t  2B .
t=0
2(nL+1)θ⌊k/2⌋
min(π)
Ininequality(a),weboundedPr(T =t)with1,andused(17)giveninTheorem2. Inequality(b)follows
nom
directly from Lemma 2 and the fact that M(η,λ) is a non-negative matrix. Next, we bound the matrix
summation in the last inequality.
 
⌊k/2⌋ ⌊k/2⌋
(cid:88) (cid:88)
M(η,λ)k−t =M(η,λ)k−⌊k/2⌋

M(η,λ)t

t=0 t=0
(cid:32) ∞ (cid:33)
(a) (cid:88)
≤ M(η,λ)k−⌊k/2⌋ M(η,λ)t
t=0
( =b) M(η,λ)k−⌊k/2⌋(I−M(η,λ))−1, (21)
where in (a), we used the non-negativity of the matrix M(η,λ). Inequality (b) comes from the infinite sum
of matrices with spectral radius less than 1 (see [66, Theorem 3.15]). Now, we will bound the second term
17in (20). Let ∆ and p (·) be as given in Proposition 1. We have
e
Pr(T >⌊k/2⌋)E[e[k]|T >⌊k/2⌋]
nom nom
 2B 
≤Pr(T nom >⌊k/2⌋) 2B 
2(nL+1)θk
min(π)
(cid:26) (cid:18) (cid:19) (cid:27)
θ−n G
≤min p L (⌊k/2⌋+1)−∆ ,1
e n (θ−G)
L
 2B 
· 2B ,
2(nL+1)θk
min(π)
where the first inequality follows from Lemma 2 and the second one follows from Corollary 3. Combining
all the bounds gives us the desired result.
Theorem 4 states that for a sufficiently large k, the expected convergence rate of the system decays
geometrically. The convergence rate depends on various properties of the system and design choices. First,
the error contractions we get from the matrix M(η,λ) depend on the choices of step sizes η and λ, as
well as the contractions we get from the matrices R¯ and C¯, and the smoothness and the convexity of the
cost functions (see (8)). Second, both of the error terms depend on B and a linear bound that grows over
time. These terms reflect the effect that the malicious agents inflict before the system reaches the nominal
behavior. Lastly, the second error term depends on the learning rate that we get from the learning protocol
with trust opinions. The ∆ term captures the impact of the graph topology on the learning rate while the
coefficient nθ L− (θn −L GG ) captures the impact of the growth rate of the set S k on the time before reaching the
nominal behavior. An interesting trade-off is that while a faster growth rate gives us a better decrease in
the probability p (·), it also increases the impact that the malicious agents can have on the system through
e
the gradient tracking terms s [k]. In the next section, we will see this impact for a different choice of S .
i k
6 Optimization Problems with Unbounded X
In this section, we extend our results to optimization problems with unbounded constraint sets. This setup
is more challenging to guard against malicious behavior and to analyze, since the inputs of the malicious
agents do not reside within a known compact set. To capture this, throughout this section, we will use the
more general Assumption 3 instead of Assumption 2.
6.1 Bounding the x- and s-variables
In this setup the input values of the legitimate and malicious agents are not necessarily bounded and can
take any choice in Rd. The main challenge in removing the limitation on malicious agents to choose input
values from a predefined bounded set is that these agents can arbitrarily influence variables z [k], hence,
i
the decision variables x [k]. Therefore, we need to confine their impact until the system reaches the nominal
i
behavior. Our strategy, in this case, is to reapply the principle of projecting the gradient tracking variables
s [k] of growing bounded set to the agent’s data values x [k] as well. Specifically, for the x [k] variables, we
i i i
will introduce an expanding bounded set X to bound the effect of the malicious agents. First, we show
k
that introducing this set does not affect the convergence of the algorithm in the nominal case without the
malicious agents.
Lemma 3 (NominalConvergencewithX ). Define X ={x∈Rd |∥x∥≤exp(θk)} with θ >0 for all k ≥0.
k k
Define the effective constraint set at time k as X¯ ≜X ∩X . Assume that there are no malicious agents in
k k
the system, and agents run the Projected Push-Pull algorithm given in (3) with the the projections on the
set X¯ instead of X at every time step k. Then, Theorem 1 holds true after time k′, where k′ ≜ 1ln∥x∗∥,
k θ
where x∗ is the optimal solution of the problem given in (1).
18Proof. When k′ ≜ 1ln∥x∗∥, then x∗ ∈ X for all k ≥ k′ by the definition of X . Hence, we also have
θ k k
x∗ ∈X¯ . Since X¯ ⊆X and x∗ ∈X¯ , the point x∗ is also the optimal solution of the problem in (1) with the
k k k
constraint set X¯ . The one-step contraction in the error given by (7) only requires the constrained set to be
k
closed and convex, i.e., satisfy Assumption 3. Since X¯ is closed and convex and x∗ is included within the
k
set after time k′, Theorem 1 holds.
Lemma 3 shows that these new growing sets only have a minimal impact on the convergence of the
ProjectedPush-Pullalgorithm,byintroducingashortdelaythathasalogarithmicdependenceonthenorm
of x∗. Notice that even when x∗ ∈/ X , the RP3 algorithm preserves the gradient tracking property when
k
legitimate agents assign positive weights to their legitimate neighbors only. Therefore, in our forthcoming
analysis where we derive a bound on s [k] variable, we do not need to consider this time delay.
i
Our derivations of the growth rate of the variables s [k] in Section 5.2 rely on the compactness of X (see
i
Proposition 2). Next, we will prove that this result also holds for the increasing sequence of sets X¯ . This
k
willlater helpusto findawaytoconstruct thenewgrowingset S whenthe bound onx [k]also growsover
k i
time.
Lemma 4. For some θ > 0 and for all k ≥ 0, let X = {x ∈ Rd | ∥x∥ ≤ exp(θk)}. Also, let X¯ = X ∩X
k k k
and define G ≜ Aexp(θk) for all k ≥ 0, where A ≜ L+max {∥∇f (0)∥}. Assume that the system has
k i∈L i
the nominal behavior from the beginning, i.e., legitimate agents assign non-zero weights to their legitimate
neighbors only and there is no projection of the s [k] variables for all k ≥0. Then,
i
(cid:88)nL (cid:88)nL
∥s [k+1]∥≤n G + ∥s [k]∥,
i L k i
i=1 i=1
for all k ≥0. Moreover, if s [0]=0 for all i∈L, we have
i
n A
∥s [k]∥≤ L exp(θk),
i exp(θ)−1
for all k ≥0 and for all i∈L.
Proof. First, we bound the gradients at time k. Using the L-smoothness of the cost functions and the
boundedness of X¯ , we get
k
∥∇f (x [k])∥=∥∇f (x [k])+∇f (0)−∇f (0)∥≤Lexp(θk)+∥∇f (0)∥,
i i i i i i i
where we used the fact that gradients are L-Lipschitz continuous. If we define G ≜ Aexp(θk) with A =
k
L+max {∥∇f (0)∥}, we have ∥∇f (x [k])∥≤G for all i and for all k ≥0. Following the same steps in the
i i i i k
proof of Proposition 2 and using the bound on ∥∇f (x [k])∥, we obtain
i i
(cid:88)nL (cid:88)nL
∥s [k+1]∥≤n G + ∥s [k]∥.
i L k i
i=1 i=1
Recall that G ≜Aexp(θk). Since (cid:80)nL ∥s [0]∥=0, by induction, we get
k i=1 i
(cid:88)nL
∥s [k]∥≤n
k (cid:88)−1
G ≤
n LA
exp(θk),
i L t exp(θ)−1
i=1 t=0
where in the last step, we used the definition of G and summed the geometric series. Since ∥s [k]∥ ≤
k i
(cid:80)nL
∥s [k]∥, this bound yields the desired relation.
i=1 i
Notice that the conditions on the growth of the sequence {X } is different from that of {S }. This
k k
is because the growth rate of {S } is affected by the bounds on the gradients. Therefore, we have more
k
flexibility in choosing the growth rate of the set sequence {X }.
k
196.2 Convergence Results
In this section, we derive convergence results for unbounded optimization problems that are analogous to
the convergence results in Section 5.3 and Section 5.4. First, we give the growing set sequence {S } that
k
helps us achieve the nominal behavior after some point.
Proposition 4. For all k ≥ 0, define X¯ = X ∩X , where X = {x ∈ Rd | ∥x∥ ≤ exp(θ k)} with θ > 0.
k k k 1 1
Similarly, define S ={s∈Rd |∥s∥≤exp(θ k)} with θ >θ . Let s [0]=0 for all i∈L. Define
k 2 2 1 i
(cid:26)
ln(2n ) ln(A)
ln∥x∗∥(cid:27)
T ≜max T + L , , , (22)
nom max θ θ −θ θ
2 2 1 1
where A =
2nL(L+maxi{∥∇fi(0)∥})2
. Then, the RP3 algorithm has the nominal behavior for all k ≥ T
exp(θ1)−1 nom
almost surely.
Proof. Define G k ≜ A 1exp(θ 1k), where A 1 ≜ L+max i{∥∇f i(0)∥} and also define A 2 ≜ G Tmaxexpn (L θA 1)1 −1.
Following the proof of Proposition 3, for k ≥T we have
max
k−1
(cid:88) (cid:88) (cid:88)
∥s [k]∥≤ ∥s [T ]∥+n G ≤n exp(θ T )+A exp(θ (k−T )),
i i max L t L 2 max 2 1 max
i∈L i∈L t=Tmax
where in the last step, we summed the geometric series. We want to find the time when
exp(θ k)>n exp(θ T )+A exp(θ (k−T )) (23)
2 L 2 max 2 1 max
is satisfied. We deal with the terms on the right hand side of (23) separately by splitting exp(θ k) into two
2
equal terms. For the first one we write
1
exp(θ k)>n exp(θ T ),
2 2 L 2 max
ln(2n )
k >T + L .
max 2
For the second term, we have
1
exp(θ k)>A exp(θ (k−T ))
2 2 2 1 max
n A
exp(θ k)>2G L 1 exp(θ (k−T ))
2 Tmaxexp(θ )−1 1 max
1
exp(θ k)>Aexp(θ T )exp(θ (k−T ))
2 1 max 1 max
ln(A)
k > ,
θ −θ
2 1
where A =
2nL(L+maxi{∥∇fi(x′)∥})2
. Then, for any k ≥
max(cid:110)
T + ln(2nL),
ln(A)(cid:111)
, the inequality (23)
exp(θ1)−1 max θ2 θ2−θ1
(cid:110) (cid:111)
will be satisfied. Hence, for any k ≥max T + ln(2nL), ln(A) the projection operator will coincide with
max θ2 θ2−θ1
the identity operator. Lastly, we must consider the time when x∗ is included in the set X . Therefore,
k
(cid:110) (cid:111)
once we choose T ≜max T + ln(2nL), ln(A),ln∥x∗∥ , the system reaches the nominal behavior after
nom max θ2 θ2−θ1 θ1
T .
nom
Since the system reaches the nominal behavior, we retrieve the almost sure convergence result.
Corollary 4. Let X¯ = X ∩X , where X = {x ∈ Rd | ∥x∥ ≤ exp(θ k)} with θ > 0 and S = {s ∈ Rd |
k k k 1 1 k
∥s∥≤exp(θ k)} with θ >θ . Then, Theorem 2 holds.
2 2 1
20The convergence in mean and the convergence in expectation results are more tricky to get in this case.
This is due to the potential for malicious agents to exponentially amplify their impact on the system over
time, as the bounds on the variables themselves expand exponentially. Therefore, we need to ensure that
the learning rate of the agents is faster. First, we show the following bound on ∥x[k]−x∗∥ which will be
ϕ
useful for proving the convergence in the r-th mean sense.
Lemma 5. Assume that agents construct the sets X = {x ∈ Rd | ∥x∥ ≤ exp(θ k)} with θ > 0 and
k 1 1
S ={s∈Rd |∥s∥≤exp(θ k)} with θ >θ . Then, for all k ≥0 and for a given r with r ≥1, there exists
k 2 2 1
almost surely a constant c>0 that depends on r such that
∥x[k]−x∗∥r ≤cexp(rθ T ), (24)
ϕ 2 nom
where T is as defined in (22).
nom
Proof. Define c such that [M(η,λ)k] ≤ min(π) for all elements of the matrix i,j and k ≥ c . Such a
1 ij 2(nL+1) 1
c always exists since ρ(M(η,λ)) < 1. We divide the proof in two parts. First, consider the case where
1
k >T +c . Then, by Proposition 4 we have
nom 1
∥x[k]−x∗∥ ≤[M(η,λ)k−Tnom] 2exp(θ T )
ϕ 11 1 nom
+[M(η,λ)k−Tnom] 2exp(θ T )
12 1 nom
2(n +1)
+[M(η,λ)k−Tnom] L exp(θ T )
13 min(π) 2 nom
≤2exp(θ T )+exp(θ T )
1 nom 2 nom
≤3exp(θ T ).
2 nom
Therefore, we have
∥x[k]−x∗∥r ≤3rexp(rθ T ). (25)
ϕ 2 nom
Next, consider the case where k ≤T +c Then, we have
nom 1
(cid:115)
∥x[k]−x∗∥ = (cid:88) ϕ ∥x [k]−x∗∥2 ( ≤a) exp(θ k)+∥x∗∥
ϕ i i 1
i∈L
≤(∥x∗∥+1)exp(θ k),
1
wherein(a)weusedtheboundsonx [k]andthestochasticityofϕ. Usingthisbound,weget∥x[k]−x∗∥r ≤
i ϕ
(∥x∗∥+1)rexp(rθ k)≤(∥x∗∥+1)rexp(rθ k). By choosing c=max{3r,(∥x∗∥+1)r}, we obtain the desired
1 2
equality.
In the next result, we show that carefully chosen θ and θ can guarantee convergence in the r-th mean.
1 2
Proposition 5. Let X¯ =X ∩X , where X ={x∈Rd |∥x∥≤exp(θ k)} with θ >0 and S ={s∈Rd |
k k k 1 1 k
∥s∥≤exp(θ k)}. Let r ≥1 be given. Choose θ and θ such that rθ <min{2E2,2E2 } and θ <θ . Then,
2 1 2 2 L M 1 2
Theorem 3 holds true for this given r.
Proof. We prove this proposition by utilizing the almost sure convergence we established in Corollary 4 via
the Dominated Convergence Theorem [65]. By Lemma 5, we have
∥x[k]−x∗∥r ≤cexp(rθ T ),
ϕ 2 nom
for some c > 0. Then, if we show that cexp(rθ T ) has a finite expectation, i.e, E[cexp(rθ T )] < ∞,
2 nom 2 nom
the desired result is obtained. Observe that the constant c in Lemma 5 does not depend on T . By the
nom
law of total expectation we have
∞
(cid:88)
E[cexp(rθ T )]=E[E[cexp(rθ T )|T ]]= E[cexp(rθ T )|T =k]Pr(T =k)
2 nom 2 nom nom 2 nom nom nom
k=0
∞
(cid:88)
≤ cexp(rθ k)Pr(T =k).
2 nom
k=0
21BydefinitionofT givenin(22), Pr(T =k)=Pr(T =k−ln(2nL))forallk ≥k′ forsomek′. Then,
nom nom max θ2
we can use the bounds provided in Proposition 1 to obtain
Pr(T =k)≤min{p (k−∆),1},
max c
where p (k) = N exp(−2kE2)+N exp(−2kE2 ) to bound the probability Pr(T = k). Therefore, for
c L L L M nom
all k ≥k′, we have Pr(T =k)≤c (exp(−2kE2)+exp(−2kE2 )) for some constant c >0.
nom 1 L M 1
Using this bound we have
∞ k′ ∞
(cid:88) (cid:88) (cid:88)
cexp(rθ k)Pr(T =k)= cexp(rθ k)Pr(T =k)+ cexp(rθ k)Pr(T =k)
2 nom 2 nom 2 nom
k=0 k=0 k=k′+1
k′ ∞
(cid:88) (cid:88)
≤ cexp(rθ k)Pr(T =k)+ cexp(rθ k)c (exp(−2kE2)+exp(−2kE2 )).
2 nom 2 1 L M
k=0 k=k′+1
The first term is a finite sum and thus bounded. The second term is equal to
∞
(cid:88)
c·c exp((rθ −2E2)k)+exp((rθ −2E2 )k).
1 2 L 2 M
k=k′+1
If we choose rθ <min{2E2,2E2 }, the second term is also finite, which concludes the proof.
2 L M
Finally, we retrieve the expected convergence rate result of Theorem 4. We show that with appropri-
ately chosen growth rates of the sets {X } and {S }, our algorithm attains geometric convergence rate in
k k
expectation.
Theorem 5. Let X¯ = X ∩X , where X = {x ∈ Rd | ∥x∥ ≤ exp(θ k)} with θ > 0 and S = {s ∈ Rd |
k k k 1 1 k
∥s∥≤exp(θ k)}withθ >θ .Define∆ ≜∆+ ln(A),whereA= 2nL(L+maxi{∥∇fi(0)∥})2 and∆isasdefined
2 2 1 T θ2−θ1 exp(θ1)−1
in Proposition 1. Then, for all k ≥
ln∥x∗∥,
we have
θ1
 2exp(θ ⌊k/2⌋) 
1
E[e[k]]≤M(η,λ)k−⌊k/2⌋(I−M(η,λ))−1

2exp(θ 1⌊k/2⌋)

2(nL+1)exp(θ ⌊k/2⌋)
min(π) 2
(26)
 2exp(θ k) 
1
+min{p e(⌊k/2⌋+1−∆ T),1} 2exp(θ 1k) .
2(nL+1)exp(θ k)
min(π) 2
Moreover, if we have
θ <min{−ln(ρ(M(η,λ))),E2,E2 },
2 L M
then the expected convergence is exponential in k for a sufficiently large k.
Proof. First, notice that T −T can be at most ln(A) from the definition of T . Therefore, we can
nom max θ2−θ1 nom
use the inequality Pr(T > k−1) ≤ min{p (k+1−∆ ),1} which follows from Proposition 1. (26) can
nom e T
be derived using the same steps as the proof of Theorem 4. To show the exponential decrease, we need to
show that both terms on the right-hand side of (26) are exponentially decreasing with k. For the first term,
we have
 2exp(θ ⌊k/2⌋) 
1
M(η,λ)k−⌊k/2⌋(I−M(η,λ))−1  2exp(θ 1⌊k/2⌋) ≤exp(θ 2⌊k/2⌋)M(η,λ)k−⌊k/2⌋v,
2(nL+1)exp(θ ⌊k/2⌋)
min(π) 2
≤(exp(θ )M(η,λ))k−⌊k/2⌋v
2
22 2 
where v = (I − M(η,λ))−1  2 . Here, we used the fact that θ 2 ≥ θ 1. Hence, guaranteeing that
2(nL+1)
min(π)
ρ(exp(θ )M(η,λ))<1 guarantees the exponential decrease for this term. For the second term, we have
2
 2exp(θ k)   2 
1
min{p e(⌊k/2⌋+1−∆ T),1} 2exp(θ 1k) ≤min{p e(k/2−∆ T),1}exp(θ 2k) 2 .
2(n+1)exp(θ k) 2(nL+1)
min(π) 2 min(π)
Forasufficientlylargek,wegetanexponentialdecreasefromp (k/2−∆ ).Hence,weonlyneedtoguarantee
e T
that the exponential decrease rate we get from p (k/2−∆ ) is faster than the increase rate of exp(θ k). By
e T 2
using the definition of p (k) given in (11), we get the conditions θ <E2 and θ <E2 . Combining all three
e 2 L 2 M
conditions gives us the desired result.
Different from the analogous result with the compact constraint set X in Theorem 4, Theorem 5 shows
that we have more control over the errors coming from the malicious agents by choosing an appropriate
growth rate for the set sequence {X }.
k
Remark 4. The conditions on the growth rates θ and θ in Theorem 5 are more strict than the one given
1 2
in Proposition 5. This is because while Proposition 5 ensures convergence in expectation, Theorem 5 provides
sufficient conditions for geometric convergence rate.
6.3 Choosing Sets X and S
k k
Our results in Theorem 5 require legitimate agents to know the expectations of trust observations E and
L
E , and Proposition 5 further requires knowledge of the spectral radius of the matrix M(η,λ) or at least
M
meaningfulupperboundsontheseterms. SomeofthesesystemparameterssuchasM(η,λ), canbedifficult
to estimate. This requirement comes from the fact that the sets X and S both grow exponentially. Their
k k
exponential growth allows for malicious agents to have exponentially growing impact on the system until
they are fully detected and excluded. Therefore, the learning part where malicious agents are detected
needs to happen faster than the growth of their impact. On the other hand, choosing sets that grow faster
decreases the time that the system reaches the nominal behaviour. Hence, there is a trade-off in the choice
ofthegrowthrateofthesesets. Inanycase,onemaywanttoguaranteegeometricconvergenceratewithout
knowing all the system parameters. This is indeed possible since our convergence results depend on two key
properties of these sets: 1) the growth of X ensures that x∗ is eventually included in X , 2) S grows faster
k k k
thanX . Thefirstoneisrequiredtoensurethatx∗ iseventuallyincludedinX . Thesecondoneisrequired
k k
since the norm of the gradient tracking variables s [k] grows with a rate that depends on the norm of the
i
gradients, which is related to the bounds on the sets X . This relationship can be seen in Proposition 2
k
and Lemma 4. Therefore, as long as we can satisfy these two conditions, the system will reach the nominal
behavior and the convergence will occur. We summarize this discussion formally in the next proposition.
Proposition 6. Assume that we have X ={x∈Rd |∥x∥≤g(k)} and S ={s∈Rd |∥s∥≤h(k)}. Then,
k k
if g(k) is an increasing function and h(k) grows faster than kg(k), i.e., h(k)=ω(kg(k)) where ω(·) denotes
the asymptotic lower bound, the nominal behavior is reached almost surely.
Proof. The proof follows similar to Lemma 4. By Assumption 1 we have
∥∇f (x [k])∥=∥∇f (x [k])+∇f (0)−∇f (0)∥
i i i i i i
≤Lg(k)+∥∇f (0)∥.
i
Using similar steps to Lemma 4, we get
(cid:88)nL
∥s [k+1]∥≤n (Lg(k)+max∥∇f (0)∥)
i L i
i∈L
i=1
(cid:88)nL
+ ∥s [k]∥,
i
i=1
23and for k ≥T we have
max
k−1
(cid:88) (cid:88) (cid:88)
∥s [k]∥≤ ∥s [T ]∥+n Lg(t)+max∥∇f (0)∥
i i max L i
i∈L
i∈L i∈L t=Tmax
≤n h(T )+n (k−T )(Lg (k−1)+max∥∇f (0)∥).
L max L max x i
i∈L
To reach the nominal convergence, we need to have a k′ such that for all k ≥k′ we have
h(k)>n h(T )+n (k−T )(Lg(k−1)
L max L max
+max∥∇f (0)∥).
i
i∈L
Such a k′ always exists if we choose h(k) to grow faster than kg(k). Hence, the nominal behavior can be
reached and the algorithm converges almost surely.
Remark 5. Legitimate agents can choose a growing set X and a corresponding S with a sub-exponential
k k
growth rate such that convergence almost surely occurs without knowing any of the system parameters.
Here, the correct choice of {X } and a corresponding {S } needs to ensure that the nominal behavior is
k k
alwaysreached. Recallthatthemisclassificationprobabilitiesofthemaliciousagentsdecreaseexponentially
as shown in Proposition 1. Therefore, if we {X } and {S } with a sub-exponential rate, the learning part
k k
will always occur faster than the growth of sets and we guarantee convergence almost surely.
6.4 Average Consensus Problem
In this part, we show that the RP3 algorithm can be used to solve the average consensus problem in the
presence of malicious agents without introducing any deviation from the optimal solution.
Corollary 5 (AverageConsensus). Consideramulti-agentsystemwhereeachlegitimateagenthasaninitial
value x [0]∈R. The goal of legitimate agents is to compute the average of their initial values denoted by xˆ,
i
i.e, to find
1 (cid:88)
xˆ≜ x [0]. (27)
n i
L
i∈L
LetAssumption4andAssumption5holdtrue. Then, agentscanusetheRP3algorithmtocomputexˆ almost
surely, even in the presence of malicious agents.
Proof. The average consensus problem can be formulated as
1 (cid:88)
min (x−x [0])2.
x∈R n
L
i
i∈L
Notice that the optimum solution x∗ of this minimization problem is x∗ = 1 (cid:80) x [0]. Moreover, the
nL i∈L i
localcostfunctionsf (x)≜(x−x [0])2 isL−smoothandstronglyconvex. Therefore,byCorollary4,agents
i i
can obtain the optimal solution almost surely using the RP3 algorithm.
Since we can formulate the consensus problem as a distributed optimization problem with L−smooth
and strongly convex cost functions, all results obtained in Section 6 are applicable if agents employ the RP3
algorithm to solve the problem. These results improve upon the previous results on the average consensus
in the presence of malicious agents when stochastic trust observations are available. The study in [14]
tackles this problem in undirected graphs but it requires values of the legitimate and malicious agents to
be bounded by a constant η. Moreover, the proposed method cannot remove the impact of the malicious
agents completely, resulting in a deviation from the true consensus value xˆ unlike our results. The resilient
distributed optimization algorithm developed in [15] ensures convergence to the true consensus value for
constrainedconsensusproblems,butonlyforundirectedcommunicationgraphsandwhenthemixingmatrix
is doubly stochastic. Furthermore, its diminishing step sizes lead to a significantly slower convergence rate
comparedtoRP3. Thealgorithmproposedin[67]removestheeffectofmaliciousagentsovertime. However,
theresultsonlyshowasymptoticconvergencewhilehere,wealsocharacterizetheexpectedconvergencerate.
24Figure 1: Constrained consensus experiment results with L=50 legitimate and M =100 malicious agents.
7 Numerical Studies
In this section, we present our numerical studies to validate our theoretical results on two optimization
problems. First, we consider a constrained average consensus problem to compare RP3 against the existing
methods that leverage inter-agent trust values. Then, we test our algorithm on multi-robot target tracking
problem and compare its performance against a data-based resilient optimization benchmark algorithm.
The first setup is a constrained optimization problem over undirected graphs while the second setup is an
unconstrained optimization problem over a directed graph, covering different use cases for our algorithm.
7.1 Constrained Consensus
Here, the agents’ goal is to solve the average consensus problem defined in Eq. (27) subject to constraints.
We compare the performance of our algorithm against several benchmarks. We consider resilient consensus
(RC)[14]andtrustworthydistributedaverageconsensus(RDA)[67]algorithmsthatarespecificallydesigned
for the consensus problem. We also include the resilient distributed optimization (RDO) algorithm in [15]
and Projected Push-Pull (PPP) algorithm as our benchmarks. PPP algorithm is oblivious to the existence
of malicious agents in the system, showcasing the worst-case scenario under malicious attacks.
In the experiments, we consider a setup with L = 50 legitimate agents and M = 100 malicious agents.
We generate an undirected cyclic graph among the legitimate agents and add additional edges between
them randomly. We also add undirected edges between malicious and legitimate agents with a probability
0.7. This process results in a graph where many legitimate agents have more malicious neighbors than
legitimate ones, making data-based methods such as [13,48,57] inapplicable. We choose [−50,50] as the
constraint set and generate random initial points for legitimate agents within this range. If the optimal
point is positive, malicious agents send −50 and they send 50 otherwise. Malicious agents also send these
values as the gradient tracking variable against RP3 and PPP algorithms. Since these methods do not filter
based on the values sent by malicious agents, using these extreme values represents a strong attack against
all trust-based methods. Similar to previous work [14,15,28], trust values are sampled from the uniform
distribution[0.35,0.75]forlegitimateneighborsandfrom[0.25,0.65]formaliciousneighbors. Asrequiredby
RC, agents update their trust opinions for an initial observation window of 30 iterations before starting to
run the optimization algorithms. We use the same trust values for all algorithms. For RP3, we choose the
bounds on sets X and S as 0.1k and 0.1k2, respectively. The results are shown in Fig. 1. We observe that
k k
RP3 has a significantly faster convergence rate than the other algorithms, even though it is not specifically
designed for consensus, unlike [14,67], and is not limited to undirected graphs and constraint optimization,
as in [15].
25(a) Loss (b) Trajectory
Figure 2: Target tracking experiment results with L = 9 legitimate and M = 6 malicious agents. (a)
Convergence to the optimal trajectory. (b) Visualization of the final trajectories for all methods.
7.2 Multi-Robot Target Tracking
Inthispart,wedemonstratetheperformanceofouralgorithmonthedistributedmulti-robottargettracking
problem, as defined in [1]. In this problem, a set of robots communicating over a directed graph aim
to estimate the trajectory of a moving target through local observations and local communication, in the
presence of malicious agents. Concretely, the position and velocity at time t of the moving target in a
2D plane is given by a vector x ∈ R4. The target is assumed to move according to the linear dynamics
t
x =A x +w whereA ∈R4×4 capturestheknowndynamics,andw ∼N(0,Q )istheGaussianprocess
t+1 t t t t t t
noise. Robot i receives noisy observations of the position of the target if the target is within its observation
range, according to y = xp+v where xp denotes the position of the target, and v ∼ N(0,R ) is the
i,t t i,t t i,t i,t
observation noise. We let T the set of all times when robot i observes the target. The initial position and
i
velocity x is modeled by a prior N(x ,P ). In this setup, the maximum likelihood estimate is given by
0 0 0
minimizing the following loss:
T−1
f(x)=∥x −x¯ ∥2 + (cid:88) ∥x −A x ∥2
0 0 P¯−1 t+1 t t Q−1
0 t
t=1
+(cid:88)(cid:88) ∥y −xp∥2 . (28)
i,t t R−1
i,t
i∈Vt∈Ti
Then, each robot has the following local cost function which is determined according to their observations
T−1
f (x)= 1 ∥x −x¯ ∥2 + (cid:88) 1 ∥x −A x ∥2
i N 0 0 P¯ 0−1 N t+1 t t Q− t1
t=1
+ (cid:88) ∥y −C x ∥2 .
i,t i,t t R−1
i,t
t∈Ti
Forthisproblem,wecompareourmethodagainstbothnon-resilientandresilientdistributedoptimization
algorithms. Similartotheconstrainedconsensusexperiments,wechoosePPPasthenon-resilientbenchmark.
For the resilient benchmark, we implement the SDMMFD algorithm in [48], which is among the best-
performingalgorithmsinitsclass[68]. Thisalgorithmdoesnotusetrustvalues,andinsteadreliesonoutlier
elimination to guarantee convergence to the convex hull of the legitimate agents’ local minimizers.
We consider a setup with L = 9, M = 6 agents. Legitimate agents are connected through a grid graph
with diagonal connections randomly included. The resulting graph is 3-Robust, satisfying the conditions for
26the SDMMFD algorithm to converge. Each malicious agent is only connected to 1 legitimate agent through
anundirectededge. Maliciousagentssendaconstantattackvalueatalltimesteps,topreventthelegitimate
agents from converging to the global minimizer. We consider a time horizon T =10 for the trajectory. We
maintain the same parameters for the trust observations and the RP3 algorithm as used in the constrained
consensus experiments and only adjust the learning rate. The results are shown in Fig. 2. We observe
that RP3 algorithm converges to the optimal trajectory that minimizes the global objective function in
(28) (shown in green in Fig. 2). The non-resilient Projected Push-Pull converges to the trajectory provided
by the malicious agents, and SDMMFD converges to a non-optimal trajectory. Note that SDMMFD and
RP3 require different assumptions to guarantee resilience, but we demonstrate that RP3 achieves better
performance when trust values are available.
8 Conclusion
In this work, we study the distributed optimization problem in the presence of malicious agents, where the
objective of legitimate agents is to minimize the sum of their individual strongly convex loss functions by
exchanging information over a directed communication graph. Our proposed algorithm, Resilient Projected
Push-Pull (RP3), leverages gradient tracking for fast convergence and exploits inter-agent trust values and
growing constraint sets to mitigate and eliminate the impact of malicious agents on the system. We char-
acterize the sufficient conditions on the growing sets to ensure geometric convergence in expectation. Our
theoretical analysis demonstrates that RP3 converges to the nominal optimal solution almost surely and in
the r-th mean for any r ≥ 1, given appropriately chosen step sizes and constraint sets. We show that RP3
cansolveaverageconsensusproblemsfasterthanexistingmethodsthatleveragetrustvalues,includingthose
specifically designed for the consensus problem.
Wevalidateourtheoreticalresultsthroughnumericalstudiesonaverageconsensusandmulti-robottarget
trackingproblems. Inthesestudies,ouralgorithmoutperformsbothexistingtrustvalue-basedmethodsand
data-based filtering algorithms. This demonstrates the practical effectiveness of RP3 in enhancing the
resilience and efficiency of distributed optimization in adversarial environments.
References
[1] O. Shorinwa, T. Halsted, J. Yu, and M. Schwager, “Distributed optimization methods for multi-robot
systems: Part 1—a tutorial,” IEEE Robotics & Automation Magazine, pp. 2–19, 2024.
[2] A. Nedi´c and J. Liu, “Distributed optimization for control,” Annual Review of Control, Robotics, and
Autonomous Systems, vol. 1, no. Volume 1, 2018, pp. 77–103, 2018.
[3] O. Shorinwa and M. Schwager, “Distributed contact-implicit trajectory optimization for collaborative
manipulation,”in2021InternationalSymposiumonMulti-RobotandMulti-AgentSystems(MRS),2021,
pp. 56–65.
[4] P. D. Christofides, R. Scattolini, D. Mun˜oz de la Pen˜a, and J. Liu, “Distributed model predictive
control: A tutorial review and future research directions,” Computers & Chemical Engineering, vol. 51,
pp. 21–41, 2013, cPC VIII.
[5] R.TronandR.Vidal,“Distributed3-dlocalizationofcamerasensornetworksfrom2-dimagemeasure-
ments,” IEEE Transactions on Automatic Control, vol. 59, no. 12, pp. 3325–3340, 2014.
[6] Dang, Vy-Long, Le, Binh-Son, Bui, Trong-Tu, Huynh, Huu-Thuan, and Pham, Cong-Kha, “A decen-
tralized localization scheme for swarm robotics based on coordinate geometry and distributed gradient
descent,” MATEC Web of Conferences, vol. 54, p. 02002, 2016.
[7] S. S. Ram, V. V. Veeravalli, and A. Nedi´c, “Distributed and recursive parameter estimation in
parametrized linear state-space models,” IEEE Transactions on Automatic Control, vol. 55, no. 2,
pp. 488–492, 2010.
27[8] O.Shorinwa,J.Yu,T.Halsted,A.Koufos,andM.Schwager,“Distributedmulti-targettrackingforau-
tonomousvehiclefleets,” in2020 IEEE International Conference on Robotics and Automation (ICRA),
2020, pp. 3495–3501.
[9] A.Nedi´candA.Olshevsky,“Distributedoptimizationovertime-varyingdirectedgraphs,”IEEE Trans-
actions on Automatic Control, vol. 60, no. 3, pp. 601–615, 2014.
[10] A. Makhdoumi and A. Ozdaglar, “Graph balancing for distributed subgradient methods over directed
graphs,” in 2015 54th IEEE Conference on Decision and Control (CDC). IEEE, 2015, pp. 1364–1371.
[11] C. Xi, Q. Wu, and U. A. Khan, “On the distributed optimization over directed networks,” Neurocom-
puting, vol. 267, pp. 508–515, 2017.
[12] S.SundaramandC.N.Hadjicostis,“Distributedfunctioncalculationvialineariterativestrategiesinthe
presence of malicious agents,” IEEE Transactions on Automatic Control, vol. 56, no. 7, pp. 1495–1508,
2010.
[13] S. Sundaram and B. Gharesifard, “Distributed optimization under adversarial nodes,” IEEE Transac-
tions on Automatic Control, vol. 64, no. 3, pp. 1063–1076, 2018.
[14] M. Yemini, A. Nedi´c, A. J. Goldsmith, and S. Gil, “Characterizing trust and resilience in distributed
consensus for cyberphysical systems,” IEEE Trans. Robot., vol. 38, no. 1, pp. 71–91, 2022.
[15] M. Yemini, A. Nedi´c, S. Gil, and A. J. Goldsmith, “Resilience to malicious activity in distributed
optimizationforcyberphysicalsystems,”in2022IEEE61stConferenceonDecisionandControl(CDC).
IEEE, 2022, pp. 4185–4192.
[16] M. Cavorsi, O. E. Akgu¨n, M. Yemini, A. J. Goldsmith, and S. Gil, “Exploiting trust for resilient
hypothesis testing with malicious robots,” in 2023 IEEE International Conference on Robotics and
Automation (ICRA), 2023, pp. 7663–7669.
[17] S. Gil, S. Kumar, M. Mazumder, D. Katabi, and D. Rus, “Guaranteeing spoof-resilient multi-robot
networks,” Autonomous Robots, vol. 41, no. 6, pp. 1383–1400, 2017.
[18] A. Pierson and M. Schwager, Adaptive Inter-Robot Trust for Robust Multi-Robot Sensor Coverage.
Cham: Springer International Publishing, 2016, pp. 167–183.
[19] J. Xiong and K. Jamieson, “Securearray: improving wifi security with fine-grained physical-layer in-
formation,” in Proceedings of the 19th Annual International Conference on Mobile Computing & Net-
working, ser. MobiCom ’13. New York, NY, USA: Association for Computing Machinery, 2013, p.
441–452.
[20] S. Gil, M. Yemini, A. Chorti, A. Nedi´c, H. V. Poor, and A. J. Goldsmith, “How physicality enables
trust: A new era of trust-centered cyberphysical systems,” 2023.
[21] S.Aydın, O.E.Akgu¨n, S.Gil, andA.Nedi´c, “Multi-agentresilientconsensusunderintermittentfaulty
and malicious transmissions (extended version),” 2024.
[22] C. N. Hadjicostis and A. D. Dominguez-Garcia, “Trustworthy distributed average consensus based on
locally assessed trust evaluations,” arXiv preprint arXiv:2309.00920, 2023.
[23] N. Ravi, A. Scaglione, and A. Nedi´c, “A case of distributed optimization in adversarial environment,”
in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019, pp. 5252–5256.
[24] R. Xin and U. A. Khan, “A linear algorithm for optimization over directed graphs with geometric
convergence,” IEEE Control Systems Letters, vol. 2, no. 3, pp. 315–320, 2018.
[25] S. Pu, W. Shi, J. Xu, and A. Nedi´c, “Push–pull gradient methods for distributed optimization in
networks,” IEEE Transactions on Automatic Control, vol. 66, no. 1, pp. 1–16, 2021.
28[26] R.Xin,C.Xi,andU.A.Khan,“Frost—fastrow-stochasticoptimizationwithuncoordinatedstep-sizes,”
EURASIP Journal on Advances in Signal Processing, vol. 2019, no. 1, pp. 1–14, 2019.
[27] G. Qu and N. Li, “Harnessing smoothness to accelerate distributed optimization,” IEEE Transactions
on Control of Network Systems, vol. 5, no. 3, pp. 1245–1260, 2018.
[28] O. E. Akgun, A. K. Dayi, S. Gil, and A. Nedi´c, “Learning trust over directed graphs in multiagent
systems,” in Proceedings of The 5th Annual Learning for Dynamics and Control Conference, ser. Pro-
ceedingsofMachineLearningResearch,N.Matni,M.Morari,andG.J.Pappas,Eds.,vol.211. PMLR,
15–16 Jun 2023, pp. 142–154.
[29] O. E. Akgu¨n, A. K. Dayı, S. Gil, and A. Nedi´c, “Learning trust over directed graphs in multiagent
systems (extended version),” arXiv preprint arXiv:2212.02661, 2022.
[30] G. Qu and N. Li, “Harnessing smoothness to accelerate distributed optimization,” in 2016 IEEE 55th
Conference on Decision and Control (CDC), 2016, pp. 159–166.
[31] J. Xu, S. Zhu, Y. C. Soh, and L. Xie, “Augmented distributed gradient methods for multi-agent op-
timization under uncoordinated constant stepsizes,” in 2015 54th IEEE Conference on Decision and
Control (CDC). IEEE, 2015, pp. 2055–2060.
[32] A. Nedi´c, A. Olshevsky, and W. Shi, “Achieving geometric convergence for distributed optimization
over time-varying graphs,” SIAM Journal on Optimization, vol. 27, no. 4, pp. 2597–2633, 2017.
[33] C.Xi,V.S.Mai,R.Xin,E.H.Abed,andU.A.Khan,“Linearconvergenceinoptimizationoverdirected
graphs with row-stochastic matrices,” IEEE Transactions on Automatic Control, vol. 63, no. 10, pp.
3558–3565, 2018.
[34] H.Liu,W.Yu,andG.Chen,“Discrete-timealgorithmsfordistributedconstrainedconvexoptimization
with linear convergence rates,” IEEE Transactions on Cybernetics, vol. 52, no. 6, pp. 4874–4885, 2020.
[35] M.Luan, G.Wen, H.Liu, T.Huang, G.Chen, andW.Yu, “Distributeddiscrete-timeconvexoptimiza-
tion with closed convex set constraints: Linearly convergent algorithm design,” IEEE Transactions on
Cybernetics, pp. 1–13, 2023.
[36] G. Scutari and Y. Sun, “Distributed nonconvex constrained optimization over time-varying digraphs,”
Mathematical Programming, vol. 176, pp. 497–544, 2019.
[37] O. E. Akgu¨n, A. K. Dayı, S. Gil, and A. Nedi´c, “Projected push-pull for distributed constrained opti-
mizationovertime-varyingdirectedgraphs(extendedversion),”arXivpreprintarXiv:2310.06223,2023.
[38] S. Pu, “A robust gradient tracking method for distributed optimization over directed networks,” in
Proc. of the 59th IEEE Conference on Decision and Control (CDC), Jeju Island, Republic of Korea,
Dec. 2020, pp. 2335–2341.
[39] Y. Wang and T. Ba¸sar, “Gradient-tracking-based distributed optimization with guaranteed optimality
under noisy information sharing,” IEEE Transactions on Automatic Control, vol. 68, no. 8, pp. 4796–
4811, 2023.
[40] Y.WangandA.Nedi´c,“Tailoringgradientmethodsfordifferentiallyprivatedistributedoptimization,”
IEEE Transactions on Automatic Control, vol. 69, no. 2, pp. 872–887, 2024.
[41] Z. Pan, H. Yang, and H. Liu, “Utilizing second-order information in noisy information-sharing envi-
ronments for distributed optimization,” in ICASSP 2024 - 2024 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2024, pp. 9156–9160.
[42] W. Wu, S. Zhu, S. Liu, and X. Guan, “Exact noise-robust distributed gradient-tracking algorithm
for constraint-coupled resource allocation problems,” in 2023 62nd IEEE Conference on Decision and
Control (CDC), 2023, pp. 7271–7276.
29[43] W.Wu,S.Liu,andS.Zhu,“Distributeddualgradienttrackingforeconomicdispatchinpowersystems
with noisy information,” Electric Power Systems Research, vol. 211, p. 108298, 2022.
[44] S. Pu and A. Nedi´c, “Distributed stochastic gradient tracking methods,” Mathematical Programming,
vol. 187, no. 1, pp. 409–457, 2021.
[45] R. Xin, A. K. Sahu, U. A. Khan, and S. Kar, “Distributed stochastic optimization with gradient
tracking over strongly-connected networks,” in 2019 IEEE 58th Conference on Decision and Control
(CDC), 2019, pp. 8353–8358.
[46] S. Zhao and Y. Liu, “Confidence region for distributed stochastic optimization problem via stochastic
gradient tracking method,” Automatica, vol. 159, p. 111352, 2024.
[47] L. Su and N. H. Vaidya, “Byzantine-resilient multiagent optimization,” IEEE Transactions on Auto-
matic Control, vol. 66, pp. 2227–2233, 2021.
[48] K. Kuwaranancharoen, L. Xin, and S. Sundaram, “Scalable distributed optimization of multi-
dimensional functions despite byzantine adversaries,” IEEE Transactions on Signal and Information
Processing over Networks, vol. 10, pp. 360–375, 2024.
[49] S. Zhang, Z. Liu, G. Wen, and Y. wu Wang, “Accelerated distributed optimization algorithm with
malicious nodes,” IEEE Transactions on Network Science and Engineering, vol. 11, pp. 2238–2248,
2024.
[50] N.Gupta,T.T.Doan,andN.H.Vaidya,“Byzantinefault-toleranceindecentralizedoptimizationunder
2f-redundancy,” 2021 American Control Conference (ACC), pp. 3632–3637, 2021.
[51] J. Zhu, Y. Lin, A. Velasquez, and J. Liu, “Resilient distributed optimization*,” 2023 American Control
Conference (ACC), pp. 1307–1312, 2022.
[52] N.GuptaandN.H.Vaidya,“Resilienceincollaborativeoptimization: Redundantandindependentcost
functions,” ArXiv, vol. abs/2003.09675, 2020.
[53] N. Ravi and A. Scaglione, “Detection and isolation of adversaries in decentralized optimization for
non-strongly convex objectives,” IFAC-PapersOnLine, vol. 52, no. 20, pp. 381–386, 2019.
[54] C. Zhao, J. He, and Q.-G. Wang, “Resilient distributed optimization algorithm against adversarial
attacks,” IEEE Transactions on Automatic Control, vol. 65, pp. 4308–4315, 2020.
[55] C.XuandQ.Liu,“Atrust-basedresilientconsensusalgorithmfordistributedoptimizationconsidering
node and edge attacks,” International Journal of Robust and Nonlinear Control, vol. 33, pp. 3517 –
3534, 2022.
[56] W.Fu,Q.Ma,J.Qin,andY.Kang,“Resilientconsensus-baseddistributedoptimizationunderdeception
attacks,” International Journal of Robust and Nonlinear Control, vol. 31, pp. 1803 – 1816, 2020.
[57] M. Kaheni, E. Usai, and M. Franceschelli, “Resilient constrained optimization in multi-agent systems
with improved guarantee on approximation bounds,” IEEE Control Systems Letters, vol. 6, pp. 2659–
2664, 2022.
[58] M. Cavorsi, O. E. Akgu¨n, M. Yemini, A. J. Goldsmith, and S. Gil, “Exploiting trust for resilient
hypothesis testing with malicious robots,” in 2023 IEEE International Conference on Robotics and
Automation (ICRA), 2023, pp. 7663–7669.
[59] M. Cheng, C. Yin, J. Zhang, S. Nazarian, J. Deshmukh, and P. Bogdan, “A general trust framework
for multi-agent systems,” in Proceedings of the 20th International Conference on Autonomous Agents
and MultiAgent Systems, 2021, pp. 332–340.
[60] C. Pippin and H. Christensen, “Trust modeling in multi-robot patrolling,” in 2014 IEEE International
Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 59–66.
30[61] Z. Yang and R. Tron, “Enhancing security in multi-robot systems through co-observation planning,
reachability analysis, and network flow,” 2024.
[62] T. Ding, Q. Xu, S. Zhu, and X. Guan, “A convergence-preserving data integrity attack on distributed
optimization using local information,” in 2020 59th IEEE Conference on Decision and Control (CDC),
2020, pp. 3598–3603.
[63] D.T.A.Nguyen,D.T.Nguyen,andA.Nedi´c,“Distributednashequilibriumseekingovertime-varying
directed communication networks,” arXiv preprint arXiv:2201.02323, 2022.
[64] S.Pu,“Arobustgradienttrackingmethodfordistributedoptimizationoverdirectednetworks,”in2020
59th IEEE Conference on Decision and Control (CDC), 2020, pp. 2335–2341.
[65] E. C¸ınlar, Measure and Integration. New York, NY: Springer New York, 2011, pp. 1–47.
[66] R. S. Varga, Basic Iterative Methods and Comparison Theorems. Berlin, Heidelberg: Springer Berlin
Heidelberg, 2000, pp. 63–110.
[67] C. N. Hadjicostis and A. D. Dom´ınguez-Garc´ıa, “Trustworthy distributed average consensus,” in 2022
IEEE 61st Conference on Decision and Control (CDC), 2022, pp. 7403–7408.
[68] K. Kuwaranancharoen and S. Sundaram, “On the geometric convergence of byzantine-resilient dis-
tributed optimization algorithms,” ArXiv, vol. abs/2305.10810, 2023.
31