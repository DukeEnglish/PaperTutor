LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D
in Large Multimodal Models
FengLi1,2∗, RenruiZhang1,3∗, HaoZhang1,2∗, YuanhanZhang1,4, BoLi1,4, WeiLi1, ZejunMa1, ChunyuanLi1
1ByteDance 2HKUST 3CUHK 4NTU ∗Corecontributor
https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/
Figure1. Performancecomparisoninthreeinterleavedscenarios,includingmulti-image,multi-frame(video),andmulti-view(3D).Our
LLaVA-NeXT-InterleavemodelachievesSoTAperformanceacrossavarietyofevaluationbenchmarks.
Abstract ments, LLaVA-NeXT-Interleave achieves leading results in
multi-image,video,and3Dbenchmarks,whilemaintaining
Visualinstructiontuninghasmadeconsiderablestrides
theperformanceofsingle-imagetasks. Besides,ourmodel
in enhancing the capabilities of Large Multimodal Mod-
also exhibits several emerging capabilities, e.g., transfer-
els (LMMs). However, existing open LMMs largely fo-
ring tasks across different settings and modalities. Code
cus on single-image tasks, their applications to multi- is available at https://github.com/LLaVA-VL/
image scenarios remains less explored. Additionally, LLaVA-NeXT.
prior LMM research separately tackles different scenar-
ios, leaving it impossible to generalize cross scenarios
with new emerging capabilities. To this end, we intro-
1.Introduction
duceLLaVA-NeXT-Interleave,whichsimultaneouslytack-
lesMulti-image,Multi-frame(video),Multi-view(3D),and Recent advancements in Large Multimodal Models
Multi-patch (single-image) scenarios in LMMs. To enable (LMMs) [11,12,26,37,43,64] have showcased impres-
these capabilities, we regard the interleaved data format sive capabilities in diverse multimodal contexts, advanc-
asageneraltemplateandcompiletheM4-Instructdataset ing the pursuit of artificial general intelligence. With ex-
with 1,177.6k samples, spanning 4 primary domains with tensivevision-languagedata[46,47], they empowerLarge
14 tasks and 41 datasets. We also curate the LLaVA- LanguageModels(LLMs)[5,8,52,53]withvisualmodal-
Interleave Bench to comprehensively evaluate the multi- itybyaligningvisionencoders[9,44,45]. Thisintegration
image performance of LMMs. Through extensive experi- haspropelledforwardthefieldofAI,enablingcomplexim-
4202
luJ
01
]VC.sc[
1v59870.7042:viXraageandlanguageunderstandingtaskstobeperformedwith different multi-image tasks compared to the previ-
unprecedentedaccuracy. ousSoTA,whilemaintainingthesingle-imageperfor-
However,mostopen-sourceLMMs[11,24,34,36]have mance,asexemplifiedinFigure1.
primarily focused on pushing the performance limit of the
• Emerging capabilities with cross-task transfer. By
single-imagescenario, themorecomplexmulti-imagesce-
jointly training on a diverse set of tasks, our model
narios remain largely less explored. This oversight is sig-
showcases emerging capabilities to transfer tasks
nificant given that many real-world applications demand
across different settings and modalities. e.g., from
multi-image capabilities, such as comprehensive multi-
spottingdifferencesbetweenimagestovideos.
imageanalyses. Traditionally,researchershaveapproached
these challenges by training separate task-specific mod-
2.RelatedWork
els for each application scenario, e.g., multi-image [1,
19,27], video [7,29,66], and 3D [14,15,58]. This is
Interleaved Image-text Training Data. As a more gen-
bothlabor-intensiveandtime-consuming,resultinginfrag-
eral format, interleaved image-text data can enable LMMs
mentedmethodologiesthatareinefficientandoftenunscal-
with two distinctive capabilities: multimodal in-context
able. Consideringthediverserangeofcomputervisionset-
learning(ICL)capabilityandinstruction-followingcapabil-
tings and data formats, there is a pressing need to develop
ityinreal-worldmulti-imageapplicationscenarios.Thefor-
ageneralframeworkforLMMsthatcanoperateeffectively
mer in-context scenarios interleave several image-text ex-
acrossthesevariedcontexts.
amples within the prompt as task demonstrations, adapt-
Inthispaper,weobservethattheimage-textinterleaved
ing LMMs to new tasks in the inference stage in a few-
format can naturally serve as a general data template to
shot manner. Flamingo [1] is first model to demonstrate
unifydifferentscenarios,e.g.,single-imageormulti-image
this capability, and thus is considered as GPT-3 moment
as special cases, video as multi-frames, and 3D as multi-
formultimodalcommunity. Typically,themultimodalICL
views, as illustrated in Figure 2. Therefore, we present
ability is emerged after pre-training on web-scale raw in-
LLaVA-NeXT-Interleave, an all-around LMM that extends
terleaved image-text sequences. In the open-source com-
the model capabilities to various real-world settings such
munity, MMC4 [67] introduces a public 101.2M inter-
as Multi-image, Multi-frame (videos), Multi-view (3D)
leaved dataset spanning everyday topics, OBELICS [22]
whilemaintainstheperformanceoftheMulti-patch(single-
alsopresentsafiltereddatasetcomprising141Minterleaved
image)performance. WedenotethefoursettingsasM4.
webpages. Kosmos-1[18]curatesa71Mmultimodalcor-
The core innovation of our approach lies in the per-
pora, including arbitrarily interleaved documents. To ex-
spective to leverage an image-text interleaved format as a
plicitlyenabletheICLcapability,MIMIC-IT[25]proposes
universal data template capable of accommodating differ-
an automatic pipeline to create 2.8M multimodal samples
ent scenarios, and construct the related visual instruction-
in the instruction-tuning stage. On the other hand, the lat-
following data. This perspective not only simplifies the
ter multi-image scenarios aim to tackle diverse real-world
trainingprocessacrossvariousdomains,butalsoallowthe
applicationsscenariosthatinvolvemulti-images. Thetrain-
modeltoemergenewcapabilitiesduetocross-domaintask
ingdataofVPG-C[27]collected4newdatasetswithChat-
composition.
GPT.Mantis-Instruct[19]compilesexisting11interleaved
Ourcontributionsaresummarizedasbelow:
datasets and creates 4 new datasets. The proposed M4-
• Interleave data format unifies different tasks. We Instruct [19]compilesexisting41interleaveddatasetsand
convertmulti-image,video,3D,andsingle-imagedata creates 6 new datasets, covering a much higher scenarios
all into an interleaved training format, which unifies diversitythanMantis-Instruct.
differenttasksinasingleLMM.
Interleaved LMMs. As representative closed-source
• New dataset and benchmark. We compile a high-
LMMs, both GPT-4V [42] and Gemini [12] support real-
quality training dataset, M4-Instruct, with 1177.6
world multi-image application scenarios with leading per-
samplestoempowerLMMswiththeM4capabilities,
formance. With various public datasets aforementioned,
which spans 4 primary domains (multi-image, video,
thecommunityhasdevelopedopen-sourceLMMsequipped
3D, and single-image) with 14 tasks and 41 datasets.
with remarkable multi-image proficiency. The ICL per-
We also curate LLaVA-Interleave Bench, a diverse
formance is typically considered to evaluate multimodal
setofbenchmarkstoevaluatethemulti-imageperfor-
pre-training, which has been adopted in several known
mance, including 7 newly collected and 13 existing
LMMs, such as OpenFlamingo [2], IDEFICS series [22,
in/out-domainbenchmarks.
23], VILA [33] and MM1 [41], Emu2 [51]. Otter [25]
• SoTA performance. With a single model, LLaVA- is initialized from OpenFlamingo, and is fine-tuned on
NeXT-Interleave can achieve leading results across the MIMIC-IT dataset to further improve ICL ability withExtract multi views Split into patches
Extract multi frames
Multi-image Multi-Frame (Video) Multi-View (3D) Multi-Patch (Single-image)
(a) (b) (c) (d)
Figure2. TasksinourM4-Instruct. (a)showcasesanexampleofinterleavedmulti-imagescenarios(visualstorytelling). (b),(c),and(d)
indicatethatvideo,3Dandsingle-imagedatacanalsobeorganizedastheinterleaveddataformatforunifiedprocessing.
instruction-tuning. In contrast, the use of instruction- 3.InterleavedMulti-imageTasks&Data
tuning in LMMs for various real-world multi-image ap-
3.1.TaskOverview
plications has been less explored, despite of Mantis [19].
The proposed LLaVA-NeXT-Interleave not only broadens We observe different computer vision scenarios can be
the multi-image scenario itself as demonstrated by the im- generally represented by the interleaved multi-image for-
provedexperimentalresults,butalsogeneralizethesettings mat, such as video, 3D, and single-image data. Therefore,
to diverse scenarios with one model, e.g., video, 3D, and to endow LLaVA-Interleave with diverse capabilities, as
single-image.Thecross-scenariotrainingleadstoemerging shown in Figure 2, we adopt the interleaved multi-image
capabilities, achieving zero-shot task composition in new formattounifythedatainputofthefollowingfourtasks:
multi-imagecontexts.
Multi-imagescenariosincludevisualinstructionsincorpo-
rating interleaved vision-language input with multiple im-
Interleaved Benchmarks. To assess the interleaved ages. This setting covers 12 challenging real-world tasks
multi-image capabilities of LMMs, there have been sev- included in our training data, such as spotting the differ-
eralhigh-qualitybenchmarksinvariousscenarios. TheICL ence,visualstorytelling,imageeditinginstructiongenera-
benchmarks [20,49] for LMMs comprehensively evaluate tion,interleavedmulti-imagedialogue,multi-imagepuzzle,
theirinterleavedskillsfromfew-shottomany-shotsettings. low-levelmulti-imageassessment,etc.
For the more challenging multi-image scenarios, previous Multi-frame scenarios refer to taking video as input data
worksmainlyfocusonaspecificdomainforevaluation,in- bysamplingitintomultipleframes,preservingtemporalvi-
cluding NLVR2 [50] for daily-life VQA, MMMU [61] for sualcuesacrossthemulti-imagesequence. Wemainlyfo-
colleague-level problem-solving, MathVerse-mv [65] and cuson2tasks: videodetailedcaptioningandvideoVQA.
SciVerse-mv [13] for mathematical and scientific reason-
Multi-view scenarios depict 3D environments by multi-
ing,BLINK[10]tochallengeLMMs,andMantis-Eval[19]
view images from different perspectives, where the visual
formulti-imageunderstanding. TofurtherevaluateLMMs
correspondence and disparity can indicate spatial informa-
on a collection of multi-image scenarios, DEMON [27] is
tion in the 3D world. For 3D perception, we include 2
the first benchmark that compiles dozens of datasets with
tasks: embodied VQA (dialogue and planning), and 3D
477Ksamples. Withthelargeamountofdataandhighdi-
sceneVQA(captioningandgrounding).
versity,DEMONlaysagoodfoundationformulti-imagere-
search. Unfortunately, it also inherits a significant amount Multi-patch scenarios represent the conventional single-
of low-quality data samples from existing datasets. To fa- imagetasks. Withthedesignof‘anyresolution’inLLaVA-
cilitate evaluation, the proposed LLaVA-Interleave Bench NeXT[36],wedivideahigh-resolutionimageintomultiple
curatehigh-qualitysamples,comprisingbothspecific(syn- low-resolution patches for efficient visual encoding, com-
thetic, mathematical, low-level) and general (daily, real- patiblewithourinterleavedmulti-imageformat.
world, text-rich) multi-image scenarios. With 9 newly cu-
3.2.M4-Instruct
rated and 13 existing datasets, we categorize them into in-
domain (12.9K) and out-domain (4.1K) schemes. Con- To empower all-round multi-image capabilities, we
current multi-image evaluation benchmarks include Muir- meticulously curate a comprehensive training dataset in-
Bench[54]andReMI[21]. cluding 1177.6K instances, termed M4-Instruct, widelyFigure3.TaskexamplesofM4-Instruct,containingdiversescenariosinmulti-image,multi-frame(video),andmulti-view(3D).
Figure4.M4-Instructtrainingdatastatistics. Figure5.LLaVA-InterleaveBenchstatistics.
spanningmulti-image,multi-frame,andmulti-viewscenar- MON [27] and Mantis [19]. On top of that, we also uti-
ios with 14 tasks and 41 datasets, along with multi-patch lize GPT-4V [43] to annotate 3 new tasks to enable more
datatopreservebasicsingle-imageperformance.Weshow- diverse capabilities, i.e., Real-world Difference, Synthetic
casetaskexamplesofthefirstthreescenariosinFigure3. Difference,andTwitterPost. Forthevideodata,wecollect
We exhibit a data overview of M4-Instruct in Figure 4, a 255K subset from LLaVA-Hound [63], including 240K
and the detailed data statistics in Table 15. For the multi- video VQA and 15K video detailed captioning. We also
imagedata,mostofthedatasetsarecollectedfromprevious includeNExT-QA[57]andSTAR[55]toexpandourvideo
publiceffortsandrigorouslyconvertedintoourunifiedfor- trainingdata.Forthe3Ddata,wewidelygatherthetraining
mat with task-specific instructions, some inspired by DE- setfromnuScenesQA[6],ALFRED[48],ScanQA[3],and3D-LLM [16], covering both outdoor and indoor scenar- place they are originally in, i.e., the positions of ⟨image⟩,
ios. Forthesingle-imagedata,werandomlysample40%of denotedas‘interleavedformat’.Inthisway,LLaVA-NeXT-
thestage-2fine-tuningdatafromLLaVA-NeXT[24],which Interleavesupportsmoreflexibleinferencemodes,exhibit-
aimstopreservethesingle-imagecapacity. ingrobustnesstodifferentinputformats.
To comprehensively evaluate the interleaved multi-
image performance, we introduce the LLaVA-Interleave Technique 3: Combining different data scenarios
Bench for LMMs, consisting of 13 challenging tasks with improves individual task performance. Most exist-
17K instances. We present a data overview of the bench- ing works conduct supervised fine-tuning with only one
markinFigure3,andthedetaileddatastatisticsinTable16. type of data source, e.g., multi-image tuning of Man-
Indetail,wecategorizemulti-imagetasksintotwoclasses: tis [19] and multi-frame tuning of LLaMA-VID [31]. In-
stead, we utilize the M4-Instruct to simultaneously con-
• In-domain Evaluation includes tasks that have been
duct instruction tuning with four different tasks (multi-
‘seen’ during our training, designed to verify the
image/frame/view/patch). With a unified interleaved for-
model performance within familiar scenarios. We
mat, distinct data scenarios have the potential to provide
adopt 5 newly curated multi-image tasks correspond-
complementary semantics and instruction-following capa-
ingtotrainingdatasets,and2existingbenchmarks,Q-
bilities.
Bench[56]andNLVR2[50],with12.9Kintotal.
5.Experiments
• Out-domainEvaluationinvolvestasksthatdon’tover-
lapwithtrainingscenarios,aimingtorevealthegener-
InSection5.1,wefirstintroduceourevaluationschemes
alizationcapacityofLMMs. Weconstruct2newtasks
and implementation details. Then, in Section 5.2, we re-
for multi-image mathematical (MathVerse [65]) and
portandanalyzethequantitativeresultsinfourinterleaved
scientific(SciVerse[13])comprehension,andutilize3
multi-imagescenarios.
existing benchmarks, Mantis-Eval [19], BLINK [10],
andMMMU[60],with4.1Kintotal. 5.1.Settings
Evaluation Schemes. We evaluate our LLaVA-NeXT-
4.InterleavedVisualInstructionTuning
Interleave model on four real-world interleaved scenarios,
i.e.,multi-image,multi-frame(video),multi-view(3D),and
Inthissection,weintroduceseveralkeytechniquesdur-
multi-patch(single-image).
ing the interleaved visual instruction tuning of LLaVA-
NeXT-Interleave. For architecture designs, we follow • For multi-image evaluation, we adopt the proposed
LLaVA-NeXT [24] to adopt the most general framework, LLaVA-Interleave Bench covering comprehensive in-
i.e.,avisionencoder[62],anintermediateprojector,anda domainandout-domaintasks.
powerful LLM [4]. Then, we consider the following three
techniquestoachieveimprovedmulti-imageperformance. • For video evaluation, we utilize the existing NExT-
QA [57], MVBench [30], Video Detailed Descrip-
tion (VDD) [66], and ActivityNet-QA (Act) [59].
Technique1:Continuetrainingfromsingle-imagemod-
For ActivityNet-QA, we present both the accuracy
els. Theinterleavedmulti-imagetaskscanberegardedas
and GPT score (Acc/Score). We also evaluate on
an extension of single-image scenarios, more flexible in
VideoChat-GPT (VCG) [40] with five metrics: CI
formats and challenging in reasoning. Therefore, to bet-
(CorrectnessofInformation),DO(DetailOrientation),
ter leverage the pre-trained single-image proficiency, we
CU (Context Understanding), TU (Temporal Under-
adoptanoff-the-shelfLLaVA-NeXT-Image[24]asthebase
standing),andCO(Consistency).
model, which has gone through a stage-1 image-caption
pre-trainingandastage-2single-imagefine-tuning. Ontop • For 3D evaluation, we select ScanQA [3], two tasks
of this model, we perform the interleaved multi-image in- from 3D-LLM [16], i.e., 3D-assisted Dialogue and
structiontuningwithourM4-Instructdataset. TaskDecomposition, andalsocuratetwonewtestset
fromnuScenesVQA[6]andALFRED[48].
Technique 2: Mixed Interleaved data formats during
training. We adopt two format choices for the positions Implementation Details. Following the same architec-
of image tokens during the interleaved multi-image train- ture in LLaVA-NeXT [24], our LLaVA-NeXT-Interleave
ing. Thefirstistoplacealltheimagetokensinfrontofthe adopts Qwen 1.5 [5] as the base LLM with 0.5B, 7B and
prompt,whilemaintainingtheplaceholders⟨image⟩within 14B parameters, SigLIP-400M [62] with 384×384 resolu-
thetext, denotedas‘In-the-frontformat’. Thesecondpre- tionsasthevisionencoder,andatwo-layerMLPasthepro-
serves the interleaved format to put image tokens in the jectionlayer.In-domainEvaluation Out-domainEvaluation
Model
Avg SD IE VST TRVQA MIVQA Puzzle QB NLVR2 Avg Math Sci Mantis BLINK MMMU-mv
GPT-4V[43] 39.2 12.5 11.0 10.9 54.5 52.0 17.1 76.5 88.8 57.8 60.3 66.9 62.7 51.1 47.9
LLaVA-NeXT-Image(7B)[36] 32.4 12.9 13.2 10.1 59.6 39.4 9.0 51.0 68.0 29.4 13.5 12.2 46.1 41.8 33.5
VPG-C(7B)[28] 35.8 27.8 15.2 21.5 38.9 46.8 2.4 57.6 73.2 34.5 24.3 23.1 52.4 43.1 29.4
Mantis(7B)[19] 39.6 17.6 11.2 12.5 45.2 52.5 25.7 69.9 87.4 39.3 27.2 29.3 59.5 46.4 34.1
LLaVA-NeXT-Interleave
0.5B 43.9 34.3 21.6 29.7 63.9 54.8 35.4 52.0 67.8 33.1 24.7 27.6 45.6 39.2 28.6
7B 58.6 37.1 24.3 33.1 76.1 87.5 48.7 74.2 88.8 42.8 32.8 31.6 62.7 52.6 34.5
14B 62.3 40.5 24.5 33.3 78.6 95.0 59.9 76.7 91.1 44.3 33.4 32.7 66.4 52.1 37.1
Table1.ResultsonourLLaVA-InterleaveBench.SD:SpottheDifference,IE:ImageEditInstruction,VST:VisualStoryTelling,TRVQA:
Text-richVQA,MIVQA:Multi-imageVQA,QB:Q-Bench,SQ:ScanQA,Math:MathVerse-mv,Sci:SciVerse-mv.
VideoChat-GPT
Model NExTQA MVBench ActivityNet-QA VDD
CI DO CU TU CO
GPT-4V[43] - - - 4.00 4.09 3.88 4.37 3.94 4.02
VideoChatGPT(7B)[39] - - 35.2/2.70 - 2.40 2.52 2.62 1.98 2.37
Video-LLaVA(7B)[32] - - 45.3/3.30 - 2.87 2.94 3.44 2.45 2.51
VISTA-LLaMA(7B)[38] - - 48.3/3.30 - 2.44 2.31 2.64 3.18 2.26
VideoChat2(7B)[29] 68.6 51.9 49.1/3.30 - 3.02 2.88 3.51 2.66 2.81
LLaMA-VID(7B)[31] - 50.2 47.4/3.30 2.84 3.01 2.97 3.54 2.53 2.60
LLaVA-NeXT-Video(7B)[66] - - 53.5/3.20 3.32 3.39 3.29 3.92 2.60 3.12
LLaVA-NeXT-Video-DPO(7B) - - 60.2/3.50 3.72 3.64 3.45 4.17 2.95 4.08
LLaVA-NeXT-Video-DPO(34B) - - 64.4/3.60 3.84 3.81 3.55 4.24 3.14 4.12
LLaVA-NeXT-Interleave
0.5B 59.5 45.6 48.0/2.84 3.25 3.12 2.97 3.62 2.36 3.27
7B 78.2 53.1 55.3/3.13 3.57 3.51 3.28 3.89 2.77 3.68
14B 79.1 54.9 56.2/3.19 3.59 3.65 3.37 3.98 2.74 3.67
7B(DPO) 77.9 52.3 55.0/3.13 3.90 3.99 3.61 4.24 3.19 4.12
Table2. Resultsonmulti-frame(video)benchmarks. VDD:VideoDetailedDescription. CI(CorrectnessofInformation), DO(Detail
Orientation),CU(ContextUnderstanding),TU(TemporalUnderstanding),andCO(Consistency).
In-domainEvaluation Model LLM Avg AI2D ChartQADocVQAMME SciQA POPE
Model
3D-assisted Task ScanQA ALFRED nuScenes
Avg Single 59.8 51.7 50.2 59.1 52.8 60.0 85.4
Dialogue Decomposition (val) VQA 0.5B
Interleave 60.5 52.2 52.2 59.2 52.0 60.6 86.8
Flamingo[1] 20.5 27.9 33.2 31.1 5.3 4.9
GPT-4V[43] 34.6 31.2 35.4 32.6 10.3 63.7 Single 72.3 72.7 66.3 75.6 61.0 71.1 86.9
Point-Bind&LLM[14] 22.5 38.3 35.8 34.6 0.6 3.3 7B
Interleave 73.3 73.9 67.2 75.7 63.5 72.6 86.8
3D-LLM[17] 22.9 39.3 37.8 35.7 1.4 0.4
Mantis(7B)[19] 18.7 2.60 14.7 16.1 14.0 46.2 Single 77.2 77.5 72.1 80.0 67.7 78.9 87.3
14B
LLaVA-NeXT-Interleave Interleave 76.4 76.5 71.2 78.9 66.2 77.4 87.9
0.5B 53.0 67.2 48.5 29.3 57.0 62.8
7B 58.2 69.3 51.4 32.2 61.6 76.5
Table4. Resultsonmulti-patch(single-image)benchmarkswith
14B 59.2 70.6 52.2 34.5 62.0 76.7
different LLM sizes. ‘Single’ and ‘Interleave’ denote LLaVA-
Table 3. Results on 3D benchmarks. 3D-assisted Dialogue and NeXT-Imageandourmodel,respectively.
TaskDecompositionareevaluationtasksfrom3D-LLM.
Interleave also showcases superior generalization capac-
5.2.MainResults ity within novel scenarios, e.g., comparable to GPT-4V on
Mantis-EvalandBLINK.
Multi-image Results. As reported in Table 1, the aver-
age multi-image performance of LLaVA-NeXT-Interleave
surpassespreviousopen-sourcemodelsinbothin-andout- Multi-frame (Video) Results. Compared with previous
domainbenchmarks. Forin-domainevaluation, ourmodel video-based LMMs under similar model sizes, LLaVA-
demonstratessignificantadvantagesacrossvarioustasksas NeXT-Interleaveachievessuperiorresultsonmanybench-
expected, due to the multi-image instruction tuning with marksinTable2,thoughnotspecificallydesignedforvideo
M4-Instruct. For out-domain evaluation, LLaVA-NeXT- tasks. We also follow LLaVA-Hound to add DPO trainingMulti-image Multi-frame Multi-view Single-image
Continuetraining
Mantis BLINK QB NLVR2 Act MVB VDD ScanQA AI2D ChartQA DocVQA MME* POPE SQA
Fromstage-1pre-training 41.0 37.6 47.0 54.0 44.7/2.17 43.0 2.96 27.7 46.3 38.3 47.5 47.1 85.4 59.4
Fromsingle-imagemodels 45.6 39.2 52.0 67.8 48.0/2.84 45.6 3.25 29.3 52.2 52.2 59.2 52.0 86.8 60.6
Table 5. Ablation on whether to continue training from single-image models. QB: Q-Bench, Act: ActivityNet-QA, MVB: MVBench,
VDD:VideoDetailedDescription,MME*: Throughoutourpaper,weconvertMME’sscoretoaccuracybysumminguptheperception
andcognitionscoresanddividing2800,SQA:Scienceqa-IMG.
VideoChatGPT
Training Inference Spotthe VisualStory Text-rich Data NExT-QA VDD
Setting Setting Avg Difference Telling VQA Q-Bench CI DO CU TU CO
Video 42.6 3.46 3.47 3.27 3.87 2.74 3.61
Interleaved 52.9 36.8 30.5 70.1 74.0
In-the-front Video+Single-image 67.7 3.49 3.46 3.30 3.85 2.71 3.60
In-the-front 54.3 36.6 32.8 74.7 75.3
Video+Multi-image 77.7 3.50 3.50 3.31 3.90 2.70 3.63
Interleaved 55.4 37.8 32.9 76.2 76.0 Video+Both 78.2 3.58 3.50 3.27 3.87 2.77 3.68
Interleaved
In-the-front 52.4 36.1 29.0 72.9 71.8
Mixed Interleaved 57.0 38.3 32.5 78.1 76.9 Table7. Ablationontheimprovementofcombineddata
In-the-front 56.6 37.9 32.5 78.4 76.3
scenariosforvideotasks.CI(CorrectnessofInformation),
DO (Detail Orientation), CU (Context Understanding),
Table6.Ablationonmixedinterleaveddataformats.Weselect
TU(TemporalUnderstanding),andCO(Consistency).
severaltaskswithinLLaVA-InterleaveBenchforablation.
after our M4-Instruct tuning. After adding DPO, our 7B • In Table 5, we compare training strategies. It is seen
modelattainsSoTAperformanceonVDDandVideoChat- that initialization from a good single-image model
GPT benchmarks, surpassing the previous LLaVA-NeXT- checkpoint (from Stage-2) can consistently enhance
Video (34B). This demonstrates the effective temporal un- theinterleavedmulti-imageperformance,thandirectly
derstandingandreasoningcapabilitiesofourmodelacross fromaStage-1modelcheckpoint.
sequentialframes.Notethatwecalculatetheaveragescores
• In Table 6, our mixed-format training can benefit the
by multiplying a weight of 10 times by the score of Video
resultsofbothtwoinputformats.
DetailedDescriptionandVideoChat-GPT.
• InTable7,weprogressivelyincorporatesingle-image
Multi-view (3D) Results. For 3D perception in Table 3, andmulti-imagedatauponthevideodata.Theintegra-
our model also obtains leading results for both indoor and tion of more sources contributes to enhanced perfor-
outdoor scenarios on five in-domain benchmarks. Com- mance, compared with models from individual visual
pared to 3D-LLM and Point-LLM with additional point scenarios.
clouds as input, LLaVA-NeXT-Interleave only accepts
multi-viewimagestointerpretthe3Dworld,attainingsig- 6.EmergingCapabilities
nificantlyhigherscoresinchallenging3Dscenarios.
In this section, we show some example to demonstrate
theemergingcapabilitiesofourmodel. Emergingcapabil-
Multi-patch (single-image) Results. We also add 307k ities means the capabilities do not trained during training
(40%) of original LLaVA-NeXT single-image data, which butdemonstratedwheninference. Wemainlyshowcasethe
makesourmodelcapableofdoingsingle-imagetasks. We emergingcapabilitiesfromthreeaspects:
usetheanyrestrainingforsingle-imagedata,whichdivides
1. Task Transfer from Single-image to Multi-image:
an image into multiple patches, forming another multi-
The capability to reason over one image and tell the
imagesetting.AsshowninTable4,wemaintainthesingle-
funny part is initially observed in single-image mod-
image performance of LLaVA-NeXT-Image. As single-
els[35],andnotincludedinourmulti-imagetraining.
image data is of high quality and diversity, adding single-
AsshowninTable8, ourmodeliscapableofanalyz-
image data also improves the instruction-following ability
ingthefunpartwithinmultipleimages. Thisnewtask
andenablestasktransferfromsingle-imagetomulti-image,
isprobablyemergedbythecompositionofthesingle-
whichisdemonstratedinSection6.
imagecapabilityandmulti-imageVQAtraining.
5.3.AblationsofProposedTechniques
2. Task Transfer from Image to Video: We only in-
Westudytheeffectivenessofthethreeproposedtraining clude the multi-image Twitter post task in the M4-
techniquesinSection4asbelow. Instructtraining,whileourmodelcandirectlyperformthewitterpostfromavideo,asshowninTable9. This
newtaskisprobablycomposedbythetrainingdataof
multi-imageTwitterpostandvideoVQAtasks.
3. Real-world Applications: In Tables 10, 11, and 12,
we showcase three real-world scenarios that are not
explicitly contained in our interleaved training data,
whicharemulti-imagepaintingstylerecognition,PPT
summary & QA, and multi-doc VQA. This demon-
stratesourgeneralizationpotentialstoabroaderspec-
trumofapplications.
Moreinterestingdemoscanbefoundinourprojectpage1.
7.Conclusion
In conclusion, our research highlights the transforma-
tive potential of LLaVA-NeXT-Interleave in unifying and
advancing the capabilities of Large Multimodal Models
(LMMs) across diverse visual tasks. By leveraging the in-
terleaveddataformat,weeffectivelyintegratemulti-image,
video, 3D,andsingle-imagescenarios, offeringacohesive
approach to handling thwoese varied challenges. The in-
troduction of the comprehensive M4-Instruct dataset and
the LLaVA-Interleave Bench provides a solid foundation
for training and evaluating LMMs, ensuring high-quality
performance across multiple domains. Our extensive ex-
periments validate that LLaVA-NeXT-Interleave not only
sets new state-of-the-art benchmarks in multi-image tasks
butalsomaintainsexceptionalperformanceinsingle-image
tasks. Furthermore, the model exhibits promising emerg-
ingcapabilities, suchascross-tasktransfer, showcasingits
versatilityandpotentialforbroaderapplications. Thiswork
setsanewprecedentinthefield,pavingthewayforfuture
advancementsinmultimodalAIandcomplexvisualunder-
standingtasks.
1
https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/TaskTransferfromSingle-imagetoMulti-image
User Whatisfunabouttheimages?
Ours Thehumorintheseimagescomesfromtheanthropomorphizedbirdsengaginginhuman-likeactivitiesandconversations.
Inthefirstimage,onebirdisaskingaboutgoingtogetsomefriesonthepier,whichisahumanactivity.Thesecondimage
showsabirdaskingaboutthe”culminatingresultofconsciousness,”whichisaphilosophicalquestionratherthanaliteral
one. Thethirdimagehasabirdexpressingitsdesiretodosomethingwiththefriesonthepier,againreferencingahuman
activity.Theanthropomorphismofthebirdsaddsalayerofhumorbyattributinghumanthoughtsanddesirestoanimals.
Table8.Anexampleofemergingcapability:multi-imagefunpartanalyze.
TaskTransferfromImagetoVideo
User Helpmewriteatwitterposttodescribethisvideo.
Ours ”Feelingthechillintheair,butthecherryblossomsareasighttobehold! Walkingdownthestreet,eachpersonbundled
upagainstthewinterchill. Thesnow-coveredtreesandmarketstallsaddtotheserenebeautyofthiswinterwonderland.
#CherryBlossoms#WinterWalk#CityScenes”.
Table9.Anexampleofemergingcapability:Twitterpostforvideos.
Real-WorldApplications
User Doyoukownwhodrawthesepaintings?
Ours Yes,thesepaintingsarebytherenownedartistVincentvanGogh.Thefirstpaintingishisfamouswork”TheStarryNight,”
whichdepictsaswirlingnightskywithabrightcrescentmoonandstars.Thesecondpaintingishisself-portrait,whichshows
himsmokingapipeandwearingahat. Thethirdpaintingisalsoaself-portrait,butitfeaturesadifferentexpressionand
hairstyle.Thefourthpaintingisbyanotherartist,notVanGogh,anddepictsapersonseatedwithflowersinthebackground.
Table10.Anexampleofemergingcapability:recognizingartists’paintingstyles.Real-WorldApplications
User TheimagesarethePPTofiPhone15review.canyousummarizethemaininformation?
Ours ThePPTfeaturesasleekdesignwithablackbackgroundandincludestextandimagesrelatedtotheiPhone15.Ithighlights
thephone’sinnovativefeaturessuchasacustomdualglassbackcover,adynamicislanddisplayfordeliverytracking,and
aUSB-Ccompatiblefastchargingcable.Thecoloroptionsareshowcasedinagradientofpink,green,gray,andonyx.The
presentationalsoemphasizesthephone’spowerfulprocessors,includingtheA16BionicchipandA17Prochip.
User WhatisthepriceofiPhone15plus?
Ours ThepriceofiPhone15plusisRs.89,000asmentionedinthePPTslide.
User Whythedynamicislanduseful?
Ours Thedynamicislandisusefulfordeliverytracking,asitallowstheusertoknowwhentheirorderwillarriveandifthereare
anydelays.Italsoprovidesanewwaytointeractwiththephone’sinterface,makingiteasiertonavigateandaccessvarious
features.
Table11.Anexampleofemergingcapability:PPTsummary&VQA.Real-WorldApplications
User Hi,therearetwocandidates,canyouprovideabriefdescriptionforeachofthemforme?
Ours Certainly! Here’sabriefdescriptionofeachcandidate: 1. OprahWinfrey: -ProfessionalExperience:
Knownforhertalkshow,”TheOprahWinfreyShow,”andherworkon”COVID-19”asapresenteron
AppleTV.-Skills: Emphasizedonemotionalintelligence,interpersonalskills,andcreativity. -Honors
& Awards: Notable achievements include the Golden Globe Award for Best Television Series and the
GoldenGlobeAwardforBestTelevisionSpecial. 2. SteveJobs: -ProfessionalExperience: Co-founder
ofAppleInc.,whereheledthedevelopmentoficonicproductsliketheiMac,iPod,andiPhone. -Skills:
Highlightedasathinkerwithafocusoninnovationandproblem-solving.-Honors&Awards:Recognized
withnumerousaccolades,includingtheMacArthurFellowshipandtheNeXTComputerScienceAward.
Pleasenotethatthesedescriptionsarebasedontheinformationprovidedintheimagesanddonotinclude
anypersonaldetailsorspecificaccomplishmentsmentionedintheimages.
User WhichofthembettersuitstheroleoftheCongressmanoftheUS?Why?
Ours Thefirstresume,featuringOprahWinfrey,wouldbettersuittheroleofaCongressmanbecauseithighlights
skillsandexperiencesthatarerelevanttopoliticaloffice,suchasleadership,publicspeaking,andmediaex-
perience.Thesequalitiesareessentialforapoliticiantoeffectivelyrepresenttheirconstituentsandengage
inlegislativeprocesses.Thesecondresume,whileitcouldbesuitableforacreativeorentrepreneurialrole,
doesnotspecificallymentionskillsorexperiencesthatalignwiththeresponsibilitiesofaCongressman.
Table12.Anexampleofemergingcapability:multi-docVQA.References Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al.
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
An image is worth 16x16 words: Transformers
Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
for image recognition at scale. arXiv preprint
Arthur Mensch, Katie Millican, Malcolm Reynolds,
arXiv:2010.11929,2020.
etal. Flamingo: avisuallanguagemodelforfew-shot
[10] XingyuFu,YushiHu,BangzhengLi,YuFeng,Haoyu
learning. arXivpreprint,2022.
Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-
[2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hes-
Chiu Ma, and Ranjay Krishna. Blink: Multimodal
sel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,
largelanguagemodelscanseebutnotperceive. arXiv
Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.
preprintarXiv:2404.12390,2024.
Openflamingo: An open-source framework for train-
[11] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu,
ing large autoregressive vision-language models.
Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie
arXivpreprintarXiv:2308.01390,2023.
Geng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling
[3] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and
dataandparametersforafamilyofmulti-modallarge
Motoaki Kawanabe. Scanqa: 3d question answering
language models. arXiv preprint arXiv:2402.05935,
forspatialsceneunderstanding. Inproceedingsofthe
2024.
IEEE/CVFconferenceoncomputervisionandpattern
[12] Google Gemini Team. Gemini: a family of
recognition,pages19129–19139,2022.
highly capable multimodal models. arXiv preprint
[4] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,
arXiv:2312.11805,2023.
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
[13] ZiyuGuo,RenruiZhang,HaoChen,JialinGao,Peng
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Gao,HongshengLi,andPheng-AnnHeng. Sciverse.
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
https://sciverse-cuhk.github.io,2024.
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong [14] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen
Tu,PengWang,ShijieWang,WeiWang,Shengguang Tang,XianzhengMa,JiamingHan,KexinChen,Peng
Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Gao, Xianzhi Li, Hongsheng Li, et al. Point-bind &
Yang,ShushengYang,YangYao,BowenYu,Hongyi point-llm: Aligning point cloud with multi-modality
Yuan,ZhengYuan,JianweiZhang,XingxuanZhang, for3dunderstanding, generation, andinstructionfol-
YichangZhang, ZhenruZhang, ChangZhou, Jingren lowing. arXivpreprintarXiv:2309.00615,2023.
Zhou,XiaohuanZhou,andTianhangZhu.Qwentech- [15] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao,
nicalreport. arXivpreprintarXiv:2309.16609,2023. PengXu,HanXiao,KaipengZhang,ChrisLiu,Song
[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Wen,ZiyuGuo,etal. Imagebind-llm: Multi-modality
SinanTan,PengWang,JunyangLin,ChangZhou,and instruction tuning. arXiv preprint arXiv:2309.03905,
Jingren Zhou. Qwen-vl: A versatile vision-language 2023.
model for understanding, localization, text reading, [16] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong
andbeyond. arXivpreprintarXiv:2308.12966,2023. Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan.
[6] Ankan Bansal, Yuting Zhang, and Rama Chellappa. 3d-llm: Injecting the 3d world into large language
Visual question answering on image sets. In Com- models. Advances in Neural Information Processing
puterVision–ECCV2020:16thEuropeanConference, Systems,36:20482–20494,2023.
Glasgow,UK,August23–28,2020,Proceedings,Part
[17] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong
XXI16,pages51–67.Springer,2020.
Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan.
[7] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, 3d-llm: Injecting the 3d world into large language
Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Yu models,2023.
Qiao, Tong Lu, et al. Videollm: Modeling video se- [18] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
quence with large language models. arXiv preprint Saksham Singhal, Shuming Ma, Tengchao Lv, Lei
arXiv:2305.13292,2023.
Cui, Owais Khan Mohammed, Qiang Liu, Kriti Ag-
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and garwal,ZewenChi,JohanBjorck,VishravChaudhary,
KristinaToutanova.Bert:Pre-trainingofdeepbidirec- SubhojitSom,XiaSong,andFuruWei. Languageis
tionaltransformersforlanguageunderstanding. arXiv not all you need: Aligning perception with language
preprintarXiv:1810.04805,2018. models. ArXiv,abs/2302.14045,2023.
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander [19] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei,
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Inter-leavedmulti-imageinstructiontuning. arXivpreprint [30] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi
arXiv:2405.01483,2024. Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping
Luo, et al. Mvbench: A comprehensive multi-modal
[20] Yixing Jiang, Jeremy Irvin, Ji Hun Wang, Muham-
video understanding benchmark. In Proceedings of
mad Ahmed Chaudhry, Jonathan H Chen, and An-
the IEEE/CVF Conference on Computer Vision and
drew Y Ng. Many-shot in-context learning in
multimodal foundation models. arXiv preprint
PatternRecognition,pages22195–22206,2024.
arXiv:2405.09798,2024. [31] YanweiLi,ChengyaoWang,andJiayaJia.Llama-vid:
Animageisworth2tokensinlargelanguagemodels.
[21] Mehran Kazemi, Nishanth Dikkala, Ankit Anand,
arXivpreprintarXiv:2311.17043,2023.
Petar Devic, Ishita Dasgupta, Fangyu Liu, Bahare
Fatemi, Pranjal Awasthi, Dee Guo, Sreenivas Golla- [32] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning,
pudi,etal. Remi: Adatasetforreasoningwithmulti- Peng Jin, and Li Yuan. Video-llava: Learning united
pleimages. arXivpreprintarXiv:2406.09175,2024. visual representation by alignment before projection,
2023.
[22] Hugo Laurenc¸on, Lucile Saulnier, Le´o Tronchon,
Stas Bekman, Amanpreet Singh, Anton Lozhkov, [33] JiLin,HongxuYin,WeiPing,PavloMolchanov,Mo-
Thomas Wang, Siddharth Karamcheti, Alexander hammad Shoeybi, and Song Han. Vila: On pre-
Rush,DouweKiela,etal.Obelics:Anopenweb-scale training for visual language models. In Proceedings
filtered dataset of interleaved image-text documents. oftheIEEE/CVFConferenceonComputerVisionand
Advances in Neural Information Processing Systems, PatternRecognition,pages26689–26699,2024.
36,2024. [34] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Long-
[23] HugoLaurenc¸on, Le´oTronchon, MatthieuCord, and tian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi
Victor Sanh. What matters when building vision- Shao, Keqin Chen, et al. Sphinx: The joint mix-
languagemodels? arXivpreprintarXiv:2405.02246, ing of weights, tasks, and visual embeddings for
2024. multi-modal large language models. arXiv preprint
arXiv:2311.07575,2023.
[24] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Ren-
rui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and [35] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
ChunyuanLi. Llava-next: Strongerllmssupercharge Lee. Improved baselines with visual instruction tun-
multimodalcapabilitiesinthewild,May2024. ing,2023.
[25] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang, [36] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuan-
Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei han Zhang, Sheng Shen, and Yong Jae Lee. Llava-
Liu.Mimic-it:Multi-modalin-contextinstructiontun- next: Improvedreasoning,ocr,andworldknowledge,
ing. arXivpreprintarXiv:2306.05425,2023. January2024.
[26] Junnan Li, Dongxu Li, Caiming Xiong, and Steven [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and
Hoi.Blip:Bootstrappinglanguage-imagepre-training YongJaeLee. Visualinstructiontuning. InNeurIPS,
forunifiedvision-languageunderstandingandgenera- 2023.
tion. InInternationalConferenceonMachineLearn- [38] FanMa,XiaojieJin,HengWang,YuchenXian,Jiashi
ing,pages12888–12900.PMLR,2022. Feng,andYiYang. Vista-llama:Reliablevideonarra-
[27] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, torviaequaldistancetovisualtokens,2023.
WeiJi,WenqiaoZhang,Tat-SengChua,SiliangTang, [39] Muhammad Maaz, Hanoona Rasheed, Salman Khan,
Hanwang Zhang, and Yueting Zhuang. Fine-tuning and Fahad Shahbaz Khan. Video-chatgpt: Towards
multimodalllmstofollowzero-shotdemonstrativein- detailedvideounderstandingvialargevisionandlan-
structions.InTheTwelfthInternationalConferenceon guagemodels,2024.
LearningRepresentations,2023.
[40] Muhammad Maaz, Hanoona Rasheed, Salman Khan,
[28] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, and Fahad Shahbaz Khan. Video-chatgpt: Towards
WeiJi,WenqiaoZhang,Tat-SengChua,SiliangTang, detailedvideounderstandingvialargevisionandlan-
Hanwang Zhang, and Yueting Zhuang. Fine-tuning guage models. In Proceedings of the 62nd Annual
multimodalllmstofollowzero-shotdemonstrativein- Meeting of the Association for Computational Lin-
structions,2024. guistics(ACL2024),2024.
[29] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen- [41] Brandon McKinzie, Zhe Gan, Jean-Philippe Faucon-
haiWang,PingLuo,YaliWang,LiminWang,andYu nier, Sam Dodge, Bowen Zhang, Philipp Dufter,
Qiao. Videochat: Chat-centric video understanding. Dhruti Shah, Xianzhi Du, Futang Peng, Floris
arXivpreprintarXiv:2305.06355,2023. Weers, et al. Mm1: Methods, analysis & insightsfrom multimodal llm pre-training. arXiv preprint [52] Hugo Touvron, Thibaut Lavril, Gautier Izacard,
arXiv:2403.09611,2024. Xavier Martinet, Marie-Anne Lachaux, Timothe´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Ham-
[42] OpenAI. Gpt-4technicalreport,2023.
bro, Faisal Azhar, et al. Llama: Open and effi-
[43] OpenAI. GPT-4V(ision)systemcard,2023.
cient foundation language models. arXiv preprint
[44] MaximeOquab,Timothe´eDarcet,The´oMoutakanni, arXiv:2302.13971,2023.
HuyVo,MarcSzafraniec,VasilKhalidov,PierreFer- [53] Hugo Touvron, Louis Martin, Kevin Stone, Peter
nandez, Daniel Haziza, Francisco Massa, Alaaeldin Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
El-Nouby, et al. Dinov2: Learning robust vi- Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
sual features without supervision. arXiv preprint Bhosale, et al. Llama 2: Open foundation and fine-
arXiv:2304.07193,2023. tunedchatmodels. arXivpreprintarXiv:2307.09288,
[45] AlecRadford,JongWookKim,ChrisHallacy,Aditya 2023.
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas- [54] Fei Wang, Xingyu Fu, James Y Huang, Zekun Li,
try, Amanda Askell, Pamela Mishkin, Jack Clark, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu,
etal.Learningtransferablevisualmodelsfromnatural WenxuanZhou,KaiZhang,etal. Muirbench: Acom-
languagesupervision. InInternationalConferenceon prehensive benchmark for robust multi-image under-
MachineLearning,pages8748–8763.PMLR,2021. standing. arXivpreprintarXiv:2406.09411,2024.
[46] ChristophSchuhmann,RichardVencu,RomainBeau- [55] BoWu,ShoubinYu,ZhenfangChen,JoshuaBTenen-
mont, Robert Kaczmarczyk, Clayton Mullis, Aarush baum,andChuangGan. Star: Abenchmarkforsitu-
Katta, Theo Coombes, Jenia Jitsev, and Aran Ko- ated reasoning in real-world videos. arXiv preprint
matsuzaki. Laion-400m: Open dataset of clip- arXiv:2405.09711,2024.
filtered 400 million image-text pairs. arXiv preprint
[56] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng
arXiv:2111.02114,2021.
Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu
[47] Piyush Sharma, Nan Ding, Sebastian Goodman, and Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: A
Radu Soricut. Conceptual captions: A cleaned, hy- benchmarkforgeneral-purposefoundationmodelson
pernymed,imagealt-textdatasetforautomaticimage low-level vision. arXiv preprint arXiv:2309.14181,
captioning. InProceedingsofthe56thAnnualMeet- 2023.
ing of the Association for Computational Linguistics
[57] JunbinXiao,XindiShang,AngelaYao,andTat-Seng
(Volume1: LongPapers),pages2556–2565,2018.
Chua. Next-qa: Nextphaseofquestion-answeringto
[48] Mohit Shridhar, Jesse Thomason, Daniel Gordon, explaining temporal actions. In Proceedings of the
YonatanBisk,WinsonHan,RoozbehMottaghi,Luke IEEE/CVFconferenceoncomputervisionandpattern
Zettlemoyer,andDieterFox.Alfred:Abenchmarkfor recognition,pages9777–9786,2021.
interpretinggroundedinstructionsforeverydaytasks. [58] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen,
InProceedingsoftheIEEE/CVFconferenceoncom- JiangmiaoPang,andDahuaLin. Pointllm: Empower-
puter vision and pattern recognition, pages 10740– inglargelanguagemodelstounderstandpointclouds.
10749,2020. arXivpreprintarXiv:2308.16911,2023.
[49] MustafaShukor,AlexandreRame,CorentinDancette, [59] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao,
andMatthieuCord. Beyondtaskperformance: Eval- Yueting Zhuang, and Dacheng Tao. Activitynet-qa:
uating and reducing the flaws of large multimodal A dataset for understanding complex web videos via
models with in-context learning. arXiv preprint questionanswering. InProceedingsoftheAAAICon-
arXiv:2310.00647,2023. ference on Artificial Intelligence, volume 33, pages
[50] AlaneSuhr,StephanieZhou,AllyZhang,IrisZhang, 9127–9134,2019.
Huajun Bai, and Yoav Artzi. A corpus for reason- [60] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,
ing about natural language grounded in photographs. RuoqiLiu,GeZhang,SamuelStevens,DongfuJiang,
arXivpreprintarXiv:1811.00491,2018. Weiming Ren, Yuxuan Sun, et al. Mmmu: A mas-
sive multi-discipline multimodal understanding and
[51] QuanSun,YufengCui,XiaosongZhang,FanZhang,
reasoning benchmark for expert agi. In Proceedings
QiyingYu,YuezeWang,YongmingRao,JingjingLiu,
oftheIEEE/CVFConferenceonComputerVisionand
TiejunHuang, andXinlongWang. Generativemulti-
PatternRecognition,pages9556–9567,2024.
modalmodelsarein-contextlearners. InProceedings
oftheIEEE/CVFConferenceonComputerVisionand [61] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,
PatternRecognition,pages14398–14409,2024. Ruoqi Liu, Ge Zhang, Samuel Stevens, DongfuJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao
Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan
Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang,
HuanSun,YuSu,andWenhuChen. Mmmu: Amas-
sive multi-discipline multimodal understanding and
reasoning benchmark for expert agi. arXiv preprint
arXiv:2311.16502,2023.
[62] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,
and Lucas Beyer. Sigmoid loss for language im-
age pre-training. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages
11975–11986,2023.
[63] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao
Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chun-
yuan Li, Alexander Hauptmann, Yonatan Bisk, et al.
Direct preference optimization of video large multi-
modal models from language model reward. arXiv
preprintarXiv:2404.01258,2024.
[64] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei
Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,
andYuQiao.LLaMA-adapter:Efficientfine-tuningof
largelanguagemodelswithzero-initializedattention.
InTheTwelfthInternationalConferenceonLearning
Representations,2024.
[65] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun
Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu,
Kai-Wei Chang, Peng Gao, et al. Mathverse: Does
yourmulti-modalllmtrulyseethediagramsinvisual
math problems? arXiv preprint arXiv:2403.14624,
2024.
[66] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee,
LiangkeGui,DiFu,JiashiFeng,ZiweiLiu,andChun-
yuanLi. Llava-next: Astrongzero-shotvideounder-
standingmodel,April2024.
[67] Wanrong Zhu, Jack Hessel, Anas Awadalla,
Samir Yitzhak Gadre, Jesse Dodge, Alex Fang,
Youngjae Yu, Ludwig Schmidt, William Yang
Wang, and Yejin Choi. Multimodal c4: An open,
billion-scale corpus of images interleaved with text.
Advances in Neural Information Processing Systems,
36,2024.A.DataStatistics
ThedetaileddatastatisticsofM4-InstructisshowninTable15.
ThedetaileddatastatisticsofLLaVA-InterleaveBenchisshowninTable16.
B.AblationStudy
B.1.PoolvsnotPoolVisionTokensforvideotasks.
SimilartoLLaVA-NEXT-Video,weadopta”Poolingto1/4”strategyforwhichwepoolthewidthandheighsoffeature
mapsto1/2thereforereducingthenumbertototalsto1/4. Westudytheimpactofimagetokenpooling. Wetrainandinfer
ourmodelundertwosettings: poolingto1/4andnotpoolingwithShareGPTVideo-Caption+QA(255K)data. Poolingtoa
1/4 setting is similar to LLaVA-NEXT-Video, which uses the pooling technique to trade-off between the number of image
tokensandthenumberofframes. Inourexperiment,wefindthatnotpoolingyieldsbetterperformanceundersimilar#image
tokens. Duringtraining,wesample10framesforvideos. Inthistable,wealsoobservethataddingmoreframes(from10to
16)duringinferenceimprovesperformance.
B.2.ImpactofvideoDPOtrainingonothertasks.
In Table 14, we compare the results of doing video DPO on other tasks. Though DPO significantly improves the video
performanceasshowninTable2,itslightlyimpactstheperformanceofothertasks.
Training Inference #frames #Imagetokens Act Avg VDD VideoChatGPT
CI DO CU TU CO
Pooling1/4 Pooling1/4 40 40x729x1/4=10x729 52.8/3.53 3.35 3.38 3.46 3.25 3.87 2.59 3.57
Pooling1/4 Pooling1/4 64 64x729x1/4=16x729 52.7/3.53 3.33 3.38 3.45 3.23 3.86 2.49 3.55
NotPooling NotPooling 10 10x729 52.9/3.48 3.38 3.46 3.43 3.26 3.85 2.64 3.61
NotPooling NotPooling 16 16x729 54.4/3.51 3.41 3.46 3.48 3.28 3.87 2.74 3.62
Table13.Ablationtocomparepoolingandnotpooling.
Multi-image Multi-view Single-image
Setting
Mantis BLINK QB NLVR2 ScanQA AI2D ChartQA DocVQA MME* POPE SQA
BeforeVideo-DPO 62.7 52.7 73 89.1 32.2 73.9 67.2 75.7 63.5 85.4 72.6
AfterVideo-DPO 60.8 51.7 86.8 87.7 25.5 72.2 56.1 73.1 62.6 86.6 71.7
Table14. Ablationontheimpactofvideodpoontheperformanceofothertasks. QB:Q-Bench,Act:ActivityNet-QA,MVB:MVBench,
VDD:VideoDetailedDescription,MME*: Throughoutourpaper,weconvertMME’sscoretoaccuracybysumminguptheperception
andcognitionscoresanddividing2800,SQA:Scienceqa-IMG.Task Dataset Scenario #Samples
Multi-imageScenarios
SpottheDifference(42.6K) Real-worldDifference Realistic 6.7K
SyntheticDifference Synthetic 7.0K
Spot-the-Diff Surveilance 10.8K
Birds-to-Words Birds 14.2K
CLEVR-Change Solids 3.9K
ImageEditInstruction(67.7K) HQ-Edit Synthetic 50K
MagicBrush Realistic 14.2K
IEdit Realistic 3.5K
VisualStoryTelling(67.5K) AESOP Cartoon 6.9K
FlintstonesSV Cartoon 22.3K
PororoSV Cartoon 12.3K
VIST Realistic 26K
Text-richVQA(21.3K) WebQA Webpage 9.3K
TQA Textbook 8.2K
OCR-VQA OCR 1.9K
DocVQA Document 1.9K
Multi-imageVQA(153.5K) NLVR2 Realistic 86.4K
MIT-States StateCoherence General 1.9K
MIT-States PropertyCoherence General 1.9K
RecipeQA ImageCoherence Recipe 8.7K
VISION Industrial 9.9K
Multi-VQA General 5K
IconQA General 34.6K
Low-levelComparison(65.9K) Coinstruct Low-level 50K
Dreamsim Low-level 15.9K
Image-captionComprehension(41.8K) ImageCoDe General 16.6K
Contrast-Caption General 25.2K
DailyScenarios(5.7K) MMChat Twitter Post General 5.7K
Multi-imagePuzzle(35K) Raven Abstract 35K
Multi-frame(Video)Scenarios
VideoQA(246.9K) NExT-QA General 3.9K
STAR General 3K
ShareGPTVideo-VQA General 240K
VideoDetailedCaptioning(15K) ShareGPTVideo-Caption General 15K
Multi-view(3D)Scenarios
SceneVQA(45.4K) Nuscenes Outdoor 9.8K
ScanQA IndoorRealistic 25.6k
3D-LLM-Scene IndoorRealistic 10K
EmbodiedVQA(62.5K) ALFRED IndoorSynthetic 22.6K
3D-LLM-Dialogue IndoorRealistic 20K
3D-LLM-Planning IndoorRealistic 19.9K
Single-imageScenarios
Single-imageTasks(307K) Randomlysampling40%SFTdataofLLaVA-NeXT General 307K
Table15.M4-Instructdetaileddatasets.Task Dataset Scenario #Samples
In-domainEvaluation-NewlyCuratedBenchmarks
SpottheDifference(0.3K) Spot-the-Diff Surveilance 0.1K
Birds-to-Words Birds 0.1K
CLEVR-Change Solids 0.1K
ImageEditInstruction(2K) HQ-Edit Sythentic 1K
MagicBrush Realistic 0.9K
IEdit Realistic 0.1K
VisualStoryTelling(0.4K) AESOP Cartoon 0.1K
FlintstonesSV Cartoon 0.1K
PororoSV Cartoon 0.1K
VIST Realistic 0.1K
Text-richVQA(0.4K) WebQA Webpage 0.1K
TQA Textbook 0.1K
OCR-VQA OCR 0.1K
DocVQA Document 0.1K
Multi-imageVQA(0.4K) MIT-States StateCoherence General 0.1K
MIT-States PropertyCoherence General 0.1K
RecipeQA ImageCoherence Recipe 0.1K
VISION Industrial 0.1K
Puzzle(1.4K) Raven Abstract 1.4K
In-domainEvaluation-ExistingBenchmarks
NLVR2(7K) NLVR2 Realistic 7K
Q-Bench(1K) Q-Bench Low-level 1K
Out-domainEvaluation-NewlyCuratedBenchmarks
MathVerse-mv(0.8K) MathVerse(VisionDominant) MathDiagram 0.8K
SciVerse-mv(0.4K) SciVerse(VisionDominant) ScientificDiagram 0.4K
Out-domainEvaluation-ExistingBenchmarks
Mantis-Eval(0.2K) Mantis-Eval General 0.2K
BLINK(1.9K) BLINK General 1.9k
MMMU-mv(test)(0.8K) MMMU ScientificDiagram 0.8K
Table16.LLaVA-InterleaveBenchdetaileddatasets.