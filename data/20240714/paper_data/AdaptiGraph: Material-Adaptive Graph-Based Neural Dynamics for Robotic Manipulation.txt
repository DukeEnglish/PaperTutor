AdaptiGraph: Material-Adaptive Graph-Based
Neural Dynamics for Robotic Manipulation
Kaifeng Zhang1*, Baoyu Li1*, Kris Hauser1, Yunzhu Li1,2
1University of Illinois Urbana-Champaign 2Columbia University
Rope Granular pile
Physical Property
Conditioned
GNN
Online
Physical Property
Estimation
t
Cotton rope Cable Coffee bean Toy block
Fig. 1: Motivation. Objects made from different materials can exhibit distinct behaviors under interaction. Even within the same object
category, varying physical parameters like stiffness can lead to different behaviors. Examples shown here include handling cotton rope and
cable, as well as arranging granular piles such as coffee beans and toy blocks. Although the initial configuration and action are the same,
different physical parameters result in distinct final states, necessitating the need for online adaptation for effective manipulation. To this end,
we introduce AdaptiGraph, a unified graph-based neural dynamics framework for real-time modeling and control of various materials with
unknown physical properties. AdaptiGraph integrates a physical property-conditioned dynamics model with online physical property
estimation. Our framework enables robots to adaptively manipulate diverse objects with varying physical properties and dynamics.
Abstract—Predictive models are a crucial component of many of pressure. On prediction and manipulation tasks involving a
robotic systems. Yet, constructing accurate predictive models diverse set of real-world deformable objects, our method exhibits
for a variety of deformable objects, especially those with superior prediction accuracy and task proficiency over non-
unknown physical properties, remains a significant challenge. material-conditioned and non-adaptive models. The project page
This paper introduces AdaptiGraph, a learning-based dynamics is available at https://robopil.github.io/adaptigraph/.
modeling approach that enables robots to predict, adapt to, and
control a wide array of challenging deformable materials with I. INTRODUCTION
unknown physical properties. AdaptiGraph leverages the highly
flexible graph-based neural dynamics (GBND) framework, which Learning predictive models, also known as system identifi-
represents material bits as particles and employs a graph neural cation, is a crucial component of many robotic tasks. Whereas
network (GNN) to predict particle motion. Its key innovation classical methods rely on the explicit parameterization of the
is a unified physical property-conditioned GBND model capable
system state and struggle with systems that have high degrees
of predicting the motions of diverse materials with varying
offreedom,asignificantbodyofworkoverthelastdecadehas
physical properties without retraining. Upon encountering new
materials during online deployment, AdaptiGraph utilizes a attempted to learn models directly from visual observations.
physical property optimization process for a few-shot adaptation Prior approaches have learned predictive models based on
of the model, enhancing its fit to the observed interaction data. pixels [11, 18] or latent representations of images [15, 16].
The adapted models can precisely simulate the dynamics and
However, such representations often overlook the structure of
predictthemotionofvariousdeformablematerials,suchasropes,
the environment and do not generalize well across different
granularmedia,rigidboxes,andcloth,whileadaptingtodifferent
physical properties, including stiffness, granular size, and center camera poses, object poses, robots, object sizes, and object
shapes. Recently, a series of studies have employed Graph
*Denotesequalcontribution. NeuralNetworks(GNN)tomodelenvironmentsas3Dparticles
4202
luJ
01
]OR.sc[
1v98870.7042:viXra
setatS
laitinI
ssergorP
nI
setatS
laniFand their pairwise interactions [23, 39, 40, 47]. A graph due to unobservable physics properties such as mass, friction,
representation has proven effective in capturing relational bias and stiffness, occluded surfaces of geometry, sensitivity to
and predicting complex motions of deformable objects, but parameter estimates, and the high computational expense of
priorworkstypicallyonlyfocusonasinglematerialandwould simulating deformable objects. To mitigate these issues, recent
require extensive training to model an object of new material approachesapplylearning-basedtechniquestoobtaindynamics
or with unknown physical properties. Hence, it is an important models directly from sensory inputs [7, 33, 51, 18, 11, 2].
challenge to provide such graph-based models to adapt to Graph-based representations and GNNs have been proven
objects and tasks involving diverse materials and varying effective in modeling the complex behaviors of non-rigid
physical properties, such as manipulating ropes with different objects due to their ability to capture spatial relational bias [3,
stiffness and granular media with different granularity (Fig. 1). 35, 23, 27, 37, 47]. Prior work has explored the application
In this work, we present a unified framework for modeling of graph-based dynamics models on a variety of material
the dynamics of objects with different materials and physical types, including rigid bodies [23, 19, 29], plasticine [39, 40],
properties. In addition to classifying objects into discrete clothes[36,27,35,30],fluids[22,37],andgranularmatter[47].
material types such as rigid objects, ropes, etc., we further However, nearly all of these approaches focus on a single
consider a range of intra-class physical property variations in type of material and fail to consider variation in physical
each material type. We propose to encode this variation using properties, thus limiting their generalization and adaptation
a continuous variable which we call the physical property capabilities. In contrast, our method considers a wider range
variable, and integrate the variable into a Graph-Based Neural of materials and variations in physical properties in a single
Dynamics (GBND) framework (Fig. 1). The physical property property-conditionedgraph-basedneuraldynamicsmodels,and
variable indicates the important intrinsic properties of each this enables our approach to adaptively estimate the unknown
material category, including stiffness for deformable objects physical properties of unseen objects through interaction.
and the center of pressure position for rigid objects. By
B. Physical Property Estimation and Few-Shot Adaptation
encoding the material type and physical property variables
into particles in the graph, the model learns material-specific Estimating and adapting to the physical properties of
dynamic functions that predict different physical behaviors unseen objects is an inherent challenge in various robotic
for objects with different physical properties. We then employ applications. Previous works have attempted to infer physical
a test-time adaptation method to reason about the physical properties by tweaking parameters in physics-based simula-
properties of novel objects. Specifically, the robot actively tions [8, 44, 25, 12, 42, 49] or utilizing extra modalities,
interacts with the novel object, observes its response, and e.g., tactile signal [38, 45, 52], but these approaches have
estimates its physical properties to optimize the model’s fit a high demand for the object’s full state information, or
to the observed reactions. The estimation is performed in a require extra sensors. In comparison, adaptively learning
few-shot manner and can be directly applied to planning and explicit physical property variables [1, 22, 50, 6, 24, 30] or
trajectory optimization for downstream manipulation tasks. low-dimensional latent representations that implicitly encode
In our experiments, we verify this framework on four physical properties [21, 10] in a neural network only requires
types of objects: rigid objects, granular objects, rope-like partial observations and few-shot exploratory interactions as
objects, and cloth-like objects. Experiments show that our input. A line of work goes further by using the large vision-
framework can distinguish and model the dynamics of objects language models to infer physical properties solely from static
across a broad range of physical properties, for instance, from observation [13, 46], but these estimations are rough and do
very soft ropes like yarn and shoelaces to very stiff ropes not involve actual interactions. For estimation/adaptation from
like cables, and from very fine-grained granular matter like interactions, previous efforts were still limited to simulations
coffee beans to very coarse-grained ones like toy blocks or focused only on single types of materials, e.g., rigid objects.
(Fig. 1). The model is trained on diverse data collected There is also a dilemma in choosing the representation form:
with a simulator and tested with online adaptation on real explicit variables suffer from domain gaps such as the sim-
objects.Theresultsdemonstratethat(1)ouradaptationmodule to-real gap, yet latent representation has relatively lower
provides consistent and interpretable estimates of the objects’ interpretability. In contrast, our approach incorporates a graph-
physical property variables, and (2) by conditioning on the structured model within an inverse optimization framework,
estimated physical property variable, the model can carry offering interpretability, generalizability to objects beyond the
out more accurate dynamics estimation and more efficient training distribution, and applicability to a broader array of
manipulation, especially for objects with extreme or out-of- materialtypes,includingrigidboxes,ropes,cloths,andgranular
distribution physical properties. substances, in the real-world scenario.
II. RELATEDWORK III. METHODS
A. Model Learning for Robotic Manipulation
We first introduce the problem formulation in Sec. III-A.
Analytical physics-based models facilitate a wide span of Then, we introduce the perception module and the structure
robotic manipulation tasks [17, 34, 53]. However, building of our physical property-conditioned graph-based dynamics
accurate physics models is often infeasible in the real world model in Sec. III-B. We discuss the test-time adaptationRigid box Rope Rigid box Rope
 <latexit sha1_base64="TDZ5kXVcRihTN/44IH5FtXDuleA=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEWo9FLx4r2A9oQ9lsN83S3U3Y3Qgl9C948aCIV/+QN/+NmzYHbX0w8Hhvhpl5QcKZNq777ZQ2Nre2d8q7lb39g8Oj6vFJV8epIrRDYh6rfoA15UzSjmGG036iKBYBp71gepf7vSeqNIvlo5kl1Bd4IlnICDa5NEwiNqrW3Lq7AFonXkFqUKA9qn4NxzFJBZWGcKz1wHMT42dYGUY4nVeGqaYJJlM8oQNLJRZU+9ni1jm6sMoYhbGyJQ1aqL8nMiy0nonAdgpsIr3q5eJ/3iA14Y2fMZmkhkqyXBSmHJkY5Y+jMVOUGD6zBBPF7K2IRFhhYmw8FRuCt/ryOule1b1GvfFwXWvdFnGU4QzO4RI8aEIL7qENHSAQwTO8wpsjnBfn3flYtpacYuYU/sD5/AEXGo5K</latexit> Robot
end-effector
−0.5
CoPX
CoPY 0.5
Object z <latexit sha1_base64="hUkLNwwBjajnsRqz2x5wKOniqug=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rGi/YA2lM120y7dbMLuRKihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LRzdRvPXJtRKwecJxwP6IDJULBKFrp/qmHvVLZrbgzkGXi5aQMOeq90le3H7M04gqZpMZ0PDdBP6MaBZN8UuymhieUjeiAdyxVNOLGz2anTsipVfokjLUthWSm/p7IaGTMOApsZ0RxaBa9qfif10kxvPIzoZIUuWLzRWEqCcZk+jfpC80ZyrEllGlhbyVsSDVlaNMp2hC8xZeXSfO84lUr1buLcu06j6MAx3ACZ+DBJdTgFurQAAYDeIZXeHOk8+K8Ox/z1hUnnzmCP3A+fwB32o3w</latexit> t z <latexit sha1_base64="hUkLNwwBjajnsRqz2x5wKOniqug=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rGi/YA2lM120y7dbMLuRKihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LRzdRvPXJtRKwecJxwP6IDJULBKFrp/qmHvVLZrbgzkGXi5aQMOeq90le3H7M04gqZpMZ0PDdBP6MaBZN8UuymhieUjeiAdyxVNOLGz2anTsipVfokjLUthWSm/p7IaGTMOApsZ0RxaBa9qfif10kxvPIzoZIUuWLzRWEqCcZk+jfpC80ZyrEllGlhbyVsSDVlaNMp2hC8xZeXSfO84lUr1buLcu06j6MAx3ACZ+DBJdTgFurQAAYDeIZXeHOk8+K8Ox/z1hUnnzmCP3A+fwB32o3w</latexit> t
Encoder
0.5
Y z<latexit sha1_base64="t+XPzy5OKKjrhibcumoyrzteO4A=">AAAB9XicbVDJSgNBEK1xjeMW9eilMQiCEGZEosegF48RzALJGHo6PUmTnoXuGiUO8x9ePCji1X/x5t/YWQ6a+KDg8V4VVfX8RAqNjvNtLS2vrK6tFzbsza3tnd3i3n5Dx6livM5iGauWTzWXIuJ1FCh5K1Gchr7kTX94PfabD1xpEUd3OEq4F9J+JALBKBrpvjOgmD3l3QxPXTvvFktO2ZmALBJ3RkowQ61b/Or0YpaGPEImqdZt10nQy6hCwSTP7U6qeULZkPZ529CIhlx72eTqnBwbpUeCWJmKkEzU3xMZDbUehb7pDCkO9Lw3Fv/z2ikGl14moiRFHrHpoiCVBGMyjoD0hOIM5cgQypQwtxI2oIoyNEHZJgR3/uVF0jgru5Vy5fa8VL2axVGAQziCE3DhAqpwAzWoAwMFz/AKb9aj9WK9Wx/T1iVrNnMAf2B9/gAozJJN</latexit>ˆ
0.5 z<latexit sha1_base64="t+XPzy5OKKjrhibcumoyrzteO4A=">AAAB9XicbVDJSgNBEK1xjeMW9eilMQiCEGZEosegF48RzALJGHo6PUmTnoXuGiUO8x9ePCji1X/x5t/YWQ6a+KDg8V4VVfX8RAqNjvNtLS2vrK6tFzbsza3tnd3i3n5Dx6livM5iGauWTzWXIuJ1FCh5K1Gchr7kTX94PfabD1xpEUd3OEq4F9J+JALBKBrpvjOgmD3l3QxPXTvvFktO2ZmALBJ3RkowQ61b/Or0YpaGPEImqdZt10nQy6hCwSTP7U6qeULZkPZ529CIhlx72eTqnBwbpUeCWJmKkEzU3xMZDbUehb7pDCkO9Lw3Fv/z2ikGl14moiRFHrHpoiCVBGMyjoD0hOIM5cgQypQwtxI2oIoyNEHZJgR3/uVF0jgru5Vy5fa8VL2axVGAQziCE3DhAqpwAzWoAwMFz/AKb9aj9WK9Wx/T1iVrNnMAf2B9/gAozJJN</latexit>ˆ t+1
-0.5 0.5X P <latexit sha1_base64="TDZ5kXVcRihTN/44IH5FtXDuleA=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEWo9FLx4r2A9oQ9lsN83S3U3Y3Qgl9C948aCIV/+QN/+NmzYHbX0w8Hhvhpl5QcKZNq777ZQ2Nre2d8q7lb39g8Oj6vFJV8epIrRDYh6rfoA15UzSjmGG036iKBYBp71gepf7vSeqNIvlo5kl1Bd4IlnICDa5NEwiNqrW3Lq7AFonXkFqUKA9qn4NxzFJBZWGcKz1wHMT42dYGUY4nVeGqaYJJlM8oQNLJRZU+9ni1jm6sMoYhbGyJQ1aqL8nMiy0nonAdgpsIr3q5eJ/3iA14Y2fMZmkhkqyXBSmHJkY5Y+jMVOUGD6zBBPF7K2IRFhhYmw8FRuCt/ryOule1b1GvfFwXWvdFnGU4QzO4RI8aEIL7qENHSAQwTO8wpsjnBfn3flYtpacYuYU/sD5/AEXGo5K</latexit> r: e sC se un rete (r C o of P ) 0.0 1.0  <latexit sha1_base64="TDZ5kXVcRihTN/44IH5FtXDuleA=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEWo9FLx4r2A9oQ9lsN83S3U3Y3Qgl9C948aCIV/+QN/+NmzYHbX0w8Hhvhpl5QcKZNq777ZQ2Nre2d8q7lb39g8Oj6vFJV8epIrRDYh6rfoA15UzSjmGG036iKBYBp71gepf7vSeqNIvlo5kl1Bd4IlnICDa5NEwiNqrW3Lq7AFonXkFqUKA9qn4NxzFJBZWGcKz1wHMT42dYGUY4nVeGqaYJJlM8oQNLJRZU+9ni1jm6sMoYhbGyJQ1aqL8nMiy0nonAdgpsIr3q5eJ/3iA14Y2fMZmkhkqyXBSmHJkY5Y+jMVOUGD6zBBPF7K2IRFhhYmw8FRuCt/ryOule1b1GvfFwXWvdFnGU4QzO4RI8aEIL7qENHSAQwTO8wpsjnBfn3flYtpacYuYU/sD5/AEXGo5K</latexit> : Stiffness t+1
Propagation
z
<latexit sha1_base64="hUkLNwwBjajnsRqz2x5wKOniqug=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rGi/YA2lM120y7dbMLuRKihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LRzdRvPXJtRKwecJxwP6IDJULBKFrp/qmHvVLZrbgzkGXi5aQMOeq90le3H7M04gqZpMZ0PDdBP6MaBZN8UuymhieUjeiAdyxVNOLGz2anTsipVfokjLUthWSm/p7IaGTMOApsZ0RxaBa9qfif10kxvPIzoZIUuWLzRWEqCcZk+jfpC80ZyrEllGlhbyVsSDVlaNMp2hC8xZeXSfO84lUr1buLcu06j6MAx3ACZ+DBJdTgFurQAAYDeIZXeHOk8+K8Ox/z1hUnnzmCP3A+fwB32o3w</latexit> t
z
<latexit sha1_base64="hUkLNwwBjajnsRqz2x5wKOniqug=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rGi/YA2lM120y7dbMLuRKihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LRzdRvPXJtRKwecJxwP6IDJULBKFrp/qmHvVLZrbgzkGXi5aQMOeq90le3H7M04gqZpMZ0PDdBP6MaBZN8UuymhieUjeiAdyxVNOLGz2anTsipVfokjLUthWSm/p7IaGTMOApsZ0RxaBa9qfif10kxvPIzoZIUuWLzRWEqCcZk+jfpC80ZyrEllGlhbyVsSDVlaNMp2hC8xZeXSfO84lUr1buLcu06j6MAx3ACZ+DBJdTgFurQAAYDeIZXeHOk8+K8Ox/z1hUnnzmCP3A+fwB32o3w</latexit> t
z<latexit sha1_base64="t+XPzy5OKKjrhibcumoyrzteO4A=">AAAB9XicbVDJSgNBEK1xjeMW9eilMQiCEGZEosegF48RzALJGHo6PUmTnoXuGiUO8x9ePCji1X/x5t/YWQ6a+KDg8V4VVfX8RAqNjvNtLS2vrK6tFzbsza3tnd3i3n5Dx6livM5iGauWTzWXIuJ1FCh5K1Gchr7kTX94PfabD1xpEUd3OEq4F9J+JALBKBrpvjOgmD3l3QxPXTvvFktO2ZmALBJ3RkowQ61b/Or0YpaGPEImqdZt10nQy6hCwSTP7U6qeULZkPZ529CIhlx72eTqnBwbpUeCWJmKkEzU3xMZDbUehb7pDCkO9Lw3Fv/z2ikGl14moiRFHrHpoiCVBGMyjoD0hOIM5cgQypQwtxI2oIoyNEHZJgR3/uVF0jgru5Vy5fa8VL2axVGAQziCE3DhAqpwAzWoAwMFz/AKb9aj9WK9Wx/T1iVrNnMAf2B9/gAozJJN</latexit>ˆ z<latexit sha1_base64="t+XPzy5OKKjrhibcumoyrzteO4A=">AAAB9XicbVDJSgNBEK1xjeMW9eilMQiCEGZEosegF48RzALJGHo6PUmTnoXuGiUO8x9ePCji1X/x5t/YWQ6a+KDg8V4VVfX8RAqNjvNtLS2vrK6tFzbsza3tnd3i3n5Dx6livM5iGauWTzWXIuJ1FCh5K1Gchr7kTX94PfabD1xpEUd3OEq4F9J+JALBKBrpvjOgmD3l3QxPXTvvFktO2ZmALBJ3RkowQ61b/Or0YpaGPEImqdZt10nQy6hCwSTP7U6qeULZkPZ529CIhlx72eTqnBwbpUeCWJmKkEzU3xMZDbUehb7pDCkO9Lw3Fv/z2ikGl14moiRFHrHpoiCVBGMyjoD0hOIM5cgQypQwtxI2oIoyNEHZJgR3/uVF0jgru5Vy5fa8VL2axVGAQziCE3DhAqpwAzWoAwMFz/AKb9aj9WK9Wx/T1iVrNnMAf2B9/gAozJJN</latexit>ˆ
 <latexit sha1_base64="TDZ5kXVcRihTN/44IH5FtXDuleA=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEWo9FLx4r2A9oQ9lsN83S3U3Y3Qgl9C948aCIV/+QN/+NmzYHbX0w8Hhvhpl5QcKZNq777ZQ2Nre2d8q7lb39g8Oj6vFJV8epIrRDYh6rfoA15UzSjmGG036iKBYBp71gepf7vSeqNIvlo5kl1Bd4IlnICDa5NEwiNqrW3Lq7AFonXkFqUKA9qn4NxzFJBZWGcKz1wHMT42dYGUY4nVeGqaYJJlM8oQNLJRZU+9ni1jm6sMoYhbGyJQ1aqL8nMiy0nonAdgpsIr3q5eJ/3iA14Y2fMZmkhkqyXBSmHJkY5Y+jMVOUGD6zBBPF7K2IRFhhYmw8FRuCt/ryOule1b1GvfFwXWvdFnGU4QzO4RI8aEIL7qENHSAQwTO8wpsjnBfn3flYtpacYuYU/sD5/AEXGo5K</latexit> : Granularity  <latexit sha1_base64="TDZ5kXVcRihTN/44IH5FtXDuleA=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEWo9FLx4r2A9oQ9lsN83S3U3Y3Qgl9C948aCIV/+QN/+NmzYHbX0w8Hhvhpl5QcKZNq777ZQ2Nre2d8q7lb39g8Oj6vFJV8epIrRDYh6rfoA15UzSjmGG036iKBYBp71gepf7vSeqNIvlo5kl1Bd4IlnICDa5NEwiNqrW3Lq7AFonXkFqUKA9qn4NxzFJBZWGcKz1wHMT42dYGUY4nVeGqaYJJlM8oQNLJRZU+9ni1jm6sMoYhbGyJQ1aqL8nMiy0nonAdgpsIr3q5eJ/3iA14Y2fMZmkhkqyXBSmHJkY5Y+jMVOUGD6zBBPF7K2IRFhhYmw8FRuCt/ryOule1b1GvfFwXWvdFnGU4QzO4RI8aEIL7qENHSAQwTO8wpsjnBfn3flYtpacYuYU/sD5/AEXGo5K</latexit> : Stiffness Decoder t+1 t+1
0.0 1.0 0.0 1.0
Granular object Cloth f<latexit sha1_base64="+qqeNhg7gs2baKY/L31S1IZaCLI=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rEF+wFtKJvtpF272YTdjVBCf4EXD4p49Sd589+4bXPQ1gcDj/dmmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8N/PbT6g0j+WDmSToR3QoecgZNVZqhP1S2a24c5BV4uWkDDnq/dJXbxCzNEJpmKBadz03MX5GleFM4LTYSzUmlI3pELuWShqh9rP5oVNybpUBCWNlSxoyV39PZDTSehIFtjOiZqSXvZn4n9dNTXjjZ1wmqUHJFovCVBATk9nXZMAVMiMmllCmuL2VsBFVlBmbTdGG4C2/vEpalxWvWqk2rsq12zyOApzCGVyAB9dQg3uoQxMYIDzDK7w5j86L8+58LFrXnHzmBP7A+fwBzoOM9Q==</latexit> : Material-conditioned GBND model Granular object Cloth
Current state and action Next observed state
u
<latexit sha1_base64="g8sSvtV5WW/7pGlTzm27ZFA79LE=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilh7SP/XLFrbpzkFXi5aQCORr98ldvELM04gqZpMZ0PTdBP6MaBZN8WuqlhieUjemQdy1VNOLGz+anTsmZVQYkjLUthWSu/p7IaGTMJApsZ0RxZJa9mfif100xvPYzoZIUuWKLRWEqCcZk9jcZCM0ZyokllGlhbyVsRDVlaNMp2RC85ZdXSeui6tWqtfvLSv0mj6MIJ3AK5+DBFdThDhrQBAZDeIZXeHOk8+K8Ox+L1oKTzxzDHzifP3A8jes=</latexit> t
u <latexit sha1_base64="g8sSvtV5WW/7pGlTzm27ZFA79LE=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilh7SP/XLFrbpzkFXi5aQCORr98ldvELM04gqZpMZ0PTdBP6MaBZN8WuqlhieUjemQdy1VNOLGz+anTsmZVQYkjLUthWSu/p7IaGTMJApsZ0RxZJa9mfif100xvPYzoZIUuWKLRWEqCcZk9jcZCM0ZyokllGlhbyVsRDVlaNMp2RC85ZdXSeui6tWqtfvLSv0mj6MIJ3AK5+DBFdThDhrQBAZDeIZXeHOk8+K8Ox+L1oKTzxzDHzifP3A8jes=</latexit> t z<latexit sha1_base64="gi4fEjSbJrS4rOwqUE1UUYA74ZA=">AAACIXicbVDLSgMxFM3UV62vqks3wSJUlDIjUgsiFN24rGAf0NYhk2ba0MyD5I7QDvMrbvwVNy4U6U78GdPHorYeuHByzr3k3uOEgiswzW8jtbK6tr6R3sxsbe/s7mX3D2oqiCRlVRqIQDYcopjgPqsCB8EaoWTEcwSrO/27sV9/ZlLxwH+EQcjaHun63OWUgJbsbKnVIxAPEzuGMyt5iifPVtjjiW0m+Aa7+aEN5ziy4RrPe6d2NmcWzAnwMrFmJIdmqNjZUasT0MhjPlBBlGpaZgjtmEjgVLAk04oUCwntky5rauoTj6l2PLkwwSda6WA3kLp8wBN1fiImnlIDz9GdHoGeWvTG4n9eMwK31I65H0bAfDr9yI0EhgCP48IdLhkFMdCEUMn1rpj2iCQUdKgZHYK1ePIyqV0UrGKh+HCZK9/O4kijI3SM8shCV6iM7lEFVRFFL+gNfaBP49V4N76M0bQ1ZcxmDtEfGD+/8RujWA==</latexit> ˆ ˆ 0 = f(z ,u ; ˆ ) z
t+1 t t 0 <latexit sha1_base64="5vfTnUKrwAApggOl6ORxKn2CFi4=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSIIQklEqseiF48V7Ae0oWy2m3bpZhN2J0IN/RFePCji1d/jzX/jts1BWx8MPN6bYWZekEhh0HW/nZXVtfWNzcJWcXtnd2+/dHDYNHGqGW+wWMa6HVDDpVC8gQIlbyea0yiQvBWMbqd+65FrI2L1gOOE+xEdKBEKRtFKradehufepFcquxV3BrJMvJyUIUe9V/rq9mOWRlwhk9SYjucm6GdUo2CST4rd1PCEshEd8I6likbc+Nns3Ak5tUqfhLG2pZDM1N8TGY2MGUeB7YwoDs2iNxX/8zophtd+JlSSIldsvihMJcGYTH8nfaE5Qzm2hDIt7K2EDammDG1CRRuCt/jyMmleVLxqpXp/Wa7d5HEU4BhO4Aw8uIIa3EEdGsBgBM/wCm9O4rw4787HvHXFyWeO4A+czx8WNo9s</latexit> t+1
z
<latexit sha1_base64="hUkLNwwBjajnsRqz2x5wKOniqug=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rGi/YA2lM120y7dbMLuRKihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LRzdRvPXJtRKwecJxwP6IDJULBKFrp/qmHvVLZrbgzkGXi5aQMOeq90le3H7M04gqZpMZ0PDdBP6MaBZN8UuymhieUjeiAdyxVNOLGz2anTsipVfokjLUthWSm/p7IaGTMOApsZ0RxaBa9qfif10kxvPIzoZIUuWLzRWEqCcZk+jfpC80ZyrEllGlhbyVsSDVlaNMp2hC8xZeXSfO84lUr1buLcu06j6MAx3ACZ+DBJdTgFurQAAYDeIZXeHOk8+K8Ox/z1hUnnzmCP3A+fwB32o3w</latexit> t
f<latexit sha1_base64="+qqeNhg7gs2baKY/L31S1IZaCLI=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rEF+wFtKJvtpF272YTdjVBCf4EXD4p49Sd589+4bXPQ1gcDj/dmmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8N/PbT6g0j+WDmSToR3QoecgZNVZqhP1S2a24c5BV4uWkDDnq/dJXbxCzNEJpmKBadz03MX5GleFM4LTYSzUmlI3pELuWShqh9rP5oVNybpUBCWNlSxoyV39PZDTSehIFtjOiZqSXvZn4n9dNTXjjZ1wmqUHJFovCVBATk9nXZMAVMiMmllCmuL2VsBFVlBmbTdGG4C2/vEpalxWvWqk2rsq12zyOApzCGVyAB9dQg3uoQxMYIDzDK7w5j86L8+58LFrXnHzmBP7A+fwBzoOM9Q==</latexit> …… m<latexit sha1_base64="PrRiRDnPiscss5GoW49ZKXpX0BE=">AAACGXicbVDLSsNAFJ3Ud31VXboZLIIvSiKiLou6cOFCwWqhiWEynbZDJ5MwcyO0Ib/hxl9x40IRl7ryb5y2Waj1wMDhnHu5c04QC67Btr+swsTk1PTM7FxxfmFxabm0snqjo0RRVqORiFQ9IJoJLlkNOAhWjxUjYSDYbdA9Hfi390xpHslr6MXMC0lb8hanBIzkl2w35BK7IYEOJSK9yPz09CzDW30/hV0n28Nuh0Daz+52RsK2XyrbFXsIPE6cnJRRjku/9OE2I5qETAIVROuGY8fgpUQBp4JlRTfRLCa0S9qsYagkIdNeOkyW4U2jNHErUuZJwEP150ZKQq17YWAmBxH0X28g/uc1EmgdeymXcQJM0tGhViIwRHhQE25yxSiIniGEKm7+immHKELBlFk0JTh/I4+Tm/2Kc1g5vDooV0/yOmbROtpAW8hBR6iKztElqiGKHtATekGv1qP1bL1Z76PRgpXvrKFfsD6/ASsSn8Y=</latexit> in LCD(z t+1,zˆ t⇤+1)
Object with unknown  <latexit sha1_base64="TDZ5kXVcRihTN/44IH5FtXDuleA=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEWo9FLx4r2A9oQ9lsN83S3U3Y3Qgl9C948aCIV/+QN/+NmzYHbX0w8Hhvhpl5QcKZNq777ZQ2Nre2d8q7lb39g8Oj6vFJV8epIrRDYh6rfoA15UzSjmGG036iKBYBp71gepf7vSeqNIvlo5kl1Bd4IlnICDa5NEwiNqrW3Lq7AFonXkFqUKA9qn4NxzFJBZWGcKz1wHMT42dYGUY4nVeGqaYJJlM8oQNLJRZU+9ni1jm6sMoYhbGyJQ1aqL8nMiy0nonAdgpsIr3q5eJ/3iA14Y2fMZmkhkqyXBSmHJkY5Y+jMVOUGD6zBBPF7K2IRFhhYmw8FRuCt/ryOule1b1GvfFwXWvdFnGU4QzO4RI8aEIL7qENHSAQwTO8wpsjnBfn3flYtpacYuYU/sD5/AEXGo5K</latexit>
ϕϕ<latexit sha1_base64="gq+qSNroDvcuefW96PppC6+xHSY=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48VbC20oWy2m2bpZhN3J4US+ju8eFDEqz/Gm//G7cdBWx8MPN6bYWZekEph0HW/ncLa+sbmVnG7tLO7t39QPjxqmSTTjDdZIhPdDqjhUijeRIGSt1PNaRxI/hgMb6f+44hrIxL1gOOU+zEdKBEKRtFKfjeimHfTSEx6ivTKFbfqzkBWibcgFVig0St/dfsJy2KukElqTMdzU/RzqlEwySelbmZ4StmQDnjHUkVjbvx8dvSEnFmlT8JE21JIZurviZzGxozjwHbGFCOz7E3F/7xOhuG1nwuVZsgVmy8KM0kwIdMESF9ozlCOLaFMC3srYRHVlKHNqWRD8JZfXiWti6pXq9buLyv1m0UcRTiBUzgHD66gDnfQgCYweIJneIU3Z+S8OO/Ox7y14CxmjuEPnM8fzIOSIg==</latexit>ˆ
1 1 
n c<latexit sha1_base64="NLpgnYQqL+0L8PkA8wkaw26sEMU=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRaheii7ItVj0YvHCvZD2rVk02wbmmSXJCuUpb/CiwdFvPpzvPlvTNs9aOuDgcd7M8zMC2LOtHHdbye3srq2vpHfLGxt7+zuFfcPmjpKFKENEvFItQOsKWeSNgwznLZjRbEIOG0Fo5up33qiSrNI3ptxTH2BB5KFjGBjpQdS7sZD9nh22iuW3Io7A1omXkZKkKHeK351+xFJBJWGcKx1x3Nj46dYGUY4nRS6iaYxJiM8oB1LJRZU++ns4Ak6sUofhZGyJQ2aqb8nUiy0HovAdgpshnrRm4r/eZ3EhFd+ymScGCrJfFGYcGQiNP0e9ZmixPCxJZgoZm9FZIgVJsZmVLAheIsvL5PmecWrVqp3F6XadRZHHo7gGMrgwSXU4Bbq0AACAp7hFd4c5bw4787HvDXnZDOH8AfO5w+4u4+4</latexit> (  )
⇤
Sampled physical parameters z<latexit sha1_base64="ixCrcjMLpNfErb5CU9oTZUiPFxQ=">AAACIXicbVDLSgMxFM3UV62vqks3wSIoSpkR0YIIRTcuK9gHtHXIpJk2mMkMyR2hHeZX3Pgrblwo0p34M6aPRbUeuHByzr3k3uNFgmuw7S8rs7C4tLySXc2trW9sbuW3d2o6jBVlVRqKUDU8opngklWBg2CNSDESeILVvcebkV9/YkrzUN5DP2LtgHQl9zklYCQ3X2r1CCSD1E3g2EkfkvGzFfV46soUX2H/cODCCY5duMSz3pGbL9hFeww8T5wpKaApKm5+2OqENA6YBCqI1k3HjqCdEAWcCpbmWrFmEaGPpMuahkoSMN1Oxhem+MAoHeyHypQEPFZnJxISaN0PPNMZEOjpv95I/M9rxuCX2gmXUQxM0slHfiwwhHgUF+5wxSiIviGEKm52xbRHFKFgQs2ZEJy/J8+T2mnROS+e350VytfTOLJoD+2jQ+SgC1RGt6iCqoiiZ/SK3tGH9WK9WZ/WcNKasaYzu+gXrO8ftF6j1A==</latexit> ˆ ˆ n = f(z ,u ; ˆ ) z<latexit sha1_base64="KnKLukD4/FTDi6LeEkno86TtQjw=">AAACDnicbVDJSgNBEO2JW4xb1KOXxhCIUcKMSBRECHrxGMEskGXo6fQkTXoWumuEZMgXePFXvHhQxKtnb/6NneWgiQ8KHu9VUVXPCQVXYJrfRmJpeWV1Lbme2tjc2t5J7+5VVRBJyio0EIGsO0QxwX1WAQ6C1UPJiOcIVnP6N2O/9sCk4oF/D4OQtTzS9bnLKQEt2elss0cgHo7sGI6tUTuPr7CbG9pwgiMbLnEz7PF2/shOZ8yCOQFeJNaMZNAMZTv91ewENPKYD1QQpRqWGUIrJhI4FWyUakaKhYT2SZc1NPWJx1QrnrwzwlmtdLAbSF0+4In6eyImnlIDz9GdHoGemvfG4n9eIwL3ohVzP4yA+XS6yI0EhgCPs8EdLhkFMdCEUMn1rZj2iCQUdIIpHYI1//IiqZ4WrGKheHeWKV3P4kiiA3SIcshC56iEblEZVRBFj+gZvaI348l4Md6Nj2lrwpjN7KM/MD5/AHgimnc=</latexit>ˆ = f(z ,u ;  )
<latexit sha1_base64="pLASM0xH/+u7YouT8PhcKx8L1dA=">AAACEXicbVBNS8NAEN3Ur1q/qh69LBahh1ISkeqx6MVjBdsKTQib7aZZutmE3YlQQv+CF/+KFw+KePXmzX/j9uNQWx8MPN6bYWZekAquwbZ/rMLa+sbmVnG7tLO7t39QPjzq6CRTlLVpIhL1EBDNBJesDRwEe0gVI3EgWDcY3kz87iNTmifyHkYp82IykDzklICR/HLVzbEbEcjdNOJj365hl/YT0LVFVWJ37Jcrdt2eAq8SZ04qaI6WX/52+wnNYiaBCqJ1z7FT8HKigFPBxiU30ywldEgGrGeoJDHTXj79aIzPjNLHYaJMScBTdXEiJ7HWozgwnTGBSC97E/E/r5dBeOXlXKYZMElni8JMYEjwJB7c54pRECNDCFXc3IppRBShYEIsmRCc5ZdXSee87jTqjbuLSvN6HkcRnaBTVEUOukRNdItaqI0oekIv6A29W8/Wq/Vhfc5aC9Z85hj9gfX1C8GXnP8=</latexit>  ˆ , , ˆ t+1 t t n t⇤+1 t t ⇤
{ 0 ··· n} Prediction with sampled  <latexit sha1_base64="KGxhZ+WzjzBeNFhVsfeUgFpJMeM=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA9hV2R6DHoxWME84DsGmYnvcmQ2QczvWpY8h9ePCji1X/x5t84m+SgiQUNRVU33V1+IoVG2/62lpZXVtfWCxvFza3tnd3S3n5Tx6ni0OCxjFXbZxqkiKCBAiW0EwUs9CW0/OF17rceQGkRR3c4SsALWT8SgeAMjXTvJgPhIjxhdqLHxW6pbFfsCegicWakTGaod0tfbi/maQgRcsm07jh2gl7GFAouYVx0Uw0J40PWh46hEQtBe9nk6jE9NkqPBrEyFSGdqL8nMhZqPQp90xkyHOh5Lxf/8zopBpdeJqIkRYj4dFGQSooxzSOgPaGAoxwZwrgS5lbKB0wxjiaoPARn/uVF0jyrONVK9fa8XLuaxVEgh+SInBKHXJAauSF10iCcKPJMXsmb9Wi9WO/Wx7R1yZrNHJA/sD5/AFYOkms=</latexit> ’s Prediction with optimal  <latexit sha1_base64="NHCtejKgD/DiDL54pl+PSMu49g0=">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRZBPJRdkeqx6MVjBfsB7VqyabaNzSZLkhXK0v/gxYMiXv0/3vw3Zts9aOuDgcd7M8zMC2LOtHHdb6ewsrq2vlHcLG1t7+zulfcPWlomitAmkVyqToA15UzQpmGG006sKI4CTtvB+Cbz209UaSbFvZnE1I/wULCQEWys1OrFI/Zw1i9X3Ko7A1omXk4qkKPRL3/1BpIkERWGcKx113Nj46dYGUY4nZZ6iaYxJmM8pF1LBY6o9tPZtVN0YpUBCqWyJQyaqb8nUhxpPYkC2xlhM9KLXib+53UTE175KRNxYqgg80VhwpGRKHsdDZiixPCJJZgoZm9FZIQVJsYGVLIheIsvL5PWedWrVWt3F5X6dR5HEY7gGE7Bg0uowy00oAkEHuEZXuHNkc6L8+58zFsLTj5zCH/gfP4AMreO5g==</latexit> ⇤
Fig. 2: Overview of proposed framework: AdaptiGraph. (a) Our graph-based dynamics model f is conditioned on the discrete material
type and continuous physical parameters ϕ. ϕ is encoded as the node features, which will be propagated and updated in the model training
process. Our model can accurately predict the future state zˆ for a variety of objects with different physical properties. (b) Our framework
t+1
performs physical property estimation for few-shot adaptation. This is achieved through an inverse optimization process to estimate the
optimal physical parameters as predicted by the learned dynamics model f. The optimal physical parameter ϕ∗ is identified by minimizing
the cost function, which is defined as the Chamfer Distance between the predicted graph state and the actual future graph state.
algorithmforphysics propertyestimationinSec. III-C.Finally, and u , z , z are the robot action, current environment state
t t t+1
inSec.III-D,weintroducehowweperformclosed-loopcontrol at time t, and the next state at time t+1, respectively. In
for downstream manipulation tasks. our approach, we train the dynamics model to minimize the
accumulated future prediction loss.
A. Problem Formulation
By conditioning on M and ϕ, the model learns to predict
Our aim is to learn a dynamics model, f, that is conditioned
material-dependent physical behaviors, based on which we can
on the material type M and continuous physical property perform physical property estimation through the following
variable ϕ, and develop a test-time few-shot adaptation scheme
optimization problem:
to infer the physical property variable for unseen objects.
Specifically, thedynamics model predicts howthe environment (cid:88)T
ϕ =argmin cost(zˆ ,z ), (2)
will change if the robot applies a given action: ∗ t+1 t+1
ϕ
t=1
zˆ =f(z ,u ;ϕ,M), (1)
t+1 t t where T is the iteration number indicating the number of
where M indicates the material type (e.g., rigid, granular, rope, interactions with the unseen object, and cost(, ) is the cost
· ·
cloth), ϕ indicates material-specific physical property variables, function measuring the discrepancy between the predicted
scimanyd
laruen
desab-hparg
denoitidnoc-lairetaM
)a(
noitamitse
ytreporp
lacisyhP
)b(
 >tixetal/<K5oGXEA/5Ds/UYuYcaptYlf3nfBnjspw8OTwQASHNEq7LIEa8IR4OzQ4UGnFdvWXwFfvG1b1eluOyr/tCuRF8wmYhhFRI2K7FPBBz6DGUOVMj+Y5YkJHmSBXyqkhkmZMf2Y41Ai3/Je5q3rIspgdAnon0yiMn8Lqa1QJyGbhYoMs6mj1in9+UZRJLNQo8MlJJYaqGeVn4YUGYd24TMHw1zKcGWZBJFzxN4nq9AKUqFkXnoFA7qL3WrqNiwEN5aDCInlI4dB1lk5olvINqeSv7fpeg17pBYBKi630GGmjSzU51Aofr6hYDRrIpe8VJFv6jO8g93bl7q8d2erN2QZ777qNZKcQ5lphvhH8w0XbHYzmN+/NQ+/VICa849C9lgQ3Y3U3S38Nsl9Qo9A2r4xLF9oWEl0UBbBL96hq/q1rU3JEAN8SNBVbci36BAAA>"=AeluDXtF5HI44/NThiRcVXk5ZDT"=46esab_1ahs
tixetal<
c( ) <latexit sha1_base64="QW9p/lvN+w3NoajPdIo7jXFAqUM=">AAAB73icbVBNSwMxEJ2tX7V+VT16CRahXsquSPVY9OKxgv2AdinZNNuGZrMxyQpl6Z/w4kERr/4db/4bs+0etPXBwOO9GWbmBZIzbVz32ymsrW9sbhW3Szu7e/sH5cOjto4TRWiLxDxW3QBrypmgLcMMp12pKI4CTjvB5DbzO09UaRaLBzOV1I/wSLCQEWys1CXVvhyz89KgXHFr7hxolXg5qUCO5qD81R/GJImoMIRjrXueK42fYmUY4XRW6ieaSkwmeER7lgocUe2n83tn6MwqQxTGypYwaK7+nkhxpPU0CmxnhM1YL3uZ+J/XS0x47adMyMRQQRaLwoQjE6PseTRkihLDp5Zgopi9FZExVpgYG1EWgrf88ippX9S8eq1+f1lp3ORxFOEETqEKHlxBA+6gCS0gwOEZXuHNeXRenHfnY9FacPKZY/gD5/MH0d2PMA==</latexit>future state zˆ and the observed state z . Training. To regulate the cumulative dynamics prediction
t+1 t+1
error, we supervise the model’s prediction results on K
B. Material-ConditionedGraph-basedNeuralDynamicsModel
prediction steps and perform backpropagation through time
We propose to instantiate the dynamics model with a graph
to optimize model parameters. In practice, we choose K =3
neural network. Following prior work on graph-based neural
for all tasks for balancing efficiency and performance. We use
dynamics (GBND) [22, 47, 39, 40], we define the environment
MSE loss on predicted object particle positions as the loss
state as a graph: z t =∆ t = ( t, t), where t is the vertex function:
G V E V
set representing object particles, and denotes the edge set
Et (cid:88)
representing interacting particle pairs at time step t. Given the = z f(z ,u ;ϕ,M) 2, (6)
L || t+1 − t t ||2
point cloud input, the object particle positions are determined t
by the farthest point sampling method [32], which ensures
To obtain training data at scale, we generate diverse object
sufficient coverage of the object’s geometry. We construct
trajectories by randomizing robot actions and object configu-
edges between particles based on a spatial distance threshold d.
rations using physics-based simulators. Most importantly, we
Wealsosampleparticlesontherobotend-effectorandconstruct
randomize the material configuration for each instance in the
relations between robot particles and object particles.
dataset.Toachievethis,weidentifythephysicspropertyϕand
The main improvement of our model over previous works
randomize the property over a wide range of feasible values.
based on GBND is our material- and physical property-
conditioning module (Fig. 2a). Suppose a vertex v has
i,t t
∈V C. Few-Shot Physical Property Adaptation
material M and physical property variable ϕ . We incorporate
i i
thismaterialinformationintothevertexfeaturesalongwiththe After learning the material-conditioned GBND model, we
3D position information over h history timesteps x i,t h:t and deploy the model to objects with unknown physical properties
the vertex attribute cv which indicates whether the−particle in the real world. Inspired by human’s ability to reason about
i,t
belongs to an object or the robot end-effector. Formally, objects’physicalpropertiesbyinteractingwiththem,wedesign
v imi,t pli= citly(x ei, nt −coh d:t e,c tv i h,t e,ϕ vi e, lM oci i) ty∈ infV ort m. aT tih oe n.h Eis mto pry iricp ao ls lyit ,io wn es a dn rivi en nve ir ns te ero acp tt ii om ni .zation pipeline through few-shot curiosity-
choose h=3. The relation features between a pair of particles Specifically, to estimate the physical property variable, the
is denoted as e k,t =(w,u,ce k,t) ∈Et, where 1 ≤w,u ≤|Vt
|
robot actively interacts with the object. In each iteration, it
are the receiver particle index and the sender particle index selects the action that maximizes the predicted displacement of
of the kth edge respectively. The edge attribute ce contains the object. Intuitively, the action that maximizes displacement
k,t
information such as whether the sender and receiver belong to is likely to reveal more information about the object’s physical
the same or different objects. properties than random actions would.
The constructed vertex and edge features are first fed into After each interaction, the robot updates its estimate of the
thevertexencoderfenc andtheedgeencoderfenc respectively object by minimizing the dynamics prediction error from pre-
to get the latent verVtex and edge embeddings Eh vi,t and h ek,t: vious interactions. As the robot undergoes several interactions,
h0 =fenc(v ), h0 =fenc(v ,v ). (3) the estimation of physical property tends to stabilize, reaching
vi,t
V
i,t ek,t
E
w,t u,t
the final optimized value.
Then, an edge propagation network fprop and vertex propa-
In our experiments, we adopt a fixed number of iterations
gation network fprop performs iterativEe update of the vertex
for adaptation. We measure the displacement of the object by
and edge embeddVings to perform multi-step message passing.
computing the Chamfer Distance (CD) between the current
Specifically,forl=0,1,2, ,L 1,asinglemessagepassing
··· − state z t and the predicted state zˆ t:
step is as follows:
(cid:88) (cid:88)
h hl
e
l+ +k,1
1t
== ff
E
pp rr oo pp (( hh ll w,t, ,hl
u,
(cid:88)t),
hl+1),
( (4 5)
)
LCD(zˆ t,z t)=
x
∈Vtym ∈i Vˆn t||x −y ||2 2+
y ∈Vˆ
txm ∈i Vn t||x −y ||2 2, (7)
vi,t
V
vi,t
j ∈N(vi,t)
ej,t
where ˆ
t
and ˆ
t
denote the vertex sets at state z
t
and zˆ t,
V V
where (v ) indicates the index set of edges in which vertex respectively. The actions for curiosity-driven interactions are
i,t
N
iisthereceiverattimet,andListhetotalnumberofmessage optimizedusingtheModel-PredictivePathIntegral(MPPI)[48]
passing steps. Finally, one vertex decoder fdec predicts the trajectory optimization algorithm to maximize the above
system’s state at the next time step: vˆ =Vfdec(hL ). Chamfer Distance.
Translation equivariance.
Translati i,t o+ n1 equiVvarianvi c, et
is a Forinverseoptimizationatthetth interactionstep,weadopt
desiredpropertyfordynamicsmodels.Formally,foranyglobal gradient-free optimizers including Bayesian Optimization (BO)
3D translation added to the particle locations, the predictions forsingle-dimensionalphysicalpropertyvariablesandCMA-ES
should also be translated identically. We enforce translation formulti-dimensionalvariables.Weinstantiatetheoptimization
equivariance by passing the position difference of receiver and problem described in Eq. 2 by specifying the cost function
sender particles to the edge encoder fenc, instead of passing cost(zˆ ,z )astheChamferDistancebetweenthedynamics
i+1 i+1
absolution particle positions to the verEtex encoder fenc. prediction and the true outcome after each interaction:
Vt 1
ϕˆ
=argmin(cid:88)−
(zˆ ,z ), (8)
t CD i+1 i+1
ϕ L
i=0
where zˆ =f(z ,u ;ϕ,M).
i+1 i i
For some materials whose physical properties span a large
range (e.g., stiffness for ropes), the test object can potentially
fall outside the training distribution of the model. Our material-
conditionedmodelallowsforgeneralizationbeyondthetraining
domain by directly setting the domain of ϕˆ at the adaptation
stage to be an extension of the maximal range of ϕ in the
training data. Specifically, the minimum value and maximum
valueforϕˆisϕ 0.2(ϕ ϕ )andϕ +0.2(ϕ
min max min max max
− − −
ϕ ),whereϕ andϕ arethemaximumandminimum
min max min
value of ϕ in the training dataset.
(a) Objects considered in this work
D. Closed-Loop Model-Based Planning
4 RealSense D455
×
Usingtheestimatedphysicsparameterϕˆ,thelearnedmodelf
xArm 6
adaptstonewobjects,yieldinglowerdynamicspredictionerrors
on the few-shot, curiosity-driven online interaction dataset. 3D-printed
Thus, we can also use the adapted model to perform closed- pusher
loop planning for material-specific manipulation tasks within a
ModelPredictiveControl(MPC)framework[5].Theimproved
dynamics prediction accuracy after few-shot adaptation will
help the robot manipulate the object more efficiently and
effectively towards goal configurations. workspace
Concretely, the model-based control pipeline is defined as
rope
follows: given the state space Z and the action space U, the
cost function is a mapping from Z U to R. For each starting
state z Z, we iteratively samp× le actions u T in the
0 ∈ { i }i=1 (b) Robot workspace
action space, apply the learned dynamics model to predict the
outcome,andapplytheMPPItrajectoryoptimizationalgorithm
for the action sequence u that minimizes the cost function.
i
{ }
In our experiments, the cost function includes a task-related
term that measures the distance from the current state to the 1 2 3
desired target, along with other penalty terms for infeasible
(c) Robot tools
actions and collision avoidance. Please refer to Sec. B.2 of the
supplementary material for details. Fig. 3: Real-world setup. (a) Our study involves 22 objects cat-
egorized into four types of materials, each with distinct physical
IV. EXPERIMENTS characteristics: (i) 9 varieties of ropes, such as cotton ropes and
cables, (ii) 9 granular materials, including items like toy blocks and
In this section, we evaluate the proposed framework across coffee beans, (iii) 5 pieces of cloth made from different fabrics like
a diverse range of object manipulation tasks. Our experiments cotton and synthetic fibers, (iv) 2 boxes of varying shapes, whose
centers of pressure we alter by placing weights inside them. (b) The
are designed to answer the following questions: (1) Is the
dashed white circles show four calibrated RGB-D cameras mounted
GBND model capable of accurately predicting the movements
at four corners of the table. The robot is outfitted with specialized
of objects with varied physical properties? (2) Can the test- end effectors to interact with the objects in its operational area. (c)
timeadaptationmoduleeffectivelyestimatereal-worldphysical We employ three different tools for specific tasks: (1) a flat pusher
propertiesofobjectsthroughfew-shotinteractions?(3)Towhat for granular piles gathering, (2) a cylindrical pusher for pushing
rigid boxes and straightening ropes, (3) an xArm gripper for cloth
extent does the integration of the adaptation module enhance
relocating.
themodel’sabilityformodel-basedplanninginthedownstream
manipulation tasks? Rigid Box Pushing. The task is to use a point contact to
push a box to a target position and orientation, which demands
A. Evaluation Materials and Corresponding Tasks
precise control over the translation and rotation motions in
To demonstrate the modeling power of our framework for the presence of uncertainty of the center of pressure [54]. The
diverse materials, we implement one task for each of the physical property variable is defined to be the normalized 2D
4 material categories: rigid box pushing, rope straightening, position of the center of mass from the top view. As illustrated
granular pile gathering, and cloth relocating. in Fig. 2a, it is a 2-dimensional variable ϕ = [c ,c ] with
x yStates and action w/o Adaptation Ours States and action w/o Adaptation Ours
: Center of Pressure Low stiffness
: Center of Pressure High stiffness
(a) Rigid box (b) Rope
Small granular piece Low stiffness
Large granular piece High stiffness
(c) Granular object (d) Cloth
Fig. 4: Qualitative results on dynamics prediction. We conduct qualitative comparisons to assess the performance of our method against
the baseline of a GNN without adaptation, focusing on the one-step prediction of dynamics across eight objects within four distinct material
categoriesexhibitingvaryingextremephysicalproperties.Theresults,delineatedbyreddashedboxes,demonstratethatourapproachsurpasses
the baseline in accurately capturing the variations in dynamics that arise due to differences in the objects’ physical properties.
target configuration on the tabletop. We consider the stiffness
of the rope as the physical property variable and define it as a
normalized continuous variable ϕ (0,1) where ϕ = 0 and
∈
ϕ=1 correspond to the minimal and maximal stiffness in the
simulator, respectively.
Granular Pile Gathering. The target is defined as a region
on the tabletop, and the task is to gather the granular piles
(a) Rigid box (b) Rope in an arbitrary initial distribution into the target region. We
consider the granular size/granularity as the physical property
variable and use a normalized variable ϕ (0,1) to represent
∈
the size of a single grain in the pile.
Cloth Relocating. The task is to use grippers to grasp the
cloth and drag it on the table to place the cloth in the target
configuration. We use a continuous variable ϕ (0,1) to
∈
represent the stiffness of the cloth, which affects whether a
piece of cloth will wrinkle or fold during a drag.
(c) Granular object (d) Cloth
B. Environment and Evaluation Setup
Fig. 5: Quantitative results on dynamics prediction. We validate
our model’s effectiveness on a test set of 200 objects with distinct Simulation. Simulationsofdeformableandgranularmaterials
physical properties for each material type in simulation. Across all
are conducted using NVIDIA FleX [22, 31], a position-based
types of materials, our approach surpasses the baseline with respect
simulation framework designed to model interactions between
to both the precision and consistency of predictions.
objects of varying materials across multiple tasks, including
range c ,c ( 0.5,0.5). We use the mean squared error as
x y pushing granular objects [47], straightening ropes [26], and
∈ −
the cost function.
unfolding clothing [14]. Additionally, Pymunk [4] is utilized
Rope Straightening. The task is to rearrange the rope to a for simulating boxes that vary in shape and center of pressure.
xob
raguS
xob
rekcarC
naeb
eeffoC
etalocohC
rorrE
rorrE
rorrE
rorrE
nraY
remyloP
ladoM
nottoCweight location
weight location Y
Y
X
X
Fig.6:Experimental results on physical property estimation.Throughtheinverseoptimizationprocess,weestimatethephysicalproperties
ofreal-worldobjects.Foreachmaterialtype,wedisplaytheoptimizationtrajectoriesalongsidetheirassociatedcosts,measuredbytheChamfer
Distance,fortwoobjectswithnotablycontrastingphysicalattributes.Therightmostcolumndemonstratesthatourestimatedvaluesalignwith
human perceptions regarding the perceptual order of objects based on their physical property values, such as stiffness and granularity.
Foreachmaterialtype,adatasetconsistingof1000episodes Real World. Fig. 3 presents the general setup in both the
is generated, with each episode featuring 5 random robot- simulator and the real world. In the real-world experiments,
object interactions. Within each episode, an object is assigned we use a UFACTORY xArm 6 robot with 6 DoF and xArm’s
random physical properties (such as stiffness and granule size) parallel gripper. For rigid box pushing and rope straightening
that fall within a pre-defined range. To simulate interactions tasks, we substitute the original grippers with a cylinder
between the robot and the object, five random trajectories, stick while we utilize a flat pusher for the granular pile
involvingeitherpushingorpullingactions,arecreatedforevery manipulationtask.Thesetoolsare3D-printedandthesamewith
object. Throughout these interactions, data on the positions of thesimulationsetuptomitigatethesim-to-realgap.Wefixfour
particles and the robot’s end-effector are gathered, which are calibrated RealSense D455 RGBD cameras at four locations
thenutilizedformodeltraining.Moredetailsonthesimulation surrounding the workspace to capture the RGBD images at
environment and data collection can be found in Sec. B.1 of 15Hz and 1280x720 resolution. The robot manipulates objects
the supplementary material. within a 70cm 45cm planar workspace.
×
xob
digiR
)a(
epoR
)b(
elip
ralunarG
)c(
htolC
)d(Time Time
Target
Target
Initial
Initial
# Steps: 3 CD = 0.01 # Steps: 6 CD = 0.09
Initial
Target
Target
Initial
# Steps: 10 CD = 0.80 # Steps: 10 CD = 0.13
(a) Rigid box pushing (b) Rope straightening
Time Time
Target
Area
Target
Initial
Initial
# Steps: 9 CD = 0.008 # Steps: 6 CD = 0.06
Target
Area
Target
Initial
Initial
# Steps: 10 CD = 0.02 # Steps: 10 CD = 0.20
(c) Granular pile gathering (d) Cloth relocating
Fig. 7: Qualitative results on closed-loop feedback planning. We present a qualitative comparison of MPC performance by contrasting our
method across four tasks with the baseline model that does not employ physical property adaptation. Visualizations shown here demonstrate
that our method effectively achieves the target configuration, whereas the baseline, even with more action steps, still exhibits a noticeable
discrepancy compared to the target.
Implementation Details. In all experiments, we assume the appendix, we include additional comparisons by finetuning
material type M to be known, and the particles of the same the GNN baseline and adapting a physics-based simulator to
object share the same M and physical property variable ϕ. To demonstrate the effectiveness of our proposed conditioning
extract object point clouds from raw RGB-D inputs, we deploy method and the benefits of learned dynamics models.
the GroundingDINO [28] and Segment Anything [20] model
to detect and segment the table surface and objects. For the C. Forward Dynamics Prediction
target object, we fuse the segmented partial point cloud from
4 views and apply a farthest point sampling method to a fixed Fig. 4 shows the qualitative comparisons between our
pointwisedistancethreshold.Forthecylinderstickandgripper, material-conditioned GBND model and the baseline model
we use one particle to represent the end effector position, and Ours w/o Adaptation. The comparisons reveal that, with
for the flat granular pusher, we use 5 points to represent the estimatedphysicalproperty,themodel’spredictionmatchesthe
end effector position and geometry. interaction outcome more accurately. For instance, in the rope
scenario, the baseline model’s prediction fails to capture both
Baselines. To demonstrate the importance of parameter the below-average stiffness of the yarn object and the above-
conditioning, we consider two baseline methods in our main average stiffness of a polymer rope. In contrast, our method
experiments: (1) GNN uses a graph neural network with the successfullyaccountsforvariationsintheirmotions,exhibiting
same architecture of our model, which is trained separately more precise forecasts of unusual behaviors. Likewise, our
for each material category, but not conditioned on the physics model surpasses the baseline in scenarios involving materials
parameter ϕ. (2) Ours w/o Adaptation is an ablated version of withextremephysicalproperties,suchasrigidboxesthatdiffer
our material-adaptive model by using only the mean physical in center of pressure, granular materials of various sizes, and
property variable ϕ¯ as input in deployment. In Sec. A of the clothes of differing stiffness.
sruO
noitatpadA
o/w
sruO
noitatpadA
o/w
sruO
noitatpadA
o/w
sruO
noitatpadA
o/w(a) Rigid box pushing (b) Rope straightening
(c) Granular pile gathering (d) Cloth relocating
Fig. 8: Quantitative results on planning. For each task, we use the same target configuration and initial configuration for the baseline
method and our approach. We repeat each experiment-model pair 5 times and visualize (i) the median error curve w.r.t. planning steps (area
between 25 and 75 percentiles are shaded) and (ii) the success rate curve w.r.t error thresholds. Our approach consistently outperforms the
baseline method by being more accurate and using fewer action steps.
Fig. 5 further validates our model’s effectiveness on a with the actual stiffness from human perception.
simulated test set of 200 objects each with distinct physical Granular. As shown in Fig. 6c, we test our model on 9
properties. Our approach surpasses both baselines, GNN and
different types of granular objects by selecting representative
Ours w/o Adaptation, demonstrating superior accuracy and
objects of each granularity level, ranging from approximately
stability for all the material types addressed in our study. Par-
1cmto3cm.Resultsshowthatthepredictedgranularityranking
ticularly, for rigid boxes, our model significantly outperforms
is consistent with the actual granular size. The model correctly
the baselines with a near-perfect prediction accuracy.
predicts granola as the smallest grains and the toy blocks as
the largest grains.
D. Physical Property Estimation
Cloth. As shown in Fig. 6d, we test our model on 5 different
For physical property estimation, we randomly initialize the
cloth instances, each with a different fabric material. The
object location on the tabletop and perform 10 interactions.
modelcorrectlyidentifiesthemodalasthesoftestcloth(lowest
Rigid Box. We use two boxes with different sizes: the sugar stiffness). As another soft material, the flannel cloth is also
box (175mm 89mm) and the cracker box (210mm 158mm). estimated to be softer than cotton and microfiber cloths. While
× ×
We initialize the center of pressure (CoP) to be at 4 different the training dataset does not contain any plastic-like materials,
locations for each box by putting weights at different locations the model generalizes to a piece of plastic sheet and correctly
insidethebox.AvisualizationofallCoPs’normalizedpositions predicts that it is very stiff.
and our predicted CoP positions is shown in Fig. 6a. From the Furthermore, in Sec. C.1 of the supplementary material,
figure, we can observe that for all 8 data points, the predicted we present additional experiments that consider multiple
CoP positions are close to the ground truth CoP position. parameters, namely the stiffness and friction of ropes. The
Moreover, the heatmap error shows that the low-error region resultsdemonstratethatourmethodcanbeextendedtorecover
for the CoP location forms a single global minima, and the more than one type of physical property simultaneously and
predictedCoPpositionsconvergetoaroundtheminimumvalue yield better accuracy in dynamics prediction.
after around 5 interaction steps.
E. Model-Based Planning
Rope. We test our model on 9 different types of ropes. As
showninFig.6b,themodelcanextrapolatebeyondthetraining Wefurtherdemonstratethatourmaterial-conditionedGBND
data range [0.0, 1.0] and estimate out-of-range values for ropes model and physical property adaptation can be integrated into
withextremestiffness/softness.ThemeanCDontheinteraction an MPC framework to achieve a series of robotic manipulation
observations gives clear and unique minimum points, and the tasks. Our experiments cover 4 distinct tasks outlined in
stiffness ranking of the different types of ropes is consistent Sec. IV-A, with a maximum limit of 10 planning stepsimposed. Across all material types, our approach consistently Experiential learning of intuitive physics. Advances in
meets the objectives within the allotted planning steps, unlike neural information processing systems, 29, 2016. 2
the baseline approach Ours w/o Adaptation, which fails to [2] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,
achieve the goals due to its disregard for physical properties. Roy H Campbell, and Sergey Levine. Stochastic varia-
For instance, in the rigid box pushing task, the baseline tional video prediction. arXiv preprint arXiv:1710.11252,
method incorrectly assumes the geometric center as the center 2017. 2
of pressure, leading to inaccurate predictions of the box’s [3] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst,
straightforward movement post-push. Conversely, our method Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz
dynamically adjusts the center of pressure estimations during Malinowski, Andrea Tacchetti, David Raposo, Adam
the interactions, thereby reaching the desired configuration in Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song,
justthreesteps.Furthermore,asdepictedinFig.1,thedynamics Andrew Ballard, Justin Gilmer, George Dahl, Ashish
of pushing granular objects of different sizes vary significantly Vaswani, Kelsey Allen, Charles Nash, Victoria Langston,
- larger granules push forward while smaller ones tend to stack Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet
andleaveatrail.Thebaselinemethod,treatingthemotionoftoy Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and
blocks and average granular piles similarly, fails to accumulate Razvan Pascanu. Relational inductive biases, deep
them in the target zone. Our method, however, identifies and learning, and graph networks, 2018. 2
adaptstothevarieddynamicsofgranularmaterials,successfully [4] Victor Blomqvist. Pymunk. https://pymunk.org, Novem-
completing the task. ber 2022. 6, 13, 14
Fig. 8 offers quantitative results comparing the performance [5] Eduardo F. Camacho and Carlos Bordons Alba. Model
of our method against the baseline method Ours w/o Adapta- Predictive Control. Springer Science & Business Media,
tion, focusing on efficiency and error tolerance. Across four 2013. 5
distincttasks,ourapproachdemonstratessuperiorperformance, [6] Zhenfang Chen, Kexin Yi, Yunzhu Li, Mingyu Ding,
achievinglowererrorswithinaconstrainednumberofplanning AntonioTorralba,JoshuaB.Tenenbaum,andChuangGan.
steps and attaining a higher success rate under a stringent error Comphy:Compositionalphysicalreasoningofobjectsand
margin. events from videos, 2022. 2
[7] KurtlandChua,RobertoCalandra,RowanMcAllister,and
V. CONCLUSIONANDFUTUREWORK Sergey Levine. Deep reinforcement learning in a handful
of trials using probabilistic dynamics models, 2018. 2
We present AdaptiGraph, a unified graph-based neural
[8] Mingyu Ding, Zhenfang Chen, Tao Du, Ping Luo,
dynamics framework for modeling multiple materials with
Joshua B. Tenenbaum, and Chuang Gan. Dynamic visual
unknown physical properties. We propose to condition the
reasoning by learning differentiable physics models from
dynamics model on physical property variables and perform
video and language, 2021. 2
online few-shot physical property estimation. Experiments
[9] Herbert Edelsbrunner and Ernst P Mücke. Three-
show that AdaptiGraph can precisely simulate the dynamics of
dimensional alpha shapes. ACM Transactions On Graph-
multipledeformablematerials,andadapttoobjectswithvarying
ics (TOG), 13(1):43–72, 1994. 13
physical properties during deployment. We demonstrate the
[10] Ben Evans, Abitha Thankaraj, and Lerrel Pinto. Context
effectiveness of our framework across a wide range of objects
is everything: Implicit identification for dynamics adapta-
in manipulation tasks.
tion. In 2022 International Conference on Robotics and
AdaptiGraphisaflexibleframework.Currently,wetrainour
Automation (ICRA), pages 2642–2648. IEEE, 2022. 2
model on four material types (ropes, granular objects, rigid
[11] Chelsea Finn and Sergey Levine. Deep visual foresight
boxes,andcloth)andasingletypeofphysicalpropertyforeach
for planning robot motion. In 2017 IEEE International
material.Afuturedirectionofourworkistoextendourmethod
Conference on Robotics and Automation (ICRA), pages
to include more object materials and a more comprehensive
2786–2793. IEEE, 2017. 1, 2
set of physical properties that determine object dynamics. It
[12] Barbara Frank, Rüdiger Schmedding, Cyrill Stachniss,
is also possible to model heterogeneous object interactions
Matthias Teschner, and Wolfram Burgard. Learning
using our framework by learning the dynamics model on a
the elasticity parameters of deformable objects with a
material-conditioned heterogeneous graph.
manipulation robot. In 2010 IEEE/RSJ International
ACKNOWLEDGMENT Conference on Intelligent Robots and Systems, pages
1877–1883. IEEE, 2010. 2
Thisworkissupported,inpart,byNIFAAward2021-67021-
[13] Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun
34418. We thank Mingtong Zhang, Binghao Huang, Yixuan
Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh.
Wang, and Hanxiao Jiang for the helpful discussions.
Physically grounded vision-language models for robotic
manipulation. arXiv preprint arXiv:2309.02561, 2023. 2
REFERENCES
[14] Huy Ha and Shuran Song. Flingbot: The unreasonable
[1] Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra effectivenessofdynamicmanipulationforclothunfolding.
Malik, and Sergey Levine. Learning to poke by poking: In Conference on Robotic Learning (CoRL), 2021. 6[15] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mo- 256–266. PMLR, 2022. 2
hammad Norouzi. Dream to control: Learning behaviors [28] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
by latent imagination. arXiv preprint arXiv:1912.01603, Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su,
2019. 1 Jun Zhu, et al. Grounding dino: Marrying dino with
[16] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, groundedpre-trainingforopen-setobjectdetection. arXiv
andJimmyBa.Masteringatariwithdiscreteworldmodels. preprint arXiv:2303.05499, 2023. 8
arXiv preprint arXiv:2010.02193, 2020. 1 [29] Ziang Liu, Genggeng Zhou, Jeff He, Tobia Marcucci,
[17] François Robert Hogan and Alberto Rodriguez. Feedback Li Fei-Fei, Jiajun Wu, and Yunzhu Li. Model-based con-
control of the pusher-slider system: A story of hybrid trol with sparse neural dynamics. In Thirty-seventh Con-
and underactuated contact dynamics. arXiv preprint ference on Neural Information Processing Systems, 2023.
arXiv:1611.08268, 2016. 2 URL https://openreview.net/forum?id=ymBG2xs9Zf. 2
[18] Ryan Hoque, Daniel Seita, Ashwin Balakrishna, Aditya [30] Alberta Longhini, Marco Moletta, Alfredo Reichlin,
Ganapathi, Ajay Kumar Tanwani, Nawid Jamali, Katsu Michael C Welle, David Held, Zackory Erickson, and
Yamane, Soshi Iba, and Ken Goldberg. Visuospatial Danica Kragic. Edo-net: Learning elastic properties of
foresight for multi-step, multi-task fabric manipulation. deformable objects from graph dynamics. In 2023 IEEE
arXiv preprint arXiv:2003.09044, 2020. 1, 2 International Conference on Robotics and Automation
[19] Isabella Huang, Yashraj Narang, Ruzena Bajcsy, Fabio (ICRA), pages 3875–3881. IEEE, 2023. 2
Ramos, Tucker Hermans, and Dieter Fox. Defgraspnets: [31] Miles Macklin, Matthias Müller, Nuttapong Chentanez,
Grasp planning on 3d fields with graph neural nets, 2023. and Tae-Yong Kim. Unified particle physics for real-time
2 applications. ACM Transactions on Graphics (TOG), 33
[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi (4):1–12, 2014. 6, 13, 14, 15
Mao,ChloeRolland,LauraGustafson,TeteXiao,Spencer [32] Carsten Moenning and Neil A Dodgson. Fast marching
Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Seg- farthest point sampling. Technical report, University of
ment anything. arXiv preprint arXiv:2304.02643, 2023. Cambridge, Computer Laboratory, 2003. 4
8 [33] Anusha Nagabandi, Kurt Konoglie, Sergey Levine, and
[21] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra VikashKumar. DeepDynamicsModelsforLearningDex-
Malik. Rma: Rapid motor adaptation for legged robots. terous Manipulation. In Conference on Robot Learning
2021. 2 (CoRL), 2019. 2
[22] Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenen- [34] Tao Pang, HJ Terry Suh, Lujie Yang, and Russ Tedrake.
baum, and Antonio Torralba. Learning particle dynamics Global planning for contact-rich manipulation via local
for manipulating rigid bodies, deformable objects, and smoothing of quasi-dynamic contact models. IEEE
fluids. In ICLR, 2019. 2, 4, 6, 13, 14, 15 Transactions on Robotics, 2023. 2
[23] YunzhuLi,JiajunWu,Jun-YanZhu,JoshuaBTenenbaum, [35] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez,
Antonio Torralba, and Russ Tedrake. Propagation net- and Peter W Battaglia. Learning mesh-based simulation
works for model-based control under partial observation. with graph networks. arXiv preprint arXiv:2010.03409,
In ICRA, 2019. 2 2020. 2
[24] Yunzhu Li, Toru Lin, Kexin Yi, Daniel Bear, Daniel [36] Kavya Puthuveetil, Sasha Wald, Atharva Pusalkar,
Yamins, Jiajun Wu, Joshua Tenenbaum, and Antonio Pratyusha Karnati, and Zackory Erickson. Robust body
Torralba. Visual grounding of learned physical models. exposure (robe): A graph-based dynamics modeling
In International conference on machine learning, pages approach to manipulating blankets over people. IEEE
5927–5936. PMLR, 2020. 2 Robotics and Automation Letters, 2023. 2
[25] Junbang Liang, Ming Lin, and Vladlen Koltun. [37] AlvaroSanchez-Gonzalez,JonathanGodwin,TobiasPfaff,
Differentiable cloth simulation for inverse problems. Rex Ying, Jure Leskovec, and Peter Battaglia. Learning
In H. Wallach, H. Larochelle, A. Beygelzimer, to simulate complex physics with graph networks. In
F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Internationalconferenceonmachinelearning,pages8459–
Advances in Neural Information Processing Systems, 8468. PMLR, 2020. 2
volume 32. Curran Associates, Inc., 2019. URL [38] Yu She, Shaoxiong Wang, Siyuan Dong, Neha Sunil,
https://proceedings.neurips.cc/paper_files/paper/2019/ Alberto Rodriguez, and Edward Adelson. Cable manipu-
file/28f0b864598a1291557bed248a998d4e-Paper.pdf. 2 lation with a tactile-reactive gripper, 2020. 2
[26] Xingyu Lin, Yufei Wang, Jake Olkin, and David Held. [39] Haochen Shi, Huazhe Xu, Zhiao Huang, Yunzhu Li, and
Softgym: Benchmarking deep reinforcement learning for Jiajun Wu. Robocraft: Learning to see, simulate, and
deformable object manipulation. In Conference on Robot shape elasto-plastic objects with graph networks. arXiv
Learning, 2020. 6, 14 preprint arXiv:2205.02909, 2022. 2, 4
[27] Xingyu Lin, Yufei Wang, Zixuan Huang, and David [40] Haochen Shi, Huazhe Xu, Samuel Clarke, Yunzhu Li,
Held. Learning visible connectivity dynamics for cloth and Jiajun Wu. Robocook: Long-horizon elasto-plastic
smoothing. In Conference on Robot Learning, pages object manipulation with diverse tools. arXiv preprintarXiv:2306.14447, 2023. 2, 4 of heterogeneous deformable objects in real time. In
[41] Dave Shreiner and The Khronos OpenGL ARB Working 2023 IEEE International Conference on Robotics and
Group. OpenGL Programming Guide: The Official Guide Automation (ICRA), pages 12583–12589, 2023. doi: 10.
to Learning OpenGL, Versions 3.0 and 3.1. Addison- 1109/ICRA48891.2023.10160731. 2
WesleyProfessional,7thedition,2009.ISBN0321552628. [53] Kuan-Ting Yu, Maria Bauza, Nima Fazeli, and Alberto
14 Rodriguez. More than a million ways to be pushed. a
[42] Priya Sundaresan, Rika Antonova, and Jeannette Bohgl. high-fidelity experimental dataset of planar pushing. In
Diffcloud: Real-to-sim from point clouds with differen- 2016 IEEE/RSJ international conference on intelligent
tiable simulation and rendering of deformable objects. In robots and systems (IROS), pages 30–37. IEEE, 2016. 2
2022 IEEE/RSJ International Conference on Intelligent [54] Jiaji Zhou, Yifan Hou, and Matthew T Mason. Pushing
Robots and Systems (IROS), pages 10828–10835. IEEE, revisited: Differential flatness, trajectory planning, and
2022. 2 stabilization. The International Journal of Robotics
[43] Jean-francois Tremblay, Francois Robert Hogan, Research, 38(12-13):1477–1489, 2019. 5
David Paul Meger, and Gregory Lewis Dudek. Learning
active tactile perception through belief-space control,
November 2 2023. US Patent App. 18/141,031. 15
[44] Fish Tung, Mingyu Ding, Zhenfang Chen, Daniel M.
Bear, Chuang Gan, Joshua B. Tenenbaum, Daniel L. K.
Yamins, Judith Fan, and Kevin A. Smith. Physion++:
Evaluating physical scene understanding that requires
online inference of different physical properties. arXiv,
2023. 2
[45] Chen Wang, Shaoxiong Wang, Branden Romero, Filipe
Veiga, and Edward H Adelson. Swingbot: Learning
physical features from in-hand tactile exploration for dy-
namic swing-up manipulation. In IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS),
2020. 2
[46] Yi Ru Wang, Jiafei Duan, Dieter Fox, and Siddhartha
Srinivasa. Newton: Are large language models capable
of physical reasoning? arXiv preprint arXiv:2310.07018,
2023. 2
[47] Yixuan Wang, Yunzhu Li, Katherine Driggs-Campbell,
Li Fei-Fei, and Jiajun Wu. Dynamic-Resolution Model
Learning for Object Pile Manipulation. In Proceedings
of Robotics: Science and Systems, Daegu, Republic of
Korea, July 2023. doi: 10.15607/RSS.2023.XIX.047. 2,
4, 6, 14
[48] Grady Williams, Andrew Aldrich, and Evangelos A
Theodorou. Model predictive path integral control: From
theory to parallel computation. Journal of Guidance,
Control, and Dynamics, 40(2):344–357, 2017. 4
[49] Jiajun Wu, Ilker Yildirim, Joseph J Lim, William T
Freeman, and Joshua B Tenenbaum. Galileo: Perceiving
physical object properties by integrating a physics engine
with deep learning. In Advances in Neural Information
Processing Systems, pages 127–135, 2015. 2
[50] ZhenjiaXu,JiajunWu,AndyZeng,JoshuaBTenenbaum,
andShuranSong. Densephysnet:Learningdensephysical
object representations via multi-step dynamic interactions.
In Robotics: Science and Systems (RSS), 2019. 2
[51] Linhan Yang, Bidan Huang, Qingbiao Li, Ya-Yen
Tsai, Wang Wei Lee, Chaoyang Song, and Jia Pan.
Tacgnn:learning tactile-based in-hand manipulation with
a blind robot, 2023. 2
[52] ShaoxiongYaoandKrisHauser.EstimatingtactilemodelsAPPENDIX Unified Separate Oursw/o
Method Ours
GNN GNN Adaptation
Contents Cond.onmaterialtype?
Cond.onphysicalproperty?
A Comparison with Additional Baselines 13 Onlineadaptation?
A.1 Ablation on Material Conditioning . . . 13 TABLE I: Difference in baseline models. We ablate the online
A.2 Adaptation Using Different Base Models 13 adaptation, material type conditioning, and continuous physical
propertyconditioningmodulesinthelistedbaselines.Thequantitative
B Additional Implementation Details 14 results are shown in Fig. 9.
B.1 Simulation and Data Collection . . . . . 14
B.2 Model-Based Planning . . . . . . . . . 14
C Discussion and Potential Extensions 15
C.1 Multiple Properties Recovering . . . . . 15
C.2 Identification with Uncertainty . . . . . 15
A COMPARISONWITHADDITIONALBASELINES (a) Rope (b) Cloth
A.1 Ablation on Material Conditioning
Expanding on Fig. 5 from the main paper, we introduce an
additional baseline, Unified GNN, to study the importance of
material type conditioning. As outlined in Tab. I, we establish
the following baselines: (1) Unified GNN, a singular GNN
model trained on a combined dataset of rope, cloth, and
granular materials, without any conditioning on material types (c) Granular object
or physical properties; (2) Separate GNN, which employs Fig. 9: Quantitative results for ablation study. We assessed our
a graph neural network with the same architecture as our method and the baselines, as outlined in Tab. I, over 200 objects
model but lacks conditioning on physical parameters, and is possessingdiversephysicalparametersacrossrope,cloth,andgranular
materials.
independently trained for each material category; (3) Ours w/o
Adaptation, an ablated version of our material-adaptive model
conditioned on the mean physical property variable ϕ¯.
Pymunk [4] for rigid boxes. Given the observed point cloud,
The quantitative findings are displayed in Fig. 9. Within
we map the point cloud to object states in simulation with
this assessment, the Unified GNN has the lowest performance,
a perception model and then use the simulators to perform
with the highest variance, showing that it fails to model the
dynamics rollout and optimization-based physical property
complex dynamics brought by distinct material types and
estimation.
physicalproperties.TheSeperateGNN baselineperformsbetter
on individual material types than the Unified GNN, reaching We design category-specific perception models to mitigate
comparable performance with Ours w/o Adaptation. However, the sim-to-real gap. For boxes, we extract the 4 corners of
thelackofphysicalpropertyadaptabilityhasledtoinaccuracies. the box from the top view and create an identical 2D box in
Overall,Ourshasachievedthebestperformanceinallmaterial Pymunk. For ropes, we apply mesh reconstruction based on
types. The results have demonstrated the relative importance alpha shapes [9] to derive the rope mesh in the FleX simulator.
of material conditioning, physical property conditioning, and For clothes and granular objects, we extract the contour of
online adaptation in dynamics prediction performance. the point cloud’s projection on the table surface, and construct
object instances, i.e., a piece of cloth or granular pieces, that
A.2 Adaptation Using Different Base Models exactly cover the contour region.
In our main paper, we have showcased the superior per- Results. From Fig. 10, we can observe that our method,
formance of our model in terms of dynamics prediction with online adaptation, exhibits the lowest dynamics prediction
error when compared to two baseline models: GNN, which error. It achieves an error reduction of 90.8% for rigid boxes,
does not use physical property conditioning, and Ours w/o 7.9% for ropes, 4.0% for clothes, and 9.0% for granular
Adaptation, where the online adaptation module is removed. In objects compared to our model without adaptation. Notably,
this section, we further compare our model to (1) simulators the error reduction ratio surpasses that achieved by fine-tuning
incorporating physical property adaptation and (2) fine-tuning an unconditional GNN-based dynamics model. Compared
unconditional GNN. We evaluate their dynamics prediction with simulator-based physical property adaptation, our model
error after adaptation in few-shot real-world interactions. demonstrates 6.0% lower dynamics prediction error for rigid
Settings. We employ the same simulators used for generating boxes,5.2%forropes,10.3%forclothes,and8.7%forgranular
our training data: FleX [22, 31] for deformable objects and objects. We attribute this improvement to the inherent systemFig.10:Quantitative comparison with baselines adaptation approaches.Wereportthemeanandstandarderrorofthedynamicsprediction
errors after online adaptation on 5 interactions. The numbers denote the mean value of each bar. For both material categories, our method
achieves the lowest error across all methods after adaptation. Error metrics: rigid box - MSE, others - CD.
identification error and the instability of the simulator. Using Cloth. The cloth simulation environment is created following
a learning-based dynamics model directly on point clouds the approach used in SoftGym [26], as shown in Fig. 11c.
enhances our model’s robustness to noisy visual inputs. For the cloth properties, we vary the stretch stiffness (which
Moreover, our model is significantly faster than simulators. determines resistance to elongation), bend stiffness (resistance
Running the Bayesian optimization algorithm for 50 iterations tobending),andshearstiffness(resistancetoslidingortwisting
takes approximately 7 seconds for our model on a desktop deformations). We randomly create rectangular clothes with
computer equipped with an i9-13900K CPU and an NVIDIA lengths and widths uniformly distributed from 19 to 21 cm
GeForce RTX 4090 GPU, whereas it takes approximately 900 and 31 to 34 cm, respectively. We collect data across 1000
seconds for the FleX simulator. episodes, with each episode involving 5 continuous random
interactions on one piece of cloth.
B ADDITIONALIMPLEMENTATIONDETAILS Granular Object. Adhering to the setup in [47], we use
irregular polygonal meshes to represent granular objects, as
B.1 Simulation and Data Collection shown in Fig. 11d. The scale of the granules is uniformly
sampledin1 3cm.Wealsorandomizethenumberofgranular
For training our GBND model, we generate datasets en-
∼
objects and the initial coverage area of the pile. The collected
compassing variable physical properties in simulators. In
data consists of 1000 episodes, each comprising 5 continuous
FleX [22, 31], we render the robot workspace and the xArm6
random interactions on one granular pile.
robot mesh through OpenGL [41] from four camera angles to
closelymirrorourreal-worldconfiguration.InPymunk[4],the
B.2 Model-Based Planning
robot pusher is represented as a circular shape with a radius of
1cm.Fig.11showsthesimulationsetupforourdatacollection. We apply the MPPI trajectory optimization algorithm for
The subsequent paragraphs detail the data generation process model-based planning. Given the dynamics model z t+1 =
for each material type in the simulation. f(z t,u t) (here we omit the material and physical property
conditions for convenience), the cost function we minimize is:
Rigid Box. As shown in Fig. 11a, a circular rigid pusher
interacts with a rigid box from random positions and angles. (u )=ϕ(z ,z )+l(z ,u ), (9)
0:T 1 T ∗ 0 0:T 1
Thelengthoftherigidboxisuniformlysampledfrom150 300 J − −
∼
mm and the width is uniformly sampled from 50 200 mm. where the task term ϕ(z T,z ∗) measures the distance from the
∼
The center of pressure (CoP), represented by a 2-dimensional currentstatetothetargetz ∗,andthepenaltyterml(z 0,u 0:T 1)
normalized coordinate in ( 0.5,0.5)2, is sampled uniformly produces high cost for infeasible actions. −
−
over the box surface. The friction coefficient between the box Task term. For rope straightening and cloth relocating, the
and the table is fixed as a control variable. We generate 1000 cost term is defined as the Chamfer Distance between the
data episodes, each containing 1 pushing action on 1 box with current state z and the target state:
T
random size and CoP.
ϕ(z ,z )=CD(z ,z ) (10)
Rope. As shown in Fig. 11b, we use a cylinder pusher with T ∗ T ∗
a radius of 1cm to randomly interact with a simulated rope. Forgranularpilegathering,weusethenearestdistancedist z∗()
The workspace in the simulation measures 90 cm 70 cm. We from object particles to the target rectangle: ·
×
uniformly randomize the length, thickness, and stiffness of the
1 (cid:88)
rope. We collect 1000 episodes of data, each comprising 5 ϕ(z T,z ∗)= dist z∗(x), (11)
continuous random pushes on one rope. z T
| |x ∈zT(a) Rigid box (b) Rope (c) Cloth (d) Granular object
Fig. 11: Simulation environment visualization. NVIDIA FleX is employed to create simulations of environments featuring ropes, cloths,
and granular objects, incorporating a robot for interactions within a unified workspace. Additionally, we utilize Pymunk for simulating
environments for rigid boxes with varying center of pressure locations.
Heuristics(Ours) Uncertainty-Driven We consider ropes with 2-dimensional physical proper-
Object Estimationϕˆ Varianceσ2 Estimationϕˆ Varianceσ2 ties: stiffness S and friction coefficient F. We generate the
πB πB
same amount of data as our previous setting in the Nvidia
Rope1 1.09 0.029 1.20 0.024
Rope2 -0.02 0.025 -0.01 0.035 FleX [22, 31] simulator with varying stiffness and friction
Rope3 -0.05 0.030 0.00 0.029 and train a model conditioned on both properties. Then, we
Rope4 0.88 0.020 0.83 0.016
applythemodeltoropesintherealworldandperformproperty
Rope5 0.67 0.018 0.63 0.019
estimationandforwarddynamicsprediction.Resultsareshown
TABLE II: Uncertainty-driven identification results. We show the in Fig. 12. As we can observe, the model can give reasonable
estimated parameter ϕˆusing both interaction selection methods for estimates by predicting high friction in w/ sheet cases and
10 interactions and the variance of the belief distribution σ2 after
πB low friction in w/o sheet cases. The stiffness estimations for
optimization. Better results are in bold.
all three ropes with and without sheets are also consistent.
The dynamics prediction error for the 2D model (conditioned
on both stiffness and friction) is generally lower than the
For rigid box pushing, since we have the correspondence
1D model (conditioned on stiffness only), showing that the
between the observed box corners and the target corners, we
dynamics prediction will be more accurate by incorporating
use the Mean Squared Error (MSE):
more relevant properties.
ϕ(z ,z )=MSE(z ,z ). (12)
T ∗ T ∗
C.2 Identification with Uncertainty
Penalty term. For all tasks, the penalty cost is defined as
The uncertainty of the physics parameters could be an
l(z ,u )= max1 x / W important indicator measuring the estimation’s confidence.
0 0:T 1
− x ∈VT { ∈ } (13) Minimizing the uncertainty can also be used as an objective
+ max 1 x x <d ,
eef obj min when selecting interactions [43]. In comparison, our method
xeef,xobj∈V0 {∥ − ∥ }
selects actions that produce maximum displacement on object
where W is the robot workspace; is the particle set in state
t particles. In this experiment, we compare an uncertainty-driven
V
z ; x and x represent end-effector and object particles,
t eef obj interaction selection scheme with our heuristics-driven scheme.
respectively.Thus,thepenaltytermpenalizesactionsthatmake
Wecandefinethebeliefstateovertheparameterspacebased
the object particles move out of the workspace and the actions
on the dynamics prediction error:
that will make the end-effector contact the object in z . We set
0
d =2cm except for clothes where d =0 as the grasping 1 (cid:104) (cid:105)
acm ti in
on allows contact.
min π B(ϕ)= ZE
(zi:i+1,ui) ∈I
e −CD(zi+1,f(zi,ui;ϕ,M))/τ , (14)
where π (ϕ) is the probability density of physics parameter ϕ
B
C DISCUSSIONANDPOTENTIALEXTENSIONS under belief B, I is the set of interaction data, CD represents
Chamfer Distance, τ is a temperature hyper-parameter which
C.1 Multiple Properties Recovering
we set to 0.05, and Z is a normalizing factor. Parameters
Expanding on one-dimensional physical parameter condi- that give a lower dynamics prediction error will have higher
tioning for deformable objects, we designed an experiment to probability density π . With this definition, a natural way to
B
show that our method can also be applied to more than one measure the uncertainty of a belief B is by the uncertainty in
physical property. the dynamics prediction outcomes, given current state z and(a) Rope & Estimation (b) Error landscape (c) Comparison of stiffness & (a) Rope & Estimation (b) Error landscape (c) Comparison of stiffness &
stiffness+friction model stiffness+friction model
w/ sheet, high friction w/o sheet, low friction
F = 0.96 F = 0.00
S = 1.00 S = 1.00
w/ sheet, high friction w/o sheet, low friction
F = 0.59 F = 0.00
S = 0.00 S = 0.00
w/ sheet, high friction w/o sheet, low friction
F = 1.00 F = 0.00
S = 0.70 S = 0.77
Fig. 12: Multiple physical properties estimations. We test a total of 3 ropes (A: polymer, high stiffness; B: yarn, low stiffness; C: paracord,
mid-level stiffness) on 2 different surface materials, w/ sheet in which a rubber sheet is applied to increase friction, and w/o sheet in which
the rope is directly in contact with the table. For each combination, we show the estimated friction F and stiffness S (column a), the error
landscape over the parameter space (column b, estimation results highlighted with yellow triangles), and the dynamics prediction error after
adaptation, compared to the 1-dimensional stiffness-only model (column c).
control action u:
E ϕ,ϕ′ ∼πB[CD(f(z,u;ϕ,M),f(z,u;ϕ ′,M))]. (15)
Intuitively, by selecting an action u that can maximize the
uncertainty in the above equation, the interaction result will
most effectively discriminate parameters sampled from the
belief and thus be more effective in reducing the variance of
π post-adaptation. In practice, we sample N parameters from
B
π and calculate the above equation as an MPPI objective.
B
Results of using this uncertainty-driven interaction selection
approach are provided in Tab. II. We compare it with our
heuristics-based approach and test on 5 different ropes. The
estimatedparametersϕˆareconsistent,andthereisnoconsistent
advantage in post-adaptation variance σ2 over one another.
πB
Given that the heuristics-based approach is computationally
faster, it is more suitable for our identification tasks.
A
epoR
B
epoR
C
epoR