Toto: Time Series Optimized
Transformer for Observability
Technical Report
BenCohen EmaadKhwaja KanWang CharlesMasson
EliseRame´ YoussefDoubli OthmaneAbou-Amal
{ben.cohen, emaad, kan.wang, charles.masson, elise.rame, youssef.doubli, othmane}@datadoghq.com
This technical report describes the Time Series Optimized Transformer for Observability (Toto), a new state-of-
the-artfoundationmodelfortimeseriesforecastingdevelopedbyDatadog. Inadditiontoadvancingthestateof
theartongeneralizedtimeseriesbenchmarksindomainssuchaselectricityandweather,thismodelisthefirst
general-purposetimeseriesforecastingfoundationmodeltobespecificallytunedforobservabilitymetrics.
Toto wastrained on a datasetof one trillion time series datapoints – the largestamong all currentlypublished
time series foundation models. Alongside publicly available time series datasets, 75% of the data used to train
TotoconsistsoffullyanonymousnumericalmetricdatapointsfromtheDatadogplatform.
Inourexperiments, Totooutperformsexistingtime seriesfoundationmodelsonobservabilitydata. Itdoesthis
while also excelling at general-purpose forecasting tasks, achieving state-of-the-art zero-shot performance on
multipleopenbenchmarkdatasets.
Inthisreport,wedetailthefollowingkeycontributions:
• Proportionalfactorizedspace-timeattention: Weintroduceanadvancedattentionmechanismthatallows
for efficient grouping of multivariate time series features, reducing computational overhead while main-
taininghighaccuracy.
• Student-Tmixturemodelhead:ThisnoveluseofaprobabilisticmodelthatrobustlygeneralizesGaussian
mixture models enables Toto to more accurately capture the complex dynamics of time series data and
providessuperiorperformanceovertraditionalapproaches.
• Domain-specific training data: In addition to general multi-domain time series data, Toto is specifically
pre-trainedonalarge-scaledatasetofDatadogobservabilitymetrics,encompassinguniquecharacteristics
notpresentinopen-sourcedatasets. Thistargetedtrainingensuresenhancedperformanceinobservability
metricforecasting.
4202
luJ
11
]GL.sc[
2v47870.7042:viXraFigure1. Totoarchitecturediagram. Inputtimeseriesof T steps(univariateexampleusedforsimplicityhere)arefirstem-
beddedusingthepatchembeddinglayer.Theythenpassthroughthetransformerstack,whichcontainsLidenticalsegments.
Eachsegmentofthetransformerconsistsofonespace-wisetransformerblockfollowedbyNtime-wiseblocks.Theflattened
transformeroutputsareprojectedtoformtheparametersoftheStudent-Tmixturemodel(SMM)head. Thefinaloutputsare
theforecastsfortheinputseries,shiftedPsteps(thepatchwidth)intothefuture.
1 Background lutionsforhigh-frequencyandhigh-dimensionaldata
commonlyencounteredinobservabilitymetrics.
We present Toto, a groundbreaking time series fore-
casting foundation model developed by Datadog. 1.1 Observability data
Toto is specifically designed to handle the complexi-
tiesofobservabilitydata,leveragingastate-of-the-art
TheDatadogobservabilityplatformcollectsavastar-
transformerarchitecturetodeliverunparalleledaccu-
ray of metrics across multiple subdomains, crucial
racy and performance. Toto is trained on a massive
formonitoringandoptimizingmoderninfrastructure
dataset of diverse time series data, enabling it to ex-
and applications. These metrics include infrastruc-
celinzero-shotpredictions. This modelistailoredto
turedatasuchasmemoryusage,CPUload,diskI/O,
meetthedemandingrequirementsofreal-timeanaly-
and network throughput, as well as application per-
sisaswellascomputeandmemory-efficientscalabil-
formance indicators like hit counts, error rates, and
ity to very large data volumes, providing robust so-
©2024Datadog.Allrightsreserved. 2Figure2. ExampleofToto's96-stepzero-shotforecastsontheETTh1dataset,showingmultivariateprobabilisticpredictions.
Solidlinesrepresentgroundtruth,dashedlinesrepresentmedianpointforecasts,andshadedregionsrepresent95%predic-
tionintervals.
latency [1]. Additionally, Datadog integrates specific data. Moreover, these models often fail to generalize
metricsfromnumerousSaaSproducts,cloudservices, across different types of metrics, leading to subopti-
open-sourceframeworks,andotherthird-partytools. mal performance on diverse datasets [5, 6]. Contin-
The platform allows users to apply various time se- uous retrainingand tuning to adapttoevolving data
riesmodelstoproactivelyalertonanomalousbehav- patternsfurtherincreasetheoperationalburden. This
ior, leading to a reduction in time to detection (TTD) scaling limitation has hindered the adoption of deep
and time to resolution (TTR) of production incidents learning–basedmethodsfortimeseriesanalysis,even
[2]. astheyshowpromiseintermsofaccuracy[7].
Thecomplexityanddiversityofthesemetricspresent
significant challengesfor time seriesforecasting. Ob- 1.3 Foundation models
servability data often requires high time resolution,
down to seconds or minutes, and is typically sparse
Large neural network-based generative models, of-
with many zero-inflated metrics. Moreover, these
ten referred to as “foundation models,” have revo-
metrics can display extreme dynamic ranges and
lutionized time series forecasting by enabling accu-
right-skewed distributions. The dynamic and non-
rate predictions on new data not seen during train-
stationarynatureofthesystemsbeingmonitoredfur-
ing,knownaszero-shotprediction[8].Thiscapability
thercomplicatestheforecastingtask,necessitatingad-
significantly reduces the need for constant retraining
vanced models that can adapt and perform under
oneachspecificmetric,thussavingconsiderabletime
theseconditions.
andcomputationalresources. Their architecturesup-
portstheparallelprocessingofvastdatavolumes,fa-
cilitatingtimelyinsightsessentialformaintainingsys-
1.2 Traditional models
temperformanceandreliability[9,10].
Historically,timeseriesforecastinghasreliedonclas- Through pretraining on diverse datasets, generative
sicalmodelssuchasARIMA,exponentialsmoothing, models exhibit strong generalization across various
and basic machine learning techniques [3]. While typesof time seriesdata. Thisenhancestheir robust-
foundational, these models necessitate individual ness and versatility, making them suitable fora wide
training for each metric, presenting several limita- range of applications. Zero-shot predictions are par-
tions [4]. Theneedtodevelopandmaintainseparate ticularlyattractiveintheobservabilitydomain,where
modelsforeachmetricimpedesscalability,especially the limitations of traditional methods are felt very
given the extensive range of metrics in observability acutely. The most common use cases for time series
©2024Datadog.Allrightsreserved. 3modelswithinanobservabilityplatformlikeDatadog 1.4 Recentwork
include automated anomalydetection and predictive
alerting. Itischallengingtoscaleclassicalforecasting
The past several years have seen the rise of
methods to handle cloud-based applications that can
transformer-based models as powerful tools for time
becomposedofmanyephemeral,dynamicallyscaling
seriesforecasting. These models leveragemulti-head
componentssuchascontainers,VMs,serverlessfunc-
self-attention mechanisms to capture long-range de-
tions,etc. Theseentitiestendtobebothhighincardi-
pendenciesandintricatepatternsindata.
nality and short-lived in time. This limits the practi-
calityoftraditionaltimeseriesmodelsintwoways:
To addressthe unique challenges of time series data,
recent advancements have introduced various mod-
ifications to the attention mechanism. For exam-
ple,Moirai[15]uses“any-variate”attentiontomodel
• First, the high cardinality and volume of data
dependencies across different series simultaneously.
canmakefittingindividualmodelstoeachtime
Factorized attention mechanisms [16] have been de-
series computationally expensive or even in-
veloped to separately capture temporal and spatial
tractable.Theabilitytotrainasinglemodeland
(cross-series)interactions,enhancingtheabilitytoun-
perform inference across a wide range of do-
derstand complex interdependencies. Other models
mainshasthepotentialtodramaticallyimprove
[17,18]haveusedcross-channelattentioninconjunc-
the efficiency, and thus the coverage, of an au-
tion with feed-forward networks for mixing in the
tonomousmonitoringsystem.
time dimension. Additionally, causal masking [19]
and hierarchical encoding [16] can improve the effi-
ciencyandaccuracyofpredictionsintimeseriescon-
• Second, ephemeral infrastructure elements of- texts.
ten lack enough historical data to confidently
fit a model. In practice, algorithmic alerting These innovative transformer-based models have
systems often require an adaptation period of demonstrated state-of-the-artperformance on bench-
days or weeks before they can usefully moni- mark datasets [14], frequently surpassing traditional
tor a new metric. However, if the object being modelsinbothaccuracyandrobustness. Theircapac-
monitored is a container with a lifespan mea- ity to process high-dimensional data efficiently [20]
suredinminutesorhours,theseclassicalmodels makes them ideal for applications involving numer-
are unable to adapt quickly enough to be use- ous time series metrics with varying characteristics,
ful. Real-world systems thus often fall back to suchasobservability.
crudeheuristics,suchasthreshold-basedalerts,
which rely on the domain knowledge of users. Even more recently, a number of time series “foun-
Zero-shot foundation models can enable accu- dation models” have been released [15, 19, 21–24].
rate predictions with much less historical con- By pre-training on extensive, multi-domain datasets,
text, by aggregating and interpolating prior in- these large models achieve impressive zero-shot pre-
formation learned from a massive and diverse dictioncapabilities,significantlyreducingtheneedfor
dataset. constant retraining. This paradigm is appealing for
the observability context, where we constantly have
new time series to process and frequent retraining is
impractical.
Theintegrationoftransformer-basedmodels[11]like
Toto into observability data analysis thus promises
significantimprovementsinforecastingaccuracyand
2 Problem statement
efficiency. These models offer a robust solution for
managing diverse, high-frequency data and deliver-
ing zero-shotpredictions. With their advancedcapa- At Datadog, our time series data encompasses a va-
bilities, transformer-basedmodelsrepresentasignifi- riety of observability metrics from numerous subdo-
cantleapforwardinthefieldofobservabilityandtime mains. These metrics present several challenges for
seriesanalysis[12–14].
©2024Datadog.Allrightsreserved. 4existingforecastingmodels: multi-head attention to multivariate time series data
(Fig.1).
• High time resolution: Users often require data
in increments of seconds or minutes, unlike 3.1 Transformerdesign
manypublicly-availabletimeseriesdatasetsthat
areathourlyfrequencyorabove.
Transformer models for time series forecasting have
• Sparsity: Metrics such as error counts often variouslyusedencoder-decoder[12,13,21],encoder-
track rare events, resulting in sparse and zero- only [14, 15, 17], and decoder-only architectures [19,
inflatedtimeseries. 23]. ForToto,weemployadecoder-onlyarchitecture.
Decoder architectureshave been shown to scale well
• Extreme right skew: Latency measurements
[25, 26], and allow for arbitrary prediction horizons.
in distributed systems exhibit positive, heavy
The causal next-patch prediction task also simplifies
taileddistributionswith extremevaluesathigh
thepre-trainingprocess.
percentiles.
• Dynamic,nonstationarysystems:Thebehavior We use techniques from some of the latest large
of monitored systems changes frequently due language model (LLM) architectures, including pre-
tocodedeployments,infrastructurescaling,fea- normalization [27], RMSNorm [28], and SwiGLU
ture flag management, and other configuration feed-forwardlayers[29].
changes,aswellasexternalfactorslikeseasonal-
ityanduser-behavior-driventrends. Sometime
3.2 Inputembedding
series, such as those monitoring fleet deploy-
ments, can also have a very low variance, ex-
hibitingapiecewise-constantshape. Time series transformers in the literature have used
various approaches for creating input embeddings.
• High-cardinality multivariate data: Monitor-
We use non-overlapping patch projections (Fig. 3),
ing large fleets of ephemeral cloud infrastruc-
first introduced for Vision Transformers [30, 31] and
turesuchasvirtualmachines(VMs),containers,
popularized in the time series context by PatchTST
serverlessfunctions, etc. leadstohighcardinal-
[14]. Totowastrainedusingafixedpatchsizeof32.
itydata,withhundredsorthousandsofindivid-
ual time series variates, often with limited his-
toricaldataforeachgroup.
• Historicalanomalies:Historicaldataoftencon-
tains outliers and anomalies caused by perfor-
manceregressionsorproductionincidents.
Foundation models pre-trained on other domains
struggletogeneralizeeffectivelytoobservabilitydata
duetothesecharacteristics. Toovercomethis, wede-
velopedToto,astate-of-the-artfoundationmodelthat
excels at observability forecasting while also achiev-
ingtopperformanceonstandardopenbenchmarks.
3 Model architecture
Figure3. Thepatchembeddingtakesasinputamultivari-
atetimeseriesofMvariatesbyNtimesteps.Itdivideseach
variatealongthetimedimensionintopatchesofsizePand
Totoisadecoder-onlyforecastingmodel. Thismodel
projectstheselinearlyintoanembeddingspaceoflatentdi-
employs many of the latest techniques from the lit- mension D. This results in an output of size M× N ×D
P
erature, and introduces a novel method for adapting whichisfedtothetransformerdecoder.
©2024Datadog.Allrightsreserved. 53.3 Attention mechanism We therefore propose a novel variant of factorized
attention, which we call “Proportional Factorized
Space-Time Attention.” We use a mixture of alternat-
Observabilitymetricsareoftenhigh-cardinality, mul-
ing space-wise and time-wise attention blocks. As a
tivariate time series. Therefore, an ideal model will
configurablehyperparameter,wecanchangetheratio
nativelyhandlemultivariateforecasting. Itshouldbe
oftime-wisetospace-wiseblocks,thusallowingusto
able to analyze relationships both in the time dimen-
devote more or less compute budget to each type of
sion (what we refer to as “time-wise” interactions)
attention. For our base model, we selected a config-
and in the channel dimension (what we refer to as
urationwithonespace-wiseattentionblockforevery
“space-wise”interactions,followingtheconventionin
twotime-wiseblocks.
the Datadog platform of describing different groups
ortagsetsofametricasthe“space”dimension).
In the time-wise attention blocks, we use causal
masking and rotary positional embeddings [34] with
In order to model both space and time-wise interac-
XPOS [35] in order to autoregressively model time-
tions, weneedtoadaptthetraditionalmulti-headat-
dependent features. In the space-wise blocks, by
tentionarchitecture[11]fromonetotwodimensions.
contrast, we use full bidirectional attention in order
Severalapproacheshave beenproposed in the litera-
to preserve permutation invariance of the covariates,
turetodothis,including:
with a block-diagonal ID mask to ensure that only
related variates attend to each other. This masking
• Assuming channel independence, and comput- allows us to pack multiple independent multivariate
ing attention only in the time dimension [14]. time series into the same batch, in order to improve
Thisisefficient,butthrowsawayallinformation trainingefficiencyandreducetheamountofpadding.
aboutspace-wiseinteractions.
• Computing attention only in the space dimen- 3.4 Probabilistic prediction head
sion, and using a feed-forward network in the
timedimension[17,18].
In order to be useful for forecasting applications, a
model should produce probabilistic predictions. A
• Concatenating variates along the time dimen-
common practice in time series models is to use an
sionandcomputingfullcross-attentionbetween
output layer where the model regresses the parame-
everyspace/timelocation[15].Thiscancapture
tersofaprobabilitydistribution. Thisallowsforpre-
everypossiblespaceandtimeinteraction,butit
diction intervals to be computed using Monte Carlo
iscomputationallycostly.
sampling[7].
• Computing “factorized attention,” where each
transformerblockcontainsaseparatespaceand Common choices for an output layer are Normal [7]
timeattentioncomputation[16,32,33]. Thisal- andStudent-T[23,36],whichcanimproverobustness
lows both space and time mixing, and is more tooutliers. Moirai[15]allowsformoreflexibleresid-
efficient than full cross-attention. However, it ualdistributionsbyproposinganovelmixturemodel
doublestheeffectivedepthofthenetwork. incorporating a weighted combination of Gaussian,
Student-T, Log-Normal, and Negative-Binomial out-
puts.
In order to design our attention mechanism, we fol-
low the intuition that for many time series, the time However, real-worldtime seriescanoften have com-
relationships are more important or predictive than plexdistributionsthatarechallengingtofit,without-
thespacerelationships. Asevidence,weobservethat liers,heavytails,extremeskew,andmultimodality. In
even models that completely ignore space-wise rela- order to accommodate these scenarios, we introduce
tionships (such as PatchTST [14] and TimesFM [19]) anevenmoreflexibleoutputlikelihood. Todothiswe
can still achieve competitive performance on multi- employamethodbasedonGaussianmixturemodels
variatedatasets. However, other studies (e.g. Moirai (GMMs), which can approximate any density func-
[15])haveshownthroughablationsthatthereissome tion ([37]). To avoid training instability in the pres-
clearbenefittoincludingspace-wiserelationships.
©2024Datadog.Allrightsreserved. 6Figure4. Example metricquery inthe Datadog platform. The metricname (1) determineswhich metricis being queried.
Thefilterclause(2)limitswhichcontextsarequeried,inthiscaserestrictingthequerytotheprodenvironment. Thespace
aggregation(3)indicatesthattheaveragemetricvalueshouldbereturnedforeachuniquecombinationofthegroup-bykeys.
Thetimeaggregation(4)indicatesthatmetricvaluesshouldbeaggregatedtotheaverageforeach60-secondinterval. The
queryresultswillbeamultivariatetimeserieswith1-minutetimesteps,andwithseparateindividualvariatesforeachunique
service,datacentertuple.
ence of outliers, we use a Student-T mixture model 3.6 Training objective
(SMM),arobustgeneralizationofGMMs[38]thathas
previouslyshownpromiseformodelingheavy-tailed
As a decoder-only model, Toto is pre-trained on the
financial time series [39, 40]. The model predicts k
next-patchpredictiontask. Weminimizethenegative
Student-Tdistributions(wherekisahyperparameter)
log-likelihoodofthenextpredictedpatchwithrespect
foreachtimestep,aswellasalearnedweighting.
to the distribution output of the model. We train the
modelusingtheAdamWoptimizer[42].
When we perform inference, we draw samples from
themixturedistributionateachtimestamp,thenfeed
eachsamplebackintothedecoderforthenextpredic- 3.7 Hyperparameters
tion. Thisallowsustoproducepredictionintervalsat
anyquantile, limited onlybythenumberofsamples;
ThehyperparametersusedforTotoaredetailedinTa-
for more precise tails, we can choose to spend more
bleA.1,with103milliontotalparameters.
computationonsampling(Fig.2).
3.5 Input/outputscaling 4 Training data
As in other time series models, we perform instance
We pretrained Toto with a dataset of approximately
normalizationoninputdatabeforepassingitthrough
one trillion time series points. Of these, roughly
thepatchembedding,inordertomakethemodelgen-
three-quarters are anonymous observability metrics
eralizebettertoinputsofdifferentscales[41].Wescale
from the Datadog platform. The remaining points
theinputstohavezeromeanandunitstandarddevia-
come from the LOTSA dataset [15], a compilation of
tion. Theoutputpredictionsarethenrescaledbackto
publicly-available time series datasets across many
theoriginalunits.
differentdomains.
©2024Datadog.Allrightsreserved. 74.1 Datadog dataset By implementing these rigorous preprocessing steps
andsophisticated datahandlingmechanisms, ween-
sure that the training data for Toto is of the highest
The Datadog platform ingests more than a hundred
quality, ultimately contributing to the model's supe-
trillion events per day. However, much of this data
riorperformanceandrobustness.
is sparse, noisy, or too granular or high in cardinal-
ity to be useful in its raw form. To curate a high-
qualitydatasetforefficientmodeltraining,wesample
4.2 Synthetic data
queries based on quality and relevance signals from
dashboards,monitoralerts,andnotebooks. Thispro-
videsastrongsignalthatthedataresultingfromthese We use a synthetic data generation process similar
queriesisofcriticalimportanceandsufficientquality toTimesFM[19]tosupplementourtrainingdatasets,
forobservabilityofreal-worldapplications. improving the diversity of the data and helping to
teachthemodelbasicstructure. Wesimulatetimese-
Datadog metrics are accessed using a specialized riesdatathroughthecompositionofcomponentssuch
querylanguagesupportingfilters,group-bys,timeag- as piecewise linear trends, ARMA processes, sinu-
gregation, and various transformations and postpro- soidal seasonal patterns, and various residual distri-
cessing functions [43]. We consider groups returned butions. Werandomlycombinefiveoftheseprocesses
fromthesamequerytoberelatedvariatesinamulti- per variate, introducing patterns not always present
variatetimeseries(Fig.4).Afterweretrievethequery inourreal-worlddatasets. Thegenerationprocessin-
results,wediscardthequerystringsandgroupiden- volves creating base series with random transforma-
tifiers,keepingonlytherawnumericdata. tions,clippingextremevalues,andrescalingtoaspec-
ified range. Bymaking synthetic dataapproximately
Handling this vast amount of data requires several 5%ofourtrainingdataset,weensureawiderangeof
preprocessing steps to ensure consistency and qual- time-seriesbehaviorsarecaptured. Thisdiversityex-
ity. Initially, we apply padding and masking tech- posesourmodelstovariousscenariosduringtraining,
niquestoaligntheserieslengths,makingthemdivis- improving their ability to generalize and effectively
ible by the patch stride. This involves adding neces- handlereal-worlddata.
saryleft-paddingtoboththetimeseriesdataandthe
IDmask, ensuring compatibility with the model's re-
quirements. 5 Results
Variousdataaugmentationsareemployedtoenhance
the dataset's robustness. We introduce random time We reportexperimentalresults for a pre-trainedToto
offsets to prevent memorization caused by having modelinSection5.1andSection5.2.
series always align the same way with the patch
grid. After concatenating the Datadog and LOTSA Toevaluatepredictions,wesequentiallydivideatime
datasets for training, we also implement a variate series into context and forecast segments. We in-
shufflingstrategytomaintaindiversityandrepresen- put the context segment into Toto and autoregres-
tation. Specifically,10%ofthetime,wecombinevari- sively generate output patches by sampling from the
atesthatarenotnecessarilyrelated,thuscreatingnew, Student-T mixture model distribution. We forecast a
diverse combinations of data points. To sample the number of steps equal to the nearest multiple of the
indices,weemployanormaldistributionwithastan- patchsize,thentruncatethepredictionstothedesired
darddeviationof1000,favoringdatapointsthatwere length. Inordertokeepinferencetimeconsistent,we
closertogetherintheoriginaldatasets. ThisGaussian vary the number of samples generated based on the
samplingensuresthat, while thereisapreferencefor cardinalityandlengthofthedataset,withaminimum
adjacentdata points, significant randomness is intro- of 100 samples. We take the median sample at each
duced to enhance the diversity of the training data. time step as the final point prediction. This predic-
This approachimprovesthe model's ability to gener- tion is then compared against the ground-truth fore-
alizeacrossdifferenttypesofdataeffectively. castsegmentforevaluation.
©2024Datadog.Allrightsreserved. 8ZeroShot FullShot
Dataset Metric Toto MoiraiSmall MoiraiBase MoiraiLarge TimesFM* iTransformer TimesNet PatchTST Crossformer TiDE DLinear SCINet FEDformer
ETTh1 MAE 0.389 0.424 0.438 0.469 0.426 0.448 0.450 0.455 0.522 0.507 0.452 0.647 0.460
MSE 0.363 0.400 0.434 0.510 - 0.454 0.458 0.469 0.529 0.541 0.456 0.747 0.440
ETTh2 MAE 0.261 0.379 0.382 0.376 0.410 0.407 0.497 0.407 0.684 0.550 0.515 0.723 0.449
MSE 0.170 0.341 0.345 0.354 - 0.383 0.414 0.387 0.942 0.611 0.559 0.954 0.437
ETTm1 MAE 0.375 0.409 0.388 0.389 0.388 0.410 0.406 0.400 0.495 0.419 0.407 0.481 0.452
MSE 0.372 0.448 0.381 0.390 - 0.407 0.400 0.387 0.513 0.419 0.403 0.486 0.448
ETTm2 MAE 0.319 0.341 0.321 0.320 0.334 0.332 0.333 0.326 0.611 0.404 0.401 0.537 0.349
MSE 0.272 0.300 0.272 0.276 - 0.288 0.291 0.281 0.757 0.358 0.350 0.571 0.305
Electricity MAE 0.246 0.320 0.274 0.273 - 0.270 0.295 0.304 0.334 0.344 0.300 0.365 0.327
MSE 0.157 0.233 0.188 0.188 - 0.178 0.193 0.216 0.244 0.252 0.212 0.268 0.214
Weather MAE 0.284 0.267 0.261 0.275 - 0.278 0.287 0.281 0.315 0.320 0.317 0.363 0.360
MSE 0.256 0.242 0.238 0.259 - 0.258 0.259 0.259 0.259 0.271 0.265 0.292 0.309
Mean MAE 0.312 0.357 0.341 0.350 - 0.357 0.378 0.362 0.493 0.424 0.399 0.519 0.400
MSE 0.265 0.328 0.315 0.330 - 0.328 0.336 0.333 0.541 0.409 0.374 0.533 0.359
Table1. ComparisonofdifferentmodelswithTotoonthe LSFbenchmark datasets. Resultsareaveragedacrossprediction
lengths of 96, 192, 336, and 720 steps. ForToto, we use astrideof 512 steps and ahistorical contextwindow of 512 steps.
Forothermodels,weusetheresultsreportedin[15]and[19]. MetricsforeachpredictionlengthareavailableinTableA.2.
*TimesFMonlyreportsvaluesforMAEonETTh1,ETTh2,ETTm1,andETTm2.Key:Bestresults,Second-bestresults.
5.1 LSFbenchmarks Several architectural choices and data features likely
contribute toToto's superior performance. The novel
Proportional Factorized Space-Time Attention mech-
Toassessgeneral-purposetimeseriesforecastingper-
anism allows Toto to efficiently capture both tempo-
formance, we use the Long Sequence Forecasting
ralandspatialdependencieswithinmultivariatetime
(LSF) benchmark datasets (ETTh1, ETTh2, ETTm1,
series data. Additionally, the extensive training on a
ETTm2, Electricity, and Weather) [12]. We evaluate
diverse dataset of one trillion time series points, in-
with forecast lengths of 96, 192, 336, and 720 time
cludingamixofreal-worldobservabilitymetricsand
steps, in sliding windows with stride 512, and aver-
multi-domain time series data, enhances Toto's abil-
agetheresults. ForToto, weusedahistoricalcontext
itytohandlevariedcharacteristicsofdifferentbench-
windowof512stepsandtookthemedianof200sam-
markdatasets.
ples. Following standardpractice,we reportnormal-
ized MeanAbsolute Error(MAE)and MeanSquared
While Toto generally excels, there are areas where
Error (MSE), fitted on a training split, in order to be
its performance is closely matched by other mod-
abletocompareperformanceacrossdifferentdatasets.
els. In full-shot scenarios, models like PatchTST,
We compared Toto's performance with the reported
Crossformer, and FEDformer show competitive re-
results of other recent zero-shot foundation models
sults. For example, on the Electricity dataset, while
[15, 19], as well as full-shot time series forecasting
Toto achieves a leading zero-shot MAE of 0.246 and
models[14,16,17,36,44–47].Wedisplaytheseresults
MSE of 0.157, iTransformer and TimesNet also show
inTable1.
strongperformance,indicatingthatthese modelscan
catchupwhenadditionaltrainingdataisavailable.
Toto demonstrates exceptional performance across a
variety of benchmark datasets, excelling in zero-shot
Overall, Toto's architectural innovations and exten-
scenarios. In the LSF datasets, Toto consistently out-
sive training data enable it to achieve state-of-the-art
performsothermodelsintermsofMAEandMSE.For
performance across diverse benchmarks, excelling in
example,ontheETTh1dataset,TotoachievesanMAE
zero-shot scenarios while remaining highly competi-
of0.389andanMSEof0.363,outperformingallzero-
tiveinfull-shotcontexts.
shotmodels,includingthepreviouslyreportedMoirai
series and TimesFM. Macro-averaging across the six
LSFdatasets,TotoachievesanMAEof0.312andMSE
5.2 Datadog benchmark
of 0.265, again exceeding Moirai's reported zero-shot
performance as well as the reported performance of
thefull-shotmodels. We created a benchmark using anonymous Datadog
datatoassessperformanceacrossvariousobservabil-
©2024Datadog.Allrightsreserved. 9Metric Toto Chronos-T5Tiny Chronos-T5Mini Chronos-T5Small Chronos-T5Base Chronos-T5Large MoiraiSmall MoiraiBase MoiraiLarge TimesFM
sMAPE 0.672 0.809 0.788 0.800 0.796 0.805 0.808 0.742 0.736 1.246
sMdAPE 0.318 0.406 0.391 0.401 0.393 0.396 0.418 0.370 0.365 0.639
Table 2. Performance of Toto and other zero-shot models on the Datadog benchmark dataset. Key: Best results,
Second-bestresults.
ity metrics. To ensure a representative and realistic Theevaluationresults(Table2)demonstratethatToto
sample, we sampled data based on quality and rel- outperforms the other models. We evaluate using a
evance signals from dashboards, monitor alerts, and prediction length of 365, the maximum forecast win-
notebooks. This benchmark comprises 983,994 data dowavailableforprevioustimeseriesmodelswithin
points from 82 distinct multivariate time series, en- theDatadogplatform.Weuseahistoricalcontextwin-
compassing1,122variates. dowof512steps. Becauseobservabilitydatacanhave
extreme variation in both magnitude and dispersion,
We analyzed summary statistics of the series in our we select symmetric mean absolute percentage error
benchmark to identify characteristics that make ob- (sMAPE)asascale-invariantperformancemetric[48].
servabilitytimeserieschallengingtoforecast.Thecat- We also report symmetric median absolute percent-
egoriesandtheirdefinitionsareasfollows: age error (sMdAPE), a robust version of sMAPE [49]
that minimizes the influence of the extreme outliers
presentinobservabilitydata.WiththelowestsMAPE
• Sparse: Series with a low density of observa-
of 0.672 and sMdAPE of 0.318, Toto proves to be the
tions,indicatinginfrequentrecordingofdataor
mostaccurateforforecastingobservabilitytimeseries
rareevents.
data.
• Extreme right skew: Series with a distribution
These resultssuggest thatcurrentopen datasetsmay
heavily skewed to the right, characterized by a
not provide sufficient information to extrapolate to
fewveryhighvaluesandmanylowervalues.
the specific nuances of observability data, highlight-
• Seasonal: Series exhibiting regular and recur- ing the importance of training on more relevantdata
ring patterns, often linked to daily, weekly, or asdemonstratedbyToto'ssuperiorperformance.
yearlycycles.
• Flat: Series with minimal variability, showing
Case %
littletonochangeovertime.
Sparse 12.20
ExtremeRightSkew 17.07
Therelativeproportionofthesecasesaredisplayedin
Seasonal 80.49
Table3.
Flat 1.22
Toassessthepredictionof otherzero-shotmodelson
Table3.BreakdownofDatadogdatasetbasedoncase,com-
the DD Benchmark, we follow sampling procedures puted based on the average characteristics of variates in
delineated in their respective manuscripts. In short, eachmultivariateseries.Notethatthesedonotaddto100%
forChronosmodels,wegenerate20samplesandtake becausetimeseriesmayfallintomultiplecategories.
the median prediction. For Moirai models, we take
the median of 100 samples and set the patch size to
“auto”. TimesFM only produces point predictions of
6 Conclusions
the mean, so we use those directly. Since TimesFM
and Chronos only supportunivariateforecasting, we
process each variate independently. Moirai, on the Toto, through a novel architecture and pre-training
otherhand,likeToto,makesjointpredictionsforeach corpus, demonstrates state-of-the-art performance
group of related variates. For Toto, we utilize the both on public benchmarks and on the Datadog ob-
sameevaluationprocedureweusedontheLSFbench- servability benchmark. We look forward to sharing
marks.
©2024Datadog.Allrightsreserved. 10manymoretechnicaldetails,experiments,andbench- versational agents capable of interpreting and
markresultsinaforthcomingpaper. reasoningabouttimeseriesdata.
• Model enhancements and scaling: Explore
ways to improve and scale model performance
7 Impact statement
through optimizations such as new types of in-
putembeddings,attentionmechanisms,andex-
IndevelopingToto, Datadogfollowsastructuredap- aminingalternativevariategroupingstocapture
proach to ensure responsible development, focusing richerinteractions.
on identifying, assessing, and mitigating potential
risksassociatedwiththeuseofourmodel. Giventhat
Totoisnotintendedformassdistributionandspecif- 9 Contributions
icallygeneratestimeseriesforecastsforobservability
data,thepotentialharmsareconsiderablylowercom-
Contributorsarelistedinalphabeticalorder.
paredto more general-purpose models. AtDatadog,
our primary focus is on ensuring the accuracy, relia-
Othmane Abou-Amal, Joseph Banks, Mayeul Blan-
bility,andsecurityoftheforecastsgeneratedbyToto,
zat, Ben Cohen, Youssef Doubli, Ben Hinthorne,
which are crucial for maintaining and optimizing in-
EmaadKhwaja,JaredLedvina,CharlesMasson,Sajid
frastructureandapplicationperformance.
Mehmood, Elise Rame´, Maxime Visonneau, Kan
Wang.
WecarefullyanalyzethepotentialforTototoproduce
incorrect or misleading forecasts that could impact
decision-making processes in critical systems. Addi-
tionally,weconsidertheimplicationsofToto'sperfor- 10 Acknowledgements
mance acrossdiverse datasets, ensuring it cangener-
alizewellwithoutintroducingsignificanterrors.
Ourworkismadepossiblebytheeffortsofnumerous
teams at Datadog. Special thanks and acknowledge-
mentto:
8 Future directions
Johan Andersen, Roashan Ayene, Romoli Bakshi,
Kevin Beach, Bill Birkholz, Rob Boll, Maxim Brown,
Manyexcitingareasofexplorationremainforfurther
Benedetto Buratti, Marion Chan-Renous, Jessica Cor-
study. Ifyouareinterestedinworkingwithus,please
donnier, Ben Donohue, Zakaria Fikrat, Quentin
reachouttotheauthorsbyemail.
Franc¸ois, Erica Hale, Michael Hoang, Joe Jones,
Max Livingston, Jesse Mack, Amine Naouas, Sean
Some future research questions that particularly in-
O'Connor, Brendan Rhoads, Phil Sarin, Vyom Shah,
trigueusinclude:
Aaron Taa, Bharath Vontimitta, Dominique West,
StevenZhou.
• Multi-modalinputs: Incorporate additionalin-
putmodalitiessuchasquerymetadataandcap-
tionstoenhanceforecastperformance. References
• Autonomous troubleshooting agents: Aug-
ment Datadog's AI agents [50] for trou- [1] Datadog.Observabilityplatform,2024.URLhttps://www.datadoghq.
com/monitoring/observability-platform/.
bleshooting and incident response by integrat-
ing modality-specific foundation models like [2] Datadog.Moderninfrastructuremonitoring,2024.URLhttps://www.
Toto to improve their reasoning and planning datadoghq.com/product/infrastructure-monitoring/.
capabilitieswithtelemetrydata. [3] RobJHyndmanandGeorgeAthanasopoulos.Forecasting:Principlesand
Practice.OTexts,3rdedition,2021.URLhttps://otexts.com/fpp3/.
• Conversational interfaces: Align time series
[4] RobertFildes,Miche`leHibon,SpyrosMakridakis,andNigelMeade.
forecasting models with LLMs to develop con- Generalisingaboutunivariateforecastingmethods: furtherempirical
©2024Datadog.Allrightsreserved. 11evidence. InternationalJournalofForecasting,14:339–358,91998. ISSN [19] AbhimanyuDas,WeihaoKong,RajatSen,andYichenZhou.Adecoder-
01692070.doi:10.1016/S0169-2070(98)00009-0. onlyfoundationmodelfortime-seriesforecasting.InForty-firstInterna-
tionalConferenceonMachineLearning,2024.URLhttps://openreview.
[5] SimonStevenson.Acomparisonoftheforecastingabilityofarimamod- net/forum?id=jn2iTJas6h.
els. JournalofPropertyInvestment&Finance,25:223–240,52007. ISSN
1463-578X.doi:10.1108/14635780710746902. [20] TianyangLin,YuxinWang,XiangyangLiu,andXipengQiu.Asurveyof
transformers. CoRR,abs/2106.04554,2021. URLhttps://arxiv.org/
abs/2106.04554.
[6] CharisiosChristodoulos,ChristosMichalakelis,andDimitrisVaroutas.
Forecastingwithlimiteddata: Combiningarimaanddiffusionmod-
[21] AbdulFatirAnsari,LorenzoStella,CanerTurkmen,XiyuanZhang,Pe-
els.TechnologicalForecastingandSocialChange,77:558–565,52010. ISSN
droMercado,HuibinShen,OleksandrShchur,SyamaSundarRanga-
00401625.doi:10.1016/j.techfore.2010.01.009.
puram,SebastianPinedaArango,ShubhamKapoor,JasperZschiegner,
DanielleC.Maddix,HaoWang,MichaelW.Mahoney,KariTorkkola,
[7] DavidSalinas,ValentinFlunkert,JanGasthaus,andTimJanuschowski. AndrewGordonWilson,MichaelBohlke-Schneider,andYuyangWang.
Deepar: Probabilistic forecasting with autoregressive recurrent net- Chronos: Learningthelanguageoftimeseries,2024. URLhttps://
works. International JournalofForecasting, 36:1181–1191, 2020. ISSN arxiv.org/abs/2403.07815.
0169-2070. doi: https://doi.org/10.1016/j.ijforecast.2019.07.
001. URLhttps://www.sciencedirect.com/science/article/pii/ [22] AzulGarzaandMaxMergenthaler-Canseco.Timegpt-1,2023.
S0169207019301888.
[23] Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Kho-
[8] Eoin Brophy, Zhengwei Wang, Qi She, and Toma´s Ward. Genera- rasani,GeorgeAdamopoulos,RishikaBhagwatkar,MarinBilosˇ,Hena
tive adversarial networks in time series: A systematic literature re- Ghonia, NadhirHassen, AndersonSchneider,SahilGarg, Alexandre
view. ACMComputingSurveys,55:1–31,102023. ISSN0360-0300. doi: Drouin, NicolasChapados, YuriyNevmyvaka, andIrinaRish. Lag-
10.1145/3559540. llama: Towardsfoundationmodelsfortimeseriesforecasting. InR0-
FoMo:RobustnessofFew-shotandZero-shotLearninginLargeFoundation
[9] Zhihao Jia, SinaLin, CharlesR Qi, and Alex Aiken. Exploringthe Models,2023.URLhttps://openreview.net/forum?id=jYluzCLFDM.
hiddendimensioninacceleratingconvolutionalneuralnetworks,2018.
[24] NateGruver,MarcAntonFinzi,ShikaiQiu,andAndrewGordonWil-
URLhttps://openreview.net/forum?id=SJCPLLpaW.
son. Largelanguagemodelsarezero-shottimeseriesforecasters. In
Thirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
[10] WeizhengXu,YoutaoZhang,andXulongTang.Parallelizingdnntrain-
URLhttps://openreview.net/forum?id=md68e8iZK1.
ingongpus: Challengesandopportunities. pages174–178.ACM,4
2021.ISBN9781450383134.doi:10.1145/3442442.3452055. [25] Alec Radford and Karthik Narasimhan. Improving language un-
derstanding by generative pre-training. 2018. URL https://api.
[11] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,Llion semanticscholar.org/CorpusID:49313245.
Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. At-
tention is all you need. volume 30. Curran Associates, Inc., [26] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei,
2017. URL https://papers.nips.cc/paper_files/paper/2017/ and Ilya Sutskever. Language models are unsupervised multitask
hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. learners. 2019. URLhttps://api.semanticscholar.org/CorpusID:
160025533.
[12] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Auto-
former: Decomposition transformers with auto-correlation for long- [27] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng,
termseriesforecasting. 2021. URLhttps://openreview.net/forum? HuishuaiZhang,YanyanLan,LiweiWang,andTie-YanLiu. Onlayer
id=J4gRj6d5Qm. normalization in the transformer architecture, 2020. URL https://
openreview.net/forum?id=B1x8anVFPr.
[13] HaoyiZhou,ShanghangZhang, JieqiPeng, ShuaiZhang, JianxinLi,
[28] BiaoZhangandRicoSennrich. RootMeanSquareLayerNormaliza-
HuiXiong,andWanZhang. Informer: Beyondefficienttransformer
tion. InAdvancesinNeuralInformationProcessingSystems32,Vancou-
forlongsequencetime-seriesforecasting. 2020. URLhttps://api.
ver,Canada,2019. URLhttps://openreview.net/references/pdf?
semanticscholar.org/CorpusID:229156802.
id=S1qBAf6rr.
[14] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant [29] NoamShazeer.Gluvariantsimprovetransformer,2020.URLhttps://
Kalagnanam. Atimeseriesisworth64words: Long-termforecast- arxiv.org/abs/2002.05202.
ingwithtransformers. 2023. URLhttps://openreview.net/forum?
id=Jbdc0vTOcol. [30] Jean-BaptisteCordonnier,AndreasLoukas,andMartinJaggi. Onthe
relationshipbetween self-attention and convolutional layers. In 8th
[15] Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio International Conference on Learning Representations, ICLR 2020, Addis
Savarese,andDoyenSahoo. Unifiedtrainingofuniversaltimeseries Ababa,Ethiopia,April26-30,2020.OpenReview.net,2020.URLhttps://
forecastingtransformers.2024.URLhttps://openreview.net/forum? openreview.net/forum?id=HJlnC1rKPB.
id=Yd8eHMY1wz.
[31] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-
senborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,
[16] Yunhao Zhangand Junchi Yan. Crossformer: Transformer utilizing
MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,and
cross-dimensiondependencyfor multivariatetime seriesforecasting.
NeilHoulsby.Animageisworth16x16words:Transformersforimage
InTheEleventhInternationalConferenceonLearningRepresentations,2023.
recognitionatscale. InInternationalConferenceonLearningRepresenta-
URLhttps://openreview.net/forum?id=vSVLM2j9eie.
tions,2021.URLhttps://openreview.net/forum?id=YicbFdNTTy.
[17] YongLiu,TenggeHu,HaoranZhang,HaixuWu,ShiyuWang,Lintao [32] RoshanMRao,JasonLiu,RobertVerkuil,JoshuaMeier,JohnCanny,
Ma,andMingshengLong. itransformer:Invertedtransformersareef- PieterAbbeel,TomSercu,andAlexanderRives. Msatransformer. In
fectivefortimeseriesforecasting. 2024. URLhttps://openreview. MarinaMeilaandTongZhang,editors,Proceedingsofthe38thInterna-
net/forum?id=JePfAI8fah. tionalConferenceonMachineLearning,volume139ofProceedingsofMa-
chineLearningResearch,pages8844–8856.PMLR,18–24Jul2021. URL
[18] RomainIlbert,AmbroiseOdonnat,VasiliiFeofanov,AladinVirmaux, https://proceedings.mlr.press/v139/rao21a.html.
Giuseppe Paolo, Themis Palpanas, and IevgenRedko. SAMformer:
Unlockingthepotentialoftransformersintimeseriesforecastingwith [33] AnuragArnab,MostafaDehghani,GeorgHeigold,ChenSun,Mario
sharpness-awareminimizationandchannel-wiseattention. InForty- Lucˇic´, andCordeliaSchmid. Vivit: Avideovisiontransformer. In
firstInternationalConferenceonMachineLearning,2024. URLhttps:// 2021IEEE/CVFInternationalConferenceonComputerVision(ICCV),pages
openreview.net/forum?id=8kLzL5QBh2. 6816–6826,2021.doi:10.1109/ICCV48922.2021.00676.
©2024Datadog.Allrightsreserved. 12[34] JianlinSu,YuLu,ShengfengPan,BoWen,andYunfengLiu.Roformer: [43] Datadog. Querying, 2024. URL https://docs.datadoghq.com/
Enhancedtransformerwithrotarypositionembedding,2021. dashboards/querying/.
[35] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang,
[44] HaixuWu,TenggeHu,YongLiu,HangZhou,JianminWang,andMing-
Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A
shengLong. Timesnet: Temporal2d-variationmodelingforgeneral
length-extrapolatable transformer. In ACL 2023, December 2022.
timeseriesanalysis. InInternationalConferenceonLearningRepresenta-
URL https://www.microsoft.com/en-us/research/publication/
tions,2023.
a-length-extrapolatable-transformer/.
[36] AbhimanyuDas,WeihaoKong,AndrewLeach,ShaanKMathur,Rajat [45] AilingZeng,MuxiChen,LeiZhang,andQiangXu. Aretransformers
Sen,andRoseYu. Long-termforecastingwithtiDE:Time-seriesdense effectivefortimeseriesforecasting? ProceedingsoftheAAAIConference
encoder.TransactionsonMachineLearningResearch,2023.ISSN2835-8856. onArtificialIntelligence,37(9):11121–11128,Jun.2023.doi:10.1609/aaai.
URLhttps://openreview.net/forum?id=pCbC3aQB5W. v37i9.26317.URLhttps://ojs.aaai.org/index.php/AAAI/article/
view/26317.
[37] IanGoodfellow,YoshuaBengio,andAaronCourville. DeepLearning.
MITPress,2016.http://www.deeplearningbook.org.
[46] MinhaoLIU,AilingZeng,MuxiChen,ZhijianXu,QiuxiaLAI,Lingna
[38] D.PeelandG.J.McLachlan. Robustmixturemodellingusingthetdis- Ma,andQiangXu.SCINet:Timeseriesmodelingandforecastingwith
tribution.StatisticsandComputing,10(4):339–348,2000. sampleconvolutionandinteraction. InAliceH.Oh,AlekhAgarwal,
DanielleBelgrave, andKyunghyun Cho, editors, Advancesin Neural
[39] MikaMeitz,DanielP.A.Preve,andPenttiSaikkonen. Amixtureau- InformationProcessingSystems,2022. URLhttps://openreview.net/
toregressivemodelbasedonstudent’st–distribution. Communications forum?id=AyajSjTAzmg.
inStatistics-TheoryandMethods,52:499–515,2018.URLhttps://api.
semanticscholar.org/CorpusID:73615847.
[47] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and
RongJin. FEDformer: Frequencyenhanceddecomposedtransformer
[40] C. S. WONG, W. S. CHAN, and P. L. KAM. A student t -mixture
forlong-termseriesforecasting.InProc.39thInternationalConferenceon
autoregressivemodelwithapplicationstoheavy-tailedfinancialdata.
Biometrika,96(3):751–760,2009.ISSN00063444,14643510.URLhttp://
MachineLearning(ICML2022),2022.
www.jstor.org/stable/27798861.
[48] J.ScottArmstrong.Long-rangeForecasting:FromCrystalBalltoComputer.
[41] TaesungKim,JinheeKim,YunwonTae,CheonbokPark,Jang-HoChoi, JohnWiley&Sons,NewYork,1985.ISBN9780471822608.
andJaegulChoo. Reversibleinstancenormalizationforaccuratetime-
seriesforecastingagainstdistributionshift.InInternationalConferenceon
[49] R.JHyndmanandA.B.Koehler.Anotherlookatmeasuresofforecast
LearningRepresentations,2022. URLhttps://openreview.net/forum?
accuracy.InternationalJournalofForecasting,22,2006.
id=cGDAkQo1C0p.
[42] IlyaLoshchilovandFrankHutter.Decoupledweightdecayregulariza- [50] Datadog. Bitsai: Reimaginingthewayyourunoperationswithau-
tion. InInternationalConferenceonLearningRepresentations,2019. URL tonomous investigations, 2024. URL https://www.datadoghq.com/
https://openreview.net/forum?id=Bkg6RiCqY7. blog/bits-ai-autonomous-investigations.
©2024Datadog.Allrightsreserved. 13Appendix
A.1 Model architecture
Hyperparameter Value
EmbeddingDimension 512
MLPDimension 2048
#Layers 24
#Heads 8
#Variates 32
(β , β ) (0.9,0.95)
1 2
WeightDecay 0.01
SpacewiseLayerCadence 3
PatchSize 32
#Student-TMixtureModelComponents 16
InitialLearningRate 0.001
AnnealingSchedule Cosine
BatchSize 192
WarmupSteps 5000
TotalTrainSteps 193000
TableA.1.HyperparametersforToto
©2024Datadog.Allrightsreserved. 14A.2 Results
ZeroShot FullShot
Dataset PredictionLength Metric Toto MoiraiSmall MoiraiBase MoiraiLarge TimesFM iTransformer TimesNet PatchTST Crossformer TiDE DLinear SCINet FEDformer
96 MAE 0.366 0.402 0.402 0.398 0.398 0.405 0.402 0.419 0.448 0.464 0.400 0.599 0.419
MSE 0.307 0.375 0.384 0.380 - 0.386 0.384 0.414 0.423 0.479 0.386 0.654 0.376
192 MAE 0.368 0.419 0.429 0.434 0.424 0.436 0.429 0.445 0.474 0.492 0.432 0.631 0.448
ETTh1 MSE 0.329 0.399 0.425 0.440 - 0.441 0.436 0.460 0.471 0.525 0.437 0.719 0.420
336 MAE 0.399 0.429 0.450 0.474 0.436 0.458 0.469 0.466 0.546 0.515 0.459 0.659 0.465
MSE 0.396 0.412 0.456 0.514 - 0.487 0.491 0.501 0.570 0.565 0.481 0.778 0.459
720 MAE 0.424 0.444 0.473 0.568 0.445 0.491 0.500 0.488 0.621 0.558 0.516 0.699 0.507
MSE 0.419 0.413 0.470 0.705 - 0.503 0.521 0.500 0.653 0.594 0.519 0.836 0.506
96 MAE 0.197 0.334 0.327 0.325 0.356 0.349 0.374 0.348 0.584 0.440 0.387 0.621 0.397
MSE 0.093 0.281 0.277 0.287 - 0.297 0.340 0.302 0.745 0.400 0.333 0.707 0.358
192 MAE 0.231 0.373 0.374 0.367 0.400 0.400 0.414 0.400 0.656 0.509 0.476 0.689 0.439
ETTh2 MSE 0.135 0.340 0.340 0.347 - 0.380 0.402 0.388 0.877 0.528 0.477 0.860 0.429
336 MAE 0.260 0.393 0.401 0.393 0.428 0.432 0.541 0.433 0.731 0.571 0.541 0.744 0.487
MSE 0.160 0.362 0.371 0.377 - 0.428 0.452 0.426 1.043 0.643 0.594 1.000 0.496
720 MAE 0.355 0.416 0.426 0.421 0.457 0.445 0.657 0.446 0.763 0.679 0.657 0.838 0.474
MSE 0.294 0.380 0.394 0.404 - 0.427 0.462 0.431 1.104 0.874 0.831 1.249 0.463
96 MAE 0.328 0.383 0.360 0.363 0.345 0.368 0.375 0.367 0.426 0.387 0.372 0.438 0.419
MSE 0.306 0.404 0.335 0.353 - 0.334 0.338 0.329 0.404 0.364 0.345 0.418 0.379
192 MAE 0.353 0.402 0.379 0.380 0.374 0.391 0.387 0.385 0.451 0.404 0.389 0.450 0.441
ETTm1 MSE 0.328 0.435 0.366 0.376 - 0.377 0.374 0.367 0.450 0.398 0.380 0.439 0.426
336 MAE 0.389 0.416 0.394 0.395 0.397 0.420 0.411 0.410 0.515 0.425 0.413 0.485 0.459
MSE 0.390 0.462 0.391 0.399 - 0.426 0.410 0.399 0.532 0.428 0.413 0.490 0.445
720 MAE 0.429 0.437 0.419 0.417 0.436 0.459 0.450 0.439 0.589 0.461 0.453 0.550 0.490
MSE 0.463 0.490 0.434 0.432 - 0.491 0.478 0.454 0.666 0.487 0.474 0.595 0.543
96 MAE 0.270 0.282 0.269 0.260 0.263 0.264 0.267 0.259 0.366 0.305 0.292 0.377 0.287
MSE 0.200 0.205 0.195 0.189 - 0.180 0.187 0.175 0.287 0.207 0.193 0.286 0.203
192 MAE 0.315 0.318 0.303 0.300 0.309 0.309 0.309 0.302 0.492 0.364 0.362 0.445 0.328
ETTm2 MSE 0.269 0.261 0.247 0.247 - 0.250 0.249 0.241 0.414 0.290 0.284 0.399 0.269
336 MAE 0.319 0.355 0.333 0.334 0.349 0.348 0.351 0.343 0.542 0.422 0.427 0.591 0.366
MSE 0.264 0.319 0.291 0.295 - 0.311 0.321 0.305 0.597 0.377 0.369 0.637 0.325
720 MAE 0.374 0.410 0.377 0.386 0.415 0.407 0.403 0.400 1.042 0.524 0.522 0.735 0.415
MSE 0.354 0.415 0.355 0.372 - 0.412 0.408 0.402 1.730 0.558 0.554 0.960 0.421
96 MAE 0.212 0.299 0.248 0.242 - 0.240 0.272 0.285 0.314 0.329 0.282 0.345 0.308
MSE 0.124 0.205 0.158 0.152 - 0.148 0.168 0.195 0.219 0.237 0.197 0.247 0.193
192 MAE 0.232 0.310 0.263 0.259 - 0.253 0.289 0.289 0.322 0.330 0.285 0.355 0.315
Electricity MSE 0.138 0.220 0.174 0.171 - 0.162 0.184 0.199 0.231 0.236 0.196 0.257 0.201
336 MAE 0.249 0.323 0.278 0.278 - 0.269 0.300 0.305 0.337 0.344 0.301 0.369 0.329
MSE 0.155 0.236 0.191 0.192 - 0.178 0.198 0.215 0.246 0.249 0.209 0.269 0.214
720 MAE 0.291 0.347 0.307 0.313 - 0.317 0.320 0.337 0.363 0.373 0.333 0.390 0.355
MSE 0.211 0.270 0.229 0.236 - 0.225 0.220 0.256 0.280 0.284 0.245 0.299 0.246
96 MAE 0.223 0.212 0.203 0.208 - 0.214 0.220 0.218 0.230 0.261 0.255 0.306 0.296
MSE 0.180 0.173 0.167 0.177 - 0.174 0.172 0.177 0.158 0.202 0.196 0.221 0.217
192 MAE 0.267 0.250 0.241 0.249 - 0.254 0.261 0.259 0.277 0.298 0.296 0.340 0.336
Weather MSE 0.235 0.216 0.209 0.219 - 0.221 0.219 0.225 0.206 0.242 0.237 0.261 0.276
336 MAE 0.291 0.282 0.276 0.292 - 0.296 0.306 0.297 0.335 0.335 0.335 0.378 0.380
MSE 0.252 0.260 0.256 0.277 - 0.278 0.280 0.278 0.272 0.287 0.283 0.309 0.339
720 MAE 0.356 0.322 0.323 0.350 - 0.349 0.359 0.348 0.418 0.386 0.381 0.427 0.428
MSE 0.356 0.320 0.321 0.365 - 0.358 0.365 0.354 0.398 0.351 0.345 0.377 0.403
TableA.2.Performancemetricsforvariousmodels.Key:Bestresults,Second-bestresults.
©2024Datadog.Allrightsreserved. 15