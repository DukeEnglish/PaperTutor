MULTI-AGENT ENVIRONMENTS FOR VEHICLE ROUTING
PROBLEMS
APREPRINT
RicardoGama
SchoolofTechnologyandManagementofLamego
PolytechnicInstituteofViseu,Portugal
rgama@estgl.ipv.pt
DanielFuertes
GrupodeTratamientodeImágenes(GTI),
InformationProcessingandTelecommunicationsCenter,ETSI
Telecomunicación,UniversidadPolitécnicadeMadrid
d.fcoiras@upm.es
CarlosR.del-Blanco HugoL.Fernandes
GrupodeTratamientodeImágenes(GTI), Singuli
InformationProcessingandTelecommunicationsCenter,ETSI NewYork,USA
Telecomunicación,UniversidadPolitécnicadeMadrid hugoguh@gmail.com
carlosrob.delblanco@upm.es
November22,2024
ABSTRACT
ResearchonReinforcementLearning(RL)approachesfordiscreteoptimizationproblemshasin-
creasedconsiderably,extendingRLtoanareaclassicallydominatedbyOperationsResearch(OR).
Vehicleroutingproblemsareagoodexampleofdiscreteoptimizationproblemswithhighpractical
relevancewhereRLtechniqueshavehadconsiderablesuccess. Despitetheseadvances,open-source
developmentframeworksremainscarce,hamperingboththetestingofalgorithmsandtheabilityto
objectivelycompareresults. Thisultimatelyslowsdownprogressinthefieldandlimitstheexchange
ofideasbetweentheRLandORcommunities.
Here we propose a library composed of multi-agent environments that simulates classic vehicle
routing problems. The library, built on PyTorch, provides a flexible modular architecture design
that allows easy customization and incorporation of new routing problems. It follows the Agent
EnvironmentCycle("AEC")gamesmodelandhasanintuitiveAPI,enablingrapidadoptionandeasy
integrationintoexistingreinforcementlearningframeworks.
ThelibraryallowsforastraightforwarduseofclassicalORbenchmarkinstancesinordertonarrow
thegapbetweenthetestbedsforalgorithmbenchmarkingusedbytheRLandORcommunities.
Additionally, we provide benchmark instance sets for each environment, as well as baseline RL
modelsandtrainingcode.
1 Introduction
SincetheseminalworkonPointernetworkmodels(Vinyalsetal.(2015);Belloetal.(2017)),therehasbeensignificant
growthinmachinelearningapproachestosolvingdiscreteoptimizationproblems. Thesepioneeringstudiesoutlined
twodistinctkindsofapprochesfortrainingPointernetworkstosolvediscreteoptimizationproblems,namelysupervised
learningapproaches(Vinyalsetal.(2015))wherethenetworkistrainedonexistingsolutionsoftheproblem, and
4202
voN
12
]GL.sc[
1v11441.1142:viXraMulti-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
reinforcement learning (Bello et al. (2017)) approaches where the network learns by evaluating the quality of the
solutionsitgenerateswhileattemptingtosolvetheproblem. Whileasupervisedlearningapproachmaybedesirable
whenground-truthdataisreadilyavailable,itbecomesalimitationforsomecombinatorialoptimizationproblems
wheredatacollectionisinfeasible. Furthermore,itislimitedtothequalityoftheexistingsolutions. Areinforcement
learningapproachbypassestheneedforground-truthdatainmodeltraining,andthusextendstheapplicationofPointer
networksmodelstoalargersetofpracticaldiscreteoptimizationproblems(seeMazyavkinaetal.(2021)forarecent
overview).
OnekindofapplicationswheredevelopmenthasbeenmorenotoriousaretheVehicleRoutingProblems(VRPs). The
VRPsareagenericclassofoptimizationproblemsaimedatdeterminingthebestrouteforafleetofvehiclestaskedwith
servingacollectionofcustomers,whilesatisfyingspecificconstraints(Braekersetal.(2016)). Followingtheseminal
worksofNazarietal.(2018)andKooletal.(2019),whichusePointernetworkmodelstosolveVRPs,thefieldhas
seensubstantialgrowth,bothinthedevelopmentofnewmethodsandintheextensiontootherVRPvariants(seeLi
etal.(2022);ShiandNiu(2023);Wuetal.(2024);Zhouetal.(2024a)forrecentreviews). Moreover,recentresearch(
Liuetal.(2024a);Zhouetal.(2024b);Bertoetal.(2024b)),hasproposedmoregeneralandversatilemodels,capable
ofsimultaneouslyhandlingvariousVRP,pavingthewayforthedevelopmentofuniversalsolvers.
Although there have been notable achievements, approaches to solving VRPs tend to simplify multi-vehicle fleet
problemsbytreatingthemassingle-agent(vehicle)problemswhereavehiclerepeatedlyreturnstothedepot(seeBono
etal.(2020),Zhangetal.(2020),Zhangetal.(2023b),ArishiandKrishnan(2023),ArishiandKrishnan(2023),Berto
etal.(2024a),Liuetal.(2024b)forcapacityvehiclerouting,Fuertesetal.(2023)fororienteering,Zongetal.(2022),
Xiangetal.(2024)forpickupanddelivery,andGuoetal.(2023);PanandLiu(2023)fordynamicvehiclerouting
problems). Treatingitasasingle-agentproblemoverlooksthetruemulti-agentnatureoftheproblemandexcludesthe
potentialbenefitsofcooperativelearning,whichmayhamperdecision-makingcapabilitiesandreducethequalityof
solutions. Moreover,whiletheresultingsolutionsarehighlyeffectiveforofflineplanning,theyfallshortforonline
applicationswherefleetvehiclesneedtomakeinstantdecisionsbasedonthecurrentstateoftheenvironment. Thisisa
crucialfactorinpracticalapplicationswhereunpredictabilityisoftenpresentandpromptdecision-makingisessential.
Infact,adoptinganonlinemulti-agentstrategycanofferbenefitsbeyonddynamicandstochasticroutingproblems,but
alsotodeterministicones,therebyimprovingthedevelopmentofmorerobustsystemstailoredforreal-worldconditions.
Inordertodevelopsuccessfulmulti-agentmodels/solutionsfordiscreteoptimizationproblemsitisimportanttohave
accesstocommondevelopmentframeworksandsimulationenvironments. These,however,remainlimited,particularly
for VRPs, which hinders an objective comparison of results, the exchange of ideas, and the development of the
field. Aimingatmitigatingthis,weproposealibrary,builtonPytorchPaszkeetal.(2019),thatprovidesmulti-agent
environmentsdesignedtosimulatetraditionalvehicleroutingproblems. WemainlyfollowPettingZooAPIlogic,and
hasbeengreatlyinfluencedbytheFlatlandenvironmentslibrary(Mohantyetal.(2020)),andwechosetoadoptsome
ofitsdesignprincipleswhichwebelieveconsiderablyimprovethegeneralityandusabilityoftheenvironments. Infact,
ourlibraryoffersaflexibleandmodulararchitecture,andallowsforeasycustomizationandimplementationofnew
VRPs. Itfeaturesauser-friendlyAPIforrapidadoptionandsmoothintegrationwithcurrentreinforcementlearning
frameworks. Thelibraryincludesbenchmarkinstancesetsforeveryenvironment,aswellasbaselineneuralnetwork
modelswithRLtrainingcode.
VRPsareimmenselydiverseastheyaimataddressingincreasinglydifficultreal-lifeapplications. Asastartingpoint,
wechoosetoimplementenvironmentsthatmainlyfocusonthelargeclassofVRPswithtime-windows. Thisclassis
comprehensiveenoughtoincludemanyproblemsofpracticalinterestandcanbeusedasafirstbaseforgeneralizingto
other,morecomplexproblems.
ThemaincontributionsoftheproposedMulti-agentEnvironmentsforVehicleRoutingProblemslibrary(MAEnvs4VRP)
are:
• Sufficientlygeneralandmodulartoallowstraightforwardimplementationandgeneralizationtootherrouting
problems.
• Allowsforthestraightforwardadoptionofmulti-agentenvironmentsintendedforonlineandofflineapplica-
tions.
• FollowsafamiliarAPI,whichenablesintegrationintoexistingmulti-agentreinforcementlearning(MARL)
algorithmtraininganddevelopmentplatforms.
• Simplebenchmarkinstancesintegrationandeasyimplementionofnewinstances,allowingforacleanand
reproducibledefinitionoftrain,validationandtestsets.
Theremainderofthismanuscriptisdelineatedasfollows. InSection2wegiveafirstoverviewofrelatedwork. Next,
inSection3wedescribethelibrary’sarchitecture: mainstructureandcorefunctionalities. InSection4,wepresenta
2Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
setofexperimentsalongsidebenchmarkresultswithrespecttoasub-setoftheavailableenvironments. InSection5we
discussthecurrentdevelopmentstateandfuturedirections.
2 BackgroundandRelatedWork
Therecentsuccessandgrowthofreinforcementlearning(RL),particularlymulti-agentreinforcementlearning(MARL),
hasbeenaccompaniedandpromotedbytheappearanceandestablishmentofseveraldevelopmentframeworks(e.g.
Raffinetal.(2021);Bouetal.(2023);Huetal.(2023)). LibrariesbasedonstandardizedAPIs,suchasGym(Brockman
etal.(2016))andPettingZoo(Terryetal.(2021)),allowthedevelopment,testingandcomparisonofalgorithmson
acommonplatform. Infact,asthefieldexpands,achievingstandardizationandreproducibilityemergesascrucial
concerns, both within individual research communities (e.g. MARL, Bettini et al. (2024)) and across intersecting
disciplines,suchastheRLandORcommunitiesinthecontextofcombinatorialoptimization(Accorsietal.(2022)).
ExistinglibrarieswithVRPenvironments.
Althoughscarce,wehaverecentlyobservedthedevelopmentofafewlibrarieswithRLenvironmentsforspecificdiscrete
optimizationproblems. ParticularlyORL(Balajietal.(2019)),OR-Gym(Hubbsetal.(2020)),Graphenv(Biagioni
etal.(2022)),RLOR(Wanetal.(2023))andJumanji(Bonnetetal.(2023))madeavailableseveralenvironmentsof
operationsresearchproblems,suchasKnapsack,BinPacking,InventoryandNetworkManagement,VehicleRouting
andTravellingSalesman.
#VRP Customizable Customizable Customizable
Library Multi-agent Vectorized On/Offline
enviroments generation observations rewards
ORL(Balajietal.(2019)) 1 ✗ ✗ ✗ ✗ ✗ online
OR-Gym(Hubbsetal.(2020)) 2 ✗ ✗ ✗ ✗ ✗ offline
Graphenv(Biagionietal.(2022)) 1 ✗ ✗ ✗ ✗ ✗ offline
RLOR(Wanetal.(2023)) 2 ✗ ✓ ✗ ✗ ✗ offline
RoutingArena(Thyssensetal.(2023)) 1 ✗ ✓ ✓ ✗ ✗ offline
Jumanji(Bonnetetal.(2023)) 3 ✓(‡) ✓ ✓ ✗ ✗ offline
RL4CO(Bertoetal.(2023)) 20 ✗ ✓ ✓ ✗ ✗ offline
Maenvs4VRP(ours) 7 ✓ ✓ ✓ ✓ ✓ both
‡Ithasonemulti-agentenvironment
Table2.1: ComparisonofexistinglibrariesthatprovideVRPsenvironments
Inabroaderapproach,twogeneralframeworks,RoutingArena(Thyssensetal.(2023))andRL4CO(Bertoetal.(2023)),
aimatfacilitatingreproducibleresearchandalgorithmbenchmarking. RoutingArenacentersontheCapacitatedVRP
andoffersabenchmarkplatformthatincludespopularORbaselinesandbenchmarksandenablesthecomparisonof
differentRLandORsolversusingstandardizedevaluationmetrics. TheRL4COlibraryprovidesaunifiedframework,
whichintegratesenvironments,policies,andreinforcementlearningalgorithmsinonecomprehensivepackage. In
addition,thereareseveralisolatedenvironments,usuallymadeavailableasaccompanyingmaterialofresearcharticles
(e.g. Kooletal.(2019);Kwonetal.(2020);Kimetal.(2022);Zhangetal.(2023a);Bertoetal.(2024a)). Theseare,
however,typicallylimitedtotheproblemsforwhichtheyweredeveloped,andtheircustomizationandreusetoother
contextsisseldomstraightforward.
Whilethesetoolsrepresentprogresstowardscreatingunifiedframeworksforresearchanddevelopment,theyfocus
almost exclusively on single-agent environments (see Table 2.1). This emphasis overlooks elements that could be
inherentlyaddressedbymulti-agentreinforcementlearningtechniques,suchas,forinstance,thecollaborationamong
fleetvehicles. Additionally,theygenerallydonotoffertheflexibilitytocustomizedifferentfunctionalpartsofthe
environmentsthatareparticularlyrelevantforRL,likerewardsandobservations,therebyrestrictingtheexploration
of new ideas and the possible customization to suit different application constraints. To address these issues, we
developed MAEnvs4VRP from the ground up in a modular manner that allows independent personalization of the
differentcomponentsoftheenvironment.
ModelingandAPIsinMARL.
ThedesignofreinforcementlearningenvironmentsandAPIs,particularlythosethatinvolveMARL,isstrictlyrelated
to the formal game models they adopt. Most MARL research has commonly used one of two underlying formal
game models: Partial Observable Stochastic Games (Albrecht et al. (2024)) and Extensive Form Games (Shoham
andLeyton-Brown(2008)). Althoughwidelyused,thesemodelssufferfromsomedrawbackswhenmovingfromthe
3Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
abstractmodeltoitspracticalcodeimplementation(Terryetal.(2021)). Inanattempttomitigatethesedrawbacks,
PettingZoo’sAgentEnvironmentCyclegamesmodel(AEC),Terryetal.(2021),emergedasaconceptualalternativeto
thecommonlyusedmodels,presentingadvantagesinthepracticalimplementationofmulti-agentenvironmentsand
theirAPIdesign.Inconcrete,intheAECmodel,eachagentactssequentially,seekingtoresolve’tie-breaking’decisions
(conflicthandling)thatoftenoccurinmulti-agentsettings,whenagentsareallowedtochooseactionssimultaneously.
Inaddition, sequentialactingalsoallowsforclearinformationmanagementinenvironmentswherethenumberof
agentscanchangethroughoutanepisode(rollout)duetocreationorelimination. Anothersignificantfeatureisthatthe
AECmodelenablesrewardstobedistributedcollectivelyorindividuallyduringoraftertheepisode,offeringversatility
inexploringrewardengineeringstrategiesthatenhanceandaidlearning.
Figure2.1: Illustrationofamulti-agentVRPinstancewithfourvehicles(left)andthecorrespondingtimeline(right).
Allvehicleshavecompletedaseriesofthreeactionssteps. Sinceagentsactasynchronously,thenextagenttointeract
withtheenvironmentmighthaveonlypartialinformationavailable.
ThesecharacteristicsoftheAECgamemodelareparticularlycrucialinVRP,andsimilarideaswereexploredinBono
etal.(2020),wheretheauthorspresentedasequentialmulti-agentMarkovdecisionprocess,toaddressmulti-agent
dynamic vehicle routing problems. In fact, for time-dependent VRPs, each agent (vehicle) action is extended in
time,and,asaconsequence,agentshavetomakedecisionsasynchronously. Asaconsequence,iftheagentsacted
simultaneously,therelativetimingoftheeventsislost,andlearningpoliciesintheseenvironmentsmightnotapply
effectivelytoreal-worldscenarios(Mendaetal.(2018)). Furthermore,thevariousconstraintsinroutingproblems
imposethecreationofconflictresolutionrules.
Asanexample,considertheproblemillustratedinFigure2.1. Therearefouragentswhohavealreadycompletedthree
stepsinanenvironmentwhereeachservicecanonlybeperformedbyasinglevehicle. Ifweconsidertheenvironment
stepswithagentsactingsimultaneously,internally,itmusthandleconflictingactionsandapplysometiebreakerrule
todetermineeachagent’sactions. Thisapproachhastwomaindrawbacks. First,onceanagent’sactionisselected,
theenvironment’sstatechanges,leavingotheragentstomakedecisionsbasedonoutdatedobservations(Terryetal.
(2021)). Moreover,assumingsimultaneousactionbyallagentsdoesnottranslatedirectlyintopracticalapplications,
whereagentshavetodecidetheiractionsatdifferenttimes(Mendaetal.(2018)). Theseissuesareabsentwhenagents
operatesequentially,asdecisionsaremadeusingthelatestenvironmentalobservationsandcantakeintoaccountthe
otheragents’timelines. ThisledustoadopttheAECmodelinthefoundationsofourlibrary.
3 Multi-agentEnvironmentsforVehicleRoutingProblems(MAEnvs4VRP)
Inthissection,weoutlinethearchitectureofourlibraryandthedesignofitsAPI.Inordertoenhancedesignfreedomand
maintenance,wehaveoptedtoimplementeachenvironmentindependentlywhilemaintainingaconsistenttransversal
structure.
3.1 MAEnvs4VRPmodules
Everyenvironmentisconstructedusingfourkeyfunctionalmodules: InstanceGenerator,ObservationsGenerator,
AgentSelector, andRewardsclasses(Fig. 3.1). Eachblockoperatesindependentlyoftheothers, managingakey
elementoftheenvironment’sdynamics.
Instance Generator Class. This module is responsible for the definition of the problem’s sample space and the
generationofinstances. Itallowsforfastexplorationofnewinstancedistributionsandstraightforwardadoptionof
standardbenchmarkinstancedatacommonlyusedtoevaluateVRPsolvers. Thisaimsatnarrowingthegapbetween
4Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
Figure3.1: Schematicrepresentationofthelibraryarchitecture.
testbedsforalgorithmbenchmarkingusedinRLandORcommunities,Accorsietal.(2022),allowingamoreobjective
performancecomparisononacommonground.
Foreachenvironment,weprovidethreeimplementations:
• ToyInstanceGenerator-generatesasmalltoyinstanceforenvironmenttestinganddebugging;
• InstanceGenerator-randominstancegenerator,following,wheneverpossible,thesamplespacedefinition
of a seminal paper for the particular VRP problem. Using this generator, we provide test and validation
instancesets;
• BenchmarkInstanceGenerator-readsandparsesstandardORcommunitytestbenchmarkinstancesinto
ourenvironmentinstancedataformat;
Alongsidemethodshandlingthedescribedfunctionalities,theseclassesincludetheaugment_generate_instance
method,whichsimplifiestheadditionofinstanceaugmentations,allowingfortheexplorationofsymmetrieswithinthe
generatedenvironments. Augmentationtechniqueshavebeensuccessfullyappliedduringtrainingandinferenceto
enhancepolicyandsolutionsquality(e.g. Kwonetal.(2020);Kimetal.(2022);Lietal.(2024)).
Observationsclass. Oneimportantaspectforthesuccessfultrainingoftheagentsistheirabilitytoretrieveuseful
informationfromtheenvironmentinordertoactonit. Maintaininganindependentandcustomizableobservationclass
empowersfeatureengineeringandobservationspaceexploration,openingthepossibilityofcreatingmoreawareand
capableagents. Italsoenablesthecreationofversatileenvironmentsthatcanbeeasilyadjustedtoreflectbothfullyand
partiallyobservablescenarios(Albrechtetal.(2024)),dependingonthespecificpracticalapplicationathand. This
classisresponsibleforcomputingtheobservationsavailabletotheactiveagentwhileinteractingwiththeenvironment.
Therearefivetypesofpossibleobservations:
• nodes_static - nodes (depots and services) intrinsic observations (e.g. location, time window width,
demands,profits,etc);
• nodes_dynamic-nodes(depotsandservices)step-dependentobservations. Usually,theseobservationsare
computedinrelationtotheactiveagent(e.g. timeusedbytheagentafternodevisit,timeleftforlocation
opening,timeleftforlocationclosing);
• agent-activeagent-relatedobservations(e.g. numberofvisitednodes,numberoffeasiblenodes,current
load,timeavailable);
• other_agents-observationsregardingallagentsstillactiveintheenvironment(e.g. location,timeavailable,
usedcapacity,distancetotheactiveagent,timedistancetotheactiveagent);
• global-globalstateobservationsoftheenvironment(e.g. numberofcompletedservices,numberofused
agents,totaldemandsatisfied,totalprofitcollected);
Bydefault,foreachenvironment,awidesetofgenericandproblem-specificobservationsarereadilyavailable.
AgentSelectorclass. EquivalenttoPettingZoo’s,theAgentSelectorclasscontrolstheselectionofthenextagent
thatwillinteractwiththeenvironment,throughthe_next_agentmethod. Thisisanimportantaspectinmulti-agent
problems,particularlyintime-dependentones,asitenablestheexplorationofdifferentstrategiesforchoosingagents.
5Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
ForabroaderclassofVRPproblems,wheretimeconstraintsorsomeformofrandomnessispresent,moresuitable
agentselectionfunctions,dependentonthestateoftheenvironment,canbedesignedorevenlearned.
Currently, Agent Selector classes available are AgentSelector, SmallestTimeAgentSelector and
RandomSelector. In AgentSelector the generator steps through the active agents in a circular fashion un-
tilnomoreactiveagentsareavailable. Italwaysselectsthesameagentuntilthatagentreturnstothedepot. Afterwards,
it selects the next active agent and repeats the process until all agents are done, thus mimicking a single-agent
environment. SmallestTimeAgentSelectorselectstheagentwiththesmallestacumulatedtimesincedeparture
fromthedepot(Bonoetal.(2020)). Thisallowstosimulateanenvironmentinwhichagentsmustactinreal-time.
Lastly,RandomSelectorselectsrandomlybetweenactiveagents.
Rewardclass.
Generally,theobjectivefunctiontobeoptimizedmaydiffersignificantlyacrossvariousVRPtypes,contingentupon
thespecificapplication. Additionally,evenforthesameVRP,theliteraturemightpresentdiverseobjectivefunctions.
Forinstance,intheCapacitatedVehicleRoutingProblemwithTimeWindows(A.1),conventionalobjectivesmight
includeminimizingthetotalroutedistance,reducingthetotalnumberofvehicles,oroptimizingalinearcombination
ofthesefactors. Toeasilyaccountforthisvariabilityintheobjectivefunction’ssetup,theRewardclassallowsfor
a straightforward reward design through the get_reward method, enabling also a broader exploration of reward
engineering.
Infact,toaccountforconstraintsviolation(e.g. maximumallowedtourduration,timewindowviolation,unvisited
customers, etc.), themethodreturnsforeveryenvironmentsteparewardandapenalty, Zhangetal.(2023a). The
abilitytodefineapenalty,alongsideandseparatelyfromthereward,isespeciallybeneficialforscenariosthataim
atminimizingoveralltraveltimeordistance. Thisapproachenables,ontheimplementationside,agentstohavethe
freedomtodecidewhethertoperformservicesorstayatthedepot,withoutneedingtoforceaminimumnumberof
services.
Currently,eachenvironmentprovidesaccesstobothdenserewards(availableateverystepoftheenvironment)and
sparserewards(retrievedattheepisode’sconclusion). Theflexibilitytochoosetherewardtypeexpandsthepossibilities
ofdownstreamalgorithmdesign.
3.2 EnvironmentsandAPI
Inordertoproviderepresentativeexamplesofvariousvariants,weimplementedenvironmentscontemplatinghard
andsofttimewindowsconstraintsandsingle/multi-depotsettings. Currently, thelibraryofferssevenoff-the-shelf
environmentstosimulatethefollowingVRP:CapacitatedVehicleRoutingProblemwithsoftandhardtimewindows,
TeamOrienteeringProblem,PickupandDeliveryProblem,SplitDeliveryVehicleRoutingProblem,Prize-Collecting
Vehicle Routing Problem and Multi-depot Vehicle Routing Problem. In line with Accorsi et al. (2022), precise
definitionsofeachenvironment,alongwiththeirobservationsandrewards,areprovidedinAppendixA.
OurlibrarymanagesalldatathroughTensorDicts,adictionary-likestructureintroducedinTorchRL(Bouetal.(2023)).
Thisallowsforefficienttensoroperations,streamlinesdatamanagement,andsupportsthedevelopmentofbatched
(vectorized)environments. AsillustratedinCodesnippet1, atypicalepisoderunfollowsthelogicfoundinmost
standardRLAPIs.
Theenvironmentissetupusingtheresetmethod. Thismethodinternallygeneratesatd_statestatedictionary
whichcontainsallthedetailsaboutnodesandagents. Italsoselectsthefirstagenttoactanddeterminesitsobservations
throughthemethodobserve. Thismethodprovidestheenvironmentstatewithobservations,alongwiththeactionand
agents’masks. ATensorDictisreturnedwhichcontainskeyssuchasagentindex,observations,reward,penalty,and
done.
Followinginitialization, theloopofagentselection, observationgathering, actiondetermination, andenvironment
updating continues until all vectorized environments are complete. This method naturally and clearly depicts the
step-by-stepprocesstheagentsuseintheirinteractionwiththeenvironment. Specifically,afteranagentisselected,it
observesthecurrentstateoftheenvironment;basedontheseobservations,anactionisdeterminedandexecuted(step),
updatingtheenvironment’sstate(andanyotheragentswithinit).
An additional feature in our environments is the stats_report method. It is used when all agents are done, and
returnsadictionarywithepisodicrelevantinformation,e.g. finalepisodicrewardandpenalties,solutionstatistics,etc.
Thisinformationisintegrated,alongwithcomplementaryagentinformation,inthelastreturnedinfodictionary.
6Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
from maenvs4vrp.environments.cvrptw.env import Environment
from maenvs4vrp.environments.cvrptw.env_agent_selector import AgentSelector
from maenvs4vrp.environments.cvrptw.observations import Observations
from maenvs4vrp.environments.cvrptw.instances_generator import InstanceGenerator
from maenvs4vrp.environments.cvrptw.env_agent_reward import DenseReward
gen = InstanceGenerator()
obs = Observations()
sel = AgentSelector()
rew = DenseReward()
env = Environment(instance_generator=gen,
obs_builder=obs,
agent_selector=sel,
reward_evaluator=rew,
seed=0)
env_state : TensorDict = env.reset()
while not env_state["done"].all():
env_state = env.sample_action(env_state)
env_state = env.step(env_state)
Codesnippet1: BasicAPIusageexample. Here,env_stateisadatacontainerTensorDict.
4 Experimentsandbenchmarkresults
WeobtainedbaselineperformancevaluesforasubsetofavailableenvironmentsbytrainingtheMARDAMmulti-agent
networkmodel(Bonoetal.(2020)). Thismodel,whoseimplementationwasmadepubliclyavailablebytheauthors,is
avariationoftheAttentionmodel(Kooletal.(2019)),andwasdesignedsothatitcouldhandledynamicmulti-agent
routingproblems,wheredecisionshavetobemadeonline. Itcombinesthestandardattentionnodeencodingblockwith
amulti-headattentionlayertoencodethevehiclefleet.
Tofullyleveragetheinformationavailableoneachagent’sobservationsbeforeselectingthenextaction,wedevelopedan
extensionofthepreviousmodel,termedtheMulti-AgentDynamicAttentionModel(MADyAM).Ateachenvironment
step,themodelreliesonthecurrentagentstateandtwodynamicencodingblocks,onefornodesandoneforthevehicle
fleet,allowingforintegratingupdatedfeaturesateverystageofthesolutionconstructionprocess(seeAppendixBfora
detaileddescriptionoftheneuralnetworkmodelarchitecture).
Wereleaseallbaselinemodelswithtrainingcodeandpre-trainedcheckpoints. Inaddition,foreachenvironment,we
provideavalidationsetcomprised2048instanceswith50and100servicestoenableanobjectivecomparisonbetween
currentandfutureapproaches.
4.1 Experimentalsetup
In our experiments, we focus on CVRTPTW, TOPTW, PCVRPTW and MDVRPTW problems. For each of these
VRPs,wetrainmodelsinenvironmentsusingtwodifferentagentselectionstrategies: sequentialagentselection,thus
simulatingasingleagentwithmultipletripstothedepot,andsmallesttimeagentselection,similartoBonoetal.(2020),
simulatinganonlinesetting.
Forthebaselineperformancestudy,weuseenvironmentscomprising50and100services. Weuseafleetof5vehicles
for both the TOPTW and the PCVRPTW problems. In the case of CVRTPTW, 25 vehicles were used, while for
MDVRPTW,theconfigurationincluded5depots,eachequippedwith5vehicles. Foradetaileddescriptionofthe
rewardsandobservationsusedbytheagents,seeAppendixA.
Training. Fortrainingtheneuralsolvers,wefollowmostsetupsin(Bonoetal.(2020)). Wetrainallmodelswith
REINFORCEalgorithm(Williams(1992))withcriticbaseline. WeuseAdamoptimizer(KingmaandBa(2015)),with
learningrateof1e−4forthepolicynetworkand1e−3forthecritic. Themodelsweretrainedfor100epochs,each
containing2500batchesofsize512traininginstances(i.e.,128Mtraininginstancesintotal).
7Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
Wealsocomputebaselinevalueswiththestate-of-the-artVRPsolverPyvrp, Woudaetal.(2024). Thisisafreely
availablehigh-performanceVRPsolverthatachievesstate-of-the-artresultsthatsupporttheseproblemsoff-the-shelf.
ForPyVRP,wemultiplyallfloatinputsby107beforeroundingtointegersandrunningthesolverfor60seconds.
Allexperimentswereperformedona2×NvidiaGeForceRTX4090(GPU)anda13thGen. IntelCorei9-13900×32
(CPU).
4.2 Experimentalresults
Foreveryenvironment,wereportthescoregaptothePyVRPsolverresultinthebenchmarkinstances. Wedefinethe
scoregaptothePyVRPsolveras
score −score
gap= model PyVRP ×100.
score
PyVRP
NoticethatforCVRPTWandMDVRPTWthegapwillbenegativeifthemodeloutperformsPyVRPsolver. Onthe
otherhand,forTOPTWandPCVRPTWgapwillbepositiveifthemodeloutperformsPyVRPsolver.
Thebaselineresultsobtained,asshowninTable4.1,indicatethatthepoorestperformanceconsistentlyoccurswhen
models are trained under an online framework with smallest time agent selection. This setup requires that agents
masterasynchronouscollaboration,considerablycomplicatingthetask. Thisunderscoresthecriticalneedforauthentic
multi-agentsimulatorsthataidininvestigatinganddevisingmethodstailoredtotheuniquechallengesofmulti-agent
scenarios.
50services 100services
problem method agentselection
av.obj. av.#agents gap(%) av.obj. av.#agents gap(%)
PyVRP - 14.478 9.6 − 24.493 16.9 −
MARDAM 16.499 11.0 14.0 29.828 19.5 21.8
singleagent
CVRPTW↓ MADyAM 16.020 10.8 10.7 27.413 18.9 11.9
MARDAM 18.787 12.1 29.8 36.993 25.0 51.0
smallesttime
MADyAM 18.450 11.9 27.4 31.827 20.7 29.9
PyVRP - 33.245 5.0 − 39.984 5.0 −
MARDAM 31.870 5.0 -4.1 39.251 5.0 -1.8
singleagent
TOPTW↑ MADyAM 32.159 5.0 -3.3 39.710 5.0 -0.7
MARDAM 31.833 5.0 -4.2 39.026 5.0 -2.4
smallesttime
MADyAM 31.927 5.0 -4.0 39.407 5.0 -1.4
PyVRP - 26.653 5.0 − 35.534 5.0 −
MARDAM 24.226 5.0 -9.1 32.325 5.0 -9.0
singleagent
PCVRPTW↑ MADyAM 24.618 5.0 -7.6 32.907 5.0 -7.4
MARDAM 23.593 5.0 -11.5 31.309 5.0 -11.9
smallesttime
MADyAM 24.388 5.0 -8.5 32.706 5.0 -8.0
PyVRP - 8.928 10.5 − 15.224 17.9 −
MARDAM 14.534 11.8 62.8 34.869 19.6 129.0
singleagent
MDVRPTW↓ MADyAM 10.312 12.7 15.5 17.530 19.1 15.1
MARDAM 16.196 13.7 81.4 26.824 17.7 76.2
smallesttime
MADyAM 11.538 13.5 29.2 19.720 20.8 29.5
Table4.1: Baselinevaluesobtained,usinggreedyinference,foreachproblemvalidationset,trainingMARDAMand
MADyAMmodelswithsingleandsmallesttimeagentselectionstrategies. Forproblemswith↑,higherobjectivevalues
arebetter,andfor↓,lowerobjectivevaluesarebetter.
Analyzingtheresultsacrossthevariousproblems,weseethatforTOPTWandPCVRPTWtheaverageabsolutegap
is considerably smaller than for the other two problems, 5.9% versus 39.7% for CPRPTW and MDVRPTW. This
differenceismainlyduetotwofactors. Infact,fortheseproblems,thenumberofagentsusedissmaller(5versus25),
andtherearenopenaltiesfornotattendingservices,whichfacilitateslearning.
IntheCPRPTWandMDVRPTWproblems,theevaluatedbaselinemodelsexhibitedinferiorperformance. Thisis
inpartbecausetheseenvironmentsinvolvealargernumberofagents,makingitmorechallengingforthemtolearn
effectivecollaboration. Specifically,insuchsettings,agentsarerequiredtolearnhowtominimizethetotaldistance
traveled,whilesimultaneouslyensuringthefulfillmentofallservicesandpreventinganypenaltiesbytheepisode’s
end. Consequently,someagentsmayinevitablydecidetoremainatthedepot. Forexample,Figure4.1showsthatin
8Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
theCVRPTWscenario,theMARDAMmodelagentswereunabletolearnthestrategyofstayinginthedepot,which
resultedinsuboptimalfinalsolutions. Conversely,intheMDVRPTWscenario,thisskillwasacquiredaroundepoch35,
resultinginasignificantincreaseinreturn. Thisobservation,alsonotedinBonoetal.(2020),indicatesthatexploring
approachestoimproveinter-agentcommunicationcouldpotentiallyenhancecollaborationandsolutionquality.
Figure4.1: Evolutionofmodelsperformanceduringtraining,usingsingleandsmallesttimeagentselectionstrategies,
averagedbyepoch. Eachproblemas100services. FortheCVRPTWthefleethas25vehicles,andforbothTOPTW
andPCVRPTW5vehicles.
9Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
5 Discussionandfuturedevelopments
HereweintroduceMAEnvs4VRP,anopen-sourcelibraryfeaturingamulti-agentenvironmentdesignedtosimulate
vehicleroutingproblems. Notonlydoesitprovideasetofready-to-useenvironmentsforclassicalVRP,butitalso
offersaflexibleframeworkthatfacilitateseasycustomizationandtheimplementationofnewVRPenvironmentsin
bothonlineandofflinesetups. Withamodulararchitecture,asimpleAPIanddetailedaccompanyingdocumentation,it
enhancesitsadoptionbyresearchers. Alongwithdetailingthelibrary,wepresentbaselineexperimentalresults,thatwe
hopewillmotivatetheexplorationofnewapproachesandmulti-agentalgorithmsaimedattacklingroutingproblems
throughreinforcementlearning. Inthenearfuture,weintendtoimplementenvironmentsfordynamicandstochastic
problems,thusexpandingtheclassofproblemscovered.
TheprojectsourcecodecanbefoundinGitHub1.
Acknowledgements
The first author is grateful to the Research Centre in Digital Services (CISeD), the Polytechnic of Viseu and
FCT - Foundation for Science and Technology, I.P., within the scope of the projects Refª UIDB/05583/2020 and
2023.13303.CPCA.A0fortheirsupport.
References
Accorsi,L.,Lodi,A.,andVigo,D.(2022). Guidelinesforthecomputationaltestingofmachinelearningapproachesto
vehicleroutingproblems. OperationsResearchLetters,50(2):229–234.
Albrecht,S.V.,Christianos,F.,andSchäfer,L.(2024). Multi-AgentReinforcementLearning: FoundationsandModern
Approaches. MITPress.
Arishi,A.andKrishnan,K.(2023). Amulti-agentdeepreinforcementlearningapproachforsolvingthemulti-depot
vehicleroutingproblem. JournalofManagementAnalytics,10(3):493–515.
Balaji,B.,Bell-Masterson,J.,Bilgin,E.,Damianou,A.,Garcia,P.M.,Jain,A.,Luo,R.,Maggiar,A.,Narayanaswamy,
B.,andYe,C.(2019). Orl: Reinforcementlearningbenchmarksforonlinestochasticoptimizationproblems. arXiv
preprintarXiv:1911.10641.
Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. (2017). Neural Combinatorial Optimization with
ReinforcementLearning. Proceedingsofthe5thInternationalConferenceonLearningRepresentations(ICLR).
Berto,F.,Hua,C.,Luttmann,L.,Son,J.,Park,J.,Ahn,K.,Kwon,C.,Xie,L.,andPark,J.(2024a). Parco: Learning
parallelautoregressivepoliciesforefficientmulti-agentcombinatorialoptimization. arXivpreprintarXiv:2409.03811.
Berto,F.,Hua,C.,Park,J.,Kim,M.,Kim,H.,Son,J.,Kim,H.,Kim,J.,andPark,J.(2023). RL4CO:anextensive
reinforcementlearningforcombinatorialoptimizationbenchmark. arXivpreprintarXiv:2306.17100.
Berto,F.,Hua,C.,Zepeda,N.G.,Hottung,A.,Wouda,N.,Lan,L.,Tierney,K.,andPark,J.(2024b). Routefinder:
Towardsfoundationmodelsforvehicleroutingproblems. arXivpreprintarXiv:2406.15007.
Bettini,M.,Prorok,A.,andMoens,V.(2024). Benchmarl: Benchmarkingmulti-agentreinforcementlearning. Journal
ofMachineLearningResearch,25(217):1–10.
Biagioni,D.,Tripp,C.E.,Clark,S.,Duplyakin,D.,Law,J.,andJohn,P.C.S.(2022). graphenv: apythonlibraryfor
reinforcementlearningongraphsearchspaces. JournalofOpenSourceSoftware,7(77):4621.
Bianchessi,N.,Drexl,M.,andIrnich,S.(2019). Thesplitdeliveryvehicleroutingproblemwithtimewindowsand
customerinconvenienceconstraints. TransportationScience,53(4):1067–1084.
Bonnet,C.,Luo,D.,Byrne,D.,Surana,S.,Coyette,V.,Duckworth,P.,Midgley,L.I.,Kalloniatis,T.,Abramowitz,
S.,Waters,C.N.,Smit,A.P.,Grinsztajn,N.,Sob,U.A.M.,Mahjoub,O.,Tegegn,E.,Mimouni,M.A.,Boige,R.,
deKock,R.,Furelos-Blanco,D.,Le,V.,Pretorius,A.,andLaterre,A.(2023). Jumanji: adiversesuiteofscalable
reinforcementlearningenvironmentsinjax.
Bono,G.,Dibangoye,J.S.,Simonin,O.,Matignon,L.,andPereyron,F.(2020). Solvingmulti-agentroutingproblems
usingdeepattentionmechanisms. IEEETransactionsonIntelligentTransportationSystems,22(12):7804–7813.
Bou,A.,Bettini,M.,Dittert,S.,Kumar,V.,Sodhani,S.,Yang,X.,Fabritiis,G.D.,andMoens,V.(2023). Torchrl: A
data-drivendecision-makinglibraryforpytorch.
1https://github.com/ricgama/maenvs4vrp
10Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
Braekers, K., Ramaekers, K., and Van Nieuwenhuyse, I. (2016). The vehicle routing problem: State of the art
classificationandreview. Computers&industrialengineering,99:300–313.
Brockman,G.,Cheung,V.,Pettersson,L.,Schneider,J.,Schulman,J.,Tang,J.,andZaremba,W.(2016). Openaigym.
arXivpreprintarXiv:1606.01540.
Dumas,Y.,Desrosiers,J.,andSoumis,F.(1991). Thepickupanddeliveryproblemwithtimewindows. European
JournalofOperationalResearch,54(1):7–22.
Figliozzi,M.A.(2010). Aniterativerouteconstructionandimprovementalgorithmforthevehicleroutingproblem
withsofttimewindows. TransportationResearchPartC:EmergingTechnologies,18(5):668–679. Applicationsof
AdvancedTechnologiesinTransportation: Selectedpapersfromthe10thAATTConference.
Fuertes,D.,delBlanco,C.R.,Jaureguizar,F.,andGarcía,N.(2023). Solvingtheteamorienteeringproblemwith
transformers. arXivpreprintarXiv:2311.18662.
Guo,F.,Wei,Q.,Wang,M.,Guo,Z.,andWallace,S.W.(2023). Deepattentionmodelswithdimension-reductionand
gatemechanismsforsolvingpracticaltime-dependentvehicleroutingproblems. TransportationResearchPartE:
LogisticsandTransportationReview,173:103095.
Hu,S.,Zhong,Y.,Gao,M.,Wang,W.,Dong,H.,Liang,X.,Li,Z.,Chang,X.,andYang,Y.(2023). Marllib: Ascalable
andefficientmulti-agentreinforcementlearninglibrary. JournalofMachineLearningResearch,24(315):1–23.
Hubbs,C.D.,Perez,H.D.,Sarwar,O.,Sahinidis,N.V.,Grossmann,I.E.,andWassick,J.M.(2020). Or-gym: A
reinforcementlearninglibraryforoperationsresearchproblems.
Kim, M., Park, J., and Park, J. (2022). Sym-nco: Leveraging symmetricity for neural combinatorial optimization.
AdvancesinNeuralInformationProcessingSystems,35:1936–1949.
Kingma,D.P.andBa,J.(2015). Adam: Amethodforstochasticoptimization. InternationalConferenceonLearning
Representations(ICLR),abs/1412.6980.
Kool, W., Van Hoof, H., and Welling, M. (2019). Attention, learn to solve routing problems! 7th International
ConferenceonLearningRepresentations,ICLR2019,pages1–25.
Kwon,Y.-D.,Choo,J.,Kim,B.,Yoon,I.,Gwon,Y.,andMin,S.(2020). Pomo: Policyoptimizationwithmultiple
optimaforreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,33:21188–21198.
Li, B., Wu, G., He, Y., Fan, M., and Pedrycz, W. (2022). An overview and experimental study of learning-based
optimizationalgorithmsforthevehicleroutingproblem. IEEE/CAAJournalofAutomaticaSinica,9(7):1115–1138.
Li,J.,Niu,Y.,Zhu,G.,andXiao,J.(2024). Solvingpick-upanddeliveryproblemsviadeepreinforcementlearning
basedsymmetricneuraloptimization. ExpertSystemswithApplications,255:124514.
Liu,F.,Lin,X.,Zhang,Q.,Tong,X.,andYuan,M.(2024a). Multi-tasklearningforroutingproblemwithcross-problem
zero-shotgeneralization. arXivpreprintarXiv:2402.16891.
Liu, Q., Liu, C., Niu, S., Long, C., Zhang, J., and Xu, M. (2024b). 2d-ptr: 2d array pointer network for solving
theheterogeneouscapacitatedvehicleroutingproblem. InProceedingsofthe23rdInternationalConferenceon
AutonomousAgentsandMultiagentSystems,pages1238–1246.
Mazyavkina,N.,Sviridov,S.,Ivanov,S.,andBurnaev,E.(2021).Reinforcementlearningforcombinatorialoptimization:
Asurvey. Computers&OperationsResearch,134:105400.
Menda,K.,Chen,Y.-C.,Grana,J.,Bono,J.W.,Tracey,B.D.,Kochenderfer,M.J.,andWolpert,D.(2018). Deeprein-
forcementlearningforevent-drivenmulti-agentdecisionprocesses. IEEETransactionsonIntelligentTransportation
Systems,20(4):1259–1268.
Mohanty, S. P., Nygren, E., Laurent, F., Schneider, M., Scheller, C. V., Bhattacharya, N., Watson, J. D., Egli, A.,
Eichenberger, C., Baumberger, C., Vienken, G., Sturm, I., Sartoretti, G., and Spigler, G. (2020). Flatland-rl :
Multi-agentreinforcementlearningontrains. ArXiv,abs/2012.05893.
Nazari,M.,Oroojlooy,A.,Snyder,L.V.,andTakácˇ,M.(2018). DeepReinforcementLearningforSolvingtheVehicle
RoutingProblem. InProceedingsNeuralInformationProcessingSystems(NIPS),pages9839–9849.
Pan,W.andLiu,S.Q.(2023). Deepreinforcementlearningforthedynamicanduncertainvehicleroutingproblem.
AppliedIntelligence,53(1):405–422.
Paszke,A.,Gross,S.,Massa,F.,Lerer,A.,Bradbury,J.,Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,N.,Antiga,L.,
Desmaison,A.,Köpf,A.,Yang,E.,DeVito,Z.,Raison,M.,Tejani,A.,Chilamkurthy,S.,Steiner,B.,Fang,L.,Bai,
J.,andChintala,S.(2019). PyTorch: animperativestyle,high-performancedeeplearninglibrary. CurranAssociates
Inc.,RedHook,NY,USA.
11Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
Raffin,A.,Hill,A.,Gleave,A.,Kanervisto,A.,Ernestus,M.,andDormann,N.(2021). Stable-baselines3: Reliable
reinforcementlearningimplementations. JournalofMachineLearningResearch,22(268):1–8.
Shi,R.andNiu,L.(2023). Abriefsurveyonlearningbasedmethodsforvehicleroutingproblems. ProcediaComputer
Science,221:773–780. TenthInternationalConferenceonInformationTechnologyandQuantitativeManagement
(ITQM2023).
Shoham,Y.andLeyton-Brown,K.(2008). Multiagentsystems: Algorithmic,game-theoretic,andlogicalfoundations.
CambridgeUniversityPress.
Solomon,M.M.(1987). Algorithmsforthevehicleroutingandschedulingproblemswithtimewindowconstraints.
OperationsResearch,35(2):254–265.
Terry,J.,Black,B.,Grammel,N.,Jayakumar,M.,Hari,A.,Sullivan,R.,Santos,L.S.,Dieffendahl,C.,Horsch,C.,
Perez-Vicente,R.,Williams,N.,Lokesh,Y.,andRavi,P.(2021). Pettingzoo: Gymformulti-agentreinforcement
learning. InRanzato,M.,Beygelzimer,A.,Dauphin,Y.,Liang,P.,andVaughan,J.W.,editors,AdvancesinNeural
InformationProcessingSystems,volume34,pages15032–15043.CurranAssociates,Inc.
Thyssens,D.,Dernedde,T.,Falkner,J.K.,andSchmidt-Thieme,L.(2023). Routingarena: Abenchmarksuitefor
neuralroutingsolvers. arXivpreprintarXiv:2310.04140.
Vansteenwegen, P. and Gunawan, A. (2019). Orienteering Problems, Models and Algorithms for Vehicle Routing
ProblemswithProfits. Springer,euroadvanedition.
Vaswani,A.(2017). Attentionisallyouneed. AdvancesinNeuralInformationProcessingSystems.
Vinyals, O., Fortunato, M., and Jaitly, N. (2015). Pointer networks. In Cortes, C., Lawrence, N. D., Lee, D. D.,
Sugiyama,M.,andGarnett,R.,editors,AdvancesinNeuralInformationProcessingSystems28,pages2692–2700.
CurranAssociates,Inc.
Wan,C.P.,Li,T.,andWang,J.M.(2023). Rlor: Aflexibleframeworkofdeepreinforcementlearningforoperation
research.
Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning.
MachineLearning,8(3):229–256.
Wouda,N.A.,Lan,L.,andKool,W.(2024). PyVRP:ahigh-performanceVRPsolverpackage. INFORMSJournalon
Computing.
Wu,X.,Wang,D.,Wen,L.,Xiao,Y.,Wu,C.,Wu,Y.,Yu,C.,Maskell,D.L.,andZhou,Y.(2024). Neuralcombinatorial
optimizationalgorithmsforsolvingvehicleroutingproblems: Acomprehensivesurveywithperspectives. arXiv
preprintarXiv:2406.00415.
Xiang,C.,Wu,Z.,Tu,J.,andHuang,J.(2024). Centralizeddeepreinforcementlearningmethodfordynamicmulti-
vehiclepickupanddeliveryproblemwithcrowdshippers. IEEETransactionsonIntelligentTransportationSystems,
25(8):9253–9267.
Zhang,K.,He,F.,Zhang,Z.,Lin,X.,andLi,M.(2020). Multi-vehicleroutingproblemswithsofttimewindows: A
multi-agentreinforcementlearningapproach. TransportationResearchPartC:EmergingTechnologies,121:102861.
Zhang, Y., Bliek, L., daCosta, P., Afshar, R. R., Reijnen, R., Catshoek, T., Vos, D., Verwer, S., Schmitt-Ulms, F.,
Hottung,A.,etal.(2023a). Thefirstai4tspcompetition: Learningtosolvestochasticroutingproblems. Artificial
Intelligence,319:103918.
Zhang,Z.,Qi,G.,andGuan,W.(2023b). Coordinatedmulti-agenthierarchicaldeepreinforcementlearningtosolve
multi-tripvehicleroutingproblemswithsofttimewindows. IETIntelligentTransportSystems,17(10):2034–2051.
Zhou,G.,Li,X.,Li,D.,andBian,J.(2024a).Learning-basedoptimizationalgorithmsforroutingproblems:Bibliometric
analysisandliteraturereview. IEEETransactionsonIntelligentTransportationSystems.
Zhou,J.,Cao,Z.,Wu,Y.,Song,W.,Ma,Y.,Zhang,J.,andXu,C.(2024b). Mvmoe: Multi-taskvehicleroutingsolver
withmixture-of-experts. arXivpreprintarXiv:2405.01029.
Zong, Z., Zheng, M., Li, Y., andJin, D.(2022). Mapdp: Cooperativemulti-agentreinforcementlearningtosolve
pickupanddeliveryproblems. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages
9980–9988.
12Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
A VehicleRoutingProblemsEnvironments
Inthissection,followingtheguidelinesofAccorsietal. (Accorsietal.(2022)),wepresentaformalproblemdefinition
andobjectivedescriptionoftheimplementedenvironments.
A.1 CapacitatedVehicleRoutingProblemwithTimeWindows(CVRPTW)
Problemdefinition. OntheCRPTWwehaveafleetofvehiclestoserveasetofcustomerswithknowndemands. Each
customercanonlybeservedoncebyasinglevehicle. Theservicemustbecarriedoutwithinadefinedtimewindowand
respectingthevehicle’scapacities(seeSolomon(1987)). Concretely,aninstanceconsistsofafleetofV homogeneous
vehicles,asetofnnodes{n }n ,adepotandasetofn−1services,withtheircorrespondingcoordinatesx ∈R2,a
i i=1 i
n×nsymmetricmatrixT withtraveltimebetweeneachpairoflocations,aquantityd thatspecifiesthedemandfor
i
someresourcebyeachcustomern ,andthemaximumquantity,Q,oftheresourcethatavehiclecancarry. Inaddition,
i
eachnoden isspecifiedwithavisitingtimewindow[o ,c ]withopeningtimeandclosingtime,andservicetimes .
i i i i
Unlessotherwisestated,timewindowconstraintsareconsideredhardconstraints,i.e. avehicleisallowedtoarriveata
customerlocationbeforeitsopeningtime,butitmustwaittomakethedelivery. Ifitarrivesafteritsclosingtime,it’s
notallowedtoservetherespectivecustomer.
Observations.
• Nodesstaticfeatures: nodecoordinates,timewindow,demand,servicetime,boolisdepot;
• Nodesdynamicfeatures: timetoopen,timetoclose,arrivingtimeatnote,timetoopenafterstep,timeto
closeafterstep,timetoendtourafterstep,fractionoftimeelapsedafterstep;
• Agentfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timetodepot,
fractionoffeasiblenodes,fractionofvisitednodes
• Otheragentsfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timeto
depot,fractionoffeasiblenodes,fractionofvisitednodes,distancetoactiveagent,timedifferencetoactive
agent,boolwaslastactiveagent;
• Globalfeatures: fractionofserveddemands,currentfractionoffleetcapacity,fractionofdoneagents;
Reward.
• Dense: Ateverystep,therewardisthenegativedistancetraveledbytheagent. Attheendoftheepisode,a
penaltyisgivenequaling10timesthenegativedistancefromthedepottothenotattendedservices.
• Sparse: Therewardis0inallstepsexceptthelast. Attheendoftheepisode,therewardisthenegativeofthe
sumofthedistancesoftheroutestraveledbyallagentsminusthesumofthepenaltiesforeachservicenot
performed. Thepenaltyforanot-performedserviceis10timesthedistancefromthedepottothatservice.
A.2 CapacitatedVehicleRoutingProblemwithSoftTimeWindows(CVRPSTW)
Problemdefinition. InthisvariationoftheCVRPTW,timewindowconstraintsarerelaxedandcanbeviolatedata
penaltycost(usuallylinear,proportionaltotheintervalbetweenopening/closingtimesandvehiclearrival. Althoughthe
penaltyfunctioncanbedefinedinseveralways,weconsidertheformulationstudiedinFigliozzi(2010)). Concretely,
thetimewindowviolationcannotexceedP ,andconsequently,foreachcustomer,wecanenlargeitstimewindowto
max
[o −P ,c +P ]=[os,cs]outsidewhichtheservicecannotbeperformed. Whenavehiclearrivesatacustomer
i max i max i i
attimet ∈ [os,cs],itcanhaveanearlyarrivalpenaltycostofp max(o −t ,0)andalatearrivalpenaltycostof
i i i e i i
p max(t −c ,0).
l i i
Furthermore,thevehicle’smaximumwaitingtimeatanycustomer,W ,isimposed. Thatis,thevehiclescanonly
max
arriveateachcustomeraftero −P −W ,sothatitswaitingtimedoesn’texceedW .
i max max max
Observations.
• Nodesstaticfeatures: nodecoordinates,timewindow,demand,servicetime,boolisdepot;
• Nodesdynamicfeatures: timetoopen,timetoclose,arrivingtimeatnote,timetoopenafterstep,timeto
closeafterstep,timetoendtourafterstep,fractionoftimeelapsedafterstep;
• Agentfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timetodepot,
fractionoffeasiblenodes,fractionofvisitednodes
13Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
• Otheragentsfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timeto
depot,fractionoffeasiblenodes,fractionofvisitednodes,distancetoactiveagent,timedifferencetoactive
agent,boolwaslastactiveagent;
• Globalfeatures: fractionofserveddemands,currentfractionoffleetcapacity,fractionofdoneagents;
Reward.
• Dense: Ateverystep,therewardisthenegativedistancetraveledbytheagentplusthepenaltyforearlyand
latetimewindowarrival. Attheendoftheepisode,apenaltyisgivenequaling10timesthenegativedistance
fromthedepottothenotattendedservices.
• Sparse: Therewardis0inallstepsexceptthelast. Attheendoftheepisode,therewardisthenegativeofthe
sumofthedistancesoftheroutestraveledbyallagentsminusthesumofthepenaltiesforeachservicenot
performedandtimewindowviolation. Thepenaltyforanot-performedserviceis10timesthedistancefrom
thedepottothatservice.
A.3 TeamOrienteeringProblemwithTimeWindows(TOPTW)
Problemdefinition. InaTOPTWinstance,asetofnnodes,{n }n withtheircorrespondingcoordinatesx ∈R2
i i=1 i
andan×nsymmetricmatrixT withtraveltimebetweeneachpairoflocations,aregiven. Everynoden hasapositive
i
scoreorprofit,p ,avisitingtimewindow[o ,c ]withopeningtime(theearliestavisitcanstart)andclosingtime(the
i i i
latesttimeforwhichavisitcanstart),anddurationofvisitd . Withoutlossofgenerality,wecanassumethatn is
i 1
thestartingandendinglocationforeveryroute. Theobjectiveistofindmrouteswiththemaximumpossiblesumof
scores,withoutrepeatingvisits,startingeachrouteonorafteragiventimet andendingbeforetimet (see
start end
VansteenwegenandGunawan(2019)).
Observations.
• Nodesstaticfeatures: nodecoordinates,timewindow,profit,servicetime,boolisdepot;
• Nodesdynamicfeatures: timetoopen,timetoclose,arrivingtimeatnote,timetoopenafterstep,timeto
closeafterstep,timetoendtourafterstep,fractionoftimeelapsedafterstep;
• Agentfeatures: presentlocationcoordinates,fractionoftimeelapsed,timetodepot;
• Other agents features: present location coordinates, fraction of time elapsed, time to depot, fraction of
feasiblenodes,fractionofvisitednodes,distancetoactiveagent,timedifferencetoactiveagent,boolwaslast
activeagent;
• Globalfeatures: fractionofprofitscollected,fractionofdoneagents;
Reward.
• Dense: Ateverystep,therewardistheprofitcollectedbytheagent.
• Sparse: Therewardis0inallstepsexceptthelast. Attheendoftheepisode,therewardisthesumofall
agent’scollectedprofits.
A.4 PickupandDeliveryProblemwithTimeWindows(PDPTW)
Problem definition. In the pickup and delivery problem with time windows (see Dumas et al. (1991)), a fleet of
vehicleswithuniformcapacityhastocollectanddeliveritemstosatisfypairsofcustomers,respectingtheirvisiting
timewindow. Concretely,aninstanceconsistsofafleetofV homogeneousvehicleswithmaximumcapacityQ. Aset
of2n+1nodes{n }2n+1,adepotandaset{1,...,n}ofpickupservicesandtheircorresponding{n+1,...,2n}
i i=1
deliveryservices,withcoordinatesx ∈R2,a2n+1×2n+1symmetricmatrixT withtraveltimebetweeneachpair
i
oflocations. Eachcustomerwithapick-upservicehasaquantityd tobedeliveredtoaparticularcustomern . In
i n+i
addition,eachnoden isspecifiedwithavisitingtimewindow[o ,c ]withopeningtimeandclosingtime,andservice
i i i
times .
i
Observations.
• Nodesstaticfeatures: nodecoordinates,timewindow,demand,servicetime,boolisdepot,boolispickup,
boolisdelivery;
14Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
• Nodesdynamicfeatures: timetoopen,timetoclose,arrivingtimeatnote,timetoopenafterstep,timeto
closeafterstep,timetoendtourafterstep,fractionoftimeelapsedafterstep;
• Agentfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timetodepot,
fractionoffeasiblenodes,fractionofvisitednodes
• Otheragentsfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timeto
depot,fractionoffeasiblenodes,fractionofvisitednodes,distancetoactiveagent,timedifferencetoactive
agent,boolwaslastactiveagent;
• Globalfeatures: fractionofserveddemands,currentfractionoffleetcapacity,fractionofdoneagents;
Reward.
• Dense: Ateverystep,therewardisthenegativedistancetraveledbytheagent. Attheendoftheepisode,a
penaltyisgivenequaling10timesthenegativedistancefromthedepottothenotattendedservices.
• Sparse: Therewardis0inallstepsexceptthelast. Attheendoftheepisode,therewardisthenegativeofthe
sumofthedistancesoftheroutestraveledbyallagentsminusthesumofthepenaltiesforeachservicenot
performed. Thepenaltyforanot-performedserviceis10timesthedistancefromthedepottothatservice.
A.5 SplitDeliveryVehicleRoutingProblemwithTimeWindows(SDVRPTW)
Problemdefinition. TheSplitDeliveryVehicleRoutingProblemwithTimeWindows(SDVRPTW)isageneralization
oftheCVRPTWwhereeachcustomercanbevisitedmorethanoncebyseveralvehiclesandafractionofthedemand
canbemet(seeBianchessietal.(2019)).
Observations.
• Nodesstaticfeatures: nodecoordinates,timewindow,demand,servicetime,boolisdepot;
• Nodesdynamicfeatures: timetoopen,timetoclose,arrivingtimeatnote,timetoopenafterstep,timeto
closeafterstep,timetoendtourafterstep,fractionoftimeelapsedafterstep;
• Agentfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timetodepot,
fractionoffeasiblenodes,fractionofvisitednodes
• Otheragentsfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timeto
depot,fractionoffeasiblenodes,fractionofvisitednodes,distancetoactiveagent,timedifferencetoactive
agent,boolwaslastactiveagent;
• Globalfeatures: fractionofserveddemands,currentfractionoffleetcapacity,fractionofdoneagents;
Reward.
• Dense: Ateverystep,therewardisthenegativedistancetraveledbytheagent. Attheendoftheepisode,a
penaltyisgivenequaling10timesthenegativedistancefromthedepottothenotattendedservices.
• Sparse: Therewardis0inallstepsexceptthelast. Attheendoftheepisode,therewardisthenegativeofthe
sumofthedistancesoftheroutestraveledbyallagentsminusthesumofthepenaltiesforeachservicenot
performed. Thepenaltyforanot-performedserviceis10timesthedistancefromthedepottothatservice.
A.6 Prize-CollectingVehicleRoutingProblemwithTimeWindows(PCVRPTW)
Problemdefinition. ThisversionoftheCVRPTWdoesnotrequirevisitingallclients. Instead,eachclienthasaprize
(profit)thatiscollecteduponvisitation. Theobjectiveistominimizethetotalcombinedtraveledtimeofallvehicle
routeswhilemaximizingthecollectedprizes.
Observations.
• Nodesstaticfeatures: nodecoordinates,timewindow,demand,prizes,servicetime,boolisdepot;
• Nodesdynamicfeatures: timetoopen,timetoclose,arrivingtimeatnote,timetoopenafterstep,timeto
closeafterstep,timetoendtourafterstep,fractionoftimeelapsedafterstep;
• Agentfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timetodepot,
fractionoffeasiblenodes,fractionofvisitednodes
15Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
• Otheragentsfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timeto
depot,fractionoffeasiblenodes,fractionofvisitednodes,distancetoactiveagent,timedifferencetoactive
agent,boolwaslastactiveagent;
• Globalfeatures: fractionofserveddemands,fractionofcollectedprizes,currentfractionoffleetcapacity,
fractionofdoneagents;
Reward.
• Dense: Ateverystep,therewardisthenegativedistancetraveledplustheclientprizecollectedbytheagent.
• Sparse: Therewardis0inallstepsexceptthelast. Attheendoftheepisode,therewardisthenegativeofthe
sumofthedistancesoftheroutestraveledplusthesumofallagentscollectedprizes.
A.7 Multi-depotVehicleRoutingProblemwithTimeWindows(MDVRPTW)
TheMulti-depotVehicleRoutingProblemwithTimeWindows(MDVRPTW)isageneralizationoftheCVRPTWwith
multipledepots. Eachdepothasitsownsetofvehiclesthatdepartandmustreturntoit.
Observations.
• Nodesstaticfeatures: nodecoordinates,timewindow,demand,servicetime,boolisdepot;
• Nodesdynamicfeatures: timetoopen,timetoclose,arrivingtimeatnote,timetoopenafterstep,timeto
closeafterstep,timetoendtourafterstep,fractionoftimeelapsedafterstep;
• Agentfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timetodepot,
fractionoffeasiblenodes,fractionofvisitednodes
• Otheragentsfeatures: presentlocationcoordinates,fractionoftimeelapsed,fractionofcurrentload,timeto
depot,fractionoffeasiblenodes,fractionofvisitednodes,distancetoactiveagent,timedifferencetoactive
agent,boolwaslastactiveagent;
• Globalfeatures: fractionofserveddemands,currentfractionoffleetcapacity,fractionofdoneagents;
Reward.
• Dense: Ateverystep,therewardisthenegativedistancetraveledbytheagent. Attheendoftheepisode,a
penaltyisgivenequaling10timesthenegativedistancefromthedepottothenotattendedservices.
• Sparse: Therewardis0inallstepsexceptthelast. Attheendoftheepisode,therewardisthenegativeofthe
sumofthedistancesoftheroutestraveledbyallagentsminusthesumofthepenaltiesforeachservicenot
performed. Thepenaltyforanot-performedserviceis10timesthedistancefromthedepottothatservice.
B Multi-AgentMulti-DynamicAttentionModel(MADyAM)
Designedtobeappliedtomulti-agentproblems,theMARDAMmodel,(Bonoetal.(2020)),isanadaptationofthe
Attention model of Kool et al. (2019). In order to leverage the MARDAMmodel to better address problems with
dynamictime-dependentconstraints,weredesignittoincludethechangesonthemulti-headself-attentionglimpse
layerproposedinKooletal.(2019);Bertoetal.(2023),seefigureB.1.
AsillustratedinfigureB.1,MADyAMarchitectureisabletoprocessthefivetypesofpossibleagentenvironment
observations, i.e. nodes_static (os), nodes_dynamic (od), agent (oagent), other_agents (ooa) and global
(oglobal).
EquivalentlytoKooletal.(2019);Bonoetal.(2020),ithasanencodingblock(I),madeupofN layersofatransformer
encoder(Vaswani(2017))(inourcaseN =3). Thisencodingmapsthestaticnodesobservationsosintocachedkeys
i
andvaluesvectors. Subsequently,duringthedecodingstep,thesecachedkeysandvaluevectorswillbeupdated,(II),
withtheprojectionsintotheembeddingspaceofdynamicnodeobservationsod:
i
V =Vs+Vd, K =Ks +Kd, K =Ks +Kd,
g g g g g g p p p
Thecontextvector,hc,istheembeddingofglobalandactingagentobservations. Thisvectorisusedtocomputea
glimpse(III)intonodes(gnodes)andotheragents(gagents)features. Thesumofthesevectors,(gnodes+gagents)
isthenfedintotheattentionpointerhead(IV)toobtainthenextactionprobabilities. Formoredetailsaboutlayers
definitionsandtheAttentionandMARDAMmodels,seeKooletal.(2019);Bonoetal.(2020).
16Multi-AgentEnvironmentsforVehicleRoutingProblems APREPRINT
FigureB.1: IllustrationofMADyAMarchitecture. (I)Encodingblockconsistingofntransformerencoderlayers;(II)
Embeddinglayersresponsiblefortheprojectionoftheobservationintotheembeddingspace;(III)Attentionglimpse
layer;(IV)Pointerlayeroutputtingactionsprobabilities.
17