Somesite I Used To Crawl: Awareness, Agency and
Efficacy in Protecting Content Creators From AI
Crawlers
Enze Liu∗ Elisa Luo∗ Shawn Shan
e7liu@ucsd.edu e4luo@ucsd.edu shansixiong@cs.uchicago.edu
UCSanDiego UCSanDiego UniversityofChicago
Geoffrey M. Voelker Ben Y. Zhao Stefan Savage
voelker@cs.ucsd.edu ravenben@cs.uchicago.edu savage@cs.ucsd.edu
UCSanDiego UniversityofChicago UCSanDiego
Abstract ofwebsitetrafficisduetoautomatedcrawlers,comparedto
ThesuccessofgenerativeAIreliesheavilyontrainingon only30%fromhumanusers[56].Otheranecdotalevidence
datascrapedthroughextensivecrawlingoftheInternet,a suggests that AI scrapers are effectively producing DDoS
practicethathasraisedsignificantcopyright,privacy,and attacksonsmallerwebsites[44,45].
ethical concerns. While few measures are designed to re- Whileinternetscrapingiswell-studied,thewidespread
sistaresource-richadversarydeterminedtoscrapeasite, adoption of generative AI and its intensive data scraping
crawlerscanbeimpactedbyarangeofexistingtoolssuch hassignificantlychangedthelandscape.Datacreatorsand
asrobots.txt,NoAImetatags,andactivecrawlerblocking hostingplatforms,whoweregenerallyambivalenttoward
byreverseproxies. datascrapinginthepast,arenowraisingseriousconcerns
Inthiswork,weseektounderstandtheabilityandeffi- aboutAI-relatedscraping,particularlyregardingcopyright,
cacyoftoday’snetworkingtoolstoprotectcontentcreators privacy,andethicalpractices.Indeed,theseconcernshave
againstAI-relatedcrawling.Fortargetedpopulationslike resultedinoverthirtyongoingcopyrightlawsuits[6,29,59],
humanartists,dotheyhavethetechnicalknowledgeand multipledatastrikes[17,62],andasurgeintheadoptionof
agencytoutilizecrawler-blockingtoolssuchasrobots.txt, anti-scrapingtools[38].
andcansuchtoolsbeeffective?Usinglargescalemeasure- GiventhisnewtensionbetweenAItrainingcompanies
mentsandatargeteduserstudyof182professionalartists,we seeking training data and content creators who consider
findstrongdemandfortoolslikerobots.txt,butsignificantly unauthorizedAItraininganexistentialthreattotheirliveli-
constrainedbysignificanthurdlesintechnicalawareness, hoods[57],anaturalquestionarises:Whattools,ifany,can
agencyindeployingthem,andlimitedefficacyagainstunre- contentcreatorsusetopreventtheircontentfrombeingscraped
sponsivecrawlers.Wefurthertestandevaluatenetworklevel forAItraining?Answeringthisquestionrequiresamorethor-
crawlerblockersbyreverse-proxies,andfindthatdespite oughunderstandingoftheneedsofcontentcreators;their
verylimiteddeploymenttoday,theirreliableandcompre- awarenessof,accessibilityto,andagencyoveranti-crawling
hensiveblockingofAI-crawlersmakethemthestrongest mechanisms;andultimately,theavailabilityandefficacyof
protectionforartistsmovingforward. currenttools.
This paper presents our efforts to address these issues
from several complementary fronts. In terms of represen-
1 Introduction
tative content creators, we focus on visual artists as the
ThesuccessofgenerativeAIreliesheavilyontrainingon
mostvulnerablepopulationbeingtargetedbyAIcrawlers.
datascrapedthroughextensivecrawlingoftheInternet,a
Intermsofanti-crawlingmechanisms,wefocusontwotools
practicethathasraisedsignificantcopyright,privacy,and
atdifferentendsofthespectrum.Themostprominentand
ethicalconcerns.Today,AImodeltrainershaveunleashed
populartoolisrobots.txt,avoluntary(andnon-enforceable)
large numbers of data scrapers on the Internet. By many
protocolthatenablessiteownerstospecifycrawlingrestric-
reports, these scrapers now dwarf the volume of human
tions.Wealsoconsidercrawlerblockingbyreverseproxies
trafficontheInternet,partlybecausehumanusersconsume
(e.g.Cloudflare),anactiveapproachthatenforcesblocking
content at a much lower rate than scrapers. For example,
buthasseenlimiteddeployment.
analysisbyAkamaisuggeststhatonaverage,roughly70%
We begin with a large-scale measurement of robots.txt
acrosstheWeb,andinclusionofdirectivesthatspecifically
*Thesetwoauthorscontributedequallytothiswork.
4202
voN
22
]CH.sc[
1v19051.1142:viXraLiu,Luo,Shan,Voelker,Zhao,andSavage
targetAI-crawlersovertime.Thiseffortservesasbroader 2 BackgroundandRelatedWork
contextonhowthearrivalofAI-crawlershaschangedviews In this section, we start by providing a brief overview of
across the Web towards web crawlers. We then turn our AI-relatedcrawlinginSection2.1.Wethendiscussexisting
attentiontovisualartists,andperformauserstudytounder- mechanismstostopcrawlinginSection2.2.
standtheirattitudestowardsAI-crawlers,andtheiraware-
ness of and accessibility to defensive tools like robots.txt.
2.1 DataScrapingofCommercialAI
Wecomplementtheseresultswithmeasurementsof1100+
professionalartistwebsitestounderstandhowdistribution Crawlers are automated programs that visit websites and
of hosting services impact artists’ control over robots.txt. download their content. In the era of AI, companies use
Next,weconductourownexperimentstodeterminewhich crawlersforavarietyofpurposes.Thusfar,thereexistthree
AI crawlers respect robots.txt. Finally, we consider active maintypesofAI-relatedcrawlers:(1)crawlersforcollect-
crawlerblockingtechniques,andperformexperimentsto ingtrainingdata(e.g.,AppleBot-Extended),(2)crawlersfor
measuretheirdeploymentaswellastheireffectivecoverage augmentingAI-backedassistants(e.g.,ChatGPT-User),and
acrossdifferentAIcrawlers. (3)crawlersforfacilitatingAI-backedsearchengines(e.g.,
Resultsfromourstudyhighlightcriticalhurdlesthatlimit OpenAI-SearchBot).
or prevent the effective utilization of protective tools by Crawlingpractices. WeidentifythreecategoriesofAI-
individualcreators,leavingthesekeystakeholdersinthedata relatedcrawling.TopAIcompaniesstarttousecrawlerswith
ecosystemvulnerableandoftenunabletosafeguardtheir differentuseragentstodeclaredifferentcrawlingintent.
work from unauthorized AI-driven use. More specifically, Crawlersforcollectingtrainingdata(AIdatacrawlers).A
ouranalysisproducesanumberofinterestingfindings: majoruseofcrawlersistocollectdatafortrainingAImodels
(hencereferredtoas.Somecompanieshavedevelopedtheir
owncrawlersforsuchpurposes,andotherrelyuponthird-
• WemeasuredtheinclusionofAI-crawlersinrobots.txtof partycrawlers(e.g.,CommonCrawl).
largecontenthosts,andfoundaninitialsurgefollowedby CrawlersforaugmentingAI-backedassistants(AIassistant
aslowincrease.Wealsofoundasmallbutgrowingnumber crawlers).ThesecondtypeofcrawlerisonesthatenhanceAI-
ofwebsitesthatexplicitlyinviteAIcrawlerstocrawltheir backedassistantswithadditionalinformationbyreal-time
content. fetchingwebpages.Forinstance,ChatGPT-Userisacrawler
thatcanvisitwebsitestofetchadditionalinformationwhena
• Weconductedasurveywith182professionalartists,and
userposesaquestionbeyondChatGPTtrainingdata.Insuch
foundthatindividualartistsoftendonothavetheknowl-
cases,thecrawlerretrievesrelevantcontentfromthesiteand
edge(>60%haveneverheardaboutrobots.txt)andtech-
deliversittotheuser.Whilesomecompanies,likeOpenAI,
nology means to include AI-crawlers in their robots.txt.
statethatwebsitecontentaccessedbyAIassistantsisnot
Oncepresentedwithmoreinformation,mostartistsindi-
directlyusedfortraining,itcouldinadvertentlycontributeif
catedthattheywouldliketouserobots.txttodisallowAI
thecompanytrainsmodelsonuserinteractionlogs,asseen
crawling.Atthesametime,themajorityoftheartistsdo
withChatGPT[42].
nottrustthatAIcompanieswillrespectit.
• Testingonourownservers,wefoundmostAIcompanies CrawlersforfacilitatingAI-backedsearchengines(AIsearch
dorespectrobots.txt.However,anumberofAI-powered
crawlers).AthirdmajoruseofcrawlersistofacilitateAI-
backedsearchengines.Forexample,OpenAI-SearchBotis
appsandcrawlersdonotrespectit(includingcrawlersfrom
a crawler that indexes websites, which in turn is used by
Meta).
AI-backedsearchengines.Whilecompaniesclaimthatthe
• Wemeasuretheadoptionandoperationofactiveblocking
contentofawebsiteretrievedbyAIsearchcrawlersisnotdi-
mechanisms.Whiletheycanofferstrongerprotection,they
rectlyusedfortraining,theuserofawebsitecannotenforce
stillsufferfromseverallimitationssuchasanincomplete
norverifythisclaim.
listofAI-crawlersblocked,andinabilitytostopAItraining
forMeta,Google,andWebzio.
2.2 MechanismsagainstCrawling
Next,wediscusscurrentmechanismsthatareusedtocontrol
Altogether,ourworkhighlightstheneedforbettermech- crawling.
anismsthataccountforthediverserangeofusecases,that Robots.txt. The Robots Exclusion Protocol (RFC9309)
makemechanismsmoreaccessibletoabroaderrangeofcon- definesrobots.txt,allowingwebsiteownerstospecifywhich
tentcreators,andthatmoreclearlyconveytheimplications URLs crawlers can access. Originally designed to reduce
andlimitationsofusingthem. serverload,itisnowwidelyusedtomanagecontentaccess.SomesiteIUsedToCrawl:Awareness,AgencyandEfficacyinProtectingContentCreatorsFromAICrawlers
# An example robots.txt file beimplementeddirectlyonawebserver(e.g.,viaApacheor
User-agent: Googlebot Nginxrules)orthroughthird-partyserviceslikeCloudflare’s
Allow: / reverseproxy.
NoAI Meta Tag. First proposed by DevianArt, noAI
User-agent: ChatGPT-User
andnoImageAIaremetatagsasiteownercaninsertinto
User-agent: GPTBot
theirHTML(underthe"robots"name).Userscanindicate
Disallow: /
tocrawlersthattheywouldliketheircontentnot tobeused
forAItrainingbyaddingsuchasnippet:
User-agent: *
<meta name=''robots'' content=''noai, noimageai''>
Disallow: /secret/
totheirHTML.Previouswork[13]suggeststhattheadop-
tionofthistagislow.Weconfirmthisbycheckingthetop
10kdomainsintheTrencodataset,findingthatonly17sites
Figure1:Anexampleofarobots.txtfile.Inthisexam-
havethenoaiand16havethenoimageaitags.
ple,GooglebotisallowedtocrawlallURLsontheweb-
ai.txt. IntroducedbySpawningAI,ai.txtallowscontent
site,ChatGPT-UserandGPTBotaredisallowedfrom
ownerstospecifywhetherAIcrawlerscanusetheirdatafor
crawling any URLs, and all other crawlers are disal-
training.Unlikerobots.txt,ai.txtisreadwhenanAImodel
lowedfromcrawlingURLsunderthe/secret/direc-
attemptstodownloadmedia,enablingreal-timeupdatesto
tory.
preferences,evenforpreviouslycollecteddata.Itscreators
argueitoffersalegallyenforceablestandard,targetingthe
As an honor-based system, compliant crawlers follow its EU TDM Article 4 exception [1], though its enforcement
directives,butadherenceisnotmandatory. differencesfromrobots.txtremainunclear.
Figure1showsanexamplerobots.txtfile.Thefirsttwo
lines allow Googlebot to crawl all URLs, while the next
2.3 RelatedWork
three disallow ChatGPT-User and GPTBot from crawling
any.Thefinallinesblockallothercrawlersfromaccessing Giventhebroadscopeofourwork,wedrawfromavarietyof
the/secret/directory.Robots.txtcanalsoincludesitemaps relatedworkintheareasofwebcontentcontrolmechanisms,
(URL lists for indexing) and crawl-delay (minimum time crawlerdetectionandblocking,andtheimpactofgenerative
betweencrawlerrequests). AIoncontentcreators.
Inthispaper,wecategorizethelevelsofrestrictionim- WebContentControlMechanisms. Robots.txt,arguably
posed by robots.txt on a given crawler into four distinct themostwidelyusedwebcontentcontrolmechanism,has
groups. The first category, no robots.txt, applies to sites beenextensivelystudied.Sunetal.[55]analyzedlarge-scale
thatdonothavearobots.txtfile.Thesecond,norestrictions, deployment,identifyingerrorsandtheincreaseduseofthe
referstocaseswheretheuser-agentisfullyallowedtoac- now-deprecated“Crawl-Delay”field.StudiesbySunetal.[54]
cessthewebsiteasspecifiedbyrobots.txt.Thethirdcategory, andKolayetal.[33]revealedbiasesfavoringmajorsearch
partiallydisallowed,indicatesthattheuser-agentisper- engines.Non-technicalaspects,suchaslegalimplications
mittedtoaccesssomepathsbutnotall.Finally,completely ofviolatingrobots.txt[49]anditsuseforexpressingcopy-
disalloweddescribesinstanceswheretheuser-agentispro- right authorization [63], have also been explored. Similar
hibitedfromaccessinganypathsonthewebsite. protocols, like security.txt [47] and ads.txt [8], have been
Morerecently,companiesbuiltmanagersforRobots.txt. examinedforpurposesbeyondwebcontentcontrol.
Thesemanagerservicessimplifymaintenancebyofferingau- Morerecently,studieshaverevisitedrobots.txtinthecon-
tomatedupdatesandinterfaces.DarkVisitorssyncswithan text of generative AI. [14] surveyed web content control
AIcrawlerdatabase,whiletoolslikeYoastSEOandAIOSEO mechanisms,whileempiricalmeasurementsin[13,38]found
provideintuitivefeaturesforconfiguringruleswithouttech- asharpincreaseinrobots.txtadoptionpost-generativeAI,
nicalexpertise. withothermechanismslikethe“noai”metatagremaining
Activeblocking. Activeblockingactivelypreventscrawlers rare.[16]conductedacasestudyonnewswebsites’adoption
fromaccessingawebsitebyfirstdetectingtheirpresence.De- ofrobots.txt.Thesestudiesfocusonbroadtrends,whileour
tectionmethodsrangefromsimpleIP-oruser-agent-based workexaminestheperspectiveofindividualcreatorsand
rules to advanced techniques like browser fingerprinting. theuniquechallengestheyface.
Oncedetected,thewebsiteownercanblockthecrawlerby DetectionandBlockingofWebCrawlers. Research
returningadifferentHTTPstatuscode(e.g.,403Forbidden) onwebcrawlerdetectionandblockinghasexploredvari-
ordisplayinganalternativepage(e.g.,aCAPTCHA).Thiscan oustechniques,includingwebtrafficanalysis[22,24,39],Liu,Luo,Shan,Voelker,Zhao,andSavage
server access logs [21, 48, 51], user behavior [11, 21], pat- 3.1 DatasetandMethodology
ternmatching[34],machinelearning[25,53],andbrowser
Domainsinscope. ToestimateintenttowardAI-related
fingerprinting [3, 28, 58]. Studies have also differentiated
crawlingofwell-resourcedwebsites,weusethesitesthat
crawlerbehaviors,suchasgoodversusbadbots[37],bogus
consistentlyappearintheTrancoTop100kdomainsevery
bots[5],andhumanversusbotaccesspatterns[2,35].Web-
monthfortwoyears,fromSeptember2023throughOctober
sitesuseblockingmethodslike403errors,CAPTCHAs,or
2024.Thereare50kdomainsthatconsistentlyappear.
alteredpages[3,46].Ourworkbuildsonanalysesofwebsite
and anti-bot service behavior, including studies by Pham ListofAI-relateduseragents. Wecompileourlistof
etal.[46]onuseragents,Azadetal.[3]onanti-botservice AIuseragentsbasedonDarkVisitors,anindustryblogthat
effectiveness,andJonesetal.[27]onautomateddetection maintains an up-to-date list of AI user agents [60]. Since
ofblockpages. DarkVisitorsalsolistsothernon-AIuser-agents,weonly
considertheAI-relateduseragentsbelongingtothefollow-
ImpactofGenerativeAIonContentCreators. Athird
ingcategories:AIAssistant(AIAssistantCrawlerinthis
area of research investigates the impact of generative AI
paper),AIDataScraper(AIDataCrawlerinthispaper),
oncontentcreators.Theworkclosestooursisthosethat
AISearchCrawler,andUndocumentedAIAgents.We
focusontheimpactofgenerativeAIonartists.Forexample,
alsocross-validatedthelistwithapriorstudythatcollected
Jiangetal.[26]categorizeddifferenttypesofharmscaused
popular user-agents in robots.txt [38] and confirmed that
bygenerativeAI,suchaseconomiclossandcopyrightin-
ourlistisasupersetoftheAIuser-agentsinthispriorstudy.
fringement.Kawakamietal.[30]identifiedsimilarkindsof
Intotal,weidentified24uniqueAI-relateduseragents,listed
harmsbysummarizingonlinediscussions.Shawnetal.[50]
inTable1.Wefocusexclusivelyontheseuseragentsforthe
highlightedthespecificconcernofstylemimicry(usingAI
restofthepaperunlessotherwisenoted.
togenerateaspecificstyleofart).Ourworkcontributesto
thisstrandofresearchbyexaminingthetechnicalneedsand HistoricRobots.txtDatafromCommonCrawl. We
challengesartistsfaceinprotectingtheironlinepresence. usedtherobots.txtfilescrawledbytheCommonCrawlfrom
Also related, but orthogonal to our work, is the study 2022to2024.WefindthatthecoverageoftheCommonCrawl
oftheimpactofgenerativeAIonothercommunities,such robots.txtdataisapproximately83%.Ultimately,forsitesthat
asUserExperienceDesignProfessionals[36],EarlyCareer didnotreturna200or404HTTPstatuscodetoCommon
GameDevelopers[10],andCreativeWriters[19,23]. Crawl, we cannot determine whether the site indeed had
arobots.txtornot.1 Asummaryofourrobot.txthistorical
datasetcanbefoundinTable4intheappendix,andmore
detailsonhowwededuplicateandcleantheCommonCrawl
datacanbefoundinAppendix10.2.
WevalidatedthattheCommonCrawldataisaccurateby
3 HowWell-resourcedWebsitesReacted comparingtherobots.txtfilesretrievedbyCommonCrawl
ToprovideabroadercontextonhowthearrivalofAI-crawlers andInternetArchive.Wealsovalidatedthelatestsnapshot
haschangedviewsacrossthewebtowardscrawlers,westart oftheCommonCrawldatabyconductingourowncrawlof
by revisiting how well-resourced websites reacted. These robots.txtofthetop10kwebsites.
websitesaremorelikelytoreactswiftly,astheyhavemore Parsingandinterpretingrobots.txt. Weparsethere-
valuablecontenttoprotectandmoreresourcestodoso. trievedrobots.txtfilesusingGoogle’srobots.txtparser[18].
Inthissection,usingacorpusofstabletop100kdomains, WerelyonGoogle’sparserasrobots.txtisacomplexstan-
weinvestigatetheextenttowhichwell-resourcedwebsites dardandourexperiencesuggestedthathome-grownparsers
adoptrobots.txttorestrictAI-relatedcrawlers.Wefindthat areerror-prone.2Werandomlyselectedasetof100robots.txt
manypopularwebsitesarequicktoaddrestrictionstoAI files,andverifiedthatGoogle’sparsercorrectlyinterpreted
crawlersinrobots.txt:over10%ofthedomainshaveexplic- therobots.txtfilesforallofthem.Wealsoverifiedthatthe
itly disallowed AI-related crawlers in their robots.txt file, parsercorrectlyinterpretedavarietyofedgecasesnotcap-
whichisinadditionto1.5%ofthedomainsthatalreadyhave turedbyotherparsers,asshowninAppendix10.3.
a blanket disallow rule for all crawlers. While there have
beenmanydifferentincentivesandefforts(e.g.,therecent
EUAIAct)torestrictAIcrawlersinrobots.txt,wealsoob- 1Forinstance,ifasiteimplementedactiveblockingonautomatedrequests
serveasmallyetnoticeablereversetrend:somesitesrecently (likethoseoftheCCcrawler),thenCommonCrawlmayrecorda403
ForbiddenHTTPstatuscodeforthosesites.
removedrestrictionsonAIcrawlers,likelyduetoreasons
2Anexampleistheparserdevelopedby[38],whichweestimatetohavea
suchashavingenteredintodatalicensingagreementswith
10%errorrateinparsingrobots.txt.Wehavenotifiedtheauthorsaboutthis
AIcompanies. issue.SomesiteIUsedToCrawl:Awareness,AgencyandEfficacyinProtectingContentCreatorsFromAICrawlers
Name Category Company PublishIP RespectRobots RespectRobots
(Claim) inPractice
Amazonbot AISearch Amazon Yes Yes Yes
AI2Bot AIData Ai2 No - -
anthropic-ai UndocumentedAgent Anthropic No - -
Applebot AISearch Apple Yes Yes Yes
Applebot-Extended* AIDataScraper Apple N/A Yes -
Bytespider AIData ByteDance No - No
CCBot AIData CommonCrawl Yes Yes Yes
ChatGPT-User AIAssistant OpenAI Yes Yes Yes
Claude-Web UndocumentedAgent Anthropic No - -
ClaudeBot AIData Anthropic No Yes Yes
cohere-ai UndocumentedAgent Cohere No - -
Diffbot AIData Diffbot No - -
FacebookBot AIData Meta Yes Yes
Google-Extended* AIData Google N/A Yes -
GPTBot AIData OpenAI Yes Yes -
KangarooBot AIData KangarooLLM No Yes -
Meta-ExternalAgent AIData Meta Yes - Yes†
Meta-ExternalFetcher AIAssistant Meta Yes No -
OAI-SearchBot AISearch OpenAI Yes Yes -
omgili AIData Webz.io No Yes -
PerplexityBot AISearchCrawler Perplexity No Yes -
Timpibot AIData Timpi No - -
Webzio-Extended* AIData Webz.io N/A Yes -
YouBot AISearchCrawler You.com No -
Table 1: Summary of AI User Agents studied in this paper. We take the category from Dark Visitors [60]. We
alsonotewhethertheuseragentpublishestheirIPaddress,andwhethertheyclaimtorespectrobots.txtintheir
document,whethertheyrespectrobots.txtinpractice(Section5).Ifwecannotfindthedocumentationassociated
withauseragentorthedocumentationdoesnotmentionwhethertheyrespectrobots.txt,wemarkitas‘-’.If
wecannottestwhetherauseragentrespectsrobots(becausetheyhavenotvisitedourwebsite),wemarkitas
‘-’.*Thesethreeuseragentsarenotusedbyrealcrawlers,butinsteadarespecialforsiteownerstomanagetheir
owndata.Asaresult,wemarktheirIPaddressasnotapplicable(N/A).†ExternalAgenthasanoticeabledelayin
updatingtheirrobots.txtfile.
WebuildawrapperaroundGoogle’sparserthatallows 3.2 IncreasingDrivetoProtectData
us to determine whether a given user-agent is completely Figure2showsthetrendofrestrictionsonAI-relatedcrawlers
disallowed, partially disallowed, or has no restrictions. We overtimefortwosetsofsites:thoseconsistentlyrankedin
also separate the case where the AI-related user-agent is thetop5k,representingintentofthelargestwebsiteopera-
restricted by the wildcard (User-agent: *) rule (i.e., the tors,andallothersiteswithinthetop100k(whoseoperators
AI-relateduser-agentisnotexplicitlymentionedinthesite’s arestillrelativelywell-resourced).Whilebothcategoriesof
robots.txt).ThesearewebsitesthattechnicallydisallowAI siteshaveaninitialsurgeindisallowingAI-relatedcrawlers
crawlers,butdonotshowasstronganintentinblockingAI inrobots.txtaroundAugust2023(aroundthereleaseofOpe-
crawlersasothersitesthathaveaspecificrule.Infurther nAI’sGPTBotandChatGPT-Useruseragentwhichidentifies
analysis, we only consider the sites that block AI-related their crawlers), the most popular websites are noticeably
crawlers through an explicit rule. The code for this inter- quickertoaddrestrictionsinrobots.txt.Likelysincethey
preterwillbepubliclyavailable. havemorevaluablecontenttoprotectandmoreresources,
wefindthatalargerproportionofthemostpopularsitesLiu,Luo,Shan,Voelker,Zhao,andSavage
3.3 RecentDecreaseinRestrictions
0.14 Top 5k
Other Sites Withinthesitesthatareconsistentlyinthetop5k,wein-
0.12
terestinglynotonlyseethedrivetoaddrestrictionstoAI-
0.10 related crawlers in robots.txt level off, but also some de-
0.08 creasesinrecentmonths.Thisrecentbehaviorisincontrast
0.06 topredictionsin[38]ofstrictlyincreasingobservableintent
todisallowAI-relatedcrawling.
0.04
0.02 Public Data Licensing Deals. One reason why a site
will remove an AI-related crawler from robots.txt is that
0.00
22 22 23 23 23 23 23 24 24 24 24 24 24 24 24 thesiteownerhasenteredintoadatalicensingagreement
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
Sep-Oct Nov-Dec Jan-Feb Mar-Apr May-Jun Sep-Oct Nov-Dec Feb-Mar Apr May Jun Jul Aug Sep Oct w coi nth fira mn eA dI tc ho am tp suan chy. pA arb tl no eg rsp ho is pt sf wro em ree ia nr dly eeO dc tt ho ebe rr ea2 s0 o2 n4
Date fortheremovalofGPTBotfromtherobots.txtfileofseveral
majorpublishers’websites,includingTheAtlanticandVox
Figure 2: Proportion of sites that fully disallow any Media[32].Thesedealsofteninvolveapublisherwhocon-
AI-relateduseragent,brokendownbysiterank trolsdozensofdomains;e.g.,Newscorpownsmorethan10
newsandmediacompanieseachhavingitsowndomains.
GPTBot In our data, we find that between the release of Ope-
CCBot
0.08 Google-Extended nAI’s GPTBot (and ChatGPT-User) user agents in August
ChatGPT-User 2023 and October 2024, 484 sites removed explicit restric-
0.06 anthropic-ai tionsonGPTBotfromtheirrobots.txt(Figure4).Manyof ClaudeBot
Claude-Web these include sites owned by publishers who have struck
0.04 PerplexityBot
publicly known data licensing agreements with OpenAI,
Bytespider
omgili suchasDotdashMeredith[40](e.g.,Investopedia.com,Peo-
0.02
ple.com,AllRecipes.com),StackExchange[43](e.g.,supe-
0.00 ruser.com, stackoverflow.com), and Conde Nast [31] (e.g.,
2 2 3 3 3 3 3 4 4 4 4 4 4 4 4
02 02 02 02 02 02 02 02 02 02 02 02 02 02 02 newyorker.com,vanityfair.com,wired.com).Someofthese
2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
Sep/Oct Nov/Dec Jan/Feb Mar/Apr May/Jun Sep/Oct Nov/Dec Feb/Mar Apr May Jun Jul Aug Sep Oct d toat ta hu eis rag se itea sgr wee hm enen Cts hr ae tq Gu Pi Tre gO ep ne en raA tI et so cp ol na tc ee nd tir be ac st el din ok ns
Date theirdata,drivingmoretraffictotheirwebsite.Thefulllist
ofsuchwebsitesisinTable5intheappendix.
Figure3:Proportionoftop100ksitesthatexplicitly
imposerestrictionsonAI-relatedcrawlersinrobots.txt Possibleprivatedeals. InthecaseofmajorAmericanpub-
lishersFuturePLC,over10oftheirsites(includingtechrada-
overtime.TheverticallineindicateswhentheEUAI
r.com,tomguide.com,andcyclingnews.com)removedrestric-
Actwasreleased.
tionsonGPTBotinMay2024,whiletherestoftherobots.txt
haverestrictionsonanyAI-relatedcrawlerswhencompared fileremainedidentical.However,inanAugust2024podcast,
totherestofthetop100ksites(8-10%). theCEOofFuturestatedthattheydidnothaveanypartner-
Whenwebreakthehistoricaltrenddownbyuseragent shipwithOpenAI[7].Afewothersmallerpublishersand
(Figure3),wecanobserveasecondarydistinctuptickofre- newssitesalsoremovedrestrictionsonGPTBot,indicating
strictionsforalluseragentsaroundAugustof2024.Thiscor- possibledeals.
respondstothereleaseoftheEUArtificialIntelligenceAct,
whichaimstoimposelegalregulationsongeneral-purpose
AI,andimploressignatoriestoensuretheyhavelawfulac-
3.4 IncreasesinExplicitlyAllowingAI
cesstocopyright-protectedcontent.InthedraftoftheCode
ofPracticefortheAIAct,theregulatorsexplicitlyrequire Crawlers
signatoriestoreadandfollowinstructionsinrobots.txt(Sub- To our surprise, we found a growing number of sites ex-
Measure4.1)[15].Themostfrequentlyrestricteduser-agents plicitlyallowingAI-relatedcrawlersintheirrobots.txt,wel-
areGPTBot(OpenAI)andCCBot(CommonCrawl).While comingAIcrawlerstoscrapetheirwebsites.Whileasmall
CommonCrawlmerelycollectsthedata(anddoesnotuseit numberofsitesfallintothisuniquecategory,interestingly,
foranyAI-relatedpurposeitself),CommonCrawlisavery theoverallnumberofsitesthatexplicitlyallow AI-related
frequentdatasourceforAItraining. crawlersisincreasingovertime(Figure4).
setis
fo
noitroporP
setis
fo
noitroporPSomesiteIUsedToCrawl:Awareness,AgencyandEfficacyinProtectingContentCreatorsFromAICrawlers
howtouserobots.txt),andagency(unabletoeditrobots.txt
fully allowed
300 removed restrictions becausetheirwebsitesarehostedwithbyathird-party)to
Condé Nast deal utilizeexistingtechnicalapproacheslikerobots.txt.
250
Stack Exchange, Dotdash Meredith deal Informedbyouruserstudy,wefollowupwithameasure-
Vox Media deal
200
mentstudyofover1100artistwebsitestofurtherexamine
150 thehostingservicesartistsuseandlevelsofcontrolthese
100 services provide. We find that the majority of artists use
third-partyhostingservicesthatdonotallowformodifica-
50
tionofrobots.txt.Amongthefewthatdo,wefindthatartists
0
donotexercisetheircontrol,withlessthan17%ofartists
2 2 3 3 3 3 3 4 4 4 4 4 4 4 4
2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 disallowingAI-relatedcrawlersintheirrobots.txt.
Sep-Oct Nov-Dec Jan-Feb Mar-Apr May-Jun Sep-Oct Nov-Dec Feb-Mar Apr May Jun Jul Aug Sep Oct
4.1 UserStudySetup
Date
Weconductedauserstudy(approvedbyouruniversity’sin-
Figure 4: Number of sites that explicitly allow AI- stitutionalreviewboard)withprofessionalartists.Wedraw
relatedcrawlersinrobots.txt,andnumberofsitesthat participantsfromprofessionalartistsinformedviatheirso-
removed restrictions on AI-related crawlers in each cialcirclesandprofessionalnetworks.Intotal,weobtained
timeperiod.Thehorizontallinesindicatepublicdata 182responsesfromartistswhosharetheirartworkonline.
dealsbetweenmajorpublishers(whocontrol40+do- Westartbygatheringthebasicinformationofeachpar-
mains)andOpenAI. ticipant.Then,weaskthemabouttheirperceptionsofAI-
generated art, concerns regarding its impact on their job
security,andactionstakeninresponsetoAI-generatedart.
Intotal,wefound79sitesthatnotonlyhadnorestrictions
Wethenexaminetheirknowledgeanduseofrobots.txt,as
onGPTBotinrobots.txt,butalsoexplicitlymentionedthem
wellastheirwillingnesstoadoptrobots.txtinthefuture.We
tobeallowed.Partofthisincrease(especiallyinmid-2024)
compensateparticipantsatarateof$15/hour.
isaccountedforbytheaforementioneddatalicensingagree-
mentsbetweenOpenAIandpublishers,buttherearealso
otherreasons. 4.2 SentimentTowardsAI-relatedCrawling
Amongstthesesites,wefoundpopularright-wingmis- WefindastrongsentimentagainstAI-relatedcrawlingand
information sites, which may be motivated by spreading desireforeffectivetoolstostopit.
misinformationtoLLMs.Inothercases,wefindshopping
ArtistsareworriedandhavetakenactionsagainstAI.
sitesthatpotentiallyseektouseLLMstoincreasetrafficto Over79%(144,n=182)ofartistsexpressconcernsaboutthe
theirsite.SeeAppendix10.4forthefulllistofsiteswherewe impactofAI-generatedartontheirjobsecurity,withmore
observethisreverseintenttowardGPTBot.Thiscasestudy than54%anticipatingthatAIartwillhaveasignificantor
highlightsthattherearealsocaseswheresiteshavedifferent severeeffectontheircareers.Anotablemajority(79%)have
motivesforallowingAIcompaniestotrainontheirdata. reported taking proactive measures to address these con-
cerns.Amongtheseartists,67%useGlaze[50],atoolthat
4 IndividualArtists’Sentimentsand employsadversarialmachinelearningtoprotectartwork,
Actions while60%havereducedthevolumeofworksharedonline,
PreviouslyinSection3,weshowthatmanywell-resourced and51%nowshareonlylower-resolutionimagestomitigate
websiteshaveswiftlyadoptedrobots.txttoprotecttheircon- potentialmisuse.
tent. In this section, we explore the question of what in- Artists would like to stop AI crawling. When intro-
dividualartiststhinkaboutAI-relatedcrawlingandwhat ducedtotheconceptofapotentialtoolforpreventingcrawlers
actions they have taken, if any. Compared to large orga- fromcrawlingtheirsites,over96%(175,n=182)oftheartists
nizations, individual artists are much more impacted yet expressedadesiretousesuchatool.Asignificantmajority
under-resourced. (91%)indicatedthattheywere“verylikely”toadoptit.The
We first present a user study, comprised of 182 profes- mostcommonlycitedreasonsincludedprotectingtheirin-
sionalartists,tounderstandtheirsentiments,actions,and tellectualpropertyfrom“theft”or“stealing,”objectingtothe
challengestheyfacewhendealingwithAI-relatedwebcrawl- unauthorizeduseoftheirwork,andnotbeingcompensated
ing.WhilemanyartistsareextremelyconcernedaboutAI, fortheircreations.Interestingly,threeartistsnotedthatsuch
theylacktheawareness(over60%ofartistshaveneverheard toolscouldprovidelegalgroundstochallengeunauthorized
abouttheterm“robots.txt”),technicalability(notknowing crawlingoftheircontent.
setiS
fo
rebmuNLiu,Luo,Shan,Voelker,Zhao,andSavage
We observe similar results when we asked artists who CMS %Sites Edit? %DisallowAI
hadnotheardofrobots.txtabouttheirwillingnesstoadopt Squarespace 20.7 No𝐴𝐼,𝑆𝐸 17
robots.txtinthefuture.Almostalloftheartistshavearea- Artstation 20.4 No 0
Wix(Paid) 9.3 Yes 0
sonableunderstandingafterreadingabriefexplanationof
robots.txt.3Amongtheseartists,73%(84,n=115)indicated AdobePortf. 4.8 No𝑆𝐸 0
Wix(Free) 3.5 No 0
thattheywouldlikelyorverylikelyadoptrobots.txtinthe
Weebly 3.1 No𝑆𝐸 0
future.Forthosewhoindicatedneutralorunlikely,themost
Shopify 1.7 No 0
commonreasonscitedwereconcernsregardingitsefficacy
Carbonmade 1.5 No 100
(thatrobots.txtdoesnotfullystopcrawling)andeaseofuse.
Table 2: The top 8 web hosting providers artists use
Artists do not trust AI-related crawlers will respect
andtheiroptionsformodifyingrobots.txt.𝐴𝐼:option
robots.txt. WhenaskedabouttheirtrustinAIcompanies,
availabletodisallowAIcrawlers;𝑆𝐸:optionavailable
only25%ofparticipantsbelievedAIcompanieswerelikelyor
todisallowsearchenginecrawlers.
verylikelytorespectrobots.txt.Artistscitedseveralreasons
forthisdistrust,includingthepotentialforloopholes,the
lackoflegalenforcement,andthepoortrackrecordofAI
companies.Oneparticipantremarked,“[AIcompanies]feel file,astheyusewebhostingplatformsorpostonwebsites
theyhavearighttoeverythingforfree,andifthingslike thatdonotallowformodificationofrobots.txt.
copyrightdon’tstopthem,whywouldapolitenoticeona
website?”.Infact,theseworriesarenotunfounded:aswe
4.4 MeasurementofArtists’Websites
showlaterinSection5,atleastoneAIcrawler(Bytespider)
Guidedbythefindingsfromouruserstudy,weperformed
doesnotrespectrobots.txt.
ameasurementstudyonover1,100artistwebsitestobet-
Despiteastronglevelofdistrust,themajorityofartistsre-
terunderstandtheservicesusedbyartistsandthelevelof
maininterestedinadopting,orhavealreadyadopted,robots.txt
controltheseservicesprovide.Wefindthatthemajorityof
(89%).Thisresultdemonstratesawillingnessamongartiststo
artistsusethird-partyhostingservicesthatdonotallowfor
exploremeasurestheyperceiveasimperfect,perhapsview-
modificationofrobots.txt.Amongthefewthatdo,wefind
ingthemasnecessarystepstowardprotectingtheirwork
thatartistsdonotexercisetheircontrol,withlessthan17%
evenifnotcompletelyeffective.
ofartistsutilizingtheoptiontodisallowAI-relatedcrawlers
intheirrobots.txt.
4.3 ChallengesinAdoptingTechnical
Below,westartbydescribingourmethodforcollecting
Measures artists’websitesandidentifyingtheservicestheyuse.We
Weidentifythreemainchallengesforartiststoutilizetech- thenpresenttheresults.
nicalmeasuressuchasrobots.txt:lackofawareness,ability, ArtistWebsitesandTheirServiceProvider. Wecol-
andagency. lectedartist’spersonalwebsitesusingdirectoriesoftwotop
Themostsignificantchallengeisthelackofawareness artistassociationsintheU.S.(ConceptArtAssociationand
among artists: over 60% of the artists (115, n = 182) have AnimationUnion).Bothorganizationspublishedtheirmem-
neverheardaboutrobots.txtpriortoourstudy.Among berlistsalongwitheachartist’spersonalwebsite.Intotal,
the40%whohadheardofrobots.txt,themajority(60outof we collected 1182 artists’ personal websites. We find that
64)demonstratedareasonableunderstandingofitspurpose, themajorityofartists(over78%)useoneofeightcontent
describingitasawayof“blocking”or“stopping”crawlers. managementsystems(CMS),suchasSquarespaceandArt-
Anothermajorchallengeisthelackoftechnicalabilityto Station,tohosttheirwebsites,followedbyalongtailofsmall
utilizerobots.txt.Amongthe33artistswhomaintainper- providers,self-hostedwebsites,andsocialmediaplatforms.
sonalwebsitesandwereawareofrobots.txtbeforethestudy, Assuch,wefocusonthetop8CMSesinouranalysis.These
mostofthem(27outof33)havenotutilizedrobots.txton platformsprovidedrag-and-droptools,allowingartiststo
their personal websites. When prompted why, the single easilyuploadtheirportfoliosandpersonalinformation.As
mostcitedreasonwasnotknowinghowtodoit. well,manyartistsgetcustomdomainnamesthroughthese
Lastly,amongthe27artistswhohadnotutilizedrobots.txt, servicesforanadditionalfee.
sevenofthemmentionedthattheydonothavetheagency: TodeterminewhichCMSanartist’swebsiteuses,werely
theydonothavecontroloverthecontentoftherobots.txt onDNS.Insomecases(e.g.,Carbonmade),theartistsitesare
subdomainsoftheirCMS(e.g.,example.carbonmade.com).
3Thatsaid,wecautionthatmanyartistsuselanguagessuchas“block”or For other services (e.g., Squarespace), the domain’s DNS
“stop”,whilerobots.txtisavoluntarymechanism. recordpointstotheservice’sinfrastructure.SomesiteIUsedToCrawl:Awareness,AgencyandEfficacyinProtectingContentCreatorsFromAICrawlers
LimitedControlandInformationAvailable. Wefind
that CMS providers give limited control and information
toartists.Table2showstheservicesusesbyartists,usage
percentage, and percentage of websites that disallow any
AI-relatedcrawlers(Table1)intheirrobots.txt.Wefound Figure 5: Screenshot of the Squarespace option that
thatthecontentsofrobots.txtfilesareidenticalforallartists lets users include AI-related crawlers in their site’s
whohostwithaCMSexceptartistswhousesSquarespace. robots.txt.
TobetterunderstandtheseCMSes,weregisteredaccounts
witheachofthem.Wefoundthatfourdonotprovideany
methodforuserstomodifytherobots.txtfile,whichisset
by default. Out of these four, only Carbonmade disallows
AI-related crawlers (GPTBot and CCBot) in their default
robots.txtfile.Twoproviders(AdobePortfolioandWeebly)
offer users the option to disallow search engine crawlers
throughtheirrobots.txtfile;however,noneofthesitesin Figure 6: Example of a GPT app (WebG) that can re-
our data set have this option enabled. Only one provider, trieveinformationfromthewebthroughathird-party
thepaidversionofWix,allowsuserstodirectlymodifythe infrastructure(mixerbox.com).Uponclicking“Allow”,
contentoftherobots.txtfile. thisGPTappcanretrieveinformationfromtheweb
Squarespaceistheonlyproviderthatgivestheusertheop- usingserviceprovidedbymixerbox.com.
tiontodisallowAI-relatedcrawlersinrobots.txt.Thisoption
addsfullrestrictionson10AI-relateduser-agents,including
currenttoolsarepoorlydesignedandinadequatelycommu-
GPTBotandanthropic-ai(fulllistinAppendix10.5).
nicated.Forexample,Squarespaceprovidesnotransparency
WealsoinvestigateifanyoftheseCMSesopttoactively
abouthowitsAI-blockingfeatureworkswhenenabled.Fig-
blockAI-relatedcrawlersinadditiontodisallowingthem
ure 5 shows a screenshot of the information provided to
inrobots.txt.(Foradetailedmethodology,seeSection6.1.)
users,whichlacksanymentionofrobots.txtordetailson
WefindthatWeeblydoesinfactspecificallyblockrequests
whichAI-relatedcrawlersareincluded.Itstates,“yoursites
that have theuser-agent set toClaudebot and Bytespider,
won’tbescannedtotrainAImodels”—amisleadingclaim,
whereasArtstationandCarbonmadeimplementcaptcha-like
asthefeatureonlymodifiestherobots.txtfileanddoesnot
challengesforallautomatedrequests.
preventallscanningordatausagebyAI.
Artists Do Not Exercise Their Control. We now ex-
aminetowhatextentartistsactivelyutilizetheseoptions.
5 DoAICrawlersRespectRobots.txt?
ForWix’spaidversion,whichprovidesthehighestlevelof
Sincerobots.txtisavoluntarymechanism,webcrawlersdo
controlovertherobots.txtfile,wefoundthatnoneofthe
nothavetorespectit.TherecentemergencyofAIassistant
110websitesinourdatasethadeditedtheirrobots.txtfile.
crawlersthatfetchwebpagesforgenerativemodelsfurther
WhenattemptingtomodifythefilethroughourpaidWix
complicates the situation — these are crawlers triggered
account,wediscoveredthattheinterfaceisveryconfusing
by users, a use case not clearly covered by the robots.txt
andunclear,makingitdifficulttodeterminehowtomake
standard.Inthissection,weexplorethequestionofwhether
changes.InthecaseofSquarespace,theplatformoffersa
AI-relatedcrawlers,includingbothAIdatacrawlersandAI
straightforwardoption:asinglebuttonthatallowsusersto
assistantcrawlers,respectrobots.txtfiles.Wefindthatthe
disallowAIaccess.However,wefoundthatfewerthan17%of
majorityoftheAIdatacrawlersrespectrobots.txt,whilethe
artistshadenabledthisoption—afiguresignificantlylower
majorityofAIassistantcrawlersdonot.
comparedtothe79%ofartistswho,inouruserstudy,ex-
Fortherestofthesection,westartbydescribingoursetup,
pressedadesiretodisallowAI-relatedcrawlerswhengiven
aswellasthemethodologyweusetoidentifyadditionalAI
thechoice.
assistantcrawlers.Wethenpresentourresultsonwhether
Wehypothesizethatthesignificantgapbetweenthehigh
AIdatacrawlersandAIassistantcrawlersrespectrobots.txt.
numberofartistswillingtotakeactionandthesmallnumber
whoactuallydosoisduetotwomainreasons.First,many
5.1 Methodology
artistslackawarenessofthesetoolsoranunderstandingof
theirfunctionality.Thisissueisevidentfromthelownumber IdentifyingadditionalAIassistantcrawlers.LLMssuch
ofrespondentswhohadeverheardofrobots.txt.Second,the asChatGPTandMeta’sLLAMAhavethebuilt-incapacity
toretrieveinformationfromtheweb.Inaddition,appsin
ChatGPT’sstore(alsoknownasGPTApps),canalsoretrieveLiu,Luo,Shan,Voelker,Zhao,andSavage
informationfromthewebthroughathird-party’sinfrastruc- Forthe24servicesthatoperateGPTappsandcanretrieve
ture(e.g.,thepartythatrunstheGPTapp).Figure6showsan informationfromtheweb,wefoundthattwoservicesfetch
exampleofathird-partyGPTapp(WebG)thatcanretrieve andrespectrobots.txtfiles.Oneservicehasabuginitsim-
informationfromthewebthroughathird-partyinfrastruc- plementationthatcausesittoincorrectlyfetchtherobots.txt
ture(mixerbox.com).Toidentifyalistofthird-partyGPT file.Othersdonotfetchtherobots.txtfileatall(andhence
appsthatcanretrieveinformationfromtheweb,westart donotrespectit).
by examining a list of top 5k GPTs listed on GPTStore (a
popularwebsitecitedinvariouspriorworkthatstudiesGPT
apps[20,52,64]).WetheninteractwitheachGPTappauto- 6 ActiveBlockingofAICrawlers
maticallytodeterminewhetheritcanretrieveinformation The effectiveness of a mechanism like robots.txt depends
fromthewebbyaskingittovisitwebsiteswecontrol.We both on the ability of content owners to express their in-
usetwodifferentprompts:a)“Startaction,fetchpage:[url]”; tent to prevent crawling, as well as the willingness of AI
andb)“Getwebpagecontent:[url].”Wethencheckifthe companiestorespecttheprohibitionsthatcontentcreators
prompttriggerstheGPTapptoretrievethecontentofthe have indicated. Instead, content owners can take matters
website. This process yields a total of 85 third-party GPT intotheirownhandsandactivelyblockcrawlersbyrefusing
apps.UsingacombinationofdomainnameandIPaddress, toreturncontentwhenHTTPrequestsincludeAIcrawler
weidentify24distinctthird-partyservicesthatoperatethese useragents.
GPTapps. Inthissection,wefirstmeasuretheuseofactiveblocking
Testingifcrawlersrespectrobots.txt.Wesetuptwo onpopularsites.Whiletheextentofactiveblockingissimi-
kindsofrobots.txt,onewitha‘*’thatdisallowsallcrawlers lartotheuseofrobots.txt,ourresultsindicatethatthereare
andanotherthatlistseveryuser-agentindividually.ForAI stillseverallimitationstoactiveblocking:itdoesnotoffera
data crawlers, we passively wait for the crawlers to visit perfectreplacementforrobots.txt,anditcanrequiretechni-
ourwebsite,aswecannottriggervisitsfromthesecrawlers. calproficiencytoconfigureproperly.Finally,asacasestudy
ForAIassistantcrawlers,weactivelytriggervisitsfromthe wecomprehensivelystudytheAI-specificactiveblocking
crawlersbysubmittingaURLtothecrawlers.Wethencheck optionprovidedbyCloudflare.Whileitsdeploymentdoes
ifthecrawlersrespecttherobots.txtfile. notrequiretechnicalsophistication,itdoeshavecoverage
limitations.
5.2 Results
5.2.1 AIdatacrawlers. Mostofthedatacrawlerswetested 6.1 Methodology
respectedtherobots.txt(Table1).Concretely,duringourtwo- Interestingly,activeblockingislargelyoverlookedasacon-
monthoftesting,wewereabletoattractsevendatacrawlers tentaccesscontrolmechanisminpriorwork[13,14,16,38],
to our website: Amazonbot, Applebot, Bytespider, CCBot, soitsadoptionforthispurposeisrelativelyunknown.Hence,
ClaudeBot, GPTBot, and Meta-ExternalAgent. We found wefirstaimtoestimatetheproportionoftopwebsitesthat
thatfivecrawlers(Amazonbot,Applebot,CCBot,ClaudeBot, haveblockingmechanismsthatspecificallytargetAI-related
andGPTBot)respectedtherobots.txtfilegivenoursetup. crawlers.
One crawler (Meta-ExternalAgent) has some delay in up- Forsimplicity,weoptedforauser-agentbasedapproach
datingtheirrobots.txtfile—itinitiallyfetchedthecontent (inspiredby[46]).Weacknowledgethatmanyadvancedbot
ofourwebsitedespitebeingdisallowed,buteventuallyre- detectionmethodsexist(e.g.,throughfingerprintingorbe-
spectedourrobots.txtfile.Onecrawler(Bytespider)fetches havioralanalysis),andconsiderourresultsaconservative
therobots.txtfilebutdoesnotrespectit. estimateoftheoverallnumberofdomainsthatspecifically
blockAIcrawlers.Following[46],foreachwebsite,weper-
5.2.2 AIassistantcrawlers. WefoundthatbothChatGPT’s formthefollowingsteps:
andFacebook’sbuilt-incrawlerrespectstherobots.txtfile. ControlCase: Togaugeasite’sAI-crawler-specificblock-
ChatGPT’s crawler can be identified with the user-agent ingbehavior,weexcludesitesthatinherentlyblockourau-
“ChatGPT-User”whileMetawouldprobetherobots.txtof tomation tool (i.e., regardless of the user-agent). In these
a website using User-Agent “facebookexternalhit”. If the cases,wecannotdeterminewhetherasiteisblockingour
robots.txt file disallows the crawler, the crawler will not tool, or is blocking based on a change in the user-agent.
fetchthecontentofthewebsite.Otherwise,thecrawlercan We do this by visiting the site with our headless browser
beidentifiedas“meta-externalagent”.Wealsonotethatthe (Chromium automated by Selenium) and setting its user-
robots.txtiscachedanddoesnotseemtoberequestedfor agenttobethatofatypicalChromeuser,andtheOSmatch-
everyvisit. ingthemachinethebrowserisrunningon.WeexcludetheSomesiteIUsedToCrawl:Awareness,AgencyandEfficacyinProtectingContentCreatorsFromAICrawlers
sitesthatreturnanon-200HTTPstatuscode.Bynotinclud- CCBot/,PiplBot,omgili/,magpie-crawler,
ingthesesites,weconsidertheactiveblockingadoptionrate MeltwaterNews,Diffbot/,Bytespider,cohere-ai,
tobealowerbound. Amazonbot,AwarioSmartBot,AwarioRssBot,
AICase:Holdingallelseconstant,wesetouruser-agent GPTBot/,ChatGPT-User,YouBot,PerplexityBot
tobethatofanAIbot.ToaccountforIP-basedbotdetection Claude-Web,ClaudeBot
mechanisms,weonlytesttheClaudebotandanthropic-ai Table 3: User agents blocked by Cloudflare’s
user-agents:accordingtoDarkVisitors[60],thesearethetop Block AI Scrapers and Crawlers option. Note that
twomost-frequentlyrestrictedAI-relateduseragentsthatdo magpie-cralwer, MeltwaterNews, AwarioSmartBot, and
nothavepublishedIPaddressorigins.Sincethecompanies AwarioRssBotarenotinDarkVisitors’listofAI-related
donotpublishIPaddressranges,siteoperatorswouldlikely agents.
blockthembasedonuser-agent.
DetectingBlockingBehaviorBasedonUser-agent:
Followingthemethdologydescribedin[46],wecheckthe
areblocked).Ononehand,ifthelistofAI-relateduser-agents
HTTPstatuscode,anyexceptionsthatoccur,andwhether
isincomplete,forexample,itcanleadtoafalsesenseofse-
therearesignificantdifferencesintheHTTPcontentlength
curityforitsuser.Ontheotherhand,awebsiteownermay
returned(inspiredby[27]).Anydifferencesinthesefeatures
unintentionallyblockAI-relatedcrawlersthroughtheuseof
betweenthe“Control”and“AI”crawlsindicateactiveblock-
suchanopaqueblockingoption.
ingbasedontheAIuser-agent.
KeyTakeaway:Foracomprehensiveapproachtoprevent
AI-relatedcrawling,itisimportantforsiteownerstostill
userobots.txtinconjunctionwithactiveblockingandverify
6.2 SitesUsingActiveBlocking
thattheiractiveblockingconfiguration’soperationisinline
Weinferthatatleast14%ofthetop10ksitesactivelyblockAI-
withtheirexpectations.
relatedcrawlers,indicatingthatactiveblocking,likerobots.txt,
isarelativelyestablishedcontentaccess-controlmechanism.
6.3 3rd-partyActiveBlocking
Manysitesuseactiveblockinginplaceofrestrictions
As a case study of 3rd-party active blocking, we examine
in robots.txt. Empirically, we find that only 2% of sites
Cloudflare’srecently-launched“BlockAIBots”feature[9].
thatactivelyblockanthropic-aiandClaudebothaveexplicit
It is a compelling feature to evaluate because Cloudflare
restrictionsontheseuser-agentsinrobots.txt,indicatingthat
is currently the only 3rd-party service that offers any AI-
manysitesindeeduseactiveblockingastheirsoleformof
specificactiveblockingmechanism,ishighlypopular[61],
restrictiononAI-relatedcrawling.
andthisfeatureisclearlytargetedtowardalesstechnically-
However,activeblockingcannotreplacerobots.txt
proficient user-base. While the feature is designed to be
for all AI-related crawlers. While active blocking may
user-friendly(a"singleclick"),itsoperationisunfortunately
seemlikeasuitableorevenstrongeralternative,itinherently
ablackboxtotheuser.Wethereforeuseourexperimentsto
cannotreplacesomedirectivesinrobots.txt.Specifically,in
alsoinferdetailsofitsbehavior.
thecasewherecompaniesusethesamecrawlertocollect
AItrainingdataasforotherpurposes,activeblockingcan
Grey-boxEvaluation.Toevaluateitsoperation,wecre-
ated an account with Cloudflare and set up their reverse-
only provide an all-or-nothing approach which can have
proxyserviceonawebsitewehostandcontrol.WhileCloud-
unwanted side-effects. Such examples of these mixed-use
flarestatesthattheAI-bot-blockingfeaturewillbeavailable
crawlersincludeGoogle’sGooglebotandApple’sApplebot:
forallpaymenttiers,forvalidationwetestedboththefree
blockingthemcompletelycanhavesevereconsequencesona
and“Pro”accounts.WeuseourwebserverlogsandCloud-
site’svisibilityontheInternet.Theonlywayforuserstoopt-
flare’sinternaldashboardasasourceofgroundtruth.
outofAItrainingforthesecompaniesisthroughrobots.txt
byaddingadisallowdirectiveforaspecial“dummy”user- InferringthelistofAI-relateduseragentscovered.
CloudflaredoesnotdocumentthelistofAI-relatedcrawlers
agent(Google-ExtendedandApplebot-Extented),highlight-
theyblockunderthisnewswitch.Thus,toinferitscoverage,
ingthatrobots.txtisindeedstillnecessaryevenwithactive
we send requests to our own website with the AI-related
blockingmeasuresinplace.
useragentsfromDarkVisitors.Forcomprehensiveness,we
Activeblockingcanbeablackboxfortheuser.While
alsotestedanadditional590useragents4fromapubliclistof
someactiveblockingconfigurationsrequiretheusertoman-
scrapersandcrawlers[41].Foreachrequest,wedetermine
uallyinputtheblockingrules(e.g.,throughApache’s.htac-
whether or not a given user-agent was blocked using the
cess), other active blocking tools (such as 3rd-party bot-
detection platforms) act as black boxes for users, leaving 4TheGitHubrepositoryweusedincludesthefulluser-agentstring,which
themunawareofitsexactbehavior(e.g.,whichuser-agents isimportanttonoteincaseaserviceusesspecificpatternmatching.Liu,Luo,Shan,Voelker,Zhao,andSavage
robots.txtfilesatamuchhigherratethanaverage:24%as
opposedto12%withintheothersites.Thesesitesshowa
strongintenttoblockAIcrawlers.
Key Takeaway: The active blocking feature provided
byCloudflaremaynotbewidelyusedyet,butitisanen-
couragingnewoption.Itisuser-friendlyandactivelyblocks
contentfrombeingreturnedtocrawlers.However,giventhe
needtocoordinateactiveblockingtogetherwithrobots.txt,
westronglyencourageplatformsprovidingsuchfeaturesto
transparentlydocumentwhichuser-agentstringstheyblock
sothatsitescancontinuetobeindexedbysearchcrawlers
whileachievingtheirgoalsofblockingAIcrawlers.
Figure7:FlowchartforinferringtheAIblockingset-
tingonwebsitesusingCloudflare.
7 Ethics
HTTPresponsecodeandthedashboardforourCloudflare Webelieveourworkhasverylowethicalrisk.Ouruserstudy
account.WemakearequestfirstwiththeAI-bot-blocking isapprovedbytheIRBatourinstitution.Ourlongitudinal
optionturnedoff,andagainwithiton.Inall,wefindthat analysis leverages common crawl data, which is publicly
Cloudflare’sfeatureencompasses17AI-relateduseragents, availableanddoesnotcontainanypersonalinformation.Our
asshowninTable3. activeblockingexperimentsareconductedataresponsible
rate,andwereportourfindingsinaggregatemannersand
InferringtheadoptionofCloudflare’sBlockAIBots
Option. Figure7showsthelogicweusedtoinferwhether donotinvolveindividualwebsites.Last,wewillmakeour
ornotawebsiteusingCloudflarehasturnedontheAIblock- dataandcodeavailabletothecommunityuponpublication.
ing setting. Cloudflare also has another managed ruleset,
calledDefinitelyAutomated,thatcoversalltheunverified5
8 Limitations
AIcrawlersshowninTable3.
WeusedtheClaudeBotandanthropic-aiuser-agentsas Inthissection,wediscusssomelimitationsofourwork.
theyarenotCloudflare-verifiedbots,anddonotpublishor ScopeofParticipants. Theuserstudyfocusedon182
documenttheirIPaddressorigins,soitisunlikelythatCloud- professionalartists,whichdoesnotfullyrepresenttheentire
flareusesIPaddressestocheckforrequestsfromthesetwo populationofcontentcreators.Inparticular,mostpartici-
crawlers.AsforinferringtheDefinitelyAutomated option, pants were based in the US, which limits the coverage of
wechosetheuser-agentsoftwolesser-usedwebautomation creatorsfromothercountries.Forexample,European-based
librariesthatareblockedbythemanagedrule,reducingthe artists might be more familiar with robots.txt due to the
chancethatawebsitehasconfiguredsomecustomblocking implicationsoftheAIAct.
ruleagainstoneofthem.
BlockedDataCollectionRequests. Inourdataset,robots.txt
ForthesetofwebsitesthatuseCloudflare,wevisitthem
files were collected by Common Crawl or our own cus-
withaheadlessbrowserandmodifytheuser-agentstringsas
tom crawlers. A small number of sites returned non-200
showninFigure7.WeinspecttheHTTPresponsecodeand
responsesandwereexcludedfromouranalysis.Thesesites
thereturnedHTMLcontenttodetectwhetheraCloudflare
likelyemployedactiveblockingmeasuresagainstCCBotor
BlockorChallengepagewasreturned,orifthesitecontent
ourcrawlerinadditiontorobots.txtblockstopreventour
wasreturned(indicatingtheuser-agentwasnotblocked).
requests.Excludingthisdatamightleadtousunderreporting
We were able to conclusively determine the setting for
theadoptionofrobots.txt.
93%6ofsitesusingCloudflare.Wefindthatonly5.7%ofsites
Automationtoolscanbeinherentlyblocked. Oures-
usingCloudflarehavethe"BlockAIBots"optionenabled.
timationoftheadoptionrateofactiveblockingpresentedin
Yet, these sites also disallow AI-related crawlers in their
section6.2isaconservativelowerbound,asfor15%ofsites
wecouldnotdeterminetheiractiveblockingbehaviordue
5The verified AI-related bots include Amazonbot, Applebot (which is
toourcrawlingsetupbeinginherentlyblocked.
notblocked),GPTBot,OAI-SearchBot(notblocked),ChatGPT-User,ICC
Crawler(notblocked),andDuckAssistbot(notblocked).Formoredetails
Custom active blocking configurations are possible.
ontheoperationofthissetting,seeAppendix10.6
In6.3,weassumethatthesitedoesnotconfigureanycus-
6Fortheremainingsites,wewereunabletodeterminethesettingasthey
tom active blocking rules against the user-agents we use.
mayhavebeenusing3rdpartyblockingmechanisms,orhavesomecustom,
non-standardCloudflareWAFconfigurations. Forexample,forasmallproportionofsiteswedeterminedSomesiteIUsedToCrawl:Awareness,AgencyandEfficacyinProtectingContentCreatorsFromAICrawlers
theywereusinganadditionalactiveblockingservice(e.g., evenwhensuchfetchesmaythemselvesbedrivenbyagen-
PerimeterX).Weexcludedthosesitesfromouranalysis. erativeAI.Forexample,Meta’sMeta-ExternalFetcheris
dispatchedbyMetaAIproductsinresponsetouserprompts
andcambypassrobots.txtrules.However,theOpenAI’suser-
9 DiscussionandConclusion
triggeredAI-relatedfetcherChatGPT-Userfollowsrobots.txt
Atthecoreoftheconflictinthispaperisthenotionthatcon-
rules.
tentcreatorsnowwishtocontrolhowtheircontentisused,
notsimplyifitisaccessible.Whilesuchrightsaretypically
9.2 RespectforSignal
explicitincopyrightlaw,theyarenotreadilyexpressible,let
aloneenforceableintoday’sInternet.Instead,aseriesofad Evenifalloftheseotherambiguitiesaresuccessfullyman-
hoccontrolshaveemergedbasedonrepurposingexisting aged,theunderlyingsignalingprotocolisvoluntary—crawlers
webnormsandfirewallcapabilities,noneofwhichmatch mustabidebythedirectivesofrobots.txt.Aswehaveshown
thespecificity,usability,orlevelofenforcementthatis,in inSection5notallcrawlersrespectrobots.txt(e.g.,ByteDance’s
fact,desiredbycontentcreators.Webelievethereexistthree Bytespiderignoresrobots.txtdirectives)andothers,while
categories of issues that need to be addressed: ambiguity, theyabide,maycacherobots.txtandmaycontinuetofetch
respectforsignal,andusercontrol. contentevenafterithaschanged(e.g.,FacebookExternalA-
gent).Attheextremesomecrawlersmaypretendtobereg-
ularuserbrowsers,thusnecessitatingtheuseofadvanced
9.1 IssuesofAmbiguity
activeblockingtechniquessuchasfingerprinting[9].
Perhaps unsurprisingly, robots.txt is an imperfect mecha-
Incomparison,activeblocking,e.g.,IP-levelblockingprac-
nismforthispurposeandintroducesarangeofambiguities–
ticedbyCloudflare,allowsbetterenforcementofanaccess
evenforthepurposeofmeasurement.Oneofthekeyissues
policy, but still suffers from issues such as dual-purpose
isinambiguityaroundwhatrobots.txtmeansandhowitis
crawlersandfetcheslaunderedviaathird-partyinfrastruc-
honored.
ture.Inaddition,someLLMcrawlersdonotuseidentifiable
SyntacticAmbiguityForexample,thesyntacticandlex-
rangesofIPsandthusIP-levelblockingisnottechnically
ical structure of robots.txt is unintuitively complex. One
feasible(e.g.,Anthropic[4]).
resultisthatdifferentparserscaninterpretthesamesetof
directivesdifferently.Forexample,theparserusedin[38],
9.3 Usercontrol
misinterpretsgroupingrulesandtheignoresUser-agent
Bothrobots.txtandactiveblocking(i.e.,viafirewallrules)
linebeingcase-insensitive,leadingtolargenumbersofdis-
presuppose that the content creator has the capability to
allowdirectivesbeingignored.Similarly,robots.txtauthors
changethisstateontheWebserverhostingtheircontentand
canmisinterpretcorrectsyntaxandwehavefoundthatap-
thattheyhavethetechnicalcapabilityanddomainknowl-
proximately 1% of sites have mistakes in their robots.txt,
edgetodosocorrectly.
suchasnotstartingapathwitha/,orusingdeprecatedor
However,mostcontentcreatorsarenotalsosystemad-
non-existentdirectives.
ministrators,nordotheyruntheirownWebservers.Thus,
NamingambiguityHowever,amoresignificantprob-
thesemechanismsareofmostutilitytolargerorganizations
lemisthatrobots.txt’sabilitytospecifythatLLM-training
whosepolicyinterestscanbealignedwiththeiruseoftech-
crawlersareunwelcomeispredicatedonthenotionthatthe
nicalcontrols.Indeed,inourdata,weobservedthatmultiple
purposeofacrawlerisclearlyanduniquelyidentifiedviathe
largepublishershaveremoved restrictionsinrobots.txtfor
User-Agentstring.Thus,anLLMcrawlerthatdoesnotself-
thesitestheyownafterstrikingdatausagedealswithAI
identifyassuchwillnotprovokethecreationofarobots.txt
companies.Thisshowsthatlargecontentownersarewilling
rule.Moreover,keepingtrackofthecurrentUser-Agent’s
tolettheirdatabeusedforAItraining,butonlyiftheyre-
ofallsuchcrawlersisaburdenplacedoneachwebadmin-
ceivemonetarycompensationand/orsitetraffic7inexchange
istrator.Lastly,anumberofcrawlersservedualpurposes—
fortheusageoftheirdata.
gatheringdatathatisusedbothforupdatingsearchindexes
Since few individual creators maintain their own Web
and fortrainingAImodels,addingadditionalcomplexity.
server, they must rely on their website hoster to provide
ModeofaccessambiguityTheRobotsExclusionProto-
auser-interfacetosuchcapabilitiesthatthatcreatorscan
col,ascurrentlyspecified,doesnotmakeclearwhata“robot”
understandandistechnicallyeffective.However,fewhosters
is. Thisis leftto theinterpretation ofeach organization’s
appeartoexportrobots.txtdirectlytotheircustomers(Wix
automotive access. For example, Google documents that
robots.txtisnotapplicabletocrawlersthatarecontrolledby
7Forexample,inthedealbetweenOpenAIandDotdashMeredith,one
users(forexample,feedsubscriptions).Thusfar,itisunclear
contracttermrequiresthatOpenAImustlinktotheirsitewhendisplaying
whetheruser-triggeredfetchesareexemptfromtheprotocol, informationrelevanttooneoftheirsubsidiaries[40].Liu,Luo,Shan,Voelker,Zhao,andSavage
isoneofthefewweidentifiedofferingthiscapability)and References
mostdonotprovideanyseparatemechanismtoexpressa [1] ai.txt: A new way for websites to set permissions for AI. 2023.
desiretoblockAIbots. Miller,Cullen. https://spawning.substack.com/p/aitxt-a-new-way-
Insummary,ourworkhighlightsthetrifectaofchallenges for-websites-to-set.
fortoday’scontentcreators.First,therearenoexistingmech- [2] YasminAAlNoamany,MicheleCWeigle,andMichaelLNelson.2013.
Accesspatternsforrobotsandhumansinwebarchives.InProceedings
anismsforexplicitlycontrollingwhetherpublicly-accessible
ofthe13thACM/IEEE-CSjointconferenceonDigitallibraries.339–348.
WebcontentisusedtrainingAImodelsornot.Second,the [3] BabakAminAzad,OleksiiStarov,PierreLaperdrix,andNickNiki-
existingmechanismsthatwehavebroughttobearforthis forakis.2020.Webrunner2049:Evaluatingthird-partyanti-botser-
purposearepoorfitsforthetask,lackappropriatespecificity, vices.InDetectionofIntrusionsandMalware,andVulnerabilityAs-
comprehensivenessorverifiability.Third,thesemechanisms sessment:17thInternationalConference,DIMVA2020,Lisbon,Portugal,
June24–26,2020,Proceedings17.Springer,135–159.
aregenerallynotreadilyavailabletoindividualcontentcre-
[4] Anthropic. 2024. Does Anthropic crawl data from the
ators and more serve the interests of large organizations web, and how can site owners block the crawler? https:
seekingtoprotectlargetrovesofcontent. //support.anthropic.com/en/articles/8896518-does-anthropic-crawl-
data-from-the-web-and-how-can-site-owners-block-the-crawler.
[5] QuanBai,GangXiong,YongZhao,andLongtaoHe.2014.Analysisand
detectionofbogusbehaviorinwebcrawlermeasurement.Procedia
ComputerScience31(2014),1084–1091.
[6] Baker Law Group LLC. 2024. CCase Tracker: Artifi-
cial Intelligence, Copyrights and Class Actions. https:
//www.bakerlaw.com/services/artificial-intelligence-ai/case-
tracker-artificial-intelligence-copyrights-and-class-actions/.
[7] KayleighBarber.2024.Future’sJonSteinbergshareshisphilosophyon
AIcontentlicensingdeals.https://digiday.com/podcasts/futures-jon-
steinberg-shares-his-philosophy-on-ai-content-licensing-deals/.
[8] Muhammad Ahmad Bashir, Sajjad Arshad, Engin Kirda, William
Robertson,andChristoWilson.2019. ALongitudinalAnalysisof
theads.txtStandard.InProceedingsoftheInternetMeasurementCon-
ference.294–307.
[9] Alex Bocharov et al. 2024. Declare your AIndependence:
block AI bots, scrapers and crawlers with a single click.
https://blog.cloudflare.com/declaring-your-aindependence-block-ai-
bots-scrapers-and-crawlers-with-a-single-click/.
[10] JosiahDBoucher,GillianSmith,andYunusDoğanTelliel.2024. Is
ResistanceFutile?:EarlyCareerGameDevelopers,GenerativeAI,and
EthicalSkepticism.InProceedingsoftheCHIConferenceonHuman
FactorsinComputingSystems.1–13.
[11] ZiChu,StevenGianvecchio,andHainingWang.2018.Botorhuman?
Abehavior-basedonlinebotdetectionsystem.FromDatabasetoCyber
Security:EssaysDedicatedtoSushilJajodiaontheOccasionofHis70th
Birthday(2018),432–449.
[12] Cloudflare.2024. VerifiedBots. https://radar.cloudflare.com/traffic/
verified-bots.
[13] MichaelDinzingerandMichaelGranitzer.2024.Alongitudinalstudy
ofcontentcontrolmechanisms.InCompanionProceedingsoftheACM
onWebConference2024.1382–1387.
[14] Michael Dinzinger, Florian Heß, and Michael Granitzer. 2024. A
SurveyofWebContentControlforGenerativeAI. arXivpreprint
arXiv:2404.02309(2024).
[15] European Commission. 2024. First Draft of the General-Purpose
AI Code of Practice published, written by independent experts.
https://digital-strategy.ec.europa.eu/en/library/first-draft-general-
purpose-ai-code-practice-published-written-independent-experts.
[16] RichardFletcher.2024.HowmanynewswebsitesblockAIcrawlers?
(2024).
[17] Sheera Frenkel and Stuart A. Thompson. 2023. Not for
Machines to Harvest: Data Revolts Break Out Against A.I.
https://www.nytimes.com/2023/07/15/technology/artificial-
intelligence-models-chat-data.html.SomesiteIUsedToCrawl:Awareness,AgencyandEfficacyinProtectingContentCreatorsFromAICrawlers
[18] Google.2024.GoogleRobots.txtParserandMatcherLibrary.https: [36] JieLi,HanchengCao,LauraLin,YouyangHou,RuihaoZhu,andAbdal-
//github.com/google/robotstxt. lahElAli.2024.Userexperiencedesignprofessionals’perceptionsof
[19] AliciaGuo,ShreyaSathyanarayanan,LeijieWang,JeffreyHeer,and generativeartificialintelligence.InProceedingsoftheCHIConference
AmyZhang.2024.FromPentoPrompt:HowCreativeWritersInte- onHumanFactorsinComputingSystems.1–18.
grateAIintotheirWritingPractice.arXivpreprintarXiv:2411.03137 [37] XigaoLi,BabakAminAzad,AmirRahmati,andNickNikiforakis.2021.
(2024). Goodbot,badbot:Characterizingautomatedbrowsingactivity.In
[20] XinyiHou,YanjieZhao,andHaoyuWang.2024.Onthe(in)security 2021IEEEsymposiumonsecurityandprivacy(sp).IEEE,1589–1605.
ofllmappstores.arXivpreprintarXiv:2407.08422(2024). [38] ShayneLongpreetal.2024.ConsentinCrisis:TheRapidDeclineof
[21] ChristosIliou,TheodorosKostoulas,TheodoraTsikrika,VasilisKatos, theAIDataCommons.arXivpreprintarXiv:2407.14933(2024).
StefanosVrochidis,andIoannisKompatsiaris.2021. Detectionof [39] AnáliaGLourençoandOrlandoOBelo.2006.Catchingwebcrawlers
advancedwebbotsbycombiningweblogswithmousebehavioural intheact.InProceedingsofthe6thinternationalConferenceonWeb
biometrics.Digitalthreats:researchandpractice2,3(2021),1–26. Engineering.265–272.
[22] ChristosIliou,TheodorosKostoulas,TheodoraTsikrika,VasilisKatos, [40] DotdashMeredith.2024. DotdashMeredithAnnouncesStrategic
StefanosVrochidis,andYiannisKompatsiaris.2019.Towardsaframe- Partnership with OpenAI, Bringing Iconic Brands and Trusted
workfordetectingadvancedwebbots.InProceedingsofthe14th ContenttoChatGPT.https://dotdashmeredith.mediaroom.com/2024-
internationalconferenceonavailability,reliabilityandsecurity.1–10. 05-07-Dotdash-Meredith-Announces-Strategic-Partnership-
[23] KatyIlonkaGero,MeeraDesai,CarlySchnitzler,NayunEom,Jack with-OpenAI,-Bringing-Iconic-Brands-and-Trusted-Content-to-
Cushman,andElenaLGlassman.2024.CreativeWriters’Attitudeson ChatGPT.
WritingasTrainingDataforLargeLanguageModels.arXive-prints [41] monperrus.2024.crawler-user-agents.https://github.com/monperrus/
(2024),arXiv–2409. crawler-user-agents.
[24] GregoireJacob,EnginKirda,ChristopherKruegel,andGiovanniVi- [42] OpenAI.2024.OurapproachtoAIsafety. https://openai.com/index/
gna.2012. {PUBCRAWL}:Protectingusersandbusinessesfrom our-approach-to-ai-safety/.
{CRAWLers}.In21stUSENIXSecuritySymposium(USENIXSecurity [43] Stack Overflow. 2024. Stack Overflow and OpenAI Partner to
12).507–522. StrengthentheWorld’sMostPopularLargeLanguageModels.https:
[25] SteveTKJan,QingyingHao,TianruiHu,JiamengPu,SonalOswal, //stackoverflow.co/company/press/archive/openai-partnership.
GangWang,andBimalViswanath.2020.Throwingdartsinthedark? [44] DavidPattern.2024. Claudebotattack. https://www.phpbb.com/
detectingbotswithlimiteddatausingneuraldataaugmentation.In community/viewtopic.php?t=2652265.
2020IEEEsymposiumonsecurityandprivacy(SP).IEEE,1190–1206. [45] DavidPattern.2024.DDoSfromAnthropicAI.https://www.linode.
[26] HarryHJiang,LaurenBrown,JessicaCheng,MehtabKhan,Abhishek com/community/questions/24842/ddos-from-anthropic-ai.
Gupta,DejaWorkman,AlexHanna,JohnathanFlowers,andTimnit [46] KienPham,AécioSantos,andJulianaFreire.2016. Understanding
Gebru.2023. AIArtanditsImpactonArtists.InProceedingsofthe websitebehaviorbasedonuseragent.InProceedingsofthe39thIn-
2023AAAI/ACMConferenceonAI,Ethics,andSociety.363–374. ternationalACMSIGIRconferenceonResearchandDevelopmentin
[27] BenJones,Tzu-WenLee,NickFeamster,andPhillipaGill.2014.Au- InformationRetrieval.1053–1056.
tomatedDetectionandFingerprintingofCensorshipBlockPages.In [47] TaraPoteatandFrankLi.2021. Whoyougonnacall?anempirical
ProceedingsoftheInternetMeasurementConference. evaluationofwebsitesecurity.txtdeployment.InProceedingsofthe
[28] HugoJonker,BenjaminKrumnow,andGabryVlot.2019.Fingerprint 21stACMInternetMeasurementConference.526–532.
surface-baseddetectionofwebbotdetectors.InComputerSecurity– [48] Stefano Rovetta, Alberto Cabri, Francesco Masulli, and Grażyna
ESORICS2019:24thEuropeanSymposiumonResearchinComputer Suchacka.2019. Botornot?acasestudyonbotrecognitionfrom
Security,Luxembourg,September23–27,2019,Proceedings,PartII24. websessionlogs.QuantifyingandProcessingBiomedicalandBehav-
Springer,586–605. ioralSignals27(2019),197–206.
[29] JosephSaveriLawFirmLLP.2023.ClassActionFiledAgainstStabil- [49] MHMSchellekens.2013. Robot.txt:balancinginterestsofcontent
ityAI,Midjourney,andDeviantArtforDMCAViolations,Rightof producersandcontentusers. BridgingDistancesinTechnologyand
PublicityViolations,UnlawfulCompetition,BreachofTOS. https: Regulation(2013),173.
//cybernews.com/news/artists-unite-in-legal-battle-against-ai/. [50] Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana
[30] ReishiroKawakamiandSukritVenkatagiri.2024.TheImpactofGener- Hanocka,andBenYZhao.2023.Glaze:Protectingartistsfromstyle
ativeAIonArtists.InProceedingsofthe16thConferenceonCreativity mimicryby{Text-to-Image}models.In32ndUSENIXSecuritySympo-
&Cognition.79–82. sium(USENIXSecurity23).2187–2204.
[31] KateKnibbs.2024. CondéNastSignsDealWithOpenAI. https: [51] DusanStevanovic,AijunAn,andNatalijaVlajic.2012.Featureevalu-
//www.wired.com/story/conde-nast-openai-deal/. ationforwebcrawlerdetectionwithdataminingtechniques.Expert
[32] KateKnibbs.2024.TheRacetoBlockOpenAI’sScrapingBotsIsSlow- SystemswithApplications39,10(2012),8707–8717.
ingDown. https://www.wired.com/story/open-ai-publisher-deals- [52] DongxunSu,YanjieZhao,XinyiHou,ShenaoWang,andHaoyuWang.
scraping-bots/. 2024.Gptstoreminingandanalysis.arXivpreprintarXiv:2405.10210
[33] SantanuKolay,PaoloD’Alberto,AliDasdan,andArnabBhattacharjee. (2024).
2008. Alargerscalestudyofrobots.txt.InProceedingsofthe17th [53] GrażynaSuchacka,AlbertoCabri,StefanoRovetta,andFrancesco
internationalconferenceonWorldWideWeb.1171–1172. Masulli.2021. Efficienton-the-flyWebbotdetection. Knowledge-
[34] ShinilKwon,Young-GabKim,andSungdeokCha.2012. Webrobot BasedSystems223(2021),107074.
detectionbasedonpattern-matchingtechnique.JournalofInformation [54] YangSun,ZimingZhuang,IsaacGCouncill,andCLeeGiles.2007.
Science38,2(2012),118–126. Determiningbiastosearchenginesfromrobots.txt.InIEEE/WIC/ACM
[35] JunsupLee,SungdeokCha,DongkunLee,andHyungkyuLee.2009. InternationalConferenceonWebIntelligence(WI’07).IEEE,149–155.
Classificationofwebrobots:anempiricalstudybasedonoverone [55] YangSun,ZimingZhuang,andCLeeGiles.2007.Alarge-scalestudy
billionrequests.computers&security28,8(2009),795–802. ofrobots.txt.InProceedingsofthe16thinternationalconferenceon
WorldWideWeb.1123–1124.Liu,Luo,Shan,Voelker,Zhao,andSavage
[56] DavidSénécal.2024.TheWebScrapingProblem:Part1.https://www. crawlasiteseveraltimesovertheperiodinwhichthedata
akamai.com/blog/security/the-web-scraping-problem-part-1. forthesnapshotwascollected.Inthesecases,wededuplicate
[57] Björn Ulvaeus. 2024. Statement on AI training. https://www. therobots.txtfiles bytakingthemost recentnon-errored
aitrainingstatement.org/.
crawlinthesnapshot.TheCommonCrawlcrawleralsodoes
[58] AntoineVastel,WalterRudametkin,RomainRouvoy,andXavierBlanc.
notfollowredirects.Toimproveourcoverage,fordomains
2020.Fp-crawlers:studyingtheresilienceofbrowserfingerprinting
toblockcrawlers.InMADWeb’20-NDSSWorkshoponMeasurements, thatreturnedanon-200HTTPstatuscodetoCommonCrawl
Attacks,andDefensesfortheWeb. (suchas301Redirect),wealsocheckedCommonCrawlfor
[59] JAMES VINCENT. 2022. Getty Images is suing the cre- therobots.txtfileforthedomainprependedwithwww.(if
ators of AI art tool Stable Diffusion for scraping its con-
notalready)andwithout(ifalreadyprepended).
tent. https://www.theverge.com/2023/1/17/23558516/ai-art-
copyright-stable-diffusion-getty-images-lawsuit.
[60] DarkVisitors.2024.DarkVisitors.https://darkvisitors.com/agents.
[61] W3Techs.2024.Usagestatisticsandmarketsharesofreverseproxy 10.3 Robots.txtedgecases
services.https://w3techs.com/technologies/overview/proxy.
Below,welistseveraledgecasesinrobots.txtthatarestill
[62] ChloeXiang.2022. ArtistsAreRevoltingAgainstAIArtonArtSta-
tion. https://www.vice.com/en/article/ake9me/artists-are-revolt- syntaticallycorrect,butareoftennotcapturedbymoresim-
against-ai-art-on-artstation. plistic,home-brewedrobots.txtparsers.
[63] ChyanYangandHsien-JyhLiao.2010.UsingtheRobots.txtandRobots
Metatagstoimplementonlinecopyrightandarelatedamendment.
User-agent: *
Libraryhitech28,1(2010),94–106.
[64] ZejunZhang,LiZhang,XinYuan,AnlanZhang,MengweiXu,and # Blog restrictions
FengQian.2024.Afirstlookatgptapps:Landscapeandvulnerability. Disallow: /blog/latest/*
arXivpreprintarXiv:2402.15105(2024). Disallow: /blogs/*
10 Appendix
10.1 CCDatasetSummary Figure8:Edgecase1:commentsornewlinesbetween
theuser-agentlineandtheDisallow/Allowdirectives
shouldbeeffectivelyignored,butwefindthatsome
#siteswith
Snapshot Month #sites parsersfailmissthedirectivesunderthecommentor
robots.txt
2022-05 Sep/Oct2022 40177 31494 newline.
2022-21 Nov/Dec2022 40614 31536
2022-40 Jan/Feb2023 39080 30063
2023-06 Mar/Apr2023 39216 29963
2023-14 May/Jun2023 39212 30107 User-agent: GPTBot
2023-23 Sep/Oct2023 39033 29721
User-agent: anthropic-ai
2023-40 Nov/Dec2023 39722 30060
User-agent: Claudebot
2023-50 Feb/Mar2024 41446 31282
2024-10 Apr2024 41640 31010 Disallow: /
2024-18 May2024 41004 30763
2024-22 Jun2024 41047 30661
2024-26 Jul2024 40927 30526
Figure9:Edgecase2:Groupingrules.TheRFCallows
2024-33 Aug2024 40455 29922
forUser-agentdirectivestobegroupedasshownabove.
2024-38 Sep2024 40444 29806
2024-42 Oct2024 40420 29867 However,wefoundtheparserin[38]toignorealluser-
Table4:NumberofsitesintheTrencotop100kinter- agentsexceptforthelastwhenparsingrobots.txt.
sectionintheCommonCrawldataset.
10.4 DomainsthatexplicitlyallowGPTBot
inrobots.txt
10.2 CleaningHistoricRobots.txtData
Table 5 shows the list of domains that explicitly fully al-
fromCommonCrawl
low GPTBot (e.g., User-agent: GPTBot\nAllow: /) in
HistoricDataFromCommonCrawl. Forourhistoric their robots.txt, and which Common Crawl snapshot we
robots.txtdataset,weusedtherobots.txtfilescrawledbythe firstobservedthisbehavior.Wenotethat5sites,nfhs.org,
CommonCrawlfrom2022to2024.Weuse15consecutive 10best.com,ground.news,network54.com,andtarleton.edu,
snapshots from Common Crawl, beginning with 2022-05, havepersistentlyallowedGPTBotsincearoundthetimeof
throughto2024-38.Foreachsnapshot,CommonCrawlmay itsreleasetoourlatestsnapshot.SomesiteIUsedToCrawl:Awareness,AgencyandEfficacyinProtectingContentCreatorsFromAICrawlers
User-agent: * 10.5 SquarespaceRestrictedAIBots
Disallow: /
Figure11showsthecontentaddedtotherobots.txtfilefor
Squarespacewhenthe"BlockAIcrawling"optionisselected.
User-agent: *
Crawl-Delay: 5
User-agent: GPTBot
User-agent: GoogleBot User-agent: ChatGPT-User
Allow: / User-agent: CCBot
Disallow: /z/ User-agent: anthropic-ai
User-agent: Google-Extended
User-agent: FacebookBot
Figure10:Edgecase3:usingdeprecatedorunsupported User-agent: Claude-Web
directivescanhaveunintendedconsequences:forthis User-agent: cohere-ai
robots.txt,sinceCrawl-Delayisignored,alluser-agents User-agent: PerplexityBot
will(unintentionally)followthedirectivesforGoogle- User-agent: Applebot-Extended
botduetothegroupingrule. Disallow: /
url snapshot url snapshot
nfhs.org 2023-40 bleedcubbieblue.com 2024-42
10best.com 2023-40 popsugar.com 2024-42 Figure 11: Content added to the robots.txt file for
ground.news 2023-40 voxmedia.com 2024-42
opindia.com 2024-42 patspulpit.com 2024-42 Squarespace websites when the "Block AI crawling"
tarleton.edu 2023-50 barcablaugranes.com 2024-42 optionisselected
alldatasheet.com 2024-42 eater.com 2024-42
bestproductsreviews.com 2024-42 popsugar.co.uk 2024-42
network54.com 2023-50 prideofdetroit.com 2024-42
care.com 2024-42 royalsreview.com 2024-42
kbs.co.kr 2024-42 truebluela.com 2024-42 10.6 Cloudflare"DefinitelyAutomated"
brit.co 2024-42 thrillist.com 2024-42 User-agents
lonza.com 2024-42 sbnation.com 2024-42
millersville.edu 2024-42 arrowheadpride.com 2024-42 Below,welisttheuser-agentsweinferredCloudflare’s"Def-
icelandair.com 2024-42 theringer.com 2024-42
initelyAutomated"settingtoblock.WenotethatIPaddress
customink.com 2024-42 adslzone.net 2024-42
celebmafia.com 2024-18 milehighreport.com 2024-42 likelyplaysaroleintheoperationofthissettingthrough
credit-agricole.fr 2024-42 polygon.com 2024-42
blocking"fake"verifiedbots(e.g.,arequestthatclaimstobe
adelaidenow.com.au 2024-42 racked.com 2024-42
dailytelegraph.com.au 2024-42 behindthesteelcurtain.com 2024-42 aparticularCloudflareVerifiedBot,butdoesnotcomefroma
walkhighlands.co.uk 2024-42 bavarianfootballworks.com 2024-42
documentedIPaddress).Weexcludetheseuser-agentsfrom
softonic-ar.com 2024-22 bleedinggreennation.com 2024-42
heraldsun.com.au 2024-42 silverscreenandroll.com 2024-42 thelist,butnotethatthelistofCloudflareverifiedbotsis
royalsocietypublishing.org 2024-22 gnc.com 2024-42 publiclyavailable[12].
softonic.com 2024-42 cagesideseats.com 2024-42
shopstyle.com 2024-42 blazersedge.com 2024-42
couriermail.com.au 2024-42 badlefthook.com 2024-42
theaustralian.com.au 2024-42 cincyjungle.com 2024-42 wget, Python-urllib, python-requests, aiohttp,
news.com.au 2024-42 hogshaven.com 2024-42
httpx, libwww-perl, Nutch, Go-http-client,
kaufland.de 2024-42 bigblueview.com 2024-42
sendpulse.com 2024-26 ninersnation.com 2024-42 Teoma, grub.org, CCBot, 360Spider, binlar,
washingtonexaminer.com 2024-33 pinstripealley.com 2024-42
Scrapy, PhantomJS, PiplBot, omgili, AHC,
thedodo.com 2024-42 bloggingtheboys.com 2024-42
g2a.com 2024-42 quickbase.com 2024-42 Apache-HttpClient, magpie-crawler,
fieldgulls.com 2024-42 embluemail.com 2024-42 MeltwaterNews, Diffbot, axios, W3C-checklink,
recode.net 2024-42 softonic.com.br 2024-42
novartis.com 2024-38 stimulustech.com 2024-42 HeadlessChrome, serpstatbot, curl, Bytespider,
mmafighting.com 2024-42 searchenginejournal.com 2024-42 PHP-Curl-Class, centurybot, node-fetch,
vox.com 2024-42 giant-bicycles.com 2024-42
mmamania.com 2024-42 realself.com 2024-42 PerplexityBot, Claudebot, anthropic-ai
Table5:DomainsthatexplicitlyfullyallowGPTBotin
theirrobots.txt,andwhichCommonCrawlsnapshot Figure12:Listofuser-agentsinferredtobeblockedby
wefirstobservedthisbehavior. Cloudflare’s"DefinitelyAutomated"rule.