Free Energy Projective Simulation (FEPS): Active inference with
interpretability
J. Pazem1,∗, M. Krumm1, A. Q. Vining1,2, L. J. Fiderer1, H. J. Briegel1,3
1 University of Innsbruck, Institute for Theoretical Physics, Technikerstraße 21a, A-6020 Innsbruck, Austria
2 Max Planck Institute of Animal Behavior, Department for the Ecology of Animal Societies,
Bu¨cklestraße 5a, 78467 Konstanz, Germany
3 Department of Philosophy, University of Konstanz, Universita¨tsstraße 10, 78464 Konstanz, Germany
∗ josephine.pazem@uibk.ac.at
November 25, 2024
In the last decade, the free energy principle (FEP) and active inference (AIF) have achieved many successes
connectingconceptualmodelsoflearningandcognitiontomathematicalmodelsofperceptionandaction. This
effort is driven by a multidisciplinary interest in understanding aspects of self-organizing complex adaptive
systems, including elements of agency. Various reinforcement learning (RL) models performing active inference
have been proposed and trained on standard RL tasks using deep neural networks. Recent work has focused
on improving such agents’ performance in complex environments by incorporating the latest machine learning
techniques. In this paper, we take an alternative approach. Within the constraints imposed by the FEP and
AIF, we attempt to model agents in an interpretable way without deep neural networks by introducing Free
EnergyProjectiveSimulation(FEPS).Usinginternalrewardsonly,FEPSagentsbuildarepresentationoftheir
partially observable environments with which they interact. Following AIF, the policy to achieve a given task
is derived from this world model by minimizing the expected free energy. Leveraging the interpretability of the
model,techniquesareintroducedtodealwithlong-termgoalsandreducepredictionerrorscausedbyerroneous
hidden state estimation. We test the FEPS model on two RL environments inspired from behavioral biology: a
timed response task and a navigation task in a partially observable grid. Our results show that FEPS agents
fully resolve the ambiguity of both environments by appropriately contextualizing their observations based on
prediction accuracy only. In addition, they infer optimal policies flexibly for any target observation in the
environment.
1 Introduction
Akeychallengeinbothcognitivescienceandartificialintelligenceisunderstandingcognitiveprocessesinbiological
systemsandapplyingthisknowledgetothedevelopmentofartificialagents. Inthiswork,wedevelopaninterpretable
artificialagentwhichintegrateskeyaspectsofbothreinforcementlearning(RL)[1]andactiveinference[2,3,4,5,6].
In RL, an external reward signal is typically used to guide an agent’s behavior while in active inference no such
reward signal exists. Instead, agents follow an intrinsic motivation which is rooted in the free energy principle
(FEP) [7, 8].
Central to the FEP is the idea that adaptive systems can be modeled as performing an approximate form of
Bayesian inference. The outcome of this process minimizes a quantity, called variational free energy (VFE), which
lendsitsnametotheFEP.TheFEPencompassesvariousparadigmsthatalignwiththegoalofBayesianinference,
such as predictive processing [9] and the Bayesian brain hypothesis [10].
OnewaytoimplementtheFEPisthroughactiveinferencewhichinvolvesaplanningmethodforactionselection
basedonaninternalmodeloftheenvironment,referredtoastheworldmodel,alongwithapreferencedistribution,
which encodes the agent’s desired states (e.g., maintaining a certain body temperature). According to the FEP,
it is—in principle—always possible to explain the observed behavior of living systems through active inference
1
4202
voN
22
]IA.sc[
1v19941.1142:viXra[11]. The Free Energy Principle has been applied to a wide range of domains, including neuroscience [11, 12, 13],
psychology [14, 15], behavior biology [16, 17, 4], and machine learning [18, 19, 20, 21, 22].
Active inference has recently gained traction in theoretical discussions [5, 23, 24, 25, 26] leading to several
successful algorithmic implementations [6, 20, 18, 14, 22, 7, 27, 19, 28]. Existing implementations are based on
methods developed in the context of model-based reinforcement learning and rely mostly on neural networks to
represent the world model [14, 7, 18, 6, 22, 29, 30, 27]. As an alternative to neural networks, this work proposes
implementing active inference using Projective Simulation (PS) [31].
InPS,memoryisorganizedasadirectedgraph, which—unlikeneuralnetworks—isdesignedtobeinterpretable
such that individual vertices carry semantic information [32, 33]. The agent’s cognitive processes are modelled as
(1) a random walk through the graph (deliberation), and (2) the updating of transition probabilities along the
graph’s edges (learning). The primary motivation for using PS over neural networks is its inherent interpretability.
While so far PS has mostly been employed in the RL framework, this work extends PS to the active inference
framework by proposing and testing an agent model called Free Energy Projective Simulation (FEPS). FEPS is
an active inference model which uses PS for both the world model and the action policy, combining existing and
novel methods for training these components. Key features of FEPS include an internal reward signal derived from
the prediction accuracy, and the use of two distinct preference distributions—one for learning the world model and
another for achieving a specific goal. For the latter purpose, we propose new heuristics to estimate the long-term
value of belief states in sight of the goal.
In the following sections, we first introduce active inference (Section 2) and PS (Section 3), before presenting
the architecture (Section 4) and algorithmic features (Section 5) of the FEPS model. We then test FEPS through
numerical simulations in biologically inspired environments, focusing on timed responses and navigation (Section
6). Finally, we conclude with a discussion of future research directions (Section 7).
2 Active Inference
The following presentation of active inference is divided into two parts. First, we outline how the world model
is constructed and updated in response to sensations (perceptual inference). Next, we explain how planning and
action selection are modeled (active inference).
During perceptual inference, the agent develops the world model [34, 35], sometimes referred to as a generative
model, highlighting its ability to generate simulated percepts. The world model [11, 6], is described as a partially
observable Markov decision process (POMDP) (B,S,A,T,L,R), with belief states B, observations S, and actions
A. The transition function T assigns probabilities to transitions between belief states given some action, while L,
sometimes referred to as the emission function, defines the probability that some belief state b∈B emits a sensory
signal s ∈ S. Note that belief states are observable to the agent and that they are generally different from the
actual, unobservable, states of the environment.
Inwhatfollows,thetime-indexedrandomvariablesB ,S ,andA representbeliefs,observations,andactionsat
t t t
agiventimet,withtheirpossiblevaluesdenotedbythecorrespondinglower-caseletters. ThetransitionfunctionT
between times t−1 and t corresponds to the conditional distribution p(B |B ,A ), while the emission function
t t−1 t−1
L at time t is the likelihood p(S |B ). Finally, rewards R are used to learn the transition and emission functions.
t t
In this work, rewards are internal (generated by the agent itself) [36] but in general they could also be external
(given by the environment). It is further assumed that action A is selected based on the current belief state B , as
t t
represented by the conditional distribution π(A |B ), which defines the policy. Combining these elements, the joint
t t
distribution of the world model up to time t can be decomposed as follows:
t
(cid:89)
p(B ,A ,S )=p(B ,S ) π(A |B )p(S |B )p(B |B ,A ), (1)
0:t 0:t−1 0:t 0 0 t′−1 t′−1 t′ t′ t′ t′−1 t′−1
t′=1
where the index notation B =(B ,B ,...,B ) is used for sequences of random variables, and p(B ,S ) denotes
0:t 0 1 t 0 0
the initial distribution over beliefs and observations. Equation (1) defines the relations between random variables:
whilethedistributionoverobservationandanactionisspecifiedbythecurrentbeliefstate,boththepreviousbelief
state and the previous action determine the current belief state.
The transition function p(B |B ,A ) plays a crucial role in active inference because it has to be learned by
t t−1 t−1
the agent. Following the idea of active inference, learning involves updating p(B |B ,A ) through variational
t t−1 t−1
inference, an approximate version of Bayesian inference. Variational inference simplifies Bayesian inference by
restricting the set of possible posterior transition functions to a family {Q (B |B ,A )} , parametrized by
ϕ t t−1 t−1 ϕ
some variable ϕ, thus reducing computational complexity. When a new observation senv is received from the
t
2environment,theapproximateposteriorisobtainedasthesolutiontoanoptimizationproblemwhichminimizesthe
VFE, here conditioned on the specific values b and a for the random variables B and A in the previous
t−1 t−1 t−1 t−1
step:
F = D [q (B |b ,a )||p(B |b ,a )]+E [−log(p(senv|b )], (2)
KL ϕ t t−1 t−1 t t−1 t−1 bt∼qϕ(Bt|bt−1,at−1) t t
(cid:80)
where E [f(b )] = q (b |b ,a )f(b ) represents the expectation value over belief states
bt∼qϕ(Bt|bt−1,at−1) t bt ϕ t t−1 t−1 t
distributed according to the posterior distribution of some function f of b . Minimizing the VFE balances two
t
effects: while the first term penalizes drastic changes in the distribution, the second promotes accuracy in the
model to predict observations coming from the environment. The Kullback-Leibler distance D (•) represents
KL
the dissimilarity between the posterior and prior distributions for the transition function, q (B |b ,a ) and
ϕ t t−1 t−1
p(B |b ,a ) respectively. The second term is the surprise of the current observation. In order to calculate
t t−1 t−1
this surprise, an expectation value is calculated from the posterior distribution q (B |b ,a ) over belief states.
ϕ t t−1 t−1
Realizing that the first term is either larger or equal to zero, the VFE is an upper bound on the surprise raised
by the current input from the environment. By updating its model and minimizing the variational free energy, the
agent therefore minimizes its surprise. Typically, the prior is replaced by the posterior after collecting a number of
observations and implementing the corresponding updates on the posterior.
Sofar,themodelacquisitionhasbeendescribedintermsofperceptualinference: thesystemhasnocontrolover
its actions yet, and constructs the internal model based solely on its observations. The distribution over actions, or
policy π(A |B ), can take any form and is not yet optimized over.
t t
A FEPS agent performs active inference [2] by exploiting the world model that has been learned through
perceptualinferenceinordertoplansequencesoffutureactions. Usingthecurrentworldmodel,theagentestimates
thefuturefreeenergyforeachofitsactionsbasedonthecurrenttransitionfunctionandimplementstheoneaction
that minimizes it. This estimate of the free energy for future states does not have a unique form [37] and different
formulations can lead to different trade-offs between exploration and exploitation. Here, we use the most common
one, denoted in the literature as the Expected Free Energy (EFE):
G [a]=E [logp(b |b ,a)−logpref(s ,b |b ,a)], (3)
bt−1 bt,st∼p(Bt,St|bt−1,a) t t−1 t t t−1
=−H[B |b ,a]+E [Spref(s ,b |b ,a)] (4)
t t−1 st,bt∼p(St,Bt|bt−1,a) t t t−1
wherep(B ,S |b ,a)referstotheworldmodeloverasingletimestep,andthesurpriseofgettingoutcomess and
t t t−1 t
b is denoted Spref(s ,b |b ,a) = −logpref(s ,b |b ,a). H(Y|x) = −(cid:80) p(Y = y|X = x)logp(Y = y|X = x)
t t t t−1 t t t−1 y
stands for the conditional entropy of the random variable Y conditioned on a specific value X =x. It corresponds
to the expected surprise over belief states and is therefore always positive. pref(S ,B |b ,a) is a distribution over
t t t−1
belief states and observations.
The EFE relies on two measures to determine actions: how much uncertainty is expected, and how useful is the
actioninordertofulfillagoal. Inanactiveinferencesetting,anaction’sutilityreferstoitsexpectedcapacitytomeet
some preferences. The first term in Equation (4) is the negative entropy over belief states, related to the expected
informationgainaboutthetransitionfunction: thelargertheentropy,thelargerthegain. Maximizingthisentropy
minimizestheexpectedfreeenergyandleadstoexplorativebehaviors[37]. Finiteentropycanhavetwocauses: (1)
the transition to the next state is still uncertain in the world model, or (2) the transition to the next state in the
environment is stochastic. In the second case, if entropy were used alone, agents could fall in the so-called curiosity
trap [38] by always choosing actions that lead to random outcomes in the environment and inherently result in
high entropy values. The second term reflects the utility gained from taking the action under consideration. When
minimized,thetransitionsareexpectedtomatchthepreferencestoagreaterextent. Therefore,minimizingtheEFE
maximizes the utility an agent expects from its action. For this purpose, a new biased distribution, the preference
distribution pref(S ,B |b ,a), encodes the desirability of some states and observations. The larger the preference
t t t−1
for a state, the larger the probability in the preference distribution, and the smaller the associated surprise. This
second term encourages exploitation of the world model in order to fulfill the preferences. In a biological agent,
these preferences could be of genetic origin (a preference for homeostatic states for example), socially learned (a
preferenceforsometypeofsongsforbirdsofcertainareas,thatwouldbedifferentforthesamespeciesinadifferent
place), acquiredorexternallygiven. Asaresult, theycanencodeasetofstatesandobservationsthatarefavorable
to the survival of the agent. As a modeling choice, preferences cover both belief states and observations, and are
conditioned on the previous state and action.
For the purpose of learning with an artificial agent, the preferences can naturally encode a task to fulfill as a
preferred target state or observation. Choosing the action that minimizes the EFE, the agent selects the transition
in its world model that will bring it as close as possible to its preferred states according to its world model.
Furthermore, by using the full world model over long periods of time, say from 0 to t, as in Equation (1), the long-
termEFEcanbecalculatedtoprojecttstepsaheadintheworldmodelandplanforsequencesoftactions. Finding
3the optimal sequence of actions can rely on tree search with pruning [39], or calculating a long-term, discounted,
expected free energy quantity [40, 41, 29] for example.
To summarize, according to the free energy principle, an agent that adapts to its surroundings can be modeled
as learning a world model of its environment. In this process, the variational free energy is minimized, reflecting
the reduction in the surprise the agent experiences about new sensory events. Active inference prescribes a method
to plan actions in order to reduce this surprise. In other words, actions are chosen to consolidate the world model.
An action is chosen if it minimizes the expected free energy, or equivalently, if it is expected to lead to transitions
associated with high uncertainty and high utility. As noticed already in [37], this can be counter-intuitive at first
glance, since minimizing the VFE and the EFE respectively minimizes and maximizes uncertainty. This paradox
can be resolved by realizing that in active inference, actions target areas in the world model whose outcomes are
leastpredictableinorderlearnaboutthemandtoreceivelesssurprisingoutcomesinthefuture,therebyminimizing
the VFE in the long run.
3 Projective Simulation
Projective Simulation (PS) [31] is a model for embodied reinforcement learning and agency inspired from physics
thatperformsassociativelearningonamemoryencodedasagraph. Itiscomposedofanetworkofvertices,denoted
clips,withadefinedstructurethatgiveseachclipasemanticmeaning,thatcanbeassignedfromthestartoracquired
progressively through past experiences. For example, a clip may represent a sensory state the agent’s sensors are
capable of receiving, or it can inherit supplementary semantics from past experiences and reinforcements. Directed
edges between clips are weighted and can be modified dynamically to learn and adapt to the environment. In
particular,PSallowsthesimulationofpercept-actionloopstorepresentpreviousinteractionswiththeenvironment
andtheassociateddecisionprocess. TheresultinggraphistheEpisodicandCompositionalMemory(ECM).When
a clip is visited, either because it is currently experienced, or because it is used for deliberation, it is excited. After
afirstclipwasexcited,deliberationtakesplaceasatraceablerandomwalkintheECMoriginatingfromthisinitial
clip. It ends when a decoupling criterion is met, which leads to an action on the environment.
For the purpose of solving different environments while retaining interpretability, the ECM can adopt different
structures. In order to imitate a percept-action loop, the ECMs are often structured as bipartite graphs, where a
firstlayercontainsperceptsandthesecondiscomposedofactions[42,43]. Inthiscase,thetrainedECMisdirectly
relatable to a policy. For more complex environments, or in order to extract abstract concepts from the percepts,
an intermediate layer can be added between the percept and action layers [44, 45]. The ECM can either contain a
fixed number of clips, or it can dynamically add and erase some of them when needed [45]. To consider composite
percepts and actions, the ECM graph can be replaced by a hypergraph, where each hyperedge connects a set of
clips to another set [46].
Eachdirectededgeisequippedwithatleastoneattributetotracklearningandallowtheagenttoreactadaptively
to the environment. h-values increase as the edges are rewarded. They encode the strength of associations between
clips that are useful to fulfill some task and record the learning of the agent. The probability of a transition
associated with an edge with h-value h connecting two clips, c →c , is inferred from the h-values:
ij i j
h
p(c |c )= ij . (5)
j i (cid:80) h
k ik
Alternatively, a softmax function can also be implemented to enhance the differences between probabilities, espe-
cially in large ECMs. Upon receiving a new percept, the agent deliberates by taking a random walk through the
ECM, using the probabilities defined from the h-values to sample a new edge.
As the agent modelled with PS interacts with its environment, it receives rewards that are distributed over the
different edges, which changes the corresponding h-values. Specifically, the h-value of the edge c → c is updated
i j
as follows:
ht+1 =ht −γ(ht −h0 )+R (6)
ij ij ij ij
where γ is the forgetting parameter, h0 is the initial h-value for c → c and R is the reward. When the reward
ij i j
is positive, the h-value of the corresponding edge is increased accordingly. If an edge is not visited or does not
receive a positive reward, the corresponding h-value decreases back to their initial value thanks to the forgetting
mechanism in the second term. In order to accept continuous percepts and to unlock generalization on some task,
neuralnetworkshavebeenusedtoupdatetheh-valuesinsomecases. Trainingwasthenimplementedbyminimizing
a loss function [47].
4Projectivesimulationhasbeentestedinmultipletasks,rangingfromthestandardRLtoyenvironments[42,43],
robotics [48], to animal behavior simulations [49, 50] and quantum computations [51, 52]. Extension of the model
include modeling the ECM with a quantum photonic circuit [33] and considering composite concepts in the form
of multiple joint excited clips using hypergraphs [46].
4 The Free Energy Projective Simulation agent
WecombineProjectiveSimulation, withActiveInference, followingthefreeenergyprinciple’sframework. AFEPS
agent is a model-based Projective Simulation agent, where the world model is an ECM with a clone-structured
architecture [53, 54]. Consequently, clone clips inherit the semantics of the unique sensory state they relate to, and
context creates distinctions between hidden states that emit the same observations. As in the FEP, the agent does
notneedexternalrewards. Instead,predictionaccuracy,weightedwithconfidence,isusedasareinforcementsignal.
The world model is directly exploited by the agent to set the edges’ h-values in the policy with active inference.
4.1 Architecture of the agent
TomimicasystemdescribedbytheFEP,theagentiscomposedoftwostructures: theworldmodelandthepolicy,
eachrepresentedbyseparategraphswithverticescorrespondingtorandomvariables,andedgesthatcanbesampled
toperformarandomwalk,asinfigure1. ConsidertheagentcanperceiveN sensorystates,hasN possiblebelief
S B
statesandarepertoireofN actions. Eachstateissupportedbyonevertexinagraph. Theworldmodel’svertices
A
caneithersupportbeliefstatesorsensorystates. Theworldmodelistherepresentationoftheenvironment(seeEq.
1) required by active inference. For reasons that will become clear shortly, the ECM of a FEPS agent is made of
all vertices, that we call clips, that support belief states, and edges that represent transitions between such clips. A
beliefstateisthenformallydefinedbytheexcitationconfigurationofclipsatonestepofthedeliberation. Welimit
the number of excitations in the ECM at any given time to one. Such a vertex deserves the name ”clip” because it
receives an excitation when the corresponding representation of an event is revisited in order to make a decision.
The policy covers all possible conditioned responses coming from any belief state, given the repertoire of actions of
the agent.
In the world model, two sets of edges can be traversed at different times during the deliberation: we denote
them emission and transition edges respectively. They aim at predicting and explaining sensory signals received
from the environment.
In the first set, emission edges b → s relate belief states and sensory states, modeling the latter as parts of
larger,possiblycontextualized,hiddenstatesbyusingclone-structuredHiddenMarkovModels(HMM)[53,55,54].
Each clip is bound to a single observation and is denoted a clone clip. A single edge in the emission set carries
a non-zero probability for each clip, as shown in figure 1a. Consequently, this set of edges defines a deterministic
likelihoodp(s |b )=δ intheworldmodelinEq. (1)andtheagentremainsinitiallyagnosticofthedynamical
t t st,s(bt)
structureofthehiddenstates {e }t intheenvironment. Meanwhile, acloneclip canreadilybe interpretedasan
τ τ=0
augmented description of a specific observation. In particular, the additional information can relate to the cause
or the context of the observation, such as the previous belief state and previous action, for example. Sampling
some sensory state sˆ amounts to predicting the next observation: if it turns out to coincide with the actual
t+1
observation perceived from the environment, a reward will be distributed to the edges contributing to the random
walk that led to this prediction. For clarity, we denote predicted states with hatted low case letters. We choose to
associate each observation to a fixed number N of clone clips, such that N = N ×N . This approach
clones B S clones
works remarkably well for navigation problems [55, 54]. Transferring the dynamics learned on some set of sensory
states to another set is also possible by keeping the transition function unchanged, but redistributing clone clips to
the sensory states in the new set [54].
The purpose of the set of transition edges b −a →t b in the world model is to encode the presumed dynamics
t t+1
in the environment as transitions between belief states, conditioned on actions. They are represented as edges
between clone clips in figure 1a. In contrast to other sets of edges, transition edges are endorsed with attributes
such as h-values that enable learning with reinforcements (as in Eq. 6 for example). Therefore, clone clips together
with transition edges constitute an ECM for the FEPS agent. From a given clone clip, for each action, a set of
edgespointstothenextpossibleestimatedbeliefstates. Theh-valuesofthoseedgesindicatehowcertaintheagent
is that taking a particular action from the current belief state will lead to any of the clone clips in the future.
There can be at most N edges in each such group of transition edges. The full set of transition edges defines
B
the transition function p(B |B ,A ) in the world model in Eq. (1) and corresponds to the trained part of the
t+1 t t
model. Before reinforcement, this distribution is referred to as the prior. The posterior is the updated version of
5Figure 1: Architecture and training of an FEPS agent a) Architecture of a FEPS agent, with four sensory
states (squares) and two possible actions (diamonds). The agent has two main components: the world model and
the policy. The world model is composed of vertices representing observations (squares) while clone clips represent
all values a belief state can take (circles). As in a clone-structured graph, each clone clip b relates to exactly one
observation s and the emission function p(s|b) is deterministic. The clone clips, together with the set of edges
between them, form an ECM. A belief state, circled in purple, is designated by an excited clone clip. The weighted
edges in the ECM encode the transition function and are trainable with reinforcement: there is one set of edges
per action (light and dark turquoise arrows). The belief state in the ECM is an input to the policy, where the
probabilityofsamplinganactionisafunctionoftheEFE.Inturn,theactionthatwasselecteddeterminestheedge
set to sample from in the world model in order to make a prediction for the next belief state and observation. b)
Training of the world model of a FEPS agent. The agent interacts with the environment by receiving observations
and implementing actions. When an action a is chosen, a corresponding edge b −a →t b is sampled in the world
t t t+1
model, from the current to the next belief state, conditioned on the action. The observation s associated with
t+1
the next belief state is the prediction for the next sensory state. Simultaneously, the action is applied to the
environmentandcreatesatransitioninthehiddenstatesoftheenvironment,e −a →t e (bottom,greenrectangle).
t t+1
Thistransitionisperceivedbytheagentthroughtheobservationsenv. Finally,theweightsoftheedgesareupdated.
t+1
The reinforcement of an edge is proportional to the number of correct predictions it enabled in a row, as depicted
with the thickness of the arrows in the world model. When the agent makes an incorrect prediction (the purple
arrow), the reinforcements are applied to the edges that contributed to the trajectory. The last, incorrect, edge is
not reinforced.
thetransitionfunction,aftertheagentdistributedrewardstotherelevanttransitionedges. Theposteriorislabeled
q(B |B ,A ).
t+1 t t
The final component of the agent guides its behavior. The policy π(A |B ) is modeled as a separate graph with
t t
two layers of N and N clips respectively. Given the clone clip corresponding to belief state b is excited in the
B A t
6worldmodel, anactiona issampledtobeappliedtotheenvironment, basedonhowmuchsurpriseitexpectsfrom
t
this decision. Each edge b →a is weighted with the expected free energy G [a ], that defines the policy.
t t bt t
4.2 Reinforcement with prediction accuracy
Each interaction step with the environment involves a deliberation over three states: (1) the next belief state is be
proposed, (2) the next sensory state is predicted and (3) an action is chosen. The agent excites a belief state b
t+1
it believes it will transition to, given its current action a , by sampling a transition edge b −a →t b . From there,
t t t+1
the agent makes a prediction sˆ about the next sensory state. Meanwhile, an action a is selected in the policy
t+1 t+1
and it is applied to the hidden state in the environment, that emits a new observation. The interaction step ends
by comparing the predicted and perceived sensory states, sˆ and senv.
t+1 t+1
Theworldmodelistrainedwithoutexternalrewards,andreinforcementisinsteadbasedonmatchingpredictions
and observations. We call trajectory a sequence of transitions that led to correct predictions about sensory states.
To record the trajectory, transition edges are equipped with a new attribute: the confidence, f. Initialized at zero,
it increases for all transitions in the trajectory every time the prediction sˆ and the actual sensory state senv
t+1 t+1
coincide. The more subsequent predictions an edge enabled, the higher the confidence for that edge: it reflects
the number of correct predictions the edge enabled until the end of the trajectory. Formally, a trajectory τ is a
sequence of transitions whose predictions were confirmed by the observations from the environment. If at step n in
the t-th trajectory τ the sensory prediction was accurate, confidence is enhanced for all edges i→j in τ :
t t
(cid:40)
f(τt),n−1+1 if b →b ∈τ
f(τt),n = ij i j t (7)
ij 0 otherwise.
When the prediction and observation do not match, the trajectory τ is interrupted, and the rewards are
t
distributed to the transition edges’ h-values proportionally to the corresponding confidence:
ht =ht−1−γ(ht−1−h0 )+f(τt)R (8)
ij ij ij ij ij
where ht−1 is the h-value at the end of the previous trajectory, h0 the initial h-value of the edge, and R scales the
ij ij
reinforcement of the edges. Confidence values are reinitialized at zero to start the next trajectory. This mechanism
provides a built-in learning schedule such that the scale of the reinforcement signals grows progressively: rewards
are initially small when trajectories are short, and they become larger when transitions are accurately captured
in the model. During the deliberations, states are sampled from the prior ECM that did not receive the rewards
yet, while the posterior ECM is updated with confidence and rewards at the end of the trajectory. It is equivalent
to sampling states from the prior ECM, but updating the posterior ECM with the rewards R for all edges in the
trajectory every time a prediction was verified by the observation in the environment. Metaphorically speaking,
this mechanism is analogous to layers of snow accumulating in time on salient features. At the end of a trajectory,
the snow is cleared away, bringing all salient points back at an equal level.
To complete the update of the FEPS agent, the policy is modified according to the EFE inferred from the new
world model and can be adjusted to promote an explorative or a conservative behavior. In particular, h-values of
an edge b → a are set to the expected free energy in Equation (4) with a world model conditioned on b and a .
i j i j
Each h-value directly carries the surprise expected from traversing the corresponding edge. As in [6], the policy is
defined using a softmax function:
π(a |b )=softmax(ζ G [a ]). (9)
j i bi j
where ζ is a (real-valued) scaling parameter and G [a ] is the value of the EFE for action a coming from state
bi j j
b . In active inference, ζ is typically negative. When it becomes more negative, actions associated with small EFE
i
receive a large probability. More specifically, looking at the decomposition of the EFE in Equation (4), actions
associatedwithlargerentropies,thatislowercertainty,togetherwithhigherchancesoflandingonpreferablestates
or observations, become more attractive during the deliberation. In contrast, if the scaling parameter is positive,
large EFE yield large probabilities in the policy, and actions with high certainty but also less chances of meeting
preferences are more likely to be sampled. In this case, the agent implements a conservative policy that is confined
to a known region of the environment at the expense of reaching the preferred states. When ζ = 0, the policy is
uniform, and the agent has no bias towards certainty nor utility.
75 Algorithmic features of the FEPS
The FEPS can be augmented with a number of techniques that take advantage of the world model and its inter-
pretability. During learning, the internal model can be leveraged to identify transitions that are instrumental to
gain information or to get closer to a preferred state. Furthermore, the performance of an agent in completing a
task can be enhanced by evaluating the correct belief state accurately and quickly. Since the policy depends on the
EFE, the preference distribution can be tuned according to the task: to seek information to complete the model,
or to complete a given goal. Therefore, we propose to separate training into two phases, depending on how the
preference distribution is constructed. We introduce a belief state estimation scheme that distributes belief states
over multiple clone clips and eliminates those that are incompatible with new observations.
5.1 Leveraging preferences as a learning tool
Sofar,thepreferencedistributionenteringtheEFEwasnotdefined. Onecanoptionallyleverageittodefineagoal
in the environment, be it for the purpose of gaining information, or to solve an actual task. Therefore, we propose
toseparatelearningintotwotasks: modeltheenvironmentandattainagoalinit. Duringthefirstphase,whichwe
denote the wandering phase, the agent explores the environment without a prescribed goal. Instead, actions whose
outcomes are expected to reduce prediction errors should be favorized. This phase spreads over multiple episodes
and relies on interacting with the environment. In contrast to the wandering phase, the second phase is dedicated
to learning to complete a given task. For this purpose, we designed an algorithm to infer a goal-oriented policy
from the world model in a single step and without further information.
5.1.1 Seek information gain about the world model
Before designating any task bound to the environment as a preference, we investigate whether the preference
distribution can be used to incentivize actions that minimize prediction errors, according to the current world
model. ThisisdirectlyrelatedtotheminimizationoftheVFE.Specifically,preferencesshouldencouragetheagent
to seek transitions the world model associates with certainty − or equivalently with high probabilities, irrespective
of actions. Sequences of interaction steps with the environment guided by this preference distribution belong to
the wandering phase. To reflect the preference for highly probable transitions in the world model regardless of the
action chosen, the preference distribution is constructed as the marginal of the world model over actions:
(cid:88)
pref(B ,S |B )= π(A =a|B )p(B |B ,A =a)p(S |B ) (10)
t+1 t+1 t t t t+1 t t t+1 t+1
a
=p(B ,S |B ). (11)
t+1 t+1 t
PluggingthisdistributionintotheexpectedfreeenergyevaluatedforanactionainEq. (4)resultsinthefollowing:
G [a ]=
(cid:88)
p(b ,s |b ,a )log
p(b t+1|b t,a t)
(12)
bt t t+1 t+1 t t (cid:80) π(a|b )p(b |b ,a)p(s |b )
bt+1,st+1 a t t+1 t t+1 t+1
=D [p(B |b ,a )||p(B |b )] (13)
KL t+1 t t t+1 t
=IG(B ,A =a), (14)
t+1 t
where IG(X,Y = y) = D [p(X|Y = y)||p(X)] is the information gain about the random variable X when the
KL
value y for the second random variable Y is known. The dependency on the observations dropped from the first to
second line thanks to the constraints the clone structure imposes on the emission function. A complete derivation
of this formula is provided in Appendix A.
If we follow the conventional formulation of active inference as in section 2, the agent should increase the
probability of sampling an action that minimizes this EFE. Doing so during the wandering phase, the agent would
therefore seek actions it estimates will yield the lowest information gain about belief states. As a result, the agent
would stay in a region of the environment where it predicts it will receive the least surprise, according to its world
model. ThissituationissometimesreferredtoastheDarkRoomproblem[56]: anagentthatadaptsbyminimizing
itssurpriseaboutobservationswouldacttostayinadarkroominsteadofusingactionstoexploreotherplacesthat
maybemoresurprising,butalsomorefavorabletoitssurvival,becauseallobservationstherewouldbepredictable.
There is, however, an easy solution to this problem for FEPS. In order to avoid the dark room problem and to
select actions that are expected to improve the model of the environment, the scaling parameter ζ in Eq. (9) can
be set to a positive value. In this case, the larger the EFE associated to an action − and the estimated information
8gain about the next belief state, the larger the probability of this action in the policy. The scaling parameter ζ can
be understood as a way to determine how greedy an agent is in its exploration, or how strongly the information
gain associated with each action influences its behavior. We call wandering phase the interaction steps in which
the agent samples its actions from such a policy.
5.1.2 Task-oriented behavior by inferring preferences on belief states
At the end of the wandering phase, a task is designated by encoding the associated targets with a high probability
in the preference distribution. From there, the agent can plan, that is sample a sequence of actions to follow to
achieve the goal. While the target is identified as a sensory state for the FEPS, transitions that are useful to reach
itarededucedfromtheworldmodel. Inourframework,updatingthepolicytakesasinglestepanddoesnotrequire
further interaction with the environment.
Though active inference commonly determines the behavior by planning sequences of actions, it becomes ex-
pensive for distant horizons T . A sequence of T actions must be chosen out of NTh possible combinations, by
h h actions
evaluating the EFE in a space of (N B×N S)Th possible outcomes for each sequence. Methods such as habitual tree
searchorsophisticatedinferencehavebeendevelopedtomitigatethisscalingissue[19,41]. Alternativeapproaches
are presented in [6].
Instead of planning by evaluating the generative distribution over all possible future sequences of outcomes,
we propose to encode the long-term value of a state directly into the preference distribution. Our scheme is
reminiscentofsuccessorrepresentation[57,58,59],inthatitestimatesavaluefunctionprovidedsomeexpectations
about occupancies of states in the future, either acquired by experience with reinforcement for example, or by
inverting a learned transition function. In contrast to searching a tree of future sequences of actions, this method
does not rely on mental time traveling [60], to the extent that agents do not simulate possible future scenarios.
Instead, they are “stuck in time”, and infer preferences in one go from stochastic quantities stored in the world
model, in contrast to [20]. Our method also departs from sophisticated inference [41, 29]: instead of bootstrapping
expected free energies in time, we bootstrap preferences over belief states and calculate the EFE only once.
We model the preference distribution to factorize over sensory and belief states [61], and we condition it on the
current belief state b :
t
pref(S =s,B =b|b )=pref(s) pref(b|b ). (15)
t+1 t+1 t t
The first part, pref(s), is an absolute preference distribution over sensory states, that is independent of where the
agent believes it is in the world model. Since the environment is assumed to be partially observable, a target that
is externally given can only be encoded in a shared state space, that is as a sensory state for the agent. More
specifically, if s∗ is the target state for an observation, a probability pref(S = s∗) = p∗ is associated to it. All
t+1
other observations are given a uniform distribution pref(S ̸=s∗)=(1−p∗)/(|S|−1). The second part pref(b|b )
t+1 t
thenreflectslook-aheadpreferencesoverbeliefstatesandreflecthowusefulanagentestimatesatransitiontobein
order to satisfy its absolute preferences. In other words, the utility of belief states over longer horizons is inferred
from the value associated with the observations they can transition to. This way, even if the goal might be far
away in the world model, the preference for the target observation propagates to intermediate belief states that
contribute to reaching it. Metaphorically speaking, the preference for a target observation propagates to the belief
states that are useful to get to that target observation. For example, consider an animal in a maze. The target is
manifested with a high preference towards observing food. However, the preference distribution over the locations
doesnotindicatehowtoreachthefood. Toremedythis, theagentinfersthevalueofbeliefstatesinordertoreach
the target: if a transition to some location brings the animal closer to the food, it is assigned a higher probability
in the preference distribution. This way, the preference distribution highlights the path of relevant actions to the
target.
Weproposeaheuristicalgorithmtoestimatethelook-aheadpreferencedistributionpref(B |b )overtransitions
t+1 t
intheworldmodel. ThealgorithmcanupdatethepolicyK timesifneeded,torefinethepreferencedistribution.
pref
The initial policy π(0)(A |B ) is uniform. During each update k, two quantities are calculated: (1) the look-ahead
t t
preference distribution results from the value of each transition, that estimates how useful a transition is to reach
a target within a prediction horizon T , and (2) the policy π(k+1) is calculated with Equation (9) and the latest
h
preference distribution.
To initialize each update step k, the policy π(k) is used together with the world model to evaluate how easy a
belief state can be reached from the current state, in a distribution we denote reachability:
(cid:88)
r(k)(b |b )= p(b |b ,a)π(k)(a|b ). (16)
t+1 t t+1 t t
a
9a
Thereachabilityofb comingfromb islargeifthereexiststransitionsb −→b associatedwithhighprobabilities
t+1 t t t+1
in the world model and the corresponding actions have high chances to be sampled in the policy.
In addition, the initial value of a belief state equals the value of the observation it corresponds to:
(cid:88)
v (b )= pref(s)p(s|b ). (17)
0 t t
s
For the clone-structured model, this sum reduces to a single term. At this stage, the only belief states associated
with a high value are those that represent a target observation in the absolute preference distribution.
Next, for each iteration n∈[1,T ], where T is a prediction horizon, the value of a belief state is increased if it
h h
can lead to transitions that are useful to reach the target within n steps in the environment:
(cid:26) (cid:27)
v (b )=max v (b ), max{βn−1r(k)(b+|b )v (b+)} , (18)
n t+1 n−1 t+1 t+1 n−1
b+
for 0 ≤ β ≤ 1 some discount factor that makes the value of the state decrease with the number of steps between
this state and the target. At each n, the value of a belief state b can either keep its previous value v (b ),
t+1 n−1 t+1
or it can take the discounted value of the best state b+ it can transition to. As a result, the value of a state can
only increase. If n=1, using this value function can point at the right decision while being one step away from
s∗. However, it does not incentivize the correct action when starting further away from the goal. To mitigate this
effect, a state can inherit the value of the states it can reach over larger time scales n>1, thereby propagating the
preference for the target to more distant but useful belief state states.
When the prediction horizon is reached, a transition b → b is associated with the following probability in
t t+1
the look-ahead preference distribution:
v (b )
pref(k)(b |b )=δ n=Th t+1 (19)
t+1 t bt+1∈ch(bt) (cid:80) v (b′ )
b′ t+1∈ch(bt) n=Th t+1
where the set ch(b )={b+ | r(k)(b+|b )>r(k)(B |b )} contains the children of b , or the states that are easily
t t t+1 t t
reachable from b . r(k)(B |b ) is the mean reachability of belief states coming from state b .
t t+1 t t
Finally,toconcludethek-thupdatestepinthealgorithm,thepolicyπ(k+1) iscalculatedbyusingthepreference
distribution pref(S )pref(k)(B |B ) in the expected free energy, as in Eq. (9). We show a possible look-ahead
t+1 t+1 t
preference distribution in the world model in figure 2.
5.2 Delineate belief states for the same observation
Inspiteofprescribingamethodtosampleactions, activeinferencedoesnotincludetechniquestoefficientlychoose
belief states when multiple of them could explain the current observation. In particular, models involving neural
networks lack the interpretability to design suitable belief state selection methods. Therefore, letting the agent
learn a world model with a clone-structured HMM has advantages beyond planning. We propose a technique to
evaluate belief states in superposition, depicted in Fig. 2. When placed in an environment and receiving its first
observation, the agent makes an initial hypothesis about its belief state by distributing an excitation to any clone
clip compatible with the observation, as shown in Figure 1c. At each step, an action is sampled from the policy for
each excited clone clip. The resulting frequencies define a new distribution, from which an action is sampled before
itisappliedtotheenvironment. Foreachcompatiblecloneclip,theagentsamplesanewcliptorepresentthebelief
stateitanticipatesitwouldtransitiontoifthecloneclipunderconsiderationstandsforthecorrectbeliefstate. The
excitationjumpsontothenewclip. Afterapplyingtheactiontotheenvironmentandreceivingtheresultingsensory
signal, the agent takes away the excitation on any clip that does not match the current observation. Depending on
the structure of the environment and the number of clones for each observation, the agent progressively narrows
down its candidate belief states to a single clone clip, in spite of the initial uncertainty. In the event that the world
model is imperfect and the agent has eliminated excitations on all clips, it starts its hypothesis over, and considers
all the clone clips of its current, unpredicted observation as candidate belief states. This mechanism allows the
agent to evolve in environments with ambiguous observations in the absence of more contextual information about
its initial conditions.
6 Numerical results
In this section, we present a numerical analysis of the model on environments inspired from behavioral psychology
tasks,namelyatimedresponseexperimentinaSkinnerboxandanavigationtasktoforageforfood. Theparameters
10Figure 2: Estimation of belief states in superposition, after the world model has been trained. To
minimizeitspredictionerrorduetofaultybeliefstateestimation,anagentconsidersmultiplecloneclipsascandidate
beliefstatessimultaneously. Fortheinitialobservation,senv (ontheleft),theagentincludesallcorrespondingclone
1
clips {bi}3 to its hypothesis, as depicted on the right. Conditioned on the chosen action, a clone clip is sampled
t i=1
for each candidate belief state to represent the next one. Finally, all clone clips that are incompatible with the
observation from the environment are eliminated from the hypothesis. The clips that remain become the current
candidate belief states. In the world model, the thickness of the arrows represents the look-ahead preferences: the
larger the arrow, the more advantageous is the transition in order to reach the target observation, s in this case.
4
used for the simulations are given in Table 1 in the appendix.
6.1 The timed response task
6.1.1 Learn short-term associations
The timed response task is a minimal environment for an agent to learn to contextualize its observations with
past states and actions when the sensory signals emitted by the environment are ambiguous. This environment
simulates an animal standing in front of a door that can be opened with a lever. The goal is for the agent to learn
a conditioned response and to press a lever at the right time in order to access food. The environment’s MDP is
depicted in figure 3. For this task, the observations combine two sensory inputs: S = { (light off, hungry), (light
off, satiated), (light on, hungry)}. Since food can be consumed only when the light is off, the observation (light
on, satiated) in excluded from the set. The actions are A= {wait, press the lever}. The environment is initialized
in the (hidden) state E , that emits observation (light off, hungry). From there, the light turns on, regardless of
0
the action taken by the agent. Once the light has turned on, the agent must learn to wait one step before pressing
the lever. If it does so too early, it gets back to the initial state. If the agent activates the lever on time, the
environment transitions to state E′, where the light is off, but the agent is satiated.
0
Forthesimulations,wegiveeachobservationtwoclones. Itmakesitpossibletoaccommodateenoughcandidate
belief states to model up to two ambiguous hidden states that emit the same sensory signal, enabling the agent to
adapt its policy to a one-step waiting time between the light turning on and the food being accessible for example.
When observations are not ambiguous and can be emitted only by a single hidden state in the environment, some
belief states become redundant. This redundancy can make the training more challenging to the extent that the
agent has to find a convention and adapt its model accordingly before it can account for all transitions in the
environment faithfully. The larger the waiting time n, the more clones might be necessary to learn. We train 100
agents for 4000 episodes of 80 steps in this environment and we test two scenarios. In the first, the agent is directly
giventhepreferenceforthetargetobservation(lightoff,satiated). TheEFEisthenscaledwithascalingparameter
of ζ = −1 in the policy in Eq. (9). In the second scenario, we test whether the agent can learn more efficiently if
it wanders aimlessly through the environment for the same number of episodes without preference for the target
before adapting its policy to the task, with a scaling parameter of 0.
11Figure 3: MDP for the timed response environment This environment has four hidden states. The obser-
vations are compositional and contain information that are both external (light on or off) and internal (hungry or
satiated)totheagent. Arrowscorrespondtothetransitionstheactionstheagentcanresultin. Inthisenvironment,
the agent can either wait or press a lever. In order to complete the task, the agent must reach E′ and feel satiated.
0
The only way to do so is to follow the actions marked with thicker arrows. The observation (light on, hungry) is
called ambiguous because it can be emitted by two hidden states E and E that can only be distinguished with
1 2
context.
6.1.2 Simulation results
The timed response environment is a minimal testbed for the FEPS, where some hidden states are not uniquely
defined by the observation they emit, but also by the recent past. The agent must learn two types of belief states.
While clones for (light off, hungry) and (light off, satiated) only support transitions that are independent of the
actions the agent takes, the clones for (light on, hungry) must appropriately use information about the previous
observation and action to be contextualized and distinguished. Some results are reported in figure 4.
During training, regardless of the strategy that the agent uses to resolve the environment, all agents follow a
similar learning pattern. The acquisition of the world model happens in stages, where the transition from one to
the next is manifested in a steep increase in the length of the trajectories of correct predictions, or equivalently, as
a steep decrease in the free energy, as in figure 4a. First, agents quickly eliminate transitions that are impossible
in the environment, leading to an initial drop in the VFE. For example, as in figure 3, a direct transition from
(light off, hungry) to (light off, satiated) is incompatible with the timed response task. During the second phase,
the number of rewards the agent can collect is limited by the absence of convention on the context-sensitive belief
states. Therefore, a plateau is observed on the VFE. A final sharp decrease in the VFE signals the adoption of
a convention between clones to accurately disentangle ambiguous hidden states that cannot be told apart with
observations alone. The EFE evolves as expected during the training: it decreases for the task-oriented agents,
whileitrapidlyplateaustoitslimit(seeAppendixB)inwithawanderingphase. However,incontrasttotheVFE,
the convergence of the EFE to its asymptote value does not reflect the fact that the model is good enough to make
accurate predictions.
Formostagents, trainingwithorwithoutapreferenceforthetargetinordertoconstructtheworldmodeldoes
notinfluencethefinalmodel. Asinfigure4a, thetwobestagents, thatis, thosethatconvergetofreeenergyvalues
below 1 the earliest, share similar learning curves. The averaged behaviors remain fairly identical, except for the
length of the second stage of the learning phase, where the agents have yet to adopt a successful convention. In
particular, in the absence of aimless experimentation with the environment, this second phase can last longer, such
12Figure 4: Training FEPS agents for the timed response task a) Evolution of the variational free energy
(top)definedinEq.(2)andexpectedfreeenergyasinEq. 4(bottom)duringthetraining,averagedover100agents
and a time window of 100 episodes. At each step, the VFE depends on the specific belief states and actions that
were sampled. Two types of training are compared: a first set, “task” in dark purple, learned the model with a
preference to fulfill a task in the environment, while the second set, “wander”, in green, experimented aimlessly
in the environment with a uniform policy before switching to the task. Both were trained for 4000 steps before
being tested on the task. The best and worst agents are represented with dashed and dotted lines respectively, and
examples of individual agents were traced with transparent lines, full for task-oriented agents, and dashed for the
wandering ones. When the VFE converges to its minimal value, the world model is precise enough for most belief
states to make planning possible. As expected from the values chosen for the scaling parameter ζ, agents select
actions that minimize and maximize the EFE for task-oriented and wandering agents respectively. The EFE of
wanderingagentsplateausquicklyatthelimitderivedforAppendixB.b)Worldmodellearnedbyoneoftheagents
trained on the task, where each circle is a belief state, whose observation is denoted by its colors and label. The
numbersatthecenterofthecirclesarethecloneindicesforeachcloneclip. Arrowsindicatethetransitionslearned
intheworldmodel,redforaction“pressthelever”,andbluefor“waiting”. Dashedlinesindicatethatbothactions
lead to the same transition. The weight on the arrow indicates its probability in the world model. Stars mark
transitions that were identified as useful to achieve the goal with a probability of 1 in the preference distribution.
The policy is indicated by the thickness of the arrows, where a thick arrow corresponds to probabilities close to 1,
and thinner close to 0.5.
that convergence is not attained for a fraction of the agents trained in this way.
After convergence of the free energy, or equivalently of the number of successful predictions, the model has
collapsed on a single representation of the environment, where the ambiguous observation is contextualized by the
previous belief state and action. For example, the observation (light on, hungry) is divided into two belief states
with different uses, reflecting the two hidden states in the environment. The corresponding clone clips bear more
information than the observation to which they are linked. In figure 4b, clone 2 for (light on, hungry) is the
first hidden state encountered when the light turns on, and clone 1 can only be accessed by waiting from clone 2.
When more clones are provided than necessary, two possibilities can arise: either the agent uses all the clones as
duplicates of the same hidden state in the environment (as for clones of (light off, hungry)), or a single clone is
reachable (clone 2 of (light off, satiated)) and the other is never trained because it was not visited. When multiple
clone clips participate equally in the representation of the observation, the actions that lead to it also split with
equal probability (as derived in Appendix B).
Finally, for the agents that converged to an appropriate representation of the environment, the look-ahead
preferences inferred from the world model result in an optimal policy when using the EFE. A visualization of the
preferences and policy is provided in figure 4. We chose a prediction horizon of 2 steps, and a single iteration
13Figure 5: Training results for the grid world environment. a) Evolution of the length of the trajectories
during the training, for different scaling parameters ranging from -3 to 3, and different preference distributions:
the agent can either learn to complete the task from the start (“task”), or first wander in the grid (“wander”).
We represent the running averages over a time window of 1500 steps of the lengths of trajectories, averaged over
30 agents. These lengths depend on the specific belief states and actions sampled by the agents. b) Evolution
of the variational and expected free energies during the training for the two best settings in a): the task-oriented
preferences are paired with a scaling parameter of -3, and the wandering preferences with a parameter of +1. The
thick lines represent the running average of energies over a time window of 1500 steps, averaged over all 30 agents,
while the dotted and dashed lines stand for the best and worst agents, that is the agents whose VFE converge
first and last, respectively, to a minimum. The transparent lines indicate the behavior of agents selected randomly:
these lines are full for task-oriented agents and dashed for wandering agents. c) Comparison of the accuracy of the
model to make predictions between the two best parameter settings in a). The policies of the agents are uniform,
suchthattheactionsyieldingmostcertainoutcomescannotbereliedupontovalidatepredictions. Twobeliefstate
estimation techniques are tested. Bare estimation samples a single clone clip at a time, whereas the evaluation of
belief states in superposition allows multiple clone clips to represent candidate belief states simultaneously, as long
as they produce predictions that are compatible with the next observation. Each individual agent is tested over
1000 trajectories of at most 80 steps.
was required to calculate appropriate look-ahead preferences for the transitions. The transitions with maximal
preference were kept so that, coming from a belief state b , a single belief state b∗ is more preferable than the
t
others. The agents were tested for 1000 rounds, each starting in E in the environment. For a prediction horizon
0
of a single step, the edge between clone 1 of (light on, hungry) and clone 2 of (light off, satiated) would be the only
edge to carry a probability larger than other transitions in the preference distribution. Thanks to the look-ahead
preferences (and the adoption of a convention between belief states), waiting between clones 1 and 2 of (light on,
hungry) is also preferred. The policy that results from this preference distribution is optimal to solve the task, in
spite of initially having no hints about the target prior to the last transition.
146.2 Navigation task in a partially observable grid
6.2.1 Long-term planning in an ambiguous environment with symmetry
TheFEPSisfurtherchallengedinalargernavigationtask,whereobservationsaresharedamonghiddenstates,and
multiple sequences of actions can emit the same observations, due to the symmetry of the environment. In order to
disentangle the hidden states, the agent must use long-term information about its past observations and actions to
contextualize its current state in a way that is consistent across actions. In this environment, food is hidden in one
position in a grid. Locations in a 3 by 3 grid world are associated with smell intensities, denoted by their integer
intensities from 0 to 3 from the lower left to the upper right corners, according to their closeness to the food, and
represented by increasingly warm colors in figure 6a.
Theworldmodelisprovidedwith3clonesforeachofthefourobservations,andthebehaviorrepertoirecomprises
directional actions A = {go right, go left, go up, go down}. 30 agents were trained on different hyperparameters
and preferences, for a total of 40000 episodes of 80 steps. The hyperparameters are provided in the appendix.
Sincethisenvironmentislargerandmorecomplex,wetestdifferenttrainingtechniques,bychangingthepreference
distributionsandvaryingthescalingparameter. Inparticular,wetesttwoscenarios. Inthefirsttheagentistrained
withpreferencespointingatatargetintheenvironment,whileinthesecond,thepreferencesareidentifiedwiththe
marginal of the world model over actions, which incentivizes dissociating the effect of actions on the environment.
While in the first case, the agent tries to directly solve the task, in the second, the agent wanders first before it
learns the task. For each scenario, different scaling parameters were implemented, ranging from -3 to 3 for the
task-oriented training, and from 0 to 3 with a wandering phase. Note that choosing a scaling parameter of 0 while
training either directly on the task or with a preceding wandering phase results in the same uniform policy.
6.2.2 Simulation results
Training task-oriented and wandering agents. Compared to agents trained with a uniform policy, agents
using policies inferred from the EFE can achieve longer trajectories. This is apparent in figure 5a), where with a
scalingparameterof0,thetrajectoriesofagentsreachalengthof60onaveragebytheendofthetraining,whereas
given a proper tuning, agents using the EFE to define the policy can predict up to 69 transitions, regardless of the
trainingmethod. Adaptingthescalingparametertothepreferencedistributioniscrucialfortheagentstolearnthe
modelandqueryactionsthatresultinlongertrajectories. Fortask-orientedagents,asexpectedfromEquation(4),
anegativescalingparameterat-3isoptimal,whereas+1yieldsthebestresultsforthewanderingagents. Forboth
trainingmethods, settingitat+3resultsintheshortesttrajectories, thatare28and50stepslong, respectively. In
the case of task-oriented agents, a possible explanation is that in this regime and when the preference distribution
designates a target in the environment, the policy that is derived from the EFE in Eq. (4) minimizes the drive
of the agent to explore and to fulfill any preferences in the environment. Therefore, the agent has no incentive
to go outside of a region of the environment where it can predict its observations, and does not try to learn the
rest of it. In contrast, a scaling parameter ζ = +3 makes wandering agents very greedy. It is then possible that
previously explored regions are not visited often enough to reinforce the correct associations in the world model in
the long-run. Finally, our simulation shows that for the best parameter settings, the task-oriented agents converge
on average faster than the wandering ones: while 17500 episodes are needed to predict 65 steps for the former, the
latter crosses this milestone at 28000 episodes.
Looking at the evolution of the free energy during the training in figure 5b), one sees that in both cases,
the VFE decreases as the length of trajectories increases. It looks as if agents minimize their free energy by
maintaining a world model and improving their predictions about future sensory states. This matches the free
energy principle. Comparing individual learning curves, wandering agents have more diverse behaviors than those
trained on the task. We define the best and worst agents as those converging the earliest and the latest to a free
energyminimum, respectively. Thebestwanderingagentscanconvergefasterthanthetask-orientedones, whereas
the worst wandering agent lands on a higher free energy by the end of the training. However, the free energy of
the worst wandering agent either goes down or plateaus, but it does not rise again, as for the worst agent directly
trained on the task. When the training scenarios influence the scale of the EFE, no significant variations around
the average behavior is observed in the EFE to distinguish successful agents from those that did not converge. In
particular, the EFE wandering agents converge very quickly to its asymptotic value, regardless of the accuracy of
the world model.
In order to fairly compare the models obtained from each training method, note that whenever the length of
trajectories or the free energy are optimized, two contributions are at play. On the one hand, training the world
modeltocapturetheenvironmentmoreaccuratelydecreasestheuncertaintyaboutthetransitionsfollowingactions,
and therefore improves the length of the trajectories and the free energy. But on the other hand, as long as the
15policy deviates from the uniform distribution, it influences how much uncertainty the agents seek. Therefore, in
order to evaluate how much the world model helps making accurate predictions, we test the length of trajectories
with a uniform policy for the agents trained in the two best parameter and preference settings. The trajectory
lengths averaged over all agents trained with the respective parameters are provided in figure 5c).
In spite of enabling faster convergence with less deviations in the final free energies, learning with task-oriented
preferences deteriorates the prediction abilities to some extent compared to wandering first. Indeed, when tested
with a bare belief estimation technique, where the agent can entertain a single belief state at a time, the median
length of trajectories is slightly higher for the wandering agents, and fewer deviations are observed between these
agents: while 50% of the wandering agents can predict between 36 and 40 steps without mistake, this range
expands to the interval between 25 and 39 steps for the task-oriented agents. Best and worst behaviors remain
similar between both training techniques.
Estimating belief states in superposition doubles the length of trajectories for both training settings, with a
stronger improvement for wandering agents. Indeed, the length of trajectories is increased to 76 steps for all wan-
dering agents whose model converged. While the length of trajectories is doubled for all task-oriented agents, large
discrepancies are noticeable by the spread of the inter-quartile space ranging from 47 to 73 steps. We expect that
if the task-oriented agents were trained with 0 as a target observation instead of 3, the ambiguity of the lower left
corner of the grid would not be lifted, and the length of the trajectories would decrease. This would also affect the
ability of the agents to adapt to different goals in the environment.
Exploit the world model to complete tasks flexibly. After the free energy converges, the belief states
in the world model no longer stand only for the observation they relate to, but also convey information about the
path they can appear in. As an example, the world model of an agent trained directly on the task is represented
in figure 6a). Because the environment the agent was trained in is deterministic, actions tend to map onto a single
belief state when the number of clones for an observation matches the number of hidden states in the environment.
In particular, in spite of being agnostic of the nature of the environment, the world model can be interpreted
as a topological map of the grid, as pictured in figure 6b). Each clone maps onto a single location of the grid,
and this mapping is consistent for all actions. When too many clones were provided compared to the degeneracy
of an observation, multiple clones are associated with a single location, as for observation 3 in the upper right
corner of the grid. This consistent mapping suggests that the agent is able to contextualize the observations with
a sequence of previous states and actions. Indeed, before adopting a convention, starting from the bottom left
corner,theagentcannotdistinguishbetweengoinguporgoingrightbasedonlyonitsobservations,becauseoftheir
symmetric distribution in the grid. Instead, using long-term context, the two paths can be identified and carried
over by different clones. Thanks to this mapping of clones onto single locations, the look-ahead preferences over
belief states encode paths ending at the target. They steer the policy toward an optimal one.
Provided the target observation is signaled in the preference distribution, the agents can adapt their policy to
reach it following near optimal trajectories. In figure 6c), agents whose model successfully converged are tested to
reach targets with observations 3 (top, red) and 0 (bottom, blue), requiring opposite policies to be attained. For
both tasks, irrespective of the initial location of the agent, the performance is drastically improved compared to
a random agent with uniform policy. In order to move the target, no other interaction with the environment was
required than changing the preference distribution over sensory states, and no new observation was exchanged.
While the paths towards observation 0 are mostly optimal, there is an overhead by at most one step to reach
observation3. Sincethepolicyisanoptimalone,asshowninfigure6b),wesuggestthatthisisduetotheestimation
of the belief states. In particular, the small size of the grid prevents agents from narrowing down their hypothesis
to a single belief state early enough to opt for the right policy at the boundary of the grid. This could explain why
on average, agents take 1.5 steps to reach the target when being right under it: they have 0.5 probability of finding
the wrong belief state, and therefore choose the wrong action 50% of the time. This eliminates the last erroneous
state from the hypothesis and the agent behaves optimally in the next step.
7 Discussion and outlook
Inthiswork, weproposeaninterpretable, minimalmodelforcognitiveprocessesthatcombinesassociativelearning
and active inference to adapt to any environment, without any prior knowledge about it and independently of
any goal tied to it. We develop the Free Energy Projective Simulation model, a physics-inspired model that seeks
inspiration in current paradigms in cognitive sciences, namely active inference, that encompasses the Bayesian
brain hypothesis (BBH) and fits more broadly into predictive processing. A FEPS agent is equipped with a
memorystructuredasagraph,theECM,wheretheagentencodesassociationsbetweeneventsrepresentedasclone
16Figure 6: Model and robustness to reward reevaluation for the grid world environment. a) Example of
a world model learned by an agent directly trained on the task, with the target positioned in the top right corner
of the grid. The circles represent the belief states as in fig.1, numbered with clone indices, and colored according
to the observation they relate to in the grid. The arrows stand for the transition probabilities: the thicker the
arrow, the more the agent believes taking an action from a belief state will lead to the state the arrow points at.
b) Mapping of the world model in a) onto the grid: each clone is associated with exactly one cell, that is, a single
hidden state in the environment. Stars stand for the preferred transitions, and the arrows for the policy resulting
fromthesepreferences. c)Mediannumberofstepsfromeachinitialpositioninthegridinordertoreachthetarget,
where belief states are estimated in superposition. Two targets are given to the agent and symbolized with a gray
triangle: reach observation of 3 (top right, red) and 0 (bottom left, blue), each requiring opposite policies. When
targetswereswapped, nointeractionwiththeenvironmentwasnecessarytore-evaluatethevalueofthetransitions
and the resulting policy. The median time to target is compared to that of a random agent with uniform policy
and tested with the same procedure.
clips. As in models of associative learning in the cerebellum [62, 63, 64], internal reinforcement signals based on
the prediction accuracy of the agent reinforce associations between observations. Clones clips are the support for
belief states and acquire contextual meaning as the agent collects longer sequences of correct predictions about its
observations and improves its world model. The behavior of FEPS agents does not depend on any reinforcement
and is fully determined with active inference, such that the policy optimizes the expected free energy estimated
for each action, given the current world model. We perform numerical analysis of the agents in two environments
inspired from behavioral psychology: a timed response environment, as well as a navigation task in an partially
observable grid world.
Leveraging the interpretability of the model, we identified three capacities a FEPS agent requires in order to
interact with an environment and to reach a goal while controlling it, that is, minimizing its surprise about future
events. (1) The representation of the environment in the world model should be accurate enough in its predictions,
i.e. itshouldbecapableofpredictingfuturesensorysignals. (2)Theagentshouldbeabletochoosetheappropriate
belief state for aliased observations. (3) For a given time horizon, the agent should be equipped with an efficient
mechanism to plan its course of actions.
First, in order to build a complete and accurate representation of the environment, we propose to start learning
with a wandering phase. It focuses on exploring the environment strategically rather than completing a task. For
17this, we provide a model-dependent preference distribution that evolves as the agent learns and we show that the
corresponding expected free energy of an action is equal to the information gain about belief states resulting from
this action. As a result, the behavior of the agent is motivated by the acquisition of information in its world model
rather than by a target bound to the environment. Our numerical simulations showed that a model learned in
such a way is able to predict longer sequences of observations when actions are selected at random, compared to
the model trained directly on a task. Previous works usually involve either adopting a different definition for the
expectedfreeenergy[37],ormorecommonly,addingtermstoittoencourageexplorativebehaviors,[20],leveraging
existing literature on curiosity mechanisms, for example [30, 36, 65, 38]. Alternatively, a count-based boredom
mechanism [66] could be well-suited for FEPS, as it could be directly implemented on the edge attributes, such as
the confidence, to deviate the agent from transitions it has already resolved.
Second,wedesignasimpleproceduretoprogressivelydisambiguatebeliefstateswithsequencesofobservations.
It can be used to interact with the environment after the model has been trained. Leveraging the clone structure
imposed to the clips in the memory of the agent, belief states can be estimated in superposition. As predictions
for each candidate belief state are validated or not by the observation collected from the environment, the agent
can eliminate candidate belief states that are incompatible with the context provided by its current sensory state.
We show in our numerical analysis that this simple mechanism allows to double the number of correct, consecutive
predictions, regardless of the technique used to learn the world model.
Thirdly, we introduce a planning method based on the expected free energy. Instead of relying on a tree search
or a simulation of future scenarios, we propose to encode the utility of a transition between two belief states in the
preference distribution. For this, we factorize the preference distribution into an absolute preference distribution
that designates a target among the possible sensory states, and a look-ahead preference distribution. The latter
assignsaprobabilitytotransitionsbetweenbeliefstatesthatiscommensuratewithitsestimatedutilityinreaching
thetargetwithinagivennumberofactions. Usingtheworldmodel,thevalueforeachtransitionisdeterminedinan
iterative manner. We tested this scheme in two numerical experiments and achieved optimal policies in both cases,
as long as the world model was predictive enough. The closest model to our knowledge is Successor Representation
[59, 58, 57], that has been hypothesized to account for so-called model-free learning in cognitive systems [58]. A
major difference is that in Successor Representation, the value of a transition depends on the prediction error
over the expected reward in the environment, whereas we assign value to a belief state via the probability of the
associatedsensorystateintheabsolutepreferencedistribution. Alimitationofourscheme,however,isthatatarget
can only be encoded as a possibly ambiguous observation. Using reinforcement and a few interactions with the
environment,thepreferenceforaparticularhiddenstatecouldbeencodedinthelook-aheadpreferencedistribution
in a hybrid scheme.
Conceptually, the FEPS framework fits into the field of NeuroAI [67], at the intersection of behavioral sciences,
engineering and neurosciences. While Projective Simulation can be used to learn in an artificial environment, its
vocationistounderstandagencyandthebehaviorofagentsintheworld. Furthermore, theFEPSattemptstogive
a biologically plausible account of learning and adaptive behavior, grounding internal computations in the active
inference framework and the predictive processing paradigm. ECMs are potentially implementable on physical
platformsandcanbeconsideredembodiedstructuresunderlyingthememoryoftheagents. Forexample,aparallel
can be drawn between the role the network of belief states plays for FEPS agents, and that of place cells and grid
cells in the hippocampus [54, 55, 68]. Both integrate stimuli to create a contextualized representation of an event
in order to make predictions about future stimuli. For the FEPS to provide a modeling platform of interest for
cognitive and behavioral sciences, the next challenge is to implement learning on real-world tasks and in a fully
embodied way, including the calculation of coincidences between predictions and observations, and the update of
the associations between states.
8 Acknowledgments
This research was funded in whole or in part by the Austrian Science Fund (FWF) [SFB BeyondC F7102,
10.55776/F71]. For open access purposes, the author has applied a CC BY public copyright license to any author
accepted manuscript version arising from this submission. We gratefully acknowledge support from the European
Union (ERC Advanced Grant, QuantAI, No. 101055129). The views and opinions expressed in this article are
however those of the author(s) only and do not necessarily reflect those of the European Union or the European
Research Council - neither the European Union nor the granting authority can be held responsible for them.
18References
[1] R. Sutton and A. Barto, Reinforcement Learning: An Introduction. A Bradford book, MIT Press, 1998.
[2] M. Kirchhoff, T. Parr, E. Palacios, K. Friston, and J. Kiverstein, “The markov blankets of life: Autonomy,
active inference and the free energy principle,” Journal of the Royal Society Interface, vol. 15, 1 2018.
[3] A. Linson, A. Clark, S. Ramamoorthy, and K. Friston, “The Active Inference Approach to Ecological Percep-
tion: General Information Dynamics for Natural and Artificial Embodied Cognition,” Frontiers in Robotics
and AI, vol. 5, p. 21, Mar. 2018.
[4] G. Pezzulo, F. Rigoli, and K. Friston, “Active Inference, homeostatic regulation and adaptive behavioural
control,” Progress in Neurobiology, vol. 134, pp. 17–35, Nov. 2015.
[5] V. Raja, D. Valluri, E. Baggs, A. Chemero, and M. L. Anderson, “The Markov blanket trick: On the scope of
the free energy principle and active inference,” Physics of Life Reviews, vol. 39, pp. 49–72, Dec. 2021.
[6] P. Mazzaglia, T. Verbelen, O. C¸atal, and B. Dhoedt, “The free energy principle for perception and action: A
deep learning perspective,” Entropy, vol. 24, 2022.
[7] A. Tschantz, B. Millidge, A. K. Seth, and C. L. Buckley, “Reinforcement learning through active inference,”
2020. arXiv:2002.12636.
[8] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, J. O’Doherty, and G. Pezzulo, “Active inference and
learning,” Neuroscience & Biobehavioral Reviews, vol. 68, pp. 862–879, Sept. 2016.
[9] A. Bubic, D. Y. Von Cramon, and R. I. Schubotz, “Prediction, cognition and the brain,” Frontiers in Human
Neuroscience, vol. 4, Mar. 2010. Publisher: Frontiers.
[10] I. Vilares and K. Kording, “Bayesian models: the structure of the world, uncertainty, behavior, and
the brain,” Annals of the New York Academy of Sciences, vol. 1224, no. 1, pp. 22–39, 2011. eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1749-6632.2011.05965.x.
[11] K.Friston,“Thefree-energyprinciple: aunifiedbraintheory?,”NatureReviewsNeuroscience,vol.11,pp.127–
138, Feb. 2010.
[12] K. Friston and S. Kiebel, “Predictive coding under the free-energy principle,” Philosophical Transactions of
the Royal Society B: Biological Sciences, vol. 364, pp. 1211–1221, May 2009. Publisher: Royal Society.
[13] G. Pezzulo, L. D’Amato, F. Mannella, M. Priorelli, T. Van De Maele, I. P. Stoianov, and K. Friston, “Neural
representationinactiveinference: Usinggenerativemodelstointeractwith—andunderstand—thelivedworld,”
Annals of the New York Academy of Sciences, vol. 1534, pp. 45–68, Apr. 2024.
[14] M.Cullen,B.Davey,K.J.Friston,andR.J.Moran,“Activeinferenceinopenaigym: Aparadigmforcomputa-
tionalinvestigationsintopsychiatricillness,”BiologicalPsychiatry: CognitiveNeuroscienceandNeuroimaging,
vol. 3, pp. 809–818, 9 2018.
[15] H. T. McGovern, A. De Foe, H. Biddell, P. Leptourgos, P. Corlett, K. Bandara, and B. T. Hutchinson,
“Learned uncertainty: The free energy principle in anxiety,” Frontiers in Psychology, vol. 13, Sept. 2022.
Publisher: Frontiers.
[16] M. J. D. Ramstead, P. B. Badcock, and K. J. Friston, “Answering Schr¨odinger’s question: A free-energy
formulation,” Physics of Life Reviews, vol. 24, pp. 1–16, Mar. 2018.
[17] C. Heins, B. Millidge, L. da Costa, R. Mann, K. Friston, and I. Couzin, “Collective behavior from sur-
prise minimization,” Proceedings of the National Academy of Sciences, vol. 121, p. e2320239121, Apr. 2024.
arXiv:2307.14804 [nlin, q-bio].
[18] P. Mazzaglia, T. Verbelen, and B. Dhoedt, “Contrastive Active Inference,” in Advances in Neural Information
Processing Systems, vol. 34, pp. 13870–13882, Curran Associates, Inc., 2021.
[19] Z. Fountas, N. Sajid, P. Mediano, and K. Friston, “Deep active inference agents using Monte-Carlo methods,”
inAdvancesinNeuralInformationProcessingSystems,vol.33,pp.11662–11675,CurranAssociates,Inc.,2020.
19[20] V. D. Nguyen, Z. Yang, C. L. Buckley, and A. Ororbia, “R-AIF: Solving Sparse-Reward Robotic Tasks from
Pixels with Active Inference and World Models,” Sept. 2024. arXiv:2409.14216.
[21] A. Tschantz, B. Millidge, A. K. Seth, and C. L. Buckley, “Reinforcement Learning through Active Inference,”
2020.
[22] D. de Tinguy, T. V. de Maele, T. Verbelen, and B. Dhoedt, “Spatial and temporal hierarchy for autonomous
navigation using active inference in minigrid environment,” Entropy, vol. 26, p. 83, 1 2024.
[23] A. Clark, “Whatever next? Predictive brains, situated agents, and the future of cognitive science,” Behavioral
and Brain Sciences, vol. 36, pp. 181–204, June 2013.
[24] A. Clark, “How to Knit Your Own Markov Blanket:: Resisting the Second Law with Metamorphic Minds,”
Philosophy and Predictive Processing, 2017. Publisher: Theoretical Philosophy/MIND Group – JGU Mainz.
[25] B. J. Kagan, A. C. Kitchen, N. T. Tran, F. Habibollahi, M. Khajehnejad, B. J. Parker, A. Bhat, B. Rollo,
A. Razi, and K. J. Friston, “In vitro neurons learn and exhibit sentience when embodied in a simulated
game-world,” Neuron, vol. 110, pp. 3952–3969.e8, Dec. 2022.
[26] R. Smith, P. Badcock, and K. J. Friston, “Recent advances in the application of predictive coding and active
inference models within clinical neuroscience,” Psychiatry and Clinical Neurosciences, vol. 75, no. 1, pp. 3–13,
2021. eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/pcn.13138.
[27] P. Lanillos, C. Meo, C. Pezzato, A. A. Meera, M. Baioumy, W. Ohata, A. Tschantz, B. Millidge, M. Wisse,
C.L.Buckley,andJ.Tani,“Activeinferenceinroboticsandartificialagents: Surveyandchallenges,”122021.
arXiv:2112.01871.
[28] T.Piriyakulkij,V.Kuleshov,andK.Ellis,“Activepreferenceinferenceusinglanguagemodelsandprobabilistic
resasoning,” Dec. 2023. arXiv:2312.12009.
[29] D.Kawahara,S.Ozeki,andI.Mizuuchi,“Acuriosityalgorithmforrobotsbasedonthefreeenergyprinciple,”in
2022 IEEE/SICE International Symposium on System Integration, SII 2022, pp. 53–59, Institute of Electrical
and Electronics Engineers Inc., 2022.
[30] T. J. Tinker, K. Doya, and J. Tani, “Intrinsic rewards for exploration without harm from observational noise:
A simulation study based on the free energy principle,” 5 2024. arXiv:2405.07473.
[31] H. J. Briegel and G. D. L. Cuevas, “Projective simulation for artificial intelligence,” Scientific Reports, vol. 2,
2012.
[32] B. Eva, K. Ried, T. Mu¨ller, and H. J. Briegel, “How a Minimal Learning Agent can Infer the Existence of
Unobserved Variables in a Complex Environment,” Minds and Machines, vol. 33, pp. 185–219, Mar. 2023.
[33] F. Flamini, M. Krumm, L. J. Fiderer, T. Mu¨ller, and H. J. Briegel, “Towards interpretable quantum machine
learning via single-photon quantum walks,” Oct. 2023. arXiv:2301.13669.
[34] D. Ha and J. Schmidhuber, “World models,” CoRR, vol. abs/1803.10122, 2018.
[35] R.Mendonca,O.Rybkin,K.Daniilidis,D.Hafner,andD.Pathak,“DiscoveringandAchievingGoalsviaWorld
Models,” in Advances in Neural Information Processing Systems, vol. 34, pp. 24379–24391, Curran Associates,
Inc., 2021.
[36] P.-Y. Oudeyer and F. Kaplan, “What is Intrinsic Motivation? A Typology of Computational Approaches,”
Frontiers in Neurorobotics, vol. 1, p. 6, Nov. 2007.
[37] B. Millidge, A. Tschantz, and C. L. Buckley, “Whence the Expected Free Energy?,” Sept. 2020.
arXiv:2004.08128.
[38] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-Driven Exploration by Self-Supervised Predic-
tion,” in 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), (Hon-
olulu, HI, USA), pp. 488–489, IEEE, July 2017.
[39] L.DaCosta,T.Parr,N.Sajid,S.Veselic,V.Neacsu,andK.Friston,“Activeinferenceondiscretestate-spaces:
A synthesis,” Journal of Mathematical Psychology, vol. 99, p. 102447, Dec. 2020.
20[40] A. Paul, N. Sajid, L. Da Costa, and A. Razi, “On efficient computation in active inference,” Expert Systems
with Applications, vol. 253, p. 124315, Nov. 2024.
[41] K. Friston, L. Da Costa, D. Hafner, C. Hesp, and T. Parr, “Sophisticated Inference,” Neural Computation,
vol. 33, pp. 713–763, Mar. 2021.
[42] A. A. Melnikov, A. Makmal, and H. J. Briegel, “Projective simulation applied to the grid-world and the
mountain-car problem,” May 2014. arXiv:1405.5459.
[43] J. Mautner, A. Makmal, D. Manzano, M. Tiersch, and H. J. Briegel, “Projective Simulation for Classical
Learning Agents: A Comprehensive Investigation,” New Generation Computing, vol. 33, pp. 69–114, Jan.
2015.
[44] B. Eva, K. Ried, T. Mu¨ller, and H. J. Briegel, “How a Minimal Learning Agent can Infer the Existence of
Unobserved Variables in a Complex Environment,” Minds and Machines, vol. 33, pp. 185–219, Mar. 2023.
[45] A. A. Melnikov, A. Makmal, V. Dunjko, and H. J. Briegel, “Projective simulation with generalization,” Scien-
tific Reports, vol. 7, p. 14430, Oct. 2017.
[46] P. A. LeMaitre, M. Krumm, and H. J. Briegel, “Multi-Excitation Projective Simulation with a Many-Body
Physics Inspired Inductive Bias,” Feb. 2024. arXiv:2402.10192.
[47] S. Jerbi, L. M. Trenkwalder, H. P. Nautrup, H. J. Briegel, and V. Dunjko, “Quantum enhancements for deep
reinforcement learning in large spaces,” PRX Quantum, vol. 2, p. 010328, Feb. 2021. arXiv:1910.12760.
[48] S.Hangl,V.Dunjko,H.J.Briegel,andJ.Piater,“SkillLearningbyAutonomousRoboticPlayingUsingActive
Learning and Exploratory Behavior Composition,” Frontiers in Robotics and AI, vol. 7, Apr. 2020. Publisher:
Frontiers.
[49] A. L´opez-Incera, K. Ried, T. Mu¨ller, and H. J. Briegel, “Development of swarm behavior in artificial learning
agents that adapt to different foraging environments,” PLOS ONE, vol. 15, p. e0243628, Dec. 2020.
[50] A. L´opez-Incera, M. Nouvian, K. Ried, T. Mu¨ller, and H. J. Briegel, “Collective defense of honeybee colonies:
experimental results and theoretical modeling,” BMC Biology, vol. 19, p. 106, Dec. 2021. arXiv:2010.07326.
[51] M. Tiersch, E. J. Ganahl, and H. J. Briegel, “Adaptive quantum computation in changing environments using
projective simulation,” Scientific Reports, vol. 5, p. 12874, Aug. 2015.
[52] L. M. Trenkwalder, A. L´opez-Incera, H. P. Nautrup, F. Flamini, and H. J. Briegel, “Automated Gadget
Discovery in Science,” Dec. 2022. arXiv:2212.12743.
[53] A. Dedieu, N. Gothoskar, S. Swingle, W. Lehrach, M. L´azaro-Gredilla, and D. George, “Learning higher-order
sequential structure with cloned HMMs,” May 2019. arXiv:1905.00507.
[54] J. S. Guntupalli, R. V. Raju, S. Kushagra, C. Wendelken, D. Sawyer, I. Deshpande, G. Zhou, M. L´azaro-
Gredilla, and D. George, “Graph schemas as abstractions for transfer learning, inference, and planning,” Dec.
2023. arXiv:2302.07350.
[55] D.George,R.V.Rikhye,N.Gothoskar,J.S.Guntupalli,A.Dedieu,andM.L´azaro-Gredilla,“Clone-structured
graph representations enable flexible learning and vicarious evaluation of cognitive maps,” Nature Communi-
cations, vol. 12, 12 2021.
[56] K. Friston, C. Thornton, and A. Clark, “Free-Energy Minimization and the Dark-Room Problem,” Frontiers
in Psychology, vol. 3, 2012.
[57] P.Dayan,“ImprovingGeneralizationforTemporalDifferenceLearning: TheSuccessorRepresentation,”Neural
Computation, vol. 5, pp. 613–624, July 1993.
[58] E. M. Russek, I. Momennejad, M. M. Botvinick, S. J. Gershman, and N. D. Daw, “Predictive representations
canlinkmodel-basedreinforcementlearningtomodel-freemechanisms,”PLOSComputationalBiology,vol.13,
Sept. 2017. Publisher: Public Library of Science.
[59] I.Momennejad,E.M.Russek,J.H.Cheong,M.M.Botvinick,N.D.Daw,andS.J.Gershman,“Thesuccessor
representation in human reinforcement learning,” Nature Human Behaviour, vol. 1, pp. 680–692, Aug. 2017.
21[60] W. A. Roberts, “Are animals stuck in time?,” Psychological Bulletin, vol. 128, pp. 473–489, May 2002.
[61] R. Wei, “Value of Information and Reward Specification in Active Inference and POMDPs,” Aug. 2024.
arXiv:2408.06542.
[62] R.F.Thompson,J.K.Thompson,J.J.Kim,D.J.Krupa,andP.G.Shinkman,“TheNatureofReinforcement
in Cerebellar Learning,” Neurobiology of Learning and Memory, vol. 70, pp. 150–176, July 1998.
[63] M.J.WagnerandL.Luo,“Neocortex-CerebellumCircuitsforCognitiveProcessing,”TrendsinNeurosciences,
vol. 43, pp. 42–54, Jan. 2020.
[64] R. P. N. Rao and D. H. Ballard, “Predictive coding in the visual cortex: a functional interpretation of some
extra-classical receptive-field effects,” Nature Neuroscience, vol. 2, pp. 79–87, Jan. 1999.
[65] Y. Kim, W. Nam, H. Kim, J.-H. Kim, and G. Kim, “Curiosity-Bottleneck: Exploration By Distilling Task-
Specific Novelty,” in Proceedings of the 36th International Conference on Machine Learning, pp. 3379–3388,
PMLR, May 2019. ISSN: 2640-3498.
[66] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos, “Unifying Count-Based
ExplorationandIntrinsicMotivation,” inAdvances in Neural Information Processing Systems, vol.29, Curran
Associates, Inc., 2016.
[67] I.Momennejad,“Arubricforhuman-likeagentsandNeuroAI,”PhilosophicalTransactionsoftheRoyalSociety
B: Biological Sciences, vol. 378, p. 20210446, Jan. 2023.
[68] J. C. R. Whittington, D. McCaffary, J. J. W. Bakermans, and T. E. J. Behrens, “How to build a cognitive
map,” Nature Neuroscience, vol. 25, pp. 1257–1272, Oct. 2022.
A Expected free energy in the wandering phase
During the wandering phase, the preference distribution is designed to encourage the agent to seek the relevant
information to complete its model. The preferences are equal to the world model marginalized over actions:
(cid:88)
pref(S ,B |b )= π(a|b )p(B |b ,a)p(S |B ). (20)
t+1 t+1 t t t+1 t t+1 t+1
a
Plugging this result in the expected free energy in Equation (4), one obtains:
G [a ]=E [logp(b |b ,a )−logpref(s ,b |b ,a )] (21)
bt t bt+1,st+1∼p(Bt+1,St+1|bt,at) t+1 t t t+1 t+1 t t
(cid:34) (cid:35)
(cid:88) (cid:88)
= p(b |b ,a )p(s |b ) logp(b |b ,a )−log π(a|b )p(b |b ,a)p(s |b ) (22)
t+1 t t t+1 t+1 t+1 t t t t+1 t t+1 t+1
bt+1,st+1 a
(cid:34) (cid:35)
(cid:88) (cid:88)
= p(b |b ,a )p(s |b ) logp(b |b ,a )−log π(a|b )p(b |b ,a)−logp(s |b ) (23)
t+1 t t t+1 t+1 t+1 t t t t+1 t t+1 t+1
bt+1,st+1 a
(cid:34) (cid:35)
(cid:88) (cid:88)
= p(b |b ,a ) − p(s |b )logp(s |b ) (24)
t+1 t t t+1 t+1 t+1 t+1
bt+1 s
(cid:34) (cid:35)
(cid:88) (cid:88)
+ p(b |b ,a ) logp(b |b ,a )−log π(a|b )p(b |b ,a) .
t+1 t t t+1 t t t t+1 t
bt+1 a
In the second line, we replace the preference distribution by its definition with respect to the world model,
and the additivity rule of the logarithm is used to move to the third. In the fourth fourth line, we separated the
expectation values over B and S by noticing that for two random variables X,Y, E [f(Y =y)]=f(y)
t+1 t+1 x∼p(X)
by the normalization constraint on probability distributions. As a result, terms are grouped by dependency, where
in particular, the second term has become independent of sensory states. The first term is the conditional entropy
of sensory states, conditioned on belief states. For some belief state b , and when the world model is clone-
t+1
structured, the likelihood is a delta function: p(s |b ) = δ where s(b ) designates the observation
t+1 t+1 st+1,s(bt+1) t+1
b isacloneof. Asaresult,whenthesensorystatesmatch,thelogarithmvanishes,andotherwise,itismultiplied
t+1
22by zero. Therefore, the entropy over observations cancels by design. The remaining term reduces to the Kullback-
Leibler divergence between the transition function and the transition function marginalized over actions:
(cid:34) (cid:35)
(cid:88)
G [a ]=D p(B |b ,a )|| π(a|b )p(B |b ,a) (25)
bt t KL t+1 t t t t+1 t
a
=D [p(B |b ,a )||p(B |b )] (26)
KL t+1 t t t+1 t
=IG(B ,A =a ), (27)
t+1 t t
where p(B |b ) is the marginal of the transition function over actions, and in the last line, IG(X|Y = y) is the
t t
information gain about X from knowing the value of Y =y.
B Limits of the EFE
In this section, we derive, under certain assumptions that we justify with our numerical simulations, a limit for
the expected free energy for environments with deterministic transitions, which include the timed response and
navigationtasks. Formally,thismeansthattheworldmodelhasbeenfullytrainedandthetransitionfunctionyields
perfect prediction accuracy over sensory states given the correct belief state is known. Equivalently, the variational
free energy is minimized, and the posterior and prior distributions for the transition functions stay essentially
identical after each update, i.e. ∃ϵ ∈ R,such that D [q (B |b ,a )||p(B |b ,a )] < ϵ, where ϵ > 0 can
KL ϕ t t−1 t−1 t t−1 t−1
come arbitrarily close to 0. Then, considering the expression for the VFE in Eq. (2), we see that the first term can
be neglected and the second term simplifies to −(cid:80) p(b |b ,a )logp(senv|b ), where we used the fact that the
bt t t−1 t−1 t t
Kullback-Leibler divergence cancels out if and only if the two distributions are equal, that is, q (B |b ,a ) =
ϕ t t−1 t−1
p(B |b ,a ). In this limit, a good approximation of the VFE is: F = −(cid:80) p(b |b ,a )logp(senv|b ) = 0
t t−1 t−1 bt t t−1 t−1 t t
for any observation received from the environment. Furthermore, our assumption of a perfect world model implies
that each belief state b is associated to exactly one hidden state e in the (deterministic) environment. That means
that when the agent is in belief state b, the environment is in hidden state e. In these conditions, we say that b
represents e. As a result, the transition function has the following form:
(cid:40)
x if s(b )=senv and b represents a hidden state e that emitted senv
p(b |b ,a )= b t t t t t (28)
t t−1 t−1
0 otherwise.
such that x ∈ [0,1] and the sum of probabilities for all belief states that represent the same hidden state e sum
b t
(cid:80)
up to 1: x = 1. In principle, the values for the x can be arbitrary, and depend on the individual models the
b b b
agents collapse onto after training. We call children of b and a the set of all belief states that can be reached
t t
with non-zero probability from b with action a : ch(b ,a ) = {b |p(b |b ,a ) > 0}. Similarly, we call e(b ,a )
t t t t t+1 t+1 t t t t
the hidden state in the environment that results from applying a from b . e(b ,a )=e designates the fact that
t t t t t+1
starting from belief state b and under action a , the environment will transition to e in the next step.
t t t+1
Two asymptotic configurations stood out during the numerical simulations: (1) the agent adopts a distributed
representation of the hidden state, and any belief state participating in it can be sampled with close to uniform
probability (that we idealize to be exactly uniform in the derivation), or (2) reinforcements of individual edges in
the models led to the adoption of a single belief state to fully represent the hidden state while ignoring the other
clonesassociatedwithzeroprobability. Abeliefstaterepresentsatmostonehiddenstate. Inordertoaccommodate
the first possibility, we define the set:
D ={b |∃a∈A, p(b |b ,a)>0 and b represents e only} (29)
bt,et+1 t+1 t+1 t t+1 t+1
thesetofallbeliefstatesthatcanbetransitionedtoandthatuniquelyrepresentthehiddenstatee(b ,a ). Further-
t t
more, we require that belief states b ,b′ ∈D are degenerate, that is, ∀a∈A, p(b |b ,a)=p(b′ |b ,a).
t+1 t+1 bt,et+1 t+1 t t+1 t
The size |D | of the set a belief state belongs to is called its degeneracy and ∀b ∈ D , p(b|b ,a ) =
bt,e bt,e(bt,at) t t
1/|D |. As a result, the world model becomes:
bt,e(bt,at)
δ
p(b ,s |b ,a )=
bt+1∈ch(bt,at)
δ , (30)
t+1 t+1 t t |D | st+1,s(bt+1)
bt,e(bt,at)
where as in the main text, δ equals 1 if b is a child of b and a and 0 otherwise, and s(b ) is
bt+1∈ch(bt,at) t+1 t t t+1
the sensory state that b is a clone of, in contrast to s , the value that the sensory state can take. Another
t+1 t+1
23important consequence is that if two actions a,a′ ∈ A lead to the same hidden state in the environment, that
is, e = e(b ,a) = e(b ,a′), then the children of b and a, and b and a′, respectively, are the same: ch(b ,a) =
t+1 t t t t t
ch(b ,a′). Conversely,ife(b ,a)̸=e(b ,a′),thenthetwosetshavenobeliefstateincommon: ch(b ,a)∩ch(b ,a′)=∅.
t t t t t
Therefore, the following holds for any two actions a,a′ ∈A:
∃b∈B such that b∈ch(b ,a) and b∈ch(b ,a′) ⇐⇒ ch(b ,a)=ch(b ,a′) (31)
t t t t
Using Eq. (30) for the world model, the new expression for the expected free energy (Eq.(4) in the main text)
is:
G [a ]=
(cid:88) δ bt+1∈ch(bt,at)
δ
log(cid:18)δ st+1,s(bt+1)/|D bt,e(bt,at)|(cid:19)
(32)
bt t |D | st+1,s(bt+1) pref(b ,s |b )
bt+1,st+1
bt,e(bt,at) t+1 t+1 t
=
(cid:88) 1
δ
log(cid:18) 1/|D bt,e(bt,at)| (cid:19)
. (33)
|D | st+1,s(bt+1) pref(b ,s |b )
bt+1∈ch(bt,at),st+1
bt,e(bt,at) t+1 t+1 t
Wandering phase: During the wandering phase, the preferences of the agents are set as the marginal of their
world model over actions:
(cid:88)
pref(b ,s |b )= p(s |b )p(b |b ,a)π(a|b ) (34)
t+1 t+1 t t+1 t+1 t+1 t t
a
=(cid:88)
δ
δ bt+1∈ch(bt,a)
π(a|b ) (35)
st+1,s(bt+1) |D | t
a bt,e(bt,a)
and the EFE is determined by the final policy:
 
G bt[a t]=
(cid:88)
|D
1
|log
(cid:88) π(a|1 b/ )|D ·1bt /,e |D(bt,at)|
|, (36)
bt+1∈ch(bt,at)
bt,e(bt,a)
a|e(bt,a)=e(bt,at)
t bt,e(bt,a)
whereby(31),thesumoveractionsinthelogarithmisrestrictedtothosewhosechildrenincludeb ,orequivalently,
t+1
representthe samehiddenstateandbelongto thesamesetof degeneratestates. Therefore, the degeneraciesinside
the logarithm cancel each other out, and each term in the sum is the same for each degenerate belief state:
 
(cid:88)
G bt[a t]=−log π(a|b t). (37)
a|e(bt,a)=e(bt,at)
In order to find the final value the EFE converges to, we must find the fixed point for the policy. This means
that the policy π(n+1) after the n-th update using the expected free energy is equal to the original policy at step n,
π(n) =π:
π(a |b )=π(n)(a |b ) (38)
t t t t
=π(n+1)(a |b ) (39)
t t
=softmax(ζG [a ]) (40)
bt t
(cid:16) (cid:16) (cid:17)(cid:17)
(cid:80)
exp −ζlog π(a|b )
=
a|e(bt,a)=e(bt,at) t
(41)
(cid:16) (cid:16) (cid:17)(cid:17)
(cid:80) exp −ζlog (cid:80) π(a′′|b )
a′ a′′|e(bt,a′′)=e(bt,a′) t
(cid:16) (cid:17)−ζ
(cid:80)
π(a|b )
=
a|e(bt,a)=e(bt,at) t
(42)
(cid:16) (cid:17)−ζ
(cid:80) (cid:80) π(a′′|b )
a′ a′′|e(bt,a′′)=e(bt,a′) t
where we have used exp(ylogx) = (exp(logx))y = xy to get the last line and b′ in the denominator is a belief
t+1
state that can be reached from b with action a′ ∈A.
t
To proceed further, we introduce the following notations:
• E :={ek | ∃a∈A such that e(b ,a)=ek }, the set of hidden states that can be reached from the belief
bt t+1 t t+1
state b ;
t
24• A :={a |e(b ,a)=ek , a∈A}, the set of actions that induce a transition to the k-th hidden state ek
bt,k t t+1 t+1
in E . |A | is the number of elements in A . It has at least one element;
bt bt,k bt,k
• ∀ek ∈ E ,∀a ∈ A , π = π(a|b ) denotes the probability of taking any action a that will result in the
t+1 bt bt,k bt,k t
k-th hidden state ek in E . All actions that result in the same hidden state ek have the same probability
t+1 bt t+1
in the policy, as can be seen in Eq. (42).
We rewrite Eq. (42) with this notations, assuming a ∈A :
t bt,k
π(a |b )=π (43)
t t bt,k
(|A |π )−ζ
= bt,k bt,k (44)
(cid:80) |A | (|A |π )−ζ
l bt,l bt,l bt,l
|A |−ζ/(1+ζ)
= bt,k (45)
(cid:16) (cid:17)1/(1+ζ)
(cid:80) |A | (|A |π )−ζ
l bt,l bt,l bt,l
=N ×|A |−ζ/(1+ζ) (46)
bt,k
where from the second to third line, we used that for x̸=0, x=yxα ⇐⇒ x1−α =y , and N is the normalization
factor in the second to last line. N does not depend on the action considered and it normalizes the policy to 1:
(cid:88) (cid:88)
|A |π =N × |A |1/(1+ζ) =1. (47)
bt,k bt,k bt,k
k k
Therefore, wefindN =1/(cid:80) |A |1/(1+ζ). Attheendofasuccessfultraininginadeterministicenvironment, the
k bt,k
policy converges to:
|A |−ζ/(1+ζ)
π = bt,k . (48)
bt,k (cid:80) |A |1/(1+ζ)
l bt,l
Finally, during the wandering phase, once the world model perfectly represents the environment, the expected free
energy in Eq. (37) related to taking an action a ∈A converges to:
t bt,k
 
G bt[a t]=−log (cid:88) (cid:80)|A b |t A,k|−ζ |1/ /(1 (1+ +ζ ζ) ) (49)
a′∈Abt,k l bt,l
(cid:18) |A |1/(1+ζ) (cid:19)
=−log bt,k . (50)
(cid:80) |A |1/(1+ζ)
l bt,l
In figures 4 and 5, we average this value over actions and previous belief states, since it would otherwise depend
on individual deliberations of the agents.
C Simulations parameters
25Parameters Skinner box Navigation
N 100 30
agents
N 2 3
clones
N 4 000 40 000
episodes
Length episodes 80
Forgetting rate γ 0.001
Reward scale R 3
scaling parameter ζ {0} {-3, -1, 0, 1, 3}
wandering
scaling parameter ζ -1 -3
task
p* 0.99
N 1
pref
Prediction horizon T 2 3
h
Table 1: Parameters used during the training and the testing of the agents
26