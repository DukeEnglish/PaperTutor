Material Anything: Generating Materials for Any 3D Object via Diffusion
XinHuang1*,TengfeiWang2†,ZiweiLiu3,QingWang1†
1NorthwesternPolytechnicalUniversity,2ShanghaiAILab,3S-Lab,NanyangTechnologicalUniversity
Figure1. MaterialAnything:Afeed-forwardPBRmaterialgenerationmodelapplicabletoadiverserangeof3Dmeshesacrossvarying
textureandlightingconditions,includingtexture-less,albedo-only,generated,andscannedobjects.
Abstract 1.Introduction
Physically Based Rendering (PBR) involves complex in-
We present Material Anything, a fully-automated, unified teractions between geometry, materials, and illumination.
diffusionframeworkdesignedtogeneratephysically-based High-quality physical materials ensure that 3D objects ap-
materialsfor3Dobjects. Unlikeexistingmethodsthatrely pear consistent and realistic under various lighting condi-
oncomplexpipelinesorcase-specificoptimizations, Mate- tions, crucial for applications such as video games, vir-
rialAnythingoffersarobust,end-to-endsolutionadaptable tual reality, and film production. Given the meshes in
toobjectsunderdiverselightingconditions. Ourapproach Fig. 1, a skilled artist can create realistic textures and ma-
leverages a pre-trained image diffusion model, enhanced terials using software like Blender [5] and Substance 3D
with a triple-head architecture and rendering loss to im- Painter [2]. However, this creation process is tedious and
prove stability and material quality. Additionally, we in- time-consuming,requiringexpertiseingraphicdesign. De-
troduceconfidencemasksasadynamicswitcherwithinthe spiterecentadvancesin3Dtexturepainting[6,24,29,47],
diffusion model, enabling it to effectively handle both tex- theyoftenfail toaccuratelymodelmaterials thatdisentan-
tured and texture-less objects across varying lighting con- gle light and texture, resulting in baked-in shading effects
ditions. By employing a progressive material generation likeunwantedhighlightsandshadows.
strategy guided by these confidence masks, along with a
Several recent works have emerged to tackle the chal-
UV-space material refiner, our method ensures consistent,
lenge of generating materials for 3D objects; however,
UV-readymaterialoutputs. Extensiveexperimentsdemon-
theyremainlargelyimpracticalduetotheircomplexityand
strateourapproachoutperformsexistingmethodsacrossa
specificity. These approaches either require specific opti-
widerangeofobjectcategoriesandlightingconditions.
mizations for each case [43, 52] or rely on multi-modal
models like GPT4-V [1] to retrieve materials for different
partsofanobject[12,51]. Consequently,suchapproaches
face significant challenges: (1) Limited scalability. Each
*WorkwasdoneduringaninternshipatShanghaiAILab. caserequiresspecificparameteradjustments,hinderingthe
†Correspondingauthors. end-to-end automation of the creation process. (2) Com-
1
4202
voN
22
]VC.sc[
1v83151.1142:viXrapromisedrobustness.Complexpipelinesthatinvolvemul- achieveremarkableperformanceinmaterialgeneration. To
tiplemodels(e.g.,SAM[20]andGPTforsegmentationand trainthemodel,webuildaMaterial3Ddataset,comprising
assignment) may lead to system instability. (3) Limited over80Kobjectswithhigh-qualityPBRmaterialsandUV
generalization. Existing methods are sensitive to lighting unwrapping. Extensive experiments demonstrate a signifi-
and struggle to handle a broad spectrum of scenarios, in- cant improvement of our method over current approaches.
cluding realistic lighting (e.g., real-world scans), unrealis- Wesummarizeourcontributionsasfollows:
ticlighting(e.g., generatedtextures), andabsenceoflight- • Afullyautomated, stable, anduniversalmodeltogener-
ing (e.g., albedo). To tackle these challenges, we propose ate physical materials for diverse 3D objects, achieving
MaterialAnything,afullyautomated,stable,anduniver- state-of-the-artperformance.
sal generative model for physical materials. Our method • Amaterialdiffusionmodelwithilluminationconfidence
accepts any 3D mesh as input and generates high-quality tohandlevariouslightingconditionswithonemodel.
material maps through a two-stage pipeline: image-space • Aprogressivematerialgenerationschemeguidedbycon-
materialgenerationandUV-spacematerialrefinement. fidencemasks,alongwithaUV-spacematerialdiffusion
Given a 3D object, the image-space material diffusion model,togenerateconsistentandUV-readymaterials.
model aims to produce PBR materials for each view of it.
2.RelatedWork
ConsideringthelimitedavailabilityofPBRdata,welever-
ageapre-trainedimagediffusionmodelandadaptittoma-
3DObjectGeneration. Previousworks[8,16,23,28,34,
terial estimation using a novel triple-head architecture and
35, 40, 41] rely on image diffusion models for 3D genera-
rendering loss, which together help stabilize training and
tion with score distillation sampling but suffer from long
bridge the gap between natural images and material maps.
optimization times for each object. To mitigate this, re-
Oncetrained,thismodelcanautomaticallygeneratemateri-
centworkshaveexploredfeed-forwardmodels. Theseap-
alsfortheviewsrenderedfromgeneral3Dobjectswithout
proacheseitherapply3Ddiffusionmodels[14,19,27,39]
predefinedcategoriesorpart-levelmasks.
oremployU-Netortransformer-basedmodels[13,15,22,
To enable the image-space material diffusion model to
36,42,53]todirectlygenerate3Drepresentations. Despite
support 3D objects across various lighting scenarios, we
achieving impressive results, these models are limited in
introduce a confidence mask to indicate illumination cer-
theirabilitytogeneraterealisticmaterials,oftenproducing
taintyandproposeadataaugmentationstrategytosimulate
texturesentangledwithcomplexlightinginformation. This
variouslightingconditionsduringtraining. (1)Formeshes
limitation hinders the adaptability of generated objects for
withrealisticlightingeffects,theconfidencemaskissetto
downstreamapplications,wherematerialpropertiesarees-
a higher value, enabling the model to utilize illumination
sential. Recentworks,suchasClay[50],Meta3DGen[4],
cuestopredictthematerialsaccurately.(2)Formesheswith
and3DTopia-XL[9],haveproposedframeworksforgener-
lighting-free textures, the confidence is set to low, allow-
ating 3D objects with PBR materials from prompts or im-
ing the model to generate materials based on prompts and
ages.However,thesemethodsstruggletohandlediverse3D
globalsemanticcues.(3)Forgeneratedobjectsandtexture-
object inputs under different texture and illumination con-
lessobjects(weinitiallyuseatexturegenerationmethodto
ditionsfordifferentapplications.
create coarse textures), their textures may exhibit unreal-
Texture Generation for 3D Object. Given a texture-
istic lighting effects that deviate from physical laws, often
less3Dmodel,TEXTure[29]introducesaprogressiveap-
resulting in exaggerated highlights and shadows. In such
proach that generates textures view-by-view using diffu-
cases,theconfidencemaskisadaptivelysettovaryingval-
sion. Latent-NeRF [25] further improves efficiency in a
ues,ensuringthemodelreliesonlocalsemantictogenerate
lower-dimensional latent space, enabling high-quality tex-
plausiblematerials,asthelightingcuesareunreliable.
ture synthesis. Text2Tex [6] introduces an automatic view
While the image-space model can effectively gener- sequencegenerationscheme,optimizingthegenerationse-
ate materials for each single view, directly applying it to quencetoachievemoreconsistenttextures.SyncMVD[24]
a 3D object can lead to appearance inconsistency across generatestexturesfrommultipleviewssimultaneously,en-
views. To boost multi-view consistency, we introduce a suring coherent alignment of textures. Paint3D [47] first
confidence-aware progressive material generation scheme creates an initial texture map, then refines it in the UV
thatusestheconfidencemasktopromptourdiffusionmodel space,achievinghighlydetailedandspatiallycoherenttex-
to produce materials consistent with previous views. Af- tures. Despite these advancements, the textures generated
terprogressivelygeneratingthematerialsforallviews,we by these methods are typically entangled with complex
projectthemintoUVspaceforfurtherrefinement, achiev- lightingandshadows,lackingrealisticmaterialmodeling.
ing3D-consistentandhigh-qualityUVmapsthatareuser- Material Generation for 3D Object. Early works em-
friendlyandeasytoedit. ploy optimization-based methods for material generation.
Together,thesecomponentsenableMaterialAnythingto Fantasia3D [7] involves learnable materials when gener-
2Figure 2. Overview of Material Anything. For texture-less objects, we first generate coarse textures using image diffusion models,
similartothetexturegenerationmethod[6]. Forobjectswithpre-existingtextures,wedirectlyprocessthem. Next,amaterialestimator
progressivelyestimatesmaterialsforeachviewfromarenderedimage,normal,andconfidencemask.Theconfidencemaskservesasaddi-
tionalguidanceforilluminanceuncertainty,addressinglightingvariationsintheinputimageandenhancingconsistencyacrossgenerated
multi-viewmaterials. ThesematerialsarethenunwrappedintoUVspaceandrefinedbyamaterialrefiner. Thefinalmaterialmapsare
integratedwiththemesh,enablingtheobjectfordownstreamapplications.
ating 3D objects. NvDiffRec [26] jointly optimizes the ationstrategythatutilizesconfidencemaskstoensurecon-
topology, materials, and lighting conditions from multi- sistencyofgeneratedmaterialsacrossviews,andfurtherin-
view image observations. Matlaber [43] proposes a la- tegrateaUV-spacediffusionmodelformaterialrefinement.
tentBRDFauto-encodertoenablematerial-aware3Dgen- (Sec. 3.2). Finally, we provide the construction details of
eration. Paint-it [45] introduces a re-parameterization of ourMaterial3DdatasetinSec.3.3.
PBRtexturemaps,facilitatingrobustandefficientoptimiza-
3.1.Image-basedMaterialDiffusion
tion. DreamMat [52] finetunes a lighting-aware diffusion
model for distilling PBR materials. These methods are Thematerialestimatoraimstoproducealbedo(R3),rough-
time-intensive and often suffer unnatural color. Retrieval- ness(R),metallic(R),andbump(R3)mapsfromaninput
based methods [12, 51] rely on large multi-modal models image. Given the limited availability of PBR data, we opt
suchasSAM[20]andGPT[1]forsegmentationandmate- to leverage the power of pre-trained image diffusion mod-
rialassignment,limitingtheirscalability. Recently,several els[30]. However,thesemodelsareprimarilydesignedfor
works[37,38,48]attempttogeneratematerialsforimages naturalimagegeneration, posingthreechallengesforPBR
with diffusion models, however, they are not applicable to materialgeneration: ChannelGap. Imagediffusionmodels
3Dobjectsduetoconsistencyissues. typicallyoperateonthreechannels(RGB),whilePBRma-
terials require more than three channels (eight channels in
3.Approach ourmethod). Thisdiscrepancycanleadtoinaccuratemate-
rial representations, as the model must adapt to producing
MaterialAnything, illustratedinFig.2, isaunifiedframe- a more complex set of outputs. Domain Gap. PBR mate-
workforgeneratinghigh-qualityphysicalmaterialsfor3D rialmapsaredifferentfromnaturalimages. Thissignificant
objects, accommodating various lighting and texture sce- differenceleadstounstabletrainingandsuboptimalperfor-
narios. It effectively handles (1) texture-less objects, (2) mance. Various Lighting. Finally, our material estimator
albedo-only objects (without lighting effects), (3) scanned must be robust across images with diverse lighting condi-
objects(realisticlighting),and(4)generatedobjects(unre- tions, ensuring consistent performance. To address these
alistic lighting). Unlike existing methods that treat these challenges,weintroduceseveralkeycomponents.
scenarios as separate tasks, our method unifies them un- Triple-Head Diffusion. To adapt the three-channel diffu-
der a single framework. To this end, we reformulate 3D sion model to handle multiple material-specific channels,
material generation as an image-based material estimation one solution is to train a material VAE [37]. However,
task,enablingtheuseofpre-trainedimagediffusionmodels this approach may discard the pre-trained priors of diffu-
and simplifying the overall process. Our framework cen- sion models, and training a customized material VAE on
tersontwocorecomponents. First,weemployadiffusion- our limited PBR data is challenging. Inspired by previous
based material estimator equipped with confidence masks, work [50], we design a triple-head U-Net architecture, il-
whichgeneratesmaterialsforeachviewoftheinputobject lustrated in Fig. 3 (a). The U-Net architecture comprises
(Sec.3.1). Next,weintroduceaprogressivematerialgener- three distinct branches for the initial convolutional layer
3Figure4.Progressivematerialgenerationprocessforatexture-
less object. “Project” denotes projecting known regions for the
latentinitializationofthenextview.“SD”denotesthepre-trained
Figure 3. Architectural design of material estimator and
stablediffusionmodel[30]withdepthControlNet[49]
refiner. Both employ a triple-head U-Net, generating albedo,
roughness-metallic,andbumpmapsviaseparatebranches.
tively. z denotes the noise latent. c denotes the condi-
t
andfirstDownBlock,followedbysharedmiddlelayersen- tioning inputs (input image, confidence mask, and normal
ablingconcurrentdenoisingacrossmaterialmodalities.The map),whiley representsthetextprompt. Vˆ referstoour
θ
finalUpBlockandoutputconvolutionallayerarealsosepa- triple-headdiffusionnetworkwithlearnableparametersθ.
ratedintothreebranches. Eachoutputheadproducesaspe- RenderingLoss. Duetothesignificantdomaingap,train-
cificmaterialmap: analbedomap,acombinedroughness- ingthematerialdiffusionmodelwithonlythev-prediction
metallic map (R channel set to 1, G for roughness, B for objectiveisunstable.Toaddressthis,weintroducearender-
metallic), and a bump map. This triple-head structure, in- inglossbydecodingalbedo,roughness-metallic,andbump
stead of combining all materials into one output, ensures map from latent space into the image for reconstruction.
that each material map is generated without mutual inter- Thesecomponentsarerenderedintoanimagerˆunderran-
ferencewhilemaintainingconsistencyamongthem. domlightingconditionsusingdifferentiablerendering[21].
Confidence-AdaptiveGeneration. Tomanageinputswith Wethencomputetheperceptualloss[18]againsttheground
variouslightingconditions, wecategorizetheseconditions truthrenderingrasfollows:
into two main groups: high confidence (e.g., scanned ob-
jects) and low confidence (e.g., no lighting and generated L =(cid:88) ∥ϕ (rˆ)−ϕ (r)∥2, (2)
p l l 2
lighting). To guide the model, we introduce a certainty l
maskthatindicatesilluminationconfidence.Forinputswith
whereϕ representsVGGnetwork[33]. Therenderingloss
realisticlighting,theconfidencevalueissetto1,encourag- l
ensuresthattheestimatedmaterialsexhibitrealisticbehav-
ingthediffusionmodeltoleveragelightingcuesformate-
iorunderdiverselightingconditions,improvingthequality
rial estimation. In contrast, for inputs lacking lighting or
ofgeneratedmaterials.Additionally,anL2lossisappliedto
with generated lighting, the confidence is set to 0, direct-
eachmaterialcomponenttofurtherimproveperformance.
ing the model to focus on material generation instead of
estimation. Note that, for images with generated lighting,
3.2.MaterialsGenerationfor3DObject
themaskcanselectivelyassignvaluesof1inknownmate-
rialregionsand0elsewheretoenhancemulti-viewmaterial Whilewehavesuccessfullyestimatedmaterialsforimages,
consistency, as detailed in the progressive material gener- applying the material estimator to multiple views of a 3D
ation (Sec. 3.2). The confidence mask enables the diffu- objectwouldleadtonoticeableinconsistencies. Onesolu-
sion model to seamlessly transition between material esti- tioninvolvestrainingamaterialestimatortosimultaneously
mation and generation, effectively managing both realistic predict materials across multiple views, similar to multi-
and synthetic lighting scenarios. The learning objective is view diffusion [32]. However, the increased number of
v-prediction[31]: views and channels poses a challenge for generating high-
resolution materials. To adapt our 2D materials estimator
(cid:13) (cid:13)2
L =E (cid:13)Vˆ (z ;c,y)−v (cid:13) , (1) for 3D objects, we propose a progressive generation strat-
v z,c,y,v,t(cid:13) θ t t(cid:13)
2 egy that dynamically estimates materials across different
where v represents the v-prediction targets at time-step t viewpoints based on the aforementioned confidence mask.
t
forthealbedo,roughness-metallic,andbumpmaps,respec- We further project the multi-view materials into UV space
4and apply a refinement diffusion model, which completes to incorporate 3D adjacency information during the diffu-
occluded regions and refines materials, ensuring seamless sionprocess,guidingtheregionsthatrequireinpainting,as
andconsistentmaterialsmaps. shown in Fig. 3 (b). By integrating these components, the
ProgressiveMaterialGenerationwithConfidenceGuid- refinerproduceshigh-quality,consistentUVmaterialmaps.
ance. Given a 3D object O, we define a set of camera
3.3.Material3DDataset
views and progressively generate materials for each view.
For meshes lacking textures, we first generate a textured TotrainMaterialAnything,webuildadatasetMaterial3D
imagev usinganimagediffusionmodelconditionedona that consists of 80K high-quality 3D objects curated from
i
depthmap,similartoText2Tex[6]. Formesheswithexist- Objaverse[11].Detailsofdatasetconstructionareprovided
ingtextures,wedirectlyrenderthetexturedimagev from inthesupplementary. Foreachmodel, werenderedmulti-
i
the input object. Figure 4 illustrates the progressive mate- viewmaterialimages(albedo, roughness, metallic, bump),
rialgenerationforatexture-lessobject. Thenextstepisto and normal maps from 10 fixed camera positions. Addi-
estimatethematerialfromtheimagev . tionally,UVmaterialmapsandtheCCMwererenderedto
i
As we generate materials for each view independently, facilitate the training of the material refiner. To enable the
our goal is to maintain consistency across views. When modeltohandlediverselightingscenarios,weincorporated
generating anew view, we aimfor the materialsto remain various lighting conditions, including Point Lighting, Area
consistent with existing regions in previous views, rather Lighting, Environment Lighting, and Without Lighting, for
than relying solely on the current view. To achieve this, renderinginputimages. Additionally, wedesignedastrat-
First,weinitializethenoiselatentusingmaterialsfrompre- egytosimulatetheimperfectandinconsistentlightingcon-
viouslyprocessedviews{v |j < i},withmaskmˆ indicat- ditionscommonduringinference.
j
ingknownregions,ensuringtheseregionsarepreservedand Simulating Inconsistent Lighting. We randomly select
consistent. Thelatentinitializationisformulatedas: two images under different lighting conditions for a cam-
eraviewandstitchportionsofeachintoacompositeduring
zˆ t =zˆ t·(1−mˆ)+z t·mˆ, (3) training. This enables a single image to exhibit two dis-
tinct lighting types, simulating the inconsistency in multi-
wherezˆ representsthenoiselatentattimesteptformate-
t viewmaterials. Furthermore,weintroducedegradationsto
rialmaps,z denotesthelatentofknownregions.
t one of the images, applying effects such as blurring and
Consistencyisespeciallychallenginginviewswithgen-
color shifts. A confidence mask is used to delineate the
erated lighting due to exaggerated highlights and shad-
regions that have undergone degradation. The final input
ows. Therefore, for these views with generated lighting,
tothematerialestimatorcomprisesthestitchedimage, the
we additionally utilize the confidence mask m introduced
confidencemask,andthenormalmap. Totrainthematerial
in Sec. 3.1 to further enhance consistency between newly
refiner,werandomlymaskregionsoftheUVmaterialmaps
generatedandknownregions. Specifically,wedynamically
andusethesemaskedmaterialmapsasinput.TheCCM,de-
adjust the mask m, setting it to 1 for known regions with
rivedfromtheUVmappingof3Dpointcoordinates,isalso
higherconfidenceandto0forregionsrequiringnewgener-
included. These maps guide the areas requiring inpainting
ation. Thisapproachguidestheestimatortoproducemate-
and facilitate the integration of 3D adjacency information
rialsthatalignseamlesslywithknownregions,asourtrain-
duringthediffusionprocess. Refertosupplementarymate-
ing data is designed to simulate these unrealistic lighting
rialformoredetailsonourdataset.
situations.
Next,webakethesematerialmapsintotheUVspaceac- 4.Experiments
cordingtotheUVunwrappingofobjectO. Afterprocess-
ingallgeneratedviewsandmaterials,weobtainthecoarse We compare our method with texture generation methods,
UVmaterialmapsMuv fortheobject. Text2Tex [6], SyncMVD [24], and Paint3D [47]. Ad-
UV Refinement Diffusion. Although coarse UV material ditionally, we assess our method alongside optimization-
maps are generated, issues such as seams (resulting from basedmaterialgenerationapproaches,NvDiffRec[26]and
bakingacrossdifferentviews)andtextureholes(duetoself- DreamMat [52], and a retrieval-based method, Make-it-
occlusionduringrendering)remain.Wethusrefinematerial Real [12]. Finally, we also include comparisons with the
mapsdirectlyinUVspaceusingadiffusionmodel. Unlike closed-sourcemethods,RodinGen-1[10]andTripo3D[3].
Paint3D[47],whichfine-tunesadiffusionmodelsolelyon
4.1.QualitativeEvaluation
albedomaps,ourtaskismorecomplex,asitinvolvesrefin-
ingalbedo,roughness-metallic,andbumpmaps.Wetrained Comparisons with Texture Generation Methods. We
amaterialrefinerthattakesthecoarsematerialmapsMuv compareMaterialAnythingwithvarioustexturegeneration
as input, completing holes and smoothing seams. Addi- methodsinFig.5.Thesemethodsemployasimilarstrategy,
tionally, a canonical coordinate map (CCM) is introduced painting the texture-less meshes with a pre-trained image
5Figure5.Comparisonswithtexturegenerationmethods.Thesemethodsdirectlypainttexture-lessobjectsusingimagediffusionmodels
butfailtogeneratethecorrespondingmaterialproperties.
Figure6. Comparisonswithoptimizationmethods. NvDiffRec[26]estimatesmaterialsusingthetexturedmodelbySyncMVD[24]as
input.Thematerialsincludealbedo(topleft);roughness(topright);metallic(bottomleft);bump(bottomright).
diffusion model. However, the lighting information gen- methodingeneratingrealisticanddiversematerialmaps.
erated by the diffusion model often results in textures that Comparisons with Retrieval Methods. For input objects
exhibitsignificantartifacts,astheyareinfluencedbycom- with existing textures, we compare ours with the retrieval
plexlightingconditionsfrommultiplegeneratedimages. In method Make-it-Real, as shown in Fig. 7. Make-it-Real
contrast, our method produces clearer textures that inher- retrieves materials based on segmenting the initial texture,
ently incorporate material properties, thus enabling robust which presents several limitations. First, the segmentation
supportfordownstreamapplications. process struggles with accurately capturing small regions,
Comparisons with Optimization Methods. We com- suchasthepeelingpaintonthefirehydrant. Additionally,
pareourapproachwithoptimizationmethods,asshownin it encounters difficulty in removing shadows in the initial
Fig.6. Thesemethodsrequireextensiveoptimizationtime texture, as observed in the shadowed albedo of the sculp-
for each object and have difficulty accurately identifying ture example. In contrast, our method generates more ac-
object materials. In contrast, our method effectively dis- curatematerial,betterpreservingfinedetailsandremoving
tinguishesmaterials,asdemonstratedinthebarrelexample, artifactssuchasshadows.
whereitaccuratelyrepresentsthemetalbandsandwooden ComparisonswithTripo3DandRodinGen-1. Wecom-
planks. This capability underscores the superiority of our pareourmethodwithtwoclosed-sourcemethods,Tripo3D
6Table1. Quantitativecomparisons. FIDandCLIPscores(sim-
ilarity between rendered views and text prompts) are computed
on1,200imagesfrom20texturedobjects. Forcomparisonwith
Make-it-Real,theCLIPscoreiscalculatedbetweenrenderedim-
agesfromgeneratedtexturesandthoseinObjaverse.
Method Type InputMesh FID↓ CLIPScore↑
Text2Tex[6] Learning Texture-less 116.41 30.33
SyncMVD[24] Learning Texture-less 118.46 30.66
Paint3D[47] Learning Texture-less 153.20 28.40
NvDiffRec[26] Optimization Texture-less 103.81 30.14
DreamMat[52] Optimization Texture-less 113.34 30.64
Ours Learning Texture-less 100.63 31.06
Make-it-Real[12] Retrieval Textured 104.38 88.62
Ours Learning Textured 101.19 89.70
Table 2. Ablation study for triple-head U-Net and rendering
loss. RMSEiscalculatedforthematerialsacrosstheviewsfrom
1,000Objaverseobjects.
Figure7. Comparisonswithretrievalmethods. Theinputsare
textured objects, including an albedo-only object and a scanned Materials W/OTriple-head W/ORenderingLoss Full
object. The materials include albedo (top left); roughness (top
Albedo 0.0800 0.1442 0.0604
right);metallic(bottomleft);bump(bottomright). Roughness 0.1196 0.1943 0.0877
Metallic 0.1584 0.2594 0.1193
Bump 0.0824 0.0716 0.0313
Table 3. Ablation study for confidence masks. Mean RMSE
iscalculatedformaterialsfrom1,000Objaverseobjectswithdif-
ferentsimulatedlightingconditions,includinglight-less(albedo-
only),realistic(scanned),andunrealisticlight(generated).
Light-less Realistic Unrealistic Mean
W/OConfidence 0.1521 0.1074 0.1111 0.1235
Full 0.1102 0.0747 0.0847 0.0899
4.3.AblationStudy
Figure8. ComparisonswithRodinGen-1andTripo3D.Rodin
Gen-1andTripo3Daretwoclosed-sourcemethods.Ourapproach EffectivenessofTriple-HeadU-Net. Weevaluatetheper-
usessignificantlylessdata,yetproducescomparableresults. formanceofourmethodusingavanillaU-Netarchitecture
that directly generates all materials as a 12-channel latent,
insteadofatriple-headU-Net. AsshowninTab.2,theper-
and Rodin Gen-1, as shown in Fig. 8. We utilize texture- formancedegradesduetothecouplingeffectbetweenma-
lessmeshesgeneratedbyTripo3Dasinputforourmaterial terialswhenoutputtingasingle12-channellatent.InFig.9,
generation. Additionally, weprovidethefront-viewimage this coupling effect is evident, where the bumps are incor-
byanimagediffusionmodeltoRodinGen-1,ensuringthe rectlycoloredduetointerferencefromthealbedo. Incon-
generation of the same 3D objects. While both Tripo3D trast,thetriple-headU-Neteffectivelydecouplesthemate-
and Rodin Gen-1 utilize significantly larger-scale training rials. Additionally, the shared backbone among the three
datasets,ourmethodproducescomparableresults. headsensuresalignmentacrossthedifferentmaterialmaps.
Effectiveness of Rendering Loss. In Tab. 2, we present
the quantitative results of our method when trained with-
4.2.QuantitativeEvaluation
outrenderingloss. Notably,performanceinthisablationis
The quantitative evaluation of our method is presented in worse compared to the variant trained with rendering loss.
Tab.1. Asshown,ourmethodachievesalowerFIDscore, As illustrated in Fig. 9, the version without rendering loss
indicatingthatourgeneratedtexturesarecloserindistribu- exhibitsnoticeabledetaildegradation,withvisibleartifacts
tion to those in Objaverse. Furthermore, the higher CLIP across the material views. Rendering loss acts as an addi-
scoreofourmethoddemonstratesitscapabilitytogenerate tionalconstraintinimagespace,ensuringconsistencyunder
texturesmoreaccuratelyalignedwiththeprompts. varyinglightingconditions,whichenhancestrainingstabil-
7Figure11.Effectivenessofstrategiesformaterialconsistency.
Figure9.Effectivenessoftriple-headU-Netandrenderingloss.
Inbothablationexperiments,theconfidencemaskissetto1.
Figure10.Effectivenessofconfidencemaskforvariouslighting Figure12. EffectivenessoftheUV-spacematerialrefiner. The
conditions. “W/O confidence mask” indicates results from the materialrefinereffectivelyfillsinholescausedbyocclusions.
materialestimatorwithouttheconfidencemaskasinput.
ity and aids in capturing finer material properties. The re- materialsfromotherviewsforinitialization. Asshown,the
sultshighlightthecriticalroleofrenderinglossinenhanc- predicted metallic properties display noticeable variations
ingthefidelityandstabilityofourmaterialestimator. acrossdifferentviews. Incontrast,byprogressivelygener-
Effectiveness of Confidence Mask. As shown in Fig. 10, atingmaterialsbasedonknownones,ourmethodproduces
the material estimator without confidence masks struggles moreconsistentmaterialsacrossmultipleviews.
to generate high-quality materials under different lighting Effectiveness of the UV-Space Material Refiner. In
conditions. In contrast, when guided by the confidence Fig. 12, we shown the results without UV refinement. As
mask, the model adapts well to these input variations. shown, several holes appear in the predicted materials due
Table 3 presents quantitative results for the model with- toself-occlusions,leadingtoincompletematerialmaps.Af-
out confidence masks on Objaverse objects across differ- terapplyingourmaterialrefiner,theseholesareeffectively
ent lighting conditions, revealing significant drops in ma- filled, resulting in a more seamless and complete material
terial accuracy. Furthermore, for objects with generated representation. Our material refiner can handle occlusions
lighting,theprogressivegenerationwithouttheconfidence andenhancetheoverallmaterialgenerationquality.
maskalsoshowsnoticeableinconsistenciesinthematerial
maps,asshowninFig.11. Conversely,withtheconfidence
5.Conclusion
maskemployed,themodelcandistinguishbetweenregions
for estimation and those for generation. By guiding the We proposed Material Anything, a unified framework to
training process to focus on relevant regions, our method generate PBR materials for various 3D objects, including
achievesmoreconsistent,artifact-freematerials. Thesere- texture-less, albedo-only, generated, and scanned meshes.
sultsdemonstratethattheconfidencemaskimprovesmate- By leveraging a well-designed material diffusion model,
rialconsistencyandaddressesvariationsinlighting. ourapproachcangeneratehigh-fidelitymaterialsinafeed-
Effectiveness of Known Material Initialization. Fig- forwardmanner. Tounifyvariousinputobjectsundercom-
ure11showstheresultsofourmethodwithoutusingknown plex lighting conditions, we introduced a mask to indicate
8confidencelevelsfordifferentilluminations,whichalsoen- [14] FangzhouHong, JiaxiangTang, ZiangCao, MinShi, Tong
hances multi-view material consistency. Extensive exper- Wu, Zhaoxi Chen, Shuai Yang, Tengfei Wang, Liang Pan,
iments have demonstrated that our method can generate Dahua Lin, et al. 3dtopia: Large text-to-3d genera-
high-qualityPBRmaterialsforvariousobjects,withaclear tion model with hybrid diffusion priors. arXiv preprint
arXiv:2403.02234,2024. 2
improvementoverexistingmethods.
[15] YicongHong,KaiZhang,JiuxiangGu,SaiBi,YangZhou,
DifanLiu,FengLiu,KalyanSunkavalli,TrungBui,andHao
References
Tan. Lrm: Largereconstructionmodelforsingleimageto
3d. In The Twelfth International Conference on Learning
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-
Representations,2024. 2
mad,IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,
JankoAltenschmidt, SamAltman, ShyamalAnadkat, etal. [16] XinHuang,RuizhiShao,QiZhang,HongwenZhang,Ying
Gpt-4 technical report. arXiv preprint arXiv:2303.08774, Feng, Yebin Liu, and Qing Wang. Humannorm: Learning
2023. 1,3 normaldiffusionmodelforhigh-qualityandrealistic3dhu-
mangeneration. InCVPR,pages4568–4577,2024. 2
[2] Adobe. Substance 3d painter. https://www.
adobe.com/my_en/products/substance3d/ [17] Huggingface. Diffusers. https://github.com/
apps/painter.html,2024. 1 huggingface/diffusers,2024. 2
[3] TripoAI.Tripo3d.https://www.tripo3d.ai/,2024. [18] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Percep-
tual losses for real-time style transfer and super-resolution.
5
In Computer Vision–ECCV 2016: 14th European Confer-
[4] RaphaelBensadoun,TomMonnier,YanirKleiman,Filippos
ence, Amsterdam, The Netherlands, October 11-14, 2016,
Kokkinos,YawarSiddiqui,MahendraKariya,OmriHarosh,
Proceedings,PartII14,pages694–711.Springer,2016. 4
Roman Shapovalov, Benjamin Graham, Emilien Garreau,
[19] Heewoo Jun and Alex Nichol. Shap-e: Generat-
etal. Meta3dgen. arXivpreprintarXiv:2407.02599,2024.
ing conditional 3d implicit functions. arXiv preprint
2
arXiv:2305.02463,2023. 2
[5] Blender. Blender project. https://www.blender.
[20] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
org/features/,2024. 1
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
[6] DaveZhenyuChen,YawarSiddiqui,Hsin-YingLee,Sergey
head,AlexanderCBerg,Wan-YenLo,etal. Segmentany-
Tulyakov,andMatthiasNießner. Text2tex: Text-driventex-
thing. InProceedingsoftheIEEE/CVFInternationalCon-
turesynthesisviadiffusionmodels. InICCV,pages18558–
ferenceonComputerVision,pages4015–4026,2023. 2,3
18568,2023. 1,2,3,5,7
[21] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,
[7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.
Jaakko Lehtinen, and Timo Aila. Modular primitives for
Fantasia3d: Disentangling geometry and appearance for
high-performance differentiable rendering. ACM Transac-
high-quality text-to-3d content creation. arXiv preprint
tionsonGraphics,39(6),2020. 4
arXiv:2303.13873,2023. 2
[22] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun
[8] YongweiChen,TengfeiWang,TongWu,XingangPan,Kui
Luan,YinghaoXu,YicongHong,KalyanSunkavalli,Greg
Jia, and Ziwei Liu. Comboverse: Compositional 3d as-
Shakhnarovich,andSaiBi. Instant3d: Fasttext-to-3dwith
setscreationusingspatially-awarediffusionguidance.arXiv
sparse-view generation and large reconstruction model. In
preprintarXiv:2403.12409,2024. 2 TheTwelfthInternationalConferenceonLearningRepresen-
[9] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, tations,2024. 2
Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, [23] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,
TongWu, ShunsukeSaito, etal. 3dtopia-xl: Scalinghigh- Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
quality 3d asset generation via primitive diffusion. arXiv Ming-YuLiu,andTsung-YiLin. Magic3d: High-resolution
preprintarXiv:2409.12957,2024. 2 text-to-3dcontentcreation. InCVPR,pages300–309,2023.
[10] Deemos. Rodin gen-1. https://hyperhuman. 2
deemos.com/rodin,2024. 5 [24] YuxinLiu,MinshanXie,HanyuanLiu,andTien-TsinWong.
[11] MattDeitke, DustinSchwenk, JordiSalvador, LucaWeihs, Text-guidedtexturingbysynchronizedmulti-viewdiffusion.
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana arXivpreprintarXiv:2311.12891,2023. 1,2,5,6,7
Ehsani,AniruddhaKembhavi,andAliFarhadi. Objaverse: [25] GalMetzer,EladRichardson,OrPatashnik,RajaGiryes,and
Auniverseofannotated3dobjects. InCVPR,pages13142– DanielCohen-Or.Latent-nerfforshape-guidedgenerationof
13153,2023. 5 3dshapesandtextures.InCVPR,pages12663–12673,2023.
[12] YeFang,ZeyiSun,TongWu,JiaqiWang,ZiweiLiu,Gordon 2
Wetzstein, andDahuaLin. Make-it-real: Unleashinglarge [26] JacobMunkberg,JonHasselgren,TianchangShen,JunGao,
multimodalmodel’sabilityforpainting3dobjectswithreal- WenzhengChen,AlexEvans,ThomasMu¨ller,andSanjaFi-
isticmaterials.arXivpreprintarXiv:2404.16829,2024.1,3, dler.Extractingtriangular3dmodels,materials,andlighting
5,7 fromimages. InCVPR,pages8280–8290,2022. 3,5,6,7
[13] ZexinHeandTengfeiWang. Openlrm: Open-sourcelarge [27] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
reconstructionmodels,2023. 2 Mishkin, and Mark Chen. Point-e: A system for generat-
9ing3dpointcloudsfromcomplexprompts. arXivpreprint aware3dassetsfromfewexemplars. InACMSIGGRAPH
arXiv:2212.08751,2022. 2 2024ConferencePapers,pages1–12,2024. 2
[28] BenPoole,AjayJain,JonathanTBarron,andBenMilden- [42] ZhenweiWang,TengfeiWang,ZexinHe,GerhardHancke,
hall. Dreamfusion: Text-to-3d using 2d diffusion. ICLR, Ziwei Liu, and Rynson WH Lau. Phidias: A generative
2023. 2 modelforcreating3dcontentfromtext,image,and3dcon-
[29] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, ditionswithreference-augmenteddiffusion. arXivpreprint
andDanielCohen-Or. Texture: Text-guidedtexturingof3d arXiv:2409.11406,2024. 2
shapes.InACMSIGGRAPHConferenceProceedings,2023. [43] XudongXu,ZhaoyangLyu,XingangPan,andBoDai.Mat-
1,2 laber:Material-awaretext-to-3dvialatentbrdfauto-encoder.
[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, arXivpreprintarXiv:2308.09278,2023. 1,3
PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn- [44] Jonathan Young. Xatlas. https://github.com/
thesiswithlatentdiffusionmodels. InCVPR,pages10684– jpcy/xatlas,2024. 2
10695,2022. 3,4,2 [45] KimYouwang,Tae-HyunOh,andGerardPons-Moll. Paint-
[31] TimSalimansandJonathanHo. Progressivedistillationfor it: Text-to-texture synthesis via deep convolutional texture
fastsamplingofdiffusionmodels. InInternationalConfer- mapoptimizationandphysically-basedrendering.InCVPR,
enceonLearningRepresentations,2022. 4 pages4347–4356,2024. 3
[32] RuoxiShi,HanshengChen,ZhuoyangZhang,MinghuaLiu, [46] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong,
ChaoXu,XinyueWei,LinghaoChen,ChongZeng,andHao HongzhiWu,andXinTong. Dilightnet: Fine-grainedlight-
Su. Zero123++:asingleimagetoconsistentmulti-viewdif- ing control for diffusion-based image generation. In ACM
fusionbasemodel. arXivpreprintarXiv:2310.15110,2023. SIGGRAPH2024ConferencePapers,pages1–12,2024. 1
4 [47] XianfangZeng,XinChen,ZhongqiQi,WenLiu,ZiboZhao,
[33] KarenSimonyanandAndrewZisserman. Verydeepconvo- ZhibinWang,BinFu,YongLiu,andGangYu.Paint3d:Paint
lutional networks for large-scale image recognition. arXiv anything3dwithlighting-lesstexturediffusionmodels. In
preprintarXiv:1409.1556,2014. 4 CVPR,pages4252–4262,2024. 1,2,5,7
[34] JingxiangSun,BoZhang,RuizhiShao,LizhenWang,Wen [48] ZhengZeng,ValentinDeschaintre,IliyanGeorgiev,Yannick
Liu,ZhendaXie,andYebinLiu.Dreamcraft3d:Hierarchical Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and
3dgenerationwithbootstrappeddiffusionprior.ICLR,2024. Milosˇ Hasˇan. Rgb-x: Image decomposition and synthesis
2 usingmaterial-andlighting-awarediffusionmodels.InACM
[35] JunshuTang,TengfeiWang,BoZhang,TingZhang,RanYi, SIGGRAPH2024ConferencePapers,pages1–11,2024. 3
LizhuangMa,andDongChen.Make-it-3d:High-fidelity3d [49] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
creationfromasingleimagewithdiffusionprior. InICCV, conditional control to text-to-image diffusion models. In
pages22819–22829,2023. 2 ICCV,pages3836–3847,2023. 4
[36] JiaxiangTang,ZhaoxiChen,XiaokangChen,TengfeiWang, [50] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu,
GangZeng,andZiweiLiu.Lgm:Largemulti-viewgaussian AnqiPang,HaoranJiang,WeiYang,LanXu,andJingyiYu.
modelforhigh-resolution3dcontentcreation. InEuropean Clay: Acontrollablelarge-scalegenerativemodelforcreat-
ConferenceonComputerVision,pages1–18.Springer,2024. inghigh-quality3dassets. ACMTransactionsonGraphics
2 (TOG),43(4):1–20,2024. 2,3
[37] Shimon Vainer, Mark Boss, Mathias Parger, Konstantin [51] ShangzhanZhang,SidaPeng,TaoXu,YuanboYang,Tian-
Kutsy, Dante De Nigris, Ciara Rowles, Nicolas Perony, runChen, NanXue, YujunShen, HujunBao, RuizhenHu,
and Simon Donne´. Collaborative control for geometry- andXiaoweiZhou. Mapa: Text-drivenphotorealisticmate-
conditionedpbrimagegeneration. InEuropeanConference rialpaintingfor3dshapes. InACMSIGGRAPH2024Con-
onComputerVision,pages127–145.Springer,2024. 3 ferencePapers,pages1–12,2024. 1,3
[38] GiuseppeVecchio, RosalieMartin, ArthurRoullier, Adrien [52] YuqingZhang,YuanLiu,ZhiyuXie,LeiYang,Zhongyuan
Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Liu,MengzhouYang,RunzeZhang,QilongKou,ChengLin,
Boubekeur. Controlmat: A controlled generative approach WenpingWang,etal. Dreammat: High-qualitypbrmaterial
tomaterialcapture. ACMTOG,43(5):1–17,2024. 3 generationwithgeometry-andlight-awarediffusionmodels.
[39] TengfeiWang,BoZhang,TingZhang,ShuyangGu,Jianmin ACMTransactionsonGraphics(TOG),43(4):1–18,2024.1,
Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang 3,5,7
Wen, Qifeng Chen, et al. Rodin: A generative model for [53] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li,
sculpting3ddigitalavatarsusingdiffusion. InCVPR,pages Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane
4563–4573,2023. 2 meetsgaussiansplatting:Fastandgeneralizablesingle-view
[40] ZhengyiWang,ChengLu,YikaiWang,FanBao,Chongxuan 3d reconstruction with transformers. In Proceedings of
Li,HangSu,andJunZhu.Prolificdreamer:High-fidelityand theIEEE/CVFConferenceonComputerVisionandPattern
diversetext-to-3dgenerationwithvariationalscoredistilla- Recognition,pages10324–10335,2024. 2
tion. NeurIPS,2023. 2
[41] ZhenweiWang,TengfeiWang,GerhardHancke,ZiweiLiu,
and Rynson WH Lau. Themestation: Generating theme-
10Material Anything: Generating Materials for Any 3D Object via Diffusion
Supplementary Material
A.Material3DDataset
A.1.DataConstruction
To train Material Anything, we constructed a dataset Ma-
terial3D that consists of 80K high-quality 3D objects cu-
rated from the Objaverse dataset, focusing specifically on
modelswithcomprehensivematerialmaps. Wefurtherfil-
tered objects using Blender, ensuring the presence of es-
sentialmaterialmaps: basecolor, roughness, metallic, and
bump. Only models containing all these material proper-
tieswereretained. EachobjectwasimportedintoBlender,
whereBlender’sSmartUVProjecttoolwasusedtogener-
ateUVmappings. Forobjectswithmultipleparts,allcom-
ponentsweremergedintoasinglemeshtoensureUVmap-
ping could be projected onto a unified 2D map. After this
filtering and preparation process, we rendered multi-view
material images (albedo, roughness, metallic, and bump)
from10fixedcamerapositionsthatareconsistentwiththe
setup used in our material generation phase, which served
as training data for the material estimator. In addition, we
rendered images under varying lighting conditions and in-
cludednormalmapsasinputformodeltraining,providing
diverselightingandsurfaceinformation.UVmaterialmaps
andCCMwerealsorenderedtofacilitatethetrainingofthe
materialrefiner.
A.2.LightingConditions
Inspiredbytheimagerelightingmethod[46], weincorpo-
rated multiple lighting categories for rendering input im-
ages,enablingthemodeltohandlediverselightingscenar-
ios.
1. Point Lighting. Point light sources are uniformly sam-
pledfromahemisphere(with0◦ ≤ θ ≤ 60◦)surround-
ing the object, with a radius sampled in the range [4m,
5m]. The number of point lights is randomly sampled
between[1,3]. Thesumpowerofalllightsisuniformly
chosen within [900W, 2400W]. To ensure the visibility
ofhighlightedregions,thehemisphereisrotatedaccord-
ing to the camera position, while the camera itself re-
mainedfixedatthetopofthehemisphere. Figure 13. The virtualization of our training data. We apply
2. Area Lighting. Similar to point lighting, area light variousdegradationsandsimulateinconsistentlightingeffectsin
sourcesaresampledfromahemisphere(with0◦ ≤ θ ≤ theinputstoenhancetherobustnessofourmethod.
60◦)witharadiusfrom4mto5m. Thesizeofthearea
lightrangesfrom3mto10m,anditspowerisuniformly
selectedwithin[1000W,2000W].Onlyonearealightis in diffusion-generated images, we employ white envi-
utilizedduringrendering. ronment lighting with strengths ranging from [0.5, 3],
3. Environment Lighting. Environmental lighting broadly avoidingcoloredHDRenvironmentmaps.
influences scene illumination beyond isolated light 4. Without lighting. To ensure the material estimator can
sources. To counter the white balance bias common accuratelypredictmaterialsindependentoflightingcon-
1Figure 15. Material editing with prompts. Material Anything
Figure14. Thecameraposesforprogressivematerialgenera- enablesflexibleeditingandcustomizationofmaterialsfortexture-
tionandbuildingtrainingdata. less3Dobjectsbysimplyadjustingtheinputprompt.
ditions, we render views of objects using only albedo the AdamW optimizer with a learning rate of 5 × 10−5.
textures, which is the same as rendering multi-view
Ourmaterialestimatorwastrainedover300Kiterationson
albedomaps.
8 NVIDIA A100 GPUs with a batch size of 32, requiring
For each camera position, we render 13 images (includ- approximately 5 days to complete. In parallel, the mate-
ing albedo, roughness, metallic, bump, normal maps, and rial refiner was trained for 150K iterations under the same
8 RGB images under point, area, and environment light- GPUconfigurationandbatchsize,withatrainingduration
ing). ForUVmaterialmaprendering,weutilizeBlender’s ofabout2days. Trainingdatawasrenderedataresolution
smart UV project to unwarp the mesh, producing five UV of 512×512 using Blender’s Cycles path tracer, ensuring
spacemaps(albedo,roughness,metallic,bump,andcanon- high-qualityreferencematerialsforrobustlearning.
icalcoordinatemaps).
B.2.MaterialGenerationDetails
A.3.SimulatingInconsistentLightingEffects
During material generation, each input object is centered
Toimprovetherobustnessofthematerialestimator,weran-
withinanormalizedboundingbox. Tocapturecomprehen-
domlyselecttwoimagesunderdifferentlightingconditions
sive material properties, 6 or 10 views are rendered, as il-
foracameraviewandstitchportionsofeachintoacompos-
lustratedinFig.14. Theinputimageresolutionforourma-
ite during training. This enables a single image to exhibit
terial refiner is set to 768 × 768, while the resolution for
two distinct lighting types, simulating the inconsistency in
UVmaterialmapsis1024×1024. Thissetupensureshigh-
multi-view materials. Furthermore, we introduce degrada-
fidelitymaterialmapsthataredetailedandadaptableacross
tions to one of the images, applying effects such as blur-
differentviewingangles. FortheinputobjectswithoutUV
ring and color shifts. A confidence mask is used to delin-
mappings, xatlas [44] is used to unwrap them. All results,
eatetheregionsthathaveundergonedegradation. Thefinal
includingthosefromourmethodandthebaselines,aregen-
input to the material estimator comprises the stitched im-
eratedonasingleNVIDIAA100GPU.
age, the confidence mask, and the normal map, as shown
in Fig. 13 (a). To train the material refiner, we randomly
C.Applications
maskregionsoftheUVmaterialmapsandapplydegrada-
tions such as blurring and color shifts. These masked ma- MaterialAnythingoffersrobustcapabilitiestoeditandcus-
terialmapsaretakenasinput,asshowninFig.13(b). The tomize materials of texture-less 3D objects by simply ad-
CCM, derived from the UV mapping of 3D point coordi- justingtheinputprompt,enablingflexibleandintuitivema-
nates,isalsoincluded. Thesemapsguidetheareasrequir- terial manipulation. As illustrated in Fig. 15, we demon-
inginpaintingandfacilitatetheintegrationof3Dadjacency stratethatabarrel’smaterialcanbetransformedintorealis-
informationduringthediffusionprocess. tictextureslikewood,gold,andstone,showcasingthever-
satilityofourapproachacrossvariousmaterialtypes. This
B.ImplementationDetails applicationallowsuserstodynamicallyadapt3Dmodelsto
specificaestheticorfunctionalrequirements,enhancingas-
B.1.TrainingDetails
set adaptability for virtual environments, simulations, and
We implemented Material Anything using the Dif- designvisualization.
fusers [17], with Stable Diffusion v2.1 [30] serving as the Furthermore, our method supports relighting, enabling
backbone diffusion model. The training process leverages objects to be viewed under different lighting conditions,
2Figure16.RelightingresultsbyMaterialAnythingundervariousHDRenvironmentmaps.Theleftcolumndisplaystheinputtexture-
lessmeshes,whilethetoprowpresentstheHDRenvironmentmapsused.
D.LimitationsandFailureCases
MaterialAnythingisdesignedtoaddressthecomplextask
of generating materials for a diverse range of 3D objects.
However, our approach has certain limitations. First, ow-
ingtothecharacteristicsoftheObjaverse,wheremanyob-
jectsexhibituniformroughnessandmetallicattributeswith
minimalsurfacedetailsinbumpmaps,ourmethodmaypro-
ducematerialswithconstrainedsurfacedetails.Thislimita-
tionisillustratedintheelephantexampleinFig.17,where
Figure17.FailureCasesbyMaterialAnything.
theresultingbumpmapslackdetails. Additionally,forob-
jectswithexistingtextures,ourmethodstrugglestoremove
prominent artifacts. For example, in the apple instance in
Fig. 17, large white artifacts are misinterpreted as part of
thetexture,resultinginaninaccuratealbedo.
as shown in Fig. 16. Material Anything generates mate- E.AdditionalResults
rial properties for each object, ensuring physically consis-
tentrelightingandenhancedrealism. Thisfunctionalityal- We present additional qualitative results to illustrate the
lowsformoreaccuratesimulationsinAR,VR,anddigital effectiveness of Material Anything. Video results are
contentcreation,whererealisticlightingisessentialforim- present in our supplementary video. In Fig. 18, we
mersion. Collectively,thesecapabilitiesmaketheproposed display results generated by our material estimator on the
methodaversatileandefficientsolutionforcontentcreators Objaverse dataset, compared with their GT materials. As
andresearchersaimingtoproducehigh-quality,relightable shown, our method effectively generates materials closely
3Dobjectswithcustomizedmaterials. aligned with the ground truth, capturing essential details
3and textures to enhance realism. In Fig. 19, we show ad-
ditional results on texture-less inputs, demonstrating our
method’scapabilitytohandlecomplexUVmappings. De-
spite the complexity of certain UV layouts, our method
consistently generates high-quality material maps in UV
space, preserving material fidelity across the entire sur-
face. Finally,wepresentadditionalresultsonvariousinput
types,includinggeneratedmodels,albedo-onlyinputs,and
scanned 3D objects. These examples, shown in Fig. 20,
highlight our method’s robustness across varied lighting
conditions and input characteristics, demonstrating its ver-
satilityinproducingrealisticmaterialsadaptabletodiverse
lightingenvironments.
4Figure18.Resultsbyourmaterialestimatoron2DrenderingsfromObjaverse.
5Figure19.AdditionalresultsbyMaterialAnythingontexture-less3Dobjects.ThegeneratedUVmaterialmapsareprovided.
6Figure20.AdditionalresultsbyMaterialAnythingonalbedo-only,generated,scanned3Dobjects.
7