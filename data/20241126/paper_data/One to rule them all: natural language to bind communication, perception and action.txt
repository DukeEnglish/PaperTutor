One to rule them all: natural language to bind
communication, perception and action
SimoneColombani1,3, DimitriOgnibene2 and GiuseppeBoccignone1
2UniversityofMilan,Italy
1UniversityofMilano-Bicocca,Milan,Italy
3OversonicRobotics,CarateBrianza,Italy
Abstract
Inrecentyears,researchintheareaofhuman-robotinteractionhasfocusedondevelopingrobotscapableof
understandingcomplexhumaninstructionsandperformingtasksindynamicanddiverseenvironments.These
systemshaveawiderangeofapplications,frompersonalassistancetoindustrialrobotics,emphasizingthe
importanceofrobotsinteractingflexibly,naturallyandsafelywithhumans.
Thispaperpresentsanadvancedarchitectureforroboticactionplanningthatintegratescommunication,percep-
tion,andplanningwithLargeLanguageModels(LLMs).Oursystemisdesignedtotranslatecommandsexpressed
innaturallanguageintoexecutablerobotactions,incorporatingenvironmentalinformationanddynamically
updatingplansbasedonreal-timefeedback.
ThePlannerModuleisthecoreofthesystemwhereLLMsembeddedinamodifiedReActframeworkareemployed
tointerpretandcarryoutusercommandslike‘Gotothekitchenandpickupthebluebottleonthetable’. By
leveragingtheirextensivepre-trainedknowledge,LLMscaneffectivelyprocessuserrequestswithouttheneed
tointroducenewknowledgeonthechangingenvironment.ThemodifiedReActframeworkfurtherenhances
theexecutionspacebyprovidingreal-timeenvironmentalperceptionandtheoutcomesofphysicalactions.By
combiningrobustanddynamicsemanticmaprepresentationsasgraphswithcontrolcomponentsandfailure
explanations,thisarchitectureenhancesarobot’sadaptability,taskexecutionefficiency,andseamlesscollabora-
tionwithhumanusersinsharedanddynamicenvironments.Throughtheintegrationofcontinuousfeedback
loopswiththeenvironmentthesystemcandynamicallyadjuststheplantoaccommodateunexpectedchanges,
optimizingtherobot’sabilitytoperformtasks. Usingadatasetofpreviousexperienceispossibletoprovide
detailedfeedbackaboutthefailure.UpdatingtheLLMscontextofthenextiterationwithsuggestiononhowto
overcametheissue.
ThissystemhasbeenimplementedonRoBee,thecognitivehumanoidrobotdevelopedbyOversonicRobotics,
showcasingitsadaptabilityandpotentialforintegrationacrossdiverseenvironments.ByleveragingLLMsand
semanticmapping,thearchitectureenablesRoBeetonavigateandrespondtoreal-timechanges.
Keywords
Human-Robotinteraction,Robottaskplanning,LargeLanguageModels,Automatedplanning
1. Introduction
TheintegrationofLLMsinroboticsystemshasopenednewavenuesforautonomoustaskplanning
and execution [2, 3]. These models demonstrate exceptional natural language understanding and
commonsensereasoningcapabilities,enhancingarobot’sabilitytocomprehendcontextsandexecute
commands[4,5]. HoweverLLMsarenotbeabletoplanautonomously,theyneedtobeintegratedin
architecturesthatenablethemtounderstandtheenvironment,therobotcapabilitiesandstate[6]. This
researchaimstoempowerrobotstocomprehenduserrequestsandautonomouslygenerateactionable
plansindiverseenvironments.
Theefficacyoftheseplansreliesontherobot’sunderstandingofitsoperatingenvironment[7]. To
bridgethisgap,ourworkemploysscenegraphs[8]asasemanticmappingtool,offeringastructured
representationofspatialandsemanticinformationwithinascene.
AI4CC-IPS-RCRA-SPIRIT2024:InternationalWorkshoponArtificialIntelligenceforClimateChange,ItalianWorkshoponPlanning
andScheduling,RCRAWorkshoponExperimentalevaluationofalgorithmsforsolvingproblemswithcombinatorialexplosion,
andSPIRITWorkshoponStrategies,Prediction,Interaction,andReasoninginItaly.November25-28th,2024,Bolzano,Italy[1].
$simone.colombani@studenti.unimi.it(S.Colombani);dimitri.ognibene@unimib.it(D.Ognibene);
giuseppe.boccignone@unimi.it(G.Boccignone)
©2024Copyrightforthispaperbyitsauthors.UsepermittedunderCreativeCommonsLicenseAttribution4.0International(CCBY4.0).
4202
voN
22
]OR.sc[
1v33051.1142:viXraInourapproach,weleverageLLMsthroughin-context[9],whichenablesthemodelstolearnandadapt
based on the information provided in the context. Our work implements a modified version of the
ReAct[10]frameworkthatexpandthecontextofLLMswithenvironmentalinformationandexecution
feedback,allowingthemodeltoplanandexecuteskills[11]translatingthemintophysicalactions.
Motivation The primary focus of our work is to enable robot to interact flexibly and robustly in
dynamicanddiverseenvironmentswithlimitedhumanintervention.Traditionalroboticsystemsusually
relyonstatic,pre-programmedinstructionsorclosedworldpredefinedknowledgeandsettings,limiting
theiradaptabilitytodynamicenvironments. Interactingwithhumansindailytaskswithincomplex
environments disrupts these assumptions. LLMs and VLM can provide open-domain knowledge to
representnovelconditionswithouthumanintervention. However,thesemodelsarenotinformedof
thespecificrobot,taskandsettingsathand,thatdefinewhatinformationcanberelevantandnecessary
tofindandreasonabout[12]. Exceedinginthelevelofdetailmayleadtoimpracticalcomputational
requirements and response time. Discarding crucial information, spatial or semantic, may lead to
repeatedfailuresduetotheintroducednon-managedpartialobservability[13]. Tofindtherelevant
informationmaybetooslow[14]. LLMscanstillproduceoutputsthatarelogicallyinconsistentor
impractical[15],expeciallyiftheyarenotintegratedintosystemsthatallowthemtoadapttochanges
in the environment and the physical capabilities of the robots. Finally task execution, robots may
encounter unexpected situations, such as unanticipated obstacles, sensor errors, or changes in the
environmentthatwerenotaccountedforintheinitialplan. Suchscenariosnecessitaterobusterror
handlingmechanismsandadaptiveplanningstrategiesthatenablethesystemtoreassessandmodify
itsactionsin real-time[16]. Byintroducing executioncontrollingandfailuremanagementintothe
planning process at different levels as well as retrieval of previous successful plans, we propose a
solutiontoenhancetherobustnessandflexibilityofLLM-basedroboticsystems. Thisapproachensures
thattherobotcaneffectivelyperceivechangesintheenvironmentandthefailuresthatmayarisefrom
them,allowingittoadaptstrategiesinresponsetonewchallenges.
Proposedapproach Oursystemaddressesthechallengesofdynamicenvironmentsthroughareal-
timeperceptionmoduleandaPlannermodulethatintegratesexecutioncontrol,andfailuremanagement.
ItcompriseaControllerthatmonitorstheexecutionoftasksanddetectserrors,whiletheExplainer
analyzes failures and suggests adjustments based on past experiences. This feedback loop enables
adaptivere-planning,allowingthesystemtomodifyitsactionsasneeded. Specifically,weproposethe
useoftheReAct[10]framework,expandingitsoperationalspacewithskills,physicalactionsofthe
robotandwithperceptionaction,toaccessinformationfromtheenvironment. ByleveragingLLMsfor
naturallanguageunderstandingandaperceptionsystem,thearchitecturesupportsautonomoustask
executionindynamicscenarios.
2. Related works
AsubstantialbodyofliteratureexplorestheutilizationofLLMsforrobotictaskplanning[4,5].
LLMforrobotplanning RecentworkshighlightthepotentialofLargeLanguageModels(LLMs)
in robotic planning [17, 18, 19]. DEPS [20] introduces an iterative planning approach for agents in
open-worldenvironments,suchasMinecraft. ItutilizesLLMstoanalyzeerrorsduringexecutionand
refineplans,improvingbothreasoningandgoalselectionprocesses. However,thisapproachhasbeen
primarily developed and tested in virtual environments, with notable differences in comparison to
real-worldsettingsduetothedynamicandunpredictablenatureofphysicalenvironments. Additionally,
DEPSdoesnotleveragepreviousissuesandsolutionsbutreliessolelyonfeedbackfromhumansand
vision-languagemodels(VLMs).Figure1:Architectureofthesystem.
Scene graph as environemental representation The use of scene graphs [21] as a means to
represent the robot’s environment has gained traction. [22] employs 3D scene graphs to represent
environmentsandusesLLMstogeneratePlanningDomainDefinitionLanguage(PDDL)files. This
methoddecomposeslong-termgoalsintonaturallanguageinstructionsandenhancescomputational
efficiencybyaddressingsub-goals. However,itlacksamechanismforreplanningbasedonfeedback
during execution, which could limit its adaptability in dynamic scenarios. SayPlan [23] integrates
semanticsearchwithscenegraphsandpathplanningtoaidrobotsinnavigatingcomplexenvironments
throughnaturallanguage. Bycombiningthesetechniques,SayPlansimulatesvariousscenariostorefine
tasksequences,whichhelpsimproveoveralltaskperformanceincomplexenvironments. However,itis
relianceonstaticpre-built3Dscenegraphs,hinderingadaptabilitytodynamicreal-worldenvironments.
Replanning Replanningenableslong-termautonomoustaskexecutioninrobotics[24]. DROC[25]
empowers robots to process natural language corrections and generalize that information to new
tasks. It introduces a mechanism to distinguish between high-level and low-level errors, allowing
moreflexibleplancorrections. However,DROCdoesnotaddressthetypesoffailuresthatmayoccur
during plan execution, focusing instead on high-level corrections provided by users. [26] supports
autonomouslong-termtaskexecutionbyintegratingLLMsforplanningandVLMsforfeedback. This
approachadaptstochangesintheenvironmentthroughastructuredcomponentsystemthatverifies
and corrects plans as needed. Yet, the feedback is limited to what is visible to the robot’s camera,
potentiallyoverlookingothersignificantenvironmentalchanges.
3. Architecture
Oursystemisbasedontwocomponents:
• PerceptionModule: itisresponsibleforsensingandinterpretingtheenvironment. Itbuilds
andmantainsasemanticmapintheformofadirectedgraphthatintegratesbothgeometricand
semanticinformation.
• PlannerModule: ittakestheinformationprovidedbythePerceptionModuletoformulateplans
andactionsthatallowtherobottoperformspecifictasks.
Figure 1 show how these components interact to allow the robot to understand its environment
andactaccordinglytosatisfyuserrequests. ThePerceptionmoduleusesdataprovidedbytherobot’s
sensors to supply the semantic map to the Planner module, which in turn processes it to generate
specificactionplans. InwhatfollowswepreciselyaddressthePlannerModulewhiledetailsontheFigure2:Architectureoftheplannermodule.
PerceptionModulewillbeprovidedinaseparatearticle.
3.1. Plannermodule
The architecture of the Planner module is designed to translate user requests, expressed in natural
language, into specific actions executableby a robot. Thismodule isresponsible for understanding
instructions,planningappropriateactions,andmanagingtheexecutionofthoseactionsinadynamic
environment. ThePlanningmoduleiscomposedbyfivesub-modules:
• Task Planner: Translates user requests, expressed in natural language, into a sequence of
high-levelskills.
• SkillPlanner: Translateshigh-levelskillsintospecific,low-levelexecutablecommands.
• Executor: Executesthelow-levelactionsgeneratedbytheSkillPlanner.
• Controller: Monitorstheexecutionofactionsandmanagesanyerrorsorunexpectedevents
duringtheprocess.
• Explainer: Interprets the causes of execution failures by analyzing data received from the
ControllerandprovidessuggestionstotheTaskPlanneronhowtoadjusttheplan.
ThearchitectureoftheplannermoduleisshowninFigure2. Themaincomponentofthesystemis
the Task Planner, which receives the user’s request and translates it into a list of high-level "skills"
thatrepresenttherobot’scapabilities. Theseskillsincludeactionssuchas"PICK"(graspanobject),
"PLACE"(placeanobject),and"GOTO"(movetoaposition).3.1.1. TaskPlanner
The decision-making process of the Task Planner is driven by a policy, which is implemented as a
LLM.Apolicyisastrategyorrulethatdefineshowactionsareselectedbasedonthecurrentstateor
context,[27].
TaskPlannerisimplementedusingtheReActframework[10], whichalternatesbetweenreasoning
and action phases during the process. In the reasoning phase, the Task Planner can access various
"perception"actionstogatherinformationfromtheenvironment,suchasthesemanticmapandthe
currentstateoftherobot,andcanexecuteoneormore"skill"actionstoperformphysicalactions.
The classical idea of ReAct is to augment the agent’s action space to 𝐴ˆ = 𝐴 ∪ 𝐿, where 𝐿 is the
spaceoflanguage-basedreasoningactions. Anaction𝑎ˆ ∈ 𝐿,referredtoasa"thought" orreasoning
𝑡
trace,doesnotdirectlyaffecttheexternalenvironmentbutinsteadupdatesthecurrentcontext𝑐 =
𝑡+1
(𝑐 ,𝑎ˆ ) by adding useful information to support future decision-making [10]. In the classical idea
𝑡 𝑡
therecouldbevarioustypesofusefulthoughts,suchasdecomposingtaskgoalsandcreatingaction
plans, injecting commonsense knowledge relevant to task solving, extracting important parts from
observations,trackingprogressandtransitioningactionplans,handlingexceptionsandadjustingaction
plans,andsoon,butalwayswithoutmodifyingthephysicalenvironment,onlyembeddingitwithin
thecontext. Interestingly,thisapproachmixesreasoningandactioninaflexiblemanner. Inthefuture,
we will analyse the potential of this approach also connecting to the planning-to-plan [28, 29] and
meta-reasoning[30,31,32]concepts.
Inourwork,weaugmenttheagent’s[33]actionspacewithtwotypesofactions:
• A skill action 𝑎
𝑡
∈ 𝐴skill, which involves physically interacting with the environment, such
asmanipulatingobjectsornavigating. Theresultofaskillactionprovidesnewfeedbackthat
updatesthecurrentcontext.
• Aperceptionaction𝑎
𝑡
∈ 𝐴perception,whichinvolvesaccessinginformationfromtheenvironment,
suchasqueryingthesemanticmaporsensors,andintegratingthatinformationintothecontext.
Theaugmentedactionspaceisdefinedas:
𝐴ˆ = 𝐴skill∪𝐴perception∪𝐿
Thus,theLLMservesasthepolicy𝜋 thatselectsdifferenttypesofactionsfromtheaugmentedaction
spaceanddynamicallyadaptingthecurrentcontext𝑐 usedtoplanbasedonreal-timeinformationand
𝑡
reasoning.
FormalDescription: TheTaskPlanner’spolicy𝜋,representedbytheLLM,canbeformalizedasa
functionthatmapsthecurrentcontext𝑐 toanaction𝑎ˆ fromtheaugmentedactionspace𝐴ˆ:
𝑡 𝑡
𝜋 : 𝐶 → 𝐴ˆ, 𝜋(𝑐 ) = 𝑎ˆ
𝑡 𝑡
Where:
• 𝐶 isthesetofallpossiblecontexts.
• 𝐴ˆ istheaugmentedactionspace𝐴ˆ = 𝐴skill∪𝐴perception∪𝐿.
• 𝑐 representsthecurrentcontextattime𝑡,whichincludesthestateoftherobot,theenvironment,
𝑡
andanypastactionsorthoughts.
• 𝑎ˆ
𝑡
∈ 𝐴ˆ istheactionchosenbythepolicy,whichcanbeaskillaction𝑎
𝑡
∈ 𝐴skill,aperception
action𝑎
𝑡
∈ 𝐴perception,orareasoningtrace𝑎ˆ
𝑡
∈ 𝐿.
Thecontext𝑐 isupdatedbasedonthechosenaction:
𝑡
• If𝑎ˆ ∈ 𝐿(areasoningaction),thecontextupdatesto:
𝑡
𝑐 = (𝑐 ,𝑎ˆ )
𝑡+1 𝑡 𝑡
This represents the thought process, where reasoning contributes new information without
affectingtheexternalenvironment.• If 𝑎ˆ
𝑡
∈ 𝐴perception (a perception action), the result of querying the environment updates the
context:
𝑐
𝑡+1
= (𝑐 𝑡,𝑓perception(𝑎ˆ 𝑡))
Here,𝑓perception representsthefunctionthatgathersinformationandmodifiesthecontextbased
ontheperceptionaction’soutcome.
• If𝑎ˆ
𝑡
∈ 𝐴skill (askillaction),therobotinteractswiththeenvironment,andthecontextupdates
basedonfeedbackfromthephysicalaction:
𝑐
𝑡+1
= (𝑐 𝑡,𝑓skill(𝑎ˆ 𝑡))
Where𝑓skill isthefunctionthatcapturestheresultofexecutingaphysicalskill,suchasmanipu-
latinganobjectormovingtoalocation.
3.1.2. SkillPlanner
Once a high-level request for the execution of a skill is made, the Skill Planner is responsible for
translatingthehigh-levelskills,providedbytheTaskPlanner,intosequencesoflow-levelcommands
executablebytherobot.WhiletheTaskPlannerfocusesonunderstandingnaturallanguageandcreating
ageneralplan,theSkillPlannerdealswiththespecificdetailsofhoweachskillshouldbeexecuted,
consideringtherobot’sstateandtheenvironment.
Let a skill be represented in the following general form, defined by the Task Planner with specific
syntax:
𝑆𝐾𝐼𝐿𝐿_𝑁𝐴𝑀𝐸(𝑝𝑎𝑟𝑎𝑚 1,𝑝𝑎𝑟𝑎𝑚 2,...,𝑝𝑎𝑟𝑎𝑚 𝑁)
Where:
• isthenameoftheskilltobeexecuted(e.g., , , ).
SKILL_NAME PICK PLACE GOTO
• are parameters for the skill, such as the object to
param_1, param_2, ..., param_N
manipulateorthedestinationtonavigateto.
UsingastrictsyntaxensuresthattheSkillPlannercancorrectlyinterpretthehigh-levelcommands
withoutambiguity. Forinstance,anaturallanguagecommandlike"Movenearthetableandgrabthe
bottle" wouldlackprecision. TheSkillPlannerneedsconcreteparametersfortherobottoacteffectively.
SkillPlannerworkflow: TheSkillPlanneroperatesbyperformingthreefunctions:
1. PreconditionVerification: Beforetranslatingaskillintolow-levelcommands,theSkillPlanner
verifiesthatthenecessarypreconditionsforexecutionaremet. Let𝑠 representthecurrentstateofthe
𝑡
robotandtheenvironmentattime𝑡,and𝑃(skill,𝑠 )denoteafunctionforeveryskillthatevaluatesthe
𝑡
preconditionsforagivenskill. Thepreconditioncheckcanbeexpressedas:
{︃
1, ifallpreconditionsaremet
𝑃(skill,𝑠 ) =
𝑡 0, otherwise
Forexample,beforeexecutingthe skill,thefollowingchecksmaybeperformed:
PICK
• Theobjectisvisiblebytherobot.
• Theobjectisreachablefortheroboticarm.
• Theroboticarmisfree.
Ifanyoftheseconditionsarenotmet(𝑃(skill,𝑠 ) = 0),theSkillPlannerreportsafailuretotheTask
𝑡
Planner.
2. Target nodes extraction: Based on the parameters of the skill, the Skill Planner extracts the
targetnodesfromthesemanticmapℳ,whichcontainsgeometricandsemanticinformationabouttheenvironment. Everynodeprovidesgeometricinformationsuchasobject’spositionandrelevant
context,whichisthenusedtogeneratelow-levelcommands.
3.GenerationofLow-LevelCommands:When𝑃(skill,𝑠 ) = 1,theSkillPlannertranslatetheskill
𝑡
intoasequenceoflow-levelcommandstocontroltherobotbehavior. Inthissystem,werepresentskill
decompositionincommandsasHierarchicalTaskNetworks(HTNs)thatcontainslow-levelcommands
executablebytherobot. Let𝐶𝑀(skill,node,𝑠 )denotethefunctionthattranslatesthegivenskillinto
𝑡
low-levelcommandsbasedonthetargetnodesextractedfromthesemanticmapandcurrentstate. The
outputisasequenceofpre-modeledcommandsparameterizedwiththeinformationoftherobotstate
andthetargetnodes,{𝑐𝑚 ,𝑐𝑚 ,...,𝑐𝑚 },whereeachcommand𝑐𝑚 directsspecificcomponentsof
1 2 𝑘 𝑖
therobot. OurimplementationuseHTNssolelyonthebreakdownofskillsintocommandswithout
usingthemwithadvancedfeatureslikere-planningorerrorrecoveryofthecommands. Inthiscase,if
anycommandfails,theentireskillfails,withnoattemptatre-planningattheskillplannerlevel. The
processcanberepresentedas:
{𝑐𝑚 ,𝑐𝑚 ,...,𝑐𝑚 } = 𝐶𝑀(skill,node,𝑠 )
1 2 𝑘 𝑡
TheSkillPlannerisdesignedtobeflexibleandextendable. Theskillfunctions𝑃,and𝐶𝑀 canbe
adaptedorextendedtoaccommodatenewskills,hardware,orenvironments.
3.1.3. Executor
TheExecutorisresponsiblefordirectlyinteractingwiththerobot’shardwaretoexecutethecommands
providedbytheSkillPlanner. Ittranslatesthelow-levelcommandsintophysicalactionsbycontrolling
varioushardwareelementssuchasmotors,roboticarmgrippers,andotheractuatorsrequiredfortask
execution.
Let the set of low-level commands generated by the Skill Planner be represented as above, i.e.,
𝑐𝑚 ,𝑐𝑚 ,...,𝑐𝑚 = 𝐶𝑀(skill,node,𝑠 ), where 𝐶𝑀(skill,node,𝑠 ) defines the sequence of com-
1 2 𝑘 𝑡 𝑡
mandsbasedontheskill,thetargetnode,andthecurrentstateoftherobotandtheenvironment.
TheExecutoristaskedwithexecutingthesecommandsonthephysicalrobot. Letthestateofthe
robotattime𝑡bedenotedbyℎ ,andthefunctionthatmapsalow-levelcommand𝑐 toaneffectonthe
𝑡 𝑖
robot’sstatebedenotedas𝐻(𝑐𝑚 ,ℎ ). Theexecutionofacommandattime𝑡canbedescribedas:
𝑖 𝑡
ℎ = 𝐻(𝑐𝑚 ,ℎ )
𝑡+1 𝑖 𝑡
whereℎ istheupdatedstateafterexecutingthecommand𝑐𝑚 . Thisprocessisrepeatedforeach
𝑡+1 𝑖
commandinthesequence{𝑐𝑚 ,𝑐𝑚 ,...,𝑐𝑚 }untiltheentireskillisexecuted.
1 2 𝑘
Executorworkflow:
• Commandreception:TheExecutorreceivesasetoflow-levelcommands{𝑐𝑚 ,𝑐𝑚 ,...,𝑐𝑚 }
1 2 𝑘
fromtheSkillPlanner. Eachcommandspecifiesaconcreteactiontobeperformedbytherobot’s
hardwarecomponents.
• Hardwareinteraction:Foreachcommand𝑐𝑚 ,theExecutorinteractswiththerobot’shardware,
𝑖
adjustingthemotors,grippers,andotheractuators. Thisinteractioncanberepresentedbythe
function𝐻(𝑐𝑚 ,ℎ )thatdeterminestheeffectofacommandontherobot’sstateℎ .
𝑖 𝑡 𝑡
• Commandexecution: TheExecutorexecuteseachcommand𝑐𝑚 inthesequence,ensuring
𝑖
thattherobot’sstatetransitionsfromstateℎ toℎ . Formally:
𝑡 𝑡+1
ℎ_𝑡+1 = 𝐻(𝑐𝑚_𝑖,ℎ_𝑡), ∀𝑖 = 1,2,...,𝑘
After executing all commands, the robot’s reaches the final state ℎ , corresponding to the
𝑡+𝑘
completionoftheskill.• Real-Timefeedback: Duringexecution,therobot’sprovidesfeedbackonitscurrentstate. Let
𝑓 denotethefeedbackattime𝑡,and𝑓 betheupdatedfeedbackafterexecutingcommand𝑐𝑚 :
𝑡 𝑡+1 𝑖
𝑓 = 𝐹(𝑐𝑚 ,ℎ )
𝑡+1 𝑖 𝑡
where 𝐹 is the feedback function. If unexpected feedback 𝑓 is received, the Executor can
𝑡+1
triggeradjustmentstotheplanorinformtheSkillPlannerofapotentialissue.
Differentrobotsmayusedifferentcommunicationprotocols,andhardwareconfigurations. Therefore,
theExecutormustbeadaptedforeachspecificrobotsystem,ensuringthatitcorrectlyinteractswith
therobot’shardware.
3.1.4. Controller
TheControllerisresponsibleformonitoringtherobot’sstatusandtheenvironmentduringcommand
execution,ensuringthattheyarecarriedoutasplanned. Aftereachcommandisexecuted,theExecutor
sendsfeedbackindicatingeithersuccessorfailure. Ifafailureoccurs,itresultsinthefailureoftheentire
skill. Uponthecompletionofallcommands,asuccessfeedbackwillindicatethesuccessfulexecutionof
theskill.
Denote 𝑓 the feedback from the Executor at time 𝑡. The Controller processes 𝑓 to determine the
𝑡 𝑡
outcomeoftheexecutedskills. Thefeedbackcanbeclassifiedintotwocategories: successandfailure.
Feedbackprocessing:
• Success: Ifthefeedback𝑓 indicatessuccessfulexecutionofacommandanditisthelastcom-
𝑡
mandtoexecute,theskillisconsideredsuccessfullycompleted,theControllersendsapositive
acknowledgmenttotheTaskPlannertocontinuetheplanningprocess. However,ifthefeedback
indicatessuccessbutthecommandisnotthelastone,theControllerwaitsfortheexecutionof
thenextcommand:
if𝑓 = Success =⇒ TaskPlannercontinues
𝑡
• Failure: Ifafailureoccursduringtheexecutionofanycommand,theplannedskillfailsandthe
Controllergeneratesafailuremessage𝑚 thatincludesthereasonforthefailure. Thismessage
𝑓
issenttotheExplainer. Let𝑒 representthespecificerrordetectedattime𝑡. Thefailuremessage
𝑡
canberepresentedas:
𝑚 = Failure(𝑒 )
𝑓 𝑡
where𝑒 canincludevariouserrorreasonssuchasobstaclesdetected,non-executabletrajectories,
𝑡
orenvironmentalchanges.
TheController’soperationishighlydependentonthespecificrobotsysteminuse,asitreliesonthe
characteristicsoftherobotandtheemployedsoftwaresystem. InaROSenvironment,forexample,the
ControllerinteractswithROSnodesthatcontroltherobot’shardware. Inourwork,RoBee,described
insection5,hasasystemthatallowstoobtainfeedbackontheexecutionofcommands.
3.1.5. Explainer
TheExplainercomponentplaysacriticalroleinenhancingtheplanningprocessbyprovidinginsights
totheTaskPlannerwhenfailuresoccurduringtheexecutionphase. Afterreceivingthefailurereason,
theExplainersearchesadataset𝒟 forpreviousinstancesofsimilarfailures. Thisdatasetcomprises
recordsoffailuresassociatedwithspecificskillsanduserrequests. Let𝒟 denotethesubsetofthe
𝑟
𝑓
datasetcontainingrecordsoffailuresandsolutionsrelatedtothesameskillanderrormessage. The
datasethasbeenmanuallybuiltbasedonpreviousexperiences,desiredbehaviors,andexpectedfailures.
Thesearchcanbeexpressedas:
𝒟 = {(𝑠 ,𝑢 ,𝑟 ) ∈ 𝒟|𝑠 = skill_name, 𝑒 = 𝑟 , 𝑢 ∼ user_request}
𝑟 𝑓 𝑘 𝑟 𝑓 𝑘 𝑟 𝑓 𝑟
where:• 𝑠 istheskillbeingexecuted(e.g.,PICK).
𝑘
• 𝑢 representsthespecificuserrequestassociatedwiththefailure.
𝑟
• 𝑟 isthefailurereasonprovidedbytheController
𝑓
• 𝑢 ∼ user_request indicates that the user request in the dataset is similar to the current user
𝑟
request.
Ratherthansearchingforanexactmatchtotheuser’srequest,theExplainerassessesthesimilarityof
theuser’srequest(𝑢 )totheinstancesinthedatasetlinkedtothesuggestion,usingcosinesimilarityin
𝑟
ourapproach[34]. Thismethodenablesthesystemtoidentifythemostrelevantpastinstances,even
whentheuser’srequestsarenotidentical.
Oncerelevantinstancesareidentified,theExplaineranalyzesthesecasestogenerateasuggestion𝑠
𝑔
fortheTaskPlanneronhowtoproceed. Thesuggestionisstructuredasfollows:
𝑠 = Suggest(𝒟 )
𝑔 𝑟
𝑓
Forinstance,iftheControllerreportsthefailurereason:
𝑟 = "CannotexecutetheapproachmovementforthePICKskill,objecttoofar"
𝑓
The Explainer analyzes this failure and may find a previous instance where the robot successfully
resolvedasimilarissue. ItcouldrecommendacommandtotheTaskPlanner:
𝑠 = "UsetheGOTOskilltomoveneartheobjecttopick"
𝑔
ThissuggestionenablestheTaskPlannertoadjustitsstrategyeffectively,movingtherobotcloserto
theobjectbeforeattemptingthePICKactionagain.
ThesuggestionsprovidedbytheExplainercanbetailoredtoaccommodatespecificbehaviorsofthe
robot. Thisadaptabilitycanbeachievedbymodifyingtheparametersofthedatautilizedtogenerate
thesuggestions. Thus, theExplainerenhancestheresilienceofthesystem, facilitatingitsabilityto
adapttochangingconditionsandrecoverfromerrorsefficiently.
4. Behavior example of the system
Toillustratetheproposedsystem’sbehaviorinadynamicenvironment,considerascenariowherethe
robotistaskedwithpickingupabottlefromatableinaroomandplacingitinadesignatedareain
otherroom. Inthisexample,thesystemmustreacttounexpectedchanges,suchasthebottlebeing
movedbyanexternalagentduringthetask.
Example User Request: The user provides the instruction: "Go to the table in the kitchen, pick
up the bottle, and place it on the table in the bedroom." Perception Module: The robot senses the
environment and generateds a semantic map, which includes the pose and types of objects. Task
Planner: Therequestisinterpreted,andtheTaskPlannerbeginstheplanningprocedure. Itstartsby
callingperceptionactionstogatherinformationabouttheenvironmentandtherobot’sstate. Using
the results of these perception actions, the Task Planner determines the next steps in its execution
bycallingskillactionsandincorporatingtheoutcomesofeachskillintotheplanningprocess. LLM
interpretstherequestandreasonsasfollows.UserRequest: "Gotothetableinthekitchen,pickupthebottle,andplaceitonthetableinthe
bedroom."
RobotState: Therobotiscurrentlyinthebedroomandhasboththerightandleftarmsempty.
Thought:Thetaskistofetchthebottlefromthekitchenandplaceitonthetableinthebedroom.
Ineedtoknowalltheroomswheretherobotcangoto.
Perceptionaction: GetMapRooms()
Observation: Therobotidentifiestherooms: [kitchen, bedroom].
Thought: Now,Iwillnavigatetothekitchentofindthebottle.
Skillaction: GOTO(kitchen)
Observation: Therobotmovestothekitchen.
Thought: Ishouldidentifythespecificlocationofthebottlewithinthekitchen.
Perceptionaction: GetObjectInRoom(kitchen)
Observation: Therobotfindsthefollowingobjectsinthekitchen: [sink on the table_1,
.
stove on the table_1, bottle, table_1, table_2]
Thought: Iwillnowmovetothelocationofthebottle.
Skillaction: GOTO(bottle)
Observation: Therobotmovestothebottle’slocationinthekitchen.
Thought: Iwillpickupthebottlenow.
Skillaction: PICK(bottle, right)
Observation: Therobotcan’tseethebottletopick,findabottleinthesemanticmapandtry
againtopickit.
Thought: Thebottleisnotvisiblewhereexpected,Ineedtocheckthelocationofthebottle
again.
Perceptionaction: GetObjectInRoom(kitchen)
Observation: Therobotfindsthefollowingobjectsinthekitchen: [sink on the table_1,
.
stove on the table_1, bottle on the table_2, table_1, table_2 ]
Thought: Iwillgotothetabletopickupthebottleinstead.
Skillaction: GOTO(table_2)
Observation: Therobotmovestothetable.
Thought: Iwillpickupthebottlefromthetablenow.
Skillaction: PICK(bottle, right)
Observation: Therobotsuccessfullypicksupthebottle.
Thought: Next,Ineedtonavigatetothebedroomtoplacethebottle.
Skillaction: GOTO(bedroom)
Observation: Therobotmovestothebedroom.
Thought: IneedtolocatethetableinthebedroomwhereIwillplacethebottle.
Perceptionaction: GetObjectInRoom(bedroom)
Observation: Therobotsees: [table, bed, lamp on the table]inthebedroom.
Thought: Iwillapproachthetabletoplacethebottle.
Skillaction: GOTO(table)
Observation: Therobotapproachesthetable.
Thought: Finally,Iwillplacethebottleonthetable.
Skillaction: PLACE(bottle, right)
Observation: Therobotplacesthebottleonthetablesuccessfully.
SkillPlanner: Foreachhigh-levelskill,oncethepreconditionsaremet,theSkillPlannertranslates
thechoosenskillinasequenceoflow-levelcommands,suchasmotormovementsfornavigation,armarticulationforpicking,andplacingactions. Forexampleoncethe skillis
PICK(bottle, right)
planned,itcanbetranslatedanddividedintothefollowingphaseswithrelativecommands:
• Approach:Therobotarmmovestowardstheobject’sposition,makinganynecessaryadjustments
toaligncorrectly,andopensthegripper.
• Grasp: The robot activates the gripping mechanisms to seize the object. This phase includes
closingthegripperandverifyingthegrasp.
• Lifting: Therobotliftstheobjectfromthesurfaceitison.
Execution: TheExecutorbeginsexecutingtheplannedskill,whichiscomposedofasequenceof
commandsbytheSkillPlanner. TheExecutorfollowstheorderedstepstoachievethegoal. Forexample
withtheskill ,theExecutorreceivethelistofcommandandexecute:
PICK(bottle, right)
• Executeapproach: Therobotarmmovestowardstheobject’spositionandopenthegripper.
• Executegrasp: Thisphaseincludesclosingthegripperandverifyingthegrasp.
• Executelifting: Therobotliftstheobjectfromthesurfaceitison.
Thus,whenanunexpectedeventoccurs,suchasthebottlebeingmovedorisnotreachabletheexecutor
mayraiseafailuremessage.
ControllerandExplainerinteraction:
• TheControllerdetectsthattheobjectisnolongerintheexpectedlocationandsendsafailure
messagetotheExplainer.
• TheExplaineranalyzesthefailure,referencingpreviousinstanceswhereobjectsweremoved
unexpectedly. ItsuggeststheTaskPlannertoreanalysethesemanticmapandupdatetheobject’s
location.
Re-planning: Basedonthesuggestion,theTaskPlannerissuesanewplan:
• Execute togoneartheidentifiedbottle.
GOTO(table)
• Afterlocatingthebottleonthetable,therobotupdatesitsactionsandproceedstoexecutethe
remainingtasks.
Thisexampledemonstrateshowthesystemadaptsinreal-time,allowingforcontinuoustaskexecution
evenindynamicandunpredictableenvironments.
Planningalgorithm Wenowformalizethisprocessintheformofanadaptiveplanningalgorithm.
Inthisalgorithm,theusedLLMisageneralistmodelsuchasLlama370BInstruct [35],whosebehavior
weinfluencethroughin-contextlearning[9].Figure3:Robee,humaniodrobotdevelopedbyOversonicRobotics.
Algorithm1PlanningwithextedendReActFramework
1: Input: Userrequest𝑟,Robotstate𝑅
𝑠
2: Output: Executionofuserrequest
3: procedurePlanning(𝑟,𝑀)
4: 𝐶 ← InitializeLLMContext(𝑟, 𝑀, 𝑅 )
0 𝑠
5: whilenotgoalachieveddo
6: 𝑎𝑐𝑡𝑖𝑜𝑛 ← TaskPlanner(𝑟,𝐶 ) ◁Getfirstskill
0
7: if 𝑎𝑐𝑡𝑖𝑜𝑛 = "Skill" then
8: 𝑐𝑜𝑚𝑚𝑎𝑛𝑑𝑠 ← SkillPlanner(𝑠𝑘𝑖𝑙𝑙,𝐶 ) ◁Translateskillintolow-levelcommands
𝑡
9: 𝑠𝑢𝑐𝑐𝑒𝑠𝑠 ← Executor(𝑐𝑜𝑚𝑚𝑎𝑛𝑑𝑠) ◁Executecommands
10: if 𝑠𝑢𝑐𝑐𝑒𝑠𝑠 = Falsethen
11: 𝑓𝑎𝑖𝑙𝑢𝑟𝑒𝑀𝑠𝑔 ← Controller(𝐶 ) ◁Detectfailure
𝑡
12: 𝑐 ← Explainer(𝑓𝑎𝑖𝑙𝑢𝑟𝑒𝑀𝑠𝑔) ◁Generatesuggestion
𝑡
13: else
14: 𝑐 ← Skillsuccesfullyexecuted
𝑡
15: endif
16: else
17: 𝑐 ← CallPerceptionAction() ◁ReadingsemanticmapfromPerceptionModule
𝑡
18: endif
19: 𝐶 ← UpdateContext(𝐶 ) ◁Updatecontext
𝑡+1 𝑡
20: 𝑠𝑘𝑖𝑙𝑙 ← TaskPlanner(𝑟,𝐶 ) ◁Getnextskillbasedonupdatedcontext
𝑡+1
21: endwhile
22: endprocedure
This algorithm shows the adaptive behavior of the system by incorporating feedback loops that
facilitate real-time re-planning. By alternating between action and reasoning phases, the robot can
continuouslyadapttochanges,ensuringtasksuccesseveninunpredictableenvironments.Figure4:Environmentusedduringtheexecutionofexperiments.
5. Robot Hardware
ThesystemwasimplementedusingRoBee,acognitivehumanoidrobotdevelopedbyOversonicRobotics.
RoBee measures 160 cm in height and weighs 60 kg. It has 32 degrees of freedom, enabling highly
flexiblemovement. Therobotisequippedwithmultiplesensors,includingcameras,microphones,and
forcesensors.
Thecamerasprovidereal-timevisualdata,supportingnavigationandobjectrecognitiontasks. The
microphonesfacilitateaudioinput,enablingspeechrecognitionandinteractionthroughnaturallan-
guageprocessing. Theforcesensorsareusedforhandlingobjects,allowingRoBeetoadjustgripforce
based on the characteristics of the item being manipulated, enhancing precision and safety during
interactions.
RoBee’smechanicalstructureincludestwoarmscapableofbimanualmanipulation,eachcapableof
handlingobjectsweighingupto5kg. Thesystemincludesatorsoandlegsystemdesignedforbal-
ance and mobility. RoBee is equipped with LIDAR sensors for real-time environment mapping and
obstacledetection. TheseLIDARsensorsenabletherobottonavigateautonomouslythroughcomplex
environments,ensuringsafeoperationinsharedspaces. Thecombinationofautonomousnavigation
technologiesandLIDAR-baseddetectionenhancestheabilityofRoBeetomoveefficientlyandavoid
collisionsindynamicindustrialenvironments.
Inadditiontoitsphysicalcapabilities,RoBeeintegrateswithcloud-basedsystems,allowingforremote
monitoring,taskscheduling,anddataanalytics.
ThePlanner-moduletakesintoaccountRoBee’sembodiment,ensuringthatthesystemisalignedwith
therobot’scapabilitiessuchasitsdegreesoffreedom,sensorsuite,andabilitytoperformmanipulation
andnavigation.
6. Preliminary results
Preliminaryexperimentswereconductedinasimulatedenvironmentreplicatingtwomainrooms: a
kitchenandabedroom,asillustratedinFigure6.
Duringtheexperiments,threetypesofrequestsweretested,eachvaryingincomplexity:
• Simplerequests: directcommandsthatinvolveonlyoneskill. Forexample,"Pickupthebottle
infrontofyou",wherethetaskplannerneedsonlytoidentifytheparametersandactivatethe
appropriateskill.
• Moderately complex requests: tasks that require the robot to perform multiple skills in
sequence,asexplicitlydescribedinthecommand. Anexampleis"Gotothekitchen,pickupthe
bottle,andbringittothetableinthebedroom",whichinvolvesmultipleskills. Thesetasksrequire
ahigherlevelofcomplexity,withplanningacrossseveralstepsandhandlingpotentialfailures.• Complexrequests: suchas"I’mthirsty,canyouhelpme?",whichweremoreopen-endedand
requiredtherobottointerpretthetaskandbreakitdownintomultiplesteps.
The results in table 1 showed that the system performed well with simple requests, followed by
moderately complex ones. However, the success rate for complex requests was significantly lower,
withonly25%ofthetaskscompletedcorrectly. Thislowerperformancewasattributedtothesystem’s
difficultyinunderstandingandmanagingambiguousorunder-specifiedinstructions.
Itisimportanttonotethatthesearepreliminaryresults,andfurtheranalysisisongoing. Athorough
evaluationofthedataiscurrentlyunderway,includingacomparisonwiththestateoftheartinrobot
taskexecutionandnaturallanguageunderstanding. Thiswillallowforadeeperunderstandingofthe
system’sstrengthsandareasforimprovement.
Requesttype Numberofattempts Successrate
Simplerequests 30 90%
Moderatelycomplexrequests 20 75%
Complexrequests 10 25%
Table1
Numberofattemptsandsuccessrateforeachrequesttype
7. Conclusions
Theproposedplanningsystemexhibitsnotablestrengths,particularlyitsadaptabilityandseamlesswith
therobot’sdiversesetofskillforexecutingcomplextasks. Thesystem’scoreadvantageliesinitsability
to interpret user commands through natural language processing, converting them into high-level
actionsthatarefurtherrefinedintolow-level,executabletasks. Byintegratingreal-timeenvironmental
feedbackfromthePerceptionModulethroughanextendedversionofReActframework,thesystemcan
dynamicallyadjusttounexpectedsituations,suchasobstaclesorexecutionfailures. Thisadaptability
is supported by an architecture, where the Task Planner, Skill Planner, Controller, and Explainer
componentsworkinharmonytoensuresmoothtaskexecutioneveninchangingenvironments.
One of the system’s key strengths is its ability to manage error recovery through feedback loops,
allowingtherobottoadaptquicklytofailuresduringtaskexecution. TheExplainermoduleprovideson
theflysuggestionstomodifytheplanbasedonpasterrors,enhancingthesystem’svalidity. Theuseof
semanticmapsandscenegraphsprovidestherobotwithastructuredunderstandingofitsenvironment,
ensuringthatactionsarecontextuallyaccurateandresponsivetoreal-worldconditions.
TheintegrationofLLMs,perceptualfeedback,andflexibletaskplanningmechanismsmakesthesystem
highlyversatileforcomplex,dynamicenvironments. ItsimplementationonRoBee,thehumanoidrobot
developedbyOversonicRobotics,hasdemonstrateditspracticalpotential,positioningitasavaluable
toolforapplicationsrequiringadvancedhuman-robotinteractionandadaptabilityinunpredictable
settings.
Inthefuture,otherthanextendingthelowlevelskillsetavailable,wewillinvestigatethepossibilityto
autonomouslyexpandtheExplainerdatasetaswellasprovidingsimilarinformationdirectlytotheTask
Planner,increasingflexibilityandreliabilityandreducingthenumberofre-planningevents. Wewill
alsostudycapabilityofthesystemtoproactivelyacquireinformationabouttheenvironment[14]and
humanpartnersboththroughsensors[36]andcommunicationstrategies,leveragingthepotentialfor
proactiveinformationgatheringbehavioursofLLMs[37,38,39]. Moreover,itwillbecrucialtoassess
thereliabilityofthesystembothattheplanninglevelaswellasthecommunicationlevel,considering
theintroductionofembodimentandenvironmentwhilethelimitationinpragmaticunderstandingof
LLMarestilltobeunderstood[39,40,41].Acknowledgments
Special thanks to Oversonic Robotics for enabling the implementation of this project using their
humanoidrobot,RoBee.
References
[1] D.Aineto,R.DeBenedictis,M.Maratea,M.Mittelmann,G.Monaco,E.Scala,L.Serafini,I.Serina,
F.Spegni,E.Tosello, A.Umbrico,M.Vallati(Eds.),ProceedingsoftheInternationalWorkshop
onArtificialIntelligenceforClimateChange,theItalianworkshoponPlanningandScheduling,
theRCRAWorkshoponExperimentalevaluationofalgorithmsforsolvingproblemswithcom-
binatorialexplosion,andtheWorkshoponStrategies,Prediction,Interaction,andReasoningin
Italy(AI4CC-IPS-RCRA-SPIRIT2024),co-locatedwith23rdInternationalConferenceoftheItalian
AssociationforArtificialIntelligence(AIxIA2024),CEURWorkshopProceedings,CEUR-WS.org,
2024.
[2] D.Driess,F.Xia,M.S.Sajjadi,C.Lynch,A.Chowdhery,B.Ichter,A.Wahid,J.Tompson,Q.Vuong,
T. Yu, et al., Palm-e: an embodied multimodal language model, in: Proceedings of the 40th
InternationalConferenceonMachineLearning,2023,pp.8469–8488.
[3] M.Ahn,A.Brohan,N.Brown,Y.Chebotar,O.Cortes,B.David,C.Finn,C.Fu,K.Gopalakrishnan,
K.Hausman,etal., Doasican,notasisay: Groundinglanguageinroboticaffordances, arXiv
e-prints(2022)arXiv–2204.
[4] J.Wang,Z.Wu,Y.Li,H.Jiang,P.Shu,E.Shi,H.Hu,C.Ma,Y.Liu,X.Wang,etal., Largelanguage
modelsforrobotics: Opportunities,challenges,andperspectives, arXivpreprintarXiv:2401.04334
(2024).
[5] F.Zeng,W.Gan,Y.Wang,N.Liu,P.S.Yu, Largelanguagemodelsforrobotics: Asurvey, arXiv
e-prints(2023)arXiv–2311.
[6] S.Kambhampati,K.Valmeekam,L.Guan,K.Stechly,M.Verma,S.Bhambri,L.Saldyt,A.Murthy,
Llmscan’tplan,butcanhelpplanninginllm-moduloframeworks, arXivpreprintarXiv:2402.01817
(2024).
[7] S.Tellex,N.Gopalan,H.Kress-Gazit,C.Matuszek, Robotsthatuselanguage, AnnualReviewof
Control,Robotics,andAutonomousSystems3(2020)25–55.
[8] G.Zhu, L.Zhang, Y.Jiang, Y.Dang, H.Hou, P.Shen, M.Feng, X.Zhao, Q.Miao, S.A.A.Shah,
etal., Scenegraphgeneration: Acomprehensivesurvey, arXive-prints(2022)arXiv–2201.
[9] Q.Dong,L.Li,D.Dai,C.Zheng,Z.Wu,B.Chang,X.Sun,J.Xu,Z.Sui, Asurveyonin-context
learning, arXivpreprintarXiv:2301.00234(2022).
[10] S.Yao,J.Zhao,D.Yu,N.Du,I.Shafran,K.Narasimhan,Y.Cao, React: Synergizingreasoningand
actinginlanguagemodels, in: InternationalConferenceonLearningRepresentations(ICLR),2023.
[11] L. Heuss, D. Gebauer, G. Reinhart, Concept for the automated adaption of abstract planning
domainsforspecificapplicationcasesinskills-basedindustrialrobotics, JournalofIntelligent
Manufacturing(2023)1–26.
[12] M.Shanahan, Frameproblem,the, EncyclopediaofCognitiveScience(2006).
[13] L. P. Kaelbling, M. L. Littman, A. R. Cassandra, Planning and acting in partially observable
stochasticdomains, Artificialintelligence101(1998)99–134.
[14] D. Ognibene, G. Baldassare, Ecological active vision: four bioinspired principles to integrate
bottom–up and adaptive top–down attention tested with a simple camera-arm robot, IEEE
transactionsonautonomousmentaldevelopment7(2014)3–25.
[15] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, P. Fung, Survey of
hallucinationinnaturallanguagegeneration, ACMComputingSurveys55(2023)1–38.
[16] O.Ruiz,J.Rosell,M.Diab, Reasoningandstatemonitoringfortherobustexecutionofrobotic
manipulationtasks, in: 2022IEEE27thInternationalConferenceonEmergingTechnologiesand
FactoryAutomation(ETFA),IEEE,2022,pp.1–4.[17] J.Liang,W.Huang,F.Xia,P.Xu,K.Hausman,B.Ichter,P.Florence,A.Zeng, Codeaspolicies:
Language model programs for embodied control, in: 2023 IEEE International Conference on
RoboticsandAutomation(ICRA),IEEE,2023,pp.9493–9500.
[18] B.Liu,Y.Jiang,X.Zhang,Q.Liu,S.Zhang,J.Biswas,P.Stone, Llm+p:Empoweringlargelanguage
modelswithoptimalplanningproficiency, arXive-prints(2023)arXiv–2304.
[19] C.H.Song,J.Wu,C.Washington,B.M.Sadler,W.-L.Chao,Y.Su, Llm-planner:Few-shotgrounded
planning for embodied agents with large language models, in: Proceedings of the IEEE/CVF
InternationalConferenceonComputerVision,2023,pp.2998–3009.
[20] Z.Wang,S.Cai,G.Chen,A.Liu,X.Ma,Y.Liang, Describe,explain,planandselect: Interactive
planningwithlargelanguagemodelsenablesopen-worldmulti-taskagents, arXive-prints(2023)
arXiv–2302.
[21] I. Armeni, Z.-Y. He, J. Gwak, A. R. Zamir, M. Fischer, J. Malik, S. Savarese, 3d scene graph: A
structureforunifiedsemantics,3dspace,andcamera,in:ProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision,2019,pp.5664–5673.
[22] Y.Liu,L.Palmieri,S.Koch,I.Georgievski,M.Aiello, Delta: Decomposedefficientlong-termrobot
taskplanningusinglargelanguagemodels, arXive-prints(2024)arXiv–2404.
[23] K.Rana,J.Haviland,S.Garg,J.Abou-Chakra,I.Reid,N.Suenderhauf, Sayplan: Groundinglarge
languagemodelsusing3dscenegraphsforscalablerobottaskplanning, in:7thAnnualConference
onRobotLearning,2023.
[24] M.Cashmore, A.Coles, B.Cserna, E.Karpas, D.Magazzeni, W.Ruml, Replanningforsituated
robots, in: ProceedingsoftheInternationalConferenceonAutomatedPlanningandScheduling,
volume29,2019,pp.665–673.
[25] L. Zha, Y. Cui, L.-H. Lin, M. Kwon, M. G. Arenas, A. Zeng, F. Xia, D. Sadigh, Distilling and
retrievinggeneralizableknowledgeforrobotmanipulationvialanguagecorrections, in: 2024IEEE
InternationalConferenceonRoboticsandAutomation(ICRA),IEEE,2024,pp.15172–15179.
[26] M.Skreta,Z.Zhou,J.L.Yuan,K.Darvish,A.Aspuru-Guzik,A.Garg, Replan: Roboticreplanning
withperceptionandlanguagemodels, arXive-prints(2024)arXiv–2401.
[27] H.Geffner, Non-classicalplanningwithaclassicalplanner: Thepoweroftransformations, in:
EuropeanWorkshoponLogicsinArtificialIntelligence,Springer,2014,pp.33–47.
[28] D. Ognibene, G. Pezzulo, H. Dindo, Resources allocation in a bayesian, schema-based model
ofdistributedactioncontrol, in: NIPS-WorkshoponProbabilisticApproachesforRoboticsand
Control,2009.
[29] M.Ho,D.Abel,J.Cohen,M.Littman,T.Griffiths, Peopledonotjustplan,theyplantoplan, in:
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,2020,pp.1300–1307.
[30] S.Russell,E.Wefald, Principlesofmetareasoning, Artificialintelligence49(1991)361–395.
[31] S.Zilberstein,S.J.Russell, Anytimesensing,planningandaction: Apracticalmodelforrobot
control, in: IJCAI,volume93,1993,pp.1402–1407.
[32] R.Ackerman,V.A.Thompson, Meta-reasoning:Monitoringandcontrolofthinkingandreasoning,
Trendsincognitivesciences21(2017)607–617.
[33] S.J.Russell,P.Norvig,Artificialintelligence: amodernapproach,Pearson,2016.
[34] F.Rahutomo,T.Kitasuka,M.Aritsugi,etal., Semanticcosinesimilarity, in: The7thinternational
studentconferenceonadvancedscienceandtechnologyICAST,volume4,UniversityofSeoul
SouthKorea,2012,p.1.
[35] A.Dubey,A.Jauhri,A.Pandey,A.Kadian,A.Al-Dahle,A.Letman,A.Mathur,A.Schelten,A.Yang,
A.Fan,etal., Thellama3herdofmodels, arXiv.org(????).
[36] D.Ognibene,Y.Demiris, Towardsactiveeventrecognition., in: IJCAI,2013,pp.2495–2501.
[37] S.Patania,E.Masiero,L.Brini,G.Donabauer,U.Kruschwitz,V.Piskovskyi,D.Ognibene, Large
languagemodelsasanactivebayesianfilter: informationacquisitionandintegration, in: Proceed-
ingsofthe28thWorkshopontheSemanticsandPragmaticsofDialogue-FullPapers,SEMDIAL,
Trento,Italy,2024.URL:http://semdial.org/anthology/Z24-Patania_semdial_0006.pdf.
[38] A.Z.Ren,A.Dixit,A.Bodrova,S.Singh,S.Tu,N.Brown,P.Xu,L.Takayama,F.Xia,J.Varley,etal.,
Robotsthataskforhelp: Uncertaintyalignmentforlargelanguagemodelplanners, ProceedingsofMachineLearningResearch229(2023).
[39] B.Magnini, Towardcollaborativellms: Investigatingproactivityintask-orienteddialogues, in:
Proceedingsofthe28thWorkshopontheSemanticsandPragmaticsofDialogue-InvitedTalks,
SEMDIAL,Trento,Italy,2024.URL:http://semdial.org/anthology/Z24-Magninini_semdial_0003a.
pdf.
[40] A.Martinenghi,C.Koyuturk,S.Amenta,M.Ruskov,G.Donabauer,U.Kruschwitz,D.Ognibene,
Vonneumidas: Enhancedannotationschemaforhuman-llminteractionscombiningmidaswith
vonneumanninspiredsemantics, in: Proceedingsofthe28thWorkshopontheSemanticsand
PragmaticsofDialogue-PosterAbstracts,SEMDIAL,Trento,Italy,2024.URL:http://semdial.org/
anthology/Z24-Martinenghi_semdial_0045.pdf.
[41] A. Martinenghi, G. Donabauer, S. Amenta, S. Bursic, M. Giudici, U. Kruschwitz, F. Garzotto,
D. Ognibene, Llms of catan: Exploring pragmatic capabilities of generative chatbots through
predictionandclassificationofdialogueactsinboardgames’multi-partydialogues, in:Proceedings
ofthe10thWorkshoponGamesandNaturalLanguageProcessing@LREC-COLING2024,2024,
pp.107–118.
8. Online Resources
MoreinformationaboutRoBeeandOversonicRoboticsareavailable:
• RoBee,
• OversonicRobotics