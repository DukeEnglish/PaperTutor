Iterative Reweighted Framework Based
Algorithms for Sparse Linear Regression with
Generalized Elastic Net Penalty
Yanyun Ding1, Zhenghua Yao2, Peili Li2,3, Yunhai Xiao3*
1Institute of Applied Mathematics, Shenzhen Polytechnic University,
Shenzhen, 518055, P. R. China.
2School of Mathematics and Statistics, Henan University, Kaifeng,
475000, P.R. China.
3Center for Applied Mathematics of Henan Province, Henan University,
Zhengzhou, 450046, P.R. China.
*Corresponding author(s). E-mail(s): yhxiao@henu.edu.cn;
Contributing authors: dingyanyun@szpu.edu.cn; zhyao2000@163.com;
lipeili@henu.edu.cn;
Abstract
The elastic net penalty is frequently employed in high-dimensional statistics for
parameterregressionandvariableselection.Itisparticularlybeneficialcompared
tolasso whenthenumberofpredictorsgreatly surpasses thenumberofobserva-
tions. However, empirical evidence has shown that the ℓ q-norm penalty (where
0 < q < 1) often provides better regression compared to the ℓ1-norm penalty,
demonstratingenhancedrobustnessinvariousscenarios.Inthispaper,weexplore
a generalized elastic net model that employs a ℓ r-norm (where r ≥ 1) in loss
function to accommodate various types of noise, and employs a ℓ q-norm (where
0 < q < 1) to replace the ℓ1-norm in elastic net penalty. Theoretically, we
establish thecomputablelower boundsforthenonzeroentriesof thegeneralized
first-order stationary points of the proposed generalized elastic net model. For
implementation, we develop two efficient algorithms based on the locally Lip-
schitz continuous ǫ-approximation to ℓ q-norm. The first algorithm employs an
alternating direction method of multipliers (ADMM), while the second utilizes
a proximal majorization-minimization method (PMM), where the subproblems
are addressed using the semismooth Newton method (SNN). We also perform
extensivenumericalexperimentswithbothsimulatedandrealdata,showingthat
1
4202
voN
22
]LM.tats[
1v57841.1142:viXraboth algorithms demonstrate superior performance. Notably, the PMM-SSN is
efficientthanADMM,eventhoughthelatterprovidesasimplerimplementation.
Keywords:Sparselinearregression,Generalizedelasticnetpenalty,Iterative
reweightedframework,Alternatingdirectionmethodofmultipliers,Proximal
majorization-minimizationmethod
1 Introduction
In this paper, we mainly consider a high-dimensional sparse linear regression model.
Suppose that the data set has n observations with p predictors. Let y Rn be the
response and X := [X ,X ,...,X ] Rn×p be the sample matrix, where∈ X Rn for
1 2 p i
∈ ∈
i = 1,...,p are the predictors. We assume that the following linear relation is data
generation process:
y =Xβ+ε,
where β Rp is the model fitting procedure produces the vector of coefficient, and
ε Rn is∈ the noise, which is not limited to a specific type of noise. Without loss of
∈
generality, we assume that the above data has been centralized. In high-dimensional
setting, the number of predictors p is much larger than the number of observations
n and that β is known to be sparse a priori. Under this assumption, penalty tech-
niques are often employed to manage overfitting and facilitate variable selection. A
typicalmethod calledLasso[1,2]addsaℓ -normpenaltyto the ordinaryleastsquare
1
loss function, which penalizes the absolute size of the coefficients. Subsequently, the
estimation model is described as following:
1
min Xβ y 2+λ β ,
β∈Rp 2k − k2 k k1
where β is ℓ -norm (named Lasso) and λ > 0 is a tuning parameter. In view of
1 1
k k
thegoodcharacteristicsoftheℓ -norm,theLassodoesbothcontinuousshrinkageand
1
automatic variable selection simultaneously. Although Lasso has received extensive
attention in different applicationfields, it has some limitations. For the case of p n
≫
and grouped variables situation, the Lasso can only select at most n variables before
it saturates, and it lacks the ability to reveal the grouping information [3–5]. As a
result, Zou et al [5] proposedan elastic net penalty approachfor both estimation and
variable selection:
1
min Xβ y 2+λ β 2+λ β , (1)
β∈Rp 2k − k2 2 k k2 1 k k1
where λ and λ are two non-negative tuning parameters. Similar to Lasso, the
1 2
elastic net penalty simultaneously does automatic variable selection and continuous
shrinkage, and it can select groups of correlated variables [5].
In fact, the ℓ -norm is a loose approximation function which leads to an over-
1
regularized problem. Motivated by the fact that the ℓ -norm penalty with 0 < q < 1
q
can usually induce better sparse regression results than ℓ -norm penalty [6], with
1
2improved robustness [7]. A natural improvement is the using of a ℓ -norm to replace
q
the ℓ -norm in elastic net penalty, see also [8]:
1
1
min Xβ y 2+λ β 2+λ β q, (2)
β∈Rp 2k − k2 2 k k2 1 k kq
where β =( p β q)1/q isaℓ -norm.Itisimportanttonotethattheerrorvector
k kq i=1| i | 1
ε may not approach zero and may not follow a normal distribution. Although least
P
squares estimation is computationally attractive, it relies on knowing the standard
deviationofGaussiannoise,whichcanbe challengingtoestimate inhigh-dimensional
settings. A key question is whether we can extend the model (2) to accommodate
different types of noise, beyond the least squares loss function.
This question is important because when noise does not follow a normal distribu-
tion, alternative loss functions may be more robust. The least absolute deviation loss
(a.k.s. ℓ -norm) is commonly used for scenarios with heavy-tailed or heterogeneous
1
noise, often resulting in better regression performance than least squares loss [9–13].
The square-rootloss [14] removesthe need to know or pre-estimate the deviation and
can achieve near-oracle rates of convergence under some appropriate conditions [15–
17]. It also has been shown that ℓ -norm loss performs better when deal with the
∞
uniformly distributed noise and quantization error [18–20]. Therefore, in this paper,
we mainly concentrate on a generalized elastic net problem with a ℓ -norm (where
r
r 1) loss function to result in the following model
≥
min F(β):= Xβ y +λ β 2+λ β q , (3)
β∈Rp k − kr 2 k k2 1 k kq
n o
where λ and λ 0 are tuning parameters. We note that the ℓ -norm, associated
1 2 r
≥
with the parameter r, is assumed to have a strongly semismooth proximal mapping.
This assumption regarding ℓ -norm is mild and often encountered. For instance, the
r
proximalmappingsassociatedwithℓ -,ℓ -andℓ -normsareallstronglysemismooth.
1 2 ∞
These specific norms are commonly employed in optimization problems that involve
lossfunctionsforaddressingvarioustypesofnoise[18,21].Theselectionofrissuitable
fordifferenttypesofnoise,forinstance,r =1isidealforheavy-tailedorheterogeneous
noise,r =2isappropriatewhenthestandarddeviationofGaussiannoiseisunknown,
and r = fits scenarios involving uniformly distributed noise.
∞
It is known that ℓ for 0 < q < 1 is nonsmooth and nonconvex, and it may not
q
even be Lipschitz continuous [22, 23]. To address this issue, several effective itera-
tive reweightedalgorithmsregardingℓ - and ℓ -norms havebeen proposed[8, 24–26].
1 2
Forinstance,Lu[27]presentedaLipschitzcontinuousǫ-approximationoftheℓ -norm
q
and empirically demonstrated that this new approach generally outperforms current
iterative reweighted methods. These algorithms rely on least absolute deviation or
least squares approximationsof the ℓ -norm, with the goal of finding an approximate
q
solution to (2). The successful execution of these algorithms primarily arises from
the smoothness of the least squares loss term in (2). However, the nonconvex non-
smooth properties of ℓ -norm, combined with the nonsmooth nature of the ℓ -norm
q r
loss function, make the numerical solving of model (3) considerably more difficult.
3Drawing inspiration from the work of Lu [27], this paper transforms the general-
ized elastic net model (3) into a convex yet nonsmooth iterative reweighted ℓ -norm
1
minimization by utilizing an ǫ-approximation. We begin by defining the generalized
first-order stationary point of (3), which helps us establish lower bounds and iden-
tify its local minimizers. Subsequently, we demonstrate that any accumulation point
obtained from the sequence generated by the approximation technique converges to
a generalizedfirst-orderstationarypoint,providedthatthe approximationparameter
ǫ remains below a certain threshold value. It is crucial to highlight that the estima-
tion model obtained from this approximation is convex but non-smooth. As a result,
we develop two efficient practical methods to fully utilize the underlying structures
of the convex approximate minimization model. The first algorithm is the alternat-
ing direction method of multipliers (ADMM), known for its ease of implementation.
However, ADMM is still a first-order method, which may restrict its efficiency when
tackling problems with high accuracy requirements. In light of this, we also employ a
novelproximalmajorizationminimization(PMM)approach,see[17],tosolvethesub-
problem through its dual using a highly efficient semismooth Newton (SSN) method.
Finally, we perform a series of numerical experiments that highlight the remarkable
superiorityofthegeneralizedelasticnetmodel(3)andshowthatbothofourproposed
algorithms are efficient.
The remainder of this paper is organized as follows. In Section 2, we summarize
key concepts and address the ǫ-approximation problem. Section 3 outlines our moti-
vation and constructs the algorithms, including their convergence results. Section 4
presentsnumericalresultsto demonstratethe effectivenessofourapproaches.Finally,
we conclude this paper in Section 5.
2 Some theoretical properties
2.1 Preliminary results
Let Rp be a p-dimensional Euclidean space endowed with an inner product , and
itsinducednorm ,respectively.Givenβ∗ Rp,letT = i:β∗ =0 represh e· n· ti sthe
support set of β∗k , · ak n2 d let T¯ denotes its comp∈ lement within{ 1,.i ..6 ,n} . Let f : Rp
{ } →
( ,+ ]beaclosedproperconvexfunction.TheproximalmappingandtheMoreau
−∞ ∞
envelope function of f with parameter t>0 are defined, respectively, as follows
1
Prox (x):=argmin f(y)+ y x 2 , x Rp,
tf y∈Rp 2tk − k2 ∀ ∈
n o
and
1
Φ (x):= min f(y)+ y x 2 , x Rp.
tf y∈Rp 2tk − k2 ∀ ∈
n o
In particular, the explicit forms of the proximal operator for the ℓ -norm with r =1,
r
2, and may refer to [18, 28]. Additionally, it is noted in [29, 30] that Φ (x) is
tf
∞
continuously differentiable and convex, with its gradient given by
Φ (x)=t−1(x Prox (x)), x Rp.
tf tf
∇ − ∀ ∈
4By the Moreau’s identity theorem [31, Theorem 35.1], it holds that
Prox tf(x)+tProx f∗/t(x/t)=x,
where f∗ represents the Fenchel conjugate function of f.
At the conclusion of this subsection, we will review some results concerning the
proximalmappings forseveralcommontypes ofvectornorms.Letf(x):=t x with
1
k k
a t>0, then f∗(x)=δ (x) with B(t) := x x t and
B∞(t) ∞
{ | k
k∞
≤ }
x , if x t,
Prox f(x)=x −Π B∞(t)(x) with (Π B∞(t)(x)) i = Sii gn(x i)t, if | xi i|≤ >t,
(cid:26) | |
where i=1,...,n and ‘Sign()’ is a sign function of a vector.Let f(x):=t x , then
2
· k k
f∗(x)=δ (x) with B(t) := x x t and
B 2(t) 2 { | k k2 ≤ }
x, if x t,
2
Prox (x)=x Π (x) with Π (x)= k k ≤
f − B 2(t) B 2(t) (cid:26)t kxx k2, if kx k2 >t.
Let f(x):=t x , then f∗(x)=δ (x) with B(t) := x x t and
k k∞ B 1(t) 1 { | k k1 ≤ }
x∗, if x t,
Prox f(x)=x −Π
B
1(t)(x) with Π
B
1(t)(x)=
(cid:26)µP xΠ ∆n(P xx/t), if
k kxk k1
1
≤
>t.
whereP :=Diag(Sign(x))andΠ ()denotestheprojectionontothesimplex∆ :=
x
Rnx
e⊤x=1,x 0 , in
whi∆ chn D·
iag() denotes a diagonal matrix with
elemn
ents
{ ∈ | n ≥ } ·
of a vector on its diagonal positions.
2.2 Lower bound for nonzero entries of generalized stationary
point
We now discuss the lower bound for nonzero entries of the generalized first-order
stationary point of the model (3). This discussion draws inspiration from the works
referenced in [27, 32]; however, the subsequent process represents an enhancement
through the use of a nonsmooth ℓ -norm loss function.
r
Atthefirstplace,wegivethedefinitionofageneralizedfirst-orderstationarypoint.
Definition 1. Suppose that β∗ is a vector in Rp and that Λ∗ = Diag(β∗). We say
that β∗ is a generalized first-order stationary point of (3) if
0 Λ∗ ∂ Xβ∗ y +2λ β∗ +λ q β∗ q . (4)
r 2 1
∈ k − k
(cid:16) (cid:17) (cid:12) (cid:12)
where β∗ q :=( β∗ q ;...; β∗ q ). (cid:12) (cid:12)
1 p
At the second place, we show that every local minimizer of (3) is indeed a
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
genera(cid:12)lize(cid:12)d first-(cid:12)ord(cid:12)er sta(cid:12)tion(cid:12)ary point.
5Theorem 1. Let β∗ be a local minimizer of (3). Then (4) holds, that is to say, β∗ is
a generalized first-order stationary point.
Proof. Suppose that β∗ is a localminimizer of(3).Evidently, β∗ alsoservesasalocal
minimizer for
min Xβ y +λ β 2+λ β q : β =0, i=T . (5)
β∈Rp k − kr 2 k k2 1 k kq i 6
n o
It is important to highlight that the objective function of (5) is locally Lipschitz
continuous at β∗ within the subspace indexed by T. In the case of i T, using the
∈
first-order optimality condition, we have
0 ∂ Xβ∗ y +2λ β∗+λ q β∗ q−1 Sign(β∗). (6)
∈ k − kr i 2 i 1 i i
(cid:16) (cid:17) (cid:12) (cid:12)
Multiplying β∗ on both sides of (6), it gets that (cid:12) (cid:12)
i
0 ∂ Xβ∗ y β∗+2λ (β∗)2+λ q β∗ q .
∈ k − kr i i 2 i 1 i
(cid:16) (cid:17) (cid:12) (cid:12)
Since β∗ = 0 for i = T, the above relation is also valid fo(cid:12)r i(cid:12)= T. Therefore, (4)
i 6 6
holds.
Thirdly, we establish a lower bound for the nonzero entries of the generalized
first-order stationary points, specifically the local minimizers of (3).
Theorem 2. Supposethat β∗ is ageneralized first-order stationary point of(3). Then
for r =1, 2, and , it holds that
∞
β∗ >
qλ
1
1−1
q , i T, (7)
i X ∀ ∈
(cid:12) (cid:12) (cid:16)k i kr (cid:17)
(cid:12) (cid:12)
where X is the i-th column of X.
i
Proof. From the definition of a generalized first-order stationary point, we have for
i T that
∈ 0 ∂ Xβ∗ y +2λ β∗+qλ β∗ q−1 Sign(β∗),
∈ k − kr i 2 i 1 i i
or equivalently, (cid:16) (cid:17) (cid:12) (cid:12)
(cid:12) (cid:12)
2λ β∗ qλ β∗ q−1 Sign(β∗) ∂ Xβ∗ y .
− 2 i − 1 i i ∈ k − kr i
(cid:12) (cid:12) (cid:16) (cid:17)
It is easy to see that Sign(β∗)(cid:12)= 0(cid:12) and Sign(β∗) = 1 in the case of i T. Then,
i 6 | i | ∈
multiplying Sign(β∗) on both sides of the above relation, it yields
i
2λ β∗ qλ β∗ q−1 Sign(β∗) ∂ Xβ∗ y .
− 2 | i|− 1 i ∈ i k − kr i
(cid:12) (cid:12) (cid:16) (cid:17)
(cid:12) (cid:12)
6For r =1, 2, and , from the definition of ∂ Xβ∗ y , we get
r
∞ k − k
Sign(β∗)(∂ Xβ∗ y ) X , (8)
i k − r i ≤ i r
which indicates 2λ β∗ +(cid:12) (cid:12) (cid:12) qλ β∗ q−1 X (cid:13) (cid:13). He(cid:12) (cid:12) (cid:12)nce(cid:13) (cid:13), it(cid:13) (cid:13)is not difficult to conclude
2 | i| 1 | i| ≤ k i kr
that
qλ β∗ q−1 < X ,
1 | i| k i kr
1
that is β∗ > qλ1 1−q for i T.
| i| kXik r ∈
(cid:16) (cid:17)
2.3 Generalized stationary point of ǫ-approximation problem
The ℓ -norm is non-Lipschitz continuity at certain points, including those with zero
q
components.Asaresult,itsClarkesubdifferentialisnotwell-defined,complicatingthe
development of algorithms. Drawing inspiration from Lu [27], we propose replacing
β term with a ǫ-approximation p h (β ) where
k kq i=1 uǫ i
P q 1 q
h uǫ(β i):= 0≤m s≤in uǫq |β
i
|s
−
−
q
sq−1 ,
(cid:16) (cid:17)
q−1
where u ǫ := (ǫ/(pλ 1)) q and ǫ > 0 is a small scalar. We observe that the Clarke
subdifferential of h , denoted as ∂h , exists everywhere and is given by
uǫ uǫ
1
q β q−1Sign(β ), if β >uq−1,
i i i ǫ
∂h (β )= | | | | (9)
uǫ i  1
qu ǫSign(β i), if β
i
u ǫq−1,
| |≤
which was used in the subsequent algorithmic developments and theoretical analysis.
For more detail, one may refer to [27].
Using the ǫ-approximation,the problem (3) transforms into
p
min F (β):= Xβ y +λ β 2+λ h (β ) . (10)
β∈Rp (ǫ) k − kr 2 k k2 1 uǫ i
n Xi=1 o
Next we show that the generalized stationary point of the corresponding ǫ-
approximation problem (10) is also the one of (3).
Theorem 3. Suppose that β∗ is a generalized first-order stationary point of (10).
Assume that ǫ be a constant satisfies
q
0<ǫ<pλ
kX
i kr
q−1
. (11)
1
qλ
(cid:18) 1 (cid:19)
Then, β∗ is also a generalized first-order stationary point of (3). Furthermore, the
nonzero entries of β∗ meet the lower bound condition given in (7).
7Proof. Using the assumption that β∗ is a generalized first-order stationary point of
(10), we get 0 F (β∗). On one hand, for any i T, it gets that
ǫ
∈ ∈
0 ∂ Xβ∗ y +2λ β∗+λ ∂h (β∗), (12)
∈ k − kr i 2 i 1 uǫ i
(cid:16) (cid:17)
which implies
2λ β∗ λ ∂h (β∗) ∂ Xβ∗ y .
− 2 i − 1 uǫ i ∈ k − kr i
Multiplying Sign(β∗) on both sides, it gets tha(cid:16)t (cid:17)
i
2λ β∗ λ Sign(β∗)∂h (β∗) Sign(β∗) ∂ Xβ∗ y .
− 2 | i|− 1 i uǫ i ∈ i k − kr i
(cid:16) (cid:17)
Combining with (9) and noting the definition of u , we get
ǫ
2λ β∗ λ ∂h (β∗) Sign(β∗) ∂ Xβ∗ y .
− 2 | i|− 1 | uǫ i |∈ i k − kr i
(cid:16) (cid:17)
From (8), it is not hard to deduce that
2λ β∗ +λ ∂h (β∗) X .
2 | i| 1 | uǫ i |≤k i kr
Notice that 2λ β∗ 0, then we have
2 | i|≥
λ ∂h (β∗) X ,
1 | uǫ i |≤k i kr
or equivalently,
∂h (β∗) X /λ . (13)
| uǫ i |≤k i kr 1
1 1
Next, we demonstrate that β∗ > uq−1 for all i T. Assume that 0 < β∗ uq−1
| i| ǫ ∈ | i| ≤ ǫ
for some i T, we get ∂h (β∗) = qu from (9). Recalling the definition of u and
∈ | uǫ i | ǫ ǫ
noting the inequality (11), it is easy to know that
ǫ q−1 X
∂h (β∗) =qu =q q > k i kr .
| uǫ i | ǫ pλ λ
1 1
(cid:16) (cid:17)
1
This conclusion contradicts (13), leading to the result that β∗ > uq−1 for all i T.
| i| ǫ ∈
From (9), it yields that
∂h (β∗)=q β∗ q−1Sign(β∗).
uǫ i | i| i
Therefore (12) can be expressed as
0 ∂ Xβ∗ y +2λ β∗+qλ β∗ q−1Sign(β∗).
∈ k − kr i 2 i 1 | i| i
(cid:16) (cid:17)
8Then, multiplying β∗ on both sides, it gets
i
0 ∂ Xβ∗ y β∗+2λ (β∗)2+λ q β∗ q.
∈ k − kr i i 2 i 1 | i|
(cid:16) (cid:17)
which means
0 Λ∗ ∂ Xβ∗ y +2λ β∗ +λ q β∗ q.
r 2 1
∈ k − k | |
On the other hand, given th(cid:16)at β∗ = 0 for i / T, it(cid:17)is clear that the aforementioned
i ∈
relation also holds for i / T. Consequently, equation (4) is satisfied, which means β∗
∈
is a generalized first-order stationary point of (3). Subsequently, the second part of
this theorem is followed from Theorem 2.
The following corollary demonstrates that the minimizer of (10) is also a gener-
alized first-order stationary point of (3), which directly follows from Theorems 3 and
1.
Corollary 2.1. Suppose that β∗ is an optimal solution of (3), then β∗ is also an
optimal solution of (10) provided that ǫ is chosen properly. Moreover, the nonzero
entries of β∗ satisfy the lower bound given in (7).
3 ADMM and PMM-SSN algorithms
This section focuses on developing algorithms to address the proposed model (3)
through an iterative reweighted approach. Let βk Rp be a given point, and let
wk ∈Rp with its i-th entry w ik :=min {u ǫ, |β ik |q−1
}
w∈ here u
ǫ
=( pλǫ 1)q− q1 .
We now consider the following iterative reweighted minimization problem
min F (β):= Xβ y +λ β 2+qλ W β , (1)
β∈Rp βk k − kr 2 k k2 1 k k k1
n o
whereW :=Diag(wk),i.e.,adiagonalmatrixwithcomponentwk atitsi-thposition.
k i
From an initial point β0, the iterative reweighted framework generates a sequence
βk according to solving the following reweighted minimization problem
{ }
βk+1 :=arg min F (β). (2)
β∈Rp
βk
Itisevidentthatthisisaconvexminimizationproblem,astheℓ -normwith0<q <1
q
has been eliminated.
The following theorem asserts that any accumulation point of the sequence βk
{ }
generatedby(2)isageneralizedfirst-orderstationarypointof(3).Theproofprocessof
thefollowingtheoremcanbeviewedasanextensionof[27,Theorem3.1],particularly
in the context of employing an ℓ -norm loss function.
r
Theorem 4. Suppose that βk be the sequence generated by (2) and that β¯ be an
accumulation point of βk .A{ ssu} methat ǫsatisfies (11).Then β¯isageneralized first-
order stationary point{ of (} 3), i.e., (4) holds at β¯. Moreover, the nonzero entries of β¯
satisfy the lower bound (7).
9Proof. At the beginning, we define
p
q 1 q
F(β,w):= Xβ y +λ β 2+qλ β w − wq−1 .
k − kr 2 k k2 1 | i | i − q i
i=1(cid:18) (cid:19)
X
Itis atrivialtaskto seethatwk is actuallyaminimizer ofF(β,w) withfixedβ :=βk
and a constraint w [0,u ], that is,
ǫ
∈
wk =arg min F(βk,w), (3)
0≤w≤uǫ
and that βk+1 is one of a minimizer of F(β,wk), that is,
βk+1 arg min F(β,wk).
∈ β∈Rp
BycomparingtheexpressionsforF (β)in(10)andF(β,w),wecangetfrom(3)that
(ǫ)
F (β)= min F(β,ω), and F (βk)=F(βk,ωk).
(ǫ) (ǫ)
0≤ω≤uǫ
Then we obtain that
F (βk+1)=F(βk+1,ωk+1) F(βk+1,ωk) F(βk,ωk)=F (βk), (4)
(ǫ) (ǫ)
≤ ≤
whichindicatesthat F (βk) isanon-increasingsequence.Letβ¯beanaccumulation
(ǫ)
{ }
point of the subsequence βk such that βk β¯. It follows that F (βk)
K K (ǫ)
F (β¯) due to the monoto{ nici} ty and continu{ ity} of F→ (). Let w¯ = min u , β¯ q−→ 1
(ǫ) (ǫ) i ǫ i
· { | | }
for all i. We then observe that wk w¯ and that F (β¯) = F(β¯,ω¯). Using (4)
K (ǫ)
{ } →
and that F (βk) F (β¯), we see that F(βk+1,ωk) F (β¯) = F(β¯,w¯). Besides,
(ǫ) (ǫ) (ǫ)
→ →
combining with (3), it holds that
F(β,wk) F(βk+1,wk), β Rp.
≥ ∀ ∈
Let k ( ) , we obtain that
∈K →∞
F(β,w¯) F(β¯,w¯), β Rp,
≥ ∀ ∈
which yields that
p
β¯ arg min Xβ y +λ β 2+λ q β w¯ . (5)
∈ β∈Rp(k − kr 2 k k2 1 | i | i )
i=1
X
From the first-order optimality condition of (5), we have
0 ∂ Xβ¯ y +2λ β¯ +qλ w¯ Sign(β¯), i. (6)
r 2 i 1 i i
∈ k − k i ∀
(cid:16) (cid:17)
10It follows from the definition of wk, we can express w¯k as
i i
β¯ q−1 , if β¯ >uq−1 1,
w¯ = i i ǫ
i 1
((cid:12)u ǫ,(cid:12) if (cid:12)β¯ i(cid:12) u ǫq−1,
(cid:12) (cid:12) (cid:12) (cid:12)≤
(cid:12) (cid:12)
Then, it shows that (6) can be expressed as (cid:12) (cid:12)
0 ∂ Xβ¯ y +2λ β¯ +λ ∂h (β¯), i.
∈ k −
kr
i
2 i 1 uǫ i
∀
(cid:16) (cid:17)
In light of the above analysis, it follows that β¯ is a generalized first-order stationary
point of F () defined in (10). Using the above results and Theorem 3, we conclude
(ǫ)
that β¯ is a g· eneralized first-order stationary point of (3). Thereafter, the nonzero
entries of β¯ satisfy the lower bound (7) from Theorem 2.
Observing that problem (1) is convex and nonsmooth, we can develop two
algorithms that effectively leverage the underlying its structures.
3.1 ADMM for (1)
In this section, we aim to utilize a well-known first-order method called ADMM to
solve (1). For this purpose, we let η := Xβ y and θ := W β, then (1) can be is
k
−
equivalent to
min η +λ β 2+λ q θ
β,η,θ k kr 2 k k2 1 k k1
s.t. Xβ y η =0, (7)
− −
W β θ =0.
k
−
Let σ > 0 be a penalty parameter, the augmented Lagrangian function associated
with problem (7) is given by
σ
(β,η,θ;u,v)= η +λ β 2+λ q θ + u,Xβ y η + Xβ y η 2
Lσ k kr 2 k k2 1 k k1 h − − i 2k − − k2
σ
+ v,W β θ + W β θ 2,
h k − i 2k k − k2
where u Rn and v Rp are multipliers associated with the constraints. When
∈ ∈
the traditionalADMM is employed,it miminizes the augmented Lagrangianfunction
alternativelywith respectto two non-overlappingblocks,suchas one block β and the
other block (η,θ). This leads to the following framework:
β(j) =argmin (β,η(j),θ(j);u(j),v(j)),
β σ
L
(η(j+1),θj+1)=argmin (β(j+1),η,θ;u(j),v(j)),
η,θ σ
u(j+1)
=uj +τσ Xβ(j+1)
−L
η(j+1) −y ,
v(j+1) =vj +τσ (cid:0)(cid:0)W kβ(j+1) −θ(j+1) (cid:1), (cid:1)
11whereτ issteplengthchosenintheinterval(0,(1+√5)/2).Clearly,thecomputational
costprimarilyarisesfromsolvingthesubproblemsrelatedtothevariablesβ and(η,θ).
We will now demonstrate that eachsubproblemhas closed-formsolutions by utilizing
the proximal mappings of the ℓ -norm for r =1,2, .
r
∞
Wewillnowconcentrateonaddressingbothsubproblemsinvolvedinthis iterative
scheme.Firstly,withgiven(η(j),θ(j) andmultipliers(u(j),v(j),itiseasytodeducethat
solving the β-subproblem is actually equivalent to finding a solution of the following
linear system:
(2λ I +σX⊤X+σW⊤W )β =σX⊤(y+η(j) σ−1u(j))+σW⊤(θ(j) σ−1v(j)),
2 p k k − k −
where I is an identity matrix in p-dimensional real space. Fortunately, solving
p
this linear system is not particularly challenging because the well-known Sherman-
Morrison-Woodburyformulacanbeusedtoobtaintheinverseofitscoefficientmatrix.
Secondly, when we focus on the augmented Lagrangianfunction with variables (η,θ),
we observe that both variables are independent. This implies that solving the (η,θ)-
subproblem together is effectively the same as solving each one individually. As a
result, it is a trivial task to deduce that the η- and θ- subproblems can be written as
the following proximal mapping forms
η(j+1) =Prox σ−1k·kr Xβ(j+1) −y+u(j)/σ ,
(cid:16) (cid:17)
and
θ(j+1) =Prox σ−1λ1qk·k1 W kβ(j+1)+v(j)/σ .
From subsection 2.1, we know that the pro(cid:16)ximal mappings of(cid:17)the ℓ r-norm function
forr =1,2, haveanalyticalexpressions.This suggeststhatthe iterativeframework
∞
can be easily implemented.
Basedontheaboveanalysis,weoutlinetheiterativestepsofADMMasitisapplied
to solve the convex subproblem (1).
Algorithm 1: ADMM for (1)
Input. With given β(0) :=βk, choose an initial point (η(0),θ(0);u(0),v(0)). Choose
positive constants σ > 0 and τ (0,(1+√5)/2). For j = 0,1,..., do the
∈
following operations iteratively:
Step 1. Given η(j), θ(j), and u(j), v(j). Employ a numerical method to compute an
solution β(j+1) to the linear system
2λ I +σX⊤X +σW⊤W β =σX⊤ y+η(j) σ−1u(j)
2 p k k −
(cid:16) (cid:17) (cid:16) (cid:17)
+σW⊤ θ(j) σ−1v(j) .
k −
(cid:16) (cid:17)
Step 2. Given β(j+1) and u(j). Compute η(j+1) according to
η(j+1) :=Prox σ−1k·kr Xβ(j+1) −y+u(j)/σ ,
(cid:16) (cid:17)
12Step 3. Given β(j+1) and v(j). Compute θ(j+1) according to
θ(j+1) :=Prox σ−1qλ1k·k1 W kβ(j+1)+v(j)/σ .
(cid:16) (cid:17)
Step 4. Given β(j+1), η(j+1), θ(j+1). Update u(j+1) and v(j+1) according to
u(j+1) :=u(j)+τσ Xβ(j+1) η(j+1) y ,
− −
(cid:0) (cid:1)
and
v(j+1) :=v(j)+τσ W β(j+1) θ(j+1) .
k
−
(cid:0) (cid:1)
The convergenceresults for the ADMM algorithmcan be directly referenced from
[33, 34]. Hence, we omit the convergence theorem here. While ADMM is straight-
forward to implement, it is only a first-order algorithm, which may not yield higher
precisionsolutionsquickly.Inthe followingsection,wewillleveragethe specificstruc-
ture of problem (1) and employ the highly efficient SSN method, utilizing the PMM
approach through its dual.
3.2 SSN based on PMM for (1)
The function F (β) obtained from (1) is convex but nonsmooth. However, it is well-
βk
known that a smooth function is required for the SSN method, which provides a
faster local convergence rate. Consequently, we can utilize the PMM approach, with
additional details available in [17].
Let µ>0 be a given positive scalar that can also be determined dynamically. We
consider (1) with an additional proximal point term:
µ
βk+1 :=arg min F˜ (β):= Xβ y +λ β 2+qλ W β + Xβ Xβk 2 ,
β∈Rp βk k − kr 2 k k2 1 k k k1 2k − k2
n (o8)
where βk Rp is a current point, and the last term can be regarded as a proximal
∈
point term. It should be noted that the last term is actually a precondition used to
derive a dual problem with favorable structures.
To derive its dual, it is convenient for us to express (8) as:
min η +λ β 2+qλ W β + µ η+y Xβk 2
β,η k kr 2 k k2 1 k k k1 2k − k2 (9)
s.t. Xβ η =y.
−
The Lagrangianfunction associated with problem (9) is given by
µ
(β,η;u)= η +λ β 2+qλ W β + η+y Xβk 2+ u,Xβ y η ,
L k kr 2 k k2 1 k k k1 2k − k2 h − − i
where u Rm is a multiplier associated with the constraint. The Lagrangian dual
∈
functionD(u)isdefinedastheminimumvalueoftheLagrangianfunctionover(β,η),
13that is
D(u)=inf (β,η;u)
β,ηL
=min λ β 2+ u,Xβ +qλ W β
β 2 k k2 h i 1 k k k1
n o
µ
+min η + η+y Xβk 2 u,η u,y .
η k kr 2k − k2−h i −h i
n o
Let g (β) = qλ W β . One can use the Moreau envelope function to express the
k 1 k 1
k k
minimize values of the β- and η-subproblems, that is,
(u):=min λ β 2+ u,Xβ +qλ W β
X β 2 k k2 h i 1 k k k1
n o
1
=Φ (2λ2)−1gk(·) −(2λ 2)−1X⊤u
− 4λ
kX⊤u k2 2, (10)
2
(cid:16) (cid:17)
and
µ
(u):=min η + η+y Xβk 2 u,η
Y η k kr 2k − k2−h i
n o
µ
=Φ µ−1k·kr µ−1u −y+Xβk
−
2kµ−1u −y+Xβk k2 2.
(cid:16) (cid:17)
Here, the minimizers of β- and η-subproblems arise from the expression for the
proximal mapping:
β =Prox (2λ2)−1gk(·) −(2λ 2)−1X⊤u ,
(cid:16) (cid:17)
and
η =Prox µ−1k·kr µ−1u −y+Xβk ,
provided that u is given. (cid:16) (cid:17)
The Lagrangian dual problem of (9) involves maximizing the dual function D(u),
which can be equivalently expressed as the following optimization problem:
min Θ(u):= u,y (u) (u) . (11)
u h i−X −Y
n o
It is established in [29, 30] that Θ(u) is convex and first-order continuously differen-
tiable, with its gradient given by:
∇Θ(u)= −XProx (2λ2)−1gk(·) −(2λ 2)−1X⊤u +Prox µ−1k·kr µ−1u −y+Xβk +y.
(cid:0) (cid:1) (cid:0) (cid:1)
As aresult,the minimizer ofΘ(u)canbe obtainedbysolvingthe followingfirst-order
optimality condition:
Θ(u)=0.
∇
14Additionally, it is important to note that the proximal mappings of the ℓ -norm for
r
r = 1, 2, and are strongly semismooth. Consequently, the above equations are
∞
also strongly semismooth, allowing for the application of the well-known semismooth
Newton method.
It is noteworthy that the proximal operators Prox () with r = 1, 2 and is
k·kr
· ∞
nonsmooth, which means that the Hessian of Θ(u) is unavailable. Fortunately, due
to the strong semismoothness of the proximal mappings embedded in Θ(u), the
∇
generalized Hessian of Θ(u) can be obtained explicitly from [18, 28]. Noting that the
proximal mapping operators with r = 1, 2 and are Lipschitz continuous, then the
∞
following multifunction is well defined:
∂˜2Θ(u):=(2λ 2)−1X∂ Prox (2λ2)−1gk(·) −(2λ 2)−1X⊤u X⊤
+µ−1∂ Prox(cid:16) µ−1k·kr µ−1u −y(cid:0)+Xβk . (cid:1) (cid:17)
(cid:16) (cid:0) (cid:1)(cid:17)
ChooseU ∈∂Prox (2λ2)−1gk(·) −(2λ 2)−1X⊤u andV ∈∂Prox µ−1k·kr µ−1u −y+Xβk .
And let
(cid:0) (cid:1) (cid:0) (cid:1)
H :=(2λ )−1XUX⊤+µ−1V.
2
ItisclearthatH isapositivesemi-definitematrix,whichimpliesthattheSSNmethod
cannot be used. To overcome this issue, we modify H by adding a term:
H˜ :=H +νI ,
n
where ν is a very small positive constant.
Based on the analysis above, we are ready to outline the iterative framework for
the SSN method within the framework of PMM approach. The iterative steps are
described as follows:
Algorithm 2: PMM based on SNN for (8) (PMM-SSN)
Step 0. At current point βk Rp choose an initial point u(0) Rn. Input µ > 0,
∈ ∈
̺ (0,1/2) and δ (0,1).
∈ ∈
Step 1. Applying SSN to find an approximate solution u¯k+1 such that
u¯k+1 argminΘ(u)
≈ u
via the following steps:
Step 1.1. Select U(j)
∈
∂Prox (2λ2)−1gk(·)
−
(2λ 2)−1X⊤u(j) and V(j)
∈
∂Prox µ−1u(j) y+Xβk , and set
µ−1k·kr − (cid:0) (cid:1)
(cid:0) (cid:1)
H˜(j) :=(2λ )−1XU(j)X⊤+µ−1V(j)+νI .
2 n
15Employ a numerical method (e.g., conjugate gradient method) to
compute an approximate solution ∆uj to the linear system
H˜(j)∆u(j)+ Θ(u(j))=0.
∇
Step 1.2. Find α
j
:=δtj, where t
j
is the first nonnegative integer t such that
Θ(u(j)+δt∆u(j)) Θ(u(j))+̺δt Θ(u(j)),∆u(j) .
≤ h∇ i
Step 1.3. Set u(j+1) :=u(j)+α ∆u(j). Let j =j+1, go to Step 1.1.
j
Step 2. Compute
βk+1 :=Prox (2λ2)−1gk(·) −2λ 2−1X⊤u¯(k+1) .
(cid:16) (cid:17)
We will now establish some relationships between the problems (8) and (1) under
certain conditions. First, we demonstrate that F˜ (β) converges to F (β) when the
βk βk
positive number µ is sufficiently small.
Theorem 5. The optimal objective value of problem (8) converges to that of (1) if
µ 0, that is,
↓ lim min F˜ (β)= min F (β).
µ↓0β∈Rp
βk
β∈Rp
βk
Proof. It holds that
min F˜ (β) min F (β).
β∈Rp
βk
≥β∈Rp
βk
For any µ>0 and β Rp, we have that
∈
µ
min F˜ (β) Xβ y +λ β 2+qλ W β + Xβ Xβk 2.
β∈Rp βk ≤k − kr 2 k k2 1 k k k1 2k − k2
Taking limits on both hand-sides of this inequality as µ 0, it gets that
↓
lim min F˜ (x) Xβ y +λ β 2+qλ W β ,
µ↓0β∈Rp βk ≤k − kr 2 k k2 1 k k k1
which means
lim min F˜ (β) min F (β),
µ↓0β∈Rp
βk
≤β∈Rp
βk
that is
lim min F˜ (β)= min F (β),
µ↓0β∈Rp
βk
β∈Rp
βk
which shows the conclusion is true.
Next,wewillshowthatthe optimalsolutionβ¯ofF˜ (β)is actuallythe minimizer
βk
of F (β) in the case of βk β¯.
Proβ pk osition 6. The vector≡ β¯ is an optimal solution of (1) if and only if there exists
a µ 0 such that
≥ β¯ ∈arg βm ∈i Rn pF˜ β¯(β).
16This means that the optimal solution tothe problem min β∈RpF˜ β¯(β) is indeed the same
as the optimal solution to min β∈RpF β¯(β). Consequently, the nonzero entries of β¯
satisfy the lower bound (7) when ǫ satisfies (11).
Proof. Noting that F β¯( ·) is locally Lipschitz continuous near β¯ and convex at β¯, we
have that 0 ∈∂F β¯(β) is equivalent to β¯ being an optimal solution of F β¯( ·). Addition-
ally, since the function F˜ β¯( ·) is convex, it follows that 0
∈
∂F˜ β¯(β) is equivalent to
β¯ ∈argmin β∈RpF˜ β¯(β). Combining these results with the fact that ∂F β¯(β¯)=∂F˜ β¯(β¯),
we conclude that the optimal solution β¯ for min β∈RpF˜ β¯(β) is also the optimal solu-
tion for F β¯(β). Subsequently, applying Theorem 4 and 2, we can directly obtain the
remaining result of this theorem.
4 Numerical experiments
In this section, we showcase several simulated and real-world examples to highlight
the performanceofthe proposedalgorithmsintacklingnon-convexandnon-Lipschitz
penalized high-dimensional linear regression problems. All the experiments are per-
formed with MicrosoftWindows 10 and MATLAB R2021b,and run on a PC with an
Intel Core i5 CPU at 2.70 GHz and 16 GB of memory.
4.1 Experiments setting
(i) Data generations: In the simulation experiments, we generate the n p matrix
×
X whose rows are drawn independently from (0,Σ) with = κ|i−j| and 1
N i−j ≤
i,j p, where κ is the correlation coefficient of matrix X. In order to generate the
targ≤ et regression coefficient β Rp, we randomly select a subP set of 1,...,p to form
∈ { }
the active set ∗ with ∗ = K < m. Let R = m /m , where m = β∗ and
A |A |
2 1 2
k
A∗ kmax
m = β∗ = 1. Here and denote the maximum and minimum
1
k
A∗ kmin k·kmax k·kmin
absolute values of a vector, respectively. Then the K nonzero coefficients in β∗ are
uniformly distributed in [m ,m ]. The response variable is derived by y =Xβ∗+αε,
1 2
where α is a noise level, and ε is a noise, e.g., log-normal noise, Gaussian noise, and
uniform noise.
(ii) Parameters settings: The values of the parameters involved in model (3) and
algorithmsADMMandPMM-SSNwillbegiveninthefollowingcontext.Additionally,
through extensive tests, we observed that the choice of the initial point has minimal
impactoneachalgorithm.Asaresult,wehavechosentosettheinitialpointuniformly
to 0.
(iii) Stopping criteria: We use the following relative residualdenoted by η and η
1 2
to measure the accuracy the approximate optimal solution:
β Prox (β X⊤(Xβ y))
η = k
k
−
λ1k·k1 k
−
k
−
k2
1 1+ β + X⊤(Xβ y)
k 2 k 2
k k k − k
and
βk+1 βk
2
η := k − k .
2 max βk ,1
2
{k k }
17Inaddition,westoptheiterativeprocessifη 1e 4ortheiterationnumberachieves
2
≤ −
2000.
(iv) Evaluation indexes: To highlight the efficiency and accuracy of PMM-SSN
and ADMM, we perform experiments comparing these two methods with each other
and with other algorithms. Specifically, we consider four evaluation metrics based
on multiple independent experiments: the average CPU time (Time) in seconds, the
standard deviation (SD), the average ℓ -norm relative error, and the average mean
2
squared error (MSE), defined as follows:
βk β βk β
2 2
RE:= k − k /N, and MSE:= k − k /N,
β p
2
k k
X X
where βk is the estimated regression coefficient after k iterations and N is the total
times of tests. Clearly,a smaller ‘Time’ indicates faster calculationspeed, while lower
values of ‘SD’, ‘RE’, and ‘MSE’ reflect higher solution quality.
4.2 Test on simulated data
In this part, we do a series of numerical experiments on fixed and random data to
numerically evaluate the superiority of our propose algorithms. Meanwhile, we also
conduct performance comparisons among these algorithms to address the proposed
problem (3).
4.2.1 Test on a fixed data
Inthissubsection,weanalyzethe numericalbehaviorofPMM-SSNandADMMalgo-
rithms based on100 independent tests. In eachtest, we use the followingparameters’
values: n=300, p=1000, K =10, R=100, and N =5.
We investigate the performance of PMM-SSN and ADMM for variable selection
and parameter estimation. In this test, we generate the coefficient matrix X with
κ = 0.2 and set the true regression parameter as β, whose 10 non-zero elements are
β = 5, β =2, β = 3, β = 2, β =4, β = 4, β =1, β =5,
30 − 198 269 − 395 − 442 495 − 637 776
β = 1, and β = 3. We set the noise level as α = 1e 3. For simplicity, we
777 − 865 −
set q = 1 here, which represents the elastic net penalty. To clearly see the numerical
performance of PMM-SSN and ADMM algorithms, we show the box plots in Figure
1. Additionally, we compute the overall averagestandard deviation (SD) to illustrate
the recovery performance of both algorithms at zero input positions. It is important
to note that, to help readers assess the bias, the values are presented to four decimal
places. Furthermore, to evaluate the estimation accuracy for a large number of zero
elements, we also provide the average relative error (RE) values.
Inthistest,wechooseσ =1e 3andτ =1.618inADMM,while,wesetµ=1e 3,
− −
ρ=0.9, ̺=0.1, δ =0.5 and ν =1e 6 in PMM-SSN. On the model parameters, we
−
choose different values based on the various types of noise. In the case of log-normal
distribution noise (LN), we set r = 1, i.e., the loss function is Ax b . In the case
1
k − k
of Gaussiandistribution noise (GN), we set r =2, i.e., the loss function is Ax b .
2
k − k
The tuning parameters’values in both cases are chosenas λ =0.02 andλ =5e 4.
1 2
−
18In the case of uniform distribution noise (UN), we set r = , i.e., the loss function
∞
is Ax b . Meanwhile, the weighting parameters are chosen as λ = 0.02 and
∞ 1
k − k
λ =5e 6.
2
−
Under these parameters’ settings, for the LN case, the PMM-SSN yields a RE
value of2.69e 2,while the ADMM yields a RE value of 3.2e 2. The corresponding
− −
SD values are 8.9e 3 for PMM-SSN and 8.8e 3 for ADMM. For the case of GN,
− −
the PMM-SSNachievesRE=2.600e 2andSD=8.900e 3,while the ADMM yields
− −
RE=3.190e 2 and SD=8.800e 3. For the case of UN, the RE values obtained by
− −
PMM-SSNandADMMare1.200e 3and1.200e 3,respectively,whiletheSDvalues
− −
are 4.8170e 4 and 2.297e 4. The results show that the performance of PMM-SSN
− −
and ADMM are comparable.
5 5 5
4 4 4
3 3 3
2 2 2
1 1 1
0 0 0
-1 -1 -1
-2 -2 -2
-3 -3 -3
-4 -4 -4
-5 -5 -5
30 198 269 395 442 495 637 766 777 865 30 198 269 395 442 495 637 766 777 865 30 198 269 395 442 495 637 766 777 865
The position of nonzero elements in * The position of nonzero elements in * The position of nonzero elements in *
(a) LN (PMM-SSN) (b) GN (PMM-SSN) (c) UN (PMM-SSN)
5 5 5
4 4 4
3 3 3
2 2 2
1 1 1
0 0 0
-1 -1 -1
-2 -2 -2
-3 -3 -3
-4 -4 -4
-5 -5 -5
30 198 269 395 442 495 637 766 777 865 30 198 269 395 442 495 637 766 777 865 30 198 269 395 442 495 637 766 777 865
The position of nonzero elements in * The position of nonzero elements in * The position of nonzero elements in *
(d) LN (ADMM) (e) GN (ADMM) (f) UN (ADMM)
Fig. 1: Distribution of predicted points at nonzero input locations
Figure 1 illustrates the distribution of the regression data generated by PMM-
SSN and ADMM at the non-zero input positions. From this figure, we can observe
that both algorithms exhibit stability in addressing linear regression problems across
different noise types, as evidenced by the average value being closely aligned with
the true non-zeroelements.This phenomenonshowsthatPMM-SSNandADMM can
estimate non-zero elements with a high accuracy.
4.2.2 Test with different parameter’s values
Inthispart,wenumericallycomparetheperformanceofPMM-SSNandADMMfrom
the perspective ofthe RE values using randomsimulateddata.Basedonthe different
typesofnoise,wereporttheaveragenumberofiterationswithdifferentsparsitylevels
in Figure 2. In this test, we use the approachdescribed in Section 4.1 to generate the
19
tneiciffeoc
detamitse
eht
fo
eulav
ehT
tneiciffeoc
detamitse
eht
fo
eulav
ehT
tneiciffeoc
detamitse
eht
fo
eulav
ehT
tneiciffeoc
detamitse
eht
fo
eulav
ehT
tneiciffeoc
detamitse
eht
fo
eulav
ehT
tneiciffeoc
detamitse
eht
fo
eulav
ehTdata except that the correlation coefficient κ is set as 0.3. In this simulation, we set
the ℓ -norm with q = [0.2,0.3,0.5,0.7,0.9] in model (3). On the tuning parameters,
q
we chose λ =0.2, λ =1e 4, andµ=1e 2 in the case of LN and GN noise. But,
1 2
− −
in the case of UN noise, we choose λ = 0.03 and λ = 1e 7. In addition, we also
1 2
−
test both algorithms using different sparsity levels K = [5,...,15]. The results are
presented in Figure 2, with noise types LN, GN, and UN arrangedfrom left to right.
0.25 0.25 0.15
q=0.2 q=0.2 0.2
q=0.3 q=0.3 0.3
0.2 q q= =0 0. .5 7 0.2 q q= =0 0. .5 7 q q= =0 0. .5 7
q=0.9 q=0.9 q=0.9
0.1
0.15 0.15
0.1 0.1
0.05
0.05 0.05
05 6 7 8 9 10 11 12 13 14 15 05 6 7 8 9 10 11 12 13 14 15 05 6 7 8 9 10 11 12 13 14 15
K K K
(a) LN (SSN-PMM) (b) GN (SSN-PMM) (c) UN (SSN-PMM)
0.25 0.25 0.45
q=0.2 q=0.2 q=0.2
q=0.3 q=0.3 0.4 q=0.3
q=0.5 q=0.5 q=0.5
0.2 q q= =0 0. .7 9 0.2 q q= =0 0. .7 9 0.35 q q= =0 0. .7 9
0.3
0.15 0.15
0.25
0.2
0.1 0.1
0.15
0.05 0.05 0.1
0.05
05 6 7 8 9 10 11 12 13 14 15 05 6 7 8 9 10 11 12 13 14 15 05 6 7 8 9 10 11 12 13 14 15
K K K
(d) LN (ADMM) (e) GN (ADMM) (f) UN (ADMM)
Fig. 2: RE values underdifferent q values and sparsity levels
We focus on the first row of Figure 2 which represents the effectiveness of PMM-
SSN in terms of relative error (RE) according to different types of noise. As shown
in each plot (a) and (b) that, except for the case of q = 0.2, the relative error lines
remainrelativelystable,whichindicatesthattheresultsareconsistentacrossdifferent
sparsity levels. Additionally, it is evident that the error is significantly larger when
q =0.2,whileitismuchsmallerintheothercases.However,asseeninplot(c),inthe
UNcase,therelativeerrorissignificantlysmallwhenthesparsitylevelexceeds9,this
suggests that the ℓ -norm in the generalized elastic net penalty has minimal impact
q
on regression accuracy when the coefficients are sufficiently sparse. Now we observe
the second row of Figure 2 to evaluate the performance of ADMM in the sense of the
value of RE. In the cases of LN and GN noise, it is evident that the penalty with
q = 0.2, the ADMM performs poorly, whereas q = 0.5 and 0.7, ADMM delivers the
best performance. However, in the case of UN noise, ADMM performs best at each
sparsity level when q = 0.3, but its fails with q = 0.2. Finally, we see from each plot
that, the RE remains relatively stable when q = 0.5 at all sparsity levels. Therefore,
in subsequent experimental tests, we set q = 0.5 for the proposed generalized elastic
net penalty.
20
ER
ER
ER
ER
ER
ERTodemonstratethenumericaladvantagesofADMMandPMM-SSN,weconducted
aseriesofexperimentsusingdifferenttuningparametervalues.Inthistest,wesetthe
sparsity level to K =10 and the penalty parameter to q =0.5, as these settings have
beenshowntobeeffective.Forsimplicityinthistest,wefixλ andgraduallyvarythe
2
valueofλ forthe ℓ -norminthe generalizedelasticnetpenalty of (3).Forλ ,weset
1 q 2
λ =1e 4 for both LN and GN noise, while for UN noise,we set λ =1e 6. In the
2 2
− −
test, we run PMM-SSN and ADMM 50 times and record the average values of MSE
values (‘MSE’), the RE values (‘RE’) and the numbers of iterations (‘Iter’) in Table
1.Fromthis table we see thatbothtested algorithmsperformeffectively ineachcase,
producingregressionsolutionswithhigheraccuracy.Additionally,weobservethatthe
values ofRE andMSE obtainedusing PMM-SSNare generallylowerthanthose from
ADMM. This indicates that the second-order SSN employed in the PMM framework
is beneficial for achieving higher accuracy solutions. Focusing on the third column
regardingtheiterations,weseethatPMM-SSNoutperformsADMM,achievingbetter
results in fewer iterations. This further demonstrates that the PMM-SSN algorithm
can produce more accurate solutions.
Table 1: Numerical results of PMM-SSN and ADMM.
PMM-SSN ADMM
Noise λ 1 MSE RE Iter MSE RE Iter
0.15 4.272e−08 4.204e−05 3 2.879e−06 3.513e−04 33
LN 0.2 3.537e−08 3.852e−05 3 1.838e−06 2.833e−04 36
0.25 1.483e−08 2.483e−05 3 2.255e−06 2.946e−04 39
0.15 1.736e−09 8.499e−06 6 2.924e−06 3.573e−04 34
GN 0.2 2.094e−09 9.476e−06 9 1.377e−06 2.429e−04 38
0.25 1.729e−06 2.645e−04 8 3.639e−06 3.940e−04 39
0.03 4.010e−08 3.227e−05 14 3.599e−06 3.906e−04 120
UN 0.04 2.018e−08 2.788e−05 9 1.292e−05 6.954e−04 226
0.05 2.974e−10 3.604e−06 8 3.432e−01 1.129e−01 295
4.3 Performance comparisons with IAGENR-Lq
In this section, we assess the superiority of the proposed model (3) and evaluage
the efficiency of PMM-SSN and ADMM by comparing them to the state-of-the-art
algorithm IAGENR-Lq developed by Zhang et al. [8]. The data utilized in this part
is identical to that in Section (4.1), but here we take the correlation coefficient κ as
0.3. It is important to note that the proposed model (3) is novel, and no existing
algorithms can be employed for comparison. Therefore, we test it against the least
squares estimation model (2) to evaluate the superiority of (3). Additionally, in this
comparison test, we specifically focus on r = 2, indicating that both models being
evaluatedaredesignedto handle Gaussiannoise.As evidentby the previoustests, we
takeq =0.5inthegeneralizedelasticnetpenalty,andchooseλ =0.2andλ =1e 4
1 2
−
for the tuning parameters. Furthermore, we select λ = 1e 8 and λ = 1e 4
1 2
− −
for IAGENR-Lq, as these values have consistently demonstrated their effectiveness in
experiments’ preparations. In this test, we consider various values of n and p, along
21Table 2: Numerical resuts of PMM-SSN, ADMM, and IAGENR-Lq.
PMM-SSN ADMM IAGENR-Lq
Matrix(Dim) K MSE RE Iter MSE RE Iter MSE RE Iter
5 4.308e−07 1.660e−04 7 7.330e−07 2.154e−04 31 2.538e−06 4.037e−04 7
800×400 7 2.157e−07 1.118e−04 8 2.434e−06 3.656e−04 29 2.620e−06 3.963e−04 7 9 1.211e−06 1.807e−04 20 1.477e−06 2.822e−04 40 3.220e−06 4.036e−04 7
5 4.252e−07 9.523e−05 8 1.053e−05 6.560e−04 36 1.177e−05 7.062e−04 9
1000×300 10 2.094e−09 9.476e−06 8 1.377e−06 2.429e−04 37 1.113e−05 6.962e−04 9
15 1.069e−08 1.829e−05 10 3.207e−06 3.648e−04 40 1.071e−05 6.795e−04 10
6 5.531e−07 3.045e−04 8 1.835e−06 5.491e−04 36 4.964e−06 8.922e−04 8
2000×500 8 8.229e−07 3.631e−04 9 1.346e−06 4.659e−04 38 5.392e−06 9.186e−04 8
10 7.9452e−07 2.4117e−04 9 1.543e−06 4.158e−04 38 6.004e−06 8.538e−04 10
22with different levels of sparsity K. The results for MSE, RE, and Iter are presented
in Table 2.
Upon analyzing the first two columns of Table 2, which present the MSE and
RE values for each algorithm, we observe that PMM-SSN achieves the lowest values,
while IAGENR-Lq records the highest. We see that PMM-SSN and ADMM slightly
outperformtheIAGENR-Lqintermsofregressionaccuracy.Insummary,theseresults
support the conclusionthat our proposedmodel (3) improve the quality of regression
quality derived from model (2).
5 Conclusions
In this paper, we introduced a generalized elastic net model for sparse linear regres-
sionthat utilizes the ℓ -normas the lossfunction andincorporatesthe ℓ -norminthe
r q
elastic net penalty.We establishedthe generalizedfirst-orderstationarypointfor this
generalized elastic net model and subsequently derived lower bounds for its non-zero
entries and local minimizers. Furthermore, utilizing the ǫ-approximationfunction h
uǫ
fromLu[27],wedemonstratedthatanyaccumulationpointofthesequencesgenerated
bythesemethodsisageneralizedfirst-orderstationarypoint.Forpracticalimplemen-
tation,weproposedtwoefficientalgorithmswithinaniterativereweightedframework:
one is the easily implementable first-order algorithm ADMM, and the other is the
faster PMM-SSN algorithm, which leverages second-order information. To demon-
stratethe effectivenessoftheproposedalgorithms,weconductedaseriesofnumerical
experiments using both simulateddata anda realdataset.We comparedtheir perfor-
manceagainstthestate-of-the-artalgorithm,IAGENR-Lq.Theresultsindicatedthat
both algorithms are effective in addressing high-dimensional sparse linear regression
problems and exhibited strong robustness across different types of noise. Our find-
ings revealed that while ADMM performed well in numerical experiments due to its
straightforward implementation, the PMM-SSN demonstrated superior performance
intacklingcomplexproblems,particularlywhensecond-orderinformationplaysacru-
cial role in improving solution accuracy. Consequently, for high-dimensional sparse
linear regression problems that demand higher precision, the PMM-SSN may offer a
more effective choice.
Acknowledgements. The work of Yanyun Ding is supported by the Shenzhen
Polytechnic University Research Fund (Grant No. 6024310021K). The work of Peili
Li is supported by the National Natural Science Foundation of China (Grant No.
12301420). The work of Yunhai Xiao is supported by the National Natural Science
Foundation of China (Grant No. 12471307 and 12271217), and the National Natural
Science Foundation of Henan Province (Grant No. 232300421018).
References
[1] Chen, S.S., Donoho, David, L., Saunders, Michael, A.: Atomic decomposition by
basis pursuit. SIAM review 43(1), 129–159(2001)
23[2] Tibshirani, R.: Regression shrinkage and selection via the lasso. Journal of the
Royal Statistical Society Series B 58(1), 267–288 (1996)
[3] Efron,B.,Hastie,T.,Johnstone,I.,Tibshirani,R.:Leastangleregression.Annals
of Statistics (2), 32 (2004)
[4] Meinshausen, N., Yu, B.: Lasso-type recovery of sparse representations for high-
dimensional data. Annals of Statistics 37(1), 246–270(2009)
[5] Zou, H., Hastie, T.: Regularization and variable selection via the elastic net.
Journal of the Royal Statistical Society Series B 67(2), 301–320(2005)
[6] Rick, C.: Exact reconstruction of sparse signals via nonconvex minimization.
IEEE Signal Processing Letters 14(10), 707–710(2007)
[7] Sun,Q.:Sparseapproximationpropertyandstablerecoveryofsparsesignalsfrom
noisy measurements. IEEE transactions on signal processing 59(10), 5086–5090
(2011)
[8] Li, S., Ye, W.: A generalizedelastic net regularizationwith smoothed l0 penalty.
Advances in Pure Mathematics 7(1), 66–74 (2017)
[9] Belloni, A., Chernozhukov, V.: ℓ -penalized quantile regression in high-
1
dimensional sparse models. Annals of Statistics 39(1), 82–130 (2011)
[10] Bradic,J.,Fan, J.,Wang,W.: Penalizedcomposite quasi-likelihoodforultrahigh
dimensional variable selection. Journal of the Royal Statistical Society Series B
73(3), 325–349(2011)
[11] Fan,J.,Fan,Y.,Barut,E.:Adaptiverobustvariableselection.AnnalsofStatistics
42(1), 324–351(2012)
[12] Wang, L.: The ℓ penalized lad estimator for high dimensional linear regression.
1
Journal of Multivariate Analysis 120, 135–151(2013)
[13] Wang,L.,Wu,Y.,Li,R.:Quantileregressionforanalyzingheterogeneityinultra-
high dimension. Journal of the American Statistical Association 107(497), 214–
222 (2012)
[14] Belloni, A., Chernozhukov, V., Wang, L.: Square-root lasso: pivotal recovery of
sparse signals via conic programming.Biometrika 98(4), 791–806 (2011)
[15] Bellec, P.C., Lecu´e, G., Tsybakov, A.B.: Slope meets lasso: improved oracle
bounds and optimality. Annals of Statistics 46(6B), 3603–3642(2016)
[16] Derumigny, A.: Improved bounds for square-root lasso and square-root slope.
Electronic Journal of Statistics 12(1), 741–766(2018)
24[17] Tang, P., Wang, C., Sun, D., Toh, K.-C.: A sparse semismooth newton based
proximal majorization-minimization algorithm for nonconvex square-root-loss
regressionproblems.JournalofMachineLearningResearch21(226),1–38(2020)
[18] Ding,Y.,Zhang,H.,Li,P.,Xiao,Y.:Anefficientsemismoothnewtonmethodfor
adaptive sparse signal recovery problems. Optimization Methods and Software
38(2), 262–288(2023)
[19] Wen, Y.-W., Ching, W.-K., Ng, M.: A semi-smooth newton method for inverse
problem with uniform noise. Journal of Scientific Computing 75(2), 713–732
(2018)
[20] Xue, Y., Feng, Y., Wu, C.: An efficient and globally convergent algorithm for
ℓ -ℓ model in group sparse optimizationn. Communications in Mathematical
p,q r
Sciences 18, 227–258(2020)
[21] Xiao, Y., Shen, J., Ding, Y., Shi, M., Li, P.: A fast and effective algorithm for
sparse linear regression with lp-norm data fidelity and elastic net regularization.
Journal of Nonlinear & Variational Analysis 8(3), 433–449(2024)
[22] Chen,X.,Ge,D.,Wang,Z.,Ye,Y.:Complexityofunconstrainedℓ -ℓ minimiza-
2 p
tion. Mathematical Programming 143(1-2), 371–383(2011)
[23] Ge, D., Jian, X., Ye, Y.: A note on the complexity of ℓ minimization.
p
Mathematical Programming 129(2), 285–299(2011)
[24] Chen, X., Zhou, W.: Convergence of the reweighted ℓ minimization algorithm
1
for ℓ -ℓ minimization. Computational Optimization and Applications 59(1-2),
2 p
47–61 (2014)
[25] Daubechies, I., DeVore, R., Fornasier, M., Gu¨ntu¨rk, C.S.: Iteratively reweighted
least squares minimization for sparse recovery. Communications on Pure and
AppliedMathematics:AJournalIssuedbytheCourantInstituteofMathematical
Sciences 63(1), 1–38 (2010)
[26] Lai, M.J., Wang, J.: An unconstrained ℓ minimization with 0 q 1 for
q
≤ ≤
sparsesolutionofunderdeterminedlinearsystems.SIAMJournalonOptimization
21(1), 82–101 (2011)
[27] Lu, Zhaosong: Iterative reweighted minimization methods for ℓ regularized
p
unconstrained nonlinear programming. Mathematical Programming 147(1–2),
277–307(2014)
[28] Lin, M., Sun, D., Toh, K.-C.: An augmented lagrangian method with constraint
generation for shape-constrained convex regression problems. Mathematical
ProgrammingComputation 14(2), 223–270(2022)
25[29] Hiriart-Urruty, J.B., Lemar´echal, C.: Convex Analysis and Minimization Algo-
rithms I: Fundamentals, Berlin Heidelberg
[30] Lemar´echal,C.,Sagastiza´bal,C.:Practicalaspectsofthemoreau–yosidaregular-
ization: Theoretical preliminaries. SIAM Journal on Optimization 7(2), 367–385
(1997)
[31] Rockafellar, R.T.: Convex Analysis, Princeton
[32] Xiu, X., Kong, L., Li, Y., Qi, H.: Iterative reweighted methods for ℓ -ℓ
1 p
minimization. Computational Optimization and Applications 70(1), 201–219
(2018)
[33] Chen, C., He, B., Ye, Y., Yuan, X.: The direct extension of admm for multi-
block convexminimization problems is not necessarilyconvergent.Mathematical
Programming 155(1), 57–79 (2016)
[34] Fazel,M.,Pong,T.-K.,Sun,D.,Tseng,P.:Hankelmatrixrankminimizationwith
applications to system identification and realization. SIAM Journal on Matrix
Analysis and Applications 34(3), 946–977(2013)
26