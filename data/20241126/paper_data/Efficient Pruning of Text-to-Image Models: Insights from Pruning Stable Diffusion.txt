Efficient Pruning of Text-to-Image Models: Insights from Pruning
Stable Diffusion
SamarthNRamesh ZhixueZhao
UniversityofSheffield UniversityofSheffield
Sheffield,UnitedKingdom Sheffield,UnitedKingdom
snramesh1@sheffield.ac.uk zhixue.zhao@sheffield.ac.uk
Abstract Keywords
Astext-to-imagemodelsgrowincreasinglypowerfulandcomplex, ComputerVision,NaturalLanguageProcessing,ModelCompres-
theirburgeoningsizepresentsasignificantobstacletowidespread sion,Pruning,Text-to-ImageGeneration
adoption,especiallyonresource-constraineddevices.Thispaper
ACMReferenceFormat:
presents a pioneering study on post-training pruning of Stable
SamarthNRameshandZhixueZhao.2025.EfficientPruningofText-to-
Diffusion2,addressingthecriticalneedformodelcompression ImageModels:InsightsfromPruningStableDiffusion.InProceedingsof
in text-to-image domain. Unlike previous work focused on lan- ACMSACConference(SAC’25).ACM,NewYork,NY,USA,Article4,10pages.
guagemodelsortraditionalimagegeneration,ourstudytackles https://doi.org/xx.xxx/xxx_x
thecomplexmultimodalitygenerationmodels,andparticularly
examinesthepruningimpactonthetextualcomponentandthe 1 Introduction
imagegenerationcomponentseparately.Weconductacomprehen- Large language models (LLMs) and diffusion models often face
sivecomparisononpruningthemodelorthesinglecomponent the challenge of having exceptionally large model sizes [2, 19].
ofthemodelinvarioussparsities.Ourresultsyieldpreviouslyun- Whiletheseextensiveparametersallowthemtounderstandhigh-
documentedfindings.Forexample,contrarytoestablishedtrends qualitytextandgenerateimages,theyalsoresultinsubstantial
inlanguagemodelpruning,wediscoverthatsimplemagnitude computationaldemandsandresourceconsumption[2,25,39].With
pruningoutperformsmoreadvancedtechniquesintext-to-image billionsofparameters,thesemodelsbecomeprohibitivelyexpensive
context.Furthermore,ourresultsshowthatStableDiffusion2can tooperate,limitingtheiruseprimarilytolargecorporations.To
beprunedto38.5%sparsitywithminimalqualityloss,achievinga addressthisissue,significantresearchhasbeendevotedtomodel
significantreductioninmodelsize.Weproposeanoptimalpruning compressiontechniquesfordeeplearningmodels.Compressing
configurationthatprunesthetextencoderto47.5%andthediffusion thesemodelscanmakethemdeployableonedgedevices[11,27,28],
generatorto35%.Thisconfigurationmaintainsimagegeneration reduceresponsetimes[14,22],enablereal-timeapplications[26,
qualitywhilesubstantiallyreducingcomputationalrequirements. 27],andgreatlyincreasetheiraccessibility.
Inaddition,ourworkuncoversintriguingquestionsaboutinfor- Quantization[10]andpruning[12,13,21]aretwoprimarycom-
mationencodingintext-to-imagemodels:weobservethatpruning pressionapproaches.Quantizationfocusesonreducingthepre-
beyondcertainthresholdsleadstosuddenperformancedrops(un- cisionofamodel’sweights,whilepruningseekstoidentifyand
readableimages),suggestingthatspecificweightsencodecritical eliminateredundantweights.Bothmethodshavebeenwellstud-
semanticsinformation.Thisfindingopensnewavenuesforfuture ied for LLMs but much less has been done for vision language
researchinmodelcompression,interoperability,andbiasidentifi- models[24].Further,recentadvancementshaveintroducedpost-
cationintext-to-imagemodels.Byprovidingcrucialinsightsinto trainingpruningmethodsforLLMs,whichoffercompressedmodels
thepruningbehavioroftext-to-imagemodels,ourstudylaysthe withouttheneedforretraining[8,37].
groundworkfordevelopingmoreefficientandaccessibleAI-driven However,therehasbeenrelativelylittleresearchonpost-training
imagegenerationsystems.1
pruningforvisionlanguagemodels.Asfarasweknow,noprior
workhasaddressedpost-trainingpruningoftext-to-imagemod-
CCSConcepts
els.Thispaperseekstofillthisgapbyinvestigatingtheimpactof
• Computing methodologies → Computer vision; Natural post-trainingpruningonStableDiffusion2[33].Additionally,this
languageprocessing. studyexamineshowpruningimpactsthetextualencoderandthe
imagegeneratorofthemodelseparately,andidentifiestheoptimal
1Wepubliclyreleasedourcodeat:https://github.com/samarthramesh/SD2-Pruning sparsitylevelsforeachcomponent.
2 RelatedWork
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed 2.1 Text-to-ImageModels
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM The field of image generation was initially dominated by Gen-
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
erativeAdversarialNetworks(GANs)[1,9]andDiffusionmod-
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee.Requestpermissionsfrompermissions@acm.org. els[35].WhileGANsproducehigh-qualityimages,theirvariabil-
SAC’25,March31–April4,2025,Sicily,Italy ity is constrained by data used in the adversarial training pro-
©2025ACM.
cessandtheyrequiresubstantialefforttoscalefordiverse,com-
ACMISBN979-8-4007-0629-5/25/03
https://doi.org/xx.xxx/xxx_x plexapplications[18].Ontheotherhand,Diffusionmodels,built
4202
voN
22
]VC.sc[
1v31151.1142:viXraSAC’25,March31–April4,2025,Sicily,Italy SamarthNRameshandZhixueZhao
ondenoisingautoencoders,excelacrossvariousimagesynthesis are defined as exceptionally large output values from neurons,
tasks[17,36],buttheirhighcomputationaldemands,requiring whichcanbeupto100timesgreaterthantypicaloutputvalues.
extensiveGPUresourcesfortrainingandinference,presentsignifi- Suchactivationsserveasvaluableindicatorsoftheimportanceof
cantchallenges[5]. specificweights.
TheStableDiffusionmodelsreleasedbyStabilityAIhavemade Wanda extends the magnitude pruning by incorporating the
significantadvancesintext-to-imagegenerationandarerecognized valuesoftheactivationsthatserveasinputsfortheseweights.The
asstate-of-the-art[6,32].Thesemodelsbuildupondiffusionmodels scoringmetricforthismethodisexpressedinEquation2,where
byincorporatingavariationalautoencodertoencodeimagesina 𝑊 𝑖𝑗 representsthevalueoftheweights,𝑋 𝑗 denotestheactivations
latentspace,offeringseveraladvantagesovertrainingintheimage inputto𝑊 𝑖𝑗,and∥𝑋 𝑗∥ 2isthe𝑙 2normoftheseactivations.Thisap-
space,suchassignificantlyreducingthecomputationalresources proachallowsforamorenuancedassessmentofweightimportance,
neededforbothtrainingandinference[32].Thediffusionprocess contributingtoimprovedpruningeffectiveness.
isappliedinthislatentspace,withcross-attentionlayersusedto
conditionthemodelonrichrepresentationsoftheinput.Fortext- 𝑆 𝑖𝑗 =|𝑊 𝑖𝑗|.||𝑋 𝑗|| 2 (2)
to-imagetasks,thisrepresentationisgeneratedbyatransformer
model[33].Inthisstudy,wefocusonStableDiffusion2,which Outlier-WeightedLayerwisePruning(OWL). Asignificantobser-
usestheCLIPmodel’stextencoder[30]asthetransformeranda vationregardingoutliersinLLMsisthatthedensityofobserved
U-Net[34]asthediffusionmodel.StableDiffusion2,astate-of-the- outlierswithinaparticularlayercorrelateswiththatlayer’sim-
artmodelandafoundationalarchitectureforlaterversions[6,31], portancetothemodel’semergentproperties[20,29,38].Based
contains1.2billionparameters,with340millionintheCLIPtext onthisinsight,Outlier-WeightedLayerwisePruning(OWL)[40]
encoderand860millionintheU-Netdiffusiongenerator.Itslarge seekstodistributesparsityacrossdifferentlayersofthemodelinan
sizemakesitanidealcandidateformodelcompressiontechniques. unevenmanner.Therequiredlayer-wisesparsityisinfluencedby
thenumberofoutlierspresent;consequently,layersdeemedmore
2.2 Pruning importantareprunedlessaggressively,whilethoseconsideredless
Pruningworksontheprinciplethatsomeconnectionsinanetwork criticalareprunedmoreextensively.OWLcanbeintegratedwith
areoflessimportancethanothers[21].Themethodoffindingthese otherpruningalgorithms,suchasMagnitudeandWandaasitdoes
connectionsisofutmostimportanceforthepost-trainingpruning. notspecifywhichweightstoprune;rather,itprovidesrecommen-
Weintroducestate-of-the-artpruningmethodsexperimentedin dationsforlocalsparsitylevelswithinthelayers.
LLMsandtheirmaindifferenceishowtheyidentifytheweightsto While post-training pruning methods have been extensively
bepruned. studiedforlanguagemodels,theireffectivenessontext-to-image
modelsremainslargelyunexplored.Giventhedistinctarchitectures
MagnitudePruning. Magnitude-basedpruning[12]isoneofthe andtrainingprotocolsoftext-to-imagemodels,itischallenging
simplestandmostcommonlyusedtechniquesforpruningneural toinferhowthesepruningtechniques,originallydevelopedfor
networks.Itoperatesundertheassumptionthataconnection’s languagemodels,willperforminthisdifferentcontext.
importance correlates with the absolutevalue of its weight. As
describedinEquation1,theweightsarerankedaccordingtotheir 3 ExperimentalSetup
magnitude,andtheleastimportantweightsareprunedbysetting
ThispaperinvestigatesthepruningofStableDiffusion2,awidely
themtozero.Whilethismethodmaynotprovideoptimalperfor-
usedtext-to-imagegenerationmodel.StableDiffusion2has1.2
mancecomparedtomoreadvancedtechniques,itiseasytoimple-
billionparametersofwhich340millionparameters(28%)areinthe
ment,computationallyefficient,andservesasastrongbaselinefor
CLIPtextencoderand860millionparameters(72%)areintheU-Net
furtherrefinement.
diffusiongenerator.Ourinitialexperimentsfocusonexamining
the effects of independently pruning individual components of
LowImportanceConnections=argmin|𝑤 𝑖| (1)
themodelwhilemaintainingtheintegrityoftheotherparts.For
𝑖
example, only prune the text encoder component and keep the
SparseGPT. SparseGPT[8]tacklesthetaskbyattemptingtosolve diffusiongeneratoruntouched.Followingthis,fullmodelpruning
theproblemofidentifyingtheoptimalpruningmask.Thegoalisto isexploredacrossmultipleaxestodeterminetheoptimalbalance
findasparsesubsetofweightsthatminimizesreconstructionerror. ofsparsitiesbetweenthetextencoderandthediffusiongenerator.
Whiletheexactsolutiontothisproblemrequires𝑂(𝑑4)timecom-
Theevaluationofthebestconfigurationforfullmodelpruningis
plexity[8],where𝑑isthedimensionofthehiddenlayer,SparseGPT
informedbytheresultsobtainedfromtheindividualcomponent
achievesanapproximatesolutionin𝑂(𝑑3),offeringamoreefficient
pruningexperiments.
approach.Thismethodhasconsistentlyoutperformedmagnitude
pruningacrossLLMsofvarioussizes. 3.1 PruningSingleComponent
Wanda. WeightandActivationPruning(Wanda)[37]isamore Fortext-encoder-onlypruning,fourdifferenttechniquesaretested,
recentmethodthatsurpassesSparseGPTinperformanceandhas namelymagnitudepruning,Wanda,magnitudepruningwithOWL
becomewidelyadoptedinvarioustext-basedmodels.Wandais andWandawithOWL.Foreachofthesetechniques,multiplesparsi-
groundedintheobservationthatemergentpropertiesinLLMsare tiesaretestedinstepsof10%andmoregranulartestsareperformed
oftenassociatedwiththepresenceof“outliers”[4].Theseoutliers atintervalsofinterest.EfficientPruningofText-to-ImageModels:InsightsfromPruningStableDiffusion SAC’25,March31–April4,2025,Sicily,Italy
FullModelSparsity Text:ImageRatio TextSparsity ImageSparsity computervisiontasks.Forourexperiments,weuse10,000ran-
20% 75:25 53% 7%
domlysampledimagesandtheircorrespondingcaptions.
20% 50:50 35% 14%
20% 25:75 18% 21%
3.4 Evaluation
30% 75:25 80% 10%
30% 50:50 53% 21% FID. TheFréchetInceptionDistance(FID)[16]definesadistance
30% 25:75 27% 31%
metricbetweentwosetsofimagesandmeasureshowdifferentthe
40% 50:50 71% 28%
twosetsare.AlowerFIDindicatesthatthetwosetsofimagesare
40% 25:75 35% 42%
verysimilar.
50% 50:50 89% 35%
50% 25:75 44% 52% Inourexperiments,weusetheFIDtocompareimagesgenerated
60% 25:75 53% 63% fromMSCOCOcaptionswithcorrespondingrealimages.Foreach
Table1:Sparsityisdistributedbetweencomponentsinthe modelwegenerate10,000imagestocalculatetheFID.
ratiosshowninText:ImageRatioandTextandImageSparsi-
CLIPScore. TheCLIPScore[15]usesamulti-modaltextandim-
tiesrepresentthefractionofweightsprunedinthosecom-
agemodeltodirectlycompareageneratedimagewiththeprovided
ponents
prompt.ItusestheCLIPmodeltocalculatethesimilarityanda
lowerCLIPScoreindicatesahighercorrelationinthesemantic
contentofthepromptandimage.
Inourexperiments,foreachprunedmodel,wegenerate10,000
Todate,therehasbeenlimitedresearchonpost-trainingpruning
imagesandcalculatetheaveragesimilarityofeachimagewithits
specificallyfordiffusionmodels.Somestudieshaveidentifiedeffi-
promptusingtheCLIPScore.
cientpruningtechniquesthatinvolveretraining[7],andarecent
proposalintroducedanovelapproachforiterativelypruningLatent
4 ResultsandAnalysis
Diffusionmodels[3],suchasStableDiffusion.However,thescope
Theprimaryobjectiveofthisstudyistoanalyzetheimpactofprun-
ofthispaperislimitedtomagnitudepruningforthediffusioncom-
ingontext-to-imagemodels,specificallyfocusingontheindividual
ponentofthemodel,leavingtheexplorationofothertechniquesfor
componentsofStableDiffusion2,namelytheCLIPtextencoder
futureresearch.Aswiththetextencoder,multiplesparsitylevels
andtheU-Netdiffusiongenerator.Ouraimistoexploretheoptimal
areevaluatedinincrementsof10%,andmoredetailedassessments
trade-offbetweenmodelperformanceandcomputationalresource
areconductedatintervalsofparticularinterest.
efficiency.Weinitiallyapproachthisproblembyexaminingthe
3.2 FullModelPruning effectsofpruningwhentargetingonlyonecomponentatatime.
Theinsightsgainedfromthisanalysissubsequentlyinformour
Twodistinctapproachesareemployedfortheexperimentstofind
strategyforfullmodelpruning.
theoptimalconfigurationforfullmodelpruning.Thefirstapproach
AllexperimentationisquantitativelyevaluatedusingFID[16]
aimstodeterminethebestratioofsparsitiesbetweenthetexten-
andCLIPScore[15]metricson10,000imagesgeneratedfromcap-
coderandimagediffusiongeneratorifaspecificfullmodelsparsity
tionspresentintheMSCOCO2017dataset[23].Thesemetricsare
isdesired.Atvariouslevelsoffullmodelsparsity,thedistribution
reinforcedbyqualitativeevaluationofgeneratedimagequality.
ofthissparsitybetweenthetwocomponentsisoutlinedinTable
1.Itisimportanttonotethatcertainratiosassociatedwithhigher
4.1 CLIPPruningOnly
fullmodelsparsitiesareunfeasibleduetothesignificantlysmaller
WeexperimentwithtwopruningmethodsontheCLIPtextencoder
sizeofthetextencodercomparedtothediffusiongenerator.For
andextendthesebyapplyingOWLtotestwhetheroutlierweight-
example,whenaimingtoprunethefullmodelto50%,attempting
ingimprovesperformanceintext-to-imagemodels.Figures1,3,5,
todistributethesparsityina75:25ratiofortextandimagewould
7showtheFIDandCLIPscoresforeachmethod.Atlowersparsity
resultinpruningthetextencoderto177%,whichisclearlynot
levels,alltechniquesresultinminimalperformancedegradation.
achievable.
However,assparsityincreases,eachmethodexperiencesasharp
Thesecondapproachtofullmodelpruningreliesonthefindings
performancedeclineatspecificthresholds.Qualitativeevaluations
fromtheindividualcomponentpruningexperiments.Boththetext
ofthegeneratedimagescorroboratethesequantitativeresults.
encoderandthediffusiongeneratorexhibitidentifiabledrop-off
pointsinperformance,whichwillbefurtherexaminedintheresults
MagnitudePruning. Asthesimplestandmoststraightforward
section.Assumingthatthesub-modelsremainlargelyunaffected
method,magnitudepruningservesasanimportantbaseline.When
untilreachingthesedrop-offpoints,thefullmodelisprunedto
pruninguptoasparsityof60%,themodelperformssimilarlytothe
alignwiththedrop-offthresholdsofbothsub-models.Thisleadsto
original,withonlyaminimaldeclineinperformance.However,at
arecommendedconfigurationformaximalpruningthatbalances
62.5%sparsity,themodelexperiencesasuddendropinperformance.
performanceandsparsityeffectively.
This is further confirmed by visual inspection of the generated
images,whichshowacleardegradationinquality.At62.5%,the
3.3 Dataset
modelstrugglestocorrectlyinterprettheprompt,asevidencedby
WeusetheMicrosoftCOCO:CommonObjectsinContext(MSCOCO) amisframeddogandtheabsenceofafieldintheimages.Beyond
2017dataset[23]whichhasalargenumberofrealimagesandcor- thisthreshold,themodel’sperformancedeterioratesdrastically,
respondingcaptions.MSCOCOisacommonlyuseddatasetfor withthegeneratedimagesbecomingcompletenoise.SAC’25,March31–April4,2025,Sicily,Italy SamarthNRameshandZhixueZhao
Figure1:PruningonlyTextEncoderofStableDiffusion2 Figure3:PruningonlyTextEncoderofStableDiffusion2
usingMagnitudePruning-Sharpdropoffinperformance usingWandaPruning-Sharpdropoffinperformanceob-
observedat62.5% servedat60%
(a)0% (b)50% (a)0% (b)50%
(c)62.5% (d)65% (c)62.5% (d)65%
Figure2:TextEncoderMagnitudePruningExamples-Text Figure4:TextEncoderWandaPruningExamples-TextEn-
Encoderbreaksdownbeyond62.5% coderbreaksdownbeyond62.5%
WandaPruning. Despitebeingoneofthemosteffectivepruning FIDandCLIPScoremetrics.Thisdiscrepancyislikelyduetothe
techniquesformanylanguagemodels,Wandaappearstobepoorly specificbehaviorofthemodelonthisparticularprompt,whereas
suitedforpruningthetextcomponentinthistext-to-imagemodel. themetricsreflecttheaverageperformanceacross10,000distinct
Whencomparedtothebaselinemagnitudepruning,Wandaexhibits prompts.
similarbehaviorbutconsistentlyunderperforms.Likemagnitude
MagnitudePruningwithOWL. Applyingoutlierweightingto
pruning,Wandashowsminimalimpactonperformanceuptoa
magnitudepruningshowsminimalimpactoverall.Whilesome
certainthreshold,afterwhichthereisasharpdecline.However,
sparsitiesexhibitslightimprovements,thedifferencesaregenerally
forWanda,thisthresholdoccursat60%sparsity,makingitless
insignificant.Thisfindingalignswiththeobservationsmadeby
effectivethanmagnitudepruning.
theauthorsoftheOWLpaper,whonotedthatoutlierweighting
Aninterestingobservationarisesat80%sparsity,wheretheFID
tendstooffermeaningfulimprovementsprimarilyathighersparsity
scoreisunexpectedlylowerthanat70%,suggestingthatWanda
levels.
mightperformbetterathighersparsities.Thisanomalywarrants
furtherinvestigation.Additionally,whenvisuallyinspectingthe WandaPruningwithOWL. Aswithmagnitudepruning,apply-
imagesinFigure4,thedeteriorationthresholdappearstobearound ingoutlier-basedweightingtoWandapruningdoesnotresultin
62.5%,slightlydifferentfromthe60%thresholdindicatedbythe significantimprovements.EfficientPruningofText-to-ImageModels:InsightsfromPruningStableDiffusion SAC’25,March31–April4,2025,Sicily,Italy
Figure5:PruningonlyTextEncoderofStableDiffusion2
usingMagnitudePruningwithOWL-Sharpdropoffinper-
formanceobservedat62.5% (a)0% (b)50%
(c)62.5% (d)65%
(a)0% (b)50%
Figure8:TextEncoderWandaPruningwithOWLExamples
-TextEncoderbreaksdownbeyond60%
ComparisonofPruningTechniques. ThefindingthatWandaprun-
ingisoutperformedbymagnitudepruningisquitesurprisingand
underscorestheneedforpruningtechniquestailoredspecifically
totext-to-imagemodels.AsshowninFigure9,magnitudepruning
(c)62.5% (d)65% consistentlyoutperformsWandaateverysparsitylevel.Thisis
particularlynotablegiventhat,inmostNLPmodels,magnitude
Figure6:TextEncoderMagnitudePruningwithOWLExam- pruningistypicallytheweakestamongpost-trainingpruningmeth-
ples-TextEncoderbreaksdownbeyond62.5% ods.Thissuggestsasignificantopportunitytodevelopspecialized
pruningalgorithmsfortext-to-imagemodels.
AnotherkeyobservationisthatOutlierWeightedLayerwise
(OWL)pruning,whenappliedontopofbothmagnitudeandWanda
pruning,resultsinonlymarginalimprovements.Figure9alsohigh-
lightsthesharpperformancedrop-off,apatternobservedacross
allpruningalgorithmstested.
4.2 U-NetPruningOnly
For the diffusion component of the model, we observe a more
Figure7:PruningonlyTextEncoderofStableDiffusion2 gradualdeclineinperformanceaspruningincreased.Themodel
usingWandaPruningwithOWL-Sharpdropoffinperfor- maintainsstrongperformanceatlowersparsitylevels,butitsper-
manceobservedat60% formancesteadilydecreasesassparsityexceeded40%.Basedona
qualitativeanalysis,50%sparsityisidentifiedasthepointwherethe
model’sperformancesignificantlydeteriorated,andthisthreshold
islaterusedinfullmodelpruning.Thisgradualdeclineisevident
whenexaminingtheprunedimagesfrom30%to60%,asshownin
Figure14intheappendix.SAC’25,March31–April4,2025,Sicily,Italy SamarthNRameshandZhixueZhao
TotalSparsity TextSparsity ImageSparsity FID CLIPScore
16.9% 60% 0% 21.7 0.282
28.7% 0% 40% 25.38 0.311
Table2:Comparisonbetweenbestmodelobtainedpruning
eachcomponentindividually
4.3 ComparisonbetweenTextandImage
Pruning
Toaddressthequestionofwhichcomponent—thetextencoderor
theimagediffusiongenerator—yieldsbetterresultswhenpruned,
Figure9:ComparingPruningTechniquesforpruningonly
wecomparethebest-performingprunedmodelsforeachapproach.
textencoder
ThiscomparisonispresentedinTable2.
Comparingthetwomodels,whichqualitativelyproducesimilar
results,revealssomeintriguingfindings.First,pruningtheimage
diffusiongeneratorachievesgreateroverallsparsityduetoitslarger
size,contributingmoresignificantlytothefullmodel’ssparsity.
Additionally,pruningthetextencoderresultsinaworseCLIPscore,
whilepruningtheimagediffusiongeneratorleadstoaworseFID
score.Thisraisesinterestingquestionsaboutwhatthesemetrics
areactuallymeasuring.Inparticular,thelowerCLIPscorefortext
encoderpruningmaybeduetoapotentialbias,asthetextencoder
Figure10:PruningonlyImageDiffusionGeneratorofStable itselfisaCLIPmodel,whichcouldinfluencehowitsperformance
Diffusion2usingMagnitudePruning-Gradualdeclinein isevaluated.
performance
4.4 FullModelPruning
FirstApproach. Giventhecomputationaldemandsofevaluating
models,conductinganexhaustivegridsearchtoidentifytheoptimal
pruningconfigurationisimpractical.Inthefirstapproachwetryto
establishguidingprinciplesforhowtobestdistributesparsitygiven
acertaintargetsparsityforthefullmodel.Theexperimentsshown
inTable1wereconductedandtheresultsareshowninTables3,4,
5.Asoutlinedinthemethodologysection,certainconfigurations
aredeemedinvalidduetothesignificantsizedisparitybetweenthe
textandimagecomponents.
Allexperimentsusedmagnitudepruningforbothcomponents
ofthemodel.Similarexperimentswereconductedusingmagnitude
(a)0% (b)40%
pruningwithOWLfortheCLIPtextencoderandproducedvery
similarresultsasseeninTables7,8,9intheappendix.
Tables3,4,5demonstratethatthemodelperformsbetterwhen
themajorityofthesparsityisallocatedtotheimagecomponent.
Notably,theconfigurationinwhich75%ofthemodel’ssparsityis
concentratedintheimagediffusiongeneratorconsistentlyoutper-
formsothersplits.Thisfindingalignswiththeearlierobservation
thatpruningtheimagecomponentyieldsgreatersparsitywithless
declineinperformancecomparedtopruningthetextcomponent.
Themodelswhereperformancedeterioratessignificantlyare
highlightedinTable5andcorrespondtosparsitiesthatexceedthe
(c)50% (d)60% drop-offthresholdsidentifiedintheindividualcomponentprun-
ingexperiments.Thisreinforcestheconsistencyofthesepruning
Figure11:DiffusionGeneratorMagnitudePruningExamples thresholdsandinformsoursecondapproachfordeterminingthe
-Performancegreatlysuffersbeyond40% optimalfullmodelpruningconfiguration.
SecondApproach. Weexaminethemodelprunedtoboththresh-
oldsestablishedintheindividualpruningexperiments.Giventhe
possibilitythatthesethresholdsmightbehavedifferentlywhenEfficientPruningofText-to-ImageModels:InsightsfromPruningStableDiffusion SAC’25,March31–April4,2025,Sicily,Italy
TotalSparsity 75:25 50:50 25:75
20% 20.92 20.89 17.85
30% 244.92 20.8 18.4
40% N/A 219.97 20.71
50% N/A 202.84 66.2
60% N/A N/A 370.2
Table3:FIDatdifferentallocationsofsparsitybetweentext
encoderandimagediffusiongenerator
TotalSparsity 75:25 50:50 25:75 Figure12:ModelImprovementassparsityisreducedfrom
20% 0.308 0.314 0.318 identifieddrop-offthresholds
30% 0.169 0.307 0.316
40% N/A 0.176 0.307
TotalSparsity TextSparsity ImageSparsity FID CLIPScore
50% N/A 0.181 0.27
0% 0% 0% 18.07 0.314
60% N/A N/A 0.202
53.5% 62.5% 50.0% 264.87 0.189
Table4:CLIPScoreatdifferentallocationsofsparsitybe- 51.0% 60.0% 47.5% 140.39 0.224
tweentextencoderandimagediffusiongenerator 48.5% 57.5% 45.0% 57.59 0.272
46.0% 55.0% 42.5% 30.69 0.296
43.5% 52.5% 40.0% 23.28 0.304
41.0% 50.0% 37.5% 20.24 0.309
TotalSparsity Text:ImageRatio TextSparsity ImageSparsity FID CLIPScore
38.5% 47.5% 35.0% 18.15 0.311
20% 75:25 53% 7% 20.92 0.308
20% 50:50 35% 14% 20.89 0.314 36.0% 45.0% 32.5% 16.66 0.314
20% 25:75 18% 21% 17.85 0.318 33.5% 42.5% 30.0% 16.53 0.315
30% 75:25 80% 10% 244.92 0.169 Table6:PruningStableDiffusion2tofoundthresholds
30% 50:50 53% 21% 20.8 0.307
30% 25:75 27% 31% 18.4 0.316
40% 50:50 71% 28% 219.97 0.176
40% 25:75 35% 42% 20.71 0.307
50% 50:50 89% 35% 202.84 0.181
50% 25:75 44% 52% 66.2 0.27 generatorto35%sparsity.Thisconfigurationresultsinminimal
60% 25:75 53% 63% 370.2 0.202
lossofimagequalitywhileprovidingasubstantialreductionin
Table5:ModelPerformancegreatlysufferswhencomponent
modelsize.Acomparisonofthegeneratedimagequalityforthe
sparsitiesviolatetheidentifieddrop-offthresholds
baselineandoptimalconfigurationsisillustratedinFigure13.
5 Conclusions
bothcomponentsareprunedsimultaneously,wealsoprunesev- Thisresearchpresentsthefirstcomprehensivestudyonpost-training
eralmodelstovaluesslightlybelowthethresholdstodetermine pruningoftext-to-imagemodels,specificallyfocusingonStable
theoptimalsparsityconfigurations.Startingfromthetwothresh- Diffusion2.Ourfindingsrevealunexpectedbehaviorsandprovide
olds,wedecreaseboththetextandimagecomponentsparsitiesin crucialinsightsintothecompressionofthesecomplexmodels.We
incrementsof2.5%,evaluatingeachoftheseconfigurations.The demonstratethatStableDiffusion2canbeeffectivelyprunedto
resultsaresummarizedinTable6,wheretheunprunedmodelis 38.5%sparsitywithminimalqualityloss,achievingasignificant
providedasabaselineandhighlightedingray.Themodelswere reductioninmodelsize.Theoptimalpruningconfiguration,with
prunedusingmagnitudepruningforbothcomponentsofthemodel. 47.5%sparsityinthetextencoderand35%inthediffusiongenerator,
Theseconfigurationswerealsotestedusingmagnitudepruning maintainsimagegenerationqualitywhilesubstantiallyreducing
withOWLforCLIPtextencoderandproducedverysimilarresults computationalrequirements.Thisachievementpavesthewayfor
asshowninTable10intheappendix the deployment of advanced text-to-image models on resource-
The results presented in Table 6 indicate that pruning both constraineddevices,potentiallybroadeningaccessibilitytothis
thresholdssimultaneouslyleadstoamodelperformancethatis technology.Contrarytoestablishedtrendsinlanguagemodelprun-
significantlyworsethananticipated.Whileeachthresholdindivid- ing,wefoundthatsimplemagnitudepruningoutperformsmore
uallyresultedinminimalperformanceloss,thecombinedpruning advancedtechniquesinthiscontext.Additionally,weuncovered
ofbothcomponentsappearstodegradethemodel’sperformance theexistenceofsharpperformancethresholds,suggestingadis-
substantially.However,aswegraduallyreducebothsparsities,we tinctiveinformationencodingmechanismintext-to-imagemodels.
observeimprovements,which,alongwithqualitativeevaluationsof Theseunexpectedresultshighlighttheuniquechallengesandop-
thegeneratedimages,enableustorecommendanoptimalpruning portunitiesinpruningtext-to-imagemodels,emphasizingtheneed
configurationforthemodel. forspecializedapproachesinthisdomain.
Specifically,StableDiffusion2canbeprunedto38.5%sparsity Ourworkopensupnewavenuesforfutureresearch,including
byadjustingthetextencoderto47.5%sparsityandthediffusion thedevelopmentofpruningtechniquestailoredtotext-to-imageSAC’25,March31–April4,2025,Sicily,Italy SamarthNRameshandZhixueZhao
models,furtherinvestigationintotheinformationencodingmech-
Prompt:Adoginafield anismsofthesemodels,andexplorationofpotentialapplications
inbiasidentificationandmodelinterpretability.
Inconclusion,thisstudynotonlyprovidespracticalinsightsfor
theefficientdeploymentoftext-to-imagemodelsbutalsolaysthe
groundworkforanewsubfieldattheintersectionofmodelcom-
pressionandtext-to-imagegeneration.Asthesemodelscontinue
togrowincomplexityandcapability,theinsightsandmethodspre-
sentedherewillbecrucialinensuringtheirwidespreadaccessibility
andefficientimplementation.
6 FutureWork
Ourresultsandanalysessuggestseveralfutureresearchdirections.
Prompt:Atreehouseatnight OurfindingsindicatethatbothWandaandSparseGPTdonoteffec-
tivelyextendtothesemodels,whilemagnitudepruning—typically
regardedasthebaselinemethodforpruning—emergesasthebest-
performingapproachinourstudy.Thishighlightsasignificant
opportunityforfurtherimprovementandoptimizationinthefield.
Additionally,ourworkcouldbeappliedtomorerecentmodels,
suchasStableDiffusion3orlargertext-to-imagemodels,toassess
thegeneralizabilityofourfindings.
A OtherResults
TotalSparsity 75:25 50:50 25:75
Prompt:Anoceansunset 20% 20.92 20.9 18.18
30% 290.97 20.81 18.51
40% 0 250.37 20.48
50% 0 226.44 67.93
60% 0 0 370.47
Table7:FIDforFullModelPruningwithOWL
TotalSparsity 75:25 50:50 25:75
20% 0.307 0.314 0.317
Prompt:TheEiffelTower 30% 0.172 0.306 0.316
40% 0 0.183 0.307
50% 0 0.182 0.269
60% 0 0 0.202
Table8:CLIPScoreforFullModelPruningwithOWL
References
[1] AndrewBrock.2018.LargeScaleGANTrainingforHighFidelityNaturalImage
Synthesis.arXivpreprintarXiv:1809.11096(2018).
[2] TomBBrown.2020. Languagemodelsarefew-shotlearners. arXivpreprint
Figure13:BaseModelvsOptimallyPrunedModelat38.5% arXiv:2005.14165(2020).
[3] ThibaultCastells,Hyoung-KyuSong,Bo-KyeongKim,andShinkookChoi.2024.
Sparsity
LD-Pruner:EfficientPruningofLatentDiffusionModelsusingTask-Agnostic
Insights. http://arxiv.org/abs/2404.11936arXiv:2404.11936[cs].
[4] TimDettmers,MikeLewis,YounesBelkada,andLukeZettlemoyer.2022.Gpt3.
int8():8-bitmatrixmultiplicationfortransformersatscale.AdvancesinNeural
InformationProcessingSystems35(2022),30318–30332.
[5] PrafullaDhariwalandAlexanderNichol.2021.Diffusionmodelsbeatganson
imagesynthesis. Advancesinneuralinformationprocessingsystems34(2021),
8780–8794.EfficientPruningofText-to-ImageModels:InsightsfromPruningStableDiffusion SAC’25,March31–April4,2025,Sicily,Italy
TotalSparsity Text:ImageRatio TextSparsity ImageSparsity FID CLIPScore
FlowTransformersforHigh-ResolutionImageSynthesis.
20% 75:25 53% 7% 20.92 0.307
[7] GongfanFang,XinyinMa,andXinchaoWang.2023. StructuralPruningfor
20% 50:50 35% 14% 20.9 0.314
DiffusionModels.InNeurIPS.
20% 25:75 18% 21% 18.18 0.317
[8] EliasFrantarandDanAlistarh.2023.SparseGPT:MassiveLanguageModelsCan
30% 75:25 80% 10% 290.97 0.172
beAccuratelyPrunedinOne-Shot.InICML(ProceedingsofMachineLearning
30% 50:50 53% 21% 20.81 0.306
30% 25:75 27% 31% 18.51 0.316 Research,Vol.202).PMLR,10323–10337.
40% 50:50 71% 28% 250.37 0.183 [9] IanJ.Goodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-
40% 25:75 35% 42% 20.48 0.307 Farley,SherjilOzair,AaronC.Courville,andYoshuaBengio.2020.Generative
50% 50:50 89% 35% 226.44 0.182 adversarialnetworks.Commun.ACM63,11(2020),139–144.
50% 25:75 44% 52% 67.93 0.269 [10] R.M.GrayandD.L.Neuhoff.1998. Quantization. IEEETransactionsonInfor-
60% 25:75 53% 63% 370.47 0.202 mationTheory44,6(Oct.1998),2325–2383. https://doi.org/10.1109/18.720541
ConferenceName:IEEETransactionsonInformationTheory.
Table9:ModelPerformancegreatlysufferswhencomponent
[11] RamyadHadidi,JiashenCao,YilunXie,BaharAsgari,TusharKrishna,andHye-
sparsitiesviolatetheidentifieddrop-offthresholds soonKim.2019. CharacterizingtheDeploymentofDeepNeuralNetworks
onCommercialEdgeDevices.In2019IEEEInternationalSymposiumonWork-
loadCharacterization(IISWC).35–48. https://doi.org/10.1109/IISWC47752.2019.
9041955
[12] SongHan,JeffPool,JohnTran,andWilliamJ.Dally.2015.LearningbothWeights
andConnectionsforEfficientNeuralNetwork.InNIPS.1135–1143.
[13] BabakHassibi,DavidGStork,andGregoryJWolff.1993.Optimalbrainsurgeon
andgeneralnetworkpruning.InIEEEinternationalconferenceonneuralnetworks.
IEEE,293–299.
[14] BenjaminHawks,JavierDuarte,NicholasJFraser,AlessandroPappalardo,Nhan
Tran,andYamanUmuroglu.2021.Psandqs:Quantization-awarepruningfor
efficientlowlatencyneuralnetworkinference.FrontiersinArtificialIntelligence
4(2021),676564.
[15] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,andYejinChoi.2021.
CLIPScore:AReference-freeEvaluationMetricforImageCaptioning.InEMNLP
(1).AssociationforComputationalLinguistics,7514–7528.
[16] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,and
SeppHochreiter.2017.GANsTrainedbyaTwoTime-ScaleUpdateRuleConverge
toaLocalNashEquilibrium.InNIPS.6626–6637.
[17] JonathanHo,AjayJain,andPieterAbbeel.2020.DenoisingDiffusionProbabilistic
(a)30% (b)40%
Models.InNeurIPS.
[18] TeroKarras,SamuliLaine,andTimoAila.2019. Astyle-basedgeneratorar-
chitectureforgenerativeadversarialnetworks.InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition.4401–4410.
[19] Hyung-KwonKo,GwanmoPark,HyeonJeon,JaeminJo,JuhoKim,andJinwook
Seo.2023. Large-scaletext-to-imagegenerationmodelsforvisualartists’cre-
ativeworks.InProceedingsofthe28thinternationalconferenceonintelligentuser
interfaces.919–933.
[20] OlgaKovaleva,SaurabhKulshreshtha,AnnaRogers,andAnnaRumshisky.2021.
BERTbusters:Outlierdimensionsthatdisrupttransformers. arXivpreprint
arXiv:2105.06990(2021).
[21] YannLeCun,JohnDenker,andSaraSolla.1989. OptimalBrainDamage.In
AdvancesinNeuralInformationProcessingSystems,D.Touretzky(Ed.),Vol.2.
Morgan-Kaufmann. https://proceedings.neurips.cc/paper_files/paper/1989/file/
6c9882bbac1c7093bd25041881277658-Paper.pdf
[22] TailinLiang,JohnGlossner,LeiWang,ShaoboShi,andXiaotongZhang.2021.
(c)50% (d)60% Pruningandquantizationfordeepneuralnetworkacceleration:Asurvey.Neu-
rocomputing461(2021),370–403.
[23] Tsung-YiLin,MichaelMaire,SergeJ.Belongie,JamesHays,PietroPerona,Deva
Figure14:MagnitudePruningDiffusionGeneratorcauses
Ramanan,PiotrDollár,andC.LawrenceZitnick.2014.MicrosoftCOCO:Common
lineardeteriorationofquality ObjectsinContext.InECCV(5)(LectureNotesinComputerScience,Vol.8693).
Springer,740–755.
[24] AlexandreLopes,FernandoPereiradosSantos,DiulhiodeOliveira,Mauricio
TotalSparsity TextSparsity ImageSparsity FID CLIPScore Schiezaro,andHelioPedrini.2024.ComputerVisionModelCompressionTech-
0% 0% 0% 18.07 0.314 niquesforEmbeddedSystems:ASurvey.Computers&Graphics123(2024),104015.
https://doi.org/10.1016/j.cag.2024.104015
53.5% 62.5% 50.0% 241.71 0.193
[25] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,
51.0% 60.0% 47.5% 122.76 0.236 MostofaPatwary,VijayKorthikanti,DmitriVainbrand,PrethviKashinkunti,
48.5% 57.5% 45.0% 59.12 0.273 JulieBernauer,BryanCatanzaro,AmarPhanishayee,andMateiZaharia.2021.
46.0% 55.0% 42.5% 33.09 0.294 Efficientlarge-scalelanguagemodeltrainingonGPUclustersusingmegatron-
43.5% 52.5% 40.0% 24.96 0.304 LM.InProceedingsoftheInternationalConferenceforHighPerformanceCom-
puting,Networking,StorageandAnalysis(St.Louis,Missouri)(SC’21).Asso-
41.0% 50.0% 37.5% 20.75 0.309
ciationforComputingMachinery,NewYork,NY,USA,Article58. https:
38.5% 47.5% 35.0% 17.97 0.311 //doi.org/10.1145/3458817.3476209
36.0% 45.0% 32.5% 16.79 0.314 [26] WeiNiu,XiaolongMa,ShengLin,ShihaoWang,XuehaiQian,XueLin,Yanzhi
33.5% 42.5% 30.0% 16.45 0.315 Wang,andBinRen.2020.Patdnn:Achievingreal-timednnexecutiononmobile
deviceswithpattern-basedweightpruning.InProceedingsoftheTwenty-Fifth
Table10:PruningStableDiffusion2tofoundthresholds
InternationalConferenceonArchitecturalSupportforProgrammingLanguagesand
OperatingSystems.907–922.
[27] Pierre-EmmanuelNovac,GhouthiBoukliHacene,AlainPegatoquet,BenoîtMira-
mond,andVincentGripon.2021.QuantizationandDeploymentofDeepNeural
NetworksonMicrocontrollers. Sensors21,9(2021). https://doi.org/10.3390/
[6] PatrickEsser,SumithKulal,AndreasBlattmann,RahimEntezari,JonasMüller, s21092984
HarrySaini,YamLevi,DominikLorenz,AxelSauer,FredericBoesel,Dustin
Podell,TimDockhorn,ZionEnglish,andRobinRombach.2024.ScalingRectifiedSAC’25,March31–April4,2025,Sicily,Italy SamarthNRameshandZhixueZhao
[28] PavanaPrakash,JiahaoDing,RuiChen,XiaoqiQin,MingleiShu,QimeiCui, andcomputer-assistedintervention–MICCAI2015:18thinternationalconference,
YuanxiongGuo,andMiaoPan.2022.IoTdevicefriendlyandcommunication- Munich,Germany,October5-9,2015,proceedings,partIII18.Springer,234–241.
efficientfederatedlearningviajointmodelpruningandquantization. IEEE [35] JaschaSohl-Dickstein,EricA.Weiss,NiruMaheswaranathan,andSuryaGanguli.
InternetofThingsJournal9,15(2022),13638–13650. 2015.DeepUnsupervisedLearningusingNonequilibriumThermodynamics.In
[29] GiovanniPuccetti,AnnaRogers,AleksandrDrozd,andFeliceDell’Orletta.2022. ICML(JMLRWorkshopandConferenceProceedings,Vol.37).JMLR.org,2256–2265.
Outliersdimensionsthatdisrupttransformersaredrivenbyfrequency.arXiv [36] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,Stefano
preprintarXiv:2205.11380(2022). Ermon,andBenPoole.2020.Score-basedgenerativemodelingthroughstochastic
[30] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh, differentialequations.arXivpreprintarXiv:2011.13456(2020).
SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark, [37] MingjieSun,ZhuangLiu,AnnaBair,andJ.ZicoKolter.2024.ASimpleandEffec-
GretchenKrueger,andIlyaSutskever.2021.LearningTransferableVisualModels tivePruningApproachforLargeLanguageModels.InICLR.OpenReview.net.
FromNaturalLanguageSupervision.InICML(ProceedingsofMachineLearning [38] WilliamTimkeyandMartenVanSchijndel.2021.Allbarkandnobite:Rogue
Research,Vol.139).PMLR,8748–8763. dimensionsintransformerlanguagemodelsobscurerepresentationalquality.
[31] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. arXivpreprintarXiv:2109.04404(2021).
2022. Hierarchicaltext-conditionalimagegenerationwithcliplatents. arXiv [39] LingYang,ZhilongZhang,YangSong,ShendaHong,RunshengXu,YueZhao,
preprintarXiv:2204.061251,2(2022),3. WentaoZhang,BinCui,andMing-HsuanYang.2023. DiffusionModels:A
[32] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörn ComprehensiveSurveyofMethodsandApplications.ACMComput.Surv.56,4,
Ommer.2021.High-ResolutionImageSynthesiswithLatentDiffusionModels. Article105(Nov.2023),39pages. https://doi.org/10.1145/3626235
arXiv:2112.10752[cs.CV] [40] LuYin,YouWu,ZhenyuZhang,Cheng-YuHsieh,YaqingWang,YilingJia,
[33] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörn GenLi,AjayKumarJaiswal,MykolaPechenizkiy,YiLiang,MichaelBendersky,
Ommer.2022.High-ResolutionImageSynthesiswithLatentDiffusionModels. ZhangyangWang,andShiweiLiu.2024.OutlierWeighedLayerwiseSparsity
InCVPR.IEEE,10674–10685. (OWL):AMissingSecretSauceforPruningLLMstoHighSparsity.InICML.
[34] OlafRonneberger,PhilippFischer,andThomasBrox.2015. U-net:Convolu- OpenReview.net.
tionalnetworksforbiomedicalimagesegmentation.InMedicalimagecomputing