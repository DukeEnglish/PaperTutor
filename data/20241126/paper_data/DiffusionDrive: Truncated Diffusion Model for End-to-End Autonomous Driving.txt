DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving
BenchengLiao1,2,⋄ ShaoyuChen2,3 HaoranYin3 BoJiang2,⋄ ChengWang1,2,⋄ SixuYan2
XinbangZhang3 XiangyuLi3 YingZhang3 QianZhang3 XinggangWang2(cid:0)
1 InstituteofArtificialIntelligence,HuazhongUniversityofScience&Technology
2 SchoolofEIC,HuazhongUniversityofScience&Technology
3 HorizonRobotics
Code&Model&Demo: hustvl/DiffusionDrive
Abstract
(a) Encode Interacting Reg Single-mode
Trajectory
Recently, the diffusion model has emerged as a powerful
Scene Representation Ego Query MLP
generativetechniqueforroboticpolicylearning,capableof
Encode Scoring Multi-mode
(b)
modeling multi-mode action distributions. Leveraging its Trajectories
capability for end-to-end autonomous driving is a promis- Scene Representation+Vocabulary Score of Each Anchor
ing direction. However, the numerous denoising steps in
Encode Denoise Multi-mode
(c)
the robotic diffusion policy and the more dynamic, open- Trajectories
world nature of traffic scenes pose substantial challenges Scene Rep.+GaussianDistribution Diffusion Policy
forgeneratingdiversedrivingactionsatareal-timespeed. Encode Denoise pp == 01 Confidence Score Multi-mode
Toaddressthesechallenges, weproposeanoveltruncated (d) Scoring Trajectories
diffusionpolicythatincorporatespriormulti-modeanchors Scene Rep.+AnchoredGaussian Dist. Truncated Diffusion Policy
Figure1. Thecomparisonofdifferentend-to-endparadigms.
and truncates the diffusion schedule, enabling the model
(a)Singlemoderegression[6,13,17].(b)Samplingfromvocabu-
to learn denoising from anchored Gaussian distribution to
lary[3,22]. (c)Vanilladiffusionpolicy[5,16]. (d)Theproposed
themulti-modedrivingactiondistribution.Additionally,we
truncateddiffusionpolicy.
designanefficientcascadediffusiondecoderforenhanced
proachoffersascalableandrobustalternativetotraditional
interaction with conditional scene context. The proposed
rule-basedmotionplanning,whichoftenstrugglestogener-
model, DiffusionDrive, demonstrates 10× reduction in de-
alizetocomplexreal-worlddrivingsettings.
noisingstepscomparedtovanilladiffusionpolicy,deliver-
To effectively learn from data, mainstream end-to-end
ing superior diversity and quality in just 2 steps. On the
planners(e.g.,Transfuser[6],UniAD[13],VAD[17])typ-
planning-oriented NAVSIM dataset, with aligned ResNet-
ically regress a single-mode trajectory from an ego-query
34 backbone, DiffusionDrive achieves 88.1 PDMS without
as shown in Fig. 1a. However, this paradigm does not ac-
bellsandwhistles,settinganewrecord,whilerunningata
count for the inherent uncertainty and multi-mode nature
real-timespeedof45FPSonanNVIDIA4090. Qualitative
of driving behaviors. Recently, VADv2 [17] introduces
results on challenging scenarios further confirm that Dif-
a large fixed vocabulary of anchor trajectories (4096 an-
fusionDrivecanrobustlygeneratediverseplausibledriving
chors) to discretize the continuous action space and cap-
actions.
ture a broader range of driving behaviors, and then sam-
plesfromtheseanchorsbasedonpredictedscoresasshown
1.Introduction in Fig. 1b. However, this large fixed-vocabulary paradigm
isfundamentallyconstrainedbythenumberandqualityof
End-to-end autonomous driving has gained significant at- anchor trajectories, often failing in out-of-vocabulary sce-
tention in recent years due to advancements in perception narios. Furthermore, managing a large number of anchors
models (detection [14, 21, 35], tracking [45–47], online presents significant computational challenges for real-time
mapping [24, 25, 27], etc.), which directly learns the driv- applications. Rather than discretizing the action space,
ingpolicyfromtherawsensorinputs. Thisdata-drivenap- diffusion model [5] has proven to be a powerful genera-
⋄ InternofHorizonRobotics; (cid:0) Correspondingauthor: Xinggang Todistinguishtheterm“multimodal”usedtodescribeinputdata,we
Wang(xgwang@hust.edu.cn). use“multi-mode”inthispapertorefertodiverseplanningdecisions.
4202
voN
22
]VC.sc[
1v93151.1142:viXrative decision-making policy in the robotics domain, which tion and closed-loop evaluations. Without bells and whis-
candirectlysamplemulti-modephysicallyplausibleactions tles, DiffusionDrive achieves 88.1 PDMS on NAVSIM
fromaGaussiandistributionviaaniterativedenoisingpro- navtestsplitwiththealignedResNet-34backbone,sig-
cess. nificantlyoutperformingpreviousstate-of-the-artmethods.
Thisinspiresustoreplicatethesuccessofthediffusion Even compared to the NAVSIM challenge-winning solu-
model in the robotics domain to end-to-end autonomous tionHydra-MDP-V 8192-W-EP[22], whichfollowsVADv2
driving. We apply the vanilla robotic diffusion policy with8192anchortrajectoriesandfurtherincorporatespost-
to the well-known single-mode-regression method, Trans- processing and additional supervision, DiffusionDrive still
fuser [6], by proposing a variant, Transfuser , which re- outperforms it by 1.6 PDMS through directly learning
DP
placesthedeterministicMLPregressionheadwithacondi- from human demonstrations and inferring without post-
tionaldiffusionmodel[28]. ThoughTransfuser improves processing, while running at real-time speed of 45 FPS on
DP
planning performance, two major issues arise: 1) The nu- an NVIDIA 4090. We further validate the superiority of
merous 20 denoising steps in the vanilla DDIM diffusion DiffusionDriveonpopularnuScenesdataset[2]withopen-
policy introduce heavy computational consumption during loopevaluations,DiffusionDriveruns1.8×fasterthanVAD
inference as shown in Tab. 2, hindering the real-time ap- andoutperformsit[17]by20.8%lowerL2errorand63.6%
plicationforautonomousdriving. 2)Thetrajectoriessam- lower collision rate with the same ResNet-50 backbone,
pled from different Gaussian noises severely overlap with demonstratingstate-of-the-artplanningperformance.
each other, as illustrated in Fig. 2. This underscores the Ourcontributionscanbesummarizedasfollows:
non-trivialchallengeoftamingthediffusionmodelsforthe • We firstly introduce the diffusion model to the field of
dynamicandopen-worldtrafficscenes. end-to-endautonomousdrivingandproposeanoveltrun-
Unlike the vanilla diffusion policy, which samples ac- cateddiffusionpolicytoaddresstheissuesofmodecol-
tions from a random Gaussian noise conditioned on scene lapse and heavy computational overhead found in direct
context, human drivers adhere to established driving pat- adaptationofvanilladiffusionpolicytothetrafficscene.
terns that they dynamically adjust in response to real-time • We design an efficient transformer-based diffusion de-
trafficconditions. Thisinsightmotivatesustoembedthese coder that interacts with the conditional information in
priordrivingpatternsintothediffusionpolicybypartition- acascadedmannerforbettertrajectoryreconstruction.
ing the Gaussian distribution into multiple sub-Gaussian • Without bells and whistles, DiffusionDrive significantly
distributions centered around prior anchors, referred to as outperformspreviousstate-of-the-artmethods,achieving
anchoredGaussiandistribution. Itisimplementedbytrun- arecord-breaking88.1PDMSontheNAVSIMnavtest
catingthediffusionscheduletointroduceasmallportionof splitwiththesamebackbone,whilemaintainingreal-time
GaussiannoisearoundtheprioranchorsasshowninFig.3. performanceat45FPSonanNVIDIA4090.
Thankstothemulti-modedistributionalexpressivityofthe • We qualitatively demonstrate that DiffusionDrive can
diffusionmodel,theproposedtruncateddiffusionpolicyef- generatemorediverseandplausibletrajectories,exhibit-
fectivelycoversthepotentialactionspacewithoutrequiring ing high-quality multi-mode driving actions in various
a large set of fixed anchors, as VADv2 does. With more challengingscenarios.
reasonable initial noise samples from the anchored Gaus-
sian distribution, we can truncate the denoising process, 2.RelatedWork
reducing the required steps from 20 to just 2—a substan-
tialspeedupthatsatisfiesthereal-timerequirementsofau-
End-to-end autonomous driving. UniAD [13], as a pio-
tonomousdriving.
neeringwork,demonstratesthepotentialofend-to-endau-
To enhance the interaction with conditional scene con- tonomous driving by integrating multiple perception tasks
text, we propose an efficient transformer-based diffusion to enhance planning performance. VAD [17] further ex-
decoderthatinteractsnotonlywithstructuredqueriesfrom plorestheuseofcompactvectorizedscenerepresentations
theperceptionmodulebutalsowithBird’sEyeView(BEV) to improve efficiency. Subsequently, a series of works [4,
and perspective view (PV) features through a sparse de- 6,10,20,23,36,38,48]haveadoptedthesingle-trajectory
formable attention mechanism [51]. Additionally, we in- planning paradigm to enhance planning performance fur-
troduce a cascade mechanism to iteratively refine the tra- ther. More recently, VADv2 [3] shifts the paradigm to-
jectory reconstruction within the diffusion decoder at each wards multi-mode planning by scoring and sampling from
denoisingstep. a large fixed vocabulary of anchor trajectories. Hydra-
With these innovations, we present DiffusionDrive, MDP [22] improves the scoring mechanism of VADv2
a diffusion model for real-time end-to-end autonomous by introducing extra supervision from a rule-based scorer.
driving. We benchmark our method on the planning- SparseDrive[32]exploresanalternativeBEV-freesolution.
oriented NAVSIM dataset [9] using non-reactive simula- Unlike existing multi-mode planning approaches, we pro-Sampled Noises DenoisedTrajs Camera Input
20 steps
Mode Collapse
Transfuser
DP
2 steps
Diverse Modes
DiffusionDrive
p=1 t=4s t=4s
Confidence Score Top-1 Scoring Traj Top-10 Scoring Traj Transfuser GT Traj
p=0 t=0s t=0s
(a)Top-1’sgoingstraightanddiversetop-10’slanechanging.
Sampled Noises DenoisedTrajs Camera Input
20 steps
Mode Collapse
Transfuser
DP
2 steps
Diverse Modes
DiffusionDrive
p=1 t=4s t=4s
Confidence Score Top-1 Scoring Traj Top-10 Scoring Traj Transfuser GT Traj
p=0 t=0s t=0s
(b)Top-1’sturningleftanddiversetop-10’slanechanging.
Figure2.QualitativecomparisonofTransfuser,Transfuser andDiffusionDriveonchallengingscenesofNAVSIMnavtestsplit.
DP
WiththesameinputsfromfrontcamerasandLiDAR,DiffusionDriveachievesthehighestplanningqualityoftop-1scoringtrajectoryas
illustratedinTab.2. WerenderthehighlighteddiversetrajectoriespredictedbyDiffusionDriveinthefrontview. (a)and(b)showsthat
the top-1 scoring trajectory of DiffusionDrive closely matches the ground truth for both going straight and turning left. Additionally,
DiffusionDrive’stop-10scoringtrajectorydemonstrateshigh-qualitylanechanging—anabilitynotobservedinmulti-modeTransfuser
DP
andimpossibleforTransfuser.
pose a novel paradigm that leverages powerful generative guided denoising with evolutionary search. VBD [15] in-
diffusionmodelsforend-to-endautonomousdriving. troducesascene-consistentscenariooptimizerusingacon-
ditionaldiffusionmodelwithgame-theoreticalguidanceto
Diffusionmodelfortrafficsimulation. Drivingdiffusion
generate abstract safety-critical driving scenarios. Moving
policyhasbeenexploredinthetrafficsimulationbylever-
beyond diffusion models limited to traffic simulation with
aging only abstract perception groundtruth [7, 15, 18, 37].
perceptiongroundtruth,ourapproachunlocksthepotential
MotionDiffuser[18]andCTG[50]arepioneeringapplica-
of diffusion models for real-time, end-to-end autonomous
tionsofdiffusionmodelsformulti-agentmotionprediction,
drivingthroughourproposedtruncateddiffusionpolicyand
usingaconditionaldiffusionmodeltosampletargettrajec-
efficientdiffusiondecoder.
tories from Gaussian noise. CTG++ [49] further incorpo-
rates a large language model (LLM) for language-driven Diffusion model for robotic policy learning. Diffusion
guidance, improving usability and enabling realistic traffic policy [5] demonstrates the great potential in robotic pol-
simulations. Diffusion-ES [41] replaces reward-gradient- icylearning,effectivelycapturingmulti-modeactiondistri-Diffusion step: 𝑞 𝜏𝑖|𝜏𝑖−1 ,𝑖∈[1,𝑇]
Denoising step: 𝑝 𝜏𝑖|𝜏𝑖−1,𝑧 ,𝑖∈[1,𝑇]
𝜃
Vanilla Diffusion Policy vs. Truncated Diffusion Policy (ours)
Diffusion step: 𝑞 𝜏𝑖|𝜏𝑖−1 ,𝑖∈[1,𝑇trunc] Denosingfrom anchored Gaussian distribution with fewer steps.
p=1
p=0 Confidence Score
Adding noise to the anchors with truncated diffusion schedule. Denoising step: 𝑝
𝜃
𝜏𝑖|𝜏𝑖−1,𝑧 ,𝑖∈[1,𝑇trunc]
Figure 3. Illustration of truncated diffusion policy by comparing with vanilla diffusion policy. We truncate the diffusion process
andonlyaddasmallportionofGaussiannoisetodiffusetheanchortrajectories. Then, wetrainthediffusionmodeltoreconstructthe
ground-truthtrajectoryfromtheanchoredGaussiandistributionwithconditionalscenecontext.Duringtheinference,wealsotruncatethe
denoisingprocessbystartingfromthebettersamplesintheanchoredGaussiandistributionthanthepureGaussiannoise.
butions and high-dimensional action spaces. Diffuser [16] ofwaypointsτ ={(x ,y
)}Tf
,whereT denotestheplan-
t t t=1 f
proposes an unconditional diffusion model for trajectory ninghorizon, and(x ,y )isthelocationofeachwaypoint
t t
sampling, incorporating techniques such as classifier-free attimetinthecurrentego-vehiclecoordinatesystem.
guidanceandimageinpaintingtoachieveguidedsampling.
Conditional diffusion model. The conditional diffu-
Subsequently,numerousworkshaveapplieddiffusionmod-
sion model poses a forward diffusion process as gradually
elstovariousrobotictasks,includingstationarymanipula-
addingnoisetothedatasample,whichcanbedefinedas:
tion [1, 44], mobile manipulation [40], autonomous nav-
igation [30, 42], quadruped locomotion [31], and dexter- q(cid:0) τi |τ0(cid:1) =N (cid:16) τi;√ α¯iτ0,(cid:0) 1−α¯i(cid:1) I(cid:17) , (1)
ous manipulation [39]. However, directly applying vanilla
diffusion policy to end-to-end autonomous driving poses where τ0 is the clean data sample, and τi is the data sam-
uniquechallenges,asitrequiresreal-timeefficiencyandthe ple with noise at time i (Note: we use superscript i to de-
generationofplausiblemulti-modetrajectoriesindynamic note diffusion timestep). The constant α¯i = (cid:81)i αs =
s=1
and open-world traffic scenes. In this work, we propose a (cid:81)i (1−βs)andβsisthenoiseschedule.Wetrainthere-
s=1
noveltruncateddiffusionpolicytoaddressthesechallenges, verseprocessmodelf (τi,z,i)topredictτ0 fromτi with
θ
introducingconceptsthathavenotyetbeenexploredinthe the guidance of conditional information z, where θ is the
roboticsfield. trainable model parameter. During inference, the trained
diffusion model f progressively refines from the random
θ
3.Method noiseτT sampledinGaussiandistributiontothepredicted
clean data sample τ0 with the guidance of conditional in-
3.1.Preliminary
formationz,whichisdefinedas:
Task formulation. End-to-end autonomous driving takes
(cid:90) T
rawsensordataasinputandpredictsthefuturetrajectoryof p (cid:0) τ0 |z(cid:1) = p(cid:0) τT(cid:1)(cid:89) p (cid:0) τi−1 |τi,z(cid:1) dτ1:T. (2)
θ θ
theego-vehicle. Thetrajectoryisrepresentedasasequence
i=1Anchored Gaussian Distribution 𝑵𝒊𝒏𝒇𝒆𝒓Noisy Trajs
𝑵𝒊𝒏𝒇𝒆𝒓Noisy Trajs 𝑵𝒊𝒏𝒇𝒆𝒓DenoisedTrajsat Step 𝒊
Sample Timestep𝑖
P (Mer ac pep /Dti eo tn
)
BA EMg Ve
a
/n
p
Pt
C Vondition
redoceD
noisuffiD
ttttpp
==
====
04
4001
ss
ss
T S
TC
S
T S
To c
rc
o c
ro
a
aopo
opn
j
jr-r
r-f
i1e
i1i
n
nd
0g
ge nce
𝑵×
ss nto
trC
-A
laitapS Iss nto
trC
-A
teD
rpaM/tnegA
i af tf iu vs ei o
DnNFF
i
fD fue sc io od ne
noitaludoM
r
D
L
epetsemiT
na oy ie sr
ingPLM S Tc rao jr .e 𝜏𝑠
On-board Sensor Data Diverse Driving Trajectories
(a) Overall pipeline (b) Diffusion decoder architecture
Figure 4. Overall architecture of DiffusionDrive. (a) DiffusionDrive can integrate various existingperception modules and different
sensor inputs. (b) The designed diffusion decoder takes the sampled noisy trajectories from anchored Gaussian distribution as input
andprogressivelydenoisesthemwithenhancedinteractionswiththeconditionalscenecontextinacascademannertogeneratethefinal
predictions.
3.2.Investigation 3.3.TruncatedDiffusion
Human driving follows fixed patterns, unlike the random
Turn Transfuser [6] into conditional diffusion model.
noise denoising in vanilla diffusion policy. Motivated by
We begin from the representative deterministic end-to-end
this,weproposeatruncateddiffusionpolicythatbeginsthe
planner Transfuser [6] and turn it into a generative model
denoising process from an anchored Gaussian distribution
Transfuser by simply replacing the regression MLP lay-
DP
instead of a standard Gaussian distribution. To enable the
ers with the conditional diffusion model UNet following
modeltolearntodenoisefromtheanchoredGaussiandis-
vanilladiffusionpolicy[5]. Duringtheevaluation,wesam-
tribution to the desired driving policy, we further truncate
plearandomnoiseandprogressivelyrefineitwith20steps.
thediffusionscheduleduringtraining,addingonlyasmall
Tab. 2 shows that Transfuser achieves better planning
DP
amountofGaussiannoisetotheanchors.
qualitythandeterministicTransfuser.
Training.Wefirstconstructthediffusionprocessbyadding
Modecollapse.Tofurtherinvestigatethemulti-modeprop- Gaussiannoisetoanchors{a }Nanchor clusteredbyK-Means
erty of the vanilla diffusion policy in driving, we sampled k k=1
onthetrainingset,wherea = {(x ,y
)}Tf
. Wetruncate
20randomnoisesfromGaussiandistributionanddenoised k t t t=1
the diffusion noise schedule to diffuse the anchors to the
themusing20steps. AsshowninFig.2,thedifferentran-
anchoredGaussiandistribution:
domnoisesconvergetosimilartrajectoriesafterthedenois-
√
ing process. To quantitatively analyze the phenomenon of (cid:112)
τi = α¯ia + 1−α¯iϵ, ϵ∼N(0,I), (4)
mode collapse, we define a mode diversity score D based k k
onthemeanIntersectionoverUnion(mIoU)betweeneach wherei ∈ [1,T ]andT ≪ T isthetruncateddiffu-
trunc trunc
denoised trajectory and the union of all denoised trajecto-
sionsteps.
ries: During training, the diffusion decoder f takes as input
θ
D =1− N1 (cid:88)N A Ar re ea a( (τ τi∩ ∪(cid:83) (cid:83)N j N=1τ τj) ), (3) N tioa nnch so cr on reo sis {y sˆt kr }a N kje =ac n 1ct ho orri ae ns d{ dτ eki n} oN k i= san e1ch dor ta ran jd ecp tr oe rd ieic sts {τˆc kla }s N ks =ai nfi 1chc ora :-
i=1 i j=1 j
{sˆ ,τˆ }Nanchor =f ({τi}Nanchor,z), (5)
whereτ representsthei-thdenoisedtrajectory,N istheto- k k k=1 θ k k=1
i
talnumberofsampledtrajectoriesand(cid:83)N
j=1τ j istheunion wherez representstheconditionalinformation. Weassign
ofalldenoisedtrajectories.AhighermIoUindicateslessdi- thenoisytrajectoryaroundtheclosestanchortotheground
versityofthedenoisedtrajectories. Thequantitativemode truthtrajectoryτ aspositivesample(y =1)andothersas
gt k
diversity results in Tab. 2 further validate the observations negativesamples(y =0).Thetrainingobjectivecombines
k
presentedinFig.2. trajectoryreconstructionandclassification:
Heavydenoisingoverhead. TheDDIM[29]diffusionpol-
icy requires 20 denoising steps to transform random noise N (cid:88)anchor
L= [y L (τˆ ,τ )+λBCE(sˆ ,y )], (6)
k rec k gt k k
intoafeasibletrajectory,whichintroducessignificantcom-
k=1
putational overhead, reducing the FPS from 60 to 7, as
showninTab.2,andmakingitimpracticalforreal-timeon- whereλbalancesthesimpleL1reconstructionlossL and
rec
linedrivingapplications. binarycross-entropy(BCE)classificationloss.Method Input Img. Backbone Anchor NC↑ DAC↑ TTC↑ Comf. ↑ EP↑ PDMS↑
UniAD[13] Camera ResNet-34[11] 0 97.8 91.9 92.9 100 78.8 83.4
PARA-Drive[38] Camera ResNet-34[11] 0 97.9 92.4 93.0 99.8 79.3 84.0
LTF[6] Camera ResNet-34[11] 0 97.4 92.8 92.4 100 79.0 83.8
Transfuser[6] C&L ResNet-34[11] 0 97.7 92.8 92.8 100 79.2 84.0
DRAMA[43] C&L ResNet-34[11] 0 98.0 93.1 94.8 100 80.1 85.5
VADv2-V [3] C&L ResNet-34[11] 8192 97.2 89.1 91.6 100 76.0 80.9
8192
Hydra-MDP-V [22] C&L ResNet-34[11] 8192 97.9 91.7 92.9 100 77.6 83.0
8192
Hydra-MDP-V -W-EP[22] C&L ResNet-34[11] 8192 98.3 96.0 94.6 100 78.7 86.5
8192
DiffusionDrive(Ours) C&L ResNet-34[11] 20 98.2 96.2 94.7 100 82.2 88.1
Table 1. Comparison on planning-oriented NAVSIM navtest split with closed-loop metrics. “C & L” denotes the use of both
cameraandLiDARassensorinputs.“V ”denotes8192anchors.“Hydra-MDP-V -W-EP”isavariantofHydra-MDP[22],whichis
8192 8192
furthertrainedtofittheEPevaluationmetricwithadditionalsupervisionfromtherule-basedevaluatorandusesweightedconfidencepost-
processing. DiffusionDrivesimplylearnsfromhumandemonstrationsandinferswithoutpost-processing. Thebestandthesecondbest
resultsaredenotedbyboldandunderline.
PlanModuleTime
Method NC↑ DAC↑ TTC↑ Comf.↑ EP↑ PDMS↑ D ↑ Para.↓ FPS↑
Arch. StepTime↓ Steps↓ Total↓
Transfuser 97.7 92.8 92.8 100 79.2 84.0 MLP 0.2ms 1 0.2ms 0% 56M 60
Transfuser 97.5 93.7 92.7 100 79.4 84.6 UNet 6.5ms 20 130.0ms 11% 101M 7
DP +0.6
Transfuser 97.9 94.2 93.9 100 80.2 85.7 UNet 6.9ms 2 13.8ms 70% 102M 27
TD +1.7
DiffusionDrive 98.2 96.2 94.7 100 82.2 88.1 Dec. 3.8ms 2 7.6ms 74% 60M 45
+4.1
Table2. RoadmapfromTransfusertoDiffusionDriveonNAVSIMnavtestsplit. “Transfuser ”denotesTransfuserwithvanilla
DP
DDIMdiffusionpolicy[5]. “Transfuser ”denotesTransfuserwithtruncateddiffusionpolicy. “StepTime”denotestheruntimeofeach
TD
denoisingstep.“FPS”andruntimearemeasuredonanNVIDIA4090GPU.“D”denotesthemodediversityscoredefinedinEq.(3).
Inference. Weuseatruncateddenoisingprocessthatstarts begin by applying deformable spatial cross-attention [26,
withnoisytrajectoriessampledfromtheanchoredGaussian 35,51]tointeractwithBird’sEyeView(BEV)orPerspec-
distributionandprogressivelydenoisesthemtofinalpredic- tiveView(PV)featuresbasedonthetrajectorycoordinates.
tions. Ateachdenoisingtimestep,theestimatedtrajectories Subsequently,cross-attentionisperformedbetweenthetra-
from the previous step are passed to the diffusion decoder jectory features and the agent/map queries derived from
f , which predicts classification scores {sˆ }Ninfer and co- theperceptionmodule,followedbyafeed-forwardnetwork
θ k k=1
ordinates {τˆ }Ninfer. After obtaining the current timestep’s (FFN). To encode the diffusion timestep information, we
k k=1
predictions,weapplytheDDIM[29]updateruletosample utilizeaTimestepModulationlayer,whichisfollowedbya
trajectoriesforthenexttimestep. Multi-LayerPerceptron(MLP)thatpredictstheconfidence
score and the offset relative to the initial noisy trajectory
Inferenceflexibility. Akeyadvantageofourapproachlies
coordinates. The output from this diffusion decoder layer
initsinferenceflexibility. Whilethemodelistrainedwith
servesastheinputforthesubsequentcascadediffusionde-
N trajectories, the inference process can accommo-
anchor
coderlayer. DiffusionDrivefurtherreusesthecascadedif-
dateanarbitrarynumberoftrajectorysamplesN ,where
infer
fusion decoder to iteratively denoise the trajectory during
N canbedynamicallyadjustedbasedoncomputational
infer
inference, with parameters shared across the different de-
resourcesorapplicationrequirements.
noisingtimesteps. Thefinaltrajectorywiththehighestcon-
3.4.Architecture fidencescoreisselectedastheoutput.
Theoverallarchitectureofourproposedmethod,Diffusion-
4.Experiment
Drive, isillustratedinFig.4. DiffusionDrivecanintegrate
various existing perception modules used in previous end-
4.1.Dataset
to-end planners [6, 13, 17, 32] and take different sensor
inputs. The designed diffusion decoder is tailored for the NAVSIM. The NAVSIM dataset [9] is a real-world
complexandchallengingdrivingapplication,whichhasen- planning-oriented dataset builds upon OpenScene [8], a
hancedinteractionswiththeconditionalscenecontext. compact redistribution of nuPlan [19], the largest publicly
Diffusiondecoder.Giventhesetofsamplednoisytrajecto- available annotated driving dataset. NAVSIM leverages
ries {τˆ }Ninfer from the anchored Gaussian distribution, we eight cameras to achieve a full 360◦ FOV, along with a
k k=1UNet EgoQuery Spatial Agent/Map Cascade PlanningMetric
ID Param.↓
Decoder Interaction Cross-attn Cross-attn Decoder NC↑ DAC↑ TTC↑ Comf.↑ EP↑ PDMS↑
1 ✓ ✓ ✗ ✗ ✗ 102M 97.9 94.2 93.9 100 80.2 85.7
2 ✗ ✓ ✗ ✗ ✗ 57M 88.7 83.2 80.0 84.8 43.3 55.1
3 ✗ ✓ ✓ ✗ ✗ 58M 98.2 95.4 94.4 100 81.3 87.1
4 ✗ ✓ ✗ ✓ ✗ 58M 97.9 93.5 93.8 100 79.8 85.1
5 ✗ ✓ ✓ ✓ ✗ 59M 98.0 95.8 94.4 100 81.7 87.4
6 ✗ ✓ ✓ ✓ ✓ 60M 98.2 96.2 94.7 100 82.2 88.1
Table 3. Ablation for design choices. “Cascade Decoder” indicates that we stack 2 cascade diffusion decoder layers. ID-1 refers to
Transfuser inTab.2,utilizingconditionalUNetandinteractionwiththeego-query,whichTransfuserusestodirectlyregressthesingle-
TD
modetrajectory.
Steps Param. NC DAC TTC Comf. EP PDMS Stages Param. NC DAC TTC Comf. EP PDMS Ninfer Param. NC DAC TTC Comf. EP PDMS
1 60M 98.3 96.0 94.7 100 82.1 87.9 1 59M 98.0 95.8 94.4 100 81.7 87.4 10 60M 97.9 93.5 93.1 100 80.0 84.9
2 60M 98.2 96.2 94.7 100 82.2 88.1 2 60M 98.2 96.2 94.7 100 82.2 88.1 20 60M 98.2 96.2 94.7 100 82.2 88.1
3 60M 98.2 96.3 94.7 100 92.2 88.1 4 65M 98.4 96.2 94.9 100 82.4 88.2 40 60M 98.5 96.2 94.8 100 82.5 88.2
Table4.Denoisingstepnumber. Table5.Cascadestages. Table6.NumberofsamplednoisesN .
infer
mergedLiDARpointcloudderivedfromfivesensors. An- augmentationisappliedandthefinaloutputforevaluation
notations are provided at a frequency of 2Hz and include onnavtestsplitis8-waypointtrajectoryover4seconds.
both HD maps and object bounding boxes. The dataset FurtherdetailsrefertoAppendix.
is designed to emphasize challenging driving scenarios in-
4.3.QuantitativeComparison
volvingdynamicchangesindrivingintentions,whiledelib-
eratelyexcludingtrivialsituationssuchasstationaryscenes
Tab.1comparesDiffusionDrivewithstate-of-the-artmeth-
orconstant-speeddriving. ods on NAVSIM navtest split. With the same ResNet-
NAVSIMbenchmarksplanningperformanceusingnon- 34 backbone, DiffusionDrive achieves 88.1 PDMS score,
reactive simulations and closed-loop metrics for compre- demonstrating significant superior performance over the
hensiveevaluation. Inthispaper, weemploytheproposed previous learning-based methods. Compared to VADv2,
PDMscore(PDMS)[9], whichisaweightedcombination DiffusionDrive surpasses it by 7.2 PDMS while reducing
of several sub-scores: no at-fault collisions (NC), drivable the number of anchors from 8192 to 20, representing a
area compliance (DAC), time-to-collision (TTC), comfort 400× reduction. DiffusionDrive also outperforms Hydra-
(Comf.),andegoprogress(EP). MDP, which follows VADv2’s sampling-from-vocabulary
paradigm, with a 5.1 PDMS improvement. Even com-
4.2.ImplementationDetail
paredtotheHydra-MDP-V -W-EP,whichisavariantof
8192
Hydra-MDP[22]byfurthertrainingtofittheEPevaluation
We adopt the same perception modules and ResNet-34
metricwithadditionalsupervisionandusingweightedcon-
backbone [11] as Transfuser for fair comparison. In the
fidence post-processing, DiffusionDrive still outperforms
diffusion decoder layer, we employ spatial cross-attention
it by 3.5 EP and 1.6 overall PDMS, relying solely on
to only interact with BEV features following Transfuser’s
a straightforward learning-from-human approach without
BEV-basedsetting. Weonlyperformagentcross-attention,
anypost-processing. ComparedtotheTransfuserbaseline,
sincetheperceptionmoduleofTransfuserdoesnotinclude
where we only differ in the planning module, Diffusion-
vectorizedmapconstruction. Westack2cascadediffusion
Drive delivers a notable 4.1 PDMS improvement, outper-
decoderlayersandapplytruncateddiffusionpolicywith20
formingitacrossallsub-scores.
clustered anchors. The training diffusion schedule is trun-
cated by 50/1000 to diffuse the anchors, while during in-
4.4.Roadmap
ference, we use only 2 denoising steps and select the top-
1 scoring predicted trajectory for evaluation. The train- In Tab. 2, converting Transfuser into the generative
ing and inference recipe directly follows Transfuser: We Transfuser using vanilla diffusion policy improves the
DP
use three cropped and downscaled forward-facing camera PDMSscoreby0.6andthemodediversityscoreDby11%.
images, concatenated as a 1024×256 image, and a raster- However,italsosignificantlyincreasestheoverheadofthe
ized BEV LiDAR as input; DiffusionDrive is trained on planning module, requiring 20× more denoising steps and
navtrainsplitfromscratchfor100epochswithAdamW 32× the step time, resulting in a total 650× increase in
optimizer on 8 NVIDIA 4090 GPUs with total batch size runtime overhead. With the proposed truncated diffusion
of512, settingthelearningrateto6×10−4. Notest-time policy,Transfuser reducesthenumberofdenoisingsteps
TDL2(m)↓ CollisionRate(%)↓
Method Input Img. Backbone FPS↑
1s 2s 3s Avg. 1s 2s 3s Avg.
ST-P3[12] Camera EffNet-b4[33] 1.33 2.11 2.90 2.11 0.23 0.62 1.27 0.71 1.6
UniAD[13] Camera ResNet-101[11] 0.45 0.70 1.04 0.73 0.62 0.58 0.63 0.61 1.8
OccNet[34] Camera ResNet-50[11] 1.29 2.13 2.99 2.14 0.21 0.59 1.37 0.72 2.6
VAD[17] Camera ResNet-50[11] 0.41 0.70 1.05 0.72 0.07 0.17 0.41 0.22 4.5
SparseDrive[32] Camera ResNet-50[11] 0.29 0.58 0.96 0.61 0.01 0.05 0.18 0.08 9.0
DiffusionDrive(Ours) Camera ResNet-50[11] 0.27 0.54 0.90 0.57 0.03 0.05 0.16 0.08 8.2
Table7. ComparisononnuScenesdatasetwithopen-loopmetrics. FPSismeasuredonasingleNVIDIA4090GPUfollowingthe
recipeofSparseDrive[32].MetriccalculationfollowsST-P3[12].
from 20 to 2 while achieving an increase of 1.1 in PDMS actionspaceandleadtoimprovedplanningquality.
and a 59% improvement in mode diversity. By further in-
4.6.QualitativeComparison
corporatingtheproposeddiffusiondecoder,thefinalmodel,
DiffusionDrive, reaches88.1PDMSand74% modediver- Since the PDMS planning metric calculates based on the
sityscoreD.ComparedtotheTransfuser ,DiffusionDrive top-1scoringtrajectoryandourproposedDscoreevaluates
DP
showsimprovementsin3.5PDMSand64%modediversity, modediversity,thesemetricsalonecannotfullycapturethe
and a 10× reduction in denoising steps, resulting in a 6× qualityofdiversetrajectories. Tofurthervalidatethequal-
speedupinFPS.Thisenablesreal-time,high-quality,multi- ity of multi-mode trajectories, we visualize the planning
modeplanning. results of Transfuser, Transfuser and DiffusionDrive on
DP
challengingscenariosofNAVSIMnavtestsplitinFig.2.
4.5.AblationStudy Theresultsindicatethatthemulti-modetrajectoriesgener-
atedbyDiffusionDrivearenotonlydiversebutalsoofhigh
Effect of designs in diffusion decoder. Tab. 3 shows the
quality. In Fig. 2a, the top-1 scoring trajectory generated
effectivenessofourdesignchoicesinthediffusiondecoder.
by DiffusionDrive closely resembles the ground-truth tra-
ID-1istheTransfuser intheTab.2. BycomapringID-6
TD jectory,whilethehighlightedtop-10scoringtrajectorysur-
and ID-1, we can see that the proposed diffusion decoder
prisingly tries to perform high-quality lane changing. In
reduce the 39% parameters and significantly improves the
Fig. 2b, the highlighted top-10 scoring trajectory also per-
planning quality by 2.4 PDMS. ID-2 shows severe perfor-
formsalanechange,andaneighboringlow-scoringtrajec-
mance degeneration due to the lack of rich and hierarchi-
toryfurtherinteractswithsurroundingagentstoeffectively
cal interaction with the environment. By comparing ID-2
avoidcollisions.
andID-3,wecanseethatspatialcross-attentionisvitalfor
accurate planning. ID-5 shows that the proposed cascade 4.7.QuantitativeComparisononnuScenesdataset
mechanismiseffectiveandcanfurtherimprovetheperfor-
ThenuScenesdatasetispreviouslypopularbenchmarkfor
mance.
end-to-endplanning.SincethemajorscenariosofnuScenes
Denoisingstepnumber. Tab.4showsthatduetotherea- are simple and trivial situations, we only perform compar-
sonablestartpoint,DiffusionDrivecanachieveagoodplan- ison in Tab. 7. We implement DiffusionDrive on top of
ning quality with only 1 step. Further increasing the de- SparseDrive[32]followingitstrainingandinferencerecipe
noising steps can improve the planning quality, and make usingopen-loopmetricsproposedinST-P3[12]. Westack
it enjoy the flexible inference given the complexity of the 2cascadediffusiondecoderlayersandapplythetruncated
environment. diffusionpolicywith18clusteredanchors.
Cascadestages. Tab.5ablatestheimpactofcascadestage AsshowninTab.7,DiffusionDrivereducestheaverage
number. Increasingthestagenumbercanimprovetheplan- L2errorofSparseDriveby0.04m,achievingthelowestL2
ning quality but saturate at the 4 stages and cost more pa- error and average collision rate against previous state-of-
rametersandinferencetimeateachstep. the-artmethods. WhileDiffusionDriveisalsoefficientand
runs1.8×fasterthanVADwith20.8%lowerL2errorand
Number of sampled noises N . As stated in Sec. 3.3,
infer 63.6%lowercollisionrate.
DiffusionDrive can generate varied trajectories by simply
samplingavariablenumberofnoisesfromanchoredGaus-
5.Conclusion
sian distribution. Tab. 6 shows that10 sampled noises can
already achieve a decent planning quality. By sampling In this work, we propose a novel generative driving
more noises, DiffusionDrive can cover potential planning decision-makingmodel,DiffusionDrive,forend-to-endau-tonomous driving by incorporating the proposed truncated [11] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
diffusion policy and efficient cascade diffusion decoder. Deep residual learning for image recognition. In CVPR,
DiffusionDrive can denoise a variable number of samples 2016. 6,7,8
fromananchoredGaussiandistributiontogeneratediverse [12] ShengchaoHu,LiChen,PenghaoWu,HongyangLi,Junchi
planning trajectories at real-time speeds. Comprehensive Yan,andDachengTao. St-p3: End-to-endvision-basedau-
tonomous driving via spatial-temporal feature learning. In
experimentsandqualitativecomparisonsvalidatethesupe-
ECCV,2022. 8
riority of DiffusionDrive in planning quality, running effi-
[13] YihanHu,JiazhiYang,LiChen,KeyuLi,ChonghaoSima,
ciency,andmodediversity.
Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai
Wang, et al. Planning-oriented autonomous driving. In
Acknowledgement
CVPR,2023. 1,2,6,8
WewouldliketoacknowledgeTianhengChengforhelpful [14] JunjieHuang,GuanHuang,ZhengZhu,YunYe,andDalong
Du. Bevdet: High-performancemulti-camera3dobjectde-
feedbackonthedraft.
tectioninbird-eye-view. arXivpreprintarXiv:2112.11790,
References 2021. 1
[15] Zhiyu Huang, Zixu Zhang, Ameya Vaidya, Yuxiao Chen,
[1] AnuragAjay,YilunDu,AbhiGupta,JoshuaB.Tenenbaum, Chen Lv, and Jaime Ferna´ndez Fisac. Versatile scene-
TommiS.Jaakkola,andPulkitAgrawal. Isconditionalgen- consistent traffic scenario generation as optimization with
erativemodelingallyouneedfordecisionmaking? InICLR, diffusion. arXivpreprintarXiv:2404.02524,2024. 3
2023. 4 [16] MichaelJanner,YilunDu,JoshuaB.Tenenbaum,andSergey
[2] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora, Levine.Planningwithdiffusionforflexiblebehaviorsynthe-
VeniceErinLiong,QiangXu,AnushKrishnan,YuPan,Gi- sis. InICLR,2022. 1,4
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi- [17] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie
modaldatasetforautonomousdriving. InCVPR,2020. 2 Chen,HelongZhou,QianZhang,WenyuLiu,ChangHuang,
[3] Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing andXinggangWang. Vad: Vectorizedscenerepresentation
Xu,QianZhang,ChangHuang,WenyuLiu,andXinggang forefficientautonomousdriving. InICCV,2023. 1,2,6,8
Wang. Vadv2: End-to-end vectorized autonomous driving [18] Chiyu Jiang, Andre Cornman, Cheolho Park, Benjamin
viaprobabilisticplanning.arXivpreprintarXiv:2402.13243, Sapp,YinZhou,DragomirAnguelov,etal. Motiondiffuser:
2024. 1,2,6 Controllablemulti-agentmotionpredictionusingdiffusion.
[4] ZhiliChen, MaoshengYe, ShuangjieXu, TongyiCao, and InCVPR,2023. 3
QifengChen. Ppad: Iterativeinteractionsofpredictionand [19] Napat Karnchanachari, Dimitris Geromichalos, Kok Seang
planning for end-to-end autonomous driving. In ECCV, Tan, NanxiangLi, ChristopherEriksen, ShakibaYaghoubi,
2024. 2 Noushin Mehdipour, Gianmarco Bernasconi, Whye Kit
[5] ChengChi,ZhenjiaXu,SiyuanFeng,EricCousineau,Yilun Fong,YiluanGuo,etal. Towardslearning-basedplanning:
Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. The nuplan benchmark for real-world autonomous driving.
Diffusionpolicy: Visuomotorpolicylearningviaactiondif- InICRA,2024. 6
fusion. InRSS,2023. 1,3,5,6 [20] YingyanLi,LueFan,JiaweiHe,YuqiWang,YuntaoChen,
[6] KashyapChitta,AdityaPrakash,BernhardJaeger,ZehaoYu, Zhaoxiang Zhang, and Tieniu Tan. Enhancing end-to-end
Katrin Renz, and Andreas Geiger. Transfuser: Imitation autonomousdrivingwithlatentworldmodel. arXivpreprint
with transformer-based sensor fusion for autonomous driv- arXiv:2406.08481,2024. 2
ing. TPAMI,2022. 1,2,5,6 [21] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-
[7] YounwooChoi,RayCodenMercurius,SoheilMohamadAl- hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:
izadeh Shabestary, and Amir Rasouli. Dice: Diverse dif- Learning bird’s-eye-view representation from multi-camera
fusion model with scoring for trajectory prediction. In IV, imagesviaspatiotemporaltransformers. InECCV,2022. 1
2024. 3 [22] ZhenxinLi,KailinLi,ShihaoWang,ShiyiLan,ZhidingYu,
[8] OpenScene Contributors. Openscene: The largest up-to- YishenJi,ZhiqiLi,ZiyueZhu,JanKautz,ZuxuanWu,etal.
date 3d occupancy prediction benchmark in autonomous Hydra-mdp: End-to-end multimodal planning with multi-
driving. https://github.com/OpenDriveLab/ target hydra-distillation. arXiv preprint arXiv:2406.06978,
OpenScene,2023. 6 2024. 1,2,6,7
[9] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo [23] ZhiqiLi,ZhidingYu,ShiyiLan,JiahanLi,JanKautz,Tong
Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Lu,andJoseMAlvarez. Isegostatusallyouneedforopen-
Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas loopend-to-endautonomousdriving? InCVPR,2024. 2
Geiger, and Kashyap Chitta. Navsim: Data-driven non- [24] Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng
reactiveautonomousvehiclesimulationandbenchmarking. Cheng,QianZhang,WenyuLiu,andChangHuang.MapTR:
InNeurIPS,2024. 2,6,7 StructuredmodelingandlearningforonlinevectorizedHD
[10] Xunjiang Gu, Guanyu Song, Igor Gilitschenski, Marco mapconstruction. InICLR,2023. 1
Pavone,andBorisIvanovic.Producingandleveragingonline [25] BenchengLiao,ShaoyuChen,YunchiZhang,BoJiang,Qian
mapuncertaintyintrajectoryprediction. InCVPR,2024. 2 Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang.Maptrv2:Anend-to-endframeworkforonlinevectorizedhd forautonomousdrivingandzero-shotinstructionfollowing.
mapconstruction. IJCV,2024. 1 InCVPR,2024. 3
[26] Xuewu Lin, Tianwei Lin, Zixiang Pei, Lichao Huang, and [42] Wenhao Yu, Jie Peng, Huanyu Yang, Junrui Zhang, Yifan
Zhizhong Su. Sparse4d: Multi-view 3d object detec- Duan, Jianmin Ji, and Yanyong Zhang. Ldp: A local dif-
tion with sparse spatial-temporal fusion. arXiv preprint fusion planner for efficient robot navigation and collision
arXiv:2211.10581,2022. 6 avoidance. arXivpreprintarXiv:2407.01950,2024. 4
[27] Yicheng Liu, Tianyuan Yuan, Yue Wang, Yilun Wang, and [43] ChengranYuan, ZhanqiZhang, JiaweiSun, ShuoSun, Ze-
Hang Zhao. Vectormapnet: End-to-end vectorized hd map fan Huang, Christina Dao Wen Lee, Dongen Li, Yuhang
learning. InICML,2023. 1 Han, Anthony Wong, Keng Peng Tee, et al. Drama: An
[28] OlafRonneberger,PhilippFischer,andThomasBrox.U-net: efficientend-to-endmotionplannerforautonomousdriving
Convolutionalnetworksforbiomedicalimagesegmentation. withmamba. arXivpreprintarXiv:2408.03601,2024. 6
InMICCAI,2015. 2 [44] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu,
[29] JiamingSong,ChenlinMeng,andStefanoErmon. Denois- MuhanWang,andHuazheXu. 3ddiffusionpolicy. InRSS,
ingdiffusionimplicitmodels. InICLR,2021. 5,6 2024. 4
[30] Ajay Sridhar, Dhruv Shah, Catherine Glossop, and Sergey [45] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xi-
Levine. Nomad: Goalmaskeddiffusionpoliciesfornaviga- angyuZhang,andYichenWei. Motr: End-to-endmultiple-
tionandexploration. InICRA,2024. 4 objecttrackingwithtransformer. InECCV,2022. 1
[31] MariaStamatopoulou,JianweiLiu,andDimitriosKanoulas. [46] YifuZhang,ChunyuWang,XinggangWang,WenjunZeng,
Dippest: Diffusion-basedpathplannerforsynthesizingtra- andWenyuLiu. Fairmot: Onthefairnessofdetectionand
jectories applied on quadruped robots. arXiv preprint re-identificationinmultipleobjecttracking. IJCV,2021.
arXiv:2405.19232,2024. 4 [47] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng
[32] WenchaoSun,XuewuLin,YiningShi,ChuangZhang,Hao- Weng, ZehuanYuan, PingLuo, WenyuLiu, andXinggang
ran Wu, and Sifa Zheng. Sparsedrive: End-to-end au- Wang.Bytetrack:Multi-objecttrackingbyassociatingevery
tonomous driving via sparse scene representation. arXiv detectionbox. InECCV,2022. 1
preprintarXiv:2405.19620,2024. 2,6,8 [48] Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming
[33] MingxingTanandQuocLe. Efficientnet:Rethinkingmodel Zhang,andLongChen. Genad: Generativeend-to-endau-
scalingforconvolutionalneuralnetworks.InICML,2019.8 tonomousdriving. InECCV,2024. 2
[34] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei [49] ZiyuanZhong,DavisRempe,YuxiaoChen,BorisIvanovic,
Wu,HanmingDeng,YiGu,LeweiLu,PingLuo,DahuaLin, YulongCao,DanfeiXu,MarcoPavone,andBaishakhiRay.
etal. Sceneasoccupancy. InICCV,2023. 8 Language-guidedtrafficsimulationviascene-leveldiffusion.
[35] Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, InCoRL,2023. 3
Yilun Wang, Hang Zhao, and Justin Solomon. Detr3d: [50] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen,
3d object detection from multi-view images via 3d-to-2d SushantVeer,TongChe,BaishakhiRay,andMarcoPavone.
queries. InCoRL,2022. 1,6 Guidedconditionaldiffusionforcontrollabletrafficsimula-
[36] YuqiWang,JiaweiHe,LueFan,HongxinLi,YuntaoChen, tion. InICRA,2023. 3
and Zhaoxiang Zhang. Driving into the future: Multiview [51] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,
visual forecasting and planning with world model for au- andJifengDai. Deformabledetr: Deformabletransformers
tonomousdriving. InCVPR,2024. 2 forend-to-endobjectdetection. InICLR,2021. 2,6
[37] Yixiao Wang, Chen Tang, Lingfeng Sun, Simone Rossi,
Yichen Xie, Chensheng Peng, Thomas Hannagan, Stefano
Sabatini, Nicola Poerio, Masayoshi Tomizuka, et al. Opti-
mizing diffusion models for joint trajectory prediction and
controllablegeneration. InECCV,2024. 3
[38] XinshuoWeng, BorisIvanovic, YanWang, YueWang, and
MarcoPavone.Para-drive:Parallelizedarchitectureforreal-
timeautonomousdriving. InCVPR,2024. 2,6
[39] Zehang Weng, Haofei Lu, Danica Kragic, and Jens Lun-
dell. Dexdiffuser: Generating dexterous grasps with diffu-
sionmodels. arXivpreprintarXiv:2402.02989,2024. 4
[40] Sixu Yan, Zeyu Zhang, Muzhi Han, Zaijin Wang, Qi Xie,
Zhitian Li, Zhehan Li, Hangxin Liu, Xinggang Wang, and
Song-Chun Zhu. M2diffuser: Diffusion-based trajectory
optimization for mobile manipulation in 3d scenes. arXiv
preprintarXiv:2410.11402,2024. 4
[41] Brian Yang, Huangyuan Su, Nikolaos Gkanatsios, Tsung-
Wei Ke, Ayush Jain, Jeff Schneider, and Katerina Fragki-
adaki. Diffusion-es: Gradient-free planning with diffusion