Time is on my sight: scene graph filtering for dynamic
environment perception in an LLM-driven robot‚ãÜ
SimoneColombani1,3, LucaBrini2, DimitriOgnibene2 and GiuseppeBoccignone1
1UniversityofMilan,Italy
2UniversityofMilano-Bicocca,Milan,Italy
3OversonicRobotics,CarateBrianza,Italy
Abstract
Robotsareincreasinglybeingusedindynamicenvironmentslikeworkplaces,hospitals,andhomes.Asaresult,
interactionswithrobotsmustbesimpleandintuitive,withrobots‚Äôperceptionadaptingefficientlytohuman-
inducedchanges.
Thispaperpresentsarobotcontrolarchitecturethataddresseskeychallengesinhuman-robotinteraction,with
a particular focus on the dynamic creation and continuous update of the robot‚Äôs state representation. The
architectureusesLargeLanguageModelstointegratediverseinformationsources,includingnaturallanguage
commands,roboticskillsrepresentation,real-timedynamicsemanticmappingoftheperceivedscene. This
enablesflexibleandadaptiveroboticbehaviorincomplex,dynamicenvironments.
Traditionalroboticsystemsoftenrelyonstatic,pre-programmedinstructionsandsettings,limitingtheiradapt-
abilitytodynamicenvironmentsandreal-timecollaboration.Incontrast,thisarchitectureusesLLMstointerpret
complex,high-levelinstructionsandgenerateactionableplansthatenhancehuman-robotcollaboration.
Atitscore,thesystem‚ÄôsPerceptionModulegeneratesandcontinuouslyupdatesasemanticscenegraphusing
RGB-Dsensordata,providingadetailedandstructuredrepresentationoftheenvironment.Aparticlefilteris
employedtoensureaccurateobjectlocalizationindynamic,real-worldsettings.
ThePlannerModuleleveragesthisup-to-datesemanticmaptobreakdownhigh-leveltasksintosub-tasksand
linkthemtoroboticskillssuchasnavigation,objectmanipulation(e.g.,PICKandPLACE),andmovement(e.g.,
GOTO).
By combining real-time perception, state tracking, and LLM-driven communication and task planning, the
architectureenhancesadaptability,taskefficiency,andhuman-robotcollaborationindynamicenvironments.
Keywords
Human-Robotinteraction,Robottaskplanning,Largelanguagemodels,Scenegraphs
1. Introduction
Immediacy is crucial in assistive robotics [1, 2, 3]. In a typical human-robot interaction scenario,
usersmayprovidecommandsinnaturallanguage,suchas‚ÄúPickthebluebottleonthetableandbring
it to me‚Äù. To such aim, the use of Large Language Models (LLM) allows robots to interpret natural
languagerequestsand‚Äútranslate‚Äùinstructionsintoplanstoachievespecificgoals;yet,thesemodels
needtoknowtheenvironmentinwhichtheyoperatesotogenerateaccurateplans[4]. Theneedfor
translationarisesfromthecomplexityofhumanlanguageandthevariabilityininstructions. Usersmay
expresscommandsdifferentlyorexploitambiguoustermsthattherobotmustcomprehend. Toaddress
thesechallenges,roboticarchitecturesmustintegratenaturallanguageprocessingwithenvironmental
understanding.
The chief concern of the work is to exploit scene graphs as semantic maps providing a structured
representation of spatial and semantic information of robot‚Äôs environment. This enables LLMs to
generateplansbasedonthisinformation. Indeed,viascenegraphsrobotscanmaptherelationships
betweenobjects,theirproperties,andtheirspatialarrangements.
Hereweaddresssuchlimitationsbyrepresentingtheenvironmentasagraphendowedwithupdatable
WorkshoponAdvancedAIMethodsandInterfacesforHuman-CenteredAssistiveandRehabilitationRobotics(aFit4MedRob
event)-AIxIA2024,November25‚Äì28,2024,Bolzano,Italy
$simone.colombani@studenti.unimi.it(S.Colombani);l.brini@campus.unimib.it(L.Brini);dimitri.ognibene@unimib.it
(D.Ognibene);giuseppe.boccignone@unimi.it(G.Boccignone)
¬©2024Copyrightforthispaperbyitsauthors.UsepermittedunderCreativeCommonsLicenseAttribution4.0International(CCBY4.0).
4202
voN
22
]OR.sc[
1v72051.1142:viXrasemanticsthatlanguagemodelscaninterpret. Moreprecisely,thedynamicsoftheupdateisachieved
viaparticlefilteringtoenhancethereliabilityandprecisionofreal-timesemanticmapping. Themodel
adopted(PSGTR)islightweightandcanbeeasilyutilized,makingitsuitableforliveapplicationsand
accessibleevenonlesspowerfulhardware. UsingRoBee,thecognitivehumanoidrobotdevelopedby
Oversonic Robotics, the system dynamically updates the environment graph and replans in case of
failure,overcomingchallengesinlong-termtaskplanning.
2. Related works
Ascenegraphcapturesdetailedscenesemanticsbyexplicitlymodelingobjects,theirattributes,andthe
relationshipsbetweenpairedobjects(e.g.,‚Äúbluebottleonthetable‚Äù)[5]. 3Dscenegraphs[6]extendthis
concepttothree-dimensionalspaces,representingenvironmentslikehousesoroffices,whereeachpiece
offurniture,room,andobjectisanode. Theedgesbetweenthesenodesdescribetheirrelationships,
suchasavaseonatableorachairinfrontof asofa.
Recent works, such as [7] and [8] have proposed to generate 3D scene graphs from RGB-D images,
combininggeometricandsemanticinformationtocreatedetailedenvironmentalrepresentations. Scene
graphshavebeenwidelyusedincomputervisionandroboticstoimprovesceneunderstanding,object
detection,andtaskplanning. Forexample,SayPlan[9]integrates3DscenegraphsandLLMsfortask
navigationandplanning,performingsemanticsearchesonthesceneandinstructionstocreateaccurate
plans,furtherrefinedthroughscenariosimulations. DELTA[10]utilizes3Dscenegraphstogenerate
PDDL files, employing multiple phases to prune irrelevant nodes and decompose long-term goals
intomanageablesub-goals,enhancingcomputationalefficiencyforexecutionwithclassicalplanners.
SayNav[11]constructsscenegraphsincrementallyfornavigationinnewenvironments,allowingthe
robottogeneratedynamicandappropriatenavigationplansinunexploredspacesbypassingthescene
graphtoaLLM,thusfacilitatingeffectivemovementandexecutionofuserrequests.
Inacrudesummary,themainlimitationsoftheabovementionedapproachestobuildenvironment
representations lie in their reliance on computationally heavy vision-language models (VLMs) and
computer vision models. Such models are not designed for precision and often demand significant
resources,whilelackingtheabilitytobeupdatedinrealtime,andthuslimitingtheirpracticalapplication.
3. Architecture
Oursystemisbasedontwocomponents:
‚Ä¢ PerceptionModule: itisresponsibleforsensingandinterpretingtheenvironmentandbuilding
a semantic map in the form of a directed graph that integrates both geometric and semantic
information. Itsarchitectureisexplainedindetailbelow.
‚Ä¢ PlannerModule: ittakestheinformationprovidedbythePerceptionModuletoformulateplans
andactionsthatallowtherobottoperformspecifictasks. Itiscomposedbythefollowing:
‚Äì TaskPlanner: Translatesuserrequests,expressedinnaturallanguage,intohigh-levelskills.
‚Äì SkillPlanner: Translateshigh-levelskillsintospecific,low-levelexecutableactions.
‚Äì Executor: Executesthelow-levelactionsgeneratedbytheSkillPlanner.
‚Äì Controller: Monitorstheexecutionofactionsandmanagesanyerrorsorunexpectedevents
duringtheprocess.
‚Äì Explainer: Interpretsthereasonsofexecutionfailuresbyanalyzingdatareceivedfromthe
ControllerandprovidessuggestionstotheTaskPlanneronhowtoadjusttheplan.
Thesecomponentsinteracttoallowtherobottounderstanditsenvironmentandactaccordinglyto
satisfyuserrequests. InwhatfollowswespecificallyaddressthePerceptionModulewhiledetailson
theplannerwillbeprovidedinaseparatearticle.RobotHardware. ThesystemwasimplementedusingRoBee,thecognitivehumanoidrobotdeveloped
byOversonicRobotics. RoBee,showninFigure3.1,stands160cmtallandweighs60kg. Itfeatures32
degreesoffreedom,andisequippedwithcameras,microphones,andforcesensors.
3.1. Perceptionmodule
ThePerceptionModuleisthecomponentresponsibleforbuildingarepresentationoftheenvironment,
which the robot can use for task planning. The representation takes the form of a semantic map, a
graphthatintegratesbothgeometricandsemanticinformationabouttheenvironment. Togenerate
thesemanticmap,theperceptionmoduleusesdatafromvarioussensors. ItrequiresRGB-Dframes
obtainedfromthecamerawhicharethenprocessedusingascenegraphgenerationmodel,suchas
PSGTR[12]toextractobjectsmasks,labelandrelationships. Alsoitusesdataonthecameraposition
relativetothegeometricmaptodeterminethelocationoftheobjectsidentifiedbythemodel. More
formally,aSemanticMapisrepresentedasadirectedgraphùê∫ = (ùëâ ,ùê∏ )where:
ùëö ùëö ùëö
‚Ä¢ Anodeùë£ ‚àà ùëâ canbeoneofthefollowingtypes:
ùëö
‚Äì Room node: Defines the different semantic areas of the environment, such as ‚Äúkitchen,‚Äù
‚Äúliving room,‚Äù or ‚Äúbedroom.‚Äù Each room node contains information about its geometric
boundariesandtheobjectnodesitcontains;
‚Äì Object node: Represents physical objects in the environment, such as ‚Äútable,‚Äù ‚Äúchair,‚Äù or
‚Äúbottle.‚Äù Eachobjectnodecontainsinformationaboutits3Dposition,semanticcategory,
dimensions,andotherrelevantproperties:
‚Ä¢ Anedgeùëí ‚àà ùê∏ canrepresent:
ùëö
‚Äì Therelationshipbetweentwoobjects;
‚Äì Theconnectionbetweentworooms;
‚Äì Thebelongingofanobjecttooneandonlyoneroom.
Thepresenceofroomnodesisimportantbecauseitfacilitatesthecategorizationofobjectsbasedon
theirrespectiverooms,whichhelpsdistinguishbetweenobjectswiththesamenameandenhances
thenaturallanguagedescriptionofthetask,whileroomnodesenabletheapplicationofgraphsearch
algorithmsforplanningpathstoobjects. Roomnodesarecreatedbasedonthegeometricmap,while
objectnodesaregeneratedfollowingthestepsexplainedbelow.
Astoedges,morespecifically:
‚Ä¢ Edgesbetweenroomsdirectlyconnecttworoomsandfacilitatenavigationbetweenthem.
‚Ä¢ Edgesbetweenobjectsrepresenttherelationshipsbetweenobjectsandaredirected,thedirection
capturingtheinfluenceofoneobjectonanother;thelabelassociatedwitheachedgeisderived
fromtheinferencesmadebythePSGTRmodel.
Figure3.1showsanexampleofasemanticmapofanoffice,builtwiththeroomnode‚ÄôOffice‚Äô(italian,
‚ÄôUfficio‚Äô)andtheobjectnodesconnectedtoeachotherbyrelationshipsandlinkedtotheroomnode.
Generatingandupdatingthesemanticmap Thescenegraphgenerationprocessisbasedonthe
PSGTRmodel,asingle-stagemodelbuiltontheTransformerarchitecture[13]. Thismodelgeneratesa
graphrepresentationofascenegivenitspanopticsegmentation. PSGTRdoesnotachievethehighest
qualityinpanopticsegmentationcomparedtobettermodels,butitprovidesreasonableinferencetimes
forreal-timeapplications,takingabout400mstoprocessa480pimageonamachinewithaccesstoan
NVIDIAT4GPU.
ThePerceptionModuleusestheresultofPSTGRandbuildsthesemanticmapfollowingthestepsbelow:
1. ReadingRGB-Dframes: Thevideoframesfromtherobot‚Äôscamerasaresenttothemodeltobe
analyzedandusedtogeneratethescenegraph.Figure1:Thefigureontheleftshowcasesanexampleofasemanticmapinanofficeenvironment,whilethe
imageontherightshowsRoBee,thehumanoidrobotdevelopedbyOversonicRobotics.
2. Reading robot poses: To generate the scene and semantic map, it is necessary to know the
robot‚Äôspositionrelativetothegeometricmap,thecamera‚Äôspositionrelativetothemap,andthe
camera‚Äôsmountingpositionontherobot.
3. Inference:Eachreceivedframeisprocessedbythemodel. Resultsareinformationaboutdetected
objects,suchaslabelsandmasks,andtherelationshipsbetweenthem,suchasrelationshiplabels
andassociatedprobabilities.
4. Graphconstruction: Thisstepinvolvesextractingdatafromtheobjectreturnedbythemodel
andcomputingvaluesdependentontherobotsystem,suchasthepositionofobjects. Atafiner
levelitconsistsofthreesub-steps:
a) Node construction: Classes and masks of detected objects are extracted. Next, the 3D
positionofeachobjectiscomputed,startinginthepixelcoordinatesystem,thentransform-
ingtothecamerasystem,andfinallytotherobot‚Äôsmapcoordinatesystem. Nodesforthe
semanticsceneandthesemanticmapareinstantiatedusingtheappropriate3Dcoordinates.
Adistance-basedfilterisappliedtopruneobjectsthataretoofarfromtherobottoavoid
issueswithobjectdetectionandtracking.
b) Edge construction: Data about relationships between objects are extracted. For each
relationship,thesourceandtargetobjectindicesareidentified. Ifbothobjectsmeetdistance
constraintsandtherelationshipprobabilityexceedsadefinedthreshold,anedgeiscreated
betweenthecorrespondingnodes.
c) Inference improvement through Particle Filter (PF): As the model‚Äôs output is not
accurateregardingmaskinference,thisleadstoerrorsincalculatingtheobject‚Äôscentroid
forobtainingitspositionrelativetothemap. APFbasedonpreviousobservationsisapplied
toimprovetheaccuracyoftheresult.
Attheendoftheprocess,thesemanticmapisupdatedwiththenewinformation,andthesemantic
sceneisgeneratedandprovidedtotheplannermodule.
ThePFisusedtotracktheobjectmasksinreal-time,providedasoutputbythePSGTRmodel,andto
improvetheestimationoftheirpositioninspace. Duringtheupdateprocess,thefilterusesinformation
fromframesacquiredtorefinethepositionestimateoftheobjects. Thelastobjectmasksidentifiedby
thePSGTRmodelarecomparedwithpreviousonesusingtheIntersectionoverUnion(IoU)metrics
andbyapplyingthemotionmodel,whichcanbedefinedasatransformationofthecameraposition
relativetothemapbetweentwotimeinstances. Denotethetransformationmatricesdescribingthe
camerapositionattimeùë°‚àí1andatsubsequenttimeùë°,T andT ,respectively;then,thechangein
ùë°‚àí1 ùë°
positionandorientationcanbeexpressedbythetransformationmatrix‚àÜT = T T‚àí1 . Toassociate
ùë° ùë°‚àí1
objectsbetweensuccessiveframes,weuseanIoUmatrixcomputedoversegmentationmasks. Fortwo
masksùê¥andùêµ,IoUisdefinedasIoU(ùê¥,ùêµ) = |ùê¥‚à©ùêµ|,where|ùê¥‚à©ùêµ|representstheareaofintersection
|ùê¥‚à™ùêµ|
betweenmasksùê¥andùêµ,and|ùê¥‚à™ùêµ|representstheareaoftheirunion. TocomparesegmentationTable1
Comparisonofpositiondata
Property NoParticle Particle
Realposition[m] (0.67,0.10,0.95) (0.67,0.10,0.95)
Meanposition[m] (0.74,-0.08,0.93) (0.65,0.08,0.94)
Meanofabsoluteerror[m] (0.07,0.18,0.02) (0.02,0.02,0.01)
Errorstandarddeviation[m] (0.35,0.24,0.03) (0.17,0.12.0.02)
masksbetweentwosuccessiveframes,wedenotethesegmentationmaskattimeùë°‚àí1asùëÄ andat
ùë°‚àí1
timeùë°asùëÄ . Thetransformationmatrix‚àÜTisappliedtothepreviousmasktoobtainatransformed
ùë°
maskùëÄ‚Ä≤ suchthatùëÄ‚Ä≤ = ‚àÜT¬∑ùëÄ . TheIntersectionoverUnion(IoU)isthencomputedbetween
ùë°‚àí1 ùë°‚àí1 ùë°‚àí1
thetransformedmaskùëÄ‚Ä≤ andthecurrentmaskùëÄ asfollows: IoU(ùëÄ‚Ä≤ ,ùëÄ ) = |ùëÄ ùë°‚Ä≤ ‚àí1‚à©ùëÄùë°|. This
ùë°‚àí1 ùë° ùë°‚àí1 ùë° |ùëÄ ùë°‚Ä≤ ‚àí1‚à™ùëÄùë°|
allowsustoidentifythesameobjectacrosssuccessiveframesbasedontheirmasks.
Moreformally,eachobjectisrepresentedbyasetofùëÅ particles,whereeachparticleùë†ùë° attimeùë°isa
ùëñ
3Dvectorrepresentingahypothesisabouttheobject‚Äôsposition: ùë†ùë° = [ùë• ,ùë¶ ,ùëß ]ùëá,whereùëñ = 1,...,ùëÅ.
ùëñ ùëñ ùëñ ùëñ
The particles are initialized with a normal distribution around the initially observed position ùúá =
0
[ùë• ,ùë¶ ,ùëß ]ùëá: ùë†0 ‚àº ùí©(ùúá ,Œ£ ), where Œ£ = diag(ùúé2,ùúé2,ùúé2) is the initial covariance matrix. Initial
0 0 0 ùëñ 0 0 0 ùë• ùë¶ ùëß
weightsareuniform: ùë§0 = 1,whereùëñ = 1,...,ùëÅ. Predictiontakesintoaccountthecameramotion.
ùëñ ùëÅ
If ùëá is the transformation matrix from frame ùë°‚àí1 to frame ùë°, each particle is updated as ùë†ùë° =
ùë°‚àí1,ùë° ùëñ
ùëá ¬∑ùë†ùë°‚àí1+ùë†0,whereùë†0representsthenoiseaddedtoaccountforuncertaintiesinmotion,maintaining
ùë°‚àí1,ùë° ùëñ ùëñ ùëñ
thesamedistributionstructureusedforinitialparticleinitialization. Givenanewobservationùë† ,the
ùëõùëíùë§
particleweightsareupdatedbasedontheEuclideandistancebetweenthepredictedpositionandthe
observedone: ùëëùë° = ‚Äñùë†ùë° ‚àíùë† ‚Äñ andùë§ùë° = 1 . Weightsarethennormalized: ùë§ùë° = ùë§ ùëñùë° . The
ùëñ ùëñ ùëõùëíùë§ 2 ùëñ 1+ùëëùë° ùëñ ‚àëÔ∏ÄùëÅ ùë§ùë°
ùëñ ùëó=1 ùëó
finalpositionoftheobjectÀÜùë† isestimatedastheweightedmeanofalltheparticles: ÀÜùë† = ‚àëÔ∏ÄùëÅ ùë§ùë°ùë†ùë°.
ùë° ùë° ùëñ=1 ùëñ ùëñ
Table1showstheimprovementobtainedover30measurementsusingparticlefilter.
Theoverallprocessforupdatingthesemanticmapusingtheparticlefiltercanbesummarizedbythe
algorithm1.
Algorithm1SemanticMapupdateusingParticleFilter
1: foreachframeùë°do
2: foreachobjectùëòdo
3: Applytransformation:ùëÄ‚Ä≤ =ŒîT¬∑ùëÄ ‚óÅTransformpreviousmasks
ùë°‚àí1 ùë°‚àí1
4: endfor
5: ComputeIoU(ùëÄ‚Ä≤ ,ùëÄ )= |ùëÄùë°‚Ä≤ ‚àí1‚à©ùëÄùë°| ‚óÅComputeIoUbetweennodesandinferenceresults
ùë°‚àí1 ùë° |ùëÄ ùë°‚Ä≤ ‚àí1‚à™ùëÄùë°|
6: foreachobjectùëòdo
7: ifIoU>ùúÜIoUthen
8: Updateweights:ùëëùë°
ùëñ
=‚Äñùë†ùë° ùëñ‚àíùë†new‚Äñ 2,ùë§ ùëñùë° = 1+1
ùëëùë°
ùëñ
9: Normalize:ùë§ùë° = ùë§ùëñùë°
ùëñ ‚àëÔ∏ÄùëÅ ùë§ùë°
ùëó=1 ùëó
10: Estimate:^ùë† =‚àëÔ∏ÄùëÅ ùë§ùë°ùë†ùë°
ùë° ùëñ=1 ùëñ ùëñ
11: endif
12: endfor
13: foreachunmatchedobservationdo
14: Initnewobject:ùë†0 ‚àºùí©(ùúá ,Œ£ )
ùëñ 0 0
15: endfor
16: Updatesemanticmapwith^ùë†
ùë°
17: endfor
4. Conclusions
Scenegraphsprovideastructuredrepresentationthatcapturesgeometricandsemanticinformation
abouttheenvironment. Thiscomprehensiveunderstandingenablesimprovedtaskplanningwithlargelanguagemodels,allowingrobotstoexecutecommands.
Inthisarticlewehaveshownhowtousereal-timesensordatatodynamicallyupdatesemanticmaps,
thusenablingtherobottoadapttoongoingchangesintheirenvironment,particularlyincollaborative
settings influenced by human actions. Here, particle filtering is applied to improve geometric data
precisionandsemanticmapaccuracy. Thiscanbeparticularlyimportantalsoforsocialinteractionand
intentionprediction[14,15]otherthanphysicalinteractionwiththeenvironment.
Theissuesaddressedinthisworkarecogent. Indeed, theeffectivenessofplannersintranslating
complexinstructionsintoactionableplansreliesonarobuststaterepresentation. Withoutanaccurate
semanticmap,plannersriskgeneratingplansthatmisalignwiththeactualenvironment,potentially
leadingtotaskfailures. Theintegrationofsemanticandgeometricinsightspermitsrobotstoreason
abouttheirenvironmentinamoreinformedandadaptiveway,ensuringthattheycanoperateeffectively
andresponsivelyindynamicenvironments.
TheadoptionofasemanticmapcontainingrichspatialinformationcombinedwithaflexibleLLM
basedplannercaneasilyallowtoexploreinthefuturetheintroductionofnewspatialrelationships,e.g.
wrapped,stuckunder,surrounding,aligned,thatcouldsupportspecificnovelrobotskills[16].
Acknowledgments
SpecialthankstoOversonicRoboticsforenablingtheimplementationofthesystemusingtheirhu-
manoidrobot,RoBee.
References
[1] C. Di Napoli, G. Ercolano, S. Rossi, Personalized home-care support for the elderly: a field
experiencewithasocialrobotathome, UserModelingandUser-AdaptedInteraction33(2023)
405‚Äì440.
[2] L.Lucignano,F.Cutugno,S.Rossi,A.Finzi, Adialoguesystemformultimodalhuman-robotinter-
action, in: Proceedingsofthe15thACMonInternationalconferenceonmultimodalinteraction,
2013,pp.197‚Äì204.
[3] D.Ognibene,L.Mirante,L.Marchegiani, Proactiveintentionrecognitionforjointhuman-robot
search and rescue missions through monte-carlo planning in pomdp environments, in: Social
Robotics: 11thInternationalConference,ICSR2019,Madrid,Spain,November26‚Äì29,2019,Pro-
ceedings11,Springer,2019,pp.332‚Äì343.
[4] C.Galindo,J.-A.Fern√°ndez-Madrigal,J.Gonz√°lez,A.Saffiotti, Robottaskplanningusingsemantic
maps, Roboticsandautonomoussystems56(2008)955‚Äì966.
[5] G.Zhu, L.Zhang, Y.Jiang, Y.Dang, H.Hou, P.Shen, M.Feng, X.Zhao, Q.Miao, S.A.A.Shah,
etal., Scenegraphgeneration: Acomprehensivesurvey, arXive-prints(2022)arXiv‚Äì2201.
[6] I. Armeni, Z.-Y. He, J. Gwak, A. R. Zamir, M. Fischer, J. Malik, S. Savarese, 3d scene graph: A
structureforunifiedsemantics,3dspace,andcamera,in:ProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision,2019,pp.5664‚Äì5673.
[7] Q. Gu, A. Kuwajerwala, S. Morin, K. M. Jatavallabhula, B. Sen, A. Agarwal, C. Rivera, W. Paul,
K.Ellis,R.Chellappa,etal., Conceptgraphs: Open-vocabulary3dscenegraphsforperceptionand
planning, in: 2024IEEEInternationalConferenceonRoboticsandAutomation(ICRA),IEEE,2024,
pp.5021‚Äì5028.
[8] H.Chang,K.Boyalakuntla,S.Lu,S.Cai,E.Jing,S.Keskar,S.Geng,A.Abbas,L.Zhou,K.Bekris,
etal., Context-awareentitygroundingwithopen-vocabulary3dscenegraphs, arXivpreprint
arXiv:2309.15940(2023).
[9] K.Rana,J.Haviland,S.Garg,J.Abou-Chakra,I.Reid,N.Suenderhauf, Sayplan: Groundinglarge
languagemodelsusing3dscenegraphsforscalablerobottaskplanning, in:7thAnnualConference
onRobotLearning,2023.[10] Y.Liu,L.Palmieri,S.Koch,I.Georgievski,M.Aiello, Delta: Decomposedefficientlong-termrobot
taskplanningusinglargelanguagemodels, arXive-prints(2024)arXiv‚Äì2404.
[11] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, A. Velasquez, Saynav: Grounding large lan-
guagemodelsfordynamicplanningtonavigationinnewenvironments, in: Proceedingsofthe
InternationalConferenceonAutomatedPlanningandScheduling,volume34,2024,pp.464‚Äì474.
[12] J. Yang, Y. Z. Ang, Z. Guo, K. Zhou, W. Zhang, Z. Liu, Panoptic scene graph generation, in:
EuropeanConferenceonComputerVision,Springer,2022,pp.178‚Äì196.
[13] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,L.Kaiser,I.Polosukhin, At-
tentionisallyouneed, in:Proceedingsofthe31stInternationalConferenceonNeuralInformation
ProcessingSystems,NIPS‚Äô17,CurranAssociatesInc.,RedHook,NY,USA,2017,p.6000‚Äì6010.
[14] D.Ognibene,E.Chinellato,M.Sarabia,Y.Demiris, Contextualactionrecognitionandtargetlocal-
izationwithanactiveallocationofattentiononahumanoidrobot, Bioinspiration&biomimetics
8(2013)035002.
[15] S.Rossi,M.Staffa,L.Bove,R.Capasso,G.Ercolano, User‚Äôspersonalityandactivityinfluenceon
hricomfortabledistances, in: SocialRobotics: 9thInternationalConference,ICSR2017,Tsukuba,
Japan,November22-24,2017,Proceedings9,Springer,2017,pp.167‚Äì177.
[16] D.Marocco,A.Cangelosi,K.Fischer,T.Belpaeme, Groundingactionwordsinthesensorimotor
interaction with the world: experiments with a simulated icub humanoid robot, Frontiers in
neurorobotics4(2010)1308.
5. Online Resources
MoreinformationaboutRoBeeandOversonicRoboticsareavailable:
‚Ä¢ RoBee,
‚Ä¢ OversonicRobotics