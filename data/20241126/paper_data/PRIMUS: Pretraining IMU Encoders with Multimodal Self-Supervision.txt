PR S: Pretraining IMU Encoders with
IMU
Multimodal Self-Supervision
Arnav M. Das†* Chi Ian Tang‡ Fahim Kawsar‡§ Mohammad Malekzadeh‡
‡ † §
Nokia Bell Labs Cambridge, UK University of Washington, USA University of Glasgow, UK
arnavmd2@uw.edu {ian.tang, fahim.kawsar, mohammad.malekzadeh}@nokia-bell-labs.com
Abstract—SensinghumanmotionsthroughInertialMeasurementUnits
(IMUs)embeddedinpersonaldeviceshasenabledsignificantapplications
inhealthandwellness.WhilelabeledIMUdataisscarce,wecancollect
unlabeled or weakly labeled IMU data to model human motions. For
video or text modalities, the “pretrain and adapt” approach utilizes
large volumes of unlabeled or weakly labeled data for pretraining,
building a strong feature extractor, followed by adaptation to specific
tasks using limited labeled data. This approach has not been widely
adoptedintheIMUdomainfortworeasons:(1)pretrainingmethodsare
poorlyunderstoodinthecontextofIMU,and(2)open-sourcepretrained
models that generalize across datasets are rarely publicly available. In
this paper, we aim to address the first issue by proposing PRIMUS, a
method for PRetraining IMU encoderS. We conduct a systematic and
unified evaluation of various self-supervised and multimodal learning
Fig.1:PRIMUSOverview.Weuseamulti-objectivepretraininginclud-
pretrainingobjectives.OurfindingsindicatethatusingPRIMUS,which
combinesself-supervision,multimodalsupervision,andnearest-neighbor
ingthreeterms,LSS,LMM,andLNN.Self-supervisedlossesencourage
supervision, can significantly enhance downstream performance. With the IMU encoder to be augmentation invariant, while multimodal and
fewerthan500labeled samples perclass,PRIMUSeffectivelyenhances nearestneighborlossesaligntheIMUdatatoco-occurringvideoand/or
downstreamperformancebyupto15%inheld-outtestdata,compared textdata.Weuseopen-sourcepretrainedmodelsdevelopedbyothersfor
tothestate-of-the-artmultimodaltrainingmethod.Tobenefitthebroader bothtextandvideoencoders.
community,ourcodeandpre-trainedIMUencoderswillbemadepublicly
availableatgithub.com/nokia-bell-labsuponpublication.
I. INTRODUCTION learning [10], [11], and masked reconstruction [12], have yet to be
evaluatedoncross-domainusecases.Multimodal(MM)learninghas
WearabledevicesembedInertialMeasurementUnit(IMU)sensors,
becomepopularinthefieldofrepresentationlearning[13]–[17],and
includingaccelerometersandgyroscopes,whichtrackthemovement,
has recently been used for pretraining IMU encoders by utilizing
acceleration, and orientation of the human body. When modeled
supervisory signals from multiple devices [18], [19] or multimodal
using machine learning (ML) methods, IMU data provides valuable
data [20]. IMU2CLIP [21] aligns the latent representations of IMU
insights into human physical and emotional behaviors, playing a
datawiththosecomingfromtextannotationsorthosefromegocentric
crucial role in health monitoring and overall well-being [1]–[5]. For
videos, where they show enhanced capabilities in multimodal data
example, step-counting data from IMU sensors is one of the most
retrieval.
effective indicators of cognitive impairment progression in elderly
While both classes of representation learning approaches, i.e., SS
individuals[6].Suchpotentialhasmotivatedthecommunitytocollect
and MM, have shown promising results, neither one fully leverages
vast amounts of IMU data in time-series form. However, obtaining
diversesourcesofinformationpresentinIMUtimeseries.Giventhe
large amounts of labeled IMU data remains a major challenge,
promisinguseofthesynergisticrelationshipbetweenself-supervised
because IMU time series are inherently difficult to interpret and
andmultimodallearninginthecomputervisionandnaturallanguage
annotate, even by experts [7].
processing fields [22]–[24], and with the recent public availability
Apromisingsolutionforlabelscarcityisthe“pretrainonce,adapt
of a large multimodal dataset EgoExo4D [25], which includes syn-
many times” approach. This involves initially training an encoder
chronized video, text, and IMU segments, we aim to explore this
on a large corpus of unlabeled or weakly labeled data. Afterward, a
combination for pretraining IMU encoders.
smallerMLmodelistrainedontopofthe(typicallyfrozen)encoder
for specific tasks, using relatively small amounts of labeled data. In this paper, we propose PRIMUS (see Figure 1): a novel
While this approach has shown significant success in image, video, method for PRetraining IMU encoderS that produces transferable
audio, and natural language processing, its potential for IMU data representations by building a multi-objective representation learning
remains underexplored, primarily because of challenges in curating strategy that combines SS and MM losses to pretrain an IMU
large volumes of quality datasets. encoder.Pretrainedon therecentlyreleasedEgoExo4D dataset[25],
Difficulties in collecting labeled data have motivated represen- we assess the effectiveness of our strategy by evaluating how well
tation learning methods for IMU encoders by using supervisory PRIMUS IMU encoder performs on both in-domain and out-of-
signals from IMU data itself (self-supervised learning), or other domainclassificationtasksusingonlyasmallamountoflabeleddata
concurrent modalities (multimodal learning). Self-supervised (SS) (i.e., few-shot learning). A consistent performance improvement of
learningapproachesbasedonmulti-tasklearning[8],[9],contrastive up to 15% in test accuracy when compared to existing state-of-the-
art multimodal and self-supervised training methods was observed
*Workhasbeendoneduringtheauthor’sinternshipatNokiaBellLabs. throughout different levels of data availability. Our ablation study
4202
voN
22
]GL.sc[
1v72151.1142:viXraFig. 2: The architecture of IMU
Encoder I. The backbone consists
of both 1D-CNN and GRU layers.
Duringpretraining,theIMUencoder
has two MLP heads: one for mul-
timodal loss and the other for uni-
modal loss. After pre-training, only
theoutputofthemultimodalheadis
kept for training downstream tasks,
as it offers a more generalized la-
tent representation. The architecture
isadoptedfrom[21].
Fig. 3: Nearest neighbor supervision. Given a query segment, we
retrieve the most similar segment in the queue, based on video-to-video
also showcased the superior performance of our proposed combined
similarity, and use all modalities to derive supervisory signals for the
trainingobjective.ThisdemonstratesthatPRIMUSenablesencoders
IMUsegment.Featuresareretrievedfromafixed-sizequeue.
to learn highly transferable representations, allowing for various
future adaptations.
and considering τ as a learnable temperature parameter, the self-
II. METHODOLOGY
supervised objective, adapted from SimCLR [10], [27], can be
LetI denoteanencoderthattakesasegmentofmultivariateIMU formally expressed as
t Aim se shs oe wri nes inas Fii gn .p 1u ,t wan ed trag ie nne Ira wte is tha thl ra ete en ot br je ep cr tie vs ee sn :ta st ei lo f-n sua ps eo rvu it sp iu ot n.
L
(B)=(cid:88)n exp(cid:0) I(m i)·I(h(m i))(cid:1)1/τ
,
loss (L SS), multimodal loss (L MM), and nearest-neighbour loss
SS
i=1
(cid:80)n k=1exp(cid:0)
I(m i)·I(h(m
k))(cid:1)1/τ
(L ). (1) L ensures that I remains invariant to noise, similar
NN SS B. Multimodal Supervision
to those that are introduced by slight changes in sensor position or
We use multimodal learning (the second block, shown in blue in
type (§II-A). (2) L pushes IMU representations towards aligned
MM
Fig.1)inordertoallowtheIMUencodertolearnsemanticfeatures
textandvideorepresentations,allowingI tolearntherichsemantic
thatarepresentinrichmodalitiessuchastextandvideo,butdifficult
information present in other modalities (§II-B). (3) L uses the
NN
to learn with self-supervision alone [21]. Many open-source video
closest examples in representation space as positive pairs, enabling
and text encoders have been pretrained on web-scale data and can
the model to leverage natural data similarities for more adaptive
be used to produce rich representations for the video/text in each
contrastive learning (§II-C).
frame. Throughout this paper, we use an open-source video encoder
In our implementation (see Fig. 2), I is a Stacked RNN consist-
V and text encoder T produced by CLIP4Clip [16] to instantiate
ing of convolutional, group normalization, and max-pooling layers,
our multimodal learning objective, since this model is designed to
toppedwithaGRUlayer,basedonthearchitectureoftheIMU2CLIP
handleshortvideoclipsandisreadilyavailable.GivenabatchB =
model [21], with a total of 1.4M parameters. Our main motivation
{(m ,v ,t )}n , the multimodal loss has two components which
for this architecture is its efficiency in deployment on mobile and i i i i=1
can be expressed as
wearable devices, which are the target platforms for collecting IMU
data [26]. Moreover, it has shown effective generalization perfor- (cid:88)n exp(cid:0) I(m i)·V(v i)(cid:1)1/τ
mance in processing ML tasks on IMU data. During pre-training, I L m2v(B)= (cid:80)n exp(cid:0)
I(m )·V(v
)(cid:1)1/τ, (1)
has two MLP heads: the first head is used to compute the unimodal i=1 j=1 i j
self-supervision loss and the second head is used to compute the and
multimodal loss. For downstream tasks, only the latter is retained as (cid:88)n exp(cid:0) I(m i)·T(t i)(cid:1)1/τ
L (B)= , (2)
it provides a richer latent representation. m2t (cid:80)n exp(cid:0)
I(m )·T(t
)(cid:1)1/τ
For pretraining, we use the EgoExo4D dataset [25], a multimodal i=1 j=1 i j
datasetcontainingIMUfromhead-placedsensors,egocentricvideos, whereagainτ isalearnabletemperature.Intuitively,L m2v (orL m2t)
and free-form text annotations. After pre-processing, this dataset encourage I to map IMU data to representations that are close to
consistsofaround250Ksegments,eachof5-secondlength,providing corresponding video (or text) representations in the latent space. We
aligned IMU, video, and text triplets. We denote the pre-training use L MM(B)=L m2v(B)+L m2t(B) as our MM objective.
dataset as D = {(m ,v ,t )}N where m , v , t correspond to
i i i i=1 i i i C. Nearest Neighbor Supervision
a single segment of time-aligned IMU, video, and text, respectively.
The loss terms introduced so far, L and L , both derive
SS MM
A. Self-Supervision supervision from within the same triplet segment. To increase the
diversityofsupervisionandgobeyondasingleinstance,weleverage
The self-supervised learning objective is an unimodal loss that
nearest-neighborsupervision[23],[30](shownintherightmostblock
encourages the representations of augmented versions of the same
in orange in Fig. 1 and in detail in Fig. 3). During training, we
data to be similar (the first block, shown in red in Fig. 1). For data
maintain a feature queue Q = {(zm,zv,zt)}K , where zm, zv,
augmentation, we define a stochastic transformation module h(.) j j j j=1 j j
and zt are cached representations of IMU, video, and text produced
consisting of two transformations: (1) scaling by a random factor j
from their respective encoders. For every given instance (m ,v ,t )
i i i
and (2) reversing the direction of time (see [10] for more details).
in a batch B, we define
These transformations were chosen after evaluating all pairs of aug-
mentations proposed in [10]. Given a batch B = {(m ,v ,t )}n , η(i)=argmax (zv·V(v )), (3)
i i i i=1 k∈[K] k iTABLE I: Downstream Tasks. A summary of the classification datasets used for downstream evaluation. Unlike previous work on IMU
representation learning, we consider tasks that have IMU data collected from unseen devices and have novel output domains.
TestSet Activities InputDomain OutputDomain SampleSize
EgoExo4D [25] 8:{playmusic,cook,medicaltest,performCPR,repairbike,climbrock,soccer,dance} Same Same Train:195K–Test:53K
Ego4D[28] 10:{playmusic,cook,eat,clean,carpenter,craft,farmer,household,walk,construction} Same Different Train:555K–Test:57K
REALWORLD[29] 8:{climbingup,climbingdown,jumping,lyingdown,run,walk,sit,down} Different Different Train:8.3K–Test:2.6K
0.7
0.45
0.25
0.6
0.40
0.5 0.20
0.35
0.4
0.15
0.30
0.3
0.2 P IMR UI 2M CU LIS P(ours) 0.10 P IMR UI 2M CU LIS P(ours) 0.25 P IMR UI 2M CU LIS P(ours)
0.1 S M Si tum al ntC i dtaL asR rk dS TS rL aining 0.05 S M Si tum al ntC i dtaL asR rk dS TS rL aining 0.20 S M Si tum al ntC i dtaL asR rk dS TS rL aining
100 200 300 400 100 200 300 400 100 200 300 400
Labeled Segments per Class Labeled Segments per Class Labeled Segments per Class
(a)EgoExo4DResults (b)Ego4DResults (c)REALWORLDResults
Fig. 4: Main Results. We report the few-shot learning performance of pretrained models on various classification datasets. PRIMUS generally
outperformsself-supervisedmethods(SimCLR,MultitaskSSL),andpriormultimodalmethods(IMU2CLIP),aswellastrainingarandomlyinitialized
model(standardtraining).Thestandarderroriscomputedover5trials.
which identifies the index k in Q corresponding to the video A. Datasets and Setup
embedding that is the most similar to v . We leverage the video
i All the baselines and our IMU encoder are pretrained on
representations for identifying the closest pairs because the video
EgoExo4D, which contains IMU data (triaxial accelerometer and
encoder is pretrained on a large dataset, and therefore produces
triaxial gyroscope) collected from head-placed sensors. Thus, we
stable representations. Also, videos capture much finer details about
focusondownstreamtasksthatuseIMUdataofhead-placedsensors.
human activities compared to text descriptions. We illustrate the
A summary of datasets is given in Table I.
queuingmechanismfornearestneighborretrievalinFig.3.Wethen
EgoExo4D[25]andEgo4D[28].Fromeachdataset,wechoosea
push I(m ) close to zm ,zv ,zt by L , which consists of a
i η(i) η(i) η(i) NN held-out test set for human activity recognition, where IMU data is
unimodal and multimodal loss similar to L and L as
SS MM labeledaccordingtotheactivitiesindicatedinthefilenames.Notethat
L (B)=
(cid:88) (cid:88)n exp(cid:0) I(m i)·z ηm (o i)d(cid:1)1/τ
. (4)
tE hg eo4 saD mi esc aa sp Etu gr oe Ed xu os 4in Dg (t ph re e-s ta rm aie nid ne gvi dc ae t, asP er to )j ,ec bt u- tA Eri ga os 4m Dar it ng cla luss de1 s,
NN mod∈{m,v,t}i=1 (cid:80)n j=1exp(cid:0) I(m i)·z ηm (o j)d(cid:1)1/τ some activities that are not present in EgoExo4D.
REALWORLD [29]. The REALWORLD dataset is a human
The final multi-objective loss that we use in PRIMUS is
activity recognition dataset with 8 predefined classes, that contain
data captured by various Samsung Galaxy-S4 and LG G-Watch-R
L(B)=αL (B)+βL (B)+γL (B). (5)
SS MM NN
placed at different positions on the body. For our analysis, we use
In our experiments we set α=β =γ =1, leave the fine-tuning of thedatafromthehead-placedsensor.Weadoptedthewell-established
hyperparameterstofuturestudies.Notethat,asthesehyperparameters user-baseddataset-splittingstrategyforourevaluations,inwhichdata
arenottuned,theresultsreportedinthefollowingsectionsrepresent from a held-out set of users are reserved for testing, measuring the
lower bounds on the performance achievable with a more thorough performanceofthemodelonunseenusers.Thisdatasetalsoevaluates
hyperparameter search. the out-of-domain performance of our pretrained models since both
the set of activities and device type are different from EgoExo4D.
III. EXPERIMENTALEVALUATION
B. Main Results
For the downstream evaluations, we focus on human activity We compare our PRIMUS against closely related training base-
recognition tasks using only IMU data. We consider different lines.(I)SimCLR[10]isaself-supervisedtrainingmethodbasedon
levels of data scarcity by varying the number of labeled segments data augmentations. (II) IMU2CLIP [21] is a multimodal training
per class (i.e., few-shot learning). We evaluate the effectiveness of method for IMU data, corresponding to using only L or L
SS MM
PRIMUSforadownstreamtask,overotherpretrainingbaselines,by as the pretraining objective on EgoExo4D. Moreover, our work
analyzingtheperformanceofalinearclassifierontherepresentations leveragessupervisorysignalsfromdifferentlearningsetupstotraina
producedbytheIMUencoder(i.e.,linearprobing[13]),atechnique better-performingfeatureextractor.Thus,wealsocomparePRIMUS
which requires few computational resources to train and retains the against (III) MultitaskSSL [8], a well-established self-supervised
robustness of the pretrained encoder. approach for IMU signals. Finally, we compare PRIMUS against
Overall, we compare PRIMUS against other pretraining (IV) Standard Training, which starts from a randomly initialized
baselines (§III-B), conduct ablations on each loss term of
PRIMUS(§III-C),andevaluatedataefficiencyofPRIMUS(§III-D). 1https://www.projectaria.com
ycaruccA
tseT
ycaruccA
tseT
ycaruccA
tseT0.7
0.45
0.25
0.6
0.40
0.5
PRIMUS(ours) 0.20 0.35 PRIMUS(ours)
0.4 MM+ NN MM+ NN
MM 0.30 MM
SS 0.15 SS
0.3 SS+ NN 0.25 SS+ NN
0.2 0.10 PRIMUS(ours) 0.20
MM+ NN
0.1 M SSM 0.15
0.05 SS+ NN
100 200 300 400 100 200 300 400 100 200 300 400
Labeled Segments per Class Labeled Segments per Class Labeled Segments per Class
(a)EgoExo4DResults (b)Ego4DResults (c)REALWORLDResults
Fig. 5: Ablations. We assess the importance of each individual term in the PRIMUS objective, by pretraining encoders with different losses and
evaluatingthembasedonfew-shotlearningperformance.Thestandarderroriscomputedover5trials.
model and updates all the parameters (as opposed to just the Fig. 6: Data Efficiency. We
final layer) with standard supervised learning. This final baseline 0.70 report few-shot performance
representsthestandardprocedureusedtotrainamodelintheabsence on the EgoExo4D classifica-
of a pretrained IMU encoder. 0.65 tiontaskat500segmentsper
Fig. 4 presents the comparison of PRIMUS with all four base- class for PRIMUS models
lines. Across all experiments, we observe that our PRIMUS model, 0.60 trainedwithvariousamounts
pretrained with the joint objective, significantly outperforms any of multimodal data. Models
0.55
pretraining strategy previously proposed. Our method consistently pretrained with the PRIMUS
outperformsallotherbaselinesbyasmuchas15%ontheEgoExo4D objective require far fewer
0.50 PRIMUS w/ Partial Dataset
dataset. On Ego4D, it performs on par with IMU2CLIP but still IMU2CLIP w/ Full Dataset IMU segments with aligned
surpasses all other baselines. Additionally, on the REALWORLD %0 of Pre20 trainin4 g0 Data 6 w0 / Vide80 o + Te10 x0 t video/textthanIMU2CLIP.
dataset, our method generally outperforms all baselines, particularly
in scenarios where labeled data is limited (fewer than 100 samples). the pretraining data and evaluate the efficacy of the resulting IMU
Notably, standard training, which updates all the parameters, fails encoderinfew-shotlearning.Fig.6showsthatthereisnostatistically
to generalize well in the low-data regime particularly for complex significant difference between an encoder pretrained with only 10%
classification tasks (on EgoExo4D and Ego4D). ofthedataalignedwithvideo/textwiththePRIMUSandanencoder
pretrainedinthestyleofIMU2CLIPonEgoExo4Dintermsoffew-
C. Ablations
shot learning performance.
Fig. 5 presents an ablation study on the pretraining objectives to
understand which components of the loss are most critical. We find IV. CONCLUSIONANDFUTUREWORK
that L MM is a key component, indicating that future studies for This paper studies pretraining objectives for building an IMU
developingIMUfoundationmodelsshouldincorporatealignedvideo, encoder that can be adapted to unseen tasks with limited labeled
text, audio, and potentially other under-explored wearable sensors. data. We empirically demonstrate the superiority of our pretraining
We also find that L NN is generally helpful, but only when we methodagainstexistingapproachesonin-domainandout-of-domain
haveareliableestimateofsimilarity.WithL SS+L NN,weobserve tasks, and identify some of the components that were critical to its
some form of collapse (with accuracy around 10-15%) since this success.Wedemonstratethatourmethodcanbeusedonpretraining
setting does not exploit any multimodal signals, and using the IMU datasets with few samples of temporally aligned video or text.
representations itself to find similar segments from the queue can Our work has its limitations and there are several promising
make training unstable. directions for future work. First, while we evaluate on out-of-
Finally, while L SS is not particularly helpful in EgoExo4D (evi- domain downstream tasks, all of our evaluation schemes assume
dentfromthefactthatL MM+L NN nearlymatchestheperformance that the sensor position on the human body is similar to that of the
ofPRIMUS),self-supervisionseemstomakeasignificantdifference pretrainingset.Trainingamodelthatiscapableofgeneralizingacross
on out-of-domain tasks, offering up to 5% of accuracy improvement humanbodypositionsisanimportantfuturedirection,butpretraining
in REALWORLD. We hypothesize that this is due to the fact that datasets to enable this are not yet available. Second, our evaluation
this loss term explicitly encourages the IMU encoder to be invariant focuses on activities of medium granularity (corresponding to ‘ac-
to some ofthe types of noisethat may be observeddue to changing tions’ according to the hierarchy of activities proposed in [31]). To
devices or positions on the body. recognizemoreabstractorprimitiveactivities,adifferentprocessing
pipeline would be needed to accommodate the different time scales
D. Pretraining Data Efficiency
in which these activities occur. Further studies might be needed to
While we showed that the MM loss is critical for PRIMUS,
adapt our proposed method for these different scenarios.
obtaining large-scale IMU datasets that are temporally aligned with
Despite open challenges, our work contributes to developing gen-
videos and text could be challenging. Therefore, we explore the
eralizableIMUmodelsbyintroducingahighlyadaptablepretraining
possibility of training an effective IMU encoder using only a small
strategy. By open-sourcing our framework, we aim to encourage the
portionofthedatathatincludesalignedvideoandtext.Specifically,
community to further build upon our efforts.
we remove the aligned video and text data for different fractions of
ycaruccA
tseT
ycaruccA
tseT
ccA
tseT
maertsnwoD
ycaruccA
tseTREFERENCES [17] B.Elizalde,S.Deshmukh,M.A.Ismail,andH.Wang,“Clap:Learning
audio concepts from natural language supervision,” 2022. [Online].
Available:https://arxiv.org/abs/2206.04769
[1] Y.Liang,X.Zhou,Z.Yu,andB.Guo,“Energy-efficientmotionrelated
[18] Y. Jain, C. I. Tang, C. Min, F. Kawsar, and A. Mathur, “Collossl:
activityrecognitiononmobiledevicesforpervasivehealthcare,”Mobile
Collaborative self-supervised learning for human activity recognition,”
NetworksandApplications,vol.19,pp.303–317,2014.
ProceedingsoftheACMonInteractive,Mobile,WearableandUbiqui-
[2] H.Ghayvat,J.Liu,S.C.Mukhopadhyay,andX.Gui,“Wellnesssensor
tousTechnologies,vol.6,no.1,pp.1–28,2022.
networks: A proposal and implementation for smart home for assisted
[19] Y.Song,S.Xia,J.Yang,andL.Pei,“Alearning-basedmulti-nodefusion
living,”IEEESensorsJournal,vol.15,no.12,pp.7341–7348,2015.
positioning method using wearable inertial sensors,” in ICASSP 2024-
[3] K.K.Rachuri,M.Musolesi,C.Mascolo,P.J.Rentfrow,C.Longworth,
2024 IEEE International Conference on Acoustics, Speech and Signal
and A. Aucinas, “Emotionsense: a mobile phones based adaptive plat-
Processing(ICASSP). IEEE,2024,pp.1976–1980.
form for experimental social psychology research,” in Proceedings of
[20] S. Deldari, D. Spathis, M. Malekzadeh, F. Kawsar, F. D. Salim, and
the12thACMinternationalconferenceonUbiquitouscomputing,2010,
A.Mathur,“Crossl:Cross-modalself-supervisedlearningfortime-series
pp.281–290.
throughlatentmasking,”inProceedingsofthe17thACMInternational
[4] S. Zhang, E. Nemati, M. Dinh, N. Folkman, T. Ahmed, M. Rahman, ConferenceonWebSearchandDataMining,2024,pp.152–160.
J.Kuang,N.Alshurafa,andA.Gao,“Coughtrigger:Earbudsimubased [21] S. Moon, A. Madotto, Z. Lin, A. Dirafzoon, A. Saraf, A. Bearman,
coughdetectionactivatorusinganenergy-efficientsensitivity-prioritized and B. Damavandi, “Imu2clip: Multimodal contrastive learning for
time series classifier,” in ICASSP 2022-2022 IEEE International Con- imu motion sensors from egocentric videos and text,” 2022. [Online].
ferenceonAcoustics,SpeechandSignalProcessing(ICASSP). IEEE, Available:https://arxiv.org/abs/2210.14395
2022,pp.1–5. [22] N.Mu,A.Kirillov,D.Wagner,andS.Xie,“Slip:Self-supervisionmeets
[5] A. G. Pacheco, F. A. Cabello, A. M. Fonoff, P. G. Rodrigues, O. A. language-imagepre-training,”arXivpreprintarXiv:2112.12750,2021.
Penatti,andP.R.Pinto,“Towardslow-powerheartrateestimationbased [23] Y. Li, F. Liang, L. Zhao, Y. Cui, W. Ouyang, J. Shao, F. Yu,
on user’s demographics and activity level for wearables,” in ICASSP and J. Yan, “Supervision exists everywhere: A data efficient
2023-2023 IEEE International Conference on Acoustics, Speech and contrastive language-image pre-training paradigm,” in International
SignalProcessing(ICASSP). IEEE,2023,pp.1–5. Conference on Learning Representations, 2022. [Online]. Available:
[6] R.Chen,F.Jankovic,N.Marinsek,L.Foschini,L.Kourtis,A.Signorini, https://openreview.net/forum?id=zq1iJkNk3uN
M.Pugh,J.Shen,R.Yaari,V.Maljkovicetal.,“Developingmeasuresof [24] S. Verma, G. Bhatt, A. Schwarzschild, S. Singhal, A. M. Das,
cognitiveimpairmentintherealworldfromconsumer-grademultimodal C.Shah,J.P.Dickerson,andJ.Bilmes,“Effectivebackdoormitigation
sensorstreams,”inProceedingsofthe25thACMSIGKDDinternational depends on the pre-training objective,” 2023. [Online]. Available:
conference on knowledge discovery & data mining, 2019, pp. 2145– https://arxiv.org/abs/2311.14948
2155. [25] K. Grauman, A. Westbury, L. Torresani, K. Kitani, J. Malik,
[7] H. Yuan, S. Chan, A. P. Creagh, C. Tong, A. Acquah, D. A. Clifton, T. Afouras, K. Ashutosh, V. Baiyya, S. Bansal, B. Boote et al.,
andA.Doherty,“Self-supervisedlearningforhumanactivityrecognition “Ego-exo4d: Understanding skilled human activity from first- and
using 700,000 person-days of wearable data,” NPJ digital medicine, third-person perspectives,” 2024. [Online]. Available: https://arxiv.org/
vol.7,no.1,p.91,2024. abs/2311.18259
[8] A. Saeed, T. Ozcelebi, and J. Lukkien, “Multi-task self-supervised [26] W. Lu, J. Wang, and Y. Chen, “Local and global alignments for gen-
learning for human activity detection,” Proceedings of the ACM on eralizable sensor-based human activity recognition,” in ICASSP 2022-
Interactive,Mobile,WearableandUbiquitousTechnologies,vol.3,no.2, 2022 IEEE International Conference on Acoustics, Speech and Signal
pp.1–30,2019. Processing(ICASSP). IEEE,2022,pp.3833–3837.
[9] C. I. Tang, I. Perez-Pozuelo, D. Spathis, S. Brage, N. Wareham, and [27] T.Chen,S.Kornblith,M.Norouzi,andG.Hinton,“Asimpleframework
C. Mascolo, “Selfhar: Improving human activity recognition through forcontrastivelearningofvisualrepresentations,”inProceedingsofthe
self-trainingwithunlabeleddata,”Proc.ACMInteract.Mob.Wearable 37th International Conference on Machine Learning, ser. Proceedings
Ubiquitous Technol., vol. 5, no. 1, Mar. 2021. [Online]. Available: of Machine Learning Research, H. D. III and A. Singh, Eds., vol.
https://doi.org/10.1145/3448112 119. PMLR, 13–18 Jul 2020, pp. 1597–1607. [Online]. Available:
[10] C. I. Tang, I. Perez-Pozuelo, D. Spathis, and C. Mascolo, “Exploring https://proceedings.mlr.press/v119/chen20j.html
contrastivelearninginhumanactivityrecognitionforhealthcare,”2021. [28] K.Grauman,A.Westbury,E.Byrne,Z.Chavis,A.Furnari,R.Girdhar,
[Online].Available:https://arxiv.org/abs/2011.11542 J. Hamburger, H. Jiang, M. Liu, X. Liu et al., “Ego4d: Around the
[11] C. Xu, Y. Li, D. Lee, D. H. Park, H. Mao, H. Do, J. Chung, and world in 3,000 hours of egocentric video,” 2022. [Online]. Available:
D.Nair,“Augmentationrobustself-supervisedlearningforhumanactiv- https://arxiv.org/abs/2110.07058
ity recognition,” in ICASSP 2023-2023 IEEE International Conference [29] T. Sztyler and H. Stuckenschmidt, “On-body localization of wearable
on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, devices:Aninvestigationofposition-awareactivityrecognition,”in2016
pp.1–5. IEEEinternationalconferenceonpervasivecomputingandcommunica-
[12] H.Haresamudram,A.Beedu,V.Agrawal,P.L.Grady,I.Essa,J.Hoff- tions(PerCom). IEEE,2016,pp.1–9.
man, and T. Plo¨tz, “Masked reconstruction based self-supervision for [30] D.Dwibedi,Y.Aytar,J.Tompson,P.Sermanet,andA.Zisserman,“With
humanactivityrecognition,”inProceedingsofthe2020ACMInterna- a little help from my friends: Nearest-neighbor contrastive learning of
tionalSymposiumonWearableComputers,2020,pp.45–49. visual representations,” in Proceedings of the IEEE/CVF International
ConferenceonComputerVision(ICCV),October2021,pp.9588–9597.
[13] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
[31] T. B. Moeslund, A. Hilton, and V. Kru¨ger, “A survey of advances in
G.Sastry,A.Askell,P.Mishkin,J.Clark,G.Krueger,andI.Sutskever,
vision-basedhumanmotioncaptureandanalysis,”Computervisionand
“Learningtransferablevisualmodelsfromnaturallanguagesupervision,”
imageunderstanding,vol.104,no.2-3,pp.90–126,2006.
in Proceedings of the 38th International Conference on Machine
Learning, ser. Proceedings of Machine Learning Research, M. Meila
andT.Zhang,Eds.,vol.139. PMLR,18–24Jul2021,pp.8748–8763.
[Online].Available:https://proceedings.mlr.press/v139/radford21a.html
[14] J. Li, D. Li, C. Xiong, and S. C. H. Hoi, “BLIP: bootstrapping
language-image pre-training for unified vision-language understanding
andgeneration,”CoRR,vol.abs/2201.12086,2022.[Online].Available:
https://arxiv.org/abs/2201.12086
[15] H.-H. Wu, P. Seetharaman, K. Kumar, and J. P. Bello, “Wav2clip:
Learning robust audio representations from clip,” in ICASSP 2022 -
2022 IEEE International Conference on Acoustics, Speech and Signal
Processing(ICASSP),2022,pp.4563–4567.
[16] H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li,
“CLIP4Clip: An empirical study of clip for end to end video clip
retrieval,”arXivpreprintarXiv:2104.08860,2021.