ON THE LINEAR SPEEDUP OF PERSONALIZED FED-
ERATED REINFORCEMENT LEARNING WITH SHARED
REPRESENTATIONS
GuojunXiong1∗, ShufanWang2,DanielJiang3,JianLi2
1HarvardUniversity,2StonyBrookUniversity,3AppliedReinforcementLearningTeam,AIatMeta
ABSTRACT
Federated reinforcement learning (FedRL) enables multiple agents to collabora-
tivelylearnapolicywithoutsharingtheirlocaltrajectoriescollectedduringagent-
environment interactions. However, in practice, the environments faced by dif-
ferentagentsareoftenheterogeneous,leadingtopoorperformancebythesingle
policylearnedbyexistingFedRLalgorithmsonindividualagents. Inthispaper,
wetakeafurtherstepandintroduceapersonalizedFedRLframework(PFEDRL)
bytakingadvantageofpossiblysharedcommonstructureamongagentsinhetero-
geneous environments. Specifically, we develop a class of PFEDRL algorithms
named PFEDRL-REP that learns (1) a shared feature representation collabora-
tively among all agents, and (2) an agent-specific weight vector personalized to
its local environment. We analyze the convergence of PFEDTD-REP, a particu-
lar instance of the framework with temporal difference (TD) learning and linear
representations. To the best of our knowledge, we are the first to prove a linear
convergencespeedupwithrespecttothenumberofagentsinthePFEDRLsetting.
Toachievethis,weshowthatPFEDTD-REPisanexampleofthefederatedtwo-
timescale stochastic approximation with Markovian noise. Experimental results
demonstrate that PFEDTD-REP, along with an extension to the control setting
based on deep Q-networks (DQN), not only improve learning in heterogeneous
settings,butalsoprovidebettergeneralizationtonewenvironments.
1 INTRODUCTION
Federated reinforcement learning (FedRL) (Nadiger et al., 2019; Liu et al., 2019; Xu et al., 2021;
Zhang et al., 2022a; Jin et al., 2022; Khodadadian et al., 2022; Yuan et al., 2023; Salgia & Chi,
2024; Woo et al., 2024; Zheng et al., 2024; Lan et al., 2024) has recently emerged as a promising
framework that blends the distributed nature of federated learning (FL) (McMahan et al., 2017)
withreinforcementlearning’s(RL)abilitytomakesequentialdecisionsovertime(Sutton&Barto,
2018). In FedRL, multiple agents collaboratively learn a single policy without sharing individual
trajectoriesthatarecollectedduringagent-environmentinteractions,protectingeachagent’sprivacy.
OnekeychallengefacingFedRLisenvironmentheterogeneity,wherethecollectedtrajectoriesmay
vary to a large extent across agents. To illustrate, consider a few existing applications of FL: on-
deviceNLPapplications(e.g., nextwordprediction, sentencecompletion, webquerysuggestions,
andspeechrecognition)fromInternetcompanies(Hardetal.,2018;Yangetal.,2018;Wangetal.,
2023b),on-devicerecommenderoradpredictionsystems(Maengetal.,2022;Kricheneetal.,2023),
andInternetofThingsapplicationslikesmarthealthcareorsmartthermostats(Nguyenetal.,2021;
Imteajetal.,2022;Zhangetal.,2022b;Boubouhetal.,2023). Notethatalloftheabove(1)existin
settingswithenvironmentheterogeneity(heterogeneoususers,devices,patients,orhomes)and(2)
couldpotentiallybenefitfromanRLproblemformulation.
Asaresult,ifallagentscollaborativelylearnasinglepolicy,whichmostexistingFedRLframeworks
do, the learned policy might perform poorly on individual agents. This calls for the design of
a personalized FedRL (PFEDRL) framework that can provide personalized policies for agents in
differentenvironments. Nevertheless,despitetherecentadvancesinFedRL,thedesignofPFEDRL
∗ThisworkwasdonewhenG.XiongwasaPhDstudentatStonyBrookUniversity.
1
4202
voN
22
]GL.sc[
1v41051.1142:viXraTable1: ComparisonofexistingFedRLframeworksintermsofnoise; environments(Homo: Ho-
mogeneous,Hetero:Heterogeneous);usingrepresentations(Rep.L)ornot;timescale(TS),singleor
two-TS(two)updates;multiplelocalupdatesornot;personalizationacrossagentsornot;andwith
orwithoutlinearconvergencespeedupguarantee.
Method Noise Env. Rep.L TS Localupdates Personalized Linearspeedup
FedTD&FedQ(Khodadadianetal.,2022) Markov Homo. ✗ Single ✓ ✗ ✓
FedTD(DalFabbroetal.,2023) Markov Homo. ✗ Single ✗ ✗ ✓
FedTD(Wangetal.,2023a) Markov Hetero. ✗ Single ✓ ✗ ✓
QAvg&PAvg(Jinetal.,2022) i.i.d. Hetero. ✗ Single ✗ ✗ ✗
FedQ(Wooetal.,2023) Markov Hetero. ✗ Single ✓ ✗ ✓
A3C(Shenetal.,2023) Markov Homo. ✗ Two ✗ ✗ ✓
FedSARSA(Zhangetal.,2024) Markov Hetero. ✗ Single ✓ ✗ ✓
PFEDRL-REP Markov Hetero. ✓ Two ✓ ✓ ✓
anditsperformanceanalysisremains,toalargeextent,anopenquestion. Motivatedbythis,thefirst
inquiryweaimtoanswerinthispaperis:
Can we design a PFEDRL framework for agents in heterogeneous environments that not
only collaboratively learns a useful global model without sharing local trajectories, but
alsolearnsapersonalizedpolicyforeachagent?
WeaddressthisquestionbyviewingthePFEDRLprobleminheterogeneousenvironmentsasN par-
allelRLtaskswithpossiblysharedcommonstructure.Thisisinspiredbyobservationsincentralized
learning(Bengioetal.,2013;LeCunetal.,2015)andfederatedordecentralizedlearning(Collins
et al., 2021; Tziotis et al., 2023; Xiong et al., 2024), where leveraging shared (low-dimensional)
representationscanimproveperformance. Atheoreticalunderstandingofusingsharedrepresenta-
tionsamongstheterogeneousagentshasreceivedrecentemphasisinthestandardsupervisedFL(or
decentralizedlearning)setting(Collinsetal.,2021;Tziotisetal.,2023;Xiongetal.,2024).
However, a theoretical analysis of PFEDRL with shared representations is more subtle because
eachagentin PFEDRL collectsdatabyfollowingitsownpolicy(therebygeneratingaMarkovian
trajectory)andsimultaneouslyupdatesitsmodelparameters.Thisisinstarkcontrasttothestandard
FLparadigm,wheredataistypicallycollectedinani.i.d. fashion. Oursecondresearchquestionis:
How do the shared representations affect the convergence of PFEDRL under Markovian
noise,andisitpossibletoachieveanN-foldlinearconvergencespeedup?
DespitetherecentprogressinthestandardsupervisedFLsetting(Collinsetal.,2021;Tziotisetal.,
2023;Xiongetal.,2024),tothebestofourknowledge,thisquestionisstillopeninthecontextof
learningpersonalizedpoliciesinFedRLunderMarkoviannoise(seeTable1). Motivatedbythese
openquestions,ourmaincontributionsare:
•PFEDRL-REPframework. WeproposePFEDRL-REP,anewPFEDRLframeworkwithshared
representations. PFEDRL-REPlearnsaglobalsharedfeaturerepresentationcollaborativelyamong
agentsthroughtheaidofacentralserver,alongwithagent-specificparametersforpersonalizingto
eachagent’slocalenvironment. The PFEDRL-REP frameworkcanbepairedwithawiderangeof
RLalgorithms, includingbothvalue-basedandpolicy-basedmethodswitharbitraryfeaturerepre-
sentations.
• Linear speedup for TD learning. We then introduce PFEDTD-REP, an instantiation of the
above PFEDRL-REP framework for TD learning (Sutton & Barto, 2018). We analyze its con-
vergence in a linear representation setting, proving the convergence rate of PFEDTD-REP to be
O˜(cid:0) N−2/3(T +2)−2/3(cid:1) ,whereN isthenumberofagentsandT isthenumberofcommunication
rounds. This implies a linear convergence speedup for PFEDTD-REP with respect to the number
ofagents,ahighlydesirablepropertythatallowsformassiveparallelisminlarge-scalesystems. To
ourknowledge,thisisthefirstlinearspeedupresultforPFEDRLwithsharedrepresentationsunder
Markoviannoise,providingatheoreticalanswertotheempiricalobservationsinMnihetal.(2016)
thatfederatedversionsofRLalgorithmsyieldfasterconvergence. Toshowthisresult,wemakeuse
of two-timescale stochastic approximation theory and address the challenges of Markovian noise
throughaLyapunovdriftapproach.
22 PROBLEM FORMULATION
In this section, we first review the standard FedRL framework and then introduce our proposed
PFEDRL-REP framework, which incorporates personalization and shared representations. Let N
and T be thenumber of agentsand communicationrounds, respectively. Denote [N] as theset of
integers{1,...,N}and∥·∥asthel -norm. Weuseboldfacetodenotematricesandvectors.
2
2.1 PRELIMINARIES: FEDERATEDREINFORCEMENTLEARNING
A FedRL system with N agents interacting with N independent heterogeneous environments is
modeledasfollows. Theenvironmentofagenti∈[N]isaMarkovdecisionprocess(MDP)Mi =
⟨S,A,Ri,Pi,γ⟩,whereS andAarefinitestateandactionsets,Riistherewardfunction,Piisthe
transition kernel, and γ ∈ (0,1) is the discount factor. Suppose agent i is equipped with a policy
πi : S → ∆(A)(amappingfromstatestoprobabilitydistributionsoverA). Ateachtimestepk,
agent i is in state si and takes action ai according πi(·|si), resulting in reward Ri(si,ai). The
k k k k k
environmentthentransitionstoanewstatesi accordingtoPi(·|si,ai). Thissequenceofstates
k+1 k k
andactionsformsaMarkovchain,thesourceoftheaforementionedMarkoviannoise. Inthispaper,
thisMarkovchainisassumedtobeunichain,whichisknowntoasymptoticallyconvergetoasteady
state. Wedenotethestationarydistributionasµi,πi.
Thevalueofπi inenvironmentMi isdefinedasVi,πi(s)=E (cid:2)(cid:80)∞ γkRi(si,ai)|si =s(cid:3) . In
πi k=0 k k 0
realisticproblemswithlargestatespaces,itisinfeasibletostoreVi,πi(s)forallstates,sofunction
approximationisoftenused.OneexampleisVi,πi(s)≈ΦΦΦ(s)θθθ,whereΦΦΦ∈R|S|×disastatefeature
representationandθθθ ∈Rdisanunknownlow-dimensionalweightvector.
One intermediate goal in RL is to estimate the value function corresponding to a policy π using
trajectories collected from the environment. This task is called policy evaluation, and one widely
used approach is temporal difference (TD) learning (Sutton, 1988). The FedRL version of TD
learning is called FedTD (Khodadadian et al., 2022; Dal Fabbro et al., 2023; Wang et al., 2023a),
whereN agentscollaborativelyevaluateasinglepolicyπbylearningacommon(non-personalized)
weight vector θθθ, using trajectories collected from N different environments. More precisely, we
haveπi ≡πandθθθi ≡θθθ,∀i∈[N]. GivenafeaturerepresentationΦΦΦ(s),∀s,thiscanbeformulated
asthefollowingoptimizationproblem:
N
m θθθin N1 (cid:88) E s∼µi,π(cid:13) (cid:13)ΦΦΦ(s)θθθ−Vi,π(s)(cid:13) (cid:13)2 . (1)
i=1
Due to space constraints, we focus our presentation on the policy evaluation problem. Note that
policy evaluation is an important part of RL and control, since it is a critical step for methods
basedonpolicyimprovement. OurproposedPFEDRLframework(seeAlgorithm1)canbedirectly
appliedtocontrolproblemsaswell,butwerelegatethesediscussionstoSection5andAppendixC.
2.2 PERSONALIZEDFEDRLWITHSHAREDREPRESENTATIONS
Since the local environments are heterogeneous across the N agents, the aforementioned FedRL
methods (in Section 2.1) that aim to learn a common weight vectorθθθ may perform poorly on in-
dividual agents. This necessitates the search for personalized local weight vectorsθθθi that can be
learned collaboratively among N agents in N heterogeneous environments (without sharing their
locally collected trajectories). As alluded to earlier, we view the personalized FedRL (PFEDRL)
problem as N parallel RL tasks with possibly shared common structure, and we propose that the
agents collaboratively learn a common features representationΦΦΦ in addition to a personalized lo-
cal weights θθθi. Specifically, the value function of agent i is approximated as Vi,πi ≈ fi(θθθi,ΦΦΦ),
where fi(·,·) is a general function parameterized by these two unknown parameters.1 The policy
1Theapproximationfi(θθθi,ΦΦΦ)isgeneralandcantakeonvariousforms,includingaslinearapproximations
orneuralnetworks. Forinstance,itcanberepresentedasalinearcombinationofΦΦΦandθθθi,i.e.,fi(θθθi,ΦΦΦ) :=
ΦΦΦθθθi inTDwithlinearfunctionapproximation(Bhandarietal.,2018). Inaddition,fi(θθθi,ΦΦΦ)canrepresenta
deepneuralnetwork;see,e.g.,ourextensionofPFEDRLtocontrolproblemsanditsinstantiationwithDQN
(deepQ-networks)(Mnihetal.,2015)inSection5andAppendixC.
3Algorithm1PFEDRL-REP: AGeneralDescription
Input: Samplingpolicyπi,∀i∈[N];
1: InitializetheglobalfeaturerepresentationΦΦΦ 0andlocalweightvectorθθθi 0,∀i∈[N]randomly;
2: forroundt=0,1,...,T −1do
3: foragent1,...,N do
4: θθθi
t+1
= WEIGHT UPDATE(ΦΦΦ t,θθθi t,α t,K);
5: ΦΦΦi
t+1/2
= FEATURE UPDATE(ΦΦΦ t,θθθi t+1,β t);
6: endfor
7: ServercomputesthenewglobalfeaturerepresentationΦΦΦ t+1 = N1 (cid:80)N i=1ΦΦΦi t+1/2.
8: endfor
evaluationproblemof(1)canbeupdatedforthisnewsettingas:
min
1 (cid:88)N
min E
(cid:13)
(cid:13)fi(θθθi,ΦΦΦ(s))−Vi,πi
(s)(cid:13) (cid:13)2
, (2)
ΦΦΦ N {θθθi,∀i} s∼µi,πi (cid:13) (cid:13)
i=1
where N agents collaboratively learn a shared feature representationΦΦΦ via a server, along with a
personalizedlocalweightvector{θθθi,∀i}usinglocaltrajectoriesateachagent.
Remark2.1. ThelearningofasharedfeaturerepresentationΦΦΦinPFEDRLisrelatedtoideasfrom
representation learning theory (Agarwal et al., 2020; 2023), and this is believed to achieve better
generalizationperformancewithrelativelysmalltrainingdata. InconventionalFedRL,thefeature
representationΦΦΦisgivenandfixed. Indeed,aswenumericallyverifyinSection4.3,our PFEDRL
presentsbettergeneralizationperformancetonewenvironments.
3 PFEDRL-REP ALGORITHMS
Wenowproposeaclassofalgorithmscalled PFEDRL-REP thatrealize PFEDRL withsharedrep-
resentations. PFEDRL-REPalternatescomprisesofthreemainstepsforeachagentateachcommu-
nicationround: (1)alocalweightvectorupdate;(2)alocalfeaturerepresentationupdate;and(3)a
globalfeaturerepresentationupdateviatheserver.
Steps1and2: Localweightandfeaturerepresentationupdates. Atroundt,agentiperformsan
updateonitslocalweightvectorgivenitscurrentglobalfeaturerepresentationΦΦΦ andlocalweight
t
vectorθθθi.WealloweachagenttoperformKstepsoflocalweightvectorupdates.Oncetheupdated
t
local weight vectorθθθi is obtained, each agent i executes a one-step local update on its feature
t+1
representationtoobtainΦΦΦi . Werepresenttheseupdatesusingthefollowinggenericnotation:
t+1/2
θθθi
t+1
= WEIGHT UPDATE(ΦΦΦ t,θθθi t,α t,K) and ΦΦΦi
t+1/2
= FEATURE UPDATE(ΦΦΦ t,θθθi t+1,β t), (3)
where α and β are learning rates for the weight and feature updates, respectively. The generic
t t
functions WEIGHT UPDATE and FEATURE UPDATE willbespecializedtotheparticularsoftheun-
derlyingRLalgorithm: inSection4wediscussthecaseofTDwithlinearfunctionapproximation
indetail,andinAppendixC,weshowinstantiationsofQ-learningandDQNinourframework.
Step3: Server-basedglobalfeaturerepresentationupdate.
N
The server computes an average of the received local feature 1 (cid:88)
ΦΦΦ = ΦΦΦi . (4)
representation updates ΦΦΦi from all agents to obtain the t+1 N t+1/2
t+1/2
i=1
nextglobalfeaturerepresentationΦΦΦ asin(4).
t+1
The PFEDRL-REP procedurerepeats(3)and(4)andissummarizedinAlgorithm1andFigure1.
We emphasize that because PFEDRL-REP operates in an RL setting, there is no ground truth for
the value function and learning occurs through interactions with an MDP environment, resulting
in non-i.i.d. data. In contrast, in the standard FL setting (where shared representations have been
investigated), there exists a known ground truth and training data are sampled in an i.i.d. fashion
(Collinsetal.,2021;Tziotisetal.,2023;Xiongetal.,2024). Thenon-i.i.d. (Markovian)dataisthe
maintechnicalchallengethatweneedtoovercome.
4Server Server Server Φ𝑡+1=1 3෍3 Φ𝑡𝑖
+1/2
Φ𝑡+1 Φ1 𝑡+Φ 1/𝑡2 2+1/2 Φ𝑡 Φ+1 𝑡3 +1/2Φ𝑡+1 𝑖=1
Agent 1 Agent 2 Agent 3 Agent 1 Agent 2 Agent 3 Agent 1 Agent 2 Agent 3
(Φ𝑡,𝛉1 𝑡) (Φ𝑡,𝛉𝑡2) (Φ𝑡,𝛉𝑡3) (Φ𝑡,𝛉1 𝑡+1) (Φ𝑡,𝛉𝑡2 +1) (Φ𝑡,𝛉𝑡3 +1) (Φ𝑡+1,𝛉1 𝑡+1) (Φ𝑡+1,𝛉𝑡2 +1) (Φt+1,𝛉𝑡3 +1)
Env. 1 Env. 2 Env. 3 Env. 1 Env. 2 Env. 3 Env. 1 Env. 2 Env. 3
At the beginning of round t Local Weight Update Local and Global Representation Update
Figure 1: An illustrative example of PFEDRL-REP for 3 agents. (a) At the beginning of round
t, each agent i = 1,2,3 has a local weight vectorθθθi and a global feature representationΦΦΦ . (b)
t t
Using(ΦΦΦ ,θθθi),eachagentiperformsaK-stepupdatetoobtainθθθi asin(3). NotethatΦΦΦ remains
t t t+1 t
unchangedatthisstep.(c)Agentiupdatesthefeaturerepresentationbyexecutingaone-stepupdate
toobtainΦΦΦi asin(3),whichdependsonbothθθθi andΦΦΦ . Finally,eachagentisharesΦΦΦi
t+1/2 t+1 t t+1/2
withtheserver,whichthenexecutesanaveragingstepasin(4)toproduceΦΦΦ .Updatedparameters
t+1
arehighlightedinred,whilesharedparameters(theglobalfeaturerepresentation)areinblue.
4 PFEDTD-REP WITH LINEAR REPRESENTATION
We present PFEDTD-REP, an instance of PFEDRL-REP paired with TD learning and analyze its
convergenceinalinearrepresentationsetting.
4.1 PFEDTD-REP: ALGORITHMDESCRIPTION
Here,thegoalofN agentsistocollaborativelysolveproblem(2)whentheunderlyingRLalgorithm
is TD learning. We first need to specify WEIGHT UPDATE and FEATURE UPDATE of Algorithm 1
forthecaseofTD.Attimestepk,thestateofagentiissi,anditsvaluefunctioncanbedenoted
k
as V(si) = ΦΦΦ(si)θθθi in a linear representation setting. By the standard one-step Monte Carlo
k k
approximationusedinTD,wecomputeVˆ(si)=ri +γΦΦΦ(si )θθθi. TheTDerrorisdefinedas
k k k+1
δi :=Vˆ(si)−V(si)=ri +γΦΦΦ(si )θθθi−ΦΦΦ(si)θθθi. (5)
k k k k k+1 k
Thegoalofagentiistominimizethefollowinglossfunctionforeverysi ∈S
k
Li(ΦΦΦ(si),θθθi)= 1(cid:13) (cid:13)V(si)−Vˆ(si)(cid:13) (cid:13)2 , (6)
k 2 k k
with Vˆ(si) treated as a constant. We now denote the Markovian observations of agent i at the
k
k-th time step of communication round t as Xi := (si ,ri ,si ). Note that the observa-
t,k t,k t,k t,k+1
tion sequences {Xi ,∀t,k} differ across agents in heterogeneous environments. We assume that
t,k
{Xi ,∀t,k}arestatisticallyindependentacrossallagents.
t,k
Localweightvectorupdate. Asinline4ofAlgorithm1,giventhecurrentglobalfeaturerepresen-
tationΦΦΦ ,eachagentitakesK localupdatestepsonitslocalweightvectorθθθi as
t t
θθθi =θθθi +α g(θθθi ,ΦΦΦ ,Xi ), (7)
t,k t,k−1 t t,k−1 t t,k−1
for k ∈ [K], where g(θθθi ,ΦΦΦ ,Xi ) is the negative stochastic gradient of the loss function
t,k−1 t t,k−1
Li(ΦΦΦ (si ),θθθi )withrespecttoθθθ,giventhecurrentfeaturerepresentationΦΦΦ :
t t,k−1 t,k−1 t
g(θθθi ,ΦΦΦ ,Xi ):=−∇ Li(ΦΦΦ (si ),θθθi )=δi ΦΦΦ (si )⊺ . (8)
t,k−1 t t,k−1 θθθ t t,k−1 t,k−1 t,k−1 t t,k−1
Sincethereare K stepsoflocalupdates, wedenoteθθθi := θθθi . Wefurtheradda norm-scaling
t+1 t,K
(i.e.,clipping)stepfortheupdatedweightvectorsθθθi ,i.e.,enforcing∥θθθi ∥≤B,tostabilizethe
t+1 t+1
update. Thisisessentialforthefinite-timeconvergenceanalysisinSection4.2,andthistechniqueis
widelyusedinconventionalTDlearningwithlinearfunctionapproximation(Bhandarietal.,2018).
Localfeaturerepresentationupdate. Asinline5ofAlgorithm1,giventheupdatedlocalweight
vectorθθθi ,agentithenexecutesaone-steplocalupdateontheglobalfeaturerepresentation:
t+1
ΦΦΦi =ΦΦΦ +β h(θθθi ,ΦΦΦ ,{Xi }K ), (9)
t+1/2 t t t+1 t t,k−1 k=1
5whereh(θθθi ,ΦΦΦ ,{Xi }K )isthenegativestochasticgradientofthelossLi(ΦΦΦ (si ),θθθi )
t+1 t t,k−1 k=1 t t,k−1 t+1
withrespecttothecurrentglobalfeaturerepresentationΦΦΦ ,satisfying
t
⊺
h(θθθi ,ΦΦΦ ,Xi ):=−∇ Li(ΦΦΦ (si ),θθθi )=δi θθθi . (10)
t+1 t t,k−1 ΦΦΦ t t,k−1 t+1 t,k−1 t+1
Server-basedglobalfeaturerepresentationupdate. Asinline7ofAlgorithm1,theserverthen
averages the received local feature representation updates in (9) to obtain the next global feature
representation:
N
1 (cid:88)
ΦΦΦ =ΦΦΦ +β · h(θθθj ,ΦΦΦ ,{Xi }K ). (11)
t+1 t t N t+1 t t,k−1 k=1
j=1
Thefullpseudo-codeofPFEDTD-REPisgiveninAppendixB.
4.2 CONVERGENCEANALYSIS
Thecoupledupdatesin(7)and(11)canbeviewedasafederatednonlineartwo-timescalestochastic
approximation(2TSA)(Doan,2021)withMarkoviannoise,withθθθi updatingonafastertimescale
t
andΦΦΦ on a slower timescale. We aim to establish the finite-time convergence rate of the 2TSA
t
coupledupdates(7)and(11). Thisisequivalenttofindingasolutionpair(ΦΦΦ∗,{θθθi,∗,∀i})suchthat2
E [g(θθθi,∗,ΦΦΦ∗,Xi)]=0 and E [h(θθθi,∗,ΦΦΦ∗,Xi)]=0 (12)
si∼µi,si ∼Pi (·|si) t si∼µi,si ∼Pi (·|si) t
t t+1 πi t t t+1 πi t
holdforallMarkovianobservationsXi. Here,µi istheunknownstationarydistributionofstatesi
t t
ofagentiatt,andPi isthetransitionkernelofagentiunderpolicyπi.
πi
Althoughtheroot(ΦΦΦ∗,{θθθi,∗,∀i})ofthenonlinear2TSAin(7)and(11)isnotuniqueduetosimple
permutations (rotations), it is proved in Tsitsiklis & Van Roy (1996) that the standard TD iterates
convergeasymptoticallytoavectorθθθ∗givenafixedfeaturerepresentationΦΦΦalmostsurely,whereθθθ∗
istheuniquesolutionofacertainprojectedBellmanequation. Hence,foragenti,inordertostudy
the stability ofθθθi when the feature representationΦΦΦ is fixed, we note that there exists a mapping
θθθi =yi(ΦΦΦ)thatmapsΦΦΦtotheuniquesolutionofE [g(θθθi,ΦΦΦ,Xi)]=0.
si∼µi,si ∼Pi (·|si) t
t t+1 πi t
Inspired by Doan (2020), the finite-time analysis of a 2TSA boils down to the choice of two step
sizes {α ,β ,∀t} and a Lyapunov function that couples the two iterates in (7) and (11). We first
t t
definethefollowingtwoerrorterms:
ΦΦΦ˜ =ΦΦΦ −ΦΦΦ∗ and θθ˜θi =θθθi−yi(ΦΦΦ ), ∀i∈[N], (13)
t t t t t
which together characterizes the coupling between {θθθi ,∀i} andΦΦΦ . If {θθ˜θi ,∀i} andΦΦΦ˜ go to
t+1 t t+1 t
zero simultaneously, the convergence of ({θθθi ,∀i},ΦΦΦ ) to ({θθθi,∗,∀i},ΦΦΦ∗) can be established.
t+1 t
Thus, to prove the convergence of ({θθθi ,∀i},ΦΦΦ ) of the 2TSA in (7) and (11) to its true value
t+1 t
(ΦΦΦ∗,{θθθi,∗,∀i}), we define the following weighted Lyapunov function to explicitly couple the fast
andslowiterates
N
M({θθθi ,∀i},ΦΦΦ ):=∥ΦΦΦ −ΦΦΦ∗∥2+ β t−1 1 (cid:88) ∥θθθi −yi(ΦΦΦ )∥2. (14)
t+1 t t α N t+1 t
t
i=1
Remark4.1. NotethattheLyapunovfunction(14)for2TSAdoesnotinherentlyrequirethesolution
tobeunique. Ifmultiplesolutionsorequilibriaexist,theLyapunovfunctionshouldstillbeableto
show that the system will converge to one of these possible equilibria, ensuring that the system’s
statedoesnotdivergeandeventuallystabilizesatsomeequilibriumpoint,whichhighlydependson
the initialization ofΦΦΦ . To clarify this, in the rest of this paper, we useΦΦΦ∗ to clearly indicate the
0 0
dependenceoftheinitializationofΦΦΦ,andΦΦΦ∗ in(14)isinterchangeablewithΦΦΦ∗,whichdenotesthe
0
optimumclosetotheinitialpoint.
2The root (ΦΦΦ∗,{θθθi,∗,∀i}) of the nonlinear 2TSA in (7) and (11) can be established by using the ODE
methodfollowingthesolutionofsuitablydefineddifferentialequations(Doan,2021;2020;Chenetal.,2019)
asin(12).
6Our goal is to characterize the finite-time convergence of E[M({θθθi ,∀i},ΦΦΦ )], the Lyapunov
t+1 t
functionin(14). Westartwithsomestandardassumptionsfirst.
Assumption4.2. Thelearningratesα andβ satisfythefollowingconditions:
(i)(cid:80)∞
α = ∞,
t t t=0 t
(ii)(cid:80)∞ α2 < ∞, (iii)(cid:80)∞ β = ∞, (iv)(cid:80)∞ β2 < ∞, (v)β /α isnon-increasingint, and
t=0 t t=0 t t=0 t t t
(vi)lim β /α =0.
t→∞ t t
Assumption4.3. Agenti’sMarkovchain{Xi}isirreducibleandaperiodic. Hence,thereexistsa
t
unique stationary distribution µi (Levin & Peres, 2017) and constants C > 0 and ρ ∈ (0,1) such
thatd (P(Xi|Xi = x),µi) ≤ Cρk,∀k ≥ 0,x ∈ X,whered (·,·)isthetotal-variation(TV)
TV k 0 TV
distance(Levin&Peres,2017).
Remark4.4. Assumption4.3impliesthattheMarkovchaininducedbyπi admitsauniquestation-
ary distribution µi. This assumption is commonly used in the asymptotic convergence analysis of
stochasticapproximationunderMarkoviannoise(Borkar,2009;Chenetal.,2019).
Wecandefinethesteady-statelocalTDupdatedirectionas
g¯(θθθi,ΦΦΦ):=E [g(θθθi,ΦΦΦ,Xi)],
si∼µi,si ∼Pi (·|si) t
t t+1 πi t
h¯(θθθi,ΦΦΦ):=E [h(θθθi,ΦΦΦ,Xi)]. (15)
si∼µi,si ∼Pi (·|si) t
t t+1 πi t
Definition4.5(Mixingtime,similartoChenetal.(2019)). First,definethediscrepancyterm
ξ (θθθi,ΦΦΦ,x)=max(cid:8) ∥E[g(θθθi,ΦΦΦ,Xi)|X =x]−g¯(θθθi,ΦΦΦ)∥, ∥E[h(θθθi,ΦΦΦ,Xi)|X =x]−h¯(θθθi,ΦΦΦ)∥(cid:9) .
t t 0 t 0
Forδ >0,themixingtimeisdefinedas
τ = maxmin(cid:8) t≥1:ξ (θθθi,ΦΦΦ,x)≤δ(∥ΦΦΦ−ΦΦΦ∗∥+∥θθθi−yi(ΦΦΦ∗)∥+1),∀k ≥t,∀(θθθi,ΦΦΦ,x)(cid:9) ,
δ k
i∈[N]
whichdescribesthetimeittakesforallagents’trajectories(Markovchains)tobewell-represented
bytheirstationarydistributions.
Lemma4.6. g(θθθ,ΦΦΦ,X)in(8)isgloballyLipschitzcontinuous w.r.tθθθ andΦΦΦ uniformlyinX, i.e.,
∥g(θθθ ,ΦΦΦ ,X)−g(θθθ ,ΦΦΦ ,X)∥≤L (∥θθθ −θθθ ∥+∥ΦΦΦ −ΦΦΦ ∥),∀X ∈X.
1 1 2 2 g 1 2 1 2
Lemma4.7. h(θθθ,ΦΦΦ,X)in(10)isgloballyLipschitzcontinuousw.r.tθθθandΦΦΦuniformlyinX,i.e.,
∥h(θθθ ,ΦΦΦ ,X)−h(θθθ ,ΦΦΦ ,X)∥≤L (∥θθθ −θθθ ∥+∥ΦΦΦ −ΦΦΦ ∥),∀X ∈X.
1 1 2 2 h 1 2 1 2
Lemma4.8. yi(ΦΦΦ),∀iisLipschitzcontinuousinΦΦΦ,i.e.,∥yi(ΦΦΦ )−yi(ΦΦΦ )∥≤L ∥ΦΦΦ −ΦΦΦ ∥.
1 2 y 1 2
Fornotationalsimplicity,weletL:=max{L ,L ,L }andassumethatListhecommonLipschitz
g h y
constantinLemmas4.6-4.8inthefollows.
Remark4.9. TheLipschitzcontinuityofhguaranteestheexistenceofasolutionΦΦΦtotheequilib-
rium(12)forafixedθθθ,whiletheLipschitzcontinuityofgandyiensurestheexistenceofasolution
θθθiof(12)whenΦΦΦisfixed.
Lemma4.10. Thereexistsaω >0suchthat∀ΦΦΦ,θθθand∀i:
⟨ΦΦΦ−ΦΦΦ∗,h¯(yi(ΦΦΦ),ΦΦΦ)⟩≤−ω∥ΦΦΦ∗−ΦΦΦ∥2, (cid:10) θθθi−yi(ΦΦΦ ),g¯(θθθi,ΦΦΦ )(cid:11) ≤−ω∥θθθ−yi(ΦΦΦ)∥2.
0 0 t t−1 t t−1
Remark4.11. Lemma4.10guaranteesthestabilityofthetwo-timescaleupdatein(7)and(11),and
can be viewed as the monotone property of nonlinear mappings leveraged in Doan (2020); Chen
etal.(2019).
Lemma 4.12. Under Assumption 4.3, and Lemma 4.6 and 4.7, there exist constants C > 0,
ρ ∈ (0,1) and L = max(L ,L ,max g(θθθ∗,ΦΦΦ∗,X),max h(θθθ∗,ΦΦΦ∗,X)) such that τ ≤
1 g h X X δ
log(1/δ)+log(2L1Cd),andlim δτ =0.
log(1/ρ) δ→0 δ
4.2.1 MAINRESULTS
Wenowpresentourmaintheoreticalresultsinthiswork.
Theorem4.13. LetT ≥2τ forsomeδ >0. Supposethatthelearningratesarechosenas
δ
α =α /(t+2)5/6 and β =β /(t+2),
t 0 t 0
7(cid:112)
whereα ≤1/(2L 2(1+L2)),β ≤ω/2,andL=max{L ,L ,L }. Wehave
0 0 g h y
M({θθθi},ΦΦΦ ) C
M({θθθi },ΦΦΦ )≤ 1 0 + 2
T+2 T+1 (T +2)2 (T +2)2/3
(cid:18) N (cid:19)
+ C 1 E[∥ΦΦΦ −ΦΦΦ∗∥2]+ 1 E(cid:88) ∥θθθi −yi(ΦΦΦ )∥2 , (16)
(T +2)2/3 0 0 N 1 0
i=1
withC =(4α β K2(3δ2(1+B2)+L2B2)+2α2(3K2B2+3K2δ2+2L2K2B2)+8α β δ2)
2 0 0 0 0 0
andC =(144τ2K2L2δ2+4L2/N)α β .
1 δ 0 0
Thefirsttermoftheright-handsideof(16)correspondstothebiasduetoinitialization,whichgoes
tozeroatarateO(1/T2). ThesecondtermisduetothevarianceoftheMarkoviannoise. Thethird
termcorrespondstotheaccumulatedestimationerrorofthetwo-timescaleupdate. Thesecondand
thirdtermsdecayatarateO(1/T2/3),andhencedominatetheoverallconvergenceratein(16).
Remark4.14. Doan(2020)providedthefirstfinite-timeanalysisforgeneralnonlinear2TSAunder
i.i.d noise, and then extended it to the Markovian noise setting under the assumptions that both
g¯ and h¯ functions are monotone in both parameters (Doan, 2021). Since Doan (2021) leverages
the methods from Doan (2020), it needs a detailed characterization of the covariance between the
error induced by Markovian noise and the residual error of the parameters in (13), rendering the
convergence analysis much more intricate. To address this and inspired by the single-timescale
SA (Srikant & Ying, 2019), we use a Lyapunov drift approach to capture the evolution of two
coupled parameters under Markovian noise, and the characterization of impacts of a norm-scaling
stepfurtherdistinguishesourwork.
Corollary4.15. Supposethatβ =o(N−2/3)andthatT2 >N. Then,wehave
0
(cid:18) (cid:19)
1
M({θθθi },ΦΦΦ )≤O .
t+2 t+1 N2/3(T +2)2/3
Remark 4.16.
Corollary4.15indicatesthattoattainanϵaccuracy,ittakesO(cid:0) ϵ−3/2(cid:1)
stepswitha
convergencerateO(cid:0) T−2/3(cid:1) ,whileO(cid:0) N−1ϵ−3/2(cid:1) stepswithaconvergencerateO(cid:0) N−2/3T−2/3(cid:1)
(the hidden constants in O(·) are the same). In this sense, we prove that PFEDTD-REP achieves
a linear convergence speedup with respect to the number of agents N, i.e., we can proportionally
decreaseT asN increaseswhilekeepingthesameconvergencerate. Toourbestknowledge,thisis
thefirstlinearspeedupresultforpersonalizedFedRLwithsharedrepresentationsunderMarkovian
noise,andishighlydesirablesinceitimpliesthatonecanefficientlyleveragethemassiveparallelism
inlarge-scalesystems.Recently,Shenetal.(2023)considereda2TSAinafederatedRLsettingand
achievedaconvergencerateofO(cid:0) T−2/5(cid:1) andthusasamplecomplexityofO(cid:0) ϵ−5/2(cid:1)
. Incontrast,
ourmethodcanconvergequickerandenjoysalowersamplecomplexity,andtheconvergencespeed
matchesthebest-knownconvergencespeedfornon-linear2TSAundereveni.i.dnoise(Doan,2020).
Inaddition,wenotethatsingle-timescale(SA)methodsmayenjoyafasterconvergencespeedanda
lowersamplecomplexity.However,itisknownthatthe2TSAsettingismuchmoreinvolvedthanthe
SAsetting,astherearetwoparameterstobeupdatedinacoupledandasynchronousmanner.Toour
bestknowledge,therearenoexistingworksinthe2TSAsettingsthatachievethesameconvergence
rateorsamplecomplexityasthoseintheSAsettings.Itmaybeaninterestingdirectiontoinvestigate
forthecommunity. Finally,similartoFLsettings(Collinsetal.,2021),thelocalstepdoesnothurt
theglobalconvergencewithaproperlearningratechoice.
Remark4.17. ThegradienttrackingtechniquediscussedinZeng&Doan(2024)couldpotentially
beeffectiveinhandlingMarkoviansamplingandimprovingthecurrentconvergenceratetoO(T−1).
However, it is unsure if it can be applied to PFEDRL-REP, i.e., personalized federated reinforce-
mentlearningsettingwithsharedrepresentations,wheremultipleagentsarecoupledthroughacen-
tral server and each agent performs multiple local updates for their personalized weights. In our
PFEDRL-REP framework, multiple agents are coupled through a central server, and each agent
performsmultiplelocalupdatesfortheirpersonalizedweights,whereasZeng&Doan(2024)only
considers single-agent settings under i.i.d. noise. Furthermore, Zeng & Doan (2024) assumes a
second-order variance bound of the stochastic function, while our analysis does not include such
anassumption. Investigatingtheseisoutofthescopeofthiswork,whichalreadyconsidersavery
challengingsetting.
810 10 10
5 5 5
TD
PFedTD-Rep
FedTD
0 0 0
0 100 200 0 100 200 0 100 200
Episode
(a)Valuefunctionestimates. (b)Convergencespeedofvaluefunctionlearning.
Figure2: ComparisonsinaCliffWalkingEnvironmentwith3agents.
4.2.2 INTUITIONSANDPROOFSKETCH
We highlight the key ideas and challenges behind the convergence rate analysis of PFEDTD-REP
withtwocoupledparameters,whichisanexampleofafederatednonlinear2TSA.Withthedefined
Lyapunovfunctionin(14), thekeyistofindthedriftbetweenM({θθθi ,∀i},ΦΦΦ )inthet-thcom-
t+1 t
municationroundandM({θθθi,∀i},ΦΦΦ )inthe(t−1)-thcommunicationround. Toachievethis,
t t−1
we separately characterize the drift betweenΦΦΦ andΦΦΦ , and the drift betweenθθθi andθθθi,∀i.
t+1 t t+1 t
Weemphasizethethreemainchallengesincharacterizingthedrift: (i)howtoboundthestochastic
gradientwithMarkoviansamples;(ii)howtoleveragemixingtimeτ tohandlethebiasedparame-
terupdatesduetoMarkoviannoise; and(iii)howtodealwithmultiplelocalupdatesforthelocal
weightvectorθθθi.
We introduce the mixing time property of MDPs and thus we have that the gap between the bi-
ased gradient at each time step and the true gradient can be bounded when the time step exceeds
the mixing time τ, as defined in Definition 4.5. To characterize the effect of local updates, the
key philosophy is to bound the gradient at the initial local step and the gradient at the final local
steps, whichcanbedonebyleveragingtheLipschitzpropertyofthosegradientfunctionsinLem-
mas 4.6, 4.7 and 4.8. See Appendix F.1.1 and Appendix F.1.2 for details. Once we establish the
driftoftheLyapunovfunction,theremainingtaskistoselectsuitabledynamictwo-timescalelearn-
ing rates {α ,∀t} and {β ,∀t} for the weight vector update in (7) and the feature representation
t t
updatein(9),respectively. SeeAppendixF.1.3fordetails. Insummary,the2TSAunder PFEDRL
setting with multiple agents, Markovian samples, and multiple local updates, highly differentiates
ourworkfromexistingworks,e.g.,Doan(2020;2021);Srikant&Ying(2019)(seediscussionsin
Remark4.14).
4.3 NUMERICALEVALUATION
We empirically evaluate the performance of PFEDTD-REP. We consider a tabular CliffWalking
environment(Brockmanetal.,2016)witha4×12gridworld,where3agentsevaluate3different
policies. Thedimensionforthefeaturerepresentationandweightvectorissettobe6. Wecompare
PFEDTD-REPwith(i)“TD”:eachagentindependentlyleveragestheconventionalTDwithoutcom-
munication;and(ii)“FedTD”withoutpersonalization(Khodadadianetal.,2022;DalFabbroetal.,
2023) as listed in Table 1. As shown in Figure 2a, PFEDTD-REP ensures personalization among
allagentswhileFedTDtendstoconvergeuniformlyamongallagents. Further, PFEDTD-REP at-
tains values much closer to the ground-truth achieved by TD for each agent compared to FedTD;
andPFEDTD-REPconvergesmuchfasterthanTD.Forinstance,agent1onlyneeds50episodesto
convergeunderPFEDTD-REP,whileittakesmorethan150episodestoconvergeunderTD,asillus-
tratedinFigure2b. TheimprovedconvergenceperformanceofPFEDTD-REPfurthersupportsour
theoreticalfindingsthatleveragingsharedrepresentationsnotonlyprovidespersonalizationamong
agentsinheterogeneousenvironmentsbutyieldfasterconvergence.
5 APPLICATION TO CONTROL PROBLEMS
Inthissection,webrieflydiscusshowourproposedPFEDRL-REPframeworkcanbeappliedtothe
controlproblemsinRL.MoredetailsareprovidedinAppendicesC.3(i.e.,Algorithm4)andG.
9
eulaV
detcepxE
evitageNPFedDQN-Rep DQN FedDQN FedQ-K PFedDQN-Rep DQN FedDQN FedQ-K
LFRL PerDQNAvg FedAsynQ-ImAvg LFRL PerDQNAvg FedAsynQ-ImAvg
2
102
2
102
0
103
0
103
1.5 1.5
nruteR
1
nruteR
1
nruteR-0.5 nruteR-0.5
0.5 0.5
0 0 -1 -1
0 100 200 300 400 500 0 100 200 300 400 500 0 50 100 0 50 100
Episode Episode Episode Episode
(a) Return pole length 0.54 (b) Generalization pole length 0.82 (a) Return pole length 0.3 (b) Generalization pole length 0.36
(a)Cartpoleenvironment. (b)Acrobotenvironment.
Figure3: Comparisonsincontrolproblems.
PFEDDQN-REP(aninstanceofPFEDRL-REPpairedwithDQN)leveragessharedrepresenta-
tionstolearnacommonfeaturespacethatcapturestheunderlyingdynamicsandfeaturesrelevant
acrossdifferentbutrelatedtasksencounteredbyvariousagents. InPFEDDQN-REP,thetargetnet-
workisacriticalcomponentthatprovidesstabilitytothelearningprocessbyservingasarelatively
static benchmark for calculating the loss during training updates (Mnih et al., 2015). The target
network’sarchitecturemirrorsthatofthemainnetwork,includingthesharedrepresentationmodel.
However,itsparametersareupdatedlessfrequently.Thissetupensuresthatthecalculatedtargetval-
ues, whichguidethepolicyupdates, arebasedonaconsistentrepresentationoftheenvironment’s
state,asencodedbythesharedrepresentationmodel. Thesynergybetweenthetargetnetworkand
the representation model is thus central to achieving stable and convergent learning. In Line 13
of Algorithm 4, the algorithm performs a scheduled update of the shared representationΦΦΦ of the
mainnetwork’sparameterswiththeguidanceofthetargetnetwork. InLine18ofAlgorithm4,for
everyT times, thealgorithmperformsascheduledupdateofthetargetnetwork’sparameters
target
by copying over the parameters from the main network. This step is essential for maintaining the
stabilityofthelearningprocess,asitensuresthatthetargetvaluesagainstwhichthepolicyupdates
arecomputedremainconsistentandreflectiveofthemostrecentknowledgeencodedintheshared
representation. The update frequency is carefully chosen to balance learning stability with model
adaptiveness.
NumericalEvaluation. WeconsideramodifiedCartPoleenvironment(Brockmanetal.,2016)by
changingthelengthofpoletocreatedifferentenvironments(Jinetal.,2022). Specifically,wecon-
sider 10 agents with varying pole length from 0.38 to 0.74 with a step size of 0.04. We compare
PFEDDQN-REP with (i) a conventional DQN that each agent learns its own environment inde-
pendently;(ii)afederatedversionDQN(FedDQN)thatallowsallagentstocollaborativelylearna
singlepolicy(withoutpersonalization);(iii)twofederatedalgorithmswithoutpersonalizingFedQ-K
(Khodadadianetal.,2022),LFRL(Liuetal.,2019); and(iv)twopersonalizedalgorithmsPerDQ-
NAvg(Jinetal.,2022)andFedAsynQ-ImAvg(Wooetal.,2023). Werandomlychooseoneagent
andpresentitsperformanceinFigure3a.Again,weobservethatourPFEDDQN-REPobtainslarger
rewardthanbenchmarkswithoutpersonalization,thankstoourpersonalizedpolicy;andachievesthe
maximizedreturnmuchfasterthanexistingpersonlalizedalgorithmsduetoleveragingonlypartial
information among agents. We further evaluate the effectiveness of shared representation learned
by PFEDDQN-REP whengeneralizesittoanewagent. AsshowninFigure3a, PFEDDQN-REP
generalizesquicklytothenewenvironment.SimilarobservationscanbemadefromFigure3busing
Acrobotenvironments(seedetailsinAppendixG).Insummary,thesignificanceofour PFEDRL-
REPframeworkliesinitssuperiorperformanceinheterogeneousenvironmentscomparedtoexisting
algorithmsthatdonotincorporatepersonalization. Additionally,ourPFEDRL-REPframeworkalso
enablesquickadaptationtonew,previouslyunobservedenvironments.
LimitationsandOpenProblems.Inthispaper,wecharacterizethefinite-timeconvergenceratefor
PFEDTD-REP with linear feature representation. However, our analysis is not directly applicable
to the control problems (e.g., PFEDQ-REP, an instance of PFEDRL-REP with Q-learning) since
Q-learning is not a linear operation with respect to the shared representation and local weights.
Additionally,Q-learningistypicallycombinedwithdeepneuralnetworks,wheretheQ-functionis
approximatedbyaneuralnetworkasin PFEDDQN-REP. Itisimportanttonotethatevenforthe
single-agentQ-learningwithneuralnetworkfunctionapproximation,thefinite-timeconvergenceof
neuralweightsisnotwell-studied. Thecomplexityisfurthercompoundedinpersonalizedfederated
RLframeworks,wheremultipleagentsshareacommonrepresentationwhilemaintainpersonalized
localweights.Giventhepromisingexperimentalresultsoncontrol,whetherwecanprovideabound
forPFEDQ-REPeitherwithlinearorneuralfeaturerepresentationsremainsanopenproblem.
10REFERENCES
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural com-
plexityandrepresentationlearningoflowrankmdps. Advancesinneuralinformationprocessing
systems,33:20095–20107,2020.
Alekh Agarwal, Yuda Song, Wen Sun, Kaiwen Wang, Mengdi Wang, and Xuezhou Zhang. Prov-
able benefits of representational transfer in reinforcement learning. In The Thirty Sixth Annual
ConferenceonLearningTheory,pp.2114–2187.PMLR,2023.
ManojGhuhanArivazhagan,VinayAggarwal,AadityaKumarSingh,andSunavChoudhary. Fed-
eratedlearningwithpersonalizationlayers. arXivpreprintarXiv:1912.00818,2019.
YoshuaBengio,AaronCourville,andPascalVincent. Representationlearning: Areviewandnew
perspectives. IEEEtransactionsonpatternanalysisandmachineintelligence,35(8):1798–1828,
2013.
ElHoucineBergou,KonstantinBurlachenko,AritraDutta,andPeterRichta´rik. Personalizedfeder-
atedlearningwithcommunicationcompression. arXivpreprintarXiv:2209.05148,2022.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learningwithlinearfunctionapproximation. InConferenceonlearningtheory,pp.1691–1692.
PMLR,2018.
VivekSBorkar. Stochasticapproximation: adynamicalsystemsviewpoint, volume48. Springer,
2009.
KarimBoubouh,RobertBasmadjian,OmidArdakanian,AlexandreMaurer,andRachidGuerraoui.
Efficacy of temporal and spatial abstraction for training accurate machine learning models: A
casestudyinsmartthermostats. EnergyandBuildings,296:113377,2023.
GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,and
WojciechZaremba. Openaigym. arXivpreprintarXiv:1606.01540,2016.
Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. Federated meta-learning with
fastconvergenceandefficientcommunication. arXivpreprintarXiv:1802.07876,2018.
Zaiwei Chen, Sheng Zhang, Thinh T Doan, Siva Theja Maguluri, and John-Paul Clarke. Perfor-
manceofq-learningwithlinearfunctionapproximation: Stabilityandfinite-timeanalysis. arXiv
preprintarXiv:1905.11425,pp. 4,2019.
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared repre-
sentationsforpersonalizedfederatedlearning. InInternationalconferenceonmachinelearning,
pp.2089–2099.PMLR,2021.
Nicolo` DalFabbro,AritraMitra,andGeorgeJPappas. Federatedtdlearningoverfinite-rateerasure
channels: Linearspeedupundermarkoviansampling. IEEEControlSystemsLetters,2023.
Thinh T Doan. Nonlinear two-time-scale stochastic approximation: Convergence and finite-time
performance. arXivpreprintarXiv:2011.01868,2020.
ThinhTDoan. Finite-timeconvergenceratesofnonlineartwo-time-scalestochasticapproximation
undermarkoviannoise. arXivpreprintarXiv:2104.01627,2021.
AlirezaFallah, AryanMokhtari, andAsumanOzdaglar. Personalizedfederatedlearning: Ameta-
learningapproach. arXivpreprintarXiv:2002.07948,2020.
Xiaofeng Fan, Yining Ma, Zhongxiang Dai, Wei Jing, Cheston Tan, and Bryan Kian Hsiang Low.
Fault-tolerant federated reinforcement learning with theoretical guarantee. Advances in Neural
InformationProcessingSystems,34:1007–1021,2021.
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Franc¸oise Beaufays, Sean
Augenstein,HubertEichner,Chloe´ Kiddon,andDanielRamage. Federatedlearningformobile
keyboardprediction. arXivpreprintarXiv:1811.03604,2018.
11AhmedImteaj,KhandakerMamunAhmed,UrmishThakker,ShiqiangWang,JianLi,andMHadi
Amini. Federatedlearningforresource-constrainedIoTdevices: Panoramasandstateoftheart.
FederatedandTransferLearning,pp.7–27,2022.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learningwithlinearfunctionapproximation. InConferenceonLearningTheory,pp.2137–2143.
PMLR,2020.
Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Federated reinforcement
learning with environment heterogeneity. In International Conference on Artificial Intelligence
andStatistics,pp.18–37.PMLR,2022.
SajadKhodadadian,PranaySharma,GauriJoshi,andSivaThejaMaguluri.Federatedreinforcement
learning: Linear speedup under markovian sampling. In International Conference on Machine
Learning,pp.10997–11057.PMLR,2022.
Walid Krichene, Nicolas Mayoraz, Steffen Rendle, Shuang Song, Abhradeep Thakurta, and
LiZhang. Privatelearningwithpublicfeatures. arXivpreprintarXiv:2310.15454,2023.
GuangchenLan,Dong-JunHan,AbolfazlHashemi,VaneetAggarwal,andChristopherGBrinton.
Asynchronousfederatedreinforcementlearningwithpolicygradientupdates: Algorithmdesign
andconvergenceanalysis. arXivpreprintarXiv:2404.08003,2024.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,
2015.
DavidALevinandYuvalPeres. Markovchainsandmixingtimes,volume107. AmericanMathe-
maticalSoc.,2017.
Boyi Liu, Lujia Wang, and Ming Liu. Lifelong federated reinforcement learning: a learning ar-
chitecturefornavigationincloudroboticsystems. IEEERoboticsandAutomationLetters,4(4):
4555–4562,2019.
KiwanMaeng,HaiyuLu,LucaMelis,JohnNguyen,MikeRabbat,andCarole-JeanWu. Towards
fairfederatedrecommendationlearning: Characterizingtheinter-dependenceofsystemanddata
heterogeneity. InProceedingsofthe16thACMConferenceonRecommenderSystems,pp.156–
167,2022.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficientlearningofdeepnetworksfromdecentralizeddata. InArtificialintelli-
genceandstatistics,pp.1273–1282.PMLR,2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare,AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal.Human-level
controlthroughdeepreinforcementlearning. nature,518(7540):529–533,2015.
VolodymyrMnih,AdriaPuigdomenechBadia,MehdiMirza,AlexGraves,TimothyLillicrap,Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. InInternationalconferenceonmachinelearning,pp.1928–1937.PMLR,2016.
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank mdps. arXiv preprint arXiv:2102.07035,
2021.
Chetan Nadiger, Anil Kumar, and Sherine Abdelhak. Federated reinforcement learning for fast
personalization. In 2019 IEEE Second International Conference on Artificial Intelligence and
KnowledgeEngineering(AIKE),pp.123–127.IEEE,2019.
DinhCNguyen,MingDing,PubuduNPathirana,ArunaSeneviratne,JunLi,andHVincentPoor.
FederatedlearningforInternetofThings: Acomprehensivesurvey. IEEECommunicationsSur-
veys&Tutorials,23(3):1622–1658,2021.
Jiaju Qi, Qihao Zhou, Lei Lei, and Kan Zheng. Federated reinforcement learning: Techniques,
applications,andopenchallenges. arXivpreprintarXiv:2108.11887,2021.
12Sudeep Salgia and Yuejie Chi. The sample-communication complexity trade-off in federated q-
learning. arXivpreprintarXiv:2408.16981,2024.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
HanShen, KaiqingZhang, MingyiHong, andTianyiChen. Towardsunderstandingasynchronous
advantageactor-critic:Convergenceandlinearspeedup.IEEETransactionsonSignalProcessing,
2023.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387–395.Pmlr,2014.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task
learning. Advancesinneuralinformationprocessingsystems,30,2017.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation
andtdlearning. InConferenceonLearningTheory,pp.2803–2830.PMLR,2019.
RichardSSutton. Learningtopredictbythemethodsoftemporaldifferences. Machinelearning,
3:9–44,1988.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
John Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function
approximation. Advancesinneuralinformationprocessingsystems,9,1996.
IsidorosTziotis,ZebangShen,RamtinPedarsani,HamedHassani,andAryanMokhtari. Straggler-
resilientpersonalizedfederatedlearning. TransactionsonMachineLearningResearch,2023.
HanWang, AritraMitra, HamedHassani, GeorgeJPappas, andJamesAnderson. Federatedtem-
poraldifferencelearningwithlinearfunctionapproximationunderenvironmentalheterogeneity.
arXivpreprintarXiv:2302.02212,2023a.
Sid Wang, Ashish Shenoy, Pierce Chuang, and John Nguyen. Now it sounds like you: Learning
personalizedvocabularyondevice. arXivpreprintarXiv:2305.03584,2023b.
ChristopherJCHWatkinsandPeterDayan. Q-learning. Machinelearning,8:279–292,1992.
JiinWoo,GauriJoshi,andYuejieChi. Theblessingofheterogeneityinfederatedq-learning:Linear
speedupandbeyond. InInternationalConferenceonMachineLearning.PMLR,2023.
Jiin Woo, Laixi Shi, Gauri Joshi, and Yuejie Chi. Federated offline reinforcement learning: Col-
laborative single-policy coverage suffices. In Forty-first International Conference on Machine
Learning,2024.
Guojun Xiong, Gang Yan, Shiqiang Wang, and Jian Li. Deprl: Achieving linear convergence
speedup in personalized decentralized learning with shared representations. In Proceedings of
theAAAIConferenceonArtificialIntelligence,volume38,pp.16103–16111,2024.
MinruiXu,JialiangPeng,BBGupta,JiawenKang,ZehuiXiong,ZhenniLi,andAhmedAAbdEl-
Latif. Multiagentfederatedreinforcementlearningforsecureincentivemechanisminintelligent
cyber–physicalsystems. IEEEInternetofThingsJournal,9(22):22095–22108,2021.
TimothyYang,GalenAndrew,HubertEichner,HaichengSun,WeiLi,NicholasKong,DanielRa-
mage, and Franc¸oise Beaufays. Applied federated learning: Improving google keyboard query
suggestions. arXivpreprintarXiv:1812.02903,2018.
ZhenyuanYuan,SiyuanXu,andMinghuiZhu. Federatedreinforcementlearningforgeneralizable
motionplanning. In2023AmericanControlConference(ACC),pp.78–83.IEEE,2023.
Sihan Zeng and Thinh Doan. Fast two-time-scale stochastic gradient method with applications in
reinforcementlearning. InTheThirtySeventhAnnualConferenceonLearningTheory,pp.5166–
5212.PMLR,2024.
13Chenyu Zhang, Han Wang, Aritra Mitra, and James Anderson. Finite-time analysis of on-policy
heterogeneousfederatedreinforcementlearning. arXivpreprintarXiv:2401.15273,2024.
KaiqingZhang,ZhuoranYang,andTamerBas¸ar. Multi-agentreinforcementlearning: Aselective
overviewoftheoriesandalgorithms. Handbookofreinforcementlearningandcontrol,pp.321–
384,2021.
Sai Qian Zhang, Jieyu Lin, and Qi Zhang. A multi-agent reinforcement learning approach for
efficientclientselectioninfederatedlearning.InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume36,pp.9091–9099,2022a.
TuoZhang,LeiGao,ChaoyangHe,MiZhang,BhaskarKrishnamachari,andASalmanAvestimehr.
FederatedlearningfortheInternetofThings: Applications,challenges,andopportunities. IEEE
InternetofThingsMagazine,5(1):24–29,2022b.
Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen Sun.
Efficientreinforcementlearninginblockmdps: Amodel-freerepresentationlearningapproach.
InInternationalConferenceonMachineLearning,pp.26517–26547.PMLR,2022c.
Zhong Zheng, Fengyu Gao, Lingzhou Xue, and Jing Yang. Federated q-learning: Linear regret
speedup with low communication cost. In The Twelfth International Conference on Learning
Representations,2024.
A RELATED WORK
Single-Agent Reinforcement Learning. RL is a machine learning paradigm that trains agents to
makesequencesofdecisionsbyrewardingdesiredbehaviorsand/orpenalizingundesiredonesina
givenenvironment(Sutton&Barto,2018). StartingfromTemporalDifference(TD)Learning(Sut-
ton, 1988), which introduced the concept of learning from the discrepancy between predicted and
actual rewards through episodes, the widely used Q-Learning (Watkins & Dayan, 1992) emerged,
advancingthefieldwithanoff-policyalgorithmthatlearnsaction-valuefunctionsandenablespol-
icyimprovementwithoutneedingamodeloftheenvironment. Lateron, theintroductionofDeep
Q-Networks(DQN)(Mnihetal.,2015)markedasignificantleap,integratingdeepneuralnetworks
withQ-Learningtohandlehigh-dimensionalstatespaces,thusenablingRLtotacklecomplexprob-
lems. Subsequently, policy-basedalgorithmssuchasProximalPolicyOptimization(PPO)(Schul-
man et al., 2017) and deep Deterministic Policy Gradients (DDPG) (Silver et al., 2014), leverage
theActor-Criticframeworktoprovidemorestableandrobustwaystodirectlyoptimizethepolicy,
overcomingchallengesrelatedtoactionspaceandvariance.
Federated Reinforcement Learning. Jin et al. (2022) introduced a FedRL framework with N
agentscollaborativelylearningapolicybyaveragingtheirQ-valuesorpolicygradients. Khodada-
dianetal.(2022)providedaconvergenceanalysisoffederatedTD(FedTD)andQ-learning(FedQ)
when N agents interact with homogeneous environments. A similar FedTD was considered in
DalFabbroetal.(2023),andexpandedtoheterogeneousenvironmentsinWangetal.(2023a). Woo
et al. (2023) analyzed (a)synchronous variants of FedQ in heterogeneous settings, and an asyn-
chronous actor-critic method was considered in Shen et al. (2023) with linear speedup guarantee
only under i.i.d. samples. Zhang et al. (2024) provided a finite-time analysis of FedSARSA with
linearfunctionapproximation(i.e.,fixedfeaturerepresentation).Tofacilitatepersonalizationinhet-
erogeneoussettings,Jinetal.(2022)proposedaheuristicpersonalizedFedRLmethodwhereagents
shareacommonmodel,butmakeuseofindividualenvironmentembeddings. Noticethatthereisan
earlyworkFanetal.(2021)whichconsideredaspecialsettingwhereeachagentcanbeByzantine
andsuffersrandomfailureineveryroundandconvergencewasestablishedbasedoni.i.dnoise.
Personalized Federated Learning (PFL). In contrast to standard FL where a single model is
learned,PFLaimstolearnN modelsspecializedforN localdatasets.ManyPFLmethodshavebeen
developed,includingbutnotlimitedtomulti-tasklearning(Smithetal.,2017),meta-learning(Chen
etal.,2018), andvariouspersonalizationtechniquessuchaslocalfine-tuning(Fallahetal.,2020),
layerpersonalization(Arivazhaganetal.,2019),andmodelcompression(Bergouetal.,2022). An-
other line of work (Collins et al., 2021; Xiong et al., 2024) leveraged the common representation
14Algorithm2PFEDTD-REP
1: Input: Samplingpolicyπi,∀i∈[N];
2: Initialize θθθi = 000, Si,∀i ∈ [N], and randomly generate ΦΦΦ ∈ R|S|×d with each row being
0 0
unit-normvector;
3: fort=0,1,...,T −1do
4: fori=1,...,N do
5: fork =1,...,K do
6: SampleobservationsXi ;
t,k−1
7: Setθθθi
t,k
=θθθi t,k−1+α tg(θθθi t,k−1,ΦΦΦ t,X ti ,k−1);
8: endfor
9: Scale∥θθθi ∥toBif∥θθθi ∥>B,otherwisekeepitunchanged;
t+1 t+1
10: SetΦΦΦi
t+1/2
=ΦΦΦ t+β th(θθθi t+1,ΦΦΦ t,{X ti ,k−1}K k=1);
11: NormalizeΦΦΦi asΦΦΦi ←
ΦΦΦi
t+1/2 ;
t+1/2 t+1/2 ∥ΦΦΦi ∥
t+1/2
12: endfor
13: ΦΦΦ t+1 = N1 (cid:80)N i=1ΦΦΦi t+1/2.
14: endfor
amongagentsinheterogeneousenvironmentstoguaranteepersonalizedmodelsforfederatedsuper-
visedlearning.
Representation Learning in MDP. Representation learning aims to transform high-dimensional
observation to low-dimensional embedding to enable efficient learning, and has received increas-
ing attention in Markov decision processs (MDP) settings, such as linear MDPs (Jin et al., 2020),
low-rank MDPs (Modi et al., 2021; Agarwal et al., 2020) and block MDPs (Zhang et al., 2022c).
However, it is open in the context of leveraging representation learning in PFedFL. In this work,
we prove that representation augmented PFedFL forms a general framework as a federated two-
timescalestochasticapproximationwithMarkoviannoise,whichdifferssignificantlyfromexisting
works,andhencenecessitatesdifferentprooftechniques.
Multi-Agent Reinforcement Learning vs. Federated Reinforcement Learning. The advent
of Multi-Agent Reinforcement Learning (MARL) expanded RL’s applications, allowing multiple
agentstolearnfrominteractionsincooperative,competitive,ormixedsettings,openingnewavenues
for complex applications and research (Zhang et al., 2021). Multi-agent Reinforcement Learning
(MARL)addressesscenarioswheremultipleagentsoperatewithinasharedorinterrelatedenviron-
ment, potentially engaging in both cooperative and competitive behaviors. The complexity arises
fromeachagentneedingtoconsiderthestrategiesandactionsofothers, makingthelearningpro-
cess highly dynamic. Federated Reinforcement Learning (FedRL)(Qi et al., 2021), contrasts with
MARLbyfocusingonprivacy-preserving,distributedlearningacrossagentsthatdonotsharetheir
rawdata. Instead,theseagentsmightcontributetowardsacentralizedlearningmodelwithoutcom-
promisingindividualdataprivacy,addressingtheuniquechallengesoflearningfromdecentralized
datasources.
B PSEUDOCODE OF PFEDTD-REP
Inthissection,wepresentthepseudocodeofPFEDQ-REPassummarizedinAlgorithm2.
C APPLICATION TO CONTROL TASKS IN RL
The Q-function of agent i in environment Mi under policy πi is defined as Qi,πi(s,a) =
E (cid:2)(cid:80)∞ γkRi(si,ai)|si =s,ai =a(cid:3) . When the state and action spaces are large, it is com-
πi k=0 k k 0 0
putationally infeasible to store Qi,πi(s,a) for all state-action pairs. One way to deal with is to
approximate the Q-function as Qi,πi(s,a) ≈ ΦΦΦ(s,a)θθθ, whereΦΦΦ ∈ R|S|×|A|×d is a feature repre-
sentationcorrespondingtostate-actions,andθθθ ∈ Rd isalow-dimensionalunknownweightvector.
15WhenΦΦΦisgivenandknown,thisfallsundertheparadigmofRLorFedRLwithfunctionapproxi-
mation.
C.1 PRELIMINARIES: CONTROLINFEDERATEDREINFORCEMENTLEARNING
Another task in RL is to search for an optimal policy, which is called a control problem, and one
commonly used approach is Q-learning (Watkins & Dayan, 1992). Under the FedRL framework,
the goal of a control problem is to let N agents collaboratively learn a policy π∗ that performs
uniformlywellacrossN differentenvironments,i.e.,π∗ =argmax 1 (cid:80)N E (cid:2) Vi,πi(si)|si ∼
π N i=1 πi 0 0
(cid:3)
d , where d is the common initial state distribution in these N environments. Similar to (1),
0 0
thiscanbeformulatedastheoptimizationproblemin(17)tocollaborativelylearnacommon(non-
personalized) weight vector θθθ ≡ θθθi,∀i ∈ [N] when the feature representation ΦΦΦ(s,a),∀s,a are
given.
L(θθθ):=m θθθin
N1 (cid:88)N
E s∼µi,π∗
(cid:13)
(cid:13) (cid:13)ΦΦΦ(s,a)θθθ−Qi,π∗
(s,a)(cid:13)
(cid:13)
(cid:13)2
. (17)
i=1 a∼π∗(·|s)
Again,weusethesuperscriptitohighlightheterogeneousenvironmentsPiamongagents.
C.2 CONTROLINPERSONALIZEDFEDRLWITHSHAREDREPRESENTATIONS
The control problem in (17) aims to learnΦΦΦ and {θθθi,∀i} simultaneously among all N agents via
solvingthefollowingoptimizationproblem:
L(ΦΦΦ,{θθθi,∀i}):=min
1 (cid:88)N
min E
(cid:13)
(cid:13)fi(θθθi,ΦΦΦ(s,a))−Qi,πi,∗
(s,a)(cid:13) (cid:13)2
. (18)
ΦΦΦ N {θθθi,∀i} s∼µi,πi,∗ (cid:13) (cid:13)
i=1 a∼πi,∗(·|s)
C.3 ALGORITHMS
Inthissubsection,wepresenttworealizationsofourproposedPFEDRL-REPinAlgorithm1,oneis
PFEDQ-REPassummarizedinAlgorithm3,federatedQ-learningwithsharedrepresentations,and
theotherisPFEDDQN-REPasoutlinedinAlgorithm4,federatedDQNwithsharedrepresentations.
D FIGURE ILLUSTRATIONS
We present some figures to further highlight the proposed personalized FedRL (PFEDRL) frame-
workwithsharedrepresentations.
SchematicframeworkofconventionalFedRL.WebeginbyintroducingtheconventionalFedRL
framework(Khodadadianetal.,2022),whereN agentscollaborativelylearnacommonpolicy(or
optimalvaluefunctions)viaaserverwhileengagingwithhomogeneousenvironments. Eachagent
generatesindependentMarkoviantrajectories,asdepictedinFigure4.
Schematic framework for our proposed PFEDRL with shared representations. We introduce
our proposed personalized FedRL (PFEDRL) framework with shared representations in Figure 5.
In PFEDRL, N agents independently interact with their own environments and execute actions
accordingtotheirindividualRLcomponentparameterizedbyΦΦΦandθθθi. Eachagentiperformslocal
updateonitslocalweightvectorθθθ ,whilejointlyupdatingtheglobalsharedfeaturerepresentation
i
ΦΦΦthroughtheserver. Similarly,theupdatefollowstheMarkoviantrajectories.
Motivation of Personalized FedRL. In the following, we also want to provide some examples
showing that the conventional FedRL framework may fail, as depicted in Figure 6. In Figure 6a,
we provide an example where three agents assess distinct policies within the same environment.
In the traditional FedRL framework, agents exchange the evaluated value functions via a central
server,leadingtoaunifiedconsensusonvaluefunctionsforthreedifferentpolicies. Thisenforced
consensus on value functions, despite the diversity in policies, is not optimal. In another scenario
depicted in Figure 6b, three agents each interact with their unique environments. The objective
for each agent is to learn an optimal policy tailored to its specific environment. However, within
16Algorithm3PFEDQ-REP
Input: Samplingpolicyπi,∀i∈[N].
1: Initializeθθθi =000,andsi,∀i∈[N],andrandomlygenerateΦΦΦ∈R|S||A|×d witheachrowbeing
0 0
unit-normvector.
2: fort=0,1,...,T −1do
3: fori=1,...,N do
4: fork =1,...,K do
5: SampleobservationsXi =(si ,si ,ai );
t,k−1 t,k t,k−1 t,k−1
6: With fixed ΦΦΦ t, update θθθi t,k ← θθθi t,k−1 +α t ·(r ti ,k−1 +γmax aΦΦΦ t(si t,k+1,a)θθθi t,k−1 −
ΦΦΦ (si )θθθi )·ΦΦΦ (si ,ai );
t t,k−1 t,k−1 t t,k−1 t,k−1
7: endfor
8: Scale∥θθθi ∥toBif∥θθθi ∥>B,otherwisekeepitunchanged.
t+1 t+1
9: if(s,a)∈Xi ,∃k ∈{0,...,K−1}then
t,k
10: UpdateΦΦΦi t+1/2(s,a)=ΦΦΦi t(s,a)+β t(r(s,a)+γmax aΦΦΦ t(s′,a)θθθi t+1−ΦΦΦ t(s,a)⊺θθθi t+1)·
θθθi ;
t+1
11: else
12: ΦΦΦi (s,a)=ΦΦΦi(s,a);
t+1/2 t
13: endif
14: NormalizeΦΦΦi asΦΦΦi ←
ΦΦΦi
t+1/2 ;
t+1/2 t+1/2 ∥ΦΦΦi ∥
t+1/2
15: endfor
16: ΦΦΦ t+1 ← N1 (cid:80)N i=1ΦΦΦi t+1/2,∀i∈[N].
17: endfor
Figure4: SchematicrepresentationofFedRL,whereN agentsinteractwithhomogeneousenviron-
ments.
the traditional FedRL framework, the central server mandates a uniform policy across all three
agents,whichclearlycontradictstheintendedgoalofachievingenvironment-specificoptimization.
This highlights the necessity for personalized decision-making, a feature that conventional FedRL
frameworksdonotaccommodate.
ExampleofRLcomponentsthatfittheproposedPFEDRLwithsharedrepresentations. Inthe
following,weaimtoshowcaseexamplesofRLcomponentsthatarecompatiblewithourproposed
PFEDRLframeworkfeaturingsharedrepresentations. Anillustrativeexampleofthisframeworkis
presented in Figure 7. It is important to note that both the DQN architecture in Figure 7a and the
policygradient(PG)approachinFigure7bseamlesslyintegrateintoourproposedframework. This
integrationisachievedbydesignatingtheparametersofthefeatureextractionnetworkastheshared
feature representationΦΦΦ, and the parameters of the fully connected network, which either predict
theQ-valuesordeterminethepolicy,asthelocalweightvectorθθθ. Thisarrangementunderscoresthe
adaptabilityofourframeworktovariousRLmethodologies,facilitatingpersonalizedlearningwhile
maintainingacommonfoundationofsharedrepresentations.
17Algorithm4PFEDDQN-REP
Initialize: The parameters (ΦΦΦ,θθθi) for each Q network Qi(s,a), the replay buffer Ri, and copy
the same parameter from Q network to initialize the target Q network Qi,′(s,a) for agent i,∀i ∈
[N];
1: forepisodee=1,...,E do
2: Gettheinitialstateoftheenvironment;
3: fort=0,1,...,T −1do
4: fori=1,...,N do
5: fork =1,...,K do
6: Select action a t,k−1 according to ϵ-greedy policy with the current network
Qi(s ,a);
t,k−1
7: Executeactiona t,k−1,receivetherewardr(s t,k−1,a t,k−1),andtheenvironmentstate
transitstos ;
t,k
8: Storethetuple(s t,k−1,a t,k−1,r(s t,k−1,a t,k−1,s t,k)intothereplaybufferRi;
9: SampleN datatuplesfromthereplaybufferRi;
10: Update the local weight θθθi(t,k) by minimizing the loss compared with the target
networkQi,′withfixedrepresentationΦΦΦ ;
t
11: endfor
12: SampleN datatuplesfromreplaybufferRi;
13: Update representation model locally by minimizing the loss compared with the target
networkQi,′withfixedweightsθθθ ,andyieldΦΦΦi ;
t+1 t+1/2
14: endfor
15: Averagetherepresentationmodelfromallagents,i.e.,ΦΦΦ t+1 := N1 (cid:80)N i=1ΦΦΦi t+1/2;
16: endfor
17: ifmod(t,T target)=0then
18: updatethetargetnetworkQi,∗ becopytheup-to-dateparametersofQnetworkQi, ∀i ∈
[N];
19: endif
20: endfor
Figure 5: Our proposed PFEDRL-REP framework where N agents independently interact with
theirownenvironmentsandtakeactionsaccordingtotheirindividualRLcomponentparameterized
by ΦΦΦ and θθθi. Agent i locally update weight vector θθθ while jointly updating the shared feature
i
representationΦΦΦthroughthecentralserver. TheupdatefollowstheMarkoviantrajectories.
18(b) Agents learn optimal policies for heterogeneous
(a)Agentsevaluatedifferencepoliciesinthesame
environments.
environment.
Figure6:AnillustrativeexamplewiththreeagentsthatdemonstratestheconventionalFedRLframe-
workfailstowork.
(a)WhenDQNmeetstheproposedframework. (b)WhenPGmeetstheproposedframework.
Figure7: Anillustrativeexamplefortheproposedframework. NoticethatboththeDQNin(a)and
policygradient(PG)in(b)canbefittedintotheproposedframeworkbytreatingtheparametersof
thefeatureextractionnetworkasthesharedfeaturerepresentationΦΦΦandtheparameterofthefully
connectednetworkwhichmapstotheQvalueofpolicyasthelocalweightvectorθθθ.
E PROOF OF LEMMAS IN SECTION 4.2
E.1 PROOFOFLEMMA4.6
Proof. Recall that for any observation X = (s,a,s′), the function g(θθθ,ΦΦΦ,X) defined in (8) is
expressedas
g(θθθ,ΦΦΦ,X):=(r(s,a)+γΦΦΦ(s′)θθθ−ΦΦΦ(s)θθθ)·ΦΦΦ(s)⊺
,
and hence we have the following inequality for any parameter pairs (θθθ ,ΦΦΦ ) and (θθθ ,λλλ ) with
1 1 2 2
X =(s,a,s′),
∥g(θθθ ,ΦΦΦ ,X)−g(θθθ ,ΦΦΦ ,X)∥
1 1 2 2
=∥(r(s,a)+γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ −(r(s,a)+γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ ∥
1 1 1 1 1 2 2 2 2 2
(a ≤1)
∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ ∥
1 1 1 1 1 1 2 1 2 1
+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ ∥
1 2 1 2 1 2 2 2 2 2
(a ≤2)
∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )−(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )∥·∥ΦΦΦ (s)∥
1 1 1 1 1 2 1 2 1
+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ ∥
1 2 1 2 1 2 2 2 2 2
(a ≤3)
(1+γ)∥θθθ −θθθ ∥+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ ∥
1 2 1 2 1 2 1 2 2 2 2 2
(a ≤4)
(1+γ)∥θθθ −θθθ ∥+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ ∥
1 2 1 2 1 2 1 1 2 1 2 2
+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·ΦΦΦ (s)⊺ ∥
1 2 1 2 2 2 2 2 2 2
(a ≤5)
(1+γ)∥θθθ −θθθ
∥+(cid:13)
(cid:13)(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ
)(cid:13) (cid:13)·(cid:13)
(cid:13)ΦΦΦ (s)−ΦΦΦ
(s)(cid:13)
(cid:13)
1 2 (cid:13) 1 2 1 2 (cid:13) (cid:13) 1 2 (cid:13)
+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )−(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )∥·∥ΦΦΦ (s)∥
1 2 1 2 2 2 2 2 2
(a ≤6) (1+γ)(cid:13)
(cid:13)θθθ −θθθ
(cid:13) (cid:13)+(cid:13)
(cid:13)(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ
)(cid:13)
(cid:13)·∥ΦΦΦ (s)−ΦΦΦ (s)∥
(cid:13) 1 2(cid:13) (cid:13) 1 2 1 2 (cid:13) 1 2
19+∥ΦΦΦ (s′)−ΦΦΦ (s′)∥·∥γθθθ ∥+∥ΦΦΦ (s)−ΦΦΦ (s)∥·∥θθθ ∥
1 2 2 1 2 2
≤(1+γ)∥θθθ −θθθ ∥+(2+2γ)∥θθθ ∥·∥ΦΦΦ −ΦΦΦ ∥
1 2 2 1 2
(a7)
≤ L (∥θθθ −θθθ ∥+∥ΦΦΦ −ΦΦΦ ∥),
g 1 2 1 2
(a ) is due to the fact that ∥x + y∥ ≤ ∥x∥ + ∥y∥,∀x,y ∈ Rd, (a ) holds due to ∥x · y∥ ≤
1 2
∥x∥·∥y∥,∀x,y ∈ Rd, (a )comesfromthefactand∥ΦΦΦ (s)∥ ≤ 1,∥ΦΦΦ (s)∥ ≤ 1∀s.(a )−(a )
3 1 2 4 6
holds for the same reason as (a )−(a ). The last inequalty (a ) comes from the fact that θθθ is
1 3 7
boundedbynormBandbysettingL :=max(1+γ,(2+2γ)B).
g
E.2 PROOFOFLEMMA4.7
Proof. Recall that for any observation X = (s,a,s′), the function h(θθθ,ΦΦΦ,X) defined in (10) is
expressedas
h(θθθ,ΦΦΦ,X):=(r(s,a)+γΦΦΦ(s′)θθθ−ΦΦΦ(s)θθθ)·θθθ⊺
,
and hence we have the following inequality for any parameter pairs (θθθ ,ΦΦΦ ) and (θθθ ,λλλ ) with
1 1 2 2
X =(s,a,s′),
∥h(θθθ ,ΦΦΦ ,X)−h(θθθ ,ΦΦΦ ,X)∥
1 1 2 2
=∥(r(s,a)+γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ −(r(s,a)+γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ ∥
1 1 1 1 1 2 2 2 2 2
( ≤b1)
∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ ∥
1 1 1 1 1 2 1 2 1 1
+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ ∥
2 1 2 1 1 2 2 2 2 2
( ≤b2)
∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )−(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )∥·∥θθθ ∥
1 1 1 1 2 1 2 1 1
+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ ∥
2 1 2 1 1 2 2 2 2 2
( ≤b3)
(1+γ)∥θθθ ∥2·∥ΦΦΦ −ΦΦΦ ∥+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ ∥
1 1 2 2 1 2 1 1 2 2 2 2 2
( ≤b4)
(1+γ)∥θθθ ∥2·∥ΦΦΦ −ΦΦΦ ∥+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ ∥
1 1 2 2 1 2 1 1 2 1 2 1 2
+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ −(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )·θθθ⊺ ∥
2 1 2 1 2 2 2 2 2 2
( ≤b5)
(1+γ)∥θθθ ∥2·∥ΦΦΦ −ΦΦΦ ∥+∥(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )∥·∥θθθ −θθθ ∥
1 1 2 2 1 2 1 1 2
+∥(γϕϕϕ (s′)θθθ −ΦΦΦ (s)θθθ )−(γΦΦΦ (s′)θθθ −ΦΦΦ (s)θθθ )∥·∥θθθ ∥
2 1 2 1 2 2 2 2 2
( ≤b6)
(1+γ)∥θθθ ∥2·∥ΦΦΦ −ΦΦΦ ∥+(1+γ)∥θθθ ∥·∥θθθ −θθθ ∥+(1+γ)∥θθθ ∥·∥θθθ −θθθ ∥
1 1 2 1 1 2 2 1 2
≤(1+γ)∥θθθ ∥2·∥ΦΦΦ −ΦΦΦ ∥+(1+γ)(∥θθθ ∥+∥θθθ ∥)·∥θθθ −θθθ ∥
1 1 2 1 2 1 2
(b7)
≤ L (∥θθθ −θθθ ∥+∥ΦΦΦ −ΦΦΦ ∥),
h 1 2 1 2
(b ) is due to the fact that ∥x + y∥ ≤ ∥x∥ + ∥y∥,∀x,y ∈ Rd, (b ) holds due to ∥x · y∥ ≤
1 2
∥x∥·∥y∥,∀x,y ∈ Rd, (b ) comes from the fact and ∥ΦΦΦ (s)∥ ≤ 1,∥ΦΦΦ (s)∥ ≤ 1∀s. (b )−(b )
3 1 2 4 6
holds for the same reason as (b )−(b ). The last inequalty (b ) comes from by setting L :=
1 3 7 h
max((1+γ)B2,(2+2γ)B).
E.3 PROOFOFLEMMA4.8
Proof. Duetothenorm-scalestep(step9)inAlgorithm2,wehave
∥yi(ΦΦΦ )−yi(ΦΦΦ )∥≤ max ∥θθθ−θθθ′∥≤2B. (19)
1 2
(∥θθθ∥≤B,∥θθθ′∥≤B)
Since the representation matricesΦΦΦ andΦΦΦ are of unit-norm in each row, there exists a positive
1 2
constantL suchthat
y
∥yi(ΦΦΦ )−yi(ΦΦΦ )∥≤L ∥ΦΦΦ −ΦΦΦ ∥. (20)
1 2 y 1 2
20E.4 PROOFOFLEMMA4.10
Proof. In the TD learning setting for our PFEDTD-REP, at time step k, the state of agent i is si,
k
anditsvaluefunctioncanbedenotedasV(si) =ΦΦΦ(si)θθθi inalinearrepresentation,whereΦΦΦ(si)
k k k
isafeaturevectorandθθθi isaweightvector. Thegoalofagentiistominimizethefollowingloss
functionforeverysi ∈S:
k
1(cid:12) (cid:12)2
Li(ΦΦΦ(si),θθθi)= (cid:12)V(si)−Vˆ(si)(cid:12) ,
k 2(cid:12) k k (cid:12)
with Vˆ(si) = ri +γΦ(si )θi being a constant. Therefore, to updateΦΦΦ(s) andθθθ, we just take
k k k+1
thenaturalgradientdescent. Specifically,weupdateθθθaccordingto(7)bytakingagradientdescent
stepwithrespecttoθθθ,withfixedΦΦΦ. Similarly,weupdateΦΦΦ(s)accordingto(9)bytakingagradient
descentstepwithrespecttoΦΦΦ(s),withfixedθθθ.
Next,weshowtheconvexityofthelossfunctionLi(ΦΦΦ(si),θθθi)withrespecttothefeaturerepresenta-
k
tionΦΦΦ(si)underafixedθθθi.SincetheestimatedvaluefunctionisapproximatedasV(si)=ΦΦΦ(si)θθθi,
k k k
whereθθθi isafixedparameter. Takingthesecond-orderderivativeofLi(ΦΦΦ(si),θθθi)w.r.t.ΦΦΦ(si)will
⊺ k k
involveθθθiθθθi ,whichisapositivesemi-definitematrixaslongasθθθi ̸=000. Positivesemi-definiteness
oftheHessianimpliesconvexity. Hence, Li(ΦΦΦ(si),θθθi)isconvexonΦΦΦ(si)underafixedθi. This
k k
propertyholdsviceversa,i.e.,Li(ΦΦΦ(si),θθθi)isconvexonθθθiunderafixedΦΦΦ(si).
k k
Recall that the optimal solution ΦΦΦ∗ and θθθ∗ is defined as the set of possible values that make the
0
expectation of stochastic gradient g and h tends to be 0, as defined in (12), which is analogy to
makethefirst-ordergradientoflossfunctionbe0andachievethelocalminima. Theinequalitiesin
Lemma4.10denotethattheupdatesmadetothefeaturematrixΦΦΦforfixedθθθinthefirstequationand
theparametersθθθforfixedΦΦΦinthesecondequationisdirectedtowardsreducingthedeviationfrom
the optimal solutions close to initial point. As we only care about the solution to make stochastic
gradients be 0, for a fixedθθθ, the loss function L is convex w.r.t. ΦΦΦ, the learning process ofΦΦΦ is
guaranteedtomovetowardsdecreasingthedifferencefromanoptimalpoint. Thisalsoholdsforthe
updateofθθθ.
E.5 PROOFOFLEMMA4.12
Proof. UnderLemma4.6,wehave
∥g(θθθ,ΦΦΦ,X)−g(yi(ΦΦΦ∗),ΦΦΦ∗,X)∥≤L(∥θθθ−yi(ΦΦΦ∗)∥+∥ΦΦΦ−ΦΦΦ∗∥),∀i∈[N]. (21)
Similarly,underLemma4.7,wehave
∥h(θθθ,ΦΦΦ,X)−h(yi(ΦΦΦ∗),ΦΦΦ∗,X)∥≤L(∥θθθ−yi(ΦΦΦ∗)∥+∥ΦΦΦ−ΦΦΦ∗∥),∀i∈[N]. (22)
LetL =max(L,max g(yi(ΦΦΦ∗),ΦΦΦ∗,X),max h(yi(ΦΦΦ∗),ΦΦΦ∗,X)),thenaccordingto(21)-(22),
1 X X
wehave
∥g(θθθ,ΦΦΦ)∥≤L (∥θθθ−yi(ΦΦΦ∗)∥+∥ΦΦΦ−ΦΦΦ∗∥+1),
1
and
∥h(θθθ,ΦΦΦ)∥≤L (∥θθθ−yi(ΦΦΦ∗)∥+∥ΦΦΦ−ΦΦΦ∗∥+1).
1
Denotehj(θθθ,ϕϕϕ,X)asthej-thelementofh(θθθ,ΦΦΦ,X). FollowingChenetal.(2019),wecanshow
thatθθθ ∈Rd,ΦΦΦ∈R|S|×d,andx∈X,
∥E[h(θθθ,ΦΦΦ,X)|X =x]−E [h(θθθ,ΦΦΦ,X)]∥
0 µ
d
(cid:88)
≤ |E[hj(θθθ,λλλ,X)|X =x]−E [hj(θθθ,ΦΦΦ,X)]|
0 µ
j=1
(cid:12)
(cid:88)d (cid:12) (cid:20) hj(θθθ,ΦΦΦ,X) (cid:12) (cid:21)
≤2L (∥θθθ−yi(ΦΦΦ∗)∥+∥ΦΦΦ−ΦΦΦ∗∥+1) (cid:12)E (cid:12)X =x
1 (cid:12) 2L (∥θθθ−yi(ΦΦΦ∗)∥+∥λ−λ∗∥+1)(cid:12) 0
(cid:12) 1
j=1
(cid:12)
(cid:20) hj(θθθ,ΦΦΦ,X) (cid:21)(cid:12)
−E (cid:12)
µ 2L (∥θθθ−yi(ΦΦΦ∗)∥+∥λ−λ∗∥+1) (cid:12)
1 (cid:12)
≤2L (∥θθθ−yi(ΦΦΦ∗)∥+∥ΦΦΦ−ΦΦΦ∗∥+1)dC ρk,
1 1 1
21wherethelastinequalityholdsduetoAssumption4.3withconstantsC > 0andρ ∈ (0,1). To
1 1
guarantee2L (∥θθθ−yi(ΦΦΦ∗)∥+∥ΦΦΦ−ΦΦΦ∗∥+1)dC ρk ≤δ(∥θθθ−yi(ΦΦΦ∗)∥+∥ΦΦΦ−ΦΦΦ∗∥+1),wehave
1 1 1
log(1/δ)+log(2L C d)
τ ≤ 1 1 . (23)
δ log(1/ρ )
1
Usingthesameprocedureswecanshowthat
∥E[g(θθθ,ΦΦΦ,X)|X =x]−E [g(θθθ,ΦΦΦ,X)]∥≤2L (∥θθθ−yi(ΦΦΦ∗)∥+∥ΦΦΦ−ΦΦΦ∗∥+1)dC ρk,
0 µ 1 2 2
hencewehave
log(1/δ)+log(2L C d)
τ ≤ 1 2 . (24)
δ log(1/ρ )
2
Bysettingτ asthelargestvaluein(23)and(24),wearriveatthefinalresultinLemma4.12.
δ
F PROOFS OF MAIN RESULTS
F.1 PROOFOFTHEOREM4.13
For notational simplicity, in the proofs, we use h(θθθi ,ΦΦΦ ) to denote h(θθθi ,ΦΦΦ ,{Xi }K ),
t+1 t t+1 t t,k−1 k=1
andg(θθθi ,ΦΦΦ )todenoteg(θθθi ,ΦΦΦ ,Xi ). Inthefollowing,wefirstfocusontheupdateof
t,k−1 t t,k−1 t t,k−1
theglobalrepresentationΦΦΦ andcharacterizethedriftofit.
t
F.1.1 DRIFTOFΦΦΦ
t
ThedriftofΦΦΦ isgiveninthefollowinglemma.
t
LemmaF.1. ThedriftbetweenΦΦΦ andΦΦΦ isgivenby
t+1 t
E[∥ΦΦΦ −ΦΦΦ∗∥2]
t+1
=E[∥ΦΦΦ t−ΦΦΦ∗∥2]+
Nβ t2 2E (cid:13) (cid:13)
(cid:13)
(cid:13)(cid:88)N
h(θθθi t+1,ΦΦΦ
t)(cid:13) (cid:13)
(cid:13)
(cid:13)2
+2β
tE(cid:34)
⟨ΦΦΦ∗−ΦΦΦ
t,− N1(cid:88)N
h¯(θθθi t+1,ΦΦΦ
t)⟩(cid:35)
(cid:13) (cid:13)
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Term1 Term2
(cid:34) N (cid:35)
+2β E ⟨ΦΦΦ −ΦΦΦ∗, 1 (cid:88) h(θθθi ,ΦΦΦ )−h¯(θθθi ,ΦΦΦ )⟩ . (25)
t t N t+1 t t+1 t
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
Term3
Proof. BasedontheupdateofΦΦΦ in(11),Wehavethefollowingequation
t
E[∥ΦΦΦ −ΦΦΦ∗∥2]−E[∥ΦΦΦ −ΦΦΦ∗∥2]
t+1 t
=E[∥ΦΦΦ∗∥2+∥ΦΦΦ ∥2−2⟨ΦΦΦ∗,ΦΦΦ ⟩]−E[∥ΦΦΦ∗∥2+∥ΦΦΦ ∥2−2⟨ΦΦΦ∗,ΦΦΦ ⟩]
t+1 t+1 t t
=E[∥ΦΦΦ ∥2]−E[∥ΦΦΦ ∥2]−2⟨ΦΦΦ∗,ΦΦΦ −ΦΦΦ ⟩]
t+1 t t+1 t
=E[⟨ΦΦΦ −ΦΦΦ ,ΦΦΦ +ΦΦΦ ⟩]−2⟨ΦΦΦ∗,ΦΦΦ −ΦΦΦ ⟩]
t+1 t t+1 t t+1 t
=E[⟨ΦΦΦ −ΦΦΦ ,ΦΦΦ −ΦΦΦ ⟩]+2E[⟨ΦΦΦ −ΦΦΦ ,ΦΦΦ ⟩]−2⟨ΦΦΦ∗,ΦΦΦ −ΦΦΦ ⟩]
t+1 t t+1 t t+1 t t t+1 t
β2 (cid:13) (cid:13)(cid:88)N (cid:13) (cid:13)2 (cid:34) 1 (cid:88)N (cid:35)
= Nt 2E (cid:13)
(cid:13)
h(θθθi t+1,ΦΦΦ t)(cid:13)
(cid:13)
−2β tE ⟨ΦΦΦ∗−ΦΦΦ t,
N
h(θθθi t+1,ΦΦΦ t)⟩ , (26)
(cid:13) (cid:13)
i=1 i=1
whichdirectlyleadsto
E[∥ΦΦΦ −ΦΦΦ∗∥2]
t+1
β2 (cid:13) (cid:13)(cid:88)N (cid:13) (cid:13)2 (cid:34) 1 (cid:88)N (cid:35)
=E[∥ΦΦΦ t−ΦΦΦ∗∥2]+ Nt 2E (cid:13)
(cid:13)
h(θθθi t+1,ΦΦΦ t)(cid:13)
(cid:13)
−2β tE ⟨ΦΦΦ∗−ΦΦΦ t,
N
h(θθθi t+1,ΦΦΦ t)⟩ .
(cid:13) (cid:13)
i=1 i=1
(27)
Rearrangingthelasttermyieldsthedesiredresult.
22Inthefollowing,weseparatelyboundTerm toTerm .
1 3
WefirstboundTerm asfollows.
1
LemmaF.2. Foranyt≥τ,wehave
4β2L2 (cid:34) (cid:88)N (cid:35)
Term ≤4β2(L2+L4)E[∥ΦΦΦ∗−ΦΦΦ ∥2]+ t E ∥θθθ −yi(ΦΦΦ )∥2 +4β2δ2 (28)
1 t t N t+1 t t
i=1
Proof. Notethat
(cid:13) (cid:13)2
β2 (cid:13)(cid:88)N (cid:88)N (cid:88)N (cid:13)
Term 1 = Nt 2E (cid:13) (cid:13) h(θθθi t+1,ΦΦΦ t)− h(yi(ΦΦΦ t),ΦΦΦ∗)+ h(yi(ΦΦΦ t),ΦΦΦ∗)(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1 i=1 i=1
(cid:13) (cid:13)2
triangularinequality 2β2 (cid:13)(cid:88)N (cid:88)N (cid:13)
≤ N2t E (cid:13) (cid:13) h(θθθi t+1,ΦΦΦ t)− h(yi(ΦΦΦ t),ΦΦΦ∗)(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
Lipschitzofh
(cid:13) (cid:13)2
2β2 (cid:13)(cid:88)N (cid:13)
+ N2t E (cid:13) (cid:13) h(yi(ΦΦΦ t),ΦΦΦ∗)(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
(a ≤1) 2β Nt2L 22 E(cid:34) 2N(cid:88)N (cid:13) (cid:13)(θθθi t+1−yi(ΦΦΦ t))(cid:13) (cid:13)2 +2N2∥(ΦΦΦ t−ΦΦΦ∗)∥2(cid:35)
i=1
(cid:13) (cid:13)2
2β2 (cid:13)(cid:88)N (cid:88)N (cid:88)N (cid:13)
+ N2t E (cid:13) (cid:13) h(yi(ΦΦΦ t),ΦΦΦ∗)− h(yi(ΦΦΦ∗),ΦΦΦ∗)+ h(yi(ΦΦΦ∗),ΦΦΦ∗)(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1 i=1 i=1
4β2L2 (cid:34) (cid:88)N (cid:35)
≤4β2L2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+ t E ∥θθθi −yi(ΦΦΦ )∥2
t t N t+1 t
i=1
(cid:13) (cid:13)2
4β2 (cid:13)(cid:88)N (cid:88)N (cid:13)
+ N2t E (cid:13) (cid:13) h(yi(ΦΦΦ t),ΦΦΦ∗)− h(yi(ΦΦΦ∗),ΦΦΦ∗)(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
Lipschitzofh,yi
(cid:13) (cid:13)2
4β2 (cid:13)(cid:88)N (cid:13)
+ N2t E (cid:13) (cid:13) h(yi(ΦΦΦ∗),ΦΦΦ∗)(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
(a ≤2)
4β2L2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+
4β t2L2 E(cid:34) (cid:88)N
∥θθθi −yi(ΦΦΦ
)∥2(cid:35)
t t N t+1 t
i=1
(cid:13) (cid:13)2
+4β
t2L4E(cid:104)
∥ΦΦΦ
t−ΦΦΦ∗∥2(cid:105)
+
4 Nβ 2t2
E
(cid:13)
(cid:13)
(cid:13)(cid:88)N h(yi(ΦΦΦ∗),ΦΦΦ∗)−(cid:88)N h¯(yi(ΦΦΦ∗),ΦΦΦ∗)(cid:13)
(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1 i=1
(a ≤3)
4β2(L2+L4)E[∥ΦΦΦ∗−ΦΦΦ ∥2]+
4β t2L2 E(cid:34) (cid:88)N
∥θθθ −yi(ΦΦΦ
)∥2(cid:35)
+4β2δ2,
t t N t+1 t t
i=1
wherethe(a )isdueto∥(cid:80)N x ∥2 ≤N(cid:80)N ∥x ∥2,(a )isduetotheLipschitzoffunctionsh
1 i=1 i i=1 i 2
andyi,and(a )holdsbasedonthemixingtimepropertyinDefinition4.3.
3
Next,weboundTerm inthefollowinglemma.
2
23LemmaF.3. Wehave
(cid:34) N (cid:35)
Term ≤β (L/α −2ω)E[∥ΦΦΦ∗−ΦΦΦ ∥2]+
β tα tL
E
(cid:88)
∥θθθi −yi(ΦΦΦ )∥2 . (29)
2 t t t N t+1 t
i=1
Proof. Wehave
(cid:34) N (cid:35)
Term =2β E ⟨ΦΦΦ∗−ΦΦΦ
,−1(cid:88)
h¯(θθθi ,ΦΦΦ )⟩
2 t t N t+1 t
i=1
(cid:34) N (cid:35)
=2β E ⟨ΦΦΦ∗−ΦΦΦ
,−1(cid:88)
h¯(yi(ΦΦΦ ),ΦΦΦ )⟩
t t N t t
i=1
 
 N 
+2β tE  ⟨ΦΦΦ∗−ΦΦΦ t, N1 (cid:88) h¯(yi(ΦΦΦ t),ΦΦΦ t)−h¯(θθθi t+1,ΦΦΦ t)⟩ 

 i=1 
(cid:124) (cid:123)(cid:122) (cid:125)
Lipschitz ofh
(cid:34) N (cid:35) (cid:34) N (cid:35)
≤2β E ⟨ΦΦΦ∗−ΦΦΦ ,−1(cid:88) h¯(yi(ΦΦΦ ),ΦΦΦ )⟩ +2β LE ⟨ΦΦΦ∗−ΦΦΦ , 1 (cid:88) (yi(ΦΦΦ )−θθθi )⟩
t t N t t t t N t t+1
i=1 i=1
(cid:34) N (cid:35)
( ≤b1)
2β E ⟨ΦΦΦ∗−ΦΦΦ
,−1(cid:88)
h¯(yi(ΦΦΦ ),ΦΦΦ )⟩ +β L/α E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t N t t t t t
i=1
(cid:34) N (cid:35)
+
β tα tL
E
∥(cid:88)
(θθθi −yi(ΦΦΦ ))∥2
N2 t+1 t
i=1
(cid:34) N (cid:35)
( ≤b2) 2β E ⟨ΦΦΦ −ΦΦΦ∗, 1 (cid:88) h¯(yi(ΦΦΦ ),ΦΦΦ )⟩ +β L/α E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t N t t t t t
i=1
(cid:34) N (cid:35)
+
β tα tL
E
(cid:88)
∥θθθi −yi(ΦΦΦ )∥2
N t+1 t
i=1
(cid:34) N (cid:35)
≤β (L/α −2ω)E[∥ΦΦΦ∗−ΦΦΦ ∥2]+
β tα tL
E
(cid:88)
∥θθθi −yi(ΦΦΦ )∥2 ,
t t t N t+1 t
i=1
where (b ) holds because 2xTy ≤ β∥x∥2 + 1/β∥y∥2,∀β > 0, (b ) is due to ∥(cid:80)N x ∥2 ≤
1 2 i=1 i
N(cid:80)N ∥x ∥2,andthelastinequalityisduetoAssumption4.10.
i=1 i
Next,weboundTerm inthefollowinglemmas.
3
LemmaF.4. Forallt≥τ wehave
Term ≤(7β /α +2β α L2+6β α δ2)E[∥ΦΦΦ −ΦΦΦ ∥2]
3 t t t t t t t−τ t
+(6β /α +6β α δ2(1+L2)+4β α L2(3+4L2))E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t t t t t
+
16β tα tL2+6β tα tδ2 E(cid:34) (cid:88)N
∥θθθi,∗−θθθ
∥2(cid:35)
+11β α δ2. (30)
N t+1 t t
i=1
Proof. WefirstdecomposeTerm asfollows
3
(cid:34) N (cid:35)
Term =2β E ⟨ΦΦΦ −ΦΦΦ∗, 1 (cid:88) h(θθθi ,ΦΦΦ )−h¯(θθθi ,ΦΦΦ )⟩
3 t t N t+1 t t+1 t
i=1
24(cid:34) N (cid:35)
=2β E ⟨ΦΦΦ −ΦΦΦ , 1 (cid:88) h(θθθi ,ΦΦΦ )−h¯(θθθi ,ΦΦΦ )⟩
t t t−τ N t+1 t t+1 t
i=1
(cid:34) N (cid:35)
+2β E ⟨ΦΦΦ −ΦΦΦ∗, 1 (cid:88) h(θθθi ,ΦΦΦ )−h¯(θθθi ,ΦΦΦ )⟩
t t−τ N t+1 t t+1 t
i=1
(cid:34) N (cid:35)
=2β E ⟨ΦΦΦ −ΦΦΦ , 1 (cid:88) h(θθθi ,ΦΦΦ )−h¯(θθθi ,ΦΦΦ )⟩
t t t−τ N t+1 t t+1 t
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
C1
(cid:34) N N (cid:35)
1 (cid:88) 1 (cid:88)
+2β E ⟨ΦΦΦ −ΦΦΦ∗, h(θθθi ,ΦΦΦ )− h(θθθi ,ΦΦΦ )⟩
t t−τ N t+1 t N t+1 t−τ
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
C2
(cid:34) N N (cid:35)
+2β E ⟨ΦΦΦ −ΦΦΦ∗, 1 (cid:88) h(θθθi ,ΦΦΦ )− 1 (cid:88) h¯(θθθi ,ΦΦΦ )⟩
t t−τ N t+1 t−τ N t+1 t−τ
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
C3
(cid:34) N N (cid:35)
+2β E ⟨ΦΦΦ −ΦΦΦ∗, 1 (cid:88) h¯(θθθi ,ΦΦΦ )− 1 (cid:88) h¯(θθθi ,ΦΦΦ )⟩ .
t t−τ N t+1 t−τ N t+1 t
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
C4
Next,weboundC as
1
(cid:34) N (cid:35)
C =2β E ⟨ΦΦΦ −ΦΦΦ , 1 (cid:88) h(θθθi ,ΦΦΦ )−h¯(θθθi ,ΦΦΦ )⟩
1 t t t−τ N t+1 t t+1 t
i=1
(cid:13) (cid:13)2
≤β t/α tE[∥ΦΦΦ t−ΦΦΦ t−τ∥2]+β tα tE
(cid:13)
(cid:13)
(cid:13)N1 (cid:88)N
h(θθθi t+1,ΦΦΦ t)−h¯(θθθi t+1,ΦΦΦ
t)+h¯(yi(ΦΦΦ∗),ΦΦΦ∗)(cid:13)
(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
(cid:13) (cid:13)2
(cid:13) 1 (cid:88)N (cid:13)
≤β t/α tE[∥ΦΦΦ t−ΦΦΦ t−τ∥2]+2β tα tE (cid:13) (cid:13)N h(θθθi t+1,ΦΦΦ t)(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
(cid:13) (cid:13)2
+2β tα tE
(cid:13)
(cid:13)
(cid:13)N1 (cid:88)N
h¯(yi(ΦΦΦ∗),ΦΦΦ∗)−h¯(θθθi t+1,ΦΦΦ
t)(cid:13)
(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
(cid:13) (cid:13)2
=β t/α tE[∥ΦΦΦ t−ΦΦΦ t−τ∥2]+
2β Ntα
2tE
(cid:13)
(cid:13)
(cid:13)(cid:88)N
h(θθθi t+1,ΦΦΦ
t)(cid:13)
(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
(cid:13) (cid:13)2
+2β tα tE
(cid:13)
(cid:13)
(cid:13)N1 (cid:88)N
h¯(yi(ΦΦΦ∗),ΦΦΦ∗)−h¯(θθθi t+1,ΦΦΦ
t)(cid:13)
(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
LemmaF.2
≤ β /α E[∥ΦΦΦ −ΦΦΦ ∥2]+8β α (L2+L4)E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t t t−τ t t t
+
8β tα tL2 E(cid:34) (cid:88)N
∥θθθi −yi(ΦΦΦ
)∥2(cid:35)
+8β α δ2
N t+1 t t t
i=1
(cid:13) (cid:13)2
+2β tα tE
(cid:13)
(cid:13)
(cid:13)N1 (cid:88)N
h¯(yi(ΦΦΦ∗),ΦΦΦ∗)−h¯(θθθi t+1,ΦΦΦ
t)(cid:13)
(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
Lipschitzof h
≤β /α E[∥ΦΦΦ −ΦΦΦ ∥2]+8β α (L2+L4)E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t t t−τ t t t
25+
8β tα tL2 E(cid:34) (cid:88)N
∥θθθi −yi(ΦΦΦ
)∥2(cid:35)
+8β α δ2
N t+1 t t t
i=1
(cid:13) (cid:13)2
(cid:13) 1 (cid:88)N (cid:13)
+2β tα tL2E (cid:13) (cid:13)N 2(ΦΦΦ∗−ΦΦΦ t)+2(θθθi t+1−yi(ΦΦΦ∗))(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
≤β /α E[∥ΦΦΦ −ΦΦΦ ∥2]+8β α (L2+L4)E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t t t−τ t t t
+
8β tα tL2 E(cid:34) (cid:88)N
∥θθθi −yi(ΦΦΦ
)∥2(cid:35)
+8β α δ2
N t+1 t t t
i=1
+4β α L2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+
4β tα tL2 E(cid:34) (cid:88)N
∥θθθi
−yi(ΦΦΦ∗)∥2(cid:35)
t t t N t+1
i=1
=β /α E[∥ΦΦΦ −ΦΦΦ ∥2]+8β α (L2+L4)E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t t t−τ t t t
+
8β tα tL2 E(cid:34) (cid:88)N
∥θθθi −yi(ΦΦΦ
)∥2(cid:35)
+8β α δ2+4β α L2E[∥ΦΦΦ∗−ΦΦΦ ∥2]
N t+1 t t t t t t
i=1
+
4β tα tL2 E(cid:34) (cid:88)N
∥θθθi −yi(ΦΦΦ )+yi(ΦΦΦ
)−yi(ΦΦΦ∗)∥2(cid:35)
N t+1 t t
i=1
≤β /α E[∥ΦΦΦ −ΦΦΦ ∥2]+8β α (L2+L4)E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t t t−τ t t t
+
8β tα tL2 E(cid:34) (cid:88)N
∥θθθi −yi(ΦΦΦ
)∥2(cid:35)
+8β α δ2
N t+1 t t t
i=1
+4β α L2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+
8β tα tL2 E(cid:34) (cid:88)N
∥θθθi −yi(ΦΦΦ
)∥2(cid:35)
t t t N t+1 t
i=1
+8β α L4E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t t
=β /α E[∥ΦΦΦ −ΦΦΦ ∥2]+4β α L2(3+4L2)E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t t t−τ t t t
+
16β tα tL2 E(cid:34) (cid:88)N
∥θθθi −yi(ΦΦΦ
)∥2(cid:35)
+8β α δ2,
N t+1 t t t
i=1
wherethelastinequalityisduetotheLipschitzofthefunctionyi.
Next,weboundC asfollows.
2
(cid:34) N N (cid:35)
1 (cid:88) 1 (cid:88)
C =2β E ⟨ΦΦΦ −ΦΦΦ∗, h(θθθi ,ΦΦΦ )− h(θθθi ,ΦΦΦ )⟩
2 t t−τ N t+1 t N t+1 t−τ
i=1 i=1
(cid:13) (cid:13)2
(cid:13) 1 (cid:88)N 1 (cid:88)N (cid:13)
≤β t/α tE[∥ΦΦΦ t−τ −ΦΦΦ∗∥2]+β tα tE (cid:13) (cid:13)N h(θθθi t+1,ΦΦΦ t)− N h(θθθi t+1,ΦΦΦ t−τ)(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
Lipschitzof h
≤β /α E[∥ΦΦΦ −ΦΦΦ∗∥2]+β α L2E[∥ΦΦΦ −ΦΦΦ ∥2]
t t t−τ t t t t−τ
=β /α E[∥ΦΦΦ −ΦΦΦ +ΦΦΦ −ΦΦΦ∗∥2]+β α L2E[∥ΦΦΦ −ΦΦΦ ∥2]
t t t−τ t t t t t t−τ
≤2β /α E[∥ΦΦΦ −ΦΦΦ ∥2]+2β /α E[∥ΦΦΦ −ΦΦΦ∗∥2]+β α L2E[∥ΦΦΦ −ΦΦΦ ∥2]
t t t−τ t t t t t t t t−τ
=(2β /α +β α L2)E[∥ΦΦΦ −ΦΦΦ ∥2]+2β /α E[∥ΦΦΦ −ΦΦΦ∗∥2].
t t t t t−τ t t t t
Similarly,C isboundedexactlysameasC ,i.e.,
4 2
C ≤(2β /α +β α L2)E[∥ΦΦΦ −ΦΦΦ ∥2]+2β /α E[∥ΦΦΦ −ΦΦΦ∗∥2].
4 t t t t t−τ t t t t
26Next,weboundC asfollows.
3
(cid:34) N N (cid:35)
C =2β E ⟨ΦΦΦ −ΦΦΦ∗, 1 (cid:88) h(θθθi ,ΦΦΦ )− 1 (cid:88) h¯(θθθi ,ΦΦΦ )⟩
3 t t−τ N t+1 t−τ N t+1 t−τ
i=1 i=1
(cid:13) (cid:13)2
≤β t/α tE[∥ΦΦΦ t−τ −ΦΦΦ∗∥2]+β tα
tN1
2E
(cid:13)
(cid:13)
(cid:13)(cid:88)N
h(θθθi t+1,ΦΦΦ
t−τ)−(cid:88)N
h¯(θθθi t+1,ΦΦΦ
t−τ)(cid:13)
(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1 i=1
Definition4.3
≤ β /α E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t−τ
(cid:32)
N
(cid:33)2
+β tα tN1 2E  Nδ∥ΦΦΦ t−τ −ΦΦΦ∗∥+Nδ+δ(cid:88)(cid:13) (cid:13)θθθi t+1−yi(ΦΦΦ∗)(cid:13) (cid:13) 
i=1
(cid:104) (cid:105)
≤β /α E[∥ΦΦΦ −ΦΦΦ∗∥2]+3β α δ2E ∥ΦΦΦ −ΦΦΦ∗∥2 +3β α δ2
t t t−τ t t t−τ t t
+ 3β tα tδ2 E(cid:34) (cid:88)N (cid:13) (cid:13)θθθi −yi(ΦΦΦ∗)(cid:13) (cid:13)2(cid:35)
N t+1
i=1
(cid:104) (cid:105)
=β /α E[∥ΦΦΦ −ΦΦΦ∗∥2]+3β α δ2E ∥ΦΦΦ −ΦΦΦ∗∥2 +3β α δ2
t t t−τ t t t−τ t t
+ 3β t Nα tδ2 E(cid:34) (cid:88)N (cid:13) (cid:13)θθθi t+1−yi(ΦΦΦ t)+yi(ΦΦΦ t)−yi(ΦΦΦ∗)(cid:13) (cid:13)2(cid:35)
i=1
(cid:104) (cid:105)
≤β /α E[∥ΦΦΦ −ΦΦΦ∗∥2]+3β α δ2E ∥ΦΦΦ −ΦΦΦ∗∥2 +3β α δ2
t t t−τ t t t−τ t t
+ 6β t Nα tδ2 E(cid:34) (cid:88)N (cid:13) (cid:13)θθθi t+1−yi(ΦΦΦ t)(cid:13) (cid:13)2(cid:35) +6β tα tL2δ2E(cid:104) ∥ΦΦΦ t−ΦΦΦ∗∥2(cid:105)
i=1
≤(2β /α +6β α δ2)E[∥ΦΦΦ −ΦΦΦ ∥2]+(2β /α +6β α δ2+6β α L2δ2)E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t t t−τ t t t t t t t t
+3β tα tδ2+ 6β t Nα tδ2 E(cid:34) (cid:88)N (cid:13) (cid:13)θθθi t+1−yi(ΦΦΦ t)(cid:13) (cid:13)2(cid:35) ,
i=1
wherethelastinequalitycomesfromE[∥ΦΦΦ −ΦΦΦ∗∥2]≤2E[∥ΦΦΦ −ΦΦΦ ∥2]+2E[∥ΦΦΦ −ΦΦΦ∗∥2].
t−τ t−τ t t
Hence,wehaveTerm asfollows
3
Term =C +C +C +C
3 1 2 3 4
≤β /α E[∥ΦΦΦ −ΦΦΦ ∥2]+4β α L2(3+4L2)E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t t t−τ t t t
+
16β tα tL2 E(cid:34) (cid:88)N
∥θθθi −yi(ΦΦΦ
)∥2(cid:35)
+8β α δ2
N t+1 t t t
i=1
+(2β /α +β α L2)E[∥ΦΦΦ −ΦΦΦ ∥2]+2β /α E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t t t−τ t t t t
+(2β /α +6β α δ2)E[∥ΦΦΦ −ΦΦΦ ∥2]
t t t t t−τ t
+(2β /α +6β α δ2+6β α L2δ2)E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t t t t t
+3β tα tδ2+ 6β t Nα tδ2 E(cid:34) (cid:88)N (cid:13) (cid:13)θθθi t+1−yi(ΦΦΦ t)(cid:13) (cid:13)2(cid:35)
i=1
+(2β /α +β α L2)E[∥ΦΦΦ −ΦΦΦ ∥2]+2β /α E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t t t−τ t t t t
≤(7β /α +2β α L2+6β α δ2)E[∥ΦΦΦ −ΦΦΦ ∥2]
t t t t t t t−τ t
+(6β /α +6β α δ2(1+L2)+4β α L2(3+4L2))E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t t t t t
+
16β tα tL2+6β tα tδ2 E(cid:34) (cid:88)N
∥yi(ΦΦΦ )−θθθi
∥2(cid:35)
+11β α δ2,
N t t+1 t t
i=1
whichcompletestheproof.
27ToboundTerm ,weneedtoboundE[∥ΦΦΦ −ΦΦΦ ∥2],whichisshowninthefollowinglemma.
3 t t−τ
LemmaF.5. wehave∀t≥2τ
E[∥ΦΦΦ −ΦΦΦ ∥2]≤4τ2β2/α2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+8β2L2B2τ2+8β2δ2τ2. (31)
t t−τ 0 0 t 0 0
Proof. The proof follows similar procedures of proof for Lemma 3 in Dal Fabbro et al. (2023).
Startingwith
(cid:13) (cid:13)2
β2 (cid:13)(cid:88)N (cid:13) 1 (cid:88)N
∥ΦΦΦ∗−ΦΦΦ ∥2 =∥ΦΦΦ∗−ΦΦΦ ∥2+ t (cid:13) h(θθθi ,ΦΦΦ )(cid:13) −2β ⟨ΦΦΦ∗−ΦΦΦ , h(θθθi ,ΦΦΦ )⟩
t+1 t N2 (cid:13) t+1 t (cid:13) t t N t+1 t
(cid:13) (cid:13)
i=1 i=1
(cid:13) (cid:13)2
≤(1+β /α )∥ΦΦΦ∗−ΦΦΦ ∥2+
(β tα 0+β t2)(cid:13) (cid:13)(cid:88)N
hi(θθθi ,ΦΦΦ
)(cid:13)
(cid:13)
t 0 t N2 (cid:13) t t+1 t (cid:13)
(cid:13) (cid:13)
i=1
(cid:13) (cid:13)2
≤(1+β /α )∥ΦΦΦ∗−ΦΦΦ ∥2+
2β tα
0
(cid:13) (cid:13)(cid:88)N
hi(θθθi ,ΦΦΦ
)(cid:13)
(cid:13) , (32)
t 0 t N2 (cid:13) t t+1 t (cid:13)
(cid:13) (cid:13)
i=1
wherethefirstinequalityholdsdueto2xTy≤γ∥x∥2+1/γ∥y∥2,∀γ >0,andthesecondinequality
holdssinceβ α ≥β2. WethenhavethefollowinginequalityaccordingtoLemmaF.2,
t 0 t
E(cid:2) ∥ΦΦΦ∗−ΦΦΦ ∥2(cid:3) ≤(1+β /α +8β α L2(1+L2))E(cid:2) ∥ΦΦΦ∗−ΦΦΦ ∥2(cid:3)
t+1 t 0 t 0 t
+
8β tα 0L2 E(cid:34) (cid:88)N
∥θθθ −yi(ΦΦΦ
)∥2(cid:35)
+8β α δ2
N t+1 t t 0
i=1
≤(1+β /α +8β α L2(1+L2))E(cid:2) ∥ΦΦΦ∗−ΦΦΦ ∥2(cid:3) +8β α (L2B2+δ2).
t 0 t 0 t t 0
(33)
Bylettingα ≤ √ 1 ,wehaveβ /α ≥8β α L2(1+L2),andhence
0 t 0 t 0
2L 2(1+L2)
E(cid:2) ∥ΦΦΦ∗−ΦΦΦ ∥2(cid:3) ≤(1+2β /α )E(cid:2) ∥ΦΦΦ∗−ΦΦΦ ∥2(cid:3) +8β α (L2B2+δ2). (34)
t+1 0 0 t 0 0
Therefore,forallt′suchthatt−τ ≤t′ ≤t,
τ−1
(cid:88)
E[∥ΦΦΦ∗−ΦΦΦ ∥2]≤(1+2β /α )τE[∥ΦΦΦ∗−ΦΦΦ ∥2]+8β α (L2B2+δ2) (1+2β /α )ℓ.
t′ 0 0 t−τ 0 0 0 0
ℓ=0
(35)
Usingthefactthat(1+x)≤ex(DalFabbroetal.,2023),ifweletβ /α ≤ 1 ,wehave
0 0 8τ
(1+2β /α )ℓ ≤(1+2β /α )τ ≤e0.25 ≤2,
0 0 0 0
and
τ−1
(cid:88)
(1+32β2)ℓ ≤2τ.
ℓ=0
Hence,wehave
E[∥ΦΦΦ∗−ΦΦΦ ∥2]≤2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+16β α τ(L2B2+δ2).
t′ t−τ 0 0
Since ∥ΦΦΦ −ΦΦΦ ∥2 ≤ τ(cid:80)t−1 ∥ΦΦΦ −ΦΦΦ ∥2 = τ β2 (cid:80)t−1 ∥(cid:80)N hi(θθθi ,ΦΦΦ )∥2, when
t t−τ ℓ=t−τ ℓ+1 ℓ N2 ℓ=t−τ i=1 ℓ ℓ+1 ℓ
t≥2τ,wehaveℓ≥τ andthus
E[∥ΦΦΦ −ΦΦΦ ∥2]
t t−τ
β2 (cid:88)t−1 (cid:88)N
≤τ ∥ hi(θθθi ,ΦΦΦ )∥2
N2 ℓ ℓ+1 ℓ
ℓ=t−τ i=1
t−1
(cid:88)
≤τ ((4β2(L2+L4)E[∥ΦΦΦ∗−ΦΦΦ ∥2]+4β2L2B2τ2+4β2δ2τ2
0 ℓ 0 0
ℓ=t−τ
28≤4β2(L2+L4)τ2(2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+16β α τ(L2B2+δ2))+4β2L2B2τ2+4β2δ2τ2
0 t−τ 0 0 0 0
=8β2(L2+L4)τ2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+4β2L2B2τ2+4β2δ2τ2
0 t−τ 0 0
≤τ2β2/α2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+4β2L2B2τ2+4β2δ2τ2
0 0 t−τ 0 0
≤2τ2β2/α2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+2τ2β2/α2E[∥ΦΦΦ −ΦΦΦ ∥2]+4β2L2B2τ2+4β2δ2τ2.
0 0 t 0 0 t t−τ 0 0
Since2τ2β2/α2 ≤1/2whenβ /α ≤ 1 ,wehave
0 0 0 0 8τ
E[∥ΦΦΦ −ΦΦΦ ∥2]≤4τ2β2/α2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+8β2L2B2τ2+8β2δ2τ2.
t t−τ 0 0 t 0 0
Thiscompletestheproof.
LemmaF.6. Term isboundedasfollows
3
Term ≤(7β /α +2β α L2+6β α δ2)(4τ2β2/α2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+8β2L2B2τ2+8β2δ2τ2)
3 t t t t t t 0 0 t 0 0
+(6β /α +6β α δ2(1+L2)+4β α L2(3+4L2))E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t t t t t
+
16β tα tL2+6β tα tδ2 E(cid:34) (cid:88)N
∥θθθi,∗−θθθ
∥2(cid:35)
+11β α δ2.
N t+1 t t
i=1
Proof. SubstitutingtheboundofE[∥ΦΦΦ −ΦΦΦ ∥2]in(31)intoTerm inLemmaF.4yieldthefinal
t t−τ 3
results.
Provided Term in Lemma F.2, Term in Lemma F.3, and Term in Lemma F.6, we have the
1 2 3
followinglemmatocharacterizethedriftbetweenΦΦΦ andΦΦΦ .
t+1 t
LemmaF.7. Fort≥2τ,thefollowingholds
E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t+1
≤(1+4β2(L2+L4)+(7β /α +2β α L2+6β α δ2)4τ2β2/α2
t t t t t t t 0 0
+(6β /α +6β α δ2(1+L2)+4β α L2(3+4L2))+β (L/α −2ω))E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t t t t t t t
+
4β t2L2+β tα tL+16β tα tL2+6β tα tδ2 E(cid:34) (cid:88)N
∥θθθi,∗−θθθ
∥2(cid:35)
N t+1
i=1
+(7β /α +2β α L2+6β α δ2)(8β2L2B2τ2+8β2δ2τ2)+4β2δ2+11β α δ2.
t t t t t t 0 0 t t t
Proof. SubstitutingTerm ,Term andTerm backintoLemmaF.1,wehave
1 2 3
E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t+1
4β2L2 (cid:34) (cid:88)N (cid:35)
≤E[∥ΦΦΦ∗−ΦΦΦ ∥2]+4β2(L2+L4)E[∥ΦΦΦ∗−ΦΦΦ ∥2]+ t E ∥θθθ −yi(ΦΦΦ )∥2 +4β2δ2
t t t N t+1 t t
i=1
(cid:34) N (cid:35)
+β (L/α −2ω)E[∥ΦΦΦ∗−ΦΦΦ ∥2]+
β tα tL
E
(cid:88)
∥θθθi −yi(ΦΦΦ )∥2
t t t N t+1 t
i=1
+(7β /α +2β α L2+6β α δ2)(4τ2β2/α2E[∥ΦΦΦ∗−ΦΦΦ ∥2]+8β2L2B2τ2+8β2δ2τ2)
t t t t t t 0 0 t 0 0
+(6β /α +6β α δ2(1+L2)+4β α L2(3+4L2))E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t t t t t
+
16β tα tL2+6β tα tδ2 E(cid:34) (cid:88)N
∥θθθi,∗−θθθ
∥2(cid:35)
+11β α δ2
N t+1 t t
i=1
=(1+4β2(L2+L4)+(7β /α +2β α L2+6β α δ2)4τ2β2/α2
t t t t t t t 0 0
+(6β /α +6β α δ2(1+L2)+4β α L2(3+4L2))+β (L/α −2ω))E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t t t t t t t
+
4β t2L2+β tα tL+16β tα tL2+6β tα tδ2 E(cid:34) (cid:88)N
∥θθθi,∗−θθθ
∥2(cid:35)
N t+1
i=1
+(7β /α +2β α L2+6β α δ2)(8β2L2B2τ2+8β2δ2τ2)+4β2δ2+11β α δ2.
t t t t t t 0 0 t t t
Thiscompletestheproof.
29F.1.2 DRIFTOFθθθi,∀i.
t
Next,wecharacterizethedriftbetweenθθθi andθθθi.
t+1 t
LemmaF.8. Thedriftbetweenθθθi andθθθi,∀iisgivenby
t+1 t
(cid:13) (cid:13)2
E[∥θθθi t+1−yi(ΦΦΦ t)∥2]=E (cid:13) (cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)+α t(cid:88)K g(θθθi t,k−1,ΦΦΦ t)(cid:13) (cid:13)
(cid:13)
+E(cid:104)(cid:13) (cid:13)yi(ΦΦΦ t−1)−yi(ΦΦΦ t)(cid:13) (cid:13)2(cid:105)
(cid:13) (cid:13)
k=1 (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) Term5
Term4
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
+2E θθθi−yi(ΦΦΦ )+α g(θθθi ,ΦΦΦ ),yi(ΦΦΦ )−yi(ΦΦΦ ) .
t t−1 t t,k−1 t t−1 t
k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Term6
(36)
Proof. Accordingtotheupdateofθθθi in(7),wehave
t
(cid:13) (cid:13)2
(cid:13) (cid:88)K (cid:13)
E[∥θθθi t+1−yi(ΦΦΦ t)∥2]=E (cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)+α t g(θθθi t,k−1,ΦΦΦ t)+yi(ΦΦΦ t−1)−yi(ΦΦΦ t)(cid:13) (cid:13) 
(cid:13) (cid:13)
k=1
(cid:13) (cid:13)2
=E (cid:13) (cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)+α t(cid:88)K g(θθθi t,k−1,ΦΦΦ t)(cid:13) (cid:13)
(cid:13)
+E(cid:104)(cid:13) (cid:13)yi(ΦΦΦ t−1)−yi(ΦΦΦ t)(cid:13) (cid:13)2(cid:105)
(cid:13) (cid:13)
k=1 (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) Term5
Term4
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
+2E θθθi−yi(ΦΦΦ )+α g(θθθi ,ΦΦΦ ),yi(ΦΦΦ )−yi(ΦΦΦ ) ,
t t−1 t t,k−1 t t−1 t
k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Term6
(37)
wherethesecondinequalityholdsdueto∥x+y∥2 =∥x∥2+∥y∥2+2⟨x,y⟩.
Wenextanalyzeeachtermin(37). First,weboundTerm inthefollowinglemma.
4
LemmaF.9. Witht≥τ,wehaveTerm boundedas
4
Term
4
≤(1+2β t−1/α t−2α tKω)E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105)
+(12α2δ2K2+6K2δ2α3/β )E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t−1 t−1
+(12α2δ2K2+2L2α3/β +6K2δ2α3/β )E[∥ΦΦΦ −ΦΦΦ ∥2]
t t t−1 t t−1 t t−1
+6α2δ2K2(1+B2)+2α2K2L2B2+2L2K2B2α3/β +α3/β (3K2B2+3K2δ2).
t t t t−1 t t−1
(38)
Proof. AccordingtothedefinitionofTerm ,wehave
4
(cid:13) (cid:13)2
(cid:13) (cid:88)K (cid:13)
Term 4 =E (cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)+α t g(θθθi t,k−1,ΦΦΦ t)(cid:13) (cid:13) 
(cid:13) (cid:13)
k=1
(cid:13) (cid:13)2
=E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105) +α t2E (cid:13) (cid:13) (cid:13)(cid:88)K g(θθθi t,k−1,ΦΦΦ t)(cid:13) (cid:13) (cid:13) 
(cid:13) (cid:13)
k=1
(cid:42) K (cid:43)
(cid:88)
+2α θθθi−yi(ΦΦΦ ), g(θθθi ,ΦΦΦ )
t t t−1 t,k−1 t
k=1
30(cid:34)(cid:42) K (cid:43)(cid:35)
=E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105) +2α tE θθθi t−yi(ΦΦΦ t−1),(cid:88) g(θθθi t,k−1,ΦΦΦ t)
k=1
(cid:13) (cid:13)2
(cid:13)(cid:88)K (cid:88)K (cid:88)K (cid:88)K (cid:13)
+α t2E (cid:13) (cid:13) g(θθθi t,k−1,ΦΦΦ t)− g¯(θθθi t,k−1,ΦΦΦ t)+ g¯(θθθi t,k−1,ΦΦΦ t)− g¯(yi(ΦΦΦ t),ΦΦΦ t)(cid:13) (cid:13) 
(cid:13) (cid:13)
k=1 k=1 k=1 k=1
(cid:13) (cid:13)2
≤E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105) +2α t2E (cid:13) (cid:13) (cid:13)(cid:88)K g(θθθi t,k−1,ΦΦΦ t)−(cid:88)K g¯(θθθi t,k−1,ΦΦΦ t)(cid:13) (cid:13) (cid:13) 
(cid:13) (cid:13)
k=1 k=1
(cid:124) (cid:123)(cid:122) (cid:125)
MixingtimepropertyinDefinition4.3
(cid:13) (cid:13)2
(cid:13)(cid:88)K (cid:88)K (cid:13)
+2α t2E (cid:13) (cid:13) g¯(θθθi t,k−1,ΦΦΦ t)− g¯(yi(ΦΦΦ t),ΦΦΦ t)(cid:13) (cid:13) 
(cid:13) (cid:13)
k=1 k=1
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
+2α E θθθi−yi(ΦΦΦ ), g(θθθi ,ΦΦΦ )
t t t−1 t,k−1 t
k=1
≤E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105) +6α t2δ2K2E(cid:104) ∥ΦΦΦ t−ΦΦΦ∗∥2(cid:105) +6α t2δ2K2(1+B2)+2α t2K2L2B2
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
+2α E θθθi−yi(ΦΦΦ ), g¯(θθθi ,ΦΦΦ )
t t t−1 t,k−1 t
k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Term41
(cid:34)(cid:42) K K (cid:43)(cid:35)
(cid:88) (cid:88)
+2α E θθθi−yi(ΦΦΦ ), g(θθθi ,ΦΦΦ )− g¯(θθθi ,ΦΦΦ ) , (39)
t t t−1 t,k−1 t t,k−1 t
k=1 k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Term42
where the first inequality holds due to the fact that ∥x+y∥2 ≤ 2∥x∥2 +2∥y∥2, and the second
inequalityisduetothemixingtimepropertyoffunctiongasinDefinition4.3.
Next,weboundTerm as
41
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
Term =2α E θθθi−yi(ΦΦΦ ), g¯(θθθi ,ΦΦΦ )
41 t t t−1 t,k−1 t
k=1
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
=2α E θθθi−yi(ΦΦΦ ), g¯(θθθi,ΦΦΦ )
t t t−1 t t−1
k=1
(cid:34)(cid:42) K K (cid:43)(cid:35)
(cid:88) (cid:88)
+2α E θθθi−yi(ΦΦΦ ), g¯(θθθi ,ΦΦΦ )− g¯(θθθi,ΦΦΦ )
t t t−1 t,k−1 t t t−1
k=1 k=1
≤−2α KωE(cid:2) ∥θθθi−yi(ΦΦΦ )∥2(cid:3)
t t t−1
(cid:34)(cid:42) K K (cid:43)(cid:35)
(cid:88) (cid:88)
+2α E θθθi−yi(ΦΦΦ ), g¯(θθθi ,ΦΦΦ )− g¯(θθθi,ΦΦΦ )
t t t−1 t,k−1 t t t−1
k=1 k=1
≤−2α tKωE(cid:2) ∥θθθi t−yi(ΦΦΦ t−1)∥2(cid:3) +β t−1/α tE(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105)
(cid:13) (cid:13)2
(cid:13)(cid:88)K (cid:88)K (cid:13)
+α t3/β t−1E (cid:13)
(cid:13)
g¯(θθθi t,k−1,ΦΦΦ t)− g¯(θθθi t,ΦΦΦ t−1)(cid:13)
(cid:13)
. (40)
(cid:13) (cid:13)
k=1 k=1
(cid:20)(cid:13) (cid:13)2(cid:21)
Inparticular,wecanboundE (cid:13)(cid:80)K g¯(θθθi ,ΦΦΦ )−(cid:80)K g¯(θθθi,ΦΦΦ )(cid:13) as
(cid:13) k=1 t,k−1 t k=1 t t−1 (cid:13)
(cid:13) (cid:13)2
(cid:13)(cid:88)K (cid:88)K (cid:13)
E (cid:13) (cid:13) g¯(θθθi t,k−1,ΦΦΦ t)− g¯(θθθi t,ΦΦΦ t−1)(cid:13) (cid:13) 
(cid:13) (cid:13)
k=1 k=1
31(cid:13) (cid:13)2
≤2L2E(cid:104)
∥ΦΦΦ t−ΦΦΦ
t−1∥2(cid:105)
+2L2E
(cid:13)
(cid:13)
(cid:13)(cid:88)K
θθθ t,k−1−θθθ
t(cid:13)
(cid:13) (cid:13) 
(cid:13) (cid:13)
k=1
(cid:34) K (cid:35)
≤2L2E(cid:104) ∥ΦΦΦ −ΦΦΦ ∥2(cid:105) +2L2KE (cid:88) ∥θθθ −θθθ ∥2
t t−1 t,k−1 t
k=1
(cid:104) (cid:105)
≤2L2E ∥ΦΦΦ −ΦΦΦ ∥2 +2L2K2B2. (41)
t t−1
Substituting(41)backinto(40),wehaveTerm boundedas
41
Term
41
≤−2α tKωE(cid:2) ∥θθθi t−yi(ΦΦΦ t−1)∥2(cid:3) +β t−1/α tE(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105)
(cid:104) (cid:105)
+α3/β (2L2E ∥ΦΦΦ −ΦΦΦ ∥2 +2L2K2B2). (42)
t t−1 t t−1
WenextboundTerm as
42
(cid:34)(cid:42) K K (cid:43)(cid:35)
(cid:88) (cid:88)
Term =2α E θθθi−yi(ΦΦΦ ), g(θθθi ,ΦΦΦ )− g¯(θθθi ,ΦΦΦ )
42 t t t−1 t,k−1 t t,k−1 t
k=1 k=1
≤β t−1/α tE(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105) +α t3/β t−1(3K2B2+3K2δ2+3K2δ2E[∥ΦΦΦ t−ΦΦΦ∗∥2])
≤β t−1/α tE(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105) +α t3/β t−1(3K2B2+3K2δ2)
+6K2δ2α3/β E[∥ΦΦΦ −ΦΦΦ∗∥2]+6K2δ2α3/β E[∥ΦΦΦ −ΦΦΦ ∥2] (43)
t t−1 t−1 t t−1 t t−1
SubstitutingTerm andTerm backinto(39),wegetthefinalresult
41 42
Term
4
≤E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105) +6α t2δ2K2E(cid:104) ∥ΦΦΦ t−ΦΦΦ∗∥2(cid:105) +6α t2δ2K2(1+B2)+2α t2K2L2B2
+Term +Term
41 42
=E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105) +6α t2δ2K2E(cid:104) ∥ΦΦΦ t−ΦΦΦ∗∥2(cid:105) +6α t2δ2K2(1+B2)+2α t2K2L2B2
−2α tKωE(cid:2) ∥θθθi t−yi(ΦΦΦ t−1)∥2(cid:3) +β t−1/α tE(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105)
(cid:104) (cid:105)
+α3/β (2L2E ∥ΦΦΦ −ΦΦΦ ∥2 +2L2K2B2)
t t−1 t t−1
+β t−1/α tE(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105) +α t3/β t−1(3K2B2+3K2δ2)
+6K2δ2α3/β E[∥ΦΦΦ −ΦΦΦ∗∥2]+6K2δ2α3/β E[∥ΦΦΦ −ΦΦΦ ∥2]
t t−1 t−1 t t−1 t t−1
≤(1+2β t−1/α t−2α tKω)E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105)
+(12α2δ2K2+6K2δ2α3/β )E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t−1 t−1
+(12α2δ2K2+2L2α3/β +6K2δ2α3/β )E[∥ΦΦΦ −ΦΦΦ ∥2]
t t t−1 t t−1 t t−1
+6α2δ2K2(1+B2)+2α2K2L2B2+2L2K2B2α3/β
t t t t−1
+α3/β (3K2B2+3K2δ2) (44)
t t−1
Thiscompletestheproof.
Next,weboundTerm inthefollowinglemma.
5
LemmaF.10. Witht≥τ,wehaveTerm boundedas
5
4β2 L4 (cid:34) (cid:88)N (cid:35)
Term ≤4β2 (L4+L6)E[∥ΦΦΦ∗−ΦΦΦ ∥2]+ t−1 E ∥θθθ −yi(ΦΦΦ )∥2 +4L2β2 δ2.
5 t−1 t−1 N t t−1 t−1
i=1
(45)
32Proof. Wehave
Term
5
=E(cid:104)(cid:13) (cid:13)yi(ΦΦΦ t−1)−yi(ΦΦΦ t)(cid:13) (cid:13)2(cid:105) =L2E(cid:2) ∥ΦΦΦ t−ΦΦΦ t−1∥2(cid:3)
(cid:13) (cid:13)2
L2β2 (cid:13)(cid:88)N (cid:13)
= Nt 2−1E (cid:13) (cid:13) h(θθθi t,ΦΦΦ t−1)(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
4β2 L4 (cid:34) (cid:88)N (cid:35)
≤4β2 (L4+L6)E[∥ΦΦΦ∗−ΦΦΦ ∥2]+ t−1 E ∥θθθ −yi(ΦΦΦ )∥2 +4L2β2 δ2,
t−1 t−1 N t t−1 t−1
i=1
(46)
wherethelastinequalityholdsduetoLemmaF.2.
Next,weboundTerm inthefollowinglemma.
6
LemmaF.11. WehaveTerm boundedas
6
Term ≤β /α Term +α /β Term . (47)
6 t−1 t 4 t t−1 5
Proof.
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
Term =2E θθθi−yi(ΦΦΦ )+α g(θθθi ,ΦΦΦ ),yi(ΦΦΦ )−yi(ΦΦΦ )
6 t t−1 t t,k−1 t t−1 t
k=1
(cid:13) (cid:13)2
≤β t−1/α tE (cid:13) (cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)+α t(cid:88)K g(θθθi t,k−1(cid:13) (cid:13)
(cid:13)
+α t/β t−1E(cid:104)(cid:13) (cid:13)yi(ΦΦΦ t−1)−yi(ΦΦΦ t)(cid:13) (cid:13)2(cid:105)
(cid:13) (cid:13)
k=1 (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) Term5
Term4
(48)
ProvidingTerm inLemmaF.9,Term inLemmaF.10,andTerm inLemmaF.11,wehave
4 5 6
thefollowingresult.
LemmaF.12. Fort≥τ,thefollowingholds
E[∥θθθi −yi(ΦΦΦ )∥2]
t+1 t
(cid:34) (cid:32)
≤ (1+β /α ) (1+2β /α −2α Kω)
t−1 t t−1 t t
(cid:33) (cid:35)
4β2 L2 4β2 L4
+(12α2δ2K2+2L2α3/β +6K2δ2α3/β ) t−1 +(1+α /β ) t−1
t t t−1 t t−1 N t t−1 N
·E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105)
(cid:34) (cid:32)
+ (1+β /α ) (12α2δ2K2+6K2δ2α3/β )
t−1 t t t t−1
(cid:33)
+(12α2δ2K2+2L2α3/β +6K2δ2α3/β )(4β (L2+L4))
t t t−1 t t−1 t−1
(cid:35)
+(1+α /β )4β2 (L4+L6) ·E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t−1 t−1 t−1
(cid:16)
+(1+β /α ) (12α2δ2K2+2L2α3/β +6K2δ2α3/β )4β2 δ2
t−1 t t t t−1 t t−1 t−1
(cid:17)
+6α2δ2K2(1+B2)+2α2K2L2B2+2L2K2B2α3/β +α3/β (3K2B2+3K2δ2)
t t t t−1 t t−1
+(1−α /β )·4L2β2 δ2. (49)
t t+1 t−1
33Proof. Accordingto(36),wehave
E[∥θθθi −yi(ΦΦΦ )∥2]=Term +Term +Term
t+1 t 4 5 6
LemmaF.11
≤ (1+β /α )Term +(1+α /β )Term
t−1 t 4 t t−1 5
(cid:32)
≤(1+β t−1/α t) (1+2β t−1/α t−2α tKω)E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105)
+(12α2δ2K2+6K2δ2α3/β )E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t−1 t−1
+(12α2δ2K2+2L2α3/β +6K2δ2α3/β )E[∥ΦΦΦ −ΦΦΦ ∥2]
t t t−1 t t−1 t t−1
(cid:33)
+6α2δ2K2(1+B2)+2α2K2L2B2+2L2K2B2α3/β +α3/β (3K2B2+3K2δ2)
t t t t−1 t t−1
(cid:32)
+(1+α /β ) 4β2 (L4+L6)E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t−1 t−1 t−1
4β2 L4 (cid:34) (cid:88)N (cid:35) (cid:33)
+ t−1 E ∥θθθ −yi(ΦΦΦ )∥2 +4L2β2 δ2
N t t−1 t−1
i=1
(cid:32)
Lemm ≤aF.10 (1+β t−1/α t) (1+2β t−1/α t−2α tKω)E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105)
+(12α2δ2K2+6K2δ2α3/β )E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t−1 t−1
+(12α2δ2K2+2L2α3/β +6K2δ2α3/β )
t t t−1 t t−1
(cid:16) 4β2 L2 (cid:34) (cid:88)N (cid:35) (cid:17)
· 4β2 (L2+L4)E[∥ΦΦΦ∗−ΦΦΦ ∥2]+ t−1 E ∥θθθ −yi(ΦΦΦ )∥2 +4β2 δ2
t−1 t−1 N t t−1 t−1
i=1
(cid:33)
+6α2δ2K2(1+B2)+2α2K2L2B2+2L2K2B2α3/β +α3/β (3K2B2+3K2δ2)
t t t t−1 t t−1
(cid:32)
+(1+α /β ) 4β2 (L4+L6)E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t−1 t−1 t−1
4β2 L4 (cid:34) (cid:88)N (cid:35) (cid:33)
+ t−1 E ∥θθθ −yi(ΦΦΦ )∥2 +4L2β2 δ2
N t t−1 t−1
i=1
(cid:34) (cid:32)
= (1+β /α ) (1+2β /α −2α Kω)
t−1 t t−1 t t
(cid:33) (cid:35)
4β2 L2 4β2 L4
+(12α2δ2K2+2L2α3/β +6K2δ2α3/β ) t−1 +(1+α /β ) t−1
t t t−1 t t−1 N t t−1 N
·E(cid:104)(cid:13) (cid:13)θθθi t−yi(ΦΦΦ t−1)(cid:13) (cid:13)2(cid:105)
(cid:34) (cid:32)
+ (1+β /α ) (12α2δ2K2+6K2δ2α3/β )
t−1 t t t t−1
(cid:33)
+(12α2δ2K2+2L2α3/β +6K2δ2α3/β )(4β (L2+L4))
t t t−1 t t−1 t−1
(cid:35)
+(1+α /β )4β2 (L4+L6) ·E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t t−1 t−1 t−1
(cid:16)
+(1+β /α ) (12α2δ2K2+2L2α3/β +6K2δ2α3/β )4β2 δ2
t−1 t t t t−1 t t−1 t−1
34(cid:17)
+6α2δ2K2(1+B2)+2α2K2L2B2+2L2K2B2α3/β +α3/β (3K2B2+3K2δ2)
t t t t−1 t t−1
+(1+α /β )·4L2β2 δ2. (50)
t t−1 t−1
Thiscompletestheproof.
F.1.3 FINALSTEPOFPROOFFORTHEOREM4.13
Now,wearereadytoproofthedesiredresultinTheorem4.13.
AccordingtothedefinitionofLyapunovfunctionin(14),Wehave
N
M({θθθi },ΦΦΦ )=∥ΦΦΦ −ΦΦΦ∗∥2+ β t · 1 (cid:88) ∥θθθi −yi(ΦΦΦ )∥2
t+2 t+1 t+1 α N t+2 t+1
t+1
i=1
≤(1+4β2(L2+L4)+(7β /α +2β α L2+6β α δ2)4τ2β2/α2
t t t t t t t 0 0
+(6β /α +6β α δ2(1+L2)+4β α L2(3+4L2))+β (L/α −2ω))E[∥ΦΦΦ −ΦΦΦ∗∥2]
t t t t t t t t t
+
4β t2L2+β tα tL+16β tα tL2+6β tα tδ2 E(cid:34) (cid:88)N
∥θθθi −yi(ΦΦΦ
)∥2(cid:35)
N t+1 t
i=1
+(7β /α +2β α L2+6β α δ2)(8β2L2B2τ2+8β2δ2τ2)+4β2δ2+11β α δ2
t t t t t t 0 0 t t t
(cid:34) (cid:32)
β
+ t · (1+β /α ) (1+2β /α −2α Kω)
α t t+1 t t+1 t+1
t+1
(cid:33) (cid:35)
4β2L2 4β2L4
+(12α2 δ2K2+2L2α3 /β +6K2δ2α3 /β ) t +(1+α /β ) t
t+1 t+1 t t+1 t N t+1 t N
(cid:34) N (cid:35)
· N1 E (cid:88)(cid:13) (cid:13)θθθi t+1−yi(ΦΦΦ t)(cid:13) (cid:13)2
i=1
(cid:34) (cid:32)
+ (1+β /α ) (12α2 δ2K2+6K2δ2α3 /β )
t t+1 t+1 t+1 t
(cid:33)
+(12α2 δ2K2+2L2α3 /β +6K2δ2α3 /β )(4β (L2+L4))
t+1 t+1 t t+1 t t
(cid:35)
+(1+α /β )4β2(L4+L6) ·E[∥ΦΦΦ∗−ΦΦΦ ∥2]
t+1 t t t
(cid:16)
+(1+β /α ) (12α2 δ2K2+2L2α3 /β +6K2δ2α3 /β )4β2δ2
t t+1 t+1 t+1 t t+1 t t
(cid:17)
+6α2 δ2K2(1+B2)+2α2 K2L2B2+2L2K2B2α3 /β +α3 /β (3K2B2+3K2δ2)
t+1 t+1 t+1 t t+1 t
(cid:35)
+(1+α /β )·4L2β2δ2 . (51)
t+1 t t
Tosimplifythenotations,wedefine
D :=(4β2(L2+L4)+(7β /α +2β α L2+6β α δ2)4τ2β2/α2
1 t t t t t t t 0 0
+(6β /α +6β α δ2(1+L2)+4β α L2(3+4L2))+β L/α )
t t t t t t t t
(cid:34) (cid:32)
β
+ t (1+β /α ) (12α2 δ2K2+6K2δ2α3 /β )
α t t+1 t+1 t+1 t
t+1
(cid:33)
+(12α2 δ2K2+2L2α3 /β +6K2δ2α3 /β )(4β (L2+L4))
t+1 t+1 t t+1 t t
(cid:35)
+(1+α /β )4β2(L4+L6) , (52)
t+1 t t
35and
D :=4β3/α L2+α2L+16α2L2+6α α δ2
2 t t+1 t t t t
(cid:34)(cid:32) (cid:33) (cid:35)
4β2L2 4β2L4
+ (2β /α )+(12α2 δ2K2+2L2α3 /β +6K2δ2α3 /β ) t +(1+α /β ) t
t t+1 t+1 t+1 t t+1 t N t+1 t N
(cid:34) (cid:32)
+ β /α (1+2β /α −2α Kω)
t t+1 t t+1 t+1
(cid:33) (cid:35)
4β2L2 4β2L4
+(12α2 δ2K2+2L2α3 /β +6K2δ2α3 /β ) t +(1+α /β ) t . (53)
t+1 t+1 t t+1 t N t+1 t N
SinceD isofhigherordersofo(β )andD isofhigherorderofo(α ), wecanletD ≤ ωβ
1 t 2 t+1 1 t
andD ≤Kωα . Therefore,wehave
2 t+1
M({θθθi },ΦΦΦ )≤(1−ωβ )M({θθθi },ΦΦΦ )
t+2 t+1 t t+1 t
(cid:34) (cid:34) N (cid:35)(cid:35)
+(144τ2K2L2δ2+4L4/N)β tα
t+1
E[∥ΦΦΦ t−ΦΦΦ∗∥2]+ N1 E (cid:88)(cid:13) (cid:13)θθθi t+1−yi(ΦΦΦ t)(cid:13) (cid:13)2
i=1
+4α β K2(3δ2(1+B2)+L2B2)+2α2 (3K2B2+3K2δ2+2L2K2B2)+8α β δ2
t+1 t t+1 t+1 t
≤(1−ωβ )M({θθθi },ΦΦΦ )
t t+1 t
(cid:34) (cid:34) N (cid:35)(cid:35)
+(144τ2K2L2δ2+4L2/N)β tα
t
E[∥ΦΦΦ t−ΦΦΦ∗∥2]+ N1 E (cid:88)(cid:13) (cid:13)θθθi t+1−yi(ΦΦΦ t)(cid:13) (cid:13)2
i=1
+4α β K2(3δ2(1+B2)+L2B2)+2α2(3K2B2+3K2δ2+2L2K2B2)+8α β δ2, (54)
t t t t t
where the first inequality holds by omitting the higher order of learning rates, and the second in-
equalityholdsduetothedecreasinglearningratesofα .
t
Wenowsettheproperdecayinglearningrates. Letα = α /(t+2)5/6 andβ = β /(t+2). We
t 0 t 0
thenhave
(t+2)2·(1−ωβ )=(t+2)2(1−ωβ )/(t+2)≤(t+1)2, (55)
t 0
ifωβ <2. Inaddition,wehavethefollowinginequalities
o
(t+2)2·α β ≤α β (t+2)1/3,
t t 0 0
(t+2)2·α2 =α2(t+2)2.
t 0
Hence,multiplyingbothsideswith(t+2)2,wehave
(t+2)2M({θθθi },ΦΦΦ )≤(t+1)2M({θθθi },ΦΦΦ )
t+2 t+1 t+1 t
(cid:34) (cid:34) N (cid:35)(cid:35)
1 (cid:88)
+(144τ2K2L2δ2+4L2/N)α β0(t+2)1/3 E[∥ΦΦΦ −ΦΦΦ∗∥2]+ E ∥θθθi −yi(ΦΦΦ )∥2
0 t N t+1 t
i=1
+(4α β K2(3δ2(1+B2)+L2B2)+2α2(3K2B2+3K2δ2+2L2K2B2)+8α β δ2)(t+2)1/3.
0 0 0 0 0
Summingtheaboveequationfromt=0,...,T,wehave
(T +2)2M({θθθi },ΦΦΦ )≤M({θθθi},ΦΦΦ )
t+2 t+1 1 0
(cid:34) (cid:34) N (cid:35)(cid:35)
1 (cid:88)
+(144τ2K2L2δ2+4L2/N)α β0(T +2)4/3 E[∥ΦΦΦ −ΦΦΦ∗∥2]+ E ∥θθθi −yi(ΦΦΦ )∥2
0 0 N 1 0
i=1
+(4α β K2(3δ2(1+B2)+L2B2)+2α2(3K2B2+3K2δ2+2L2K2B2)+8α β δ2)(T +2)4/3.
0 0 0 0 0
Dividingbothsidesby(T +2)2,wehave
M({θθθi},ΦΦΦ )
M({θθθi },ΦΦΦ )≤ 1 0
t+2 t+1 (T +2)2
36Table2: Parametersetting
Parameter Description
Inputsize 4
Hiddensize 128×128×128
Outputsize 2
Activationfunction ReLu
Numberofepisodes 500
Batchsize 64
Discountfactor 0.98
ϵgreedyparameter 0.01
Targetupdate 30
Buffersize 10000
Minimalsize 500
Learningrate 0.002,decaysevery100episodes
(cid:34) (cid:34) N (cid:35)(cid:35)
1 (cid:88)
+(144τ2K2L2δ2+4L2/N)α β (T +2)−2/3 E[∥ΦΦΦ −ΦΦΦ∗∥2]+ E ∥θθθi −yi(ΦΦΦ )∥2
0 0 0 N 1 0
i=1
+(4α β K2(3δ2(1+B2)+L2B2)+2α2(3K2B2+3K2δ2+2L2K2B2)+8α β δ2)(T +2)−2/3.
0 0 0 0 0
Thiscompletestheproof.
F.2 PROOFOFCOROLLARY4.15
Ifα =β =o(N−1/3K−1/2),wehave
0 0
(cid:18) (cid:19)
1 1 1 1
M({θθθi },ΦΦΦ )≤O + + + ,
t+2 t+1 (T +2)2 N2/3(T +2)2/3 K2N5/3(T +2)2/3 K2N2/3(T +2)2/3
(cid:16) (cid:17)
whichisdominatedbyO 1 ifT2 >N.
N2/3(T+2)2/3
G ADDITIONAL EXPERIMENT DETAILS
Compute resources. The experiments are performed on a computer with Intel 14900k CPU with
48GBofRAM.NoGPUisinvolved.
PFEDDQN-REP inCartPoleEnvironment. Weevaluatetheperformance PFEDDQN-REP ina
modifiedCartPoleenvironment(Brockmanetal.,2016). SimilartoJinetal.(2022),wechangethe
length of pole to create different environments. Specifically, we consider 10 agents with varying
pole length from 0.38 to 0.74 with a step size of 0.04. We compare PFEDDQN-REP with (i) a
conventional DQN that each agent learns its own environment independently; and (ii) a federated
versionDQN(FedDQN)thatallowsallagentstocollaborativelylearnasinglepolicy(withoutper-
sonalization). WerandomlychooseoneagentandpresentitsperformanceinFigure3(top)(a). The
resultsoftheotheragentsarepresentedinFigure8. Again, weobservethatour PFEDDQN-REP
achieves the maximized return much faster than the conventional DQN due to leveraging shared
representationsamongagents;andobtainslargerrewardthanFedDQN,thankstoourpersonalized
policy. We further evaluate the effectiveness of shared representation learned by PFEDDQN-REP
whengeneralizesittoanewagent. AsshowninFigure3(top)(b),ourPFEDDQN-REPgeneralizes
quicklytothenewenvironment. DetailedparametersettingscanbefoundinTable2.
PFEDDQN-REP in Acrobot Environment. We further evaluate FEDDQN-REP in a modified
Acrobot environment (Brockman et al., 2016). The pole length is adjusted with [-0.3, 0.3] with
a step size of 0.06, and the pole mass with be adjusted accordingly (Jin et al., 2022). The same
twobenchmarksarecomparedasinFigure3(top). Theparametersettingremainsthesameexcept
numberofepisodesdecreasesto100. SimilarobservationscanbemadefromFigure3(bottom)and
Figure9asthosefortheCartpoleenviroments.
Hopper Environment. We further consider an environment Hopper from gym, whose state and
actionspacearebothcontinuous. Wevarythelengthoflegstobe0.02+0.001∗i,whereiisthe
37PFedDQN-Rep DQN FedDQN FedQ-K LFRL PerDQNAvg FedAsynQ-ImAvg
102 102 102
2 2 2
n n n
ru
te1
ru
te1
ru
te1
R R R
0 0 0
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode Episode
0.38 0.42 0.46
102 102 102
2 2 2
n n n
ru
te1
ru
te1
ru
te1
R R R
0 0 0
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode Episode
0.5 0.58 0.62
102 102 102
2 2 2
n n n
ru
te1
ru
te1
ru
te1
R R R
0 0 0
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode Episode
0.66 0.7 0.74
Figure8:ComparisonofcontrolbyDQN,FedDQNandPFEDDQN-REPinCartpoleEnvironments.
PFedDQN-Rep DQN FedDQN FedQ-K LFRL PerDQNAvg FedAsynQ-ImAvg
102 102 102
0 0 0
n n n
ru
te -5
ru
te -5
ru
te -5
R R R
-10 -10 -10
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Episode Episode Episode
-0.24 -0.18 -0.12
102 102 102
0 0 0
n n n
ru
te -5
ru
te -5
ru
te -5
R R R
-10 -10 -10
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Episode Episode Episode
-0.06 0 0.06
102 102 102
0 0 0
n n n
ru
te -5
ru
te -5
ru
te -5
R R R
-10 -10 -10
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Episode Episode Episode
0.12 0.18 0.24
Figure9:ComparisonofcontrolbyDQN,FedDQNandPFEDDQN-REPinAcrobotEnvironments.
i-thindexedagent,whilekeepingthesameparameterssuchashealthyreward,forwardrewardand
ctrl cost (l2 cost function to penalize large actions). We increase the number of agents to 20, and
plotthereturnwithrespecttoframes. Wegenerateanewsampledtransitiontovalidatethegener-
alization nature of the algorithms. In order to fit the algorithm to continuous setting, we modified
theproposedalgorithmtoaDDPGbasedalgorithm, similartoanyDQNrelatedbenchmarks. For
FedQ-K,LFRLandFedAsynQ-Imavg,wediscretizethestateandactionspace. SimilartoCartpole
andAcrobotenvironment,ourproposedPFedDDPG-Repachievesthebestrewardandgeneralizeto
newenvironmentsquicklyasinFigure10.
Linear Speedup. We now verify the main theoretical result of the linear speedup. We vary the
number of agents from 2 to 10. We compute the convergence time and compare it to the non-
38PFedDDPG-Rep DDPG FedDDPG FedQ-K
LFRL PerDDPGAvg FedAsynQ-ImAvg
103 103
0.6 0.6
n0.4 n0.4
ru ru
te te
R0.2 R0.2
0 0
0 500 1000 1500 2000 0 500 1000 1500 2000
Frame Frame
(a) Average return for 20 agents (b) Generalization parameter 0.05
Figure10: Hopperenvironment.
15
Cartpole
Acrobot
10
p
u
d
e
e
p
S 5
0
2 4 6 8 10
Number of agents
Figure11: Speedupwithdifferentnumberofagents.
personalization counterpart. From Figure 11, we observe that the speedup (convergence time) is
almostlinearlyincreasing(decreasing)asthenumberofclientsincreases.
Statistical Significance. We report the return average, variance average, return median and total
runningtimefor10environmentsforCartpoleandAcrobotenvironments.BycomparingwithDQN
algorithmwithoutpersonalization,wecanvalidatethelinearspeedupinrunningtime. Amongall
algorithms,ourPFedDQN-Repachievesthebestreturnaverageandmedian,withtopvarianceand
runningtime,assummarizedinTables3and4. Wealsoprovideazoom-inshortenedplotforboth
environmentstoshowthequickadaptationspeedwhensharingrepresentationsasinFigure12.
Personalization. To validate that the personalization is reached, we first compute the cosine sim-
ilarity matrix of transition probabilities in both environments. After the algorithm converges, we
computethecosinesimilaritymatrixofpolicylayer(lastlayer)intheneuronnetworkinFigures13
and14. Wenoticethatwhilethenearbyagentsmightsharesimilarityintheirpolicy, personaliza-
tion is reached corresponds to their transition probabilities. The shared representation layer stay
identical.
Table3: StatisticsforCartpoleenvironment.
Algorithm Returnaverage Varianceaverage Returnmedian Totalrunningtime(s)
PFedDQN-Rep 143 43 154 466
DQN 135 54 127 3840
FedDQN 101 67 88 387
FedQ-K 112 34 107 490
LFRL 117 47 99 434
PerDQNAvg 127 48 131 520
FedAsynQ-ImAvg 119 51 117 501
39Table4: StatisticsforAcrobotenvironment.
Algorithm Returnaverage Varianceaverage Returnmedian Totalrunningtime(s)
PFedDQN-Rep -42 37 -29 618
DQN -63 67 -57 5854
FedDQN -714 162 -625 571
FedQ-K -213 41 -202 621
LFRL -207 58 -194 676
PerDQNAvg -295 64 -277 602
FedAsynQ-ImAvg -191 36 -186 664
PFedDQN-Rep DQN FedDQN FedQ-K PFedDQN-Rep DQN FedDQN FedQ-K
LFRL PerDQNAvg FedAsynQ-ImAvg LFRL PerDQNAvg FedAsynQ-ImAvg
2
102
2
102
0
103
0
103
1.5 1.5
nruteR
1
nruteR
1
nruteR-0.5 nruteR-0.5
0.5 0.5
0 0 -1 -1
0 50 100 150 200 0 50 100 150 200 0 10 20 30 0 10 20 30
Episode Episode Episode Episode
(a) Return pole length 0.54 (b) Generalization pole length 0.82 (a) Return pole length 0.3 (b) Generalization pole length 0.36
(a)Cartpoleenvironment. (b)Acrobotenvironment.
Figure12: Shortenedplotforcartpoleandacrobotenvironment.
1.0 1.0 1.0
0.280.350.31 0.2 0.370.350.360.440.36 1 0.3 0.350.330.210.380.340.360.450.39 1 1 1 1 1 1 1 1 1 1 1
0.390.690.530.340.550.450.660.38 1 0.36 0.43 0.7 0.560.390.56 0.5 0.680.41 1 0.39 1 1 1 1 1 1 1 1 1 1
0.8 0.8 0.8
0.480.540.480.660.730.750.46 1 0.380.44 0.520.560.480.710.750.770.48 1 0.410.45 1 1 1 1 1 1 1 1 1 1
0.460.710.680.390.640.56 1 0.460.660.36 0.5 0.72 0.7 0.440.660.58 1 0.480.680.36 1 1 1 1 1 1 1 1 1 1
0.6 0.6 0.6
0.570.630.650.780.81 1 0.560.750.450.35 0.590.660.650.810.83 1 0.580.77 0.5 0.34 1 1 1 1 1 1 1 1 1 1
0.650.720.760.77 1 0.810.640.730.550.37 0.680.720.760.81 1 0.830.660.750.560.38 1 1 1 1 1 1 1 1 1 1
0.4 0.4 0.4
0.520.530.58 1 0.770.780.390.660.34 0.2 0.560.570.58 1 0.810.810.440.710.390.21 1 1 1 1 1 1 1 1 1 1
0.560.61 1 0.580.760.650.680.480.530.31 0.570.63 1 0.580.760.65 0.7 0.480.560.33 1 1 1 1 1 1 1 1 1 1
0.2 0.2 0.2
0.73 1 0.610.530.720.630.710.540.690.35 0.74 1 0.630.570.720.660.720.56 0.7 0.35 1 1 1 1 1 1 1 1 1 1
1 0.730.560.520.650.570.460.480.390.28 1 0.740.570.560.680.59 0.5 0.520.43 0.3 1 1 1 1 1 1 1 1 1 1
1 2 3 4 5 6 7 8 9 10 0.0 1 2 3 4 5 6 7 8 9 10 0.0 1 2 3 4 5 6 7 8 9 10 0.0
Agent Agent Agent
(a)Transitionprobabilityheatmap (b)Personalizationlayerheatmap. (c)Representationlayerheatmap.
Figure13: HeatmapofCartpoleenvironment.
1.0 1.0 1.0
0.15 0.11 0.120.0540.0670.12-0.098-0.150.032 1 0.0470.0320.061-0.15-0.067-0.140.0073-0.24-0.16 1 1 1 1 1 1 1 1 1 1 1
0.17 0.160.0210.24 0.19 0.310.0160.056 1 0.032 -0.026-0.036-0.0950.015-0.00530.0160.030.014 1 -0.16 1 1 1 1 1 1 1 1 1 1
0.8 0.8 0.8
0.180.0460.0560.15 0.1 0.25-0.055 1 0.056-0.15 0.081-0.0570.0270.01-0.0680.0990.037 1 0.014-0.24 1 1 1 1 1 1 1 1 1 1
-0.074-0.11-0.14-0.170.061-0.18 1 -0.0550.016-0.098 -0.023-0.049-0.069-0.230.0084-0.28 1 0.0370.030.0073 1 1 1 1 1 1 1 1 1 1
0.6 0.6 0.6
0.38 0.23 0.18 0.52 0.44 1 -0.18 0.25 0.31 0.12 0.1 -0.0140.00440.34 0.26 1 -0.280.0990.016-0.14 1 1 1 1 1 1 1 1 1 1
0.27 0.24 0.1 0.26 1 0.440.061 0.1 0.190.067 0.0870.11 -0.05-0.0063 1 0.260.0084-0.068-0.0053-0.067 1 1 1 1 1 1 1 1 1 1
0.4 0.4 0.4
0.33 0.28 0.18 1 0.26 0.52 -0.17 0.15 0.240.054 0.0990.0620.052 1 -0.00630.34 -0.23 0.010.015-0.15 1 1 1 1 1 1 1 1 1 1
0.0015-0.0061 1 0.18 0.1 0.18 -0.140.0560.0210.12 -0.18-0.098 1 0.052-0.050.0044-0.0690.027-0.0950.061 1 1 1 1 1 1 1 1 1 1
0.2 0.2 0.2
0.2 1 -0.00610.28 0.24 0.23 -0.110.0460.16 0.11 0.0016 1 -0.0980.0620.11-0.014-0.049-0.057-0.0360.032 1 1 1 1 1 1 1 1 1 1
1 0.20.00150.33 0.27 0.38-0.0740.18 0.17 0.15 1 0.0016-0.180.0990.087 0.1 -0.0230.081-0.0260.047 1 1 1 1 1 1 1 1 1 1
1 2 3 4 5 6 7 8 9 10 0.0 1 2 3 4 5 6 7 8 9 10 0.0 1 2 3 4 5 6 7 8 9 10 0.0
Agent Agent Agent
(a)Transitionprobabilityheatmap (b)Personalizationlayerheatmap. (c)Representationlayerheatmap.
Figure14: HeatmapofAcrobotenvironment.
40
tnegA
tnegA
01
9
8
7
6
5
4
3
2
1
01
9
8
7
6
5
4
3
2
1
tnegA
tnegA
01
9
8
7
6
5
4
3
2
1
01
9
8
7
6
5
4
3
2
1
tnegA
tnegA
01
9
8
7
6
5
4
3
2
1
01
9
8
7
6
5
4
3
2
1