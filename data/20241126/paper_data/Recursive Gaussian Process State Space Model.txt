1
Recursive Gaussian Process State Space Model
Tengjie Zheng, Lin Cheng, Shengping Gong, Xu Huang
Abstract—Learning dynamical models from data is not only Among these, most studies focus solely on offline scenarios.
fundamental but also holds great promise for advancing prin- However, in engineering practice, many systems experience
ciple discovery, time-series prediction, and controller design.
non-stationary changes (e.g., drone propeller performance
Among various approaches, Gaussian Process State-Space Mod-
varies with battery levels), and there is the possibility of
els (GPSSMs) have recently gained significant attention due to
theircombinationofflexibilityandinterpretability.However,for operating in out-of-distribution settings. This underscores the
online learning, the field lacks an efficient method suitable for urgentneedforanonlinelearningmethodforGPSSMs,which
scenarios where prior information regarding data distribution is the focus of this paper.
and model function is limited. To address this issue, this paper FacilitatingonlinelearningforGPSSMspresentssignificant
proposes a recursive GPSSM method with adaptive capabilities
challenges. Beyond the usual constraints associated with on-
for both operating domains and Gaussian process (GP) hyper-
line learning—such as minimizing memory usage, reducing
parameters. Specifically, we first utilize first-order linearization
to derive a Bayesian update equation for the joint distribution computational complexity, and preventing catastrophic for-
betweenthesystemstateandtheGPmodel,enablingclosed-form getting—GPSSMs pose additional, unique difficulties. These
and domain-independent learning. Second, an online selection challenges are detailed as follows. (1) Nonlinearity compli-
algorithm for inducing points is developed based on informative
cates inference. In GPSSMs, the system state is included in
criteria to achieve lightweight learning. Third, to support online
the input to the GP model, introducing inherent nonlinearity.
hyperparameteroptimization,werecoverhistoricalmeasurement
information from the current filtering distribution. Compre- Thisnonlinearityisintractable,whichpreventsexactinference
hensive evaluations on both synthetic and real-world datasets through analytic methods. (2) Nonparametric nature of GP.
demonstratethesuperioraccuracy,computationalefficiency,and For standard Gaussian process regression (GPR), the training
adaptability of our method compared to state-of-the-art online
complexity scales cubically with the number of samples [19].
GPSSM techniques.
In an online context, as new data arrives continuously, the
Index Terms—Gaussian process, state-space model, recursive
computational cost of GPs increases rapidly, rendering infer-
algorithm, online learning, system identification.
enceintractable.(3) Coupling between the system state and
the system model. Since the state is implicitly measured,
I. INTRODUCTION learning the system model requires inferring the system state.
STATE-space models (SSMs), also known as hidden However, state inference depends on the system model itself.
Markov models (HMMs), describe the evolution and ob- Asaresult,thereisacouplingbetweenthestateandthemodel,
servation processes of latent states through a state transition which increases the complexity of inference. Theoretically,
model and a measurement model. By summarizing historical since both are uncertain, they should be treated as latent
information into the latent state, SSMs provide a concise variables and inferred simultaneously. (4) Hyperparameter
representation of system dynamics and are widely applied in online optimization. GPSSMs involve several hyperparame-
fieldssuchasfinance[1],neuroscience[2],targettracking[3], ters, such as GP kernel parameters, which impact the learn-
and control systems [4]. Nevertheless, due to the complexity ing accuracy. In online settings, hyperparameter optimization
of real-world systems, establishing precise SSMs based on presents two main challenges. First, due to memory efficiency
first principles is challenging. To achieve sufficiently accurate considerations, historical measurements are not retained in
system models, extensive research has focused on data-driven online scenarios, depriving hyperparameter optimization of
SSM modeling [5]–[7]. explicit information sources. Secondly, since the GP prior
In the past decade, a novel SSM learning method based depends on kernel hyperparameters, tuning these parameters
on the Gaussian process (GP), a nonparametric modeling requires corresponding adjustments to the posterior distribu-
technique, has been explored [7], [8]. This approach models tion. Hence, the online GPSSM problem is quite challenging,
the transition and measurement functions of SSMs as GPs, and as a result, relevant research remains limited.
and is therefore referred to as Gaussian process state-space Theoretically, implementing online GPSSMs involves
models (GPSSMs) [7]. By employing GPs, GPSSMs offer Bayesianinference.Therefore,existingmethodsprimarilyrely
flexible learning for the transition and measurement functions on Bayesian filtering or online variational inference tech-
andprovidepredictionuncertainty,whichiscrucialforsafety- niques, which can be classified into three categories:
critical applications [9]. To date, various GPSSM methods 1) AugmentedKalmanFilter(AKF)-basedmethod[3],[20],
have been developed, including sample-based methods [10]– [21].Thisapproachcanjointlyinferthesystemstateand
[12] and variational inference (VI)-based methods [13]–[18]. theGPmodelonline,anditsimplementationinvolvestwo
steps: first, parameterizing the GP with a limited number
Tengjie Zheng, Lin Cheng, Shengping Gong, and Xu Huang are of parameters using inducing-point approximation [3],
with the School of Astronautics, Beihang University, Beijing 102206,
[20], or spectral approximation [21]; and second, aug-
China. Email: ZhengTengjie@buaa.edu.cn; chenglin5580@buaa.edu.cn;
gongsp@buaa.edu.cn;xunudt@126.com.(Correspondingauthor:LinCheng.) mentingtheGPmodelparametersintothestatespaceand
4202
voN
22
]GL.sc[
1v97641.1142:viXra2
inferringtheaugmentedstate usingtheextendedKalman anapproximateobservationlikelihoodmodelisextractedfrom
filter (EKF). Through the AKF-based framework, this the obtained posterior distribution. Utilizing this likelihood
methodachievesrecursivelearningandoffersadvantages model as the information source, the GP hyperparameters are
in learning speed and stability. However, there is a optimized,andtheposteriordistributioncanbeadjustedasthe
key limitation: the parameterization of the GP requires hyperparameterschange.Finally,wevalidatetheeffectiveness
priorknowledgeoftheoperatingdomain,specificallythe of the proposed method through two synthetic simulations
locationofinducingpointsortherangeofbasisfunctions andareal-worlddataset.Theexperimentalresultsdemonstrate
in spectral approximation [21]. This drawback limits the the superior performance of the proposed method in terms
approach’s ability to adapt to the flexible distribution of of learning accuracy, computational efficiency, and adaptive
online data. In addition, there is no feasible method for capacity for operating domains and hyperparameters.
hyperparameter optimization for this approach.
2) Particle Filter (PF)-based method [22]–[24]. This ap-
II. PROBLEMFORMULATION
proach is similar to the first, with the key difference
being the use of the particle filter (PF) for the infer- In this section, we first briefly provide the background of
ence process. Since PF is inefficient at handling high- Gaussian processes (GPs) and then give a definition of Gaus-
dimensionalstates,GPparametersarenotaugmentedinto sian process state-space models (GPSSMs) and the associated
the state space. Instead, each particle is assigned a GP, online learning problem.
which is recursively learned using the particle state. This
method can theoretically achieve precise inference for
A. Gaussian Processes
arbitraryobservationaldistributions,providedthenumber
Mathematically, GP is a probability distribution over func-
of particles is sufficient. However, despite the inherent
tion space, which can be employed for regression task. For
issues with PF, such as high computational cost and
learning a single-input function f(·), one can first place a GP
particledegeneracy[25],italsorequirespriorknowledge
prior on it:
of the operating domain.
3) Stochastic Variational Inference (SVI)-based method
[11], [18], [26]. This approach extends the offline varia- f(x)∼GP(m 0(x),K 0(x,x′;θ)) (1)
tionalinferencemethodofGPSSMsbyutilizingstochas-
where m (·) denotes the prior mean function and K (·,·)
tic optimization techniques. It aims to retain many of the 0 0
representsthepriorcovariancefunction(alsoknownaskennel
benefits of offline GPSSMs, such as a flexible operat-
function), and θ denotes the kernel hyperparameters. As a
ingdomainandhyperparameteroptimizationcapabilities.
common setting, we assume zero-mean GP prior, namely,
However, stochastic optimization assumes that the data
m (·)≡0. Then, given function values f at input locations
subsampling process is uniformly random, which is not 0 X
X,theposteriordistributionfortestfunctionvaluesf is[19]:
thecaseinmostonlinesituations.Asaresult,thelearning ∗
process is driven by instantaneous data, leading to slow
learning and catastrophic forgetting. p(f |f )=N(K K−1f ,K −K K−1K ) (2)
∗ X ∗X XX X ∗∗ ∗X XX X∗
In summary, the existing methods either lack the ability to Here, we use shorthand notation for the kernel matrix, e.g.,
tune the operating domains and hyperparameters, or they face K denotes the auto-covariance of f , and K the cross-
∗∗ ∗ ∗X
issues with learning speed and convergence. covariancebetweenf andf .Thisnotationisusedthrough-
∗ X
To overcome the aforementioned shortcomings, a recursive out.Indeed,GPscanbeutilizedtolearnmulti-outputfunctions
learning algorithm for GPSSMs is proposed in this paper, by employing an associated kernel. However, for simplicity,
capable of online adaptation to the operating domain and GP the elaboration of the proposed method is based on the
hyperparameters. The main contributions can be summarized single-output GP, and its extension to the multi-output case is
as follows: Firstly, to implement a domain-independent recur- straightforward and will be demonstrated in the experiments.
sivelearning algorithmfor GPSSMs,anovel Bayesianupdate
equation is derived without pre-parameterizing the GP. In the
B. Online Gaussain Processs State Space Models
derivation process, only mild linearization approximations are
appliedtoachieveaclosed-formsolution,offeringadvantages In this paper, we consider the following discrete-time state-
in learning speed and stability. Through continuously expand- space model (SSM):
ingtheinducing-pointset,thismethodcanaccommodatereal-
timedatadistributions.Secondly,toaddressthecomputational x =F(x ,f(x ))+ω , ω ∼N(0,Σ )
t+1 t t f f f
costissuecausedbytheincreasingnumberofinducingpoints, (3)
y =g(x )+ω , ω ∼N(0,Σ )
an selection algorithm is developed with both addition and t t g g g
removal operations. To minimize accuracy loss, an optimal where the vector x ∈ Rnx and y ∈ Rny respectively denote
discarding equation and selection metric for inducing points the system state and measurement. The vectors ω and ω
f g
are derived based on informative criteria. Thirdly, to alleviate represent the process and measurement noise, with covari-
theburdenofofflinehyperparametertuning,anonlineGPhy- ances Σ and Σ , respectively. Additionally, the function
f g
perparameteroptimizationmethodisproposed.Inthismethod, F : Rnx → Rnx denotes a known function structure for the3
transition model1 , which contains an unknown component, the EKF propagates the state distribution and then corrects
represented as the function f : Rnx → R. To ensure it with the current measurement, which are known as the
identifiability, the measurement model g : Rnx → Rny is prediction (time update) and correction (measurement update)
assumed to be known2. steps,respectively.InthecontextofGPSSMs,consideringthe
To learn the unknown SSMs, GPSSMs place GP priors on couplingbetweenthesystemstateandtheGP,thedistribution
the unknown function f(·), resulting in the following model tobepropagatedandcorrectedisthejointdistributionofboth.
[7]: These two operations can be expressed by:
(cid:90)
x 0 ∼p(x 0) (4a) p(x t+1,f|y 1:t)= p(x t+1|x t,f)p(x t,f|y 1:t)dx t (5)
f(·)∼GP(m (·),K (·,·);θ) (4b)
0 0 p(y |x )p(x ,f|y )
x t+1|x t,f ∼N (cid:0) x t(cid:12) (cid:12)F(x t,f(x t)), Σ f(cid:1) (4c) p(x t+1,f|y 1:t+1)= t+1 pt (+ y1 |yt+1
)
1:t (6)
(cid:0) (cid:12) (cid:1) t+1 1:t
y t|x
t
∼N y t(cid:12)g(x t), Σ
g
(4d)
where p(x |x ,f) and p(y |x ) are the transition
t+1 t t+1 t+1
where p(x 0) denotes a prior Gaussian distribution for the and measurement densities as defined in (4), p(x t,f|y 1:t)
initial state x 0, and note that we use the notation f in (4c) denotes the joint posterior at time t, and p(x t+1,f|y 1:t)
to denote any collection of function values of interest, which is the joint prior or predicted distribution at time t + 1.
includes f(x t). For conciseness, we omit the system input To evaluate the distribution in equations (5) and (6), the
from the transition and measurement models, which can be key challenges are the nonparametric nature of the GP and
easily incorporated by augmenting it into the input of these the nonlinearity in the transition and measurement models.
models and the GP. Existing methods [3], [20]–[23] typically first address the
In the online setting, measurements y arrive sequentially, former by pre-parameterizing the GP into a fixed structure
and we assume that historical measurements are not retained and then applying nonlinear filtering to handle the latter.
for computational and storage reasons. Given this streaming However, pre-parameterization limits the operating domain,
data setting, the inference task of online GPSSMs is to se- thus reducing the algorithm’s flexibility. To overcome this,
quentially approximate the filtering distribution p(x t,f|y 1:t), we reverse the process: first handling the nonlinearity without
using the previous result p(x t−1,f|y 1:t−1) and the current any GP approximation, then addressing the nonparametric
measurement y . As illustrated in the introduction, several nature. We achieve the first procedure in the next subsection,
t
challenges exist in this task, including the nonlinearity and resulting in an update equation with a factorized approximate
coupling involved, as well as the nonparametric nature of distribution.
GPsandthehyperparameteroptimizationproblem.Toaddress
these difficulties, a novel online GPSSM approach will be
B. Factorized Approximate Distribution
developed in the next two sections, with the ability to adapt
To address the nonlinearity within the transition and mea-
to both operating domains and hyperparameters.
surement model, first-order linearization is applied in this
subsection, whose result is the following theorem.
III. BAYESIANUPDATEEQUATIONFORONLINEGPSSMS
Theorem 1: By approximating the transition and measure-
This section derives a Bayesian update equation for online
mentmodelswithfirst-orderlinearization,thejointdistribution
GPSSMs with minimal approximations to achieve recursive
between the state and GP obtained in each prediction and
learning in an arbitrarily operating domain.
correction step follows the factorized form:
A. Two-Step Inference q(x t,f)=p(f|u)q(x t,u) (7)
The online inference of GPSSMs is essentially a filtering where u ≜ {u }nu is a collection of a finite number of in-
i i=1
problem; therefore, most methods are based on filtering tech- ducing points, p(f|u) is the exact GP conditional distribution
niques, such as the extended Kalman filter (EKF) and particle similar to (2), and q(x ,u) is a Gaussian distribution.
t
filter (PF). Considering that the PF will exacerbate the com- GivenTheorem1,theapproximatejointdistributionremains
putational cost issues due to GP and its particle degradation Gaussian, which can be evaluated using closed-form update
problem,wechoosetheEKFinthispaper.Thewidelyadopted equations. In the following, we will prove Theorem 1 and
EKF is a two-step version, which is more numerically stable then derive these equations.
[27] and can handle irregular measurements. In each update, To facilitate the proof, we first introduce some notation
regardingtheapproximatedistributionsq(x ,u)andq(x ,f).
1This structure of F(·) can represent various models of interest, such t t
Specifically, these two distributions are expressed as:
as when only some dimensions of the transition model are unknown, or a
discretizedcontinuous-timetransitionmodelF(xt,f(xt))=xt+f(xt)∆t,
w mh oe dr ee l∆ x˙t =is fth (e xt )i .m Te hs ot uep ghan ud sit nh ge aun vk an ro yw inn gc to imm epo sn tee pnt ∆is t,th te hetim lae t- ted rer civ aa nti bv ee
q(x ,u)=N
(cid:32)(cid:20) x t(cid:21)(cid:12) (cid:12) (cid:12)(cid:20) µ t(cid:21) ,(cid:20) P
t
V xu(cid:21)(cid:33)
usedforirregularmeasurementscenarioscommonlyoccurringinbiochemical t u (cid:12) m VT S
sciences. (cid:12) u xu uu
2Evenifthemeasurementmodelisunknowninpractice,wecanaugment (cid:32)(cid:20)
x
(cid:21)(cid:12) (cid:12)(cid:20)
µ
(cid:21) (cid:20)
P V
(cid:21)(cid:33) (8)
the measurement y into the state space and transform the problem into one q(x ,f)=N t (cid:12) t , t xf
withaknownmeasurementmodel[7]. t f (cid:12) (cid:12) m f VT xf S ff4
In this expression, the moments of the joint distribution are It can be observed that the factorized form is preserved at
distinguished based on the state x and inducing points u the cost of augmenting the inducing-point set, an inevitable
t
or function values f. For q (x ,u), the vectors µ and m consequence of the non-parametric nature of the GP. This
t t t u
represent the means of the state x and the inducing points increase in the number of inducing points leads to a rapid
t
u, respectively. P denotes the autocovariance of x , while rise in computational burden, rendering inference intractable,
t t
V represents the cross-covariance between x and u, and which will be addressed in Section IV-A.
xu t
S signifies the autocovariance of u. For the moments of For the correction step, the derivation is similar. To handle
uu
q (x ,f), its notation follow the same logic, and the values thenonlinearityinthemeasurementmodel(4d),thefirst-order
t t
are fully specified by the moments of q(x ,u), specifically: linearization is used again:
t
m =k m
f fu u p(y |x )
V S fx ff == KV fx fuk +T fu k fu(S uu−K uu)kT
fu
(9) ≈Nt+1 (cid:18) yt t+ +1 1(cid:12) (cid:12) (cid:12) (cid:12)g(µ− t+1)+C(x t+1−µ− t+1),Σ g(cid:19) (13)
where k
fu
=K fuK− uu1. This equation can be easily derived =pˆ(y t+1|x t+1)
by using (7) and the conditional distribution p(f|u). where µ− denotes the predicted mean of state x and the
Bymeansoftheabovenotation,wewillillustratetheproof t+1 (cid:12) t+1
measurement Jacobian is C = ∂g(x)/∂x(cid:12) . By incorpo-
ofTheorem1,whichisbasedonmathematicalinduction.First, µ−
t+1
rating this linearized model into the correction step (6), the
it can be demonstrated that at time 0, the joint distribution
factorized form can be retained as follows:
naturally satisfies the factorized form (7):
p(x 0,f)=p(f)p(x 0) q(x t+1,f)∝q−(x t+1,f)pˆ(y t+1|x t+1)
=p(f|u)p(u)p(x 0) (10) =p(f|u¯)q−(x t+1,u¯)pˆ(y t+1|x t+1) (14)
=p(f|u)q(x 0,u) ∝p(f|u¯)q(x t+1,u¯)
where p(f) denotes the GP prior over function values f where q(x ,u¯)∝q−(x ,u¯)pˆ(y |x ).
t+1 t+1 t+1 t+1
and u can be any collection of function values or an empty Thusfar,wehaveprovenTheorem1,whereonlyfirst-order
set. Therefore, the next step is to prove that the approximate linearization is used, and the inducing-point set is continually
updatesofthepredictionandcorrectionstepscanpreservethe expanded to match the data distribution. For the linearization
factorized form. treatment involved, it provides a high-accuracy approximation
For the prediction step, to address the nonlinearity in the when the nonlinearity is weak and the variance of the joint
transition model (4c), we employ first-order linearization as distribution is small. In the derivation, we also obtain the
in the EKF, specifically linearizing the transition model as approximate prediction equation (12) and correction equation
follows: (14) for the joint distribution q(x ,f), which can be used to
t
derive practical update equations for it. It is evident that the
p(x t+1|x t,f) jointdistributionq(x t,u)containstheessentialinformationof
≈N(cid:16)
x
(cid:12)
(cid:12)F +A (x −µ )+A (f −m ),Σ
(cid:17) q(x t,f)andthusistheonlycomponentthatrequiresupdating.
t+1(cid:12) t x t t f t ft f The moment update equations for it are derived in the next
=pˆ(x t+1|x t,f t). subsection.
(11)
where we use the notation for moments in (8) and the short-
C. Moment Update Equation
hand notation f = f(µ ), F = F(µ ,m ). Additionally,
t t t t ft
Forsimplicityinthemomentupdateequation,wedefinethe
the two Jacobian matrices are A = ∂F(x,m )/∂x|
x f(x) µ
t unionofthesystemstateandinducingpointsasanaugmented
and A = ∂F(µ ,f)/∂f| . Note that, we use the hat
f t mft state, i.e., X = [xT,uT]T. Consequently, the approximate
notationpˆ(x |x ,f )in(11)todenotethelinearizedmodel. t t
t+1 t t
distributionq(x ,u)canbeexpressedasq(X ),withitsmean
As shown in (11), this linearized transition model depends t t
and covariance denoted as ξ and Σ , respectively. Based
solelyonthefunctionvaluef .Therefore,ifweincorporatef t t
t t
intotheinducing-pointset,i.e.,u¯ =[uT,fT]T,theprediction on this definition, the moment update equation becomes a
t
recursive equation for ξ and Σ .
result in (5) by using the linearized model (11) retains the t t
For the prediction step, given the update equation (12),
factorized form (7):
we should first add the point f into the inducing point
t
(cid:90) set u, thus obtaining the new set u¯. Therefore, we define
q−(x t+1,f)= q(x t,f)pˆ(x t+1|x t,f t)dx t the new augmented state X¯ t = [xT t ,u¯T]T, similar to X t.
(cid:90) Correspondingly, we have the approximate distribution q(X¯ t)
=p(f|u¯) q(x ,u¯)pˆ(x |x ,f )dx (12) withmeanξ¯ andcovarianceΣ¯ ,whichcanbeevaluatedusing
t t+1 t t t t t
(8) and (9) by letting f = u¯. Then, by applying the update
=p(f|u¯)q−(x ,u¯)
t+1 equation (12), the moment update equation for the prediction
where we use the superscript ”-” to denote predicted dis- step can be easily derived, with a result similar to that of the
tributions, and q−(x ,u¯) = (cid:82) q(x ,u¯)pˆ(x |x ,f )dx . EKF.
t+1 t t+1 t t t5
points. This challenge is also encountered in the development
ξ¯−
t+1
=[FT
t
,mT u¯]T
of kernel-based online regression algorithms, such as Kernel
Σ¯− t+1 =ΦΣ¯ tΦT +Σ f,X¯ Recursive Least Squares (KRLS) [28] and Sparse Online
  GaussianProcesses(SOGP)[29],[30].Thesetwomethodsare,
A 0 A
Φ= 0x I nu 0f  (15) t lo ims io tim nge te hx ete sn izt, ee oq fui tv ha ele inn dt ua cn id ngco pn ot ir no tl sc eo tm wp itu ht ia ntio an pa rl ec do es fit nb ey d
0 0 1
budget M. Specifically, this strategy is achieved through two
(cid:20) (cid:21)
Σ 0
Σ f,X¯ = 0f 0 operations: discarding the least important point when the set
exceeds the budget M, and adding only sufficiently novel
whereξ¯− andΣ¯−
representthepredictedmeanandcovari-
points. To implement the discarding and adding operations,
ance fort+ th1 e augmt e+ n1 ted state X¯ , Φ denotes its transition the following two key problems must be addressed: (1) how
t+1
to remove a point with minimal accuracy loss, and (2) how to
Jacobian matrix, and Σ f,X¯ is its process noise covariance.
evaluatetheimportanceornoveltyofapoint.Inthefollowing,
For the correction step, the moment update equation can be
wewilladdressthesetwoproblemsinthecontextofGPSSMs
easily obtained using the approximate correction equation in
and develop an adjustment method for the inducing point
(14), with the result as follows:
set, thereby overcoming the computational burden issue. For
H =(cid:2) C 0(cid:3) convenience, let u d denote the inducing point to be discarded,
G=Σ¯− t+1HT (cid:16) CP− t+1CT +Σ g(cid:17)−1 w rei mth ainin ind gex indi d uci in ngt ph oe inp tso .int set, and let u l represent the
(16)
ξ¯ =ξ¯− +G(cid:2) y −g(µ− )(cid:3) For the first problem, we define that the new joint distri-
t+1 t+1 t+1 t+1 bution, after discarding point u , still retains the factorized
Σ¯
t+1
=Σ¯− t+1−GHΣ¯−
t+1 form (7), namely, qˆ(x,f) =
p(fd
|u)qˆ(x,u l), where qˆ(x,u l)
whereP− denotesthepredictedvarianceofthesystemstate is Gaussian. Then, to ensure minimal accuracy loss, we can
x ,ξ¯ t+1 andΣ¯ representtheposteriormeanandcovari- seek the optimal solution within the distribution family of
ant c+ e1 fot r+ t1 he augmt+ e1 nted state X¯ , H is its measurement qˆ(x,f), such that the distance between it and the original
t+1
joint distribution q(x,f) = p(f|u)q(x,u) is minimized.
Jacobian, and G corresponds to the Kalman gain. Therefore,
Since qˆ(x,u ) controls the approximate distribution qˆ(x,f),
through the moment update equations (15) and (16), closed- l
it is sufficient to find its optimal distribution. Here, we use
form recursive inference for online GPSSMs can be achieved.
the inclusive KL divergence as a distance metric, namely
In summary, this section derives a two-step form of the
(cid:82)
KL[q∥qˆ] = qlog(q/qˆ), which results in a simple optimal
Bayesian update equation for online GPSSMs, where first-
distribution:
order linearization approximations are used to handle the
nonlinearity of the transition and measurement models. The
qˆ∗(x,u )=arg min KL[q(x,f)∥qˆ(x,f)]
derivation shows that, by applying linearization and augment- l
qˆ(x,ul)
ing the inducing-point set, the joint distribution retains a
=arg min KL[p(f|u)q(x,u)∥p(f|u )qˆ(x,u )]
l l
factorized structure, leading to the moment update equations qˆ(x,ul)
(15) and (16). These equations enable recursive learning for =arg min KL[q(x,u)∥p(u |u )qˆ(x,u )]
d l l
GPSSMswithouttheconstraintofanoperatingdomain.How- qˆ(x,ul)
(cid:90)
ever, as discussed in Section III-B, the increasing number of
= q(x,u)du
d
inducing points significantly raises the computational burden
(17)
for moment evaluation, making inference intractable. In the
Itcanbeobservedthattheoptimaldistributionqˆ∗(x,u )only
next section, an approximation method will be introduced to l
involves marginalizing the discarded inducing point u from
address this issue. d
the original distribution. Through (17), optimal deletion for a
given inducing point can be achieved.
IV. ADAPTATIONOFINDUCINGPOINTSAND
Furthermore, for the second problem, the importance or
HYPERPARAMETERS
novelty of a point can be assessed by the accuracy loss
While the recursive learning algorithm for GPSSMs has
incurred from optimally discarding it. Specifically, this can
been developed, two issues remain: the computational burden
be done by calculating the KL divergence in (17) with
problem due to the non-parameteric nature of GP, and the
qˆ(x,u )=qˆ∗(x,u ):
l l
hyperparameter online adaptation problem. In Section IV-A,
a pseudo-point set adjustment algorithm is introduced to D∗ =KL[q(x,u)∥p(u |u )qˆ∗(x,u )]
d l l
maintain limited computational complexity, and in Section
=D −D −D
1 2 3
IV-B, an online hyperparameter adaptation method is derived (cid:90)
to improve learning accuracy. D = q(x,u)logq(x,u)dxdu
1
(18)
(cid:90)
A. Dynamic Adjusting the Inducing Points Set D = q(u)logp(u |u )du
2 d l
AsdiscussedinSectionIII,thecomputationalcomplexityof (cid:90)
the learning algorithm increases with the addition of inducing D 3 = qˆ∗(x,u l)logqˆ∗(x,u l)du l6
Through the KL divergence D∗, the accuracy loss from dis- equation (15) will be modificated. We derive this equation in
cardingtheinducingpointu canbeevaluated.Acomputable Appendex B, whose result is;
d
score s quantifying the accuracy loss can be obtained by
d
dropping the u d-irrelevant terms in (18) (the derivation is
ξ−
t+1
=[FT
t
,mT u]T
deferred to Appendix A), namely: Σ− =ΦΣ ΦT +Σ
t+1 t f,X
(cid:20) (cid:21)
A A k
s =∆ +∆ +∆ Φ= x f tu (21)
d 1 2 3 kT AT I
∆ 1 =mT uQT duQ− dd1Q dum u tu (cid:20) Af γAT n +u Σ 0(cid:21)
(cid:16) (cid:17) (19) Σ = f f f
∆ =tr Q S QT Q−1 f,X 0 0
2 du uu du dd
∆ =log|Ω |−log|Q | Thisequationissimilartotheoriginalequation(15),differing
3 dd dd
only in the expression for the transition Jacobian Φ and the
where we denote the inverse of the joint covariance as Ω =
process noise covariance Σ .
Σ−1, and Ω is the element of the matrix Ω corresponding f,X
t dd In summary, this subsection derives the optimal deletion
to the discarded point u , namely, its (n +i )-th diagonal
d x d method for inducing points and presents a metric for quanti-
element. In addition, Q and Q are the i -th diagonal ele-
dd du d fying the importance or novelty of points. Based on these,
mentandthei -throwoftheinversekernelmatrixQ=K−1.
d uu a dynamic adjustment algorithm, including discarding and
Among the terms in (19), ∆ represents the accuracy loss in
1 adding operations for the inducing point set, is developed to
the mean of joint distribution q(x ,f), which matches the
t ensure limited computational cost and algorithm stability. In
result in KRLS [28], while ∆ and ∆ correspond to the
1 2 thenextsubsection,wewilladdresstheonlinehyperparameter
loss in the covariance. Overall, the score s quantifies the
d optimization problem to enhance learning accuracy.
relativeaccuracylossfromdiscardingu ,withasmallerscore
d
indicating a smaller accuracy loss.
B. Online Optimization of GP Hyperparameters
Through (17) and (19), we have addressed the two key
challenges. Based on these, we can provide the adjustment In GP learning, if the hyperparameters θ significantly mis-
rulefortheinducingpointset,whichisdividedintodiscarding match the function to be learned, it will result in substantial
and adding operations. For the discarding operation, the rule learning errors. In offline learning, the hyperparameters θ
is as follows: if the size of the inducing point set exceeds the can be optimized using data by maximizing the log marginal
budget M, evaluate the score of each point using (19) and likelihood [19]. However, as illustrated in the Introduction,
remove the one with the lowest score. According to (17), the the online setting presents two challenges for hyperparameter
moments of the new joint distribution qˆ∗(x,u ) can be easily optimization: 1) the information source problem due to the
l
evaluated by deleting the corresponding elements associated lack of data retention, and 2) the coupling between GP
with u from the original moments. hyperparameters and the posterior distribution. In this sub-
d
For the adding operation, as in the KRLS [28], we will section, we will address these two challenges and propose an
use a more intuitive metric for selecting points, which is online GP hyperparameter optimization method for GPSSMs.
derived from the score in (19). Specifically, by observing For clarity, we denote the quantities corresponding to before
the score s in (19), it can be seen that if Q −1 → 0, and after hyperparameter optimization with the superscripts
d dd
the score will approach its minimum value, i.e., s → −∞, or subscripts ”old” and ”new”, respectively. Additionally, to
d
indicating that the accuracy loss from deleting u approaches highlight the impact of the GP hyperparameters, probability
d
0. According to the matrix inversion formula (see Appendix distributions depending on θ will be rewritten as p(·;θ).
A of [30]), Q −1 =K −K K−1K , which is the GP Firstly, to find the information source for hyperparameters,
dd dd dl ll ld
prior conditional variance of u given u . Therefore, we can we can first observe the excat joint posterior distribution,
d l
use the GP prior conditional variance of f given the current which can be expressed as follows for time t:
t
inducing points u as the metric for novelty, namely:
p(f;θ)p(x ,y |f)
p(x ,f|y ;θ)= t 1:t (22)
t 1:t p(y ;θ)
γ =K −K K−1K (20) 1:t
tt tu uu ut
where p(f;θ) denotes the GP prior on function values f, and
Then, if γ is less than a certain threshold, denoted as ε tol, the likelihood model p(x t,y 1:t|f) is given by:
we will not add the new point to the inducing points set.
(cid:90) t
This filtering process helps to slow down the increase in the (cid:89)
p(x ,y |f)= p(y ,x |x ,f)p(x )dx
size of the inducing points set. Besides, more importantly, it t 1:t i i i−1 0 0:t−1
i=1
plays a crucial role in ensuring the numerical stability of the (23)
learningalgorithm.Specifically,whenγ issmall,addinganew Usingthislikelihoodmodel,wecanevaluatethemarginallike-
(cid:82)
point can cause the updated kernel matrix K to approach lihood p(y ;θ) = p(f;θ)p(x ,y |f)dx df, and then
u¯u¯ 1:t t 1:t t
singularity, which would negatively affect the stability of the optimize the GP hyperparameters by maximizing it. However,
algorithm.Therefore,thevalueofthethresholdε foradding for online hyperparameter optimization, an explicit likelihood
tol
a point can be determined based on the machine accuracy. model is unavailable because the past measurements y
1:t−1
Accordingly, when not adding points, the moment updating are not stored. Instead, we only have an approximation of the7
left-hand side of (22), namely the approximate distribution First, by eliminating some θ -irrelevant terms in (27)
new
q (x ,f) ≈ p(x ,f|y ;θ ). Fortunately, by observing (see Appendix C), a practical optimization objective for GP
old t t 1:t old
(22),itcanbefoundthatthelikelihoodmodelcanbeimplicitly hyperparameters is to minimize:
obtained by:
L=L +L
1 2
p(x ,f|y ;θ )p(y ;θ )
p(x t,y 1:t|f)= t 1 p:t (f;o θld
)
1:t old L 1 =(mo uld)T(So ul ud +∆K)−1mo uld
(28)
old (24) L =log|Knew+[I −Knew(Kold)−1]Sold|
q (x ,f)p(y ;θ ) 2 uu nu uu uu uu
≈ old t 1:t old ∆K =[(Knew)−1−(Kold)−1]−1
p(f;θ old) uu uu
In other words, the measurement information is distilled into whereKnew andKold denotethekernelmatrixofuevaluated
uu uu
the approximate distribution q (x ,f) and the latter can re- with new and old GP hyperparameters. Therefore, the GP
old t
coverthelikelihoodmodel,therebyaddressingtheinformation hyperparameters can be optimized online by implementing
source problem. gradient descent on the loss function (28) in each algorithm
Secondly, utilizing the information source in (24), we iteration. Since the approximate posterior q(x,f) retains the
can find the optimization objective for GP hyperparameters measurement information, it is not necessary to optimize to
and evaluate the posterior distribution after hyperparameter convergence in a single algorithm iteration.
optimization. The key idea in achieving this is to evaluate Second, the moments of the new posterior q (x ,u) after
new t
the joint probability model p(f;θ)p(x ,y |f), which is the hyperparameter update can be evaluated using a Kalman
t 1:t
proportionaltotheposteriordistribution,anditsintegralisthe filter-like equation (derivation in Appendix C):
marginal likelihood that can serve as the objective function.
For convenience, we actually derive the posterior distribution H˜ =(cid:2) 0 I nu(cid:3)
first, which can be obtained by combining the expression of G˜ =ΣoldH˜T (cid:16) Sold +∆K(cid:17)−1
the posterior distribution (22) with the approximate likelihood t uu (29)
model (24), namely: ξnew =ξold−G˜ξold
t t t
Σnew =Σold−G˜H˜Σold
p(x t,f|y 1:t;θ new) t t t
=
p(f;θ new)p(x t,y 1:t|f) Insummary,byrecoveringanapproximatelikelihoodmodel
p(y ;θ ) from the current filtering distribution, this subsection derives
1:t new (25)
p(f;θ )q (x ,f)p(y ;θ ) theobjectivefunctionandposteriorupdateequationforonline
≈ new old t 1:t old
hyperparameter optimization. This method reduces the burden
p(y ;θ )p(f;θ )
1:t new old
ofofflinehyperparametertuningandimprovesonlinelearning
=q (x ,f)
new t accuracybyadjustingtheGPpriortomatchtheactualfunction
where q (x ,f) denotes the approximate joint distribution characteristics.
new t
correspondingtothenewhyperparametersθ .In(25),since Remark4.1:TheonlineoptimizationmethodforGPhyper-
new
the distributions p(f;θ ), p(f;θ ), and q (x ,f) are all parametersmostcloselyrelatedtothispaperis[31],anonline
old new old t
Gaussian,thenewapproximatedistributionq (x ,f)isalso GPRmethod.Inthismethod,thelikelihoodmodelissimilarly
new t
Gaussian. By expanding the distribution related to f in the recoveredfromtheapproximateposteriordistribution,anditis
third row of (25) with the conditional distribution p(f|u), it usedtooptimizethelocationofinducingpointsandGPhyper-
can be shown that the new distribution retains the factorized parameters. Clearly, in GPSSMs, it is possible to optimize the
form q (x ,f)=p(f|u;θ )q (x ,u), where: inducing inputs in the same way. However, we do not pursue
new t new new t
this approach because simultaneously optimizing the inducing
p(u;θ )q (x ,u)p(y ;θ ) inputsandhyperparameterswouldcomplicatethelossfunction
q new(x t,u)= pn (e yw o ;l θd t )p(u;θ1:t
)
old and the adjustment equation for the posterior distribution.
1:t new old (26) Additionally, we observe that in GPSSMs, due to the system
p(u;θ )q (x ,u)
∝ new old t states being implicitly measured, the difficulty of optimizing
p(u;θ )
old the inducing inputs increases. Therefore, we separate the
Therefore, we can obtain the new posterior distribution after optimization of inducing inputs and GP hyperparameters and
the GP hyperparameters change. Furthermore, by integrating determine the inducing-point set using a discretized selection
both sides of the first row of (26), we can obtain the marginal method.
likelihood for hyperparameter optimization, which leads to:
C. Algorithm Summary
p(y ;θ ) (cid:90) p(u;θ )q (x ,u)
1:t new = new old t dx du (27) Insummary,wehaveaddressedthefourchallengesoutlined
p(y ;θ ) p(u;θ ) t
1:t old old in the Introduction for implementing online GPSSMs. Specif-
Based on these results, the practical optimization objective ically, first, nonlinearity and coupling are handled through
and moment update equation for the posterior distribution can first-order linearization and joint inference of the system state
be derived, which are presented below. and GP, leading to a closed-form Bayesian update equation.8
and y = x , where c and y represent function input and
t t t t
Second,adynamicinducing-pointadjustmentalgorithmisde- output, RGPSSM is similar to KRLS and SOGP. Moreover,
veloped to overcome the computational burden caused by the benefitingfromlimitedassumptionsandthetwo-stepinference
non-parametric nature of the GP. Third, we recover the like- format, the RGPSSM is suitable for arbitrary kernel functions
lihood model from the approximate posterior distribution and and irregular measurement intervals or measurement losses.
utilize it for the online optimization of GP hyperparameters, SinceitbelongstotheAKFframework,theoretically,existing
thus enhancing learning accuracy. Based on these solutions, adaptive Kalman filter methods may be seamlessly integrated
wehavedevelopedanefficientonlineGPSSMmethodthatcan to adapt to the process and measurement noise covariance
adapt to both the operating domain and GP hyperparameters. within the RGPSSM. In the next section, we will validate
Due to its flexible recursive inference capability for GPSSMs, the effectiveness and advantages of RGPSSM through several
thismethodisreferredtoastherecursiveGPSSM(RGPSSM), experiments.
which is summarized in Algorithm 1.
V. EXPERIMENTRESULT
Algorithm 1 Recursive Gaussian Process State Space Model
(RGPSSM) Thissectiondemonstratestheproposedmethodthroughtwo
synthetic simulations and a real-world dataset. Section V-A
Input: initial inducing points u, threshold for adding
presents a comparative experiment on a synthetic NASCAR®
points ε , budget for the inducing-point set M.
tol
dataset.SectionV-BshowcasesthecapabilityofRGPSSMfor
1: repeat
hyperparameter and inducing points adaptation in a synthetic
2: Assess the novelty γ of the new point f t using (20).
wing rock dynamics learning task. Section V-C evaluates
3: if γ >ε tol then
the learning performance of RGPSSM on various real-world
4: Add f t to the inducing-point set and propagate the
datasets.
moments using (15).
The learning algorithm is implemented on a desktop com-
5: else
puter running Python 3.9, with an Intel(R) Core(TM) i7-
6: Propagate the moments using (21).
14700F 2.10 GHz processor and 32 GB of RAM, and the
7: end if
associated code is published online3. In all experiments, the
8: if the number of inducing points n u >M then
GP kernel employed is the Squared Exponential Automatic
9: Identify the least important point based on the score
Relevance Determination (SEARD) [19] kernel:
in (19) and remove it using (17).
10: end if
(cid:18) (cid:19)
11: if a new measurement y t is available then K (x,x′)=exp −1 (x−x′)TΛ−1(x−x′) (30)
12: Correct the moments using (16). 0 2
13: end if
wherethediagonalmatrixΛ=diag(l2,l2,...,l2 )represents
14: Minimize L in (28) to update the GP hyperparameters 1 2 nx
the length scales. To enable multi-output function learning
θ using the Adam optimizer [32], and subsequently
involved in the experiment, the kernel matrix actually used
update the moments using (29).
is K (·,·)⊗diag(σ2,σ2,...,σ2 ), with ⊗ denoting the Kro-
15: until operation ends 0 1 2 nf
necker product and σ2 representing the signal variance of the
i
i-th dimension of the function. In this kernel, both the length
In theory, Algorithm 1 has the recursive learning capability
scales l and signal variance σ2 are GP hyperparameters; the
for GPSSMs; however, as with the Kalman filter, there are i i
online optimization method for them will be evaluated in the
numerical stability issues when implemented on a computer.
following experiments.
Specifically, the algorithm requires the propagation of the
covariance matrix Σ of the augmented state X , which can
t t
easily lose positive definiteness due to the accumulation of A. Synthetic NASCAR® Dynamics Learning
rounding errors. To address this problem, a stable implemen- We evaluate the online learning performance of RG-
tation based on Cholesky factorization is derived, as detailed PSSM by comparing it with two representative algorithms:
in Appendix D. Then, considering the Cholesky factorization, SVMC [23], which represents PF-based methods, and OEnVI
along with the matrix inversion and determinant operations [18], which exemplifies SVI-based methods. Since the pre-
involved in Algorithm 1, the computational complexity of a parameterized AKF-based method is a special case of the
single update for RGPSSM is limited to O(M3). proposed method, it is not included in the comparison. In the
In addition, RGPSSM can be viewed as a generalization experiment, all methods are tested using synthetic NASCAR®
that encompasses the pre-parameterized AKF-based online
data [33], which involves a two-dimensional state with dy-
GPSSM method using inducing points [20] and the kernel-
namics following a recurrent switching linear dynamical sys-
basedonlineregressionmethodKRLS[28]andSOGP[30]as tem. The measurement model is given by y = Cx +
t t
specialcases.Moreprecisely,whentheRGPSSMisinitialized w , w ∼ N(0,0.12I ), where C is a 4-by-2 matrix with
t t 2
with pre-selected inducing points and does not update them
random elements. The three methods are trained with 500
by setting ε = +∞, it is similar to the pre-parameterized
tol measurements and tested by predicting the subsequent 500
AKF-based method. Additionally, if the transition and mea-
surement functions of GPSSMs are taken as x
t
= f(c t) 3https://github.com/TengjieZheng/rgpssm9
steps. Note that, unlike the experiments in [23] and [18], we
use fewer dimensions and a smaller number of measurements and an unknown uncertainty model ∆(θ,p). The uncertainty
for training, providing a more rigorous evaluation. For a fair model used in the simulation is taken from [34], namely:
comparison, the maximum number of inducing points across
all methods is limited to 20, and the GP hyperparameters
∆(θ,p)=W +W θ+W p
0 1 2
are set to the same values and are not adjusted online. The (32)
+W |θ|p+W |p|p+W θ2
implementation and other parameter settings for the SVMC 3 4 5
andOEnVIalgorithmsarebasedonthecodeprovidedonline4
where W = 0.8,W = 0.2314,W = 0.6918,W =
5. 0 1 2 3
−0.6245,W =0.0095,W =0.0214.Tofurtherdemonstrate
The experimental results are presented in Fig. 1, where the 4 5
the learning capacity of the proposed method under sparse
top row showcases the true and filtered state trajectories, and
sensorconditions,themeasurementislimitedtotherollangle
the bottom row depicts the filtering and prediction results. It
θ,whichiscorruptedbyGaussianwhitenoisewithastandard
is observed that RGPSSM and SVMC effectively extract the
deviation of 0.2 degrees.
dynamicsandprovidehigh-accuracypredictions,whileOEnVI
Toapplytheproposedmethodtolearnwingrockdynamics,
fails to learn the dynamics due to slow convergence and the
we specify a GP prior on the uncertainty model ∆(θ,p) and
large data requirements associated with SVI. Additionally, the
then apply Algorithm 1 to the discretized-time form of (31).
learning accuracy and computational efficiency are quantified
Forhyperparameteradaptation,aniterationofhyperparameter
in Table I. As shown in the table, although SVMC can learn
optimization is performed in each RGPSSM update with a
the dynamics from limited measurements, it incurs a high
learning rate of 0.01. Additionally, the budget for inducing
computational cost. On the other hand, while OEnVI has the
points is set to 20. In the following, we test the proposed
lowest computation time, it suffers from a slower learning
method under these conditions and evaluate the effects of
convergencerate.However,theproposedmethod,basedonthe
hyperparameter and inducing point adaptation.
AKF framework, strikes a superior balance between learning
convergence and computational efficiency. These results un- Firstly, we conduct a Monte Carlo test of the proposed
derscore the superior performance of the proposed RGPSSM learning algorithm with different initial values of GP hyper-
method. parameters. The simulation results are depicted in Fig. 2 and
3.AsshowninFig.2,allthreeGPhyperparametersgradually
TABLEI convergenearthereferencevalues,whichareobtainedthrough
PREDICTIONACCURACYANDCOMPUTATIONALEFFICIENCYOFTHE offline GP training using data pairs (θ,p,∆). In terms of
THREEONLINEGPSSMMETHODS.
learning accuracy, the prediction RMSE over the last 1/4
segment improves by 29.3% with hyperparameter adaptation.
Method RGPSSM SVMC OEnVI
To more intuitively illustrate the benefits of hyperparameter
PredictionRMSE 1.2552 5.9180 7.8583
adaptation, we present the state profile from one Monte Carlo
RunningTime(s) 8.59 42.69 6.55
simulationcase,asshowninFig.3.Inthefigure,thebluesolid
lines depict the true values of state θ(t),p(t) and uncertainty
∆(t); the orange dashed lines represent the filtered state and
B. Synthetic Wing Rock Dynamics Learning uncertaintypredictionsobtainedwithouthyperparameteradap-
tation; and the green dashed lines show the predictions with
To further evaluate RGPSSM’s ability to online adapt GP
hyperparameter adaptation. It is evident that hyperparameter
hyperparameters and inducing points, it is tested on the
adaptationimprovesboththemeanandvariancepredictionsof
synthetic wing rock dynamics learning task. In aerospace
the GP, thereby enhancing the filtering accuracy of the state,
engineering, wing rock dynamics can cause roll oscillations
especially the roll rate p.
in slender delta-winged aircraft, posing a significant threat
Secondly,toillustratetheadaptationprocessoftheinducing
to flight safety. To address this issue, most approaches rely
points and their advantages, we present the associated results
on online learning of wing rock dynamics [34]. The asso-
in Fig. 4. In this figure, the inducing inputs are represented
ciated dynamical system can be expressed by the following
by yellow diamonds, while the true state values are shown as
continuous-time SSM:
white circles. It is evident that the number of inducing points
θ˙ =p gradually increases to the predefined budget, and the points
(31) areconcentratedaroundtheexistingstates.Giventhelocalized
p˙ =L δ +∆(θ,p)
δa a nature of the SE kernel, this result indicates that the selected
whereθ,p∈Rdenotetherollangleandrollrate,respectively. inducing points can effectively represent the GP model. Ad-
ditionally, to assess learning accuracy, we use the background
Asillustratedin(31),thedynamicsmodelconsistsofaknown
contourplottovisualizethedistributionofpredictionerrorsin
partL δ ,whereδ istheaileroncontrolinputandL =3,
δa a a δa
thestatespace.Itisevidentthatthepredictionerrordecreases
4https://github.com/catniplab/svmc over time, particularly around the existing states, benefiting
5https://github.com/zhidilin/gpssmProj from the effective selection of inducing points.10
8 True 8 True 8 True
Inferred Inferred Inferred
6 s st ta oprt 6 s st ta oprt 6 s st ta oprt
4 4 4
2 2 2
0 0 0
2 2 2
4 4 4
6 6 6
8 8 8
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
(a)RGPSSM (b)SVMC (c)OEnVI
10 10 10
5 5 5
0 0 0
5 5 5
10 10 10
0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000
7.5 true true 7.5 true
5.0 f pil rt ee dr ie cd ted 5 f pil rt ee dr ie cd ted 5.0 f pil rt ee dr ie cd ted
2.5 2.5
0.0 0 0.0
2.5 2.5
5.0 5 5.0
7.5 7.5
0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000
t t t
(d)RGPSSM(ours) (e)SVMC[23] (f)OEnVI[18]
Fig.1. OnlinelearningresultsforNASCAR® dynamics.Top:filteredstatetrajectory;Bottom:filteringandprediction.
4
reference value 4 z1 z1 z1,adapt measurement
2
2 0
2
4
4 z2 z2 z2,adapt
2
2
0
10 2
6
fGP fGP,adapt
5 4
2
0
0 20 40 60 80 100 120 140
Time [s] 2
0 10 20 30 40 50
Fig.2. EvolutionoftheGPhyperparametersduringRGPSSMlearning Time [s]
Fig.3. Evolutionofthesystemstatesandtheirestimation,wheretheinitial
C. Real-World System Identification Task
v sha alu de os wo sf int dh ie caG teP 9h 5y %pe cr op na fira dm ene cte ers ina ter re val l1 s.= l2 = 5 and σ 12 = 10. The
To evaluate the generalization capability of the proposed
method, we conducted experiments on five real-world system
identification benchmark datasets6. The first half of each capped at 20, and a 4-dimensional state GPSSM was used
dataset was used for training, while the second half was to capture system dynamics, with a transition model defined
reserved for testing predictive performance. Due to limited as x t+1 = f(x t,u t+1), where x t ∈ R4, and a measurement
prior knowledge of the models and the restricted amount of model given by y t =[1,0,0,0]x t. In addition, to activate the
training data, these datasets are mainly used for assessing estimation for the latent state within RGPSSM, we randomly
offline GPSSM approaches. In this study, we performed an assign an inducing point around the origin before learning
online learning experiment with the proposed method, com- to make the transition Jacobian A x non-zero for each state
paring its performance against three state-of-the-art offline dimension (see our code for details). The experimental results
GPSSM techniques: PRSSM [14], VCDT [15], and EnVI involving multiple simulations with different random seeds
[18]. For each method, the number of inducing points was are summarized in Table II. As shown in the table, although
online learning is challenging, the proposed method achieves
6https://homes.esat.kuleuven.be/˜smc/daisy/daisydata.html thebestpredictionaccuracyinthreedatasets,whichismainly
1x
2x
1l
2l
21
1x
2x
]ged[
elgna
lloR
]s/ged[
etar
lloR
]2s/ged[
ytniatrecnU
1x
2x11
prediction
t=1.0s: 8 points t=4.0s: 12 points t=7.0s: 15 points t=10.0s: 20 points t=13.0s: 20 points error
3 3 3 3 3 7.2
6.4
2 2 2 2 2 5.6
1 1 1 1 1 4.8
4.0
0 0 0 0 0
3.2
1 1 1 1 1 2.4
2 sample 2 2 2 2 01 .. 86
3 inducing point 3 3 3 3 0.0
2 0 2 2 0 2 2 0 2 2 0 2 2 0 2
[deg] [deg] [deg] [deg] [deg]
Fig.4. Evolutionofinducing-pointsetandpredictionerrorovertime.
to handle non-Gaussian data likelihoods, and adapting it to
due to the explicit consideration of the dependence between process and measurement noise covariances, as well as time-
the system state and the GP, as shown in (8). These findings varying dynamics.
underscore the method’s robustness and ability to generalize
across diverse datasets.
APPENDIXA
SCOREFORSELECTIONOFINDUCINGPOINT
TABLEII
PERFORMANCECOMPARISONOFDIFFERENTGPSSMMETHODSACROSS This section derives the score quantifying the accuracy loss
FIVESYSTEMIDENTIFICATIONBENCHMARKS.PREDICTIONRMSEIS for discarding an inducing point, which is achieved based on
AVERAGEDOVERFIVESEEDS,WITHSTANDARDDEVIATIONSREPORTED
the KL divergence in (18). As shown in (18), D is unrelated
INPARENTHESES. 1
to the point to discard u . Therefore, only D and D need
d 2 3
Offline Online to be evaluated. Without loss of generality, suppose the index
Methods
PRSSM VCDT EnVI RGPSSM of the discarded point u d is i d =1.
Firstly, for D , the log-term in the intergration is:
0.691 0.815 0.657 1.048 2
Actuator
(0.148) (0.012) (0.095) (0.142)
0.074 0.065 0.055 0.046 1 1
BallBeam logp(u |u )=− log(2π)− log|γ |
(0.010) (0.005) (0.002) (0.002) d l 2 2 d
(33)
0.647 0.735 0.703 0.863 1
Drive − (u −k u )Tγ−1(u −k u )
(0.057) (0.005) (0.050) (0.083) 2 d dl l d d dl l
0.174 0.667 0.125 0.105
Dryer (0.013) (0.266) (0.017) (0.004) where k dl = K dlK− ll1 and γ d = K dd − K dlK− ll1K ld.
Denote Q = K−1, and using the matrix inversion formula
1.503 2.052 1.388 1.373 uu
GasFurnace (0.196) (0.163) (0.123) (0.061) (see Appendix A of [30]), we have:
γ =Q −1
d dd (34)
k =−Q −1Q
VI. CONCLUSION dl dd dl
where Q denotes the i th diagonal element of matrix Q,
Inthispaper,weproposeanovelrecursivelearningmethod, dd d
and Q denotes the i th row of Q with Q removed.
RGPSSM, for online inference in Gaussian Process State dl d dd
Using (18), (33) and (34), the u relevant term in D is:
Space Models (GPSSMs). First, to enable learning in any d 2
operating domain, we derive a two-step Bayesian update
equation that does not rely on any GP approximation. The D˜ = 1 log|Q |− 1 ∆
derivation incorporates a mild first-order linearization to ad- 2 2 dd 2
(cid:90)
dress the inherent nonlinearity within GPSSMs. Second, to ∆= q(u)(u −k u )TQ (u −k u )du
mitigate the computational burden imposed by the nonpara- d dl l dd d dl l (35)
metric nature of GPs, we introduce a dynamic inducing-point =E (cid:2) tr(cid:0) Q (u −k u )(u −k u )T(cid:1)(cid:3)
q(u) dd d dl l d dl l
set adjustment algorithm, ensuring computational efficiency. =tr(cid:0) Q E (cid:2) (u −k u )(u −k u )T(cid:3)(cid:1)
Third, to eliminate the burden for offline tuning of GP dd q(u) d dl l d dl l
hyperparameters, we develop an online optimization method Let ϕ = [1,−k ], and then we have u −k u = ϕu.
dl d dl l
that extracts measurement information from the approximate Using (34), we further have:
posterior distribution. Through extensive evaluations on a
diverse set of real and synthetic datasets, we demonstrate the ϕ=(cid:2) 1 Q−1Q (cid:3)
dd dl
superiorlearningperformanceandadaptabilityoftheproposed =(cid:2) Q−1Q Q−1Q (cid:3) (36)
dd dd dd dl
method. In the future, research may focus on improving the
=Q−1Q
accuracy of the first-order linearization by using techniques dd du
such as the iterative Kalman filter, extending the method where Q denotes the i th row of Q. Therefore:
du d
]s/ged[
p
]s/ged[
p
]s/ged[
p
]s/ged[
p
]s/ged[
p12
(cid:16) (cid:104) (cid:105)(cid:17)
∆=tr Q E ϕuuTϕT (cid:90) (cid:90)
dd q(u) qˆ∗(x ,u)= pˆ(x |x ,f )q(x ,u¯)dx df
(cid:16) (cid:104) (cid:105)(cid:17) t+1 t+1 t t t t t
=tr Q dd ϕ(S uu+m umT u)ϕT (37) (cid:90) (cid:90)
= pˆ(x |x ,f )p(f |u)q(x ,u)dx df
=∆ +∆ t+1 t t t t t t
1 2
(cid:90)
where = pˆ(x |x ,u)q(x ,u)dx
t+1 t t t
∆ =mTQT Q−1Q m
1 u du dd du u (44)
(cid:16) (cid:17) (38)
∆
2
=tr Q duS uuQT duQ− dd1 wherepˆ(x t+1|x t,u)representsanewtransitionmodel,whose
specific expression is:
For D , we can use the differential entropy identical for
3
Gaussian distribution: pˆ(x |x ,u)
t+1 t
(cid:90)
(cid:90) 1 = pˆ(x t+1|x t,f t)p(f t|u)df t
p(a)logp(a)=− [n +n log(2π)+log|Σ |] (39)
2 a a a (cid:90)
= pˆ(x |x ,f )N(f |k u,γ)df
t+1 t t t tu t
where p(a) is a n -dimension Gaussian distribution with
a (cid:16) (cid:12) (cid:17)
covariance Σ a. Deonte the covariance of joint distribution =N x t+1(cid:12) (cid:12)F t+A x(x t−µ t)+A fk tu(u−m u),Σ˜ f
q(X ) that marginal out u as Σ , and deonte the inversion (45)
t d t,l
of orignial covariance as Ω=Σ−1. Then, given (39), the u where γ =K −K K−1K and Σ˜ =A γAT +Σ .
t d tt tu uu ut f f f f
relevant term in D is: Combining (44) and (45), the prediction equation (21) can
3
be obtained.
1 1 (cid:16) (cid:17)
− log|Σ |=− log |Σ |/|S −ζTΣ ζ |
2 t,l 2 t dd d t,l d APPENDIXC
1 1 DERIVATIONDETAILSOFGPHYPERPARAMETERS
=− 2log|Σ t|− 2log|Ω dd| (40)
OPTIMIZATION
(cid:20) (cid:21)
V
ζ = xd This section derives the moments of the approximate joint
d S
ud distribution q (x ,f) in (26), along with the optimizing
new t
where Ω is the element of the matrix Ω corresponding objective for GP hyperparameters.
dd
to the discarded point u , namely, its (n +i )-th diagonal Firstly, to derive the moments of the approximate joint
d x d
element. In addition, (40) is dervied by using the properties distribution q new(x t,f), we can utilize the following result:
ofthedeterminantsofblockmatricesandthematrixinversion
p(u;θ ) N(u|0,Knew)
formula (see Appendix A of [30]). new = uu
Based on the KL divergence (18) and combing (35), (38)
p(u;θ old) N(u|0,Ko ul ud)
(46)
and (40), we have the scroe: =ZN(u|0,∆K)
=ZN(0|u,∆K)
s =∆ +∆ +∆ (41)
d 1 2 3 where
where
∆K =[(Knew)−1−(Kold)−1]−1
uu uu
|Kold|1/2|∆K|1/2 (47)
∆ 3 =log|Ω dd|−log|Q dd| (42) Z ∝ u |u
Knew|1/2
uu
Intermsofeffect,alowervalueofthescores impliesless
d Note that, the above equations hold only in a mathematical
accuracy loss for discarding the point u .
d sense and does not have probabilistic meaning, as ∆K may
not be positive definite. Combining (26) and (46), we have:
APPENDIXB
PREDICTIONEQUATIONWITHOUTADDINGPOINTS q new(x t,u)∝N(0|u,∆K)q old(x t,u) (48)
Accordingto(17),ifthenewinducingpointf t isdiscarded whose moments can be obtained by the correction eqution of
in the prediction step, the distribution after discarding can be Kalman filter:
given by:
H˜ =(cid:2) 0 I (cid:3)
(cid:90) nu
qˆ∗(x t+1,u)= q−(x t+1,u¯)df t (43) G˜ =ΣoldH˜T (cid:16) Sold+∆K(cid:17)−1
t uu (49)
Then, by incorporating the original prediction equation (12),
ξnew =ξold−G˜ξold
t t t
we have: Σnew =Σold−G˜H˜Σold
t t t13
where ξ and Σ represents the mean and covariance of the Then, for the case of prediction with adding points, it
t t
augmented state X =[xT,uT]T. involves one additional step: evaluating the Cholesky factor
t t
Secondly, in order to optimize the GP hyperparameters, L¯ of the augmented covariance Σ¯ after adding a point.
t t
considering (27) and (46), we can maximize: According to the definition, the augmented covariance Σ¯ can
t
be expressed as follows:
p(y ;θ )
log 1:t new (cid:20) Σ ζ (cid:21) (cid:20) V (cid:21)
p( (cid:90)y 1:t;θ old)
(50)
Σ¯
t
=
ζT
tt
S
tt
t
, ζ
t
=
S
ux tt (55)
=log ZN(0|u,∆K)q (x ,u)dx du
old t t In view of this expression, we can define:
=logN(0|mold,∆K+Sold)+logZ
(cid:20) (cid:21)
u uu L 0
L¯ = t (56)
Then, by dropping some θ -irrelevant terms, the opti- t α β
new
mization objective can be transformed to minimize:
and correspondingly we have:
L=L +L (51) (cid:20) L LT L αT (cid:21)
1 2 Σ¯ = t t t (57)
t αTLT ααT +ββT
where t
Combining (55) and (57), we can obtain α by solving
the linear equation L αT = ζ , and then compute β =
L 1 =(mo uld)T(So ul ud+∆K)−1mo uld cholesky(cid:0) S −ααT(cid:1)t , thus obtt aining the Cholesky factor
tt
L 2 =−log|∆K|+log|Kn uuew|+log|∆K+So ul ud|
(52)
L¯ t. Then, similar operations as in (53) and (54) can be
=log|Knew|+log|I +∆K−1Sold| implemented to evaluate the predicted Cholesky factor.
uu nu uu
For the correction step, given (16), the update equation for
=log|Knew+[I −Knew(Kold)−1]Sold|
uu nu uu uu uu covariance can be rewritten as:
Here, we avoid the appearance of ∆K in the loss function
through some operations, because when Kn uuew =Ko ul ud, ∆K Σ
t+1
=Σ− t+1−Σ− t+1HT(CP− t+1CT +Σ g)−1HΣ−
t+1
is not well-defined.
=Σ− −Σ− HT(ρρT)−1HΣ−
t+1 t+1 t+1
=L− L−T −ηηT
APPENDIXD t+1 t+1
(58)
STABLEIMPLEMENTATIONMETHOD
where ρ = cholesky(CP− CT + Σ ) and η =
t+1 g
To solve the numerical stability problem in RGPSSM, this Σ− HT(ρ−1)T. Considering (58), we can evaluate the
t+1
section develops a stable implementation method based on Cholesky factor L of the posterior covariance Σ using
t+1 t+1
Cholesky decomposition. Specifically, the covariance Σ t of the Cholesky downdate algorithm (See Section 6.5.4 of [35]),
theaugmentedstateX t isdecomposedasΣ t =L tLT t ,where namely, L t+1 =choldowndate(L− t+1,η).
L t isalowertriangularmatrix,knownastheCholeskyfactor. For discarding an inducing point, to evaluate the updated
Based on this decomposition, the evolution of the covariance Cholesky factor, we can express the original Cholesky factor
Σ t is replaced by the evolution of the Cholesky factor, which before discarding as follows:
canreducetheaccumulationofcomputationalroundingerrors.
 
To implement this method, we must modify the moment A 0 0
update equations within the prediction and correction steps, L t =a b 0 (59)
as well as during the discarding of inducing points and B c C
hyperparameter optimization.
Here, we reuse the notation A,B,C,a,b,c, where a,b,c
For the prediction step, there exist two cases: not adding
denote the blocks corresponding to the discarded point u .
d
points and adding points. Firstly, for the prediction equation Given(17),theupdatedcovarianceΣ′ istheoriginalonewith
t
without adding points (21), the covariance update can be
the i th row and i th column removed, which can be given
d d
rewritten by:
using the expression in (59):
Σ− t+1 =(cid:2) Φ tL t D f(cid:3)(cid:20) L DT t Φ T fT t (cid:21) (53) Σ′ t =(cid:20) BAA AT T BBT +A cB cTT +CCT(cid:21) (60)
where D f is the Cholesky factor of the process noise covari- Inviewofthisexpression,theupdatedCholeskyfactorL′ can
t
ance Σ . Then, through the following QR decomposition:
f,X be given by:
(cid:20) LTΦT(cid:21) (cid:20)
A 0
(cid:21)
t t =OR (54) L′ = (61)
DT t B cholupdate(C,c)
f
whereO isanorthogonalmatrixandRisanuppertriangular wherecholupdate(·,·)denotestheCholeskyupdatealgorithm
matrix, we can obtain the Cholesky factor of the predicted (see Section 6.5.4 of [35]), namely a efficient method for
covariance Σ− , namely, L− =RT. evaluating the Cholesky factor of (ccT +CCT).
t+1 t+114
For hyperparameter optimization, since ∆K in (29) may [20] A. Kullberg, I. Skog, and G. Hendeby, “Learning driver behaviors
not be positive definite as illustrated in Appendix C, we usingagaussianprocessaugmentedstate-spacemodel,”in2020IEEE
23rd International Conference on Information Fusion (FUSION), pp.
cannot update the Cholesky factor as in the correction step.
1–7. [Online]. Available: https://ieeexplore.ieee.org/abstract/document/
Therefore,theupdatedCholeskyfactorisevaluatedbydirectly 9190245
decomposing the updated covariance Σnew, which is obtained [21] A.Kullberg,“Onjointstateestimationandmodellearningusinggaus-
t sian process approximations,” Ph.D. dissertation, Linko¨ping University
using (29).
ElectronicPress,2021.
Overall, the Cholesky versions of the update equations in [22] K. Berntorp, “Online bayesian inference and learning of gaussian-
the prediction and correction steps, as well as for discarding processstate–spacemodels,”Automatica,vol.129,p.109613,2021.
[23] Y. Zhao, J. Nassar, I. Jordan, M. Bugallo, and I. M. Park, “Streaming
inducing points and optimizing hyperparameters, have been
variational monte carlo,” IEEE transactions on pattern analysis and
derived,whichmakesthecomputationsmorecompact,thereby machineintelligence,vol.45,no.1,pp.1150–1161,2022.
enhancing numerical stability. [24] Y.Liu,M.Ajirak,andP.M.Djuric´,“Sequentialestimationofgaussian
process-based deep state-space models,” IEEE Transactions on Signal
REFERENCES Processing,2023.
[25] A. Doucet, A. M. Johansen et al., “A tutorial on particle filtering and
[1] Y.Wu,J.M.Herna´ndez-Lobato,andZ.Ghahramani,“Gaussianprocess smoothing:Fifteenyearslater,”Handbookofnonlinearfiltering,vol.12,
volatility model,” Advances in neural information processing systems, no.656-704,p.3,2009.
vol.27,2014. [26] S.-S. Park, Y.-J. Park, Y. Min, and H.-L. Choi, “Online gaussian pro-
[2] M. Dowling, “Approximate bayesian inference for state-space models cess state-space model: Learning and planning for partially observable
of neural dynamics,” Ph.D. dissertation, State University of New York dynamical systems,” International Journal of Control, Automation and
atStonyBrook,2023. Systems,vol.20,no.2,pp.601–617,2022.
[3] C. Veiba¨ck, J. Olofsson, T. R. Lauknes, and G. Hendeby, “Learning [27] M. O’Connell, G. Shi, X. Shi, K. Azizzadenesheli, A. Anandkumar,
targetdynamicswhiletrackingusinggaussianprocesses,”IEEETrans- Y. Yue, and S.-J. Chung, “Neural-fly enables rapid learning for agile
actionsonAerospaceandElectronicSystems,vol.56,no.4,pp.2591– flightinstrongwinds,”ScienceRobotics,vol.7,no.66,p.eabm6597,
2602,2019. 2022.
[4] A.J.McHutchonetal.,“Nonlinearmodellingandcontrolusinggaussian [28] S. Van Vaerenbergh, M. La´zaro-Gredilla, and I. Santamar´ıa, “Kernel
processes,”Ph.D.dissertation,Citeseer,2015. recursiveleast-squarestrackerfortime-varyingregression,”IEEEtrans-
[5] S.S.Rangapuram,M.W.Seeger,J.Gasthaus,L.Stella,Y.Wang,and actions on neural networks and learning systems, vol. 23, no. 8, pp.
T.Januschowski,“Deepstatespacemodelsfortimeseriesforecasting,” 1313–1326,2012.
Advancesinneuralinformationprocessingsystems,vol.31,2018. [29] L. Csato´ and M. Opper, “Sparse on-line gaussian processes,” Neural
[6] D. Gedon, N. Wahlstro¨m, T. B. Scho¨n, and L. Ljung, “Deep state computation,vol.14,no.3,pp.641–668,2002.
spacemodelsfornonlinearsystemidentification,”IFAC-PapersOnLine, [30] L. Csato´, “Gaussian processes: iterative sparse approximations,” Ph.D.
vol.54,no.7,pp.481–486,2021. dissertation,Citeseer,2002.
[7] R. Frigola, “Bayesian time series learning with gaussian processes,” [31] T. D. Bui, C. Nguyen, and R. E. Turner, “Streaming sparse gaussian
Ph.D.dissertation,2015. process approximations,” Advances in Neural Information Processing
[8] R. Turner, M. Deisenroth, and C. Rasmussen, “State-space inference Systems,vol.30,2017.
andlearningwithgaussianprocesses,”inProceedingsofthethirteenth [32] D. P. Kingma, “Adam: A method for stochastic optimization,” arXiv
internationalconferenceonartificialintelligenceandstatistics. JMLR preprintarXiv:1412.6980,2014.
WorkshopandConferenceProceedings,2010,pp.868–875. [33] S.Linderman,M.Johnson,A.Miller,R.Adams,D.Blei,andL.Panin-
[9] J.F.Fisac,A.K.Akametalu,M.N.Zeilinger,S.Kaynama,J.Gillula, ski, “Bayesian learning and inference in recurrent switching linear
andC.J.Tomlin,“Ageneralsafetyframeworkforlearning-basedcontrol dynamical systems,” in Artificial intelligence and statistics. PMLR,
inuncertainroboticsystems,”IEEETransactionsonAutomaticControl, 2017,pp.914–922.
vol.64,no.7,pp.2737–2752,2018. [34] G. Chowdhary, H. A. Kingravi, J. P. How, and P. A. Vela, “Bayesian
[10] R.Frigola, F.Lindsten, T.B. Scho¨n,and C.E. Rasmussen,“Bayesian nonparametric adaptive control using gaussian processes,” IEEE trans-
inference and learning in gaussian process state-space models with actions on neural networks and learning systems, vol. 26, no. 3, pp.
particle mcmc,” Advances in neural information processing systems, 537–550,2014.
vol.26,2013. [35] G. H. Golub and C. F. Van Loan, Matrix computations. JHU press,
[11] R.Frigola,Y.Chen,andC.E.Rasmussen,“Variationalgaussianprocess 2013.
state-spacemodels,”Advancesinneuralinformationprocessingsystems,
vol.27,2014.
[12] A.SvenssonandT.B.Scho¨n,“Aflexiblestate–spacemodelforlearning
nonlineardynamicalsystems,”Automatica,vol.80,pp.189–199,2017.
[13] S. Eleftheriadis, T. Nicholson, M. Deisenroth, and J. Hensman, “Iden-
tification of gaussian process state space models,” Advances in neural
informationprocessingsystems,vol.30,2017.
[14] A. Doerr, C. Daniel, M. Schiegg, N.-T. Duy, S. Schaal, M. Toussaint,
andT.Sebastian,“Probabilisticrecurrentstate-spacemodels,”inInter-
nationalconferenceonmachinelearning. PMLR,2018,pp.1280–1289.
[15] A. D. Ialongo, M. Van Der Wilk, J. Hensman, and C. E. Rasmussen,
“Overcoming mean-field approximations in recurrent gaussian process
models,” in International Conference on Machine Learning. PMLR,
2019,pp.2931–2940.
[16] J.Lindinger,B.Rakitsch,andC.Lippert,“Laplaceapproximatedgaus-
sianprocessstate-spacemodels,”inUncertaintyinArtificialIntelligence.
PMLR,2022,pp.1199–1209.
[17] X. Fan, E. V. Bonilla, T. O’Kane, and S. A. Sisson, “Free-form
variationalinferenceforgaussianprocessstate-spacemodels,”inInter-
national Conference on Machine Learning. PMLR, 2023, pp. 9603–
9622.
[18] Z. Lin, Y. Sun, F. Yin, and A. H. Thie´ry, “Ensemble kalman filtering
meets gaussian process ssm for non-mean-field and online inference,”
IEEETransactionsonSignalProcessing,2024.
[19] C.E.RasmussenandC.K.I.Williams,GaussianProcessesforMachine
Learning, 3rd ed., ser. Adaptive Computation and Machine Learning.
MITPress.