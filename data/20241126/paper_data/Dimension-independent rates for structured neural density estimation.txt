Dimension-independent rates for structured neural density
estimation
Robert A. Vandermeulen∗ Wai Ming Tai† Bryon Aragam
November 25, 2024
Abstract
Weshowthatdeepneuralnetworksachievedimension-independentratesofconvergence
forlearningstructureddensitiessuchasthosearisinginimage,audio,video,andtextappli-
cations. Moreprecisely,wedemonstratethatneuralnetworkswithasimpleL2-minimizing
loss achieve a rate of n−1/(4+r) in nonparametric density estimation when the underlying
density is Markov to a graph whose maximum clique size is at most r, and we provide evi-
dencethatintheaforementionedapplications,thissizeistypicallyconstant,i.e.,r=O(1).
WethenestablishthattheoptimalrateinL1 isn−1/(2+r) which,comparedtothestandard
nonparametric rate of n−1/(2+d), reveals that the effective dimension of such problems is
the size of the largest clique in the Markov random field. These rates are independent of
thedata’sambientdimension,makingthemapplicabletorealisticmodelsofimage,sound,
video, and text data. Our results provide a novel justification for deep learning’s ability to
circumvent the curse of dimensionality, demonstrating dimension-independent convergence
rates in these contexts.
1 Introduction
Deeplearninghasemergedasaremarkablyeffectivetechniquefornumerousstatisticalproblems
that were historically extremely challenging, especially in high-dimensional settings. In the
realm of deep generative models, which can be framed as density estimation, deep methods
have showcased the ability to learn density functions with thousands or millions of dimensions
using merely a few million data points (Oussidi & Elhassouny, 2018; Ho et al., 2020; Cao et al.,
2024). Thisstandsinstarkcontrasttostandarddensityestimationtheory,whichwoulddemand
astronomicalsamplesizesduetothecurseofdimensionality. Themanifoldhypothesis isperhaps
themostwidelyacceptedexplanationfordeeplearning’sabilitytocircumventthiscurse(Bengio
et al., 2013; Brahma et al., 2016). This hypothesis posits that, despite a distribution’s ambient
space being high-dimensional, the mass of the density is heavily concentrated around a lower-
dimensional subset of that space, such as an embedded manifold. As we will argue later, for
complex data types of significant interest—images, video, sound, and text—this assumption is
intimately linked to spatio-temporal locality. For instance, covariates that are nearby spatio-
temporally, e.g., neighboring pixels, tend to be strongly dependent, suggesting they lie near a
lower-dimensional subspace.
Thispaperinvestigatesthebenefitsofleveragingtheconversestructure: Theindependence of
spatio-temporallydistantcovariates. Covariatesthatarespatio-temporallydistantoftenexhibit
near-independence, particularly when conditioned on intervening covariates. Consider a sound
recording: two one-second segments separated by a minute might share common elements, such
as the same speaker. However, given the intervening minute of audio, these segments become
effectively independent. The minute-long interval contains sufficient information to render the
separatedsegmentsmutuallyuninformative. Thisprincipleofconditionalindependenceextends
to various data types, including images, where pixels far apart tend to be independent when
conditioned on the surrounding region. This sort of dependence structure is naturally described
by a Markov random field (MRF).
∗Email: robert.anton.vandermeulen@gmail.com. †The work was done when the author was at Nanyang
TechnologicalUniversityandwassupportedbySingaporeAcRFTier2grantMOE-T2EP20122-0001.
1
4202
voN
22
]LM.tats[
1v59051.1142:viXraPath (L ) Grid (L )
4 3×3
Grid with Diagonals (L+ )
X=x X=x 3×4
(a) (b)
Figure1: (a): ExamplesofconditioningX =x(dottedlines)whenadensity’ssupportisaman-
ifold (solid lines). (b): Highly simplified examples of common MRF graphs. Paths correspond
to sequential data and grids to spatial.
In this work, we show that, for a very general class of densities that are Markov to an
undirectedgraph(a.k.a. MRF),densityestimationcanbeachievedwithaneuralnetworkanda
simpleL2 minimizinglossatarateofapproximatelyn−1/(4+r), wherer isthesizeofthelargest
clique in the graph. To compare, without the MRF assumption, the same class of densities
can be estimated at a rate no better than n−1/(2+d), implying that the effective dimension is
approximately r. We argue and show evidence that the MRF assumption is valid for many data
types where neural networks excel (images, sound, video, etc.; see Figure 1b) and demonstrate
that this approach to density estimation causes the effective dimension r of these problems to
remain constant or at least be orders of magnitude smaller than the ambient dimension d.
2 Background and Related Work
In this section, we lay the foundation for our main results by introducing key concepts and re-
lated work. We begin by discussing traditional approaches to nonparametric density estimation
and their limitations, particularly the curse of dimensionality. We then explore the manifold
hypothesis, a widely accepted explanation for the success of deep learning in high-dimensional
settings. Following this, we introduce Markov random fields (MRFs) and their applications in
modeling various types of data, including images and sequential information. This background
will provide the necessary context for understanding the novelty of our approach, which lever-
agesMRFstructurestoachievedimension-independentconvergenceratesindensityestimation,
offering an alternative perspective to the manifold hypothesis.
2.1 Nonparametric Density Estimation
Density estimation is the task of estimating a d-dimensional target probability density p from
iid
observeddata,x ,...,x ∼ p. Ofcourse,thisisaclassicalproblemforwhichwedonotintendto
1 n
provide a comprehensive overview, and instead refer readers to books such as Devroye & Gyorfi
(1985); Devroye & Lugosi (2001); Tsybakov (2009) for additional background. Historically, p
was assumed to belong to a specific class of distributions, such as Gaussian, and the estimator
pˆ was selected accordingly. For more complex p, nonparametric density estimators like kernel
n
density estimators or histograms are employed (Devroye & Gyorfi, 1985; Devroye & Lugosi,
2001). These methods converge to p for any density given sufficient data, but notably suffer
from the curse of dimensionality. For instance, when p is Lipschitz continuous2 and estimator
parameters are optimally chosen, the L1 error, (cid:82) |p(x)−pˆ (x)|dx = ∥p−pˆ ∥ , converges at
n n 1
rate O(n−1/(2+d)). This nonparametric rate is known to be optimal for Lipschitz continuous
densities, and numerous studies over the past decade have established that neural networks and
generative models can achieve this optimal rate (Liang, 2017; Singh et al., 2018; Uppal et al.,
2019; Oko et al., 2023; Zhang et al., 2024; Kwon & Chae, 2024). This rate implies that the
sample complexity grows exponentially in the dimension d, making the success of deep neural
2Afunctionf :Rd→RisLipschitz continuous ifthereexistsL≥0suchthat|f(x)−f(y)|≤L∥x−y∥ for
2
allx,y.
2networks for estimating densities with millions of dimensions all the more remarkable. This is
often explained via the manifold hypothesis.
2.2 Manifold Hypothesis
Themanifoldhypothesispositsthatmanyhigh-dimensionalreal-worlddistributionsconcentrate
around lower-dimensional spaces, such as submanifolds of the ambient space. This assumption
underpins, either explicitly or implicitly, numerous machine learning methods. For instance,
principal component analysis assumes that a distribution is concentrated around an affine sub-
space, while sparsity assumptions can be formulated as a union of manifolds, where the subsets
of the union are axis-aligned subspaces.
The success of deep learning methods in handling high-dimensional data, such as images,
videos, and audio, is frequently attributed to the manifold hypothesis. Experimental validation
of the manifold hypothesis has been conducted for image datasets. In Pope et al. (2021), the
authors determined that the intrinsic dimension of the ImageNet dataset lies between 25 and 40
dimensions, significantly lower than its ambient dimension. This hypothesis is closely linked to
correlation and dependency between covariates. In images, for example, adjacent pixels x and
i
x typically have similar values, causing the dataset to concentrate towards the linear subspace
j
x =x ,whichisasubmanifold. Figure3(a)illustratesthisconcept,showingthevaluesofpixels
i j
(8,8) and (8,9) for 100 randomly selected images from the grayscaled CIFAR-10 dataset, where
a strong concentration along the diagonal is evident.
Further supporting this hypothesis, Carlsson et al. (2008) discovered that the set of 3×3
pixel patches from natural images concentrates around a 2-dimensional manifold. Theoretically,
distributions concentrating around lower-dimensional subsets of the ambient space have been
showntoyieldimprovedestimationproperties. Forinstance,Weed&Bach(2019)demonstrated
that while a dataset typically converges at rate n−1/d to the true distribution in Wasserstein
distance, when the dataset exhibits a lower d′-dimensional structure, it converges at the faster
rate of n−1/d′. Similar results illustrating the manifold hypothesis and its benefits can be found
in Pelletier (2005); Ozakin & Gray (2009); Jiang (2017); Schmidt-Hieber (2019); Nakada &
Imaizumi (2020); Berenfeld et al. (2022); Jiao et al. (2023); Tang & Yang (2024). Another line
ofrelatedworkusesBarronfunctions,whichareaclassoffunctionsinspiredbyneuralnetworks,
to achieve dimension-free rates (Barron, 1993; Klusowski & Barron, 2018; Ma et al., 2022; Cole
& Lu, 2024), in contrast to our use of weaker Lipschitz-type assumptions.
While the manifold hypothesis explains local dependencies, it’s worth considering scenarios
that deviate from this model. For example, the manifold hypothesis cannot be satisfied when
covariates are independent. For example, if two covariates x ∼ p and y ∼ p are indepen-
x y
dent, their joint density p (x,y) = p (x)p (y) fills a rectangle in their product space. As one
x,y x y
may expect, pixels that are distant from one another tend to become more independent. This
phenomenon is illustrated in Figure 3(d), which plots the grayscale values of pixels (8,8) and
(14,28), showing a more dispersed pattern. This observation naturally leads to modeling the
spaceofimagesasaMarkovrandomfield, wherelocaldependenciesarecapturedwhileallowing
for independence between distant pixels.
2.3 Markov Random Fields
A Markov random field (MRF) consists of a random vector x = (x ,...,x ) and a graph G =
1 d
(V,E), where the graph’s vertices correspond to the entries of the random vector, i.e., V =
{x ,...,x }. Thegraphencodesinformationabouttheconditionalindependenceofthevector’s
1 d
(cid:0) (cid:1)
entries. For a set A = {a ,...,a } ⊂ {1,...,d}, let x = x ,...,x . Given three disjoint
1 d′ A a1 a d′
subsets A,B,C of {x ,...,x }, the graph G indicates that the random vectors x and x are
1 d A B
conditionally independent given x if there is no path from A to B that doesn’t pass through
C
C.
Considerasimpleexamplewithrandomvariablesx,y,andz,wherex=y+ϵ andz=y+ϵ ,
x z
with ϵ , ϵ , and y being jointly independent. In this scenario, the distributions of x and z are
x z
conditionally independent given y. Figure 2 illustrates the corresponding MRF graph for this
example.
It’s important to understand that while an MRF conveys information about conditional in-
dependence, the absence of such information in the MRF does not necessarily imply dependence
3(a) (8,8) vs (8,9) (b) (8,8) vs (8,10) (c) (8,8) vs (9,12) (d) (8,8) vs (14,28)
(e) (8,8) vs (8,9) cond. (f) (8,8) vs (8,10) cond. (g) (8,8) vs (9,12) cond. (h) (8,8)vs(14,28) cond.
Figure 3: Top row: Scatterplots comparing the grayscale values of pixel (8,8) with various other
pixels for 100 randomly selected images. The decreasing correlation between pixels as their
distance increases is evident.
Bottom row: The same comparisons as the top row, but conditioned on pixel (9,8) having a
value approximately equal to 0.48 (the median value for this pixel across the dataset). Note
the increased concentration of points towards the center along the horizontal axis, indicating
reduced correlation when conditioned on a neighboring pixel.
These plots demonstrate how pixel correlations decrease with distance and how conditioning on
a neighboring pixel can significantly reduce correlations, supporting the use of Markov Random
Field models for image data. Similar plots for the COCO dataset can be found in Appendix F.
in the actual data. In other words, covariates can be conditionally independent in reality even
if this independence is not explicitly represented in the MRF structure. The MRF provides
a conservative model of independence relationships, capturing known or assumed conditional
independencies without ruling out additional independencies that may exist in the data. Con-
sequently, any random vector associated with a complete graph—where all vertices are adjacent
to one another—is a valid MRF, since it provides no information about the independence of the
covariates. This is because every vertex is connected to every other vertex, so removing any
number of vertices will never separate the graph into multiple components. Because it conveys
noinformationabouttheconditionalindependenceofthecovariates,itevenappliestoarandom
vector where all entries are independent.
One of the most well-known MRFs is the Markov chain. A
Markovchainoflengthdcorrespondstothe“path” graphL ofd
d x y z
random variables. The above example with x,y,z corresponds to
the graph L and the MRF corresponding to L shown in Figure
3 4 Figure 2: An example MRF.
1b. InaMarkovchain,theindicesareofteninterpretedasatime
The random variables x and
parameter. A classic example is a gambling scenario: A person’s
y are independent given z.
money at time t+1 is conditionally independent of their total
value at time t−s (for s > 0), given their value at time t. This
property, known as the Markov property, encapsulates the idea that the future state depends
only on the present state, not on past states. Markov chains have a long history of use for
modeling sequential information, including audio and text data.
Beyondsequentialdata,MRFshaveseensignificantuseinimageprocessing. Inthiswork,we
focus on grayscale images for simplicity, bearing in mind that the results extend to RGB/color
imagesaswell. Forimageprocessing,theclassicMRFmodelconsistsofarandomvariablethatis
a2-dimensionalgridX=[X ] andagraphG whereallpixelsadjacentinXarealsoadjacent
i,j i,j
inG. Figure1bcontainsoneexamplefromtwodifferenttypesofgridgraphs: onestandard“grid”
4graphL andone“gridwithdiagonals” graphL+ ,wherethesubscriptsindicatethenumber
3×3 4×3
of rows and columns of vertices, respectively. For the remainder of this work, our references to
“grid” graphs encompass both variants—those with and without diagonal connections—unless
explicitly stated otherwise. Such models have seen wide use in image processing and computer
vision(seeLi,1994,foranoverview). Denoisingstandsasperhapsthemostcommonapplication
ofMRFsinimageprocessing. Thisapproachassumesthateachpixelisbestpredictedusingjust
itsneighborsandignoringtherestoftheimage. Whilethismodelproveseffectiveformitigating
phenomena like additive white noise (Keener, 2010), it falls short as a comprehensive image
model. Similarly, the path graph, often used for sequential data, oversimplifies the complex
dependencies in real-world sequential information.
3 Improving The Path and Grid Markov Random Field
Models
While standard path and grid MRF models may suffice for correcting extremely local or high-
frequencynoiseinsequentialorspatialdata,theyfallfarshortofcapturingthetruedistribution
of complex data types. Consider, for example, audio data consisting of 21-second clips where
the middle second is missing and needs to be predicted. According to the path MRF model,
this missing second would depend solely on the audio samples directly preceding and following
it. Consequently, under a Markov chain (i.e. path MRF) model, the remaining 20 seconds of
audio (less two samples) would be deemed completely uninformative for predicting the middle
second, given these two adjacent samples.
This simplistic model fails to capture the richer, longer-range dependencies present in real-
worldaudiodata. Inpractice,thecontentofthemissingsecondislikelyinfluencedbyabroader
contextthanjustitsimmediateneighbors. Forinstance, therhythmorthemeestablishedinthe
precedingfewseconds,ortheanticipationofwhatfollowsimmediatelyafter,couldbecrucialfor
predicting the missing segment. This moderately broader context is entirely discarded by the
basicpathMRF.Similarly,forimagedata,thestandardgridMRFmodelsuggeststhataregion
of an image depends only on its immediate bordering pixels. However, realistic images often
exhibitpatternsandstructuresthatspanmultiplepixelsinvariousdirections. Forexample, the
edge of an object or a gradient in lighting might extend across several pixels, creating depen-
dencies that the basic grid model fails to capture. Figure 4 illustrates this concept concretely,
demonstrating the effects of different MRF models on image inpainting tasks and highlighting
theimplicationsofvaryinglevelsofcontextualinformation. Theselimitationsmotivatetheneed
for more sophisticated MRF models where segments or regions are more extensively connected,
allowing for the incorporation of relevant contextual information without necessarily spanning
the entire dataset.
To model sequential and spatial data more realistically, we propose using the “power graph”
of the path and grid models. For a graph G, the power graph Gt with t ∈ N is defined as the
graph where an edge exists between every pair of vertices within t steps of each other in G, with
G1 =G. Figures 5 and 6 illustrate this concept using path graphs and grid graphs, respectively.
This construction causes contiguous sections of sequences and patches of grids to become fully
connected, as demonstrated in Figure 5.
Applying this power graph concept to a grid graph assumes that local patches of images
are highly dependent, making no assumptions about conditional independence within a patch.
It also implies that distant regions of an image become independent as the distance between
them increases, and that these regions are independent when conditioned on a sufficiently wide
separating region of pixels.
We can experimentally validate these assumptions using image data. The top row of Figure
3 shows the grayscale values of pixel (8,8) versus selected other pixels for 100 randomly chosen
images from the CIFAR-10 training dataset. The bottom row repeats this experiment, but
conditioned on the value of the adjacent pixel (9,8) being near its median value.
These experiments reveal that, when conditioned on the adjacent pixel, the dependence (as
measured by correlation) decreases significantly. Notably, pixels (8,8) and (9,12) appear almost
completely independent when conditioned on pixel (9,8). This provides strong evidence for the
validityoftheMRFmodel. TheMRFmodelpredictsthat(8,8)and(9,12)shouldbeindependent
when conditioned on surrounding pixels (the number of which depends on the graph power of
5(a) (b) (c) (d)
Figure 4: The leftmost image (a) is a 640×427 pixel photograph from the COCO 2014 dataset
(Lin et al., 2014). Image (b) shows an enlarged version of the 102×102 pixel region outlined in
(a). Images(c)and(d)displaythe12-pixeland1-pixelwidthbordersofthatregion,respectively.
ModelingthisimagewithanMRFgraphL orL+ wouldimplythatthedistribution
640×427 640×427
of the missing interior in (d) depends exclusively on its 1-pixel wide border, with the rest of the
image in (a) being uninformative for predicting this interior region. In contrast, predicting the
interiorusingthe12-pixelborderin(c)ismorereasonable. Thisscenariocorrespondstomodels
like L6 or (cid:0) L+ (cid:1)6 , which capture more extensive local dependencies. It’s important
640×427 640×427
to note that for the MRF model to hold, the interior doesn’t need to be deterministically con-
structedfromthesurroundingpixels. Rather,thesurroundingpixelsneedonlyprovidesufficient
informationabouttheinterior(e.g.,thatit’sacat’sface)suchthattherestoftheimagedoesn’t
contribute any additional information for predicting the interior region.
Figure 5: Illustrations of a path graph and its powers. Left: The path graph L . Center: The
5
power graph L2. Right: The power graph L3. In L , only immediately contiguous vertices
5 5 5
are connected. In L2, every group of three contiguous vertices forms a complete subgraph.
5
In L3, every group of four contiguous vertices forms a complete subgraph. This progression
5
demonstrates increasing connectivity among nearby vertices in the graph.
the MRF graph). Remarkably, we observe that pixels appear independent when conditioned
on just a single adjacent pixel, suggesting that the grid MRF assumption may be even more
conservative than necessary.
The power graph extension of path and grid MRFs presents a fundamentally different per-
spective on modeling high-dimensional data compared to the widely accepted manifold hypoth-
esis. While the manifold hypothesis posits that high-dimensional data concentrates around
lower-dimensional structures, our MRF approach embraces the full dimensionality of the data,
focusing instead on the independence structure between variables. This model aligns well with
the observed structure in various data types, capturing local dependencies while allowing for
long-range independencies. For sequential data such as audio or text, it accounts for strong
dependencies between nearby elements while acknowledging the decreasing influence of distant
context. In spatial data like images, it models high correlation between neighboring pixels
and gradual decorrelation as distance increases. Our experimental results provide compelling
evidence for this MRF model’s validity. The observed conditional independence between dis-
tant pixels, given intervening pixels, supports our power graph MRF approach’s fundamental
assumptions.
It’simportanttonotethatthisapproachisnotmeanttosupersedethemanifoldhypothesis,
but instead to augment it. The manifold hypothesis explains sample efficiency from local struc-
ture,whiletheMRFmodeladdsadditionalmodelefficiencyfromaglobalperspective. Together,
they provide a more comprehensive framework for understanding high-dimensional data.
Remarkably,inthefollowingsection,wewilldemonstratethatundertheseMRFassumptions,
there exist estimators based on neural networks with standard loss functions (e.g. squared loss)
that can achieve dimension-independent rates of convergence for density estimation. This result
is particularly significant as it suggests a path to overcoming the curse of dimensionality in
high-dimensional density estimation tasks without relying on low-dimensional embeddings. By
6(a) (b) (c) (d)
Figure6: Comparisonofvertexneighborhoodsindifferentgraphstructures. (a)Neighborhoodof
avertexinastandardgridgraphL . (b)Neighborhoodofthesamevertexinthepowergraph
d×d
L2 . (c) Neighborhood of a vertex in a grid graph with diagonals L+ . (d) Neighborhood of
d×d d×d
the same vertex in the power graph (L+ )2.
d×d
focusingonindependencestructuresratherthandimensionreduction,ourapproachoffersanovel
explanation for the success of deep learning methods in processing complex, high-dimensional
data, complementing and contrasting with the insights provided by the manifold hypothesis.
4 MRF-Based Density Estimation with Neural Networks
We begin by presenting the foundational results of this work that demonstrate that one can
estimate a density p given its Markov graph, at a rate that depends only on the size of the
largest clique of the graph. We will present two results, one using a neural network style archi-
tecture using a practical empirical risk minimization style training and a second estimator that
is more complex and computationally intractable that achieves approximately optimal rates of
convergence.
4.1 Structured neural density estimation
OurestimatorsarebasedontheclassicalHammersley-CliffordTheorem(Hammersley&Clifford,
1971). Before presenting the theorem we must review a few concepts. A graph G is called
complete if every vertex is adjacent to every other vertex. For a graph G = (V,E) a clique is a
complete subgraph, i.e., G′ = (V′,E′) with V′ ⊂ V and E′ ⊂ E, that is complete. A maximal
clique of a graph is a set of cliques which are not contained within another clique. Observe
that maximal cliques of the same graph can have different numbers of vertices. See Figure 7 for
examples of maximal cliques. The collection of the maximal cliques of a graph will be denoted
C(G).
Proposition 4.1 (Hammersley & Clifford, 1971). Let G = (V,E) be a graph and p be a prob-
ability density function satisfying the Markov property with respect to G. Let C(G) be the set of
maximal cliques in G. Then
(cid:89)
p(x)= ψ (x ),
V′ V′
V′∈C(G)
where x are the indices of x corresponding to V′.
V′
For neural networks we investigate estimators of the form:
(cid:89)
pˆ(x)= ψ(cid:98)V′(x V′),
V′∈C(G)
whereψ(cid:98)V′ areReLUnetworkswitharchitecturesdependentonly Figure7: Agraphwithmax-
on G, |V′|, and the sample size n. The weights are constrained imal cliques denoted by sur-
to [−1,1], effectively implementing weight decay via constrained rounding rectangles.
optimization rather than norm penalization.
We analyze an estimator that minimizes the integrated
squared error between p and our estimator pˆ:
(cid:90) (cid:90) (cid:90) (cid:90)
(p(x)−pˆ(x))2dx= p(x)2dx−2 p(x)pˆ(x)dx+ pˆ(x)2dx. (1)
7In empirical minimization, the first term of equation 1 is constant. The second term can be
estimated using the law of large numbers:
(cid:90)
p(x)pˆ(x)dx=E [pˆ(x)]≈
1 (cid:88)n
pˆ(x ) where x ,...,x i. ∼i.d. p.
x∼p n i 1 n
i=1
The last term can be estimated stochastically. Let U be the d-dimensional uniform distri-
d
i.i.d.
bution on the unit cube and ϵ ,ϵ ,...,ϵ ∼ U . Then:
1 2 n′ d
(cid:90) pˆ(x)2dx=E (cid:2) pˆ(ϵ)2(cid:3) ≈ 1 (cid:88)n′ pˆ(ϵ )2.
ϵ∼Ud n′ i
i=1
4.2 Main result
We now present our theorem on the convergence rate for L2-minimizing neural network-based
density estimators:
Theorem 4.2. Let G =(V,E) be a finite graph and r be the size of the largest clique in G.
There exists a known sequence of architectures F∗ such that for
(cid:32) n (cid:33)
pˆ =arg min ∥f∥2− 2 (cid:88) f(x ) ,
n f∈F∗ 2 n i
i=1
i.i.d.
where x ,...,x ∼ p, we have
1 n
(cid:16) (cid:17)
∥p−pˆ n∥
1
∈O(cid:101)p n−1/(4+r) ,
for any Lipschitz continuous, positive density p satisfying the Markov property with respect
to G.
The proof ofthetheorem, based onresults fromSchmidt-Hieber (2017), detailsthearchitec-
tures and specifies how their parameters scale with the sample size. The proof of this theorem,
and all results in this work, can be found in the appendices. The minimax rate for density esti-
mationond-dimensionaldensitiesisO(cid:0) n−1/(2+d)(cid:1)
,sothe“effectivedimension” ofanestimating
adensityusingtheestimatorfromthetheoremaboveisr+2. Consequentlyweseethattherate
of convergence for density estimation can be greatly improved for MRFs with certain graphs G.
We will discuss the consequences of this in greater detail in Section 4.3.
4.3 Consequences of Main Results
Our results indicate that the effective dimension of any density estimation problem under MRF
assumptions is the size of its largest clique. The following results demonstrate examples where
the largest clique is significantly smaller than the full dimensionality.
For definitions of the graphs L and L+ , we refer the reader to the examples in Figure
d×d′ d×d′
1b. While these examples should provide intuitive understanding, formal definitions can be
found in Appendix D.
Images We begin with the most compelling setting, corresponding to images:
Lemma 4.3. Let L be a d×d′ grid graph with t < d,d′. The size of the largest clique in
d×d′
Lt is less than or equal to t2+4t+3.
d×d′ 2
Lemma 4.4. Let L+ be the d×d′ grid graph with diagonals, and t < d,d′. The size of the
d×d′
largest clique in the graph (cid:0) L+ (cid:1)t is (t+1)2.
d×d′
Judging from the exmaple in Figure 3, t = 2 already gives a fairly reasonable model for
images. Thus we have the following dimension-independent rate:
8Corollary 4.5 (Dimension-independent rates). The neural density estimator in Theorem 4.2
achieves a rate of
(cid:16) (cid:17)
∥p−pˆ n∥
1
∈O(cid:101)p n−1/7 for the grid graph L2
d×d′
and
∥p−pˆ n∥
1
∈O(cid:101)p(cid:16) n−1/9(cid:17) for the grid with diagonals graph (cid:0) L+ d×d′(cid:1)2 .
Even when t > 1, we have r = O(t2) with t ≪ d. In practice we expect t = O(1), so even
with t>1, the rates are still dimension-independent.
Recall that if a density p is an MRF with respect to a graph G = (V,E), it is also an
MRF with respect to any graph G′ = (V,E′) that contains all the edges from G, i.e., E ⊆ E′.
Thus, the absence of edges in an MRF represents a stronger condition on p. In the graph
Lt , every (t+1)×(t+1) block of vertices is fully connected. As demonstrated in Figure
d×d′
3, when conditioned on an adjacent pixel, pixels tend to become independent with very little
distance between them. Figure 3h shows that pixels (8,8) and (9,12) are seemingly independent
conditioned on (9,8). Modeling CIFAR-10 as an MRF graph L+ would imply that (8,8)
32×32
and(9,12)areindependentconditionedonevery pixel surrounding (8,8), amuchmorestringent
requirement than conditioning on one adjacent pixel. Thus, modeling CIFAR-10 as (L+ )2
32×32
appears to be a conservative approach. Consequently, the effective dimension for estimating
CIFAR-10 is (2+1)2 =9 rather than 32×32=1024, an over 100-fold improvement!
Sequences For sequential data, we have the following lemma:
Lemma 4.6. Let L be a d-length path graph. The size of the largest clique in Lt is equal to
d d
min(t+1,d).
Again, we observe that the effective dimension can be far less than the ambient dimension
for sequential data, such as audio.
TheMRFapproachcanbeextendedtovariousdatatypes,yieldingsimilardimensionreduc-
tion results. For instance, color images can be modeled as a three-dimensional random tensor
X∈Rc×w×h with a graph G. In this model, the vertices in X ∪X are fully connected for
:,i,j :,i′,j′
|i−i′|≤1 and |j−j′|≤1, corresponding to a grid graph with diagonals where all channels are
connected. Video data can be represented by four-dimensional graphs corresponding to order-4
tensors in Rt×c×w×h, with a similar connective structure. While text data is discrete in nature,
once tokenized and passed through d-dimensional word embeddings, it resembles spatial data
with dimensions Rd×t and can benefit from independence structure.
In all these cases, the maximum clique size is determined by how quickly independence is
achieved spatio-temporally or in the embedding space, rather than by the overall data dimen-
sionality. This approach yields effective dimensions that are orders of magnitude smaller than
the ambient dimension, leading to dimension-independent learning rates.
Crucially,thisdimensionindependenceismaintainedacrossvaryingdatasizes. Forinstance,
croppinganimagewouldleavethemaximumcliquesizeunchanged(providedthecroppingisn’t
too extreme), while expanding an image would create a larger MRF graph but, assuming the
underlyingpatternholds,themaximumcliquesizewouldremainconstant. Thispropertyresults
in a dimension-independent rate of learning that remains consistent across different image sizes.
Thus, whether dealing with a 100×100 pixel image or a 1000×1000 pixel image of similar
content, the effective learning rate remains tied to the maximum clique size rather than the
total number of pixels, exemplifying true dimension independence in the learning process.
These extensions demonstrate the versatility of the MRF approach in modeling complex,
high-dimensional data structures across various modalities, while significantly reducing the ef-
fective dimensionality of the problem.
Hierarchical models Althoughnottheprimaryfocusofthiswork,ourresultshavepotential
applications to other data types not typically associated with deep learning. For instance,
hierarchical data is often modeled as a rooted tree. For tree-structured MRFs, the following is
a well-known:
Lemma 4.7. Let G be a tree with at least two vertices. The size of the largest clique in G is 2.
9Estimating densities with a tree MRF has been studied previously and is called “tree den-
sity estimation.” The largest clique size being 2 and yielding a O(cid:101)(n−1/4) rate of convergence
approximately matches previous work on this problem. In Liu et al. (2011); Györfi et al. (2022)
it was found that one can estimate a density with an unknown tree MRF, without the strong
density assumption at a rate O(n−1/4). Compared to Theorem 4.2, this is an improvement by a
factor of n2, but these estimators are not based on neural networks, which is our focus. In the
next section, we show that the O(n−1/4) is not only optimal for trees, but can be generalized to
arbitrary MRFs.
4.4 Approximately Optimal Estimator
Althoughnottheprimaryfocusofthiswork,wepresentthefollowingresultwhichapproximately
matches (up to log terms) the best possible rate for MRFs.
Theorem 4.8. Let G = (V,E) be a finite graph. There exists an estimator V such that
n
for any Lipschitz continuous density p satisfying the strong density assumption and Markov
property with respect to G, we have
(cid:16) (cid:17)
∥p−V n∥
1
∈O(cid:101)p n−1/(2+r) ,
where V is a function of n i.i.d. samples from p, and r is the size of the largest clique in G.
n
For this estimator, the effective dimension is r. The estimator analyzed in this theorem
is based on Scheffé Tournaments over functions akin to histograms (Scheffe, 1947; Yatracos,
1985). This estimator is not computationally tractable and is presented solely to demonstrate
the theoretical possibility of this optimal rate. An open question remains as to whether this
optimalrateisachievablewithneuralnetworksandatractableloss/algorithm. Thisratecannot
be substantially improved for any MRF graph which we argue in Appendix E.
5 Conclusion
Neuraldensityestimationhasbeenthesubjectofintensestudyoverthepastfewdecades,dating
at least back to Magdon-Ismail & Atiya (1998). There has recently been interest in designing
structured neural density estimators that exploit graphical structure (Germain et al., 2015;
Johnson et al., 2016; Khemakhem et al., 2021; Wehenkel & Louppe, 2021; Chen et al., 2024).
In this work, we have presented a novel perspective on the success of neural networks in density
estimation problems. Our approach, based on Markov Random Field (MRF) structures, offers
an alternative explanation to the widely accepted manifold hypothesis for why deep learning
methods can circumvent the curse of dimensionality, and aligns with these recent developments
on structured density estimation.
WehavedemonstratedthatleveragingMRFassumptionscanachievedimension-independent
convergence rates for density estimation, with the effective dimensionality determined by the
largest clique in the MRF graph rather than the ambient data dimension. This potentially
explains the efficacy of neural networks in domains like image and sequential data processing.
Our MRF-based approach complements, rather than replaces, the manifold hypothesis. We
envisionacombinationoflocalmanifold-likestructuresandglobalMRF-likeindependenceprop-
erties at play in real-world scenarios, with the manifold hypothesis explaining local features and
our MRF approach capturing broader independence structures.
This work opens avenues for future research, including investigating the interplay between
local manifold structures and global MRF properties, and developing practical algorithms ex-
ploiting these structures within deep learning frameworks.
In conclusion, our work provides a novel theoretical framework for understanding neural
networks in high-dimensional spaces, offering an alternative to the manifold hypothesis and
potentially stimulating new directions in both the theory and practice of machine learning for
high-dimensional tasks.
10References
Hassan Ashtiani, Shai Ben-David, Nicholas Harvey, Christopher Liaw, Abbas Mehrabian, and
Yaniv Plan. Nearly tight sample complexity bounds for learning mixtures of gaussians via
sample compression schemes. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp.
3412–3421. Curran Associates, Inc., 2018.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information theory, 39(3):930–945, 1993.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and
new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):
1798–1828, 2013. doi: 10.1109/TPAMI.2013.50.
Clément Berenfeld, Paul Rosa, and Judith Rousseau. Estimating a density near an unknown
manifold: a bayesian nonparametric approach. arXiv preprint arXiv:2205.15717, 2022.
PratikPrabhanjanBrahma,DapengWu,andYiyuanShe.Whydeeplearningworks: Amanifold
disentanglement perspective. IEEE Transactions on Neural Networks and Learning Systems,
27(10):1997–2008, 2016. doi: 10.1109/TNNLS.2015.2496947.
H.Cao,C.Tan,Z.Gao,Y.Xu,G.Chen,P.Heng,andS.Z.Li. Asurveyongenerativediffusion
models. IEEE Transactions on Knowledge and Data Engineering, 36(07):2814–2830, jul2024.
ISSN 1558-2191. doi: 10.1109/TKDE.2024.3361474.
GunnarCarlsson,TigranIshkhanov,VindeSilva,andAfraZomorodian. OntheLocalBehavior
of Spaces of Natural Images. International Journal of Computer Vision, 76(1):1–12, January
2008. ISSN 1573-1405. doi: 10.1007/s11263-007-0056-x. URL https://doi.org/10.1007/
s11263-007-0056-x.
Joseph Chang. Stochastic processes, 2007. URL http://www.stat.yale.edu/~pollard/
Courses/251.spring2013/Handouts/Chang-notes.pdf. Accessed: Oct 1, 2024.
Asic Chen, Ruian Ian Shi, Xiang Gao, Ricardo Baptista, and Rahul G Krishnan. Structured
neural networks for density estimation and causal inference. Advances in Neural Information
Processing Systems, 36, 2024.
Frank Cole and Yulong Lu. Score-based generative models break the curse of dimensionality in
learning a family of sub-gaussian probability distributions. arXiv preprint arXiv:2402.08082,
2024.
L.DevroyeandL.Gyorfi. Nonparametric Density Estimation: The L1 View. WileyInterscience
Series in Discrete Mathematics. Wiley, 1985. ISBN 9780471816461.
L. Devroye and G. Lugosi. Combinatorial Methods in Density Estimation. Springer, New York,
2001.
Gerald B. Folland. Real analysis: modern techniques and their applications. Pure and applied
mathematics. Wiley, 1999. ISBN 9780471317166.
MathieuGermain,KarolGregor,IainMurray,andHugoLarochelle. Made: Maskedautoencoder
for distribution estimation. In International Conference on Machine Learning, pp. 881–889.
PMLR, 2015.
László Györfi, Aryeh Kontorovich, and Roi Weiss. Tree density estimation. IEEE Transactions
on Information Theory, 69(2):1168–1176, 2022.
JohnMHammersleyandPeterClifford. Markovfieldsonfinitegraphsandlattices. Unpublished
manuscript, 46, 1971.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.
In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 6840–6851. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/
file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.
Heinrich Jiang. Uniform convergence rates for kernel density estimation. In International Con-
11ference on Machine Learning, pp. 1694–1703, 2017.
Yuling Jiao, Guohao Shen, Yuanyuan Lin, and Jian Huang. Deep nonparametric regression on
approximatemanifolds: Nonasymptoticerrorboundswithpolynomialprefactors. The Annals
of Statistics, 51(2):691–716, 2023.
Matthew J Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R
Datta. Composing graphical models with neural networks for structured representations and
fast inference. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 29, pp. 2946–2954. Curran Associates,
Inc., 2016.
RobertW.Keener. Bayesian Inference: Modeling and Computation,pp.301–318. SpringerNew
York, New York, NY, 2010. ISBN 978-0-387-93839-4. doi: 10.1007/978-0-387-93839-4_15.
URL https://doi.org/10.1007/978-0-387-93839-4_15.
Ilyes Khemakhem, Ricardo Monti, Robert Leech, and Aapo Hyvarinen. Causal autoregressive
flows. In International Conference on Artificial Intelligence and statistics, pp. 3520–3528.
PMLR, 2021.
JasonM.KlusowskiandAndrewR.Barron. Approximationbycombinationsofreluandsquared
relu ridge functions with ℓ1 and ℓ0 controls. IEEE Transactions on Information Theory, 64
(12):7649–7656, 2018. doi: 10.1109/TIT.2018.2874447.
Hyeok Kyu Kwon and Minwoo Chae. Minimax optimal density estimation using a shallow
generative model with a one-dimensional latent variable. In International Conference on
Artificial Intelligence and Statistics, pp. 469–477. PMLR, 2024.
S. Z. Li. Markov random field models in computer vision. In Jan-Olof Eklundh (ed.), Computer
Vision—ECCV’94,pp.361–370,Berlin,Heidelberg,1994.SpringerBerlinHeidelberg. ISBN
978-3-540-48400-4.
Tengyuan Liang. How well can generative adversarial networks (GAN) learn densities: A non-
parametric view. arXiv preprint arXiv:1712.08244, 2017.
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays,
Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft COCO:
Common Objects in Context. arXiv e-prints, art. arXiv:1405.0312, May 2014. doi: 10.48550/
arXiv.1405.0312.
Han Liu, Min Xu, Haijie Gu, Anupam Gupta, John Lafferty, and Larry Wasserman. Forest
density estimation. The Journal of Machine Learning Research, 12:907–951, 2011.
Chao Ma, Lei Wu, et al. The barron space and the flow-induced function spaces for neural
network models. Constructive Approximation, 55(1):369–406, 2022.
Malik Magdon-Ismail and Amir Atiya. Neural networks for density estimation. Advances in
Neural Information Processing Systems, 11, 1998.
Ryumei Nakada and Masaaki Imaizumi. Adaptive approximation and generalization of deep
neuralnetworkwithintrinsicdimensionality. Journal of Machine Learning Research, 21(174):
1–38, 2020.
Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal dis-
tribution estimators. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara En-
gelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International
Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,
pp. 26517–26582. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.press/v202/
oko23a.html.
AchrafOussidiandAzeddineElhassouny. Deepgenerativemodels: Survey. In2018International
Conference on Intelligent Systems and Computer Vision (ISCV),pp.1–8,2018. doi: 10.1109/
ISACV.2018.8354080.
ArkadasOzakinandAlexanderGray. Submanifolddensityestimation. InY.Bengio,D.Schuur-
mans,J.Lafferty,C.Williams,andA.Culotta(eds.),AdvancesinNeuralInformationProcess-
ingSystems,volume22.CurranAssociates,Inc.,2009.URLhttps://proceedings.neurips.
12cc/paper_files/paper/2009/file/2ac2406e835bd49c70469acae337d292-Paper.pdf.
Bruno Pelletier. Kernel density estimation on riemannian manifolds. Statistics & probability
letters, 73(3):297–304, 2005.
Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic
dimension of images and its impact on learning. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=XJk19XzGq2J.
Henry Scheffe. A Useful Convergence Theorem for Probability Distributions. The Annals of
Mathematical Statistics, 18(3):434 – 438, 1947. doi: 10.1214/aoms/1177730390. URL https:
//doi.org/10.1214/aoms/1177730390.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu acti-
vation function. arXiv preprint arXiv:1708.06633, 2017.
JohannesSchmidt-Hieber. DeepReLUnetworkapproximationoffunctionsonamanifold. arXiv
preprint arXiv:1908.00695, 2019.
ShashankSingh,AnanyaUppal,BoyueLi,Chun-LiangLi,ManzilZaheer,andBarnabásPóczos.
Nonparametric density estimation under adversarial losses. Advances in Neural Information
Processing Systems, 31, 2018.
RongTangandYunYang.Adaptivityofdiffusionmodelstomanifoldstructures.InInternational
Conference on Artificial Intelligence and Statistics, pp. 1648–1656. PMLR, 2024.
A.B. Tsybakov. Introduction to nonparametric estimation. Springer Series in Statistics, New
York, pp. 214, 2009. cited By 1.
Ananya Uppal, Shashank Singh, and Barnabás Póczos. Nonparametric density estimation &
convergence rates for GANs under Besov IPM losses. In Hanna M. Wallach, Hugo Larochelle,
Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Ad-
vances in Neural Information Processing Systems 32: Annual Conference on Neural In-
formation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada, pp. 9086–9097, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
fb09f481d40c4d3c0861a46bd2dc52c0-Abstract.html.
Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence
of empirical measures in Wasserstein distance. Bernoulli, 25(4A):2620 – 2648, 2019. doi:
10.3150/18-BEJ1065. URL https://doi.org/10.3150/18-BEJ1065.
AntoineWehenkelandGillesLouppe. Graphicalnormalizingflows. InInternational Conference
on Artificial Intelligence and Statistics, pp. 37–45. PMLR, 2021.
Yannis G. Yatracos. Rates of Convergence of Minimum Distance Estimators and Kolmogorov’s
Entropy. The Annals of Statistics, 13(2):768 – 774, 1985. doi: 10.1214/aos/1176349553. URL
https://doi.org/10.1214/aos/1176349553.
KaihongZhang,HeqiYin,FengLiang,andJingboLiu. Minimaxoptimalityofscore-baseddiffu-
sion models: Beyond the density lower bound assumptions. arXiv preprint arXiv:2402.15602,
2024.
A Notations and Preliminaries
Before proving the main theorem we will first establish some notation and auxiliary results.
For a pair of functions f,g : X → R where X is an arbitrary domain, we define the f ·g to
be pointwise function multiplication so (f · g)(x) = f(x)g(x) for all x ∈ X. For a tuple of
functions f ,...,f : X → R, the product symbol (cid:81)m f is defined to be pointwise function
1 m i=1 i
multiplication, i.e., f (x)·f (x)·····f (x) for all x∈X. Let N be the set of positive integers.
1 2 m
For any d∈N, let [d]={1,2,...,d}.
For a set V ⊂ [d] with V = {v ,...,v } where v < v for all i < j, let e : Rd →
1 |V| i j d,V
R|V|;x (cid:55)→ [x ,...,x ], i.e., e accepts a d-dimensional vector and outputs the indices at
v1 v|V| d,V
V, in order. The function e can be thought of as selecting some indices from a vector. As a
V,d
slight abuse of notation, the d subscript will be omitted.
13For a graph G =(V,E), the set of maximal cliques in G will be denoted C(G), and is a set of
subsets of V.
All of our results will assume the domain of the data is the unit cube [0,1]d. A
density p will be called positive if p(x) > 0 for all x ∈ [0,1]d. Since [0,1]d is compact, a direct
consequence of this is that there exists c>0 such that p(x)>c for all x.
A.1 Preliminary Results
Proposition A.1. LetpbeaLipschitzcontinuousprobabilitydensity[0,1]d,whichiseverywhere
positive on [0,1]d and satisfies the Markov property with respect to a graph G = (V,E). Then,
for all x∈[0,1]d,
(cid:89)
p(x)= ψ (e (x)),
V′ V′
V′∈C(G)
where each ψ is Lipschitz continuous, and there exist constants c,C such that 0<c≤C and
V′
c≤ψ ≤C for all V′ ∈C(G).
V′
Before proving this proposition we first prove the following support lemma.
Lemma A.2. Let f,g : [0,1]d → R be Lipschitz continuous with f ≥ δ and g ≥ δ for some
δ >0. Thenf·g and1/f arebothLipschitzcontinuousandthereexistsδ′ >0suchthatf·g ≥δ′
and 1/f ≥δ′.
Proof of Lemma A.2. Let f be L -Lipschitz and g be L -Lipschitz. Because f and g are Lip-
f g
schitz on a bounded set there exists C > 0 and C > 0 such that f ≤ C and g ≤ C . Let
f g f g
x,y ∈[0,1]d be arbitrary.
We will begin by proving the product portion of the lemma:
|f(x)g(x)−f(y)g(y)|≤|f(x)g(x)−f(x)g(y)|+|f(x)g(y)−f(y)g(y)|
≤C |g(x)−g(y)|+C |f(x)−f(y)|
f g
≤C L ∥x−y∥ +C L ∥x−y∥
f g 2 g f 2
≤2max(C L ,C L )∥x−y∥ .
f g g f 2
The existence of a positive lower bound for f ·g follows immediately from f ·g ≥δ2.
Toprovethereciprocalportionofthelemma, observethatthefunctionx(cid:55)→1/xisLipschitz
on the range of f. Since the composition of Lipschitz functions is itself Lipschitz, it follows that
1/f is Lipschitz. Finally we have that 1/f ≥1/C >0, finishing the proof.
f
Proof of Proposition A.1. ThisproofutilizesresultsfromChang(2007),awork-in-progressbook
currentlyusedprimarilyaslecturenotes. ThisworkhasaconstructiveproofoftheHammersley-
Clifford Theorem. For S ⊂[d], let γ :Rd →Rd;x(cid:55)→[x 1(i∈S)] , i.e., indices outside of S are
S i i
set to zero (the zero vector could actually be set to any arbitrary but fixed vector, e.g., set a
vector y ∈ Rd and [γ ] = y ). From the Chang (2007) (3.15), the proof of the Hammersley-
S i̸∈S i
Clifford Theorem, it is shown that:
(cid:89)
p(x)= ψ (x),
V′
V′∈C(G)
where,
(cid:81)
p(γ (x))
V′′⊂V′:|V′′\V′| mod2=0 V′′
ψ (x)= . (2)
V′ (cid:81)
p(γ (x))
V′′⊂V′:|V′′\V′| mod2=1 V′′
For clarity we do an example of the index set of the product; so
V′′ ⊂V′ :|V′′\V′| mod 2=0
denotes a product over all subsets of V′ where the set V′′ where V′′ ∩V′C contains an even
numberofelements. Fromequation2itisclearthatψ onlydependsontheindicesofxinV′.
V′
The regularity conditions hold due to Lemma A.2.
14Given any set S. For any subset S′ ⊆ S, let 1 be the indicator function from S to {0,1},
S′
i.e.
(cid:40)
0 if x∈/ S′
1 (x)= for all x∈S.
S′ 1 if x∈S′
Given any d,b ∈ N, V = {v ,...,v } ⊂ [d] and C ≥ 1. For any A ∈ [b]|V|, let Λ be
1 |V| d,b,A,V
the subset of [0,1]d
(cid:26) (cid:20) (cid:21) (cid:27)
A −1 A
Λ := x∈[0,1]d |x ∈ i , i for all i∈[|V|] . (3)
d,b,A,V vi b b
Let Q be the set of functions from [0,1]d →R
d,b,V,C
(cid:26) (cid:27)
(cid:88)
Q := x(cid:55)→ w 1 (x)|w ∈[0,C] . (4)
d,b,V,C A Λd,b,A,V A
A∈[b]|V|
For a set L ⊂
Lβ(cid:0) [0,1]d(cid:1)
where 1 ≤ β < ∞ and ϵ > 0, a subset C ⊆ L is called an ϵ-cover
of L in Lβ norm if, for any f ∈L, there exists a g ∈C such that ∥f −g∥ ≤ϵ. Also, we define
β
N(L,ϵ) to be the cardinality of the smallest subset of L that is a (closed) ϵ-cover of L in Lβ
norm. Note that N(L,ϵ) depends on β. We will not specify it when it is clear in the context.
B Proof of Theorem 4.2
Theorem B.1. Let G = (V,E) be a finite graph and r is the size of the largest clique in G.
There exists a neural network architecture F∗, such that, for
n
pˆ =arg min ∥f∥2− 2 (cid:88) f(X )
n f∈F∗ 2 n i
i=1
i.i.d.
where X ,...,X ∼ p, then
1 n
(cid:16) (cid:17)
∥p−pˆ n∥
2
∈O(cid:101)p n−1/(4+r) ,
for any Lipschitz continuous, positive density p satisfying the Markov property with G.
This is stronger than L1 convergence since, through Hölder’s inequality, we get L1 convergence
at the same rate.
Lemma B.2. Let (Ω,Σ,µ) be a measure space, and let f ,...,f and g ,...,g be measurable
1 m 1 m
and absolutely integrable functions on Ω. Further suppose there exists a constant C ≥ 0 such
that, for all i∈[m],
∥f ∥ ≤C and ∥g ∥ ≤C.
i ∞ i ∞
Then the following inequality holds:
(cid:13) (cid:13)
(cid:13)(cid:89)m (cid:89)m (cid:13) (cid:88)m
(cid:13) f − g (cid:13) ≤Cm−1 ∥f −g ∥ .
(cid:13) i i(cid:13) i i ∞
(cid:13) (cid:13)
i=1 i=1 ∞ i=1
Proof of Lemma B.2. We will proceed by induction on m.
Case m=1: Trivial.
Induction: Suppose the lemma holds for some value of m. From the inductive hypothesis we
15have that
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)m (cid:89)+1 m (cid:89)+1 (cid:13) (cid:13)(cid:89)m (cid:89)m (cid:13) (cid:13)(cid:89)m (cid:89)m (cid:13)
(cid:13) f − g (cid:13) ≤(cid:13) f ·f − g ·f (cid:13) +(cid:13) g ·f − g ·g (cid:13)
(cid:13) i i(cid:13) (cid:13) i m+1 i m+1(cid:13) (cid:13) i m+1 i m+1(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
i=1 i=1 ∞ i=1 i=1 ∞ i=1 i=1 ∞
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)(cid:89)m (cid:89)m (cid:13) (cid:13)(cid:89)m (cid:13)
≤(cid:13) f − g (cid:13) ∥f ∥ +(cid:13) g (cid:13) ∥f −g ∥
(cid:13) i i(cid:13) m+1 ∞ (cid:13) i(cid:13) m+1 m+1 ∞
(cid:13) (cid:13) (cid:13) (cid:13)
i=1 i=1 ∞ i=1 ∞
(cid:13) (cid:13)
(cid:13)(cid:89)m (cid:89)m (cid:13)
≤(cid:13) f − g (cid:13) C+Cm∥f −g ∥
(cid:13) i i(cid:13) m+1 m+1 ∞
(cid:13) (cid:13)
i=1 i=1 ∞
m
(cid:88)
≤Cm−1 ∥f −g ∥ C+Cm∥f −g ∥
i i ∞ m+1 m+1 ∞
i=1
m+1
(cid:88)
≤Cm ∥f −g ∥ .
i i ∞
i=1
Space of Neural Network Architectures Define a space of neural networks as follows.
Let σ be the ReLU activation function with will act element-wise on vectors. For any ℓ ∈ N,
w = (w ,...,w ) with w ∈ N, s ∈ N and F > 0, the space F(ℓ,w,s,F) is defined by the
0 ℓ+1 i
functions f :[0,1]w0 →Rwℓ+1 which have the form:
f(x)=W σ W σ ···W σ W x,
ℓ vℓ ℓ−1 vℓ−1 1 v1 0
where σ vi(y) = σ(y−v i), W
i
∈ Rwi+1×wi, where every entry in W
i
and v
i
have absolute value
less than or equal to 1, ∥f∥ ≤ F, and sum of the total number of nonzero entries of W and
∞ i
v is less than or equal to s. In this work the output dimension of all neural networks will be
i
1, i.e. w will always be assumed to be 1. This is the same space of neural network models
ℓ+1
employed by Schmidt-Hieber (2017).
TheoremB.3 (Theorem5,Schmidt-Hieber,2017). Foranyf ∈Cβ([0,1]d,K)andanyintegers
d
m ≥ 1 and N ≥ max((β+1)d,(K +1)ed), there exists a ReLU network f˜∈ F(ℓ,w,s,∞) with
depth
ℓ=8+(m+5)(1+⌈log (max(d,β))⌉), (5)
2
widths
w =(d,6(d+⌈β⌉)N,...,6(d+⌈β⌉)N,1), (6)
and sparsity
s≤141(d+β+1)d+3N(m+6) (7)
such that
(cid:13) (cid:13)
(cid:13)f˜−f(cid:13) ≤(2K+1)(1+d2+β2)6dN2−m+K3βN−β/d. (8)
(cid:13) (cid:13)
L∞([0,1]d)
Lemma B.4 (Lemma 5, Remark 1, Schmidt-Hieber, 2017). For any δ >0,
logN(F(ℓ,w,s,∞),ϵ,∥·∥ )≤(s+1)log(22ℓ+5ϵ−1(ℓ+1)w2w2 s2ℓ).
∞ 0 ℓ+1
B.1 Squared-L2ersion
Proof of Theorem B.1. Recallthat,givenagraphG,C(G)isthesetofmaximalcliquesinG. For
any V′ ∈ C(G), let F = F(ℓ ,w ,s,C) where ℓ ,w ,s,C will be determined later. Also,
V′ V′ V′ V′ V′
let
(cid:26) (cid:89) (cid:12) (cid:27)
F∗ = q ◦e (cid:12)q ∈F . (9)
V′ V′(cid:12) V′ V′
V′∈C(G)
16We shall show that F∗ is the neural network architecture satisfying the desired guarantees in
Theorem B.1.
For any set of n i.i.d. samples X ,...,X drawn from p, let
1 n
(cid:18) n (cid:19)
p∗ =arg min ∥p−f∥2 and pˆ =arg min ∥f∥2− 2 (cid:88) f(X ) . (10)
n f∈F∗ 2 n f∈F∗ 2 n i
i=1
Now, we would like to bound the term ∥pˆ −p∥2. We first express it as
n 2
∥pˆ −p∥2 =(cid:0) ∥pˆ −p∥2−∥p∗ −p∥2(cid:1) +∥p∗ −p∥2.
n 2 n 2 n 2 n 2
For the term ∥pˆ −p∥2−∥p∗ −p∥2, we further express it as
n 2 n 2
(cid:18) n (cid:19)
∥pˆ −p∥2−∥p∗ −p∥2 =∥pˆ −p∥2− ∥p∥2+∥pˆ ∥2− 2 (cid:88) pˆ (X )
n 2 n 2 n 2 2 n 2 n n i
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
:=A
(cid:18) n (cid:19)
+ ∥p∥2+∥pˆ ∥2− 2 (cid:88) pˆ (X ) −∥p∗ −p∥2 (11)
2 n 2 n n i n 2
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
:=B
Before we bound A and B, we first provide a useful inequality. For any p′ ∈F∗, we have
(cid:18) n (cid:19)
∥p′−p∥2− ∥p∥2+∥p′∥2− 2 (cid:88) p′(X )
2 2 2 n i
i=1
n
2 (cid:88)
= p′(X )−2⟨p′,p⟩
n i
i=1
(cid:12) n (cid:12)
≤2 fm ∈a Fx ∗(cid:12) (cid:12) (cid:12)E p(f)− n1 (cid:88) f(X i)(cid:12) (cid:12)
(cid:12)
since ⟨p′,p⟩=E p(p′) and p′ ∈F∗. (12)
i=1
For the term A, we immediately have
(cid:18) n (cid:19)
A=∥pˆ −p∥2− ∥p∥2+∥pˆ ∥2− 2 (cid:88) pˆ (X )
n 2 2 n 2 n n i
i=1
(cid:12) n (cid:12)
≤2 fm ∈a Fx ∗(cid:12) (cid:12) (cid:12)E p(f)− n1 (cid:88) f(X i)(cid:12) (cid:12)
(cid:12)
since pˆ n ∈F∗.
i=1
For the term B, we have
(cid:18) n (cid:19)
B = ∥p∥2+∥pˆ ∥2− 2 (cid:88) pˆ (X ) −∥p∗ −p∥2
2 n 2 n n i n 2
i=1
(cid:18) n (cid:19)
≤ ∥p∥2+∥p∗∥2− 2 (cid:88) p∗(X ) −∥p∗ −p∥2 by the optimality of pˆ
2 n 2 n n i n 2 n
i=1
(cid:12) n (cid:12)
≤2 fm ∈a Fx ∗(cid:12) (cid:12) (cid:12)E p(f)− n1 (cid:88) f(X i)(cid:12) (cid:12)
(cid:12)
since pˆ n ∈F∗.
i=1
By plugging them into equation 11, we have
(cid:12) n (cid:12)
∥pˆ n−p∥2
2
≤4 fm ∈a Fx ∗(cid:12) (cid:12) (cid:12)E p(f)− n1 (cid:88) f(X i)(cid:12) (cid:12) (cid:12)+∥p∗ n−p∥2 2. (13)
i=1
We first analyze the term ∥p∗ −p∥2 in equation 13. From Proposition A.1, we have that
n 2
(cid:89)
p= ψ ◦e
V′ V′
V′∈C(G)
17and there exists some C > 0 so that ψ ≤ C for all V′ and that, for some L , all ψ are
ψ V′ ψ ψ V′
L -Lipschitz continuous. We pick a sufficiently large C that is greater than C . Also, by the
ψ ψ
definition of F∗ in equation 9, we can pick a q ∈F for each V′ ∈C(G) and form an f ∈F∗
V′ V′
such that
(cid:89)
f = q ◦e .
V′ V′
V′∈C(G)
We will specify each q later. Then, we have
V′
(cid:13) (cid:13)
(cid:13) (cid:89) (cid:89) (cid:13)
∥f −p∥ ∞ =(cid:13) (cid:13) q V′ ◦e V′ − ψ V′ ◦e V′(cid:13) (cid:13)
V′∈C(G) V′∈C(G) ∞
(cid:13) (cid:13)
≤C|C(G)|−1 (cid:88) (cid:13) (cid:13)q V′ ◦e V′ −ψ V′ ◦e V′(cid:13) (cid:13) by Lemma B.2 (14)
(cid:13) (cid:13)
V′∈C(G) ∞
Recall that F = F(ℓ ,w ,s,C). For any sufficiently large m,N ∈ N which we will
V′ V′ V′
determine later, we pick
ℓ =8+(m+5)(1+⌈log |V′|⌉),
V′ 2
w =(|V′|,6(|V′|+1)N,6(|V′|+1)N,...,6(|V′|+1)N,1),
V′
s=⌊141(r+2)r+3N(m+6)⌋
and recall that we have picked C to be a constant larger than C before. It is easy to check
ψ
that the hypotheses of Theorem B.3 are satisfied with K = L , β = 1 and d = |V′| and hence,
ψ
by Theorem B.3, if we pick
(cid:13) (cid:13)
q V′ =arg
q
V′m ′∈i Fn V′(cid:13) (cid:13) (cid:13)q V′ ′ ◦e V′ −ψ V′ ◦e V′(cid:13) (cid:13)
(cid:13)
∞
then we have
(cid:13) (cid:13)
(cid:13)
(cid:13)q V′ ◦e V′ −ψ V′ ◦e
V′(cid:13)
(cid:13) ≤(2L
ψ+1)(1+|V′|2+1)6|V′|N2−m+L ψ3N−1/|V′|
(cid:13) (cid:13)
∞
=O(N2−m+N−1/r) (15)
By plugging equation 15 into equation 14, we have
(cid:88)
∥f −p∥ ≤C|C(G)|−1 O(N2−m+N−1/r)=O(N2−m+N−1/r)
∞
V′∈C(G)
Recall that the domain is [0,1]d and hence we have
(cid:90)
∥f −p∥2 = |f(x)−p(x)|2dx≤∥f −p∥2 .
2 ∞
[0,1]d
Now, by the optimality of p∗ in equation 10, we have
n
∥p∗ −p∥2 ≤∥f −p∥2 ≤∥f −p∥2 =O(N22−2m+N−2/r). (16)
n 2 2 ∞
(cid:12) (cid:12)
Now, we take care of the term max f∈F∗(cid:12) (cid:12) (cid:12)E p(f)− n1 (cid:80)n i=1f(X i)(cid:12) (cid:12)
(cid:12)
in equation 13. To bound
thistermforallf ∈F∗,wefirstconstructanϵ-coverofF∗ inL∞. Then,weusetheHoeffding’s
inequality to bound this term for each f in the ϵ-cover and use the union bound to control the
total failure probability. To construct an ϵ-cover, we define the following notations. For any
V′ ∈C(G),letF(cid:101)V′ beaminimal C|C(Gϵ )|−1-coverofF
V′
inL∞ whereϵisasufficientlysmallvalue
and we will determine it later. Also, let
 
 (cid:89) 
F(cid:101)∗ = q˜
V′
◦e
V′
|q˜
V′
∈F(cid:101)V′ . (17)
 
V′∈C(G)
18We will show that F(cid:101)∗ is an ϵ-cover of F in L∞. For any f ∈F, it can be expressed as
(cid:89)
f = q ◦e for some q ∈Q
V′ V′ V′ V′
V′∈C(G)
Since F(cid:101)V′ is an C|C(Gϵ )|−1-cover of F
V′
in L∞ for all V′ ∈ C(G), there exists a q˜
V′
∈ F(cid:101)V′ such
that
ϵ
∥q −q˜ ∥ ≤ .
V′ V′ ∞ C|C(G)|−1
By the definition of F(cid:101), we set f˜∈F(cid:101) to be
f˜= (cid:89) q˜ ◦e
V′ V′
V′∈C(G)
By Lemma B.2, we check that
(cid:13) (cid:13) (cid:13)f −f˜(cid:13) (cid:13) (cid:13) ∞ =(cid:13) (cid:13) (cid:13) (cid:13) V′(cid:89) ∈C(G)q V′ − V′(cid:89) ∈C(G)q˜ V′(cid:13) (cid:13) (cid:13) (cid:13) ∞ =C|C(G)|−1· V′(cid:88) ∈C(G)∥q V′ −q˜ V′∥ ∞
ϵ
≤C|C(G)|−1·
C|C(G)|−1
=ϵ.
(cid:12) (cid:12)
Now, we return to the term max f∈F(cid:12) (cid:12) (cid:12)E p(f)− n1 (cid:80)n i=1f(X i)(cid:12) (cid:12) (cid:12). Since F(cid:101)∗ is an ϵ-cover of F∗
in L∞, for any f ∈F∗, there exists a f˜∈F(cid:101)∗ such that ∥f −f˜∥
∞
≤ϵ and we have
(cid:12) n (cid:12)
(cid:12) (cid:12) (cid:12)E p(f)− n1 (cid:88) f(X i)(cid:12) (cid:12)
(cid:12)
i=1
(cid:12) (cid:12) (cid:12) n (cid:12) (cid:12) n n (cid:12)
≤(cid:12) (cid:12) (cid:12)E p(f)−E p(f˜)(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12) (cid:12)E p(f˜)− n1 (cid:88) f˜(X i)(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12) (cid:12)n1 (cid:88) f˜(X i)− n1 (cid:88) f(X i)(cid:12) (cid:12)
(cid:12)
i=1 i=1 i=1
(cid:12) n (cid:12)
≤2ϵ+(cid:12)
(cid:12)
(cid:12)E p(f˜)− n1 (cid:88) f˜(X i)(cid:12)
(cid:12)
(cid:12)
i=1
which implies
(cid:12) n (cid:12) (cid:12) n (cid:12)
fm ∈a Fx ∗(cid:12) (cid:12) (cid:12)E p(f)− n1 (cid:88) i=1f(X i)(cid:12) (cid:12) (cid:12)≤2ϵ+ fm ˜∈a F(cid:101)x ∗(cid:12) (cid:12) (cid:12)E p(f˜)− n1 (cid:88) i=1f˜(X i)(cid:12) (cid:12) (cid:12). (18)
By Hoeffding’s inequality and the union bound, for any t>0, the probability of
(cid:12) n (cid:12)
fm ˜∈a F(cid:101)x ∗(cid:12) (cid:12) (cid:12)E p(f˜)− n1 (cid:88) i=1f˜(X i)(cid:12) (cid:12) (cid:12)>t
is bounded by |F(cid:101)∗|·e−Ω(nt2).
To bound the term |F(cid:101)∗|, by the definition of F(cid:101)∗ in equation 17, we first have
(cid:88)
log|F(cid:101)∗|= log|F(cid:101)V′|.
V′∈C(G)
For each term log|F(cid:101)V′|, by Lemma B.4, we have
log|F(cid:101)V′|≤(s+1)log(22L V′+5ϵ−1(L
V′
+1)|V′|2s2L V′).
We now bound the architecture parameters. Recall that
ℓ =8+(m+5)(1+⌈log |V′|⌉) for any V′ ∈C(G) and
V′ 2
s=⌊141(r+2)r+3N(m+6)⌋.
19Namely, we have
Nm
ℓ
V′
=O(m) and s=O(Nm) which implies log|F(cid:101)V′|≤O(Nm2log
ϵ
).
That means we have
(cid:88) Nm Nm
log|F(cid:101)∗|≤ O(Nm2log )=O(Nm2log ).
ϵ ϵ
V′∈C(G)
(cid:113)
By setting t=O( Nm2 logNnm), we have
n ϵ
fm ˜∈a F(cid:101)x ∗(cid:12) (cid:12) (cid:12) (cid:12)E p(f˜)− n1 (cid:88) i=n 1f˜(X i)(cid:12) (cid:12) (cid:12) (cid:12)<O((cid:114) N nm2 logNn ϵm )
withatleastprobability1−|F(cid:101)∗|·e−Ω(nt2) →1asn→∞. Pluggingitintoequation18,wehave
fm ∈a Fx ∗(cid:12) (cid:12) (cid:12) (cid:12)E p(f)− n1 (cid:88)n f(X i)(cid:12) (cid:12) (cid:12) (cid:12)≤2ϵ+O((cid:114) N nm2 logNn ϵm ). (19)
i=1
Furthermore, by plugging equation 16 and equation 19 into equation 13, we have
(cid:12) n (cid:12)
∥pˆ n−p∥2
2
≤4m f∈a Qx(cid:12) (cid:12) (cid:12)E p(f)− n1 (cid:88) f(X i)(cid:12) (cid:12) (cid:12)+∥p∗ n−p∥2
2
i=1
(cid:114)
Nm2 Nnm
<O(ϵ+ log +N22−2m+N−2/r).
n ϵ
By picking
r+1
ϵ=n− r+2 4, N =nr+r 4 and m= logn,
r+4
we have
∥pˆ n−p∥2
2
≤O(cid:101)(n− r+2 4).
C Proof of Theorem 4.8
Theorem C.1. Let G =(V,E) be a finite graph. There exists an estimator V such that for
n
any positive Lipschitz continuous density p satisfying the Markov property with respect to a
graph G, we have that
(cid:16) (cid:17)
∥p−V n∥
1
∈O(cid:101)p n−1/(2+r) ,
where V is a function of n iid samples from p, and r is the size of the largest clique in G.
n
Lemma C.2. Let (Ω,Σ,µ) be a measure space, and let f ,...,f and g ,...,g be measurable
1 m 1 m
and absolutely integrable functions on Ω. Further suppose there exists a constant C ≥ 0 such
that, for all i∈[m],
∥f ∥ ≤C and ∥g ∥ ≤C.
i ∞ i ∞
Then the following inequality holds:
(cid:13) (cid:13)
(cid:13)(cid:89)m (cid:89)m (cid:13) (cid:88)m
(cid:13) f − g (cid:13) ≤Cm−1 ∥f −g ∥ .
(cid:13) i i(cid:13) i i 1
(cid:13) (cid:13)
i=1 i=1 1 i=1
The proof of this lemma will rely heavily on the 1-∞ form of Hölder’s Inequality. The
following is such a version of Hölder’s Inequality, as stated in (Folland, 1999, Theorem 6.8a).
20Theorem C.3 (Hölder’s Inequality). If f and g are measurable functions on a measure space
(Ω,Σ,µ), then
∥f ·g∥ ≤∥f∥ ∥g∥ .
1 1 ∞
Proof of Lemma C.2. We will proceed by induction on m.
Case m=1: Trivial.
Induction: Supposethelemmaholdsforsomevalueofm. AconsequenceofHölder’sInequality
is that for general functions f and g, with ∥f∥ ,∥f∥ ,∥g∥ ,∥g∥ finite, both ∥f ·g∥ and
1 ∞ 1 ∞ 1
∥f ·g∥ are also finite. From the inductive hypothesis we have that
∞
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)m (cid:89)+1 m (cid:89)+1 (cid:13) (cid:13)(cid:89)m (cid:89)m (cid:13) (cid:13)(cid:89)m (cid:89)m (cid:13)
(cid:13) f − g (cid:13) ≤(cid:13) f ·f − g ·f (cid:13) +(cid:13) g ·f − g ·g (cid:13)
(cid:13) i i(cid:13) (cid:13) i m+1 i m+1(cid:13) (cid:13) i m+1 i m+1(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
i=1 i=1 1 i=1 i=1 1 i=1 i=1 1
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)(cid:89)m (cid:89)m (cid:13) (cid:13)(cid:89)m (cid:13)
≤(cid:13) f − g (cid:13) ∥f ∥ +(cid:13) g (cid:13) ∥f −g ∥
(cid:13) i i(cid:13) m+1 ∞ (cid:13) i(cid:13) m+1 m+1 1
(cid:13) (cid:13) (cid:13) (cid:13)
i=1 i=1 1 i=1 ∞
(cid:13) (cid:13)
(cid:13)(cid:89)m (cid:89)m (cid:13)
≤(cid:13) f − g (cid:13) C+Cm∥f −g ∥
(cid:13) i i(cid:13) m+1 m+1 1
(cid:13) (cid:13)
i=1 i=1 1
m
(cid:88)
≤Cm−1 ∥f −g ∥ C+Cm∥f −g ∥
i i 1 m+1 m+1 1
i=1
m+1
(cid:88)
≤Cm ∥f −g ∥ .
i i 1
i=1
√
Lemma C.4. Let p be an L-Lipschitz probability density on [0,1]d then ∥p∥ ≤1+L d.
∞
Proof of Lemma C.4. Since p is L-Lipschitz, we have
|p(x)−p(y)|≤L·∥x−y∥ for any x,y ∈[0,1]d
2
√
≤L d.
Also, since p is a probability density, we have
(cid:90)
1=∥p∥ = p(x)dx≥ min p(x).
1
x∈[0,1]d x∈[0,1]d
Combining these two inequalities, we have
√ √
∥p∥ = max p(x)≤ min p(x)+L d≤1+L d.
∞
x∈[0,1]d x∈[0,1]d
Lemma C.5. Let 1≥ϵ>0 and C ≥1. Then,
N(Q ,ϵ)≤(2C/ϵ)(b|V|) .
d,b,V,C
Proof of Lemma C.5. Given an ϵ∈(0,1], consider the set
(cid:26) (cid:27)
(cid:88)
Q (cid:101)d,b,V,C,ϵ := x(cid:55)→ w A1 Λd,b,A,V(x)|w A ∈{0,ϵ,2ϵ,...,⌊C/ϵ⌋ϵ} .
A∈[b]|V|
Clearly, Q (cid:101)d,b,V,C,ϵ ⊂ Q d,b,V,C. We have that |{0,ϵ,2ϵ,...,⌊C/ϵ⌋ϵ}| ≤ 1+C/ϵ ≤ 2C/ϵ. Thus,
from a simple combinatorics argument, it follows that
(cid:12) (cid:12)
(cid:12) (cid:12)Q (cid:101)d,b,V,C,ϵ(cid:12) (cid:12)≤(2C/ϵ)(b|V|) .
21Now, we argue that Q (cid:101)d,b,V,C,ϵ is an ϵ-cover of Q d,b,V,C in L1 distance. Let q ∈ Q d,b,V,C. From
the definition of Q , it follows that
d,b,V,C
(cid:88)
q(x)= w 1 (x) for all x∈[0,1]d.
A Λd,b,A,V
A∈[b]|V|
From the definition of Q (cid:101)d,b,V,C,ϵ, there exists a q˜∈Q (cid:101)d,b,V,C,ϵ, with
(cid:88)
q˜(x)= w˜ 1 (x) for all x∈[0,1]d,
A Λd,b,A,V
A∈[b]|V|
where |w −w˜ |≤ϵ for all A. It therefore follows that
A A
(cid:12) (cid:12)
(cid:90) (cid:12) (cid:12)
(cid:12) (cid:88) (cid:88) (cid:12)
∥q−q˜∥
1
= (cid:12)
(cid:12)
w A1 Λd,b,A,V(x)− w˜ A1 Λd,b,A,V(x)(cid:12) (cid:12)dx
[0,1]d(cid:12)A∈[b]|V| A∈[b]|V| (cid:12)
(cid:12) (cid:12)
(cid:90) (cid:12) (cid:12)
(cid:12) (cid:88) (cid:12)
= (cid:12)
(cid:12)
(w A−w˜ A)1 Λd,b,A,V(x)(cid:12) (cid:12)dx
[0,1]d(cid:12)A∈[b]|V| (cid:12)
(cid:90)
(cid:88)
= |w −w˜ | 1 (x)dx
A A Λd,b,A,V
[0,1]d
A∈[b]|V|
(cid:88) 1
≤ ϵ·
b|V|
A∈[b]|V|
=ϵ.
Lemma C.6. Let V ⊂ [d] and f : [0,1]|V| (cid:55)→ R be an L-Lipschitz function with 0 ≤ f ≤ C for
some C. Then,
(cid:112)
min ∥f ◦e −q∥ ≤ |V|L/(2b).
q∈Q
d,b,V,C
V 1
Proof of Lemma C.6. For any A∈[b]|V|, recall that Λ is defined in equation 3 as
|V|,b,A,[|V|]
|V|(cid:20) (cid:21)
Λ =(cid:89) A i−1 ,A i
|V|,b,A,[|V|] b b
i=1
and let λ be the centroid of Λ Also, let q be the function on [0,1]d such that
A |V|,b,A,[|V|]
(cid:88)
q(x)= f(λ )1 (x) for all x∈[0,1]d.
A Λd,b,A,V
A∈[b]|V|
By the assumption of f ≤C, we have q ∈Q . Now, we bound ∥f ◦e −q∥ .
d,b,V,C V 1
(cid:90) (cid:12) (cid:12)
(cid:12) (cid:88) (cid:12)
∥f ◦e V −q∥ 1 = (cid:12) (cid:12)f(e v(x))− f(λ A)1 Λd,b,A,V(x)(cid:12) (cid:12)dx
x∈[0,1]d
A∈[b]|V|
(cid:90) (cid:12) (cid:12)
(cid:12) (cid:88) (cid:12)
= (cid:12) (cid:12)f(x)− f(λ A)1 Λ|V|,b,A,[|V|](x)(cid:12) (cid:12)dx by Tonelli’s Theorem
x∈[0,1]|V|
A∈[b]|V|
(cid:90)
(cid:88)
≤ |f(x)−f(λ )1 (x)|dx. (20)
A Λ|V|,b,A,[|V|]
A∈[b]|V|
x∈Λ|V|,b,A,[|V|]
Note that if x∈Λ then 1 (x)=1. Hence, for x∈Λ , we have
|V|,b,A,[|V|] Λ|V|,b,A,[|V|] |V|,b,A,[|V|]
(cid:112)
|V|
|f(x)−f(λ )1 (x)|=|f(x)−f(λ )|≤L∥x−λ ∥ ≤L· .
A Λ|V|,b,A,[|V|] A A 2 2b
22Plugging this into equation 20, we have
(cid:88) (cid:90) (cid:112) |V| (cid:112) |V|
∥f ◦e −q∥ ≤ L· dx=L· .
V 1 2b 2b
A∈[b]|V|
x∈Λ|V|,b,A,[|V|]
Theorem C.7 (Theorem3.4page7ofAshtianietal.(2018),Theorem3.6page54ofDevroye&
Lugosi (2001)). There exists a deterministic algorithm that, given a collection C of distributions
log(3M2/δ)
{p ,...,p },aparameterε>0andatleast iidsamplesfromanunknowndistribution
1 M 2ε2
p, outputs an index j ∈[M] such that
∥p −p∥ ≤3 min ∥p −p∥ +4ε (21)
j 1 i 1
i∈[M]
with probability at least 1−δ/3.
Proof of Theorem C.1. For this proof, we will be employing Theorem C.7. For a nonnegative
integrable function f on [0,1]d, define N¯ : f (cid:55)→ f/∥f∥ , with N¯(0) being set to the constant
1
uniform density.
For any d,b ∈ N, V′ ⊂ [d] and sufficiently large constant C > 0, recall that Q is the
d,b,V′,C
setofhistogramsofwidthbonV′ whosemaximumweightisatmostC asdefinedinequation4
and let Q (cid:101)d,b,V′,C,b−1 be a minimal b−1 cover of Q d,b,V′,C. Also, recall that C(G) is the set of
maximal cliques in G. Let
(cid:26) (cid:27) (cid:26) (cid:27)
(cid:89) (cid:89)
Q n = q V′ |q V′ ∈Q d,b,V′,C and Q(cid:101)n = q˜ V′ |q˜ V′ ∈Q (cid:101)d,b,V′,C,b−1 . (22)
V′∈C(G) V′∈C(G)
ThecollectionC ofdensitiesfromTheoremC.7correspondtothesetN¯(Q(cid:101)n):={N¯(q)|q ∈Q(cid:101)n}.
To show that Theorem C.7 applies, we will first show that, for sufficiently large n,
log(cid:0) 3M2/δ(cid:1)
n≥ .
2ε2
We first give a bound on M. Note that
M
=(cid:12)
(cid:12)
(cid:12)N¯(cid:16) Q(cid:101)n(cid:17)(cid:12)
(cid:12)
(cid:12)≤(cid:12)
(cid:12)
(cid:12)Q(cid:101)n(cid:12)
(cid:12) (cid:12)=
(cid:89) (cid:12)
(cid:12) (cid:12)Q
(cid:101)d,b,V′,C(cid:12)
(cid:12) (cid:12).
V′∈C(G)
Since each Q (cid:101)d,b,V′,C is a minimal b−1 cover of Q d,b,V′,C, we have
(cid:12) (cid:12)
(cid:12) (cid:12)Q (cid:101)d,b,V′,C(cid:12) (cid:12)=N(Q d,b,V′,C,b−1) by the definition of a b−1-cover.
By Lemma C.5 and |V′|≤r, we have
N(Q
,b−1)≤(2Cb)(cid:16) b|V′|(cid:17)
≤(2Cb)br .
d,b,V′,C
It implies that
M ≤ (cid:89) (2Cb)br ≤(2Cb)|C(G)|·br ≤(2Cb)2d·br since |C(G)|≤2|V| =2d.
V′∈C(G)
By applying a logarithm yields, we have
logM ≤2dbrlog(2Cb)=O(brlogb)
Now, we have
log(cid:0) 3M2/δ(cid:1)
1 1 1
= ·(log3+2logM +log(1/δ))≤O( (brlogb+log )).
2ε2 2ϵ2 ϵ2 δ
23By picking
1
ϵ=n− r+1 2 logn, b=n− r+1 2 and δ = , (23)
n
log(3M2/δ)
we have, for a sufficiently large n, ≤n.
2ε2
Now, we are going to examine the RHS of equation 21 in Theorem C.7, i.e. bound the term
min q˜∈N¯(Q(cid:101)n)∥p−q˜∥ 1. We first bound the term min q˜∈Q(cid:101)n∥p−q˜∥
1
and return to N¯(Q(cid:101)n) later.
Note that
(cid:18) (cid:19)
min ∥p−q˜∥ ≤ min ∥p−q∥ + min ∥q−q˜∥ .
1 1 1
q˜∈Q(cid:101)n q∈Qn q˜∈Q(cid:101)n
Recall the definition of Q
n
and Q(cid:101)n in equation 22. We have, for each q ∈Q n, there is a q˜∈Q(cid:101)n
such that, by Lemma C.2,
(cid:18) C|C(G)|−1(cid:19)
∥q˜−q∥
1
=O
b
=O(cid:101)(n− 2+1 r). (24)
Therefore, we have
min ∥p−q˜∥ ≤ min ∥p−q∥ +O(cid:101)(n− 2+1 r).
1 1
q˜∈Q(cid:101)n q∈Qn
Now, we investigate the term min ∥p−q∥ . From Proposition A.1 it follows that
q∈Qn 1
(cid:89)
p(x)= ψ ◦e where all ψ are all Lipschitz continuous for some L. (25)
V′ V′ V′
V′∈C(G)
Because ψ are all Lipschitz continuous on a bounded set, they must all be bounded and, for
V′
sufficientlylargen,ψ ≤C forallV′ ∈C(G). Byequation25andthedefinitioninequation22,
V′
we can express
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:89) (cid:13) (cid:13) (cid:89) (cid:13)
qm ∈Qin n∥p−q∥ 1 = qm ∈Qin n(cid:13) (cid:13) V′∈C(G)ψ V′ ◦e V′ −q(cid:13) (cid:13)
1
= q V′∈m Q di ,n b,V′,C(cid:13) (cid:13) V′∈C(G)(ψ V′ ◦e V′ −q V′)(cid:13) (cid:13)
1
(26)
By Lemma C.2, for any q ∈Q , we have
V′ d,b,V′,C
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:89) (ψ V′ ◦e V′ −q V′)(cid:13) (cid:13) ≤Cd−1 (cid:88) (cid:13) (cid:13)ψ V′ ◦e V′ −q V′(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
V′∈C(G) 1 V′∈C(G) 1
and, by Lemma C.6, we have
(cid:13) (cid:13)
(cid:13) (cid:13) L
q
V′∈m
Q
di ,n b,V′,C(cid:13) (cid:13)ψ V′ ◦e V′ −q V′(cid:13)
(cid:13)
1
≤ b.
Plugging them into equation 26, we have
qm ∈Qin n∥p−q∥
1
≤ V′(cid:88) ∈C(G)Cd−1· L
b
=O(cid:101)(n− r+1 2).
If q∗ is minimizer of argmin ∥p−q˜∥ , it also implies that
q˜∈Qn 1
|∥q∗∥ 1−1|≤∥p−q∗∥
1
=O(cid:101)(n− r+1 2) (27)
Combining with equation 24, if q˜∗ is a minimizer of argmin ∥p−q∥ , we have
q∈Q(cid:101)n 1
|∥q˜∗∥ 1−1|=O(cid:101)(n− r+1 2) which means ∥q˜∗∥
1
→1 as n→∞.
NotethatQ(cid:101)nmaycontainthe0function. Thefactthat∥q˜∗∥
1
→1suggeststhat,forasufficiently
large n, the 0 function is not a minimizer of argmin ∥p−q˜∥ . For any n ∈ N, let Q(cid:101)′ the
q˜∈Q(cid:101)n 1 n
set Q(cid:101)n with 0 removed, i.e.,
(cid:40)
Q(cid:101)′ =
Q(cid:101)n if 0̸∈Q(cid:101)n
n Q(cid:101)n\{0} if 0∈Q(cid:101)n.
24Hence, for a sufficiently large n, we have min ∥p−q˜∥ = min ∥p−q˜∥ . Now, we are
q˜∈Q(cid:101)′
n
1 q˜∈Q(cid:101)n 1
ready to analyze the term min ∥p−q˜∥ . We have
q˜∈N¯(Q(cid:101)n) 1
(cid:18) (cid:19)
min ∥p−q˜∥ ≤ min ∥p−q˜∥ ≤ min
(cid:13) (cid:13)p−N¯(q˜)(cid:13)
(cid:13) ≤ min ∥p−q˜∥
+(cid:13) (cid:13)q˜−N¯(q˜)(cid:13)
(cid:13) .
q˜∈N¯(Q(cid:101)n) 1 q˜∈N¯(Q(cid:101)′ n) 1 q˜∈Q(cid:101)′
n
1 q˜∈Q(cid:101)′
n
1 1
For any q˜∈Q(cid:101)′ , we have
n
(cid:13) (cid:13)
(cid:13) (cid:13)q˜−N¯(q˜)(cid:13)
(cid:13)
1
=(cid:13)
(cid:13) (cid:13)q˜−
∥q˜q˜
∥
(cid:13)
(cid:13)
(cid:13)
=|1−∥q˜∥ 1|=|∥p∥ 1−∥q˜∥ 1|≤∥p−q˜∥ 1.
1 1
Hence, by min ∥p−q˜∥ =min ∥p−q˜∥ and equation 27, we have
q˜∈Q(cid:101)′
n
1 q˜∈Q(cid:101)n 1
min ∥p−q˜∥
1
≤2 min ∥p−q˜∥
1
=O(cid:101)(n− r+1 2).
q˜∈N¯(Q(cid:101)n) q˜∈Q(cid:101)′
n
Recall that we set ε = n− r+1 2 log(n) in equation 23. Finally, suppose q′ is the output of the
algorithm, we have
∥q′−p∥
1
≤3· min ∥p−q˜∥ 1+4·ϵ=O(cid:101)(n− r+1 2).
q˜∈N¯(Q(cid:101)n)
D Graph Proofs
For any d,d′,t∈N, define L to be the graph whose vertex set is [d]×[d′] and edge set is
d×d′
{((i,j),(i′,j′))|i,i′ ∈[d],j,j′ ∈[d′],(i,j)̸=(i′,j′),|i−j|+|i′−j′|≤1}
and Lt to be the graph whose vertex set is [d]×[d′] and edge set is
d×d′
{((i,j),(i′,j′))|i,i′ ∈[d],j,j′ ∈[d′],(i,j)̸=(i′,j′),|i−j|+|i′−j′|≤t}.
For any d,d′,t∈N, define L+ to be the graph whose vertex set is [d]×[d′] and edge set is
d×d′
{((i,j),(i′,j′))|i,i′ ∈[d],j,j′ ∈[d′],(i,j)̸=(i′,j′),max{|i−j|,|i′−j′|}≤1}
and (L+ )t to be the graph whose vertex set is [d]×[d′] and edge set is
d×d′
{((i,j),(i′,j′))|i,i′ ∈[d],j,j′ ∈[d′],(i,j)̸=(i′,j′),max{|i−j|,|i′−j′|}≤t}.
For any d,t ∈ N, define L to be the graph whose vertex set is [d] and edge set is {(i,j) |
d
i ̸= j,|i−j| ≥ 1} and Lt to be the graph whose vertex set is [d] and edge set is {(i,j) | i ̸=
d
j,|i−j|≥t}.
Proof of Lemma 4.3. For any clique C in (L )t, let (i ,j ) (resp. (i ,j ), (i′,j′) and (i′,j′))
d×d′ 0 0 1 1 0 0 1 1
be the vertex in C such that i +j is maximal (resp. i +j is minimal, i′ −j′ is maximal and
0 0 1 1 0 0
i′ −j′ is minimal). Namely, the vertex set of C is a subset of
1 1
S :={(i,j)|i∈[d],j ∈[d′],i +j ≤i+j ≤i +j ,i′ −j′ ≤i−j ≤i′ −j′}.
1 1 0 0 1 1 0 0
By the definition of cliques and (L )t, we have
d×d′
(i +j )−(i +j )≤|i −i |+|j −j |≤t since there is an edge between (i ,j ) and (i ,j )
0 0 1 1 0 1 0 1 0 0 1 1
(i′ −j′)−(i′ −j′)≤|i′ −i′|+|j′ −j′|≤t since there is an edge between (i′,j′) and (i′,j′)
0 0 1 1 0 1 0 1 0 0 1 1
ToboundthesizeofS, weobservethat, foreachoftheatmostt+1possiblevaluesi +j ,i +
1 1 1
j +1,...,i +j equal to i+j, there are at most ⌈t+1⌉ possible values among i′ +j′,i′ +j′ +
1 0 0 2 1 1 1 1
1,...,i′ +j′ equal to i−j by considering the parity. Therefore, |S| is at most (t+1)·⌈t+1⌉.
0 0 2
Hence, the size of the largest clique in (L )t is at most (t+1)·⌈t+1⌉≤ t2+4t+3.
d×d′ 2 2
25Proof of Lemma 4.4. It is easy to check that the subgraph of (L+ )t induced by the vertex set
d×d
[t+1]×[t+1] is a clique. Hence, the size of the largest clique in (L+ )t is at least (t+1)2.
d×d′
ForanycliqueC in(L+ )t, leti (resp. i′)bethesmallest(resp. largest)firstindexofthe
d×d′ 0 0
vertices in C and j (resp. j′) be the smallest (resp. largest) second index of the vertices in C.
0 0
Namely, the vertex set of C is a subset of
S :={(i,j)|i∈[d],j ∈[d′],i ≤i≤i′,j ≤j ≤j′}.
0 0 0 0
To bound the size of S, by the definition of cliques and (L+ )t, we have
d×d′
i′ −i ≤t and j′ −j ≤t
0 0 0 0
Therefore, |S| is at most (t+1)2.
Hence, the size of the largest clique in (L+ )t is (t+1)2.
d×d′
Proof of Lemma 4.6. It is easy to check that the subgraph of Lt induced by the vertex set
d
[min{t+1,d}] is a clique. Hence, the size of the largest clique in Lt is at least min{t+1,d}.
d
For any clique C in Lt, let i (resp. j ) be the smallest (resp. largest) index of the vertex in
d 0 0
C. Namely, the vertex set of C is be a subset of S := {i|i ∈ [d],i ≤ i ≤ j }. By the definition
0 0
of cliques and Lt, we have |i−j|≤min{t,d−1}. Therefore, |S| is at most min{t+1,d}.
d
Hence, the size of the largest clique in Lt is min{t+1,d}.
d
E Lower Bound for MRF Rates
We approach this problem assuming the data domain is [0,1]d. For any MRF graph G with
maximum clique size r, no estimator can achieve a rate of
O(cid:0) n−1/(2+r−ε)(cid:1)
for the set of all
Lipschitz continuous densities, for any ε>0. We prove this by contradiction.
SupposethereexistsagraphG,ε>0,andanestimatorpˆthatachievesthisrateonLipschitz
continuousdensitiessatisfyingtheMarkovpropertywithrespecttoG. Withoutlossofgenerality,
assume the first r entries of the random vector, X ,...,X , form a maximal clique in G.
1 r
Consider an arbitrary r-dimensional Lipschitz continuous density q and let q′ be the density
where,forY ∼q′,(Y ,...,Y )∼q andY ,...,Y arei.i.d. uniformrandomvariableson[0,1],
1 r r+1 d
jointly independent of (Y ,...,Y ). Note that q′ is a Lipschitz continuous density satisfying the
1 r
Markov property with respect to G.
Usingpˆtoestimateq′,weget∥q′−pˆ∥ ∈O(cid:0) n−1/(2+r−ε)(cid:1) . LetLdenotethelawofarandom
1
variable, e.g., L(Y)=q′.
Itiswell-knownthatapplyingthesamefunctiontoapairofrandomvariablesneverincreases
theirL1 distance(seeDevroye&Lugosi(2001),Section5.4). Letf :(x ,...,x )(cid:55)→(x ,...,x ).
1 d 1 r
Let Xˆ ∼pˆ. We then have L(f(Y))=q and:
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)q−L(f(Xˆ))(cid:13) =(cid:13)L(f(Y))−L(f(Xˆ))(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
1 1
(cid:13) (cid:13)
≤(cid:13)L(Y)−L(Xˆ)(cid:13)
(cid:13) (cid:13)
1
=∥q′−pˆ∥
1
(cid:16) (cid:17)
=O n−1/(2+r−ε)
Thus, L(f(Xˆ)) is an estimator that achieves a rate of O(cid:0) n−1/(2+r−ε)(cid:1) on r-dimensional
Lipschitz continuous densities. However, it is known that no estimator can achieve this rate,
leading to a contradiction.
F COCO Scatter Plots
Due to memory constraints, we used a subset of the data:
26(a) (120,160)v(120,161) (b) (120,160)v(120,162) (c) (120,160)v(120,400) (d) (120,160)v(320,520)
(e) (120,160)v(120,161) (f) (120,160)v(120,162) (g) (120,160)v(120,400) (h) (120,160)v(320,520)
cond. cond. cond. cond.
Figure 8: This figure presents scatter plots analogous to those in Figure 3 of the main text, but
derived from the COCO training set (Lin et al., 2014). The conditional scatter plots are based
on pixel (121,160) being near its median value.
1. 4000 random samples were initially selected.
2. From these, 100 images with pixel (121,160) nearest to the median were chosen for the
conditional plots.
Note that increasing the sample size for conditioning resulted in lower observed correlation.
Thisisbecausealargersampleallowsforamorepreciseconditioning, betterapproximatingthe
trueconditionaldistribution. Thewidertherangeofvaluesfortheconditioningpixel(121,160),
the more the selected points resemble the unconditional distribution, potentially introducing
spurious correlation.
These experiments provide an approximation of the conditional data. In our observations,
using larger datasets consistently and significantly reduced the observed correlation. This sug-
gests that using an even larger dataset would likely further reduce the observed correlation,
bringing the results closer to the true conditional independence structure.
27