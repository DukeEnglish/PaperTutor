ReXrank: A Public Leaderboard for AI-Powered Radiology
Report Generation
Xiaoman Zhang1(cid:0), Hong-Yu Zhou1, Xiaoli Yang1, Oishi Banerjee1,
Julián N. Acosta1, Josh Miller2, Ouwen Huang2,3 and Pranav Rajpurkar1
1Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA
2Gradient Health, Durham, NC, USA
3Department of Statistical Science, Duke University, Durham, NC, USA
AI-driven models have demonstrated significant potential in automating radiology report generation for
chest X-rays. However, there is no standardized benchmark for objectively evaluating their performance.
To address this, we present ReXrank (https://rexrank.ai), a public leaderboard and challenge for
assessing AI-powered radiology report generation. Our framework incorporates ReXGradient, the largest
test dataset consisting of 10,000 studies, and three public datasets (MIMIC-CXR, IU-Xray, CheXpert
Plus) for report generation assessment. ReXrank employs 8 evaluation metrics and separately assesses
models capable of generating only findings sections and those providing both findings and impressions
sections. Byprovidingthisstandardizedevaluationframework,ReXrankenablesmeaningfulcomparisonsof
model performance and offers crucial insights into their robustness across diverse clinical settings. Beyond
its current focus on chest X-rays, ReXrank’s framework sets the stage for comprehensive evaluation of
automated reporting across the full spectrum of medical imaging.
1 Introduction
Writing accurate radiology reports from medical images is a critical but complex task, requiring both deep
expertise in medical imaging and the ability to accurately interpret and articulate intricate findings. The
demand for such reports has surged with the rapid advancements in imaging technologies, leading to increased
workloads for radiologists, risks of information loss, and longer report turnaround times [3].
AI-driven solutions have emerged as a potential answer to these challenges, serving as assistive tools to
enhance reporting efficiency and ensure access to high-quality, specialty-level interpretations. Medical visual-
language models have shown promise in automating the generation of radiology reports from chest X-ray
images [7, 19, 5, 14, 30]. However, as the field of AI-assisted medical reporting rapidly evolves, there is a
growing need for standardized benchmarks to objectively assess and compare the performance of these models.
Existing datasets for chest X-ray report generation, such as MIMIC-CXR [12], are valuable but exhibit
limitations that hinder their effectiveness for benchmarking. These datasets frequently suffer from inconsistent
data splits and a lack of standardized metrics during evaluation, which impedes reliable comparative analysis
across different model architectures. Furthermore, the data distribution in MIMIC-CXR, commonly used
in model training, fails to adequately test the models’ ability to generalize to new, unseen distributions. To
fill this gap, we introduce ReXrank (https://rexrank.ai), a public leaderboard and challenge specifically
designed for evaluating AI-powered radiology report generation from chest X-ray images.
ReXrank offers a comprehensive evaluation framework that sets a standardized benchmark for assessing
the effectiveness of different radiology report generation models. To ensure robust and clinically relevant
evaluations, it integrates diverse datasets, including MIMIC-CXR [12], IU-Xray [8], CheXpert Plus [4], and
ReXGradient, a large-scale private dataset of 10,000 studies. This broad dataset spectrum allows us to
evaluate model performance on data with varying distributions, providing deeper insights into the models’
generalization capabilities. Furthermore, ReXrank implements various report evaluation metrics, including
BLEU-2[18],BERTScore[28],SembScore[20],RadGraph-F1[26],RadCliQ[26],RaTEScore[29],GREEN[17],
FineRadScore[10], etc., toofferadetailedviewofeachmodel’sstrengthsandweaknesses. Thiscomprehensive
approach enables a more nuanced understanding of model performance and facilitates meaningful comparisons
between different AI-powered radiology report generation systems.
Corresponding Author: Xiaoman Zhang xiaoman_zhang@hms.harvard.edu.
4202
voN
22
]VC.sc[
1v22151.1142:viXraDATA
Public Datasets Private Datasets Input Sample
Age: 70-80. Gender: F.
Indication: Nausea, vomiting and
abdominal pain. Question free air.
Comparison: None.
MIMIC-CXR IU X-ray CheXpert Plus ReXGradient
Frontal Image Lateral Image
Evaluation
Evaluation
10 Institutions, 11 Methods, 16 Models LexicalMetrics
BiomedGPT CheXagent CheXpertPlus BLEU-2 BERTScore
Clinical Metrics Ranking
Cvt2distilgpt2 GPT-4V LLM-CXR
SembScore RaTEScore
MAIRA-2 MedVersa RadFM
RadGraph-F1 GREEN
RaDialog VLCI RadCliQ FineRadScore
Analysis
ReXrank: A Public Radiology Report Generation Leaderboard
Figure 1 | AnillustrationofReXrank,apublicleaderboardandchallengeforAI-poweredradiologyreportgenerationfrom
chestX-rayimages. ReXranksupportsmodelsubmissionsandevaluatesthemonbothpublicdatasetsandalarge-scaleprivate
dataset,providingcomprehensiverankingsofallsubmittedmodels.
2 Overview
Datasets. ReXrank leverages three public datasets and one comprehensive private dataset for report
generation assessment. The private ReXGradient dataset comprises 10,000 studies from 67 U.S. medical sites,
making it one of the largest and most geographically diverse evaluation sets. For public datasets, we utilize
the official test splits of MIMIC-CXR (2,347 studies) and IU-Xray (590 studies), along with CheXpert Plus’s
validation set (200 studies) as no test split is available.
Models. ReXrank currently includes 16 report generation models from 10 different institutions, including
BiomedGPT_IU [27], CheXagent [7], CheXpertPlus_CheX [4], CheXpertPlus_CheX_MIMIC [4], CheX-
pertPlus_MIMIC [4], Cvt2distilgpt2_IU [16], Cvt2distilgpt2_MIMIC [16], GPT4V [25], LLM-CXR [14],
MAIRA-2 [2], MedVersa [30], RadFM [22], RaDialog [19], RGRG [21], VLCI_IU [5] and VLCI_MIMIC [5].
Thesemodelsweretrainedondifferentmedicaldatasets,primarilyMIMIC-CXR,CheXpertPlus,andIU-Xray,
with some models capable of handling multiple tasks beyond just report generation.
Metrics. ReXrank employs 8 different metrics to comprehensively assess the quality of generated radiology
reports, including traditional text generation metrics like BLEU-2 [18] and BERTScore [28], as well as
domain-specific metrics designed for radiology report evaluation such as SembScore [20], RadGraph-F1 [26],
RadCliQ-v1 [26] and RaTEScore [29]. The framework also incorporates recently developed LLM-based
metrics including GREEN [17], and FineRadScore [10], which focus on identifying clinically significant errors.
Each metric evaluates different aspects of the generated reports, from textual similarity to clinical accuracy,
providing a thorough assessment of model performance. We default use RadCliQ-v1 as the primary metric.
Results. MedVersa emerges as one of the top-performing models (Figure 2), with best 1/RadCliQ-v1 scores
of 0.98 ± 0.05 on ReXGradient and 0.92 ± 0.02 on MIMIC-CXR. However, its performance on the CheXpert
Plus dataset is comparatively lower, ranking fourth with a 1/RadCliQ-v1 score of 0.72 ± 0.10 on the Findings.
MedVersa consistently outperforms GPT4V, the state-of-the-art generalist vision-language model, across
multiple metrics and datasets. We further analyze the distribution of evaluation metrics across datasets.
Among the four datasets, IU X-ray stands out as the least challenging, consistently yielding high performance
|2
Test
Datasetsacross models. In contrast, CheXpert Plus exhibits the highest variance and lower overall performance, likely
due to its distinct data distribution and small validation set (200 studies). The private ReXGradient dataset
demonstrates remarkably low-performance variance across models, underscoring its high data quality and
utility as a benchmark for assessing model robustness.
Figure2|Comprehensiveperformanceevaluationandrankingofreportgenerationmodelsbasedontheaverage1/RadCliQ-v1
metricoffourdistinctdatasets: ReXGradient,MIMIC-CXR,IUX-ray,andCheXpertPlus. MedVersa(highlightedinpurple)
demonstratesconsistentlysuperiorperformance,achievingsignificantlyhigherscorescomparedtoothermodels,includingGPT4V
(highlightedinyellow).
BLEU-2 BertScore SembScore RadGraph
0.30
0.60
0.25 0.50
0.25
0.45 0.50
0.20
0.40 0.20 0.40
0.15 0.35
0.15
0.30
0.30
0.10 0.10
0.25 0.20
0.05 0.20 0.10 0.05
0.15
ReXGradient MIMIC-CXR IU X-ray Che X Pp le ur st ReXGradient MIMIC-CXR IU X-ray Che X Pp le ur st ReXGradient MIMIC-CXR IU X-ray Che X Pp le ur st ReXGradient MIMIC-CXR IU X-ray Che X Pp le ur st
RaTEScore GREEN 1/FineRadScore 1/RadCliQ-v1
0.70
0.65 0.60 1.40
0.60
0.60 0.55
0.55 0.50 1.20 0.50
0.50 0.40 1.00
0.45
0.45 0.30
0.40 0.40 0.80
0.20
0.35 0.35
0.10 0.60
0.30 0.30
0.00
ReXGradient MIMIC-CXR IU X-ray Che X Pp le ur st ReXGradient MIMIC-CXR IU X-ray Che X Pp le ur st ReXGradient MIMIC-CXR IU X-ray Che X Pp le ur st ReXGradient MIMIC-CXR IU X-ray Che X Pp le ur st
Figure 3 | Distributionofevaluationmetricsacrossdifferentdatasets: ReXGradient,andMIMIC-CXR,IUX-ray,CheXpert
Plus. Boxplotsshowthevariationinmodelperformanceforeachmetric. Forconsistencyinvisualization,weplotthereciprocals
(1/x)ofFineRadScoreandRadCliQ-v1,sohighervaluesindicatebetterperformanceacrossallmetrics.
3 Method
3.1 Datasets
Our evaluation leverages four distinct datasets: ReXGradient, MIMIC-CXR, IU-Xray, and CheXpert Plus.
Thesedatasetsprovidediversetestingdistributionsacrossdifferentmedicalinstitutionsandpatientpopulations.
ReXGradient. ThisprivatetestsetisprovidedbyGradientHealth,whichconsistsof10,000studiescollected
from 7,004 patients across 67 medical sites in the United States.
|3
2-UELB
erocSETaR
erocStreB
NEERG
erocSbmeS
erocSdaReniF/1
hparGdaR
1v-QilCdaR/1MIMIC-CXR [12]. This is a large, publicly accessible dataset comprising 377,110 chest X-rays (CXRs)
corresponding to 227,835 radiographic studies performed at the Beth Israel Deaconess Medical Center in
Boston, MA. For this dataset, we extracted sections of indication, comparison, findings, and impression via
keyword matching. In our experiments, we follow MIMIC-CXR’s official split and report scores on the test
set, which consists of 2,347 studies.
IU-Xray [8]. This is a publicly accessible dataset containing 7,470 pairs of CXRs and radiology reports.
Each study in this dataset includes one frontal and one lateral CXR, associated with a single radiology report.
We follow the split provided by R2Gen [6] and report scores on the test set, which comprises 590 studies.
CheXpert Plus [4]. This is a large, publicly accessible consisting of 223,462 unique pairs of radiology
reports and chest X-rays. These correspond to 187,711 radiographic studies from 64,725 patients. We follow
the official split of CheXpert Plus and report scores on the validation set, which contains 200 studies.
3.2 Data format
For each study in our test datasets, the data is organized in a structured format.
• id: Unique identifier for the study
• image_path: List of paths to all relevant chest X-ray images
• frontal_lateral: List indicating the view type of each image
• key_image_path: Path to the primary image (typically frontal view)
• context: Patient information and clinical context
• report: Radiologist’s findings and impressions
3.3 Metrics
BLEU-2 [18]. BLEU (Bilingual Evaluation Understudy) is a widely used metric in machine translation and
text generation tasks. It evaluates the quality of generated text by comparing n-gram precision between the
candidate and reference texts, with scores ranging from 0 to 1. In this work, we specifically use BLEU-2,
which focuses on bigram precision to assess the quality of the generated text.
BERTScore [28]. BERTScore is a neural metric that uses pre-trained BERT models [13] to evaluate text
similarity. It computes cosine similarity between the BERT embeddings of model-generated and groundtruth
radiology reports.
SembScore [20]. SembScore (CheXbert labeler vector similarity) is a domain-specific metric for radiology
report evaluation. It computes the cosine similarity between the indicator vectors of 14 pathologies that the
CheXbert automatic labeler extracts from model-generated and groundtruth radiology reports.
RadGraph-F1 [26]. RadGraph-F1 is a metric for radiology report evaluation. It computes the overlap in
clinical entities and relations that RadGraph [11] extracts from candidate and reference reports.
1/RadCliQ-v1 [26]. RadCliQ is a composite metric designed for evaluating radiology report generation,
combining BLEU, BERTScore, SembScore, and RadGraph-F1 to provide a comprehensive assessment of
generated reports. For our evaluation, we utilized the official implementation1, which also includes BLEU,
BERTScore, SembScore, RadGraph-F1. While the original RadCliQ metric is designed as lower-is-better,
we first calculate the average RadCliQ-v1 score for each model across the dataset, then take its reciprocal
(1/RadCliQ-v1) to maintain consistency with other metrics where higher values indicate better performance.
RaTEScore [29]. RaTEScore is an entity-aware metric for radiology report evaluation. It emphasizes
crucial medical entities like diagnostic outcomes and anatomical details and is robust against complex medical
synonyms and sensitive to negation expressions. For our evaluation, we utilized the official implementation2.
GREEN [17]. GREEN (Generative Radiology Report Evaluation and Error Notation) is an LLM-based
metricforevaluatingradiologyreportgeneration. Itleverageslanguagemodelstoidentifyandexplainclinically
significant errors in quantitative and qualitative candidate reports. We utilized the official implementation3
1https://github.com/rajpurkarlab/CXR-Report-Metric
2https://github.com/MAGIC-AI4Med/RaTEScore
3https://github.com/Stanford-AIMI/GREEN
|4for our evaluation.
1/FineRadScore [10]. FineRadScore is an automated evaluation metric leveraging an LLM to assess the
qualityofgeneratedchestX-rayreports. Itdeterminestheminimumnumberofline-by-linecorrectionsneeded,
with severity ratings from 1 to 4, to transform a candidate report into a ground-truth report. For evaluation,
we employ GPT-4o as the LLM and use the official implementation4. To obtain a single FineRadScore for
each report, we take the maximum clinical severity across all lines. Similar to 1/RadCliQ-v1, we present
1/FineRadScore to maintain consistency where higher values indicate better performance.
3.4 Confidence Intervals
In our analysis, we generate confidence intervals (CIs) by assuming a normal distribution of data. This
statistical method calculates the mean and standard deviation of our data and then uses the standard error
of the mean to estimate variability. For a 95% confidence level, a Z-score of approximately 1.96 is used to
determine the interval. This Z-score indicates that the true mean is likely within 1.96 standard errors of the
sample mean. By multiplying the Z-score by the standard error, we obtain the CI, providing a range that
encapsulates the true average value with 95% certainty.
3.5 Participating Models
BiomedGPT_IU [27]. BiomedGPTisalightweight,open-sourcevision-languagemodeldesignedfordiverse
biomedical tasks across modalities. The model was fine-tuned for VQA and image captioning tasks using
multiple datasets, including radiology and pathology data. BiomedGPT_IU is fine-tuned on the IU X-ray
dataset for image captioning tasks. In our evaluation, we used the publicly available checkpoints trained on
the IU-Xray dataset5.
CheXagent [7]. CheXagent is an instruction-tuned foundation model specifically designed for chest X-ray
interpretation. The model consists of a vision encoder for representing CXR images, and a network to bridge
the vision and language modalities. This model is trained on CheXinstruct, a large-scale instruction-tuning
dataset curated from 28 publicly-available datasets. For our evaluation, we utilized the publicly available 8
billion parameter checkpoint from Hugging Face6.
CheXpertPlus_CheX [4]. CheXpertPlus_CheX, introduced in the CheXpert Plus paper, utilizes a
Swinv2 [15] architecture with a two-layer BERT decoder [13] for medical report generation. CheXpert-
Plus_CheX is trained exclusively on the CheXpert Plus dataset. In our evaluation, we utilized the publicly
availableFindingsCheckpoint7 andImpressionCheckpoint8. Ourevaluationemploysthesemodelssequentially,
generating the findings and impression sections separately, and then combining them with appropriate headers
to form the complete report.
CheXpertPlus_CheX_MIMIC [4]. CheXpertPlus_CheX_MIMIC shares the same architectural design
as CheXpertPlus_MIMIC, employing the Swinv2 architecture with a two-layer BERT decoder. CheXpert-
Plus_CheX is trained exclusively on the combination of MIMIC-CXR and CheXpert Plus dataset. In
our evaluation, we utilized the publicly available Findings Checkpoint9 and Impression Checkpoint10. Our
evaluation employs these models sequentially, generating the findings and impression sections separately, and
then combining them with appropriate headers to form the complete report.
CheXpertPlus_MIMIC [4]. CheXpertPlus_MIMIC shares the same architectural design as CheXpert-
Plus_CheX, employing the Swinv2 architecture with a two-layer BERT decoder. CheXpertPlus_MIMIC
comprises two distinct models trained on MIMIC-CXR: one for findings and another for impressions. In
our evaluation, we utilized the publicly available Findings Checkpoint11 and Impression Checkpoint12. Our
4https://github.com/rajpurkarlab/FineRadScore
5https://github.com/taokz/BiomedGPT
6https://huggingface.co/StanfordAIMI/CheXagent-8b
7https://huggingface.co/IAMJBchexpert-findings-baseline
8https://huggingface.co/IAMJB/chexpert-impression-baseline
9https://huggingface.co/IAMJB/chexpert-mimic-cxr-findings-baseline
10https://huggingface.co/IAMJB/chexpert-mimic-cxr-impression-baseline
11https://huggingface.co/IAMJB/mimic-cxr-findings-baseline
12https://huggingface.co/IAMJB/mimic-cxr-impression-baseline
|5evaluation employs these models sequentially, generating the findings and impression sections separately, and
then combining them with appropriate headers to form the complete report.
Cvt2distilgpt2_IU [16] CvT2DistilGPT2_IU employs a hybrid architecture combining a Convolutional
vision Transformer (CvT) encoder [23] pre-trained on ImageNet-21K [9] with a DistilGPT2 [1] decoder for
chest X-ray report generation. This model leverages the CvT’s efficient hierarchical design for image feature
extraction and DistilGPT2’s natural language generation capabilities. In our evaluation, we utilized the
publicly available checkpoint from Github13 and followed the official evaluation guidelines.
Cvt2distilgpt2_MIMIC [16] Cvt2distilgpt2_MIMIC applies the same Cvt2distilgpt2 architecture but
is trained on the MIMIC-CXR dataset. Our evaluation utilized the publicly available MIMIC-CXR-trained
checkpoints.
RGRG [21]. RGRG (Region-Guided Radiology Report Generation) employs object detection to extract
localized visual features from 29 anatomical regions in chest X-rays. It uses binary classifiers to select salient
features and encode abnormalities, followed by a language model generating sentences for each selected region.
RGRG was trained on the Chest ImaGenome dataset [24]. In our evaluation, we utilized the publicly available
checkpoint from Github14 and followed the official evaluation guidelines.
RaDialog [19]. RaDialog is a large vision-language model for radiology report generation and interactive
dialogue. It integrates visual image features and structured pathology findings with a large language model
(LLM), adapted to radiology using parameter-efficient fine-tuning. RaDialog was trained on the MIMIC-CXR
for radiology report generation tasks. In our evaluation, we utilized the publicly available LLaVA version
checkpoint from Huggingface15 and followed the official evaluation guidelines.
GPT-4V [25]. GPT-4V (GPT-4 with vision) is a multimodal large language model released by OpenAI,
which enables users to instruct GPT-4 to analyze image inputs provided by the user. In our evaluation, we
used the API of model “gpt4o05132024” and followed the official evaluation protocols to assess its performance.
The prompt we used is “You are a helpful assistant. Please generate a report for the given images, including
both findings and impressions. Return the report in the following format: Findings: {} Impression: {}. ”.
LLM-CXR [14]. LLM-CXR is a multimodal large language model that utilizes VQ-GAN to tokenize
images, integrating both image and text tokens as input to its base LLM architecture. This model enables
CXR-to-report generation, report-to-CXR generation, and CXR-related visual question answering (VQA).
For our evaluation, we used the publicly available checkpoints16 and followed the official evaluation guidelines.
MAIRA-2 [2]. MAIRA-2 is a multimodal large language model that combines a radiology-specific image
encoder with a Large Language Model (LLM), trained for grounded report generation from chest X-rays. For
input, the model accepts X-ray images along with indication, comparison, and technique information. For our
evaluation, we used the publicly available checkpoints17 and followed the official evaluation guidelines. For
studies containing both frontal and lateral views, we input the technique that “PA and lateral views of the
chest were obtained.”. For studies with only frontal views, we use “PA view of the chest was obtained.”.
MedVersa [30]. MedVersa is a versatile medical AI system that can coordinate multiple models and tools to
perform various tasks and generate multimodal outputs. MedVersa was trained on the MIMIC-CXR training
andvalidationdatasetformedicalreportgenerationtasks. Inourevaluation, weutilizedthepubliclyavailable
checkpoint from Huggingface18 and followed the official evaluation guidelines. The standard prompt structure
we employed for report generation was “Can you provide a report of {input_image_token} with findings and
impression?”, where {input_image_token} represents the placeholder for the input image.
RadFM [22]. RadFM is a radiology foundation model trained on large-scale multi-modal datasets. It
supports both 2D and 3D scans, multi-image input, and visual-language interleaving cases. The model’s
training included the MIMIC-CXR dataset. For our evaluation, we utilized the publicly available checkpoint
13https://github.com/aehrc/cvt2distilgpt2
14https://github.com/ttanida/rgrg
15https://huggingface.co/ChantalPellegrini/RaDialog-interactive-radiology-report-generation
16https://github.com/hyn2028/llm-cxr
17https://huggingface.co/microsoft/maira-2
18https://huggingface.co/hyzhou/MedVersa
|6from Huggingface19 and followed the official evaluation guidelines. The prompt we employed for report
generation was “Can you provide a radiology report for this medical image?”.
VLCI_IU [5]. VLCI (Visual-Linguistic Causal Intervention) is a vision-language model using a multiway
transformer for cross-modal alignment with Visual-linguistic causal intervention, integrating a pre-trained
transformer and Visual and linguistic de-confounding Modules to mitigate cross-modal bias through local and
global visual sampling and linguistic estimation using a vocabulary dictionary and visual features. In our
evaluation, we used the publicly available checkpoints trained on the IU-Xray dataset20.
VLCI_MIMIC [5]. VLCI_MIMIC applies the same VLCI architecture but is trained on the MIMIC-CXR
dataset. Our evaluation utilized the publicly available MIMIC-CXR-trained checkpoints.
4 Results
Table1,2,3and 4summarizetheperformanceofvariousmedicalreportgenerationmodelsacrossfourdifferent
datasets: ReXGradient, MIMIC-CXR, IU X-ray and CheXpert Plus. Among these, MedVersa demonstrates
superior performance, achieving the best 1/RadCliQ-v1 scores on ReXGradient (1.01 ± 0.01), MIMIC-CXR
(1.10 ± 0.02), and IU X-ray (1.46 ± 0.03) on the findings section. This consistent top performance across
different datasets indicates MedVersa’s robust generalization capabilities and effectiveness in medical report
generation.
Moreover, the results displayed in the tables show that ReXGradient serves as a dataset where models
consistently exhibit minimal confidence intervals of 0.01 for most models on the RadCliQ-v1 metric, thereby
supporting its utility as a reliable benchmark for medical report generation models. Figure 3 illustrates the
distribution of evaluation metrics. The IU X-ray dataset, while always showing high-performance scores (the
best model achieving a 1/RadCliQ-v1 score of 1.46 ± 0.03 on the findings sections), suggests that it may be
too simplistic or lacking in complexity necessary for rigorous model differentiation. In contrast, CheXpert
Plus shows lower overall performance (the best model obtaining a 1/RadCliQ-v1 score of 0.81 ± 0.12 on the
findings sections) with higher variance, potentially indicating dataset distribution shifts or noise.
Models trained on multiple datasets (e.g., CheXpertPlus_CheX_MIMIC) tend to outperform those trained
on individual datasets, suggesting that a multi-dataset training approach helps bridge the distributional gap
and enhances generalization. Models perform better when evaluated on the same distribution seen in training,
for instance, VLCI_IU achieves superior performance on IU X-ray (RadCliQ-v1: 1.38 ± 0.04) compared to
VLCI_MIMIC (0.91 ± 0.04), while VLCI_MIMIC performs better on MIMIC-CXR (0.68 ± 0.02 vs 0.60 ±
0.02 for VLCI_IU).
Thecomparisonbetweenfindings-onlyandfindings+impressiontasksrevealsinterestingmodelbehaviors. On
ReXGradient, MedVersa shows slight performance degradation when generating both findings and impressions
(1/RadCliQ-v1 decreasing from 1.01 ± 0.01 to 0.98 ± 0.05), while CheXpertPlus_CheX_MIMIC shows
improvement (from 0.83 ± 0.01 to 0.85 ± 0.01). This may be due to CheXpertPlus_CheX_MIMIC using
separate models for findings and impression generation, while MedVersa only uses a single model architecture.
The separate models may allow CheXpertPlus_CheX_MIMIC to better specialize in each subtask.
Disclosures
O.H and J.M are founders and hold equity in Gradient Health, a private company focused on health data
accessibility and availability for commercial research. Gradient Health provided the Private Dataset used in
this work and did not provide funding for this research and had no role in its design, execution, or publication.
The Private Dataset’s 4x downsampled version is available under the Gradient Health Public License.
19https://huggingface.co/chaoyi-wu/RadFM
20https://github.com/WissingChen/VLCI
|7Table 1 | ComprehensiveevaluationofmedicalreportgenerationmodelsonReXGradient. Modelsarerankedby1/RadCliQ-v1.
Modelevaluationresultswith95%confidenceintervals(mean±CI)undernormalityassumption. Thebestresultsforeach
metricareshowninbold.
Model 1/RadCliQ-v1↑ BLEU-2↑ BertScore↑ SembScore↑ RadGraph↑ RaTEScore↑ GREEN↑ 1/FineRadScore↑
Findings+Impression
MedVersa 0.98±0.01 0.17±0.01 0.44±0.01 0.48±0.02 0.19±0.01 0.53±0.00 0.52±0.01 0.47±0.02
CheXpertPlus_CheX_MIMIC 0.85±0.01 0.20±0.00 0.39±0.00 0.43±0.00 0.17±0.00 0.50±0.00 0.51±0.01 0.47±0.02
CheXpertPlus_MIMIC 0.80±0.01 0.18±0.00 0.36±0.00 0.43±0.00 0.14±0.00 0.48±0.00 0.52±0.01 0.47±0.02
CheXpertPlus_CheX 0.76±0.01 0.17±0.00 0.33±0.00 0.40±0.00 0.15±0.00 0.50±0.00 0.47±0.01 0.42±0.02
RadFM 0.74±0.01 0.13±0.00 0.34±0.00 0.38±0.00 0.13±0.00 0.47±0.00 0.41±0.01 0.43±0.02
GPT4V 0.66±0.01 0.07±0.00 0.21±0.00 0.36±0.00 0.17±0.00 0.46±0.00 0.36±0.01 0.42±0.02
Findings
MedVersa 1.01±0.01 0.21±0.00 0.43±0.00 0.50±0.00 0.20±0.00 0.53±0.00 0.53±0.01 0.47±0.02
MAIRA-2 0.96±0.01 0.20±0.00 0.44±0.00 0.46±0.00 0.19±0.00 0.56±0.00 0.53±0.01 0.47±0.02
VLCI_IU 0.90±0.01 0.21±0.00 0.36±0.00 0.47±0.00 0.21±0.00 0.57±0.00 0.54±0.01 0.45±0.02
RGRG 0.89±0.01 0.19±0.00 0.39±0.00 0.47±0.00 0.17±0.00 0.54±0.00 0.49±0.01 0.46±0.02
RaDialog 0.88±0.01 0.19±0.00 0.40±0.00 0.45±0.00 0.16±0.00 0.52±0.00 0.43±0.01 0.46±0.02
Cvt2distilgpt2_MIMIC 0.87±0.01 0.19±0.00 0.37±0.00 0.46±0.00 0.18±0.00 0.52±0.00 0.51±0.01 0.47±0.02
Cvt2distilgpt2_IU 0.84±0.01 0.18±0.00 0.40±0.00 0.41±0.00 0.17±0.00 0.52±0.00 0.47±0.01 0.46±0.02
CheXpertPlus_CheX_MIMIC 0.83±0.01 0.17±0.00 0.37±0.00 0.44±0.00 0.15±0.00 0.52±0.00 0.49±0.01 0.47±0.02
CheXpertPlus_CheX 0.79±0.01 0.14±0.00 0.36±0.00 0.43±0.00 0.12±0.00 0.48±0.00 0.41±0.01 0.41±0.02
CheXpertPlus_MIMIC 0.78±0.01 0.15±0.00 0.34±0.00 0.44±0.00 0.13±0.00 0.50±0.00 0.52±0.01 0.47±0.02
RadFM 0.78±0.01 0.16±0.00 0.36±0.00 0.39±0.00 0.14±0.00 0.50±0.00 0.41±0.01 0.44±0.02
BiomedGPT_IU 0.77±0.01 0.10±0.00 0.32±0.00 0.44±0.00 0.16±0.00 0.47±0.00 0.39±0.01 0.45±0.02
VLCI_MIMIC 0.72±0.01 0.16±0.00 0.31±0.00 0.40±0.00 0.12±0.00 0.49±0.00 0.48±0.01 0.46±0.02
CheXagent 0.67±0.01 0.09±0.00 0.30±0.00 0.37±0.00 0.08±0.00 0.43±0.00 0.24±0.01 0.46±0.02
GPT4V 0.63±0.01 0.07±0.00 0.21±0.00 0.34±0.00 0.14±0.00 0.47±0.00 0.50±0.01 0.43±0.02
LLM-CXR 0.51±0.01 0.04±0.00 0.18±0.00 0.14±0.00 0.03±0.00 0.32±0.00 0.04±0.00 0.33±0.02
Table 2 | ComprehensiveevaluationofmedicalreportgenerationmodelsontheMIMIC-CXRdatasets. Modelsarerankedby
1/RadCliQ-v1. Model evaluation results with 95% confidence intervals (mean ± CI) under normality assumption. The best
resultsforeachmetricareshowninbold.
Model 1/RadCliQ-v1↑ BLEU-2↑ BertScore↑ SembScore↑ RadGraph↑ RaTEScore↑ GREEN↑ 1/FineRadScore↑
Findings+Impression
MedVersa 0.92±0.02 0.19±0.00 0.43±0.01 0.32±0.01 0.27±0.01 0.55±0.01 0.42±0.01 0.36±0.03
CheXpertPlus_CheX_MIMIC 0.83±0.02 0.17±0.00 0.36±0.00 0.39±0.01 0.20±0.00 0.52±0.00 0.37±0.01 0.36±0.03
CheXpertPlus_MIMIC 0.80±0.02 0.17±0.00 0.35±0.00 0.38±0.01 0.19±0.00 0.51±0.00 0.38±0.01 0.36±0.03
CheXpertPlus_CheX 0.71±0.01 0.13±0.00 0.30±0.00 0.34±0.01 0.17±0.00 0.51±0.00 0.30±0.01 0.35±0.03
RadFM 0.62±0.02 0.08±0.00 0.28±0.00 0.24±0.01 0.11±0.00 0.45±0.00 0.21±0.01 0.35±0.03
GPT4V 0.55±0.01 0.07±0.00 0.20±0.00 0.19±0.01 0.09±0.00 0.43±0.00 0.13±0.01 0.33±0.03
Findings
MedVersa 1.10±0.02 0.21±0.00 0.45±0.01 0.47±0.01 0.27±0.01 0.55±0.01 0.37±0.01 0.36±0.03
CheXpertPlus_CheX_MIMIC 0.81±0.02 0.14±0.00 0.37±0.00 0.38±0.01 0.18±0.01 0.49±0.01 0.30±0.01 0.36±0.03
RaDialog 0.80±0.02 0.13±0.00 0.36±0.00 0.39±0.01 0.17±0.00 0.48±0.00 0.27±0.01 0.36±0.03
CheXpertPlus_MIMIC 0.79±0.02 0.14±0.00 0.36±0.00 0.38±0.01 0.17±0.00 0.48±0.01 0.31±0.01 0.36±0.03
RGRG 0.76±0.02 0.13±0.00 0.35±0.00 0.34±0.01 0.17±0.00 0.49±0.00 0.27±0.01 0.35±0.03
CheXagent 0.74±0.02 0.11±0.00 0.35±0.01 0.35±0.01 0.15±0.00 0.47±0.01 0.26±0.01 0.35±0.03
Cvt2distilgpt2_MIMIC 0.72±0.02 0.13±0.00 0.33±0.01 0.33±0.01 0.15±0.01 0.43±0.01 0.27±0.01 0.36±0.03
CheXpertPlus_CheX 0.70±0.01 0.08±0.00 0.31±0.00 0.33±0.01 0.14±0.00 0.47±0.00 0.23±0.01 0.35±0.03
MAIRA-2 0.69±0.02 0.09±0.00 0.31±0.00 0.34±0.01 0.13±0.00 0.52±0.00 0.22±0.01 0.36±0.03
VLCI_MIMIC 0.68±0.02 0.14±0.00 0.30±0.00 0.30±0.01 0.14±0.00 0.45±0.01 0.26±0.01 0.36±0.03
RadFM 0.65±0.02 0.09±0.00 0.31±0.00 0.26±0.01 0.11±0.00 0.45±0.01 0.18±0.01 0.35±0.03
Cvt2distilgpt2_IU 0.61±0.02 0.06±0.00 0.30±0.00 0.19±0.01 0.10±0.00 0.45±0.01 0.16±0.01 0.35±0.03
VLCI_IU 0.60±0.02 0.07±0.00 0.26±0.00 0.21±0.01 0.11±0.00 0.45±0.01 0.21±0.01 0.35±0.03
GPT4V 0.56±0.01 0.07±0.00 0.21±0.00 0.21±0.01 0.08±0.00 0.42±0.00 0.16±0.01 0.34±0.03
BiomedGPT_IU 0.54±0.01 0.02±0.00 0.19±0.00 0.22±0.01 0.06±0.00 0.36±0.00 0.12±0.01 0.34±0.03
LLM-CXR 0.52±0.01 0.04±0.00 0.18±0.00 0.16±0.01 0.05±0.00 0.34±0.00 0.04±0.00 0.31±0.03
|8Table 3 | Comprehensive evaluation of medical report generation models on the IU X-ray datasets. Models are ranked by
1/RadCliQ-v1. Model evaluation results with 95% confidence intervals (mean ± CI) under normality assumption. The best
resultsforeachmetricareshowninbold.
Model 1/RadCliQ-v1↑ BLEU↑ BertScore↑ SembScore↑ RadGraph↑ RaTEScore↑ GREEN↑ 1/FineRadScore↑
Findings+Impression
MedVersa 1.45±0.04 0.20±0.01 0.52±0.01 0.60±0.02 0.24±0.01 0.63±0.01 0.66±0.02 0.58±0.07
CheXpertPlus_CheX_MIMIC 1.28±0.04 0.24±0.01 0.48±0.01 0.60±0.02 0.23±0.01 0.61±0.01 0.69±0.02 0.59±0.07
RadFM 1.23±0.05 0.20±0.01 0.48±0.01 0.56±0.02 0.23±0.01 0.60±0.01 0.64±0.02 0.55±0.08
CheXpertPlus_MIMIC 1.13±0.04 0.23±0.01 0.45±0.01 0.59±0.02 0.19±0.01 0.57±0.01 0.68±0.02 0.61±0.07
CheXpertPlus_CheX 1.01±0.03 0.20±0.01 0.39±0.01 0.55±0.02 0.21±0.01 0.60±0.01 0.71±0.02 0.57±0.07
GPT4V 0.68±0.03 0.08±0.00 0.23±0.01 0.40±0.02 0.16±0.01 0.52±0.01 0.40±0.03 0.53±0.08
Findings
MedVersa 1.46±0.03 0.21±0.01 0.53±0.01 0.61±0.02 0.23±0.01 0.65±0.01 0.63±0.02 0.57±0.07
VLCI_IU 1.38±0.04 0.27±0.01 0.46±0.01 0.62±0.02 0.29±0.01 0.68±0.01 0.70±0.02 0.55±0.07
MAIRA-2 1.30±0.04 0.22±0.01 0.48±0.01 0.60±0.02 0.23±0.01 0.63±0.01 0.19±0.02 0.60±0.07
Cvt2distilgpt2_IU 1.28±0.05 0.24±0.01 0.48±0.01 0.55±0.02 0.27±0.02 0.62±0.01 0.69±0.02 0.56±0.08
RadFM 1.19±0.04 0.20±0.01 0.46±0.01 0.57±0.02 0.23±0.01 0.63±0.01 0.61±0.02 0.57±0.07
CheXpertPlus_CheX_MIMIC 1.18±0.04 0.20±0.01 0.45±0.01 0.59±0.02 0.21±0.01 0.62±0.01 0.65±0.02 0.58±0.07
RGRG 1.17±0.04 0.22±0.01 0.44±0.01 0.60±0.02 0.22±0.01 0.62±0.01 0.67±0.02 0.60±0.07
Cvt2distilgpt2_MIMIC 1.13±0.04 0.20±0.01 0.42±0.01 0.61±0.02 0.21±0.01 0.61±0.01 0.68±0.02 0.61±0.07
RaDialog 1.09±0.03 0.20±0.01 0.44±0.01 0.54±0.02 0.20±0.01 0.59±0.01 0.59±0.02 0.54±0.07
CheXpertPlus_MIMIC 0.99±0.04 0.18±0.01 0.39±0.01 0.59±0.02 0.17±0.01 0.58±0.01 0.66±0.02 0.62±0.07
BiomedGPT_IU 0.96±0.05 0.14±0.01 0.38±0.01 0.52±0.02 0.21±0.01 0.54±0.01 0.52±0.02 0.54±0.07
CheXpertPlus_CheX 0.92±0.03 0.16±0.01 0.41±0.01 0.49±0.02 0.15±0.01 0.53±0.01 0.54±0.02 0.55±0.07
VLCI_MIMIC 0.91±0.04 0.14±0.01 0.36±0.01 0.48±0.02 0.22±0.01 0.58±0.01 0.47±0.02 0.49±0.08
CheXagent 0.83±0.05 0.12±0.01 0.35±0.01 0.49±0.02 0.14±0.01 0.50±0.01 0.39±0.03 0.57±0.07
GPT4V 0.71±0.03 0.08±0.00 0.27±0.01 0.41±0.02 0.15±0.01 0.52±0.01 0.65±0.03 0.55±0.08
LLM-CXR 0.49±0.02 0.03±0.00 0.19±0.01 0.06±0.01 0.02±0.00 0.28±0.01 0.03±0.01 0.30±0.06
Table 4 | ComprehensiveevaluationofmedicalreportgenerationmodelsontheCheXpertPlusdatasets. Modelsarerankedby
1/RadCliQ-v1. Model evaluation results with 95% confidence intervals (mean ± CI) under normality assumption. The best
resultsforeachmetricareshowninbold.
Model 1/RadCliQ-v1↑ BLEU-2↑ BertScore↑ SembScore↑ RadGraph↑ RaTEScore↑ GREEN↑ 1/FineRadScore↑
Findings+Impression
CheXpertPlus_CheX 0.51±0.07 0.14±0.01 0.02±0.02 0.38±0.03 0.07±0.02 0.49±0.02 0.36±0.05 0.35±0.11
CheXpertPlus_CheX_MIMIC 0.51±0.07 0.14±0.01 0.01±0.02 0.39±0.03 0.07±0.02 0.50±0.02 0.38±0.04 0.36±0.12
MedVersa 0.49±0.06 0.09±0.01 0.01±0.02 0.34±0.03 0.05±0.01 0.45±0.02 0.33±0.05 0.35±0.11
CheXpertPlus_MIMIC 0.48±0.06 0.10±0.01 0.00±0.02 0.32±0.03 0.05±0.01 0.43±0.02 0.29±0.04 0.35±0.11
RadFM 0.44±0.05 0.07±0.01 -0.04±0.02 0.23±0.03 0.03±0.01 0.39±0.02 0.14±0.03 0.34±0.09
GPT4V 0.43±0.05 0.06±0.01 -0.07±0.02 0.21±0.02 0.03±0.01 0.39±0.01 0.18±0.04 0.33±0.10
Findings
CheXpertPlus_CheX_MIMIC 0.81±0.12 0.15±0.03 0.34±0.04 0.40±0.05 0.21±0.03 0.50±0.03 0.27±0.05 0.35±0.18
MAIRA-2 0.79±0.10 0.16±0.03 0.36±0.03 0.35±0.04 0.19±0.03 0.48±0.02 0.27±0.05 0.35±0.18
CheXpertPlus_CheX 0.79±0.10 0.15±0.03 0.34±0.03 0.38±0.04 0.19±0.03 0.49±0.03 0.24±0.05 0.34±0.20
MedVersa 0.72±0.10 0.13±0.02 0.32±0.03 0.34±0.05 0.15±0.02 0.47±0.03 0.24±0.05 0.34±0.18
RaDialog 0.71±0.09 0.13±0.02 0.31±0.03 0.35±0.05 0.14±0.02 0.45±0.02 0.21±0.04 0.33±0.17
RGRG 0.67±0.11 0.15±0.02 0.32±0.04 0.27±0.05 0.14±0.02 0.45±0.03 0.22±0.04 0.34±0.17
CheXpertPlus_MIMIC 0.66±0.10 0.14±0.02 0.29±0.03 0.29±0.05 0.13±0.03 0.43±0.03 0.24±0.05 0.34±0.19
CheXagent 0.64±0.11 0.12±0.02 0.28±0.04 0.27±0.04 0.12±0.02 0.43±0.02 0.18±0.05 0.34±0.19
Cvt2distilgpt2_MIMIC 0.63±0.08 0.12±0.02 0.27±0.03 0.27±0.04 0.12±0.02 0.42±0.03 0.21±0.05 0.35±0.19
VLCI_MIMIC 0.59±0.09 0.12±0.02 0.23±0.03 0.25±0.04 0.10±0.02 0.38±0.03 0.17±0.05 0.33±0.20
Cvt2distilgpt2_IU 0.58±0.11 0.08±0.02 0.27±0.03 0.15±0.05 0.10±0.02 0.38±0.03 0.15±0.05 0.33±0.20
RadFM 0.57±0.08 0.08±0.02 0.23±0.03 0.22±0.04 0.08±0.01 0.40±0.03 0.10±0.03 0.33±0.16
GPT4V 0.57±0.08 0.08±0.01 0.21±0.03 0.23±0.04 0.08±0.02 0.41±0.02 0.15±0.05 0.34±0.20
VLCI_IU 0.56±0.09 0.11±0.02 0.22±0.03 0.17±0.05 0.09±0.02 0.42±0.03 0.19±0.05 0.34±0.20
BiomedGPT_IU 0.55±0.08 0.02±0.01 0.20±0.03 0.24±0.04 0.06±0.01 0.35±0.03 0.12±0.04 0.32±0.18
LLM-CXR 0.52±0.06 0.04±0.01 0.16±0.02 0.21±0.04 0.04±0.01 0.32±0.02 0.02±0.01 0.29±0.13
|9References
[1] Omar Alfarghaly, Rana Khaled, Abeer Elkorany, Maha Helal, and Aly Fahmy. Automated radiology
report generation using conditioned transformers. Informatics in Medicine Unlocked, 24:100557, 2021.
[2] ShruthiBannur, KenzaBouzid, DanielCCastro, AntonSchwaighofer, SamBond-Taylor, MaximilianIlse,
Fernando Pérez-García, Valentina Salvatelli, Harshita Sharma, Felix Meissen, et al. Maira-2: Grounded
radiology report generation. arXiv preprint arXiv:2406.04449, 2024.
[3] RJM Bruls and RM Kwee. Workload for radiologists during on-call hours: dramatic increase in the past
15 years. Insights into imaging, 11:1–7, 2020.
[4] Pierre Chambon, Jean-Benoit Delbrouck, Thomas Sounack, Shih-Cheng Huang, Zhihong Chen, Maya
Varma, Steven QH Truong, Chu The Chuong, and Curtis P Langlotz. Chexpert plus: Hundreds of
thousands of aligned radiology texts, images and patients. arXiv preprint arXiv:2405.19538, 2024.
[5] Weixing Chen, Yang Liu, Ce Wang, Jiarui Zhu, Shen Zhao, Guanbin Li, Cheng-Lin Liu, and Liang Lin.
Cross-modal causal intervention for medical report generation. arXiv preprint arXiv:2303.09117, 2023.
[6] Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. Generating radiology reports via memory-
driven transformer. arXiv preprint arXiv:2010.16056, 2020.
[7] Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave
Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, et al.
Chexagent: Towards a foundation model for chest x-ray interpretation. arXiv preprint arXiv:2401.12208,
2024.
[8] DinaDemner-Fushman,MarcDKohli,MarcBRosenman,SonyaEShooshan,LaritzaRodriguez,Sameer
Antani, George R Thoma, and Clement J McDonald. Preparing a collection of radiology examinations
for distribution and retrieval. Journal of the American Medical Informatics Association, 23(2):304–310,
2016.
[9] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. Imagenet: Alarge-scalehierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.
Ieee, 2009.
[10] Alyssa Huang, Oishi Banerjee, Kay Wu, Eduardo Pontes Reis, and Pranav Rajpurkar. Fineradscore:
A radiology report line-by-line evaluation technique generating corrections with severity scores. arXiv
preprint arXiv:2405.20613, 2024.
[11] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du Nguyen Duong, Tan Bui, Pierre
Chambon, Yuhao Zhang, Matthew P Lungren, Andrew Y Ng, et al. Radgraph: Extracting clinical
entities and relations from radiology reports. arXiv preprint arXiv:2106.14463, 2021.
[12] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren,
Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, ade-identified publicly available database
of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019.
[13] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidi-
rectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1, page 2,
2019.
[14] Suhyeon Lee, Won Jun Kim, Jinho Chang, and Jong Chul Ye. Llm-cxr: Instruction-finetuned llm for cxr
image understanding and generation. arXiv preprint arXiv:2305.11490, 2023.
[15] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang,
LiDong,etal. Swintransformerv2: Scalingupcapacityandresolution. InProceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pages 12009–12019, 2022.
[16] Aaron Nicolson, Jason Dowling, and Bevan Koopman. Improving chest x-ray report generation by
leveraging warm starting. Artificial intelligence in medicine, 144:102633, 2023.
[17] Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen,
ArneEdwardMichalson,MichaelMoseley,CurtisLanglotz,AkshaySChaudhari,etal. Green: Generative
radiology report evaluation and error notation. arXiv preprint arXiv:2405.03595, 2024.
|10[18] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu. Bleu: amethodforautomaticevaluation
of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational
Linguistics, pages 311–318, 2002.
[19] Chantal Pellegrini, Ege Özsoy, Benjamin Busam, Nassir Navab, and Matthias Keicher. Radialog: A
large vision-language model for radiology report generation and conversational assistance. arXiv preprint
arXiv:2311.18681, 2023.
[20] Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Y Ng, and Matthew P Lungren.
Chexbert: combining automatic labelers and expert annotations for accurate radiology report labeling
using bert. arXiv preprint arXiv:2004.09167, 2020.
[21] Tim Tanida, Philip Müller, Georgios Kaissis, and Daniel Rueckert. Interactive and explainable region-
guided radiology report generation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 7433–7442, 2023.
[22] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation
model for radiology. arXiv preprint arXiv:2308.02463, 2023.
[23] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:
Introducingconvolutionstovisiontransformers. InProceedingsoftheIEEE/CVFinternationalconference
on computer vision, pages 22–31, 2021.
[24] Joy T Wu, Nkechinyere N Agu, Ismini Lourentzou, Arjun Sharma, Joseph A Paguio, Jasper S Yao,
Edward C Dee, William Mitchell, Satyananda Kashyap, Andrea Giovannini, et al. Chest imagenome
dataset for clinical reasoning. arXiv preprint arXiv:2108.00316, 2021.
[25] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.
Thedawnoflmms: Preliminaryexplorationswithgpt-4v(ision). arXiv preprint arXiv:2309.17421,9(1):1,
2023.
[26] Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser
Ururahy Nunes Fonseca, Henrique Min Ho Lee, Zahra Shakeri Hossein Abad, Andrew Y Ng, et al.
Evaluating progress in automatic chest x-ray radiology report generation. Patterns, 4(9), 2023.
[27] Kai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun Chen,
Brian D Davison, Hui Ren, et al. A generalist vision–language foundation model for diverse biomedical
tasks. Nature Medicine, pages 1–13, 2024.
[28] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating
text generation with bert. arXiv preprint arXiv:1904.09675, 2019.
[29] Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Ratescore: A metric
for radiology report generation. medRxiv, pages 2024–06, 2024.
[30] Hong-Yu Zhou, Subathra Adithan, Julián Nicolás Acosta, Eric J Topol, and Pranav Rajpurkar. A
generalist learner for multifaceted medical image interpretation. arXiv preprint arXiv:2405.07988, 2024.
|11