Synthesising Robust Controllers for Robot Collectives
with Recurrent Tasks: A Case Study
TillSchnittkaandMarioGleirscher
UniversityofBremen,Germany
{schnitti,gleirsch}@uni-bremen.de
Whendesigningcorrect-by-constructioncontrollersforautonomouscollectives,threekeychallenges
arethetaskspecification, themodelling, anditsuseatpracticalscale. Inthispaper, wefocusona
simpleyetusefulabstractionforhigh-levelcontrollersynthesisforrobotcollectiveswithoptimisation
goals(e.g.,maximumcleanliness,minimumenergyconsumption)andrecurrence(e.g.,re-establish
contaminationandchargethresholds)andsafety(e.g.,avoidfulldischarge,mutuallyexclusiveroom
occupation) constraints. Due to technical limitations (related to scalability and using constraints
in the synthesis), we simplify our graph-based setting from a stochastic two-player game into a
single-playergameonapartiallyobservableMarkovdecisionprocess(POMDP).Robustnessagainst
environmental uncertainty is encoded via partial observability. Linear-time correctness properties
are verified separately after synthesising the POMDP strategy. We contribute at-scale guidance on
POMDPmodellingandcontrollersynthesisfortaskedrobotcollectivesexemplifiedbythescenario
ofbattery-drivenrobotsresponsibleforcleaningpublicbuildingswithutilisationconstraints.
1 Introduction
Hygiene in public buildings has been hotly debated ever since the increased safety requirements during
the coronavirus pandemic. Autonomous robot collectives can help to relieve cleaning staff and keep
highly frequented buildings (e.g., hospitals, schools) clean. However, commercially available solutions
havetheirlimitations,forexample,aneedformanualtaskprogrammingoralackofonlineadaptability.
In contrast to domestic homes, there are strict regulations (e.g., [8] for schools) that stipulate which
areas must be cleaned and at what intervals. Changing operational conditions (e.g., room occupation,
equipment reconfiguration, cleaning profile, regulations) create the need for an automatic generation of
cleaning schedules for robot collectives and for proving their compliance with hygiene requirements.
Thisscenarioisaninstanceofamulti-facetedrecurrentschedulingproblemdiscussedbelow.
CleaningBuildingsasaRunningExample. Whenplanningthecleaningofabuilding(e.g.,aschool),
we can use its layout in the form of a room plan (Figure 1a). Apart from a set of m rooms R =
{R ,...,R }withanassignedarea,suchplansincludetheconnectionsbetweenroomsandthelocations
1 m
ofnchargingstationsC={C ,...,C }. Acollectiveofk≤nrobotsB={B ,...,B }isresponsiblefor
1 n 1 k
cleaning R. B is tasked to keep R clean while charging its batteries usingC. Each robot has a limited
batterysizeandachargingpointinC asanassignedrestingposition. Additionally,thereisaroomutil-
isation plan (Figure 1b) containing the times when the rooms are in use, which can change on a daily
basis. Whilearoomisinuse,nocleaningrobotmaybeinsideand,thus,cannotcleanit.
The task is to create a cleaning strategy (or schedule) for B, constrained by the utilisation plan and
charging needs. Moreover, this strategy should keep the rooms clean enough, while minimising battery
consumption. To specify this task adequately, we define cleanliness in terms of contamination. Since
MattLuckcuckandMengweiXu(Eds.):
©TillSchnittkaandMarioGleirscher
SixthInternationalWorkshoponFormalMethods
Thisworkislicensedunderthe
forAutonomousSystems(FMAS2024)
CreativeCommonsAttributionLicense.
EPTCS411,2024,pp.109–125,doi:10.4204/EPTCS.411.7110 RobustlyRecurrentStrategiesforRobotCollectives
Room Timeofuse
R 8:00–10:00
1
R 12:00–14:00
2
R 10:00–12:00
3
R 15:00–16:00
4
(a)Roomplan (b)Roomutilisationplan
Figure1: Examplesofaroomplanandtheper-roomutilisation
wearedealingwithfloorcleaningrobots,welimitourdefinitiontofloorsurfaces. Forexample,hygiene
guidelines for schools [8] and the associated standard DIN77400 [3] recommend cleaning intervals for
floorsurfaces. Wereflectthisrecommendationinourdefinitionbycontaminationratesandthresholds.
Each room has a certain contamination rate. Over time, total contamination accumulates in a room and
canberesetbycleaning. Clearly,thetotalcontaminationofaroomshouldnotexceedacertainthreshold.
Approach. Weproposeaquantitativestochasticapproachtosynthesisestrategiesforrobotcollectives
withrecurrenttasks,suchthatthestrategiesarerobustly(i.e.,underuncertainty)complianttorecurrence
and safety constraints and optimisation goals. We consider (i) weighted stochastic models and strategy
synthesis for (ii) optimally coordinating robot collectives while (iii) providing guarantees (e.g., recur-
rence, safety) on the resulting strategies (iv) under uncertainty (e.g., partial observability). We select
POMDPs,ageneralisationofMarkovdecisionprocesses(MDPs),forourproblem.
Related Work. Among the works employing POMDPs for optimal planning, Macindoe et al. [11]
show strategy synthesis for human-robot cooperative pursuit games with robots and humans acting in
turns. Moreover, Thomas et al. [16] combine PDDL-based high-level task scheduling and POMDP-
based low-level navigation. They use a Kalman filter to predict the distribution of the POMDP’s belief
statebasedondiscretisedrobotdynamics.
UsingthePRISMmodelchecker,Giaquintaetal.[4]showthesynthesisofminimal-energystrategies
forrobotsfindingfixedobjects. Objectfindingasaninstanceofnavigationcanbesolvedwithmemory-
less strategies, that is, functions of the current state (here, positions of robot and object). Lacerda et
al. [9] propose multi-objective synthesis of MDP policies satisfying bounded co-safe LTL properties
using PRISM. Illustrated by a care robot, they employ a timed MDP and filter irrelevant transitions and
states, resulting in a reduced MDP where time as a state variable preserves bounded properties. Basile
etal.[2]usestochasticpricedtimedgamesandreinforcementlearning(RL)-enhancedstatisticalmodel
checking(withUPPAALStratego)tosynthesisesafe,goal-reaching,andminimal-arrival-timestrategies
forasingleautonomoustrainoperatedundermoving-blocksignalling.
El Mqirmi et al. [12] combine multi-agent RL and verification to coordinate robot collectives. An
abstractMDPisgeneratedfromexpertknowledgeforoptimalsynthesisofajointabstractstrategy(using
PRISM, STORM) under PCTL (safety) constraints. RL identifies a concrete strategy within these con-
straintsbyusingshielding(i.e.,onlychoosingactionscompliantwiththeabstractstrategy). Guetal.[7]
tackle state space reduction via RL to synthesise optimal navigation and task schedules for collectives
(e.g., a quarry with autonomous vehicles) and timed games (in a UPPAAL Stratego extension) to check
timedCTLproperties(e.g.,liveness,safety,reachability)ofthesynthesisedstrategies.TillSchnittkaandMarioGleirscher 111
Vázquezetal.[17]developedadomain-specificlanguage(DSL)forspecifyingtasksforcollectives.
Task allocation constraints are solved by ALLOY and plain MDPs are employed (via PRISM, EVOC-
HECKER)tosynthesisegoal-reaching,minimum-travel-timeschedules.
Contributions. Our approach enhances works in optimal planning [11, 16] by a step of strategy veri-
ficationagainststochastictemporalproperties. Objectfinding[4]correspondstoareachabilityproperty,
whereascontinuouscontaminationanditsremovaltoaresponseproperty. Moreover,objectfindingdif-
fersfromrecurrentschedulinginthatitisstaticandcanberealisedwithasimplerrewardfunctionanda
smallerstatespace. Ourprobleminvolvesadynamicgoalwithrobotsoperating24hoursaday, having
tocoordinatetheirworkcontinuously. Beyondboundedco-safeLTL[9],ourapproachsupportsbounded
responsepropertiesG(φ →F≤Tψ). Theaboveworks[9,2,12,7]underpintheusefulnessofstochastic
abstractionsforsynthesisinvariousdomains. OuruseofPOMDPstohidepartsofthestochasticprocess
(e.g., contamination) and explicit concurrency (e.g., for many simultaneous robot movements) offers
an alternative to obtaining small models for multi-agent synthesis under limited resources (e.g., time
constraints to clean rooms). Additionally, we argue how the model of our case study—a collection of
cleaningrobotssubjectedto hygienerequirements—, whilekeptsimplefor illustrativepurposes, scales
and generalises to a range of similar scenarios in other application domains. Apart from our focus on
recurrenceandmodelreduction,aDSL[17]canwrapourapproachintoapracticalworkflow.
Overview. AftergivingkeydefinitionsinSection2,wepresentourapproachinSection3. IntheSec-
tions3.1to3.3, weexplainthemodelling, inSection3.4thetreatmentofcollectives, inSection3.5the
constrainedPOMDPsynthesisproblem, and, intheSections3.6and3.7, theextractionandverification
of a strategy. We evaluate our approach in Section 4, discuss issues we encountered during modelling
andsynthesisinSection5,andaddconcludingremarksinSection6.
2 Preliminaries
Stochasticmodellingisaboutdescribinguncertainreal-worldbehaviourintermsofstatesandprobabil-
istic actions producing transitions between these states. For stochastic reasoning (i.e., drawing conclu-
sionsaboutsuchbehaviour),weuseprobabilisticmodelchecking. Thissectionintroducesthestochastic
models,temporallogic,andtoolsweemployforsynthesisandverification.
PartiallyObservableMarkovDecisionProcesses. LetDist(X)denotethesetofdiscreteprobability
distributionsoverasetX,andR
bethenon-negativerealnumbers. Then,aPOMDP[13]isgivenby
≥0
Definition2.1. APOMDPisatupleM=(S,s,A,P,R,O,obs),where
• Sisasetofstateswiths∈Sbeingtheinitialstate,
• Aisasetofactions(oractionlabels),
• P: S×A→Dist(S)isa(partial)probabilistictransitionfunction,
• R=(R ,R )isastructuredefiningstateandactionrewardsR : S→R andR : S×A→R ,
S A S ≥0 A ≥0
• O isafinitesetofobservations,and
• obs:S→O isalabellingofstateswithobservations.112 RobustlyRecurrentStrategiesforRobotCollectives
Moreover, A(s)={a∈A|P(s,a) isdefined} describes the actions available in s. A path in M is
definedasafiniteorinfinitesequenceπ =s
→a0
s
→a1
...wheres ∈S,a ∈A(s),andP(s,a)(s )>0
0 1 i i i i i i+1
for all i∈N . Let last(π)be the last state of π. FPaths and IPaths denote all finite and infinite paths
M M
ofM startingatstates. Non-determinisminM isresolvedthroughastrategyaccordingto
Definition2.2(POMDPStrategy). AstrategyforaPOMDPM isamapσ :FPaths →Dist(A),where
M
• foranyπ ∈FPaths ,wehaveσ(π)(a)>0onlyifa∈A(last(π)),and
M
• foranypathπ =s →a0 s →a1 ...andπ′=s′ →a′ 0 s′ →a′ 1 ...satisfyingobs(s)=obs(s′)anda =a′ for
0 1 0 1 i i i i
alli,wehaveσ(π)=σ(π′).
Wecallσ memorylessifσ’schoicesonlydependonthemostrecentstate(last(π)),anddeterministic
ifσ alwaysselectsanactionwithprobability1. Below,weconsidermemorylessdeterministicstrategies.
Probabilistic Linear Temporal Logic (PLTL). A POMDP M describes a stochastic process, such
thateverypossibleexecutionofthatprocesscorrespondstoapaththroughM’stransitiongraph. Todraw
qualitative conclusions about M and its associated strategies, we express properties in linear temporal
logic(LTL).AnLTLformulaφ overatomicpropositionsAPfollowsthegrammar
φ ::=ap|¬φ |φ∧φ |Xφ |φ Uφ (1)
where ap∈AP. In LTL, we make statements about M’s path structure and specify admissible sets of
paths. Informally,Xφ describesthatφ holdsinthenextstateofagivenpath,andφ Uψ describesthatφ
holdsuntilψ occurs,orglobally,ifψ neveroccurs. WeallowtheabbreviationsFφ ≡TUφ,describing
thatφ holdsatsomepointonapath,andGφ ≡¬F¬φ,describingthatφ appliestotheentirepath.
TodrawquantitativeconclusionsaboutM (orqueryprobabilitiesandrewards),weuseprobabilistic
LTL(PLTL),whoseformulasφ areformedalong(1)andbythetwooperatorsPandR:
• P[min|max]∼p|=?[ψ] describes that the [minimum|maximum] probability of ψ (under all possible
POMDPstrategies)beingvalidis∼ p,and
• RR [ψ] expresses that the [minimum|maximum] expected reward R associated with ψ
[min|max]∼r|=?
(underallpossiblePOMDPstrategies)meetsthebound∼r,
whereψ isanLTLformula, ∼∈{<,≤,=,≥,>}, and=?isusedforqueries. Givenatimert inM and
thatallactionsinAincrementt by1,weallowF∼Tψ ≡F(t ∼T∧ψ). InLTL,M|=φ expressesthatall
pathsofMfromsarepermittedbyφ,and,inPLTL,thataprobabilitymeasureoverM’spathssatisfiesφ.
Forconvenience,weuseφ torefertoboth,theexpressionandtheregioninSwhereitevaluatestotrue.
LTLandPLTL’ssemanticsareexplainedindetailin,forexample,[1,p.231andSec.6.2].
The PRISM Model Checker can check M |=φ using exact and approximate algorithms [14]. It sup-
portsavarietyofstochasticmodelsandlogicsandhasitsownlanguagesformodellingandforspecifying
properties. In PRISM, a reward structure R is used as a parameter in RR[·]. For POMDPs, PRISM can
synthesisestrategiesintheformofDefinition2.2.
A PRISM model consists of a series of modules, each defining a fraction of a state s using its own
variables. Modules can synchronise their transitions by sharing labels from A, such that transitions in
several modules using the same label can only switch in a state s∈S if each transition is enabled in s.
AnexamplemodulewithstatevariablexandcommandincreaseisgiveninListing1.TillSchnittkaandMarioGleirscher 113
Action Controller Controller Verified
World
specification POMDPM Synthesis POMDPstrategyσ Verification strategyσ
Modelling
usingB(M) usingMσ
Regulations (Secs.3.1to3.4) Arena(graph,3.1), Taskschedule Taskschedule
(Section3.5) (Secs.3.6and3.7)
(e.g.DIN77400), states&actions forcollective satisfyingTable2
Table1 (CPN/automata,3.2), Strategyspecification
c(
orp
e
llwe en caa
tr
il
d
vti ese ts
(r
Cu&
c
Ptc N.,o ,3s 3.t 3s
.4),
)
inPLTL(Table2) C2 1110 R5 129 R4
132024
1223819
514
R12162135 6C 11 7718124
R2
R3
Figure2: Overviewoftheproposedsynthesisapproachforrobotcollectives
1module Example
2 x : [0..2] init 0; // state variable x with range {0,1,2} and initial value 0
3 [increase] x<2 -> (x‘=x+1); // command increase increasing x by 1 if x<2
4endmodule
Listing1: Exampleofamodulein PRISM’sguardedcommandlanguage
Fixed-Grid Approximation in PRISM. When analysing a POMDP M, PRISM computes an approx-
imate(finite-state)belief-MDPB(M)[13],eachbelief beingaprobabilitydistributionoverthepartially
observable states (e.g., the possible room contamination). The size of B(M)’s state space, called belief
space [10], is exponential in PRISM’s grid-resolution parameter g controlling the approximation of the
upperandlowerboundstobedeterminedforP|Rmin|max-propertiesusingB(M).
3 Developing Controllers by Example of the Cleaning Scenario
Inthissection,wefirststateoursynthesisproblemandthendescribeourapproachtostrategysynthesis
andverificationasillustratedinFigure2.
ProblemStatement. OuraimtosynthesiseacollectivecontrollerisspecifiedintheLTLproperty
reachtaskgoal keepsafe
(cid:122) (cid:125)(cid:124) (cid:123) (cid:122) (cid:125)(cid:124) (cid:123)
G(cid:0)
(ω
→F≤T(ω∧φr))∧G≤Tφs(cid:1)
, (2)
(cid:124) (cid:123)(cid:122) (cid:125)
periodicallyachievetasksafely(impliedbyourapproach)
where ω is the recurrence area (including s and acting as a task invariant), φr specifies an invariant-
narrowingcondition,1 φs specifiestasksafety,andT >1istherecurrenceinterval(anuppercycle-time
bound). Among the controllers satisfying (2), we look for an optimal (e.g., one minimising energy
consumption)androbust(e.g.,underpartialobservabilityofstochasticroomcontamination)one.
OverviewofControllerDevelopment. First,aspatio-temporalabstractionofthecleaningscenariois
modelledusingacolouredPetrinet(CPN)forcoordinationmodellingandfiniteautomatafordescribing
robot-local behaviour. These aspects are translated into a reward-enhanced POMDP M (Sections 3.1
to 3.3) in support of multiple robots (Section 3.4), which uses probabilistic actions to reduce the state
count of a hypothetical detailed model. Then, a strategy σ is synthesised for M (Section 3.5), which is
used to derive a deterministic, non-probabilistic, and integer-valued model Mσ (Section 3.6). Finally,
1φronlyhasmethodologicalrelevance.Itcould,forexample,beusedtodevelopanincreasinglystronginvariantω.114 RobustlyRecurrentStrategiesforRobotCollectives
C 1 ℛ1 ℛ2 ℛ4 ℛ5 C 2
ℛ3
Figure3: Exampleofaroomplangraph
Table1: Requirementsofthecleaningscenariodefiningtheatactionimplementedinthreemodules
Id. ActionSpecification(BehaviouralRequirement) Impl.inModule
Ba Theroboteitherstaysintheroomitiscurrentlyinormovestoanotherroom,whereby cleaner
theremustbeanedgeintheroomplangraphbetweenthecurrentandnextroom.
Bb Iftherobotisonachargingstation,itsbatterychargelevelisincreasedbythechar-
gingrateoftherobot.
Bc If the robot is not on a charging station, its battery charge level is reduced by the
robot’sdischargerate.
Da Thetotalcontaminationofeachroomincreasesbyitscontaminationrateifthereis contamination
norobotinthatroom.
Db Iftherobotisinaroom,thetotalcontaminationofthatroomisreset.
Ta Thetimecounterisincrementedbyone. time
Mσ, representing the high-level controller, is verified (Section 3.7) against strategy requirements that,
duetocurrentlimitationsintheformalismsandtools,cannotbecheckeddirectlyduringsynthesis.
3.1 Spatio-temporalAbstraction
State Space. For the implementation of the problem (e.g., cleaning task), it is important to keep the
numberofstatesassmallaspossible. Therefore,largepartsoftheinitialproblemareabstracted.
Insteadofacompleteroomplanwithareaassignment,theabstractedenvironment usesagraphthat
only contains the different rooms and charging stations. An example of such a graph for the room plan
inFigure1acanbeseeninFigure3. ApointerB.x,i∈0..k,toaroomorchargingstationinthisgraph
i
is used to keep track of the position of robot B. The behaviour of the charging state B.c of the robot’s
i i
batteryisdescribedbyanumberofdiscretecharginglevelsandchargeanddischargerates.
The total contamination is represented by a counter R .d, j ∈0..m, whose maximum value is the
j
contamination threshold R .threshold. Since we want to avoid reaching a state with the contamination
j
atthisthreshold,itisnotnecessarytomodelcontaminationbeyondR .threshold.
j
Actions and High-level Behaviour. Discrete values are used to model time as well, where the action
at,asspecifiedinTable1anddescribedbelow,isperformedateachdiscretetimestep.
TheCPNinFigure4aprovidesahigh-leveldescriptionofthemovesofthecollectiveBacrosschar-
gingstationsCandroomsR(Figure3). Theabstractataction(blackbar)expandstoarangeofconcrete
POMDP actionsatj ...j with j ∈0..m. Wheneveratis taken, any number oftokens (black dots,
B1 Bk Bi
representingrobots)ontheplaces(greycircles,representingroomsandchargingstations)canflowsim-TillSchnittkaandMarioGleirscher 115
B
C 1 1 R 1 CleanerB i atj Bi
C
2
R
2
B
2 charging
(movetoR j)
cleaning(R j)
. .
. .
. . at0
C n (atjB1a .t ..jBk) R m B k at0 Bi (idle) (moveB toi C i) atj Bi (movetoR j)
(a)Coordinationofk≤ncleaners (b)ControlofcleanerB
i
Figure4: Cleanercoordination(a)asaCPNandlocalcontrol(b)asafiniteautomaton
ultaneously, such that B can move from one place via at to an adjacent empty (possibly same) place.2
i
Figure 4b outlines the control of a particular robot B. When composed (in parallel), as formalised in
i
M byimplicitconstraintsonthe j-indices,simultaneousmovesofseveralrobotsintoasingleplaceand
jumpstonon-adjacentplacesareprohibitedbythecoordinationconstraintinFigure4a.3
3.2 QuantitativeandStochasticAbstraction
Forthesakeofsimplicity,thissectionandthefollowingwillfocusonareducedproblemwithonlyone
robot. ThecaseofmultiplerobotswillbereintroducedinSection3.4.
As already mentioned, the PRISM-encoding of M is divided into three modules operating on three
independentfragmentsofthestatespaceS: Thestateoftherobots,theroomcontamination,andtime.
Thecleanermodule describes the behaviour of cleaning robot B . An integer is used to model the
1
robot position B .x. For this, each room and charging station is mapped to an integer bijectively.
1
Forexample,theroomgraphinFigure3canbedescribedbythefollowingrelation:C →0,R →
0 1
1,R →3,R →4,R →5,R →6,C →7. ThebatterystatusB .cisalsodescribedasaninteger
2 3 4 5 2 1
whoseupperboundisthemaximumchargeB .maxchargeofB ’sbattery,seeListing2.
1 1
1module cleaners
2 x : [0..m−1] init B 1.start; // position of cleaner B 1
3 c : [0..B 1.maxcharge] init B 1.ωchgthres; // battery status of B 1
4 [at0] (x=0|x=1)
5 -> (x’=0) & (c=min(c+B 1.chargerate, B 1.maxcharge)); // charge B 1
6 [at1] (x=0|x=1|x=2|x=3|x=4)
7 -> (x’=1) & (c=max(c-B 1.dischargerate, 0)); // move to R1
8 ...
9endmodule
Listing2: Amodelfragmentofthecleanersmodulehighlightingitsstatesandactions
Foreachchargingstationandeachroomthereisatransition atN (where N istheintegerassigned
to the room), which models entering or staying in this room. The precondition for this transition
is that the robot must already be in that room or a neighbouring room. If a robot enters or stays
atachargingstation, thechargeincreasesbythechargingrate; ifarobotisinaroom, thecharge
decreasesbythedischargerate,seelines5and7respectively.
Thecontaminationmodule describesthecontaminationstatusofR. Toreducethenumberofstates,
contaminationismodelledbybooleans—thecontaminationflagsR .d, j∈1..m—ratherthanin-
j
2Weassumeforanyinitialstates∈Sthatnomorethanonerobotisataparticularplace.
3ThisconstructionreducesSandPofMincomparisonwithusingalphabetisedsynchronouscomposition.116 RobustlyRecurrentStrategiesforRobotCollectives
tegers. The probability R .pr of R .d getting true is used to model the state in which the con-
j j
taminationofthecorrespondingroomhasreacheditsthresholdR .threshold. Ifthereisnorobot
j
in R , we set R .d = true with probability R .pr inversely proportional to the contamination
j j j
thresholdR .threshold. IfarobotvisitsorstaysinR thenR .d issetto false,seeListing3.
j j j
1module contamination // sequential stochastic contamination
2 R1.d : boolean init false; R2.d : boolean init false;
3 ...
4 [at0] true -> 1−(∑ i∈1..mRi.pr): true
5 + R1.pr: (R1.d’=true) + R2.pr: (R2.d’=true) + ...; // prob. contam. all while charg.
6 [at1] true -> 1−(∑ i∈1..m\1Ri.pr): (R1.d’=false)
7 + R2.pr: (R1.d’=false)&(R2.d’=true) + ...; // clean R1 and prob. cont. other rooms
8 [at2] true -> 1−(∑ i∈1..m\2Ri.pr): (R2.d’=false)
9 + R1.pr: (R2.d’=false)&(R1.d’=true) + ...; // clean R2 and prob. cont. other rooms
10 ...
11endmodule
Listing3: Afragmentofthecontaminationmodule(e.g.R.pr=0.05fori∈1..m)
i
Thetimemodule describes the progression of time and manages the switching to the error and final
state (the model handles both the same). time also restricts the atN transitions so that they can
onlybeusedaslongasthemodelisnotintheerrororfinalstate. Thisispossiblebecauseatrans-
itioncanonlytriggerifitcantriggerineachmodule. Therefore,preconditioninthetimemodule
canpreventatransitionfromtriggeringeventhoughitismarkedwith true inthecleanerand
contaminationmodules. ThetimeisincreasedbyoneunitwitheachtransitionuntilitreachesT.
Atthispoint(duetothedefinitionof error_or_final),onlythe fin transitioncanswitchand
themodelendsinaloop,seeListing4.
1formula error_or_final = (c=0|!(t<T));
2module time
3 t : [0..T] init 0;
4 [at0] !error_or_final -> (t’=min(t+1,T)); // charge
5 [at1] !error_or_final -> (t’=min(t+1,T)); // move to R1
6 [fin] error_or_final -> (t’=min(t+1,T)); // finish cycle
7endmodule
Listing4: Afragmentofthetimemodule
3.3 ChoiceoftheRewardFunction
Fourrequirements,avalidstrategymustsatisfy,canbederivedfromFormula(2)andTable1:
FR AttimeT,allrobotsmustbebackintheirinitiallocation,sothattheplancanberepeated.
ωC AttimeT,thebatteryofarobotmustnotbelowerthanitsthresholdchargelevel.
BC Thebatteryofarobotmustneverbeempty.
CT Thetotalcontaminationofanyroommustneverexceeditscontaminationthreshold.
When just focusing MDP verification rather than synthesis, it would be sufficient to describe these
as PLTL constraints. However, PLTL constraints cannot be used as queries for synthesising strategies,
sincegeneratingstrategiesthroughPRISMrequireseachpathtobeabletofulfilallconstraintseventually.TillSchnittkaandMarioGleirscher 117
Using constraints that are violated on some of M’s paths (e.g., if we require BC, any path leading to an
emptybatteryeventuallyviolatesBC)willprevent PRISM fromfindingareward-optimalstrategy. This
is a specific known limitation of the used formalism. Even though we cannot use PLTL constraints to
encodeallourrequirements,therewardstructureRcanbeusedtoprioritiseselectingstrategiesthatfulfil
theserequirements. TheencodingofourrequirementsinRcanbeachievedbypenalisingstatesthatdo
notfulfilsomeoralloftherequirements,seeListing5.
1const a_lot = 10000000;
2const a_bit = 10000;
3rewards "penalties"
4 c=0: a_lot; // BC: battery empty
5 t=T & (x!=B 1.start|c<B 1.ωchgthres): a_lot; // FR: robot not at initial loc. at time T
6 R1.d=true: a_bit; // Room 1’s contamination flag is set
7 R2.d=true: a_bit; // Room 2’s contamination flag is set
8 ...
9endrewards
Listing5: Anexampleoftherewardstructureforthestatepenalities
We previously found it ineffective to penalise the contamination flags the same as the constraints
FR, ωC, and BC. Whereas the latter can be determined from M’s state, contamination flags only carry
the probability of a requirement being violated. Hence, we apply lower penalties to the contamination
flags. Further reward structures are used to model optimisation goals, such as energy consumption, see
Listing6a. However,thepenaltyforconstraintsischosensuchthatitisnotpossibletooffsetthepenalty
ofaninvalidstatebythereducedpenaltyforalessenergy-consumingstrategy.
1rewards "energy consumption" 1const a_lot = 10000000;
2 t < T: B 1.maxcharge - c; 2rewards "utilisation"
3endrewards 3 x=2 & t>=8 & t<10: a_lot;
4 x=2 & t>=12 & t<14: a_lot; ...
(a)Afragmentoftheoptimisationrewards 5endrewards
(b)Afragmentofroomutilisationrewards
Listing6: Fragmentsoftherewardstructureusedforthecleaningscenario
Room Utilisation. As with the other requirements, a room utilisation profile to be respected by the
cleaners(UT)canbesoftlyspecifiedusingadditionalrewardfunctionsasshowninListing6b. However,
UTasasafetypropertywilllateralsobespecifiedinPLTLandcheckedofthesynthesisedstrategy.
3.4 CooperationbetweenMultipleRobots
1x
1
: [0..m] init B 1.start;
To keep M simple, robots are not modelled as sep- 2x 2 : [0..m] init B 2.start;
arate modules, but the state of the cleaner module is
3...
4c
1
: [0..B 1.maxcharge] init B 1.maxcharge;
extended to include the positions of all robots (see Fig-
5c
2
: [0..B 2.maxcharge] init B 2.maxcharge;
ure4a). Thissimplificationexcludesalltransitionsfrom 6...
themodelthatwouldleadtoconflictsinrobotbehaviour
Figure5: Thestructureofthecleanerstate
(e.g.,thecasewhereseveralrobotscleanthesameroom
atthesametime). Additionally,eachrobothasitsownbatterycharge,seeListing5.118 RobustlyRecurrentStrategiesforRobotCollectives
The atN actionsforasinglerobotarenowextendedto atN_N_... actions,whichthenmodelthe
simultaneousmovementofkrobots,asillustratedinFigure4aandimplementedinListing7.
1[at0_1] !error & (x 1=0|x 1=1) & (x 2=0|x 2=1|x 2=2|x 2=3|x 2=4) // charge B 1 and move B 2 to R1
2 -> (x 1’=0) & (x 2’=1)
3 & (c 1’=min(c 1+B 1.chargerate,B 1.maxcharge))
4 & (c 2’=max(c 2-B 2.dischargerate,0));
5[at1_2] !error & (x 1=0|x 1=1|x 1=2|x 1=3|x 1=4) & (x 2=1|x 2=2) // move B 1 to R1 and B 2 to R2
6 -> (x 1’=1) & (x 2’=2)
7 & (c 1’=max(c 1-B 1.dischargerate,0))
8 & (c 2’=max(c 2-B 2.dischargerate,0));
9...
Listing7: Twoexamplesof atN_N actions
This solution increases the number of states per robot considerably, but the complexity of M is still
withinapracticallyverifiablerange. Additionally,therewardstructuresthatdependonthepositionand
chargeofarobotareadaptedtoincludethepositionofallrobots,ascanbeseeninListing8.
1rewards "penalties" 10rewards "utilisation"
2 c 1=0: a_lot; 11 x 1=2 & t>=8 & t<10: a_lot;
3 c 2=0: a_lot; 12 x 2=2 & t>=8 & t<10: a_lot;
4 ... 13 x 1=2 & t>=12 & t<14: a_lot;
5 t=T & (x 1!=B 1.start|c 1<B 1.ωchgthres): a_lot; 14 x 2=2 & t>=12 & t<14: a_lot;
6 t=T & (x 2!=B 2.start|c 2<B 2.ωchgthres): a_lot; 15 ...
7 ... 16endrewards
8endrewards
Listing8: Rewardstructureforacollective
9
3.5 SynthesisingStrategies(underUncertainty)fortheCleaningScenario
PRISM’s POMDP strategy synthesis works under certain limitations. As indicated in Section 3.3, it is
not possible to use RR min=?[ψ] for synthesis if Pmin=?[ψ] < 1, that is, if M contains ψ-violating paths
under some strategy σ. Hence, we choose a ψ that defines a state that all paths converge at, and syn-
thesiseastrategythatminimisesthetotalreward(sincewemodelRusingpenalties)uptothatpoint. A
commonalityofallpathsistheflowoftime,sothereachabilityreward-basedsynthesisquery
∑ R∈{penalties,energyconsumption,utilisation}RR min=?[Ft =T] (3)
usesatargetstatewheretimet isequaltosomemaximumtimeT.
Additionally, amappingobsneedstobespecified, whichdefinestheobserva-
tions of M that σ can use to make choices. In this case, σ cannot use the con-
1observables
tamination flags to make its choices. If σ could consider the contamination flag,
2 t,
it would not need to account for the accumulative probability; σ could just check 3 x 1, x 2, ...,
ifacontaminationflagistrueandactaccordingly. Wecanhidethecontamination 4 c 1, c 2, ...
flags from σ by defining obs to just include the position and charge of the robot
5endobservables
andthetime,seethelistingontheright.
PRISM allows the explicit generation of deterministic strategies. Such strategies are useful in this
case,since,exceptforthefinalstate,themodelhasnoloops(i.e.,timeisalwaysadvancing). Becauseσ
isdeterministic,itcanbethoughtofasalistofactionsforeachtimestepofthecleaningschedule,where
the transition of each step of the strategy denotes the action of the robot at that time step within period
T. NotethattheobservableenvironmentalpartofM isdeterministicsuchthatafterapplyingσ,Mσ has
exactlyonepath,hence,σ onlydependsontimet.TillSchnittkaandMarioGleirscher 119
3.6 CreatinganInducedModelfromtheStrategy
Itispossibletocreateacleaningschedulefromthesynthesisedstrategyσ. However,itisnotyetpossible
toverifyσ regardingtheconstraintslistedinSection3.3. Thisisbecause,uptothispoint,contamination
was modelled in M only as a probabilistic factor. To verify the contamination constraint CT, below, we
includeanon-probabilisticcontaminationmodelinM′ usingcounterstorepresentcontamination.
Modelling the Contamination Value. To verify that the contamination value (modelled as a boolean
sub-MDPofM)neveractuallyreachesthethresholdsR .threshold, j∈1..m,itisnecessarytotransform
j
M intoanMDPM′ thataccountsfortheactualvaluesR .d. WeaccomplishthisinM′ byinteger-valued
j
contaminationcountersR .d (Listing9)replacingthebooleanvariablesR .d inM (Listing3).
j j
1module contamination
2 R1.d : [0..R1.threshold] init 0;
3 R2.d : [0..R2.threshold] init 0;
4 ...
5 [at0_6] true -> (R1.d’=min(R1.d+R1.contaminationrate,R1.threshold))
6 & (R2.d’=min(R2.d+R2.contaminationrate,R2.threshold)) & ...;
7 [at0_1] true -> (R1.d’=0)
8 & (R2.d’=min(R2.d+R2.contaminationrate,R2.threshold)) & ...;
9 [at0_2] true -> (R1.d’=min(R1.d+R1.contaminationrate,R1.threshold))
10 & (R2.d’=0) & ...;
11 ...
12endmodule
Listing9: Anexampleofthestructureofthecontaminationmoduleusingdiscretecontaminationvalues
ApplyingtheStrategy. Apartfromusingcontaminationcounters,
ourmodeldoesnolongercontainprobabilisticchoices. Concretely,
1module time
each probabilistic choice in M (branching to each possible selec-
2 ...
tionoffullycontaminatedrooms,Listing3)isreplacedbyanaction 3 [at0_1] !error_or_final
in M′ performing a simultaneous update of all contamination coun- 4 & (t=8|t=10)
ters (Listing 9). We can use the generated strategy σ to derive the 5 -> (t’=min(t+1,T))
induced deterministic model Mσ from M′, which acts according to 6 ...
7endmodule
thestrategy. Thisstepisdonebymodifyingthepreconditionsofthe
atN_N_... actionsofthetimemoduletoonlybeabletotriggerwhenthatactionischosenatthesame
pointinthestrategy. If,forexample,withinσ,the at0_1 actionisonlychosenintimesteps8and10,
wemodifythetimemodule,seethelistingontheright.
NotesontheRelationshipbetweenM,M′ ,andMσ. ThestatespaceofM′ issignificantlylargerthan
the one of M as the latter contains intermediate contamination R .d up to R .d =R .threshold. The
j j j
fact that R gets contaminated in M corresponds to all shortest sequences of transitions with non-zero
j
probabilitytoastatewhereR .d=true. ThesamefactinM′ correspondstoallsequencesoftransitions
j
leadingtoR .d=R .threshold. Hence,thetimemoduleinMσ isarefinementofthetimemoduleinM
j j
and,duetosynchronisation(viaactionlabels),theresettingofcontamination’s(i.e.,thecleaning)inMσ
isarefinementofthecorrespondingresetsinM.
ApropertyofM′preservingquantitativestrategycorrectness,thatweleftforfuturework,istocheck
whethertheprobabilitiesofthecontaminationflagssetinM aregreaterthanorequaltothehypothetical120 RobustlyRecurrentStrategiesforRobotCollectives
Table2: Requirementsforvalidatingthesynthesisedstrategies(checksofMσ by PRISM)
Id. StrategySpecification(BehaviouralRequirement) ...expressedinPLTL
FR CleanerB iFinallyReturnstoitsstartingposition. (cid:86) i∈[1..k]P≥1[F=TB i.x=B i.start]
ωR CleanerB ihasafinalchargeofatleastB i.ωchgthres. (cid:86) i∈[1..k]P≤0[F=TB i.c<B i.ωchgthres]
ωC Contamin.ofR iisfinallylessthanR i.ωcontthres. (cid:86) i∈[1..m]P≤0[F=TR i.d<R i.ωcontthres]
(cid:86)
BC BatterychargeofCleanerB iisnever0. i∈[1..k]P≤0[FB i.c=0]
CT ContaminationofR ineverexceedsR i’sthreshold. (cid:86) i∈[1..m]P≤0[FR i.d≥R i.threshold]
UT RoomR iisnotcleanedwhileoccupied. (cid:86) i∈[1..m](cid:86) T∈util(Ri)(cid:86) j∈[1..k]P≤0[F=TB j.x=R i]
probabilitiesofthecorrespondingcountersinM′and,thus,Mσ,reachingtheirthresholds. Thisproperty,
whentrue,expressesthattheflagsareasound(i.e.,conservative)quantitativeabstractionofthecounters.
3.7 StrategyVerificationviaVerifyingtheInducedModel
Now, we use Mσ to check recurrence and safety from Formula (2), that is, ω →F≤Tω and G≤Tφs.4
In particular, we check their decomposed translations into PLTL requirements5 listed in Table 2. The
recurrenceareaω isencodedbythestatepropositionsinFR,ωR,andωC,whilesafetyφs isencodedby
thestatepropositionsin BC,CT,andUT.Notethatthe upperboundT oftherecurrenceinterval ismet
byallpathsinMσ. util(R)isthesetoftimeslotsinwhichR isutilised.
i i
For checking ω →F≤Tω, we define ω to be a (not necessarily maximum) region in S from where
σ can be applied and ω is revisited after T steps. The requirements for σ need to be true for every
initial state in ω. FR, ωR, and ωC ensure that applying σ leads to a state within ω. To specify ω, we
use thresholds for the battery charge of robots (B.c for every robot B) and the contamination of rooms
i i
(R.ωcontthresforeveryroomR). ω thencharacteriseseverystateofMσ wherethebatterychargeand
i i
roomcontaminationarewithinthesethresholdsandallrobotsareontheirstartingposition. Recurrence
canevenbecheckedmoreeasilyforMσ byselectingtheworststateinω,whichisthestatewhereevery
valueliesexactlyatthethresholds,andverifyingtherequirementsinTable2forthisstate.
4 Experimental Evaluation
Ourexperimentalevaluationaddressestworesearchquestions(RQs).
4.1 RQ1: Canwesynthesisereasonablestrategiesformultiplerobots?
Inthefollowing,aninstanceoftheexamplemodelwithonlyonerobotisconsideredfirst. Thecontam-
inationrateisthesameforallrooms,exceptthatR .d hasathresholdvalueR .threshold of24(based
5 5
onacontaminationrateof1h−1),whichistwiceashigh,whereasallotherroomshaveathresholdvalue
of 12. We identified ω manually by examining the generated strategy. Using the method described in
Section 3, a strategy was generated that meets all the requirements. This strategy is visualised in Fig-
ure 6a. Each action is shown with a blue arrow, at which the time step in which the action is to be
executed is annotated. To develop a strategy for two robots, the battery charge was halved to keep the
4Forthesakeofsimplicityoftheexample,weuseφr≡⊤andcanomitω→F≤Tφr.
5Allpropertiesareexpressedinquasi-LTL,thatis,ACTL*allowingonlyoneuniversalquantifierattheoutermostlevel.TillSchnittkaandMarioGleirscher 121
C1 1 24
2
1615
23
10
C2
9
3
11 R5 12
R4
22819 R1
18
4 6 17 7
1320
21
5
14
R2
R3
(a)Strategyforonerobot
C1
1517
913
218 16
4 122420
2614
C2 7
22 429
2137181510 513
3 81 11 116 9 R5 2 10 24 21 R4 16 422 R1 2 1018 311 19
R2
715
23
R3
(b)Strategyfortworobots(blueandgreendirectedarcs)
t 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ... 24
B 1 C1 R1 R2 R1 C 1 R1 R3 R1 C1 R1 R2 R1 C 1 R1 R3 R1 ... C1
B 2 C2 R1 C 2 R5 R4 R5 C 2 idle R5 C 2 idle R5 R4 R5 C 2 idle ... C2
(c)Executionofa24hmodelcycleusingthestrategyinFigure6b;(sub-)cyclesindicatedinbold
Figure6: Synthesisedstrategies. Nodesrepresentchargingstationsandrooms; undirectedarcsindicate
roomconnections(doors);edgelabelsspecifytheexecutionorderofparticularatactions.
model less complex. The corresponding strategy can be seen in Figure 6b. This result turns out to be a
partition-basedpatrollingstrategy,consideredeffectiveunderrandomdisturbance[15,143].
4.2 RQ2: Howdomodelparametersinfluencethesynthesisofrecurrentstrategies?
We evaluate the model using a_bit, R.pr, and the fixed-grid resolution g as parameters. Figure 7 visu-
i
alisestheresultusingtheseparametersonasimplifiedmodel, whichonlycontainsonerobotandomits
roomR andchargerC . Whenbuildingσ,wesetR.pr=cumulative_probability/|R|. Thegenerated
5 2 i
strategieswereverifiedbyiterativelyassumingω-thresholds(Table2)fromasetchosenappropriately.
Figure7containsfourplotsforresolutionsg=1..4. Correctnon-recurrentstrategiesarerepresented
byabluedot,correctrecurrentstrategiesbyagreendot,andincorrectstrategiesinred.
We deemed the simplification of the model necessary to allow a timely execution of the test series,
whichcontained400experimentsintotal(100foreachgridresolution). Detailedinformationaboutthe122 RobustlyRecurrentStrategiesforRobotCollectives
resolution=1 resolution=2 resolution=3 resolution=4
246
08 0000
energy
consumption
246
08 0000
energy
consumption
246
08 0000
energy
consumption
246
08 0000
energy
consumption
cum0. u0 lati0 v. e2 pro0 p.4
abili0 ty.6 0
1 val2 ue
3
of
a_bit
cum0. u0 lati0 v. e2 pro0 p.4
abili0 ty.6 0
1 val2 ue
3
of
a_bit
cum0. u0 lati0 v. e2 pro0 p.4
abili0 ty.6 0
1 val2 ue
3
of
a_bit
cum0. u0 lati0 v. e2 pro0 p.4
abili0 ty.6 0
1 val2 ue
3
of
a_bit
Figure 7: Parameterised evaluation. Non-recurrent strategies are marked in blue, recurrent strategies in
green. Thescalefora_bit islogarithmic.
timeoftheevaluationissharedattheendofthesection.
Asexpected,asmallercumulativecontaminationprobabilityleadsthestrategytode-prioritiseclean-
ing the rooms regularly, since the probability of them being contaminated stays low. This, at some
point, causes the strategy to not satisfy the recurrence criteria. Similarly, a lower value of a_bit causes
thestrategytoprioritisesavingbatteryoverresettingthecontaminationflags,whichreducestheenergy
consumptionsignificantly,butatsomepointatthecostofstrategycorrectness.
Intheevaluation,theoptimalstrategypergridresolutionuseslessorequalenergywithalargergrid
resolution (84, 64, 64, and 36 for g of 1, 2, 3, and 4, respectively). There is also a difference in the
number of distinct strategies that are generated between the different g, where the experiment with a
resolutionof4generatesmoreuniquestrategiesthantheexperimentswithlowergridresolutions,where
manyparametersleadtothesamegeneratedstrategy.
Finally,wecanobservedistinctareasofincorrect,non-recurrentandrecurrentstrategiesthatdepend
ontheseparameters,wheretheoptimalrecurrentstrategyliesontheborderbetweenrecurrentandnon-
recurrentstrategies. Whilethestatesinω areorderedforthechoiceofaworstcase,theborderareaisat
bestanapproximationofaParetofront,thenon-convexrewardfunctiondefinedbyRcombinedwiththe
belief-MDPapproximationB(M)mayleadtooptimalstrategiesremaininghiddenfromthesearch.
Beyond the reward structure Rutilisation used for strategy pre-selection, checking the PLTL safety
property UT (Table 2) ensures that the finally chosen strategy only cleans outside the room utilisation
schedule(Figure1b).
Some Key Data. The experiments were conducted on an AMD FX(tm)-8350 Eight-Core Processor
with 32GiB of RAM running Ubuntu 22.04.4 LTS. However, PRISM was restricted to one core and
12GiB of RAM. The reduced model in Section 4.2 contains 4879 states and 35014 transitions, while
the reduced model Mσ contains 23 states and 23 transitions. The verification consists of 13 PLTL
formulas containing 25 propositions. We ran the experiments with parameters of m = 4, R.pr =
i
0.02,0.04,...,0.16 and a_bit = 1,3,6,10,17,32,100,316,1000,3162. The cumulative probability can
be derived from R: ∑ R.pr=0.08,0.16,...,0.64. The strategy synthesis took about 5, 16, 60,
i i=1,...,m i
and 200 seconds for a grid resolution of 1, 2, 3, and 4, respectively, while the verification took about
onesecondforagivenω. Evaluatingtheentiretestseriestookabout8hourssequentially,althoughthis
processcouldbeeasilyparallelised.TillSchnittkaandMarioGleirscher 123
5 Discussion
Selecting the Recurrence Area ω. When evaluating the strategy, ω was chosen either by examining
the strategy manually (Figure 6c) or by verifying a list of probable ωs. Further work may focus on
findingprobableωsfromtheroomlayout,andgeneratingstrategieswhichfulfiltheseωs.
ComplexityoftheCleaningScenario. Fortheevaluation,weconsideredarathersimpleroomlayout.
Theperformanceoftheabovedescribedmethodmaybedifferentwithlargerroomgraphs,morecomplex
roomlayouts,alargernumberofrobots,andtighterrestrictionsonbatterychargeandroomutilisation.
Moreover,ourmodelallowsustofindstrategiesthatoperatewithavaryingnumberofrobots. Given
thatsomerobotsremainidlingallthetime,ouroptimalsynthesiscouldalsobeusedtofindthesmallest
subsetB ⊆Borminimalnumberk ≤kofrobotsforanoptimaltaskperformance.
min min
The complexity of the model is heavily dependant on the number of rooms, the maximum time T,
and the maximum charge of the robots. Following the comprehensive scheme in Section 4.2, we were
abletocalculatea12-hour(T =12)cleaningschedulefor3robotswith11roomsandamaximumcharge
of6in15hours. Thecorrespondingbelief-MDPB(M)contains≈690kstatesand≈11.8mtransitions.
AdjustingGrid-Resolutionvs.FilteringStrategies. Forindustry-sizePOMDPs, ahighresolutiong
can lead to an impractically high computational effort when solving the mostly NP-hard approximate
analysis (i.e., verification, synthesis) problems. Hence, our approach is to keep g just fine enough to
findsome(notnecessarilygloballyoptimal)strategyσ andverifymorenuancedpropertiesofthequasi-
MDP6 Mσ derived from M by applying σ. In Mσ, verification is simpler (no belief-MDP B(M) is
computed),alsothestrategy(integratedinMσ)candirectlyobservetheoutcomeofeachactionanddoes
nothavetomemoriseafiniteobservationhistory. DespitetheexpansionofR.d tointegers,Mσ’sstate
i
spaceisexpectedtobesmallerthanB(M)’sstatespacefortheappliedvaluesofg.
Parameter Selection. For the evaluation in Section 4.2, a set of values for the parameters a_bit and
the contamination probability was chosen. Via g (Section 2), we reduced the resolution of the fixed
grid (i.e., a wider grid width) to limit the number of states in the belief space approximation B(M).
However,ourfindingsinSection4.2suggestthatincreasingtheresolution,whilekeepingthecumulative
contaminationprobabilityaround40%andthevalueofa_bitaround300leadstothesynthesisofbetter
strategies. However, thesevaluesmaynotbeuniversallyfavourableforanyroomlayout, anditmaybe
possible to synthesise better strategies using a different set of parameters. Further work may focus on
betterwaysofparameterselection.
GeneralisationtoOtherApplications. Therunningexampleinourcasestudyfocusesonacleaning
robot collective. However, we think that our approach and model can be transferred rather straightfor-
wardlytootherspatio-temporalsettingswithrecurrenttasks,forexample,
• firefightingdronecollectivestaskedwithrepetitivesector-wisefiredetectionandwaterdistribution
andwithpartiallyobservablequantitiessuchasgroundtemperatureandextinctionlevel;
• geriatric care robot collectives tasked with recurrent monitoring and care-taking tasks (e.g., med-
icationsupply)withpatientsatisfactionandhealthstatusbeingpartiallyobservable;
• generalpatrollingcollectivestaskedwithmonitoringorsupervisingspecificenvironments[15].
6non-probabilistic,deterministic,withfullobservability124 RobustlyRecurrentStrategiesforRobotCollectives
6 Conclusion
Weproposedanapproachusingweighted,partiallyobservablestochasticmodels(i.e.,reward-enhanced
POMDPs) and strategy synthesis for optimally coordinating tasked robot collectives while providing
recurrence and safety guarantees on the resulting strategies under uncertainty. Along with that, we dis-
cussedguidanceonPOMDPmodellingandstrategysynthesis. Wefocusedonacleaningrobotscenario
forpublicbuildings,suchasschools. Ournotionofcorrectnesscombines(i)saferecurrence(e.g.,repet-
itivelyaccomplishthecleaningtaskwhileavoidingtocollectpenalties),(ii)robustness(e.g.,correctness
underworst-casecontamination),and(iii)optimality(e.g.,minimalenergyconsumption).
Forscalingupstrategysynthesistoscenariosbeyondwhatcaneasilybetackledbystochasticgame-
basedsynthesis,weaddressedthekeychallenge[6]ofreducingthestatespaceandthetransitionrelation
of a naïve model via partial observability (hiding details of stochastic room contamination) and by em-
ploying simultaneous composition (for robot movement). PRISM’s grid-based POMDP approximation
allowed us to adjust the level of detail of the belief space to synthesise strategies more efficiently. Fur-
thermore, we softly encode the strategy search space using penalties and optimisation rewards and can,
thus, shift the verification of more complicated properties to a later stage working with an unweighted
andnon-probabilisticbehaviouralmodel,againusingamoredetailed,numericalstateandactionspace.
However,decouplingsynthesisfromverificationcanrequiretime-consumingexperiments(Section4.2)
toidentifyregionsoftheparameterspaceforensuringtheexistenceofgoodrecurrentstrategies.
Infuturework,wewillimprovefindingω ensuringtheexistenceofcorrectstrategies(i.e.,greendots
inFigure7). Ideally,weavoiddefiningω explicitly(e.g.,byhidingtime). Inalargerexample,wewant
to allow invariant-narrowing with φr and observable stochasticity in the environment, such that σ can
dependonarbitraryvariables. Theresetofthecontaminationflagonaroomvisit(Db)couldberefined
by a decontamination rate in Mσ. Moreover, we aim to use multi-objective queries to include further
criteria (e.g., minimal contamination) for Pareto-optimal strategy choice. While PRISM imposes some
limits on the combination of queries and constraints, we will need to see how we can use tools such as
EVOCHECKER(as,e.g.,usedin[17])forPOMDPs. Also,wecanfurtherreducetheactionsetbytaking
intoaccounttrajectoryintersectionsinthesimultaneousmovements(cf.Figure4a). Finally,wewantto
connectthesynthesispipelinewithcodegeneration,suchasdemonstratedinourpreviouswork[5].
References
[1] ChristelBaier&Joost-PieterKatoen(2008): PrinciplesofModelChecking. MITPress.
[2] Davide Basile, Maurice H. ter Beek & Axel Legay (2020): Strategy Synthesis for Autonomous Driving in
a Moving Block Railway System with UPPAAL Stratego. In: FORTE, LNPSE 12136, Springer, pp. 3–21,
doi:10.1007/978-3-030-50086-3_1.
[3] DIN (2015): DIN 77400: Reinigungsdienstleistungen - Schulgebäude - Anforderungen an die Reinigung.
Standard,DIN. Availableathttps://www.dinmedia.de/de/norm/din-77400/237208488.
[4] RubenGiaquinta,RuthHoffmann,MurrayIreland,AliceMiller&GethinNorman(2018):StrategySynthesis
forAutonomousAgentsUsingPRISM,p.220–236. Springer,doi:10.1007/978-3-319-77935-5_16.
[5] Mario Gleirscher, Radu Calinescu, James Douthwaite, Benjamin Lesage, Colin Paterson, Jonathan Aitken,
RobertAlexander&JamesLaw(2022): VerifiedSynthesisofOptimalSafetyControllersforHuman-Robot
Collaboration. Sci.Comput.Program.218,p.102809,doi:10.1016/j.scico.2022.102809. arXiv:2106.06604.
[6] MarioGleirscher,JacovandePol&JamesWoodcock(2023): AManifestoforApplicableFormalMethods.
Softw.Syst.Model.22,pp.1737–1749,doi:10.1007/s10270-023-01124-2. arXiv:2112.12758.TillSchnittkaandMarioGleirscher 125
[7] Rong Gu, Peter G. Jensen, Cristina Seceleanu, Eduard Enoiu & Kristina Lundqvist (2022): Correctness-
guaranteedstrategysynthesisandcompressionformulti-agentautonomoussystems. Sci.Comput.Program.
224,p.102894,doi:10.1016/j.scico.2022.102894.
[8] Umweltbundesamt (Hrsg.) (2008): Leitfaden für die Innenraumhygiene in Schulgebäuden. Available
at https://www.umweltbundesamt.de/sites/default/files/medien/publikation/long/3689.
pdf.
[9] BrunoLacerda,DavidParker&NickHawes(2017): Multi-ObjectivePolicyGenerationforMobileRobots
UnderProbabilisticTime-BoundedGuarantees. In: AutomatedPlanningandScheduling(ICAPS),27thInt.
Conf.,pp.504–512,doi:10.1609/icaps.v27i1.13865.
[10] WilliamS.Lovejoy(1991): ComputationallyFeasibleBoundsforPartiallyObservedMarkovDecisionPro-
cesses. Oper.Res.39(1),p.162–175,doi:10.1287/opre.39.1.162.
[11] Owen Macindoe, Leslie Pack Kaelbling & Tomás Lozano-Pérez (2012): POMCoP: Belief Space Planning
forSidekicksinCooperativeGames. In: AIIDE,TheAAAIPress,pp.38–43,doi:10.1609/aiide.v8i1.12510.
[12] Pierre El Mqirmi, Francesco Belardinelli & Borja G. León (2021): An Abstraction-based Method
to Check Multi-Agent Deep Reinforcement-Learning Behaviors. In: AAMAS, pp. 474–482,
doi:10.5555/3463952.3464012. arXiv:2102.01434.
[13] GethinNorman,DavidParker&XueyiZou(2017): Verificationandcontrolofpartiallyobservableprobab-
ilisticsystems. Real-TimeSystems53(3),p.354–402,doi:10.1007/s11241-017-9269-4.
[14] Dave Parker, Gethin Norman & Marta Kwiatkowska (2024): PRISM Model Checker. Available at http:
//www.prismmodelchecker.org/manual/.
[15] DavidPortugal&RuiRocha(2011): ASurveyonMulti-robotPatrollingAlgorithms,pp.139–146. Springer,
doi:10.1007/978-3-642-19170-1_15.
[16] Antony Thomas, Fulvio Mastrogiovanni & Marco Baglietto (2021): MPTP: Motion-planning-
aware task planning for navigation in belief space. Robot. Auton. Syst. 141, p. 103786,
doi:10.1016/j.robot.2021.103786.
[17] Gricel Vázquez, Radu Calinescu & Javier Cámara (2022): Scheduling of Missions with Constrained
Tasks for Heterogeneous Robot Systems. In: Proceedings Fourth International Workshop on Formal
Methods for Autonomous Systems (FMAS) and Fourth International Workshop on Automated and verifi-
able Software sYstem DEvelopment (ASYDE), EPTCS 371, Open Publishing Association, pp. 156–174,
doi:10.4204/eptcs.371.11. arXiv:2209.14040.