[
    {
        "title": "Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs",
        "authors": "Rudolf LaineBilal ChughtaiJan BetleyKaivalya HariharanJeremy ScheurerMikita BalesniMarius HobbhahnAlexander MeinkeOwain Evans",
        "links": "http://arxiv.org/abs/2407.04694v1",
        "entry_id": "http://arxiv.org/abs/2407.04694v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04694v1",
        "summary": "AI assistants such as ChatGPT are trained to respond to users by saying, \"I\nam a large language model\". This raises questions. Do such models know that\nthey are LLMs and reliably act on this knowledge? Are they aware of their\ncurrent circumstances, such as being deployed to the public? We refer to a\nmodel's knowledge of itself and its circumstances as situational awareness. To\nquantify situational awareness in LLMs, we introduce a range of behavioral\ntests, based on question answering and instruction following. These tests form\nthe $\\textbf{Situational Awareness Dataset (SAD)}$, a benchmark comprising 7\ntask categories and over 13,000 questions. The benchmark tests numerous\nabilities, including the capacity of LLMs to (i) recognize their own generated\ntext, (ii) predict their own behavior, (iii) determine whether a prompt is from\ninternal evaluation or real-world deployment, and (iv) follow instructions that\ndepend on self-knowledge.\n  We evaluate 16 LLMs on SAD, including both base (pretrained) and chat models.\nWhile all models perform better than chance, even the highest-scoring model\n(Claude 3 Opus) is far from a human baseline on certain tasks. We also observe\nthat performance on SAD is only partially predicted by metrics of general\nknowledge (e.g. MMLU). Chat models, which are finetuned to serve as AI\nassistants, outperform their corresponding base models on SAD but not on\ngeneral knowledge tasks. The purpose of SAD is to facilitate scientific\nunderstanding of situational awareness in LLMs by breaking it down into\nquantitative abilities. Situational awareness is important because it enhances\na model's capacity for autonomous planning and action. While this has potential\nbenefits for automation, it also introduces novel risks related to AI safety\nand control. Code and latest results available at\nhttps://situational-awareness-dataset.org .",
        "updated": "2024-07-05 17:57:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04694v1"
    },
    {
        "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models",
        "authors": "Yuzhe GuZiwei JiWenwei ZhangChengqi LyuDahua LinKai Chen",
        "links": "http://arxiv.org/abs/2407.04693v1",
        "entry_id": "http://arxiv.org/abs/2407.04693v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04693v1",
        "summary": "Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.",
        "updated": "2024-07-05 17:56:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04693v1"
    },
    {
        "title": "Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for Interpreting Neural Networks",
        "authors": "Aaron Mueller",
        "links": "http://arxiv.org/abs/2407.04690v1",
        "entry_id": "http://arxiv.org/abs/2407.04690v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04690v1",
        "summary": "Interpretability research takes counterfactual theories of causality for\ngranted. Most causal methods rely on counterfactual interventions to inputs or\nthe activations of particular model components, followed by observations of the\nchange in models' output logits or behaviors. While this yields more faithful\nevidence than correlational methods, counterfactuals nonetheless have key\nproblems that bias our findings in specific and predictable ways. Specifically,\n(i) counterfactual theories do not effectively capture multiple independently\nsufficient causes of the same effect, which leads us to miss certain causes\nentirely; and (ii) counterfactual dependencies in neural networks are generally\nnot transitive, which complicates methods for extracting and interpreting\ncausal graphs from neural networks. We discuss the implications of these\nchallenges for interpretability researchers and propose concrete suggestions\nfor future work.",
        "updated": "2024-07-05 17:53:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04690v1"
    },
    {
        "title": "Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge",
        "authors": "Yuanze LinYunsheng LiDongdong ChenWeijian XuRonald ClarkPhilip TorrLu Yuan",
        "links": "http://arxiv.org/abs/2407.04681v1",
        "entry_id": "http://arxiv.org/abs/2407.04681v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04681v1",
        "summary": "In recent years, multimodal large language models (MLLMs) have made\nsignificant strides by training on vast high-quality image-text datasets,\nenabling them to generally understand images well. However, the inherent\ndifficulty in explicitly conveying fine-grained or spatially dense information\nin text, such as masks, poses a challenge for MLLMs, limiting their ability to\nanswer questions requiring an understanding of detailed or localized visual\nelements. Drawing inspiration from the Retrieval-Augmented Generation (RAG)\nconcept, this paper proposes a new visual prompt approach to integrate\nfine-grained external knowledge, gleaned from specialized vision models (e.g.,\ninstance segmentation/OCR models), into MLLMs. This is a promising yet\nunderexplored direction for enhancing MLLMs' performance. Our approach diverges\nfrom concurrent works, which transform external knowledge into additional text\nprompts, necessitating the model to indirectly learn the correspondence between\nvisual content and text coordinates. Instead, we propose embedding fine-grained\nknowledge information directly into a spatial embedding map as a visual prompt.\nThis design can be effortlessly incorporated into various MLLMs, such as LLaVA\nand Mipha, considerably improving their visual understanding performance.\nThrough rigorous experiments, we demonstrate that our method can enhance MLLM\nperformance across nine benchmarks, amplifying their fine-grained context-aware\ncapabilities.",
        "updated": "2024-07-05 17:43:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04681v1"
    },
    {
        "title": "Lost in Translation: The Algorithmic Gap Between LMs and the Brain",
        "authors": "Tommaso TosatoPascal Jr Tikeng NotsawoSaskia HelblingIrina RishGuillaume Dumas",
        "links": "http://arxiv.org/abs/2407.04680v1",
        "entry_id": "http://arxiv.org/abs/2407.04680v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04680v1",
        "summary": "Language Models (LMs) have achieved impressive performance on various\nlinguistic tasks, but their relationship to human language processing in the\nbrain remains unclear. This paper examines the gaps and overlaps between LMs\nand the brain at different levels of analysis, emphasizing the importance of\nlooking beyond input-output behavior to examine and compare the internal\nprocesses of these systems. We discuss how insights from neuroscience, such as\nsparsity, modularity, internal states, and interactive learning, can inform the\ndevelopment of more biologically plausible language models. Furthermore, we\nexplore the role of scaling laws in bridging the gap between LMs and human\ncognition, highlighting the need for efficiency constraints analogous to those\nin biological systems. By developing LMs that more closely mimic brain\nfunction, we aim to advance both artificial intelligence and our understanding\nof human cognition.",
        "updated": "2024-07-05 17:43:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04680v1"
    }
]