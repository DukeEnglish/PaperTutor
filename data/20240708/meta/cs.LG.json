[
    {
        "title": "Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs",
        "authors": "Rudolf LaineBilal ChughtaiJan BetleyKaivalya HariharanJeremy ScheurerMikita BalesniMarius HobbhahnAlexander MeinkeOwain Evans",
        "links": "http://arxiv.org/abs/2407.04694v1",
        "entry_id": "http://arxiv.org/abs/2407.04694v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04694v1",
        "summary": "AI assistants such as ChatGPT are trained to respond to users by saying, \"I\nam a large language model\". This raises questions. Do such models know that\nthey are LLMs and reliably act on this knowledge? Are they aware of their\ncurrent circumstances, such as being deployed to the public? We refer to a\nmodel's knowledge of itself and its circumstances as situational awareness. To\nquantify situational awareness in LLMs, we introduce a range of behavioral\ntests, based on question answering and instruction following. These tests form\nthe $\\textbf{Situational Awareness Dataset (SAD)}$, a benchmark comprising 7\ntask categories and over 13,000 questions. The benchmark tests numerous\nabilities, including the capacity of LLMs to (i) recognize their own generated\ntext, (ii) predict their own behavior, (iii) determine whether a prompt is from\ninternal evaluation or real-world deployment, and (iv) follow instructions that\ndepend on self-knowledge.\n  We evaluate 16 LLMs on SAD, including both base (pretrained) and chat models.\nWhile all models perform better than chance, even the highest-scoring model\n(Claude 3 Opus) is far from a human baseline on certain tasks. We also observe\nthat performance on SAD is only partially predicted by metrics of general\nknowledge (e.g. MMLU). Chat models, which are finetuned to serve as AI\nassistants, outperform their corresponding base models on SAD but not on\ngeneral knowledge tasks. The purpose of SAD is to facilitate scientific\nunderstanding of situational awareness in LLMs by breaking it down into\nquantitative abilities. Situational awareness is important because it enhances\na model's capacity for autonomous planning and action. While this has potential\nbenefits for automation, it also introduces novel risks related to AI safety\nand control. Code and latest results available at\nhttps://situational-awareness-dataset.org .",
        "updated": "2024-07-05 17:57:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04694v1"
    },
    {
        "title": "Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for Interpreting Neural Networks",
        "authors": "Aaron Mueller",
        "links": "http://arxiv.org/abs/2407.04690v1",
        "entry_id": "http://arxiv.org/abs/2407.04690v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04690v1",
        "summary": "Interpretability research takes counterfactual theories of causality for\ngranted. Most causal methods rely on counterfactual interventions to inputs or\nthe activations of particular model components, followed by observations of the\nchange in models' output logits or behaviors. While this yields more faithful\nevidence than correlational methods, counterfactuals nonetheless have key\nproblems that bias our findings in specific and predictable ways. Specifically,\n(i) counterfactual theories do not effectively capture multiple independently\nsufficient causes of the same effect, which leads us to miss certain causes\nentirely; and (ii) counterfactual dependencies in neural networks are generally\nnot transitive, which complicates methods for extracting and interpreting\ncausal graphs from neural networks. We discuss the implications of these\nchallenges for interpretability researchers and propose concrete suggestions\nfor future work.",
        "updated": "2024-07-05 17:53:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04690v1"
    },
    {
        "title": "Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge",
        "authors": "Yuanze LinYunsheng LiDongdong ChenWeijian XuRonald ClarkPhilip TorrLu Yuan",
        "links": "http://arxiv.org/abs/2407.04681v1",
        "entry_id": "http://arxiv.org/abs/2407.04681v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04681v1",
        "summary": "In recent years, multimodal large language models (MLLMs) have made\nsignificant strides by training on vast high-quality image-text datasets,\nenabling them to generally understand images well. However, the inherent\ndifficulty in explicitly conveying fine-grained or spatially dense information\nin text, such as masks, poses a challenge for MLLMs, limiting their ability to\nanswer questions requiring an understanding of detailed or localized visual\nelements. Drawing inspiration from the Retrieval-Augmented Generation (RAG)\nconcept, this paper proposes a new visual prompt approach to integrate\nfine-grained external knowledge, gleaned from specialized vision models (e.g.,\ninstance segmentation/OCR models), into MLLMs. This is a promising yet\nunderexplored direction for enhancing MLLMs' performance. Our approach diverges\nfrom concurrent works, which transform external knowledge into additional text\nprompts, necessitating the model to indirectly learn the correspondence between\nvisual content and text coordinates. Instead, we propose embedding fine-grained\nknowledge information directly into a spatial embedding map as a visual prompt.\nThis design can be effortlessly incorporated into various MLLMs, such as LLaVA\nand Mipha, considerably improving their visual understanding performance.\nThrough rigorous experiments, we demonstrate that our method can enhance MLLM\nperformance across nine benchmarks, amplifying their fine-grained context-aware\ncapabilities.",
        "updated": "2024-07-05 17:43:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04681v1"
    },
    {
        "title": "The diameter of a stochastic matrix: A new measure for sensitivity analysis in Bayesian networks",
        "authors": "Manuele LeonelliJim Q. SmithSophia K. Wright",
        "links": "http://arxiv.org/abs/2407.04667v1",
        "entry_id": "http://arxiv.org/abs/2407.04667v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04667v1",
        "summary": "Bayesian networks are one of the most widely used classes of probabilistic\nmodels for risk management and decision support because of their\ninterpretability and flexibility in including heterogeneous pieces of\ninformation. In any applied modelling, it is critical to assess how robust the\ninferences on certain target variables are to changes in the model. In Bayesian\nnetworks, these analyses fall under the umbrella of sensitivity analysis, which\nis most commonly carried out by quantifying dissimilarities using\nKullback-Leibler information measures. In this paper, we argue that robustness\nmethods based instead on the familiar total variation distance provide simple\nand more valuable bounds on robustness to misspecification, which are both\nformally justifiable and transparent. We introduce a novel measure of\ndependence in conditional probability tables called the diameter to derive such\nbounds. This measure quantifies the strength of dependence between a variable\nand its parents. We demonstrate how such formal robustness considerations can\nbe embedded in building a Bayesian network.",
        "updated": "2024-07-05 17:22:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04667v1"
    },
    {
        "title": "Unsupervised 4D Cardiac Motion Tracking with Spatiotemporal Optical Flow Networks",
        "authors": "Long TengWei FengMenglong ZhuXinchao Li",
        "links": "http://arxiv.org/abs/2407.04663v1",
        "entry_id": "http://arxiv.org/abs/2407.04663v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04663v1",
        "summary": "Cardiac motion tracking from echocardiography can be used to estimate and\nquantify myocardial motion within a cardiac cycle. It is a cost-efficient and\neffective approach for assessing myocardial function. However, ultrasound\nimaging has the inherent characteristics of spatially low resolution and\ntemporally random noise, which leads to difficulties in obtaining reliable\nannotation. Thus it is difficult to perform supervised learning for motion\ntracking. In addition, there is no end-to-end unsupervised method currently in\nthe literature. This paper presents a motion tracking method where unsupervised\noptical flow networks are designed with spatial reconstruction loss and\ntemporal-consistency loss. Our proposed loss functions make use of the\npair-wise and temporal correlation to estimate cardiac motion from noisy\nbackground. Experiments using a synthetic 4D echocardiography dataset has shown\nthe effectiveness of our approach, and its superiority over existing methods on\nboth accuracy and running speed. To the best of our knowledge, this is the\nfirst work performed that uses unsupervised end-to-end deep learning optical\nflow network for 4D cardiac motion tracking.",
        "updated": "2024-07-05 17:18:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04663v1"
    }
]