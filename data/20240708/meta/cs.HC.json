[
    {
        "title": "Gamification of Motor Imagery Brain-Computer Interface Training Protocols: a systematic review",
        "authors": "Fred AtillaMarie PostmaMaryam Alimardani",
        "links": "http://arxiv.org/abs/2407.04610v1",
        "entry_id": "http://arxiv.org/abs/2407.04610v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04610v1",
        "summary": "Current Motor Imagery Brain-Computer Interfaces (MI-BCI) require a lengthy\nand monotonous training procedure to train both the system and the user.\nConsidering many users struggle with effective control of MI-BCI systems, a\nmore user-centered approach to training might help motivate users and\nfacilitate learning, alleviating inefficiency of the BCI system. With the\nincrease of BCI-controlled games, researchers have suggested using game\nprinciples for BCI training, as games are naturally centered on the player.\nThis review identifies and evaluates the application of game design elements to\nMI-BCI training, a process known as gamification. Through a systematic\nliterature search, we examined how MI-BCI training protocols have been gamified\nand how specific game elements impacted the training outcomes. We identified 86\nstudies that employed gamified MI-BCI protocols in the past decade. The\nprevalence and reported effects of individual game elements on user experience\nand performance were extracted and synthesized. Results reveal that MI-BCI\ntraining protocols are most often gamified by having users move an avatar in a\nvirtual environment that provides visual feedback. Furthermore, in these\nvirtual environments, users were provided with goals that guided their actions.\nUsing gamification, the reviewed protocols allowed users to reach effective\nMI-BCI control, with studies reporting positive effects of four individual\nelements on user performance and experience, namely: feedback, avatars,\nassistance, and social interaction. Based on these elements, this review makes\ncurrent and future recommendations for effective gamification, such as the use\nof virtual reality and adaptation of game difficulty to user skill level.",
        "updated": "2024-07-05 16:03:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04610v1"
    },
    {
        "title": "Enabling On-Device LLMs Personalization with Smartphone Sensing",
        "authors": "Shiquan ZhangYing MaLe FangHong JiaSimon D'AlfonsoVassilis Kostakos",
        "links": "http://arxiv.org/abs/2407.04418v1",
        "entry_id": "http://arxiv.org/abs/2407.04418v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04418v1",
        "summary": "This demo presents a novel end-to-end framework that combines on-device large\nlanguage models (LLMs) with smartphone sensing technologies to achieve\ncontext-aware and personalized services. The framework addresses critical\nlimitations of current personalization solutions via cloud-based LLMs, such as\nprivacy concerns, latency and cost, and limited personal sensor data. To\nachieve this, we innovatively proposed deploying LLMs on smartphones with\nmultimodal sensor data and customized prompt engineering, ensuring privacy and\nenhancing personalization performance through context-aware sensing. A case\nstudy involving a university student demonstrated the proposed framework's\ncapability to provide tailored recommendations. In addition, we show that the\nproposed framework achieves the best trade-off in privacy, performance,\nlatency, cost, battery and energy consumption between on-device and cloud LLMs.\nFuture work aims to integrate more diverse sensor data and conduct large-scale\nuser studies to further refine the personalization. We envision the proposed\nframework could significantly improve user experiences in various domains such\nas healthcare, productivity, and entertainment by providing secure,\ncontext-aware, and efficient interactions directly on users' devices.",
        "updated": "2024-07-05 11:09:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04418v1"
    },
    {
        "title": "A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials",
        "authors": "Shuoyang ZhengAnna Xambó SedóNick Bryan-Kinns",
        "links": "http://arxiv.org/abs/2407.04379v1",
        "entry_id": "http://arxiv.org/abs/2407.04379v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04379v1",
        "summary": "This paper presents a mapping strategy for interacting with the latent spaces\nof generative AI models. Our approach involves using unsupervised feature\nlearning to encode a human control space and mapping it to an audio synthesis\nmodel's latent space. To demonstrate how this mapping strategy can turn\nhigh-dimensional sensor data into control mechanisms of a deep generative\nmodel, we present a proof-of-concept system that uses visual sketches to\ncontrol an audio synthesis model. We draw on emerging discourses in XAIxArts to\ndiscuss how this approach can contribute to XAI in artistic and creative\ncontexts, we also discuss its current limitations and propose future research\ndirections.",
        "updated": "2024-07-05 09:32:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04379v1"
    },
    {
        "title": "Towards Context-aware Support for Color Vision Deficiency: An Approach Integrating LLM and AR",
        "authors": "Shogo MoritaYan ZhangTakuto YamauchiSinan ChenJialong LiKenji Tei",
        "links": "http://arxiv.org/abs/2407.04362v1",
        "entry_id": "http://arxiv.org/abs/2407.04362v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04362v1",
        "summary": "People with color vision deficiency often face challenges in distinguishing\ncolors such as red and green, which can complicate daily tasks and require the\nuse of assistive tools or environmental adjustments. Current support tools\nmainly focus on presentation-based aids, like the color vision modes found in\niPhone accessibility settings. However, offering context-aware support, like\nindicating the doneness of meat, remains a challenge since task-specific\nsolutions are not cost-effective for all possible scenarios. To address this,\nour paper proposes an application that provides contextual and autonomous\nassistance. This application is mainly composed of: (i) an augmented reality\ninterface that efficiently captures context; and (ii) a multi-modal large\nlanguage model-based reasoner that serves to cognitize the context and then\nreason about the appropriate support contents. Preliminary user experiments\nwith two color vision deficient users across five different scenarios have\ndemonstrated the effectiveness and universality of our application.",
        "updated": "2024-07-05 09:03:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04362v1"
    },
    {
        "title": "UpStory: the Uppsala Storytelling dataset",
        "authors": "Marc FraileNatalia Calvo-BarajasAnastasia Sophia ApeironGiovanna VarniJoakim LindbladNataša SladojeGinevra Castellano",
        "links": "http://arxiv.org/abs/2407.04352v1",
        "entry_id": "http://arxiv.org/abs/2407.04352v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04352v1",
        "summary": "Friendship and rapport play an important role in the formation of\nconstructive social interactions, and have been widely studied in educational\nsettings due to their impact on student outcomes. Given the growing interest in\nautomating the analysis of such phenomena through Machine Learning (ML), access\nto annotated interaction datasets is highly valuable. However, no dataset on\ndyadic child-child interactions explicitly capturing rapport currently exists.\nMoreover, despite advances in the automatic analysis of human behaviour, no\nprevious work has addressed the prediction of rapport in child-child dyadic\ninteractions in educational settings. We present UpStory -- the Uppsala\nStorytelling dataset: a novel dataset of naturalistic dyadic interactions\nbetween primary school aged children, with an experimental manipulation of\nrapport. Pairs of children aged 8-10 participate in a task-oriented activity:\ndesigning a story together, while being allowed free movement within the play\narea. We promote balanced collection of different levels of rapport by using a\nwithin-subjects design: self-reported friendships are used to pair each child\ntwice, either minimizing or maximizing pair separation in the friendship\nnetwork. The dataset contains data for 35 pairs, totalling 3h 40m of audio and\nvideo recordings. It includes two video sources covering the play area, as well\nas separate voice recordings for each child. An anonymized version of the\ndataset is made publicly available, containing per-frame head pose, body pose,\nand face features; as well as per-pair information, including the level of\nrapport. Finally, we provide ML baselines for the prediction of rapport.",
        "updated": "2024-07-05 08:46:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04352v1"
    }
]