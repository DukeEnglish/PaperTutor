[
    {
        "title": "LaRa: Efficient Large-Baseline Radiance Fields",
        "authors": "Anpei ChenHaofei XuStefano EspositoSiyu TangAndreas Geiger",
        "links": "http://arxiv.org/abs/2407.04699v1",
        "entry_id": "http://arxiv.org/abs/2407.04699v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04699v1",
        "summary": "Radiance field methods have achieved photorealistic novel view synthesis and\ngeometry reconstruction. But they are mostly applied in per-scene optimization\nor small-baseline settings. While several recent works investigate feed-forward\nreconstruction with large baselines by utilizing transformers, they all operate\nwith a standard global attention mechanism and hence ignore the local nature of\n3D reconstruction. We propose a method that unifies local and global reasoning\nin transformer layers, resulting in improved quality and faster convergence.\nOur model represents scenes as Gaussian Volumes and combines this with an image\nencoder and Group Attention Layers for efficient feed-forward reconstruction.\nExperimental results demonstrate that our model, trained for two days on four\nGPUs, demonstrates high fidelity in reconstructing 360&deg radiance fields, and\nrobustness to zero-shot and out-of-domain testing.",
        "updated": "2024-07-05 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04699v1"
    },
    {
        "title": "VCoME: Verbal Video Composition with Multimodal Editing Effects",
        "authors": "Weibo GongXiaojie JinXin LiDongliang HeXinglong Wu",
        "links": "http://arxiv.org/abs/2407.04697v1",
        "entry_id": "http://arxiv.org/abs/2407.04697v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04697v1",
        "summary": "Verbal videos, featuring voice-overs or text overlays, provide valuable\ncontent but present significant challenges in composition, especially when\nincorporating editing effects to enhance clarity and visual appeal. In this\npaper, we introduce the novel task of verbal video composition with editing\neffects. This task aims to generate coherent and visually appealing verbal\nvideos by integrating multimodal editing effects across textual, visual, and\naudio categories. To achieve this, we curate a large-scale dataset of video\neffects compositions from publicly available sources. We then formulate this\ntask as a generative problem, involving the identification of appropriate\npositions in the verbal content and the recommendation of editing effects for\nthese positions. To address this task, we propose VCoME, a general framework\nthat employs a large multimodal model to generate editing effects for video\ncomposition. Specifically, VCoME takes in the multimodal video context and\nautoregressively outputs where to apply effects within the verbal content and\nwhich effects are most appropriate for each position. VCoME also supports\nprompt-based control of composition density and style, providing substantial\nflexibility for diverse applications. Through extensive quantitative and\nqualitative evaluations, we clearly demonstrate the effectiveness of VCoME. A\ncomprehensive user study shows that our method produces videos of professional\nquality while being 85$\\times$ more efficient than professional editors.",
        "updated": "2024-07-05 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04697v1"
    },
    {
        "title": "RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation",
        "authors": "Yuxuan KuangJunjie YeHaoran GengJiageng MaoCongyue DengLeonidas GuibasHe WangYue Wang",
        "links": "http://arxiv.org/abs/2407.04689v1",
        "entry_id": "http://arxiv.org/abs/2407.04689v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04689v1",
        "summary": "This work proposes a retrieve-and-transfer framework for zero-shot robotic\nmanipulation, dubbed RAM, featuring generalizability across various objects,\nenvironments, and embodiments. Unlike existing approaches that learn\nmanipulation from expensive in-domain demonstrations, RAM capitalizes on a\nretrieval-based affordance transfer paradigm to acquire versatile manipulation\ncapabilities from abundant out-of-domain data. First, RAM extracts unified\naffordance at scale from diverse sources of demonstrations including robotic\ndata, human-object interaction (HOI) data, and custom data to construct a\ncomprehensive affordance memory. Then given a language instruction, RAM\nhierarchically retrieves the most similar demonstration from the affordance\nmemory and transfers such out-of-domain 2D affordance to in-domain 3D\nexecutable affordance in a zero-shot and embodiment-agnostic manner. Extensive\nsimulation and real-world evaluations demonstrate that our RAM consistently\noutperforms existing works in diverse daily tasks. Additionally, RAM shows\nsignificant potential for downstream applications such as automatic and\nefficient data collection, one-shot visual imitation, and LLM/VLM-integrated\nlong-horizon manipulation. For more details, please check our website at\nhttps://yxkryptonite.github.io/RAM/.",
        "updated": "2024-07-05 17:50:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04689v1"
    },
    {
        "title": "Enhancing Vehicle Re-identification and Matching for Weaving Analysis",
        "authors": "Mei QiuWei LinStanley ChienLauren ChristopherYaobin ChenShu Hu",
        "links": "http://arxiv.org/abs/2407.04688v1",
        "entry_id": "http://arxiv.org/abs/2407.04688v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04688v1",
        "summary": "Vehicle weaving on highways contributes to traffic congestion, raises safety\nissues, and underscores the need for sophisticated traffic management systems.\nCurrent tools are inadequate in offering precise and comprehensive data on\nlane-specific weaving patterns. This paper introduces an innovative method for\ncollecting non-overlapping video data in weaving zones, enabling the generation\nof quantitative insights into lane-specific weaving behaviors. Our experimental\nresults confirm the efficacy of this approach, delivering critical data that\ncan assist transportation authorities in enhancing traffic control and roadway\ninfrastructure.",
        "updated": "2024-07-05 17:50:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04688v1"
    },
    {
        "title": "Embracing Massive Medical Data",
        "authors": "Yu-Cheng ChouZongwei ZhouAlan Yuille",
        "links": "http://arxiv.org/abs/2407.04687v1",
        "entry_id": "http://arxiv.org/abs/2407.04687v1",
        "pdf_url": "http://arxiv.org/pdf/2407.04687v1",
        "summary": "As massive medical data become available with an increasing number of scans,\nexpanding classes, and varying sources, prevalent training paradigms -- where\nAI is trained with multiple passes over fixed, finite datasets -- face\nsignificant challenges. First, training AI all at once on such massive data is\nimpractical as new scans/sources/classes continuously arrive. Second, training\nAI continuously on new scans/sources/classes can lead to catastrophic\nforgetting, where AI forgets old data as it learns new data, and vice versa. To\naddress these two challenges, we propose an online learning method that enables\ntraining AI from massive medical data. Instead of repeatedly training AI on\nrandomly selected data samples, our method identifies the most significant\nsamples for the current AI model based on their data uniqueness and prediction\nuncertainty, then trains the AI on these selective data samples. Compared with\nprevalent training paradigms, our method not only improves data efficiency by\nenabling training on continual data streams, but also mitigates catastrophic\nforgetting by selectively training AI on significant data samples that might\notherwise be forgotten, outperforming by 15% in Dice score for multi-organ and\ntumor segmentation.\n  The code is available at https://github.com/MrGiovanni/OnlineLearning",
        "updated": "2024-07-05 17:50:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.04687v1"
    }
]