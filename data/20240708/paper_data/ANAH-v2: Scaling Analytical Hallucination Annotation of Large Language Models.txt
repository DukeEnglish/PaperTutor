ANAH-v2: Scaling Analytical Hallucination
Annotation of Large Language Models
YuzheGu1∗ ZiweiJi1,2∗ WenweiZhang1† ChengqiLyu1 DahuaLin1,3 KaiChen1†
1ShanghaiAILaboratory 2HongKongUniversityofScienceandTechnology
3TheChineseUniversityofHongKong
{guyuzhe,zhangwenwei,lvchengqi,chenkai}@pjlab.org.cn zjiad@connect.ust.hk
Abstract
Large language models (LLMs) exhibit hallucinations in long-form question-
answering tasks across various domains and wide applications. Current hallu-
cinationdetectionandmitigationdatasetsarelimitedindomainsandsizes,which
struggletoscaleduetoprohibitivelaborcostsandinsufficientreliabilityofexisting
hallucinationannotators. TofacilitatethescalableoversightofLLMhallucinations,
thispaperintroducesaniterativeself-trainingframeworkthatsimultaneouslyand
progressivelyscalesupthehallucinationannotationdatasetandimprovestheaccu-
racyofthehallucinationannotator. BasedontheExpectationMaximization(EM)
algorithm,ineachiteration,theframeworkfirstappliesahallucinationannotation
pipelinetoannotateascaleddatasetandthentrainsamoreaccuratehallucination
annotatoronthedataset. Thisnewhallucinationannotatorisadoptedinthehal-
lucinationannotationpipelineusedforthenextiteration. Extensiveexperimental
resultsdemonstratethatthefinallyobtainedhallucinationannotatorwithonly7B
parameterssurpassestheperformanceofGPT-4andobtainsnewstate-of-the-art
hallucinationdetectionresultsonHaluEvalandHalluQAbyzero-shotinference.
SuchanannotatorcannotonlyevaluatethehallucinationlevelsofvariousLLMs
on the large-scale dataset but also help to mitigate the hallucination of LLMs
generations,withtheNaturalLanguageInference(NLI)metricincreasingfrom
25%to37%onHaluEval. 1
1 Introduction
LargeLanguageModels(LLMs)haveshownremarkablecapabilitiesinvarioustasks[10,11,31,
46, 51]. However, they tend to produce hallucination, i.e., plausible-sounding but unfaithful or
nonsensicalinformation[5,26],thatsignificantlyhinderstheirreal-worldapplications. Initialsteps
toaddressthisissueinvolvethecreationofdatasetsthatcanhelptodetect,annotate,andmitigate
hallucinations [12, 25, 36]. Since the potential hallucinations of LLMs are in various fields, the
spectrum of knowledge in the dataset is expected to be large-scale and comprehensive, covering
variousdomains. Consequently,thesizeanddiversityofdatasetsarecriticalfortheoversightofLLM
hallucinations.
However,constructingandscaling-uphallucinationannotationdatasetsfacesignificanthurdles[8,9,
25,39]. Oneprimarychallengeistheprohibitivelyhighcostsandlaborintensityrequiredfortheir
accurateassessment[39,43],sincethefine-grainedhallucinationannotationrequiresintensiveslabor
forreadinglongdocumentsandannotatingthehallucinationdetailssentencebysentence. Moreover,
due to the insufficiency of accurate human annotations, the reliability of existing hallucination
∗Equalcontribution†Correspondingauthor
1Dataset,code,andmodelarereleasedathttps://github.com/open-compass/ANAH.
Preprint.Underreview.
4202
luJ
5
]LC.sc[
1v39640.7042:viXraannotatorsanddetectorsbecomesanotherpressingconcern[25]. Thesetoolshavebeenfoundto
produceinaccurateresults[9,42,55],e.g.,evenGPT4[1],oneofthemostpowerfulLLMs,isnot
satisfactoryandcannotachieveacompatibleperformanceofhumans[25].
Existingworks[3,22,32,34,50,52,63]haveexploredstrategiesindataaugmentationandself-
trainingtoextenddatasetsizeandboosttheperformanceofmodelsinthefieldsofimagesegmentation,
multi-lingualtranslation,mathreasoning,etc. However,howtoscalethehallucinationannotation
datasets efficiently is under-explored in the community, which significantly hinders the in-depth
analysisandfurthermitigationofLLMshallucinationsatalargescale.
Toaddresstheresearchgap,thispaperproposes
aniterativeself-trainingframeworkdesignedto
scaleupthehallucinationannotationdatasetand
simultaneouslyincreasetheaccuracyofannota-
tors(Fig.1). Theiterativeframeworkcanbeex-
plainedfromtheperspectiveoftheExpectation
Maximization(EM)algorithm. IntheExpecta-
tion(E)step,weapplytheexistingbesthallu-
cinationannotatortoestimatetheground-truth
hallucinationannotationsofthescaleddataset.
We adopt an inference pipeline on top of the
annotatorwithself-consistencystrategy[57]to
provideamorerobustestimationoftheannota-
tions,whichlaysthegroundworkfortraininga
morepreciseannotatorinthesubsequentstep. Figure1:Ouriterativeself-trainingframeworkpro-
IntheMaximization(M)step,wecombinethe gressivelyscalesupthehallucinationannotation
existingannotationswiththescaleddataannota- datasetsize(left)andsimultaneouslyincreasesthe
tionsderivedfromthepreviousEstepstotrain annotator’saccuracy(right)inthreestages.
anewhallucinationannotator. Trainingonmore
dataleadstoamoreaccurateannotatorandamorerobustannotationpipeline,settingthestagefor
thesubsequentroundofannotations.
Theiterativeprocessconsistsofthreestagesofmulti-dimensionaldatascalingasshowninFig.1.
Initially, we train a weak annotator on human annotations. In the second stage, we collect the
hallucination responses from more open-source LLMs for the same questions in the dataset to
improvethegeneralizationabilityofhallucinationannotatorstomodelresponses. Inthethirdstage,
weexpandthenumberoftopicsandquestionsinthedatasetandcollecthallucinationannotationswith
themorerobustannotator. Thisprogressivescalingstrategystabilizestheannotator’sperformance
whenevaluatingfamiliarandunfamiliarresponsesacrossdiversetopics.
Extensiveexperimentalresultsshowthatourenhancedannotatorsignificantlyoutperformsexisting
models,includingtheadvancedGPT-4,intermsofaccuracy. Ourannotatornotonlyperformsbest
onthein-domainfine-grainedhallucinationannotationdatasetANAH(89.24%)butalsoobtains
newstate-of-the-art(SOTA)resultsonHaluEval(81.54%)andHalluQA(94.44%)underzero-shot
setting. Inaddition, theannotatorautomatesthehallucinationevaluationonthedataset, offering
a comprehensive benchmark for the research community to evaluate the hallucination levels of
numerousopen-sourcemodels,providingapracticalreferenceforfuturehallucinationmitigationof
LLMs. Withasimplererankingstrategywiththeannotator,wereducethehallucinationofthefinal
LLMgenerationsfrom25%to37%.
2 RelatedWork
Self-improvement of Large Models. As Large Language Models (LLMs) become more and
morepowerful,thecommunitystartstoexploredifferentstrategiestoachievetheself-improvement
of LLMs, i.e., to improve the LLMs using the supervision from LLMs. For example, existing
workshaveexploredself-alignmentusingLLMswithethicalprinciples[3,52,63]. Therearealso
methods[22,34,50,64]strengthenLLM’scapabilitiesontaskssuchasreasoningbytrainingthe
LLMsonthehigh-qualityresponsesfromthemselvesonthesamequestions. Inthefieldofcomputer
vision,SAM[32]introducesmanualandmodel-assistedlabelingtoexpandtheimagesegmentation
datasetandenhancetheperformanceofimagesegmentationmodels. However,theapplicationofself-
2Figure2:TheschemaofEM-basedinteractiveself-trainingframework.IntheE-step,givenunlabeled
newdatafromtheDataGrowthFlywheel,theannotatorpredictsNcandidateoutputsy. Thenthe
representativeannotationy∗ischosenviaself-consistency. Asaresult,weconstructalargerdataset
bycollectingthenewannotations. IntheM-step,wetrainanannotatoronthelargerdatasetaligned
toourtrainingformat. Thisannotationprocessconsistsofthreephases: FactualExistenceJudgment,
ReferenceInformationExtraction,andHallucinationTypeJudgment. Asaresult,wegainastronger
annotatorwithhigheraccuracy.
improvementisunder-exploredinfine-grainedhallucinationannotation. Thisfieldischallengingfor
automaticannotatorsduetoitsmeticulousnature,whichrequiresfinevalidationwithlongdocuments.
Itisalsonoteworthythatmostself-improvementworksrequireextraresourcessuchashumanlabor
orasupplementarymodel[32,34]. Incontrast,ourpipelineisself-sufficient,relyingsolelyonthe
annotatormodelandtheinitialdataset.
Hallucination Annotation Dataset. The development of the hallucination annotation dataset is
thecornerstonefordetectinghallucinationsinmodels’output. Thesedatasetscanbeusedtotrain
a hallucination detector/annotator and evaluate the hallucination level via the detector/annotator.
Early works [19, 20, 23, 33, 36, 40, 44, 54, 56, 61] in this domain tended to broadly classify
entireresponsesaseitherhallucinatoryornot,providingacoarse-grainedanalysisofhallucination
occurrences. Recentworks[25,43]annotatehallucinationsinamorefine-grainedandmeticulous
way. Despitethisprogress,thesedatasets,especiallythosehavingfine-grainedannotations,suffer
fromlimitationsinsizeandscalabilityduetothehighcostsassociatedwiththeusageofhuman
annotatorsorcommercialmodelslikeGPT4. Inaddition,thedifficultyofthistaskandthelimited
humanannotationsresultinunsatisfactoryperformanceoftheautomatichallucinationannotator.
HallucinationMitigation.Consideringtheharmofhallucinations,researchershaveexploredvarious
techniquesformitigatinghallucinations. Techniquessuchasmulti-tasklearning[21,59], model
editing [16, 27], and fine-grained RLHF [60] are proposed to suppress hallucination tendencies
duringtraining. Alternativestrategieshavebeenproposedthatdonotrequirefurthermodeltraining,
includingdifferentdecodingstrategies[13,37,47,49],multi-agentmethods[18],andvariantsof
theChain-of-Thoughtapproachinvolvingverificationorreflection[17,28,35,58]. OurANAH-v2
showsefficiencyinhallucinationmitigationasare-rankerandhasthepotentialtocombinewiththe
existingmethodssuchasfine-grainedRLHF.
3 Method
Thispaperproposesaniterativeself-trainingframeworktosimultaneouslyscaleupthehallucination
datasetandimprovetheaccuracyofthehallucinationannotator.Wefollowtheanalyticalhallucination
annotation(§3.1)toannotatethehallucinationsentence-by-sentence. Themulti-iterationframework
istheoreticallygroundedintheEMalgorithm(§3.2)andinvolvesthreestagestoprogressivelyscale
thedatasetinmultipledimensions(§3.3). Wealsorevealhowthehallucinationannotatorscanbe
appliedforhallucinationevaluationandmitigation(§3.4).
33.1 AnalyticalHallucinationAnnotation
Theaimofahallucinationannotatoristoidentifyhallucinationsinthemodelresponses. ANAH[25]
developedafine-grainedannotationmethodthatlocatesreferencepointsinthedocumentforeach
sentenceandmakeshallucination-typejudgments,withthewholeprocesscompletedinoneturn
of dialog. However, this hybrid task diverges from the human judgment processes and fails to
clearlyindicatetherelationshipbetweenreferencepointsandhallucinationjudgments,resultingin
unsatisfactoryannotationaccuracy.
InsteadofusingtheoriginalANAHtrainingprompts,wedevelopedamorereliabletrainingmethod
tailoredtothehallucinationannotationprocess. AsdepictedinthelowerrightpartofFig.2,the
processisoutlinedinthreephases: (1)FactualExistenceJudgment,wheretheannotatorassesses
whethertheprovidedsentencecontainsverifiablefacts.Ifnofactualcontentispresent,thesentenceis
categorizedas‘NoFact’andrequiresnofurtherannotation. (2)ReferenceInformationExtraction,
wheretheannotatorextractsrelevantreferencepointsfromthedocumentsrelatedtothequestionand
answer. (3)Hallucination-TypeJudgment,wheretheannotatordeterminesthetypeofhallucination
basedontheextractedreferencepoints. Ifthesentencealignswiththereferences,itisclassified
as‘NoHallucination’. Ifitcontradictsthereferences,itisdeemeda‘ContradictoryHallucination’.
Ifitlackssupportingevidenceandcannotbeverified,itislabeledas‘UnverifiableHallucination’.
Theabovethreephaseswillformamulti-turndialogueintrainingdata. ComparedtotheANAH
approach,whichinvolvessimultaneousjudgmentsonmultiplecriteria,ourphasedprocessaligns
morecloselywithhumancognitivejudgmentprocesses. Thedetaileddataformatandpromptsfor
ourannotationprocessareinAppendixA.
3.2 Expectation-MaximizationAlgorithm
Simultaneouslyscalingupthedatasetandimprovingtheaccuracyoftheannotatorcanbeformulated
bytheEMalgorithm. FortheinputsetX,weneedtoestimatetwohiddenvariablessimultaneously,
theoutputsetY andthemodelparametersθ. Specifically,basedonthetaskformulationin§3.1,
wedefinetheinputxfromtheinputsetX ofthehallucinationannotatorconsistsofaquestion,a
sentencetobeannotated,andareferencedocument. Theexpectedoutputytobeestimatedinthe
dataoutputsetY includesthefactualinformationf,thekeyreferencepointsrfromthereference
document, and the type of hallucination h. We maximize the log-likelihood estimation of Y by
alternatelyperformingtheE-StepandtheM-Steptoupdatethemodelparametersθ:
θ =argmaxE [logp (X,Y |θ)] (1)
θ
pθ(Y|X,θ) θ
E-Step. AstraightforwardapproachtoestimatingY istouseasinglemodeltopredictannotations.
However,thismethodlackssufficientaccuracy[41]. Toimprovetheaccuracyandstabilityofthe
estimation of Y, we introduce the self-consistency method [57], which provides a more robust
representationofthedistributionoftheY. AsshowninFig.2. Foreachinputx,weperformmultiple
samplingstoyieldKindependentoutputsy ={y1,··· ,yi,··· ,yK},wherethei-thoutputsample
yiiscomposedoffactualinformation(fi),referencepoint(ri)andhallucinationtype(hi). Weusea
self-consistencymetrictoselectthemostrepresentativesampley∗amongalloutputs:
y∗ =(f∗,r∗,h∗)=self-consistency(y) (2)
Duringthisselectionprocess,weconsiderthehallucinationtypeh,referencepointr,andfactual
informationf inturn. Wedeterminethemostcommonhallucinationtypeh∗bytallyingamajority
voteacrossallsamples,denotedash∗ =argmax (cid:80)K I(h =h). Then,weformthecandidate
h i=1 i
reference set R by taking the corresponding r from the output containing the h∗. We select the
most“consistent”referencepointr∗bycomparingthecosinesimilarities. ForeachriinR,wefirst
calculateitsaveragecosinesimilaritywiththeotherelementsinR. Afterthat,weselectthereference
pointr∗withthehighestaveragecosinesimilarity: r∗ =argmax ( 1 (cid:80)n sim(ri,rj)).
ri∈R n−1 j=1,j̸=i
Finally,with(r∗,h∗),wecanuniquelyselectthecorrespondingf∗.
M-Step. FollowingtherobustestimationintheE-step,theM-stepupdatesthemodelparameters
tomaximizethelikelihoodoftheselectedoutputy∗. CombiningEq.1andEq2,weformulatethe
parameterupdatestrategyatiterationt:
θt+1 =argmaxE (cid:2) E [logp (x,y∗ |θ)](cid:3) (3)
θ
x∼X y∼p θt(y|x,θ) θ
4Stage #Topic #Response #Sentence
Stage1 800 2798 12188
Stage2 800 46006 209241
Stage3 3172 196930 822520
Table1: ThedatasetsizeforANAH-v2indif-
ferentstages,includingthenumberoftopics,
modelresponses,andannotatedsentences.
Figure3: Thetopicdistributionbychartof
categories(inner)anddomains(outer).
3.3 Multi-dimensionalDataScaling
GroundedintheEMalgorithm,ourframeworkoperatesinaniterativemanner. Thismulti-iteration
processactsasadatagrowthflywheeltoprogressivelyscaleupthedatasetinmultipledimensions,
consistingofthreestages:
Stage1: SeedDataandBasicAnnotator. WeutilizeANAHdataset[25]asourseeddata,which
includes over 700 topics and around 4,300 LLM-generated questions and responses. For each
response,ANAHprovidesthehallucinationtypeforeverysentence,determinedthroughahuman-in-
the-loopapproach. Wetrainaninitialhallucinationannotator,notedasANAH-v2Stage1,withthis
seeddatausingtheannotationmethoddescribedin§3.1.
Stage 2: Scaling up in Response Dimension. In Stage 1, for each question, ANAH provides
responsesthatGPT-3.5generateswiththereferencedocument,whileInternLM-7Bgenerateswithout
anyreferencedocument. Wefirstaugmentthedataset’smodelresponsesbycollectingresponses
tothesameexistingquestionsfrom13additionalopen-sourcemodelsofvarioussizesandseries.
Foreachmodel,responseswerecollectedwithandwithoutknowledgeofreferencedocuments. The
promptdetailsareinAppendixB.Afterfilteringoutsimilarmodelresponses,theseresponsesare
annotatedsentencebysentenceusingtheself-consistencypipelinewithANAH-v2Stage1. Thenewly
annotateddata,combinedwiththeseeddata,wasusedtotrainANAH-v2Stage2.
Stage 3: Scaling up in Topic Dimension. We expand the topic coverage along four categories:
location,person,event,andthing,parallelingANAH’sconfiguration. Foreachtopic,wegenerate
severalquestionsbasedontheprovidedreferencedocuments(moredetailsinAppendixB).Then,we
usethesamemethodinStage2tocollectresponsesfrommultiplemodelsandannotatetheresponse
followingthesameprocedureasinStage2,usingANAH-v2Stage2annotator. Theresultingdataset,
combinedwithdatafromthepreviousstages,isusedtotraintheultimateannotatorversion.
OveralStatistics. Thefinaldatasetencompassesbothover∼3ktopics,∼196kmodelresponses,and
∼822kannotatedsentences,inEnglishandChinese(Tab.1). Thetopicscovercelebrities,events,
locations,andthings,andspanawidearrayofdomains,suchaspolitics,health,andsports(Fig.3).
Thestatisticsunderscorethecomprehensivenessandextensivescaleofourdataset.
3.4 Applications
HallucinationEvaluation. Astheaccuracyofthehallucinationannotatorsbecomessatisfactory,we
canapplyittoautomatetheprocessofevaluatingthehallucinationlevelsofexistingopen-source
models. Aftercategorizingsentencesintofourdistincttypes(introducedin§3.1),weconsidertype
ContradictoryandUnverifiableHallucinationassentenceswithhallucinations,andtypeNoFact
andNoHallucinationassentenceswithouthallucinations. Thistoolenablesresearcherstoassess
thereliabilityandaccuracyofgeneratedtexts,ensuringmodelscanberesponsiblyintegratedinto
practicalapplications.
Hallucination Mitigation. We further show a simple re-ranking strategy to mitigate the LLM’s
hallucinationswiththeannotator,whereasmoreadvancedstrategiescanbeexploredinfutureresearch.
Specifically,weadoptourannotatorθ forresponsere-ranking. LLMfirstgeneratesN candidate
responses{G ,··· ,G }bytop-ksampling. ThenweselectthebestresponseG∗withthelowest
1 N
5hallucinationrateoverallthegeneratedresponsesasbelow:
|{a |a ∈{A ,A }|
G∗ = argmin θ,i,n θ,i,n C U (4)
L
n∈{1,···,N} n
wherea isthegeneratedannotationtypebyθgiventheinputx includingaquestion,thei-th
θ,i,n i,n
sentence to be annotated from G , and a reference document. A and A means sentence type
n C U
ContradictoryandUnverifiableHallucination,respectively. L isthesentencenumberofG .
n n
4 Experiment
4.1 ExperimentalSetup
Implementation. Inourexperimentalframework,weadoptthepre-trainedInternLM2-7B[7]model
tofine-tunethehallucinationannotator. FurtherimplementationdetailscanbefoundinAppendixC.
Evaluation. WeuseasubsetoftheANAH[25]dataasatestset,whichisnotusedfortrainingin
stage1. Toassesstheperformanceoftheannotatorinpredictinghallucinationtypes,weutilizeF1
andAccuracy. WealsoemployRougeL[38]andBertScore[66]tocomparethegeneratedtextwith
gold-standardhumanreferenceintermsofgram,continuity,orderandsemantics.
4.2 OverallResults
The last 3 rows of Tab. 2 illustrate
theperformanceofANAH-v2ateach
Model F1↑ ACC↑ R↑ BERT↑
stage of Data Scaling in § 3.3. The
performanceprogressivelyimproves GPT-4 87.11 86.97 86.32 96.21
with the increasing dataset number
ANAH-7B 78.69 79.92 58.51 87.27
(see in Tab. 1) in successive stages. ANAH-20B 80.49 81.01 58.82 88.44
This trend underscores the scalabil-
ANAH-v2-Stage1 84.45 84.85 60.10 88.43
ityandeffectivenessofourhallucina-
ANAH-v2-Stage2 87.75 88.18 67.28 90.80
tionannotationframework. Remark-
ANAH-v2-Stage3 89.30 89.55 69.44 91.43
ably,ANAH-v2surpassesGPT-4with
the F1 of 87.78% and the accuracy
of 88.03% at Stage 2 3. Eventually, Table2:EvaluationresultsforGPT4,ANAH,andANAH-v2
weachievetheF1of89.30%andthe at each stage, where “R” and “BERT”, refer to “RougeL”
accuracyof89.55%atStage3.
and“BERTScore”,respectively.2
WealsoobservethatANAH-v2alreadyoutperformsANAH-20BatStage1(84.85%v.s. 81.01%
inaccuracy)withonly7Bparameters,whenbeingtrainedonthesamehallucinationcorpus. This
superiorperformanceisattributedtotheinnovativemulti-turndialoguetrainingstrategy(§3.1).
4.3 AblationStudies
Impact of Self-Consistency. To verify the effectiveness of self-consistency during inference
inE-Step(introducedin§3.2),wecomparetheperformanceoftheannotatorwithdifferentself-
consistencysettingsinTab.3. Whentheannotatormodelwiththesametrainingdataateachdata
scalingstage,theinferencestrategywithself-consistency(w/SC)consistentlyoutperformswithout
self-consistency (w/o SC), where the annotator generates only once for each input. Therefore,
self-consistencyimprovestheaccuracyandstabilityoftheestimationofhallucinationannotations.
InM-Step,wetrainthemodelondatafromtheE-Stepoftheprecedingiteration. Weobservethat
whentheannotatormodelwiththesameinferencestrategy,themodeltrainedonself-consistently
processeddata(w/SC)surpassestheperformancewithdatageneratedthroughasinglepass(w/o
SC).Thisfindingindicatesthattrainingdataprocessedthroughself-consistencyleadstoastronger
2ThefirstthreerowsofdataarefromANAH[25].
3Notably,GPT-4annotationsareusedaspre-annotationinANAH,whichmakestheRougeLandBERTScore
higher than the zero-shot setting. We use “accuracy” as our primary metric because the type judgment is
determinativeoftheannotationquality.Forexample,anannotationthatwronglyjudgestype(lowaccuracy)but
findsthecorrectreferencefragment(highRougeL/BERTScore)remainscompletelyunacceptable.
6Model TrainData InferStrategy F1↑ ACC↑ R↑ BERT↑
- w/oSC 80.95 81.67 58.26 88.70
ANAH-v2-Stage1
- w/SC 84.45 84.85 60.10 88.43
w/oSC w/oSC 83.80 83.94 62.93 89.20
w/oSC w/SC 83.98 84.24 64.92 90.01
ANAH-v2-Stage2
w/SC w/oSC 84.65 85.15 61.08 88.47
w/SC w/SC 87.75 88.18 67.28 90.80
w/oSC w/oSC 86.24 86.67 66.10 90.26
w/oSC w/SC 87.78 88.18 68.18 91.01
ANAH-v2-Stage3
w/SC w/oSC 87.71 88.03 67.45 90.63
w/SC w/SC 89.30 89.55 69.44 91.43
Table3: Ablationstudyforannotatorsindifferentself-consistencysettings. Here,forInferStrategy,
“w/SC”meansinferencewithself-consistency,whichisthedefaultsettingofANAH-v2. “w/oSC”
meansinferencewithoutself-consistency,wheretheannotatorgeneratesonlyonceforeachinput. For
TrainData,“w/SC”meansthetrainingdatafromthepreviousstageisgeneratedbyself-consistency,
wherethedefaultsettingofANAH-v2,while“w/oSC”meansthetraindataisgeneratedwithout
self-consistency.
Model Setting F1↑ ACC↑ R↑ BERT↑
progressive 89.30 89.55 69.44 91.43
ANAH-v2-Stage3
non-progressive 85.88 86.36 66.10 90.26
Table4: Ablationstudyforannotatorstrainedwithprogressiveandnon-progressivedatascaling.
Here,“progressive”meansthatthetrainingdataisprogressivelyannotatedbythecontinuallyupdated
annotator,whichisthedefaultsettingofANAH-v2. “non-progressive”meansthatthetrainingdata
scalingonlyleveragesannotationsgeneratedbythebasicannotatorfromStage1.
annotator. This improvement can be attributed to the reduced distribution variance between the
inferredlabelsandtruelabels.
ImpactofProgressiveDataScaling. Toassesstheimpactofprogressivedatascaling(introduced
in§3.3),wecomparetheperformanceofannotatorswithdifferenttypesofdatascalinginTab.4.
In our progressive approach, the updated annotator from Stage 2 is employed to annotate the
responsesfromadditionaltopics,continuouslyenrichingthetrainingdata. Conversely,inthenon-
progressiveapproach,thebasicannotatorfromStage1isemployedtogenerateannotationsforthe
additionaltrainingdataduringStage3. Withthesamesizeoftrainingdata,theannotatortrained
onnon-progressivedatascalingunderperformsthatwithourprogressivedatascaling,provingthe
effectivenessofourprogressivedatascaling.
Impact of Training Strategy. We also analyze different training strategies for annotators in
differentdatascalingstagesinTab.5. Inourdefaulttrainingprocess,wemixthenewlyannotated
datawitholddatatore-trainanannotator. Alternatively,weonlyusethenewlyannotateddatato
furthertraintheannotatormodelfromthepreviousstage. Theresultsdemonstratethatourtraining
strategywithmixedtrainingdataperformsbetterthanfurthertrainingwithnewdata. Theintegration
ofdifferentdataqualitiesacrosstrainingstagesimprovestherobustnessoftheannotatormodel.
4.4 GeneralizationCapabilityAnalysis
WefurthervalidatetheeffectivenessofANAH-v2onotherhallucinationdetectiondatasetsusing
twothird-partydatasets: HaluEval[36]forEnglishandHalluQA[12]forChinese. Eachdataset
providesfourcomponents: questions,referencedocuments,responses,andlabelsindicatingwhether
theresponsescontainhallucination. Foreachquestion,weletANAH-v2judgethetypeofresponses
containingandnotcontainingthehallucinationseparately. NotethatinHaluEvalweonlyusetheQA
samplesandinHalluQA,weonlyusethesamplesthatprovideatextualreferencedocument,which
alignswithourannotator’sdesignedsetting.
4ThefirstsixrowsofdataarefromKnowHalu[65].
7Model TrainStrategy F1↑ ACC↑ R↑ BERT↑
mix 87.75 88.18 67.28 90.80
ANAH-v2-Stage2
further 85.50 85.91 62.15 89.30
mix 89.30 89.55 69.44 91.43
ANAH-v2-Stage3
further 87.73 86.52 68.58 91.03
Table5: Ablationstudyforannotatorindifferenttrainstrategysettings. Here,“mix”meansthatthe
newdatageneratedinthecurrentiterationismixedwiththeolddatatotrainanewannotator,which
isthedefaultsettingofANAH-v2. “further”meansthatonlythenewdataisusedtofurthertrainthe
annotatorfromthepreviousstage.
TheprimarymetricweuseforevaluationisAc-
Dataset Model Method ACC↑
curacyindeterminingthetypeofresponse. We
GPT4 Zero-Shot 65.05
comparethezero-shotperformanceofANAH-
WiKiChat[48] 49.10
v2 with current SOTA results on HaluEval
GPT3.5 HaluEval 56.90
achievedbyKnowHalu[65]andbaselineresults
KnowHalu 80.30
byGPT-4.
HaluEval 61.00
HaluEval Starling-7B
The results in Tab. 6 reveal that our annota- KnowHalu 80.70
tionmodelachievesnotableaccuraciesonboth ANAH-v2-Stage1 Zero-Shot 79.85
ANAH-v2-Stage2 Zero-Shot 81.24
HaluEval and HalluQA. Remarkably, ANAH-
ANAH-v2-Stage3 Zero-Shot 81.54
v2-Stage3 obtains new SOTA accuracy on
GPT4 Zero-Shot 62.81
HaluEval(81.54%)andHalluQA(94.44%)even
underazero-shotsetting,underscoringthegen- HalluQA ANAH-v2-Stage1 Zero-Shot 91.74
ANAH-v2-Stage2 Zero-Shot 92.63
eralizationcapabilityofANAH-v2. Moreover,
ANAH-v2-Stage3 Zero-Shot 94.44
wefindthatANAH-v2-Stage3outperformsthe
annotatorsfromStage1andStage2,furtherprov- Table6:Annotatoraccuracyusingdifferentmodels
ingthedatascalingstrategyeffectivelystabilizes andmethodsonHaluEvalandHalluQA.4
performancewhendealingwithunfamiliarre-
sponses.
4.5 Application
Hallucination Evaluation Benchmark. Our ANAH-v2 dataset and annotator can serve as a
benchmarkforthehallucinationlevelsingeneratedtextsbyexistingmodels. AsshowninTab.7,we
evaluatetheperformanceofvariousLLMs,includingInternLM2[7],Qwen1.5[2],Baichuan2[4],
Mistral[29,30],DeepSeek-LLM [6],andLlama2[53],spanningdifferentmodelsizes. Wealsooffer
detailedevaluationresultsondifferentlanguagesandcategoriesoftopicstodeepenourunderstanding.
WefindthatallmodelsexhibitsuperiorperformanceinEnglishcomparedtoChinese,underscoring
the need for further research to understand and mitigate language-dependent discrepancy. The
performancesofallmodelswithreferencedocumentsarebetterthanthosewithout. Qwen1.5-14B
achieves the lowest hallucination rate when using reference documents (5.33%) and Deepseek-
67Bachievesthelowesthallucinationratewhenreferencedocumentsarenotprovided(47.17%).
Moreover,wefindnocleartrendintheperformancedistributionacrossfourcategoriesoftopics.
Inaddition,theresultsofdifferentstagesofannotatorsinTab.A1, A2,and 7showthatthereisa
consistenttrendandfixedbiasedorderingrelationshipbetweenLLMs,thusconfirmingthereliability
ofourassessmentmethod. MoredetailsareinAppendixD.
HallucinationMitigation. Besidesbeingusedtomeasurehallucinationlevels,ANAH-v2canalso
beusedtomitigatehallucinations.WeusetheQAsamplesfromHaluEval,whichcomprisesquestions
and correct answers from HotPotQA [62]. We use two models InternLm2-7B and LLaMA2-7B.
Foreachmodel, wegenerate36candidateresponsesbytop-ksampling(k=40), thenre-rankthe
responsesusingourannotator. Toquantifythehallucinationdegree,weemployRougeL,BertScore,
NLI,andQuestionEval. Thesemetricsmeasurethecongruencebetweenthegeneratedresponses
withthegoldenresponsesand/orreferencedocuments.
ResultsinTab.8showaclearreductionofhallucinationlevelsafterthere-rankingprocessviaour
annotator.Forinstance,theNLImetricforLLaMA2-7Bshowsanotableincrease,risingfrom25.00%
to37.01%. Thissuggeststhattheapplicationofourannotativeapproachcansignificantlymitigate
theissueofhallucinationsinlanguagemodeloutputs.
8Person↓ Event↓ Thing↓ Location↓
Model Setting Overall↓
ZH EN ZH EN ZH EN ZH EN
w/oRef 87.84 87.24 47.65 91.37 73.83 89.49 77.13 94.26 82.92
InternLM2-7B
w/Ref 19.02 26.12 5.57 19.26 4.20 19.40 2.59 9.77 3.26
w/oRef 78.20 74.67 47.25 82.36 72.32 82.16 80.29 87.32 81.49
InternLM2-20B
w/Ref 16.52 19.99 4.25 15.23 7.00 19.66 7.20 3.42 5.76
w/oRef 80.09 79.22 52.81 82.61 75.7 83.26 73.85 86.56 78.09
Qwen1.5-7B
w/Ref 6.96 5.82 2.77 5.27 3.76 9.70 3.69 4.90 4.40
w/oRef 68.82 65.63 44.91 70.25 68.24 72.36 70.76 73.37 72.69
Qwen1.5-14B
w/Ref 5.33 5.01 1.23 4.56 2.02 7.38 2.70 2.53 2.00
w/oRef 61.62 56.49 29.76 61.62 56.78 67.42 62.92 67.97 64.36
Qwen1.5-72B
w/Ref 15.89 19.27 4.62 13.85 3.18 18.99 3.80 5.50 4.26
w/oRef 73.99 72.13 44.99 75.77 65.98 76.84 73.01 71.51 74.17
Baichuan2-7B
w/Ref 43.68 61.71 64.56 37.87 26.41 35.4 29.17 54.3 14.51
w/oRef 69.85 67.02 41.24 71.63 63.13 73.32 66.2 68.77 71.35
Baichuan2-13B
w/Ref 38.39 58.86 60.53 43.20 21.9 25.74 17.81 28.99 7.23
w/oRef 85.40 89.98 52.32 87.03 72.33 87.41 71.97 91.19 77.25
Mistral-7B
w/Ref 30.24 42.66 22.83 30.85 13.77 26.02 27.15 42.11 7.23
w/oRef 76.12 80.96 30.75 76.78 55.32 83.32 61.61 87.28 65.51
Mistral-8x7B
w/Ref 7.95 8.29 2.78 6.17 5.84 9.91 7.94 3.99 6.63
w/oRef 64.46 65.98 39.62 67.59 60.69 69.51 56.15 69.29 59.15
Deepseek-7B
w/Ref 23.02 6.73 44.95 28.38 4.92 25.00 24.25 18.18 12.61
w/oRef 47.17 54.91 15.81 46.28 31.48 65.57 34.23 59.96 36.02
Deepseek-67B
w/Ref 12.05 12.61 4.17 9.52 2.00 15.79 13.36 18.65 8.33
w/oRef 84.22 88.36 52.00 84.95 74.18 92.48 77.89 89.84 78.91
Llama2-7B
w/Ref 58.16 82.5 10.64 76.96 10.00 64.72 12.33 69.75 20.48
w/oRef 78.84 80.26 43.18 81.88 70.25 87.85 70.52 84.44 73.94
Llama2-13B
w/Ref 52.17 79.43 14.81 47.85 4.00 49.59 11.72 77.50 27.53
Table7: Hallucinationrateofopen-sourcemodelsaccordingtoANAH-v2annotatoranddataset.
Model Setting QuestEval↑ NLI↑ BERT↑ RourgeL↑
baseline 37.84 31.25 83.76 19.34
LLaMA2-7B
re-rank 38.50 36.03 84.45 21.92
baseline 37.33 25.00 83.57 20.55
InternLM2-7B
re-rank 38.89 37.01 84.57 22.39
Table8: EvaluationresultsforhallucinationmitigationwithLLaMA2-7BandInternLM2-7Bon
HaluEval. Here,“baseline”meansthedirectgenerationresults,and“re-rank”meanstheresultswith
ourre-rankingmitigationmethod.
5 ConclusionandFutureWork
In this paper, we aim to explore a scalable framework for the oversight of LLM hallucinations.
Throughiterativeself-training,weprogressivelyexpandthediversityandscaleofthedatasetand
improvetheaccuracyofthehallucinationannotator. ThefinallyobtainedANAH-v2,forthefirst
time,outperformsGPT-4invarioushallucinationdetectionbenchmarkswithonly7Bparametersand
obtainssuperiorzero-shotperformanceonthird-partyhallucinationdetectionbenchmarks. ANAH-
v2 not only provides an automatic hallucination evaluation benchmark with the scaled dataset,
whichpavesthewayforfutureresearchonhallucinationmitigationbutalsoexhibitspotentialin
hallucinationmitigationbythesimplere-rankingstrategy. WebelieveANAH-v2canalsobenefit
morehallucinationmitigationstrategiessuchasfine-grainedRLHF.
With the large-scale dataset as seed data, future work can explore creating hallucination annota-
tion data in other NLG tasks such as dialogue generation. Another direction is to improve the
generalizabilityoftheannotatoracrossdifferentlanguages,tasks,andtopics.
9References
[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D.,
Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint
arXiv:2303.08774(2023)
[2] Bai,J.,Bai,S.,Chu,Y.,Cui,Z.,Dang,K.,Deng,X.,Fan,Y.,Ge,W.,Han,Y.,Huang,F.,etal.:
Qwentechnicalreport.arXivpreprintarXiv:2309.16609(2023)
[3] Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A.,
Mirhoseini,A.,McKinnon,C.,etal.: Constitutionalai: Harmlessnessfromaifeedback.arXiv
preprintarXiv:2212.08073(2022)
[4] Baichuan: Baichuan2: Openlarge-scalelanguagemodels.arXivpreprintarXiv:2309.10305
(2023),https://arxiv.org/abs/2309.10305
[5] Bang,Y.,Cahyawijaya,S.,Lee,N.,Dai,W.,Su,D.,Wilie,B.,Lovenia,H.,Ji,Z.,Yu,T.,Chung,
W.,Do,Q.V.,Xu,Y.,Fung,P.: Amultitask,multilingual,multimodalevaluationofchatgpton
reasoning,hallucination,andinteractivity.arXivpreprintarXiv:2302.04023(2023)
[6] Bi,X.,Chen,D.,Chen,G.,Chen,S.,Dai,D.,Deng,C.,Ding,H.,Dong,K.,Du,Q.,Fu,Z.,
etal.: Deepseekllm: Scalingopen-sourcelanguagemodelswithlongtermism.arXivpreprint
arXiv:2401.02954(2024)
[7] Cai,Z.,Cao,M.,Chen,H.,Chen,K.,Chen,K.,Chen,X.,Chen,X.,Chen,Z.,Chen,Z.,Chu,P.,
etal.: Internlm2technicalreport.arXivpreprintarXiv:2403.17297(2024)
[8] Cao,Z.,Yang,Y.,Zhao,H.: Autohall: Automatedhallucinationdatasetgenerationforlarge
languagemodels.arXivpreprintarXiv:2310.00259(2023)
[9] Chen,X.,Song,D.,Gui,H.,Wang,C.,Zhang,N.,Yong,J.,Huang,F.,Lv,C.,Zhang,D.,Chen,
H.: Factchd: Benchmarkingfact-conflictinghallucinationdetection.IJCAIabs/2310.12086
(2023),https://api.semanticscholar.org/CorpusID:264289140
[10] Chen,Z.,Du,W.,Zhang,W.,Liu,K.,Liu,J.,Zheng,M.,Zhuo,J.,Zhang,S.,Lin,D.,Chen,K.,
Zhao,F.: T-eval: Evaluatingthetoolutilizationcapabilitystepbystep.CoRRabs/2312.14033
(2023).https://doi.org/10.48550/ARXIV.2312.14033,https://doi.org/10.48550/arXiv.
2312.14033
[11] Chen,Z.,Liu,K.,Wang,Q.,Zhang,W.,Liu,J.,Lin,D.,Chen,K.,Zhao,F.: Agent-flan: Design-
ingdataandmethodsofeffectiveagenttuningforlargelanguagemodels.CoRRabs/2403.12881
(2024).https://doi.org/10.48550/ARXIV.2403.12881,https://doi.org/10.48550/arXiv.
2403.12881
[12] Cheng, Q., Sun, T., Zhang, W., Wang, S., Liu, X., Zhang, M., He, J., Huang, M., Yin, Z.,
Chen,K.,etal.: Evaluatinghallucinationsinchineselargelanguagemodels.arXivpreprint
arXiv:2310.03368(2023)
[13] Chuang,Y.S.,Xie,Y.,Luo,H.,Kim,Y.,Glass,J.,He,P.: Dola: Decodingbycontrastinglayers
improvesfactualityinlargelanguagemodels.arXivpreprintarXiv:2309.03883(2023)
[14] Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A.,
Wendell, P., Zaharia, M., Xin, R.: Free dolly: Introducing the world’s first truly
open instruction-tuned llm (2023), https://www.databricks.com/blog/2023/04/12/
dolly-first-open-commercially-viable-instruction-tuned-llm
[15] Contributors,L.: Lmdeploy: Atoolkitforcompressing,deploying,andservingllm.https:
//github.com/InternLM/lmdeploy(2023)
[16] Daheim,N.,Dziri,N.,Sachan,M.,Gurevych,I.,Ponti,E.M.:Elasticweightremovalforfaithful
andabstractivedialoguegeneration.arXivpreprintarXiv:2303.17574(2023)
[17] Dhuliawala,S.,Komeili,M.,Xu,J.,Raileanu,R.,Li,X.,Celikyilmaz,A.,Weston,J.: Chain-of-
verificationreduceshallucinationinlargelanguagemodels.arXivpreprintarXiv:2309.11495
(2023)
10[18] Du,Y.,Li,S.,Torralba,A.,Tenenbaum,J.B.,Mordatch,I.:Improvingfactualityandreasoningin
languagemodelsthroughmultiagentdebate(52023),http://arxiv.org/abs/2305.14325
[19] Durmus,E.,He,H.,Diab,M.:Feqa:Aquestionansweringevaluationframeworkforfaithfulness
assessmentinabstractivesummarization.In: Proceedingsofthe58thAnnualMeetingofthe
AssociationforComputationalLinguistics.pp.5055–5070(2020)
[20] Dziri, N., Kamalloo, E., Milton, S., Zaiane, O., Yu, M., Ponti, E.M., Reddy, S.: Faithdial:
Afaithfulbenchmarkforinformation-seekingdialogue.TransactionsoftheAssociationfor
ComputationalLinguistics10,1473–1490(2022)
[21] Garg, S., Peitz, S., Nallasamy, U., Paulik, M.: Jointly learning to align and translate with
transformermodels.In: Proceedingsofthe2019ConferenceonEmpiricalMethodsinNatural
LanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing
(EMNLP-IJCNLP).pp.4453–4462(2019)
[22] Gulcehre,C.,Paine,T.L.,Srinivasan,S.,Konyushkova,K.,Weerts,L.,Sharma,A.,Siddhant,
A.,Ahern,A.,Wang,M.,Gu,C.,etal.: Reinforcedself-training(rest)forlanguagemodeling.
arXivpreprintarXiv:2308.08998(2023)
[23] Gupta,P.,Wu,C.S.,Liu,W.,Xiong,C.: Dialfact: Abenchmarkforfact-checkingindialogue.
In: Proceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume1: LongPapers).pp.3785–3801(2022)
[24] He,C.,Li,W.,Jin,Z.,Wang,B.,Xu,C.,Lin: Opendatalab: Empoweringgeneralartificialintel-
ligencewithopendatasets.https://opendatalab.com(2022)
[25] Ji, Z., Gu, Y., Zhang, W., Lyu, C., Lin, D., Chen, K.: ANAH: analytical an-
notation of hallucinations in large language models. CoRR abs/2405.20315 (2024).
https://doi.org/10.48550/ARXIV.2405.20315, https://doi.org/10.48550/arXiv.2405.
20315
[26] Ji,Z.,Lee,N.,Frieske,R.,Yu,T.,Su,D.,Xu,Y.,Ishii,E.,Bang,Y.,Madotto,A.,Fung,P.:
Surveyofhallucinationinnaturallanguagegeneration.ACMComputingSurveys(2022)
[27] Ji, Z., Liu, Z., Lee, N., Yu, T., Wilie, B., Zeng, M., Fung, P.: RHO: Reducing hal-
lucination in open-domain dialogues with knowledge grounding. In: Rogers, A., Boyd-
Graber, J., Okazaki, N. (eds.) Findings of the Association for Computational Linguistics:
ACL 2023. pp. 4504–4522. Association for Computational Linguistics, Toronto, Canada
(Jul2023).https://doi.org/10.18653/v1/2023.findings-acl.275,https://aclanthology.org/
2023.findings-acl.275
[28] Ji, Z., Yu, T., Xu, Y., Lee, N., Ishii, E., Fung, P.: Towards mitigating hallucinationin large
languagemodelsviaself-reflection.EMNLPFindings(2023)
[29] Jiang,A.Q.,Sablayrolles,A.,Mensch,A.,Bamford,C.,Chaplot,D.S.,Casas,D.d.l.,Bressand,
F.,Lengyel,G.,Lample,G.,Saulnier,L.,etal.: Mistral7b.arXivpreprintarXiv:2310.06825
(2023)
[30] Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot,
D.S., Casas, D.d.l., Hanna, E.B., Bressand, F., et al.: Mixtral of experts. arXiv preprint
arXiv:2401.04088(2024)
[31] Kamalloo,E.,Dziri,N.,Clarke,C.L.A.,Rafiei,D.:Evaluatingopen-domainquestionanswering
in the era of large language models. In: Rogers, A., Boyd-Graber, J.L., Okazaki, N. (eds.)
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
(Volume1: LongPapers),ACL2023,Toronto,Canada,July9-14,2023.pp.5591–5606.Asso-
ciationforComputationalLinguistics(2023).https://doi.org/10.18653/v1/2023.acl-long.307,
https://doi.org/10.18653/v1/2023.acl-long.307
[32] Kirillov,A.,Mintun,E.,Ravi,N.,Mao,H.,Rolland,C.,Gustafson,L.,Xiao,T.,Whitehead,S.,
Berg,A.C.,Lo,W.Y.,etal.: Segmentanything.In: ProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision.pp.4015–4026(2023)
11[33] Laban,P.,Schnabel,T.,Bennett,P.N.,Hearst,M.A.: Summac: Re-visitingnli-basedmodelsfor
inconsistencydetectioninsummarization.TransactionsoftheAssociationforComputational
Linguistics10,163–177(2022)
[34] Lee,N.,Wattanawong,T.,Kim,S.,Mangalam,K.,Shen,S.,Anumanchipali,G.,Mahoney,
M.W.,Keutzer,K.,Gholami,A.:Llm2llm:Boostingllmswithnoveliterativedataenhancement.
arXivpreprintarXiv:2403.15042(2024)
[35] Lei, D., Li, Y., Wang, M., Yun, V., Ching, E., Kamal, E., et al.: Chain of natural lan-
guageinferenceforreducinglargelanguagemodelungroundedhallucinations.arXivpreprint
arXiv:2310.03951(2023)
[36] Li, J., Cheng, X., Zhao, W.X., Nie, J.Y., Wen, J.R.: Halueval: A large-scale hallucination
evaluationbenchmarkforlargelanguagemodels.In: Proceedingsofthe2023Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing.pp.6449–6464(2023)
[37] Li,K.,Patel,O.,Viégas,F.,Pfister,H.,Wattenberg,M.: Inference-timeintervention: Eliciting
truthfulanswersfromalanguagemodel.arXivpreprintarXiv:2306.03341(2023)
[38] Lin,C.Y.: Rouge: Apackageforautomaticevaluationofsummaries.In: Textsummarization
branchesout.pp.74–81(2004)
[39] Liu,J.,Fu,Y.,Xie,R.,Xie,R.,Sun,X.,Lian,F.,Kang,Z.,Li,X.: Phd: Apromptedvisual
hallucinationevaluationdataset.arXivpreprintarXiv:2403.11116(2024)
[40] Liu, T., Zhang, Y., Brockett, C., Mao, Y., Sui, Z., Chen, W., Dolan, B.: A token-level
reference-freehallucinationdetectionbenchmarkforfree-formtextgeneration.arXivpreprint
arXiv:2104.08704(2021)
[41] Mena,J.,Pujol,O.,Vitrià,J.: Asurveyonuncertaintyestimationindeeplearningclassification
systemsfromabayesianperspective.ACMComputingSurveys(CSUR)54(9),1–35(2021)
[42] Min, S., Krishna, K., Lyu, X., Lewis, M., Yih, W.t., Koh, P.W., Iyyer, M., Zettlemoyer, L.,
Hajishirzi,H.: FActScore: Fine-grainedatomicevaluationoffactualprecisioninlongformtext
generation.In: EMNLP(2023),https://arxiv.org/abs/2305.14251
[43] Mishra, A., Asai, A., Balachandran, V., Wang, Y., Neubig, G., Tsvetkov, Y., Hajishirzi,
H.: Fine-grained hallucination detection and editing for language models. arXiv preprint
arXiv:2401.06855(2024)
[44] Muhlgay,D.,Ram,O.,Magar,I.,Levine,Y.,Ratner,N.,Belinkov,Y.,Abend,O.,Leyton-Brown,
K., Shashua, A., Shoham, Y.: Generatingbenchmarksforfactualityevaluationoflanguage
models.arXivpreprintarXiv:2307.06908(2023)
[45] None: Sharegpt(2023),https://huggingface.co/datasets/RyokoAI/ShareGPT52K
[46] Petroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., De Cao, N., Thorne, J., Jernite, Y.,
Karpukhin,V.,Maillard,J.,etal.: Kilt: abenchmarkforknowledgeintensivelanguagetasks.
In: Proceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: HumanLanguageTechnologies.pp.2523–2544(2021)
[47] Rebuffel,C.,Roberti,M.,Soulier,L.,Scoutheeten,G.,Cancelliere,R.,Gallinari,P.:Controlling
hallucinationsatwordlevelindata-to-textgeneration.DataMiningandKnowledgeDiscovery
36(1),318–354(2022)
[48] Semnani, S., Yao, V., Zhang, H., Lam, M.: Wikichat: Stopping the hallucination of large
languagemodelchatbotsbyfew-shotgroundingonwikipedia.In: FindingsoftheAssociation
forComputationalLinguistics: EMNLP2023.pp.2387–2413(2023)
[49] Shi,W.,Han,X.,Lewis,M.,Tsvetkov,Y.,Zettlemoyer,L.,Yih,S.W.t.: Trustingyourevidence:
Hallucinatelesswithcontext-awaredecoding.arXivpreprintarXiv:2305.14739(2023)
[50] Singh, A., Co-Reyes, J.D., Agarwal, R., Anand, A., Patil, P., Liu, P.J., Harrison, J., Lee, J.,
Xu,K.,Parisi,A.,etal.: Beyondhumandata: Scalingself-trainingforproblem-solvingwith
languagemodels.arXivpreprintarXiv:2312.06585(2023)
12[51] Sun,K.,Xu,Y.E.,Zha,H.,Liu,Y.,Dong,X.L.: Head-to-tail: Howknowledgeablearelargelan-
guagemodels(llm)? akawillllmsreplaceknowledgegraphs? arXivpreprintarXiv:2308.10168
(2023)
[52] Sun,Z.,Shen,Y.,Zhou,Q.,Zhang,H.,Chen,Z.,Cox,D.,Yang,Y.,Gan,C.: Principle-driven
self-alignmentoflanguagemodelsfromscratchwithminimalhumansupervision.Advancesin
NeuralInformationProcessingSystems36(2024)
[53] Touvron,H.,Martin,L.,Stone,K.,Albert,P.,Almahairi,A.,Babaei,Y.,Bashlykov,N.,Batra,
S., Bhargava, P., Bhosale, S., etal.: Llama2: Openfoundationandfine-tunedchatmodels.
arXivpreprintarXiv:2307.09288(2023)
[54] Varshney, N., Yao, W., Zhang, H., Chen, J., Yu, D.: A stitch in time saves nine: Detecting
andmitigatinghallucinationsofllmsbyvalidatinglow-confidencegeneration.arXivpreprint
arXiv:2307.03987(2023)
[55] Vectara: vectara hallucination evaluation model. https://huggingface.co/vectara/
hallucination_evaluation_model
[56] Wang,A.,Cho,K.,Lewis,M.: Askingandansweringquestionstoevaluatethefactualconsis-
tencyofsummaries.arXivpreprintarXiv:2004.04228(2020)
[57] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., Zhou, D.:
Self-consistency improves chain of thought reasoning in language models. arXiv preprint
arXiv:2203.11171(2022)
[58] Wang, Z., Mao, S., Wu, W., Ge, T., Wei, F., Ji, H.: Unleashing cognitive synergy in large
languagemodels: Atask-solvingagentthroughmulti-personaselfcollaboration.arXivpreprint
arXiv:2307.053001(2), 3(2023)
[59] Weng, R., Yu, H., Wei, X., Luo, W.: Towards enhancing faithfulness for neural machine
translation.In: Proceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP).pp.2675–2684(2020)
[60] Wu,Z.,Hu,Y.,Shi,W.,Dziri,N.,Suhr,A.,Ammanabrolu,P.,Smith,N.A.,Ostendorf,M.,
Hajishirzi,H.: Fine-grainedhumanfeedbackgivesbetterrewardsforlanguagemodeltraining.
arXivpreprintarXiv:2306.01693(2023)
[61] Yang,S.,Sun,R.,Wan,X.: Anewbenchmarkandreversevalidationmethodforpassage-level
hallucinationdetection.arXivpreprintarXiv:2310.06498(2023)
[62] Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.W., Salakhutdinov, R., Manning, C.D.:
HotpotQA:Adatasetfordiverse,explainablemulti-hopquestionanswering.In: Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing(EMNLP)(2018)
[63] Yuan, W., Pang, R.Y., Cho, K., Sukhbaatar, S., Xu, J., Weston, J.: Self-rewardinglanguage
models.arXivpreprintarXiv:2401.10020(2024)
[64] Yuan, Z., Yuan, H., Li, C., Dong, G., Tan, C., Zhou, C.: Scaling relationship on learning
mathematicalreasoningwithlargelanguagemodels.arXivpreprintarXiv:2308.01825(2023)
[65] Zhang,J.,Xu,C.,Gai,Y.,Lecue,F.,Song,D.,Li,B.: Knowhalu: Hallucinationdetectionvia
multi-formknowledgebasedfactualchecking.arXivpreprintarXiv:2404.02935(2024)
[66] Zhang*, T., Kishore*, V., Wu*, F., Weinberger, K.Q., Artzi, Y.: Bertscore: Evaluating text
generationwithbert.In: InternationalConferenceonLearningRepresentations(2020),https:
//openreview.net/forum?id=SkeHuCVFDr
13A TrainingPrompt
Asdescribedin§3.1,ourannotationprocessconsistsofthreephases:(1)FactualExistenceJudgment
via the prompt in Fig. A1, (2) Reference Information Extraction via the prompt in Fig. A2 (3)
Hallucination-TypeJudgmentviathepromptinFig.A3.
B DataScalingDetails
As described in § 3.3, we collect model responses via Fig. A5. The open-source models in-
clude InternLM2(7B&20B) [7], Baichuan2 (7B&13B) [4], LLama2 (7B&13B) [53], Qwen1.5
(7B&14B&72B)[2],Deepseek(7B&67B)[6],andMistral(7B&7×8B)[29,30].
WeautomatethetopicselectionbasedonoccurrencefrequencyviaGoogleNgramViewer5 and
retrievecorrespondingreferencedocumentsfrompre-trainingdatabases[24].
WegeneratequestionsoneachtopicviaFig.A4.
C ImplementationDetails
Inourexperimentalframework,weadoptthepre-trainedInternLM2-7B[7]modeltofinetuningthe
hallucinationannotator.
InE-Step,wegenerateresponsesbyimplementingsamplingviatheLMDeploylibrary[15]. During
eachiteration,wegenerate32candidateresponsesperinputandapplyaself-consistencyquality
control mechanism to them. The decoding strategy involves the top-k (k = 40) sampling with a
temperatureof0.8.
InM-Step,wetraintheannotatormodelwiththefollowingsettingsandhyper-parameters: theepoch
is1,thelearningrateis1e-5,andtheAdamWoptimizeriswithalinearscheduler,themaximum
sequencelengthissetto32k. Additionally,followingtheconfigurationinANAH[25],weperform
amulti-tasksettingwhereadditionaltaskssuchasdialoguegenerationfromShareGPT[45]and
Dolly[14]areintegratedwiththefine-grainedhallucinationannotation. Ourmodelistrainedon32
NVIDIAA100GPUs.
D HallucinationEvaluation
Toassessthereliabilityofourhallucinationannotator,wemeasurethehallucinationlevelsofthe
aboveLLMsusingtheannotatorfromdifferentstages.Tab.A1, A2,and 7showtheresultsmeasured
byannotatorANAH-v2fromStage1, 2, and3, respectively. Thetrendsinthesethreetablesare
consistent where Qwen1.5-14B achieves the lowest hallucination rate with reference documents
andDeepseekLM-67Bachievesthelowesthallucinationratewithoutreferencedocuments. This
consistency and fixed biased ordering relationship between LLMs confirm the reliability of our
assessmentmethod.
E Limitation
Althoughthisstudypresentsanovelmulti-iterationself-trainingframeworkforthescalableoversight
ofLLMhallucinationsandachievessignificantimprovementsinhallucinationannotation,thereare
somelimitations.
Despitetheprogressivescalingandincreasingaccuracyofthehallucinationannotator,theremaystill
existanon-negligiblemarginoferrorintheannotations. Thismargincouldaffecttheconvergence
ofthemodelandthequalityofthefinalhallucinationannotator. Furthermore,thesuccessofour
frameworkismeasuredlargelybyitsperformanceonourowndatasetandotherbenchmarkssuchas
HalluEvalandHalluQA.However,thesedatasetsmightnotencompassthefullspectrumofreal-world
scenarioswherehallucinationsposeaproblem. Lastly,thisworkprimarilyusesInternLM2-7Basthe
backboneofthehallucinationannotator. Otherdifferentunderlyingmodelsanddifferentnumbersof
parametersarenotexplored.
5https://books.google.com/ngrams/
14F BroaderImpacts
By exploring the hallucination annotation and mitigation in LLMs, this paper contributes to the
developmentofmorereliableandtrustworthyAItechnologies. Ourinnovativemulti-iterativeself-
training framework significantly reduces the reliance on expensive and time-consuming manual
annotationsbyautomatingthehallucinationdetectionprocess. Ourhallucinationannotatoroffersa
benchmarkfortheresearchcommunityevaluatingthehallucinationlevelsofexistingopen-source
models. Additionally,weprovidealarge-scaleanddiversedatasetfromwhichthebroaderresearch
communitycanbenefit,fosteringfurtherinnovationandstudyinthisdomain.
EnglishPrompt:
Youwillactasafactchecker,andIwillprovideyouwithaquestionandacorrespondingpartialanswer. Your
taskistodeterminewhetherthecontentoftheanswercontainsverifiablefacts.
##JudgmentCriteria:
-VerifiableFacts: Specific,objectivepointsofinformationthatcanbeverifiedthroughdata,researchresults,
orotherreliablesources. Examplesincludestatisticaldata,historicalevents,scientificlaws,andspecificcase
studies.
-Non-factualDescriptions: Personalopinions,subjectivejudgments,orunverifiablestatements.
##TaskProcess:
1. Carefullyreadthequestion,whichisasfollows: {question}
2. Carefullyreadthepartialanswer,whichisasfollows: {annotation}
3. ConducttheAnalysis: Basedontheabovejudgmentcriteria,determineiftheanswercontainsverifiablefacts.
-Iftherearenoverifiablefactsintheanswer,output“<NoFacts>”.
-Ifthereareverifiablefactsintheanswer,output“<FactsPresent>”.
ChinesePrompt:
你将作为一个事实判断器，我会给你提供一个问题和一个针对该问题的部分回答，你的任务是判断回
答中的内容是否存在可以判断的事实。
##判断标准：
-可以判断的事实：具体的、客观的信息点，这些信息可以通过数据、研究结果或其他可靠来源进行
验证。例如，统计数据、历史事件、科学定律、具体案例等。-非事实描述：个人意见、主观判断或
无法验证的声明。
##任务流程：
1. 仔细阅读问题，问题如下：{question}
2. 仔细阅读回答，部分回答如下：{annotation}
3. 进行分析：根据上述判断标准，判断回答中是否包含可以判断的事实。
-如果回答中不存在可以判断的事实，则输出“<无事实>”。-如果回答中存在可以判断的事实，则
输出“<有事实>”。
FigureA1: Promptsforfactualexistencejudgment.
15EnglishPrompt:
Youwillactasaninformationextractor. Iwillprovideyouwithaquestion,arelatedreferencedocument,anda
partialanswertothatquestion. Yourtaskistoextractinformationfromthereferencedocumentthatisrelevantto
thequestionandanswer.
##OperationalSteps:
1. Carefullyreadthequestion,whichisasfollows: {question}
2. Carefullyreadthepartialanswer,whichisasfollows: {annotation}
3. Analyze the Reference Document: Identify information most relevant to the question and answer. This
informationmaybecompletelythesame,partiallysimilar,orconflictingwiththecontentoftheanswer. The
referencedocumentisasfollows: {reference}
4. ListtheRelevantInformation: Listalltherelevantinformationfoundinorder,separatedby<SEP>ifthereare
multiplepiecesofinformation.
5. Output When No Information Is Found: If no relevant information is found, output <No Reference
Information>.
ChinesePrompt:
你将作为一个信息提取器，我将给你提供一个问题、一份相关的参考文档，以及一个针对该问题的部
分回答，你的任务是从参考文档中提炼出与问题和回答相关的信息。
##操作步骤：
1. 仔细阅读问题，问题如下：{question}
2. 仔细阅读回答，部分回答如下：{annotation}
3. 分析参考文档：找出与问题和回答最相关的信息，这些信息可能与回答内容完全相同、部分相同，
或存在冲突。参考文档如下：{reference}
4. 列出相关信息：按顺序列出所有发现的相关信息，如果有多条信息的话以<SEP>作为分隔。
5. 无相关信息时输出：如果没有找到相关信息，请输出<无参考信息>。
FigureA2: Promptsforreferenceinformationextraction.
16EnglishPrompt:
Youwillactasa’Hallucination’annotator. Iwillprovideyouwithaquestion,apartialanswertothatquestion,
andrelatedreferencepoints. Youneedtodeterminewhethertheprovidedanswercontainsanyhallucinatory
contentandannotatethetypeofhallucination.
’Hallucination’referstocontentthatcontradictsthereferencepointsorisunsupportedbythem.
##JudgmentCriteria:
1. NoHallucination: Iftheansweriscompletelyconsistentwiththereferencepointsanddoesnotintroduceany
contradictoryinformation,output: <NoHallucination>.
2. Contradiction: Iftheanswerclearlycontradictsthereferencepoints,output: <Contradictory>.
3.Unverifiable:Iftheanswercontainsinformationnotmentionedinthereferencepointsandcannotbesupported
orverifiedbythem,output: <Unverifiable>.
##TaskProcess:
1. Carefullyreadthequestion,whichisasfollows: {question}
2. Carefullyreadthepartialanswer,whichisasfollows: {annotation}
3. Carefullyreadthereferencepoints,whichareasfollows: {reference}
4. Conducttheanalysis: Basedontheabovejudgmentcriteria,determineiftheanswercontainshallucinations
andoutputthetypeofhallucination.
ChinesePrompt:
你将作为一个‘幻觉’标注器，我将会给你提供一个一个问题，一个针对该问题的部分回答和相关的
参考要点。你需要判断提供的回答中是否含有幻觉性内容，并标注幻觉类型。
‘幻觉’指的是与参考要点相矛盾或在参考要点中没有依据的内容。
##判断准则：
1. 无幻觉：如果回答与参考要点完全一致，且没有引入与参考要点相矛盾的信息，请输出：<无幻
觉>。
2. 矛盾：如果回答内容与参考要点存在明显矛盾，请输出：<矛盾>。
3. 无法验证：如果回答包含的信息在参考要点中没有提及，且无法从参考要点中得到支持或验证，请
输出：<无法验证>。
##任务流程：
1. 仔细阅读问题，问题如下：{question}
2. 仔细阅读回答，部分回答如下：{annotation}
3. 仔细阅读参考要点，参考要点如下：{reference}
4. 进行分析：根据上述判断标准，判断回答中是否包含幻觉，并输出幻觉类型。
FigureA3: Promptsforhallucinationtypejudgment.
17EnglishPrompt:
Iwouldlikeyoutoactasaquestiongenerator. Iwillprovidereferencesandyouwillgenerate10questionsabout
"{topic}"basedonthereference. Thespecificrequirementsareasfollows:
1. thequestionscanbefullyansweredbasedonlyonthereferencedocument,i.e. theanswerstothequestions
are fully contained in the reference document. The questions should be objective and not too subjective or
open-ended.
2. the10questionsshouldbeofasmanydifferenttypesaspossible,e.g. what,when,where,why. Questionscan
beaskedfromdifferentperspectives,e.g. descriptions,explanations,reasons,etc. Ensurethatthequestionsare
ofdifferenttypesandcoverallaspectsoftheinformation.
3. 10questionscancoverdifferentlevelsofknowledge,fromgeneral,basicknowledgetomorespecialized,
complexsubjectknowledgeordomainknowledge.
4. haveonlyonequestionperitem.
Reference: {referencedocument}
Pleaselistthe10questionsdirectlybasedontheabovereferencewithoutanyexplanation:
ChinesePrompt:
我希望你充当一个问题生成器。我将提供参考资料，你将根据资料生成关于“{topic}”的10个问题。
具体要求如下：
1. 只根据参考资料，完全可以回答问题，即问题的答案完全包含在参考资料中。问题要客观，不要太
过主观和开放。
2. 10个问题尽量是不同类型的，比如：什么、何时、何地、为什么。问题可以从不同的角度出发，例
如描述、解释、原因等。确保问题类型多样，覆盖资料的各个方面。
3. 10个问题可以涉及不同层次的知识，从常识性、基本性的知识，到更专业化、复杂化的学科知识或
领域知识。
4. 每条只有一个问题。
参考资料：{referencedocument}
请根据以上参考资料，不做说明直接列出10个问题：
FigureA4: Promptsforquestiongeneration.
EnglishPrompt:
Referencedocument: {referencedocument}
Pleaseanswerthequestionbasedontheabovereference: {question}
ChinesePrompt:
参考资料：{referencedocument}
请根据以上参考资料，回答问题：{question}
FigureA5: Promptsforanswering.
18Person↓ Event↓ Thing↓ Location↓
Model Setting Overall↓
ZH EN ZH EN ZH EN ZH EN
w/oRef 74.8 94.03 40.74 86.42 67.68 93.83 75.03 86.24 53.03
InternLM2-7B
w/Ref 13.09 18.94 5.48 33.93 8.77 16.03 2.7 18.09 4.85
w/oRef 63.94 91.87 38.01 75.24 71.76 90.04 78.02 78 54.86
InternLM2-20B
w/Ref 12.84 29.76 5.65 29.24 12.72 13.78 7.67 16.09 12.62
w/oRef 64.04 87.68 42.61 70.86 65.62 88.05 72.62 79.91 54.22
Qwen1.5-7B
w/Ref 6.92 8.62 4.3 16.32 10.57 8.86 3.44 9.46 6.54
w/oRef 55.88 71.83 36.81 59.78 67.15 79.94 69.02 67.03 51.62
Qwen1.5-14B
w/Ref 5.96 10 2.29 12.88 2.93 13.19 2.02 7.77 6.67
w/oRef 49.25 67.67 25.47 56.33 58.88 77.72 60.79 62.81 41.98
Qwen1.5-72B
w/Ref 12.72 11.35 7.13 27.87 10.91 15.63 4.39 17.78 9.77
w/oRef 63.19 77.99 39.49 70.56 61.66 79.26 71.72 72.7 52.71
Baichuan2-7B
w/Ref 52.38 18.02 64.27 60.78 37.38 29.17 27.71 43.61 23.55
w/oRef 57.66 70.66 32.95 66.52 63.04 79.17 65.14 70.45 47.62
Baichuan2-13B
w/Ref 46.47 12.2 52.02 63.41 33.15 40.99 16.58 44.78 42.96
w/oRef 70.86 92.31 43 89.63 67.09 87.6 71.25 87.11 47.99
Mistral-7B
w/Ref 32.22 10.77 23.45 48.37 17.74 43.8 27.8 27 30.29
w/oRef 55.72 82.39 26.16 77.09 54.9 90.51 60.17 80.96 42.86
Mistral-8x7B
w/Ref 8.17 9.45 3.7 14.92 7.06 14.57 7.69 7.67 6.41
w/oRef 50 62.26 32.38 63.85 62.81 77.99 54.9 68.51 50.56
Deepseek-7B
w/Ref 23.1 11.54 43.94 22.97 8.2 26.19 24.25 17.8 13.51
w/oRef 33.68 52.86 17.89 57.79 37.41 72.91 33.93 64.62 33.33
Deepseek-67B
w/Ref 13.4 9.91 10 12.7 2 11.84 10.53 21.03 15
w/oRef 67.81 90.22 42.44 88.27 70.81 94.44 76.28 91.84 56.95
Llama2-7B
w/Ref 50.65 66.67 13.83 73.04 11.76 54.3 13.7 60.5 21.43
w/oRef 62.69 84.73 36.43 85.38 67.01 87.09 69.6 90.07 51.3
Llama2-13B
w/Ref 46.59 62.86 14.81 50.54 4.37 13.1 44.66 73.75 28.65
TableA1: Hallucinationrateofopen-sourcemodelsaccordingtoANAH-v2-Stage1.
19Person↓ Event↓ Thing↓ Location↓
Model Setting Overall↓
ZH EN ZH EN ZH EN ZH EN
w/oRef 75.83 95.28 42.49 84.58 70.88 94.79 77.13 86.2 57.73
InternLM2-7B
w/Ref 12.14 18.18 4.74 28.59 9.86 15.69 2.59 17.47 5.22
w/oRef 65.8 92.82 39.75 76.74 75.19 90.95 80.29 79.71 57.24
InternLM2-20B
w/Ref 12.13 28.57 5.82 25.2 13.15 12.64 7.2 15.4 13.46
w/oRef 65.38 86.23 44.07 68.66 71.35 89.71 73.85 80.68 56.72
Qwen1.5-7B
w/Ref 6.28 6.03 3.81 10.39 11.38 9.11 3.69 7.8 7.07
w/oRef 57.42 71.83 38.14 56.28 71.52 81.31 70.76 69.81 54.07
Qwen1.5-14B
w/Ref 4.92 5 1.06 9.83 2.51 12.47 2.7 7.01 6.44
w/oRef 50.17 66.17 25.92 52.74 60.24 79.45 62.92 63.9 46.34
Qwen1.5-72B
w/Ref 11.46 10.64 6.09 25.09 11.42 15.81 3.8 15.87 8.94
w/oRef 63.47 78.62 39.22 67.77 64.13 80.74 73.01 73.65 54.89
Baichuan2-7B
w/Ref 53.25 17.12 65.63 60.61 39.08 30.87 29.17 43.25 25.09
w/oRef 58.4 73.05 33.42 64.06 67.5 80.85 66.2 70.45 50.83
Baichuan2-13B
w/Ref 47.48 12.2 54.36 61.66 35.91 42.46 17.81 44.78 42.26
w/oRef 71.47 93.01 43.53 89.82 68.15 88.06 71.97 87.75 49.6
Mistral-7B
w/Ref 32.04 11.28 23.51 46.9 18.28 44.49 27.15 26.91 32.15
w/oRef 56.91 84.09 27.4 76.25 57.26 91.35 61.61 82.25 45.11
Mistral-8x7B
w/Ref 7.86 8.66 2.69 13.39 7.66 14.3 7.94 8.12 8.05
w/oRef 51.09 64.15 33.43 61.41 65.51 79.43 56.15 69.55 53.6
Deepseek-7B
w/Ref 23.35 7.69 45.96 22.97 4.92 25.79 25.5 17.8 13.51
w/oRef 33.4 60 17 53.44 38.15 73.47 34.23 63.82 34.7
Deepseek-67B
w/Ref 11.53 9.01 5 9.52 2 14.47 11.74 19.44 8.33
w/oRef 69.26 91.3 44.15 89.3 74.65 94.99 77.89 93 58.01
Llama2-7B
w/Ref 55.47 78.33 11.7 74.02 10.34 61.43 11.64 67.23 19.05
w/oRef 63.59 83.97 37.17 85.48 68.69 87.4 70.52 91 53.5
Llama2-13B
w/Ref 50.7 75.43 12.35 4.18 13.1 50 47.4 78.75 27.53
TableA2: Hallucinationrateofopen-sourcemodelsaccordingtoANAH-v2-Stage2.
20