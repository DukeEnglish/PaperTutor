Missed Causes and Ambiguous Effects:
Counterfactuals Pose Challenges
for Interpreting Neural Networks
AaronMueller
NortheasternUniversity
Technion–IsraelInstituteofTechnology
aa.mueller@northeastern.edu
Abstract
Interpretabilityresearchtakescounterfactualtheoriesofcausalityforgranted.Most
causalmethodsrelyoncounterfactualinterventionstoinputsortheactivationsof
particularmodelcomponents,followedbyobservationsofthechangeinmodels’
output logits or behaviors. While this yields more faithful evidence than corre-
lational methods, counterfactuals nonetheless have key problems that bias our
findingsinspecificandpredictableways. Specifically,(i)counterfactualtheories
do not effectively capture multiple independently sufficient causes of the same
effect,whichleadsustomisscertaincausesentirely;and(ii)counterfactualdepen-
denciesinneuralnetworksaregenerallynottransitive,whichcomplicatesmethods
forextractingandinterpretingcausalgraphsfromneuralnetworks. Wediscuss
theimplicationsofthesechallengesforinterpretabilityresearchersandpropose
concretesuggestionsforfuturework.
1 Introduction
Causalinterpretabilitytechniqueshavebecomepopular,inlargepartduetotheincreasingpopularity
ofmechanisticinterpretability. Suchtechniquesaimtofaithfullyunderstandthecausalmechanisms
underlyinganobservedmodelbehavior. Inpractice,“causalmethod”isgenerallysynonymouswith
“methodthatemployscounterfactualinterventions”;assumptionslikethesetakethecounterfactual
theoryofcausality(Lewis,1973)forgranted,oftenassumingitsbenefitsbutignoringitspitfalls.This
isproblematic:counterfactualtheories,whileintuitiveatfirstglance,leaveuspronetocounterintuitive
fallaciesthatcansystematicallybiasourresultsandinterpretationsifwearenotcareful.
Inthispaper,wewillarguethattherearetwotrickyproblemsthatarisewhenemployingcounterfactual
interventions to interpret neural networks: (i) overdetermination, where multiple simultaneous
andsimilarcausesofthesameeffectwillbesystematicallymissed;and(ii)thenon-transitivityof
counterfactualdependencies,whichmakeitdifficulttointerprettherelationshipbetweenpairsof
nodesthatarenotdirectlyconnected.Wewillstartbydescribingthecounterfactualtheoryofcausality
(§2)anditsroleininterpretabilityresearch(§2.1). Then,wewilldescribetheresultingproblemsthat
havepermeatedinterpretabilityresearch(§3,4). Finally,wewilldiscusstheimplicationsofthese
problemsandproposesuggestionsforfuturework(§5).
2 TheCounterfactualTheoryofCausality
ThefirstcounterfactualinterpretationofcausalitywasposedbyDavidHume: “Wemaydefinea
causetobeanobject...where,ifthefirstobjecthadnotbeen,thesecondhadneverexisted”(Hume,
1748). ThiswaslaterformalizedbyLewis(1973)intowhatwenowrecognizeasthecounterfactual
Preprint.Underreview.
4202
luJ
5
]GL.sc[
1v09640.7042:viXratheory. Histheoryisbasedoncounterfactualconditionals,whichtaketheform“IfcauseC,then
eventE.” Theseconditionalsdonotnecessarilyindicatecausation(e.g.,“IfIwereinBerlin,Iwould
beinGermany”isavalidcounterfactualconditional,buttheinversedoesnothold).
Lewis(1973)posesthatacausaldependenceholdsiffthefollowingconditionholds:
“AneventE causallydependsonC [iff](i)ifC hadoccurred,thenE wouldhave
occurred,and(ii)ifC hadnotoccurred,thenE wouldnothaveoccurred.”
Lewis(1986)notesthatcounterfactualdependenceissufficientbutnotnecessaryforcausation;he
thereforeextendshisdefinitionofcausaldependencetobewhetherthereisacausalchainlinking
C toE. Thisapproachwaslaterextendedfromabinarynotionofwhethertheeffecthappenstoa
morenuancednotionofcauseshavinginfluenceonhoworwheneventsoccur(Lewis,2000). Other
workextendsnotionsofcauseandeffecttocontinuousmeasurablequantities(Pearl,2000),including
directandindirecteffects(Robins&Greenland,1992;Pearl,2001);suchworkgenerallybasesits
computationsonstructuralequationmodeling(Goldberger,1972).
Inthecausalityliterature,acausalchainfromC toE isreferredtoasamechanism. Thefieldof
mechanisticinterpretabilitydoesnotusetheterm“mechanistic”inthisway(orinanystandard
way;Andreas,2024),butmanyofitsmethodsdofitthisdefinition: theyaimtolocatethecausal
chainthatexplainshowaninputistransformedintoagivenmodelbehavior.1
2.1 CounterfactualDependenciesinMechanisticInterpretability
Whenaninterpretabilitystudydescribesitsmethodsas“causal”,thistypicallyreferstoitsuseof
counterfactualinterventions. Here,wewilluse“counterfactual”torefertoanyactivationthata
modelcomponentwouldnotnaturallytakegiveninputxwithoutintervention.2
In this paradigm, we conceptualize a neural network model M as a causal graph from inputs to
outputs,whereeachmodelcomponent(e.g.,aneuronoranattentionhead)isamediatorornode,
andeachconnectionbetweencomponentsinthecomputationgraphisacausaledge. Thisfollows
terminologyandmodelingpracticesincausalmediationanalysis(Pearl,2001). Neuralnetworks
areamenabletothisinterpretationbecausetheyarecompletecausalgraphsthatfullyexplainhow
anyvalidinputxwillbetransformedintoacorrespondingoutputy.
Atypicalmethodforimplicatingamodelcomponentinsomebehaviorisactivationpatching(Vig
et al., 2020; Finlayson et al., 2021; Meng et al., 2022). In this method, we first define an input
x andacorrespondingcorrectcompletion. Ifweareusingapatchingablation,wealsodefine
original
aminimallydifferentincorrectcompletion. Forexample,insubject–verbagreement,iftheinput
x is “The manager”, a minimal pair of completions could be y = “is” (correct) and
original original
y =“are”(incorrect). Onethendefinesaminimallydifferentinputx wherethecorrectand
patch patch
incorrectanswerareflipped;inthisexample,x wouldbe“Themanagers”.
patch
Giventhesevariables,onerunsx throughM inaforwardpass(denotedM(x ))andcollects
patch patch
theactivationa ofamodelcomponenta. Then,onecanrunM(x )andcollectsomeoutput
patch original
metricthatshouldbesensitivetothiscomponent’sactivation;often,thismetricisthelogitdifference
y = (p −p ). Then, onererunsM(x ), butwherea isreplacedwitha ; i.e.,
patch original clean original patch
1Webelievethistheoreticallygroundeddefinitionof“mechanism”shouldbeadoptedinmechanisticinter-
pretability. Theredoexistnon-causalmethodswhicharelabeledasmechanisticinterpretability,butevenif
themethoddoesnotrelyoncounterfactualsperse,seekingalgorithmicunderstandingisessentiallyequivalent
toseekinghuman-understandablecausalmechanismsthatconciselyexplainmodelbehavior.Ifso,theneven
correlationalmethodscanclaimtopursuemechanisticunderstanding,albeitwithlowerfaithfulnessandcausal
efficacyonaveragethanmethodswhichcausallyimplicateunitsofmodelcomputation.Abenefitisthatthis
definitionmayencouragemorecausallyrigorousevaluationstoverifyclaimsofmechanisticunderstanding.
2Someuse“counterfactual”toreferspecificallytoinputswheretheanswerisdifferentfromthatgiventhe
original(oftentermed“clean”)input.Webelievethisusageistoonarrow:ifaneuronisresponsibleforsome
behavior,andonemanuallysetsitsactivationtosomealternatevaluethatflips,nullifies,orotherwisemodifies
itscausalcontribution,onewillstillobserveaneffectonthe(probabilityofthe)behaviorinamannerthatwould
nothavebeenpossibleotherwise.Wewilluse“patchingablation”torefertotheformofcounterfactualbased
oninputsthatfliptheanswer,butwewillalsorefertozeroablationsandmeanablationsascounterfactuals.
2weperformacounterfactualinterventiontoabysettingitsvaluetowhatitwouldhavebeeninan
alternateinput. Onethenmeasureshowmuchthisinterventionchangesthetargetmetricy.3
Thischangeintheoutputmetricistheindirecteffect(IE;Pearl,2001;Robins&Greenland,1992)
ofaony. TheIEafteractivationpatchingisacommonmetricincausalinterpretability(Vigetal.,
2020; Finlayson et al., 2021; Soulos et al., 2020), and has been extended for use in mechanistic
interpretabilitytechniqueswhichchaincausesintographstructures. Variantsincludepatchingvia
addingGaussiannoise(alsotermedcausaltracing;Mengetal.,2022). Onecangeneralizepatching
toimplicatemultiplecomponentssimultaneously,asinpathpatching(Wangetal.,2023;Goldowsky-
Dilletal.,2023). Thistechniquehasbeenappliedextensivelytounderstandmechanismsunderlying
sophisticatedlanguagemodelbehaviors(Gevaetal.,2023;Yuetal.,2023;Toddetal.,2024)andto
editlanguagemodels(Mengetal.,2022,2023).
Perhaps the most relevant methods to this work are those used to discover and analyze circuits
(Wangetal.,2023;Conmyetal.,2023),whicharesubgraphsofaneuralnetworkthatarecausally
implicatedinperformingaspecifictask. Thesegenerallyemployactivationpatching,pathpatching,
orsomecombinationthereof. Circuitscontainnesteddependencies,butevenifanodedoesnothave
adirectedgetothemodeloutput,itisoftenrequiredthateachcircuitnodedirectlyinfluencethe
outputwhenablated. Thismaybethoughtofasageneralizationofindividualcomponentprobing,in
thatcomponentprobingsearchesforcausalgraphsofdepth=1,whereascircuitdiscoverylocates
causal graphs of depth > 1. Circuits are often used for post-hoc interpretability (Hanna et al.,
2023;Lieberumetal.,2023;Chughtaietal.,2023),andsometimestomodifymodelbehaviorsvia
targetedablations(Lietal.,2023;Marksetal.,2024). Anothercausallygroundedlineofworktakes
hypothesizedcausalgraphsasinputsandalignsthemwith(potentiallymultidimensional)model
subspaces,eitherviasearchordirectoptimizationoverinterventionsiteboundaries(Geigeretal.,
2021;Wuetal.,2023;Geigeretal.,2024). Here,patchingablationsarereferredtoasinterchange
interventions.
3 Counterfactualdependenceisnottransitive.
ItisnaturaltoassumethatifC counterfactuallydependsonB,andBdependsonA,thenC must
dependonA. Thisisafallacy. ConsiderthefollowingcanonicalexamplefromNedHall:4
(1) A: Ahikeriswalkingupamountain. Alargeboulderbeginsrollingdownthemountain
towardthehiker.
B: Thehiker,noticingtherollingboulder,ducksoutoftheway.
C: Thehikerlives.
Inthisscenario,C dependsonB,andBdependsonA. Doesitthereforemakesensetosaythatthe
boulderrollingdownthemountaincausedthehikertolive? Thereasonthisfeelsstrangeisbecause
C isnotcounterfactuallydependentonA: thehikerwouldhavelivedeveniftheboulderhadnever
rolleddownthemountain. Thus,whileeachlocalcounterfactualdependenceholds,thetransitive
dependenceofC onAdoesnot. Inotherwords,counterfactualdependenceisnotnecessarily
transitive. SeeFigure1foranillustration.
However, causality intuitively feels transitive, and there are many cases where it is.5 Con-
sider playing a game of billiards, where one hits billiard ball A, which then hits ball
3Thereexistotherinterventionswhichdonotrequireminimallydifferentpatchinputsorbinaryanswers.For
example,intheabsenceofaclearbinaryincorrectanswer,onecanalsomeasurethenegativelogitofthecorrect
completion. Ratherthanpatchingina ,onecouldinsteadsetatoitsmeanactivationacrossasampleof
patch
generaltext(referredtoasameanablation),orrandomlysampleanactivationfromanunrelatedinput(known
asapatchingablation).Azeroablation,whereaisreplacedwith0,isgenerallynotconsideredprincipled,
unlessoneisworkingwithunitswhosedefaultvaluesare0(aswhenworkingwithsparseautoencoders;see
Brickenetal.2023andCunninghametal.2024).
4AsreportedinHitchcock(2001)withcredittoHall.
5Ithaslongbeendebatedwhetheractualcausality,asseparatefromcounterfactualdependence,isalways
transitive(cf.Paul&Hall2013;Lewis1973,1986,whoclaimthattransitivityisakeydesideratumoftheoriesof
causality).WeareinclinedtofollowtheanalysisofBeckers&Vennekens(2017),whoclaimthatthetransitivity
ofcausalityisconditional,andwhothengivesufficientconditionsforcausaltransitivity.
3B, which sinks ball C. In this case, C does counterfactually depend on A. What,
then, is the fundamental difference between transitive and non-transitive dependency chains?
Halpern(2016)posesnecessaryandsufficientcon-
ditionstoguaranteethetransitivityofcounterfac-
tualdependence. Supposethereexistrandomvari-
ablesA,B,andC. Letthereexistvaluesa and
1
a thatAcouldtake,andthereexistsimilarpairs
2
of values b , b and c , c for B and C, respec-
1 2 1 2
tively. Also let “→” indicate entailment. Then
thefollowingconditionsguaranteethetransitive
Figure 1: Illustration of the non-transitivity of
dependenceofC onA:
counterfactual dependence. If an output y de-
1. A=a
1
→B =b
1
pendsonB,andifBdependsonA(potentially
2. B =b 1 →C =c 1 inadditiontoothernodes),thenitdoesnotnec-
3. c
1
̸=c
2
essarilyentailthatydependsonA.
4. A=a →B =b
2 2
5. A=a ∧B =b →C =c
2 2 2
TheintuitionisthatAcantakevalueswhichdetermineatleasttwodistinctvaluesofB,6andthat
B can determine at least one value of C. Then, if the disjunction of A and B also determines a
particularvalueofC thatisdistinctfromtheonethatB alonecaused,thenthedependencychain
fromAtoC istransitive,andC thereforecounterfactuallydependsonA.
Halpernposesanadditionalsetofsufficient(butnotnecessary)conditionstoguaranteetransitivity:
1. ForeveryvaluebthatBcantake,thereexistsavalueathatAcantakesuchthatA=a→
B =b.
2. BisineverycausalpathfromAtoC.
Inneuralnetworks,counterfactualdependencieswillnotoftenbetransitive. Whilepossible,it
seemsunlikelythateithersetofconditionswouldholdinmostneuralnetworks. Thefirstisunlikely
duetotheexistenceofmanyneuronsthatsimultaneouslyinfluenceB,suchthattheprobabilityis
lowofthereexistingsomevaluea ofAthatalonecausesBtotakesomevalueb . Thesecondis
1 1
unlikelybecause,inthesimplestcasewhereB isinahiddenlayerbetweenAandC,therearen
possiblepathsfromaneuroninA’slayertoaneuroninC’slayer,wherenisthenumberofneurons
inB’slayer. ThisnumbergrowsexponentiallyasweincreasethelayerdistancebetweenAandC.
Thus,itisreasonabletoassumebydefaultthatcounterfactualdependencieswillgenerallynotbe
transitiveinneuralnetworks,unlessBissomecausalbottleneckthroughwhichinformationfrom
nodeAmustpasstoarriveatC. Thereisnoreasonaprioritoexpectsuchbottleneckstoexistat
thelevelofneurons,attentionheads,orsparseautoencoderfeatures,whichisunfortunategiventhat
thesefine-grainedunitsaremorelikelytobehuman-interpretable. Onecouldviewresidualsascausal
bottlenecksintowhichallinformationiscollapsedattheendofalayer,socausalchainswouldbe
transitiveiftheintermediatenodeBweretheentireresidualvector;nonetheless,suchcoarse-grained
unitsaregenerallydifficulttointerpret. Futureworkcouldconsidermethodsfordemonstratingthe
existenceandfrequencyofinterpretablecausalbottlenecksinneuralnetworks.
Thispresentsaproblematictrade-off. Methodsthatrequirecomponentstosignificantlychangethe
outputwillnotrecallcomponentsthatexplainhowintermediatecausalnodesarise. Thishindersour
understandingofhowamodelcomposesincreasinglyhigh-levelintermediateabstractionstoperform
atask. However,ifweemploymethodswhichsearchoveralllocaldependenciesanddonotrequire
asignificantchangetotheoutput,wewillincludemanymorenodesthanisnecessarytoexplainthe
modelbehavior,andprobablysignificantlyincreaseourmethod’stimecomplexity.
3.1 Casestudy: SuccessionfeaturesinPythia70M
Circuits are directed acyclic graphs (DAGs), where each node represents a model component or
setofcomponents,andeachedgerepresentsacausallinkfromanupstreamnodetoadownstream
node. The leaf node is typically the target metric, such as the logit difference between a correct
6Thismaybeeasiertoconceptualizeifonetreatsa ,b ,andc asthedefaultvaluesofeachvariable,as
2 2 2
Halperndoesinleadinguptothismoregeneralsetofconditions.
4Figure 2: Sparse feature circuit for layer 3 attention feature 14579, a general succession feature
(denotedyinthefigure). Bestviewedzoomedin. Darkerblueshadesindicatehigher-magnitude
positive effects, and darker red shades indicate higher-magnitude negative effects. Given inputs
suchas“1, 2, 3,”, themodel’sfirstlayersdetecteachnumberandthepunctuationbetweenthem
(a). The number detectors are inputs to number list detectors and increasing number sequence
detectors (b). These features then inform induction features (c) that are only active in specific
contextsoronparticularclassesofphenomena(likenumbersequences). Theseaidmoregeneral
incrementor/successionfeatures(d).
andaminimallydifferentincorrectcompletiontosomecontext. Circuitsareintendedtorepresent
thesubgraphofthemodel’scomputationgraphthatiscausallyimplicatedinincreasingthetarget
metric. Toensurethatnodesinthecircuitarehuman-interpretable,onecanusesparseautoencoder
dictionaries(Brickenetal.,2023;Cunninghametal.,2024)todecomposesubmodules(likeMLPor
attentionlayers)intohuman-interpretablesparsefeatures.
InMarksetal.(2024),theauthorsdiscoveredsparsefeaturecircuitsforvariousmodelbehaviors,
including a specific succession behavior where Pythia 70M predicts “4” given inputs containing
numberedlists(whereeachnumberispotentiallyseparatedbyvariable-lengthtext),asin“1: ...\n2:
...\n3: ...\n”. After generating a sparse feature circuit for this seemingly coherent behavior,
theyfindthatitisactuallytheintersectionoftwodistinctmechanisms: (1)ageneralsuccession
mechanism(Gouldetal.,2024),whichpromotesthenextiteminordinalornumericlists,and(2)a
specificinductionmechanism(Olssonetal.,2022),whichcopiespreviouselementsoflistsinthe
samecontext. Aspresented,thesuccessormechanismarisesspontaneouslyatalayer3attention
feature,whichhasnoinedges;becauselocaldependenciesarenotconsidered,itisunclearhowthis
successionabstractioniscomposedfromtheinputs.
Thus, we obtain a sparse feature circuit (SFC) for cluster 382 from Marks et al. (2024), where
thetargetmetricy hasbeenchangedfromthelogitdifferenceoftwopossiblenexttokenstothe
activation of the layer 3 attention succession feature (Figure 2). We use the same method as in
Marksetal.(2024),changingonlythetargetmetric;seeApp.Afordetailsandhyperparameters. We
manuallyannotateeachfeaturebyinspectingitsactivationsandthelogitsthatchangemostwhenthe
featureisablated.7 ThisSFCcontainsfeaturesandedgesthatwerenotintheoriginalcircuit,but
whichhelpexplainhowsuccessioniscomputedinPythia. Forexample,thereisacausalchainfrom
layer-0featureswhichdetectspecificnumbers,tolayer-1featureswhichdetectsequencesofnumbers
(regardlessoforder),tolayer-2featureswhichdetectincreasingsequencesofnumbers. Thereis
another mechanism where layer-0 features activate on all commas, then layer-1 features activate
onlyoncommaswithinlistsofnumbers,thenlayer-2featuresactivateoncommasandelementsof
comma-separatednumberlists. Thisyieldsevenstrongerevidencethatthetargetattentionfeature
actuallyisasuccessorhead—butalsothatitisprimarilysensitivetonumericsuccession.
This is helpful for understanding the original feature circuit: the numeric successor feature that
spontaneouslyaroseintheoriginalSFCisactuallycomposedfromtheintersectionofspecificnumber
detectors and number list detectors. This anecdote suggests that we should include some purely
localdependenciesifourgoal istofullyunderstandhowhigher-levelabstractionsanddecisions
are composed from low-level input features. That said, it is easy to see how this would yield an
explodingnumberoffeaturesifweincludeallpossiblelocaldependencies: thegraphwouldbecome
7Thereareproblemsinassigningnaturallanguageexplanationstomodelcomponents.Thesearedetailedin
Huangetal.(2023).
5uninterpretableduetoarapidlyincreasingdescriptionlength. Forexample,intheaboveSFC,we
alsofoundfeatureswhichactivateinsidecitationsoftheform“[@<num>]”,where<num>isaninteger.
Isthisuseful? Maybe,butitmayreflecthowaparticularinputdistributionishandled(thiscluster
contains a significant number of citations in this format) rather than how a generic behavior is
composed. Anotherframingisthatthecitationfeatureislikelytoreducethedescriptionlengthofthe
featurelessthanthenumberlistfeaturewould,soweshouldprioritizeexplainingthenumberlist
features.
Given this discussion, should one keep local dependencies—i.e., upstream nodes that influence
downstream nodes which affect the target metric, but not the target metric directly? Or should
one instead mandate that each causal node we keep have some direct measurable effect on the
target metric? If one’s goal is to understand how a model will generalize, one should also
consideratleastsomelocalcausaldependencies. However,ifone’sgoalismerelytounderstand
whichcomponentswilldirectlyaffectdownstreamperformance(e.g.,wheneditingorpruning
models), it may suffice to only include components that directly affect the output.8 This is
fundamentally a trade-offbetween minimizing description length and increasing recall of highly
explanative components, similar to the trade-off described in Sharkey (2024). In such cases, the
bestpointintheParetofrontiermaysimplyconsistoftheminimalnumberoffeaturesneededto
understandwhetheramodelismakingtherightdecisionsintherightway.
Topreventtheproblemofoverlylongdescriptionlengthswhenincludinglocaldependencies,we
recommendto(1)usemorediversedatasetswhendiscoveringandinterpretingcounterfactual
dependencies,andto(2)directlyevaluatetheout-of-distributiongeneralizationofinterpretabil-
itymethodsbyusingheld-outevaluationdatasetsthataredistinctincontrolledwaysfromthe
datasetoverwhichwediscoverdependencies. Thesewillallowustorecovermorecausallyrelevant
nodeswhilealsohelpingusfilteroutthosewhicharetooinput-specificorhigh-variance.
4 Redundantcausesaresystematicallymissed.
Counterfactualtheoriesofcausalitystruggletohandleoverdetermination—i.e., caseswherean
effecthasmultipledistinctcauseswhichareeachsufficientontheirown. Considerthefollowing
classicexamplefromHall(2004):
(2) A : Suzythrowsarockataglassbottle.
1
A : Atthesametime,Billythrowsarockatthesamebottle.
2
B: Therocksshatterthebottle.
If A had not occurred, B still would have happened because of A , and vice versa. Thus, B is
1 2
notcounterfactuallydependentoneitherA orA inisolation: ablatingonlyoneofthemwould
1 2
producenochangetoB. Doesthismeanthatneitherrockcausedthebottletobreak? Thisisan
uncomfortableconclusion.
Theaboveexamplefeelscontrivedbecausesimultaneousredundantcausesofthesameeventarerare
intherealworld. However,inneuralnetworks,redundancyisverycommon. Consideralayerwithn
neuronsintheoutputvectoroftheMLPsubmodule: thesearensimultaneouscauses,atleastsome
ofwhicharelikelytohavesimilareffectsonsomedownstreammodelcomponentoronthemodel
behavior. Indeed,redundancyisfoundtobeverycommoninpre-trainedneuralnetworks(Dalvietal.,
2020),evenforabstractphenomenalikesyntacticagreement(Tuckeretal.,2022);furtherevidence
comesfromthefactthatmodelscanbedistilled(Sanhetal.,2020)orpruned(Frantar&Alistarh,
2023)withoutsignificantlossinperformance. Thisimpliesthatcurrentinterpretabilitymethods
thatrelyonablationsprobablyhavesignificantlylowerrecallthanexpected,anditisunclear
howmanyorwhatkindsofcomponentsaremissing.
Suppose for example that we have two neurons A and A which have activations a and a ,
1 2 1 2
respectively, given input x. A = a and A = a cause the activation of B to increase. In the
1 1 2 2
absenceoftheother,bothA =a andA =a increaseBtosomevalueclosetob,whichcauses
1 1 2 2
modelbehaviory. Butifbothareactive,thensupposethattheincreaseinB’sactivationbeyondb
8ThoughHaseetal.(2023)findthatcausallocalizationsdonotaccuratelyreflecttheoptimallocationsto
updateparameters.
6Figure3: Illustrationoftheresultofinterveningonindividualmodelcomponents. Thecaptions
depicttheactualcausalgraph,whilethevisualizationsdepictthecausalgraphthatwouldhavebeen
discovered via ablations to individual components. Nodes in blue are in the actual causal graph
andwouldbediscoverable. Nodesinpinkareintheactualcausalgraph,butwouldnothavebeen
discovered. Blackedgesarecause-effectrelationships,whereasrededgesindicatepreemption.
isnon-linearandsmall—e.g.,itcouldbelogarithmicorattheupperplateauofalogisticfunction.
Thus, in a search over individual model components, intervening on A or A will have a small
1 2
marginaleffectonB. WemightthereforebemisledintoconcludingthatneitherA norA causally
1 2
contributetoB, andthusdonotcausethetargetbehavior. SeeFigure3foranillustration. This
appliesstraightforwardlytocaseswheretwoneuronshavestronglycorrelatedeffects,eveniftheir
functionalrolesarenotthesame. Asalesstrivialexample,assumewearesearchingforcomponents
responsibleforsuccessfullyperformingaquestionansweringtask. IfA detectsgeographyterms,A
1 2
detectspoliticalterms,andBpredictscountrynames,thenA andA mayhaveverysimilareffects
1 2
onBoncertainnarrowdistributions,despitebeingsensitivetodifferentphenomena. Wemightbe
abletofindthemifA andA affectothercomponentsormetrics(e.g.,examplesofoppositelabels
1 2
orfromdifferentdomains)indistinctandmeasurableways. Thisimpliesthatdiverseinputdata
andthechoiceofmetricwillbecrucialtodiscoveringallrelevantcomponents.
Therealsoexistlesstrivialformsofredundantcausationthathavealreadybeenattestedinneural
networks. Backupheads(Wangetal.,2023)andHydraeffects(McGrathetal.,2023)areexamples
ofwhatiscalledpreemptioninthecausalityliterature. ReturningtoEx.(2),ifSuzy’srockA had
1
hitthebottlefirst,thenBilly’srockA wouldnothaveshatteredthebottleB—butpreventingA
2 1
wouldstillnotpreventBduetoA . SeeFigure3foranillustration. Inpreemption,A andA are
2 1 2
sufficientontheirowntocausey. Additionally,whenA isactive,ittriggerssomemechanismthat
1
causesA ’smarginaleffectony tobenullified. WhenA isablated, A willcausey. Here, we
2 1 2
woulddiscoverA throughsimplecounterfactualablations,butwewouldnotdiscoverA without
1 2
firstablatingA .
1
Probingindividualmodelcomponentsisproblematic. Theimplicationsofthesephenomenaare
far-reaching: overdeterminationandpreemptionaffectnotonlycircuitdiscoverymethods,butalso
anyinterventionmethodwherewehavemanypossiblesimultaneouscauses. Ifonedoesnottestall
possiblecombinationsofmodelcomponents,thenoneisnotguaranteedtorecallthefullmechanism
leadingtosomemodelbehavior. Itmaybeeasiertodiscovercasesofpreemption,sinceonecan
ablatecomponentsthathavealreadybeendiscoveredandobservewhethernewcomponentsthen
affecttheoutput. Withoverdetermination,itisnotclearwheretosearchforthesecomponentsets;
thus,sometypeofsearchoversetsofcomponents—ratherthanindividualcomponents—maybe
necessary.
Givenminimallydifferinginput/outputpairs,Heimersheim&Nanda(2024)advocateovercoming
thisproblemwithoutmulti-componentablationsbyusingmultipletypesofpatchingablations: oneas
describedin§2.1,andanotherwherex andx areswapped(termed“noising”and“denoising”
original patch
ablations,respectively). Weagree,andtogeneralizethisbeyondminimalpairs,werecommendusing
bothpositiveandnegativecounterfactualswhendiscoveringcausaldependencies—i.e.,donot
justablateacomponentwhenitisactive,butalsoinjecthighactivationsintoacomponentwhenitis
notactive. Thiscorrespondstoasearchwhereacausalnodecanbeincludedinthegraphifeither(i)
ablatingthenodehasanegativeeffectonthebehavior,or(ii)injectingitcausesthebehaviortooccur.
75 Discussion
Thesechallengesposethefollowingquestionsforinterpretabilityresearcherswhoemploycounter-
factuals:
1. Whatshouldwedowithnodesthatarelocallyimportant,buthavelittleeffectontheoutput?
2. Howcanweincreaserecallwithoutsignificantlyharmingprecision?
3. Arecounterfactualtheoriesofcausalitysufficientforinterpretingneuralnetworks?
1. Keep atleast somelocaldependencies. On(1), one shouldinterpret eachedgein acausal
graph—suchasacircuitedgeoraconnectionbetweenalignedsubspaces—asapurelylocalcausal
relationshipbydefault.9 Oneshouldremainagnosticabouttherelationshipbetweenagivenpairof
nodesAandBthatarenotdirectlyconnectedinthegraph—unlessthenode(s)betweenthemarea
causalbottleneckand/orcanbedeterminedbyA,inwhichcaseonemayassumethatBisdirectly
counterfactuallydependentonA. Toensurethatthenodesinacircuitactuallyaffecttheoutputs,one
mightrespondbytestingtheeffectofagivennodeontheoutputmetricandonlyincludingitifit
directlyaffectsthelogits. Thisposesitsownproblems: justasitdidnotmakesensetotalkaboutthe
hikerduckingandthensurvivingwithoutfirstmentioningtheboulderrollingdownthemountain
inEx.(1),itwouldsimilarlynotmakesensetoexcludealllocalcausalrelationshipswhereanode
doesnotaffecttheoutputdirectly. Indeed,thiswouldbeacatastrophicomissionwhentellingthe
fullcausalstoryofhowcertainintermediaterepresentationswhichareimportantarecomposedfrom
simplerabstractions. Wethereforeadvocatetokeepandanalyzecomponentsthatdonotdirectly
affecttheoutputsbythemselves,butthatdocausallyeffectotherdownstreamnodesinacircuit.
Topreventtheoppositeproblemofincludingtoomanynodesanddecreasingprecision,weshould
prioritizeincludinginformationwhichismostlikelytoreducethedescriptionlengthoftheprogram
thatimplementsthetargetbehavior. Onewaytodothisisbydeployingmorediversedatasets,
suchthatcomponentswhichonlyexplainasmallsubsetofinputsorminoraspectsofthebehavior
wouldbelesslikelytopassoureffectsizethresholds. Weshoulduseinputdatasetsandcontrolled
evaluationdatasetswhichareneitherconfinedtosingledomainsnorhighlytemplatic.
Theabovediscussionfocusesoncaseswheremediatorsareneuronsorattentionheadswithinasingle
layer. Thisbecomesmoreconfusingwheninterveningonmultiplecomponents: ifpathpatchingis
used,allnodeswithinthepathshouldbeconsideredasasinglenode/mediator. Effectswithinapath
p onywillbetransitive,buteffectsbetweenpathp ontheoutputyviap maynotbetransitive
1 1 2
unlessp andp satisfyasetofconditionsfrom§3. Thisraisesanotherrecommendation: ifyou
1 2
employinterventions,beexplicitaboutwhatyourcausalnodes/mediatorsare,especiallyifyou
aredrawingconnectionsbetweenthem.
2. Robustmethodswillrequireadiversityofinterventionsand/orinterventionstocombinations
of components. On (2), the naïve brute-force solution would be to intervene on all possible
combinations of components. Of course, it is not tractable to test all possible combinations of
components,evenwhenemployingefficientapproximationslikeattributionpatching(Syedetal.,
2023;Kramáretal.,2024). Areasonablecompromisemaybetorelyongreedysearchmethods(as
in§4.2ofVigetal.,2020).
Moretractablemethodsforcapturingoverdeterminedcauseswillprobablyentailadiversityofdata
and intervention types. We can, for example, use both positive and negative counterfactuals (as
describedin§4). Asastartingpoint,wecouldconstructsharedtoymodelswhichhaveoverdeter-
minationandpreemptionbuiltin;usingthese,wecandevelopmethodsthatcantractablydiscover
redundantcauses. Thesemethodscanlaterbescaledtolargerandmorecapableneuralmodels.
3. Counterfactualtheoriesaresufficient—butwemustinterpretthemcorrectly. On(3): if
counterfactualtheoriesdonothandletheseissueswell,perhapsthereexistbettertheories. Itwould
not suffice to return to regularity theories of causality (Mill, 1846), which ultimately reduce to
equatingcorrelationandcausation,andthereforereduceprecisionviatheinclusionoffalsepositives.
Inferentialtheoriesproposerulesunderwhichcertaincausesarealwaysfollowedbyspecificeffects
(Mackie,1965). Therehavebeensignificantadvancesininferentialtheoriesthatexplicitlytackle
overdetermination and non-transitivity (e.g.,Andreas&Günther,2021,2024), but despite this, it
9Unlessalledgesrepresentinfluenceontheoutput,asinMarksetal.(2024).
8wouldbedifficulttoobtainsatisfyingempiricalevidenceofcausalrelationshipsasdistinctfrom
strongcorrelations.
Anappealingalternativetheorycouldbecausalityasatransferofconservedquantitities(Salmon,
1984,1994): ifcauseAtransferssomeconservedquantitytoBintheintervaloftime(A,B],and
thesameoverallquantityremainsinA ∨B throughoutthattime,thenAhascausalinfluenceon
B. InLSTMs(Hochreiter&Schmidhuber,1997)orstatespacemodels(Guetal.,2021,2022),this
quantitycouldberelatedtothestatevectorcarriedacrosstimesteps;causalanalysescouldtherefore
be framed in terms of components which write to and erase from this state.10 In a Transformer
network (Vaswani et al., 2017), what might this conserved quantity be? There is also causality
asKolmogorovcomplexityreduction(Alexander&Gilboa,2023;Janzing&Schölkopf,2010),
wheretheinclusionofcauseC reducestheminimumdescriptionlengthoftheeffectE. However,
thismerelycapturesvariableswhichsufficientlydescribetheeffect,andwhichdonotnecessarily
executeit. Further,thistheoryhasbeenproposedprimarilytocapturethehumannotionofsubjective
causality,ratherthanactualcausality.
Importantly,manynon-counterfactualtheoriesarenotnecessarilymotivatedbycounterfactualsbeing
inaccurateapproximationsofcausality;rather,manyaremotivatedbycounterfactualsbeingdifficult
toobtaininobservationalsettings,asinclinicaltrials.Fortunately,inneuralnetworks,counterfactuals
areeasytoobtain: wehavecompletecontrolovercomputationgraphsandfullaccesstothevalues
of and connections between each node in the graph. Thus, despite its challenges, counterfactual
theoriesarestilllikelythemostactionableforinterpretabilityresearchbasedonneuralnetworks.
Evenwithoutaccesstocounterfactualinputs,theactivationofaparticularnodecansimplybenulled
outviasettingittoitsmeanactivation,orsamplinganactivationfromanunrelatedcontext.Whilenot
fullyprincipled,11thisisstillatractablewaytoestablishasufficientconditionforcausaldependence.
Inshort,thechallengeisnotthatthecounterfactualtheoryhasproblems: thechallengeisthatthese
problemsaredifficult—butpossible—totractablyaddressatscale. Asastartingpoint,futurework
could proceed by creating toy models where overdetermination, preemption, and non-transitive
dependenciesarebuiltintothenetwork. Thiswouldallowustodesignmethodsthatcanadequately
handletheproblemsoutlinedhere;later,thesecouldbemademorescalableviaapproximatemetrics
orheuristic-basedsearchmethods.
5.1 Weneedbettercausalmediators.
Muchofthispaperhasassumedthat, givenaccesstoacausalgraph, onecaninterpretthemedi-
ators/nodesinthegraph. Thisisoftennotthecase. Methodswherethenodescanbeinterpreted
tendtobelessflexibleandopen-endedduetotherequiredeffortandcompute: forexample,sparse
featurecircuits(Marksetal.,2024)requiresignificantcomputefortrainingautoencoders,andthese
autoencodersmaynotgeneralizetoamodelthathasbeensignificantlytuned,pruned,and/oredited.
Thatsaid,thisisaone-timecost: oncetheseautoencodershavebeentrained,theycanbereleased
andeasilydeployedforotherstouse. Indistributedalignmentsearch(Wuetal.,2023;Geigeretal.,
2024),wearerequiredtohaveapriorhypothesisastohowamodelaccomplishesatask,andthe
evaluationmetricsdonottellusinwhatwaysourhypothesesarewrong;thus,refininghypotheses
maybenon-trivialwhenamodelimplementsacomplexbehaviorinanon-human-likeway.
Ontheotherhand, methodsthataremoreflexibleandthatdonotrequirepriorhypothesesoften
yieldcausalgraphswithuninterpretablenodes. Thus,moreworkisneededonproducingParetoim-
provementswithrespecttohumaninterpretabilityandcompute-/labor-efficiency. Sparseautoencoder
dictionariesrepresentedapushinamorefine-graineddirection,butothermethodscouldconsider
learningmorecoarseandhigh-levelabstractions—e.g.,high-dimensionalstructuresorsupersetsof
componentswhichhaveacoherentfunctionalrole.
10ThisissimilartoideasproposedinFerrando&Voita(2024)forTransformers,thoughtheyusesimilarities
asaproxyforwhichcomponentswereimportanttoadownstreamsubmoduleoutput,whichisnotguaranteedto
capturefaithfulcausalsubgraphs.
11ConsiderthisexamplefromNanda(2023):apairofneurons’activationsmaybeuniformlydistributedat
pointsalongacircle;here,themeanactivationofthepairwouldbethecenterofthecircle,whichresultsin
activationsthattheneuronswouldneveractuallyhaveonanyinput.
95.2 PossibleCounterarguments
Possiblecounterargument1:“Sacrificingasmallamountofrecallisacceptable.” Weanticipate
one common counterargument to be that sacrificing some recall is acceptable when the search
spaceofmediators(andcombinationsthereof)islarge. Wewouldargueagainstthisasageneral
recommendation:theremayexistclassesofphenomenathataremorelikelytobeencodedredundantly
thanothers. Thismeansthatsystematicallyexcludingoverdeterminersmaybiasourfindingsinaway
thathidesparticulartypesoffeaturesmorethanothers. Thus,ifoneaimstoobtainthemostaccurate
andcompletepictureofhowamodelaccomplishesabehavior, thenonemustcaptureredundant
causes,aswellasmediatorsthatprimarilyaffectdownstreammediatorsandnottheoutputmetric.
Possiblecounterargument2: “Theseproblemsprimarilyapplytobinarynotionsofcausality.
Neuralnetworksarecontinuous.” Anotherconcernisthattheaboveexampleshavedepended
onbinaryaccountsofcausality,whereaneventeithercausesaneffectoritdoesnot. Indeed,neural
networksaremuchmoreforgiving: theymaybeinterpretedasprobabilisticorcontinuouscausal
graphs,whereedgesbetweennodesareweightedandwhereeffectsmaybemeasuredviacontinuous
metrics like indirect effects. Current interpretability methods already rely on such metrics. And
indeed,Lewis(2000)definescausalityaswhetheracausecounterfactuallyinfluencesthemannerin
whichaneffectoccurs,ratherthanasabinarynotionofwhetherE happens. Thismeansthatcauses
canhavevaryingdegreesandtypesofinfluenceonagiveneffect,whichmakespartialcauseseasier
tohandle.
Nonetheless,thesamenon-transitivityandoverdeterminationissuesstillapply. Theissuehassimply
shifted: indirecteffectsarenottransitivelyconservedincausalchains. Forexample,givenaneuron
AthatstronglyinfluencestheactivationofneuronB,andneuronBthatstronglyinfluencesoutput
y,Amaystillhaveaverysmalleffectony. Amighttriggerorfrequentlyco-occurwithanother
mechanism that nullifies B’s effect on y. And sets of variables A that result in B taking
1,2,...,n
similarvalueswouldstillbedifficulttocompletelyrecallwhenusinginterventionstojustoneof
thesevariablesatatime.
Moreover,causalinterpretabilitymethodsoftendefinesomearbitraryeffectsizethresholdabove
whichwekeepsomecomponentinacircuit,orwhereweconsiderthecomponenttohaveasignificant
enoughcontributitiontotheoutputphenomenontobeconsideredcausallyrelevant. Inotherwords,
we start with continuous effect sizes, but then collapse our final results into discrete subsets or
subgraphsofcomponentsthatareconsideredcauses,whileexcludedcomponentsarenotconsidered
causes. Thiseffectivelygivesusbinarycausalgraphs,whichmakesstructuressuchascircuitseven
moreeasilysusceptibletothesamefallaciesdescribedhere—atleastintheircurrentpresentation.
6 BroaderImpactandLimitations
Thispaperdiscussescounterintuitivechallengesthatariseundercounterfactualtheoryofcausality.
Thesechallengescouldpartiallyexplainwhymethodsthatapplyoreditdiscoveredcomponentsoften
donotgeneralizerobustly. Forexample,inmodelediting,couldthisexplaininpartwhythemost
causallyrelevantcomponentsdiscoveredviacounterfactualinterventionsarenotthemosteffective
locationstoedit(Haseetal.,2023)? Additionally,giventhatinformationisgraduallyaccumulated
acrosslayers(Mengetal.,2022,2023;Gevaetal.,2021,2022),mighttheinclusionoflocallybutnot
necessarilygloballyrelevantcausaldependenciesimprovethefaithfulnessofthecausalsubgraphs
weextract?
We acknowledge that this analysis could be improved or expanded in various ways. While the
problemsdiscussedhereshouldimpactanymethodthatreliesoncounterfactualinterventionstoa
subsetofneuralnetworkcomponents,thispaperiswrittenprimarilywithcircuitdiscovery(Elhage
etal.,2021;Conmyetal.,2023),alignmentsearch(Geigeretal.,2021),orcomponentsetdiscovery
methods(Vigetal.,2020)inmind. Thisisnotarepresentativesampleofcausalandmechanistic
interpretability methods. For example, it is not immediately intuitive how these problems affect
methodsthatderiveprojectionstoeraseorguardparticularconceptsinlatentspace(Belroseetal.,
2023;Ravfogeletal.,2020;Elazaretal.,2021),ormethodsthatleveragegradientsfromparametric
classifierstoupdatethemodelatspecifictokenpositions(e.g.,Giulianellietal.,2018). Onecould
arguethatlearnedprojectionsormodificationstoasinglelayerwillstillbesusceptibletopreemption
andnon-transitivityconcerns,sincelocallyrelevantearlyinformationandpreemptedlaterinformation
10maybemissedbyinterventionsthatonlyaffectthemodelatasinglelayer;nonetheless,ifthislayer
isacausalbottleneck,non-transitivitymaynotapplyiftheentirevectorismodified. Thisshouldbe
exploredinmoredetailinfuturework.
Additionally,simultaneousoverdeterminationhasnotyetbeenattestedinlargelanguagemodels.
Whiletherearealreadyexamplesofpreemption(Wangetal.,2023;McGrathetal.,2023),demon-
strationsthatoverdeterminationispossibleinlanguagemodels(Heimersheim&Nanda,2024),and
demonstrationsthatredundancyiscommon(Tuckeretal.,2022),itisnotclearwhetherthereexist
groups of components in large language models that have the same effect on some downstream
componentoroutputbehaviorwheneithersomesubsetorallsuchcomponentsinthesetareactive.
Generally,whenmultiplecausestriggerthesameeffectinaneuralnetwork,itwilloftenbeadditive,
rather than truly redundant: i.e., each component has a measurable marginal contribution to the
magnitudeoftheeffect,whichmakeseachcomponentdiscoverable. Thus,itisstillnotclearinwhat
settingswewouldmisssimultaneouscauses,orinwhichsettingstheyaremostlikelytoarise.
7 Conclusion
Wehavedescribedtwoproblemsforinterpretabilityresearcherswhobasetheirmethodsoncounter-
factualinterventions: non-transitivityandoverdetermination. Theformercomplicatesmethodsfor
discoveringandinterpretingdeepcausalgraphs,whilethelatterlowersrecallinsystematicways.
Wehavediscussedtheimplicationsofthesefindingswheninterpretingfindingsfrommechanistic
interpretabilitystudies,andproposedsuggestionsforfutureworkinthisspace.
Acknowledgments
WearegratefultoEricTodd,CanRager,YonatanBelinkov,andDavidBauforprovidingfeedback
onanearlierversionofthiswork. Wearealsothankfultothereviewersofthe2024ICMLWorkshop
onMechanisticInterpretabilityfortheirhelpfulcomments. ThisworkwasproducedwhileA.M.was
fundedbyapostdoctoralfellowshipundertheZuckermanSTEMLeadershipprogram.
References
YotamAlexanderandItzhakGilboa. Subjectivecausality. Revueéconomique,74(4):619–633,2023.
HolgerAndreasandMarioGünther. ARamseytestanalysisofcausationforcausalmodels. The
BritishJournalforthePhilosophyofScience,2021.
HolgerAndreasandMarioGünther.Aregularitytheoryofcausation.PacificPhilosophicalQuarterly,
2024.
JacobAndreas. TweetinresponsetoAlexanderRushontheriseofmechanisticinterpretability,2024.
URLhttps://twitter.com/jacobandreas/status/1749917888230883566.
SanderBeckersandJoostVennekens. Thetransitivityandasymmetryofactualcausation. Ergo: An
OpenAccessJournalofPhilosophy,4:1–27,2017.
NoraBelrose,DavidSchneider-Joseph,ShauliRavfogel,RyanCotterell,EdwardRaff,andStella
Biderman. LEACE:Perfectlinearconcepterasureinclosedform. InThirty-seventhConference
onNeuralInformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=
awIpKpwTwF.
StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO’Brien,Eric
Hallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal.
Pythia: Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. InInternational
ConferenceonMachineLearning,pp.2397–2430.PMLR,2023.
TrentonBricken,AdlyTempleton,JoshuaBatson,BrianChen,AdamJermyn,TomConerly,Nick
Turner,CemAnil,CarsonDenison,AmandaAskell,RobertLasenby,YifanWu,ShaunaKravec,
NicholasSchiefer, TimMaxwell, NicholasJoseph, ZacHatfield-Dodds, AlexTamkin, Karina
11Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and
ChristopherOlah. Towardsmonosemanticity: Decomposinglanguagemodelswithdictionary
learning. Transformer Circuits Thread, 2023. URL https://transformer-circuits.pub/
2023/monosemantic-features/index.html.
BilalChughtai,LawrenceChan,andNeelNanda. Atoymodelofuniversality: Reverseengineering
hownetworkslearngroupoperations. InProceedingsofthe40thInternationalConferenceon
MachineLearning.JMLR,2023.
ArthurConmy,AugustineN.Mavor-Parker,AengusLynch,StefanHeimersheim,andAdriàGarriga-
Alonso. Towardsautomatedcircuitdiscoveryformechanisticinterpretability. InThirty-seventh
ConferenceonNeuralInformationProcessingSystems,2023.
Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse au-
toencodersfindhighlyinterpretablefeaturesinlanguagemodels. InTheTwelfthInternational
Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=
F76bwRSLeK.
Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in
pretrainedtransformermodels. InBonnieWebber,TrevorCohn,YulanHe,andYangLiu(eds.),
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP),pp.4908–4926,Online,November2020.AssociationforComputationalLinguistics.
doi:10.18653/v1/2020.emnlp-main.398.URLhttps://aclanthology.org/2020.emnlp-main.
398.
Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic Probing: Behavioral
ExplanationwithAmnesicCounterfactuals. TransactionsoftheAssociationforComputational
Linguistics,9:160–175,032021. ISSN2307-387X. doi: 10.1162/tacl_a_00359. URLhttps:
//doi.org/10.1162/tacl_a_00359.
NelsonElhage,NeelNanda,CatherineOlsson,TomHenighan,NicholasJoseph,BenMann,Amanda
Askell,YuntaoBai,AnnaChen,TomConerly,NovaDasSarma,DawnDrain,DeepGanguli,Zac
Hatfield-Dodds,DannyHernandez,AndyJones,JacksonKernion,LianeLovitt,KamalNdousse,
Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A
mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. URL
https://transformer-circuits.pub/2021/framework/index.html.
Javier Ferrando and Elena Voita. Information flow routes: Automatically interpreting language
modelsatscale. ComputingResearchRepository,arXiv:2403.00824,2024. URLhttps://arxiv.
org/abs/2403.00824.
MatthewFinlayson,AaronMueller,SebastianGehrmann,StuartShieber,TalLinzen,andYonatan
Belinkov. Causal analysis of syntactic agreement mechanisms in neural language models. In
ChengqingZong, FeiXia, WenjieLi, andRobertoNavigli(eds.), Proceedingsofthe59thAn-
nualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJoint
ConferenceonNaturalLanguageProcessing(Volume1: LongPapers),pp.1828–1843,Online,
August2021.AssociationforComputationalLinguistics. doi: 10.18653/v1/2021.acl-long.144.
URLhttps://aclanthology.org/2021.acl-long.144.
Elias Frantarand DanAlistarh. SparseGPT: massive language modelscan be accuratelypruned
inone-shot. InProceedingsofthe40thInternationalConferenceonMachineLearning,2023
InternationalConferenceonMachineLearning.JournalofMachineLearningResearch,2023.
AtticusGeiger,HansonLu,ThomasFIcard,andChristopherPotts. Causalabstractionsofneural
networks. InA.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan(eds.),Advances
inNeuralInformationProcessingSystems,2021. URLhttps://openreview.net/forum?id=
RmuXDtjDhG.
AtticusGeiger,ZhengxuanWu,ChristopherPotts,ThomasIcard,andNoahD.Goodman. Finding
alignmentsbetweeninterpretablecausalvariablesanddistributedneuralrepresentations. Comput-
ingResearchRepository,arXiv:2303.02536,2024. URLhttps://arxiv.org/abs/2303.02536.
12MorGeva,RoeiSchuster,JonathanBerant,andOmerLevy. Transformerfeed-forwardlayersare
key-valuememories. InMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tau
Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, pp. 5484–5495, Online and Punta Cana, Dominican Republic, November 2021.
AssociationforComputationalLinguistics. doi: 10.18653/v1/2021.emnlp-main.446. URLhttps:
//aclanthology.org/2021.emnlp-main.446.
Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers
buildpredictionsbypromotingconceptsinthevocabularyspace. InYoavGoldberg, Zornitsa
Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing, pp. 30–45, Abu Dhabi, United Arab Emirates, December
2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/2022.emnlp-main.3. URL
https://aclanthology.org/2022.emnlp-main.3.
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual
associationsinauto-regressivelanguagemodels. InHoudaBouamor,JuanPino,andKalikaBali
(eds.),Proceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
pp.12216–12235,Singapore,December2023.AssociationforComputationalLinguistics. doi: 10.
18653/v1/2023.emnlp-main.751. URLhttps://aclanthology.org/2023.emnlp-main.751.
MarioGiulianelli,JackHarding,FlorianMohnert,DieuwkeHupkes,andWillemZuidema. Under
the hood: Using diagnostic classifiers to investigate and improve how language models track
agreementinformation. InTalLinzen,GrzegorzChrupała,andAfraAlishahi(eds.),Proceedings
ofthe2018EMNLPWorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksfor
NLP,pp.240–248,Brussels,Belgium,November2018.AssociationforComputationalLinguistics.
doi: 10.18653/v1/W18-5426. URLhttps://aclanthology.org/W18-5426.
Arthur S. Goldberger. Structural equation methods in the social sciences. Econometrica, 40(6):
979–1001,1972. ISSN00129682,14680262. URLhttp://www.jstor.org/stable/1913851.
Nicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato, and Aryaman Arora. Localizing model
behaviorwithpathpatching. ComputingResearchRepository,arXiv:2304.05969,2023. URL
https://arxiv.org/abs/2304.05969.
RhysGould,EuanOng,GeorgeOgden,andArthurConmy.Successorheads:Recurring,interpretable
attentionheadsinthewild. InTheTwelfthInternationalConferenceonLearningRepresentations,
2024. URLhttps://openreview.net/forum?id=kvcbV8KQsi.
AlbertGu,IsysJohnson,KaranGoel,KhaledKamalSaab,TriDao,AtriRudra,andChristopher
Re. Combining recurrent, convolutional, and continuous-time models with linear state space
layers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances
inNeuralInformationProcessingSystems,2021. URLhttps://openreview.net/forum?id=
yWd42CWN3c.
AlbertGu,KaranGoel,andChristopherRe. Efficientlymodelinglongsequenceswithstructured
state spaces. In International Conference on Learning Representations, 2022. URL https:
//openreview.net/forum?id=uYLFoz1vlAC.
NedHall. Twoconceptsofcausation. InJohnCollins,NedHall,andLauriePaul(eds.),Causation
andCounterfactuals,pp.225–276.MITPress,2004.
JosephYHalpern. Sufficientconditionsforcausalitytobetransitive. PhilosophyofScience,83(2):
213–226,2016.
Michael Hanna, Ollie Liu, and Alexandre Variengien. How does GPT-2 compute greater-than?:
Interpretingmathematicalabilitiesinapre-trainedlanguagemodel. InThirty-seventhConference
onNeuralInformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=
p4PckNQR8k.
PeterHase,MohitBansal,BeenKim,andAsmaGhandeharioun. Doeslocalizationinformediting?
Surprisingdifferencesincausality-basedlocalizationvs.knowledgeeditinginlanguagemodels.
In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:
//openreview.net/forum?id=EldbUlZtbd.
13StefanHeimersheimandNeelNanda. Howtouseandinterpretactivationpatching. Computing
ResearchRepository,arXiv:2404.15255,2024. URLhttps://arxiv.org/abs/2404.15255.
ChristopherHitchcock. Theintransitivityofcausationrevealedinequationsandgraphs. TheJournal
ofPhilosophy,98(6):273–299,2001. ISSN0022362X. URLhttp://www.jstor.org/stable/
2678432.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):
1735–1780, nov 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https:
//doi.org/10.1162/neco.1997.9.8.1735.
JingHuang,AtticusGeiger,KarelD’Oosterlinck,ZhengxuanWu,andChristopherPotts. Rigorously
assessing natural language explanations of neurons. In Yonatan Belinkov, Sophie Hao, Jaap
Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi (eds.), Proceedings of the 6th
BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pp. 317–331,
Singapore,December2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.
blackboxnlp-1.24. URLhttps://aclanthology.org/2023.blackboxnlp-1.24.
DavidHume. AnEnquiryConcerningHumanUnderstanding. 1748.
DominikJanzingandBernhardSchölkopf. Causalinferenceusingthealgorithmicmarkovcondition.
IEEETransactionsonInformationTheory,56(10):5168–5194,2010.
JánosKramár,TomLieberum,RohinShah,andNeelNanda. Atp*: Anefficientandscalablemethod
forlocalizingllmbehaviourtocomponents. ComputingResearchRepository,arXiv:2403.00745,
2024. URLhttps://arxiv.org/abs/2403.00745.
David Lewis. Causation. In Tim Crane and Katalin Farkas (eds.), Philosophical Papers II, pp.
159–213.OxfordUniversityPress,1986.
David Lewis. Causation as influence. The Journal of Philosophy, 97(4):182–197, 2000. ISSN
0022362X. URLhttp://www.jstor.org/stable/2678389.
DavidK.Lewis. Counterfactuals. Blackwell,Malden,Massachusetts,1973.
MaximilianLi,XanderDavies,andMaxNadeau. Circuitbreaking: Removingmodelbehaviorswith
targetedablation. InWorkshoponChallengesinDeployableGenerativeAIattheInternational
ConferenceonMachineLearning(ICML),2023. URLhttps://arxiv.org/abs/2309.05973.
Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, and
VladimirMikulik. Doescircuitanalysisinterpretabilityscale? evidencefrommultiplechoice
capabilities in chinchilla. Computing Research Repository, arXiv:2307.09458, 2023. URL
https://arxiv.org/abs/2307.09458.
J.L.Mackie. Causesandconditions. AmericanPhilosophicalQuarterly,2(4):245–264,1965.
Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.
Sparsefeaturecircuits: Discoveringandeditinginterpretablecausalgraphsinlanguagemodels.
ComputingResearchRepository,arXiv:2403.19647,2024. URLhttps://arxiv.org/abs/2403.
19647.
ThomasMcGrath,MatthewRahtz,JanosKramar,VladimirMikulik,andShaneLegg. Thehydra
effect: Emergentself-repairinlanguagemodelcomputations. ComputingResearchRepository,
arXiv:2307.15771,2023. URLhttps://arxiv.org/abs/2307.15771.
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual
associationsinGPT. AdvancesinNeuralInformationProcessingSystems,35,2022.
KevinMeng,ArnabSenSharma,AlexAndonian,YonatanBelinkov,andDavidBau. Massediting
memoryinatransformer. TheEleventhInternationalConferenceonLearningRepresentations
(ICLR),2023.
14JohnStuartMill. ASystemofLogic,RatiocinativeandInductive: BeingaConnectedViewofthe
PrinciplesofEvidence,andtheMethodsofScientificInvestigation. CambridgeUniversityPress,
London,England,1846.
NeelNanda. Acomprehensivemechanisticinterpretabilityexplainer&glossary. Personalblog,
2023. URLhttps://www.neelnanda.io/mechanistic-interpretability/glossary.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn
Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones,
Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,
Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads.
Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/
in-context-learning-and-induction-heads/index.html.
L.A.PaulandNedHall. Causation: AUser’sGuide. OxfordUniversityPressUK,Oxford,2013.
JudeaPearl. Causality: Models,Reasoning,andInference. CambridgeUniversityPress,2000.
JudeaPearl. Directandindirecteffects. InProceedingsoftheSeventeenthConferenceonUncertainty
inArtificialIntelligence,pp.411–420.MorganKaufmann,2001.
Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out:
Guardingprotectedattributesbyiterativenullspaceprojection.InDanJurafsky,JoyceChai,Natalie
Schluter,andJoelTetreault(eds.),Proceedingsofthe58thAnnualMeetingoftheAssociationfor
ComputationalLinguistics,pp.7237–7256,Online,July2020.AssociationforComputational
Linguistics. doi: 10.18653/v1/2020.acl-main.647. URL https://aclanthology.org/2020.
acl-main.647.
JamesM.RobinsandSanderGreenland. Identifiabilityandexchangeabilityfordirectandindirect
effects. Epidemiology, 3(2):143–155, 1992. ISSN10443983. URLhttp://www.jstor.org/
stable/3702894.
WesleyC.Salmon.ScientificExplanationandtheCausalStructureoftheWorld.PrincetonUniversity
Press,1984.
WesleyC.Salmon. Causalitywithoutcounterfactuals. PhilosophyofScience,61(2):297–312,1994.
ISSN00318248,1539767X. URLhttp://www.jstor.org/stable/188214.
VictorSanh,LysandreDebut,JulienChaumond,andThomasWolf. DistilBERT,adistilledversion
ofBERT:smaller,faster,cheaperandlighter. ComputingResearchRepository,arXiv:1910.01108,
2020. URLhttps://arxiv.org/abs/1910.01108.
Lee Sharkey. Sparsify: A mechanistic interpretability research agenda,
2024. URL https://www.lesswrong.com/posts/64MizJXzyvrYpeKqm/
sparsify-a-mechanistic-interpretability-research-agenda.
PaulSoulos,R.ThomasMcCoy,TalLinzen,andPaulSmolensky. Discoveringthecompositional
structureofvectorrepresentationswithrolelearningnetworks.InAfraAlishahi,YonatanBelinkov,
Grzegorz Chrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad (eds.), Proceedings of
theThirdBlackboxNLPWorkshoponAnalyzingandInterpretingNeuralNetworksforNLP,pp.
238–254,Online,November2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2020.blackboxnlp-1.23. URLhttps://aclanthology.org/2020.blackboxnlp-1.23.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
Proceedingsofthe34thInternationalConferenceonMachineLearning-Volume70,ICML’17,
pp.3319–3328.JMLR.org,2017.
AaquibSyed,CanRager,andArthurConmy. Attributionpatchingoutperformsautomatedcircuit
discovery. InNeurIPSWorkshoponAttributingModelBehavioratScale, 2023. URLhttps:
//openreview.net/forum?id=tiLbFR4bJW.
15EricTodd,MillicentL.Li,ArnabSenSharma,AaronMueller,ByronC.Wallace,andDavidBau.
Functionvectorsinlargelanguagemodels. InProceedingsofthe2024InternationalConference
onLearningRepresentations,2024.
MycalTucker,TiwalayoEisape,PengQian,RogerLevy,andJulieShah. Whendoessyntaxmediate
neurallanguagemodelperformance? evidencefromdropoutprobes. InMarineCarpuat,Marie-
CatherinedeMarneffe,andIvanVladimirMezaRuiz(eds.),Proceedingsofthe2022Conferenceof
theNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage
Technologies,pp.5393–5408,Seattle,UnitedStates,July2022.AssociationforComputational
Linguistics. doi: 10.18653/v1/2022.naacl-main.394. URLhttps://aclanthology.org/2022.
naacl-main.394.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,
U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates,Inc.,2017.URLhttps://proceedings.neurips.cc/paper_files/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer,
and Stuart Shieber. Investigating gender bias in language models using causal mediation
analysis. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-
vancesinNeuralInformationProcessingSystems,volume33,pp.12388–12401.CurranAsso-
ciates,Inc.,2020.URLhttps://proceedings.neurips.cc/paper_files/paper/2020/file/
92650b2e92217715fe312e6fa7b90d82-Paper.pdf.
Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.
Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In
The Eleventh International Conference on Learning Representations, 2023. URL https:
//openreview.net/forum?id=NpsVSN6o4ul.
ZhengxuanWu,AtticusGeiger,ThomasIcard,ChristopherPotts,andNoahGoodman.Interpretability
atscale: Identifyingcausalmechanismsinalpaca. InThirty-seventhConferenceonNeuralInfor-
mationProcessingSystems,2023. URLhttps://openreview.net/forum?id=nRfClnMhVX.
QinanYu,JackMerullo,andElliePavlick. Characterizingmechanismsforfactualrecallinlanguage
models.InHoudaBouamor,JuanPino,andKalikaBali(eds.),Proceedingsofthe2023Conference
onEmpiricalMethodsinNaturalLanguageProcessing,pp.9924–9959,Singapore,December
2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.emnlp-main.615. URL
https://aclanthology.org/2023.emnlp-main.615.
A DiscoveringaSparseFeatureCircuitforaSparseFeature
In §3, we discovered a sparse feature circuit containing components that cause a downstream
sparsefeaturetoactivateathigherpositivevalues. ThiscircuitwasdiscoveredwithPythia70M
(Bidermanetal.,2023)usingthesparseautoencoderdictionariesreleasedwithMarksetal.(2024).12
We use the code and data released by the authors; the dataset for this succession cluster is from
https://feature-circuits.xyz,andcorrespondstoCluster382inMarksetal.(2024).
Theprimarydifferencebetweenthissparsefeaturecircuitandtheoriginalcircuitforthisclusteris
thetargetmetric. Intheoriginalwork,themetricwasdefinedasthenegativelog-probabilityofthe
tokenx givensomecontextx :
t+1 1..t
y =−logp (x |x ,...,x )
θ t+1 1 t
In§3,thetargetmetricisthenegativeactivationofthelayer3attentionfeatureatindex14579. Here,
weuse“feature”torefertoasingleindexofasparseautoencoder’sencodedlatentspace. Specifically,
12Releasedathttps://github.com/saprmarks/dictionary_learning.
16giventheoutputvectorxoftheoutprojectionofthelayer3attentionblock,thesuccessionfeatureis
index14579ofthevectorf definedbytheencoder
f =ReLU(W (x−b )+b )
e d e
This operation represents the encoder of the sparse autoencoder dictionary; W , b , and b are
e d e
learnedterms. Then,wesimplysety =−f .
14579
Afeatureiskeptinthecircuitifitsindirecteffect(IE)onysurpassesnodethresholdT ,ahyper-
N
parameter. TheIEiscomputedbyablatingtheactivationofupstreamsparsefeaturea, andthen
measuringthechangeiny;thischangeshouldbepositiveifthenodecausallycontributestoy. In
practice,tocomputetheIE,onemustrunO(n)forwardpasses,wherenisthenumberofcomponents;
giventhattherearenearly600,000sparsefeaturestosearchover,thisisnottractablenorscalable.
Thus,alinearapproximationoftheindirecteffectIˆEisused,asitrequiresonlytwoforwardpasses
andonebackwardpass(O(1)passes)forallmodelcomponents:
∂y(cid:12)
IˆE= (cid:12) ·(a −a )
∂a(cid:12)
a=aoriginal
patch original
(cid:12)
Here, ∂y(cid:12) isthegradientofthesparsefeatureagiventheoriginalinput,whereasa and
∂a(cid:12) patch
a=aoriginal
a aretheactivationofthefeaturegiventhepatchinputandoriginalinput,respectively. Forthis
original
clustercircuit,thereisnopatchinput;therefore,weseta =0,oftencalledazeroablation. Note
patch
thatzeroablations,whilenotprincipledforneurons(whosedefaultvaluesmaynotbe0andwhere
differencesinsignarenotnecessarilymeaningful),sparseautoencodersaretrainedsuchthatany
givenfeature’sactivationshouldbezeroonmostinputs,andonlytakepositiveactivationswhenthe
featurecontributestoxtakingdifferentvaluesthanitsmean. Tomakethismoreaccurate,aslightly
moreexpensiveandmoreaccurateapproximationbasedonintegratedgradients(Sundararajanetal.,
2017)isused;seeMarksetal.(2024)fordetails.
Intheseexperiments,thenodethresholdT issetto0.4andtheedgethresholdT issetto0.04.
N E
ThecircuitwasdiscoveredusingasingleA6000GPU.
17