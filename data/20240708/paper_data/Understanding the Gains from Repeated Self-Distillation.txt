Understanding the Gains from Repeated Self-Distillation
Divyansh Pareek Simon S. Du Sewoong Oh
Paul G. Allen School of Computer Science and Engineering
University of Washington, Seattle, WA
{dpareek,ssdu,sewoong}@cs.washington.edu
Abstract
Self-Distillation is a special type of knowledge distillation where the student model has the same
architecture as the teacher model. Despite using the same architecture and the same training data,
self-distillationhasbeenempiricallyobservedtoimproveperformance,especiallywhenappliedrepeatedly.
For such a process, there is a fundamental question of interest: How much gain is possible by applying
multiple steps of self-distillation? To investigate this relative gain, we propose studying the simple
but canonical task of linear regression. Our analysis shows that the excess risk achieved by multi-step
self-distillation can significantly improve upon a single step of self-distillation, reducing the excess risk by
a factor as large as d, where d is the input dimension. Empirical results on regression tasks from the UCI
repository show a reduction in the learnt model’s risk (MSE) by up to 47%.
1 Introduction
Knowledgedistillation[11]wasinitiallyproposedasawaytotransfertheknowledgelearntbyalargerteacher
model to a smaller student model, which can then be deployed in limited resource settings. The process is
as follows: Train a teacher (T) model using ground-truth labels, then use its predictions to supervise the
training of a student (S) model via a combined per-sample loss,
ξ·ℓ(cid:0) yˆ ,y (θ)(cid:1) +(1−ξ)·ℓ(cid:0) y,y (θ)(cid:1) , (1)
T S S
where ℓ denotes the loss function, y is the ground-truth label, yˆ denotes the teacher’s prediction, and y (θ)
T S
denotes the student’s prediction, parameterized by the learnable θ. The extra hyperparameter ξ is called
the imitation parameter [24], generally restricted to ξ ∈ [0,1]. It gives additional freedom to the student
to balance importance between labels and teacher’s predictions. The student trained via this distillation
objective (i.e., utilizing the teacher’s predictions through ξ ̸=0) has been widely observed to generalize better
than when trained only on the labels (i.e., ξ =0). This gain has been attributed to ‘dark knowledge’ that is
(i) impossible to be directly extracted from the training data by the small model, but (ii) easily learnt by the
large model and transferred to the small model.
Challenging this interpretation, Li et al. [19] and Furlanello et al. [9] empirically observed performance gains
through distillation even when the teacher and student are same-sized models. One can set T and S to have
the same architecture, and S trained with the objective in Eq. (1) outperforms T. This is referred to as
Born-Again Networks (BANs) or Self-Distillation (SD). Furthermore, repeatedly applying self-distillation
on the same training data with a student model having the same architecture provides additional gains on
benchmark datasets and architectures [9, 35, 43]. At each step, the student from the previous step acts as
the teacher used to train a new student model under the self-distillation loss of Eq. (1). For such multi-step
self-distillation, there is a fundamental question of interest: How much more gain can we get by repeatedly
applying self-distillation?
Recently, Das and Sanghavi [8] provided theoretical understanding of the original one-step self-distillation.
For the canonical task of fixed design linear regression, considering the standard ridge estimator as both the
teacherand student model, [8]showedthat thereis indeedaregime ofproblem instancesin whichthe optimal
1
4202
luJ
5
]GL.sc[
1v00640.7042:viXrastudent (i.e., with optimally tuned ridge parameter λ and imitation parameter ξ) can provably achieve a
strictly lower test error than the optimal teacher (i.e. with optimally tuned λ). However, the amount of this
gain has not been characterized in closed form, and can only be numerically evaluated for a given problem
instance. Inspired by this work, we aim to study the performance gains from multi-step self-distillation under
linear regression.
Contributions. We summarize our contributions below.
• Under the fixed design linear regression defined in Section 3.1, we show that the optimal multi-step
self-distilled model (i.e., each ξ value at each step is optimized for the validation accuracy of the
final multi-step self-distilled model) can achieve a test error that is a factor of d smaller than the
optimal one-step self-distillation (Theorem 1), under certain assumptions on the problem parameters
(Assumption 2). Here, d is the dimension of the input. Our analysis in Theorem 1 suggests that the
sequence of ξ parameters provides additional freedom that can control the spectrum of eigenvalues of
the linear estimator. Optimally choosing these ξ parameters can significantly reduce the variance of the
estimator, leading to a factor of (up to) d difference in the overall test errors of multi-step SD compared
to 1-step SD. We note that Das and Sanghavi [8] also observed a bias-variance tradeoff associated with
the ξ parameter for 1-step SD compared to the ridge, which was the reason behind 1-step SD strictly
outperforming the ridge.
• We demonstrate the necessity of Assumption 2 theoretically (Theorems 2 and 3) and numerically
(Figure 3). Further, we provide a lower bound for the test error that any repeated SD can achieve, and
show that only r steps of SD (with optimally chosen ξ at each step) are sufficient to achieve this lower
bound, when the input data matrix has rank r (Theorem 4).
• Bycapturingthefunctionalformofthetesterrorinξ (Theorem5), wealsoshowamethodtopractically
select the ξ parameters for real-world regression tasks. In Section 5, we empirically show that this
theoretical insight leads to selecting effective ξ values, which can indeed achieve a lower test error on
real-world regression tasks.
2 Related Work
Knowledge distillation and self-distillation. Hinton et al. [11], Ba and Caruana [3] proposed knowledge
distillation to transfer knowledge learnt by large teacher models into smaller student models without any
substantial performance drop (e.g., [29, 30, 13, 7, 32, 23, 31] and surveys in [10, 12]). Distillation also
provides interpretability [22], robustness to adversarial examples [27], and defense against backdoor attacks
[38, 20, 34], although stronger backdoor attacks have been proposed that bypass distillation defense [16].
Perhapssurprisingly,empiricalobservationsshowthatperformanceimproveswhenateachermodelisdistilled
into a student model with the same architecture on the same training data (self-distillation). Performance
gains with one-step self-distillation of the form Eq. (1) were first demonstrated by Li et al. [19] for AlexNet
on YFCC100M. Further gains can be achieved by repeating self-distillation, as shown for the DenseNet
architecture family on CIFAR10 and CIFAR100 [9, Table 2]. To empirically explain such gains, Zhang
and Sabuncu [43] measured prediction uncertainty on the same multi-step experiments and offered an
interpretation that soft labels capture sample-level uncertainties. Yang et al. [35] also reproduced the same
experiments and explained the gains as knowledge refinement on the class similarities. We will analytically
study the gains that can be achieved with such repeated self-distillation.
Many variations of self-distillation have also been proposed. Snapshot Distillation [36] tries to treat previous
checkpoints (snapshots) of the same model as the teacher. Zhang et al. [42] employ a group of collaborative
students with no teacher. Zhang et al. [41, 40] use it for model self-improvement, and DINO [6] adopts
self-distillation for self-supervised learning. Knowledge distillation is also popular for transfer learning, where
the student model is trained on a different dataset than the teacher model [37, 39, 1], which is not a setting
we address. With the recent scaling of data, [44], [21] are relevant works using a teacher model for either
label editing or data reweighing.
2Theory of distillation and self-distillation. TheoreticalunderstandingofdistillationstartedwithPhuong
and Lampert [28] studying linear student networks. Mobahi et al. [26] studied self-distillation theoretically
in the restricted setting of ξ = 1 (i.e. only teacher supervision, no ground-truth labels), showing that in
this setting, the SD process acts as a regularizer, with a few steps of SD helping, but further steps hurting
model performance. We study the setting where ξ is not restricted to 1, and show a different conclusion. In
particular, we observe that more steps of SD always provide an improvement, if the ξ parameters are chosen
optimally. Allen-Zhu and Li [2] analyzed a stylized setting, where a different view of the data is learned by
different models, and show how ensemble methods can combine the views, achieving improved test accuracy.
Thisframeworkisusedtoshowhowself-distillationcanalsoimprovemodelaccuracybyimplicitlyperforming
ensembling. Menon et al. [25] theoretically studied distillation in the classification setting, and also observed
a bias-variance tradeoff underlying teacher supervision. Das and Sanghavi [8] theoretically studied one-step
self-distillation for fixed design linear regression and binary classification, and showed that the student can
provably achieve a lower test error than the teacher. We take inspiration from them and study the multi-step
SD to characterize this performance gain, showing that the multi-step SD can outperform one-step SD by a
large factor. Jeong and Chung [15] also take inspiration from [8] and provide understanding of multi-step
self-distillation in the multi-class classification setting.
3 Problem formulation and background on self-distillation
Focusing on the simple but canonical task of linear regression, we investigate the performance gain from
applying repeated self-distillation.
3.1 Linear regression
For the observed response Y ∈R and the covariate X ∈Rd, the following assumption is standard in linear
regression, e.g., [8].
Assumption 1. There exist θ⋆ ∈Rd and γ >0 such that (i) E[Y|X]=⟨θ⋆,X⟩; (ii) Var[Y|X]=γ2 for all
X ∈Rd; and (iii) (Y −E[Y|X])⊥⊥X, i.e. the label noise is independent of X.
The training set of size n is denoted by X∈Rd×n, the collection of covariates, and Y ∈Rn, the responses;
Y =X⊤θ⋆+η,withη satisfyingE[η]=0,E[ηη⊤]=γ2I . Theprobleminstanceisdefinedbyitsparameters
n
(X,θ⋆,γ2),treatingX=[X ,X ,···X ]asfixedbutY asrandom. Thetrainingset(X,Y)isoneoccurrence
1 2 n
of the random noise η ∈Rn. In this fixed design setup, the excess risk of an estimator θˆis defined using the
standard Σˆ -norm, ∥v∥ =∥Σˆ1/2v∥ , as
n Σˆ
n
n 2
(cid:104) (cid:105)
ExcessRisk(θˆ) := E ∥θˆ−θ⋆∥2 , (2)
η Σˆ
n
where Σˆ
n
:=(1/n)XX⊤ is the covariance matrix, and the expectation is over the randomness in η. Measuring
the error in the Σˆ -norm ensures that the signal-to-noise ratio is uniform in all directions. The popular ridge
n
estimator serves as a baseline, using a single hyperparameter λ>0:
θˆ(λ) := argmin(cid:0) ∥Y−X⊤θ∥2+λ∥θ∥2(cid:1) = (cid:0) XX⊤+λI (cid:1)−1 XY . (3)
d
θ∈Rd
We use Ω :=XX⊤+λI throughout. We consider only λ>0, but surprisingly, Kobak et al. [18] showed
λ d
that the optimal penalty λ⋆ (one with the lowest risk) can indeed be negative. However we will largely work
in the non-overparameterized case (n>d), where λ⋆ >0 holds.
3.2 Self-distillation
Applying the self-distillation loss in Eq. (1) to linear regression with hyperparameters λ>0 and ξ ∈R,
(cid:16) (cid:17)
θˆ(λ,ξ) := argmin ξ∥X⊤θˆ(λ)−X⊤θ∥2+(1−ξ)∥Y−X⊤θ∥2+λ∥θ∥2 (4)
θ∈Rd
3= (cid:0) XX⊤+λI (cid:1)−1 X(cid:16) ξ·X⊤θˆ(λ)+(1−ξ)·Y(cid:17) (5)
d
(cid:124) (cid:123)(cid:122) (cid:125)
Newlabel
= (cid:8) (1−ξ)·I +ξ·Ω−1XX⊤(cid:9) ·Ω−1XY , (6)
d λ λ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Pre-conditioner: functionof(λ,ξ) Ridgeθˆ(λ)
where ξ ∈R is not restricted to the conventional [0,1] interval. This additional freedom is meaningful since it
can result in a strictly better solution, as noted by Das and Sanghavi [8] both theoretically (Remark 3.6)
and empirically (Table 1). It is worth noting that the optimization problem in eq. (4) remains convex for
any ξ ∈R, since its hessian evaluates to ξ·2XX⊤+(1−ξ)·2XX⊤ =2XX⊤ ⪰0. On the other hand, the
teacher and student use the same ridge penalty λ for simplicity.
We call this estimator 1-step self-distillation. This can be interpreted as (i) assigning new labels that combine
the ground-truth labels with the teacher’s predictions, or (ii) pre-multiplying the usual ridge estimator with
a pre-conditioner. Note that ξ = 0 recovers ridge. Das and Sanghavi [8, Theorem 3.8] show that under a
certain condition, 1-step self-distillation strictly dominates ridge, i.e.,
(cid:104) (cid:105) (cid:104) (cid:105)
min E ∥θˆ(λ,ξ)−θ⋆∥2 < minE ∥θˆ(λ)−θ⋆∥2 , (7)
λ≥0,ξ∈R η 2 λ≥0 η 2
wheretheriskismeasuredinthenon-standardEuclideannorm. Thesamestrictinequalitycanbeshownunder
the standard Σˆ -norm under a slightly modified condition stated in Proposition B.1. This naturally leads to
n
a fundamental question: How much more gain can we get by repeatedly applying self-distillation?
ξ
T S 1-step self-distillation
ξ(k) ξ(k) ξ(k)
1 2 k
S S S ··· S k-step self-distillation
0 1 2 k
Figure 1: The standard 1-step self-distillation defined in Eq. (1) with parameter ξ and k-step self-distillation
that repeatedly applies Eq. (1) with parameter ξ(k) =[ξ(k),ξ(k),...,ξ(k)]∈Rk.
1 2 k
3.3 Repeated self-distillation
The standard repeated application of self-distillation starts with the teacher model, T (which we also refer
to as the zeroth model, S ), and applies self-distillation sequentially for k steps. At each step i, Eq. (1) is
0
applied with the (i−1)th model, S as the teacher, and the ith model, S as the student, with an imitation
i−1 i
parameter ξ(k), i.e., θˆ∈argmin (cid:8) ξ(k)ℓ(cid:0) yˆ ,y (θ)(cid:1) +(1−ξ(k))ℓ(cid:0) y,y (θ)(cid:1)(cid:9) for i∈[k]. The collection of
i θ i Si−1 Si i Si
parameters is denoted by ξ(k) =[ξ(k),ξ(k),...,ξ(k)]∈Rk.
1 2 k
This repeated self-distillation has been studied, for example, theoretically in [26] and empirically in [9]. We
aim to understand its gain under linear regression, where we prove that
(cid:40)(cid:32) k (cid:33) k (cid:41)
θˆ(λ,ξ(k)) = 1−(cid:88) ξ¯(k) I +(cid:88) ξ¯(k)(cid:0) Ω−1XX⊤(cid:1)i ·Ω−1XY , (8)
i d i λ λ
(cid:124)(cid:123)(cid:122)(cid:125) i=1 i=1 (cid:124) (cid:123)(cid:122) (cid:125)
∈Rk (cid:124) (cid:123)(cid:122) (cid:125) Ridgeθˆ(λ)
Pre-conditioner: P(λ,ξ(k))
with ξ¯(k) :=(1−ξ(k))(cid:81)k ξ(k) for each i∈[k], and we let ξ(k) =0. The proof that repeated SD with
i k−i l=k−i+1 l 0
ξ(k) ∈Rk results in Eq. (8) is provided in Appendix C.2. Here ξ(k) ∈Rk denote the imitation parameters, λ
denotes the ridge coefficient for all the models, and ξ¯(k) ∈ Rk is a reparametrization of ξ(k) ∈ Rk (details
in Appendix C.2). We call this k-step self-distillation. Note the increasing flexibility in the pre-conditioner
matrix. The increasing powers of Ω−1XX⊤ in the above expression are still numerically stable, since, for
λ
4λ > 0, Ω−1XX⊤ is PSD with all eigenvalues in [0,1]. As an aside, one can also consider a version of SD
λ
where the ith model receives supervision from all S instead of just S . Appendix C.1 shows that this
<i i−1
version provides no extra representational capacity over the repeated version presented above, when all k
entries of ξ(k) are optimized as free parameters. Hence, the procedure in Figure 1 suffices for analysis.
4 Main results for linear regression
The main object of our study is to theoretically demonstrate the gains from repeated self-distillation.
Concretely, we aim to show that there can be a significant multiplicative separation between the excess risk
achieved by r-step SD (Self-Distillation), where r is the rank of the input X; compared to the ridge estimator,
as well as the 1-step SD (Section 4.1). The necessity of the two main assumptions is shown in Section 4.2.
The sufficiency of r steps of SD is shown in Section 4.3. In Section 4.4, we provide an exact characterization
of the excess risk achieved by k-step SD (for any k).
4.1 Ther-stepself-distillationsignificantlyimprovesuponthe1-stepself-distillation
We show the desired separation under the following assumption (and two more mild technical assumptions
specified fully in Appendix E).
Assumption 2. Assume the following two conditions hold on the problem instance
(cid:0) X,θ⋆,γ2(cid:1)
:
1. No two non-zero singular values of X collide, i.e. s > s > ··· > s > 0, where {s }r denote the
1 2 r j j=1
non-zero singular values of the input data matrix X whose rank is denoted by r.
2. ∡(θ⋆,u )=0, where {u }d denote the eigenvectors of XX⊤, u being the leading one.
1 j j=1 1
Assumption 2 is needed to show that r-step SD achieves a small excess risk in Eq. (9). In general, both
these conditions are somewhat necessary for the separation, as we show in Theorems 2 and 3. We now state
our main result. We show that under the above assumption, there exists a family of problem instances,
(X,θ⋆,γ2), such that the excess risk achieved by r-step SD is a factor of r :=rank(X) smaller than that of
the ridge estimator and the 1-step SD.
Theorem 1. Under the fixed design linear regression in Assumption 1, there exists a family of problem
instances satisfying Assumption 2 such that for any instance (X,θ⋆,γ2) in the family, it holds that
(cid:16) (cid:17) γ2
∃λ>0,∃ξ(r) ∈Rr, ExcessRisk θˆ(λ,ξ(r)) ≤ , (9)
n
(cid:16) (cid:17) rγ2
∀λ>0,∀ξ ∈R, ExcessRisk θˆ(λ,ξ) ≥(0.99/29) , and (10)
n
(cid:16) (cid:17) rγ2
∀λ>0, ExcessRisk θˆ(λ) ≥(0.98) , (11)
n
where r :=rank(X), n is the number of samples, θˆ(λ,ξ(r)) and θˆ(λ,ξ) are the r-step and 1-step SD estimators
defined in Eqs. (8) and (4) respectively, and θˆ(λ) is the ridge estimator defined in Eq. (3).
We provide precise conditions and a proof in Appendix E. Since each k-step SD includes (k−1)-step SD
as a special case with the proper choice of ξ(k), the hyperparameter-tuned excess risk of repeated SD is
monotonically non-increasing. However, it is perhaps unexpected that the multiplicative separation between
r-step SD and 1-step SD can be as large as Ω(r), demonstrating the gains of repeated SD. Figure 2 illustrates
this Ω(r) multiplicative separation on a synthetic family of problems. Note that Ω(d) separation can be
achieved by choosing the problem instance to have rank r =d, at the cost of requiring many more steps of
SD. This Ω(d) factor is the largest multiplicative separation possible with self-distillation, as shown by the
fundamental lower bound in Theorem 4 for any pre-conditioning based approach.
Remark 4.1. SD significantly outperforms ridge by primarily reducing the variance. For the lower bound on
ridge’s excess risk, i.e., Eq. (11), we ignored the bias term and only used the variance term. The repeated SD
(Eq. (9)) primarily reduces the variance to improve the excess risk over Eq. (11).
5Figure 2: On a synthetic problem family with dimension d = 100, noise variance γ = 0.1, and θ⋆ = u
1
(agreement with Asmp. 2.2); we set the singular values of X with a power law from s =1 to s ={0.8,0.5}
1 r
(left and right panels) and vary r = rank(X). Both plots show a linear increase of the relative gain of
r-step self-distillation in excess risk, i.e. the ratio A/B where A := min λ>0ExcessRisk(cid:0) θˆ(λ)(cid:1) and B :=
min ExcessRisk(cid:0) θˆ(λ,ξ(r))(cid:1); demonstrating that r-step SD outperforms ridge by a factor of Ω(r),
λ>0,ξ(r)∈Rr
with the constant inside the Ω (i.e. slope of the line) changing with the effective condition number, s1/sr.
4.2 Necessity of Assumption 2
In Figure 3, we empirically show on synthetic tasks how violating Assumption 2.1 or 2.2 leads to higher
excess risks, even for the r-step SD (r =4 in the example). This supports the necessity of both assumptions,
which we analytically investigate in the following.
Necessity of Assumption 2.1 on X. We assume that the non-zero singular values of X are unique. This
allows us to tightly upper bound the excess risk achieved by r-step SD in Eq. (9) via Theorem 4. We show in
the following that some version of Assumption 2.1 is also necessary. For a more detailed explanation of why
we need Assumption 2.1, we refer the reader to Remark 4.2.
Theorem 2. Under the hypotheses of Theorem 1 except for Assumption 2.1, if the singular values of X
satisfy s =...=s =1, where r =rank(X), for all k ≥1, λ>0, and ξ(k) ∈Rk, we have
1 r
(cid:32) (cid:33)−1
(cid:16) (cid:16) (cid:17)(cid:17) rγ2 rγ2
ExcessRisk θˆ λ,ξ(k) ≥ 1+ . (12)
n (cid:80)r ⟨θ⋆,u ⟩2
j=1 j
Furthermore, there exists λ>0 such that the ridge, θˆ(λ), achieves this lower bound with equality.
We provide a proof in Appendix F. This implies that when there is no gap in the singular values of the
input X, there is no separation between ridge and SD estimators (repeated or not). Intuitively, if the s
j
are all equal, the pre-conditioner for ridge (i.e., Ω−1) and the pre-conditioner for the repeated SD, both are
λ
restricted to have all eigenvalues to be equal. (Repeated) SD has no degrees of freedom to deviate from this.
However, s ’s being unequal provides the freedom for the ξ(k) to control the SD’s pre-conditioner matrix, and
j
reduce the excess risk. This is also why in Remark 4.2, we hypothesize that numerically, the optimal (cid:0) ξ(k)(cid:1)⋆
depends inversely on the min-gap of the singular values. Figure 4 demonstrates this increasing relationship of
the magnitude of the optimal ξ parameters w.r.to the decreasing singular gap.
6(a) s:=[1,1/2,1/3,1/4],θ⋆ =u
1
(b) s:=[1,1,1/2,1/3] (c) θ⋆ :=0.5(u 1+u 2+u 3+u 4)
Figure 3: On a synthetic task (explained in Section 5.1), X has rank 4 with (a) θ⋆ =u and distinct s ’s; (b)
1 j
s = [1,1,1/2,1/3]; (c) θ⋆ = 0.5(u 1+u 2+u 3+u 4). Each additional step of SD with optimal choice of ξ(k)
reduces ExcessRisk(θˆ(λ,(ξ(k))⋆)) for any choice of λ on the x-axis. Panel (a) satisfies Asmp. 2 and hence
4-step SD is necessary to achieve the optimal excess risk. This is no longer true when Asmp. 2.1 is violated
(b) or Asmp. 2.2 is violated (c). Excess risk achieved by 4-step SD (i.e. the green lines) in panels (a) and (c)
exactly match the numerical value given by RHS of eq. (14), i.e. the fundamental lower bound for any SD
estimator. But this is not the case in panel (b) [which has the same lower bound from eq. (14) as panel (a)],
because Asmp. 2.1 is violated.
Figure 4: On the synthetic problem from Figure 3a, we fix λ = 0.125 and set the singular values of X as
s = {1−(j −1)ϵ}, i.e. consecutive values are separated by ϵ. For k-step SD with k = {1,2,3}, we plot
j
(ξ(k))⋆(λ) (i.e. optimal values of the ξ parameters) by varying ϵ∈{0.2,0.1,0.05,0.02,0.01}. The magnitude
of ξ(k) values increases as the singular gap ϵ decreases, verifying Remark 4.2.
k
Necessity of Assumption 2.2 on θ⋆. For simplicity, we assumed that θ⋆ is aligned solely with u (i.e.,
1
projection onto any other u is zero for j ≥2). In general, it is sufficient that θ⋆ is completely aligned with
j
any one of the eigenvectors of XX⊤ (not necessarily the leading eigenvector) to prove a large separation
between ridge and repeated SD. We make this precise in Appendix E.1. We show next that if θ⋆ is equally
(mis)aligned with all the eigenvectors {u }r of XX⊤, then again there is no separation between ridge and
j j=1
repeated SD.
Theorem 3. Under the hypotheses of Theorem 1 except for Assumption 2.2, if the true parameter θ⋆ satisfies
⟨θ⋆,u ⟩2 =z for all j ∈[r], it holds that for all z >0, k ≥1,λ>0, and ξ(k) ∈Rk,
j
ExcessRisk(cid:16) θˆ(λ,ξ(k))(cid:17)
≥
γ2 (cid:88)r (cid:32)
1+
γ2 (cid:33)−1
. (13)
n zs2
j=1 j
Furthermore, there exists λ>0 such that the ridge, θˆ(λ), achieves this lower bound with equality.
7We provide a proof in Appendix G. Similar conditions are needed when analyzing 1-step SD in [8] as well; [8,
Eq. (9) in Theorem 3.8] is required for the 1-step SD to strictly outperform ridge. Observe that Eq. (9) is
violated when either (i) s ’s are all equal or (ii) ⟨θ⋆,u ⟩2’s are all equal.
j j
4.3 r steps of self-distillation are sufficient
For a given problem instance (X,θ⋆,γ2), the excess risk achieved by the k-step SD with parameter ξ(k) can
be exactly characterized (Theorem 5), but it is complicated and can only be evaluated numerically in general.
On the other hand, we show that there exists a fundamental lower bound that holds for a linear family
of estimators including all repeated SD, and this lower bound has a simple characterization (Lemma 4.1).
Furthermore, we show that under a mild assumption on the eigenvalues of XX⊤ in Assumption 2.1, the
r-step SD achieves the lower bound (Theorem 4). This allows a precise characterization of the performance
of r-step SD.
Theorem 4. Under the fixed design linear regression in Assumption 1, the excess risk of any k-step SD
estimator on an instance (X,θ⋆,γ2), is lower bounded for all k ≥1, λ>0, and ξ(k) ∈Rk by
ExcessRisk(cid:16) θˆ(λ,ξ(k))(cid:17)
≥
γ2 (cid:88)r ⟨θ⋆,u j⟩2
, (14)
(cid:16) (cid:17)
n ⟨θ⋆,u ⟩2+ γ2
j=1 j s2
j
where (s ,u ) is the jth eigenvalue and eigenvector of X and r := rank(X). Furthermore, if Assumption
j j
2.1 holds then there exists λ>0 and ξ(r) ∈Rr such that the equality is achieved by the r-step SD estimator
θˆ(λ,ξ(r)).
A proof is provided in Appendix H and we provide a sketch below.
Proof sketch. The lower bound in Eq. (14) is an instantiation of Lemma 4.1, since θˆ(λ,ξ(k)) is a specific
linear family estimator with P=P(cid:0) λ,ξ(k)(cid:1) Ω−1 defined in Eq. (8). To show achievability, we need to show
λ
that P(cid:0) λ,ξ(k)(cid:1) Ω−1 = P⋆ for some value of k, λ, and ξ(k). This holds when the below system of r linear
λ
equations admits a solution for the k parameters (i.e. ξ(k)), with an extra free parameter λ>0. We show
that with k =r and under Assumption 2.1, there exists λ>0 that will ensure the existence of a solution for
this system of equations.
 1−(cid:88)k ξ¯(k)  1−(cid:32) s2
j
(cid:33)i 

s2
j =
⟨θ⋆,u j⟩2
∀j ∈[r] (15)
i=1 i  λ+s2 j  λ+s2 j ⟨θ⋆,u j⟩2+ γ s22
j
Remark 4.2 (Necessity of Assumption 2.1). This assumption is required for (15). Otherwise, the LHS would
be the same for indices j and j+1 if s =s , but the RHS could still be different as ⟨θ⋆,u ⟩≠ ⟨θ⋆,u ⟩
j j+1 j j+1
generally. If Assumption 2.1 does not hold, there might not be any ξ(k) satisfying the set of equations for
a general θ⋆ ∈Rd. Further, the system of linear equations in Eq. (15) becomes more ill-conditioned as the
singular values s ,j ∈[r] get closer to each other. Capturing this dependence explicitly is outside the scope of
j
this paper.
Lower bound for a linear family. Consider a linear family of estimators of the form θˆ(P):=P·XY, for
P:=U S˜U⊤, whose eigenspace coincides with that of XX⊤ (i.e., U =[u ,...,u ]) and has d degrees of
d d d 1 d
freedom represented by the eigenvalues S˜=diag[s˜ ,··· ,s˜ ]. This is a generic form of any linear estimator,
1 d
albeit with the restriction of the eigenvectors matching the underlying U . In particular, k-step SD is an
d
instantiation of this with P=P(λ,ξ(k))Ω−1 (refer to Eq.( 8)).
λ
Lemma 4.1. The Excess Risk for θˆ(P)=P·XY where P:=U S˜U⊤, satisfies
d d
ExcessRisk(cid:16) θˆ(P)(cid:17)
≥
γ2 (cid:88)r ⟨θ⋆,u j⟩2
, (16)
(cid:16) (cid:17)
n ⟨θ⋆,u ⟩2+ γ2
j=1 j s2
j
8with equality achieved at P=P⋆ =U S˜⋆U⊤, given by
d d

 ⟨θ⋆,uj⟩2 , j ≤r (i.e., s >0)
s˜⋆
j
= (⟨θ⋆,uj⟩2s2 j+γ2) j . (17)
any real value , j ≥r+1 (i.e., s =0)
j
Proof sketch. This is straightforward. One can expand the excess risk for θˆ(P), which is a quadratic
expression in s˜ ,j ∈[d]. Completing the squares shows the lower bound of Eq. (16) and the optimal values
j
s˜⋆ of Eq. (17). A full proof is given in Appendix H.1.
j
4.4 The excess risk for the k-step SD estimator is quadratic in ξ¯(k) ∈ Rk
We give an explicit formula for the excess risk achieved by for the k-step SD estimator from Eq. (8). Since
θˆ(λ,ξ(k)) is linear in each of ξ¯(k),i ∈ [k] (recall that ξ¯(k) is a reparametrization of ξ(k)), the overall excess
i
risk is quadratic in ξ¯(k) as shown below. Appendix I provides a proof and the expressions for M(k), m(k), and
c. This quadratic form will be especially useful in experiments.
Theorem 5 (Informal version of Theorem 7 in Appendix I). Under the fixed design linear regression in
Assumption 1, the excess risk achieved by the k-step SD is quadratic in ξ¯(k) ∈Rk:
(cid:16) (cid:17) (cid:16) (cid:17)⊤ (cid:16) (cid:17) (cid:16) (cid:17)⊤
ExcessRisk θˆ(λ,ξ(k)) = ξ¯(k) M(k) ξ¯(k) +2 ξ¯(k) m(k)+ c. (18)
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
∈Rk×k ∈Rk
FromthedetailedexpressionsgiveninAppendixI,wenotethatM(k) isasumofr symmetricrank-1matrices,
which means it can have a maximum rank of r. This implies that M(k) ∈Rk×k for k >r is rank-deficient
(causing no additional decrease in the excess risk if the ξ¯(r) ∈ Rr were chosen optimally to minimize the
excess risk). This indicates that r steps of SD might be sufficient to achieve the optimal excess risk, which is
indeed what we observe in Theorem 4.
5 Experiments
In this section, we empirically show that multi-step SD can outperform the ridge and single-step SD. We first
present a synthetic setting (section 5.1) to validate our theory. In section 5.2, we discuss a strategy to select
ξ parameters based on the theoretical insight from section 4.4. In section 5.3, we implement that strategy on
real-world regression tasks and show that it can indeed select performant ξ values that provide multi-step SD
estimators that achieve a smaller test risk.
5.1 Synthetic Experiments
We validate our theoretical results with a fixed design synthetic experiment. We consider a problem with
d=r =4, and set problem parameters (X,θ⋆,γ2). Namely, X’s singular values are set as s
j
:=1/j for j ∈[4],
and θ⋆ :=u as in Theorem 1. Figure 3 shows the result for γ =0.125, along with two more settings that
1
validate the necessity of our assumptions (validating Theorems 2 and 3). Figure 3a confirms that repeated
steps of SD do provide a reduction in the excess risk, since the lowest point of the curve for each k reduces
as k increases. Also note that the optimal λ for each k (one that produces lowest excess risk estimator) is
different. Appendix J presents some more settings, including θ⋆ :=1/√ 2(u 1+u 2) for comparison with [8]. An
interesting phenomenon in Figure 3 is that local maxima in k-step SD’s curve coincide with local minima in
(k−1)-step SD’s curve, which was proven for k =1 in [8], and we observe empirically for all k.
Explanation of Figures 3, 5. For the fixed design synthetic experiment in Figure 3 and the random design
real-world experiment in Figure 5 (section 5.3), the curves plotted are with the optimal (ξ(k))⋆ for each λ.
Hence, the curve of k-step SD will point-wise be lower/equal to the curve of (k−1)-step SD, since more steps
of SD only provide more freedom. We say k-step SD strictly dominates (k−1)-step SD when the minimum
value of k-step SD’s excess risk is strictly lower than that of (k−1)-step SD. For Figure 3, the optimal (ξ(k))⋆
9is found analytically from the problem parameters. For real-world datasets in Figure 5, we describe how to
find (ξ(k))⋆ for any given λ in the next section.
5.2 Choosing the hyperparameters ξ for real-world datasets
We have shown that at the cost of introducing additional hyperparameters and setting them to their optimal
values, one can extract a large (upto Ω(d)) performance gain. However, how does one select these ξ’s for
real-world datasets? The standard practice is to use a validation set, and perform a grid search. But this
becomes infeasible for k-step SD for larger values of k, since performing a search over parameters in Rk+1
(i.e k values of ξ(k) and 1 value of λ) quickly becomes impractical. But our theoretical analysis provides an
insight that can be used to directly compute the optimal ξ(k) ∈Rk (for a chosen λ) given a few evaluations
on the validation set with certain chosen ξ(k) values. Namely, Theorem 5 tells us that the ExcessRisk is
quadratic in ξ¯(k) (the reparameterized version). Now the coefficients of the quadratic depend on unknown
quantities (like θ⋆,γ2), however we can use the validation set to estimate these coefficients.
For example, for 1-step SD, we know that the ExcessRisk(θˆ(λ,ξ)) = Aξ2+2Bξ+C for unknown A,B,C
(that depend on λ). Training 3 specific 1-step SD estimators, with ξ = {−1,0,1}, and measuring each of
those 3 estimators’ Risk on the validation set, lets us estimate A,B,C. We then know that ξ⋆ =−B/A, for
the chosen value of λ. Hence, we only need to perform a grid search over one parameter, λ. Appendix K
provides more discussion, and provides a detailed illustration of the above process for k =2. Note that this is
feasible when the cost/time needed for a single training run of a k-step SD is small (since we need to do it
multiple times for various ξ(k) values).
5.3 Real-world regression experiments
We implement multi-step SD for real-world regression tasks from the UCI repository [17], and demonstrate
that 2-step SD can outperform ridge and 1-step SD. Note that for this section, the test set will contain fresh
samples of X ∈Rd, i.e. random design linear regression instead of fixed design. Our metric for an estimator’s
performance will be mean squared error (MSE) on a test set of unseen examples, which is the empirical
version of total risk (i.e. excess risk + unknown noise variance γ2). Given n such examples denoted
test
by {(x ,y )},i∈[n ], the MSE of an estimator θˆis given by the mean of per-sample squared errors, i.e.,
i i test
MSE(θˆ)=(cid:80) i(⟨θˆ,xi⟩−yi)2/ntest.
Using the training and validation splits, we compute (i) Optimal ridge: θˆ(λ⋆), (ii) Optimal 1-step SD:
0
θˆ(λ⋆,ξ⋆), and (iii) Optimal 2-step SD: θˆ(cid:0) λ⋆,(ξ⋆,ξ⋆)(cid:1). This is done by plotting the MSE on the validation set
1 2 1 2
for a grid of λ values for all three estimators, and choosing the λ that achieves the lowest error for each one.
For any given λ, the optimal ξ⋆(λ) is chosen by the strategy described in Section 5.2. Finally, we evaluate the
MSE of all three chosen estimators on the test set, which serves as our performance metric (refer to Table 1).
Appendix L explains the overall methodology in greater detail. We apply this methodology on three datasets
(descriptions in Appendix L.1).
Table 1 describes the results we observe. For two of the three datasets, 2-step SD can outperform both ridge
and 1-step SD. For the Air Quality dataset, 2-step SD significantly outperforms both ridge and 1-step SD,
reducing the MSE by 47.2% compared to the optimal ridge. In contrast, for the AEP dataset, we observe
that the SD process cannot improve upon the ridge at all. The MSE curves in Figure 5 shed more light on
these observations. Notice how Figures 5a, 5b show a gap in the ridge and 2-step SD (similar to Figure 3a),
whereas Figure 5c shows no such gap (similar to Figure 3c).
We also verify that the strategy described in section 5.2 indeed selects performant ξs. Appendix L.2 shows
that the optimal ξ values given in Table 1, selected via the strategy in section 5.2, are indeed the ones
that minimize the validation MSE. Further, we empirically observe the quadratic nature of risk (MSE) vs ξ
described in Theorem 5 (Figure 10 in Appendix L.2).
10Table 1: Chosen hyperparameter values and the achieved test set MSE for ridge and 1,2-step SD.
Dataset Optimal ridge Optimal 1-step SD Optimal 2-step SD
Air Quality
Optimalityhyperparameters λ⋆ 0=102 λ⋆ 1,ξ⋆=103,−4.1 λ⋆ 2,(ξ 1⋆,ξ 2⋆)=103,(−0.9,−16.2)
TestsetMSE 2.01 1.99 1.06
Airfoil
Optimalityhyperparameters λ⋆ 0=102 λ⋆ 1,ξ⋆=100,66.5 λ⋆ 2,(ξ 1⋆,ξ 2⋆)=103,(−1.8,−7.8)
TestsetMSE 1.34 1.22 1.19
AEP
Optimalityhyperparameters λ⋆ 0=102.5 λ⋆ 1,ξ⋆=102.5,0.1 λ⋆ 2,(ξ 1⋆,ξ 2⋆)=102.5,(−2.4,−2.3)
TestsetMSE 0.62 0.62 0.63
(a) Air Quality dataset (b) Airfoil dataset (c) AEP dataset
Figure 5: Validation set MSE vs λ for three estimators: Ridge, 1-step SD and 2-step SD.
6 Conclusion and Broader Impacts
Inthispaper,wetheoreticallystudiedthemulti-stepself-distillationforfixeddesignlinearregression,withthe
goal of characterizing its performance compared to the single-step SD. Perhaps surprisingly, we demonstrated
that the optimal multi-step SD can outperform the optimal single-step SD by a factor as large as d in the
estimator’s excess risk, where d is the input dimension of the regression. Our analysis is limited by the fixed
design assumption, and it would be useful to study the case of random design linear regression as well. We
empirically demonstrated the gains from using 2-step SD on simple linear regression tasks. Larger scale
empirical studies of multi-step SD, especially leveraging the insights of Section 5.2 on hyperparameter search,
remain as a direction of future work.
Our contributions are largely on the theoretical understanding of multi-step self-distillation, and its potential
performance gains. At a high-level, self-distillation can use data more effectively, since it allows us to extract
more knowledge from the same training dataset. In today’s age with data being one of the most important
resources, this has positive potential impacts through more judicious use of data. On the other hand, we
propose to use multiple steps of self-distillation, requiring more compute and potentially contributing to
higher environmental costs.
Acknowledgements
ThisworkissupportedbyNSFgrantsno.2019844,2112471,2229876,2134106,2143493,and2229881.
References
[1] S. Ahn, S. X. Hu, A. Damianou, N. D. Lawrence, and Z. Dai. Variational information distillation
for knowledge transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 9163–9171, 2019.
11[2] Z. Allen-Zhu and Y. Li. Towards understanding ensemble, knowledge distillation and self-distillation in
deep learning. In The Eleventh International Conference on Learning Representations, 2023.
[3] J. Ba and R. Caruana. Do deep nets really need to be deep? Advances in neural information processing
systems, 27, 2014.
[4] T. Brooks, D. Pope, and M. Marcolini. Airfoil Self-Noise. UCI Machine Learning Repository, 2014. DOI:
https://doi.org/10.24432/C5VW2C.
[5] L. Candanedo. Appliances Energy Prediction. UCI Machine Learning Repository, 2017. DOI:
https://doi.org/10.24432/C5VC8G.
[6] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties
in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on
computer vision, pages 9650–9660, 2021.
[7] G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker. Learning efficient object detection models with
knowledge distillation. Advances in neural information processing systems, 30, 2017.
[8] R. Das and S. Sanghavi. Understanding self-distillation in the presence of label noise. In Proceedings of
the 40th International Conference on Machine Learning, pages 7102–7140. PMLR, 2023.
[9] T. Furlanello, Z. Lipton, M. Tschannen, L. Itti, and A. Anandkumar. Born again neural networks. In
International Conference on Machine Learning, pages 1607–1616. PMLR, 2018.
[10] J. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge distillation: A survey. International Journal of
Computer Vision, 129(6):1789–1819, 2021.
[11] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NIPS Deep
Learning and Representation Learning Workshop, 2015. URL http://arxiv.org/abs/1503.02531.
[12] C.Hu, X.Li, D.Liu, X.Chen, J.Wang, andX.Liu. Teacher-studentarchitectureforknowledgelearning:
A survey. arXiv preprint arXiv:2210.17332, 2022.
[13] Z. Huang and N. Wang. Like what you like: Knowledge distill via neuron selectivity transfer. arXiv
preprint arXiv:1707.01219, 2017.
[14] V. Jankovic. Quadratic functions in several variables. http://elib.mi.sanu.ac.rs/files/journals/
tm/15/tm821.pdf.
[15] H. Jeong and H. W. Chung. Understanding self-distillation and partial label learning in multi-class
classification with label noise. arXiv preprint arXiv:2402.10482, 2024.
[16] R.Jha,J.Hayase,andS.Oh. Labelpoisoningisallyouneed. Advances in Neural Information Processing
Systems, 36:71029–71052, 2023.
[17] M. Kelly, R. Longjohn, and K. Nottingham. The uci machine learning repository. https://archive.
ics.uci.edu.
[18] D. Kobak, J. Lomond, and B. Sanchez. The optimal ridge penalty for real-world high-dimensional data
can be zero or negative due to the implicit ridge regularization. Journal of Machine Learning Research,
21(169):1–16, 2020.
[19] Y. Li, J. Yang, Y. Song, L. Cao, J. Luo, and L.-J. Li. Learning from noisy labels with distillation. In
Proceedings of the IEEE international conference on computer vision, pages 1910–1918, 2017.
[20] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma. Neural attention distillation: Erasing backdoor
triggers from deep neural networks, 2021.
[21] Z. Lin, Z. Gou, Y. Gong, X. Liu, Y. Shen, R. Xu, C. Lin, Y. Yang, J. Jiao, N. Duan, et al. Rho-1: Not
all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024.
12[22] X. Liu, X. Wang, and S. Matwin. Improving the interpretability of deep neural networks with knowledge
distillation. In 2018 IEEE International Conference on Data Mining Workshops (ICDMW), pages
905–912. IEEE, 2018.
[23] Y. Liu, K. Chen, C. Liu, Z. Qin, Z. Luo, and J. Wang. Structured knowledge distillation for semantic
segmentation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 2604–2613, 2019.
[24] D. Lopez-Paz, L. Bottou, B. Schölkopf, and V. Vapnik. Unifying distillation and privileged information.
arXiv preprint arXiv:1511.03643, 2015.
[25] A. K. Menon, A. S. Rawat, S. Reddi, S. Kim, and S. Kumar. A statistical perspective on distillation. In
International Conference on Machine Learning, pages 7632–7642. PMLR, 2021.
[26] H. Mobahi, M. Farajtabar, and P. Bartlett. Self-distillation amplifies regularization in hilbert space.
Advances in Neural Information Processing Systems, pages 3351–3361, 2020.
[27] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. In 2016 IEEE symposium on security and privacy (SP),
pages 582–597. IEEE, 2016.
[28] M. Phuong and C. Lampert. Towards understanding knowledge distillation. In International conference
on machine learning, pages 5142–5151. PMLR, 2019.
[29] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep
nets; 2014. arXiv preprint arXiv:1412.6550, 3, 2014.
[30] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick, R. Pascanu, V. Mnih,
K. Kavukcuoglu, and R. Hadsell. Policy distillation. In International Conference on Learning Represen-
tations, 2016.
[31] S. Sun, Y. Cheng, Z. Gan, and J. Liu. Patient knowledge distillation for bert model compression. arXiv
preprint arXiv:1908.09355, 2019.
[32] G. Urban, K. J. Geras, S. E. Kahou, O. Aslan, S. Wang, A. Mohamed, M. Philipose, M. Richardson,
and R. Caruana. Do deep convolutional nets really need to be deep and convolutional? In International
Conference on Learning Representations, 2017.
[33] S. Vito. Air Quality. UCI Machine Learning Repository, 2016. DOI: https://doi.org/10.24432/C59K5F.
[34] J. Xia, T. Wang, J. Ding, X. Wei, and M. Chen. Eliminating backdoor triggers for deep neural networks
using attention relation graph distillation, 2022.
[35] C.Yang,L.Xie,S.Qiao,andA.L.Yuille. Trainingdeepneuralnetworksingenerations: Amoretolerant
teacher educates better students. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pages 5628–5635, 2019.
[36] C. Yang, L. Xie, C. Su, and A. L. Yuille. Snapshot distillation: Teacher-student optimization in one
generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 2859–2868, 2019.
[37] J. Yim, D. Joo, J. Bae, and J. Kim. A gift from knowledge distillation: Fast optimization, network
minimization and transfer learning. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 4133–4141, 2017.
[38] K. Yoshida and T. Fujino. Countermeasure against backdoor attack on neural networks utilizing
knowledge distillation. Journal of Signal Processing, 2020.
[39] S. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance
of convolutional neural networks via attention transfer. In International Conference on Learning
Representations, 2017.
13[40] L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma. Be your own teacher: Improve the performance
of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 3713–3722, 2019.
[41] L. Zhang, C. Bao, and K. Ma. Self-distillation: Towards efficient and compact neural networks. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 44(8):4388–4403, 2021.
[42] Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu. Deep mutual learning. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 4320–4328, 2018.
[43] Z. Zhang and M. Sabuncu. Self-distillation as instance-specific label smoothing. Advances in Neural
Information Processing Systems, 33:2184–2195, 2020.
[44] B. Zhu, M. I. Jordan, and J. Jiao. Iterative data smoothing: Mitigating reward overfitting and
overoptimization in rlhf. arXiv preprint arXiv:2401.16335, 2024.
14A Notation
In this short section, we collect some notation used throughout the proofs.
Decomposition of X. Let rank(X)=r and the SVD of X be X=(cid:80)r s u vT where s ≥s ≥···≥
j=1 j j j 1 2
s >0, and each u ∈Rd and v ∈Rn. Further, let {u ,u ,··· ,u } be the full set of left singular vectors of
r j j 1 2 d
X (even those corresponding to zero singular values), forming an orthonormal basis of Rd. Let U ∈Rd×d
d
and U ∈ Rd×r denote the left singular matrix of X for the full and truncated set of left singular vectors
r
respectively. Similarly,letV ∈Rn×d andV ∈Rn×r denotethefullandtruncatedrightsingularmatrix. Let
d r
S ∈Rd×d and S ∈Rr×r denote the collection of the singular values (with and without zeros respectively).
d r
Then, it holds that
X=U S V⊤ =U S V⊤ . (19)
r r r d d d
Indices. Throughout the text, the indices i lie in [k], i.e. they denote subsequent steps of self-distillation.
The indices j lie in [r] or [d], ie they denote dimensions of the d-dimensional space (aligned with vectors of
U or U ). v ,V will denote indexing into a vector v, matrix V. There is one exception to v denoting a
r d j i,j j
vector’s jth element, which is the below.
Components of θ⋆ on U . We will denote θ⋆ :=⟨θ⋆,u ⟩2, j ∈[d] as the components of θ⋆ onto X’s left
d j j
singular space. Note that (cid:80)d θ⋆ =∥θ⋆∥2.
j=1 j 2
B Discussion on norm used in excess risk metric
We point out that Das and Sanghavi [8] used the ∥.∥ norm instead of the more natural ∥.∥ norm to
measure their fixed design excess risk (in eq (2)). Alm2 ost all our results have an equivalent veΣ rˆ snion in the
∥.∥ norm setting also, since the only difference is in the relative weighing of the underlying dimensions of
2
variation, i.e. with the ∥.∥ norm, ∀j ∈[d], direction u is weighed by s2/n instead of a constant 1 weight
(independent of j). In partΣ iˆ cnular, we also present the versj ion of [8, Eq. (9)j from Theorem 3.8] that will result
in a strict dominance like Eq. (7) under the Σˆ norm, i.e.
n
(cid:104) (cid:105) (cid:104) (cid:105)
min E ∥θˆ(λ,ξ)−θ⋆∥2 < minE ∥θˆ(λ)−θ⋆∥2 . (20)
λ≥0,ξ∈R η Σˆ n λ≥0 η Σˆ n
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=ExcessRisk(θˆ(λ,ξ)) =ExcessRisk(θˆ(λ))
Proposition B.1. Let λ⋆ := argmin ExcessRisk(θˆ(λ)). Then Eq. (20) holds on a problem instance
λ>0
(X,θ⋆,γ2) when
(cid:88)r k (cid:88)−1s4 js4 k(cid:0) s2
j
−s2 k(cid:1)(cid:0) ⟨θ⋆,u k⟩2−⟨θ⋆,u j⟩2(cid:1)
<0 . (21)
(cid:0) λ⋆+s2(cid:1)4 (λ⋆+s2)4
k=1j=1 j k
Note that this differs from [8, Eq. (9)] in just one respect: it has s4s4 instead of s2s2.
j k j k
C Details on ξ parameters for general k-step SD
C.1 Full k-step SD is representationally no larger than Repeated k-step SD
We first illustrate the Full k-step SD in Figure 6. The repeated version introduces k extra hyperparameters
in the form of ξ(k) ∈Rk parameters, whereas the full version introduces k(k+1)/2.
Consider the case of k = 2, since that is the lowest value of k for which the full version and the repeated
versiondiffer. Figure7illustratesthisdifferenceexplicitly. Wewillshowthatthefreedomofξ˜∈R3 isnomore
than the freedom of ξ ∈R2. This shows the equivalence of the full 2-step and the repeated 2-step versions,
when ξ˜∈R3, ξ ∈R2 are free parameters optimized over the entire respective spaces. Such equivalence for
the general k-step case is then easy to see.
15S S S ··· S Repeated k-step self-distillation
0 1 2 k
S S S ··· S Full k-step self-distillation
0 1 2 k
Figure 6: Illustrating two possible generalizations of 1-step SD to a k-step process.
Nodes S ,S˜ are both solving the ridge regression problem (3). Let θ = θ˜ = Ω−1XY denote the
0 0 0 0 λ
estimator for both these nodes. Similarly S and S˜ are solving the same problem, although with different
1 1
parameters. We have θ (ξ ) = Ω−1X(cid:0) ξ ·X⊤θ¯ +(1−ξ )·Y(cid:1) (and similarly, one can write θ˜ with ξ˜
1 1 λ 1 0 1 1 1
instead of ξ ). Now the node S is also solving a 1 parameter supervised SD problem, so θ (ξ ,ξ ) =
1 2 2 1 2
Ω−1X(cid:0) ξ ·X⊤θ (ξ )+(1−ξ )·Y(cid:1). Expanding this gives
λ 2 1 1 2
θ (ξ ,ξ )=(cid:8) (1−ξ )·I +(ξ −ξ ξ )·Ω−1XX⊤+ξ ξ ·(Ω−1XX⊤)2(cid:9) Ω−1XY . (22)
2 1 2 2 d 2 1 2 λ 1 2 λ λ
But the optimization problem for S˜ is a 2 parameter supervised SD. It evaluates to
2
(cid:32) (cid:33)
ξ˜ ξ˜ (1−ξ˜ −ξ˜ ) λ
argmin 2a∥X⊤θ˜ −X⊤θ∥2+ 2b∥X⊤θ˜ −X⊤θ∥2+ 2a 2b ∥Y−X⊤θ∥2+ ∥θ∥2 .
θ∈Rd 2 0 2 1 2 2
Following through a similar calculation, we observe that θ˜ for node S˜ is given by
2 2
(cid:110) (cid:111)
θ˜ (ξ˜,ξ˜ ,ξ˜ )= (1−ξ˜ −ξ˜ )·I +(ξ˜ +ξ˜ −ξ˜ξ˜ )·Ω−1XX⊤+ξ˜ξ˜ ·(Ω−1XX⊤)2 Ω−1XY .
2 1 2a 2b 2a 2b d 2a 2b 1 2b λ 1 2b λ λ
(23)
ξ ξ
1 2
S S S Repeated 2-step self-distillation
0 1 2
ξ˜
2a
ξ˜ ξ˜
S˜ 1 S˜ 2b S˜ Full 2-step self-distillation
0 1 2
Figure 7: Repeated vs Full 2-step SD. We show that the extra freedom of the parameter ξ˜ does not provide
2a
any additional freedom, when the other two ξ˜,ξ˜ are optimized as free parameters.
1 2b
Equations (22) vs (23) show that the full 2-step offers the same freedom as the repeated 2-step. However the
repeated version has one shortcoming. To generate the optimal 2-step SD estimator, ξ needs to be different
1
than the value needed for generating the optimal 1-step SD estimator. That is, the repeated k-step version,
with its k free parameters, allows only to generate the final kth-step estimator as the optimal one (i.e. if we
choose the (ξ ,ξ ) values so that the 2nd estimator is the optimal 2-step SD estimator, then the 1st estimator
1 2
with the chosen ξ won’t be the optimal 1-step SD estimator). Whereas the full k-step version, with all its
1
k(k+1)/2 free parameters, allows us to generate a sequence of all k optimal estimators.
C.2 Proof that k-step SD estimator with ξ(k) ∈ Rk will have the form given in
eq (8)
Consider Figure 1. ξ(k) ∈ Rk is the set of actual imitation parameters used for running k-step (repeated)
SD. Let θˆ(λ,ξ(k)) denote the k-step estimator generated by using ξ(k) parameters. In what follows, we will
prove that θˆ(λ,ξ(k)) will have the form given in eq (8), with ξ¯(k) ∈Rk as the described reparametrization of
16ξ(k) ∈Rk. Since we’re in the repeated version, the kth step objective will be a combination of losses w.r.to
ground-truth labels and predictions from the (k−1)th step. So, the objective for the kth step is
(cid:32) (cid:33)
ξ(k) (1−ξ(k)) λ
θˆ(λ,ξ(k)):=argmin k ∥X⊤θˆ(λ,ξ(k−1))−X⊤θ∥2+ k ∥Y−X⊤θ∥2+ ∥θ∥2 . (24)
θ∈Rd
2 2 2
Note that θˆ(λ,ξ(k)) recursively depends on predictions from the previous θˆ(λ,ξ(k−1)). This objective is of the
form in eq (4), so similar to eq (5), we have the following expression
θˆ(λ,ξ(k))=(cid:0) XX⊤+λI (cid:1)−1 X(cid:110) ξ(k)·X⊤θˆ(λ,ξ(k−1))+(1−ξ(k))·Y(cid:111) (25)
d k k
(cid:110) (cid:111)
= (1−ξ(k))·Ω−1XY+ξ(k)·Ω−1XX⊤θˆ(λ,ξ(k−1)) . (26)
k λ k λ
Using this, we can inductively prove that the general form of this estimator is captured by Eq. (8).
Claim. With the described reparametrization, i.e., ξ¯(k) :=(1−ξ(k))(cid:81)k ξ(k) for each i∈[k] (where
i k−i l=k−i+1 l
we let ξ(k) =0), Eq. (8) is the solution to the recursive form Eq. (26).
0
Proof. We will prove this by induction.
Base Case. From eqs (8) and (26), the case for k =1 is true (with ξ¯(1) =ξ(1)).
Inductive Step. Assuming Eq. (8) captures the solution of Eq. (26) for k−1, we get
(cid:40) (cid:32)(cid:40) k−1 (cid:41) k−1 (cid:33)(cid:41)
θˆ(λ,ξ(k))= (cid:110) 1−ξ(k)(cid:111) I +ξ(k)Ω−1XX⊤ 1−(cid:88) ξ¯(k−1) I +(cid:88) ξ¯(k−1)(cid:0) Ω−1XX⊤(cid:1)i ·Ω−1XY .
k d k λ i d i λ λ
i=1 i=1
This again satisfies the form in equation (8), when the following coefficients match
k
ξ(k) =(cid:88) ξ¯(k) ,
k i
i=1
k−1
ξ(k)·(1−(cid:88) ξ¯(k−1))=ξ¯(k) ,
k i 1
i=1
ξ(k)·ξ¯(k−1) =ξ¯(k) ∀i∈{2,3,··· ,k} .
k i−1 i
One can then see that the described reparametrization makes the above hold true.
Remark. Since θˆ(λ,ξ(1)) is simply the 1-step SD estimator with the form θˆ(λ,ξ(1))=P·Ω−1XY for some
λ
preconditioner P, plugging this in the equation eq (26), we realize that we can factor out Ω−1XY (i.e. the
λ
ridge solution) from the expression on the right side. That is, θˆ(λ,ξ(2)) = P′·Ω−1XY for some different
λ
pre-conditioner P′. This is why inductively we get that θˆ(λ,ξ(k)), k ≥1 all produce a pre-conditioning on
the ridge solution, as shown in eq (8). Further, eq (26) also dictates why we get increasing powers of the
term Ω−1XX⊤ in the final expression.
λ
C.3 Explicit reparametrization for k = 2,3
We explicitly demonstrate the reparametrization for k =2,3. As noted in Section 5.2 (and Appendix K),
owing to the quadratic form of excess risk in ξ¯(k), one can find the optimal ξ¯(k) analytically. It can then be
translated back to the original ξ(k) as follows.
For k =2, the form is (dropping the .(2) for ease)
ξ¯
ξ =1− 1 , ξ =ξ¯ +ξ¯ . (27)
1 (cid:0) ξ¯ +ξ¯(cid:1) 2 1 2
1 2
For k =3, the form is (dropping the .(3) for ease)
ξ¯ ξ¯
ξ =1− 2 , ξ =1− 1 , ξ =ξ¯ +ξ¯ +ξ¯ . (28)
1 ξ¯ +ξ¯ 2 ξ¯ +ξ¯ +ξ¯ 3 1 2 3
2 3 1 2 3
17D Algebraic expansion of θˆ (λ,ξ(k)) from eq (8)
Lemma D.1. The estimator θˆ(λ,ξ(k)) given in eq (8) can be expanded as
θˆ(λ,ξ(k))=(cid:88)d  1−(cid:88)k
ξ¯
i(k)  1−(cid:32) λ+s2
j
s2(cid:33)i 
·
λ+s2
j
s2
·⟨θ⋆,u j⟩·u
j
j=1 i=1  j  j
+(cid:88)d  1−(cid:88)k
ξ¯
i(k)  1−(cid:32) λ+s2
j
s2(cid:33)i 
·
λ+s
j
s2
·⟨η,v j⟩·u
j
j=1 i=1  j  j
Proof. The expansion relies on XX⊤ =U S2U⊤ and XX⊤+λI =U (S2+λI )U⊤. U is orthonormal
d d d d d d d d d
(U U⊤ =I =U⊤U ), which neatly cancels all occurences of U in the middle, and allows us to directly
d d d d d d
combine the matrices in the eigenvalue space.
E Detailed version and a proof of Theorem 1
We first write a detailed version of the theorem that exactly characterizes the family of problem instances
that admits the separation. We then present a proof. We point the reader to Appendix A for notations used
throughout the proof.
Theorem 6 (Detailed version). Consider the following conditions on θ⋆,X that characterize a family of
problem instances {(X,θ⋆,γ2)}:
1. θ⋆ =∥θ⋆∥·u (implying θ⋆ =∥θ⋆∥2 and θ⋆ =0 for j ≥2) [Assumption 2.2]
1 1 j
2. ∥θ⋆∥2 >99·(cid:0) rγ2/s2 r(cid:1) ·(cid:0) s2 1/s2 r(cid:1)
3. Assumption 2.1 holds on the singular values of X [Assumption 2.1]
4. (s2 1/s2 r)≤2 holds on the singular values of X
Under Condition #1 + Condition #3, it holds that
(cid:16) (cid:17) γ2
∃λ>0,∃ξ(r) ∈Rr, ExcessRisk θˆ(λ,ξ(r)) ≤ (Rewriting eq (9))
n
Under Condition #1 + Condition #2 + Condition #4, it holds that
(cid:16) (cid:17) 0.99 rγ2
∀λ>0,∀ξ ∈R, ExcessRisk θˆ(λ,ξ) ≥ · (Rewriting eq (10))
29 n
Under Condition #1 + Condition #2, it holds that
(cid:16) (cid:17) rγ2
∀λ>0, ExcessRisk θˆ(λ) ≥0.98· (Rewriting eq (11))
n
Proof. We will analyze all three: k-step SD, ridge, and 1-step SD.
k-step SD: Since Assumption 2.1 holds, from Theorem 4 we directly have
∃λ>0,∃ξ(r) ∈Rr,
ExcessRisk(cid:16) θˆ(cid:16) λ,ξ(r)(cid:17)(cid:17)
=
γ2 (cid:88)r θ j⋆
(29)
(cid:16) (cid:17)
n θ⋆+ γ2
j=1 j s2
j
18Since θ⋆ =0,j ≥2 from Condition #1, we have
j
γ2 θ⋆
= · 1 (30)
n θ⋆+ γ2
1 s2
1
γ2
≤ (31)
n
This completes the proof for (9).
Ridge: We will now show (11), by characterizing the Excess Risk for the ridge estimator in this regime,
showing that it can be upto r times worse. From Lemma I.1, we realize that the Excess Risk expression for
the simple ridge estimator θˆ(λ) is given by
∀λ>0,
ExcessRisk(cid:16) θˆ(λ)(cid:17)
=
1 (cid:88)r (cid:0) λ2θ j⋆+γ2s2 j(cid:1) ·s2
j (32)
n (λ+s2)2
j=1 j
≥
γ2 (cid:88)r s4
j (33)
n (λ+s2)2
j=1 j
(cid:124) (cid:123)(cid:122) (cid:125)
JusttheVarianceterm
Inequality (33) above comes from ignoring the bias term (since it is non-negative). Note that the variance
term is a decreasing function of λ. And again from Lemma I.1, we get the following expression for λ⋆ >0
that minimizes the ExcessRisk.
(cid:80)r s4
j
λ⋆ =γ2·
j=1 (λ⋆+s2 j)3
(34)
(cid:80)r θ j⋆s4
j
j=1 (λ⋆+s2)3
j
=
γ2 ·(cid:88)r (cid:18) s j(cid:19)4 ·(cid:32) λ⋆+s2 1(cid:33)3
(θ⋆ =0, j ≥2 and θ⋆ =∥θ⋆∥2 from Cond #1)
∥θ⋆∥2 s λ⋆+s2 j 1
j=1 1 j
≤ γ2 ·(cid:88)r (cid:18) s j(cid:19)4 ·(cid:32) s2 1(cid:33)3 (Since λ⋆+s2 1 ≤ s2 1, as λ⋆ >0)
∥θ⋆∥2 s s2 λ⋆+s2 s2
j=1 1 j j j
≤
γ2 ·(cid:88)r (cid:18) s 1(cid:19)2
(35)
∥θ⋆∥2 s
j
j=1
γ2 (cid:18) s (cid:19)2
≤ ·r 1 (36)
∥θ⋆∥2 s
r
Now since the variance term in (33) is a decreasing function of λ, so we can use the upper bound of λ⋆ from
(36) to lower bound the ExcessRisk of optimal ridge as
ExcessRisk(cid:16) θˆ(λ⋆)(cid:17)
≥
γ2 ·(cid:88)r 1
n (cid:18) (cid:16) (cid:17)2(cid:19)2
j=1 1+ 1 · γ2 ·r s1
s2
j
∥θ⋆∥2 sr
γ2 (cid:88)r 1
≥ · (Using Condition #2)
n j=1 (cid:16) 1+ s2 r · 1 (cid:17)2
s2 99
j
γ2 (cid:88)r 1
≥ · (Since s ≤s , ∀j ≤r)
n (cid:0) 1+ 1 (cid:1)2 r j
j=1 99
19γ2 992
≥ ·r· (37)
n 1002
(cid:124)(cid:123)(cid:122)(cid:125)
≥0.98
This completes the proof of eq (11).
1-step SD: To evaluate 1-step SD’s ExcessRisk, we make use of Theorem 5. Observe that since k =1, the
ExcessRisk is a simple quadratic in one variable. We will call ξ(1) ∈ R as just ξ (similar to eq (6)). And
similarly, we will call M(1),m(1) as just M,m, both real-valued. We then have
(cid:16) (cid:17)
ExcessRisk θˆ(λ,ξ) =Mξ2+2mξ+c
m2
≥c− (By simple quadratic min)
M
Mc−m2
= (38)
M
Note that M,m,c are all functions of λ. Now we evaluate their expressions. c is simply ExcessRisk of ridge,
which we will fetch from Lemma I.1. Evaluate M from Theorem 5:
r
(cid:16) λθ j⋆ +γ2(cid:17)
M =
1 (cid:88) ρj
·C (1)·C (1)
n (1+ρ )2 j j
j
j=1
=
1 (cid:88)r
(cid:16) λ ρθ jj⋆ +γ2(cid:17)
·
ρ2
j (Using C (1)=1− 1 )
n (1+ρ j)2 (1+ρ j)2 j 1+ρj
j=1
=
1 (cid:88)r (cid:0) λθ j⋆ρ j +γ2ρ2 j(cid:1)
n (1+ρ )4
j
j=1
 
=
n1 (cid:88)r (cid:0) λ2θ j⋆ (λs6
j
++ sγ 2)2 4λ2s4 j(cid:1)
 (Using ρ j = sλ 2)
j=1 j j
Evaluate m from Theorem 5:
m=
1 (cid:88)r (cid:0) λθ j⋆−γ2(cid:1)
·C (1)
n (1+ρ )2 j
j
j=1
=
1 (cid:88)r (cid:0) λθ j⋆−γ2(cid:1)
·
ρ
j (Using C (1)=1− 1 )
n (1+ρ j)2 1+ρ
j
j 1+ρj
j=1
 
=
n1 (cid:88)r (cid:0) λ2θ (j⋆ λs4
j
+− s2γ )2 3λs4 j(cid:1)
 (Using ρ j = sλ 2)
j=1 j j
We now write the expressions together for comparison, before we use them.
n·M
=λ2(cid:88)r θ j⋆s6
j
+λ2γ2(cid:88)r s4
j
(λ+s2)4 (λ+s2)4
j=1 j j=1 j
n·m=λ2(cid:88)r θ j⋆s4
j
−λγ2(cid:88)r s4
j
(λ+s2)3 (λ+s2)3
j=1 j j=1 j
n·c=λ2(cid:88)r θ j⋆s2
j
+γ2(cid:88)r s4
j (Directly from Lemma I.1)
(λ+s2)2 (λ+s2)2
j=1 j j=1 j
20With all these pieces, for the numerator in eq (38), we have
n2·(cid:0) Mc−m2(cid:1)
=T +T +T
1 2 3
where we use T ,T ,T to capture terms of different forms. T will capture the θ⋆ product terms, T will
1 2 3 1 j 2
capture the γ2 product terms, and T will capture the cross terms. Namely,
3
T
=λ4(cid:88)r (cid:88)r θ j⋆θ l⋆s2 js2
l
·(cid:32) s4
j +
s4
l −
2s2 js2
l
(cid:33)
1 (λ+s2)2(λ+s2)2 (λ+s2)2 (λ+s2)2 (λ+s2)(λ+s2)
j=1l=1 j l j l j l
=λ4(cid:88)r (cid:88)r θ j⋆θ l⋆s2 js2
l
·(cid:32) s2
j −
s2
l
(cid:33)2
(λ+s2)2(λ+s2)2 (λ+s2) (λ+s2)
j=1l=1 j l j l
≥0 (†)
T
=λ2γ4(cid:88)r (cid:88)r s4 js4
l
·(cid:32) 1
+
1
−
2 (cid:33)
2 (λ+s2)2(λ+s2)2 (λ+s2)2 (λ+s2)2 (λ+s2)(λ+s2)
j=1l=1 j l j l j l
=λ2γ4(cid:88)r (cid:88)r s4 js4
l
·(cid:32) 1
−
1 (cid:33)2
(λ+s2)2(λ+s2)2 (λ+s2) (λ+s2)2
j=1l=1 j l j l
≥0 (††)
  
T 3 =λ2γ2
(cid:88)r (λθ +j⋆s s6
j
2)4(cid:88)r (λ+s4
j s2)2
j=1 j j=1 j
  
+λ4γ2
(cid:88)r θ j⋆s2
j
(cid:88)r s4
j

(λ+s2)2 (λ+s2)4
j=1 j j=1 j
  
+2λ3γ2
(cid:88)r θ j⋆s4
j
(cid:88)r s4
j

(λ+s2)3 (λ+s2)3
j=1 j j=1 j
     
≥λ2γ2
(cid:88)r θ j⋆s6
j
(cid:88)r s4
j +λ4γ2
(cid:88)r θ j⋆s2
j
(cid:88)r s4
j

(λ+s2)4 (λ+s2)2 (λ+s2)2 (λ+s2)4
j=1 j j=1 j j=1 j j=1 j
(Ignoring the 3rd term)
For the overall lower bound on the numerator of eq (38), we combine the above lower bound on T with eqs
3
(†), (††). Under Condition #1, this simplifies to
   
n2·(cid:0) Mc−m2(cid:1) ≥λ2γ2·
∥θ⋆∥2s6
1
(cid:88)r s4
j +λ4γ2·
∥θ⋆∥2s2
1
(cid:88)r s4
j

(λ+s2)4 (λ+s2)2 (λ+s2)2 (λ+s2)4
1 j=1 j 1 j=1 j
    
=γ2·∥θ⋆∥2s2
1 (λλ +2s s4
1
2)4
(cid:88)r (λ+s4
j s2)2+
(λ+λ4
s2)2
(cid:88)r (λ+s4
j
s2)4
(39)
 1 j=1 j 1 j=1 j 
And for the denominator in eq (38), we have
n·M
=λ2(cid:88)r θ j⋆s6
j
+λ2γ2(cid:88)r s4
j
(λ+s2)4 (λ+s2)4
j=1 j j=1 j
 
=λ2
(cid:88)r θ j⋆s6
j
+γ2(cid:88)r s4
j

(40)
(λ+s2)4 (λ+s2)4
j=1 j j=1 j
21Upper Bound ∝λ2 from eq (40):
 
≤λ2
(cid:88)r θ j⋆s6
j
+γ2(cid:88)r s4
j

s8 s8
j=1 j j=1 j
 
≤λ2
(cid:88)r θ j⋆s2
1
+γ2(cid:88)r 1

s4 s4
j=1 j j=1 j
≤
λ2
(cid:0) ∥θ⋆∥2s2+rγ2(cid:1) (41)
s4 1
r
Upper Bound ∝1/λ2 from eq (40):
 
≤λ2
(cid:88)r θ j⋆s6
j
+γ2(cid:88)r s4
j

λ4 λ4
j=1 j=1
 
r r
1 (cid:88) (cid:88)
= λ2  θ j⋆s6 j +γ2 s4 j
j=1 j=1
 
r r
1 (cid:88) (cid:88)
≤
λ2
s6
1
θ j⋆+γ2s4
1
1
j=1 j=1
≤
s4
1 (cid:0) ∥θ⋆∥2s2+rγ2(cid:1) (42)
λ2 1
Now we put together the numerator lower bound from eq (39), with the denominator upper bound from eq
(41) for the first term, and from eq (42) for the second term. We then get
    
n·
Mc−m2
≥γ2·
∥θ⋆∥2s2
1 
s4 1s4
r
(cid:88)r s4
j +
λ6/s4
1
(cid:88)r s4
j 
M (∥θ⋆∥2s2+rγ2) (λ+s2)4 (λ+s2)2 (λ+s2)2 (λ+s2)4
1 1 j=1 j 1 j=1 j
Using s j ≥s r and 1/(λ+s2 j)≥1/(λ+s2 1) for all j ∈[r], we get
    
≥γ2·
∥θ⋆∥2s2
1 
s4 1s8
r
(cid:88)r
1+
λ6·(s4 r/s4 1) (cid:88)r
1
(∥θ⋆∥2s2+rγ2) (λ+s2)6 (λ+s2)6
1 1 j=1 1 j=1
=rγ2·
∥θ⋆∥2s2
1
(cid:18) s4 1s8 r+(s4 r/s4 1)λ6(cid:19)
(43)
(∥θ⋆∥2s2+rγ2) (λ+s2)6
1 1
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
Q1 Q2
Under Condition #2, it holds that
Q ≥
99·s4 1/s4
r ≥
99
=0.99 (since s ≥s ) (44)
1 99·s4 1/s4 r+1 100 1 r
And now we analyze Q using simple calculus. Note that
2
t +t λ6 s4
Q = 1 2 where t =s4s8, t = r
2 (λ+s2)6 1 1 r 2 s4
1 1
Simple calculus shows that this function is minimized at λ¯ =(t1/(t2s2 1))0.2 =(cid:0) s6 1s4 r(cid:1)0.2 ≤s2
1
(since s
r
≤s 1).
And the min value of the function (evaluated at λ¯) is
t t 1
(cid:18)
s
(cid:19)8
Q ≥ 1 ≥ 1 = · r (45)
2 s2 1(cid:0) λ¯+s2 1(cid:1)5 s2 1·25s1 10 32 s 1
22Combining eqs (38), (43), (44), (45), we get
(cid:16) (cid:17) 0.99 (cid:18) s (cid:19)8 rγ2
∀λ>0,∀ξ ∈R ExcessRisk θˆ(λ,ξ) ≥ · r ·
25 s n
1
Using Condition #4 in the above gives the desired eq (10).
E.1 Discussion on Condition #1 in Theorem 6
One does not strictly need θ⋆ to be aligned with the leading eigenvector u . If instead we had the below
1
conditions (call them Conditions #1’ and #2’), then also the Theorem would hold.
1. θ⋆ =∥θ⋆∥·u
r
2. ∥θ⋆∥2 >99· rγ2 ·(cid:16) s2 1(cid:17)2 (i.e., the dependence has changed to (cid:16) s2 1(cid:17)2 from (cid:16) s2 1(cid:17) )
s2 s2 s2 s2
r r r r
Proof. This is because the Upper Bound for the r-step SD estimator still holds with the identical calculation
(with θ⋆,s instead of θ⋆,s in eq (30)). And the Lower Bound for the ridge estimator works similarly (with
r r 1 1
just the Variance term sufficing, i.e. eq (33)), but with a slightly different upper bound on λ⋆ than eq (36).
Namely, from eq (34), we will get
λ⋆ =
γ2 ·(cid:88)r (cid:18) s j(cid:19)4 ·(cid:32) λ⋆+s2 r(cid:33)3
(θ⋆ =0, j ≤r−1 and θ⋆ =∥θ⋆∥2 from Cond #1’)
∥θ⋆∥2 s λ⋆+s2 j r
j=1 r j
≤ ∥θγ ⋆2 ∥2 ·(cid:88)r (cid:18) ss j(cid:19)4 (Since λ λ⋆ ⋆+ +s s2 r 2 ≤1, as λ⋆ >0, s r ≤s j)
r j
j=1
γ2 (cid:18) s (cid:19)4
≤ ·r 1 (46)
∥θ⋆∥2 s
r
Using (46) instead of (36) completes the argument.
F Proof of Theorem 2
We point the reader to Appendix A for notations used throughout the proof.
Proof. Under the condition of ∀j ∈[r],s =1, we need to analyze the ExcessRisk for both k-step SD and
j
ridge. Denote Q:=(cid:80)r θ⋆ for simplicity.
j=1 j
k-step SD: Since assumption 2.1 is violated, we cannot use Theorem 4 for the k-step SD. Instead, we will
work with the quadratic expansion of ExcessRisk from Theorem 5. We will rewrite the expressions from
Appendix I for the quadratic coefs. Note that ρ =λ/s2 =λ for all j ∈[r] becomes independent of j in this
j j
case. And so C (i)=1− 1 for all j ∈[r],i∈[k] also becomes independent of j. Let C(i):=1− 1
j (1+λ)i (1+λ)i
denote the coefs C (i), since they’re now independent of j. Let ω :=[C(1),C(2),··· ,C(k)]∈Rk. Then, we
j
get
∀(i ,i )∈[k]×[k] M(k) =
1 (cid:88)r θ j⋆+γ2
·C(i )·C(i )
1 2 i1,i2 n (1+λ)2 1 2
j=1
1 Q+rγ2
= · ·C(i )·C(i )
n (1+λ)2 1 2
1 Q+rγ2
=⇒ M(k) = · ·ωω⊤ ∈Rk×k becomes a rank-1 matrix
n (1+λ)2
23Similarly, for m(k) we get
∀i ∈[k] m(k) =
1 (cid:88)r λθ j⋆−γ2
·C(i )
1 i1 n (1+λ)2 1
j=1
1 λQ−rγ2
=⇒ m(k) = · ·ω ∈Rk
n (1+λ)2
And similarly, for c we get
1 λ2Q+rγ2
c= · ∈R
n (1+λ)2
Using the above expressions, we rewrite the overall ExcessRisk for any k-step SD (k ≥1) as
(cid:16) (cid:17) 1 (cid:18) Q+rγ2 λQ−rγ2 λ2Q+rγ2(cid:19)
ExcessRisk θˆ(λ,ξ(k)) = ·⟨ω,ξ¯(k)⟩2+2· ·⟨ω,ξ¯(k)⟩+
n (1+λ)2 (1+λ)2 (1+λ)2
We’re aiming for a lower bound on the above quantity, so we can first minimize with respect to ξ(k), and
then analyze the remaining as a function of λ. Note that this is a quadratic in the scalar ⟨ω,ξ¯(k)⟩. Since
q(x):=ax2+2bx+c=c− b2 +a(cid:0) x+ b(cid:1)2 ≥c− b2, we have
a a a
(cid:16) (cid:17) 1 1
(cid:32) (cid:0) λQ−rγ2(cid:1)2(cid:33)
∀k ≥1,∀ξ(k) ∈Rk ExcessRisk θˆ(λ,ξ(k)) ≥ · λ2Q+rγ2−
n (1+λ)2 Q+rγ2
1 1
(cid:18) λ2Qrγ2+Qrγ2+2λQrγ2(cid:19)
= ·
n (1+λ)2 Q+rγ2
1 Qrγ2
= ·
n Q+rγ2
rγ2 1
= ·
n 1+ rγ2
Q
Note this expression is independent of λ. Hence the above lower bound holds ∀k ≥ 1,∀ξ(k) ∈ Rk,∀λ > 0.
This concludes the proof of eq (12).
Ridge: For ridge, we can simply borrow the expression of c from above for its ExcessRisk. That is
(cid:16) (cid:17) 1 λ2Q+rγ2
ExcessRisk θˆ(λ) = ·
n (1+λ)2
Let λ⋆ denote its minimizer over λ>0. Simple calculus gives λ⋆ = rγ2. Plugging this in, we get the same
Q
expression
(cid:16) (cid:17) rγ2 1
ExcessRisk θˆ(λ⋆) = ·
n 1+ rγ2
Q
This completes the proof of ridge achieving the lower bound in eq (12).
G Proof of Theorem 3
We point the reader to Appendix A for notations used throughout the proof.
Proof. Under the condition of ∀j ∈ [r],θ⋆ = z > 0, we need to analyze the ExcessRisk for both k-step SD
j
and ridge.
24k-step SD: Again from Theorem 4, any k-step SD estimator’s ExcessRisk is
∀k ≥1,∀λ>0,∀ξ(k) ∈Rk,
ExcessRisk(cid:16) θˆ(λ,ξ(k))(cid:17)
≥
γ2 (cid:88)r θ j⋆
(cid:16) (cid:17)
n θ⋆+ γ2
j=1 j s2
j
γ2 (cid:88)r 1
≥ (Since ∀j ∈[r],θ⋆ =z)
n (cid:16) 1+ γ2 · 1 (cid:17) j
j=1 z s2
j
This completes the proof of eq (13).
Ridge: Now for the ridge estimator, we will use Lemma I.1. From eq (53), we get an exact expression for
λ⋆ >0 that minimizes the ExcessRisk. Namely
γ2
λ⋆ =
z
Substituting this in eq (52), we get
ExcessRisk(cid:16) θˆ(λ⋆)(cid:17)
=
1 (cid:88)r (cid:0) (λ⋆)2θ j⋆+γ2s2 j(cid:1) ·s2
j
n (λ⋆+s2)2
j=1 j
=
γ2 (cid:88)r (cid:0) λ⋆+s2 j(cid:1) ·s2
j (Since λ⋆θ⋆ = γ2 z =γ2)
n (λ⋆+s2)2 j z
j=1 j
γ2 (cid:88)r 1
=
n (1+ λ⋆)
j=1 s2
j
=
γ2 (cid:88)r 1
(Substituting λ⋆ = γ2)
n (1+ γ2 · 1 ) z
j=1 z s2
j
This completes the proof of ridge achieving the lower bound in eq (13).
H Proof of Theorem 4
We point the reader to Appendix A for notations used throughout the proof.
Proof. The proof of (14) is a simple instantiation of Lemma 4.1. Since the SD estimator is a particular
instance of the general θˆ(P), i.e.
(cid:16) (cid:17) (cid:16) (cid:16) (cid:17) (cid:17)
θˆ λ,ξ(k) =θˆ P←P λ,ξ(k) ·Ω−1
λ
Eq (14) follows from the lower bound in Lemma 4.1.
For proving the equality being achieved, we will work with the general k-step SD estimator, and show that:
Under assumption 2.1, k = r steps are sufficient to provide enough freedom to ξ(k) so that the k-step SD
estimator achieves the lowest possible ExcessRisk.
Usinglemma4.1,notethattheconditionP(cid:0) λ,ξ(k)(cid:1) ·Ω−1 =P⋆ issufficienttoensurethatθˆ(cid:0) λ,ξ(k)(cid:1) =θˆ(P⋆),
λ
which would mean that the k-step SD estimator admits the lowest possible ExcessRisk. Since the eigenspaces
for both sides of the equation are the same (U ), we only need to ensure that the eigenvalues match on both
d
sides. That is, we need the following condition (for indices j ≥r+1, there’s no condition since lemma 4.1
tells us that any real value of s˜ suffices).
j
∀j ∈[r]
 1−(cid:88)k
ξ¯
i(k)  1−(cid:32) λ+s2
j
s2(cid:33)i 
·
λ+1
s2
=s˜⋆
j
(47)
i=1  j  j
25⇐⇒ ∀j ∈[r]
 1−(cid:88)k ξ¯(k)+(cid:88)k ξ¯(k)(cid:32) s2
j
(cid:33)i
·
s2
j =
θ j⋆
(48)
i i λ+s2 λ+s2 θ⋆+ γ2
i=1 i=1 j j j s2
j
Let a (λ):=
s2
j . Since λ>0, a ∈(0,1),∀j ∈[r]. The above condition can then be written succinctly in
j λ+s2 j
matrix form as j
A(k)(λ)·ξ¯(k) =α(λ) (49)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
∈Rr×k ∈Rk ∈Rr
With the following describing the elements of A(k)(λ),α(λ) as
[α(λ)] :=1−(cid:0) λ+s2(cid:1) s˜⋆ j ∈[r]
j j j
(cid:104) (cid:105)
A(k)(λ) :=1−(a (λ))i j ∈[r],i∈[k]
j
j,i
The notation A(k)(λ),α(λ) explicitly denotes the dependence on λ.
Now, A(k)(λ) being invertible would ensure that ∃ξ¯(k) ∈ Rk that makes equation (49) hold true (i.e. the
system of equations admits a solution). For that, we need (1) k =r and (2) A(r)(λ) being full-rank. The first
condition is stated in the lemma. In what follows, we will prove that ∃λ>0 such that the second condition
is satisfied, i.e. A(r)(λ) being full-rank. Decompose A(r)(λ) as
 1 1 ··· 1  a (λ) a (λ)2 ··· a (λ)r
1 1 1
1 1 ··· 1 a 2(λ) a 2(λ)2 ··· a 2(λ)r 
A(r)(λ)= . . . . . . ... . . .  −  . . . . . . ... . . .  
   
1 1 ··· 1 a (λ) a (λ)2 ··· a (λ)r
r×r r r r r×r
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
W V(λ)
WhereW =11⊤ isthematrixofallones,andV(λ)isakintothesquareVandermondematrix(onlydifference
being that the standard definition of Vandermonde also has a column of ones).
Using the Matrix Determinant Lemma, we can write
det(A(r)(λ))=(cid:0) 1−1⊤V(λ)−11(cid:1)
·det(−V(λ))
First note that, with the determinant expansion rule based on the first row
(cid:20) 1 0⊤ (cid:21)
detV(λ)=det
1 V(λ)
(r+1)×(r+1)
The matrix on the right is exactly the Vandermonde matrix with a =0,a (λ),a (λ),···a (λ). Using the
0 1 2 r
formula for the det of a standard (with a row of ones) Vandermonde matrix, we get
(cid:89) (cid:89) (cid:89)
detV(λ)= (a (λ)−a (λ))= a (λ)· (a (λ)−a (λ))
j i i j i
0≤i<j≤r 1≤i≤r 1≤i<j≤r
Since a (λ) ∈ (0,1) for all j ∈ [r], and a (λ)’s are all distinct, we conclude that detV(λ) ̸= 0, i.e. V(λ) is
j j
full-rank. What remains to show is that the scalar (cid:0) 1−1⊤V(λ)−11(cid:1) is non-zero for some λ>0. With the
SVD of V(λ)=CDE−1 where C,E are orthonormal and D is diagonal with positive entries, one can expand
this term as:
1
1⊤V(λ)−11=(E−11)⊤D−1(C−11) =⇒ |1⊤V−11|≥ ·|⟨C−11,E−11⟩|
d
max
Now observe that as λ→∞, a (λ)→0 for all j ∈[r], which means that V(λ)→0 , which means that (1)
j r×r
d →0+, and (2) C ↔E (since V(λ) becomes closer to a symmetric matrix, allowing its left and right
max
singular matrices to approach equality to each other). That is, 1 →∞ and ⟨C−11,E−11⟩→∥1∥2 =r.
dmax
This would ensure that |1⊤V(λ)−11| > 1, meaning that the scalar (1−1⊤V(λ)−11) would be non-zero.
Hence ∃λ>0, such that A(r)(λ) is full-rank.
26H.1 Proof of Lemma 4.1
Proof. The estimator θˆ(P) expands as
θˆ(P)=U S˜S2U⊤θ⋆+U S˜S V⊤η .
d d d d d d
Expanding the Excess Risk shows
d d
ExcessRisk(cid:16) θˆ(P)(cid:17) =(cid:88)(cid:16)(cid:0) s˜ ·s2−1(cid:1)2 ·θ⋆·w (cid:17) + γ2(cid:88)(cid:0) s˜2·s2·w (cid:1) ,
j j j j j j j
j=1 j=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Bias Variance
where w =
s2
j for all j ∈[d]. This is a simple quadratic expression in s˜. Completing the squares gives the
j n
desired optimal values of s˜⋆,j ∈[d].
j
I Quadratic ExcessRisk: detailed version of Theorem 5 and a proof
We point the reader to Appendix A for notations used throughout the proof.
Theorem 7 (Formal version of Theorem 5). The Excess Risk is quadratic in ξ¯(k) ∈Rk. Namely
(cid:16) (cid:17) (cid:16) (cid:17)⊤ (cid:16) (cid:17) (cid:16) (cid:17)⊤
ExcessRisk θˆ(λ,ξ(k)) = ξ¯(k) M(k) ξ¯(k) +2 ξ¯(k) m(k)+ c (50)
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
∈Rk×k ∈Rk
where the below holds, for indices (i ,i ) in [k]×[k],
1 2
M(k) =
λ(cid:88)r θ j⋆
·C (i )·C (i
)+γ2 (cid:88)r 1
·C (i )·C (i )
i1,i2 n ρ j(1+ρ j)2 j 1 j 2 n (1+ρ j)2 j 1 j 2
j=1 j=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
B(k) V(k)
i1,i2 i1,i2
r
(cid:16) λθ j⋆ +γ2(cid:17)
=
1 (cid:88) ρj
·C (i )·C (i ) ,
n (1+ρ )2 j 1 j 2
j
j=1
m(k) =
λ(cid:88)r θ j⋆
·C (i
)+γ2 (cid:88)r (−1)
·C (i )
i1 n (1+ρ j)2 j 1 n (1+ρ j)2 j 1
j=1 j=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
b(k) v(k)
i1 i1
=
1 (cid:88)r (cid:0) λθ j⋆−γ2(cid:1)
·C (i ) ,
n (1+ρ )2 j 1
j
j=1
c=
λ(cid:88)r θ j⋆ρ j +γ2 (cid:88)r 1
n (1+ρ )2 n (1+ρ )2
j j
j=1 j=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
FromBias FromVariance
=
1 (cid:88)r (cid:0) λθ j⋆ρ j +γ2(cid:1)
.
n (1+ρ )2
j
j=1
The B,b and V,v notation is used to indicate the (squared) Bias and Variance terms respectively. Here
ρ ,j ∈[r] is used to simplify the notation, and is defined as ρ := λ,j ∈[r].
j j s2
j
And the coefs C , j ∈[r] with i∈[k] have the form
j
1
C (i):=1− j ∈[r],i∈[k] . (51)
j (1+ρ )i
j
27Proof. We first use the expansion from Lemma D.1. Secondly, expand θ⋆ =(cid:80)d ⟨θ⋆,u ⟩u . Using these, we
j=1 j j
can expand Excess Risk as
(cid:16) (cid:17) (cid:20)(cid:13) (cid:13)2 (cid:21)
ExcessRisk θˆ(λ,ξ(k)) =E (cid:13)θˆ(λ,ξ(k))−θ⋆(cid:13)
η (cid:13) (cid:13)
Σˆ
n
=(cid:88)d s n2
j ·θ
j⋆·(cid:32) λ+s2
j
s2(cid:33)2 ·

sλ
2
+(cid:88)k
ξ¯
i(k)·  1−(cid:32) λ+s2
j
s2(cid:33)i  2
j=1 (cid:124)(cid:123)(cid:122)(cid:125) j j i=1  j 
Σˆ n weighing
+(cid:88)d s2
j ·γ2·
s2
j
· −1+(cid:88)k ξ¯(k)·  1−(cid:32) s2
j
(cid:33)i  2
n (λ+s2)2 i λ+s2
j=1 (cid:124)(cid:123)(cid:122)(cid:125) j i=1  j 
Σˆ n weighing
Writing down the above expansion and collecting the corresponding quadratic, linear and constant terms’
coefficients in the ξ¯(k), give the desired expressions.
I.1 ExcessRisk for ridge
Since we will compare the ridge estimator’s ExcessRisk to the k-step SD estimator, we state the ExcessRisk
expression for the ridge estimator (eq (3)) formally here.
Lemma I.1. The ridge estimator θˆ(λ) satisfies
∀λ>0,
ExcessRisk(cid:16) θˆ(λ)(cid:17)
=
1 (cid:88)r (cid:0) λ2θ j⋆+γ2s2 j(cid:1) ·s2
j . (52)
n (λ+s2)2
j=1 j
And the optimal penalty λ⋆ >0 that minimizes this ExcessRisk satisfies
(cid:80)r s4
j
λ⋆ =γ2·
j=1 (λ⋆+s2 j)3
. (53)
(cid:80)r θ j⋆s4
j
j=1 (λ⋆+s2)3
j
Proof. Eq (52) is a simple instantiation of Theorem 5 in the vacuous case of k =0. Specifically, the quantity
c captures exactly what we need. Borrowing its expression from the detailed theorem statement (Appendix I)
gives us eq (52). Now, taking the derivative of the expression in eq (52) and setting it to zero, we get the
stated expression for λ⋆ >0.
J Discussion on synthetic experiments
This section follows up Section 5.1 with more details and examples about the synthetic problem. Figure 8
shows four more settings, with γ ∈{0.125,0.25} and θ⋆ ∈{u 1,1/√ 2(u 1+u 2)}.
Figure 8a validates that repeated steps of SD do provide a reduction in the excess risk, since the lowest point
of the curve for each k is reducing as k increases. Observe that at k = r = 4 steps of SD, the curves in
Figure 8 become flat. This is because we stated Theorem 4 with a "∃λ>0" such that r-step SD can achieve
the lower bound, but in practice it can happen "∀λ>0".
K Discussion on choosing ξ parameters
For the k-step SD estimator (eq (8)), for any chosen λ, Theorem 5 tells us that the ExcessRisk is quadratic
in ξ¯(k) ∈Rk. To estimate the coefficients of this quadratic from certain chosen evaluations of ξ(k), we need
a total of k(k+3)/2+1 evaluations. This is because the number of unknown coefficients are (i) k for square
terms, (ii) k(k−1)/2 for cross-square terms, (iii) k for linear terms, and (iv) 1 for the constant term. We now
illustrate this in detail for k =2.
28(a) γ =0.125,θ⋆ =u (b) γ =0.25,θ⋆ =u
1 1
(c) γ =0.125,θ⋆ =1/√ 2(u 1+u 2) (d) γ =0.25,θ⋆ =1/√ 2(u 1+u 2)
Figure 8: Plot of excess risk of k-step SD with optimal ξ(k) for each λ, for k ∈{0,1,2,3,4} on a synthetic
problem (Section 5.1).
K.1 Illustration for k = 2
ξ ξ
1 2
S S S
0 1 2
Figure 9: Illustrating 2-step SD.
As explained above, the ExcessRisk for a chosen λ has the following form
ExcessRisk(cid:0) θˆ(λ, ξ )(cid:1) =Aξ¯2+Bξ¯2+2Cξ¯ξ¯ +2Dξ¯ +2Eξ¯ +F
1 2 1 2 1 2
(cid:124)(cid:123)(cid:122)(cid:125)
∈R2
where the reparametrization is ξ¯ ← ξ (1−ξ ), and ξ¯ ← ξ ξ (refer to Appendix C.3 for details on this
1 2 1 2 2 1
reparametrization). Let Eval(ξ ,ξ ) denote the result of measuring this estimator’s Risk on the validation
2 1
dataset. We get the below system of equations:
29F =Eval(ξ =0,ξ =0)
2 1
A+2D+F =Eval(ξ =1,ξ =0)
2 1
A−2D+F =Eval(ξ =−1,ξ =0)
2 1
B+2E+F =Eval(ξ =1,ξ =1)
2 1
B−2E+F =Eval(ξ =−1,ξ =1)
2 1
A+B C
+ +D+E+F =Eval(ξ =1,ξ =0.5)
4 2 2 1
The above 6 Eval operations help us identify all the 6 unknown coefficients. Given AB−C2 >0 ([14]), we
will have the below ξ¯⋆,ξ¯⋆ minimizing the test loss (i.e. giving the optimal 2-step SD coefs for the chosen λ).
1 2
And using them, we calculate the actual ξ⋆,ξ⋆ by doing the inverse mapping of the reparametrization.
1 2
CE−DB CD−AE
ξ¯⋆ = ,ξ¯⋆ =
1 AB−C2 2 AB−C2
ξ¯⋆
ξ⋆ = 2 ,ξ⋆ =ξ¯⋆+ξ¯⋆
1 (ξ¯⋆+ξ¯⋆) 2 1 2
1 2
L Details on regression experiments
We note that all experiments run on a single CPU within 60 seconds (wall-clock time). We utilize sklearn’s
implementation of the Ridge.
Methodology. We now explain our methodology in detail, which is followed for all datasets:
1. First, split the original dataset into three parts for a Train-Validation-Test split. We divide all datasets
in a 30−30−40 split. Note that 30% of the data suffices for training since we work with small datasets
which have d on the order of ten (and total number of samples n on the order of thousands). For
datasets that have a temporal notion (e.g. date/timestamps), we do the three-way split sequentially.
2. Perform two standard transformations on all three splits of the data: (i) Remove records with missing
values, and (ii) coordinate-wise whitening for all X features and the Y target, i.e., subtracting the mean
(computed on the train set) and dividing by the standard deviation (also computed on the train set).
3. Select a grid of λ values (and ensure that it is large enough so that the optimal λ lies in it). The grid
√ √
has a factor of 10 difference between consecutive values (e.g., {1, 10,10,··· ,104}). Then, for all λ
in the grid, compute:
• The ridge estimator θˆ(λ) using the train set.
• The 1-step SD estimator θˆ(λ,ξ⋆) using the train set, where ξ⋆ for each λ is found using the
validation set by the strategy described in Section 5.2.
• The 2-step SD estimator θˆ(λ,(ξ(2))⋆) using the train set, where (ξ(2))⋆ for each λ is found using
the validation set by the strategy described in Appendix K.1.
Let λ⋆ denote the optimal penalty for ridge that minimizes the MSE on the validation set. Similarly,
0
let (λ⋆,ξ⋆) and (cid:0) λ⋆,(ξ⋆,ξ⋆)(cid:1) denote the optimal parameters for 1-step, 2-step SD, again chosen via the
1 2 1 2
validation set.
4. Evaluate the MSE on the test set (unseen as yet) for all three computed estimators: θˆ(λ⋆), θˆ(λ⋆,ξ⋆),
0 1
θˆ(cid:0) λ⋆,(ξ⋆,ξ⋆)(cid:1).
2 1 2
L.1 Description of the datasets used
UCIAirQuality. TheUCIAirQualitydataset[33]containsrecordsofhourlyaveragedreadingsfrom5metal
oxide chemical sensors (which serve as covariates), along with ground-truth hourly averaged concentrations of
30the pollutants from a co-located reference certified analyzer. The recordings are from an Italian city over a
one year period in 2004-2005. After removing records with missing entries, this dataset has n=6941 rows
and we consider d=8 relevant covariates (5 values of the metal oxide chemical sensor readings + 3 values
of Temperature, Relative Humidity, and Absolute Humidity). We explicitly mention the field names (all
real-numbered non-negative).
• X covariates’namesinthedataset: [PT08.S1(CO), PT08.S2(NMHC), PT08.S3(NOx), PT08.S4(NO2),
PT08.S5(O3), T, RH, AH]
• Y target’s name in the dataset: NO2(GT)
UCI Airfoil Self-Noise. The UCI Airfoil Self-Noise dataset [4] contains data obtained from NASA’s
aerodynamicandacoustictestsofairfoilbladesections. Itcontains5real-valuedcovariates,whicharerelevant
physical quantities. The target is also real-valued, which is the sound pressure created (in dB). There are
no missing entries. This dataset has n = 1503 rows and d = 5 covariates. We explicitly mention the field
names:
• Xcovariates’namesinthedataset: [frequency, attack-angle, chord-length, free-stream-velocity,
suction-side-displacement-thickness]
• Y target’s name in the dataset: scaled-sound-pressure
UCI Appliances Energy Prediction. The UCI AEP dataset [5] contains energy appliances’ data collected
at 10 min frequency for about 4.5 months. This dataset has no missing entries, and a total of 19735 instances
with 28 covariates. We downsample the dataset to hourly frequency, giving n = 3290 rows with d = 24
relevant covariates (removing degenerate covariates: ‘date’, ‘lights’, ‘windspeed’, ‘visiblity’). The full list of
covariates is 24 long, and so we do not list it out here (we already mentioned the 4 we removed from the
total of 28).
L.2 Observed quadratic nature of MSE w.r.t. ξ parameters
Theorem 5 showed that the excess risk is quadratic w.r.t. ξ¯(k) variables (the reparameterized version). We
used this theoretical insight to devise a hyperparameter tuning strategy in section 5.2. The below curves
ratify this empirically. Inspecting the minima of each of the below curves, we observe that the values of ξ⋆ for
1-step SD in Table 1 (found through the strategy described in section 5.2) indeed coincide with the minima
producing values in the below curves.
(a) Air Quality dataset (b) Airfoil dataset (c) AEP dataset
Figure 10: Observed quadratic nature of MSE (on validation set) of 1-step SD vs ξ for λ=λ⋆. This agrees
1
with Theorem 5 and validates the hyperparameter tuning strategy outlined in section 5.2.
31