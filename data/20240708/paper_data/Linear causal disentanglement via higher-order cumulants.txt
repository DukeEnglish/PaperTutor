Linear causal disentanglement
via higher-order cumulants
Paula Leyes Carreno1, Chiara Meroni 2, and Anna Seigal3
pleyescarreno@college.harvard.edu, chiara.meroni@eth-its.ethz.ch, aseigal@seas.harvard.edu
1,3Harvard University, Cambridge, MA, United States
2ETH-ITS, Zurich, Switzerland
Abstract
Linear causal disentanglement is a recent method in causal representation learning to
describe a collection of observed variables via latent variables with causal dependencies
between them. It can be viewed as a generalization of both independent component
analysis and linear structural equation models. We study the identifiability of linear
causal disentanglement, assuming access to data under multiple contexts, each given by
an intervention on a latent variable. We show that one perfect intervention on each latent
variable is sufficient and in the worst case necessary to recover parameters under perfect
interventions, generalizingpreviousworktoallowmorelatentthanobservedvariables. We
give a constructive proof that computes parameters via a coupled tensor decomposition.
For soft interventions, we find the equivalence class of latent graphs and parameters that
are consistent with observed data, via the study of a system of polynomial equations.
Our results hold assuming the existence of non-zero higher-order cumulants, which implies
non-Gaussianity of variables.
Keywords. Causal inference, disentanglement, higher-order cumulants, tensor decomposi-
tion, causal representation learning, interventions.
MSC classes. 13P15, 15A69, 62H22, 62R01, 68Q32.
Acknowledgements. We thank Kexin Wang for helpful discussions. CM was supported by
Dr. Max Rössler, the Walter Haefner Foundation and the ETH Zürich Foundation. AS was
partially supported by the NSF (DMR-2011754).
1 Introduction
A key challenge of data science is to find useful and interpretable ways to model complex data,
such as those collected from a biological experiment or a physical system. In this paper, we
study linear causal disentanglement (LCD), a framework to model such data. LCD generalizes
two 20th century data analysis models: independent component analysis (ICA) [Jut87, Com94,
CJ10] and linear structural equation models (LSEMs) [Bol89, Sul18]. Before defining it, we
briefly recall these older models.
1
4202
luJ
5
]LM.tats[
1v50640.7042:viXraICA is a blind source separation method that expresses observed variables X = (X ,...,X )
1 p
as a linear mixture
X = Aε, (1)
where A ∈ Rp×q is a mixing matrix and ε = (ε ,...,ε ) is a vector of independent latent
1 q
variables. ICA has been used in applications including brain dynamics [JMM+01] and astro-
physics [BJD+07]. LSEMs are another linear model to describe collections of variables. They
model variables Z = (Z ,...,Z ) as
1 q
Z = ΛZ +ε, (2)
where Λ ∈ Rq×q is a matrix whose entry λ encodes the dependence of Z on Z and ε is a
i,j i j
vector of noise variables, often assumed to be independent. The variables are typically assumed
to relate via the recursive structure of a directed acyclic graph (DAG); that is, fixing a DAG G
on nodes [q] = {1,...,q}, with directed edges denoted j → i, we have
λ ̸= 0 ⇐⇒ (j → i) ∈ G.
i,j
Equation (2) can be re-written as Z = (I − Λ)−1ε, where acyclicity of G ensures that the
matrix I−Λ is invertible. This places LSEMs in the context of ICA, since the variables Z are a
linear mixing of independent latent variables [Shi14]. LSEMs appear in applications including
epidemiology [SBJRH05] and causal inference [Pea00]. In causal inference, the quantity λ is
i,j
interpreted as the causal effect of Z on Z .
j i
The idea of linear causal disentanglement [SSBU23] is that the assumptions of ICA and LSEMs
may be too strict: interpretable latent variables may not be independent, and variables that
relate via a graph may not have been directly measured. To get around this, LCD is defined as
follows. As in ICA, we observe variables X = (X ,...,X ) that are a linear mixing of latent
1 p
variables. However, unlike ICA, the latent variables are not independent, instead they follow
the structure of an LSEM; that is,
X = FZ, where Z = ΛZ +ε, (3)
for F ∈ Rp×q a linear transformation, Λ a matrix that encodes causal dependencies among the
latent variables Z = (Z ,...,Z ), and ε = (ε ,...,ε ) a vector of independent noise variables.
1 q 1 q
As often the case in ICA and LSEMs, variables ε are assumed to be mean-centered. LCD
specializes to ICA when Λ is the zero matrix (i.e. when G is the empty graph) and to an LSEM
when F = I.
LCD falls into the setting of causal representation learning [SLB+21], an area of machine
learning that aims to describe and explain the structure of a complex system by learning
variables together with the causal dependencies among them. The idea is that learned latent
representationsofdata[BCV14]canbedifficulttointerpretandanalyze,andmaynotgeneralize
well, but that they improve by using latent representations with causal structure [YLC+21].
Central to interpretability and downstream analysis is the identifiability of a representation.
The LCD model (3) is identifiable if the mixing matrix F and matrix of dependencies Λ, and
therefore also the latent DAG G, can be recovered uniquely (or up to a well-described set of
possibilities) from observations of X.
In this paper, we study the identifiability of LCD and develop algorithms to recover the param-
eters F and Λ using tensor decomposition of higher-order cumulants. Higher-order cumulants
2havebeenusedtorecoverparametersinbothICAandLSEMs[Shi14,WD20,Com94,DLCC07,
WS24]. We build on these insights to use it for LCD. For ICA and LSEMs, parameters can
be recovered from tensor decomposition of a single higher-order cumulant. For LCD one ten-
sor decomposition no longer suffices to recover parameters and we will instead use a coupled
tensor decomposition. Identifiability of LCD from covariance matrices (that is, second-order
cumulants) was studied in [SSBU23]. Our results extend these insights to identifiability via
higher-order cumulants.
The setup. Our goal in this paper is to use observations of X to recover the parameters F
and Λ in an LCD model (3). We assume access to observations of X under multiple contexts.
The contexts differ from an observational context by an intervention. Interventions appear in
biological applications such as [SPP+05, DPL+16, MHM+16, TLHD+17, XH24]. An interven-
tion at a variable affects the downstream variables but not those that are upstream. It thus
enables one to learn the direction of a causal dependency between two variables. We study
multiple contexts for two reasons: inferring causal dependencies in general necessitates inter-
ventions and one context is insufficient for recovery of parameters in the model. We consider
two types of interventions.
Definition 1.1. Let variables Z relate via a linear structural equation model. A soft (resp.
i
perfect) intervention at Z changes (resp. zeros out) all non-zero weights λ and changes the
i i,j
error distribution ε .
i
A third widely-studied type of intervention is a do-intervention, which sets a variable to a
deterministic value. We focus on soft interventions and perfect interventions, so that we do not
assumeaccesstoafixedvalueofanunobservedvariable. Forrelatedresultsfordo-interventions,
see [YLC+21, AMWB23].
We denote the set of contexts by K. Each context k ∈ K is assumed to be an intervention at
a single latent variable, as in [SSBU23]. The target of each intervention is unknown: context
k is an intervention on Z for some i ∈ [q]. The observational setting, in which no variable
i k
k
is intervened on, is indexed by k = 0 and assumed to be known. The intervention changes the
latentLSEMbutnotthemixingmapF. Undercontextk, wedenotethematrixofcausaleffects
by Λ(k), the latent variables by Z(k), and the error distributions by ε(k). Error distributions ε(k)
and ε(0) agree, except at the i -th entry. From Definition 1.1, we see that a perfect intervention
k
setsthei -throwofΛ(k) tozerowhileasoftinterventionsatisfiesλ(k) ̸= λ(0) wheneverλ(0) ̸= 0,
k i ,j i ,j i ,j
i.e. for all j with edge j → i present in G. Our setup can now beksummakrized as followsk.
k
Fix p ≥ 2 observed variables. We observe distributions X(k) on Rp for k ∈ K ∪ {0} of the
form
X(k) = FZ(k), where Z(k) = Λ(k)Z(k) +ε(k), (4)
for Z(k) some distributions on Rq where q ≥ 2 is the number of latent variables. The variables
Z(0) on Rq follow a linear structural equation model on an unknown DAG G on q nodes, and
Z(k) relates to Z(0) via a single-node perfect or soft intervention with unknown target. See
Figure 1 for a cartoon of our setup. We make the following genericity assumptions.
Assumption 1.2.
(a) All noise variables ε(k) are non-Gaussian.
i
3(b) Matrix F ∈ Rp×q is unknown and generic; matrices Λ(k) ∈ Rq×q, k ∈ K∪{0} are unknown
with generic non-zero entries.
(c) For all contexts k ∈ K there exists a large enough d (d ≥ 3(q−1) is sufficient) such that
the d-th order cumulant of ε(i k) is not 0 or ±1.
i
k
λ
1,3
latent variables: Z Z Z
3 λ 2 1
2,3
f
1,2
mixing map: f 1,3 f 2,2 f
2,1
f
2,3 f
1,1
observed variables: X X
2 1
Figure 1: A cartoon of the setup for p = 2 observed variables and q = 3 latent
variables.
Problem 1.3. In the setup (4) under Assumption 1.2, learn the number of latent variables q,
the latent DAG G, the mixing matrix F and the matrices of dependencies {Λ(k)|k ∈ K∪{0}}.
We can rearrange (4) to write variables X(k) as a linear mixture of independent latent vari-
ables
X(k) = F(I −Λ(k))−1ε(k).
This relates LCD to ICA. Just as for ICA, we have the following non-identifiability.
Remark 1.4 (Benign non-identifiability). Uniqueness of F and Λ(k) is impossible in LCD,
since one can rescale or reorder the latent variables without affecting membership in the model.
That is, for a diagonal matrix D ∈ Rq×q and a permutation matrix P ∈ Rq×q, setting
F(cid:101) = FM, Λ(cid:101)(k) = M−1Λ(k)M, ε(k) = M−1ε(k), where M = DP, (5)
(cid:101)
we have
F(cid:101)(I −Λ(cid:101)(k))−1ε(k) = F(I −Λ(k))−1ε(k).
(cid:101)
Hence such rescaling and reordering does not affect X(k). Such transformations do not change
the latent graph G except by a relabelling of its nodes under the permutation P. When we
discuss identifiability, we mean uniqueness up to the benign rescaling and reordering transfor-
mations in (5). Given multiple contexts k ∈ K ∪{0}, the scaling and ordering transformations
D and P are the same for all k.
Main results. We find the perfect interventions needed for identifiability of LCD.
Theorem 1.5. Consider LCD under Assumption 1.2 with perfect interventions. Then one
perfect intervention on each latent node is sufficient and, in the worst case, necessary to recover
the latent DAG G and the parameters F and Λ(k) from observations of X(k).
4For p observed variables and q latent variables, Theorem 1.5 says that we need q interventions
for identifiability of LCD. We do not impose the injectivity of the mixing map F : Rq → Rp;
the pair (p,q) can take any values provided p,q ≥ 2. Our proof is constructive: we carry out a
coupled tensor decomposition of higher-order cumulants of the distributions X(k), and compare
the factors recovered to learn the parameters. This extends [Shi14, WD20] from observed to
latent causal variables, and extends [Com94, DLCC07, WS24] from independent to dependent
latent variables. It relates to [EGS05], which says that q − 1 interventions are sufficient and
in the worst case necessary to recover a DAG on q observed variables. It builds on [SSBU23,
Theorem 1], which says that one intervention on each latent node is sufficient and in the worst
case necessary when the mixing F is injective. When the mixing map is injective, Theorem 1.5
is weaker than [SSBU23, Theorem 1], since it requires non-Gaussian errors. When F is not
injective non-Gaussianity is necessary for identifiability, see Proposition 3.6.
We present two algorithms for the recovery of the model parameters using q perfect inter-
ventions. The first algorithm can be used for any (p,q). It takes as input a tuple of q + 1
cumulants, and returns the parameters F and Λ(k). The second algorithm applies to the set-
ting q ≤ p. Here Moore-Penrose pseudo-inverses can be used to simplify the recovery. We
illustrate the performance of the algorithms in Figures 2 and ??. Both are implemented
in Python, version 3.12.2. The code is available at https://github.com/paulaleyes14/
linear-causal-disentanglement-via-cumulants.
We now turn to soft interventions. The transitive closure G of a DAG G is the DAG with all
edges j → i whenever j → ··· → i is a path in G. We can recover the transitive closure G of a
latent DAG G in LCD from the second-order cumulants, see [SSBU23, Theorem 1]. We show
that, if the errors are non-Gaussian, we can distinguish certain DAGs with the same transitive
closure. We define the set of soft-compatible DAGs soft(G). It is a set of DAGs with the same
transitive closure, which also satisfy additional compatibility conditions coming from ranks of
matrices. Define the set of children of node j by ch (j) = {i|(j → i) ∈ G} and the descendants
G
by de (j) = {i|(j → ··· → i) ∈ G}. Then,
G
(cid:110) (cid:12) (cid:111)
soft(G) = G′ (cid:12) G′ = G and rank[(I−Λ )−1] = rank[(I−Λ )−1] for all j ∈ [q] ,
(cid:12) G rj,cj G rj,cj∪{j}
where r := de (j)\ch (j), c := ch (j), and Λ is a generic matrix of dependencies in an
j G′ G′ j G′ G
LSEM on DAG G, and [M] denotes the submatrix of M with row indices in r and column
r,c
indices in c. See Definition 3.14 for more details.
Theorem 1.6. Consider LCD under Assumption 1.2 with soft interventions. Then one soft
intervention on each latent node is sufficient and, in the worst case, necessary to recover the
set of DAGs soft(G). Given G′ ∈ soft(G), the set of parameters F and Λ(k) that are compatible
with the observations is a positive dimensional linear space.
Theproofreliesonthestudyofthesolutionspacetoasystemofpolynomialequations, encoding
the conditions that parameters compatible with the observations must satisfy. That space is
linear and always positive dimensional, even if we allow multiple interventions on each latent
node. This leads to a negative identifiability result, in the same spirit as [LBL+19].
Corollary 1.7. Consider LCD under Assumption 1.2. With any number of soft interventions,
identifiability of all parameters in the model does not hold.
5The non-Gaussianity assumption is required for the linear space of parameters in Theorem 1.6:
with Gaussian errors, the space of parameters may be non-linear, see Proposition 3.3.
Related work. Higher-order cumulants have been shown to lead to improved identifiability
in related contexts. They extend principal component analysis, which requires an orthog-
onal transformation for identifiability, to ICA, which is identifiable for general linear mix-
ings [CJ10, Com94]. For LSEMs, they facilitate the recovery of a full DAG, rather than its
Markov equivalence class [VP22], see [Shi14, WD20]. They have been used to recover parame-
ters in other latent variable models [AGH+14].
Identifiabilityofcausalrepresentationlearningisanactiveareaofstudy. Itbuildsonworkinthe
identifiability of representation learning [AHB21, ZSS+22, KKMH20] and latent DAG models.
These include work that imposes sparsity on the causal relations [MSWB22, AHB21, HP99,
AHZ21, SSGS06, CXG+19, XCH+20, XHC+22, LZG+23, JA23, ZSG+23, LLS+24, ZXNZ24]
and latent variable models on discrete variables [HHS15, KRRA21]. There are many works
related to LCD, due in part to the many possible assumptions that one can make in a causal
disentanglement model. These include the structure (polynomial, non-linear) of the maps in-
volved [vKBW+23, BRR+23, VAS+23, VAST24, LZG+24a, LZG+24b, VAST23] and the choice
of data generating process [LMMLJ23, BWNR23, KSB24, SMA+24]. In general, allowing more
freedom on one side, implies more restrictions on the other side.
Outline. We cast LCD as the problem of aligning the outputs of a coupled tensor decompo-
sition in Section 2. We discuss the recovery of parameters for perfect and soft interventions in
Section 3. We prove our main results Theorem 1.5 in Section 3.2 and Theorem 1.6 in Section
3.3. We discuss our algorithms in Section 4 and future directions in Section 5. Appendix A
contains pseudo-code for our algorithms.
2 Coupled tensor decomposition
Thecumulantsareasequenceoftensorsthatencodeadistribution[McC18]. Thed-thcumulant
of a distribution X on Rp is an order d tensor, denoted by κ (X), of format p×···×p. The first
d
and second order cumulants are the mean and covariance, respectively. Higher-order cumulants
are those of order three and above.
We describe the higher-order cumulant tensors of distributions X(k) coming from LCD, as
in (3), as k ranges over contexts. We study a coupled decomposition of these tensors. This will
enable us to study the identifiability of LCD and to design tensor decompositions to recover
parameters in the model. We first consider a single context.
2.1 Decomposing cumulants
Let X be a distribution on Rp and assume X = Aε, where ε = (ε ,...,ε ) is a vector of
1 q
independent variables on Rq and A ∈ Rp×q is a linear map, as in ICA (1). Then the d-th
cumulant of X is the order d tensor
q
(cid:88)
κ (X) = κ (ε )a⊗d, (6)
d d i i
i=1
6where the scalar κ (ε ) is the d-th cumulant of variable ε and a is the i-th column of matrix
d i i i
A, as follows. The cumulants κ (ε) are order d tensors of format q×···×q. Since the variables
d
ε are independent, by assumption, their cross-cumulants vanish [McC18, Section 2.1]. Hence
i
the tensor κ (ε) is diagonal: its entries vanish away from the κ (ε ),...,κ (ε ) on the main
d d 1 d q
diagonal. A linear transformation of variables results in a multi-linear transformation of their
cumulants. This gives the expression in (6), which writes the cumulant as a sum of symmetric
rank one tensors.
If q ≤ p then κ (X) has a unique rank q decomposition, whenever cumulants κ (ε ) are all
d d i
non-zero and the columns of A are linearly independent, by [Har70]. Hence the vectors a can
i
be recovered uniquely, up to permutation and scaling. This extends to q > p, as follows.
Proposition 2.1. Assume that no pair of columns of A ∈ Rp×q are collinear and that the q
entries of ε are independent. Then, for d sufficiently large, all columns a with κ (ε ) ̸= 0 can
i d i
be uniquely recovered, up to permutation and scaling, from the d-th cumulant of X = Aε.
Proof. Form ≥ q−1,thetensorsa⊗m,...,a⊗m arelinearlyindependent,by[Lan11,Proposition
1 q
4.3.7.6], since no pair of columns a are collinear. Let d ≥ 3m ≥ 3(q − 1) and consider
i
κ (X) = (cid:80)q λ a⊗d, where λ := κ (ε ). Consider its flattening of size qm ×qm ×qd−2m. The
d i=1 i i i d i
decomposition of this flattened tensor is unique, by [Har70], since the vectors that appear in
it are linearly independent. Hence the tensors a⊗m and a⊗(d−2m), and thus also the vectors a ,
i i i
can be uniquely recovered, up to permutation and scaling, for all indices i with λ ̸= 0.
i
For a sufficiently generic matrix A, one can recover the vectors uniquely, up to permutation and
scaling, from the above tensor decomposition provided q is strictly less than the generic rank
of an order d tensor of format p×···×p, by [COV17]. The generic rank is usually ⌈1(cid:0)p+d−1(cid:1) ⌉,
p d
see [Lan11, Theorem 3.2.2.4]. Since for fixed p and large d, 1(cid:0)p+d−1(cid:1) ∼ dp−1, this result allows
p d
for larger q relative to d than the condition d ≥ 3(q −1) coming from Proposition 2.1.
Corollary 2.2. Assume that the entries of ε are independent and non-Gaussian and that no
pair of columns of A are collinear. Then tensor decomposition of the cumulants of X recovers
the matrix A, up to permutation and scaling of its columns.
Proof. The cumulant sequence (κ (ε )) has infinitely many non-zero terms, since ε is non-
d i d i
Gaussian [Mar39]. Hence there are non-zero cumulants at high enough d to satisfy the hy-
potheses of Proposition 2.1. This is an alternative proof of [EK04, Theorems 1(i) and 3(i)].
The impossibility of recovering the columns without scaling ambiguity comes from the fact
that we can extract or insert a global scalar from the factor a⊗d. We have (λa)⊗d = λda⊗d,
i
hence

 (cid:80)q
(cid:16)
(cid:112) d κ (ε )a
(cid:17)⊗d
d odd
κ (X) = i=1 d i i (7)
d  (cid:80)q i=1sign(κ d(ε i))(cid:16) ±(cid:112) d |κ d(ε i)|a i(cid:17)⊗d d even.
(cid:112)
TensordecompositionwillthereforerecoverthecolumnsofAuptothefactors± d |κ (ε )|.
d i
Consider the LCD setting of (3). We have X = FZ = F(I−Λ)−1ε. The discussion above shows
that the product F(I −Λ)−1 ∈ Rp×q can be recovered (up to permutation and scaling), since
7the entries of the random vector ε are independent. However, it is not possible to recover the
latent DAG G from the product F(I−Λ)−1: a solution with empty DAG (that is, independent
Z variables) is always consistent with the observations, since
F(I −Λ)−1ε = F(cid:101)Z(cid:101),
where F(cid:101) = F(I − Λ)−1 and Z(cid:101) = ε. This demonstrates the need for observations of X under
multiple contexts.
2.2 Coupling contexts
Distributions X(k) are linear mixtures of independent variables, since X(k) = F(I−Λ(k))−1ε(k),
where the entries of ε(k) are independent. Our goal is to recover the parameters F and Λ(k) for
all k ∈ K ∪{0}. We first prove the following.
Proposition 2.3. Consider LCD under Assumption 1.2. Then we can recover q and the
matrices F(I −Λ(k))−1 up to scaling and permutation for all k ∈ K ∪{0}; i.e., we can recover
F(I −Λ(k))−1D(k)P(k) ∈ Rp×q, (8)
where D(k) ∈ Rq×q is diagonal, with non-zero diagonal entries, and P(k) ∈ Rq×q is a permutation
matrix. The diagonal matrix D(k) can be assumed to have entries
 (cid:113)
 di κ (ε(k)) d odd,
D(k) = (cid:113) di i i (9)
i,i
± di |κ (ε(k))| d even,
di i i
where, for all i ∈ [q], d is large enough (d ≥ 3(q −1) suffices) and satisfies κ (ε(k)) ̸= 0.
i i di i
Proof. We have X(k) = A(k)ε(k), where A(k) = F(I−Λ(k))−1 and k ranges over contexts K∪{0}.
Wefirstprovetheresultunderanadditionalassumption, thatthereexistsasinglenumberd ≥ 3
that satisfies:
(a) the tensor decomposition
q
(cid:88)
κ (X(k)) = κ (ε(k))(a(k))⊗d (10)
d d i i
i=1
is unique for all contexts k, where (a(k)) is the i-th column of A(k) = F(I −Λ(k))−1,
i
(b) κ (ε(k)) ̸= 0 for all contexts k and all i ∈ [q],
d i
(c) κ (ε(i k)) ̸= ±1 for all contexts k.
d i
k
Fix such a d. No pair of columns of A(k) are collinear, since collinearity is a Zariski closed
condition with non-empty complement and the entries of F and the non-zero values λ(k) are
i,j
generic, byAssumption1.2(b). HenceProposition2.1applies, andwerecoverthemixingmatrix
A(k) up to permutation and scaling; i.e., we recover the matrices in (8). The number of columns
of these matrices is q. Absorbing the coefficients of the tensor decomposition into the vectors
as in (7), the diagonal matrices in (8) satisfy (9) for every i ∈ [q], k ∈ K, where d = d for all i.
i
8We now show why such a d as above is not required. Part (a) holds for any d ≥ 3(q − 1),
see the proof of Proposition 2.1. Part (b) is subtle: the existence of a sufficiently large d with
κ (ε(k)) ̸= 0 is equivalent to Assumption 1.2(a) that the distribution ε(k) is non-Gaussian, by
d i i
Marcinkiewicz’s theorem [Mar39]. However, this does not imply the existence of a common
d with that property, as we assumed above. If such a common d does not exist, we instead
recover the columns of F(I −Λ(k))−1 up to permutation and scaling, as well as the entries of
D(k), using a set of large enough cumulants κ (X(k)),...,κ (X(k)) such that for all i there
d1 dm
exists ℓ ∈ [m] with κ (ε(k)) ̸= 0. The non-Gaussianity assures that such a set exists, and the
d ℓ i
number of non-collinear vectors recovered from these tensor decompositions is q. Part (c) can
be avoided in the same way as (b), using Assumption 1.2(c).
Column scaling and permutation as in Proposition 2.3 have natural interpretations in LCD:
there is no natural order on the latent variables, and they can be re-scaled without affecting
membership in the model, see Remark 1.4. The goal of this section is to show that it is possible
to fix an order and scaling of latent variables that is consistent across contexts. The upshot is
the following result.
Proposition 2.4. Consider LCD under Assumption 1.2. Then we can recover the number of
latent nodes q and the matrices
A(k) := F(I −Λ(k))−1 for all k ∈ K ∪{0}. (11)
We fix a scaling of errors and an order on latent variables when k = 0, as follows.
Proposition 2.5. Without loss of generality P(0) = D(0) = I.
Proof. We have recovered ADP for some scaling D and permutation P, by Proposition 2.3,
where we drop the superscripts since we refer only to the observational context. The permuta-
tion P orders the latent variables. We fix it to be the identity, thereby fixing an order of latent
variables. We now consider D. Define F(cid:101) = FD and Λ(cid:101) = D−1ΛD. Then
F(I −Λ)−1D = F(cid:101)(I −Λ(cid:101))−1.
and matrices Λ and Λ(cid:101) have the same support. Hence F(cid:101) and Λ(cid:101) are valid parameters in the
model, so we can without loss of generality set D = I.
The choice in Proposition 2.5 sets a non-zero cumulant κ (ε(0)) to ±1 for each i ∈ [q], see (9).
di i
Hence D(k)±1 for all i ̸= i , by Proposition 2.3, since ε(k) and ε(0) differ only at the intervention
i,i k
target i . We now compare A(0) and A(k)D(k)P(k). The parents of a node j are the set pa (j) =
k G
{i ∈ G|i → j ∈ G} and the ancestors of j are an (j) = {i ∈ G|i → ··· → j ∈ G}. We drop
G
the subscript since G is fixed.
Proposition 2.6. Recall that i ∈ [q] is the intervention target of context k and let j ∈ [q].
k
Assume that F is generic and that the non-zero entries of Λ(k) are generic. Then one of three
possibilities arises.
(i) j = i and the j-th column of A(0) equals one of the columns of A(k)D(k)P(k) up to a
k
scaling that is not ±1;
9(ii) j ̸∈ an(i )∪{i } and the j-th column of A(0) equals one of the columns of A(k)D(k)P(k),
k k
up to sign;
(iii) j ∈ an(i )andthej-thcolumnofA(0) isnotparalleltoanyofthecolumnsofA(k)D(k)P(k).
k
Proof. (i) Assume j = i , and let j = P(k)(j). The (i,j ) entry of A(k)D(k)P(k) is
k k k
(A(k)D(k)) = A(k)D(k) = A(0)D(k). (12)
i,j i,j j,j i,j j,j
Therefore, the j -th column of A(k)D(k)P(k) is a non-trivial (not 0 or ±1) multiple of the j-th
k
column of A(0), since D(k) ̸= ±1 when j is the intervention target.
j,j
(ii) Assume j ̸∈ an(i )∪{i } and let j = P(k)(j). The chain of equalities in (12) holds true,
k k k
but D(k) = ±1. Hence, the j -th column of A(k)D(k)P(k) is the j-th column of A(0) up to sign.
j,j k
(iii) Let j ∈ an(i ). Assume for contradiction that there exists a column r of A(k)D(k)P(k) that
k
is parallel to the j-th column of A(0). Let r = P(k)(r). Then, there exists α such that for every
k
i ∈ [p],
(cid:88) (cid:88)
f (I −Λ(0))−1 = α f (I −Λ(k))−1 D(k) .
i,ℓ ℓ,j i,ℓ ℓ,r k r k,r k
ℓ∈[q] ℓ∈[q]
By genericity of F and Λ(k), the equality holds if and only if it holds for the coefficient of every
f independently. It is therefore equivalent to
i,ℓ
(I −Λ(0))−1 = α(I −Λ(k))−1 D(k)
ℓ,j ℓ,r k r k,r k
for all ℓ ∈ [q]. If ℓ = j, then by genericity of Λ(k) we have r = j and α = D(k). However, since
k j,j
D(k) = ±1, this leads to the equality
j,j
(I −Λ(0))−1 = (I −Λ(k))−1
ℓ,j ℓ,j
for every ℓ ∈ [q], which implies by genericity that λ(0) = λ(k) for every m, a contradiction.
i ,m i ,m
k k
Proposition 2.9 recovers the target of each intervention. It also recovers the ancestors of each
latent node. That is, it recovers the transitive closure G, providing a simpler proof of the
following result, proven without the non-Gaussian assumption in [SSBU23, Theorem 1].
Corollary 2.7. Consider LCD under Assumption 1.2 with one intervention (either perfect or
soft) on each latent node. Then we can recover the transitive closure G of the latent DAG G.
Remark 2.8 (Paths in G). Entry (i,j) of the matrix (I −Λ(k))−1 is a sum over all the paths
j → ··· → i in G, where each path contributes the product λ(k) over all edges n → m in the
m,n
path. For instance, for the DAG 3 → 2 → 1 we have (I −Λ(k))−1 = λ(k)λ(k). Adding the edge
1,3 1,2 2,3
3 → 1 gives (I −Λ(k))−1 = λ(k)λ(k) +λ(k).
1,3 1,2 2,3 1,3
Proposition 2.6 partially recovers the permutation P(k), as it pairs all columns j ∈/ an(i ).
k
We can therefore assume without loss of generality that i = k and that P(k) = δ for every
k i,j i,j
j ̸∈ an(k). We are left to pair the columns of an(k).
10Proposition 2.9. For j ,j ∈ an(k), there exists α ∈ R such that
1 2
(cid:16) (cid:17)
(cid:0) (cid:1) (cid:0) (cid:1)
(I −Λ(0))−1 − (I −Λ(k))−1D(k) = α (I −Λ(0))−1 − (I −Λ(k))−1D(k)
i,j1 i,j2 i,k i,k
for all i ∈ [q], if and only if j = j and D(k) = 1.
1 2 j1,j1
Proof. Fix j := j = j and assume D(k) = 1. The left hand side is a sum over all paths from
1 2 j1,j1
Z to Z , through Z , since the paths that do not go through Z cancel:
j i k k
(I −Λ(0))−1 −(I −Λ(k))−1 = (I −Λ(0))−1(I −Λ(0))−1 −(I −Λ(k))−1(I −Λ(k))−1
i,j i,j i,k k,j i,k k,j
(cid:0) (cid:1)
= (I −Λ(0))−1 (I −Λ(0))−1 −(I −Λ(k))−1
i,k k,j k,j
=
(I−Λ(0))− k,1 j−(I−Λ(k))− k,1
j
(cid:16)
(I −Λ(0))−1 −(I
−Λ(k))−1D(k)(cid:17)
,
1−D(k) i,k i,k k,k
k,k
where we used (I−Λ(0))−1−(I−Λ(k))−1D(k) = (I−Λ(0))−1(1−D(k)). This proves one direction.
i,k i,k k,k i,k k,k
Assume conversely that the equality in the statement holds for some j ,j ∈ an(k), and let
1 2
i = j . Then
1
(cid:16) (cid:17)
(cid:0) (cid:1) (cid:0) (cid:1)
(I −Λ(0))−1 − (I −Λ(k))−1D(k) = α (I −Λ(0))−1 − (I −Λ(k))−1D(k) .
j1,j1 j1,j2 j1,k j1,k
The right-hand side is zero since the latent graph is a DAG and j ∈ an(k). Hence
1
(cid:0) (cid:1)
0 = (I −Λ(0))−1 − (I −Λ(k))−1D(k) = 1±(I −Λ(0))−1
j1,j1 j1,j2 j1,j2
whereweusedthattherearenopathsfromj tok andD(k) = ±1. Therefore,(I−Λ(0))−1 = 1,
2 j2,j2 j1,j2
which implies by genericity that j = j and D(k) = 1.
1 2 j2,j2
Corollary 2.10. For every k and for generic parameters in F,Λ(0),Λ(k), we have
(cid:0) (cid:1)
rank A(0) −A(k)D(k)P(k) = 1
if and only if P(k) = I and D(k) = 1 for all j ̸= k.
j,j
Proof. A matrix has rank one if and only if all its columns are scalar multiples. Therefore, our
claim is equivalent to the existence for every j ∈ [q] of some α ∈ R such that
(cid:88) (cid:16) (cid:0) (cid:1) (cid:17)
f (I −Λ(0))−1 − (I −Λ(k))−1D(k)P(k)
ℓ,i i,j i,j
i∈[q]
= α(cid:88) f (cid:16) (I −Λ(0))−1 −(cid:0) (I −Λ(k))−1D(k)P(k)(cid:1) (cid:17) , (13)
ℓ,i i,k i,k
i∈[q]
for every ℓ ∈ [p]. If we treat the parameters in F,Λ(0),Λ(k),D(k) as indeterminates, the equa-
k,k
tion holds if and only if all the summands are equal. Analogously, this is the case if the f
ℓ,i
parameters are generic.
11For j ̸∈ an(k), we have P(k) = δ . Assume for contradiction that D(k) = −1. Then, for
i,j i,j j,j
i = k, the left-hand side of (13) is 0 and the right-hand side is α(1−D(k)), which forces α = 0.
k,k
However, for i = j, the left-hand side is 2, so α ̸= 0, a contradiction. This forces D(k) = 1 for
j,j
the non-ancestors of k. Putting this together with Proposition 2.9, we deduce that the matrix
(I −Λ(0))−1 −(I −Λ(k))−1D(k)P(k) has rank at most 1 if and only if P(k) = I and D(k) = 1 for
j,j
all j ̸= k. Moreover, because D(k) ̸= 1, the k-th column of the difference matrix is non-zero,
k,k
hence the rank is exactly 1.
Proof of Proposition 2.4. We recover the matrices A(k) up to scaling and permutation, by
Proposition 2.3. The upshot of Corollary 2.10 is that we can identify the target i of the inter-
k
ventionandthepermutation. HencewecangetridofP(k) byrightmultiplicationwithitstrans-
pose. Now the i -th column of F(I−Λ(0))−1 differs from the i -th column of F(I−Λ(k))−1D(k)
k k
by the scaling D(k) , so we can also recover the diagonal matrix, and hence A(k) itself.
i ,i
k k
Remark 2.11. While Proposition 2.4 holds for any p,q ≥ 2, the proof is simpler when q ≤ p.
Then, the Moore-Penrose pseudo-inverse satisfies
(cid:0)
F(I
−Λ(k))−1D(k)P(k)(cid:1)+
= (P(k))⊤(D(k))−1(I −Λ(k))F+.
Findingthepermutationandinterventiontargetsisdoneasfollows. Thereisjustonerowinthe
pseudo-inverse of the context k that does not appear in the pseudo-inverse of the observational
context. Hence it indexes the intervention target. The permutation is found by matching the
remaining rows of the two matrices. The expression relating the psuedo-inverse of the product
to the product of psuedo-inverses does not hold in general when q > p.
3 Recovery via interventions
In this section, we identify when two latent graphs and parameters F,Λ(k) give the same distri-
butionsX(k). Atthisstage, wehaveaccesstothematricesA(k) in(11),byProposition2.4.
Proposition 3.1. Distributions F(I − Λ(k))−1ε(k) and F(cid:101)(I − Λ(cid:101)(k))−1ε(k) coincide for all k ∈
(cid:101)
K ∪ {0} if and only if there exists a reordering of the sets {ε(0)}, {ε(0)} and a rescaling of
i (cid:101)i
F,Λ(0),F(cid:101),Λ(cid:101)(0) via (5) such that F(I −Λ(k))−1 = F(cid:101)(I −Λ(cid:101)(k))−1 for all k ∈ K ∪{0}.
Proof. Define A(k) = F(I−Λ(k))−1 and A(cid:101)(k) = F(cid:101)(I−Λ(cid:101)(k))−1. The equality of matrices A(k) and
A(cid:101)(k) implies the equality of the distributions X(k) = A(k)ε(k) and X(cid:101)(k) = A(cid:101)(k)ε(k). Conversely,
assume that distributions X(k) and X(cid:101)(k) coincide. Then, we have the equality of cumulants
κ (X(k)) = κ (X(cid:101)(k)) for all k and d. To simplify the exposition, we assume that there exists d as
d d
in the proof of Proposition 2.3 (this assumption can be avoided using the same argument as in
the proof of Proposition 2.3). For this fixed d and for each context k, the tensor decomposition
of κ (X(k)) is unique up to rescaling and permutation. Since the cumulant is the same for both
d
distributions, from the decomposition we get
F(I −Λ(k))−1D(k)P(k) = F(cid:101)(I −Λ(cid:101)(k))−1D(cid:101)(k)P(cid:101)(k).
12Fix k = 0. We can reorder the variables ε(0) to set P(0) = I and we can absorb D(0) into F and
i
Λ(0), as in Proposition 2.5. Analogously, we can do the same in the tilde setting. Therefore, up
to reordering and rescaling via (5) we have
F(I −Λ(0))−1 = F(cid:101)(I −Λ(cid:101)(0))−1.
Then, Corollary 2.10 implies that there exists a unique choice of signs of the diagonal matrices
and a unique permutation matrix Q satisfying
(cid:16) (cid:17)
(cid:0) (cid:1)
rank A(0) −A(k)D(k)P(k)Q = 1 = rank A(cid:101)(0) −A(cid:101)(k)D(cid:101)(k)P(cid:101)(k)Q ,
and Q = (P(k))⊤ and Q = (P(cid:101)(k))⊤. Therefore, P(k) = P(cid:101)(k), and by comparing the intervened
columns of the difference matrices we have D(k) = D(cid:101)(k). This implies A(k) = A(cid:101)(k).
The upshot is that solving Problem 1.3 is equivalent to solve the following problem.
Problem 3.2. Given a generic matrix F(cid:101) ∈ Rp×q and matrices Λ(cid:101)(0),...,Λ(cid:101)(q) ∈ Rq×q constructed
according to a model with DAG G(cid:101), with generic non-zero entries, do there exist a generic matrix
F ∈ Rp×q, and matrices Λ(0),...,Λ(q) ∈ Rq×q constructed according to a model with DAG G,
such that
F(I −Λ(k))−1 = F(cid:101)(I −Λ(cid:101)(k))−1 (14)
for all k ∈ K ∪{0}? If so, how are the DAGs and the corresponding matrices related?
We solve the system of polynomial equations (14). The solution is unique if and only if the
DAG and the matrices are identifiable. Otherwise, the set of solutions is the set of possible
DAGs and space of possible parameters.
From now on, unless otherwise stated, we assume that we have the observational context and
one intervention per latent node. We re-index contexts so that the k-th intervention (either
soft or perfect) is on Z , hence K = [q].
k
3.1 A linear system
Let A(k) = F(cid:101)(I −Λ(cid:101)(k))−1 and let S be the space of solutions to (14). The algebraic variety S
is associated to the ideal
I = ⟨F(I −Λ(k))−1 −A(k), k = 0,...,q⟩.
The matrices F,Λ(k) are filled with indeterminates. Each point of S provides a graph and
parameters compatible with the given model. At first sight, S might have high degree, since
the degree of the generators can reach q+1. However, there is a simpler set of generators:
I = ⟨F −A(k)(I −Λ(k)), k = 0,...,q⟩. (15)
Assuming the A(k) are known, I has (q+1)qp linear generators in a polynomial ring with qp+
(q+1)|e(G)|indeterminates. WefindasetofminimalgeneratorsforI tocomputethedimension
of the associated algebraic variety; i.e., to find the identifiability of the parameters.
13Proposition 3.3. ConsiderthesetupinAssumption1.2withq softinterventions. Whend > 2,
the space of parameters F and Λ(k), k ∈ [q] ∪ {0}, such that κ (X(k)) is a given tensor is a
d
linear space. When d = 2, for any q ∈ N there exists a DAG on q nodes such that the space of
parameters for which κ (X(k)) is a given matrix for all k ∈ [q]∪{0} is non-linear.
2
Proof. The case d > 2 follows from (15). For the case d = 2, consider a model on two
latent nodes with one edge 2 → 1, with parameters F(cid:101) = (2 3 ), λ(cid:101)(0) = λ(cid:101)(2) = 7, λ(cid:101)(1) = 13.
5 11 1,2 1,2 1,2
Symbolic computation with, e.g., Macaulay2 or Oscar.jl shows that the space of parameters
that satisfy κ (X(k)) = κ (X(cid:101)(k)) for k ∈ {0,1,2} is 1-dimensional and of degree 8. It is the
2 2
union of 6 irreducible components, four linear and two quadratic. The same happens for generic
parameters. We can embed this DAG into a DAG on q nodes with only one edge 2 → 1. Then,
the space of solutions has the same dimension (= 1) and degree (= 8) as the space of solutions
for the DAG on two nodes.
Proposition 3.3 shows that the non-Gaussianity assumption is required in Theorem 1.6. We
conclude this subsection with an example, to see the linear structure of I.
Example 3.4. Consider the latent DAG
G = 4 3 2 1
with parameters
   
2 6 10 1 0 9 3 0
 2 9 −3 8  0 0 0 10
F =  , Λ(0) = Λ(4) =  ,
−8 4 7 2  0 0 0 7
−9 8 2 −5 0 0 0 0
     
0 −5 8 0 0 9 3 0 0 9 3 0
0 0 0 10 0 0 0 2 0 0 0 10
Λ(1) =  , Λ(2) =  , Λ(3) =  .
0 0 0 7 0 0 0 7 0 0 0 −1
0 0 0 0 0 0 0 0 0 0 0 0
Then, assuming known G, the ideal I (15) is minimally generated by 21 linear polynomials
f −2, f −2, f +8, f +9,
1,1 2,1 3,1 4,1
f +2λ(0) −24, f +2λ(0) −27, f −8λ(0) +68, f −9λ(0) +73,
1,2 1,2 2,2 1,2 3,2 1,2 4,2 1,2
f +2λ(0) −16, f +2λ(0) −3, f −8λ(0) +17, f −9λ(0) +25,
1,3 1,3 2,3 1,3 3,3 1,3 4,3 1,3
f + 172λ(0) −173, f + 177λ(0) − 193, f − 289λ(0) +287, f − 715λ(0) + 725,
1,4 7 3,4 2,4 14 3,4 2 3,4 7 3,4 4,4 14 3,4 2
λ(1) −λ(0) +14, λ(1) −λ(0) −5, λ(2) −λ(0) +8, λ(3) −λ(0) +8, −14λ(0) +5λ(0) +105.
1,2 1,2 1,3 1,3 2,4 2,4 3,4 3,4 2,4 3,4
ThesecanbefoundbycomputingtheprimarydecompositionofI incomputeralgebrasoftware,
such as Macaulay2 [M2] or Oscar.jl [OSC24, DEF+24]. ⋄
143.2 Perfect interventions
When the interventions are perfect, namely λ(k) = 0 for every k ∈ [q], there is a unique solution
k,j
to the linear system in (15). In other words, the ideal I is zero dimensional and defines a point.
This is Theorem 1.5.
Proof of Theorem 1.5. Worst case necessity of one intervention per node for identifiability is
a direct consequence of [SSBU23, Proposition 5]. We prove sufficiency. We have matrices
A(k) = F(I −Λ(k))−1, by Proposition 2.4. Pick k,j ∈ [q] with k ̸= j. Then,
(cid:0) (cid:1) (cid:88) (cid:0) (cid:1)
A(0) −A(k) = f (I −Λ(0))−1 −(I −Λ(k))−1
1,j 1,ℓ ℓ,i
ℓ∈[q]
(cid:88) (cid:0) (cid:1)
= f (I −Λ(0))−1 (I −Λ(0))−1 −(I −Λ(k))−1
1,ℓ ℓ,k k,j
ℓ∈de(k)
= A(0) (I −Λ(0))−1.
1,k k,j
With this, we construct (I − Λ(0))−1 and hence recover Λ(0). We multiply A(0)(I − Λ(0)) to
obtain F.
The above result shows that q perfect interventions are sufficient to recover the DAG and the
parameters of a model. To find the parameters (and hence the latent DAG), one can solve the
linear system (15) or follow the procedure in the proof.
Remark 3.5. When q ≤ p, an alternative proof via pseudo-inverses exists, see Section 4.3.
When q > p, the non-Gaussianity assumption is necessary for Theorem 1.5, as follows.
Proposition 3.6. Consider LCD under Assumption 1.2 with perfect interventions and q > p.
Then one perfect intervention on each latent node is not sufficient to recover the latent DAG
G and the parameters F and Λ(k) from the covariance matrices of X(k).
Proof. For (p,q) = (2,3) and G = ∅, we compute the parameters F and Λ(k) for which the co-
variancematricesF(I−Λ(k))−1(D(k))2(I−Λ(k))−⊤F⊤ coincidewiththetruecovariancematrices
for k = 0,1,2. We choose and fix an ordering of the nodes, and we fix the scaling by imposing
D(0) = I. This space has dimension 2, so the parameters cannot be recovered uniquely. We
can embed this DAG in a DAG with q nodes, for any q. Hence the p×p covariance matrices
do not contain enough information to recover the parameters, when p < q.
3.3 Soft interventions
In this section we compute the dimension of solutions of the linear system F−A(k)(I−Λ(k)) = 0
fork = 0,...,q, undersoftinterventions. Foreveryk andforeveryℓ ∈ [p],j ∈ [q], wehave
(cid:88)
f + A(k)λ(k) = A(k).
ℓ,j ℓ,i i,j ℓ,j
i∈ch(j)
There are pq(q+1) equations in pq+2|e(G)| indeterminates, namely f for all ℓ ∈ [p], j ∈ [q],
ℓ,j
and λ(0) for all (j → i) ∈ e(G), and λ(k) for all (j → k) ∈ e(G). For each (ℓ,j), we subtract the
i,j k,j
15equation for k = 0 from the equations for k ∈ [q]. Then the (cid:0) pq(q+1)(cid:1) ×(cid:0) pq+2|e(G)|(cid:1) matrix
of the linear system has block structure
(cid:18) (cid:19)
I ⋆
pq .
0 ⋆
We can focus on the (cid:0) pq2(cid:1) × (cid:0) 2|e(G)|(cid:1) bottom-right block, involving only the indeterminates
Λ(k). The equations of this smaller linear system are A(k)(I−Λ(k))−A(0)(I−Λ(0)) = 0, or
(cid:88) (cid:88)
A(k)λ(k) − A(0)λ(0) = A(k) −A(0), (16)
ℓ,i i,j ℓ,i i,j ℓ,j ℓ,j
i∈ch(j) i∈ch(j)
for k,j ∈ [q], ℓ ∈ [p]. There are three cases:
1. If k ̸∈ ch(j), then (16) becomes
(cid:88)
(A(k) −A(0))λ(0) = (A(k) −A(0)). (17)
ℓ,i ℓ,i i,j ℓ,j ℓ,j
i∈ch(j)
The (ℓ,i) entry of A(k)−A(0) is by definition (cid:80) f (cid:0) (I −Λ(k))−1 −(I −Λ(0))−1(cid:1) .
n∈de(i) ℓ,n n,i
If k ̸∈ de(j), then (cid:0) (I −Λ(k))−1 −(I −Λ(0))−1(cid:1) = 0 for every n since by construction
n,i
de(j) ⊃ de(i). Hence, (17) reads 0 = 0 and it imposes no condition on our indeterminates.
2. If k ∈ de(j)\ch(j), then (cid:0) (I −Λ(k))−1 −(I −Λ(0))−1(cid:1) ̸= 0, since there is a path from i
n,i
tonthroughk. Suchapathmustexistforsomen,sincek isadescendantofsomei. Hence
the coefficients of (17) are non-zero, and we get linear conditions on the indeterminates.
3. Finally, if k ∈ ch(j), we get an expression for λ(k) in terms of the λ(0):
k,j i,j
 
λ(k) = 1  A(0)λ(0) + (cid:88) (A(0) −A(k))λ(0) +(A(k) −A(0)) 
k,j A(k)  ℓ,k k,j ℓ,i ℓ,i i,j ℓ,j ℓ,j 
ℓ,k i∈ch(j)
i̸=k (18)
A(0) −A(k) A(k) −A(0)
(cid:88)
= λ(0) + ℓ,i ℓ,i λ(0) + ℓ,j ℓ,j ,
k,j A(0) i,j A(0)
i∈ch(j) ℓ,k ℓ,k
i̸=k
where we used A(k) −A(0) = 0 because (cid:0) (I −Λ(k))−1 −(I −Λ(0))−1(cid:1) = 0 for every n.
ℓ,k ℓ,k n,k
We get (18) for every ℓ ∈ [p]. However, most equations are redundant.
The following result mimics Proposition 2.9.
Proposition 3.7. For k ∈ [q], let ∆(k) = (I −Λ(k))−1 −(I −Λ(0))−1. Then, rank(cid:0) ∆(k)(cid:1) ≤ 1,
with equality if and only if an(k) ̸= ∅.
Proof. Fix k ∈ [q] and recall that the (i,j) entry of (I−Λ(k))−1 is the sum of all paths from Z
j
to Z , where a path is encoded as the product of λ(k) for all edges n → m in the path. Then,
i m,n
16the only non-zero columns of ∆(k) are those indexed by j for j ∈ an(k). We prove that these
columns are multiple of each other. Let j ,j ∈ an(k), then
1 2
∆(k) = (I −Λ(k))−1(I −Λ(k))−1 −(I −Λ(0))−1(I −Λ(0))−1
i,jm i,k k,jm i,k k,jm
= (I −Λ(0))−1∆(k)
i,k k,jm
for m = 1,2. Hence, for every i ∈ [q], the (i,j ) entry equals the (i,j ) entry up to
∆( kk ,j)
2.
1 2 ∆(k)
k,j1
For generic parameters we have rank(A(k) − A(0)) ≤ 1 for all k ∈ [q], with equality whenever
an(k) ̸= ∅, by Proposition 3.7, with proof is analogous to that of Corollary 2.10. Hence the
conditions in (17) are equivalent for all ℓ ∈ [p], and the same is true of the conditions in (18).
This reduces the size of the linear system, taking only the equations for ℓ = 1 ∈ [p]. We obtain
a reduced matrix of the linear system
 
I 0 ⋆
pq
 0 I |e(G)| ⋆ , (19)
0 0 ⋆
where the top block writes F in terms of Λ(0), the second block writes Λ(k) in terms of Λ(0),
and the bottom block gives the conditions (17) on Λ(0). The latter are (cid:80) |de(j) \ ch(j)|
j∈[q]
equations in (cid:80) |ch(j)| = |e(G)| indeterminates. The conditions are independent for each j.
j∈[q]
Namely, the block has the form
 
M[1] 0 ··· 0
 0 M[2] ··· 0 
M =   . . . ··· ... . . .  .
 
0 ··· 0 M[q]
(cid:16) (cid:17)
Eachsub-blockhassize|de(j)\ch(j)|×|ch(j)|anddefinesM[j]· λ(0) = b[j]where
i,j
i∈ch(j)
(cid:16) (cid:17)
(cid:0) (cid:1)
M[j] = A(k) −A(0) , k ∈ de(j)\ch(j), i ∈ ch(j),
1,i
(cid:16) (cid:17)
(cid:0) (cid:1)
b[j] = A(k) −A(0) , k ∈ de(j)\ch(j).
1,j
At this point, it seems that the matrices defining the linear system depend on F and Λ(k).
However, following the proof of Proposition 3.7, we have
(cid:0) A(k) −A(0)(cid:1) = (cid:88) f ∆(k) = (cid:88) f (I −Λ(0))−1 ∆(k) = A(0) ∆(k).
1,i 1,n n,i 1,n n,k k,i 1,k k,i
n∈[q] n∈de(k)
Assuming A(0) ̸= 0 for every k ∈ [q], which holds generically, we can rescale to obtain
1,k
(cid:16) (cid:17)
M[j] = ∆(k) , k ∈ de(j)\ch(j), i ∈ ch(j),
k,i
(20)
(cid:16) (cid:17)
b[j] = ∆(k) , k ∈ de(j)\ch(j),
k,j
17where ∆(k) = (I −Λ(k))−1−(I −Λ(0))−1. From this, we see that M[j] and b[j] depend only on
the latent DAG and its parameters: the linear system (17) becomes
(cid:88)
∆(k)λ(0) = ∆(k), (21)
k,i i,j k,j
i∈ch(j)
for all j ∈ [q]. We compute the dimension of the solution space by comparing the ranks
|de(j)\ch(j)|×|ch(j)|matrixM[j]andthe|de(j)\ch(j)|×(|ch(j)|+1)matrix(M[j]|b[j]).
Proposition 3.8. Assume that the interventions are soft. For each node j, let
(cid:40)
−1 if rankM[j] ̸= rank(M[j]|b[j]),
c =
j |ch(j)|−rankM[j] otherwise,
where M[j] and b[j] are defined in (20). Then, the ideal I in (15) has dimension
(cid:40)
−1 if c = −1 for some j ∈ [q],
j
dimI =
(cid:80)q c otherwise.
j=1 j
Proof. The dimension of I is the dimension of the solution space of (21), that is,
(cid:16) (cid:17)
M[j]· λ(0) = b[j]
i,j
i∈ch(j)
for all j ∈ [q], by (19). Its dimension c is |ch(j)|−rankM[j] if rankM[j] ̸= rank(M[j]|b[j]).
j
Otherwise, the solution space is empty and we set c = −1, as is convention.
j
Corollary 3.9. With one soft intervention per latent node it is never possible to recover
uniquely all the parameters of the model.
Proof. The result remains true if we assume knowledge of the latent DAG G. Let ch(i) = ∅.
Take j ∈ pa(i) such that an(j) \ ch(j) = ∅. Then M[j] = ∅, so c = |ch(j)| ≥ 1. Therefore,
j
dimI ≥ 1 and it is not possible to identify uniquely the parameters f and λ(k), for ℓ ∈ [p]
ℓ,j k,j
and k ∈ ch(j)\{i}.
Adding interventions does not affect the matrices M[j] in the proof of Corollary 3.9. Therefore
Corollary 1.7 follows: non-identifiability holds regardless of the number of interventions.
When c = 0 it is possible to identify uniquely all parameters λ(0) for i ∈ ch(j), as well as f
j i,j ℓ,j
and λ(k), for ℓ ∈ [p] and k ∈ ch(j). The condition c = 0 holds, for example, when ch(j) = {i }
k,j j 1
and de(j) = {i ,i }.
1 2
Example 3.10. We continue Example 3.4. The matrices are
(cid:0) (cid:1)
M[1] = M[2] = M[3] = b[1] = b[2] = b[3] = ∅, M[4] = −14 5 , b[4] = −105,
hence c = |ch(1)| = 0, c = |ch(2)| = 1, c = |ch(3)| = 1, c = |ch(4)|−rankM[4] = 2−1 = 1.
1 2 3 4
By Proposition 3.8, we have dimI = 3 and in fact it is minimally generated by the 21 linear
polynomials in 24 indeterminates in Example 3.4. ⋄
18The rank of M[j] depends on the structure of the DAG G beyond the number of children and
descendants of j. This is highlighted in the following example.
Example 3.11. Consider the DAG
G = 5 4 3 2 1
Node j = 5 has 2 children and 4 descendants, hence (21) consists of equations
(cid:88)
∆(k)λ(0) = ∆(k) k = 1,2. (22)
k,i i,5 k,5
i=3,4
They impose conditions on the λ(0). There are two λ(0) from node 5, namely λ(0),λ(0), and two
i,j i,j 3,5 4,5
equations. However, the equations in (22) are dependent. We have
(cid:32) (cid:33) (cid:32) (cid:33)
λ(1) −λ(0) (λ(1) −λ(0))λ(0) (λ(1) −λ(0))(λ(0)λ(0) +λ(0))
M[5] = 1,3 1,3 1,3 1,3 3,4 , b[5] = 1,3 1,3 3,4 4,5 3,5 ,
λ(2) −λ(0) (λ(2) −λ(0))λ(0) (λ(2) −λ(0))(λ(0)λ(0) +λ(0))
2,3 2,3 2,3 2,3 3,4 2,3 2,3 3,4 4,5 3,5
so rankM[5] = rank(M[5]|b[5]) = 1 < 2. Hence we cannot recover the parameters λ(0),λ(0).
3,5 4,5
The reason rankM[5] < 2 is that all the paths from 4 to 1 or 2 (encoded in the second column
of M[5]) and all the paths from 5 to 1 or 2 (encoded in b[5]) go through 3. This factorization
of paths creates dependencies in M[5],b[5], preventing identifiability. ⋄
To recover as many parameters as possible, DAGs should balance between too many children,
hence too many indeterminates, and too few children, hence paths factorize more easily.
From an algorithmic point of view, we can check the rank condition in Proposition 3.8. Indeed,
we have matrices A(k), and we can compute, for all i,k ∈ [q] with i ̸= k, the entries
A(k) −A(0)
∆(k) = 1,i 1,i .
k,i A(0)
1,k
Remark 3.12. If q ≤ p, the computations can be simplified. We can write (16) as
Λ(k) = (A(k))+A(0)Λ(0) +I −(A(k))+A(0).
This writes the λ(k) indeterminates in terms of λ(0) indeterminates, and enables us to find the
k,j i,j
linear conditions on the λ(0) indeterminates. The reduction of the linear system is the same as
i,j
in (20), as can be proved by noticing that for any k ̸= i, the k-th row of (A(k))+A(0) equals the
k-th row of ∆(k) up to sign. Indeed,
(cid:0) (cid:1) (cid:0) (cid:1)
(A(k))+A(0) = (I −Λ(k))(I −Λ(0))−1
k,i k,i
(cid:88)
= −λ(k)(I −Λ(0))−1 +(I −Λ(0))−1
k,ℓ ℓ,i k,i
ℓ∈pa(k)
= −(I −Λ(k))−1 +(I −Λ(0))−1 = −∆(k),
k,i k,i k,i
where the last row used (I −Λ(0))−1 = (I −Λ(k))−1 for every ℓ ∈ pa(k).
ℓ,i ℓ,i
193.3.1 Identifiability of the latent DAG
For perfect interventions, Theorem 1.5 shows that we can recover the latent DAG of the model,
as well as the parameters. With soft interventions, we cannot recover the whole DAG and all
the parameters (see Corollary 3.9). It is natural to wonder to what extent we can recover the
latent DAG. Thanks to Proposition 3.8, we can turn this into 2q rank computations.
Definition 3.13. Given a model with matrices A(k) ∈ Rp×q for k = 0,...,q, we say that a
DAG G′ is compatible with the model if there exist parameters F ∈ Rp×q, Λ(k) ∈ Rq×q defined
according to the latent DAG G′, such that A(k) = F(I −Λ(k))−1 for all k.
If the true latent DAG is G, a compatible DAG G′ must satisfy G = G′, by Corollary 2.7. To
emphasize that a matrix such as Λ(k) depends on a DAG G, we write Λ(k). In the same spirit,
G
we define
(cid:16) (cid:17)
M [j] = (∆(k)) k ∈ de (j)\ch (j), i ∈ ch (j),
G,G′ G k,i G′ G′ G′
(cid:16) (cid:17)
b [j] = (∆(k)) k ∈ de (j)\ch (j),
G,G′ G k,j G′ G′
where the indexing depends on G′ and ∆(k) = (I −Λ(k))−1 −(I −Λ(0))−1. Recall that
G G G
[(I −Λ(0))−1]
r,c
denotes the submatrix with rows in r ⊂ [q] and columns in c ⊂ [q]. We give the following
definition, already mentioned in the introduction.
Definition 3.14. Given a DAG G on q nodes, its soft-compatible class is
(cid:110)
soft(G) = G′ DAG | G′ = G and for all j ∈ [q]
(cid:111)
rank[(I −Λ(0))−1] = rank[(I −Λ(0))−1] , (23)
G de G′(j)\ch G′(j),ch G′(j) G de G′(j)\ch G′(j),ch G′(j)
where ch(j) := ch(j)∪{j} for j ∈ [q].
The soft-compatible class of a DAG G is the set of all graphs that are compatible with a model
with latent DAG G, as follows.
Theorem 3.15. Consider LCD under under Assumption 1.2 with DAG G. Then a graph G′ is
compatible with the model if and only if G′ ∈ soft(G).
Proof. The ranks of the matrices in (23) are the same as the ranks of the matrices M [j]
G,G′
and (M [j]|b [j]), since the first matrices can be obtained from the second by replacing
G,G′ G,G′
each term λ(k) − λ(0) with λ(0). By the genericity of the parameters, this does not affect the
k,i k,i k,i
rank. The genericity assumption allows us to switch between the parameters of the model and
abstract indeterminates without change. Therefore, the ranks of the matrices in (23) coincide
with the ranks of M[j] and (M[j]|b[j]) from (20).
The DAG G′ is compatible with the model if and only if the corresponding ideal I has non-
negative dimension. Here the indeterminates of the system defined by I are the entries of F
and of Λ(k). By Proposition 3.8, it is equivalent to c ̸= −1 for all j, which holds if and only if
G′ j
rankM[j] = rank(M[j]|b[j]) for all j. This is equivalent to the condition G′ ∈ soft(G).
20Proof of Theorem 1.6. The linearity follows from equation (15). The positive dimensionality
follows from Corollary 3.9. The compatibility class of DAGs is Theorem 3.15.
We investigate the concept of soft-compatible class. If G′ has the same transitive closure as G
and if ch (j) ⊃ ch (j) for all j, then G′ ∈ soft(G). This is true because
G′ G
[(I −Λ(0))−1] and [(I −Λ(0))−1]
G deG(j)\chG(j),chG(j) G deG(j)\chG(j),chG(j)
always satisfies the rank condition (since by construction a solution with DAG G exists). How-
ever, the matrix [(I −Λ(0))−1] is obtained from [(I −Λ(0))−1]
G de G′(j)\ch G′(j),ch G′(j) G deG(j)\chG(j),chG(j)
by adding columns and deleting rows, hence the column indexed by j remains in the span of
the columns indexed by its children. Therefore, in order to exit the soft-compatible class of G, a
DAG G′ with the same transitive closure must have enough more children at some node.
If q = 2, soft(G) = G for every DAG, since there are no distinct DAGs with the same transitive
closure. For q = 3, the only case in which two DAGs have the same transitive closure (up to
relabeling the nodes) is the segment DAG Z → Z → Z , denoted by , and the DAG with
3 2 1
extra edge Z → Z , denoted by ▲. Since ▲ is obtained from by adding Z to the children
3 1 1
of Z , we know that ▲ ∈ soft( ). On the other hand, since de (j)\ch (j) = ∅ for j = 1,2,
3
the only relevant submatrices are obtained for j = 3 and they are
[(I −Λ(0))−1] and [(I −Λ(0))−1]
▲ 1,2 ▲ 1,{2,3}
for which the rank condition is trivially satisfied, hence ∈ soft(▲). Therefore, for DAGs on 3
nodes,thesoft-compatibleclassesaretheclassesofDAGswiththesametransitiveclosure.
For q = 4, if every node has at most 1 descendant that is not a child, the rank condition is
always satisfied. The only time this might not hold is for DAGs whose transitive closure is
the complete DAG on 4 nodes. There are 8 such DAGs (up to relabeling). We compute their
soft-compatible classes.
Example 3.16. Consider DAGs on 4 nodes, with transitive closure the complete DAG
, ,
G = 4 3 2 1 G = 4 3 2 1
1 2
, ,
G = 4 3 2 1 G = 4 3 2 1
3 4
, ,
G = 4 3 2 1 G = 4 3 2 1
5 6
G = 4 3 2 1 , G = 4 3 2 1 .
7 8
The submatrices of (I − Λ(0))−1 for j = 1,2,3 are either empty or they have one row, for
G
every G′. Therefore, on these nodes the rank condition is always satisfied. The interesting rank
21condition comes from the submatrices for j = 4. By computing these submatrices for all pairs
of DAGs, one obtains
soft(G ) = soft(G ) = {G ,G ,G ,G ,G ,G ,G ,G },
1 2 1 2 3 4 5 6 7 8
soft(G ) = soft(G ) = soft(G ) = soft(G ) = soft(G ) = soft(G ) = {G ,G ,G ,G ,G ,G }.
3 4 5 6 7 8 3 4 5 6 7 8
For n = 1,2 and m = 3,...,8, we have G ̸∈ soft(G ), since
n m
rank[(I −Λ(0))−1] = rank[(I −Λ(0))−1] = 1,
Gm deGn(4)\chGn(4),chGn(4) Gm {1,2},3
rank[(I −Λ(0))−1] = rank[(I −Λ(0))−1] = 2. ⋄
Gm deGn(4)\chGn(4),chGn(4) Gm {1,2},{3,4}
Soft-compatible classes are in general smaller than the DAGs with the same transitive closure.
In a soft-compatible class a unique sparsest DAG does not exist, see Example 3.16 where the
sparsest DAGs in soft(G ) are G ,G . Moreover, soft-compatible classes are not equivalence
8 3 4
classes, see Example 3.16 where G ∈ soft(G ) but G ̸∈ soft(G ).
8 1 1 8
3.3.2 More interventions
So far in Section 3 we have mostly assumed one intervention per latent node, and we proved
that this is not sufficient to recover the latent DAG or the parameters. It is natural to wonder
whether more interventions would allow the recovery. We already discussed after Corollary 3.9
that complete identifiability is impossible regardless of the number of interventions; in other
words, Corollary 1.7 holds.
In some cases, increasing the number of interventions allows us to recover more parameters (see
Example 3.18). In other cases, more interventions do not improve the parameters that can be
recovered (see Example 3.17).
Example 3.17. Consider the DAG
G = 4 3 2 1
Node j = 4 has 2 children and 1 descendant, so c = 1. We are interested in the question: if
4
we have more interventions, can we recover more? In particular, can we recover λ(0),λ(0)?
2,4 3,4
The only intervention that could give us more information is an extra intervention on node 1,
since that is the only descendant of 4 which is not its child. Assume a fifth context also with
intervention target 1. We have a linear system with two equations and two unknowns:
∆(1)λ(0) +∆(1)λ(0) = ∆(1),
1,2 2,4 1,3 3,4 1,4
∆(5)λ(0) +∆(5)λ(0) = ∆(5).
1,2 2,4 1,3 3,4 1,4
The matrix of this linear system, however, has rank 1 because
(cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33)
∆(1) ∆(1) ∆(1) ∆(1)
1,3 = λ(0) 1,2 and 1,4 = (λ(0) +λ(0)λ(0)) 1,2 .
∆(5) 2,3 ∆(5) ∆(5) 2,4 2,3 3,4 ∆(5)
1,3 1,2 1,4 1,2
All paths from 4 to 1 must go through 2 and this means no additional parameters can be
recovered from the extra intervention. ⋄
22Example 3.18. Consider the DAG
G = 4 3 2 1
and focus on the node j = 4. The linear system is as above, but now
(cid:32) (cid:33) (cid:32) (cid:33)
∆(1) ∆(1)
1,3 ∦ 1,2 ,
∆(5) ∆(5)
1,3 1,2
so the matrix is full rank and we can recover the parameters λ(0),λ(0). ⋄
2,4 3,4
To conclude, more interventions do not necessarily allow us to recover the DAG or the pa-
rameters. Indeterminates λ(k) with ch(i) = ∅ are not the only ones that cannot be recovered
i,j
regardless of how many soft interventions we have - this is also the case for λ(k) and λ(k) in
2,4 3,4
Example 3.17. But there are examples where more interventions reduce the dimension of the
solution space - we can recover λ(k) and λ(k) in Example 3.18.
2,4 3,4
4 Algorithm
Coupled tensor decomposition of higher-order cumulants enables us to recover parameters in an
LCD model. In this section, we explain how to turn our results into a numerical algorithm for
LCD. The algorithm has two main steps. The first (see Section 4.1) is to identify intervention
targets, permutation, and scaling. This turns the results of Section 2 into an algorithm, and
works for both perfect and soft interventions. The second step (see Section 4.2) is to recover
the parameters of the model. We do this step for perfect interventions (following Theorem 1.5)
and not for soft interventions, in light of Theorem 1.6. Both steps simplify when q ≤ p; that is,
whenthenumberoflatentvariablesisatmostthenumberofobservedvariables, seeSection4.3.
We test our algorithms on synthetic data in Section 4.4.
4.1 Recovery of intervention targets, permutation, and scaling
The algorithm input is the d-th order cumulants κ (X(k)) as in (10), for k ∈ K ∪{0} ranging
d
over contexts and a fixed d ≥ 3. The input tensors are either exact (population cumulants) or
approximate (sample cumulants).
For our fixed d, we assume that the decomposition in (10) is unique, that κ (ε(k)) ̸= 0 for all
d i
k and all i ∈ [q], and that κ (ε(i k)) ̸= ±1 for all k. This is the same assumption as appears
d i
temporarily in the proof of Propkosition 2.3. The assumption is not necessary, since one can
combine information from multiple higher-order cumulants, but it helps our exposition, as in
the proof of Proposition 2.3. In our experiments, we consider d = 3 and d = 4.
Tensor decomposition recovers the matrices A(k) up to permutation and scaling, by Proposi-
tion 2.1. Tensor decomposition thus recovers a set of matrices {A(k)D(k)P(k) : k ∈ K ∪ {0}}
for unknown scaling matrices D(k) and unknown permutations P(k). In practice, any numer-
ical tensor decomposition algorithm can be used for this step. We use the subspace power
23method [KP19] or simultaneous diagonalization [Har70] when q ≤ p. We can assume without
loss of generality that D(0) = P(0) = I, see Proposition 2.5. Then the other scaling matrices
D(k) have all entries ±1 except one, by (9) of Proposition 2.3. There is one entry that is not
±1, which corresponds to the intervention target of context k.
To find the permutation, we consider the difference A(0) − A(k)D(k)P(k), as D(k) varies over
diagonal matrices with diagonal ±1 and P(k) varies over permutation matrices. That is, the
product D(k)P(k) is a signed permutation matrix. The rank of the difference equals one if and
only if we have the correct sign and order, see Corollary 2.10. This suggests an algorithm: for
all q × q signed permutation matrices Q (of which there are 2q × q!) compute A(k)D(k)P(k)Q
and compute the second largest eigenvalue of the matrix A(0) −A(k)D(k)P(k)Q. Choose Q for
which this eigenvalue is smallest. This Q is (P(k))⊤, up to sign, where the sign gives all but
entry D(k) of D(k). See Algorithm 1.
i ,i
k k
To find the remaining entry of D(k), we compare the columns of A(0) and A(k)D(k). The only
column that differs between the two matrices is the i -th column. The i -th columns of the
k k
two matrices are collinear, with scaling D(k) . This recovers the intervention target i and the
i ,i k
k k
scaling matrix D(k). See Algorithm 2.
A faster way approach to find the intervention targets, permutation, and scaling could be to im-
plement Proposition 2.6. One can compare each column of A(0) to each column of A(k)D(k)P(k),
e.g. by projecting a column v of one matrix onto another column v of the other. Each time,
1 2
the residue of the projection ∥v −π (v )∥ and the scaling 1 ∥π (v )∥ can be stored and
thresholds can be used to
decid1
e
wh⟨ iv c2 h⟩ nu1
merical values
are∥ zv e2r∥
o
o⟨ rv2 ±⟩ 1.1
Such a procedure is
numericallysensitiveandinfluencedbythethreshold. Thethresholddeterminestheassignment
of intervention target, and we want the |K| intervention targets to cover all latent nodes. One
must choose a threshold that gives such an assignment. We leave this for future work.
4.2 Recovery of parameters
Once the intervention targets, permutation, and scaling are recovered, using Section 4.1, we
have matrices A(k) = F(I − Λ(k))−1 for k = 0,...,q. We can relabel contexts so that the
intervention target of the k-th context is k. We construct (I −Λ(0))−1 as

1 i = j,

(I −Λ(0))− i,j1 = A( 10 ,j)−A( 1i ,)
j i ̸= j,

A(0)
1,i
following the proof of Theorem 1.5. We invert this matrix to find Λ(0). This is Algorithm 3.
Finally, we recover F using F = A(0)(I −Λ(0)). One can compare the products A(k)(I −Λ(k))
for different contexts k to test the goodness of fit of the LCD model. In theory, these should
all return the same mixing matrix F.
4.3 The injective case
Restricting to the case q ≤ p allows for simplifications, cf. Remarks 2.11, 3.5, and 3.12. First,
the tensor decomposition step can achieved using simultaneous diagonalization [Har70]. We
24explain how to recover the intervention targets, permutation, and scaling in this setting. When
q ≤ p the Moore-Penrose pseudo-inverse satisfies
C(k) := (cid:0) F(I −Λ(k))−1D(k)P(k)(cid:1)+ = (P(k))⊤(D(k))−1(I −Λ(k))H, (24)
where H = F+. In particular, C(0) = (I−Λ(0))H. Let (c(k))ℓ denote the ℓ-th row of C(k). Then
we have the following result, in the same spirit as Proposition 2.6.
Proposition 4.1. Consider LCD under Assumption 1.2 where q ≤ p. Fix k ∈ K and let σ be
the permutation associated to the permutation matrix P(k). Then
(c(0))ℓ = (c(k))σ(ℓ)
if and only if ℓ ̸= i , where i is the target of the k-th intervention.
k k
Proof. We have formulae
 
(cid:88) 1 (cid:88)
(c(0))ℓ = hℓ − λ(0)hj, (c(k))σ(ℓ) = hℓ − λ(k)hj ,
ℓ,j D(k) ℓ,j
j∈pa(ℓ) ℓ,ℓ j∈pa(ℓ)
by (24). If ℓ ̸= i , then D(k) = 1 and λ(k) = λ(0) for all j ∈ [q]. Hence these two expressions
k ℓ,ℓ ℓ,j ℓ,j
coincide. If ℓ = i then under the genericity assumption we have (c(0))ℓ ̸= (c(k))σ(ℓ).
k
Proposition 4.1 enables us to find the intervention target i and the permutation matrix P(k),
k
as follows. For sufficiently general parameters Λ(0) and Λ(k) and error distribution ε(k), the
rows (c(0))i k and (c(k))σ(i k) differ. Hence, i is the index of the row of C(0) without a match
k
in C(k). We recover P(k) by matching the remaining rows: if ℓ ̸= i , then P(k) = 1, where
k ℓ,j
(c(0))ℓ = (c(k))j. This finds all but one row of P(k). It has a unique completion to a permutation
matrix: P(k) = 1, where (c(k))j is the row of C(k) without a match in C(0). See Algorithms 4
i ,j
k
and 5. Algorithm 6 recovers the scalings D(k).
Wenowexplainhowtorecovertheparameters. Inlightoftheabove,wehavematrices(A(k))+ =
(I −Λ(k))H. We find H = F+ as follows. The i -th row of (A(k))+ has entries
k
(cid:88)
(A(k))+ = (I −Λ(k)) H = H .
i k,j i k,ℓ ℓ,j i k,j
ℓ∈[q]
This is Algorithm 7. To find Λ(0), we use
(A(0))+H+ = (I −Λ(0))HH+ = (I −Λ(0)).
Following the initial tensor decomposition, the time complexity of this algorithm is determined
by the time required for the alignment and to calculate the pseudo-inverses of the products.
The former takes time O(q3p) and the latter O(p2q2), so the overall runtime is O(q2pmax(p,q)).
This improves on the algorithm in [SSBU23] provided we ignore the time taken to construct
and decompose the higher-order cumulants.
254.4 Numerical experiments
We test our algorithms on synthetic data. The general procedure for any (p,q) is implemented
in Algorithms 1-3 in Appendix A and the injective case (q ≤ p) is implemented in Algorithms
4-7. For the general setting, we use d = 4, since we use the subspace power method for tensor
decomposition and it requires input of even order. For the injective case, we study d = 3.
We sample graphs using the Python package causaldag [Cha18]. It extends the Erdős–Rényi
model [Ren59] to DAGs: given an edge density ρ, the edge i → j is added to the graph with
probability ρ, and if and only if i > j. We fix ρ = 0.75, sample the entries of F+ independently
from Unif([−2,2]), and the non-zero entries of Λ(0) independently from Unif(±[0.25,1]), as in
[SSBU23]. We fix p = 5 and vary q from 2 to 7. For each value of q, we generate 500 models
and calculate the relative Frobenius error for the recovery of F and Λ(0), which is 1 ∥M(cid:102)−M∥,
∥M∥
where ∥·∥ denotes the Frobenius norm, M is the true matrix, and M(cid:102) is the recovered matrix.
We also calculate DAG recovery error, as follows. A penalty of 1 is incurred if the algorithm
recovers a non-existent edge or misses an existing one, while recovering an edge in the wrong
direction incurs a penalty of 2. Then we sum the penalty over all edges.
Figure 2: MedianrelativeFrobeniuserrorintherecoveryofF (left)andΛ(0) (right)
when p = 5. Note the logarithmic scale on the y-axis for all positive y-coordinates.
The five algorithms are: (i) Covariance, the algorithm used in [SSBU23] to recover
the parameters from the covariance matrices of X(k) (blue), (ii) Tensor (general),
the general algorithm with cumulants as input (orange), (iii) Matrix (general), the
general algorithm with factor matrices as input (green), (iv) Tensor (injective), the
injective algorithm with cumulants as input (red), and (v) Matrix (injective), the
injective algorithm with factor matrices as input (purple). For DAG recovery, all
methods recovered the correct DAG every time, except the general tensor method
when q ≥ 6, which had a median DAG error of 3.6 for q = 6 and 4.1 for q = 7.
We plot the median error in recovering F, Λ(0) in Figure 2. A significant portion of the error
is due to the tensor decomposition step, so we display the error of the recovery starting from
the cumulants κ (X(k)) as well as directly from the factor matrices A(k)D(k)P(k), as if these
d
had been recovered perfectly from tensor decomposition. We used the same 500 models for
algorithms (ii), (iii), (iv), and (v), but a different set of 500 models for algorithm (i), since
model generation is part of the pipeline in [SSBU23].
26Figure 3: Median relative Frobenius error in the recovery of F (left) and Λ(0)
(right) when p = 10 and q ≤ p. Note the logarithmic scale on the y-axis for all
positive y-coordinates. The three algorithms are: (i) Covariance, the algorithm
used in [SSBU23] to recover the parameters from the covariance matrices of X(k)
(blue), (ii) Tensor (injective), the injective algorithm with cumulants as input (red),
and (iii) Matrix (injective), the injective algorithm with factor matrices as input
(purple). For DAG recovery, all methods recovered the correct DAG every time.
We can push our computation further when specializing to the injective case q ≤ p, as shown
in Figure 3. Here we fix p = 10 and plot again the median error in recovering F,Λ(0), using the
same approach as in the general case.
5 Outlook
We have studied the identifiability of linear causal disentanglement using tensor decomposition
of higher-order cumulants. We view the parameters compatible with a given model as the
solution space to a system of equations. Identifiability holds when the space has dimension
zero, and can be achieved using perfect interventions. Here, we give an algorithm to recover the
parameters. For soft interventions, we recover a compatibility class of graphs and parameters.
We conclude with some open problems for future investigation.
On the theoretical side, the first question is of combinatorial nature. The definition of soft(G)
in Definition 3.14 involves ranks of matrices. These rank conditions encode information about
the paths in G. This suggests the following problem.
Problem 5.1. Find a combinatorial description of soft(G), based on the structure of G.
Higher-order cumulants can reduce the degree of the solution space of parameters, as compared
to covariance matrices, see Proposition 3.3. An open problem is whether they restricts the set
of compatible DAGs.
Problem 5.2. Letsoft (G)denotethegraphsG′ forwhichthereexistparametersF,Λ(k) defined
2
according to G′ such that the covariance matrix coincides with the covariance matrix of a model
27with latent DAG G. Are the following containments strict in general
soft(G) ⊂ soft (G) ⊂ {G′|G′ = G}?
2
Under soft interventions, the space of parameters compatible with a given model is linear and
positive dimensional, by Theorem 1.6. It is then natural to ask for the best solution in this
space, for an appropriate notion of best. This would give a choice of unique parameters under
soft interventions.
Finally, our assumptions require that all the latent error distributions are non-Gaussian. The
results may extend to the case where some are Gaussian, cf. [WS24].
On the algorithmic side, there are multiple possible improvements. The tensor decomposition
contributes significantly to the error in the recovered parameters, see Figure 2. Other tensor
decomposition algorithms might give more accurate output. One could test our algorithm
starting from the factor matrices plus random noise, to study the extent to which our algorithm
would work with a sufficiently accurate tensor decomposition. Next, one could implement a
greedy search over permutations and signs to speed up the recovery of P(k) and D(k). Finally,
it would be interesting to study the robustness to non-linearity in the latent space (e.g., Z =
(I −Λ)−1ε+αε2 for small α ∈ R) or in the mixing map (e.g., X = FZ +αZ2, where Z2 is a
vector with entries Z Z for all i,j ∈ [q] and α ∈ R small).
i j
References
[AGH+14] Animashree Anandkumar, Rong Ge, Daniel J Hsu, Sham M Kakade, Matus Tel-
garsky, et al. Tensor decompositions for learning latent variable models. J. Mach.
Learn. Res., 15(1):2773–2832, 2014.
[AHB21] Kartik Ahuja, Jason Hartford, and Yoshua Bengio. Properties from mechanisms:
An equivariance perspective on identifiable representation learning, 2021.
[AHZ21] Jeffrey Adams, Niels Hansen, and Kun Zhang. Identification of partially observed
linearcausalmodels: Graphicalconditionsforthenon-gaussianandheterogeneous
cases. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,
pages 22822–22833. Curran Associates, Inc., 2021.
[AMWB23] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional
causalrepresentationlearning. InProceedings of the 40th International Conference
on Machine Learning, ICML’23. JMLR.org, 2023.
[BCV14] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A
review and new perspectives, 2014.
[BJD+07] OlivierBerne, CJoblin, YDeville, JDSmith, MRapacioli, JPBernard, JThomas,
WReach, andAAbergel. Analysisoftheemissionofverysmalldustparticlesfrom
spitzer spectro-imagery data using blind signal separation methods. Astronomy &
Astrophysics, 469(2):575–586, 2007.
28[Bol89] Kenneth A Bollen. Structural equations with latent variables, volume 210. John
Wiley & Sons, 1989.
[BRR+23] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard
Schölkopf, and Pradeep Ravikumar. Learning linear causal representations from
interventions under general nonlinear mixing. In A. Oh, T. Naumann, A. Glober-
son, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information
Processing Systems, volume 36, pages 45419–45462. Curran Associates, Inc., 2023.
[BWNR23] Simon Bing, Jonas Wahl, Urmi Ninad, and Jakob Runge. Invariance & causal rep-
resentation learning: Prospects and limitations. In Causal Representation Learn-
ing Workshop at NeurIPS 2023, 2023.
[Cha18] Chandler Squires. causaldag: creation, manipulation, and learning of causal
models. https://github.com/uhlerlab/causaldag, 2018.
[CJ10] Pierre Comon and Christian Jutten. Handbook of Blind Source Separation: Inde-
pendent component analysis and applications. Academic press, 2010.
[Com94] Pierre Comon. Independent component analysis, a new concept? Signal Process-
ing, 36:287–314, 1994.
[COV17] Luca Chiantini, Giorgio Ottaviani, and Nick Vannieuwenhoven. On generic iden-
tifiability of symmetric tensors of subgeneric rank. Transactions of the American
Mathematical Society, 369(6):4021–4042, 2017.
[CXG+19] Ruichu Cai, Feng Xie, Clark Glymour, Zhifeng Hao, and Kun Zhang. Triad
constraints for learning causal structure of latent variables. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019.
[DEF+24] Wolfram Decker, Christian Eder, Claus Fieker, Max Horn, and Michael Joswig,
editors. The Computer Algebra System OSCAR: Algorithms and Examples. Algo-
rithms and Computation in Mathematics. Springer, 2024.
[DLCC07] Lieven De Lathauwer, Josphine Castaing, and Jean-Franois Cardoso. Fourth-
order cumulant-based blind identification of underdetermined mixtures. IEEE
Transactions on Signal Processing, 55(6):2965–2973, 2007.
[DPL+16] Atray Dixit, Oren Parnas, Biyu Li, Jenny Chen, Charles P Fulco, Livnat Jerby-
Arnon, Nemanja D Marjanovic, Danielle Dionne, Tyler Burks, Raktima Ray-
chowdhury, et al. Perturb-seq: dissecting molecular circuits with scalable single-
cell rna profiling of pooled genetic screens. cell, 167(7):1853–1866, 2016.
[EGS05] Frederick Eberhardt, Clark Glymour, and Richard Scheines. On the number of ex-
periments sufficient and in the worst case necessary to identify all causal relations
among n variables. In Proceedings of the Twenty-First Conference on Uncertainty
in Artificial Intelligence, pages 178–184, 2005.
[EK04] Jan Eriksson and Visa Koivunen. Identifiability, separability, and uniqueness of
linear ica models. IEEE signal processing letters, 11(7):601–604, 2004.
29[Har70] Richard A. Harshman. Foundations of the parafac procedure: Models and condi-
tions for an "explanatory" multimodal factor analysis. UCLA Working Papers in
Phonetics, 16:1–84, 1970.
[HHS15] YoniHalpern, StevenHorng, andDavidSontag. Anchoreddiscretefactoranalysis.
arXiv:1511.03299, 2015.
[HP99] AapoHyvärinenandPetteriPajunen. Nonlinearindependentcomponentanalysis:
Existence and uniqueness results. Neural Networks, 12:429–439, 1999.
[JA23] Yibo Jiang and Bryon Aragam. Learning nonparametric latent causal graphs
with unknown interventions. In A. Oh, T. Naumann, A. Globerson, K. Saenko,
M. Hardt, and S. Levine, editors, Advances in Neural Information Processing
Systems, volume 36, pages 60468–60513. Curran Associates, Inc., 2023.
[JMM+01] T-P Jung, Scott Makeig, Martin J McKeown, Anthony J Bell, T-W Lee, and
Terrence J Sejnowski. Imaging brain dynamics using independent component
analysis. Proceedings of the IEEE, 89(7):1107–1122, 2001.
[Jut87] Christian Jutten. Calcul neuromimétique et traitement du signal: analyse en com-
posantes indépendantes. PhD thesis, Grenoble INPG, 1987.
[KKMH20] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Vari-
ational autoencoders and nonlinear ica: A unifying framework. In International
Conference on Artificial Intelligence and Statistics,pages2207–2217.PMLR,2020.
[KP19] Joe Kileel and Joao M. Pereira. Subspace power method for symmetric tensor
decomposition and generalized PCA. arXiv preprint arXiv:1912.04007, 2019.
[KRRA21] Bohdan Kivva, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam.
Learning latent causal graphs via mixture oracles, 2021.
[KSB24] Armin Kekić, Bernhard Schölkopf, and Michel Besserve. Targeted reduction of
causal models. In ICLR 2024 Workshop on AI4DifferentialEquations In Science,
2024.
[Lan11] Joseph M Landsberg. Tensors: geometry and applications: geometry and applica-
tions, volume 128. American Mathematical Soc., 2011.
[LBL+19] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly,
Bernhard Schölkopf, and Olivier Bachem. Challenging common assumptions in
the unsupervised learning of disentangled representations, 2019.
[LLS+24] SébastienLachapelle, PauRodríguezLópez, YashSharma, KatieEverett, RémiLe
Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Nonparametric partial dis-
entanglement via mechanism sparsity: Sparse actions, interventions and sparse
temporal dependencies, 2024.
[LMMLJ23] Sébastien Lachapelle, Divyat Mahajan, Ioannis Mitliagkas, and Simon Lacoste-
Julien. Additive decoders for latent variables identification and cartesian-product
extrapolation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
S.Levine, editors, Advances in Neural Information Processing Systems, volume36,
pages 25112–25150. Curran Associates, Inc., 2023.
30[LZG+23] Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton
van den Hengel, Kun Zhang, and Javen Qinfeng Shi. Identifying weight-variant
latent causal models. arXiv:2208.14153, 2023.
[LZG+24a] Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton
van den Hengel, Kun Zhang, and Javen Qinfeng Shi. Identifiable latent neural
causal models, 2024.
[LZG+24b] Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton
vandenHengel, KunZhang, andJavenQinfengShi. Identifiablelatentpolynomial
causalmodelsthroughthelensofchange. InThe Twelfth International Conference
on Learning Representations, 2024.
[M2] Daniel R. Grayson and Michael E. Stillman. Macaulay2, a software system for
research in algebraic geometry. Available at http://www2.macaulay2.com.
[Mar39] Józef Marcinkiewicz. Sur une propriété de la loi de Gauss. Mathematische
Zeitschrift, 44(1):612–618, 1939.
[McC18] Peter McCullagh. Tensor methods in statistics: Monographs on statistics and
applied probability. Chapman and Hall/CRC, 2018.
[MHM+16] Nicolai Meinshausen, Alain Hauser, Joris M. Mooij, Jonas Peters, Philip Ver-
steeg, and Peter Bühlmann. Methods for causal inference from gene perturbation
experiments and validation. Proceedings of the National Academy of Sciences,
113(27):7361–7368, 2016.
[MSWB22] Gemma Elyse Moran, Dhanya Sridhar, Yixin Wang, and David Blei. Identifiable
deep generative models via sparse decoding. Transactions on Machine Learning
Research, 2022.
[OSC24] OSCAR – Open Source Computer Algebra Research system, Version 1.0.0. Avail-
able at https://www.oscar-system.org, 2024.
[Pea00] Judea Pearl. Models, reasoning and inference. Cambridge, UK: CambridgeUni-
versityPress, 19(2):3, 2000.
[Ren59] Erdos Renyi. On random graph. Publicationes Mathematicate, 6:290–297, 1959.
[SBJRH05] Brisa N Sánchez, Esben Budtz-Jørgensen, Louise M Ryan, and Howard Hu. Struc-
tural equation models: a review with applications to environmental epidemiology.
Journal of the American Statistical Association, 100(472):1443–1455, 2005.
[Shi14] Shohei Shimizu. Lingam: Non-gaussian methods for estimating causal structures.
Behaviormetrika, 41:65–98, 2014.
[SLB+21] Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal
Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation
learning. Proceedings of the IEEE, 109(5):612–634, 2021.
[SMA+24] Kian Shamsaie, Stathis Megas, Hesam Asadollahzadeh, Sarah A Teichmann, and
Mohammad Lotfollahi. Disentangling covariates to predict counterfactuals for
single-cell data, 2024.
31[SPP+05] Karen Sachs, Omar Perez, Dana Pe’er, Douglas A Lauffenburger, and Garry P
Nolan. Causal protein-signaling networks derived from multiparameter single-cell
data. Science, 308(5721):523–529, 2005.
[SSBU23] Chandler Squires, Anna Seigal, Salil Bhate, and Caroline Uhler. Linear causal
disentanglement via interventions. In Proceedings of the 40th International Con-
ference on Machine Learning, ICML’23. JMLR.org, 2023.
[SSGS06] Ricardo Silva, Richard Scheine, Clark Glymour, and Peter Spirtes. Learning the
structure of linear latent variable models. Journal of Machine Learning Research,
7(8):191–246, 2006.
[Sul18] Seth Sullivant. Algebraic statistics, volume 194. American Mathematical Soc.,
2018.
[TLHD+17] Sofia Triantafillou, Vincenzo Lagani, Christina Heinze-Deml, Angelika Schmidt,
Jesper Tegner, and Ioannis Tsamardinos. Predicting causal relationships from
biological data: Applying automated causal discovery on mass cytometry data of
human immune cells. Scientific Reports, 7(1):12724, 2017.
[VAS+23] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, Abhishek Kumar, and
Ali Tajer. Score-based causal representation learning with interventions, 2023.
[VAST23] Burak Varici, Emre Acartürk, Karthikeyan Shanmugam, and Ali Tajer. Score-
based causal representation learning from interventions: Nonparametric identifia-
bility. In Causal Representation Learning Workshop at NeurIPS 2023, 2023.
[VAST24] Burak Varici, Emre Acartürk, Karthikeyan Shanmugam, and Ali Tajer. Gen-
eral identifiability and achievability for causal representation learning. In Sanjoy
Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th
International Conference on Artificial Intelligence and Statistics, volume 238 of
Proceedings of Machine Learning Research, pages 2314–2322. PMLR, 02–04 May
2024.
[vKBW+23] Julius von Kügelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin
Kekić, Elias Bareinboim, David Blei, and Bernhard Schölkopf. Nonparametric
identifiability of causal representations from unknown interventions. In A. Oh,
T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Ad-
vances in Neural Information Processing Systems, volume 36, pages 48603–48638.
Curran Associates, Inc., 2023.
[VP22] TS Verma and Judea Pearl. Equivalence and synthesis of causal models. In
Probabilistic and Causal Inference: The Works of Judea Pearl, page 221–236.
Association for Computing Machinery, New York, NY, USA, 1 edition, 2022.
[WD20] Y Samuel Wang and Mathias Drton. High-dimensional causal discovery under
non-gaussianity. Biometrika, 107(1):41–59, 2020.
[WS24] Kexin Wang and Anna Seigal. Identifiability of overcomplete independent com-
ponent analysis. arXiv:2401.14709, 2024.
32[XCH+20] F. Xie, R. Cai, B. Huang, C. Glymour, Z. Hao, and K. Zhang. Generalized inde-
pendent noise condition for estimat- ing latent variable causal graphs. Advances
in Neural Information Processing Systems, 33:14891–14902, 2020.
[XH24] JohnnyXiandJasonHartford. Propensityscorealignmentofunpairedmultimodal
data, 2024.
[XHC+22] Feng Xie, Biwei Huang, Zhengming Chen, Yangbo He, Zhi Geng, and Kun Zhang.
Identification of linear non-Gaussian latent hierarchical structure. In Kamalika
Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan
Sabato, editors, Proceedings of the 39th International Conference on Machine
Learning, volume 162 of Proceedings of Machine Learning Research, pages 24370–
24387. PMLR, 17–23 Jul 2022.
[YLC+21] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun
Wang. CausalVAE: Disentangled representation learning via neural structural
causal models. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 9593–9602, 2021.
[ZSG+23] Jiaqi Zhang, Chandler Squires, Kristjan Greenewald, Akash Srivastava,
KarthikeyanShanmugam, andCarolineUhler. Identifiabilityguaranteesforcausal
disentanglement from soft interventions. arXiv:2307.06250, 2023.
[ZSS+22] Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and
Wieland Brendel. Contrastive learning inverts the data generating process, 2022.
[ZXNZ24] Kun Zhang, Shaoan Xie, Ignavier Ng, and Yujia Zheng. Causal representation
learning from multiple distributions: A general setting, 2024.
A Pseudocode
We provide pseudocode for the algorithms in Section 4. Their implementations are available
at https://github.com/paulaleyes14/linear-causal-disentanglement-via-cumulants.
Below, the i-th row of a matrix M is denoted by mi and the i-th column by m .
i
Algorithms 3 and 7 below work if the set of intervention targets coincides with the set of latent
variables. In theory, this is true by our assumptions. In practice, the algorithm could assign the
wrong target to an intervention due to numerical errors. When implementing the algorithm,
we force the interventions to be on distinct nodes.
A.1 General case
Algorithm 1 Recovery of the permutation matrix (recover_perm)
1: Input: M = A(0) and M(k) = A(k)D(k)P(k).
2:
Output: P(k), the permutation matrix encoding the relabeling of the latent nodes in the
context corresponding to an intervention at node i .
k
3: q ←number of columns of M
334: perm ← set of all q ×q permutation matrices with entries ±1
5: σ ← maximum float
6: P ← None
7: for mat in perm do
8: newmat ← M −M(k) ·mat
9: ss ← second largest abs(singular value) of newmat
10: if ss < σ then
11: σ ← ss
12: P ← mat⊤
13: else
continue
14:
15: end if
16: end for
17: return P
Algorithm 2 Recovery of the intervention target and scaling (recover_target_scaling)
1: Input: M = A(0), M(k) = A(k)D(k)P(k) and a threshold thr.
2: Output: the target of the k-th intervention i and the diagonal matrix D(k).
k
3: q ← number of columns of M
4: D ← I
q×q
5: P ← recover_perm(M,M(k))
6: N ← M(k)P⊤
7: scalings ← list()
8: indices ← list()
9: for i = 1 to q do
10: v ← project ni onto mi
11: if |v −ni| < thr then
12: Add v[1]/mi[1] to scalings
13: Add i to indices
14: end if
15: end for
16: d ← largest entry of scalings
17: i ← indices(index of d)
k
18: D[i ,i ] ← d
k k
19: return i , D
k
Algorithm 3 Recovery of Λ(0) (recover_Lambda)
1: Input: M = A(0), M(k) = A(k)D(k)P(k) for all k ∈ [q], and a threshold thr.
2:
Output: Λ(0).
3: q ← number of columns of M
4: L ← I
q×q
5: for k = 1 to q do
346: P ← recover_perm(M,M(k))
7: (i ,D) ← recover_target_scaling(M,M(k),thr)
k
8: A = M(k) P⊤ inverse(D)
9: for j = 1 to q do
10: if j ̸= i then
k
11: L[i ,j] ← M[1,j]−A[1,j]
k M[1,i ]
k
12: end if
13: end for
14: end for
15: return I −inverse(L)
q×q
A.2 Injective case
Algorithm 4 Recovery of the intervention target (recover_target)
1: Input: C = C(0) and C(k) = (P(k))⊤(D(k))−1(I −Λ(k))H.
2: Output: (i , j ) such that i is the intervention target of the k-th context, and P(k) = 1.
k k k i ,j
k k
3: q ←number of rows of C
4: matched ← set()
obs
5: matched ← set()
int
6: for i = 1 to q do
7: if ci has matching row in C(k) then
8: j ← index of matching row
9: Add i to matched
obs
10: Add j to matched
int
11: end if
12: end for
13: i ← [q]\matched
k obs
14: j ← [q]\matched
k int
15: return (i ,j )
k k
Algorithm 5 Recovery of the permutation matrix (recover_perm)
1: Input: C = C(0) and C(k), recovered from the context with intervention target i .
k
2:
Output: the permutation matrix P(k).
3: q ←number of rows of C
4: P ← 0
q×q
5: (i ,j ) ← recover_target(C,C(k))
k k
6: P[i ,j ] ← 1
k k
7: for i = 1 to q do
8: if i = i then
k
35continue
9:
10: else
11: j ← index of matching row in C(k) to ci
12: P[i,j] ← 1
13: end if
14: end for
15: return P
Algorithm 6 Recovery of the scaling (recover_scaling)
1: Input: C = C(0) and C(k).
2:
Output: the diagonal matrix D(k).
3: q ←number of rows of C
4: D ← I
q×q
5: (i ,j ) ← recover_target(C,C(k))
k k
6: P ← recover_perm(C,C(k))
7: B(0) ← pseudoinverse(PC(0))
8: B(k) ← pseudoinverse(PC(k))
9: v ← project b(k) onto b(0)
i i
k k
10: D[i ,i ] ← v[1]/b(0) [1]
k k i
k
11: return D
Algorithm 7 Recovery of F = H+ (recover_F)
1: Input: C = C(0) and C(k) for all k ∈ [q].
2: Output: F such that C(0) = (I −Λ(0))H and H = F+.
3: q ← number of rows in C
4: for k = 1 to q do
5: (i ,j ) ← recover_target(C,C(k))
k k
6: P ← recover_perm(C,C(k))
7: D ← recover_scaling(C,C(k))
8: A = PDC(k)
9: hi k = ai k
10: end for
11: return pseudoinverse(H)
36