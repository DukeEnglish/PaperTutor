AMappingStrategyforInteractingwithLatentAudioSynthesisUsingArtistic
Materials
SHUOYANGZHENG,CentreforDigitalMusic,QueenMaryUniversityofLondon,UK
ANNAXAMBÓSEDÓ,CentreforDigitalMusic,QueenMaryUniversityofLondon,UK
NICKBRYAN-KINNS,CreativeComputingInstitute,UniversityoftheArtsLondon,UK
ThispaperpresentsamappingstrategyforinteractingwiththelatentspacesofgenerativeAImodels.Ourapproachinvolvesusing
unsupervisedfeaturelearningtoencodeahumancontrolspaceandmappingittoanaudiosynthesismodel’slatentspace.To
demonstratehowthismappingstrategycanturnhigh-dimensionalsensordataintocontrolmechanismsofadeepgenerativemodel,
wepresentaproof-of-conceptsystemthatusesvisualsketchestocontrolanaudiosynthesismodel.Wedrawonemergingdiscourses
inXAIxArtstodiscusshowthisapproachcancontributetoXAIinartisticandcreativecontexts,wealsodiscussitscurrentlimitations
andproposefutureresearchdirections.
ACMReferenceFormat:
ShuoyangZheng,AnnaXambóSedó,andNickBryan-Kinns.2024.AMappingStrategyforInteractingwithLatentAudioSynthesis
UsingArtisticMaterials.InProceedingsofExplainableAIfortheArtsWorkshop2024(XAIxArts2024).ACM,NewYork,NY,USA,4pages.
1 INTRODUCTION
ResearchinthefieldofeXplainableAIfortheArts(XAIxArts)examineshowAImodelscouldbemademoreunder-
standableforcreativeactivities[1].OneapproachtoincreasingtheexplainabilityofanAImodelistoprovidereal-time
interactionandfeedback[2].Thisapproachallowstheusertolearnhowthemodelworksandfindwaystointeract
withitbyexploringhowtheiractionsaffectthemodel’soutput[11].Inpractice,itinvolvesmappingahumancontrol
spacetotheparameterspaceoftheAImodelandprovidingreal-timeaudiofeedback.Forexample,mappingauser’s
spatialcoordinationinanimmersiveenvironmenttothelatentspaceofadeepaudiosynthesismodelsothattheuser
learnstheeffectofspecificlatentspaceaxesby‘walkingthrough’theenvironment[13].
ThedesignofthishumancontrolspaceisanopenquestioninthefieldofXAI[3],especiallyinacreativeandartistic
context.Whiletypicalapproacheshaveexploredparametriccontrolmechanismssuchasslidersand2Dpads[2,15],
itisimportanttoconsiderotherinteractionmediumsthatmaybemoreexpressiveincreativeandartisticcontexts
[5].Theseinteractionmediumscanbetheartisticmaterialsfromtheartists’performancespace[1],forexample,the
movementsofdancers[11],sketches[9],orelectromyographydataofperformers[14].However,materialsinanartistic
performancespacecanbehigh-dimensionalsensordata,whicharedifficulttomaptotheAImodelanduseasamedium
forinteraction[12].Totacklethis,weproposeamappingstrategythatencodesahumanperformancespaceandmapsit
tothelatentspaceofanAImodeltohelpusersinteractwiththemodel’sgenerationprocess.Weuseasketch-to-sound
controllerwedevelopedinapreviouswork[16]todemonstratehowthismappingstrategycanturnhigh-dimensional
sensordataintocontrolmechanismsofanAImodel.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenot
madeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforcomponents
ofthisworkownedbyothersthantheauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,toposton
serversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ManuscriptsubmittedtoACM
ManuscriptsubmittedtoACM 1
4202
luJ
5
]DS.sc[
1v97340.7042:viXra2 Zhengetal.
2 MAPPINGSTRATEGY
Materialsinanartisticperformancespacecanbehigh-dimensionalsensordatasuchassketches[9]ormovementsof
dancers[11].InordertomapartisticmaterialstothelatentspaceofanAImodel,atwo-stepapproachisneeded:(i)
featureextractiontechniquesarerequiredtoencodethesedataintorepresentativefeatures[12],(ii)theserepresentative
featuresneedtobemeaningfullymappedtothelatentspaceoftheAImodel.Therefore,asshowninFigure1,we
proposeusinglatentmapping[11]andInteractiveMachineLearning(IML)[7]toachievethesetwosteps.
Fig.1. Proposedmappingstrategy
Step1useslatentmapping[11],anunsupervisedfeaturelearningapproach,tolearnrepresentativefeaturesfroma
largecorpusofunlabelleddata.Atrainedlatentmappingmodelencodeshigh-dimensionalinputsintolower-dimensional
latentrepresentations,holdingrepresentativefeaturesoftheartist’scurrentmovements,sketches,orotherartistic
practices,whichcanbemappedtosoundcontrols.Commonlyusedunsupervisedfeaturelearningmethodsinclude
VariationalAutoEncoders[11]andunsuperviseddimensionalityreduction[12].Inpractice,trainingalatentmapping
modelrequirestheartisttocreateatrainingdatasetfortheirartisticmaterials.Inotherwords,theyneedtorecordafull
setofpossibleoutputsthattheymightproduceintheperformancespace[11].Forexample,trainingalatentmapping
modelforadancermayrequirerecordingbodyskeletaldatafromthedancerofalltheirpossibledancemovements,
includingintermediatetransitionsbetweentwomovements.
Step2usesInteractiveMachineLearning(IML)toconnecttheextractedfeaturesandtheAImodel’sparameters.
IMLallowsuserstoconstructmappingsbetweentwospacesusingafewpersonalisedtrainingexamples[7].Inthis
way,itcreatesamappingbetweenthehumanperformancespaceandtheAImodel’soutput,allowingartiststointeract
withtheAImodelusingdatafromtheperformancespace.Inpractice,theartistmanuallydefinesafewpaireddata
pointsintheperformancespaceandtheAImodel’sparameterspaceastrainingexamples.ToolssuchasWekinator[7]
canbeusedtofacilitatethisprocess.
3 DEMONSTRATION
Todemonstratehowthismappingstrategyworksinpractice,webuildonourpreviousworkonasketch-to-sound
controllerthatencodesvisualsketchesintosoundcontrols.Inthiswork,weuseitasthelatentmappingmodeltocontrol
alatentaudiosynthesismodel.Technicaldetailsforthesketch-to-soundcontrollercanbefoundin[16].WeuseRAVE
[4],avariationalautoencoderthatsynthesisesaudiofromalatentspaceinreal-time.WeuseaRAVEmodelpre-trained
onorganmusicrecordings[8]thatrunsinaMax4Live1device.Thepre-trainedmodelhas16latentspacedimensions,
andthesketch-to-soundmodelencodessketchesintoa32-dimensionallatentspace.Therefore,theIMLmodeltakes
a32-dimensionalinputandmapsittoa16-dimensionaloutput.Thecommunicationsbetweenthesketch-to-sound
model,theIMLmodel,andtheRAVEmodelareachievedthroughopensoundcontrol(OSC)2.
1https://maxforlive.com/library/
2https://ccrma.stanford.edu/groups/osc/index.html
ManuscriptsubmittedtoACMAMappingStrategyforInteractingwithLatentAudioSynthesisUsingArtisticMaterials 3
Fig.2. Twoscreenshotsofuserinterfacesshowingthesketchandtheaudiosynthesismodel’slatentspaces.Afulldemonstration
videocanbeviewedathttps://tinyurl.com/xaixarts24-mapping.
Figure2showstwoscreenshotsoftheuserinterface.Ineachscreenshot,theleftpanelallowstheusertointeract
withtheRAVEaudiosynthesismodelbysketchingonthecanvas,whiletherightpanelallowsuserstotraintheIML
model.WhenrecordingtrainingexamplesfortheIMLmodel,userscanstartbyeithermanuallyadjustingthelatent
parametersorrandomisingtheRAVEmodel’slatentparametersusingthe“randomise”button,thenclickingthe“record”
buttonanddrawingsketchesthatdepictthecurrentsoundproducedbytheRAVEmodel.AftertrainingtheIMLmodel,
userscanclickthe“run”buttonandinteractwiththeRAVEmodelbydrawingsketches.
4 LIMITATIONSANDFUTUREWORK
ThissectiondrawsonemergingdiscoursesinXAIxArts[3]todiscusstheproposedmappingstrategyintermsofthe
temporalandcross-modalaspects.Italsodiscussesthelimitationsandcontributeswithfutureresearchdirections.
First,thetemporalaspect.Real-timeinteractionandfeedbackinanXAImodelcanhelpartistsdiscoverandreuse
unexpectedphenomenafortheirartisticexpression[1].Forexample,inourdemonstrationvideo,movementssuchas
quicktransitionsfromshortlinestofullcirclestriggerinterestingsounds.Userscanreusethesesoundsbymemorising
thesketchtrajectoriesthattriggerthem,butisthereawaytoexplainthesetrajectoriestotheusersinthefirstplace?
TypicalXAIapproachesfocusonexplainingastaticstateofthemodelparameters[6],butexplainingthetrajectoriesof
sketchesfocusesonexplainingthetransition,ortheinterpolationfromonestatetoanother,whichinvolvestaking
accountofthetemporaldimension.ThisaspectofexplanationcontributestotheembodiedknowledgeabouttheAI
modelbecauseittellsartistshowtoactwithasystem,whichisanimportantcomponentinartisticandcreativecontexts
[10].
Second,thecross-modalaspect.Investigatinginteractionsthatinvolvecross-modalities,suchasshape-sound
associations,isusefultoexpandtheexplanationmediumsincreativeandartisticcontexts[5].Inlatentaudiomodels,
forexample,futureresearchcanexplorehowhuman-interpretablefeaturesinnon-audiodomainscanbeappliedto
audiolatentspace.TypicalXAIapproachesaimtoestablishmeaningfulfeaturesintheaudiodomain,andanumber
ofthesefeaturesalreadyrelyoncross-modalmetaphorstoexpressthesoniccharacteristics,suchas“brightness”,
“noisiness”,andsoon[9].Ourmappingstrategycanbeusedtostudyothercross-modalmappingsthatmightbeuseful
inXAI.Forexample,futureworkonoursketch-to-soundsystemcanexploreprogrammingandanimatingsketches
usinginterpretablefeaturessuchasrotation,zoom,orotheraffinetransformations,andthenuseourmappingstrategy
toconnectthesefeatureswiththeaudiolatentspacetodiscovermeaningfulshape-soundassociationsthatcanbeused
toexplaintheaudiosynthesismodel.
Themotivationforthemappingstrategypresentedinthispaperwastoencourageexplorationsonexplanation
mediumsthataresalientinthecreativeandartisticcontexts[5].However,weonlyusesketch-to-soundtodemonstrate
ManuscriptsubmittedtoACM4 Zhengetal.
howthemappingstrategyworks.Itwouldbenecessarytoexploreotherformsofcreativepracticesandgobeyond
sketchandsoundtogeneralisethediscussionintowidercreativeandartisticcontexts.
5 CONCLUSION
Real-timeinteractionandfeedbackincreasetheexplainabilityofanAImodelbysupportingusersinexploringhowtheir
actionsaffecttheaudiogeneration.Followingthisdirection,thispaperproposedamappingstrategyforinteracting
withlatentaudiosynthesismodelsusingdatafromanartisticperformancespace.Wehavedescribedhowthismapping
strategyworksinpracticeanddemonstrateditwithasystemthatcontrolsanAIaudiosynthesismodelusingvisual
sketches.Wediscussedthatfutureworkusingthismappingstrategyshouldtakeaccountofthetemporalandcross-
modalXAIaspects,anditisimportanttoexperimentwithhowthismappingstrategycanbeusedinotherformsof
creativepracticesinfuturework.
ACKNOWLEDGMENTS
ShuoyangZhengisaresearchstudentattheUKRICentreforDoctoralTraininginArtificialIntelligenceandMusic,
supportedbyUKResearchandInnovation[grantnumberEP/S022694/1].
REFERENCES
[1] NickBryan-Kinns.2024.ReflectionsonExplainableAIfortheArts(XAIxArts).Interactions31,1(Jan.2024),43–47. https://doi.org/10.1145/3636457
[2] NickBryan-Kinns,BerkerBanar,CoreyFord,CourtneyNReed,YixiaoZhang,SimonColton,andJackArmitage.2021.ExploringXAIfortheArts:
Explaininglatentspaceingenerativemusic.In1stWorkshoponeXplainableAIapproachesfordebugginganddiagnosis.
[3] NickBryan-Kinns,CoreyFord,AlanChamberlain,StevenDavidBenford,HelenKennedy,ZijinLi,WuQiong,GusG.Xia,andJebaRezwana.2023.
ExplainableAIfortheArts:XAIxArts.InCreativityandCognition.ACM,VirtualEventUSA,1–7. https://doi.org/10.1145/3591196.3593517
[4] AntoineCaillonandPhilippeEsling.2021. RAVE:Avariationalautoencoderforfastandhigh-qualityneuralaudiosynthesis. (2021). https:
//arxiv.org/abs/2111.05011arXiv:2111.05011.
[5] MichaelClemens.2023.Explainingthearts:Towardaframeworkformatchingcreativetaskswithappropriateexplanationmediums.InProceedings
ofThefirstinternationalworkshoponeXplainableAIfortheArts(XAIxArts).
[6] ShipiDhanorkar,ChristineT.Wolf,KunQian,AnbangXu,LucianPopa,andYunyaoLi.2021.Whoneedstoknowwhat,when?:Broadeningthe
ExplainableAI(XAI)DesignSpacebyLookingatExplanationsAcrosstheAILifecycle.InProceedingsofthe2021ACMDesigningInteractiveSystems
Conference(DIS’21).AssociationforComputingMachinery,NewYork,NY,USA,1591–1602. https://doi.org/10.1145/3461778.3462131
[7] RebeccaFiebrink,DanTrueman,andPerryR.Cook.2009.Ameta-instrumentforinteractive,on-the-flymachinelearning.InProceedingsofthe
InternationalConferenceonNewInterfacesforMusicalExpression.Zenodo,Pittsburgh,PA,UnitedStates,280–285. https://zenodo.org/record/1177513
[8] IntelligentInstrumentsLab.2023.rave-models(Revisionad15daf). https://doi.org/10.57967/hf/1235
[9] SebastianLobbersandGeorgeFazekas.2023.SketchSynth:Abrowser-basedsketchinginterfaceforsoundcontrol.ProceedingsoftheInternational
ConferenceonNewInterfacesforMusicalExpression(May2023),637–641. http://nime.org/proceedings/2023/nime2023_95.pdf
[10] TimMurray-BrowneandPanagiotisTigas.2021.Emergentinterfaces:Vague,complex,bespokeandembodiedinteractionbetweenhumansand
computers.AppliedSciences11,18(2021). https://doi.org/10.3390/app11188531
[11] TimMurray-BrowneandPanagiotisTigas.2021.Latentmappings:Generatingopen-endedexpressivemappingsusingvariationalautoencoders.In
InternationalConferenceonNewInterfacesforMusicalExpression. https://doi.org/10.21428/92fbeb44.9d4bcd4b
[12] GerardRoma,OwenGreen,andPierreAlexandreTremblay.2019.Adaptivemappingofsoundcollectionsfordata-drivenmusicalinterfaces.In
ProceedingsoftheInternationalConferenceonNewInterfacesforMusicalExpression,MarceloQueirozandAnnaXambóSedó(Eds.).UFRGS,Porto
Alegre,Brazil,313–318. https://doi.org/10.5281/zenodo.3672976ISSN:2220-4806.
[13] HugoScurtoandLudmilaPostel.2023.Soundwalkingdeeplatentspaces.InProceedingsofthe23rdInternationalConferenceonNewInterfacesfor
MusicalExpression(NIME’23).Mexico,Mexico. https://hal.science/hal-04108997
[14] HugoSilva,MichaelZbyszynski,AtauTanaka,andFedericoVisi.2020. Interactivemachinelearning:Strategiesforliveperformanceusing
electromyography.(2020). https://research.gold.ac.uk/id/eprint/28215/
[15] GabrielVigliensoniandRebeccaFiebrink.2023.Steeringlatentaudiomodelsthroughinteractivemachinelearning.InInProceedingsofthe14th
InternationalConferenceonComputationalCreativity.Ontario,Canada. https://computationalcreativity.net/iccc23/papers/ICCC-2023_paper_100.pdf
[16] ShuoyangZheng,BleizM.DelSette,CharalamposSaitis,AnnaXambó,andNickBryan-Kinns.Accepted.Buildingsketch-to-soundmappingwith
unsupervisedfeatureextractionandinteractivemachinelearning.ProceedingsoftheInternationalConferenceonNewInterfacesforMusicalExpression
(Accepted),Utrecht,Netherlands.
ManuscriptsubmittedtoACM