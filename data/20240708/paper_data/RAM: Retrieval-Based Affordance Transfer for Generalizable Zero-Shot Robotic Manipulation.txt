RAM: Retrieval-Based Affordance Transfer for
Generalizable Zero-Shot Robotic Manipulation
YuxuanKuang1,2∗,JunjieYe1∗,HaoranGeng2,3∗,JiagengMao1,CongyueDeng3,
LeonidasGuibas3,HeWang2,YueWang1
1UniversityofSouthernCalifornia,2PekingUniversity,3StanfordUniversity
∗Equalcontributions
Abstract: This work proposes a retrieve-and-transfer framework for zero-shot
roboticmanipulation,dubbedRAM,featuringgeneralizabilityacrossvariousob-
jects, environments, and embodiments. Unlike existing approaches that learn
manipulationfromexpensivein-domaindemonstrations, RAMcapitalizesona
retrieval-based affordance transfer paradigm to acquire versatile manipulation
capabilities from abundant out-of-domain data. First, RAM extracts unified af-
fordanceatscalefromdiversesourcesofdemonstrationsincludingroboticdata,
human-objectinteraction(HOI)data,andcustomdatatoconstructacomprehen-
siveaffordancememory. Thengivenalanguageinstruction,RAMhierarchically
retrievesthemostsimilardemonstrationfromtheaffordancememoryandtransfers
such out-of-domain 2D affordance to in-domain 3D executable affordance in a
zero-shotandembodiment-agnosticmanner. Extensivesimulationandreal-world
evaluationsdemonstratethatourRAMconsistentlyoutperformsexistingworks
indiversedailytasks. Additionally,RAMshowssignificantpotentialfordown-
streamapplicationssuchasautomaticandefficientdatacollection,one-shotvisual
imitation,andLLM/VLM-integratedlong-horizonmanipulation. Formoredetails,
pleasecheckourwebsiteathttps://yxkryptonite.github.io/RAM/.
Keywords: HierarchicalRetrieval,AffordanceTransfer,Zero-ShotRoboticMa-
nipulation,VisualFoundationModels
Open the
drawer please.
Hierarchical
Retrieval
…
Affordance
Transfer &
Lifting
Action
(a) The Pipeline of RAM. (b) Zero-Shot Manipulation.
(c) One-Shot Visual Imitation. (d) LLM/VLM Integration.
Figure1: (a)Weextractunifiedaffordancerepresentationfromin-the-wildmulti-sourcedemonstra-
tions,includingroboticdata,HOIdata,andcustomdata,toconstructalarge-scaleaffordancememory.
Given language instructions, RAM hierarchically retrieves and transfers the 2D affordance from
memoryandliftsitto3Dforroboticmanipulation.(b-d)Ourframeworkshowsrobustgeneralizability
acrossdiverseobjectsandembodimentsinvarioussettings.
4202
luJ
5
]OR.sc[
1v98640.7042:viXra1 Introduction
Alongstandinggoalinrobotlearningistodevelopageneralistrobotagentcapableofperforming
diverseroboticmanipulationtasksoncommonhouseholdobjectsinreal-worldsettings. Crucially,
suchanagentmustlearntogeneralizetomanipulateunseenobjectsinunseenenvironmentswith
unseenembodiments. Traditionalapproachestoachievingthisgoalofteninvolvemanuallycollect-
ingextensiveexpertdemonstrationsthroughteleoperation,scriptedpolicies,andsimilarmethods,
followedbyimitationlearningfromthesedatasets [1,2]. However,thesemethodsarecostlyand
labor-intensive,requiringsignificanthumaneffortandpriorknowledgeofvariousobjectsandtasks.
Asaresult,thescarcityofusablereal-worlddatachallengesthegeneralizationofpoliciestrainedon
thesedatasetstounseenobjectsorenvironments.
Ontheotherhand,apartfromreal-worldandsyntheticroboticdatasets[1,2,3,4,5,6,7,8]that
agentscandirectlylearnmanipulationpoliciesfrom,thereexistvastamountsofout-of-domaindata
richwithactionableknowledge,rangingfromHand-ObjectInteraction(HOI)data[9,10,11,12],to
Internet-scalevideosofdailyactivities,AI-generatedvideos[13],andevensketches[14]. Despite
effortstoleveragethesedatasourcesforroboticmanipulation[15,16,17], learningfromout-of-
domain samples remains elusive due to significant domain shifts. Addressing this issue requires
rethinkinghowtounifyandutilizetheactionableknowledgehiddenwithintheseheterogeneousand
oftennoisydatasources. Tothatend,weidentifythatthekeyistorepresenttheactionableknowledge
astransferrableaffordance,i.e.,‘where’and‘how’toact[18,19]. Byefficientlyextractingand
transferring these affordances from diverse data sources to our target domain, we can overcome
domain-specifichurdlesanddatascarcity. Inaddition,thefastdevelopmentofgeneralizablevisual
foundationmodels(VFMs),trainedonenormousInternetimages,offersastreamlinedsolutionto
seamlesslyconnectingdiversedatarealmswithourtargetdomain,facilitatingthedistillationofrich
affordancefromthesedatasourcestoenablegeneralizablezero-shotroboticmanipulation.
Therefore,weintroduceRAM,aRetrieval-basedAffordancetransferapproachforgeneralizable
zero-shotroboticManipulation. RAMutilizesaretrieve-and-transferparadigmforeverydayrobotic
tasksinazero-shotmanner. First,weextract2Daffordancefromdiversedatasourceslikerobotic
datasets,HOIdatasets,Internetimages,etc.,toconstructacomprehensiveaffordancememory. Upon
receiving a monocular RGBD observation of an unseen object in an unseen environment, RAM
employsaneffectivehierarchicalretrievalpipeline,whichselectsthemostsimilardemonstration
from the affordance memory and leverages VFMs to transfer the pixel-aligned 2D affordance to
thetargetunseendomain. Toconvertthe2Daffordancetoexecutableroboticactions,wefurther
developasampling-basedaffordanceliftingmodule. Thismoduleliftsthe2Daffordancetoa3D
representationthatincludesacontactpointandapost-contactdirection,whichisdirectlyexecutable
byvariousroboticsystemsusingoff-the-shelfgraspgenerators[20,21]andmotionplanners[22,23].
Extensiveexperimentsinbothsimulationandtherealworldshowthatourmethodsurpassesexisting
worksbyalargemargin(§4.3),therebyconfirmingitseffectivenessandsuperiorityinleveraging
largeout-of-domaindata. Notably,ourmethodisembodiment-agnosticanddata-efficient—qualities
thatpriormethodsarestrugglingwith—whichissubstantiatedbyabundantexperimentsconducted
acrossvariousroboticplatforms. Furthermore,weshowcasetheversatilityofourretrieval-based
frameworkthroughitsapplicationsinseveralkeyareasin§4.5,suchasautomaticandefficientdata
collection, one-shot visual imitation conditioned on human preference, and seamless integration
withLLMs/VLMsforlong-horizontaskswithfree-formhumaninstructions. Insummary,thekey
contributionsofourproposedRAMarethree-fold:
• Weproposearetrieval-basedaffordancetransferframeworkforzero-shotroboticmanipulation,
significantlyoutperformingpriorworks,bothinsimulationandtherealworld. Ourkeyinsightis
totransferaffordancesin2D,followedbyamoduletolifttheminto3Dfordirectexecution.
• Weproposeascalablemoduleforextractingunifiedaffordanceinformationfromdiverseout-of-
domainheterogeneousdataforretrieval. Thismodulecanpotentiallygeneralizetootherrobotic
tasksbeyondthemanipulationconsideredbythiswork.
• Ourpipelineenablesavarietyofintriguingdownstreamapplications,includingpolicydistillation,
one-shotvisualimitation,andLLM/VLMintegrationtofacilitatefutureresearch.
22 RelatedWorks
AffordanceforRoboticManipulation.Visualaffordance,whichindicateswhereandhowtointeract
withdiverseobjectsfromvisualinputs,playsanimportantroleinroboticmanipulationthanksto
itssimplicityandinterpretability. Pointcloud-basedmethods[18,19,24,25]focusonlearninga
point-wisescoremapastherepresentationofaffordanceandutilizingittoregressend-effectorposes.
However, these methods face severe sim-to-real gaps due to the depth noises of RGBD cameras
in the real world. Other series of works such as HOI-Forecast [15] and VRB [16] aim to learn
affordance from human videos in an end-to-end manner. However, the scarcity of training data
diversityhinderstheirgeneralizationtounseentasksandobjects. Comparedtotheseexistingworks,
ourmethodleveragestheunifiedaffordancerepresentationextractedfromdiversedatasourcesandis
thusgeneralizabletoawiderangeofunseenobjectsandenvironmentsintherealworld.
Zero-ShotRoboticManipulation. Duetotheissuesmentionedabove,howtomanipulatediverse
kindsofobjectsinazero-shotmannerremainsachallengingtopic.Existingworks[26,27,28,29,30,
31,32,33,34,35]attempttosolvethisproblembyleveragingthereasoningabilitiesofLLMs/VLMs.
However,theyheavilyrelyonpre-programmedheuristicactionprimitivestoexecutelow-leveltasks
like“graspingahandle”.Otherworks[36,37,38]leverageintermediaterepresentationsasconditions
tothepolicytoachievedata-efficientimitationlearningandcangeneralizetoin-domaintaskswithout
directdemonstrations. However,thesemethodsstillrequirecollectingin-domaindemonstrationfor
test-timetrainingandthuscannotbegeneralizedtoin-the-wilddomains. Comparedtotheseworks,
ourmethodistraining-free,neglectingtheneedforheuristicpoliciesortest-timetraining.
Learning from Demonstrations. Recent years have witnessed the rapid progress of imitation
learning (IL). Multiple teleoperation systems and algorithms [39, 40, 41, 42] have shown great
capabilitiestoimitatehumandemonstrationssmoothly. Therearealsomethods[43,1,2,44]that
leverageVLMsorco-traindatapriorstoreducetherequirementfordemonstrations. Otherseries
ofworks[45,46,47,48,49,50,51,52,53,54]learnmanipulationfromdemonstrationsbasedon
retrievingin-domaindemonstrationsforILordirecttrajectoryreplay. However,thesemethodsfail
togeneralizetothewildwithoutin-domaindemonstrations. RecentworkRobo-ABC[55]triesto
leverageCLIP[56]andStableDiffusion[57]toretrieveandtransfercontactpoints,butitislimited
toHOIandtable-topgraspingscenariosandcannotperformobjectmanipulationinthe3Dspace.
Ourmethod,instead,notonlymakesthemostofexistingdiverseout-of-distributiondatasourcesto
guideaffordancetransferin3Dbutalsogeneralizestoadiversesetofin-the-wildtasksinazero-shot
manner.
3 Method
Toaddressthein-domaindatascarcity,thecoreofRAMinvolvestransferringout-of-domainknowl-
edge to our unseen target domain. To achieve so, we introduce a retrieve-and-transfer approach
thatcapitalizesonvisualaffordance. Incontrasttoexistingworks[16,55]thatonlypredicts2D
affordanceinthepixelspace,RAMgeneratesexecutable3DaffordanceA3D =(c3D,τ3D)where
c3D,τ3D ∈R3arethe3Dcontactpointandthepost-contactdirectionrespectively.
First,RAMextractsaffordanceinformationfromdiverseout-of-domaindatasources,constructingan
affordancememoryM(§3.1)insourcedomainS. Then,givenamonocularRGBDimage(IT,D)
fromthenoveltargetdomainT alongwithalanguageinstructionL,RAMleveragesathree-step
hierarchicalretrievalprocedure(§3.2)toidentifythemostsimilardemonstrationfromM.Leveraging
VFMs,weestablishimage-paircorrespondencestotransferthe2Daffordancefromthedemonstration
insourcedomainS toourtargetdomainT (§3.3). Finally,throughasampling-basedmethod,the2D
affordanceinthetargetdomainisliftedbackto3D,resultinginthe3DaffordanceA3D(§3.4),which
canbedirectlyexecutedthroughgraspgenerators[20,21]andmotionplanners[22,23].
3.1 AffordanceMemory
Toachievegeneralizationacrossobjects,environments,andembodiments,RAMoptsforaunified
affordancerepresentationthatcanbeeasilyacquiredfromdiversedatasourcesinascalablemanner,
3constructingacomprehensiveaffordancememory,M,thatencompassesawiderangeofskillsand
objects. This memory integrates subsets of demonstrations from real-world or synthetic robotic
dataM ,HOIdataM ,andcustomdataM . Tounifyaffordanceinformationfromthesevaried
R H C
sources,eachentryintheaffordancememoryincludesanobject-centricRGBimageIS whichisthe
initialframeofinteraction,asetof2DwaypointsC = (c2D,c2D,c2D,···)indicatingthecontact
0 1 2
pointc2Dandpost-contacttrajectories,alongwithataskcategoryT expressedinnaturallanguage.
0
Therefore,theconstructedaffordancememorycanbedenotedas:
M=M ∪M ∪M ={(IS,T,C)|C =(c2D,c2D,c2D,···)}. (1)
R H C 0 1 2
Fordifferentdatasources,weemploydifferentstrategiestoextractandannotateaffordance:
Robotic Data. In the real world or sim-
ulators, obtaining camera parameters and
robotproprioceptionisstraightforward. This
allows us to seamlessly project the end-
effector’s3Dpositionontoa2Dimageplane.
Foraffordanceextraction,westartwiththe
firstframeofarolloutfortheimageIS,en-
(a) Robotic Data (b) HOI Data (c) Custom Data
suring the object is unobscured. Then, we
Figure2: Affordanceannotationofdemonstrations
identifytheframewhentherobot’sgripper
fromvariousdatasources. Weextractunifiedaffor-
closes,andusetheend-effector’s2Dposition danceinformationautomatically(forroboticandHOI
atthismomentasthecontactpointc2D. Sub- data)orwithminimalhumaneffort(forcustomdata).
0
sequentframesareusedtotracepost-contact
trajectories. Inourexperiments,weutilizeDROID[2]datasetforroboticdatasubsetconstruction.
HOIData. Humandemonstrations,comparedtoroboticdata,areoftenconsideredmoreintuitiveand
efficient. Foraffordanceextraction,webeginbyidentifyingandgroundingvideosegmentswhere
relevanteventsoccur. Similartotheapproachwithroboticdata, weselecttheinitialframefrom
eachsegmenttoserveasIS. Then,thecontactpointandpost-contacttrajectoriesaredeterminedby
averagingthehandkeypointsacrossframes. Wefoundthissimplestrategycangeneratereliablehand
waypoints. WeutilizetheannotationsofvideosegmentsandhandkeypointsfromtheHOI4D[12]
datasettoconstructthesubsetwhilethoseannotationscanalsobeeasilyacquiredthroughaction
segmentation[58]andhand-objectsegmentation[59]techniques.
CustomData. BeyondtheautomaticcollectionofaffordanceinformationfromroboticandHOI
data,wealsoconsidercustomdatathataremanuallyannotatedwithminimalhumaneffort. This
allowsustotailorourmanipulationstrategiesfornovelobjectsbyaugmentingtheaffordancememory
withnewlyannotateddemonstrations. Theannotationprocesscanberealizedbyselectingthestart
andendpointsonanRGBimage,followedbyautomaticinterpolationbetweenthetwopoints.
Notably,ouraffordancememoryisbuiltscalablyandsemi-automatically,featuringmanipulation
customization with minimal human intervention. Putting all subsets from different data sources
together,weeasilyestablishanaffordancememoryMspanningdiverseobjects,environments,and
embodiments,enablingourmethod’sgeneralizabilityacrossvarioussettings.
3.2 HierarchicalRetrieval
Tocorrectlymanipulateanewobject,humansoftenturntotheirmemoryforsimilarscenariosfor
guidance. Inspired by how humans think and Retrieval-Augmented Generation (RAG) in LLM
inference [60], we proposed a three-step hierarchical retrieval pipeline to effectively retrieve the
mostsimilardemonstrationfromtheagent’saffordancememoryMinacoarse-to-finemanner,and
leveragethedemonstrationasahinttoguidethemanipulation.
TaskRetrieval. BasedonthelanguageinstructionL(e.g.,“Pleaseopenthedrawertofindsome
utensils.”), we leverage text encoders (e.g., CLIP [56]) or language models (e.g., GPT-4 [61]) to
retrievethetaskT thattheinstructionfallswithin(“openthedrawer”)andextractstheobjectname
N (“drawer”). Ifthememorydoesn’tcontainthequeriedobject,themodelwillreasontheobject
geometryandproposeatask(ormultipletasks)thatpotentiallysharessimilaraffordance.
4SemanticFiltering. Thenintheretrievedtasks,wecangetthesourceRGBimages{IS}ofall
demonstrations.Foreachsourceimage,wecancalculateitsjointsimilaritywiththetargetobservation
imageIT andtheobjectnameN,intheformof
similarity=cos(CLIP (IS),CLIP (IT))·cos(CLIP (IS),CLIP (N)), (2)
v v v t
where CLIP (·) and CLIP (·) are CLIP [56] visual and text encoders, and cos(·) measures the
v t
cosinesimilaritybetweentwoembeddings. Then, wesetathresholdtofilteroutdemonstrations
that have very low similarities—which can be caused by a lack of intended objects, low-quality
segmentationmasks,low-lightenvironments,andsoon—toimprovetherobustnessofsubsequent
geometricalretrievalandaffordancetransfer.
Geometrical Retrieval. Large amounts of previous
researchwork[62,63,64]revealtheemergentcorrespon-
…
dence from large-scale unsupervised visual foundation
models[65,56,57,66]. Thecoreconceptisthatthedeep
densefeaturemapsproducedbyvisualfoundationmodels
contain rich geometrical and semantic information that
canbeusedfordensekeypointcorrespondencematching.
Task Semantic Geometrical
However,asillustratedin[67,55],thesevisualfoundation Retrieval Filtering Retrieval
models often struggle to understand the orientation of
Open the
instances. Therefore,itisessentialforourmethodtofind drawer
ademonstrationwheretheobjectisorientedsimilarlyto
thetargetimage. Tothisend,aftertaskretrievalandse- Figure3: Illustrationofourhierarchical
manticfiltering,wecalculateInstanceMatchingDistance retrievalpipeline.
(IMD)[67]usingSDfeaturemapstoperformgeometrical
retrievalfromtheremainingdemonstrationstofindtheonewiththemostsimilarviewpoint. More
details can be found in the supplementary material. By leveraging the hierarchical retrieval, we
canretrievethemostsuitabledemonstrationformanipulation,bothsemanticallyandgeometrically.
Experimentresultsshowthatitperformsbetterthanasingle-stageretrievalpipeline,foundinTable3.
3.3 2DAffordanceTransfer
Aftergettingthemostsimilardemonstrationfromtheaffordancememory,weaimtotransferthe2D
affordancefromthesourcedomainS tothetargetdomainT inageneralizableway. Giventhesource
imageIS anditscontactwaypointsC,leveragingdensefeaturemapsbyvisualfoundationmodels,
wefirstperformper-pointcorrespondencematchingtoobtainthecorrespondingwaypointsinthe
targetimage. WethenemployRANSACalgorithm[68]toremoveoutliersoftargetwaypointsandfit
alineinthe2Dspace. Inthisway,weobtainthe2Dcontactpointc2D,whichisthefirsttransferred
waypoint,andthepost-contactdirectionτ2D,leadingtothe2DaffordanceA2D =(c2D,τ2D). By
doingso,wecanestablisharobustaffordancetransferfromS toT.
3.4 Sampling-BasedAffordanceLifting
The2DaffordanceA2D =(c2D,τ2D)obtainedfromtheaffordancetransferstepcannotbedirectly
usedinthe3Dspace. Therefore,weproposeasimpleyeteffectivesampling-basedmethodtoliftthe
affordanceto3D,whichcanbeexecutedbytheend-effectorinthe3Dspace.
Basedonthecontactpointc2Dandthepartialpointcloud
generatedfromthedepthmapD,weback-projectthe2D
contactpointto3Dtogetthe3Dcontactpointc3Dusing
depth D. Then we crop the point cloud around c3D to
acquirethelocalgeometryofthecontactarea. Basedon 2D Affordance 3D Affordance 3D Affordance
Transfer Sampling Selection
thecroppedpointcloud,wefirstestimatethenormalvector
ofeachpointandthenperformK-Meansalgorithm[69]to Figure4: Affordancetransferandlifting.
clusterthesenormalsandgetTop-K clustercenters. Then
weback-projecttheseclustercenternormalvectorsintothe2Dspacethroughcameraparametersand
5Object AVG
Task O C O C O C O O O P P P P /
Where2Act[18] 2 34 2 54 2 68 2 0 / / / / / 20.50
VRB*[16] 8 62 6 56 16 66 4 12 10 18 28 44 60 30.77
Robo-ABC*[55] 20 58 22 60 30 46 30 28 26 40 54 66 60 41.54
RAM(Ours) 38 68 32 76 32 50 66 54 38 46 56 72 64 52.62
Table1: Successratesofobjectmanipulationfordifferentmethodsinthesimulationenvironment. O,
C,andPstandforOpen,Close,andPickup,respectively. *denotesnecessarymodificationfor3D
adaptation. RAMconsistentlyandsignificantlyoutperformsbaselinemethodsinthevastmajorityof
tasks. Moredetails,suchasicon-objectcorrespondencesandaffordancememorystatistics,canbe
foundinthesupplementarymaterial.
selecttheonewiththeleastincludedanglewiththe2Dpost-contactdirectionτ2D. Inthisway,we
canobtainthe3DaffordanceA3D =(c3D,τ3D)forroboticmanipulation.
Tomapthe3Daffordancetoconcreterobotactions,wesampledensegraspproposalsonthecropped
pointcloudleveragingoff-the-shelfdensegraspgenerators[20,21]andselecttheclosestgraspfrom
c3D. Aftergrasping,wecanutilizepositioncontroltomovetheend-effectorinthedirectionofτ3D,
orleverageimpedancecontrolintherealworldbasedonthereal-timeimpedanceforcefeedbackas
in[70]toadjusttheend-effectorandsafelyperformtheactions.
4 Experiments
4.1 ExperimentalSetup
To verify the effectiveness of the proposed method, we conduct extensive evaluations in both
simulationandreal-worldsettings.
Insimulation,weadoptIsaacGym[71]asthesimulator,GAPartNet[5],andYCB[72]datasetsas
objectassets,andaflyingFrankaPandagripperformanipulation. Wecollectedover70objectsin10
categoriesandevaluatedthemon3kindsoftasks,includingOpen,Close,andPickup,comprising
13differenttasks. Foreachtask,weconduct50experiments. Thecameraviewpointisfixed,andthe
objects’positionsandrotationsarerandomlyinitializedwithinacertainrange. ForPickuptasks,
severaldistractorobjectsarealsorandomlyplaced. Amanipulationsuccessisdefinedbywhetherthe
DoFofinterest(articulationjointorheight)exceedsathreshold.
Inthereal-worldsetting,weconductexperimentsthatinvolveinteractingwithvariousreal-world
household objects. We employ a Franka Emika robotic arm with a parallel gripper and utilize
an on-hand RealSense D415 camera to capture the RGBD image. In addition, we also utilize a
UnitreeB1robotdogequippedwithaZ1armandaRealSenseD415cameratoshowourmethod’s
cross-embodimentnature[73,74],whichiscoveredin§4.5.
4.2 BaselineMethods
Wecompareourmethodagainstthreebaselines,namelyWhere2Act[18],VRB[16],andRobo-
ABC [55], where they represent training-based, 2D affordance-based and retrieve-and-transfer
methodsrespectively. Where2Act[18]andVRB[16]aretrainedonvastamountsofpointcloudsand
egocentricHOIimages,respectively,whileRobo-ABCandourmethodaretraining-free.
Note that VRB and Robo-ABC only predict 2D affordance in either contact point and direction
(VRB)orcontactpointonly(Robo-ABC).Toadaptthemtothe3Dspace, wefaithfullyconduct
necessarymodifications. ForRobo-ABC,wefeeditwithourcollectedaffordancememoryanduse
theproposed2Daffordancetransfermoduletogeneratea2Dtrajectory. Subsequently,wefollowthe
sameprocedureasinourmethodtolift2Daffordanceto3Dforbothmethods. Adetaileddiscussion
ofbaselineimplementationcanbefoundinthesupplementarymaterial.
64.3 ResultsandAnalysis
Forsimulationandreal-worldsettings,weadopt
SuccessRate(SR)asthemajorevaluationmet-
Object AVG
ric.Resultsofsimulationexperimentsareshown
Task O O O P P P /
in Table 1, from which we can see that our
Robo-ABC*[55] 2/5 1/5 1/5 3/5 4/5 4/5 50.0
method outperforms all baselines in the vast RAM(Ours) 3/5 2/5 3/5 3/5 4/5 5/5 66.7
majorityoftasks, yieldinganaveragesuccess
rate of 52.62%. Compared to our method, Table2: Successratesofobjectmanipulationfor
differentmethodsinthereal-worldenvironment.
Where2Act [18] largely fails in the opening
RAMyieldsafavorablereal-worldperformance.
tasks that require a precise contact point and
grasppose. VRB[16]alsosuffersfromprovid-
ingprecisecontactpointsforgrasping,leadingtosuboptimalperformanceinopeningandpickingup
tasks. OurreimplementedandimprovedRobo-ABC[55]yieldsthesecond-bestperformance. We
furthernotesuchaperformanceofRobo-ABCsignificantlydependsonouraffordancetransferand
liftingmodules.
We also compare our method with Robo-
RAM (Ours) Robo-ABC* VRB*
ABC [55] in the real world on 6 tasks with 5
rollouts each, as shown in Table. 2. Results
showthatourmethodalsooutperformsthebase-
lineduetoourmoreeffectiveretrievalpipeline.
Italsoshowsourmethod’scross-domainnature
andeasysim-to-realtransfer. Real-worldvideos
canbefoundinthesupplementarymaterial.
Apartfromquantitativestudies, wealsoshow Simulation Real-World
Open the dishwasher Open the cabinet Open the drawer Open the microwave
somequalitativeresultsonaffordancetransfer
Figure5: Qualitativecomparisonofgenerated2D
withrespecttomultipledatasourcesinFig.5.
affordance. Source demonstrations retrieved by
Visualizationresultsshowthatourmethodcan
RAMareshowninthefirstrow.
establish a more reliable affordance transfer,
leadingtomorerobustperformance.
4.4 AblationStudies
Probing deeper into our framework designs, we perform extensive ablation studies on various
components. AllstudiesareconductedonOpentasksofdrawers,cabinets,andmicrowaves,with
20episodeseach. ThemetricofDistancetoMask(DTM)isreportedalongwithSRasin[55],which
measuresthedistanceofthetransferredcontactpointtotheobjecthandleinpixelspace.
AblationonHierarchicalRetrieval. AsshowninTable3,theremovalofanysubmodulesinthe
hierarchical retrieval pipeline results in a degradation of SR and DTM. We further observed the
deactivationofgeometricalretrievalleadstoasignificantSRdropfrom38.3%to26.7%,showing
theimportanceofgeometricalalignmentinaffordancetransfer.
Cross-TaskTransfer. Althoughouraffordancememorycon-
AblationMethod SR↑ DTM↓
structionpipelineenablesscalableskillacquisitionfromvarious
w/oTaskRtrvl. 31.7 4.99
datasources,itisstillvaluabletoprobehowRAMperforms w/oSem.Filtering 33.3 6.69
w/oGeom.Rtrvl. 26.7 9.36
when the target task is beyond the scope of its memory. We
Cross-Task 25.0 9.33
evaluate this by blocking the given task from the affordance
SD→CLIP[56] 15.0 57.4
memory. Asshowninthe4throwofTable3,RAMmaintains
SD→DINOv2[66] 23.3 5.09
adecentsuccessrateof25.0%,demonstratinganimpressive SD→SD-DINOv2[63] 36.7 3.04
cross-tasktransfercapabilitywhenfacingunseentasks. FullPipeline 38.3 3.39
Table3: Ablationstudiesondiffer-
EffectsofDifferentVFMs. WefurtherreplacedStableDiffu-
entcomponentsofourmethod.
sion(SD)usedinourpipelinewithothervisualfoundationmod-
elstoinvestigatetheeffectsofdifferentVFMsforaffordance
transfer. AsdepictedinTable3,wefoundthatSD[57]performsbestonSR,andSD-DINOv2[63]
7
enecS
deveirteR
enecS
tegraTperformsbestonDTM,bothsurpassingCLIP[56]andDINOv2[66]byalargemargin.AlthoughSD-
DINOv2[63]excelsincontactpointmatching,wechooseSDforourpipelineduetoitsrobustness
inpost-contactcorrespondence,highersuccessrate,andlessinferencetime.
EffectsofDataAmount. Ablationonthedataamount
40 25 in our retrieval memory M is conducted as shown
35 20
inFig.6. Wereducedtheretrievalmemoryforeach 30 15
taskfromaminimumof10%tofulldata. Theresults 25 10
indicatethatasthedataamountincreases,theoverall 20 5
15 0
performance also improves. Notably, using 50% of 20 40 60 80 100
Data Amount (%)
original data yields comparable results to using full
Figure6: Performancesofdifferentdata
data,demonstratingthedataefficiencyofRAM. amountsforretrieval.
4.5 DownstreamApplicationsandDiscussions
In this section, we aim to show that our method has the potential to enable a broad spectrum of
downstreamapplications,tacklinggeneralroboticproblems.
PolicyDistillation. Asazero-shotroboticmanipula-
Task AVG
tionmethod,conditionedonlanguageinstructions,our
VRB[16] 1/20 1/20 2/20 6.67
RAMcanperformfullyautonomousexplorationofthe
Zero-Shot 8/20 4/20 6/20 30.0
surrounding environment by effective trial-and-error Distilled 12/20 14/20 13/20 65.0
withoutanyhumanpriorsorrewardshaping.Therefore,
Table4: Policydistillationresultsonsuc-
wecanusetheproposedRAMforefficientannotated
cessrate. RAMperformsfullyautonomous
datacollectionandlearnanend-to-endpolicyfromthe explorationtocollecthigh-qualitydemon-
affordance knowledge. To that end, we leverage our strationsefficientlyforpolicylearning.
zero-shotpipelinetoautomaticallycollectsuccessful
demonstrationsofopeningadrawer,acabinet,oramicrowave. Then,welearnanACTpolicy[39]
fromthesedemonstrations. AsshowninTable4,usingonly50demonstrationsforeachtask,our
distilledpolicycanachievea+35.0%performanceboostoverthezero-shotpipeline,indicatingthe
hugepotentialforautomaticandscalablehigh-qualitydatacollectionforpolicylearning.
One-ShotVisualImitationwithHumanPreference. Apartfromutilizingout-of-domaindemon-
strationretrievalformanipulation,ourmethodisnaturallyadaptableforone-shotvisualimitationfor
bettercontrollability,givenaspecificin-domainorout-of-domaindemonstration. Forexample,as
showninFig.1c,givenatissueboxwithhumanpreferencesofpickingupthetissuepaperortissue
box,ourmethodcanactcorrespondinglytoperformdifferentvisualimitations. Anotherexampleof
TomandJerryshowsthatourmethodisabletobridgethegreatdomaingapbetweentherealworld
andcartoonimages,thankstothegeneralizabilityofvisualfoundationmodels.
LLM/VLM Integration. Our method can also be easily integrated with LLMs/VLMs for open-
setinstructions[75]andlong-horizontasks, bydecomposingthemintosmalleronessuitablefor
affordancetransferandotheractionprimitives. AsshowninFig.1d,givenaninstruction(“Clearthe
table.”),wecanleverageaVLM[76]tointerpretanddecomposeitintoseveralactionsandprimitives
andleverageaffordancetransferforcertainactionstobehaveinahuman-orientedway. Thisenables
moreflexibletaskswithhighercomplexity. Moredetailscanbefoundinthesupplementarymaterial.
5 ConclusionsandLimitations
Conclusions.WeproposeRAM,anovelpipelineforgeneralizablezero-shotroboticmanipulation.At
thecoreofRAMisaretrieval-basedaffordancetransferandliftingmechanismthateffectivelydistills
actionableknowledgefromlargeout-of-domaindatatoanunseentargetdomain. Thisframework
canpotentiallygeneralizetootherrobotictasksbeyondmanipulation,highlightingitsversatility. In
additiontothemajorexperimentswhereRAMoutperformsitscounterpartsbylargemargins,we
alsodemonstratetheflexibilityofRAMthroughitsapplicationsinseveralkeyareas.
8
)%(
etaR
sseccuS
ksaM
ot
ecnatsiDLimitations. Despite compelling results, RAM shares certain limitations with prior works. For
long-horizontasks,ittakesmultiplestepsforourmethodtochaintheactions,whichmightgenerate
less natural long-horizon behaviors. Also, our method struggles with complex actions, such as
screwing. Futureworkstoaddresstheseissuesincludeintegratingbetterfoundationmodelsfortask
planninganddevelopingmoreadvancedaffordancetransfermethodsthatdirectlylift2Dtrajectories
into3Dspacetoenablemorecomplextasks.
Acknowledgments
TheauthorsexpresstheirsinceregratitudetoDieterFox,RuihaiWu,YuanchenJu,JiazhaoZhang,
XiaomengFangforfruitfuldiscussionsandvaluablefeedback. Thisworkispartlysupportedbya
giftfromGoogle.
References
[1] A.Padalkar,A.Pooley,A.Jain,A.Bewley,A.Herzog,A.Irpan,A.Khazatsky,A.Rai,A.Singh,
A.Brohan,etal.Openx-embodiment:Roboticlearningdatasetsandrt-xmodels.arXivpreprint
arXiv:2310.08864,2023.
[2] A.Khazatsky,K.Pertsch,S.Nair,A.Balakrishna,S.Dasari,S.Karamcheti,S.Nasiriany,M.K.
Srirama,L.Y.Chen,K.Ellis,etal. Droid: Alarge-scalein-the-wildrobotmanipulationdataset.
arXivpreprintarXiv:2403.12945,2024.
[3] S.James,Z.Ma,D.R.Arrojo,andA.J.Davison. Rlbench: Therobotlearningbenchmark&
learningenvironment. IEEERoboticsandAutomationLetters,5(2):3019–3026,2020.
[4] O.Mees,L.Hermann,E.Rosete-Beas,andW.Burgard. Calvin: Abenchmarkforlanguage-
conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and
AutomationLetters,7(3):7327–7334,2022.
[5] H.Geng,H.Xu,C.Zhao,C.Xu,L.Yi,S.Huang,andH.Wang. Gapartnet: Cross-category
domain-generalizableobjectperceptionandmanipulationviageneralizableandactionableparts.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages7081–7091,2023.
[6] H. Geng, Z. Li, Y. Geng, J. Chen, H. Dong, and H. Wang. Partmanip: Learning cross-
categorygeneralizablepartmanipulationpolicyfrompointcloudobservations. arXivpreprint
arXiv:2303.16958,2023.
[7] Y.Xu,W.Wan,J.Zhang,H.Liu,Z.Shan,H.Shen,R.Wang,H.Geng,Y.Weng,J.Chen,etal.
Unidexgrasp: Universalroboticdexterousgraspingvialearningdiverseproposalgenerationand
goal-conditionedpolicy. arXivpreprintarXiv:2303.00938,2023.
[8] W.Wan,H.Geng,Y.Liu,Z.Shan,Y.Yang,L.Yi,andH.Wang. Unidexgrasp++: Improving
dexterous grasping policy learning via geometry-aware curriculum and iterative generalist-
specialistlearning. arXivpreprintarXiv:2304.00464,2023.
[9] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti,
J.Munro,T.Perrett,W.Price,etal. Scalingegocentricvision: Theepic-kitchensdataset. In
ProceedingsoftheEuropeanconferenceoncomputervision(ECCV),pages720–736,2018.
[10] H.Luo,W.Zhai,J.Zhang,Y.Cao,andD.Tao. Learningaffordancegroundingfromexocentric
images.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pages2252–2261,2022.
[11] K.Grauman,A.Westbury,E.Byrne,Z.Chavis,A.Furnari,R.Girdhar,J.Hamburger,H.Jiang,
M. Liu, X. Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In
9ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
18995–19012,2022.
[12] Y. Liu, Y. Liu, C. Jiang, K. Lyu, W. Wan, H. Shen, B. Liang, Z. Fu, H. Wang, and L. Yi.
Hoi4d: A4degocentricdatasetforcategory-levelhuman-objectinteraction. InProceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
21013–21022,June2022.
[13] Y.Liu,K.Zhang,Y.Li,Z.Yan,C.Gao,R.Chen,Z.Yuan,Y.Huang,H.Sun,J.Gao,L.He,
andL.Sun. Sora: Areviewonbackground,technology,limitations,andopportunitiesoflarge
visionmodels,2024.
[14] P.Sundaresan,Q.Vuong,J.Gu,P.Xu,T.Xiao,S.Kirmani,T.Yu,M.Stark,A.Jain,K.Hausman,
D.Sadigh,J.Bohg,andS.Schaal. Rt-sketch: Goal-conditionedimitationlearningfromhand-
drawnsketches,2024.
[15] S.Liu, S.Tripathi,S.Majumdar,andX.Wang. Jointhandmotionandinteractionhotspots
predictionfromegocentricvideos. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages3282–3292,2022.
[16] S.Bahl,R.Mendonca,L.Chen,U.Jain,andD.Pathak. Affordancesfromhumanvideosasa
versatilerepresentationforrobotics. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages13778–13790,2023.
[17] C.Yuan,C.Wen,T.Zhang,andY.Gao. Generalflowasfoundationaffordanceforscalable
robotlearning,2024.
[18] K. Mo, L. J. Guibas, M. Mukadam, A. Gupta, and S. Tulsiani. Where2act: From pixels to
actionsforarticulated3dobjects. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages6813–6823,2021.
[19] R.Wu,Y.Zhao,K.Mo,Z.Guo,Y.Wang,T.Wu,Q.Fan,X.Chen,L.Guibas,andH.Dong.
VAT-mart: Learningvisualactiontrajectoryproposalsformanipulating3dARTiculatedobjects.
InInternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.
net/forum?id=iEx3PiooLy.
[20] C.Wang,H.-S.Fang,M.Gou,H.Fang,J.Gao,andC.Lu. Graspnessdiscoveryincluttersfor
fastandaccurategraspdetection. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages15964–15973,2021.
[21] H.-S.Fang,C.Wang,H.Fang,M.Gou,J.Liu,H.Yan,W.Liu,Y.Xie,andC.Lu. Anygrasp:
Robustandefficientgraspperceptioninspatialandtemporaldomains. IEEETransactionson
Robotics,2023.
[22] D. Coleman, I. Sucan, S. Chitta, and N. Correll. Reducing the barrier to entry of complex
roboticsoftware: amoveit! casestudy. arXivpreprintarXiv:1404.3785,2014.
[23] B.Sundaralingam,S.K.S.Hari,A.Fishman,C.Garrett,K.VanWyk,V.Blukis,A.Millane,
H.Oleynikova,A.Handa,F.Ramos,etal. Curobo: Parallelizedcollision-freerobotmotion
generation. In2023IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages
8112–8119.IEEE,2023.
[24] Y.Wang,R.Wu,K.Mo,J.Ke,Q.Fan,L.J.Guibas,andH.Dong. Adaafford: Learningto
adaptmanipulationaffordancefor3darticulatedobjectsviafew-shotinteractions. InEuropean
conferenceoncomputervision,pages90–107.Springer,2022.
[25] Y.Geng,B.An,H.Geng,Y.Chen,Y.Yang,andH.Dong. End-to-endaffordancelearningfor
roboticmanipulation. InternationalConferenceonRoboticsandAutomation(ICRA),2023.
10[26] W.Huang,C.Wang,R.Zhang,Y.Li,J.Wu,andL.Fei-Fei. Voxposer: Composable3dvalue
mapsforroboticmanipulationwithlanguagemodels. arXivpreprintarXiv:2307.05973,2023.
[27] F. Liu, K. Fang, P. Abbeel, and S. Levine. Moka: Open-vocabulary robotic manipulation
throughmark-basedvisualprompting. arXivpreprintarXiv:2403.03174,2024.
[28] Y.Hu,F.Lin,T.Zhang,L.Yi,andY.Gao. Lookbeforeyouleap:Unveilingthepowerofgpt-4v
inroboticvision-languageplanning. arXivpreprintarXiv:2311.17842,2023.
[29] H.Huang,F.Lin,Y.Hu,S.Wang,andY.Gao. Copa: Generalroboticmanipulationthrough
spatialconstraintsofpartswithfoundationmodels,2024.
[30] H.Geng,S.Wei,C.Deng,B.Shen,H.Wang,andL.Guibas. Sage: Bridgingsemanticand
actionablepartsforgeneralizablearticulated-objectmanipulationunderlanguageinstructions,
2023.
[31] Y.Ding,H.Geng,C.Xu,X.Fang,J.Zhang,S.Wei,Q.Dai,Z.Zhang,andH.Wang.Open6DOR:
Benchmarkingopen-instruction6-dofobjectrearrangementandaVLM-basedapproach. In
First Vision and Language for Autonomous Driving and Robotics Workshop, 2024. URL
https://openreview.net/forum?id=RclUiexKMt.
[32] Y.You,B.Shen,C.Deng,H.Geng,H.Wang,andL.Guibas. Makeadonut: Language-guided
hierarchicalemd-spaceplanningforzero-shotdeformableobjectmanipulation,2023.
[33] R.Gong,J.Huang,Y.Zhao,H.Geng,X.Gao,Q.Wu,W.Ai,Z.Zhou,D.Terzopoulos,S.-C.
Zhu,B.Jia,andS.Huang. Arnold: Abenchmarkforlanguage-groundedtasklearningwith
continuousstatesinrealistic3dscenes. arXivpreprintarXiv:2304.04321,2023.
[34] P.Li,T.Liu,Y.Li,M.Han,H.Geng,S.Wang,Y.Zhu,S.-C.Zhu,andS.Huang. Ag2manip:
Learningnovelmanipulationskillswithagent-agnosticvisualandactionrepresentations. arXiv
preprintarXiv:2404.17521,2024.
[35] S.Nasiriany,F.Xia,W.Yu,T.Xiao,J.Liang,I.Dasgupta,A.Xie,D.Driess,A.Wahid,Z.Xu,
etal. Pivot: Iterativevisualpromptingelicitsactionableknowledgeforvlms. arXivpreprint
arXiv:2402.07872,2024.
[36] H.Bharadhwaj,A.Gupta,V.Kumar,andS.Tulsiani. Towardsgeneralizablezero-shotmanipu-
lationviatranslatinghumaninteractionplans. arXivpreprintarXiv:2312.00775,2023.
[37] K. Black, M. Nakamoto, P. Atreya, H. Walke, C. Finn, A. Kumar, and S. Levine. Zero-
shot robotic manipulation with pretrained image-editing diffusion models. arXiv preprint
arXiv:2310.10639,2023.
[38] C.Wen,X.Lin,J.So,K.Chen,Q.Dou,Y.Gao,andP.Abbeel. Any-pointtrajectorymodeling
forpolicylearning,2023.
[39] T.Z.Zhao,V.Kumar,S.Levine,andC.Finn. Learningfine-grainedbimanualmanipulation
withlow-costhardware. arXivpreprintarXiv:2304.13705,2023.
[40] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy:
Visuomotorpolicylearningviaactiondiffusion. arXivpreprintarXiv:2303.04137,2023.
[41] Y. Qin, W. Yang, B. Huang, K. Van Wyk, H. Su, X. Wang, Y.-W. Chao, and D. Fox.
Anyteleop: A general vision-based dexterous robot arm-hand teleoperation system. arXiv
preprintarXiv:2307.04577,2023.
[42] Z.Fu,T.Z.Zhao,andC.Finn. Mobilealoha: Learningbimanualmobilemanipulationwith
low-costwhole-bodyteleoperation. arXivpreprintarXiv:2401.02117,2024.
11[43] S.Sontakke,J.Zhang,S.Arnold,K.Pertsch,E.Bıyık,D.Sadigh,C.Finn,andL.Itti. Roboclip:
Onedemonstrationisenoughtolearnrobotpolicies.AdvancesinNeuralInformationProcessing
Systems,36,2024.
[44] O.M.Team,D.Ghosh,H.Walke,K.Pertsch,K.Black,O.Mees,S.Dasari,J.Hejna,T.Kreiman,
C.Xu,etal. Octo: Anopen-sourcegeneralistrobotpolicy. arXivpreprintarXiv:2405.12213,
2024.
[45] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine. One-shot visual imitation learning via
meta-learning,2017.
[46] Z.Mandi,F.Liu,K.Lee,andP.Abbeel. Towardsmoregeneralizableone-shotvisualimitation
learning. In2022InternationalConferenceonRoboticsandAutomation(ICRA),pages2434–
2444.IEEE,2022.
[47] M.Du,S.Nair,D.Sadigh,andC.Finn. Behaviorretrieval: Few-shotimitationlearningby
queryingunlabeleddatasets. arXivpreprintarXiv:2304.08742,2023.
[48] M.Vecerik,C.Doersch,Y.Yang,T.Davchev,Y.Aytar,G.Zhou,R.Hadsell,L.Agapito,and
J.Scholz. Robotap: Trackingarbitrarypointsforfew-shotvisualimitation. arXivpreprint
arXiv:2308.15975,2023.
[49] N.DiPaloandE.Johns. Dinobot: Robotmanipulationviaretrievalandalignmentwithvision
foundationmodels. arXivpreprintarXiv:2402.13181,2024.
[50] S. Bahl, A. Gupta, and D. Pathak. Human-to-robot imitation in the wild. arXiv preprint
arXiv:2207.09450,2022.
[51] J.Gao,Z.Tao,N.Jaquier,andT.Asfour. K-vil: Keypoints-basedvisualimitationlearning.
IEEETransactionsonRobotics,2023.
[52] X.ZhangandA.Boularias. One-shotimitationlearningwithinvariancematchingforrobotic
manipulation. arXivpreprintarXiv:2405.13178,2024.
[53] F.Malato,F.Leopold,A.Melnik,andV.Hautamäki. Zero-shotimitationpolicyviasearchin
demonstrationdataset. InICASSP2024-2024IEEEInternationalConferenceonAcoustics,
SpeechandSignalProcessing(ICASSP),pages7590–7594.IEEE,2024.
[54] J.Sheikh,A.Melnik,G.C.Nandi,andR.Haschke. Language-conditionedsemanticsearch-
basedpolicyforroboticmanipulationtasks. arXivpreprintarXiv:2312.05925,2023.
[55] Y.Ju,K.Hu,G.Zhang,G.Zhang,M.Jiang,andH.Xu. Robo-abc: Affordancegeneraliza-
tionbeyondcategoriesviasemanticcorrespondenceforrobotmanipulation. arXivpreprint
arXiv:2401.07487,2024.
[56] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,2021.
[57] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer.High-resolutionimagesynthesis
withlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages10684–10695,2022.
[58] F.Yi,H.Wen,andT.Jiang. Asformer: Transformerforactionsegmentation. arXivpreprint
arXiv:2110.08568,2021.
[59] L.Zhang, S.Zhou, S.Stent, andJ.Shi. Fine-grainedegocentrichand-objectsegmentation:
Dataset,model,andapplications. InEuropeanConferenceonComputerVision,pages127–145.
Springer,2022.
12[60] P.Lewis,E.Perez,A.Piktus,F.Petroni,V.Karpukhin,N.Goyal,H.Küttler,M.Lewis,W.-t.
Yih,T.Rocktäschel,etal. Retrieval-augmentedgenerationforknowledge-intensivenlptasks.
AdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.
[61] OpenAI. Gpt-4technicalreport,2023.
[62] L.Tang,M.Jia,Q.Wang,C.P.Phoo,andB.Hariharan. Emergentcorrespondencefromimage
diffusion. AdvancesinNeuralInformationProcessingSystems,36:1363–1389,2023.
[63] J.Zhang,C.Herrmann,J.Hur,L.PolaniaCabrera,V.Jampani,D.Sun,andM.-H.Yang. A
taleoftwofeatures: Stablediffusioncomplementsdinoforzero-shotsemanticcorrespondence.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[64] M.E.Banani,A.Raj,K.-K.Maninis,A.Kar,Y.Li,M.Rubinstein,D.Sun,L.Guibas,J.Johnson,
andV.Jampani. Probingthe3dawarenessofvisualfoundationmodels,2024.
[65] M.Caron,H.Touvron,I.Misra,H.Jégou,J.Mairal,P.Bojanowski,andA.Joulin. Emerging
propertiesinself-supervisedvisiontransformers. InProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision,pages9650–9660,2021.
[66] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,P.Fernandez,D.Haziza,
F.Massa, A.El-Nouby, etal. Dinov2: Learningrobustvisualfeatureswithoutsupervision.
arXivpreprintarXiv:2304.07193,2023.
[67] J.Zhang,C.Herrmann,J.Hur,E.Chen,V.Jampani,D.Sun,andM.-H.Yang. Tellingleftfrom
right: Identifyinggeometry-awaresemanticcorrespondence. arXivpreprintarXiv:2311.17034,
2023.
[68] M.A.FischlerandR.C.Bolles. Randomsampleconsensus: aparadigmformodelfittingwith
applicationstoimageanalysisandautomatedcartography. CommunicationsoftheACM,24(6):
381–395,1981.
[69] S.Lloyd. Leastsquaresquantizationinpcm. IEEEtransactionsoninformationtheory,28(2):
129–137,1982.
[70] X.Li,M.Zhang,Y.Geng,H.Geng,Y.Long,Y.Shen,R.Zhang,J.Liu,andH.Dong.Manipllm:
Embodiedmultimodallargelanguagemodelforobject-centricroboticmanipulation. arXiv
preprintarXiv:2312.16217,2023.
[71] V.Makoviychuk,L.Wawrzyniak,Y.Guo,M.Lu,K.Storey,M.Macklin,D.Hoeller,N.Rudin,
A.Allshire,A.Handa,andG.State.Isaacgym:Highperformancegpu-basedphysicssimulation
forrobotlearning,2021.
[72] B.Calli,A.Singh,A.Walsman,S.Srinivasa,P.Abbeel,andA.M.Dollar. Theycbobjectand
modelset: Towardscommonbenchmarksformanipulationresearch. In2015international
conferenceonadvancedrobotics(ICAR),pages510–517.IEEE,2015.
[73] Y.Kuang,Q.Han,D.Li,Q.Dai,L.Ding,D.Sun,H.Zhao,andH.Wang. Stopnet: Multiview-
based 6-dof suction detection for transparent objects on production lines. arXiv preprint
arXiv:2310.05717,2023.
[74] J. Zhang, N. Gireesh, J. Wang, X. Fang, C. Xu, W. Chen, L. Dai, and H. Wang. Gamma:
Graspability-awaremobilemanipulationpolicylearningbasedononlinegraspingposefusion.
arXivpreprintarXiv:2309.15459,2023.
[75] Y.Kuang,H.Lin,andM.Jiang. Openfmnav: Towardsopen-setzero-shotobjectnavigationvia
vision-languagefoundationmodels. arXivpreprintarXiv:2402.10670,2024.
[76] Z.Yang,L.Li,K.Lin,J.Wang,C.-C.Lin,Z.Liu,andL.Wang.Thedawnoflmms:Preliminary
explorationswithgpt-4v(ision),2023.
13[77] T.Ren,S.Liu,A.Zeng,J.Lin,K.Li,H.Cao,J.Chen,X.Huang,Y.Chen,F.Yan,Z.Zeng,
H.Zhang,F.Li,J.Yang,H.Li,Q.Jiang,andL.Zhang. Groundedsam:Assemblingopen-world
modelsfordiversevisualtasks,2024.
A Real-RobotRollouts
Dynamicvideosofreal-robotrolloutscanbefoundinthesupplementaryvideoandatourwebsite:
https://yxkryptonite.github.io/RAM/.
B DataCollectionandAffordanceExtraction
B.1 RoboticData
WeadoptDROID[2]asoursourceofroboticdata,whichincludes76,000experttrajectoriesofrobots
conductingdailytasks,alongwithcorrespondingtaskinstructions. Toextractaffordanceinformation
fromDROID,wefirstqueryinstructionsfortasksofinterest. Forexample,forthetask"openthe
drawer,"weperformahardsearchtofilteroutinstructionsthatdonotincludetheaction"open"and
theobject"drawer."
Next,thefilteredinstructionsaresortedbasedontheL2distancesbetweentheirlanguageembeddings
andthequeryembedding. Wegeneratetheseembeddingsusingthe"text-embedding-3-small"
modelofOpenAI.Basedonthesortedinstructions,wethenselecttheTop-kepisodesforfurther
affordanceextraction.
Toextractaffordanceinformationfromtheseselectedepisodes,weidentifythecartesianposition
when the gripper is closed as the 3D contact point. We then track the gripper’s position for the
following10timestepsoruntilthegripperstopsmovingforconsecutivesteps. Thisprovidesuswith
the3Dpost-contacttrajectory.Usingtheprovidedcameraparameters,weprojectthe3Dcontactpoint
andthepost-contacttrajectoryontothefirstframeofeachepisodewheretheobjectisunobscured.
Notethatsomeaffordancedemonstrationsarefurtherrefinedmanuallybyaddinganoffsetorbeing
removedduetoinaccuratecameraparameters. Shouldaccuratecameracalibrationbeavailable,the
manualcorrectionisnotnecessary.
ThismethodensuresapreciseandreliableextractionofaffordanceinformationfromtheDROID[2]
dataset. Notethatthismethodcanalsoadapttootherroboticdatasets(real-worldorsynthetic),such
as[1,3,4].
B.2 HOIData
Inadditiontothedetailsinthemaintext,wefurthernotethatweonlyaveragethehandkeypoints
withintheobjectmasktodeterminethecontactpointandshiftthepost-contacttrajectoryaccordingly.
Thisensuresthatthecontactpointiswithintheobjectforrobustaffordancetransfer.
Notethatthisaffordanceextractionprocedurecanalsobeappliedtootherdatasourceswherehand-
objectinteractionsareinvolved,suchasmoreHOIdatasets[9,10,11],vastamountsofunannotated
humanegocentricvideosontheInternet,anduser-provideddemonstrations.
B.3 CustomData
Theannotationprocessforcustomdataaffordancehasbeendiscussedinthemaintext. Additionally,
thesourcesofcustomdataarehighlydiverseandconfigurable. Forourexperiments,weannotated
objectimagesofourinterestobtainedbysimplysearchingfromtheInternet(Google,YouTube,etc.).
Notably,customdatacanalsocomefromavarietyofsources,rangingfromuser-capturedimages,
cartoon images, AI-generated content, and even sketches, etc., demonstrating the flexibility and
diversityofourdatasources. Thisflexibilityallowsforawiderangeofpotentialapplicationsand
extendsouraffordancememory’sscalabilitytoagreaterextent.
14B.4 AffordanceMemoryStatistics
ThestatisticsofouraffordancememorycanbefoundinTable5.
TaskName Icon DataSource Size
Openthedrawer DROID 30
Closethedrawer DROID 20
Openthecabinet DROID 12
Closethecabinet DROID 6
Openthemicrowave DROID 42
Closethemicrowave DROID 10
Openthedishwasher Custom 10
Opentherefrigerator Custom 20
Openthetrashcan Custom 20
Pickupthepot DROID 11
Pickupthemug HOI4D 149
Pickupthebowl HOI4D 252
Pickupthebottle HOI4D 78
Total / / 660
Table5: Affordancememorystatistics.
C ImplementationDetails
C.1 FeatureExtractionUsingFoundationModels
Weusedifferentfoundationmodelsasvisualfeatureextractors,including:
• StableDiffusion(SD)[57]. Asillustratedin[62],givenanoriginalimagex ,wefirstaddnoise
0
oftimestepttoittomoveittodistributionx ,andthenfeedittothestablediffusionnetworkf
t θ
alongwithtfordenoisingtoextracttheintermediatelayeractivationsasthediffusionfeatures
(DIFT).Weusethesameconfigurationasin[62].
• DINOv2 [66]. Extracting DINOv2 features is implemented by feeding the original image to
theDINOv2modelandextractingtheintermediatelayeractivationsofDINOv2ViTduringthe
feed-forwardprocess.
• SD-DINOv2[63]. Asin[63],wefirstextractSDfeaturesandDINOv2featuresandthendoL2
normalizationonthemtoaligntheirscalesanddistributions. Afterthat,weconcatenatethesetwo
featurestogethertogettheSD-DINOv2feature.
• CLIP[56]. SimilartoDINOv2,WeextractdenseCLIPfeaturesbyutilizingtheintermediatelayer
activationsofCLIPViT.
C.2 IMDMetricCalculation
Asin[67],InstanceMatchingDistance(IMD)isoriginallyproposedtoexamineposeprediction
accuracy. GivenasourceimageIS andatargetimageIT,theirnormalizedandmaskedfeaturemaps
FS andFT,andasourceinstancemaskMS,theIMDmetricisdefinedas:
IMD(IS,IT,MS)= (cid:88) (cid:13) (cid:13)FS(p)−NN(FS(p),FT)(cid:13) (cid:13) , (3)
2
p∈MS
wherepdenotesapixelwithinthesourceinstancemask,FS(p)isthesourcefeaturevectoratpixelp,
andNN(FS(p),FT)denotesthenearestneighborvectorinthetargetfeaturemapFT withrespectto
thesourcefeaturevector. IMDmeasuresthesimilarityoftwoimagesviatheaveragefeaturedistance
ofcorrespondingpixels[67]. UsingIMDinthegeometricalretrievalstage,wecanaccuratelyretrieve
thedemonstrationwheretheobjectisorientedinthemostsimilarwayasintheobservation.
15C.3 BaselineMethods
• Where2Act[18]isdesignedforarticulatedobjectmanipulationonly,whichtakesanobjectpoint
cloudasinputandpredictspoint-wiseactionabilityscores,actionproposals,andactionscoreswith
threeseparatemodels. Anotherdrawbackofthismethodisthatitprocessesthepointcloudina
task-agnosticway,leadingtoambiguityofthegeneratedaffordance. Weadoptittotheevaluation
tasksby1)randomlysamplingthecontactpointfromthepredictedtop-5actionablepoints, 2)
proposing100actionsusingtheactionproposalmodel,and3)selectingtheactionwiththehighest
actionscore.
• VRB[16]predictsthecontactpointanddirectiononlyon2Dimages. Tomakeitapplicableinreal
manipulationtasks,welifttheestimated2Daffordanceto3Dusingourproposedsampling-based
affordanceliftingmodule.
• Robo-ABC[55]isinitiallydesignedforobjectgraspingonly,whereonlythecontactpointofa
source demonstration retrieved by CLIP [56] feature similarity is transferred on the 2D image,
followed by AnyGrasp [21] for grasp pose selection. For a fair comparison, we feed it with
ourcollectedaffordancememory. Toextenditforarticulatedobjects,weusetheproposed2D
affordancetransfermoduletotransferboththecontactpointandpostdirection. Subsequently,we
followthesameprocedureasinourmethodtolift2Daffordanceto3D.
D ExperimentDetails
Inthesimulation,weutilizeaflyingFrankaPandagripperforsimplicity. WeutilizecuRobo[23]
motionplannerforpositioncontrolofthegripper.
Intherealworld,weadopttwodifferentroboticsystems.IntheFrankaEmikaroboticarmsetting,we
leverageanon-handRealSenseD415cameraforRGBDperceptionandutilizeMoveIt![22]motion
plannerforthetransformationfromthetargetend-effectorposetojointpositiontrajectories. Inthe
Unitreerobotdogsetting,weleverageaUnitreeB1dogwithaZ1arm,alongwithaRobotiq2F-85
parallelgripper. TheRealSenseD415cameraisalsoon-handmounted,andwecontrolthearmusing
theZ1SDKfordeltacartesian-spacecontrol.
Forgraspgeneration,weutilizeAnyGrasp[21]toproducegraspproposals,alongwithGSNet[20]
witharelativelylowgraspnessscorethresholdandcollisionthresholdformoredensegraspproposals
incasethereisnograspposecloseenough.
E DownstreamApplicationDetails
E.1 TrainingACTPolicy
Forpolicydistillation,weutilizeanACTpolicy[39]toperformimitationlearningfromourself-
collecteddemonstrations. ACTisbasedonCVAETransformerarchitectureandadoptstheideaof
actionchunkingtomitigatecompoundingerrorsthatarecommoninbehaviorcloning(BC).More
detailscanbefoundintheiroriginalpaper[39].
Weuse5RGBviews(5×640×480×3)andtherobot’sproprioceptionasobservation. Weset
thechunksizeto60,andthelatentspacedimensionto512. WeuseL1lossplusKLdivergence
regularization for supervision. The number of training iterations is set to 200K, and we set the
learningrateto1×10−5andbatchsizeto8.
E.2 One-ShotVisualImitationDetails
Forone-shotvisualimitationconditionedonhumanpreference,wepickoutdemonstrationseither
fromourownin-domaindemonstrationsorfromout-of-domaincartoonimages(TomandJerryinthis
case). WegroundandchoosethefirstframeofinteractionforIS andusethecustomdataannotation
methodforaffordanceextraction. Wethenskipthehierarchicalretrievalstepanddirectlyusethese
chosendemonstrationsforaffordancetransferandlifting,followedby3Daffordanceexecution.
16E.3 LLM/VLMIntegrationDetails
ForLLM/VLMintegration,weutilizeGPT-4V(gpt-4-vision-preview)[76]fortaskdecomposi-
tionandsceneunderstanding.WealsouseGrounded-SAM[77]forobjectdetectionandsegmentation
toproduce3Dboundingboxesofobjectsinthescene.
Specifically,wedefine3basicprimitives: grasp(),move_to(),andrelease()forVLMoutput.
Notethatthesethreeprimitivesdonotinvolveheuristicsonspecificobjectmanipulation. Otherthan
theseprimitives,whentheVLMfindsouttherearerelevantdemonstrationsintheaffordancememory,
itwillscheduletheproposedRAMsystemasaretrieval-augmentedmoduletoperformtheactionas
awhole,followedbyotherdefinedprimitives.
AnexampleofourpromptandtheVLMoutputisshowninFig.7.
17Prompt
===========
You are an intelligent robot dog that has an arm with a parallel gripper
for object manipulation.
You are given a human instruction and a scene observation. Your task is
to correctly manipulate the objects safely conditioned on the
instruction.
===========
You have a series of primitives and demonstrations you can leverage to perform the task. Based on the instruction, you can freely decompose it into
several sub-tasks that are easier to finish and then chain them together.
First, you are endowed with 3 primitives, which are:
1. grasp(), which takes in object name, detects the object, moves to a graspable pose, and closes the gripper.
2. move_to(), which takes in a 6D pose and does motion planning to it.
3. release(), open the gripper to release the holding object.
The calling format should be like release(), etc.
Apart from the primitives, you also have an affordance memory, which contains lots of tasks that already have plenty of demonstrations. You are also
equipped with a powerful robotic system called RAM, which can utilize those demonstrations in your affordance memory to directlyperform
corresponding sub-tasks, without calling the primitives. The tasks in your affordance memory are:
-Open: drawer, cabinet, microwave, dishwasher, refrigerator, trashcan
-Close: drawer, cabinet, microwave
-Pickup: pot, mug, bowl, bottle
The calling format should be like Open_drawer(), etc.
For the instruction and observation, please output a sequence of actions or primitives in the format of function calling as above. And finally, you
should convert the sequence into a python list for convenience.
===========
A basic example:
Human instruction: Please open the drawer and pick up the utensil.
Observation: A closed drawer.
Your output:
<agent>
+++
First, I will decompose the instruction into several sub-tasks. The instruction is 'Please open the drawer and pick up the utensil.', so I need to
first open the drawer, and then pick up the utensil.
Sub-tasks:
1. open the drawer
2. pick up the untensil
+++
Then, for each subtask, I will see whether it can directly be performed using my affordance memory.
For 1, there are demonstrations regarding opening drawers, so I will directly use it.
For 2, there are no demonstrations for utensils, but picking up bottle is similar. Although usable, I will use primitives to ensure safety.
Action sequence:
1. Open_drawer()
2. grasp(utensil)
3. move_to(utensil_above_position)
4. move_to(release_position)
5. release()
+++
Finally, I will convert the sequence into a python list:
<output>
['Open_drawer()', 'grasp(utensil)', 'move_to(utensil_above_position)', 'move_to(release_position)', 'release()']
</output>
+++
</agent>
===========
Now you need to clear this table, please find a feasible way of doing that. Follow the example above:
Human instruction: Please clear the table. VLM Output
Observation: There is a bottle on the table, and a trashcan with cups and bottles next to it.
<agent>
+++
First, I will decompose the instruction into several sub-tasks. The instruction is 'Please clear the table', so I need to first pick up the bottle
from the table, move it to the trashcan, and release it.
Sub-tasks:
1.Pick up the bottle from the table.
2.Move the bottle to the trashcan.
3.Release the bottle into the trashcan.
+++
Then, for each subtask, I will see whether it can directly be performed using my affordance memory.
For 1, there are demonstrations regarding picking up a bottle, so I will directly use it.
For 2 and 3, I will use the primitives to ensure the correct and safe movement to the trashcan and releasing the bottle.
Action sequence:
1.Pickup_bottle()
2.move_to(trashcan_position)
3.release()
+++
Finally, I will convert the sequence into a python list:
<output>
['Pickup_bottle()', 'move_to(trashcan_position)', 'release()']
</output>
+++
</agent>
Figure7: ExamplepromptandVLMoutputsofourLLM/VLMintegrationsystem.
18