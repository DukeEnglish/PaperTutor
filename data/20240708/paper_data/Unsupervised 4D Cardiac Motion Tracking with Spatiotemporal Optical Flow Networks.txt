Unsupervised 4D Cardiac Motion Tracking with
Spatiotemporal Optical Flow Networks
Long Teng1,2,3, wei.feng1,2,4, menglong.zhu3, and xinchao.li3
1 Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,
Shenzhen 518055, China
2 University of Chinese Academy of Sciences, Beijing 100049, China
3 DJI Technology Co., Ltd., DJI Sky City, No. 55 Xianyuan Road, Nanshan District,
Shenzhen, China
4 Shenzhen University of Advanced Technology
Abstract. Cardiacmotiontrackingfromechocardiographycanbeused
toestimateandquantifymyocardialmotionwithinacardiaccycle.Itis
a cost-efficient and effective approach for assessing myocardial function.
However,ultrasoundimaginghastheinherentcharacteristicsofspatially
lowresolutionandtemporallyrandomnoise,whichleadstodifficultiesin
obtaining reliable annotation. Thus it is difficult to perform supervised
learningformotiontracking.Inaddition,thereisnoend-to-endunsuper-
vised method currently in the literature. This paper presents a motion
tracking method where unsupervised optical flow networks are designed
withspatialreconstructionlossandtemporal-consistencyloss.Ourpro-
posed loss functions make use of the pair-wise and temporal correlation
to estimate cardiac motion from noisy background. Experiments using
a synthetic 4D echocardiography dataset has shown the effectiveness of
ourapproach,anditssuperiorityoverexistingmethodsonbothaccuracy
and running speed. To the best of our knowledge, this is the first work
performed that uses unsupervised end-to-end deep learning optical flow
network for 4D cardiac motion tracking.
Keywords: Echocardiography·Motion·Opticalflow·NeuralNetworks.
1 Introduction
4D motion tracking is used to describe and quantify how the myocardial wall
moves in 3D sequence data. It is a fundamental analysis of 3D image sequences,
which also helps other tasks like segmentation, classification, and abnormal de-
tection. Other than pair-wise 3D image registration or optical flow estimation,
4D motion tracking depends on both pair-wise constraint and sequence correla-
tion to estimate the possible motion fields.
Severalapproacheshavebeenproposedtoapproximatemotiontracking[1,2,3].
De Craene et al. proposed a 3D+t diffeomorphic based registration to estimate
the motion fields [1]. Their registration uses a b-spline parameterization over
the velocity field, but its Lagrangian displacements are prone to accumulate
4202
luJ
5
]VC.sc[
1v36640.7042:viXra2 Long Teng, wei.feng, menglong.zhu, and xinchao.li
error by integrating the velocities. Nripesh et al. proposed a dynamic program-
ming method to accomplish patch matching. However, they did not make use
of the spatial constraints [2]. A recent improvement from Nripesh et al. uses
theSiamesesimilarityastheweightsofdynamicprogrammingtoconstructspa-
tiotemporalconstraints[3].Althoughthisworkmakesusesofspatialconstraints,
their dynamic programming still relies on the segmentation result. To summa-
rize, the key limitations of existing methods for 4D cardiac motion tracking are:
1)consideringonlyspatialortemporalcorrelation[2],2)extraerrorsfromeachof
multi-stage method [3], and 3) error accumulation when integrate the velocities
over time [1].
Recent research efforts have shown the possibility of using a Convolutional
NeuralNetwork(CNN)basedopticalflowestimationbasedonimageprocessing
[4,5,6] and can potentially overcome the limitations of the existing methods for
4D cardiac motion tracking as we state above. Dosovitskiy et al. proposed a
Flow-Net which stacks two images as input and output the flow field in cascade
multi-scale [4]. Later, Fan et al. considered temporal correlation and proposed
a TVNet [5], which is based on TVL1 [6] to classify videos using optical flow
estimation. However, Flow-Net [4] was trained on large scale synthetic images
withgroundtruthlabels,whichisinfeasibleformedicaldataanalysis,wherewe
usuallydonothavegroundtruthannotationfortracking.TVNet[5]ispromising,
but it was designed for 2D image sequence and directly extend the 3D version
on4Dcardiacmotiontrackingproblemwouldbeproblematiconcomputational
memory, hence hurt model performance .
Toovercomethoselimitations,wemakeuseof4Dspatiotemporalinformation
in designing the CNN model and loss function (shown in figure 1). From the
model aspect, a trainable optical flow is used as a guideline to estimate motion
fields.Fromthelossfunctionaspect,temporal-consistencyisusedforleveraging
thetemporalcorrelation,andreconstructionlossisusedforleveragingthespatial
correlation. A reference frame is used to reduce the accumulation error. The
contributions of this paper, therefore, can be summarized in two ways:
– Anovel,3Dunsupervisedopticalflownetworktoestimatemotionfieldswas
developed.
– Different than existing methods, we simultaneously invoke constraints on
temporal-consistency and spatial reconstruction by using novel losses to
leverage the spatiotemporal correlation.
2 Model Architecture
Among previous optical flow network architectures, we use the TV-L1 based
networks and extend it into 3D [7]. The benefit of this network is the minimal
requirement of the training dataset. To derive the formulation of 3D networks,4D Cardiac Motion Tracking with Optical Flow Networks 3
Fig.1. Overview of our Cardiac Motion Tracking with Spatialtemporal Networks
framework:Theopticalflownetworktakesthereferenceframe,andanimagesequence
asinputandoutputsreconstructedsequence.Threelossfunctionsareillustratedwith
the orange arrows and equations. Notice that, the 3D sequence contains both forward
time sequence and reverse time sequence will be discussed in section 3.
webeginfromtheoriginalTV-L1withL1normHornandSchunck’sequation[8].
ϕˆ =argminL(I ,I ,ϕ )=argminλL (I ,I ,ϕ )+L (ϕ )
i 0 i i sim 0 i i smooth i
ϕi ϕi
(cid:90) (cid:90) (1)
=argminλ |ρ(ϕ )|dΩ+ |∇ϕx|+|∇ϕy|+|∇ϕz|dΩ
i i i i
ϕi Ω Ω
whereI isthereferenceframe,I istheithframeinthesequencewithlengthn
0 i
and i ∈ {1,...,n}. ϕ = [ϕx,ϕy,ϕz]T represents the motion component toward
i i i i
three coordinate directions. ρ(ϕ ) = I − m(I ,ϕ ) is the similarity between
i 0 i i
inference frame and a specific frame in the sequence. With the help of fixed-
point iteration, Eq. (1) can be solved by iteration of Eq. (2) and Eq. (3) [5,9].
First,whenauxiliaryvariablevisfixed,weupdatemotionfieldϕ andthevector
i
field p that is initialized to minimize ϕ with
i
p+τ/θ·∇ϕ
ϕ =v+θ·divp; p= i , (2)
i 1+τ/θ·|∇ϕ |
i
where τ is the time-step, div is the operation of divergence. θ is the weight in
regularization term. Then ϕ is fixed, we update v by the optimal solution:
i
λθ∇I , if ρ(ϕ )<−λθ|∇I |2
 i i i
v = −λθ∇I i, if ρ(ϕ i)>λθ|∇I i|2 (3)
−ρ(ϕi)∇Ii, if |ρ(ϕ )|≤λθ|∇I |2
|∇Ii|2 i i
where ∇I =∇I (x+ϕ ) for short.
i i i
As it is shown in Fig. 1, the base module of optical flow networks contains
only9convolutionlayersanda3Dwarplayer.Theconvolutionblocksinfigure1
are marked as Conv1, Conv2 and Conv3. Each block contains three convolution
layers in three directions. They are trainable and initialized by,
∂I (x,y,z) ∂I (x,y,z) ∂I (x,y,z)
∇I = i + i + i (4)
i ∂x ∂y ∂z4 Long Teng, wei.feng, menglong.zhu, and xinchao.li
The optical flow estimation process are described in Algorithm 1.
Algorithm 1: Optical flow estimation process
Pre-defined Hyper-parameters: λ, θ, N
iters
Initial Convolution Weights:I , I , ϕ=0
0 1
w ←[[−0.5,0,0.5]] w ←[[[−1,1]]] w ←[[[−1,1]]],∀j ∈{x,y,z}
Conv1j Conv2j Conv3j
p =[p ,p ,p ]←[0,0,0] p =[p ,p ,p ]←[0,0,0]
x xx xy xz y yz yy yz
p =[p ,p ,p ]←[0,0,0] ρ(ϕ )←I (x+ϕ )+(ϕ −ϕ )∇I (x+ϕ )−I (x)
z zx zy zz 1 1 0 1 0 1 0 0
Input: I , I (i∈{1,...,n})
0 i
for N interations do
iters
λθ∇I , if ρ(ϕ )<−λθ|∇I |2
 i i i
v← −λθ∇I i, if ρ(ϕ i)>λθ|∇I i|2
−ρ(ϕi)∇Ii, if |ρ(ϕ)|≤λθ|∇I |2
|∇Ii|2 i
where ∇I =∇I (x+ϕ ) for short;
i i i
ϕ ←v+θ·divp p ← p+τ/θ·∇ϕi ,∀j ∈{x,y,z}
ij j j 1+τ/θ·|∇ϕi|
end
Output: I (x+ϕˆ), ϕˆ
i i i
3 Loss Function
Loss function is important to the deep learning based methods. In this paper,
we propose to use both spatial constraint and temporal constraints to estimate
thedesiredmotiontrackingfromopticalflow.Asisshowninfigure1,theorange
lines and equations represent the loss functions. The final loss function contains
three loss functions. Those functions are divided into two categories, temporal-
consistency loss, and spatial-reconstruction loss.
3.1 Temporal-Consistency Loss
The myocardium moves periodically in echocardiography while the noise ran-
domly appears. This physical meaning indicates that the desired motion fields
meet the temporal consistency.
As it is shown in figure 2, {I ···I } represent the 3D echocardiography
1 n
sequence. The optical flow field ϕ i→−i+1 means the optical flow from I i to I i+1.
Correspondingly, ϕ i→−i−1 is the optical flow from I i to I i−1. Every point in I 1
changes its position from frame to frame by interpolation with the optical flow
of ϕ. The forward interpolation loop is defined as follows,
f wd(ϕ)=h(h(ϕ 1→−2,ϕ 1→−2),··· ,ϕ n−1→−n) (5)
where function h represents interpolation operation that maps images by the
given optical flow. And the backward interpolation loop is,
b wd(ϕ)=h(h(ϕ n→−n−1,ϕ n→−n−1),··· ,ϕ 2→−1) (6)4D Cardiac Motion Tracking with Optical Flow Networks 5
As it is illustrated in figure 2, a selected point in I has meshed in orange
1
color.Itspositionchangesovertimebytheinterpolationwithϕ.Whenitmoves
forward to I and then moves back to I , it may not move back to its original
n 1
position.Therearetworeasonsforthiserror.Firstly,errorsexistinthepairwise
optical flow estimate. These errors accumulate throughout time and lead to
temporal inconsistency. Secondly, the inherent low resolution and random noise
of echocardiography also lead to this temporal inconsistency. To minimize the
temporal inconsistency, Temporal-Consistency loss is introduced as follows,
L Cycle(ϕ)=|ϕ 1→−2−b wd(f wd(ϕ))| (7)
The final Temporal-Consistency loss is the combination of whole cycle loss and
single cycle loss,
L =L (ϕ)+ωL (ϕ) (8)
TemporalConsistency Sig Cycle
where L Sig(ϕ)= n1 (cid:80)n i=1|ϕ i→−i+1+m(ϕ i+1→i,ϕ i+1→−i)| is the single cycle loss.
3.2 Spatial Reconstruction Loss
The spatial reconstruction loss contains two parts. First part is the similarity
between original I and the warped I with respect to ϕ. Second part is the
0 1
smoothness function of optical flow ϕ. The loss function is similar to Eq. (1).
The overall learning objective sums combines temporal loss and spatial loss,
L =γL +βL (9)
ST TemporalConsistency rec
(cid:82) (cid:82)
WherethespatiallossfunctionisL =λ |ρ(ϕ)|dΩ+ |∇ϕ |+|∇ϕ |+|∇ϕ |dΩ.
rec Ω Ω x y z
4 Experiment
4.1 Dataset
The data we used to conduct the experiment was from an open-access dataset,
3D Strain Assessment in Ultrasound(STRAUS) [10]. It contains 8 groups of B-
mode simulated voxel data of ischemic sequences with ischemia in the distal
Fig.2. The Temporal-Consistency loss for motion estimation.6 Long Teng, wei.feng, menglong.zhu, and xinchao.li
(LADDIST), proximal left anterior descending artery (LADPROX), left bundle
branch block (LBBB and LBBBSMALL), left circumflex artery (LCX), right
circumflexartery(RCA),synchronous(SYNC)andanormalgroup(NORMAL).
Eachgrouphas34to41framesof3Dimages.Andthereare286framesintotal.
4.2 Implementation Details
AllexperimentswereaccomplishedwithGTX1080GPU.Theoriginalimagehad
224×176×208voxelsofsize0.7×0.9×0.6mm3.Weperformedrandomshifting,
flipping, and cropping in three coordinate directions for the data augmentation.
Theinput3Dimageswereresizedto64×64×64withbilinearinterpolation.To
compare with ground truth, the outputs were resized back to the original size.
The scaling is a trade-off between accuracy and GPU memory limits.
The iteration number of optical flow network N is 40. The temporal
iters
sequence length is 4. The parameters in Eq. (9), γ, ω, β, λ are 1.0, 1.0, 0.5
and 0.15 representatively. The illustrated resulting frame is randomly selected
throughout the cardiac cycle.
4.3 Comparison with ground truth
Methods MSE(mm)
BSR 1.66±1.05
Data MSE(mm) EPE
DST 1.23±0.89
LADPROX0.38±0.310.78±0.74
GRPM 1.21±1.13
LBBB 0.38±0.400.88±0.73
FNT 1.10±0.74
LADDIST 0.45±0.430.95±0.79
Ours(No temporal loss) 0.42±0.52
Ours(With temporal loss)0.41±0.47
Table 1. Results of MSE and EPE on
three testing dataset
Table 2. MSE for related motion
tracking methods and our result.
Three groups of data (LADDIST, LBBB, LADDIST) with 106 frames of 3D
imageswereusedfortesting.Therest180frameswereusedfortraining.Weap-
pliedtheAverageMeanSquareError(MSE)andAverageEnd-PointError(EPE)
as the criterions for comparison. Results are listed in tabel 1,
Figure 3 is the result slice of LADDIST in x, y, z directions. The first row is
thegroundtruth,thesecondrowisourresult.Themotionfieldaresampledinto
agridatintervalsof7.Onlythemyocardiumdataisusedtoestimatethecardiac
motion.Thegroundtruthofthemotionfieldandourresultbothshowthesame
heterogeneousmotionpatterns.Normalheartcontracttowardstheheartcenter,
while ischemia in the distal lead to abnormal diastole. The estimated motion in
Figure 3 captures the abnormal motions. This could help to the diagnosis.
4.4 Ablation study of Temporal-Consistency
Reconstruction loss is the basic loss for regulation, we only perform ablation
study on temporal loss. The MSE index with Temporal-Consistency loss im-
proves 0.01±0.05 in the three dataset. The estimation error is greatest around4D Cardiac Motion Tracking with Optical Flow Networks 7
Fig.3. LADDIST:Illustratedframeisthe30thof34frames.Heterogeneousmotion
caused by ischemia is captured.
the 10th frame and smallest in the first and last frames. Although the MSE
index improvement is not obvious, it leads to visually more accurate details.
Figure 4 gives randomly selected frames from three datasets. The three
dataset contains different types of heart disease. After training with Temporal-
Consistency loss, the motion field captures more accurate heterogenours mo-
tion to the ground truth. As we can see in figure 4, although the results with
Temporal-Consistence are similar to the result without Temporal-Consistence
(consistent with limited MSE improvement shown in Table 2), it provides more
accurate details. Due to the limitation of GPU memory, we only use 4 frames in
Temproal-Consistency. We noticed that the descrenpcies in the middel frames
werelarger(showninsupplymentary).Aswecanimagining,withmoretemporal
lengthasinputfortraining,theimprovementofusingtemporalconsistencyloss
will be enlarged.
4.5 Comparison with other methods
We compared our result with the recently published results of 4 methods [11],
B-splinebasedfreeformdeformationregistration(BSR)[12],thedynamicshape
tracking(DST)[2],asimplifiedversionofthegeneralizedrobustpointmatching
algorithm (GRPM) [13] and flow net motion tracking (FNT) [11]. The results
were listed in table 2.
Ourresultssignificantlyoutperformedothermethodsandwasalmosthalfof
their errors. Beside, our average running speed is 0.3s per frame on GTX1080
GPU which was promising for clinical application.8 Long Teng, wei.feng, menglong.zhu, and xinchao.li
Fig.4. LBBB, LADDIST, LADPROX: Randomly selected results from three
testing datasets. The comparison of the motion field with and without training with
the Temporal-Consistency loss function.
5 Conclusion and future work
Inthispaper,weproposedanovelend-to-endunsupervisedopticalflownetwork
for4Dcardiacmotiontracking.Ourmodelleveragesthespatio-temporalcorrela-
tion by using the reconstruction loss and temporal-consistency loss. We validate
our method quantitatively and qualitatively on three synthetic 4D echocardiog-
raphy datasets. The presented networks outperformed the alternative methods
in terms of accuracy and running speed (0.3s per frame). Most importantly, our
method does not require ground truth for training, and thus overcomes the an-
notation limitation. To the best of our knowledge, this is the first effort that
uses unsupervised end-to-end deep learning on 4D cardiac motion tracking. Our
method shows the potential of applying to real data. However, real data usually
only has limited sparse key points as the ground truth for motion tracking[14]
and lacks fair evaluation metrics for motion tracking results. Our future work is4D Cardiac Motion Tracking with Optical Flow Networks 9
to adapt the proposed algorithm on real data and propose suitable evaluation
metrics.10 Long Teng, wei.feng, menglong.zhu, and xinchao.li
References
1. Mathieu De Craene, Gemma Piella, Oscar Camara, Nicolas Duchateau, Etelvino
Silva, Adelina Doltra, Jan D¡¯hooge, Josep Brugada, Marta Sitges, and Alejan-
dro F Frangi. Temporal diffeomorphic free-form deformation: Application to mo-
tion and strain estimation from 3d echocardiography. Medical image analysis,
16(2):427–450, 2012.
2. Nripesh Parajuli, Allen Lu, John C Stendahl, Maria Zontak, Nabil Boutagy,
Melissa Eberle, Imran Alkhalil, Matthew O¡¯Donnell, Albert J Sinusas, and
James S Duncan. Integrated dynamic shape tracking and rf speckle tracking for
cardiacmotionanalysis. InInternationalConferenceonMedicalImageComputing
and Computer-Assisted Intervention, pages 431–438. Springer, 2016.
3. Nripesh Parajuli, Allen Lu, Kevinminh Ta, John Stendahl, Nabil Boutagy, Im-
ranAlkhalil,MelissaEberle,Geng-ShiJeng,MariaZontak,MatthewO¡¯Donnell,
et al. Flow network tracking for spatiotemporal and periodic point matching:
Applied to cardiac motion analysis. Medical image analysis, 55:116–135, 2019.
4. Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,
Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox.
Flownet: Learning optical flow with convolutional networks. In Proceedings of the
IEEE international conference on computer vision, pages 2758–2766, 2015.
5. Lijie Fan, Wenbing Huang, Chuang Gan, Stefano Ermon, Boqing Gong, and Jun-
zhou Huang. End-to-end learning of motion representation for video understand-
ing. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 6016–6025, 2018.
6. ChristopherZach,ThomasPock,andHorstBischof. Adualitybasedapproachfor
realtimetv-l1opticalflow. InJointpatternrecognitionsymposium,pages214–223.
Springer, 2007.
7. Kai Guo, Prakash Ishwar, and Janusz Konrad. Action recognition using sparse
representationoncovariancemanifoldsofopticalflow. In2010 7th IEEE interna-
tional conference on advanced video and signal based surveillance, pages 188–195.
IEEE, 2010.
8. Berthold KP Horn and Brian G Schunck. Determining optical flow. Artificial
intelligence, 17(1-3):185–203, 1981.
9. JavierSa´nchezP´erez,EnricMeinhardt-Llopis,andGabrieleFacciolo.Tv-l1optical
flow estimation. Image Processing On Line, 2013:137–150, 2013.
10. MartinoAlessandrini,MathieuDeCraene,OlivierBernard,SophieGiffard-Roisin,
PascalAllain,IrinaWaechter-Stehle,Ju¨rgenWeese,EricSaloux,Herv´eDelingette,
Maxime Sermesant, et al. A pipeline for the generation of realistic 3d synthetic
echocardiographicsequences:Methodologyandopen-accessdatabase. IEEEtrans-
actions on medical imaging, 34(7):1436–1451, 2015.
11. NripeshParajuli,AllenLu,JohnCStendahl,MariaZontak,NabilBoutagy,Imran
Alkhalil,MelissaEberle,BenALin,MatthewO¡¯Donnell,AlbertJSinusas,etal.
Flow network based cardiac motion tracking leveraging learned feature matching.
InInternational Conference on Medical Image Computing and Computer-Assisted
Intervention, pages 279–286. Springer, 2017.
12. Dirk-Jan Kroon. B-spline grid, image and point based registration. 2008.
13. Ning Lin and James S Duncan. Generalized robust point matching using an ex-
tended free-form deformation model: Application to cardiac images. In 2004 2nd
IEEEInternationalSymposiumonBiomedicalImaging:NanotoMacro(IEEECat
No. 04EX821), pages 320–323. IEEE, 2004.4D Cardiac Motion Tracking with Optical Flow Networks 11
14. CatalinaTobon-Gomez,MathieuDeCraene,KristinMcleod,LennartTautz,Wen-
zheShi,AnjaHennemuth,AdityoPrakosa,HenguiWang,GerryCarr-White,Stam
Kapetanakis,etal.Benchmarkingframeworkformyocardialtrackinganddeforma-
tionalgorithms:Anopenaccessdatabase. Medical image analysis,17(6):632–648,
2013.