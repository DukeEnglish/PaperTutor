Lost in Translation: The Algorithmic Gap Between LMs and the Brain
TommasoTosato1 PascalJrTikengNotsawo23 SaskiaHelbling4 IrinaRish23 GuillaumeDumas12
Abstract ablelensforcomparingLMsandhumanlanguageprocess-
Language Models (LMs) have achieved impres- ing. Marr proposed that information processing systems
siveperformanceonvariouslinguistictasks, but canbeunderstoodatthreedistinctlevels:
their relationshipto humanlanguageprocessing
• Computationallevel: Whatisthegoalofthecompu-
in the brain remains unclear. This paper exam-
tation? Thislevelfocusesontheabstractfunctionbe-
inesthegapsandoverlapsbetweenLMsandthe
ingcomputed,independentofhowitisimplemented.
brainatdifferentlevelsofanalysis,emphasizing
In this context, it refersto tasks like language under-
the importance of looking beyond input-output
standing,questionanswering,andtranslatingbetween
behavior to examine and compare the internal
languages. Comparisons of input-output behaviors
processes of these systems. We discuss how in-
pertaintothislevel.
sightsfromneuroscience,such assparsity,mod-
ularity, internal states, and interactive learning, • Algorithmiclevel: Howarethe computationalgoals
caninformthedevelopmentofmorebiologically achieved? This level deals with the specific meth-
plausiblelanguagemodels. Furthermore,weex- ods and strategies employed to transform inputs into
ploretheroleofscalinglawsinbridgingthegap outputs. Inthis context, it examinesthe mechanisms
betweenLMsandhumancognition,highlighting used to parse sentences, represent semantic informa-
the need for efficiency constraints analogous to tion,andgeneratecoherentanswers.Analyzingwhich
thoseinbiologicalsystems. BydevelopingLMs operations intermediate representations undergo per-
that more closely mimic brain function, we aim tainstothislevel.
toadvancebothartificialintelligenceandourun-
• Implementationallevel: Howarethesemechanisms
derstandingofhumancognition.
physically realized? This level concerns the actual
hardware (biologicalor artificial) that carries out the
1. Introduction computations. For example, the brain uses spiking
neuronsandelectrochemicalsignaling,whileLMsrun
LargeLanguageModels(LMs)haveachievedremarkable
onsiliconhardwarewithfloating-pointarithmetic.
performance on a wide range of language tasks, from
machine translation to question answering (Brownetal., ClarifyingwhataspectsofcognitionLMsareactuallymod-
2020). This success has led to speculation that LMs may eling is crucial. Mahowaldetal. (2024) argue that while
provide valid models of human intelligence. However, LMsexhibitremarkableformallinguisticcompetence(the
comparisonsbetweenLMsandthebrainareoftenlimited ability to produce fluent and grammatical text), they still
totheinput-outputlevel,whichcanbemisleading. lackrobustfunctionalcompetence(thecapacitytouselan-
guage to reason and achieve real-world goals). However,
ItwouldbeincorrecttoassumethatLMs,becausetheypro-
theirperformancehasledtotheideathat,beyondmodeling
ducehuman-likelinguistic outputs, mustprocessinforma-
language perception, they may also model more complex
tion similarly to the human brain. David Marr’s levels of
cognitiveabilities such as reasoning(Jones, 2024), theory
analysisframework(Marr&Vision,1982)providesavalu-
ofmind(Strachanetal.,2024),andbuildingworldmodels
1CHUSainteJustineResearchCenter,DepartmentofPsychi- (Lietal.,2023).
atry,Universite´ deMontre´al,Montreal,QC,Canada2Mila,Mon-
treal,QC,Canada3DepartmentofComputerScienceandOpera- Mitchell&Krakauer (2023) contend that despite their
tionResearch,Universite´ deMontre´al, Montre´al, Canada4Ernst seeminglyintelligentbehavior,LMsdonotunderstandthe
Stru¨ngmannInstituteforNeuroscienceinCooperationwithMax datatheyprocessinthesamewayhumansdo. Thebrittle-
Planck Society, Frankfurt am Main, Germany. Correspondence
nessandlack ofrobustgeneralizationin these modelsare
to:TommasoTosato<tosato.tommaso.office@gmail.com>.
keyindicatorsoftheirlackoftrueunderstanding.LMsare
WorkshoponLargeLanguageModelsandCognition,41stInter- fundamentally retrieval systems that generate outputs by
nationalConferenceonMachineLearning,Vienna,Austria. recognizingandinterpolatingpatternsintheirtrainingdata
1
4202
luJ
5
]CN.oib-q[
1v08640.7042:viXraLostinTranslation:TheAlgorithmicGapBetweenLMsandtheBrain
(i.e., approximate retrieval) (Kambhampati, 2024). This 2014).Thiswasdemonstratedbyusingtechniqueslikerep-
leadstohighperformancefortaskinstanceswhicharewell resentationalsimilarityanalysis(RSA)(Kriegeskorteetal.,
representedin the trainingdataand failuresforanalogous 2008) to compare the geometry of neural representations
instances which are less represented (McCoyetal., 2023; acrossmodelsandbrainregions.Similarly,inthelanguage
Wuetal., 2024; Lewis&Mitchell, 2024). In contrast, hu- domain,studieshavefoundcorrespondencesbetweenLM
man understanding is based on rich, causal mental mod- activations across layers and brain activity in different
elsoftheworld,ratherthanmassivestatisticalcorrelations parts of the language network (Caucheteuxetal., 2023;
learned by LMs. These mental models enable humans to Milletetal., 2023). Shared representational spaces have
makerobustpredictions,generalizations,andanalogies,to been uncoveredthroughencodingmodels, which produce
reason compositionally and counterfactually, and to inter- simulated brain activity from LM embeddings through
veneintheworldtotesthypothesesactively. linear regression and correlate the simulated activity
with recorded neural data (Goldsteinetal., 2022). The
A key challenge in understanding the true relationship
correlation strength is reflected in an index called brain
between LMs and the brain is to move beyond compar-
score (Schrimpfetal., 2021). However, it is important
isons of input-output behavior and intermediate represen-
to note that both the visual system and the language
tations(i.e.,thecomputationallevel),andfocusonprobing
network are not simple feedforward hierarchies but
the internal processes of these systems (i.e., the algorith-
rather highly recurrent networks with pervasive feedback
miclevel)usingcausalinterventionsandmechanisticinter-
connections and extensive cross-talk between regions
pretability. This argumentis developedin the first partof
(Felleman&VanEssen, 1991; Pessoa, 2023). More-
thepaper.Wethenproceedtoidentifykeypropertiesinthe
over, it is crucial to understand that representational
architecturesandtrainingproceduresofLMswhichdiffer
similarity between LMs and brains pertains primarily to
fromthebrainandproposepossibilitiestoreconcilethem.
the computational level, as it captures the input-output
Bydoingso,weaimtodevelopLMsthatbettercapturehu-
mappings of intermediate computational steps, but does
mancognitiveprocessing,andassuch,couldserveasmore
notnecessarilyimplythatsimilaroperationsoralgorithms
faithfulmodelsofthebrain.
are used to transform those representations (Schynsetal.,
2022;Antonello&Huth,2024;Tuckuteetal.,2024). Two
2. Probing Representations and Processes
systemscanexhibitsimilarrepresentationswhilediffering
To determine whether LMs process information in ways substantially in their algorithms. This is evidenced by
analogous to the human brain, it is essential to look be- the fact that not only causal transformers (e.g., GPT),
yond surface-level behavior. While LLMs have recently but also masked transformers (e.g., BERT), LSTMs and
been claimed to pass the Turing test (Jones&Bergen, RNNsgenerateinternalrepresentationsthatcorrelatewith
2024), Dumasetal. (2014) argue that the classic Turing brain activity, and there is no clear agreement on which
test, which focuses solely on external behavior, is insuffi- one correlates best (Pasquiouetal., 2022; Andersonetal.,
cientforinvestigatingthegenuinenessoftheinternalmech- 2021;Toneva&Wehbe,2019;Ootaetal.,2022). Tomake
anismsofmachineintelligence. Thisisespeciallytruefor claims about algorithmic similarities, we need evaluation
language, where the ELIZA effect can lead people to at- approachesthatcanprobetheinternaldynamicsandcausal
tribute human-likequalitiesto any system capableof gen- structureofLMs(Belinkov,2022)andcomparethemwith
eratingcoherenttext(Weizenbaum,1976). Moreover,per- resultsobtainedusingsimilarapproachesinthebrain.
formance comparisons between LMs and humans at the
behavioral level are subject to several pitfalls. These in-
Mechanistic Interpretability. This approach involves
cludemodelstrainedspecificallytosolvebenchmarksthat
analyzing and understandingthe internalcomponentsand
claim to quantify general cognitive abilities but do so in
processes of a machine learning model to explain how it
arbitrary and limited ways (Liuetal., 2024), and the risk
transforms inputs into outputs at a granular, algorithmic
of data contamination, where test data may have been in-
level (Elhageetal., 2021). It can be used to find units or
cluded in the model’s training set. This section outlines
circuits within LMs that encodeparticular syntactic or se-
possible approachesfor comparingLMs and the brain be-
mantic properties. However, individual neurons in LMs
yondtheinput-outputlevel:
respond to a variety of seemingly unrelated and heteroge-
neousfeatures,posinga challengeforinterpretingthenet-
Hierarchical Correspondences of Representations. work’s behavior. Recent work addresses this issue by us-
Research in computer vision has shown that repre- ing sparse autoencodersto extract monosemanticfeatures
sentations learned by Convolutional Neural Networks from transformer language models (Brickenetal., 2023;
across layers correlate with brain activity in different Cunninghametal., 2023; Templetonetal., 2024), includ-
regions of the visual hierarchy (Yamins&DiCarlo, ingabstractconceptsthatgeneralizeacrossmodalitiesand
2016;Kriegeskorte,2015;Khaligh-Razavi&Kriegeskorte, languages. These learned features are significantly more
2LostinTranslation:TheAlgorithmicGapBetweenLMsandtheBrain
interpretable than the model’s original neurons and ex- processmodelsofthebrain(Dohmatobetal.,2020),based
hibit interesting propertiessuch as universality across dif- on,e.g.,structuralandfunctionalconnectivity.
ferent random seeds and feature splitting as the number
oflearnedfeaturesincreases. Comparingthesefeaturesto 3. TowardBrain-Inspired LanguageModels
thoserecordedinbrainnetworks(Jamalietal.,2024)could
WhileLanguageModels(LMs)haveachievedhuman-like
provideamoreaccuratecorrespondencebetweenLMsand
performanceonseveraltasks, theystill differsignificantly
the brain. Notably, neurons in the brain are also seman-
from the human brain in the ways they achieve these re-
tically heterogeneous to a certain degree (Leonardetal.,
sults. As discussed in the previous section, better meth-
2024;Tyeetal.,2024),suggestingthatsomedegreeofpol-
ods to test for these differences at the algorithmic level
ysemanticitymaybeageneralpropertyofartificialandbi-
are needed. In this section, we explorehow LMs and the
ologicalneuralsystems.
brain differ in terms of architectures and learning dynam-
ics, and how these differences lead to divergences at the
Ablation and Stimulation Studies. These techniques
algorithmiclevel. By examiningthese disparities, we can
canprovidecausalinsightsinboththebrainandLMs. Le-
identify key areas where LMs could be modified to more
sions (Brocaetal., 1861; Dronkersetal., 2004) and elec-
closelyresemblethebrain’sinformationprocessingmech-
tricstimulation(Doron&Brecht,2015;Rofesetal.,2019)
anisms. Themotivationforreconcilingthesedifferencesis
studies have allowed mapping of the functional organi-
twofold: 1) By developing LMs that more closely mimic
zation of language in the human brain. More recently,
thebrain’sarchitectureanddynamics,wecanprovideneu-
optogenetics has enabled targeting and re-activating spe-
roscientists with more accurate computational models of
cificneuronalensembles,whichwerepreviouslyactivated
language processing in the brain. This can lead to new
by certain stimuli (Liuetal., 2012). Analogous experi-
hypothesesandinsightsinneuroscience(Jainetal.,2024),
ments in LMs, such as pruning, editing or fixing specific
fosteringadeeperunderstandingofhumancognition.2)In-
weights,canrevealhowdifferentpartsofthenetworkcon-
corporatingbrain-inspiredfeaturesintoLMsmayresultin
tributetolinguisticbehavior(Mengetal.,2023;Vigetal.,
artificialsystemsthatexhibitmorehuman-likeintelligence.
2020). Additionally, activatingor suppressingspecific ex-
This could lead to AI that is more flexible, efficient, and
tractedfeaturescaninfluencethemodel’soutput,andallow
capable of handling reasoning and planning in ways that
steering it in specific directions (Templetonetal., 2024;
currentmodelsstrugglewith.
Marksetal.,2024).Forinstance,clampingcertainfeatures
tohighvaluescaninducemodelstogeneratespecifictypes
ofcontentoraltertheirbehaviorinpredictableways. Com- SparseConnectivityandModularity. Thebrainischar-
parableeffectsofperturbationsinLMsandthebrainwould acterized by sparse connectivity and modularity, with
suggestalgorithmicequivalence. specialized networks for different cognitive functions
(Bassettetal.,2018;Seguinetal.,2023).Incontrast,trans-
Controlled Linguistic Probes and Interventions. former models have dense, all-to-all connectivity. Dur-
These can be combined with interpretability techniques ingdevelopment,thebrainundergoessynapticpruning,re-
to gain a more mechanistic understanding of language sultinginsparser,moreefficientnetworks(Paolicellietal.,
processing in LMs (Marvin&Linzen, 2018; Geigeretal., 2011). Similar pruning emerges in biologically inspired
2021; Zhouetal., 2024). By systematically perturbing spiking neural networks (Volzheninetal., 2022). To in-
inputsorfine-tuningLMsonspecifictasks,wecanobserve corporate some level of modularity in LMs, Mixture-of-
how internal representations change in response to these experts (MoE) architectures (Xueetal., 2024) can be em-
interventions. Similar approaches in neuroscience can ployed, leading to improvedcomputationalefficiencyand
provide complementary insights and offer ground for morefunctionallyspecializedsub-networks. However,the
comparison. modulesin MoE are notinterconnected,hierarchicallyor-
ganized, or able to learn to attend to specific parts of the
Meta-representationsasRepresentationofProcesses . inputs.TheRecurrentIndependentMechanisms(RIMs)ar-
Proposed by Kanaietal. (2024), this framework offers a chitecture (Goyaletal., 2019) addresses these limitations
promising approach for comparing the internal workings byintroducingsparseinteractionsamongfunctionallyspe-
of LMs and the brainat the algorithmiclevelby focusing cialized modules that interact sparsely through attention.
on the processesthatgeneraterepresentations, ratherthan Thisenablesthesemodulestolearntoattendtospecificin-
just the representations themselves. In the case of LMs, putpartsandallowsforhierarchicalstacking. Sparsitycan
we can use the patterns of weights across different layers be promotedthroughadjustmentof attention mechanisms
andattentionheadsasawaytorepresenttheprocessesthat (e.g., limiting the number of tokens each token attends to
generatethemodel’soutputs.Wecanthencomparethetwo (Childetal.,2019)),choosingregularizationmethodsalter-
atthealgorithmiclevelbyrelatingtheseweightpatternsto native to L2 (e.g., L1 regularization and elastic net), and
3LostinTranslation:TheAlgorithmicGapBetweenLMsandtheBrain
applyingdropoutandpost-trainingpruning. readyshowsomesimilaritiestobiologicallearning,exhibit-
ingprogressivelearningoffeaturesofincreasingcomplex-
Internal States. The human brain maintains and con- ity(Nakkiranetal.,2019;Mangalam&Prabhu,2019).
stantly updates rich internal representations over time,
whereasLMshaveafixedcontextwindow,andonlyfeed- Embodiment, Grounding, and Active Learning. Hu-
forward connections. Incorporating internal states into manslearnlanguagethroughsituatedinteractionswiththe
LMscouldchangethewaytheymaintainanduse context. physical and social world, grounding linguistic meanings
Recent work on adding recurrence to transformer models inperceptual,motor,andaffectiveexperiences(Bisketal.,
(Hwangetal.,2024)isastepinthisdirection. Statespace 2020;Barsalou,2008;DiPaoloetal.,2018). LMs,incon-
models(Gu&Dao, 2023), which explicitlymodelthe dy- trast, are typically trained on disembodied text data with
namics of latent states, are another promising approach. a fixed training objective, limiting their ability to capture
Models that learn to update and maintain internal repre- the full depth of linguistic meaning. To address this limi-
sentations, may also dynamicallyencodeand integrate in- tation, multimodalmodelsthatintegrateinformationfrom
formation over time in a brain-like manner (Hassonetal., vision,audition,andothersensorymodalitiesareapromis-
2015). Moreover,recurrentprocessingappearsto be criti- ing approach (Team, 2024). However, true embodiment
calforconsciousness(Chalmers,2023). may require more than just passive perception—it may
needactiveinteractionandexploration(Engeletal.,2013).
Learning Dynamics. The brain’s learning process is Coupling LMs with external execution modules that can
characterized by continual, incremental acquisition of perform actions or specialized reasoning tasks could pro-
knowledgeoveralifetime,utilizingmoderateresourcesef- vide an action space. However, just adding an action
ficiently. A key feature of brain developmentis the pres- space when the model’s weights are frozen won’t gener-
enceofcriticalperiods,duringwhichthebrainisespecially ate active learning. For true active learning, the model
receptivetospecifictypesofinputandlearningexperiences needstobeabletoselectthetrainingdatathroughactions.
(Hensch,2005). Moreover,brainlearningdynamicsmain- Integrating LMs into robotic systems (Tellexetal., 2020;
tainadelicatebalancebetweenacquiringnewinformation Collaborationetal.,2024)isanotheravenueforgrounding
andselectivelyforgettingunnecessarydetails. In contrast, languageinembodiedexperience.
current LMs are typically trained from scratch using ran-
dom weight initialization on massive corpora, requiring SocialSkillsandInteractiveLearning. Thebrain’slan-
multiple passes over large datasets and consuming enor- guage system is fundamentally geared towards communi-
mous computational resources. Additionally, fine-tuning cation and social interaction, with dedicated neuralmech-
or training LMs on new tasks often leads to catastrophic anisms for cognitive skills like pragmatics, social reason-
forgetting (Kirkpatricketal., 2017), highlighting the lack ing, andtheoryofmind(Scott, 2019;Hagoort&Indefrey,
ofbalanceinretainingoldknowledgewhileacquiringnew 2014). WhilerecentstudieshaveshownthatLMsarestart-
information. To enable more brain-like learning in LMs, ingtoexhibitsomeoftheseabilities(Strachanetal.,2024),
severalapproachescanbeconsidered:(1)ContinualLearn- it is unclear if they are achieving these capabilities in a
ing:Adaptingcontinuallearningtechniquessuchaselastic way related to humans. To improveLMs’ social abilities,
weight consolidation(Kirkpatricketal., 2017) and experi- they could be trained on socially interactive data, such as
encereplay(Rolnicketal.,2019)couldhelpLMsmaintain multi-turndialogues,incorporatingexplicitrepresentations
abalancebetweenlearningnewinformationandpreserving ofcommunicativeintentionsandthementalstatesofinter-
importantpastknowledge.(2)CurriculumLearning:Grad- locutors. Training objectives that maximize relevance for
uallyincreasingthedifficultyoftrainingdata(Bengioetal., acommunicativecontextorrewardsuccessfulandattuned
2009)canenhancelearningefficiency. ThePAIREDalgo- communicationin interactivesettings(Jaquesetal., 2019)
rithm(Dennisetal.,2020),whichusesamulti-agentsetup couldleadto morehuman-likelanguageuse. Multi-agent
wherean adversarygeneratesincreasinglychallengingen- setups where LMs interact with each other or humans in
vironments for the learning agent, is a possible approach goal-directed dialogues are promising (Bolotta&Dumas,
to implementing adaptive curriculum learning. (3) Crit- 2022).Byengagingincooperativeorcompetitivelanguage
ical Periods: Implementing a system of ”sensitive peri- games,LMscouldlearntoreasonaboutthebeliefsandin-
ods” in LM training could mimic the brain’s critical pe- tentions of other agents. Inverse reinforcement learning
riods. This could involve dynamically adjusting learning techniques (Hadfield-Menelletal., 2016) could help the
ratesfordifferentpartsofthenetworkduringtraining. For LMs to infer and align with the communicative goals of
instance, early in training, the model could have higher humaninterlocutors.
plasticity in lower layersto learn basic linguistic features,
graduallyshiftingfocustohigher-levelabstractionsinlater Reasoning and Compositional Representations. LMs’
stages. Interestingly, current artificial neural networks al- abilityto performreasoningandplanningremainslimited
4LostinTranslation:TheAlgorithmicGapBetweenLMsandtheBrain
compared to humans (Wuetal., 2023; Valmeekametal., (Kaplanetal., 2020). These scaling relationships can be
2023;Zhouetal.,2023). Techniqueslikechain-of-thought seenasglobalsignaturesofcomplexsystems(West,2018),
prompting (Weietal., 2023) and the use of scratchpads providing a quantitative way to compare brains and LMs.
(Nyeetal.,2021)togeneratestep-by-stepreasoningtraces, However, LMs benefit from virtually unlimited computa-
haveshownpromisein improvingreasoningabilities. Us- tional resources, leading to overparameterization without
ing reasoning traces as fine-tuning data has also been thesameefficiencyconstraintsasbiologicalsystems(West,
shown to enhance performance (Zelikmanetal., 2022). 1999). To promote algorithmic similarity, it may be es-
However, the brain’s ability to perform logical reasoning sential to introduce analogous pressures on LMs. Imple-
likelyreliesonmorestructured,compositionalrepresenta- mentingsuchconstraintscanhelpbridgethegapbetween
tionsof meaning(Dehaeneetal., 2015; Lakeetal., 2017). the computational prowess of LMs and the adaptive effi-
GFlowNets(Bengioetal.,2021)aregenerativemodelsthat ciency of biologicalneuralsystems, ensuring more frugal
can sample from complex joint distributions, such as the andmeaningfulapplicationofscalinglawsinLMdevelop-
distributionoversentencesandtheirmeanings.GFlowNets ment.
couldpotentiallycapturethe brain’sabilityto generatedi-
verseoutputsandlearnstructured,compositionalrepresen- 4. Conclusion
tations. Initial applications of GFlowNets to LMs are al-
The rapid advancement of Language Models (LMs) has
readyshowingpromisingresults(Huetal.,2024).Another
led to impressive performance on various linguistic tasks,
ideaistomovebeyondnext-wordpredictionastheprimary
prompting comparisons with human cognition. However,
trainingobjectiveforLMs. Thebraindoesnotsimplypre-
thispaperarguesthatdespiteachievinghuman-levelperfor-
dict the next word in a sequence but constructs rich, hier-
mance in several tasks (computational level equivalence),
archicalmeaningrepresentationsthatspanmultiplewords
LMs exhibitsignificantdivergencefromhumancognition
and sentences (Heilbronetal., 2022; Lerneretal., 2011).
inhowtheseperformancesareachieved(algorithmiclevel
Training objectives that encourage LMs to predict larger
discrepancy). We contend that the intelligence emerging
chunks of text, such as entire sentences or paragraphs,
from currentLMs is fundamentallydifferentfrom human
couldlead tomorestructuredandcompositionalrepresen-
cognition. As models continue to scale, this divergence
tations(Gloeckleetal.,2024).
may widen, potentially resulting in AI systems that excel
atspecifictasksbutlackthehallmarkcharacteristicsofhu-
OscillatoryDynamics. Onekeyfeatureoflanguagepro-
manintelligence:flexibility,robustness,andgeneralization
cessing in the brain is the presence of oscillatory neu-
capabilities.
ral dynamics at multiple timescales, with different fre-
quencybandsappearingto trackthestructureoflanguage Inthispaper,wefirstexaminedapproachesforcomparing
at different levels, from phonemes to words to phrases LMs and the brain beyondinput-outputlevel assessments.
(Giraud&Poeppel, 2012; Dingetal., 2016; Grossetal., Wehighlightedthelimitationsofsolelyanalyzinginterme-
2013). Theseoscillationsarethoughttoplayacrucialrole diate representations and emphasized the need to under-
in segmenting and chunking continuous speech into dis- standtheprocessesdrivingtransformationsbetweenthese
crete units. Incorporatingoscillatory mechanismsin deep representations. We thenexploredmajordifferencesinar-
learningarchitectures,especiallywhentheinputisrawau- chitectureandtrainingdynamicsbetweenLMsandbiolog-
ditory signals, could potentially improve their efficiency ical neural networks that likely contribute to algorithmic-
andleadtomorehuman-likespeechprocessing,giventhe level divergence, and we proposed several neuroscience-
existing gaps(Tuckuteetal., 2023). Simulation work has inspiredapproachestonarrowthisdivide.
beenexploringsubstitutingstandardneuralnetworknodes
While these biologically-inspiredapproachesmay not im-
withdampedharmonicoscillators(Rusch&Mishra,2020;
mediatelyenhancecomputationalefficiencyortaskperfor-
Effenbergeretal., 2022), demonstrating the potential use
mance,theyarecrucialfordevelopingLMsthatcanserve
ofoscillatorydynamicsforcomputation.
as more accurate models of human cognition. Such mod-
els would offer in silico representations of cognitive pro-
Allometryand Scaling Laws. Allometrystudies the re- cesses to neuroscientists and psychologists, illuminating
lationship between the size and shape of biological sys- keymechanismsunderlyinghuman-likeintelligence.Addi-
tems. Scalinglawsinthehumanbrain,suchastherelation- tionally,byconstrainingthespaceofpossiblealgorithmsto
ship between brain size and neuron count, play a crucial thosethataremorebiologicallyplausible,wemaydiscover
role in the evolution of cognitive abilities, including lan- novelarchitecturesandlearningparadigmsthatcapturethe
guage (Changeuxetal., 2021). Recently, the field of neu- keyprinciplesofhumancognition,andleadtoAIsystems
ral scaling laws has similarly started to chart the relation- thatexhibittheadaptabilityandgeneralizationcapabilities
shipsbetweenthenumberofparameters,datasetsize,com- characteristicofhumanintelligence.
putingcost, and performanceof machinelearningmodels
5LostinTranslation:TheAlgorithmicGapBetweenLMsandtheBrain
References Broca, P. et al. Remarks on the seat of the faculty of ar-
ticulatedlanguage,followinganobservationofaphemia
Anderson, A. J., Kiela, D., Binder, J. R., Fernandino, L.,
(loss of speech). Bulletin de la Socie´te´ Anatomique, 6:
Humphries,C.J.,Conant,L.L.,Raizada,R.D.,Grimm,
330–357,1861.
S., and Lalor, E. C. Deep artificial neural networks
reveal a distributed cortical network encoding proposi- Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
tionalsentence-levelmeaning. JournalofNeuroscience, Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
41(18):4100–4119,2021. Askell, A., et al. Language models are few-shot learn-
ers. Advancesinneuralinformationprocessingsystems,
Antonello, R. and Huth, A. Predictive coding or just fea-
33:1877–1901,2020.
turediscovery? an alternativeaccountofwhylanguage
models fit brain data. Neurobiology of Language, 5(1): Caucheteux,C.,Gramfort,A.,andKing,J.-R. Evidenceof
64–79,2024. a predictivecodinghierarchyin the humanbrainlisten-
ing to speech. Nature humanbehaviour, 7(3):430–441,
Barsalou,L.W. Groundedcognition. Annu.Rev.Psychol.,
2023.
59:617–645,2008.
Chalmers, D. J. Could a large lan-
Bassett, D.S., Zurn,P., andGold, J. I. Onthe natureand
guage model be conscious?, 2023. URL
useofmodelsinnetworkneuroscience. NatureReviews
https://arxiv.org/abs/2303.07103.
Neuroscience,19(9):566–578,2018.
Changeux, J.-P., Goulas, A., and Hilgetag, C. C. A con-
Belinkov, Y. Probingclassifiers: Promises, shortcomings,
nectomic hypothesis for the hominization of the brain.
and advances. Computational Linguistics, 48(1):207–
Cerebralcortex,31(5):2425–2449,2021.
219,2022.
Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
Bengio, Y., Louradour, J., Collobert, R., and Weston,
eratinglongsequenceswith sparse transformers. arXiv
J. Curriculum learning. In Proceedings of the 26th
preprintarXiv:1904.10509,2019.
Annual International Conference on Machine Learn-
ing, ICML ’09, pp. 41–48, New York, NY, USA, Collaboration, E., O’Neill, A., Rehman, A., Gupta, A.,
2009. Association for Computing Machinery. ISBN Maddukuri,A.,Gupta,A.,Padalkar,A.,Lee,A.,Pooley,
9781605585161. doi: 10.1145/1553374.1553380. URL A., Gupta, A., Mandlekar, A., Jain, A., Tung, A., Bew-
https://doi.org/10.1145/1553374.1553380. ley, A., Herzog, A., Irpan, A., Khazatsky, A., Rai, A.,
Gupta, A., Wang, A., Kolobov,A., Singh, A., Garg,A.,
Bengio,Y., Jain, M.,Korablyov,M., Precup,D., andBen-
Kembhavi,A., Xie, A., Brohan,A., Raffin,A., Sharma,
gio, S. Flow networkbasedgenerativemodelsfor non-
A., Yavary, A., Jain, A., Balakrishna, A., Wahid, A.,
iterativediversecandidategeneration. AdvancesinNeu-
Burgess-Limerick, B., Kim, B., Scho¨lkopf, B., Wulfe,
ral Information Processing Systems, 34:27381–27394,
B.,Ichter,B.,Lu,C.,Xu,C.,Le,C.,Finn,C.,Wang,C.,
2021.
Xu,C., Chi,C., Huang,C., Chan,C.,Agia,C., Pan,C.,
Bisk, Y., Holtzman, A., Thomason, J., Andreas, J., Ben- Fu,C.,Devin,C.,Xu,D.,Morton,D.,Driess,D.,Chen,
gio,Y.,Chai,J.,Lapata,M.,Lazaridou,A.,May,J.,Nis- D., Pathak, D., Shah, D., Bu¨chler, D., Jayaraman, D.,
nevich,A.,Pinto,N.,andTurian,J. Experiencegrounds Kalashnikov, D., Sadigh, D., Johns, E., Foster, E., Liu,
language,2020. F., Ceola, F., Xia, F., Zhao, F., Frujeri, F. V., Stulp, F.,
Zhou,G.,Sukhatme,G.S.,Salhotra,G.,Yan,G.,Feng,
Bolotta, S. and Dumas, G. Social neuro ai: Social inter-
G.,Schiavi,G.,Berseth,G.,Kahn,G.,Yang,G.,Wang,
actionasthe’darkmatter’ofai. FrontiersinComputer
G., Su, H., Fang, H.-S., Shi, H., Bao, H., Amor, H. B.,
Science,4,2022.
Christensen, H. I., Furuta, H., Bharadhwaj, H., Walke,
Bricken, T., Templeton, A., Batson, J., Chen, B., H.,Fang,H.,Ha,H.,Mordatch,I.,Radosavovic,I.,Leal,
Jermyn, A., Conerly, T., Turner, N. L., Anil, C., I.,Liang,J.,Abou-Chakra,J.,Kim,J.,Drake,J.,Peters,
Denison, C., Askell, A., Lasenby, R., Wu, Y., J., Schneider, J., Hsu, J., Vakil, J., Bohg, J., Bingham,
Kravec, S., Schiefer, N., Maxwell, T., Joseph, J., Wu, J., Gao, J., Hu, J., Wu, J., Wu, J., Sun, J., Luo,
N., Tamkin, A., Nguyen, K., McLean, B., Burke, J., Gu, J., Tan, J., Oh, J., Wu, J., Lu, J., Yang, J., Ma-
J. E., Hume, T., Carter, S., Henighan, T., and lik, J., Silve´rio, J., Hejna, J., Booher, J., Tompson, J.,
Olah, C. Towards monosemanticity: Decomposing Yang,J.,Salvador,J.,Lim,J.J.,Han,J.,Wang,K.,Rao,
language models with dictionary learning. Trans- K., Pertsch, K., Hausman, K., Go, K., Gopalakrishnan,
former Circuits Thread, Oct 2023. https://transformer- K.,Goldberg,K.,Byrne,K.,Oslund,K.,Kawaharazuka,
circuits.pub/2023/monosemantic-features/index.html. K., Black, K., Lin, K., Zhang, K., Ehsani, K., Lekkala,
K., Ellis, K., Rana, K., Srinivasan, K., Fang, K., Singh,
6LostinTranslation:TheAlgorithmicGapBetweenLMsandtheBrain
K. P., Zeng, K.-H., Hatch, K., Hsu, K., Itti, L., Chen, 2016.
L. Y., Pinto, L., Fei-Fei, L., Tan, L., Fan, L. J., Ott, L.,
Dohmatob, E., Dumas, G., and Bzdok, D. Dark control:
Lee,L.,Weihs, L.,Chen,M.,Lepert,M.,Memmel,M.,
The default mode network as a reinforcement learning
Tomizuka,M.,Itkina,M.,Castro,M.G.,Spero,M.,Du,
agent. Humanbrainmapping,41(12):3318–3341,2020.
M.,Ahn,M.,Yip,M.C.,Zhang,M.,Ding,M.,Heo,M.,
Srirama,M.K.,Sharma,M.,Kim,M.J.,Kanazawa,N., Doron,G.andBrecht,M. Whatsingle-cellstimulationhas
Hansen,N.,Heess,N.,Joshi,N.J.,Suenderhauf,N.,Liu, toldusaboutneuralcoding. PhilosophicalTransactions
N., Palo, N. D., Shafiullah, N. M. M., Mees, O., Kroe- ofthe RoyalSociety B: BiologicalSciences, 370(1677):
mer, O., Bastani, O., Sanketi, P. R., Miller, P. T., Yin, 20140204,2015.
P., Wohlhart, P., Xu, P., Fagan, P. D., Mitrano, P., Ser-
Dronkers,N.F.,Wilkins,D.P.,VanValinJr,R.D.,Redfern,
manet, P., Abbeel, P., Sundaresan, P., Chen, Q., Vuong,
B.B.,andJaeger,J.J. Lesionanalysisofthebrainareas
Q., Rafailov, R., Tian, R., Doshi, R., Mart’in-Mart’in,
involved in language comprehension. Cognition, 92(1-
R., Baijal, R., Scalise, R., Hendrix, R., Lin, R., Qian,
2):145–177,2004.
R., Zhang, R., Mendonca, R., Shah, R., Hoque, R., Ju-
lian, R., Bustamante, S., Kirmani, S., Levine, S., Lin, Dumas,G.,deGuzman,G.C.,Tognoli,E.,andKelso,J.S.
S., Moore, S., Bahl, S., Dass, S., Sonawani, S., Tul- The humandynamicclamp as a paradigmfor social in-
siani, S., Song, S., Xu, S., Haldar, S., Karamcheti, S., teraction. Proceedingsofthe NationalAcademyof Sci-
Adebola,S.,Guist,S.,Nasiriany,S.,Schaal,S.,Welker, ences,111(35):E3726–E3734,2014.
S., Tian, S., Ramamoorthy,S., Dasari, S., Belkhale, S.,
Effenberger, F., Carvalho, P., Dubinin, I., and Singer, W.
Park, S., Nair, S., Mirchandani, S., Osa, T., Gupta, T.,
The functionalrole of oscillatory dynamicsin neocorti-
Harada, T., Matsushima, T., Xiao, T., Kollar, T., Yu, T.,
cal circuits: a computational perspective. bioRxiv, pp.
Ding, T., Davchev, T., Zhao, T. Z., Armstrong, T., Dar-
2022–11,2022.
rell, T., Chung, T., Jain, V., Kumar, V., Vanhoucke, V.,
Zhan, W., Zhou, W., Burgard, W., Chen, X., Chen, X., Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph,
Wang, X., Zhu, X., Geng, X., Liu, X., Liangwei, X., N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly,
Li, X., Pang, Y., Lu, Y., Ma, Y. J., Kim, Y., Chebotar, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-
Y., Zhou, Y., Zhu, Y., Wu, Y., Xu, Y., Wang, Y., Bisk, Dodds,Z.,Hernandez,D.,Jones,A.,Kernion,J.,Lovitt,
Y., Dou, Y., Cho, Y., Lee, Y., Cui, Y., Cao, Y., Wu, Y.- L., Ndousse, K., Amodei, D., Brown, T., Clark, J.,
H., Tang, Y., Zhu, Y., Zhang, Y., Jiang, Y., Li, Y., Li, Kaplan, J., McCandlish, S., and Olah, C. A math-
Y., Iwasawa, Y., Matsuo, Y., Ma, Z., Xu, Z., Cui, Z. J., ematical framework for transformer circuits. Trans-
Zhang, Z., Fu, Z., and Lin, Z. Open x-embodiment: former Circuits Thread, 2021. https://transformer-
Robotic learning datasets and rt-x models, 2024. URL circuits.pub/2021/framework/index.html.
https://arxiv.org/abs/2310.08864.
Engel, A. K., Maye, A., Kurthen, M., and Ko¨nig, P.
Cunningham, H., Ewart, A., Riggs, L., Huben, R., and Where’stheaction? thepragmaticturnin cognitivesci-
Sharkey, L. Sparse autoencoders find highly inter- ence.Trendsincognitivesciences,17(5):202–209,2013.
pretablefeaturesinlanguagemodels,2023.
Felleman,D.J.andVanEssen,D.C. Distributedhierarchi-
Dehaene, S., Meyniel, F., Wacongne, C., Wang, L., and cal processing in the primate cerebralcortex. Cerebral
Pallier,C. Theneuralrepresentationofsequences:from cortex(NewYork,NY:1991),1(1):1–47,1991.
transitionprobabilitiestoalgebraicpatternsandlinguis-
Geiger,A.,Lu,H.,Icard,T.,andPotts, C. Causalabstrac-
tictrees. Neuron,88(1):2–19,2015.
tions of neural networks. Advancesin Neural Informa-
Dennis, M., Jaques, N., Vinitsky, E., Bayen, A., Russell, tionProcessingSystems,34:9574–9586,2021.
S.,Critch,A.,andLevine,S. Emergentcomplexityand
Giraud, A.-L. and Poeppel, D. Cortical oscillations and
zero-shottransferviaunsupervisedenvironmentdesign.
speech processing: emerging computational principles
Advancesin neuralinformationprocessing systems, 33:
and operations. Nature neuroscience, 15(4):511–517,
13049–13061,2020.
2012.
Di Paolo, E. A., Cuffari, E. C., and De Jaegher, H. Lin-
Gloeckle, F., Idrissi, B. Y., Rozie`re, B., Lopez-Paz, D.,
guisticbodies:Thecontinuitybetweenlifeandlanguage.
and Synnaeve, G. Better & faster large language mod-
MITpress,2018.
els via multi-token prediction. arXiv preprint arXiv:
Ding,N.,Melloni,L.,Zhang,H.,Tian,X.,andPoeppel,D. 2404.19737,2024.
Cortical tracking of hierarchical linguistic structures in
Goldstein,A.,Zada,Z.,Buchnik,E.,Schain,M.,Price,A.,
connectedspeech. Natureneuroscience,19(1):158–164,
Aubrey,B., Nastase, S.A., Feder,A.,Emanuel,D.,Co-
7LostinTranslation:TheAlgorithmicGapBetweenLMsandtheBrain
hen, A., et al. Shared computationalprinciplesfor lan- tega, P., Strouse, D., Leibo, J. Z., and De Freitas, N.
guageprocessinginhumansanddeeplanguagemodels. Social influence as intrinsic motivation for multi-agent
Natureneuroscience,25(3):369–380,2022. deepreinforcementlearning.InInternationalconference
onmachinelearning,pp.3040–3049.PMLR,2019.
Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine,
S.,Bengio,Y.,andScho¨lkopf,B. Recurrentindependent Jones, C. R. and Bergen, B. K. People cannot distin-
mechanisms. arXivpreprintarXiv:1909.10893,2019. guish gpt-4 from a human in a turing test, 2024. URL
https://arxiv.org/abs/2405.08007.
Gross, J., Hoogenboom,N., Thut, G., Schyns, P., Panzeri,
S., Belin, P., and Garrod, S. Speech rhythmsand mul- Jones,N. Ainowbeatshumansatbasictasks—newbench-
tiplexed oscillatory sensory coding in the human brain. marksareneeded,saysmajorreport. Nature,628(8009):
PLoSbiology,11(12):e1001752,2013. 700–701,2024.
Gu,A.andDao,T. Mamba: Linear-timesequencemodel- Kambhampati, S. Can large language models reason and
ingwithselectivestatespaces,2023. plan? Annals of the New York Academy of Sciences,
1534(1):15–18,2024.
Hadfield-Menell, D., Russell, S. J., Abbeel, P., and Dra-
gan,A. Cooperativeinversereinforcementlearning. Ad- Kanai, R., Takatsuki, R., and Fujisawa,
vances in neural information processing systems, 29, I. Meta-representations as representa-
2016. tions of processes, Mar 2024. URL
osf.io/preprints/psyarxiv/zg27u.
Hagoort,P.andIndefrey,P. Theneurobiologyoflanguage
beyondsinglewords.Annualreviewofneuroscience,37: Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
347–362,2014. Chess, B., Child, R., Gray,S., Radford,A., Wu, J., and
Amodei, D. Scaling laws for neural language models.
Hasson, U., Chen, J., and Honey, C. J. Hierarchical pro-
arXivpreprintarXiv:2001.08361,2020.
cess memory: memory as an integral componentof in-
formation processing. Trends in cognitive sciences, 19 Khaligh-Razavi, S.-M. and Kriegeskorte, N. Deep super-
(6):304–313,2015. vised,butnotunsupervised,modelsmayexplainitcorti-
calrepresentation. PLoScomputationalbiology,10(11):
Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P.,
e1003915,2014.
and De Lange, F. P. A hierarchy of linguistic pre-
dictions during natural language comprehension. Pro- Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J.,
ceedingsoftheNationalAcademyofSciences,119(32): Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ra-
e2201968119,2022. malho, T., Grabska-Barwinska, A., et al. Overcoming
catastrophicforgettinginneuralnetworks. Proceedings
Hensch, T. K. Critical period plasticity in local cortical
ofthenationalacademyofsciences,114(13):3521–3526,
circuits. Nature Reviews Neuroscience, 6(11):877–888,
2017.
2005.
Kriegeskorte,N. Deepneuralnetworks: anewframework
Hu, E. J., Jain, M., Elmoznino,E., Kaddar,Y., Lajoie, G.,
formodelingbiologicalvisionandbraininformationpro-
Bengio,Y.,andMalkin,N. Amortizingintractableinfer-
cessing. Annual review of vision science, 1:417–446,
enceinlargelanguagemodels,2024.
2015.
Hwang,D.,Wang,W.,Huo,Z.,Sim,K.C.,andMengibar,
Kriegeskorte,N.,Mur,M.,andBandettini,P.A. Represen-
P. M. Transformerfam: Feedback attention is working
tational similarity analysis-connecting the branches of
memory,2024.
systemsneuroscience.Frontiersinsystemsneuroscience,
Jain, S., Vo, V. A., Wehbe, L., andHuth, A.G. Computa- 2:4,2008.
tionallanguagemodelingandthepromiseofinsilicoex-
Lake,B.M., Ullman,T.D., Tenenbaum,J.B., andGersh-
perimentation. NeurobiologyofLanguage,5(1):80–106,
man, S. J. Building machines that learn and think like
2024.
people. Behavioralandbrainsciences,40:e253,2017.
Jamali, M., Grannan, B., Cai, J., Khanna, A. R., Mun˜oz,
Leonard,M.K.,Gwilliams,L.,Sellers,K.K.,Chung,J.E.,
W.,Caprara,I.,Paulk,A.C.,Cash,S.S.,Fedorenko,E.,
Xu,D.,Mischler,G.,Mesgarani,N.,Welkenhuysen,M.,
andWilliams,Z.M.Semanticencodingduringlanguage
Dutta, B., and Chang, E. F. Large-scale single-neuron
comprehensionatsingle-cellresolution.Nature,pp.1–7,
speechsoundencodingacrossthedepthofhumancortex.
2024.
Nature,626(7999):593–602,2024.
Jaques, N., Lazaridou, A., Hughes, E., Gulcehre, C., Or-
Lerner,Y.,Honey,C.J., Silbert,L.J.,andHasson,U. To-
8LostinTranslation:TheAlgorithmicGapBetweenLMsandtheBrain
pographicmappingofahierarchyoftemporalreceptive fort,A.,Dunbar,E.,Pallier,C.,andKing,J.-R. Toward
windowsusinganarratedstory.JournalofNeuroscience, a realistic modelof speechprocessinginthe brainwith
31(8):2906–2915,2011. self-supervisedlearning,2023.
Lewis, M.andMitchell, M. Usingcounterfactualtasksto Mitchell, M. and Krakauer, D. C. The debate over un-
evaluate the generality of analogical reasoning in large derstanding in ai’s large language models. Proceed-
language models. arXiv preprint arXiv:2402.08955, ings of the National Academy of Sciences, 120(13):
2024. e2215907120,2023.
Li, J., Karamolegkou, A., Kementchedjhieva, Y., Abdou, Nakkiran,P.,Kaplun,G.,Kalimeris,D.,Yang,T.,Edelman,
M., Lehmann, S., and Søgaard, A. Structural simi- B.L.,Zhang,F.,andBarak,B. Sgdonneuralnetworks
larities between language models and neural response learnsfunctionsofincreasingcomplexity.arXivpreprint
measurements. In NeurIPS 2023 Workshop on Symme- arXiv:1905.11604,2019.
tryandGeometryinNeuralRepresentations,2023.URL
Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski,
https://openreview.net/forum?id=ZobkKCTaiY.
H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A.,
Liu, X., Ramirez, S., Pang, P. T., Puryear, C. B., Govin- Bosma, M., Luan, D., et al. Show your work: Scratch-
darajan, A., Deisseroth, K., and Tonegawa, S. Opto- pads for intermediate computation with language mod-
genetic stimulation of a hippocampal engram activates els. arXivpreprintarXiv:2112.00114,2021.
fear memory recall. Nature, 484:381–385, 2012. doi:
Oota,S.R.,Alexandre,F.,andHinaut,X. Long-termplau-
10.1038/nature11028.
sibilityoflanguagemodelsandneuraldynamicsduring
Liu, Y. L., Blodgett, S. L., Cheung, J. C. K., Liao, narrativelistening.InProceedingsoftheannualmeeting
Q. V., Olteanu, A., and Xiao, Z. Ecbd: Evidence- ofthecognitivesciencesociety,volume44,2022.
centered benchmark design for nlp. arXiv preprint
Paolicelli, R. C., Bolasco, G., Pagani, F., Maggi, L.,
arXiv:2406.08723,2024.
Scianni,M.,Panzanelli,P.,Giustetto,M.,Ferreira,T.A.,
Mahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, Guiducci,E.,Dumas,L.,etal. Synapticpruningbymi-
N., Tenenbaum, J. B., and Fedorenko, E. Dissociating croglia is necessary for normalbrain development. sci-
languageandthoughtinlargelanguagemodels. Trends ence,333(6048):1456–1458,2011.
inCognitiveSciences,2024.
Pasquiou, A., Lakretz, Y., Hale, J., Thirion, B., and Pal-
Mangalam, K. and Prabhu, V. Do deep neural networks lier, C. Neural language models are not born equal
learnshallowlearnableexamplesfirst? InProceedings to fit brain data, but training helps. arXiv preprint
oftheWorkshoponIdentifyingandUnderstandingDeep arXiv:2207.03380,2022.
Learning Phenomena at 36th InternationalConference
Pessoa, L. Theentangledbrain. Journalofcognitiveneu-
onMachineLearning.PMLR,2019.
roscience,35(3):349–360,2023.
Marks, S., Rager, C., Michaud, E. J., Belinkov,
Rofes, A., Mandonnet, E., de Aguiar, V., Rapp, B., Tsap-
Y., Bau, D., and Mueller, A. Sparse feature
kini, K., and Miceli, G. Languageprocessingfrom the
circuits: Discovering and editing interpretable
perspectiveofelectricalstimulationmapping. Cognitive
causal graphs in language models, 2024. URL
neuropsychology,36(3-4):117–139,2019.
https://arxiv.org/abs/2403.19647.
Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T., and
Marr, D. and Vision, A. Vision: A computationalinvesti-
Wayne, G. Experience replay for continual learning.
gation into the human representationand processing of
Advancesin neuralinformation processing systems, 32,
visualinformation. 1982.
2019.
Marvin,R.andLinzen,T. Targetedsyntacticevaluationof
Rusch, T. K. and Mishra, S. Coupled oscillatory recur-
languagemodels,2018.
rentneuralnetwork(cornn): Anaccurateand(gradient)
McCoy, R. T., Yao, S., Friedman, D., Hardy, M., and stable architecture for learning long time dependencies.
Griffiths, T. L. Embers of autoregression: Understand- arXivpreprintarXiv:2010.00951,2020.
inglargelanguagemodelsthroughtheproblemtheyare
Schrimpf, M., Blank, I., Tuckute, G., Kauf, C., Hos-
trainedtosolve. arXivpreprintarXiv:2309.13638,2023.
seini, E. A., Kanwisher, N., Tenenbaum, J., and Fe-
Meng,K.,Bau,D.,Andonian,A.,andBelinkov,Y. Locat- dorenko,E.Theneuralarchitectureoflanguage:Integra-
ingandeditingfactualassociationsingpt,2023. tive modelingconvergeson predictiveprocessing. Pro-
ceedingsoftheNationalAcademyofSciences,118(45):
Millet,J.,Caucheteux,C.,Orhan,P.,Boubenec,Y.,Gram-
9LostinTranslation:TheAlgorithmicGapBetweenLMsandtheBrain
e2105646118,2021. markfor evaluatinglarge languagemodelsonplanning
andreasoningaboutchange,2023.
Schyns, P. G., Snoek, L., andDaube, C. Degreesofalgo-
rithmicequivalencebetweenthebrainandits dnnmod- Vig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D.,
els. Trends in Cognitive Sciences, 26(12):1090–1102, Singer, Y., and Shieber, S. Investigatinggenderbiasin
2022. language models using causal mediation analysis. Ad-
vances in neural information processing systems, 33:
Scott, S. K. From speech and talkers to the social world:
12388–12401,2020.
The neuralprocessing of human spoken language. Sci-
ence,366(6461):58–62,2019. Volzhenin,K.,Changeux,J.-P.,andDumas,G. Multilevel
developmentofcognitiveabilitiesin an artificialneural
Seguin,C.,Sporns,O.,andZalesky,A.Brainnetworkcom-
network. Proceedings of the National Academy of Sci-
munication: concepts,modelsandapplications. Nature
ences,119(39):e2201304119,2022.
reviewsneuroscience,24(9):557–574,2023.
Wei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,Ichter,B.,
Strachan, J. W. A., Albergo, D., Borghini, G., Pansardi,
Xia,F.,Chi,E.,Le,Q.,andZhou,D. Chain-of-thought
O., Scaliti, E., Gupta, S., Saxena, K., Rufo, A., Panz-
prompting elicits reasoning in large language models,
eri, S., Manzi, G., Graziano, M. S. A., and Becchio,
2023.
C. Testing theory of mind in large language models
and humans. Nature Human Behaviour, 2024. doi: Weizenbaum,J. Computerpowerandhumanreason:From
10.1038/s41562-024-01882-z. judgmenttocalculation. 1976.
Team, C. Chameleon: Mixed-modalearly-fusion founda- West, G. Scale: The universal laws of life, growth, and
tionmodels,2024. death in organisms, cities, and companies. Penguin,
2018.
Tellex,S.,Gopalan,N.,Kress-Gazit,H.,andMatuszek,C.
Robots that use language. Annual Review of Control, West, G. B. The origin of universal scaling laws in biol-
Robotics,andAutonomousSystems,3:25–55,2020. ogy. Physica A: Statistical Mechanics and its Applica-
tions,263(1-4):104–113,1999.
Templeton, A., Conerly, T., Marcus, J., Lindsey, J.,
Bricken, T., Chen, B., Pearce, A., Citro, C., Ameisen, Wu, Z., Qiu, L., Ross, A., Akyu¨rek, E., Chen, B., Wang,
E., Jones, A., Cunningham, H., Turner, N. L., Mc- B., Kim, N., Andreas, J., and Kim, Y. Reasoning or
Dougall,C., MacDiarmid,M.,Freeman,C.D., Sumers, reciting? exploring the capabilities and limitations of
T. R., Rees, E., Batson, J., Jermyn, A., Carter, S., language models through counterfactual tasks. arXiv
Olah, C., and Henighan, T. Scaling monoseman- preprintarXiv:2307.02477,2023.
ticity: Extracting interpretable features from claude
Wu, Z., Qiu, L., Ross, A., Akyu¨rek, E., Chen, B., Wang,
3 sonnet. Transformer Circuits Thread, 2024. URL
B., Kim, N., Andreas, J., and Kim, Y. Reasoning or
https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html.
reciting?exploringthecapabilitiesandlimitationsoflan-
Toneva, M. and Wehbe, L. Interpreting and improving guagemodelsthroughcounterfactualtasks, 2024. URL
natural-languageprocessing (in machines) with natural https://arxiv.org/abs/2307.02477.
language-processing(in the brain). Advancesin neural
Xue,F.,Zheng,Z.,Fu,Y.,Ni,J.,Zheng,Z.,Zhou,W.,and
informationprocessingsystems,32,2019.
You, Y. Openmoe: An early efforton openmixture-of-
Tuckute, G., Feather, J., Boebinger, D., and McDermott, expertslanguagemodels,2024.
J.H.Manybutnotalldeepneuralnetworkaudiomodels
Yamins, D. L. and DiCarlo, J. J. Using goal-drivendeep
capture brain responses and exhibit correspondencebe-
learning models to understand sensory cortex. Nature
tweenmodelstagesandbrainregions. PlosBiology,21
neuroscience,19(3):356–365,2016.
(12):e3002366,2023.
Zelikman, E., Wu, Y., Mu, J., and Goodman, N.
Tuckute,G.,Kanwisher,N.,andFedorenko,E. Language
Star: Bootstrapping reasoning with reasoning. In
inbrains,minds,andmachines. AnnualReviewofNeu-
Koyejo, S., Mohamed, S., Agarwal, A., Belgrave,
roscience,47,2024.
D., Cho, K., and Oh, A. (eds.), Advances in Neu-
Tye,K.M.,Miller,E.K.,Taschbach,F.H.,Benna,M.K., ral Information Processing Systems, volume 35,
Rigotti,M.,andFusi,S.Mixedselectivity:Cellularcom- pp. 15476–15488.Curran Associates, Inc., 2022. URL
putationsforcomplexity. Neuron,2024. https://proceedings.neurips.cc/paper_files/paper/20
Valmeekam, K., Marquez, M., Olmo, A., Sreedharan, S., Zhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O.,
andKambhampati,S. Planbench: Anextensiblebench- Susskind, J., Bengio, S., and Nakkiran, P. What algo-
10LostinTranslation:TheAlgorithmicGapBetweenLMsandtheBrain
rithmscantransformerslearn?astudyinlengthgeneral-
ization. arXivpreprintarXiv:2310.16028,2023.
Zhou,S.,Weissweiler, L.,He, T.,Schu¨tze,H., Mortensen,
D. R., and Levin, L. Constructionsare so difficultthat
evenlargelanguagemodelsgetthemrightforthewrong
reasons. arXivpreprintarXiv:2403.17760,2024.
11