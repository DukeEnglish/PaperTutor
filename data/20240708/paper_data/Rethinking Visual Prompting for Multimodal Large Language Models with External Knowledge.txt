Rethinking Visual Prompting for Multimodal Large
Language Models with External Knowledge
YuanzeLin‚ô£‚àó YunshengLi‚ô† DongdongChen‚ô†
WeijianXu‚ô† RonaldClark‚ô£ PhilipTorr‚ô£ LuYuan‚ô†
‚ô£UniversityofOxford ‚ô†Microsoft
Abstract
Inrecentyears,multimodallargelanguagemodels(MLLMs)havemadesignif-
icantstridesbytrainingonvasthigh-qualityimage-textdatasets,enablingthem
togenerallyunderstandimageswell. However,theinherentdifficultyinexplic-
itlyconveyingfine-grainedorspatiallydenseinformationintext,suchasmasks,
posesachallengeforMLLMs,limitingtheirabilitytoanswerquestionsrequiring
an understanding of detailed or localized visual elements. Drawing inspiration
fromtheRetrieval-AugmentedGeneration(RAG)concept,thispaperproposesa
newvisualpromptapproachtointegratefine-grainedexternalknowledge,gleaned
fromspecializedvisionmodels(e.g.,instancesegmentation/OCRmodels),into
MLLMs. ThisisapromisingyetunderexploreddirectionforenhancingMLLMs‚Äô
performance. Ourapproachdivergesfromconcurrentworks,whichtransformex-
ternalknowledgeintoadditionaltextprompts,necessitatingthemodeltoindirectly
learnthecorrespondencebetweenvisualcontentandtextcoordinates. Instead,we
proposeembeddingfine-grainedknowledgeinformationdirectlyintoaspatialem-
beddingmapasavisualprompt. Thisdesigncanbeeffortlesslyincorporatedinto
variousMLLMs,suchasLLaVAandMipha,considerablyimprovingtheirvisual
understandingperformance. Throughrigorousexperiments,wedemonstratethat
ourmethodcanenhanceMLLMperformanceacrossninebenchmarks,amplifying
theirfine-grainedcontext-awarecapabilities.
1 Introduction
Theadvancementoflargelanguagemodels(LLMs)[56,42,43,17]hasrevolutionizedhowmachines
processandgeneratehuman-liketext,demonstratingremarkableabilitiesinreasoning,translation,
andcontextualunderstanding. Theintegrationoflanguageandvisionintounifiedmodels,suchas
GPT-4V[41],representsasignificantleapforwardinenablingmachinestounderstandandinteract
with the world in a manner akin to human cognition. As these models continue to evolve, they
promisetofurtherblurthelinesbetweenhumanandmachinecognition,openingnewfrontiersinAI
researchandapplication[32,51,46,33,55,31].
Despitetheirremarkablecapabilities,mostoftheMLLMs(showninFigure1(a))trainedwithimage-
textpairsstilloftenstruggleinfine-grainedmultimodalcomprehensioncapacities,e.g.,correctly
countobjectsoroutputpreciselocationofonespecificobject. Thisispartiallybecauseofthelack
ofhigh-qualitydatawithexceptionallyfine-grainedtextdescription. Moreimportantly,textitself
hastheinherentdifficultyinaccuratelyconveyinghighlyfine-grainedorspatiallydenseinformation.
Asaresult,currentMLLMsoftenfailtoaccuratelyinterpretpixel-levelvisualcontentoflocalized
regionswithinanimage,whichinreturnharmstheoverallcomprehensioncapacityfortheimageand
therebycausesthenotorious‚Äúhallucination‚Äùproblem[25].
‚àóWorkdoneduringaninternshipatMicrosoftRedmond.
Preprint.Underreview.
4202
luJ
5
]VC.sc[
1v18640.7042:viXraInput Image Question Input Image Question Input Image Visual Prompt Question
What position does What position does What position does
thelittle league little leagueplayer the ùë°ùë°"" thelittle league
pl Pa iy tce hr es re e om r f it eo l db ee r i ?n? s oe r e fm ielt do e b r?e 1in p? e P rsi otc nh :e r ùë°!ùë°! ùë°ùë°## pl Pa iy tce hr es re e om r f it eo l db ee r i ?n?
{[0.55, 0.49]} ‚Ä¶
Image tokens Image tokens
‚Ä¶ ‚Ä¶ Image ‚Ä¶
tokens
Large Language Model Large Language Model Large Language Model
Response Response Response
(a) VanillaVisual Instruction (b) Visual Instruction Tuning with (c) Visual Instruction Tuning with
Tuning (LLaVA) Textual Prompt Visual Prompt (Ours)
Figure 1: Different training paradigms. (a) means the original visual instruction tuning of
LLaVA [34]. (b) denotes visual instruction tuning with external textual prompts [23] (e.g., 1
personandthecentercoordinatesofitsboundingbox: [0.55,0.49]),notethatweneglectthetemplate
prefixoftextualpromptsforvisualization. (c)istheproposedauxiliaryvisualprompt,whichisa
featuremapcomposedwithdifferentobjectregions. Foreachpixel,itisfilledoutwiththetextual
embeddingofthecorrespondingcategoriesorOCRtext(t ,t andt intheexamplevisualprompt
g p b
meanthetextualembeddingsofgrass,personandbaseballglove).
To tackle this challenge, one line of work [6, 62, 5] explicitly integrates region coordinates in-
formationintothetextpromptandtrainsonspecializedregion-levelchattingdata. However,this
stilldemandsthatthemodelimplicitlylearnstounderstandcoordinatesandestablishconnections
withvisualcontent,therebyincreasingthelearningcomplexity. Anotherlineofwork [50,61,30]
proposesincorporatingRegionofInterest(ROI)featuresdirectlyintomodellearning,necessitating
bespokemodelarchitectures. Incontrasttotheseapproaches,ratherthanstartingfromscratchto
learnregioninformation,thispaperexploresleveragingfinely-grainedrecognitionoutcomesdirectly
obtainablefromexistingvisionmodelsasexternalknowledgeforMLLMs,inspiredbytheRAG
concept. Concurrentwithourwork,onerecentapproach [23]introducesexternalknowledge,such
asregionalcoordinatesfromobjectdetectionandOpticalCharacterRecognition(OCR)technologies,
intoMLLMs(showninFigure1(b)),helpingunderstandlocalizedmultimodalcontent.However,this
methodstillintegratesexternalknowledgethroughthetextprompt,mandatingimplicitlearningof
content-to-coordinatecorrespondencebythemodel. Furthermore,itlackssupportformorenuanced
externalknowledge,suchasinstancemasks.
Inthispaper,weproposeanewvisualpromptparadigmtoinsertexternalknowledge,e.g.,localized
information,intoMLLMsaddressingthechallengeoffine-grainedmultimodalcontentcorrespon-
dence. AsillustratedinFigure1(c),thecoreideais,ratherthantreatinglocalcontextinformation
asapartoftextprompts,weembedthemdirectlywithinthevisualprompts. Specifically,westart
byleveragingpanopticsegmentation[60]andOCRdetection[15]models,andapre-trainedtext
encodertogeneratepixel-wisetextembeddings,whichareservedasthelocalcontextinformation
forMLLMs. Subsequently,weextendtheoriginalvisualpromptsbyaddingthenewlygenerated
contextinformationinaspatial-wisemanner. ThisintegratedpromptisthenassimilatedintoMLLMs,
improvingfine-grainedvisualcontentcomprehension. Consequently, ourapproachiscapableof
enablingMLLMstodiscerncontextsinthepixel-levelspaceandimprovetheirperformance.
With the proposed visual prompt paradigm, we train a bunch of MLLMs on the LLaVA-1.5
datasets [34]. The experimental results show that, even with 3 billion parameters, our method
improvesupontheleadingopen-sourceMLLMssuchasLLaVA-1.5[35,34]andQwen-VL[3],with-
outneedingadditionaltrainingdata. Remarkably,ourmodelsshowcasesuperiorperformanceacross
awidearrayofbenchmarkswhencomparedtothe7-billionMLLMvariants,includingLLaVA-1.5,
Qwen-VL,andInstructBLIP[13],andinsomeinstances,evenoutperformtheir13-billionMLLM
counterparts. Ourexperimentalresultsconfirmthesignificanceofintegratingourproposedprompt
approachwithMLLMstoenhancecognitivecapabilities.
Thecontributionscanbesummarizedasfollows:
‚Ä¢ WesystematicallyinvestigateintegratinglocalizedinformationintoMLLMs. Empirical
findings suggest that our proposed visual prompt significantly outperforms the previous
promptparadigmrelyingsolelyontextualpromptscontainingcoordinates.
2Text
ùë°!""# ùë°!$% ùë°&‚Äô(
Class Prompt book, bed, cat
Encoder
Textual Embedding
ùë°!$%
SeM gma es nk ter Fill ùë°!""# ùë°&‚Äô(
Input Image Semantic-Aware
Object Masks Object Binary Masks Visual Prompt
Figure 2: Auxiliary visual prompt generation. It firstly generates the panoptic segmentation
masks[60]fortheinputimage,there‚Äôsaclasscategoryforeachmaskregion,thenwecanobtainthe
textualembeddings(e.g.,t ,t andt )throughapre-trainedtextencoderforalltheclasses
book bed cat
(e.g.,book,bed,cat). Finally,theauxiliaryvisualpromptcanbegeneratedbyconcatenatingthese
textualembeddingswithinthecorrespondingmaskregionstogether. Notethatwecanalsoadoptthe
OCRmodel[15]toobtainthetextsandtheregions,wedon‚Äôtdisplayithereforclearerexplanation.
Question:‚ÄúPlease provide a short Response: ‚ÄúThe cat
description for this region: LLM looks surprised‚Äù
[0.45, 0.39, 0.67, 0.63]‚Äù
Vision
ùë°$%& Encoder MLP PEN
ùë°$‚Äô‚Äô( ùë°!"#
Input Image Visual Prompt
Figure3: Theillustrationofvisualinstructiontuningwiththegeneratedvisualprompt. Our
proposedvisualpromptcanbeeasilycombinedwithexistingmultimodallargelanguagemodels(e.g.,
LLaVA[34]),notethatPEN meanspromptembeddingnetwork.
‚Ä¢ Weproposetointegratecontextualembeddingswithinlocalcontours(e.g.,objectmasks)as
thevisualprompt,whichfacilitatestheestablishmentofcorrelationsbetweenimagepixels
andcontexts,therebyenhancingthefine-grainedcognitivecapabilitiesofvariousMLLMs
acrossaspectrumofbenchmarks.
‚Ä¢ Basedonourproposedapproach,ourmodelwith3Bparameterssurpassesbothexisting7B
and13Bmodelsacrossdiversebenchmarks,allwithouttheneedforextratrainingdata.
2 RelatedWork
LargeLanguageModels. Theinitialpotentialoflargelanguagemodels(LLMs)wasshowcased
byfoundationalworkslikeBERT[14]andGPT[47]. Theysparkedawaveofscalingefforts,leading
to a range of influential projects, such as T5 [49], GPT-3 [4], Flan-T5 [12], and PaLM [9]. As
thevolumeoftrainingdataexpandedandthedimensionsofmodelparametersgrew,thesescaling
endeavorsledtothecreationofChatGPT[40,44]. ModelslikeLLaMA[56]andGPT-4[43]have
beentrainedonextensivecorporaanddemonstratedremarkablecapabilitiesindiversecognitivetasks.
Additionally,lightweightLLMswithfewerthan3Bparameters,i.e.,Phi[2,39]andStableLM-2[53]
haveshownperformancecomparabletolargermodels[8]. Inourwork,weadoptPhi-2[39]and
Vicuna-7B[8]asourlanguagebackbone.
Multimodal Large Language Models. Influenced by the success of instruction tuning from
LLM,LLaVA[35]andMiniGPT-4[64]haveadoptedvisualinstructiontuningtoimproveLLMs‚Äô
interaction with visual data, yielding impressive outcomes. Kosmos-2 [45] and Shikra [7] have
advancedMLLMsbyenhancingvisualcomprehensioncapabilities.whileworkslikeLLaVA-Phi[66],
MobileVLM[11]andBunny[20]mainlyfocusonoptimizingtrainingrecipesandarchitecturedesign
forlightweightMLLMs. Tosolvethechallengeofunderstandingfine-grainedinformationinimages,
existingapproachesproposetolearncoordinaterepresentations[6,7,62]andRegionofInterest
(ROI)features [45,61],whichuseinflexiblevisualreferralformatsornecessitatethecollectionof
3region-leveltrainingdata. Onthecontrary,wefocusonutilizingexternalknowledgetoimprovethe
fine-grainedvision-languagealignmentforMLLMswithoutcollectingextrachattingdata.
Prompting Multimodal Large Language Models. Inspired by the ability of GPT-4V [41] to
processdiverseinputs,ViP-LLaVA[5]collectsavisualpromptinstructiondatasetcontainingvarious
visualprompts,e.g.,scribblesandarrows,forMLLMfine-tuning. Contemporarytoourwork, [23]
hasofferedadvancedinsightsinpromptingMLLMsthroughexternalknowledge,whichintroduces
boundingboxandOCRcoordinatesintotextprompt,however,it‚Äôsstillchallengingtointerpretthe
pixel-levelcontexts. Inthispaper,weinvestigatehowtoefficientlyutilizeexternalknowledgeto
enhancemultimodalfine-grainedalignmentofMLLMsandintroduceanovelvisualpromptparadigm
incorporatingpixel-levelcontextualinformation.
3 ProposedMethod
Inthissection,weproposeanewvisualpromptparadigmthatintegrateslocalexternalinformation
toenhancethecapabilityofMLLMs. Insection3.1,weoutlinethedesignoftheauxiliaryvisual
promptthatcontainslocalcontextualinformation. Usingtheauxiliaryvisualprompt,insection3.2,
wefurtherembeditintoMLLMsbymergingitwiththeoriginalvisualtokens. Finally,webriefly
introducethedetailsoftraininginsection3.3.
3.1 AuxiliaryVisualPromptwithExternalKnowledge
Inthissection,weproposeamethodtogeneratelocalcontextualexternalknowledgetoassistMLLMs.
Incontrastto[23],whichfocusessolelyonobjectdetectionandOCRinformationandintegrates
themaspartofthetextprompt,weenhancethegranularityoflocalexternalknowledgebyleveraging
apanopticsegmentationmodel. Additionally,wecontinuetoutilizeanOCRmodelbuttransform
bothtypesofexternalknowledgeintopixel-wiseembeddings. Furtherdetailsareprovidedbelow.
AsshowninFigure2,giventheinputimageI ‚ààR3√óH√óW,wecanobtainthefine-grainedexternal
knowledge by an off-the-shelf panoptic segmentation model [60] and an OCR model [15]. The
generationoftheexternalknowledgecanbeexpressedas:
{M ,C }Ns =f (I), {B ,T }No =f (I), (1)
j j j=1 seg j j j=1 ocr
where f (¬∑) and f (¬∑) mean panoptic segmentation and optical character recognition (OCR)
seg ocr
models,N andN arethenumbersofdetectedmaskregionsandOCRboundingboxes.{M ,C }Ns
s o j j j=1
is the set of mask regions and the corresponding classes, and {B ,T }No represents the set of
j j j=1
detectedOCRboundingboxesandtexts.
Withthedetectedclasses{C }Ns andOCRtexts{T }No ,apre-trainedtextencoder(f (¬∑))is
j j=1 j j=1 text
leveragedtogeneratethetextureembeddingsas:
T ={t ,...,t }={f (C ),...,f (C )},
s 1 Ns text 1 text Ns
(2)
T ={tÀÜ,...,tÀÜ }={(f (T ),...,f (T )},
o 1 No text 1 text No
wheret ‚ààR1√ód(1‚©Ωi‚©ΩN )andtÀÜ ‚ààR1√ód(1‚©Ωi‚©ΩN )denotethei textualembeddingvector
i s i o th
oftheclassesforthedetectedmaskregionandOCRtextsrespectively,whiledistheembedding
dimension.
Inordertogenerateapixel-wisevisualpromptfortheexternalknowledgeinsteadofapuretext
description for the regions with coordinates and category names, the auxiliary visual prompt is
initializedasazerotensorP ‚ààRH√óW√ódandthenfilledwiththenewlygeneratedtextureembeddings
fortheexternalknowledgeas:
(cid:26)
t if(j,k)‚ààM
P = u u ‚àÄu‚àà{1,...,N },
j,k P otherwise s
j,k
(3)
(cid:26) tÀÜ if(j,k)‚ààB
P =P + v v ‚àÄv ‚àà{1,...,N }.
j,k j,k 0 otherwise o
4Note,forsomeregions,iftheconfidenceoftheclasspredictiongivenbythesegmentationmodel
islowortheOCRmodelfailstodetectanytext,weleavetheregionareawithzerovalues. Forthe
regionsthatareoccupiedbybothmodel,wesimplyaddthetextembeddingsdirectly. Weleavethe
investigationofmorerefinedfusiontechniquestofutureresearch.
Withtheauxiliaryvisualpromptcontainingpixel-levellocalcontextualinformationfrompanoptic
segmentationandOCRmodels,MLLMscaneffectivelycapturefiner-grainedfeatures. Thenext
challengeistoestablishaclearerconnectionbetweenthenewlygeneratedexternalknowledgeandthe
originalimagefeature. Thiswillhelpalleviatethemodel‚Äôsdifficultiesinlearningtheirrelationship
effectively.
3.2 VisualPromptInfusion
In this section, we introduce the visual prompt infusion that incorporates the proposed auxiliary
visualpromptsintotheMLLMs. Previousmethods[23]choosetoappendtheexternalknowledge
(embeddingsforobjectcategoryanditscoordinates)tothetextprompts,whichrequiresthemodelto
learnthecorrespondenceofvisualcontentwithinthespecifiedcoordinatesencodedintheexternal
knowledgeand,asaresult,increasingthedifficultiesofthelearningprocessofthemodel. Toaddress
thischallenge,weproposetomergetheauxiliaryvisualpromptdirectlywiththeimagefeaturesina
pixel-wisemanner.
Specifically,asshowninFigure3,theimagetokensarefirstgeneratedviaanimageencoderf (¬∑)
img
andanMLPprojector(f (¬∑)):
MLP
F =f (f (I)), (4)
v MLP img
whereF
v
‚ààRNv√ódv,N vandd vrepresentthenumberofimagetokensandtheembeddingdimension.
Then,theauxiliaryvisualpromptisfurtherprocessedbyapromptembeddingnetwork(PEN)as
F =f (P). (5)
p PEN
Forthepromptembeddingnetwork,weemploythreeconvolutionallayers,withanactivationlayer
(ReLU)insertedbetweeneachpairofthem. Thisnetworkprimarilyservestoalignthefeaturespace
andspatialsizebetweentheimagetokensandtheauxiliaryvisualprompts.
Whencombiningtheimagetokensandtheprocessedauxiliaryvisualprompt,wemainlyconsider
twooptions,bothofwhichoperatepixel-wise. (1)featurefusion: FÀÜ =f(Concat(F ,F )),where
v v p
f isalinearlayerthatmapstheembeddingRNv√ód2v ‚Üí RNv√ódv tomaintainthetotalnumberof
imagetokensunchanged;(2)featureaddition,FÀÜ =F +F ,whichsumsthetwotypesoffeatures
v v p
directly.
The advantages of the pixel-wise infusion for both options facilitate the model‚Äôs comprehension
of the correspondence between external knowledge and original visual features. This explicit
guidanceenablesthemodeltoeasilyunderstandthepixelcategoriesaswellasthepotentialOCR
text description it conveys. Consequently, it aids the model in disambiguating complex scenes,
accentuatingsalientfeatures,anddistinguishingfinerobjects.
3.3 Training
TrainingMLLMsinvolvespredictingresponsesbasedonmultimodalinputsusinganautoregressive
approach. Theobjectiveistomaximizetheprobabilityofgeneratingtokensthatmatchtheground-
truth answer Y . With the new visual embedding FÀÜ , this can be mathematically expressed as
a v
follows:
L
P(Y |FÀÜ ,F )=(cid:89) P (y |FÀÜ ,F ,Y ). (6)
a v t Œ∏ i v t a,<i
i=1
Here, L represents the sequence length of the ground truth answer Y , Œ∏ means the trainable
a
parameters. Y representsalltheanswertokensprecedingthecurrentpredictiontokenx ,where
a,<i i
idenotesthestepinthesequenceoftexttokengeneration. F
t
‚àà RNt√ódt isthetokenembedding
oftheinputquestion,N andd denotethenumberoftexttokensandtokenembeddingdimension.
t t
By infusing these enriched visual cues into the training pipeline, MLLMs can develop a more
5Table1: Theablationstudyofdifferentvisualprompts. Mipha-3Bisthebaselinewithstandardvisual
&textprompt. Mipha-3B+LAFdenotesusingtextualpromptingwithLoRAAugmentedFine-tuning
following(LAF)[23]. featurefusionandfeatureadditionrepresenttwopromptfusionmethodswe
usetoinserttheauxiliaryvisualprompttotheoriginalimagefeatures.
Method VQAv2 GQA SQAI VQAT MME-P MME-C MMB MM-Vet POPE
Mipha-3B 81.3 63.9 70.9 56.6 1488.9 295.0 69.7 32.1 86.7
Mipha-3B+LAF 81.6‚Üë 62.6‚Üì 71.4‚Üë 57.8‚Üë 1472.3‚Üì 356.8‚Üë 71.0‚Üë 34.8‚Üë 88.5‚Üë
Ours(featurefusion) 81.9‚Üë 64.8‚Üë 71.6‚Üë 57.6‚Üë 1493.5‚Üë 345.5‚Üë 71.3‚Üë 34.3‚Üë 88.5‚Üë
Ours(featureaddition) 82.4‚Üë 65.3‚Üë 71.8‚Üë 57.8‚Üë 1501.2‚Üë 369.1‚Üë 71.5‚Üë 35.1‚Üë 88.7‚Üë
Table2: Theablationstudyofusingdifferentvisionencoders,i.e.,SigLIPv.s. CLIP.Theresultsof
Mipha-3BonCLIParefrom [65].
Method VisEnc VQAv2 GQA SQAI VQAT MME-P MME-C MMB MM-Vet POPE
Mipha-3B CLIP 78.6 62.3 68.2 53.0 - - 68.4 31.0 86.9
Ours CLIP 79.7‚Üë 63.7‚Üë 70.1‚Üë 54.8‚Üë 1445.5 308.4 70.1‚Üë 33.7‚Üë 88.8‚Üë
Mipha-3B SigLIP 81.3 63.9 70.9 56.6 1488.9 295.0 69.7 32.1 86.7
Ours SigLIP 82.4‚Üë 65.3‚Üë 71.8‚Üë 57.8‚Üë 1501.2‚Üë 369.1‚Üë 71.5‚Üë 35.1‚Üë 88.7‚Üë
comprehensive understanding of visual content, leading to better alignment between visual and
textualrepresentations. Toacceleratethetrainingprocess,wefollowLoRAAugmentedTraining
(LAF) strategy [23] to perform fine-tuning on Mipha-3B [65] and LLaVA-1.5 [34] using LoRA
[21].
4 Experiment
Inthissection,weconductacomprehensivecomparisonofourmethodwithexistingstate-of-the-art
(SOTA)multimodalmodels. Additionally,weperformaseriesofablationstudiestofurthervalidate
theproposedmethod. Finally,weprovidevisualizationexamplesforin-depthanalysis.
Models. Forthevisionencoder,weadoptSigLIP-384px[59]forexperiments. WeleveragePhi-2-
2.7B[39]andVicuna-7B[8]modelasthelanguagedecoder. Forthemultimodalprojector,sameas
LLaVA[34],weadoptatwo-layerMLP.WeuseOpenSeed[60]andPaddleOCRv2[15]togenerate
theper-pixelexternallyknowledgeforpixelclassandOCRtext,andleverageUAE-Large-V1[27]to
extractthetextualembedding.
TrainingSetting. Wefine-tunethemodelsonLLaVA-Instruct-150Kdataset[34]usingLoRA[21]
for1epoch,atalearningrateof2e-4andabatchsizeof256on32√óV10032GBGPUs. Forthe
settingofLoRA,wesetLoRAranktobe128andLoRA‚ÄôshyperparameterŒ±as256. Notethatwe
fixalltheweightsofpre-trainedmodules,i.e.,visionencoder,languageencoderandMLP,during
training. Ourmodels‚ÄôweightsareinitializedfromMipha-3B[65]andLLava-7B[34].
BenchmarksandBaselines.Weevaluateourapproachusing9popularbenchmarkstocomprehen-
sivelyassessitsmultimodalcapabilities. Thesebenchmarksinclude: VQA-v2test-devsplit[18],
GQA test-dev-balanced split [22], ScienceQA-IMG test split [38], MME perception [16], MME
cognition[16],MMBenchtestsplit[37],MM-Vettestsplit[58],TextVQA[52],andPOPE[28].
Wecompareourresultswithabunchofstate-of-the-artmultimodallargelanguagemodels(MLLMs):
BLIP-2 [26], InstructBLIP [13], Shikra-13B [7], IDEFICS80/9B [24], Qwen-VL [3], mPLUG-
Owl2[57],LLaVA-v1.5-13/7B[34],LAF-7B[23],andmultimodalsmalllanguagemodels(MSLMs)
[65]: MobileVLM[11], LLaVA-Phi[66], MC-LLaVA[1], Imp-v1[54], MoE-LLaVA-3.6B[29],
TinyLLaVA-share-Sig-Phi[63],Bunny[20]andMipha[65].
4.1 AblationStudies
Inthissection,weconductanablationstudytoassesstheeffectivenessoftheproposedapproach.
By default, the experiments are conducted using Mipha-3B [65] with Phi-2 [39] as the language
backboneunlessotherwisespecified.
6Visualinputexample,context-relatedquestions:
Whatisfunnyaboutthisimage? Whyarethepersonswearingblackclothes?
LLaVA-7B Thefunnyaspectofthisimageisthejuxtapo- The persons in the image are wearing black
sitionoftwowomenpointingfingersatacat, clothes,whichisacommonattireforreligious
whichissittingatatablewithaplateoffood. ceremoniesorevents.Thegroupofpeople,in-
Thecatappearstobelookingatthewomenwith cludingamanandawoman,arewalkingdown
anangryexpression,asifitisreactingtotheir thestreet,possiblyparticipatinginareligious
pointingfingers. Thiscreatesahumorousand processionorafuneral.Theblackclothingsig-
unexpectedscene,asitisnotcommontoseea nifies their respect and adherence to the reli-
catreactingtohumangesturesinsuchamanner. giousorculturaltraditionsassociatedwiththe
Theimagecapturesalight-heartedandamusing eventtheyareattending.
momentbetweenthewomenandthecat.
Ours Thefunnyaspectofthisimageisthecontrast The persons in the image are wearing black
betweenthetwowomenpointingatthecatand clothes, which are typically associated with
thecat‚Äôsreactiontothesituation.Onewomanis mourning or a period of grieving. This sug-
makingafunnyface,whilethecatissittingona geststhattheymightbeattendingafuneralora
tablewithaplateoffood,seeminglyunbothered memorialserviceforsomeonewhohaspassed
bytheattention.Thecat‚Äôscalmdemeanorinthe away.Theblackattiresignifiesrespectandsoli-
faceofthewomen‚Äôsplayfulbehaviorcreatesa daritywiththebereavedfamilyandfriendsdur-
humorousscene,asthewomentrytoentertain ingthisdifficulttime.Itisacommonpracticein
themselvesandthecatseemstobeenjoyingthe manyculturestowearblackclothestoexpress
attentionwithoutreactingtotheirattempts. sympathyandsupportforthosepersonswho
haveexperiencedaloss.
Table3: ThechallengingexamplesonLLaVA-1.5,ourapproachcanprovidereasonablereponses
forthetrickyquestionswhichneedtofullyunderstandthecontextsoftheimages.
Table4: TheablationstudyofintroducingOCRinformationintothevisualprompt.
Method VQAv2 GQA SQAI VQAT MME-P MME-C MMB MM-Vet POPE
Mipha-3B 81.3 63.9 70.9 56.6 1488.9 295.0 69.7 32.1 86.7
Ours(w/oOCR) 81.9‚Üë 64.7‚Üë 71.3‚Üë 57.1‚Üë 1498.3‚Üë 355.2‚Üë 70.8‚Üë 34.0‚Üë 87.9‚Üë
Ours 82.4‚Üë 65.3‚Üë 71.8‚Üë 57.8‚Üë 1501.2‚Üë 369.1‚Üë 71.5‚Üë 35.1‚Üë 88.7‚Üë
PromptingMLLMswithDifferentApproaches. InTable1,wepresenttheresultsoftheablation
studyforfourdifferentpromptingstrategies:(1)Mihpa-3Bbaselineswithvanillatextprompt,asused
byLLaVA-1.5[34]. (2)Mihpa-3B+LAFproposedin[23]thatappendsexternellocalcontextual
knowledgetothetextprompts. (3)Theproposedauxiliaryvisualpromptinsertedviafeaturefusion.
(4)Theproposedauxiliaryvisualpromptaddedviafeatureaddition.
FromTable1,wenotethatcomparedtothebaseline(1)withvanillaprompts,bothproposedfusion
strategies(3)and(4)exhibitasignificantimprovement. Thissuggeststhatexternalknowledgeis
indeedbeneficialinenhancingthecapabilitiesofMLLMs. IncomparisontoMihpa-3B+LAF(2),
whichinsertsexternallocalcontextualknowledgeintothetextprompt,(4)outperformsitin8outof
9benchmarks,notablyforGQA[22]andMME-P[16]. Thisimpliesthatexplicitlylinkingexternal
localknowledgetotheoriginalvisualfeaturesreducesthemodel‚Äôslearningburdeninestablishing
spatialrelationships,consequentlyenhancingperformance. Furthermore,weempiricallyobservethat
directlyaddingauxiliaryvisualpromptsyieldsslightlybetterresultsthanconcatenation. Therefore,
weadoptfeatureadditionasourdefaultsettingforsubsequentexperiments.
TheEffectofUsingDifferentVisionEncoders. InTable2,wefurtherablatetheeffectiveness
broughtbydifferentvisionencoders,i.e.,CLIP[48]v.s. SigLIP[59]. Fromtheresults,wecandraw
twoconclusions. First,forbothvisionencoders,ourmethodshaveconsistentimprovementcompared
7Table 5: The comprehensive multi-modal evaluation across 9 distinct benchmarks to thoroughly
assess model performance: VQAv2 [18], GQA [22], SQAI: ScienceQA-IMG [38], VQAT:
TextVQA [52], MME-P: MME Perception [16], MME-C: MME Cognition [16], MMB: MM-
Bench [37], MM-Vet [58], and POPE [28]. The included proprietary in-house data not publicly
accessible,denotedas‚Ä†. Theimageresolutionusedbythevisualbackboneisindicatedinthecolumn
labeled Res., while the columns PT and IT represent the data sizes in the pretraining and visual
instructiontuningstages,respectively.
Method LM Res. PT IT VQAv2 GQA SQAI VQAT MME-P MME-C MMB MM-Vet POPE
MultimodalLargeLanguageModels
BLIP-2[26] Vicuna(13B) 224 129M - 65.0 41.0 61.0 42.5 1293.8 290.0 - 22.4 85.3
InstructBLIP[13] Vicuna(7B) 224 129M 1.2M - 49.2 60.5 50.1 - - 36 26.2 -
InstructBLIP[13] Vicuna(13B) 224 129M 1.2M - 49.5 63.1 50.7 1212.8 291.8 - 25.6 78.9
Shikra[7] Vicuna(13B) 224 600K 5.5M 77.4 - - - - - 58.8 - -
IDEFICS-9B[24] LLaMA(7B) 224 353M 1M 50.9 38.4 - 25.9 - - 48.2 - -
IDEFICS-80B[24] LLaMA(65B) 224 353M 1M 60.0 45.2 - 30.9 - - 54.5 - -
Qwen-VL[3] Qwen(7B) 448 1.4B‚Ä† 50M‚Ä† 78.8 59.3 67.1 63.8 - - 38.2 - -
Qwen-VL-Chat[3] Qwen(7B) 448 1.4B‚Ä† 50M‚Ä† 78.2 57.5 68.2 61.5 1487.5 360.7 60.6 - -
mPLUG-Owl2[57] LLaMA(7B) 448 400M 1.23M 79.4 56.1 68.7 58.2 1450.2 313.2 64.5 36.2 85.8
LLaVA-1.5[34] Vicuna(7B) 336 558K 665K 78.5 62.0 66.8 58.2 1510.7 316.1 64.3 30.5 85.9
LAF-7B[23] Vicuna(7B) 336 558K 665K 79.0 60.5 - 60.1 1482.7 397.9 67.3 35.2 88.9
LLaVA-1.5+(Ours) Vicuna(7B) 336 558K 665K 79.8‚Üë 63.3‚Üë 69.5‚Üë 59.8‚Üë 1515.3‚Üë 399.5‚Üë 67.6‚Üë 34.9‚Üë 88.9‚Üë
MultimodalSmallLanguageModels
MobileVLM-1.7B[10] M-LLaMA(1.4B) 336 558K 665K - 56.1 57.3 41.5 1196.2 - 53.2 - 84.5
MobileVLM-3B[10] M-LLaMA(2.7B) 336 558K 665K - 59.0 61.2 47.5 1288.9 - 59.6 - 84.9
MobileVLM-v2-1.7B[11] M-LLaMA(1.4B) 336 1.2M 2.4M - 59.3 66.7 52.1 1302.8 - 57.7 - 84.3
MobileVLM-v2-3B[11] M-LLaMA(2.7B) 336 1.2M 2.4M - 61.1 70.0 57.5 1440.5 - 63.2 - 84.7
LLaVA-Phi[66] Phi-2(2.7B) 336 558k 665K 71.4 - 68.4 48.6 1335.1 - 59.8 28.9 85.0
MC-LLaVA[1] Phi-2(2.7B) 384 558k 665K 64.2 49.6 - 38.6 - - - - 80.6
Imp-v1[54] Phi-2(2.7B) 384 558K 665K 79.5 58.6 70.0 59.4 1434.0 - 66.5 33.1 88.0
MoE-LLaVA-3.6B[29] Phi-2(2.7B) 384 558k 1.59M 79.9 62.6 70.3 57.0 1431.3 - 68.0 35.9 85.7
TinyLLaVA[63] Phi-2(2.7B) 384 1.2M 665k 79.9 62.0 69.1 59.1 1464.9 - 66.9 32.0 86.4
Bunny-3B[20] Phi-2(2.7B) 384 2M 695K 79.8 62.5 70.9 - 1488.8 289.3 68.6 - 86.8
Mipha-3B[65] Phi-2(2.7B) 384 558K 665K 81.3 63.9 70.9 56.6 1488.9 295.0 69.7 32.1 86.7
Mipha-3B+(Ours) Phi-2(2.7B) 384 558K 665K 82.4‚Üë 65.3‚Üë 71.8‚Üë 57.8‚Üë 1501.2‚Üë 369.1‚Üë 71.5‚Üë 35.1‚Üë 88.7‚Üë
tothebaselines,whichvalidatesthestabilityofourmethods. Second,SigLIPemergesasthestronger
visionencoderwhencomparedtoCLIP.Therefore,weopttoutilizeSigLIPasthedefaultvision
encoderinsubsequentsections.
The Effect of Introducing OCR Information Into Visual Prompt. In Table 4, we perform
theablationofusingOCRinformationornot,wecanconcludethatadoptingtheinformationfrom
OCRcanfurtherimprovethemodel‚Äôsoverallperformance, especially, whenincorporatingOCR
information for some text-specific tasks (e.g., TextVQA [52] and MM-Vet [58]), it can achieve
remarkableperformanceboosts.
4.2 MainResults
In Table 5, we compare our methods with other state-of-the-art (SOTA) models. We divide the
tableintosectionsforlanguagemodelssmallerthan3Bandthosebeyond7Btoprovideaclearer
comparison. Fromtheresults,weobservethatourmodelachievesthebestperformanceon7out
of9benchmarksforlargerlanguagemodels(>7B)andattainsthehighestaccuracyon7outof9
benchmarksforrelativelysmallerlanguagemodels(<3B).Notethat,inTable5,somemodels,e.g.
Shikra-13B [7], Qwen-VL [3], are trained with million or billion level data, while our model is
onlytrainedonthedatasetusedbyLLaVA-1.5withoutanyextradataforneitherpre-trainingnor
fine-tuning,whichhighlightstheexceptionalmultimodalunderstandingandreasoningcapabilities
of our models. In addition, on top of the LLaVA-1.5 framework, our approach can bring more
remarkableandconsistentimprovementonallbenchmarkscomparedwithLAF[23]. Itjustifies
theproposedinfusionstrategy,whichinvolvesinsertingexternalknowledgeinapixel-wisemanner
directlyintothevisualfeatures,asbeingmoreeffectivethanappendingittothetextprompt[23].
4.3 QuantitativeResultAnalysis
WepresentvisualizationresultsinTable3and6tofurtherillustratetheimprovementofourmodelin
termsofbothglobalimageunderstandingandlocalobjectandtextrecognition. Table3demonstrates
thatcomparedtoLLaVA-1.57B[34],ourapproachgeneratesmoredetailedandcontextuallyrelevant
responses,e.g.,‚ÄúThecat‚Äôscalmdemeanorinthefaceofthewomen‚Äôsplayfulbehavior‚Äùfortheleft
8Visualinputexample,relationship-awareandtext-relatedquestions:
Ontherightdesk, whatistotheleftofthe Whatareallthescenetextintheimage?
laptop?
LLaVA-1.57B Tothe left of thelaptop, thereis a stack of Thescenetextintheimageis"Shakespeare‚Äôs
booksonthedesk. Dramas,Sonnets,&Poems".
Ours Thereisadesklamptotheleftofthelaptop The scene text in the image includes the ti-
ontherightdesk. tle"Shakespeare‚ÄôsComedies,Histories,and
Tragedies".
Table6: ThechallengingexamplesonLLaVA-1.5. Ourapproachcangenerateaccurateresponses
fortext-relatedquestions.
example;‚Äúmourningoraperiodofgrieving‚Äùand‚Äúexpresssympathyandsupportforthosepersons
whohaveexperiencedaloss‚Äùfortherightexample,whichallneedadeeperunderstandingofthe
global image context. Meanwhile, Table 6 highlights our model‚Äôs ability to correctly recognize
objects‚Äôspatialrelationships, suchasbetweena‚Äúdesklamp‚Äùanda‚Äúlaptop‚Äùfromtheleftimage,
and exhibit stronger OCR capability in detecting words written on a book from the right image,
comparedtoLLaVA-1.57B[34]. Thesevisualizationsvalidatetheeffectivenessofourproposed
methodsandsupporttheconclusionthatincorporatingexternallocalcontextualinformationina
spatial-wisemannerimprovesthemodel‚Äôsfine-grainedrecognitioncapabilityandenhancesitsoverall
abilityforglobalimageunderstanding. Notethatwe‚Äôveshownmoreablationstudyexperimentsand
visualizationresultanalysisintheAppendix.
5 LimitationsandBroaderImpact
Ourmethodreliesonpre-trainedmodelsforpanopticsegmentationandOCRdetectioninazero-
shot manner. The performance of these models will significantly impact the performance of our
proposed method, particularly when there is a substantial domain gap between the images from
specificbenchmarksandthetrainingsetofthesegmentationorOCRmodels. Whiletheproposed
approachholdspromiseforsignificantlyenhancingthecognitivecapabilitiesofmultimodalmodels
andmayinspirenewmethodologiesandtechniquesinthedevelopmentofrobustmultimodalAI
systems,usersmustbeawareofpotentialnegativesocietalimpacts.Forinstance,biasesmaymanifest
invariousforms;forexample,biasedresponsesmaybegeneratedbythemodelifthetrainingdataof
MLLMs,panopticsegmentation,andOCRdetectionmodelscontaincertainbiases.
6 Conclusion
In this paper, we have proposed a method for leveraging external knowledge, such as localized
contextualinformation,toenhancethecapabilitiesofmultimodallanguagemodels(MLLMs). To
accomplishthisobjective,weproposeextractingpixel-wisecontextualinformationusingapanoptic
segmentationandOCRmodel,andthendirectlyintegratethiswiththevisualfeatures. Thisenables
the model to better understand both fine-grained objects and the overall global image context.
Experimentalresultsfromablationsandcomparisonswithstate-of-the-artmethodsdemonstratethe
effectiveness of our approach. We hope this paper can shed light on the importance of external
knowledgeforMLLMsandaneffectivewaytoleveragesuchknowledge.
9References
[1] Multi-cropllava-3b,2023. URLhttps://huggingface.co/visheratin/MC-LLaVA-3b.
[2] MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,JyotiAneja,AhmedAwadallah,Hany
Awadalla,NguyenBach,AmitBahree,ArashBakhtiari,HarkiratBehl,etal. Phi-3technicalre-
port:Ahighlycapablelanguagemodellocallyonyourphone.arXivpreprintarXiv:2404.14219,
2024.
[3] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. arXivpreprintarXiv:2308.12966,2023.
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877‚Äì1901,2020.
[5] MuCai,HaotianLiu,SivaKarthikMustikovela,GregoryPMeyer,YuningChai,DennisPark,
andYongJaeLee. Makinglargemultimodalmodelsunderstandarbitraryvisualprompts. arXiv
preprintarXiv:2312.00784,2023.
[6] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechuLiu,PengchuanZhang,Raghuraman
Krishnamoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. Minigpt-v2: large
languagemodelasaunifiedinterfaceforvision-languagemulti-tasklearning. arXivpreprint
arXiv:2310.09478,2023.
[7] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra:
Unleashingmultimodalllm‚Äôsreferentialdialoguemagic. arXivpreprintarXiv:2306.15195,
2023.
[8] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephEGonzalez,etal. Vicuna: Anopen-sourcechatbot
impressinggpt-4with90%*chatgptquality. Seehttps://vicuna.lmsys.org(accessed14April
2023),2023.
[9] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,Adam
Roberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm:
Scalinglanguagemodelingwithpathways. arXivpreprintarXiv:2204.02311,2022.
[10] XiangxiangChu,LimengQiao,XinyangLin,ShuangXu,YangYang,YimingHu,FeiWei,
XinyuZhang,BoZhang,XiaolinWei,etal. Mobilevlm: Afast,reproducibleandstrongvision
languageassistantformobiledevices. arXivpreprintarXiv:2312.16886,2023.
[11] XiangxiangChu,LimengQiao,XinyuZhang,ShuangXu,FeiWei,YangYang,XiaofeiSun,
YimingHu, XinyangLin, BoZhang, etal. Mobilevlmv2: Fasterandstrongerbaselinefor
visionlanguagemodel. arXivpreprintarXiv:2402.03766,2024.
[12] HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,EricLi,
Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned
languagemodels. arXivpreprintarXiv:2210.11416,2022.
[13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, BoyangLi, PascaleNFung, andStevenHoi. Instructblip: Towardsgeneral-purpose
vision-languagemodelswithinstructiontuning. AdvancesinNeuralInformationProcessing
Systems,36,2024.
[14] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,
2018.
[15] YuningDu,ChenxiaLi,RuoyuGuo,ChengCui,WeiweiLiu,JunZhou,BinLu,YehuaYang,
QiwenLiu,XiaoguangHu,etal. Pp-ocrv2:Bagoftricksforultralightweightocrsystem. arXiv
preprintarXiv:2109.03144,2021.
10[16] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,
Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for
multimodallargelanguagemodels. arXivpreprintarXiv:2306.13394,2023.
[17] Google. Googlebard. https://bard.google.com/chat/,2023.
[18] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making
thevinvqamatter: Elevatingtheroleofimageunderstandinginvisualquestionanswering.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
6904‚Äì6913,2017.
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:
Surpassinghuman-levelperformanceonimagenetclassification. InProceedingsoftheIEEE
internationalconferenceoncomputervision,pages1026‚Äì1034,2015.
[20] MuyangHe,YexinLiu,BoyaWu,JianhaoYuan,YuezeWang,TiejunHuang,andBoZhao.
Efficientmultimodallearningfromdata-centricperspective. arXivpreprintarXiv:2402.11530,
2024.
[21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprintarXiv:2106.09685,2021.
[22] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual
reasoningandcompositionalquestionanswering. InProceedingsoftheIEEE/CVFconference
oncomputervisionandpatternrecognition,pages6700‚Äì6709,2019.
[23] QiruiJiao,DaoyuanChen,YilunHuang,YaliangLi,andYingShen. Enhancingmultimodal
large language models with vision detection models: An empirical study. arXiv preprint
arXiv:2401.17981,2024.
[24] Hugo Lauren√ßon, Lucile Saulnier, L√©o Tronchon, Stas Bekman, Amanpreet Singh, Anton
Lozhkov,ThomasWang,SiddharthKaramcheti,AlexanderRush,DouweKiela,etal. Obelics:
Anopenweb-scalefiltereddatasetofinterleavedimage-textdocuments. AdvancesinNeural
InformationProcessingSystems,36,2024.
[25] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng
Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants.
FoundationsandTrends¬ÆinComputerGraphicsandVision,16(1-2):1‚Äì214,2024.
[26] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InInternationalconference
onmachinelearning,pages19730‚Äì19742.PMLR,2023.
[27] XianmingLiandJingLi. Angle-optimizedtextembeddings. arXivpreprintarXiv:2309.12871,
2023.
[28] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluating
objecthallucinationinlargevision-languagemodels. arXivpreprintarXiv:2305.10355,2023.
[29] BinLin, ZhenyuTang, YangYe, JiaxiCui, BinZhu, PengJin, JunwuZhang, MunanNing,
andLiYuan. Moe-llava: Mixtureofexpertsforlargevision-languagemodels. arXivpreprint
arXiv:2401.15947,2024.
[30] YuanzeLin,YujiaXie,DongdongChen,YichongXu,ChenguangZhu,andLuYuan. Revive:
Regionalvisualrepresentationmattersinknowledge-basedvisualquestionanswering.Advances
inNeuralInformationProcessingSystems,35:10560‚Äì10571,2022.
[31] YuanzeLin,ChenWei,HuiyuWang,AlanYuille,andCihangXie. Smaug: Sparsemasked
autoencoderforefficientvideo-languagepre-training. InProceedingsoftheIEEE/CVFInterna-
tionalConferenceonComputerVision,pages2459‚Äì2469,2023.
[32] YuanzeLin,Yi-WenChen,Yi-HsuanTsai,LuJiang,andMing-HsuanYang. Text-drivenimage
editingvialearnableregions. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages7059‚Äì7068,2024.
11[33] YuanzeLin,RonaldClark,andPhilipTorr. Dreampolisher: Towardshigh-qualitytext-to-3d
generationviageometricdiffusion. arXivpreprintarXiv:2403.17237,2024.
[34] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning. arXivpreprintarXiv:2310.03744,2023.
[35] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advances
inneuralinformationprocessingsystems,36,2024.
[36] ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyuanLi,Jianwei
Yang,HangSu,JunZhu,etal. Groundingdino: Marryingdinowithgroundedpre-trainingfor
open-setobjectdetection. arXivpreprintarXiv:2303.05499,2023.
[37] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023.
[38] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,Oyvind
Tafjord,PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathought
chainsforsciencequestionanswering. AdvancesinNeuralInformationProcessingSystems,35:
2507‚Äì2521,2022.
[39] Microsoft. Phi-2: The surprising power of small language mod-
els, 2023. URL https://www.microsoft.com/en-us/research/blog/
phi-2-the-surprising-power-of-small-language-models.
[40] OpenAI. Chatgpt. https://openai.com/blog/chatgpt,2022.
[41] OpenAI. Gpt-4v(ision)system card. https://cdn.openai.com/papers/GPTV_System_
Card.pdf,2023.
[42] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/,2023.
[43] OpenAI. Gpt-4technicalreport. 2023.
[44] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelsto
followinstructionswithhumanfeedback. AdvancesinNeuralInformationProcessingSystems,
35:27730‚Äì27744,2022.
[45] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,ShaohanHuang,ShumingMa,andFuru
Wei. Kosmos-2: Groundingmultimodallargelanguagemodelstotheworld. arXivpreprint
arXiv:2306.14824,2023.
[46] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall. Dreamfusion: Text-to-3dusing
2ddiffusion. arXivpreprintarXiv:2209.14988,2022.
[47] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguage
understandingbygenerativepre-training. OpenAI,2018.
[48] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748‚Äì8763.PMLR,2021.
[49] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
YanqiZhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunified
text-to-texttransformer. TheJournalofMachineLearningResearch,21(1):5485‚Äì5551,2020.
[50] HanoonaRasheed,MuhammadMaaz,SahalShaji,AbdelrahmanShaker,SalmanKhan,Hisham
Cholakkal,RaoMAnwer,ErixXing,Ming-HsuanYang,andFahadSKhan. Glamm: Pixel
groundinglargemultimodalmodel. arXivpreprintarXiv:2311.03356,2023.
12[51] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rnOmmer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10684‚Äì10695,2022.
[52] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi
Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pages8317‚Äì8326,2019.
[53] Stability AI. Introducing stable lm 2, 2024. URL https://stability.ai/news/
introducing-stable-lm-2.
[54] Yi-LinSung,LinjieLi,KevinLin,ZheGan,MohitBansal,andLijuanWang. Anempirical
studyofmultimodalmodelmerging. arXivpreprintarXiv:2304.14933,2023.
[55] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[56] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
th√©eLacroix,BaptisteRozi√®re,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[57] QinghaoYe,HaiyangXu,JiaboYe,MingYan,HaoweiLiu,QiQian,JiZhang,FeiHuang,and
JingrenZhou. mplug-owl2: Revolutionizingmulti-modallargelanguagemodelwithmodality
collaboration. arXivpreprintarXiv:2311.04257,2023.
[58] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,
andLijuanWang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabilities.
arXivpreprintarXiv:2308.02490,2023.
[59] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for
languageimagepre-training. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages11975‚Äì11986,2023.
[60] HaoZhang,FengLi,XueyanZou,ShilongLiu,ChunyuanLi,JianweiYang,andLeiZhang.
Asimpleframeworkforopen-vocabularysegmentationanddetection. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages1020‚Äì1031,2023.
[61] ShilongZhang,PeizeSun,ShoufaChen,MinXiao,WenqiShao,WenweiZhang,KaiChen,
andPingLuo. Gpt4roi: Instructiontuninglargelanguagemodelonregion-of-interest. arXiv
preprintarXiv:2307.03601,2023.
[62] LiangZhao,EnYu,ZhengGe,JinrongYang,HaoranWei,HongyuZhou,JianjianSun,Yuang
Peng,RunpeiDong,ChunruiHan,etal. Chatspot: Bootstrappingmultimodalllmsviaprecise
referringinstructiontuning. arXivpreprintarXiv:2307.09474,2023.
[63] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei
Huang. Tinyllava: A framework of small-scale large multimodal models. arXiv preprint
arXiv:2402.14289,2024.
[64] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4:
Enhancingvision-languageunderstandingwithadvancedlargelanguagemodels. InTheTwelfth
InternationalConferenceonLearningRepresentations,2024. URLhttps://openreview.
net/forum?id=1tZbq88f27.
[65] MinjieZhu,YichenZhu,XinLiu,NingLiu,ZhiyuanXu,ChaominShen,YaxinPeng,Zhicai
Ou,FeifeiFeng,andJianTang. Acomprehensiveoverhaulofmultimodalassistantwithsmall
languagemodels. arXivpreprintarXiv:2403.06199,2024.
[66] Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang. Llava-phi:
Efficientmulti-modalassistantwithsmalllanguagemodel. arXivpreprintarXiv:2401.02330,
2024.
13A Appendix
Inthesupplementarymaterials,weprovidethefollowingsections:
(a) MoreimplementationdetailsinSectionB.
(b) AblationstudyexperimentsinSectionC.
(c) VisualizationresultanalysisinSectionD.
B ImplementationDetails
ThetrainingtimeforLLaVA-1.57B[34]andMipha-3B[65]isapproximately14hoursand9hours,
respectively,withabatchsizeof256on32√óNVIDIAV10032GBGPUs. Fortheinitializationof
theproposedpromptembeddingnetwork(PEN),weuseKaiminginitializationtechnology[19]. The
UAE-Large-V12modelisadoptedasthepre-trainedtextualencodertoextracttextualembeddings
forthevisualprompt.
C AblationStudy
Next,weconductmoreablationstudyexperimentstoprovidedeeperinsightintothecomponentsof
ourproposedapproach.
TheEffectofUsingDifferentPre-trainedTextualEncoders. InTable7,weperformanablation
studyusingdifferenttextualencoders,i.e.,CLIP[48]vs. UAE[27],toextracttextualembeddings
fortheproposedvisualprompt. WedrawtwoconclusionsfromTable7: (1)Usingdifferenttextual
encoders,theproposedapproachconsistentlyoutperformsthebaseline,demonstratingtherobustness
ofourmethod. (2)AdoptingUAEasthepre-trainedtextualencoderachievessignificantlybetter
performance.Therefore,wechooseUAEasthedefaultpre-trainedtextualencoderinourexperiments.
ObjectDetectorv.s. SegmentationModel. Todeterminetheeffectofusinganobjectdetector
or segmentation model to incorporate pixel-level semantics into the proposed visual prompt, we
conductanablationstudywiththepopularobjectdetectorGroundingDINO[36]andthesegmentation
modelOpenSeed[60]. TheresultsareshowninTable8. WeobservethatbothGroundingDINO
andOpenSeedsignificantlyboostperformanceacrossallbenchmarks. However,utilizingOpenSeed
achievesbetterperformancegainsduetoitsfine-grainedmaskregions. Thus,weadoptOpenSeedby
defaulttogenerateobjectregions.
TheEffectofFine-TuningwiththeVisualPrompt. AsdisplayedinTable9,themodelfine-tuned
withtheproposedvisualprompt(i.e.,thethirdrow)achievesremarkablybetterperformancethanthe
onefine-tunedwithoutourvisualprompt(i.e.,thesecondrow)acrossallbenchmarks. Specifically,
withoutusingourvisualpromptforfine-tuning,themodelevenshowsperformancedegradationon
Text-VQAbenchmark[52]andhasnegligiblegainsonScience-QA[38],VQAv2[18],MME-P[16],
andMME-C[16]benchmarks. Alltheseresultsdemonstratethesuperiorityoftheproposedmethod.
D VisualizationResultAnalysis
We‚ÄôveprovidedmorevisualizationresultsinTable10, 11, 12, and13. ComparedtoLLaVA-1.5
7B[34],ourmethodgeneratesmorereasonableandaccurateresponsestothequestions.
AsshowninTable10,ourapproachcangenerateaccuratemovietitles,suchas‚ÄúTheGodfather‚Äù,
andthetwoactors‚Äônames,suchas‚ÄúAlPacino‚Äùand‚ÄôRobertDeNiro‚Äù. Additionally,itprovidesa
correspondingintroduction,suchas‚ÄúThemovieisaclassiccrimedramafilmdirectedbyFrancisFord
Coppola,knownforitsiconiccharacters,storytelling,andmemorablescenes"fortheleftexample. In
therightexample,ourmethodgeneratestheprecisetitle‚ÄúTheLordoftheRings: TheFellowshipof
theRing‚Äùandprovidesanaccurateintroduction,suchas‚ÄúThemovieisanepicfantasyadventurethat
2https://huggingface.co/WhereIsAI/UAE-Large-V1
14Table7: Theablationstudyofusingdifferenttextualencoders,i.e.,CLIPv.s. UAE,toextracttextual
embeddingsfortheproposedvisualprompt.
Method TextEnc VQAv2 GQA SQAI VQAT MME-P MME-C MMB MM-Vet POPE
Mipha-3B - 81.3 63.9 70.9 56.6 1488.9 295.0 69.7 32.1 86.7
Mipha-3B CLIP 82.1‚Üë 64.9‚Üë 71.3‚Üë 57.4‚Üë 1497.2‚Üë 361.5‚Üë 71.1‚Üë 34.6‚Üë 88.5‚Üë
Ours UAE 82.4‚Üë 65.3‚Üë 71.8‚Üë 57.8‚Üë 1501.2‚Üë 369.1‚Üë 71.5‚Üë 35.1‚Üë 88.7‚Üë
Table8: Theablationstudyofusinganobjectdetectororapanopticsegmentationmodeltoextract
objectregionsforpixel-leveltextualembeddings.
Method RegionGenerator VQAv2 GQA SQAI VQAT MME-P MME-C MMB MM-Vet POPE
Mipha-3B - 81.3 63.9 70.9 56.6 1488.9 295.0 69.7 32.1 86.7
Mipha-3B GroundingDINO 82.0‚Üë 64.9‚Üë 71.4‚Üë 57.2‚Üë 1491.7‚Üë 350.2‚Üë 71.0‚Üë 34.5‚Üë 88.4‚Üë
Ours OpenSeed 82.4‚Üë 65.3‚Üë 71.8‚Üë 57.8‚Üë 1501.2‚Üë 369.1‚Üë 71.5‚Üë 35.1‚Üë 88.7‚Üë
Table9: Theablationstudyoffine-tuningwithandwithouttheproposedvisualprompt. Thefirst,
second and third rows mean Mipha-3B baseline, fine-tuning on Mipha-3B without and with the
proposedvisualpromptusingLoRA[21].
Method VisualPrompt VQAv2 GQA SQAI VQAT MME-P MME-C MMB MM-Vet POPE
Mipha-3B - 81.3 63.9 70.9 56.6 1488.9 295.0 69.7 32.1 86.7
Mipha-3B+ ‚úó 81.4‚Üë 64.3‚Üë 71.0‚Üë 56.5‚Üì 1489.2‚Üë 303.2‚Üë 70.4‚Üë 33.5‚Üë 87.4‚Üë
Ours ‚úì 82.4‚Üë 65.3‚Üë 71.8‚Üë 57.8‚Üë 1501.2‚Üë 369.1‚Üë 71.5‚Üë 35.1‚Üë 88.7‚Üë
followsthejourneyofafellowshipofcharacters". Incontrast,LLaVA‚Äôsresponsesareverygeneral
withoutfullyunderstandingtheglobalcontextswithintheimages.
InTable11,ourmethodnotonlyrecognizesthetext‚ÄúMe: I‚Äôlldoitat8. Time: 8.05. LookslikeI
gottawaittill9now‚Äù,butalsounderstandsitsimplication,suchas‚Äúthepuppetisawareofthetime
andisintentionallydelayinganyworkortaskitmightbeassigned‚Äù,intheleftimage. Fortheright
image,ourmethodgeneratesmorepotentialoptions,suchasa‚Äúprivateschool‚Äù,‚Äúareligiousschool‚Äù,
etc. Incontrast,LLaVA‚Äôsanswersareeitherinappropriateortoolimited.
Finally,asshowninTable12and13,ourmethodunderstandsspatialrelationshipsandaccurately
recognizesthetextwithintheimages. Forexample,intheleftexampleofTable12,ourapproach
accuratelynamesthepersonontheleftas‚ÄúKeiraKnightley‚Äù. Italsorecognizestheplatenumber
‚ÄúS3302CD‚ÄùintheleftexampleofTable13,whileLLaVA‚Äôsanswersareallincorrect. Thisindicates
thestrongfine-grainedmultimodalunderstandingcapacityofourproposedmethod.
15Visualinputexample,context-relatedquestions:
Can you give a short introduction to this Can you give a short introduction to this
movie? movie?
LLaVA-7B The image features a scene from a movie Theimagefeaturesayoungmanwearinga
where two men are standing close to each brownshirtandholdingaringinhishand.
other,withonemanplacinghishandonthe Heappearstobeinadarksetting,possiblya
other‚Äôsshoulder.Theyarebothwearingtuxe- caveoradungeon.Themanislookingatthe
dos,whichsuggeststhattheyareattending camera,possiblyengaginginaconversation
a formal event or a special occasion. The or sharing a story. The ring he is holding
menappeartobeengagedinaconversation suggeststhatthescenemightberelatedtoa
orinteractingwitheachotherinsomeway, fantasyoradventuremovie,wherethering
possiblydiscussingtheeventorsharingamo- couldrepresentamagicalorpowerfulobject.
mentofcamaraderie.
Ours Theimagefeaturesascenefromthemovie Theimagefeaturesascenefromthemovie
‚ÄúTheGodfather‚Äù,wheretwomenplayedby ‚ÄúTheLordoftheRings: TheFellowshipof
actors Al Pacino and Robert De Niro, are the Ring‚Äù, where the character Gandalf is
standingclosetoeachother. Onemanhas holdingouthishandtothehobbitFrodo,of-
his hand on the other‚Äôs shoulder, and they fering him the One Ring. This iconic mo-
appeartobeengagedinaconversationora mentcapturestheessenceofthestory,where
confrontation.Themovieisaclassiccrime Frodoisentrustedwiththetaskofdestroying
dramafilmdirectedbyFrancisFordCoppola, theringtopreventitspowerfromfallinginto
knownforitsiconiccharacters,storytelling, thewronghands. Themovieisanepicfan-
andmemorablescenes. tasyadventurethatfollowsthejourneyofa
fellowshipofcharactersastheyembarkona
perilousquesttoridMiddle-earthofthedark
lordSauronandhisarmyoforcs.
Table10: ThechallengingexamplesonLLaVA-1.5,ourapproachcanprovidereasonablereponses
forthetrickyquestionswhichneedtofullyunderstandthecontextsoftheimages.
16Visualinputexample,context-relatedquestions:
Canyouexplainthismeme? Whatkindofschooldoestheboygoto?An-
sweritandgivetherationale.
LLaVA-7B ThememefeaturesagreenKermittheFrog Theboylikelygoestoaprivateschool,ashe
puppet lying on a bed, with a caption that iswearingasuitandtie,whichisacommon
reads,‚ÄúMelookslikeIgottawaittill9‚ÄùThe attire for students in private schools. The
puppetappearstoberelaxingonthebed,pos- presenceofatiesuggeststhattheschoolhas
siblywaitingforsomeoneorsomething.The aformaldresscode,andtheboyisdressed
imageisaplayfulandlightheartedrepresen- appropriatelyfortheoccasion.
tationofacommonsituationwherepeople
mighthavetowaitforacertaintimebefore
theycanproceedwiththeirplans.
Ours Theimagefeaturesagreenfrog,whichisa Theboyintheimageiswearingatie,which
popularcharacterfromthetelevisionshow suggeststhathemightbeattendingaschool
‚ÄúTheMuppetShow‚Äù,thefrogissittingona withaformaldresscodeoraschoolthatre-
bed,appearingtobeinarelaxedandcom- quiresstudentstoweartiesaspartoftheir
fortableposition. Thepuppetisholdingits uniform. Thiscouldbeaprivateschool, a
bellyandseemstobeenjoyingitstime.The religious school, or a school with a strong
bedhasawhitesheet,andthesceneappears emphasisondisciplineandprofessionalism.
tobesetinacozybedroom. Thememeis Thepresenceofawoman,whoiswearinga
captionedwithahumorousmessagewhich dress,intheimage,possiblyhismother,indi-
reads‚ÄúMe:I‚Äôlldoitat8.Time:8.05.Looks catesthattheboymightbegoingtoschool
likeIgottawaittill9now‚Äù,thisimpliesthat withhersupportandguidance.
thepuppetisawareofthetimeandisinten-
tionallydelayinganyworkortaskitmight
beassigned.Thememeislikelymeanttobe
amusingandentertaining,asitcombinesthe
ideaofapuppetwithahuman-likesenseof
humorandawarenessoftime.
Table11: ThechallengingexamplesonLLaVA-1.5,ourapproachcanprovidereasonablereponses
forthetrickyquestionswhichneedtofullyunderstandthecontextsoftheimages.
17Visualinputexample,relationship-awarequestions:
Whoisthepersonontheleft? Whoisthepersoninthebottomright?
LLaVA-7B ThepersonontheleftisReeseWitherspoon. The person in the bottom right is a man
namedTomHiddleston.
Ours ThepersonontheleftisKeiraKnightley. ThepersoninthebottomrightsisJamieDor-
nan.
Table12: ThechallengingexamplesonLLaVA-1.5,ourapproachcanprovidereasonablereponses
for the tricky questions which need to fully understand the relationships among objects within
images.
18Visualinputexample,text-relatedquestions:
Whatareallthescenetextintheimage? Whatareallthescenetextintheimage?
LLaVA-7B Thescenetextintheimageis‚ÄúS33020CD‚Äù. Thescenetextintheimageis‚ÄúConnecticut
Lawof1652‚Äù.
Ours Thescenetextintheimageincludestheli- Thescenetextintheimageincludes‚ÄúCon-
cense plate number ‚ÄúS3302CD‚Äù which is necticutLawof1642‚Äù,whichstates‚ÄúIfany
writteninwhiteonablackbackground. manorwomanbeawitch-thatishathor
consultethwithafamiliarspirit-theyshall
beputtodeath‚Äù.
Table13: ThechallengingexamplesonLLaVA-1.5,ourapproachcanprovidereasonablereponses
forthetrickyquestionswhichneedtoaccuratelyrecognizethetextswithintheimages.
19