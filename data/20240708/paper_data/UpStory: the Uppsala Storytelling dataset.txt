UpStory: the Uppsala Storytelling dataset
Marc Fraile1*, Natalia Calvo-Barajas1, Anastasia Sophia Apeiron2, Giovanna Varni3,
Joakim Lindblad1, Nataˇsa Sladoje1, Ginevra Castellano1
1 Department of Information Technology, Uppsala University, Uppsala, Sweden
2 Department of Information and Computing Sciences, Utrecht University, Utrecht,
Netherlands
3 Department of Information Engineering and Computer Science, University of Trento,
Trento, Italy
* marc.fraile.fabrega@it.uu.se
Abstract
Friendship and rapport play an important role in the formation of constructive social
interactions, and have been widely studied in educational settings due to their impact
on student outcomes. Given the growing interest in automating the analysis of such
phenomena through Machine Learning (ML), access to annotated interaction datasets is
highly valuable. However, no dataset on dyadic child-child interactions explicitly
capturing rapport currently exists. Moreover, despite advances in the automatic
analysis of human behaviour, no previous work has addressed the prediction of rapport
in child-child dyadic interactions in educational settings. We present UpStory — the
Uppsala Storytelling dataset: a novel dataset of naturalistic dyadic interactions between
primary school aged children, with an experimental manipulation of rapport. Pairs of
children aged 8-10 participate in a task-oriented activity: designing a story together,
while being allowed free movement within the play area. We promote balanced
collection of different levels of rapport by using a within-subjects design: self-reported
friendships are used to pair each child twice, either minimizing or maximizing pair
separation in the friendship network. The dataset contains data for 35 pairs, totalling
3h 40m of audio and video recordings. It includes two video sources covering the play
area, as well as separate voice recordings for each child. An anonymized version of the
dataset is made publicly available, containing per-frame head pose, body pose, and face
features; as well as per-pair information, including the level of rapport. Finally, we
provide ML baselines for the prediction of rapport.
Introduction
Rapport, the establishment of a close relationship based on mutual understanding, has
long been considered to be an important facet of social interactions [1], and particularly
in dyadic (one-on-one) interactions [2]. In the educational context, teacher-student
rapport and student-student rapport have been shown to have a positive impact on the
learning experience [3]. Friendship between classmates is naturally a high-rapport
relationship and has been shown to improve measurable outcomes such as task
performance, effective problem-solving, and learning outcomes [4–6].
While the ML-based analysis of affect and related interpersonal constructs has been
well studied (see e.g. [7–9]), no previous work has investigated the prediction of rapport
in child-child dyadic interactions in educational settings. Moreover, no dataset
July 8, 2024 1/30
4202
luJ
5
]CH.sc[
1v25340.7042:viXracapturing this interaction modality is available. The primary goal of this study is to
address these gaps by introducing a publicly available child-child interaction dataset
annotated for rapport, and providing ML baselines for its automatic prediction.
In order to learn useful models of social interaction through ML techniques,
practitioners require access to high-quality multimodal datasets. Due to the sensitive
nature of the recorded data (including the likeness and voice of the participants, as well
as the information they disclose through speech), the number of such datasets is
relatively small, and access is constrained. Some well-known datasets are available for
adult-adult interactions [10] and child-child interactions [11,12], but in these, available
annotations for social constructs are obtained from external raters. This has
implications on the reliability of the annotations, and an objective measure of rapport
would be preferable when studying this phenomenon. We address this issue by relying
on the friendship network: a graph representing friendships in the classroom through
self-reported friendship nominations. Friendship networks have long been used to study
the social relationships in educational institutions [13], and have been analyzed using
graph theory to evaluate the social fabric of the student population [14]. In this study,
we propose a partition scheme based on friendship network analysis to obtain
high-rapport and low-rapport pairings of the student population. We validate the
partition scheme using graph analysis and questionnaire responses.
The data collection effort is centered on an educational collaborative game designed
to elicit pair interactions. Pairs of children determined with the proposed partition
scheme are invited to play a card-based storytelling game together. They plan a story in
a private space, while being recorded by two overhead cameras and head-mounted
microphones. Each pair plays several rounds of the game; the obtained recordings form
a private multimodal dataset. The UpStory dataset is subsequently obtained by
extracting anonymized face and pose features from the video recordings. Finally, we set
ML baselines for the prediction of rapport, either using features from a single child, or
considering the joint data from both children in a game round.
In summary, the main contributions of this paper are:
1. the proposal of a novel experimental manipulation based on friendship network
analysis to obtain high-rapport and low-rapport pairings of the student population
in educational settings;
2. the collection of a new multimodal dataset on child-child interaction, with an
experimental manipulation for the level of rapport, including recordings of
children interacting in a task-oriented activity, captured from two audio sources
(head-mounted microphones) and two video sources (overhead cameras);
3. the publishing of UpStory, an open access feature dataset; and
4. the establishment of ML baselines for the prediction of rapport in the feature
dataset.
The UpStory dataset is publicly available at
https://zenodo.org/doi/10.5281/zenodo.12635620
The source code for this project is available at
https://github.com/MarcFraile/dyadic-storytelling
Related Work
Children’s rapport
Rapport requires two individuals willing to make and establish a meaningful connection.
As rapport occurs when people’s communication styles synchronize, it facilitates
July 8, 2024 2/30communication during task-oriented activities, fostering a higher degree of cooperation,
and leading to a more profound level of engagement [4–6]. Children’s relationships with
others evolve as they progress through various developmental stages impacting their
cognitive development. Engaging in discussions and collaborative tasks with friends and
peers prompts children to explore and critically evaluate varying perspectives,
contributing to their cognitive growth [15,16]. For these positive effects to materialize,
establishing relationships characterized by quality is crucial. In this context,
rapport-building becomes essential in assessing children’s positive connections with
others, involving mutual understanding and caring [1].
Behaviors such as cooperation, social contact, positive affect, and verbal
communication are significantly more prevalent among individuals who share rapport [1].
Numerous studies have aimed to develop tools for predicting and identifying peer
relationships, employing diverse methodologies such as direct observation [17], manual
video annotation [18], sensor-based assessment [19], and automatic prediction
techniques [20]. These approaches are based on analyzing group behaviors, where social
relationships are evaluated through measurements of member distances, facial
expressions, conversational patterns, synchrony, and time spent together [20–23].
While those studies have utilized diverse approaches to analyze group behaviors and
evaluate social relationships, there is a need to understand the broader context of peer
interactions. As such, research in Child Development has emphasized the exploration of
social networks to understand children’s status within a social group [24]. More
specifically, friendship networks provide insights into the dynamics of peer relationships
and the formation of social bonds and rapport. Approaches such as self-reported
friendship nominations contribute to the identification of social networks by identifying
key players and patterns of connections [25,26]. This research aims to provide a dataset
for the automatic prediction of rapport in child-child dyadic interactions. Therefore, we
relied on social networks to identify dyads with high and low rapport and, consequently,
to comprehend the dynamics in these dyads.
Datasets on Dyadic Interactions
Dyadic social interaction datasets are crucial for understanding how individuals engage
in various activities and gaining insights into interaction dynamics. For instance, the
IEMOCAP dataset [27] consists of audiovisual recordings of English-speaking actors,
either (1) acting out a scene or (2) improvising on a pre-defined topic. The dataset aims
to elicit emotional expressions and provides annotated data for different categories of
emotions such as happiness, anger, and sadness, and labels for valence and dominance
dimensions. Another dataset is ALICO [28], which consists of audiovisual recordings of
German-speaking adults taking either the role of a storyteller (sharing a vacation story)
or an active listener. The ALICO dataset provides hand annotations for head
movements and the function of the listener’s responses, among others, characterizing
mechanisms in interpersonal communication.
While these datasets provide great value to the study of social interactions between
dyads, they suffer from low agreement between raters when annotating the data, which
could affect the reliability and quality of it [29]. Cohen’s Kappa is a statistical criterion
used to measure the reliability between raters, for the ALICO dataset, this value is
around κ=0.30, which is considered low [30]. Moreover, the majority of these existing
datasets focus on examining dyads with adults as the target population with a limited
focus on capturing children’s social interactions [10,27,28]. The lack of available
datasets to study children’s interactions makes it difficult to generalize findings across
different age groups and understand the developmental aspects of social interactions.
Datasets designed to capture children’s social interactions often emphasize the
adult-child context. These datasets have primarily concentrated on evaluating children’s
July 8, 2024 3/30reactive emotions to objects [9], exploring play in the context of autism diagnoses [31],
or annotating and estimating synchrony between a child and a therapist [32]. Recent
efforts have introduced a comprehensive multimodal dataset capturing the dynamics of
parent-child interaction within a storytelling context [33]. However, the tasks selected to
capture social interactions in these datasets do not encompass the natural interactions
that arise when children engage with their peers.
Only a few datasets focus on child-child interactions. The P2PSTORY dataset [11]
focuses on child-child interactions during a collaborative storytelling task, with one child
as the listener and the other as the storyteller. The dataset includes video and audio
data, along with annotations of non-verbal behaviors such as gaze, smiles, forward leans,
nods, and verbal cues like short utterances and backchanneling. Multiple coders
manually annotated children’s behaviors following a defined coding scheme. While
adhering to standard annotation practices, coder agreement varied across behaviors,
with more observable actions achieving higher agreement levels than subjective
evaluations, highlighting the complexity of annotating behaviors with multiple label
options.
The PInSoRo dataset [12] is also dedicated to understanding the social dynamics
among children in free-play interactions. This multimodal dataset provides video and
audio recordings, employing automated computational methods for feature extraction,
such as prosody, voice quality, facial landmarks, Action Units (AUs), gaze, and skeleton
data. It also includes manual annotations of social interaction, focusing on task and
social engagement, and social attitudes in child-child dyads and child-robot dyads.
Similar to the P2PSTORY Dataset, PInSoRo’s manual annotations involved multiple
experts in human behavior; however, the challenge of coding social interaction is once
again echoed, as it resulted in a low agreement between coders.
While both datasets contribute to understanding the dynamics in children’s
interactions with others, they do not focus on the differences in interactions with friends
versus acquaintances. This distinction is crucial, as interaction patterns can significantly
vary based on the relationship between the individuals. Therefore, there is a need for
datasets that explicitly categorize interactions based on the nature of the relationship,
enabling a more nuanced analysis of children’s social dynamics.
Automatic prediction of rapport
Computational models in Affective Computing are emerging to better understand the
complex dynamics of social interactions, such as analyzing and predicting human
behavior, affect, and social constructs [34]. Therefore, different verbal and non-verbal
behaviors could provide information on rapport in an interaction. For instance, the
estimation of children’s closeness to others using computational approaches has
primarily been explored through location-based techniques, specifically by computing
the distance separating two children to predict their friendship status. Kanda and
Ishiguro implemented a friendship estimation algorithm by using wearable sensors’ data
to calculate potential friendships between children based on their interaction times, with
specific attention to simultaneous interactions [19]. In another study, depth sensors’
data was employed to design a tracking algorithm aimed at estimating the child’s
position. Additionally, RGB cameras were utilized for face identification. These data
sources were subsequently integrated to provide an estimation of children’s relationships
with their peers within the classroom [35].
In addition, synchrony and mimicry have been studied due to their impact on
children’s attitudes toward each other, enhancing social bonds with friends [22,36]. For
instance, in a peer-play context, a study involving children aged 4 to 6 years revealed
that when interacting synchronously with their peers, children exhibited higher
frequencies of helping behaviors, mutual smiles, and eye contact compared to
July 8, 2024 4/30asynchronous interactions [37]. Notably, mutual smiling and sustained eye contact were
prominent features of interactions specifically among friends [38], suggesting that
non-verbal behaviors play a crucial role when assessing children’s attitudes and
relationships in different social interactions.
Other studies employ benchmark tools such as OpenFace [39] for the extraction of
facial features, OpenPose [40] for the extraction of body features, and OpenSMILE [41]
for the extraction of voice features. Wu and colleagues conducted a user study in which
university students participated in video call consultations with simulated patients [7].
Motor mimicry episodes were automatically detected using features extracted with
OpenFace. Statistical analysis revealed a correlation between mimicry and
communication skills in dyadic interactions. Alsofyani and Vinciarelli designed a
multimodal approach for attachment recognition in children aged 5 to 9 [8].
Participants were asked to participate in a storytelling-based psychiatric test typically
scored by an expert rater. In this case, an automated pipeline using OpenFace and
OpenSMILE was used to predict the expert annotations, using a combination of logistic
regression models for unimodal and multimodal data. Similar methodologies have been
applied to analyze behaviors such as gaze duration and direction, and engagement,
utilizing video data and exploring interactions between infants and children with
adults [42,43]. However, the existing literature remains scarce in its examination of the
dynamics of social interactions when assessing children’s relationships with others.
Consequently, tools that automatically predict the level of rapport based on children’s
behaviors are still limited. This limitation arises from the lack of appropriate datasets
that distinguish between different types of rapport in children’s dyads and the relevant
features that enable accurate rapport prediction
To address these gaps, we present the UpStory multimodal dataset, which focuses
on capturing the dynamics of social interactions in children’s dyads. We designed a
study based on friendship network analysis to obtain low-rapport and high-rapport
dyads. This method allowed us to study dynamic interactions between children and
their peers in an educational context. Contrary to (subjective) manual annotation of
the existing datasets, we used a friendship network methodology to obtain more
objective annotations. In addition, we extracted behavioral features using benchmark
tools such as OpenFace and OpenPose, and provided ML baselines for predicting the
level of rapport in the dataset. By implementing the friendship network methodology,
we ensured that the behaviors captured within the dataset were representative of social
dynamics inherent to children’s interactions, where rapport levels can vary significantly
depending on their social status (i.e., the position that one holds in a group [44]). As a
result, the UpStory dataset facilitates a more robust analysis of children’s social
interactions and the automatic prediction of rapport.
Materials and Methods
In order to collect the UpStory dataset, we designed a study with the explicit goal to
collect samples of high-rapport and low-rapport pairs of children participating in a
collaborative storytelling activity. This section details the design of the study, including
the participant population, the pair-making strategy, and the primary data collection
effort.
Participants
The study was performed at a local primary school during free play hours. Prior to
execution, the research plan was reviewed and approved by a local ethics committee1.
1Etikpr¨ovningsmyndigheten,diarienummer2022-04863-01.
July 8, 2024 5/30We collaborated with the school’s teaching team to inform the students and their
families about the study, and to collect consent forms signed by the children’s legal
guardians. Only children who presented a completed consent form and showed interest
in participating were retained for the study. The children were informed that they could
refuse to participate at any point, without consequences.
The recruited children were students in Year 2 (ages 8-9) and Year 3 (ages 9-10) who
could speak English fluently. The collaborating school had a designated
English-language class per year; students in the Swedish-language classes who were
fluent in English were also allowed to participate. A total of 39 children signed up for
the study and were assigned a two-digit random ID. One student was subsequently
excluded due to lack of availability, resulting in a population of 38 participants: 28
students in Year 2 (14 boys and 14 girls), and 10 students in Year 3 (4 boys and 6 girls).
Experimental Conditions
Pair Making with a Social Distance Heuristic
Since manual annotation of social constructs is prone to produce disagreement between
coders [11,12,45], it is preferable to capture high-rapport and low-rapport pairs through
an explicit experimental manipulation. Given a large cohort size, a between-subjects
design could be achieved by choosing pairs of participants that are either known to be
close friends (high-rapport condition), or known to have separate social circles
(low-rapport condition), at the cost of introducing selection bias. However, this is not a
valid approach if we have access to a smaller cohort (such as classmates in a primary
school), or if we wish to avoid the implicit bias. To address this, we propose a
pair-assigning strategy that is suitable for small cohorts, using a within-subjects design
which ensures that each child participates once in each condition. Our method is based
on relaxing the criteria for pair selection: we seek to optimize a social distance heuristic,
instead of choosing guaranteed close or distant pairs.
An established strategy to quantify social relations in the classroom is to form a
friendship network by asking students to list their friends, and using graph theory to
analyze the collected data [25,26]. The particular form used in this study is a directed
graph G=(V,E), with participants as vertices v ∈V, and nominations as directed
edges e∈E. While the friendship network is a coarse approximation of the complex
social relationships in the classroom, it allows us to introduce a social distance heuristic
d(a,b), described in Algorithm 1.
Algorithm 1 Social distance heuristic: mathematical distance measuring separation
between individuals in a directed friendship network G = (V,E). Individuals are
represented by vertices v ∈V; friendship nominations are represented by directed edges
e∈E.
Require: Directed graph G=(V,E). ▷ Represents the friendship network.
Require: Vertices a,b∈V. ▷ Represent individuals in the network.
if paths from a to b exist then
d′(a,b)← (length of the shortest path connecting a to b).
else
d′(a,b)←|V|. ▷ Paths can be at most |V|−1 edges long.
end if
d(a,b)←d′(a,b)+d′(b,a). ▷ 2≤d(a,b)≤2|V|, assuming a̸=b.
Our solution to obtain balanced experimental conditions is to ask each child to
participate twice; once in each condition. In the high-rapport condition, we split the
cohort into low-distance pairs by minimizing the sum of distances; in the low-rapport
July 8, 2024 6/30condition, we split the cohort into high-distance pairs by maximizing the sum of
distances. If we consider the fully connected weighted graph G(cid:101) =(V,E(cid:101)) having the
participants as vertices, and having the (bidirectional) edge between a and b weighted
by d(a,b), the high-rapport partition corresponds to solving the minimum-weight
maximal matching problem; while the low-rapport condition corresponds to solving the
maximum-weight maximal matching problem.
Validation of the Pair-Making Procedure
In the weeks before performing the main study, we asked each participant to privately
fill in a form listing their “closest friends”. We refer to this document as the friendship
nomination form. The form contained the prompt and 10 empty lines, with no specific
instruction on how many names to include. Respondents were instructed to identify at
least one close friend, and encouraged to list a good amount of friends. While
respondents were encouraged to list classmates from school, they were also allowed to
add any other names.
All 38 included participants filled in the friendship nomination. After matching the
nominations to IDs with help from the teaching team, we obtained 2.68±1.97
within-cohort nominations per child (min 0, max 8). Since only one nomination involved
students from different years, all subsequent analysis was performed separately per year.
Fig 1. Friendship network of the Year 2 cohort. Vertex color indicates class (labeled
randomly as A, C, D for anonymity). Edge color and shape indicate type of connection
(one-way or mutual). The light gray loop corresponds to a child who nominated
themselves.
Figure 1 shows the friendship network for Year 2. We can see the subgraphs
corresponding to each class tend to be quite connected, while very few connections exist
accross class boundaries. We hypothesize this helped the quality of the pair-making
process. Figure 2 shows the friendship network for Year 3. Since only one class
participated, the graph is more evenly connected, although some students are still
completely separated from each other.
We used our pair-making procedure to obtain two partitions of the student
population (a high-rapport pairing and a low-rapport pairing), calculated separately per
year. This resulted in 14 pairs per condition in Year 2, as well as 5 pairs per condition
in Year 3, totalling 38 pairs (19 per condition). The pairings were validated by the
teaching team, who confirmed no problematic pairs were suggested by the algorithm
July 8, 2024 7/30Fig 2. Friendship network of the Year 3 cohort. All children belonged to the same class
(labeled randomly as B; shown in purple). Edge color and shape indicate type of
connection (one-way or mutual).
(e.g., children likely to get in a fight), and that the high-rapport pairings generally
corresponded to closer acquaintances than the low-rapport pairings.
Table 1 shows statistics for the social distance heuristic in each year and condition.
In both cohorts, we obtain a desired lower social distance for the high-rapport condition,
with large effect sizes (as measured by Cohen’s d [46]), but the effect is stronger in Year
2, and a t-test only indicates significant results in Year 2. Given the sample size, the
lack of significance in Year 3 is expected; however, we are confident that the effect size
supports our choice of methodology. Note that distances depend on the cohort size: in
Year 2, distances are in range 2≤d(a,b)≤56, while in Year 3, distances are in range
2≤d(a,b)≤20.
Table 1. Statistics for the social distance heuristic. The p-value corresponds to a t-test
with α=0.05; asterisk denotes significance. The reported effect sizes (measured with
Cohen’s d) are typically described as large to huge [47]. The statistics are calculated
separately for each year. Note the small sample sizes.
Year Condition count mean std p (t-test) Cohen’s d
high rapport 14 13.64 17.40
2 <0.001∗ 3.08
low rapport 14 54.21 6.68
high rapport 5 7.60 4.67
3 0.12 1.07
low rapport 5 12.80 5.02
July 8, 2024 8/30Task Design
The primary activity in this study is a collaborative storytelling game in which a pair of
children develop and present a story together, with the help of a virtual deck of picture
cards. In the planning phase, the children are led into a private space (the planning
area), and a random selection of 12 virtual cards is presented face-down in a standing
touchscreen. The children can reveal as many cards as they see fit by pressing on them,
and are instructed to select a subset of 6 cards to be incorporated into the story. They
are allowed as much time as needed to plan the story and to move freely within the
planning area. Later, in the presentation phase, the children jointly present the story
they designed to the experimenters in a designated presentation area. The planning and
presentation phases are then repeated for each additional game round.
The card game was implemented as a static webpage, using basic web technologies
(HTML, JavaScript, CSS). The cards displayed to the children are chosen at random
from a pool of settings and types. There are nine settings available, chosen to be a mix
of realistic scenarios (e.g., hiking in nature, going to the hospital) and fantastic
scenarios (e.g., knights and wizards, halloween monsters). All cards are displayed in a
similar cartoon style, in order to promote free mixing of options. Each setting contains
a number of location cards, character cards, and object cards. In each round of the
game, three settings are chosen at random (ensuring the same option is not chosen in
two consecutive round). One location, two characters, and one object are chosen from
each scenario, forming a grid of 12 cards. We collected the original card images from the
free-use image website Freepik2, and processed them to a uniform size. Figure 3 shows
an example board with most cards revealed.
Fig 3. Example of a playing board, showing face-up and face-down cards. Each row
corresponds to one setting. Top to bottom: halloween monsters, hiking, fantasy. Each
column corresponds to a card type. Left to right: location, character, character, object.
Hypotheses and Measures
In Validation of the Pair-Making Procedure, we provided statistical evidence that the
pair-making procedure attained larger pair distances in the low-rapport condition when
2https://www.freepik.com/
July 8, 2024 9/30compared to the high-rapport condition, but we did not address whether rapport was
successfully manipulated. In order to measure social and emotional effects, we employed
questionnaires to validate the following experimental hypotheses:
H1 Children feel closer to their partner when playing in the high-rapport condition
than when playing in the low-rapport condition.
H2 Children report higher scores in the Valence-Arousal-Dominance model of emotion
when playing in the high-rapport condition than when playing in the low-rapport
condition.
We tested H1 through a post-interaction administration of Inclusion of Other in the
Self (IOS) [48] (shown in Figure 4), a single-item 7-point pictographic scale used to
measure a person’s perceived closeness to another individual. Following
Kory-Westlund [49], we add two calibration items to verify that the children understood
the questionnaire. We asked the children to rate their closeness with three individuals:
(1) their best friend, (2) a “bad guy” from fictional media, and (3) their partner during
the storytelling activity.
Fig 4. IOS questionnaire [48], used to measure the closeness of each participant with
their best friend, a “bad guy” from fictional media, and their partner during the game.
Measured after the interaction.
Regarding H2, the Valence-Arousal-Dominance model is a continuous-variable
representation of emotion [50]. Valence represents pleasantness of emotion, Arousal
measures intensity of emotion, and Dominance refers to the degree of control
experienced. We tested H2 through pre-interaction and post-interaction administration
of the Self-Assessment Manikin (SAM) [51] (shown in Figure 5), a pictographic
questionnaire used to measure a person’s emotional state. It consists of three
single-item scales directly measuring Valence, Arousal, and Dominance. Each scale
contains 5 pictures and is graded 1-5, but the original procedure allows adding crosses
between pictures, making it a 9-point scale. Given our young audience, we chose to
simplify it as a 5-point scale.
Experimental Apparatus
The main activity was performed inside the collaborating school, within the English
library room. Figure 6 illustrates the experimental setup.
The room’s desks (wide rectangles in the diagram) were arranged to delineate the
planning area, a rectangular enclosure of approximately 160 cm × 140 cm where the
participants performed the primary activity (planning a story). The planning area was
further separated from the rest of the room by a set of mobile walls (thin rectangles in
the diagram), creating visual isolation from the area occupied by the researchers.
One of the sides delimited by desks was designated as the front of the planning area.
A touchscreen was placed at its center in standing mode, and used to display the
July 8, 2024 10/30Fig 5. SAM questionnaire [51], used to measure the Valence (top row, left to right),
Arousal (middle row, right to left), and Dominance (bottom row, left to right)
dimensions of emotion. Measured before and after the interaction.
Fig 6. Diagram depicting the experimental area used for the storytelling activity.
Children first designed a story in the planning area, and then emerged into the
presentation area to retell the story to the experimenters.
storytelling card game. Two Panasonic HC-V380 cameras (1920×1080px at 25fps;
stereo audio at 48kHz) were mounted on tripods at either side of the touchscreen, at a
height of 180cm. These are referred from the child’s point of view as left camera and
right camera. A Logitech c920 webcam (1920×1080px at 30fps; stereo audio at 48kHz)
was placed right in front of the touchscreen to compensate for any occlusions caused by
the screen itself. This is referred to as the frontal camera.
Per-child audio of the whole interaction was recorded using head-mounted
microphones. Each child was fitted with a SubZero SZW-20 wireless microphone set.
July 8, 2024 11/30The audio was captured with a Tascam US-2X2HR audio interface.
In contrast to the walled-off planning area, the presentation area was the working
space used by the experimenters. The presentation area contained a Windows laptop
used to (1) run the game application shown on the touchscreen, using the Firefox web
browser; (2) record the frontal camera video stream, using Logitech recording software;
and (3) record the headset audio streams, using Audacity (a popular open-source audio
editing and recording application).
Once a participating pair had finished planning their story, they were instructed to
enter the presentation area, stand in front of the presentation camera, and retell their
story to the experimenters. A Canon EOS Rebel T3i (1920×1080px at 29.97fps; stereo
audio at 48kHz) was used in all sessions except one; an iPhone Pro 11 (3840×2160px at
30fps; stereo audio at 44.1kHz) was used in the remaining session. The presentation
camera was mounted at a height of approximately 120 centimeters, 2 meters away from
the children.
Procedure and Protocol
The study started with an introductory meeting with each participating class. During
the meeting, the whole classroom was introduced to the storytelling game, and allowed
to try it in groups of 2-5 students. This was done to familiarize the children with the
activity, and to register further interest from the students. During the following days,
registered participants filled out the friendship nomination form in private. Each child
was subsequently assigned to a high-rapport pair and a low-rapport pair, as indicated in
Pair Making with a Social Distance Heuristic.
For the remainder of the study, we invited one pair at a time to participate in the
storytelling game. Pairs were chosen based on availability, prioritizing balance between
experimental conditions to avoid order effects. Participating pairs were asked to
communicate in English during the activity.
The pair were first called into the experimental space, where they played a short
warm-up round of the game with help from two researchers, in order to remind them of
the rules and familiarize them with the planning area and presentation area. Following
this, the pair were separated, and each child filled the pre-activity questionnaire (SAM)
with help from a researcher.
Next, the children donned the microphones with help from the researchers and
proceeded to play several rounds of the game. They were allowed to play up to three
rounds, or until the total planning time exceeded 10 minutes. Some exceptions were
made for fast pairs, allowing a fourth round if the children requested it. The
participants could ask to stop early at any time.
Finally, the children removed the microphones with help from the researchers and
were separated again to fill the post-activity questionnaire (SAM and IOS).
Data Analysis
Data Collection
Of the 38 scheduled sessions, 33 were performed as planned. Two low-rapport sessions
were canceled due to a lack of participant availability. One high-rapport session was
discarded because it was a false positive: the name-based nomination system produced
a mismatch due to two children sharing the same name. Finally, an error in
communication caused a non-scheduled pair to play together, forcing us to substitute
two distance-2 pairs (children had nominated each other directly) for two distance-4
July 8, 2024 12/30pairs (children had nominated common friends). All children from the re-scheduled
pairs were friends and could be found in the same playgroup.
In total, 35 sessions were included in the recordings: 18 high-rapport sessions,
totalling 57 rounds (3.17±0.62 rounds per session); and 17 low-rapport sessions,
totalling 49 rounds (2.88±0.70 rounds per session). Proportionally, high-rapport data
corresponds to 51.4% of the sessions, and 53.8% of the rounds.
For each of the 3 cameras recording the plannnig area (left, right, frontal), as well as
the 2 head-mounted microphones, we obtained one continuous recording covering all
rounds of the game (except the warm-up round). Upon review, the frontal camera
recordings were deemed low-quality and were discarded from further analysis. The main
factors for this decision were a disproportionately high number of frames in which the
participants’ faces were out of view, and the fact that some children played with the
camera, moving it around or even flipping it.
Data Processing
The unprocessed audio sources (per-child headphone recordings) and video sources (left
and right cameras) consisted of one continuous recording for each participating pair,
containing all the game rounds that the pair played. Video sources were manually cut
and processed using FFmpeg3; audio sources were similarly cut and processed within
Audacity. A Python script was used to further synchronize the files based on audio
contentanalysis. Thepackageusedforsynchronization4 claimsthe“accuracy is typically
to within about 0.01s”; manual observation indicates no perceivable time differences.
After processing the data from all 35 included pairs, we obtained 106 sets of
synchronized multimodal recordings, corresponding to the planning phase of each game
round played, for a total of 3h 40m of recorded interaction time. The presentation phase
recordings were not processed, and are left for future analysis. Table 2 shows duration
statistics, including per-round duration in each experimental condition. While mean
comparison suggests high-rapport pairs played longer, standard deviations are high, and
a cursory t-test did not indicate ordering of the variables.
Table 2. Duration of the processed recordings. Times per round are given as mean and
standard deviation.
high-rapport low-rapport overall
per-round 2m 03s ± 1m 08s 2m 06s ± 1m 31s 2m 04s ± 1m 19s
cumulative 1h 57m 01s 1h 43m 06s 3h 40m 07s
Questionnaire Analysis
From the collected questionnaire data, two entries were discarded due to unclear
responses (the child selected multiple options, and the accompanying researcher could
not clarify the child’s intent). Further entries were discarded if they could not be paired
within-subjects (because the child only participated in one accepted pair, or because the
child had missing data in one of their questionnaire responses).
After curation, the questionnaire data contained full sets of responses for 30 children,
across 34 pairs. 17 children participated in the high-rapport condition first, and 13
children participated in the low-rapport condition first (56.67% high-rapport first).
3https://ffmpeg.org/
4https://github.com/bbc/audio-offset-finder
July 8, 2024 13/30We used the Python package Pingouin5 [52] to perform statistical analysis of the
questionnaire data. Shapiro-Wilks tests suggest none of the questionnaire item
responses are normally distributed, either overall or per condition (p<0.01). Therefore,
all subsequent analysis is done using nonparametric approaches. All statistical tests use
the standard significance level α=0.05.
Overall response distributions for the IOS post-test are displayed in Figure 7 as a
boxplot, with significant differences highlighted. The control items Bad Guy and Best
Friend followed the expected ordering with respect to the target item Partner: A
two-sided Wilcoxon signed-rank test showed that Bad Guy scores are significantly lower
than Partner scores (W =118,p<0.001), Partner scores are significantly lower than
Best Friend scores (W =221.5,p<0.05), and Bad Guy scores are significantly lower
than Best Friend scores (W =57, p<0.001). These results show that the children
understood the IOS test. Figure 8 similarly shows a boxplot of the target item Partner
across experimental conditions. Another two-sided Wilcoxon test showed the scores in
the high-rapport condition are significantly higher than in the low-rapport condition
(W =42.5,p<0.05). This validates that the pairing scheme produced significantly
closer pairs in the high-rapport condition, when compared to the low-rapport condition.
Based on the aforementioned comparisons, we can conclude there is clear statistical
evidence in favor of H1.
Fig 7. Response distribution for the IOS questionnaire items Bad Guy, Partner, and
Best Friend. Boxes show the 25% (bottom), 50% (red, middle), and 75% (top) quartiles.
Whiskers extend to the lowest and highest samples within 1.5 interquartile ranges from
the box. Samples outside the whiskers are shown as circles. Significant differences are
shown (∗p<0.05,∗∗p<0.01,∗∗∗p<0.001).
Figure 9 shows the response distribution for each SAM item, comparing pre-test to
post-test responses. We observed a strong ceiling effect, causing the data to be
inconclusive. Most participants immediately chose the options they perceived as most
positive, resulting in a response distribution heavily biased towards the high end of each
scale. In concordance with this observation, a cursory repeated-measures ANOVA
analysis shows no significant effects of condition nor moment (pre-interaction vs.
post-interaction) on the SAM responses. We are therefore forced to reject H2 due to a
5https://pingouin-stats.org
July 8, 2024 14/30Fig 8. Response distribution for the IOS questionnaire item Partner, under the
high-rapport (orange) and low-rapport (blue) conditions. The difference is significant
(∗p<0.05).
lack of information.
Fig 9. Response distribution for the SAM questionnaire items Valence, Arousal, and
Dominance, in the pre-test (blue) and post-test (orange). No significant relations were
found.
The observed results (strong support for H1 despite the small sample size; rejection
of H2 due to a ceiling effect) suggest that our pair-making strategy successfully
promoted the formation of high-rapport vs. low-rapport pairs at the population level.
July 8, 2024 15/30The UpStory Dataset
The UpStory dataset is publicly available at
https://zenodo.org/doi/10.5281/zenodo.12635620. It is an anonymized
feature-based dataset, containing OpenPose and OpenFace features extracted per-frame.
Data extracted from both the left camera and right camera sources is available. It
provides data for all 35 pairs included in the recordings, totalling 3h 40m 07s of
interaction data sampled at 25Hz. The following per-pair metadata is included:
experimental condition (high-rapport or low-rapport), social distance heuristic, school
year, number of rounds played, and each child’s ID. As observed in Data Collection, this
corresponds to a total of 106 game rounds (57 high-rapport rounds and 49 low-rapport
rounds). The remainder of this section gives detailed information on the choice of
feature extractors, the extracted features, and custom post-processing required to track
individuals over time.
In recent years, OpenPose and OpenFace have become the reference feature
extractors for pose estimation and facial feature estimation in the literature [7,8,53].
For this reason, we chose these two feature extractors to obtain an anonymized version
of the dataset that is amenable to ML research. When applied to our synchronized
video sources (left and right cameras), they produce time series of features: an entry is
produced for each tracked feature, for each frame in the recording, for each detected
individual. In our case, we can obtain a 25Hz time series per child for each tracked
feature. OpenPose provides the position and confidence of 25 body keypoints. OpenFace
provides 2D and 3D gaze direction estimates; 2D and 3D keypoint locations for the eyes;
2D and 3D keypoint locations for the whole face; position and angle estimates for the
face as a whole; intensity estimates for 17 AUs; and presence estimates for 18 AUs. AUs
represent specific muscle activations that produce facial appearance changes [54]. Table
3 shows all extracted metadata and features, and gives a brief description of each item.
Table 3. Metadata and feature sets contained in the UpStory feature dataset. Metadata is given per-pair; features are
provided as time series, captured at 25Hz.
origin variable details
metadata condition Experimental condition (high-rapport or low-rapport).
social distance Symmetric distance in the friendship network.
year academic year the children belonged to.
rounds Total number of game rounds played by this pair.
Child IDs The IDs of the two participating children.
OpenPose joint position 25 body keypoints; (x,y) in pixels.
joint confidence 25 body keypoints; confidence as fraction.
OpenFace timestamp Time from beginning of recording, in seconds.
confidence Face detection confidence, as a fraction.
success Is this a successful registration? (0 or 1).
gaze Gaze vector as (x,y,z) components, per-eye.
gaze angle Overall gaze direction, as (x,y) angles.
2D eye landmark 56 eye keypoints; (x,y) in pixels.
3D eye landmark 56 eye keypoints; (X,Y,Z) in estimated millimeters.
head position head position as (x,y,z) in estimated millimeters.
head rotation head rotation as (x,y,z) angles.
2D face landmark 68 face keypoints; (x,y) in pixels.
3D face landmark 68 face keypoints; (X,Y,Z) in estimated millimeters.
AU presence 18 AU activations, as binary variables (0 or 1).
AU intensity 17 AU activations, as continuous values (0 to 5).
July 8, 2024 16/30A processing challenge in this study is the fact that both participants could move
freely in the experiment space. This means the feature extraction pipeline needs to deal
with movement over time, occlusions, and temporary loss of detection. Neither
OpenFace not OpenPose offer identification functionality, meaning their output needs to
be processed to maintain stable child identity over time. This was achieved with a
custom post-processing pipeline based on three principles: minimizing the distance each
body keypoint moves between consecutive frames (accounting for reported detection
confidence), caching the last known location when tracking is lost, and ensuring face
and body identities are consistent with respect to each other.
Fig 10. OpenPose output visualization, displaying the joint positions and confidences,
overlayed on the original right camera frame. Hue indicates child identity; transparency
indicates detection confidence. In this frame, OpenPose successfully disambiguated a
complex crossing of limbs. Child identity over time is not provided by either feature
extraction tool, and is calculated as a post-processing step.
Fig 11. OpenFace output visualization, displaying the eye locations, gaze vectors, and
face keypoint bounding box, overlayed on the original left camera frame. Hue indicates
child identity; gaze vectors are color-coded Blue for Left and Red for Right. Child
identity over time is deduced from the corresponding body pose estimate.
Figure 10 shows post-processed OpenPose output with attached identities
(represented by the skeleton color and the ID tag). Each detected body keypoint is
shown as a circle, with opacity indicating detection confidence. Similarly, Figure 11
shows post-processed OpenFace output with attached identities. For simplicity, we only
display the eye positions, gaze vectors, and bounding boxes containing all face keypoints.
July 8, 2024 17/30ML Baselines
In order to validate the predictive power of the UpStory dataset, we established
baselines for the prediction of the level of rapport based on the publicly available
features. Two different baselines are provided: Single-Child Baseline considers data
from a single child as a sample, while Joint Pair Baseline considers joint data from both
children in a pair as a sample. In both cases, the task is binary classification: using the
AU feature time-series as input, the algorithm predicts the experimental condition
(high-rapport pair vs. low-rapport pair). We first discuss Feature Selection, Data
Stratification, and Model Selection, since they apply to both experiments.
Feature Selection
Following an established procedure [53,55], we focused on the AU estimates provided by
OpenFace, and reduced each AU time series to a selection of summary statistics,
calculated per-child. As listed in Table 3, OpenFace produces independently calculated
estimates of presence (binary variable) and intensity (continuous variable); 17 AUs are
covered in both modalities. For each AU estimate, we calculated the following summary
statistics: mean, standard deviation, and 95% percentile. The mean and standard
deviation have been used by Paetzel [55] and by Srivastava [53], among others. Usage of
the 95% percentile is adapted from a similar idea in Alsofyani and Vinciarelli’s work [8].
If we consider all possible combinations of AU, estimate type, and summary statistic,
we obtain 102 features (e.g., AU10-intensity-q95, or AU17-presence-mean). Table 4
shows the possible combinations.
Table 4. A total of 102 features are considered for the ML baselines. Each feature is a
combination of (1) a target AU, (2) an estimate type, and (3) a summary statistic. E.g.,
AU06-presence-mean (we track the AU06 presence estimate, and aggregate by taking
the mean), or AU12-intensity-q95 (we track the AU12 intensity estimate, and aggregate
by taking the 95% quantile). This table lists all considered values for (1), (2), and (3).
In particular, only AUs with both estimates available are considered. q95 indicates 95%
quantile; std indicates standard deviation.
AU estimate type statistic
AU01, AU02, AU04, AU05, AU06, AU07, presence mean
AU09, AU10, AU12, AU14, AU15, AU17, intensity std
AU20, AU23, AU25, AU26, AU45 q95
While other authors working with larger datasets have chosen to use all 17 AUs as
input features [8], due to our comparatively small number of samples, we decided to
train on small feature sets consisting of 1-4 features to avoid overfitting. Two different
feature selection approaches were combined: theory-based and data-driven.
The theory-based feature sets are based on expressed happiness: the joint activation
of AU06 (cheek raiser) and AU12 (lip corner puller) is identified by Ekman as the
indicator of a “genuine smile”, and associated with the basic emotion of happiness [54].
We considered both presence and intensity estimates (without mixing estimate types).
As summary statistics, we either took the more established combination of mean and
standard deviation, or alternatively the 95% quantile for extreme event quantification.
In total, 4 theory-based feature sets were considered. They are listed as the last block in
Table 7.
For the data-driven approach, we used a battery of t-tests with Bonferroni correction
to find variables having significantly different values between experimental conditions.
July 8, 2024 18/30We chose combinations of significantly different variables based on the following
soundness rules: do not use more than 2 AUs at once, only use one type of statistic at a
time (with mean and standard deviation being allowed as either two separate statistics,
or a double combination), do not mix presence and intensity variables.
Table 5 lists the features that were significantly different across experimental
conditions, as extracted from the left camera, and provides the t-test results (with
Bonferroni correction). Table 6 lists the corresponding features and statistics for the
right camera. In both sources, AU12 (lip corner puller) showed significant differences in
standard deviation and 95% quantile for both estimate types, though the difference in
means was not significantly different under Bonferroni correction. AU06 (cheek raiser),
AU10 (upper lip raiser) and AU25 (lips part) displayed significant differences in the left
camera, while AU26 (jaw drop) displayed significant differences in the right camera.
Note that AU25 and AU26 can indicate speech, suggesting that the amount of speech
might act as a predictor of the experimental condition. Notably, only one of the
significant statistics is a mean. This suggests children in the high-rapport condition
expressed a wider range of emotions, with more extreme values being detected.
Table 5. Significantly different summary AU statistics accross experimental conditions
for the left camera, as measured by a t-test with Bonferroni correction. q95 indicates
95% quantile; std indicates standard deviation.
AU estimate type statistic adjusted p-value Cohen’s d
AU10 presence std 0.006 0.58
q95 <0.001 0.70
intensity q95 0.012 0.55
AU12 presence std 0.015 0.54
q95 0.012 0.55
intensity std 0.006 0.57
q95 0.010 0.55
AU25 presence std 0.024 0.53
intensity std 0.026 0.52
q95 0.038 0.51
AU06 intensity std 0.020 0.52
q95 0.031 0.51
Table 6. Significantly different summary AU statistics across experimental conditions
for the right camera, as measured by a t-test with Bonferroni correction. q95 indicates
95% quantile; std indicates standard deviation.
AU estimate type statistic adjusted p-value Cohen’s d
AU12 presence std 0.016 0.55
q95 0.017 0.56
intensity std 0.009 0.56
q95 0.005 0.58
AU26 presence mean 0.001 0.62
std <0.001 0.72
Each AU selected by the data-driven approach was considered on its own, and
combined with other AUs that were significant in the same source (e.g., AU10 and
AU12 — left camera —, but not AU25 and AU26 — mix of cameras). For each of the
July 8, 2024 19/30selected AU combinations, 95% quantiles of the presence and intensity estimates were
considered. Finally, the mean and standard deviation of the presence estimates was also
considered. In total, 21 data-driven feature sets were considered. Together with the
theory-driven feature sets, a total of 25 feature sets were considered. All combinations
are listed in Table 7. The first three blocks correspond to features identified from the
left camera; the third block corresponds to features identified from the right camera.
Table 7. Feature sets considered for ML training. Each row shows a feature set consisting of 1-4 individual features. Each
feature represents a choice of tracked AU, estimate type, and summary statistic, as shown in Table 4. AU06 and AU12
combinations (last block) are chosen based on prior literature; other combinations are surfaced through data analysis.
AU10-presence-q95 AU12-presence-q95
AU10-intensity-q95 AU12-intensity-q95
AU10-presence-q95
AU10-intensity-q95
AU12-presence-q95
AU12-intensity-q95
AU10-presence-mean AU10-presence-std AU12-presence-mean AU12-presence-std
AU10-presence-mean AU10-presence-std
AU12-presence-mean AU12-presence-std
AU25-presence-q95 AU12-presence-q95
AU25-intensity-q95 AU12-intensity-q95
AU25-presence-q95
AU25-intensity-q95
AU25-presence-mean AU25-presence-std AU12-presence-mean AU12-presence-std
AU25-presence-mean AU25-presence-std
AU26-presence-q95 AU12-presence-q95
AU26-intensity-q95 AU12-intensity-q95
AU26-presence-q95
AU26-intensity-q95
AU26-presence-mean AU26-presence-std AU12-presence-mean AU12-presence-std
AU26-presence-mean AU26-presence-std
AU06-presence-q95 AU12-presence-q95
AU06-intensity-q95 AU12-intensity-q95
AU06-presence-mean AU06-presence-std AU12-presence-mean AU12-presence-std
AU06-intensity-mean AU06-intensity-std AU12-intensity-mean AU12-intensity-std
Data Stratification
In order to provide high-quality validation and test sets, we partitioned the dataset into
5 folds (disjoint subsets of samples), separated at the pair level, and using stratification
to ensure good sample balancing. Sampling was designed to always distribute pairs
from the same year and condition evenly over the folds; further rejection sampling was
performed to enforce good statistical qualities of each fold. In particular, the following
quantities were optimized by random sampling, in order of importance:
• Number of pairs.
• Fraction of rounds belonging to high-rapport pairs (class balance at sample level).
• Number of rounds (total samples).
• Fraction of seconds belonging to high-rapport pairs (class balance at duration
level).
July 8, 2024 20/30• Total video length.
Each listed quantity was evaluated using the relative square error compared to a
uniform distribution over the folds, and weighted according to its importance. This
resulted in 5 folds, each containing 7 pairs (3 to 4 pairs per condition). Distributions of
game rounds per fold range from 48% high-rapport to 57% high-rapport (in total, 21 to
22 rounds per fold).
Model Selection
Three shallow ML models were chosen, due to their interpretability, and to set a
statistically sound baseline for further study: logistic regression, Support Vector
Machines (SVM), and decision trees. We performed the subsequent analysis in Python,
using scikit-learn6 [56]. A set of hyperparameter values was identified for each model;
the hyper-parameters and their tested values are shown in Table 8.
For each possible combination of video source, feature set, and classification model,
we performed nested k-folds cross-validation. The inner 4-fold cross-validation loop was
used to perform a grid search and obtain optimal hyper-parameters. The outer 5-fold
cross-validation loop was used to estimate the model’s performance. To be precise, the
following procedure was followed:
1. For each of the five pre-determined folds:
(a) The fold is marked as the test set.
(b) For each remaining fold:
i. The fold is marked as the validation set. The remaining three folds are
marked as the train set.
ii. Each combination of hyper-parameters (from the valid combinations
shown in Table 8) is used to train on the train set, and evaluated on the
validation set.
(c) The best performing hyper-parameters are chosen (based on average
accuracy on the validation sets), and evaluated on the test set.
2. The expected accuracy of the classification model is estimated as the average test
accuracy.
Table 8. Hyperparameters optimized for each model type. All valid combinations were
tested in a grid search (e.g., SVM’s gamma parameter only applies to the radial basis
function kernel).
model hyperparameters tested values
logistic solver { lbfgs, saga }
penalty { None, L1, L2, elasticnet }
C 10λ, λ∈{−2,−1.ˆ6,...,+2}
SVM C 10λ, λ∈{−2,−1.ˆ6,...,+2}
kernel { linear, rbf }
gamma { scale, auto }
tree criterion { gini, entropy }
max depth { 2, 3, 4, 5 }
6https://scikit-learn.org/
July 8, 2024 21/30Single-Child Baseline
In the single-child baseline, summary statistics for one child (calculated over the AU
data from one game round) are used as features to predict the pair-level label
(high-rapport vs. low-rapport). This means we obtain two separate samples from one
game round, for a total of 212 samples. Feature sets range from 1 to 4 features per
sample. Combining the 25 feature sets described in Feature Selection and the three
models described in Model Selection, we obtain 75 separate experiments for the
single-child baseline.
Compared to the baseline random classifier accuracy of 53.8%, all 75 feature-model
combinations trained on the left camera obtained better-than-random accuracy on the
train set, and 64/75 obtained better-than-random accuracy on the test set. Respectively
for the right camera, 74/75 models performed better-than-random on the train set, and
58/75 performed better-than-random on the test set. These high success rates suggest
that the AU features generally contain rapport information, and some amount can be
extracted with any simple ML strategy.
Table 9 lists the top 10 models, ranked by test accuracy on the left camera. Two
entries tie for best performance: training a decision tree using AU10 and AU12 presence
with either the mean-standard deviation combination or the 95% quantile yields a test
accuracy of 68.40%. Most listed models rely on AU10, which was selected using the
data-driven approach. Comparing model performance between the two camera streams
(rightmost columns on the table), there seems to be a substantial generalization gap,
with few models performing comparatively well on both sources.
Table 9. Top 10 single-child models ranked by left camera test accuracy. Listed values include the left camera’s test
accuracy, and the right camera’s test accuracy. q95 indicates 95% quantile; std indicates standard deviation.
model AUs statistic type left camera right camera
decision tree AU10, AU12 mean, std presence 68.40%±6.16% 55.71%±9.00%
decision tree AU10, AU12 q95 presence 68.40%±6.60% 63.33%±14.15%
SVM AU10 q95 presence 66.06%±4.66% 62.38%±13.08%
decision tree AU10 q95 presence 66.06%±4.66% 61.90%±13.98%
linear AU10 q95 presence 66.06%±4.66% 61.90%±13.98%
SVM AU10, AU12 q95 presence 65.11%±4.98% 57.62%±12.19%
linear AU10, AU12 q95 presence 65.11%±5.52% 57.62%±12.19%
SVM AU12, AU26 q95 intensity 64.24%±7.96% 57.62%±12.30%
linear AU12, AU25 mean, std presence 64.22%±8.97% 52.86%±4.26%
SVM AU12 q95 intensity 63.79%±8.85% 61.90%±5.32%
Table 10 lists the top 10 models, ranked by test accuracy on the right camera. The
best performing model is a decision tree trained using the AU12 intensity 95% quantile,
with a train accuracy of 69.77%, and a test accuracy of 65.71%. A theory-based
combination ranks second; all listed solutions rely on AU12. While overall performance
is slightly lower than in the left camera leaderboard, the generalization gap is also
smaller between camera sources.
Figure 12 shows a comparison of the single-child test accuracies across both camera
streams. We can see there is no strong preference for either of the three models, and the
best-performing AU estimate type depends on the evaluated source.
Joint Pair Baseline
In the joint pair baseline, the AU summary statistics from both children in a game
round are concatenated together. Compared to the Single-Child Baseline, this means
the feature count is doubled to 2-8 features per sample, and the sample count is halved
July 8, 2024 22/30Table 10. Top 10 single-child models ranked by right camera test accuracy. Listed values include the left camera’s test
accuracy, and the right camera’s test accuracy. q95 indicates 95% quantile; std indicates standard deviation.
model AUs statistic type left camera right camera
decision tree AU12 q95 intensity 54.76%±5.83% 65.71%±3.98%
linear AU06, AU12 q95 intensity 57.10%±1.76% 64.29%±9.07%
linear AU12 q95 intensity 62.36%±9.61% 63.81%±7.97%
decision tree AU12, AU26 q95 presence 63.70%±7.06% 63.33%±8.52%
SVM AU12, AU26 q95 presence 63.70%±7.06% 63.33%±8.52%
linear AU12, AU26 q95 presence 62.27%±4.90% 63.33%±8.52%
decision tree AU10, AU12 q95 presence 68.40%±6.60% 63.33%±14.15%
decision tree AU10, AU12 q95 intensity 52.40%±8.40% 62.86%±6.43%
decision tree AU12, AU25 q95 intensity 54.70%±6.49% 62.38%±5.16%
linear AU12, AU26 q95 intensity 62.79%±6.51% 62.38%±7.02%
Fig 12. Performance comparison of each single-child model across video sources. Axes
indicate the test accuracies in the left camera (horizontal axis) and right camera
(vertical axis). Color indicates model type; shape indicates feature type. Gray dashed
lines indicate the random chance baseline.
July 8, 2024 23/30to 106 samples. We use the same methodology, including the same choice of feature sets
and ML models, again adding up to 75 feature-model combinations per video source.
All 75 left camera feature-model combinations obtained better-than-random train
accuracy, and 64/75 obtained better-than-random test accuracy. Respectively for the
right camera, 73/75 model-feature combinations obtain better-than-random train
accuracy, and 54/75 obtain better-than-random test accuracy. These numbers are
similar to the ones obtained in single-child experiments.
Table 11 lists the top 10 joint pair models, ranked by test accuracy on the left
camera. Again, we get a tie: training either a decision tree or an SVM on the AU10
presence 95% quantile yields a test accuracy of 70.74%. While the model choice doesn’t
seem to matter much, most entries rely on AU10 presence estimates, with the notable
exception of a theory-based result. Test accuracies are generally higher than in the
single-child baseline, but standard deviations are larger — suggesting that the sample
size reduction causes some instability.
Table 11. Top 10 joint pair models ranked by left camera test accuracy. Listed values include the left camera’s test accuracy,
and the right camera’s test accuracy. q95 indicates 95% quantile; std indicates standard deviation.
model AUs statistic type left camera right camera
decision tree AU10 q95 presence 70.74%±10.90% 61.90%±17.50%
SVM AU10 q95 presence 70.74%±10.90% 61.90%±17.50%
linear AU10 q95 presence 68.83%±13.38% 61.90%±17.50%
linear AU10, AU12 q95 presence 65.97%±10.52% 64.76%±17.37%
decision tree AU06, AU12 q95 intensity 65.06%±11.05% 65.71%±9.16%
decision tree AU12 mean, std presence 64.98%±9.01% 49.52%±7.97%
SVM AU25 mean, std presence 64.20%±7.60% 49.52%±7.97%
linear AU10, AU12 mean, std presence 64.11%±8.80% 56.19%±12.33%
linear AU12, AU26 q95 intensity 64.11%±9.42% 58.10%±6.21%
SVM AU10, AU12 q95 presence 64.07%±8.95% 65.71%±16.97%
Table 12 lists the top 10 joint pair models, ranked by test accuracy on the right
camera. The best-performing model is a decision tree trained using a pure theory-based
approach: AU06 and AU12 mean-standard deviation combination. It attains a train
accuracy of 81.43%, and a test accuracy of 70.48%. Again, we observe higher accuracies
but larger standard deviations when compared to single-child models.
Table 12. Top 10 joint pair models ranked by right camera test accuracy. Listed values include the left camera’s test
accuracy, and the right camera’s test accuracy. q95 indicates 95% quantile; std indicates standard deviation.
model AUs statistic type left camera right camera
decision tree AU06, AU12 mean, std intensity 57.53%±12.17% 70.48%±12.78%
SVM AU12, AU26 q95 presence 56.67%±7.97% 68.57%±10.96%
linear AU06, AU12 q95 intensity 61.26%±6.58% 67.62%±15.58%
decision tree AU12 q95 presence 61.21%±9.54% 66.67%±11.17%
linear AU12, AU26 q95 presence 59.31%±9.72% 66.67%±11.17%
SVM AU12 q95 presence 56.67%±7.97% 66.67%±11.17%
linear AU06, AU12 q95 presence 54.68%±6.66% 65.71%±11.37%
decision tree AU12, AU26 q95 presence 61.21%±9.54% 65.71%±11.86%
decision tree AU12, AU25 q95 presence 60.30%±8.31% 65.71%±11.86%
SVM AU12, AU25 q95 presence 55.76%±7.75% 65.71%±11.86%
Figure 13 shows a comparison of the joint pair test accuracies across both camera
streams. We can see a similar overall distribution as in Figure 12, with higher overall
accuracies reported.
July 8, 2024 24/30Fig 13. Performance comparison of each joint-pair model across video sources. Axes
indicate the test accuracies in the left camera (horizontal axis) and right camera
(vertical axis). Color indicates model type; shape indicates feature type. Gray dashed
lines indicate the random chance baseline.
Overall, using joint data from both children resulted in slightly better accuracies,
but noticeably higher standard deviations. In this scenario, taking the interpersonal
aspect of rapport into account did not outweigh the benefits from having a bigger
population size.
Discussion and Conclusions
To better understand the social dynamics and behaviors that emerge in the classroom
when children interact with their friends and peers, we collected UpStory: a new dataset
of dyadic interactions with different levels of rapport. In order to provide an objective
measure of rapport, we leveraged friendship network analysis to propose and validate a
novel pair-making technique. Our approach allowed us to control for different levels of
rapport, resulting in a multimodal dataset on child-child interactions with high-rapport
and low-rapport levels.
July 8, 2024 25/30We recorded pairs of children participating in collaborative storytelling play for a
total of 35 sessions, adding up to 106 game rounds, with a duration of 3h 40m. Each
session is annotated with the pair’s condition (high-rapport or low-rapport) and a social
distance heuristic, allowing practitioners to train ML models based on the children’s
self-reported friendships. The resulting private dataset contains three video feeds with
associated audio, and two additional audio feeds from head-mounted microphones,
providing clean recordings for each child’s voice. UpStory is the anonymized dataset
obtained by extracting frame-by-frame body and face features from two video sources; it
is made publicly available at https://zenodo.org/doi/10.5281/zenodo.12635620.
The provided features were extracted using OpenPose and OpenFace, with a custom
solution for identity tracking over time.
We followed two methodologies to validate our pair-making technique. Firstly,
analysis of the social distance heuristic’s distribution suggested that the pairing strategy
worked at the population level, but not at the case-by-case level. This was expected,
since we optimized for the sum of distances, but it is a limitation when strong labels are
required. A possibility for further ML experiments would be to base the labels on the
pair distances instead of the assigned condition, although this can introduce
confounding variables (e.g., sociable children being over-represented in the low-distance
case). We observed better quality pairings in the Year 2 cohort, both due to a bigger
cohort size and due to the participation of several classes. A possible extension to this
method is to ensure even participation across several classes; in our study, we were
limited by the requirement that the students speak English fluently.
Secondly, our questionnaire analysis showed that children felt significantly closer to
their pair in the high-rapport condition, supporting H1. However, our survey data also
revealed that children did not necessarily have more positive emotions when playing in
the high-rapport condition than in the low-rapport condition, rejecting H2. This result
is not surprising, as the novelty effect of the activity could lead to the observed ceiling
effects when questionnaires are used to capture children’s subjective perceptions [57].
Overall, we demonstrated that our proposed pair-making technique is reliable in
controlling rapport levels. We believe that our experimental manipulation captures
more naturalistic behaviors inherent to the social dynamics of children’s dyads providing
a more reliable level of rapport than post-hoc annotations encountered in the literature.
The naturalistic social dynamics captured by our dataset hold promise for the
training of ML-based automatic prediction of rapport. To build on this, we have
provided ML baselines for the prediction of the experimental condition. Both baselines
using the data from a single child and baselines using joint pair data are provided. In all
cases, the best-performing models attained test accuracies in the 60-70% range, well
above random chance.
We hope the UpStory dataset will contribute to the design of data-driven
methodologies to further our understanding of children’s relationships with their friends
and peers, as well as the development of new technologies that interact with children in
an educational context.
Acknowledgments
This work was partly funded by the Centre for Interdisciplinary Mathematics, Uppsala
University, and the Swedish Research Council (grant n. 2020-03167).
The authors wish to thank all the colleagues who donated time and effort to help
complete this project. In no particular order: Alessio Galatolo, Gustaf Gredeb¨ack,
Johan O¨fverstedt, and Rebecca Stower.
Attribution for the card images displayed in Figure 3, in reading order: graveyard by
upklyak; mummy by upklyak; grim reaper by upklyak; savannah by macrovector; boy
July 8, 2024 26/30scout by brgfx; hearth by macrovector; dragon hoard by upklyak; elf by pch.vector;
shield by vectorpocket.
References
1. Tickle-Degnen L, Rosenthal R. The nature of rapport and its nonverbal
correlates. Psychological inquiry. 1990;1(4):285–293.
2. Travelbee J. What do we mean by rapport? The American journal of nursing.
1963; p. 70–72.
3. Frisby BN, Martin MM. Instructor–student and student–student rapport in the
classroom. Communication Education. 2010;59(2):146–164.
4. Azmitia M, Montgomery R. Friendship, transactive dialogues, and the
development of scientific reasoning. Social Development. 1993;2.
doi:10.1111/j.1467-9507.1993.tb00014.x.
5. Newcomb AF, Bagwell CL. Children’s friendship relations: A meta-analytic
review. Psychological Bulletin. 1995;117:306–347.
doi:10.1037/0033-2909.117.2.306.
6. Wentzel KR, Jablansky S, Scalise NR. Do Friendships Afford Academic Benefits?
A Meta-analytic Study. Educational Psychology Review. 2018;30.
doi:10.1007/s10648-018-9447-5.
7. Wu K, Liu C, Calvo RA. Automatic Nonverbal Mimicry Detection and Analysis
in Medical Video Consultations. International Journal of Human-Computer
Interaction. 2020;36:1379–1392. doi:10.1080/10447318.2020.1752474.
8. Alsofyani H, Vinciarelli A. Attachment Recognition in School Age Children
Based on Automatic Analysis of Facial Expressions and Nonverbal Vocal
Behaviour. In: Proceedings of the 2021 International Conference on Multimodal
Interaction; 2021. p. 221–228.
9. Nojavanasghari B, Baltruˇsaitis T, Hughes CE, Morency LP. Emoreact: a
multimodal approach and dataset for recognizing emotional responses in children.
In: Proceedings of the 18th acm international conference on multimodal
interaction; 2016. p. 137–144.
10. Bilakhia S, Petridis S, Nijholt A, Pantic M. The MAHNOB Mimicry Database:
A database of naturalistic human interactions. Pattern Recognition Letters.
2015;66. doi:10.1016/j.patrec.2015.03.005.
11. Singh N, Lee JJ, Grover I, Breazeal C. P2PSTORY: Dataset of children as
storytellers and listeners in peer-to-peer interactions. vol. 2018-April; 2018.
12. Lemaignan S, Edmunds CE, Senft E, Belpaeme T. The PInSoRo dataset:
Supporting the data-driven study of child-child and child-robot social dynamics.
PloS one. 2018;13(10):e0205999.
13. Hallinan MT. Structural effects on children’s friendships and cliques. Social
Psychology Quarterly. 1979; p. 43–54.
14. Hansell S. Adolescent friendship networks and distress in school. Social Forces.
1985;63(3):698–715.
July 8, 2024 27/3015. Hartup WW. The Company They Keep: Friendships and Their Developmental
Significance. Child Development. 1996;67.
doi:10.1111/j.1467-8624.1996.tb01714.x.
16. Bukowski WM, Newcomb AF, Hartup WW. The company they keep:
Friendships in childhood and adolescence. Cambridge University Press; 1998.
17. Berndt TJ, Perry TB, Miller KE. Friends’ and classmates’ interactions on
academic tasks. Journal of educational psychology. 1988;80(4):506.
18. Ladd GW. Preschoolers’ behavioral orientations and patterns of peer contact:
Predictive of peer status? Asher, Steven R [Ed]; Coie, John D [Ed]. 1990;:.
19. Kanda T, Ishiguro H. An approach for a social robot to understand human
relationships: Friendship estimation through interaction with robots. Interaction
Studies: Social Behaviour and Communication in Biological and Artificial
Systems. 2006;7.
20. Messinger DS, Perry LK, Mitsven SG, Tao Y, Moffitt J, Fasano RM, et al. In:
Computational approaches to understanding interaction and development. vol. 62;
2022.
21. Lin L, Feldman MJ, Tudder A, Gresham AM, Peters BJ, Dodell-Feder D. Friends
in Sync? Examining the Relationship Between the Degree of Nonverbal
Synchrony, Friendship Satisfaction and Support. Journal of Nonverbal Behavior.
2023;doi:10.1007/s10919-023-00431-y.
22. Rabinowitch TC, Meltzoff AN. Synchronized movement experience enhances peer
cooperation in preschool children. Journal of Experimental Child Psychology.
2017;160. doi:10.1016/j.jecp.2017.03.001.
23. Maman L, Ceccaldi E, Lehmann-Willenbrock N, Likforman-Sulem L, Chetouani
M, Volpe G, et al. Game-on: A multimodal dataset for cohesion and group
analysis. IEEE Access. 2020;8:124185–124203.
24. Kasari C, Locke J, Gulsrud A, Rotheram-Fuller E. Social networks and
friendships at school: Comparing children with and without ASD. Journal of
autism and developmental disorders. 2011;41:533–544.
25. George TP, Hartmann DP. Friendship networks of unpopular, average, and
popular children. Child development. 1996;67(5):2301–2316.
26. Strauss RS, Pollack HA. Social marginalization of overweight children. Archives
of pediatrics & adolescent medicine. 2003;157(8):746–752.
27. Busso C, Bulut M, Lee CC, Kazemzadeh A, Mower E, Kim S, et al. IEMOCAP:
Interactive emotional dyadic motion capture database. Language Resources and
Evaluation. 2008;42. doi:10.1007/s10579-008-9076-6.
28. Malisz Z, Wl(cid:32)odarczak M, Buschmeier H, Skubisz J, Kopp S, Wagner P. The
ALICO corpus: analysing the active listener. Language Resources and Evaluation.
2016;50. doi:10.1007/s10579-016-9355-6.
29. Alhazmi K, Alsumari W, Seppo I, Podkuiko L, Simon M. Effects of annotation
quality on model performance. In: 2021 International Conference on Artificial
Intelligence in Information and Communication (ICAIIC). IEEE; 2021. p.
063–067.
July 8, 2024 28/3030. McHugh ML. Interrater reliability: the kappa statistic. Biochemia medica.
2012;22(3):276–282.
31. Rehg JM, Abowd GD, Rozga A, Romero M, Clements MA, Sclaroff S, et al.
Decoding children’s social behavior; 2013.
32. Li J, Bhat A, Barmaki R. Improving the movement synchrony estimation with
action quality assessment in children play therapy. In: Proceedings of the 2021
International Conference on Multimodal Interaction; 2021. p. 397–406.
33. Chen H, Alghowinem S, Jang SJ, Breazeal C, Park HW. Dyadic Affect in
Parent-Child Multimodal Interaction: Introducing the DAMI-P2C Dataset and
its Preliminary Analysis. IEEE Transactions on Affective Computing. 2023;14.
doi:10.1109/TAFFC.2022.3178689.
34. Wang Y, Song W, Tao W, Liotta A, Yang D, Li X, et al. A systematic review on
affective computing: Emotion models, databases, and recent advances.
Information Fusion. 2022;83:19–52.
35. Komatsubara T, Shiomi M, Kaczmarek T, Kanda T, Ishiguro H. Estimating
Children’s Social Status Through Their Interaction Activities in Classrooms with
a Social Robot. International Journal of Social Robotics. 2019;11.
doi:10.1007/s12369-018-0474-7.
36. Denham SA, Bassett HH, Brown C, Way E, Steed J. “I Know How You Feel”:
Preschoolers’ emotion knowledge contributes to early school success. Journal of
Early Childhood Research. 2015;13. doi:10.1177/1476718X13497354.
37. Tunc¸gen¸c B, Cohen E. Interpersonal movement synchrony facilitates pro-social
behavior in children’s peer-play. Developmental Science. 2018;21.
doi:10.1111/desc.12505.
38. Goldman BD, Buysse V. FRIENDSHIPS IN VERY YOUNG CHILDREN; 2007.
39. Baltruˇsaitis T, Robinson P, Morency LP. Openface: an open source facial
behavior analysis toolkit. In: 2016 IEEE Winter Conference on Applications of
Computer Vision (WACV). IEEE; 2016. p. 1–10.
40. Cao Z, Hidalgo G, Simon T, Wei SE, Sheikh Y. OpenPose: realtime multi-person
2D pose estimation using Part Affinity Fields. IEEE transactions on pattern
analysis and machine intelligence. 2019;43(1):172–186.
41. Eyben F, W¨ollmer M, Schuller B. Opensmile: the munich versatile and fast
open-source audio feature extractor. In: Proceedings of the 18th ACM
international conference on Multimedia; 2010. p. 1459–1462.
42. Erel Y, Shannon KA, Chu J, Scott K, Struhl MK, Cao P, et al. iCatcher+:
Robust and Automated Annotation of Infants’ and Young Children’s Gaze
Behavior From Videos Collected in Laboratory, Field, and Online Studies.
Advances in Methods and Practices in Psychological Science. 2023;6.
doi:10.1177/25152459221147250.
43. Fraile M, Lindblad J, Fawcett C, Sladoje N, Castellano G. Automatic analysis of
infant engagement during play: An end-to-end learning and Explainable AI pilot
experiment. In: Companion Publication of the 2021 International Conference on
Multimodal Interaction; 2021. p. 403–407.
July 8, 2024 29/3044. Coie JD, Dodge KA, Kupersmidt JB. Peer group behavior and social status. In:
Peer rejection in childhood. Cambridge University Press; 1990. p. 17–59.
45. Fraile M, Fawcett C, Lindblad J, Sladoje N, Castellano G. End-to-End Learning
and Analysis of Infant Engagement During Guided Play: Prediction and
Explainability. In: Proceedings of the 2022 International Conference on
Multimodal Interaction; 2022. p. 444–454.
46. Cohen J. Statistical power analysis for the behavioral sciences. Routledge; 1988.
47. Sawilowsky SS. New effect size rules of thumb. Journal of modern applied
statistical methods. 2009;8:597–599.
48. Aron A, Aron EN, Smollan D. Inclusion of other in the self scale and the
structure of interpersonal closeness. Journal of personality and social psychology.
1992;63(4):596.
49. Westlund JMK, Park HW, Williams R, Breazeal C. Measuring young children’s
long-term relationships with social robots. In: Proceedings of the 17th ACM
conference on interaction design and children; 2018. p. 207–218.
50. Russell JA. A circumplex model of affect. Journal of personality and social
psychology. 1980;39(6):1161.
51. Bradley MM, Lang PJ. Measuring emotion: the self-assessment manikin and the
semantic differential. Journal of behavior therapy and experimental psychiatry.
1994;25(1):49–59.
52. Vallat R. Pingouin: statistics in Python. J Open Source Softw. 2018;3(31):1026.
53. Srivastava S, Lakshminarayan SAS, Hinduja S, Jannat SR, Elhamdadi H,
Canavan S. Recognizing emotion in the wild using multimodal data. In:
Proceedings of the 2020 International Conference on Multimodal Interaction;
2020. p. 849–857.
54. Ekman P, Friesen WV. Facial action coding system: Investigator’s guide.
Consulting Psychologists Press; 1978.
55. Paetzel M, Varni G, Hupont I, Chetouani M, Peters C, Castellano G.
Investigating the influence of embodiment on facial mimicry in HRI using
computer vision-based measures. In: 2017 26th IEEE International Symposium
on Robot and Human Interactive Communication (RO-MAN). IEEE; 2017. p.
579–586.
56. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, et al.
Scikit-learn: Machine learning in Python. the Journal of machine Learning
research. 2011;12:2825–2830.
57. Ligthart M, Peters R. The challenges of evaluating child-robot interaction with
questionnaires. What Could Go Wrong. 2018;.
July 8, 2024 30/30