Towards Context-aware Support for Color Vision
Deficiency: An Approach Integrating LLM and AR
Shogo Morita1, Yan Zhang2, Takuto Yamauchi3, Sinan Chen4, Jialong Li3, Kenji Tei1
1 School of Computing, Tokyo Institute of Technology, Tokyo, Japan
2International Research Center for Neurointelligence, the University of Tokyo, Tokyo, Japan
3Department of Computer Science and Engineering, Waseda University, Tokyo, Japan
4Center of Mathematical and Data Sciences, Kobe University, Kobe, Japan
Correspondence: lijialong@fuji.waseda.jp
Abstract—People with color vision deficiency often face chal- In this paper, we introduce a general context-aware sup-
lenges in distinguishing colors such as red and green, which can port approach for individuals with color vision deficiency.
complicate daily tasks and require the use of assistive tools or
Specifically, as a user interface, we integrate augmented re-
environmental adjustments. Current support tools mainly focus
ality (AR) to capture real-world data and display supportive
on presentation-based aids, like the color vision modes found
in iPhone accessibility settings. However, offering context-aware content. Internally, we develop a set of prompt strategies to
support,likeindicatingthedonenessofmeat,remainsachallenge leverage the expansive understanding and potent reasoning
since task-specific solutions are not cost-effective for all possible abilities of multi-modal large language models (LLMs) to
scenarios.Toaddressthis,ourpaperproposesanapplicationthat
generate appropriate content. This approach aims to provide
providescontextualandautonomousassistance.Thisapplication
more autonomous and context-sensitive support for everyday is mainly composed of: (i) an augmented reality interface that
efficientlycapturescontext;and(ii)amulti-modallargelanguage challenges faced by those with color vision deficiencies.
model-based reasoner that serves to cognitize the context and
II. PROPOSEDAPPLICATION
thenreasonabouttheappropriatesupportcontents.Preliminary
user experiments with two color vision deficient users across
five different scenarios have demonstrated the effectiveness and pyhsical
universality of our application. environment ②context AR ③request Multi
application -modal
Index Terms—Color Vision Deficiency, Augmented Reality, user
Large Language Model, Color-blind, visual impairment ①request ④ LLM
⑤display supportive
content
I. INTRODUCTION
Fig. 1: Overview of the Application Interaction.
Individuals with color vision deficiency often struggle to
distinguish certain colors, particularly red and green. This Application Overview. Figure 1 illustrates the workflow
inability creates various challenges in their daily lives, such of our application. Initially, the AR application receives a
as identifying traffic lights, choosing clothing, or engaging in support request from the user. Subsequently, the application
tasks that require color differentiation. As a result, they may utilizes a camera to collect environmental data. This data is
need to rely on special assistive tools or adjust environmental then integrated with other relevant information to create a
settings to reduce obstacles in everyday life. prompt for invoking a multi-modal LLM. Following this, the
Currently,toolsforsuchgroupstendtofocusonadjustments LLM generates supportive content, and this content is finally
in display or presentation, like the VoiceOver function and displayed to the user through the AR application.
the option of high contrast settings found in iPhone’s acces- AR Interface. The AR interface serves two primary func-
sibility. Additionally, many studies have explored enhancing tions: input and output. For input, it utilizes a camera to
readabilitybyrecoloring,forinstance,[1]utilizesTransformer capture environmental information around the user, supple-
in image recoloring for color vision deficiency compensation. mented by a microphone or a button to record user requests.
However, a more complex challenge is supporting individ- We categorize user requests into two types. The first involves
uals with color vision deficiency from a context perspective direct input from the user specifying the needed support, such
rather than just a display perspective [2]. For example, indi- assaying”Pleasetellmethecolorofthetrafficlight”through
viduals with visual impairments may not be able to determine the microphone. The second type is a simplified interaction
the doneness of meat on a grill, requiring assistance from where the user only presses a button indicating the need for
shop attendants or companions to inform them “if the meat help, with the specifics inferred by the LLMs. This simplified
is cooked” [3]. While specialized supporting systems have approach aims to reduce the user’s operational burden and
been developed for some frequent scenarios, such as cloth enhance privacy. For output, the system displays supportive
coordination [4], it is not economically feasible to develop content on the AR device screen, presented in a textbox
dedicated systems for every possible scenario. at the bottom of the user’s field of view. We chose AR
4202
luJ
5
]VC.sc[
1v26340.7042:viXraover mobile devices like smartphones because AR offers a Experiment results. Firstly, for the objective accuracy, in
more intuitive and efficient interaction experience, capturing ten tests (five scenarios times two different environments),
directlythearea theuserisfocusingon andoverlayingdigital the multi-modal LLMs were able to correctly recognize the
informationinrealtime.Whileglasses-typeARdeviceswould environment context and user intentions, and provide accurate
be ideal, we used the Meta Quest 3 for its cost-effectiveness supportive contents. Secondly, in actual user experience and
and comprehensive development kit. userinterviews,usersacknowledgedthepracticaleffectiveness
LLM-based support contents generation. We employ of our application (8.5 out of 10 in average). This includes
multi-modalLLMstogeneratesupportivecontent.Toenhance (i) the convenience provided by the AR interface, which is
the effectiveness and accurancy of content generation, we more straightforward than having to align and photograph a
carefully designed prompts. Initially, as context information, targetwithasmartphone;and(ii)theaccuracyoftheLLMsin
we provide basic information about the user and their spe- inferringuserintentions,whichsignificantlyreducestheuser’s
cific characteristics, such as reduced sensitivity to red light effort and increases response efficiency as users do not need
in individuals with Protanomaly. Secondly, we implement a to provide specific vocal instructions.
Chain-of-Thought (CoT) prompting approach, including the Discussion and Limitations. We believe the preliminary
following four steps: (i) analyzing the current environmental user experiments have already demonstrated the effectiveness
situation;(ii)determiningtheuser’sintentandthetypeofhelp and usability of our application. However, several limitations
required;(iii)generatingconcisesupportivecontent,limitedto were identified. Firstly, participants noted that the disadvan-
10 words to ensure readability; and (iv) identifying key terms tages for color vision deficient individuals are more pro-
foremphasispresentation(e.g.,boldtext)withinthesupportive nouncedintheworkplacethanineverydaylife,suggestingthat
content. The first three steps of the CoT are modeled after further evaluation of the application’s usefulness in working
the human cognitive decision-making process, which includes settings is necessary. Secondly, the risk associated with result
perception of the environment, high-level decision-making, accuracy was highlighted, as LLMs cannot guarantee the
and low-level realization. accuracyoftheiroutputs.However,participantsalsoindicated
that they often integrate information from other sources (such
III. PRELIMINARYEVALUATION
as observing the behavior of others at a traffic light) to make
judgments. Thus, the supportive context generated by our
application, serving as one of the reference sources, is still
beneficial. Thirdly, in complex scenarios, such as identifying
the doneness of multiple pieces of meat on a baking tray,
there is a need for a more precise and intuitive explanation.
Thissuggestsfurtherenhancementofthecontentvisualization,
such as marking numbers on each piece of meat.
IV. CONCLUSIONANDFUTUREWORK
This paper introduces a general context-aware support ap-
proachforindividualswithcolorvisiondeficiency,employing
AR and multi-modal LLMs. Preliminary user experiments
involvingtwocolorvisiondeficientusersacrossfiveeveryday
Fig. 2: Scene of Usage and Screen in Device (bottom left). scenariosdemonstratetheuniversalityandeffectivenessofour
approach. Future research will focus on two main areas. First,
Experiment setting. To evaluate the effectiveness of our weplantoimproveresultaccuracythroughappropriateprompt
application, we conducted an experiment with two actual engineering. Second, we aim to conduct further experiments
color vision deficient participants from a university campus. in a broader range of everyday and workplace scenarios with
Informed consent forms were distributed before participation, a larger number of participants to more comprehensively
and data handling complied with the ethics board recommen- validate our approach.
dations of the university. Specifically, we had the participant
REFERENCES
utilize our application, followed by an in-depth interview to
[1] L.Chen,Z.Zhu,W.Huang,K.Go,X.Chen,andX.Mao,“Imagerecol-
gather qualitative and practical feedback. To validate the uni-
oringforcolorvisiondeficiencycompensationusingswintransformer,”
versality of our application, affirming its applicability across NeuralComput.Appl.,2024.
various everyday scenarios, we employed five distinct experi- [2] J. Li, M. Zhang, N. Li, D. Weyns, Z. Jin, and K. Tei, “Exploring the
potential of large language models in self-adaptive systems,” SEAMS,
mental scenarios. These scenarios were: (i) identifying traffic
2024.
lights; (ii) judging the doneness of meat; (iii) selecting ripe [3] N. Kishida, “Today, whose perspective? - color blindness and me,”
fruits; (iv) coordinating clothing; and (v) reading color-coded https://heart-design.jp/column/%E4%BB%8A%E6%97%A5%E3%81%
AF%E8%AA%B0%E7%9B%AE%E7%B7%9A%EF%BC%9F/, 2024,
signsinpublictransportation.Foreachscenario,weconducted
accessed:2024-06-20.
testsintwodifferentenvironments.Formulti-modalLLM,we [4] Y.TianandS.Yuan,“Clothesmatchingforblindandcolorblindpeople,”
selected the most advanced and popular GPT4-o. ComputersHelpingPeoplewithSpecialNeeds,2010.