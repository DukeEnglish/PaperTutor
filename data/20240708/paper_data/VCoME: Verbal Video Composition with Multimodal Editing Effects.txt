VCoME: Verbal Video Composition with Multimodal
Editing Effects
WeiboGong,XiaojieJin†,XinLi,DongliangHe,XinglongWu
ByteDanceInc.
Abstract
Verbalvideos,featuringvoice-oversortextoverlays,providevaluablecontentbut
presentsignificantchallengesincomposition,especiallywhenincorporatingediting
effectstoenhanceclarityandvisualappeal. Inthispaper,weintroducethenovel
taskofverbalvideocompositionwitheditingeffects. Thistaskaimstogenerate
coherentandvisuallyappealingverbalvideosbyintegratingmultimodalediting
effectsacrosstextual,visual,andaudiocategories. Toachievethis,wecuratea
large-scaledatasetofvideoeffectscompositionsfrompubliclyavailablesources.
Wethenformulatethistaskasagenerativeproblem,involvingtheidentification
ofappropriatepositionsintheverbalcontentandtherecommendationofediting
effectsforthesepositions. Toaddressthistask,weproposeVCoME,ageneral
frameworkthatemploysalargemultimodalmodeltogenerateeditingeffectsfor
videocomposition. Specifically,VCoMEtakesinthemultimodalvideocontext
and autoregressively outputs where to apply effects within the verbal content
andwhicheffectsaremostappropriateforeachposition. VCoMEalsosupports
prompt-based control of composition density and style, providing substantial
flexibilityfordiverseapplications. Throughextensivequantitativeandqualitative
evaluations,weclearlydemonstratetheeffectivenessofVCoME.Acomprehensive
userstudyshowsthatourmethodproducesvideosofprofessionalqualitywhile
being 85× more efficient than professional editors. To foster further research
in this emerging direction, we will release all codes, models, and datasets at
https://github.com/LetsGoFir/VCoME.
1 Introduction
Video platforms such as YouTube and Reels attract billions of users, establishing videos as the
predominantmediumforinformationdissemination. Amongthem,verbalvideosareparticularly
valuableduetotheirrichcontentconveyedthroughvoice-overs(asshownin[1])ortextoverlays.
However,composingverbalvideosposesasignificantchallengeforindividualswithoutprofessional
expertise. Theprocessinvolvesusingcomplexeditingeffectstohighlightkeyinformation,enhance
coherence,andincreasevisualappealbasedonthemultimodalcontentwithinthevideos.Forinstance,
keywordsarehighlightedwithtexteffects,emotionsareintensifiedwithsoundeffects,andvideo
effectsareusedtoincreaseattractiveness. Theseeffectsencompassnumeroustypesandintricate
combinations,makingthecompositionademandingtaskthatrequiresahighlevelofeditingskill
andprecision,whichisdifficultfornon-professionalstoachieve.
Thispaperintroducesthenoveltaskofverbalvideocompositionwitheditingeffectsandpresents
ageneralframework,VCoME,toaddressit. Thetaskisdefinedasfollows: givenaverbalvideo
containing voice-over or text, the objective is to generate a composited video that incorporates
multimodaleditingeffectsacrosstexts,audioandvisualelements. Thepositionandstyleofthese
†Correspondingauthor<jinxiaojie@bytedance.com>.
Preprint.Underreview.
4202
luJ
5
]VC.sc[
1v79640.7042:viXraeffectsshouldalignwiththevideo’snarrativetoachievecoherence. Bydoingso,thecomposition
canhighlightkeycontentinoriginalvideos,suchasproductadvantagesorkeyopinions,enhancing
bothclarityandaestheticattractiveness.
Thistaskpresentsseveralchallenges. First,therearenopubliclyavailabledatasetsfortrainingand
evaluation. It requires substantial effort to collect and annotate a large-scale video dataset. The
diversestylesandcontentofverbalvideosfurthercomplicatethisprocess. Beyondthedatasetissue,
theinherentcomplexityofthetaskalsoposessignificantchallenges. High-qualityvideocomposition
necessitatestheseamlessintegrationandharmonybetweeneffectsandtheassociatedmultimodal
contentinvideos,aswellascoherenceamongtheeffectsthemselves. Giventhevastnumberofeffect
typesandcombinations,themodelmustefficientlydeterminetheappropriateeffects. Asthefirst
explorationofthistask,weneedtoaddressthesechallengestodelivertheoptimalsolution.
Webeginbyconstructingaverbalvideowitheffectsdatasetsourcedfrompubliclyavailablevideos
onJianying[2]. Usingvideoplay-relatedmetricssuchasviewcountsandlikecounts,wecuratea
selectionofhigh-qualityvideoswithappliedmultimodaleditingeffects. Subsequently,weannotate
thepositionsandtypesofeffectsfromthesevideostoformulatethegroundtruthforthistask,on
whichtrainingandevaluationareperformed.
Totacklethistask,weexplorebothclassification-basedandgenerativemethods. Initially,weadopta
classification-basedapproachusingtransformerstodirectlyoutputtheIDsofrecommendedeffects.
Althoughstraightforward,thisapproachencountersconvergenceissues,possiblyduetoitsincapacity
to model the task’s high complexity. Motivated by the advanced reasoning and understanding
capabilitiesoflargemultimodalmodels(LMMs),wetransitiontoagenerativeapproach. Wefirst
transformthetask’soutputsintoasequentialcompositiontarget,enablingseamlessintegrationwith
popularLMMsandend-to-endtraining. Next,weintroduceasegment-levelencodingmethodto
extract embeddings for each input modality from video segments separated by sentences. This
technique is crucial for the efficient fusion and understanding of correlated multimodal content.
Theresultingframework,VCoME,demonstratesstrongcapabilitiestogeneratehigh-qualityeffect
compositionsfromdiversemultimodalcontent(visual,audio,andtext)ininputvideos.
Insummary,thispaperpresentsthreekeycontributions:
• Weintroduceanoveltaskofverbalvideocompositionwitheditingeffects,whichhaswide
applicationsinvideoediting. Wecuratealarge-scaledatasetofvideoeffectscompositionto
facilitatefutureresearchonthistask.
• Weformulatethistaskasagenerativelearningproblemandproposeageneralmultimodal
framework VCoME that effectively models the relationship between multimodal inputs
(visual,audio,andtext)andvariouseffectselements.
• Throughbothquantitativeandqualitativeanalyses,wedemonstratethatVCoMEeffectively
acquiresprofessionaleditingknowledge,producinghigh-qualityvideocompositionresults.
Acomprehensiveuserstudyfurtherverifiesitseffectivenessandefficiency.
2 Relatedwork
Video Editing Video editing presents significant challenges for two primary reasons. First, it
requires a deep understanding and manipulation of diverse elements across multiple modalities.
Second,editingishighlysubjective,heavilyinfluencedbythecreator’saestheticpreferences. We
categorizevideoeditingtasksbasedontheirobjectives,asillustratedinFigure1. Visualgeneration
models,includingstyletransferring[3],text-to-image[4,5],text-to-video[6](mentionedby[7]),
text-to-4D [8], and audio-to-video [9], are employed to generate visual materials. Furthermore,
storytellingiscrucialinvideoediting,whichinvolveseithernarrativeconstruction[10,11,12,13]or
timelinestructuring[14,15]. Additionally,avatartechnologyisvaluableforverbalvideos,allowing
for control over head shapes [16] and human actions [17], and can be driven by voice [18] or
text[19]. Audioalsoplaysavitalroleinvideoediting, inspiringtechnologiessuchasAImusic
composition[20],voicecloning[21],speechprocessing[22,23],andmusic-drivenvideomontage
[24]. Integratingthesemultimodalelementsintoacohesivevideopresentsasignificantchallenge. In
thiswork,wefocusonverbalvideocompositionwitheffectsacrossmultiplemodalities.
2Music Composition[20] Voice Cloning[21] AutoTransition[25] Text-to-Video[6] Audio-to-Video[9]
VCoME
Text-to-Speech [22] Speech Denoising[23] Effects Representation[26] Text-to-4D[8] Text-to-Image[4]
Head Modeling[16] Text-to-Motion[19] Gesture Modeling[17] Video-Story[10]TimelineModeling[14]TrailerCreation[15]
Video Editor
ATT ee xx tt
u
&
a
lV Eo fic fee
c
T tr sack Sound Avatar Effects Text Visual
Visual Effects
Sound Effects
Figure1:Tasksinvideoediting,groupedbytargetmodality.Sometasksgeneratevisuals,whileothersprovide
audio,text,andadditionalelements. Ourmethodgeneratescompositionswithmultimodaleffectsforverbal
videos.Comprehensivevideoeditingshouldconsiderthecompositionofalltheseelements.
EffectComposition Editingeffectsareessentialforvideoproductionandcomeinvariouscate-
gories. Visualeffectscanenhancethevisualexperienceandmaintainviewerinterest. Forexample,
AutoTransition[25]recommendstransitioneffectsbetweenvideoclipsbasedonvideoandaudioin-
formation,seamlesslyconnectingneighboringshotsandscenes. Edit3k[26]learnstherepresentation
ofmorevisualeffects. Textualeffectscansignificantlyimproveinformationdeliveryandthevisual
appealofavideo. MovieStudio[27]integratesbothvisualandtextualeffects,enablingittogenerate
music videos with a strong sense of rhythm like [28]. Additionally, audio effects can strengthen
emotionalexpressionandatmosphere,providingaudiocuesandambientsounds. Consideringall
theseeffects,VCoMEintroducescomprehensivevideoeffectscompositionintotheresearchdomain
forthefirsttime,whichissignificantforbothvideoeditingapplicationsandacademicresearch.
LargeMultimodalModel(LMM) Multimodalpre-trainingmodels[29,30,31,32]demonstrate
remarkablecapabilitiesinunderstandingandgenerationacrosstaskssuchasvideounderstanding,
videocaptioning,andvideo-textretrieval. Videocompositionrequiresthecomprehensionofmul-
timodalinputsanddesignknowledge,motivatingustoutilizemultimodallargemodels. Weadopt
largemultimodalmodelslikeLLaVA[33]asourbaseframework. WeuseViT[34]andImageBind
[35] to extract features from video and audio, respectively. These embeddings are then fed into
large language models such as Mistral [36] or LLaMA [37]. We treat the input text as temporal
dependenciesandsequentiallypredictthetriggerpositionsandvariouseditingeffects,followingthe
Chain-of-Thought[38]methodology.
3 TaskDefinition
Theobjectiveofverbalvideocompositionwitheffectsistodetermineboththepositionsandtypes
of editing effects used in a verbal video. This aligns with the workflow of professional human
creators with track editing tools, such as Adobe After Effects (AE). Therefore, the verbal video
compositionprocesscanbedividedintotwosteps: positioningandrecommendation. Theformer
involves identifying key content within the video to enhance and trigger effects, while the latter
entailsgeneratingappropriateeffectnamesbasedonthepositionandinputcontext.
Triggerpositioning Thefirststepistoidentifyappropriatepositionsintheverbalvideotoapply(or
trigger)editingeffects,withtheaimofhighlightingthecorrespondingverbalcontent. Thesetrigger
31
Composition Result
Have you used
g bl ea fs os re es ? wipes 1 1 Positions Effects
“glasses wipes” text effect yellow blue
<whole sentence>sound effect biu
<whole sentence>image sticker Notice
S
S S Le eg vm ee lnt- Positions Effects Render
“Easy” text effect yellow blue
Easy to use and Encoding <whole sentence>image sticker Great
inexpensive
text effect image sticker
Effect Pool
sound effect text animation
Prompt
S
Please edit my
v mid ae ko e s i tt o … image sticker. . . text effect
Figure2:Ourframework.Theinputdatacomprisesthreemodalitiesandissegmentedbysentences.Afterbeing
processedbyencodersandprojectors,themultimodaltokensareuniformlyhandledbytheLMM.Subsequently,
thecompositionisgenerated,detailingthearrangementofvariouseffectsontheoriginalvideo.Thefinalvideo
isrenderedaccordingtothiscomposition,utilizingtheeffectpool.
positionsaredeterminedbyoneorseveralconsecutivewordswithintheverbalcontent. Theapplied
effectsstartatthetimestampofthefirstwordandendwhenthelastwordfinishes. Anexampleis
showninFigure2,whereweapplyatexteffecttohighlight“glasswipes”alongwithasoundeffect
(named“biu”sincesoundslikeit)andaimagesticker(named“Notice”andlookslikeahorn)to
allwordsindicatedby“<wholesentence>”insegment1,enhancingthepresentationofobjectsin
thevideo. Thisprocessimprovesthevideo’sclarityandstructure,makingitbettercommunicatethe
emphasizedcontentandemotionstoviewers.
Effectsrecommendation Thesecondstepinvolvesdeterminingwhicheditingeffectstoapplyto
thevideo.Thisprocessrequiresensuringstylisticconsistencybetweentheeffectsandtheirintegration
intotheoverallcomposition,whilealsoconsideringtherichnessoftheeffects. Additionally,effects
havecontrollableparameterssuchasintensity,transformation,andtexture. Tosimplifytheprocess,
onlyeffecttypesneedtobepredictedwhilesettingtherestattributesofeffectsatdefaultvalues. We
carefullyconstructtheeffectspool,takingintoaccountallthreemodalities,asillustratedinFigure5.
Thecategoriesinclude“textanimation”,“texteffect”and“texttemplate”fortextualeffects,“image
sticker”forvisualeffects,and“soundeffect”foraudioeffects. Adetailedintroductiontooureffects
canbefoundinAppendixA.1.
Notations Tocarryoutabovetwosteps,wedesigntheinputandoutputformatofVCoMEasin
Figure2. Specifically,weintroducesegment-levelencoding,whichcutstheinputvideointoaseries
ofsegmentsXcontent ={x ,...,x }basedonthesentencebreaksinverbalcontent,whereS isthe
1 S
numberofvideosegments. Thenforthei-thsegment,wehavex ={s ,v ,a }asinput,inwhich
i i i i
s ,v ,a representsthesentence,videoclipandaudioclip,respectively. Thisdivisionisperformed
i i i
withinasinglevideo. Whenmultiplevideosareprovided,wefirstcombinethemintoasinglevideo
andthenapplythesameprocedure.
Fortheoutput,wearrangetherecommendedeffectsaccordingtothesegmentsindices. Thelearning
objectiveforthei-thsegmentincludesmultipleeditingeffects,denotedas
(cid:0) (cid:1)
y = i,(l ,e ),(l ,e ),... .
i 1 1 2 2
Besidestheindexi,eachelementiny isrepresentedintheformatof(l,e). eisthenameofeffect
i
and l is its trigger position. After processing Xcontent through multimodal encoders, the context
informationismappedtoaunifiedjointspace. Subsequently,theLMMprocessesthisinformation
4
…
❄!!
❄!"
❄!#
❄ "#$%!
!
"#$%"
!"#$%#
!
LMM
. . .
tceffe
dnuos
…Distribution of Data Length Frequency for Editing of Data Frequency of Sticker
8000
7000
4000 103
6000
3000
5000 102
4000 2000
3000 101
2000 1000
1000 100
0 0
21 23 Segmen2 t5 Number27 29 0 20
Trigger
R4 a0
tio(%)
o6
f
0 Positions80 100 21 23
Number
2 o5
f Uses
27 29
(a)Distributionofdatalength. (b)Distributionoftriggerpositions. (c)Distributionofstickersused.
Figure3:Distributionofourdataset.
andautoregressivelygeneratesthecompositiontext,whichistheconcatenationofy intheform
i
Y =(y ,...,y ). Thedisplaydurationofeacheffectalignswithitscorrespondingtriggerposition
1 S
withinthesegment.
4 VideoCompositionDataset
4.1 DataCollection
A vast number of high-quality publicly available videos can be found on popular video sharing
platforms. Thesevideosarecreatedbyalargeamountofcreators,displayingawiderangeofdesign
stylesandnarrativecontent. Well-producedverbalvideosoftenadoptstructuredstorytellingand
incorporatevariouseditingeffects,suchasimagestickers,text/videoeffects,andsoundeffects. To
construct our dataset, we collect videos that contain both verbal content and editing effects, and
applyannotations. Theparametersofeffectsarediverseandintricate. Forexample,textanimations
haveintensityandspeedparameters,whilesoundeffectshavevolumeanddurationparameters. To
focusonthecorepositioningandrecommendationofeffects,wedonotinferthespecificparameter
values. Instead, we directly use the default parameters of effects for rendering purposes. This
simplificationfacilitatesthelearningofvideocompositionwhilepreservingitsintegritybyretaining
thekeyattributesoftheeffects.
4.2 DataFiltering
Thedistributionofvideolengthexhibitssignificantvariation,asdepictedinFigure3a. Somesamples
consistofonlyafewsentences,whilesomeexceedthemaximumlengthofourmodel,extendingto
overahundredsentences. Toaddressthisissue,weemploytwostrategies: filteringoutbothdata
withfewerthanthreesentencesandtruncatingexcessivelylongtexts. Thesecondstrategyallows
us to utilize a sufficient amount of data within the model’s length limitations and acts as a form
ofdataaugmentationsincethetruncateddatadoesnotbeginwiththefirstsegment. Additionally,
thefrequencyoftriggerpositions(Figure3b)andtheusagedistributionofeffects,suchasimage
stickers(Figure3c),areimbalanced. Thisphenomenonincreasesthedifficultyofthistask. Finally,
wegatheradatasetcomprisingapproximately53,000instances,whichwesubsequentlydivideinto
51000samplesforthetrainingsetand2,100samplesforthevalidationset. Eachdatasampleconsists
ofmultiplesegments.
5 VerbalVideoComposition
5.1 ContextModeling
Weformulatethetaskasamultimodalgenerationproblem. Theprimarychallengeweaddressis
enablingtheLMMtocomprehendtherelationshipbetweenmultimodalinputsandthecomposition
ofeffects. Giventhatvisualandtonalchangeswithineachsentenceareminimal,wedesignsegment-
levelencodingtoachieveefficientunderstandingandmultimodalfusionasinFigure2. Thisprocess
involvessegmentingthevideoatthesentencelevel,followedbyencodinginthreedifferentmodalities.
5
tesataD
ni
ytitnauQ
tesataD
ni
ytitnauQ
tesataD
ni
ytitnauQToprocessvisualandtextinput,weadoptasimilarapproachtoLLaVA[33]. Wepre-trainthemodel
onimage-textpairdata,developingaprojectorfromthevisualencodertothelargemultimodalmodel.
Forfinetuningonthevideocompositiondataset,wesimplifytheprocessbyextractingasingleframe
foreachsegmentx ,astheframeswithinasinglesegmentaresimilar,andintegratingitintothe
i
LMMforcomprehension. Specifically,weobtainvisualembeddingsusingE ,thepre-trainedVision
V
TransformerinCLIP[39]andmapthemtothelatentspaceofthelargemultimodalmodelusingan
alignedMLPProj . Becausetheinputcanbepotentiallylengthy,e.g.,morethan100segments,it
V
isnotfeasibletoretainallViT[34]patchesforeachframe. Therefore,wepreserveonlythe[CLS]
tokenforeachframe. Similarly,weprocessaudioinputwiththeencoder-projectionstructure. Inour
experiments,weextractaudiofeaturesusingImageBind[35]asE foreachsegment.
A
It is noteworthy that encoders for both visual and audio inputs can be trivially replaced by other
widelyusedmodalityencoders. Weprojectthevisualandaudioinputsintothesamedimension
throughindependentlineartransformationsProj andProj . Theembeddingsofsentences,video
V A
clips,andaudioclipscanberepresentedasfollows:
f =Emb(tokenizer(s )), f =Proj (E (v ), f =Proj (E (a )).
s i v V V i a A A i
s ,v anda representthesentence,videoclip,andaudioclip,respectively. f ,f ,andf represent
i i i s v a
theircorrespondingembeddings. ThetermsEmbandtokenizercorrespondtotheProj andE in
T T
Figure2respectively. Subsequently,weinputtheseembeddingsintotheLMM,alongwithlearnable
positionalembeddings,enablingthetransformertoeffectivelycapturesegment-levelinformation.
Themodelthensequentiallygeneratesthecompositionresults.
5.2 CompositionLearning
AsintroducedinSection5.1,themultimodalembeddingsareextractedusingtheirrespectiveencoders
and serve as contextual information. During the composition process, we use learnable linear
transformationstomapthevideoandaudioembeddingsintothelargemultimodalmodel,facilitating
thepositioningandrecommendationofeffects.
Wedenote
Eelem ={etext-animation,etext-effect,etext-template,esound-effect,eimage-sticker,...,
1 1 1 1 1
etext-animation,etext-effect,etext-template,esound-effect,eimage-sticker},
Nt-ani Nt-eff Nt-tem Ns-eff Ni-stk
as the set of editing effects where N , N , N , N , N are numbers of each cat-
t-ani t-eff t-tem s-eff i-stk
egory of effects. N = 10,000 and others are set as 1,000 in experiments. The input is
i-stk
X = {Xcontent,Xprompt},whereXcontent representsthemultimodalinputinasample,andXprompt
representstheprompt,whichisdiscussedinSection5.3. Foreachsegmentx inXcontent,wegenerate
i
thecorrespondingcompositionresulty usingthemultimodallargemodelp (y |X,y ). Thetarget
i θ i j<i
is formed as Y = (y ,...,y ). A detailed introduction and design concepts for our format are
1 S
presentedinAppendixA.2.
5.3 PromptDesign
Inspiredby[40],wedesignpromptstocatertodiverseuserintents. Forinstance,usersmaycontrol
thefrequencyofeffectoccurrenceorchoosespecificcategoriesofeffects. Wedesignseveralprompts
toaddressthisissue. Forinstance,duringtraining,weusepromptssuchas“Pleaseeditavideowith
asuitablefrequencyoftriggerpositionsandincludeimagestickers”and“Pleaseeditavideowith
a50%frequencyoftriggerpositionsandmakeuseofanimatedtext.” Duringinference,prompts
like“Pleaseeditavideowitha70%frequencyoftriggerpositions,simultaneouslyincorporating
imagestickersandanimatedtext”canbeusedtogeneratethedesiredresult. Thisapproachleverages
thecapabilitiesoflargemultimodalmodelstogeneratemorediversedata,effectivelyaddressingthe
long-tailphenomenonandfacilitatingintelligentinteractionforusers. AsshowninFigure6d,weget
moreimagestickerswhenpromptingwith“moreimagestickers”comparedto6bwithoutprompts.
By considering all input X = {Xcontent,Xprompt}, we can offer users more diverse and enriched
compositionsthroughuser-friendlyinteractions.
65.4 TrainingandEvaluation
Following LLaVA, we adopt a two-stage training procedure: pre-training and finetuning. For
evaluation,weestablishbothobjectiveandsubjectivemetricsspecifictothistask. Thedetailsofour
trainingandevaluationprocessareoutlinedbelow.
Training Weemploythecross-entropylossasourmodel’slearningobjective:
S
L(X)=(cid:89)
p
(cid:0)
y |y
,Xcontent,Xprompt(cid:1)
, (1)
θ i j<i
i=1
whereθ representstheparametersofourmodelandS isthenumberofsegments. Byoptimizing
Equation1,themodellearnstoincorporatethemultimodalcontext,prompt,andpreviouscompo-
sitionresults. Thisoptimizationencouragestheidentificationofsuitabletriggerpositionsandthe
recommendationandcombinationofappropriateeffectsduringthegenerationprocess.
Evaluation Wedefinetheevaluationmetricsacrosstwodimensions: objectiveandsubjective. The
objective metrics primarily focus on the accuracy of trigger positions and corresponding effects.
We use the Dice similarity [41] to calculate the average accuracy of trigger positions, as shown
inEquation2. Onceatriggerwordispredictedcorrectly,wecalculatetheprecisionoftheeffect
categoryusingEquation3. Inadditiontowords,effectsliketextanimations,soundeffects,orimage
stickersareappliedtoallwordsinonesentence. Thus,forthesesentence-leveleffects,wecalculate
therecallofthecorrespondingelementswithinthespecificsegment,asshowninEquation4.
Mword_accuracy = 1 (cid:88) Dice(cid:0) w ,w′(cid:1) (2)
X |Y ∪Y′| i i
w w i∈|Yw∪Yw′|
(cid:40)
Melem@word =
1 (cid:88) 1, ifDice(w i,w i′)≥0.5andEq(w i,w i′)
(3)
X |Y | 0, otherwise
w
i∈|Yw|
Melem@sentence =
1 (cid:88) |h k∩h′ k|
. (4)
X |Y | |h ∪h′|
h k∈|Yh| k k
Y = {Y ,Y }isthegroundtruth,whichincludeselementswithtriggerpositionsl andeffectse
w h
asshowninSection3. Y compriseselementscorrespondingtowords,denotedbyw ,whileY
w i h
contains elements for the entire sentence, denoted by h . Y′ represents the results generated by
k
themodel,withpredictedelementsdenotedasw′ andh′,respectively. ThemetricMword_accuracy
i k X
representsthewordaccuracyofdatasampleX,where“Dice()”calculatestheDicesimilarityoflfor
thegroundtruthw andpredictedelementsw′. WhencalculatingMelem@wordinEquation3forw′
i i i
andw ,wesetthevalueto1ifthematchedwordachievesaDicesimilaritygreaterthan0.5andthe
i
effectscategoryiscorrect,denotedby“Eq(w ,w′)”;otherwise,itissetto0. Fortheconvenienceof
i i
comparingcompetingmethods,weaggregatethesethreeindicatorsintoanoverallscore.
Subjectiveevaluationinvolvesusing“Win/Tie/Loss”and“MeanOpinionScore”toassessvarious
qualitativeaspects. Inconsultationwithprofessionals,weestablishevaluationdimensionsincluding
videoaestheticsanddecorativerichness,alongwiththeircorrespondingscores. Foreachsamplein
theuserstudy,weconductblindtestingwithvolunteerstoobtainratings.
6 Experiments
6.1 ImplementationDetails
Model details We utilize the open-source bilingual Chinese-LLaMA-2 [42] as the backbone,
incorporatingCLIP-ViT-L/14VisionTransformerforvisualfeatureextraction. Foraudio,weemploy
ImageBind[35]toextractlocalaudiofeatures.Bothmodalitiesarelinearlyprojectedtoasharedlatent
spaceandusedasinputembeddingsfortheLMM.TheLMMsupports2048tokensandcomprises
approximately40stackedtransformerblocks. TosaveGPUmemory,wefreezetheparametersofthe
VisionTransformerandtrainonlytheLMMandtheprojectorsforeachmodality.
7Method WordAccuracy Elem@Word Elem@Sentence OverallScore↑
Baseline 13.78% 0 0 13.78
two-stage
Baseline 32.05% 65.31% 28.5% 125.86
trigger-index
Ours 34.46% 69.08% 30.52% 134.06
Ours 37.88% 68.25% 31.90% 138.03
prompt
Table1:Mainresults.Thebaselineof“twostage”representstheBERTfollowedbyaclassifier,andthebaseline
of“triggerindex”predicttheindiceswithinstringsratherthanwords.SeemoredetailsinSection6.1.
Datapre-processingandtraining Foreachvideoclipinadatasample,weextractthemiddle
frameasthevisualinput. Theseinputframeshavearesolutionof336×336andarerepresentedbya
[CLS]tokenwithadimensionof2048asthevisualfeature. ImageBindgenerates1024-dimensional
featurevectors,sampledevery0.5seconds,butlimitedtoamaximumofthreeembeddingspervideo
clipduetomemoryconstraints. Inallexperiments,weemploytheAdamoptimizerwithaninitial
learningrateof1e-4, decayedusingthecosinestrategy. Duringpre-training, wealignthevisual
modelwiththelargelanguagemodelusingtheLLaVApipelineandaligntheaudiofeaturestothe
jointspaceusingaprojectoronourdataset. Thebatchsizeforeachdeviceis16. Ourexperimentsare
conductedusing8NVIDIAA100GPUs,witheachtrainingsessiontakingapproximately4.5hours.
Metrics In Section 5.2, we define three objective metrics to evaluate the performance of the
compositionmodel: wordaccuracy,effectprecisionforwords,andeffectrecallforsentences. The
overallscoreiscalculatedasthesumoftheseindividualmetrics. Duetospacelimit,thevariationsin
experimentalresultsofourbest-performingmethodareprovidedinAppendixA.3.
Inadditiontotheobjectivemetrics, weconsidermanyotherfactorswhenperformingsubjective
evaluation. Forfurtherdetails,pleaserefertoouruserstudySection6.3.
Baselines ThelistofcomparedbaselinemethodsisshowninTable1. Initially,weimplementeda
two-stagebaseline,whichutilizesBERTfortriggerpositioning,followedbyaclassifierforeffect
recommendation. Despiteitssimplicityandefficiency,thismethoddoesnotconvergeduringthe
classifier’strainingphase. Next,weemployaLargeMultimodalModel(LMM)andsuccessfully
generatereasonableresults. Wethendesignthesecondbaseline,whichinvolveslearningtheindices
withinthesentenceastriggerpositions. Thismeansthatthetriggerpositionliny isrepresented
i
by indices, such as “4-5” for “glass wipes” in Figure 2, rather than the words themselves. This
approachreducestheoutputtokenlengthofthelargemultimodalmodelcomparedtodirectword
prediction. Lastly, we explore the use of prompts to increase the frequency of trigger positions,
reducingemptypredictionsforeachsegmentandapplyingmoreeffects. AsshowninFigure6c,
weobserveahigherratiooftriggerpositionswithpromptingmoreeffects,comparedtoFigure6a
withoutprompts,therebyachievingbetteralignmentwiththegroundtruth.
6.2 AblationStudiesandComparisons
Wefirstcompareinputsacrossdifferentmodalitiestohighlighttheadvantagesofmultimodalcontext.
Then,weperformcomparisonsinvolvingvaryingdatascalesandmodelsizes. Finally,weexplore
differentlearningtargetstovalidatethesuperiorityofourtargetdesign.
Multimodalityandmultilinguality Inthisexperiment,weexaminetheinfluenceofthebasemodel
andmultimodalinputsonthequalityofthegeneratedoutputs,asillustratedinTable2a. Theterm
“bilingual”denotestheuseofabilingualpre-trainedmodel. UtilizingEnglish-onlymodels,such
asLLaVA-vicuna,asthebasemodelresultsinsuboptimalgenerationoutcomes,primarilydueto
theirlimitedabilitytohandlemultiplelanguages. Comparingthesecondandthirdrowsofthetable,
weobservethattheabsenceofthevisualmodalitysignificantlyimpairsthemodel’sperformance,
underscoring the importance of visual content for our task. Finally, the last row demonstrates
thatincorporatingallmodalinputsyieldsthegreatestenhancementinthequalityofthegenerated
compositions.
8Modal Order Indices OverallScore↑
Bilingual OverallScore↑
Visual Audio Order ✓ 116.49
random
✓ ✗ ✗ 111.92 Order string ✓ 122.35
✗ ✗ ✓ 128.83 Order category ✓ 123.21
✓ ✗ ✓ 134.07 Order ✗ 114.66
✓ ✓ ✓ 138.03 Ordertime ✓ 130.73
time
(a) The impact of different modality inputs on the (b)Theimpactoftargetdesignonthegeneratedre-
resultsandtheeffectivenessofbilingualpre-trained sults,includingtheorderofeffectsintargetandthe
models. indicesofeachsegment.
Table2:Ablationsonmodeldesignandtargetdesign.Wefindboththemultimodalinputandorderedtargetsare
importantforlearning.
DataScale ModelScale WordAccuracy Elem@Word Elem@Sentence OverallScore↑
50k 7B 34.78% 68.50% 27.45% 130.73
50k 13B 39.33% 69.62% 27.90% 136.85
10k 7B 24.55% 55.17% 16.93% 96.65
20k 7B 28.66% 61.62% 21.99% 112.27
300k 7B 45.49% 77.70% 42.15% 165.34
Table3:Theablationresultsofdifferentdatascalesandmodelscales.Themodelperformanceincreasesasthe
dataandmodelsizesscaleup.
Scale In the data scale experiment, we constructed training sets by randomly selecting 10,000,
20,000, 50,000, and 300,000 instances, respectively. Audio input is not included for efficiency.
AspresentedinTable3,ourmodelsshowconsistentimprovementinperformancewithincreased
datascale. Thisindicatesthestrongpotentialofourmodeltofurtherenhanceitsperformancewith
additionaldata.
Furthermore,intheablationonmodelsize,weutilizealarger13Blanguagemodel,whichyields
bettergenerationresultscomparedtothe7Bmodel.
Targetdesign Inthisexperiment, weinvestigatetheimpactofdifferentlearningobjectives, as
showninTable2b. Audioisexcludedfromthisexperimenttoexpeditethetrainingprocess. The
resultsinthefirstcolumndemonstratethatdesigningamorestructuredlearningobjectiveforeach
segmentenhancesthequalityofthemodel’sgeneration. Thisapproachalignswiththeworkflowof
humandesigners,whotypicallyapplyeffectstotheirvideosinchronologicalorder,resultingina
morelogicalandorganizedcomposition. Specifically,“random”representsnospecificorderforthe
targetswithineachsegment,while“category”indicatesthattheeffectsareorderedbycategory,such
aspredicting“text-effect”first,followedby“sound-effect.” “String”referstotheorderoftrigger
wordsinthesentence,and“time”pertainstothetimestampsinthecompositioninformation. The
secondcolumnshowsthattheperformanceissignificantlyimpairedwhentheindicesofthesegments
arenotexplicitlyindicatedinXcontentandY.
6.3 UserStudy
Due to theartistic and subjectivenature of benchmarking videocomposition, user studiesare of
utmostimportance. Wecollectvideosfrompublicsourcescoveringvariousdomainssuchasfood,
parenting,realestate,andbroadmarketing. Thesevideosarethenmanuallyeditedbyprofessional
videoeditorstocomparewithourmodel’scompositionresults. Weconductablindtest,enlisting
volunteerstoassessthescoresanddeterminethewin/tie/lossoutcomesforeachresultpair. The
evaluationresultsaredetailedinFigure4. Ourmodeldemonstratesperformancecomparabletothat
ofhumancreators. Webelievethatfurtherenhancingthedataqualitywillleadtoevenmorefavorable
outcomes. Notably,ourmodelsignificantlyimproveseditingefficiency,reducingtheeditingtimefor
verbalvideosbyapproximately85times.
9Mean Opinion Score Efficiency Winning Rate
100% 1000 Win
90% Manually Edit Manually Edit Tie
80% Our Model Our Model Loss, Win, Loss
15%
70% 100 20%
60% 85 ×
50%
40%
30% 10
Tie,
20%
65%
10%
0% 1
Richness Aesthetics Avg Time (s)
(1) (2) (3)
Figure4:Userstudy.(1)Weconductacomparisonusingmeanopinionscores,assessingbothcontentrichness
andvisualaesthetics. (2)Thetimerequiredformanualeditingversusmodelinference. (3)Thecomparative
analysisrevealsthattheacceptabilityofboththemodel-generatedandmanuallyeditedvideosissimilar.
7 Limitations
Ourmethodhasthefollowinglimitations. First,wedonotaccountforallcategoriesofeffects,such
asfacialeffects,videoeffects,andillustrations,necessitatingthecollectionofmoreextensivedatato
encompassthesecategories. Additionally,thegranularityofourvisualencodingmethodisrelatively
coarsetosavememory,extractingonlyoneframepersegment. Thisapproachmayresultintheloss
ofvisualdetails. Efficientvisualrepresentationssuchasdynamicimage/videotokenizationmayhelp
achievebettertrade-offbetweenperformanceandefficiency. Moreover,itisessentialtointroduce
multimodalgenerationtechniquestodirectlycreateeditingeffects,auxiliaryshots,andevenallthe
targetsinFigure1withaunifiedmodel.
8 Conclusion
Therapiddevelopmentofsocialmediaplatformsandeditingtoolshascreatedasignificantdemand
forvideoediting,highlightingtheneedformoreefficientmethodstoreducethelearningcurveand
usage cost. In this paper, we propose a novel task of verbal video composition with effects and
presentageneralframework. Webuildthefirstmultimodalvideocompositiondataset,formulatethe
taskasagenerationproblem,andsolveitbyleveragingtheunderstandingandreasoningcapabilities
oflargemultimodalmodels(LMMs). Ourapproachproveseffectiveingeneratinghigh-qualityvideo
compositions. Wehopeourworkwillbenefitnon-professionalsandinspirefurtherresearchonvideo
editingtasks. Futureworkincludesexpandingtomorevideoeditingelements,suchasmusic,facial
effects,andvirtualhumans,andsupportingmoreuserintentionstocontrolthedensity,themes,and
parametersofeffects.
References
[1] MaxwellGamer. Tierlistofrobloxgames. https://youtube.com/shorts/Kb8_LD2EsA4?
si=r7TeD0Ky0IzqO_S2,2024.
[2] Jianying. https://www.capcut.cn/.
[3] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image
translationusingcycle-consistentadversarialnetworks.InProceedingsoftheIEEEinternational
conferenceoncomputervision,pages2223–2232,2017.
[4] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasMüller,Joe
Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXivpreprintarXiv:2307.01952,2023.
[5] Ying Shen, Yizhe Zhang, Shuangfei Zhai, Lifu Huang, Joshua M Susskind, and Jiatao
Gu. Many-to-manyimagegenerationwithauto-regressivediffusionmodels. arXivpreprint
arXiv:2404.03109,2024.
10[6] OpenAI. Sora: Creatingvideofromtext. https://openai.com/index/sora/,2024.
[7] YixinLiu,KaiZhang,YuanLi,ZhilingYan,ChujieGao,RuoxiChen,ZhengqingYuan,Yue
Huang,HanchiSun,JianfengGao,etal. Sora:Areviewonbackground,technology,limitations,
andopportunitiesoflargevisionmodels. arXivpreprintarXiv:2402.17177,2024.
[8] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos N
Plataniotis, Yao Zhao, and Yunchao Wei. Diffusion4d: Fast spatial-temporal consistent 4d
generationviavideodiffusionmodels. arXivpreprintarXiv:2405.16645,2024.
[9] Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo, Wonmin Byeon, Sangpil Kim,
and Jinkyu Kim. The power of sound (tpos): Audio reactive video generation with stable
diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages7822–7832,2023.
[10] MengLiu,MingdaZhang,JialuLiu,HanjunDai,Ming-HsuanYang,ShuiwangJi,Zheyun
Feng,andBoqingGong. Videotimelinemodelingfornewsstoryunderstanding. Advancesin
NeuralInformationProcessingSystems,36,2024.
[11] MiaoWang,Guo-WeiYang,Shi-MinHu,Shing-TungYau,ArielShamir,etal. Write-a-video:
computationalvideomontagefromthemedtext. ACMTrans.Graph.,38(6):177–1,2019.
[12] SharathKoorathota,PatrickAdelman,KellyCotton,andPaulSajda. Editinglikehumans: a
contextual,multimodalframeworkforautomatedvideoediting.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages1701–1709,2021.
[13] DingyiYang,ChunruZhan,ZihengWang,BiaoWang,TiezhengGe,BoZheng,andQinJin.
Synchronizedvideostorytelling: Generatingvideonarrationswithstructuredstoryline. arXiv
preprintarXiv:2405.14040,2024.
[14] Jinsoo Choi, Tae-Hyun Oh, and In So Kweon. Video-story composition via plot analysis.
InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages
3122–3130,2016.
[15] PinelopiPapalampidi,FrankKeller,andMirellaLapata. Findingtherightmoment: Human-
assisted trailer creation via task composition. IEEE Transactions on Pattern Analysis and
MachineIntelligence,2023.
[16] YangHong,BoPeng,HaiyaoXiao,LigangLiu,andJuyongZhang. Headnerf: Areal-time
nerf-basedparametricheadmodel. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages20374–20384,2022.
[17] HaiyangLiu,ZihaoZhu,GiorgioBecherini,YichenPeng,MingyangSu,YouZhou,Naoya
Iwamoto,BoZheng,andMichaelJBlack. Emage: Towardsunifiedholisticco-speechgesture
generationviamaskedaudiogesturemodeling. arXivpreprintarXiv:2401.00374,2023.
[18] KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, and CV Jawahar. A lip sync
expertisallyouneedforspeechtolipgenerationinthewild. InProceedingsofthe28thACM
internationalconferenceonmultimedia,pages484–492,2020.
[19] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask:
Generativemaskedmodelingof3dhumanmotions. arXivpreprintarXiv:2312.00063,2023.
[20] CarlosHernandez-OlivanandJoseRBeltran. Musiccompositionwithdeeplearning: Areview.
Advances in speech and music technology: computational aspects and applications, pages
25–50,2022.
[21] Qi Chen, Mingkui Tan, Yuankai Qi, Jiaqiu Zhou, Yuanqing Li, and Qi Wu. V2c: visual
voicecloning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages21242–21251,2022.
[22] Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian
Cong,LelaiDeng,ChuangDing,LuGao,etal. Seed-tts: Afamilyofhigh-qualityversatile
speechgenerationmodels. arXivpreprintarXiv:2406.02430,2024.
11[23] ZhifengKong, WeiPing, AmbrishDantrey, andBryanCatanzaro. Speechdenoisinginthe
waveformdomainwithself-attention. InICASSP2022-2022IEEEInternationalConferenceon
Acoustics,SpeechandSignalProcessing(ICASSP),pages7867–7871.IEEE,2022.
[24] Zicheng Liao, Yizhou Yu, Bingchen Gong, and Lechao Cheng. Audeosynth: music-driven
videomontage. ACMTransactionsonGraphics(TOG),34(4):1–10,2015.
[25] YaojieShen,LiboZhang,KaiXu,andXiaojieJin. Autotransition: Learningtorecommend
videotransitioneffects. InEuropeanConferenceonComputerVision,pages285–300.Springer,
2022.
[26] Xin Gu, Libo Zhang, Fan Chen, Longyin Wen, Yufei Wang, Tiejian Luo, and Sijie Zhu.
Edit3k: Universal representation learning for video editing components. arXiv preprint
arXiv:2403.16048,2024.
[27] WeiboGong,XiaojieJin,DingLiu,andXiaohuiShen.Methods,apparatus,devices,andstorage
mediaforvideocomposition,March2023.
[28] HyvineYator. #autocut#capcut#goviral#trend#giviralgo#bimkuruu#hyvineyator#bimkuruu.
https://www.tiktok.com/@hyvine_yator/video/7356174567347326214,2024.
[29] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InInternationalconference
onmachinelearning,pages19730–19742.PMLR,2023.
[30] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. arXivpreprintarXiv:2308.12966,2023.
[31] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[32] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, AntoineMiech, IainBarr, YanaHasson,
KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisual
languagemodelforfew-shotlearning. Advancesinneuralinformationprocessingsystems,35:
23716–23736,2022.
[33] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advances
inneuralinformationprocessingsystems,36,2024.
[34] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,
ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.
Animageisworth16x16words: Transformersforimagerecognitionatscale. arXivpreprint
arXiv:2010.11929,2020.
[35] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala,
Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
15180–15190,2023.
[36] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
[37] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[38] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,
DennyZhou, etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.
Advancesinneuralinformationprocessingsystems,35:24824–24837,2022.
12[39] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748–8763.PMLR,2021.
[40] JulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,Ashraf
Elnashar,JesseSpencer-Smith,andDouglasCSchmidt. Apromptpatterncatalogtoenhance
promptengineeringwithchatgpt. arXivpreprintarXiv:2302.11382,2023.
[41] Kelly H Zou, Simon K Warfield, Aditya Bharatha, Clare MC Tempany, Michael R Kaus,
StevenJHaker,WilliamMWellsIII,FerencAJolesz,andRonKikinis. Statisticalvalidation
ofimagesegmentationqualitybasedonaspatialoverlapindex1: scientificreports. Academic
radiology,11(2):178–189,2004.
[42] YimingCui,ZiqingYang,andXinYao. Efficientandeffectivetextencodingforchinesellama
andalpaca. arXivpreprintarXiv:2304.08177,2023.
13A Appendix/supplementalmaterial
A.1 EffectsIntroduction
Wehaveeffectsin3modalities,asfollows:
1 Textualeffects: Thiscategoryincludestexteffects,textanimations,andtexttemplates. Text
effectsinvolvemodifyingattributessuchascolor,style,andotherfeatures. Textanimations
incorporatetransformationslikerotationandtranslationappliedtothetext. Texttemplates
offermoreintricatedesignsandincludesupplementarygraphicelements. Theseeffectsare
categorizedintoapproximately1,000distincttypes.
2 AudioEffects: Wefocusonsoundeffectsforaudio,suchas“ding”or“whoosh,”whichare
triggeredbasedontheemotionaltoneandcontentofthetext. Theseeffectscaptureviewers’
attentionandeffectivelyenhancethevideo’sclarity. Althoughmusicisalsoavailable,wedo
notfocusonitinthiscontext. Thereareapproximately1,000distinctsoundeffectsavailable
intotal.
3 Visualeffects:Stickers.Weemphasizestickersduetotheirvasttypesandrichthemes.These
includesmallimagesdepictingarangeofexpressions. Recommendingappropriatestickers
isachallengingtaskduetotheirextensivevariety. Ourdatasetcomprisesapproximately
10,000distinctstickers.
A.2 LearningObjects
The following part provides a comprehensive introduction to the input and learning objectives
designedbyus.
• ContextXcontentThispartdescribesthemultimodalinformationprovided,whichincludes
videoclips,audios,andtexts. Inourdataset,thereareinstanceswherethevideoandaudio
inputsareempty. Thecollecteddataconsistsofverbalvideos,typicallycontainingaround
20sentencesandhundredsofcorrespondingtokens.
• PromptXprompt Inreal-worldapplicationscenarios,usersmaydesirecontroloverdeco-
rationeffects,includingthefrequencyandspecificcategorytobeused. Thiscanresultin
distributionsthatdifferfromthetrainingdata. Toassessthemodel’sresponsivenesstouser
intent,wedesigntaskpromptsthatencompassthesetwoscenarios.
• IndexiTheindexingofsegmentsisdesignedtoassistthemodelineffectivelyidentifying
contextualinformationwithinthegivencontext,thuscapturingthecontentoftheresponse.
Additionally,itenablesamoreintuitiveobservationandinterpretationofthemodel’soutput.
• Trigger position l In our task, each sentence contains trigger positions represented by
specific words known as trigger words, which are derived from the original text. By
incorporatingthelearningobjectiveofinferringtriggerwords,weenablethemodeltolearn
theeditingworkflowinamoreorganizedmanner. Forinstance, inasentencelike“The
creambreadisdelicious”,themodelidentifies“delicious”asatriggerword,indicatingthe
needforediting,andthendeterminestheappropriateeffecttoapply. Inadditiontotrigger
words,wecanalsoemploytriggercaptions,triggeremotions,ortriggerintervalstoleverage
themultimodalinputandincorporatereasoningforediting.
• EditingEffectseOncethetriggerpositionisidentified,themodelcaninferthespecifictype
ofeffectwithoutrequiringadditionaltokensinthevocabulary. Weusethe“category-name”
representationtouniquelydenoteeacheffect. ThisapproachleveragestheLMM’sabilityto
effectivelyexploretherelationshipbetweeneffectnamesandvideocontext. Furthermore,
addingextratokenstothevocabularycancomplicatetheinitializationprocess. Forinstance,
randominitializationcouldresultinthelossofvaluableinformationofeffects.
A.3 ErrorBar
ThestandarderrorofthemeanforthemodelinthelastrowofTable1is0.54forwordprecision,0.46
forelem@word,0.52forelem@sentence,and1.25fortheoverallscore. Thesevaluesareobtained
fromfiveinferencesconductedontwoidenticallytrainedmodels.
14Figure5:Categoriesofeditingeffectsusedinourtask.
Frequency for Editing When Inference Distribution of Inference - Sticker - tot 328
175 102
150
125
100 101
75
50
25 100
0
0 20 40 60 80 100 20 21 22 23 24
Trigger Ratio(%) of Positions Number of Uses
(a)Distributionoftriggerpositionsintest. (b)Distributionofstickersoccurrencesintest.
Frequency for Editing When Inference Distribution of Inference - Sticker - tot 1115
200
103
175
150 102
125
100 101
75
50 100
25
0
0 20 40 60 80 100 20 21 22 23 24 25 26 27
Trigger Ratio(%) of Positions Number of Uses
(c) Distribution of trigger positions in test with (d)Distributionofstickersintestwithprompting
promptingmoretriggerpositions. morestickers.
Figure6:Comparisonofdifferentpromptinference.Comparedtotheaverageof50%inFigure6a,weobserve
a70%ratiooftriggerpositionswhenpromptedwithmoretriggerpositions,asshowninFigure6c. When
promptedwithmorestickers,weobtainthousandsofstickers,asillustratedinFigure6d,whereasonly328
stickersareobtainedwithoutprompting,asseeninFigure6b.
15
tluseR
tseT
ni
ytitnauQ
tluseR
tseT
ni
ytitnauQ
tluseR
tseT
ni
ytitnauQ
tluseR
tseT
ni
ytitnauQ