[
    {
        "title": "InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction",
        "authors": "Sirui XuZiyin WangYu-Xiong WangLiang-Yan Gui",
        "links": "http://arxiv.org/abs/2403.19652v1",
        "entry_id": "http://arxiv.org/abs/2403.19652v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19652v1",
        "summary": "Text-conditioned human motion generation has experienced significant\nadvancements with diffusion models trained on extensive motion capture data and\ncorresponding textual annotations. However, extending such success to 3D\ndynamic human-object interaction (HOI) generation faces notable challenges,\nprimarily due to the lack of large-scale interaction data and comprehensive\ndescriptions that align with these interactions. This paper takes the\ninitiative and showcases the potential of generating human-object interactions\nwithout direct training on text-interaction pair data. Our key insight in\nachieving this is that interaction semantics and dynamics can be decoupled.\nBeing unable to learn interaction semantics through supervised training, we\ninstead leverage pre-trained large models, synergizing knowledge from a large\nlanguage model and a text-to-motion model. While such knowledge offers\nhigh-level control over interaction semantics, it cannot grasp the intricacies\nof low-level interaction dynamics. To overcome this issue, we further introduce\na world model designed to comprehend simple physics, modeling how human actions\ninfluence object motion. By integrating these components, our novel framework,\nInterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot\nmanner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our\ncomprehensive experimental analysis demonstrates its capability to generate\nrealistic and coherent interaction sequences that seamlessly align with the\ntext directives.",
        "updated": "2024-03-28 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19652v1"
    },
    {
        "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
        "authors": "Kai ZhangYi LuanHexiang HuKenton LeeSiyuan QiaoWenhu ChenYu SuMing-Wei Chang",
        "links": "http://arxiv.org/abs/2403.19651v1",
        "entry_id": "http://arxiv.org/abs/2403.19651v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19651v1",
        "summary": "Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent work leverages text\ninstructions to allow users to more freely express their search intents.\nHowever, existing work primarily focuses on image pairs that are visually\nsimilar and/or can be characterized by a small set of pre-defined relations.\nThe core thesis of this paper is that text instructions can enable retrieving\nimages with richer relations beyond visual similarity. To show this, we\nintroduce MagicLens, a series of self-supervised image retrieval models that\nsupport open-ended instructions. MagicLens is built on a key novel insight:\nimage pairs that naturally occur on the same web pages contain a wide range of\nimplicit relations (e.g., inside view of), and we can bring those implicit\nrelations explicit by synthesizing instructions via large multimodal models\n(LMMs) and large language models (LLMs). Trained on 36.7M (query image,\ninstruction, target image) triplets with rich semantic relations mined from the\nweb, MagicLens achieves comparable or better results on eight benchmarks of\nvarious image retrieval tasks than prior state-of-the-art (SOTA) methods.\nRemarkably, it outperforms previous SOTA but with a 50X smaller model size on\nmultiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus\nfurther demonstrate the diversity of search intents supported by MagicLens.",
        "updated": "2024-03-28 17:59:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19651v1"
    },
    {
        "title": "Human-compatible driving partners through data-regularized self-play reinforcement learning",
        "authors": "Daphne CornelisseEugene Vinitsky",
        "links": "http://arxiv.org/abs/2403.19648v1",
        "entry_id": "http://arxiv.org/abs/2403.19648v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19648v1",
        "summary": "A central challenge for autonomous vehicles is coordinating with humans.\nTherefore, incorporating realistic human agents is essential for scalable\ntraining and evaluation of autonomous driving systems in simulation. Simulation\nagents are typically developed by imitating large-scale, high-quality datasets\nof human driving. However, pure imitation learning agents empirically have high\ncollision rates when executed in a multi-agent closed-loop setting. To build\nagents that are realistic and effective in closed-loop settings, we propose\nHuman-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are\ntrained through self-play with a small penalty for deviating from a human\nreference policy. In contrast to prior work, our approach is RL-first and only\nuses 30 minutes of imperfect human demonstrations. We evaluate agents in a\nlarge set of multi-agent traffic scenes. Results show our HR-PPO agents are\nhighly effective in achieving goals, with a success rate of 93%, an off-road\nrate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in\na human-like manner, as measured by their similarity to existing human driving\nlogs. We also find that HR-PPO agents show considerable improvements on proxy\nmeasures for coordination with human driving, particularly in highly\ninteractive scenarios. We open-source our code and trained agents at\nhttps://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent\nbehaviors at https://sites.google.com/view/driving-partners.",
        "updated": "2024-03-28 17:56:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19648v1"
    },
    {
        "title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models",
        "authors": "Samuel MarksCan RagerEric J. MichaudYonatan BelinkovDavid BauAaron Mueller",
        "links": "http://arxiv.org/abs/2403.19647v1",
        "entry_id": "http://arxiv.org/abs/2403.19647v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19647v1",
        "summary": "We introduce methods for discovering and applying sparse feature circuits.\nThese are causally implicated subnetworks of human-interpretable features for\nexplaining language model behaviors. Circuits identified in prior work consist\nof polysemantic and difficult-to-interpret units like attention heads or\nneurons, rendering them unsuitable for many downstream applications. In\ncontrast, sparse feature circuits enable detailed understanding of\nunanticipated mechanisms. Because they are based on fine-grained units, sparse\nfeature circuits are useful for downstream tasks: We introduce SHIFT, where we\nimprove the generalization of a classifier by ablating features that a human\njudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised\nand scalable interpretability pipeline by discovering thousands of sparse\nfeature circuits for automatically discovered model behaviors.",
        "updated": "2024-03-28 17:56:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19647v1"
    },
    {
        "title": "Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models",
        "authors": "Yucheng ShiQiaoyu TanXuansheng WuShaochen ZhongKaixiong ZhouNinghao Liu",
        "links": "http://arxiv.org/abs/2403.19631v1",
        "entry_id": "http://arxiv.org/abs/2403.19631v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19631v1",
        "summary": "Large Language Models (LLMs) have shown proficiency in question-answering\ntasks but often struggle to integrate real-time knowledge updates, leading to\npotentially outdated or inaccurate responses. This problem becomes even more\nchallenging when dealing with multi-hop questions since they require LLMs to\nupdate and integrate multiple knowledge pieces relevant to the questions. To\ntackle the problem, we propose the Retrieval-Augmented model Editing (RAE)\nframework tailored for multi-hop question answering. RAE first retrieves edited\nfacts and then refines the language model through in-context learning.\nSpecifically, our retrieval approach, based on mutual information maximization,\nleverages the reasoning abilities of LLMs to identify chain facts that na\\\"ive\nsimilarity-based searches might miss. Additionally, our framework incorporates\na pruning strategy to eliminate redundant information from the retrieved facts,\nwhich enhances the editing accuracy and mitigates the hallucination problem.\nOur framework is supported by theoretical justification for its fact retrieval\nefficacy. Finally, comprehensive evaluation across various LLMs validates RAE's\nability in providing accurate answers with updated knowledge.",
        "updated": "2024-03-28 17:47:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19631v1"
    }
]