Metric Learning from Limited Pairwise Preference Comparisons
Zhi Wang Geelon So
University of California San Diego University of California San Diego
zhiwang@ucsd.edu agso@ucsd.edu
Ramya Korlakai Vinayak
University of Wisconsin–Madison
ramya@ece.wisc.edu
Abstract
We study metric learning from preference comparisons under the ideal point model, in which a user
prefers an item over another if it is closer to their latent ideal item. These items are embedded into Rd
equippedwithanunknownMahalanobisdistancesharedacrossusers. Whilerecentworkshowsthatitis
possibleto simultaneously recover the metric and ideal itemsgiven O(d) pairwise comparisons per user,
in practice we often have a limited budget of o(d) comparisons. We study whether the metric can still
berecovered,eventhoughitisknownthatlearningindividualidealitemsisnownolongerpossible. We
showthatingeneral,o(d)comparisonsrevealsnoinformationaboutthemetric,evenwithinfinitelymany
users. However,whencomparisonsaremadeoveritemsthatexhibitlow-dimensionalstructure,eachuser
cancontributetolearningthemetricrestrictedtoalow-dimensionalsubspacesothatthemetriccanbe
jointly identified. We present a divide-and-conquer approach that achieves this, and provide theoretical
recovery guarantees and empirical validation.
1 Introduction
Metriclearningiscommonlyusedtodiscovermeasuresofsimilarityfordownstreamapplications(e.g.,Kulis
et al., 2013). In this paper, we study metric learning from pairwise preference comparisons. In particular,
we consider the ideal point model (Coombs, 1950), in which a set of items are embedded into Rd, and a user
prefers an item x over another x′ if it is closer to the user’s latent ideal point u∈Rd, that is,
ρ(x,u)<ρ(x′,u),
for some underlying metric ρ : Rd×Rd → R . While high-quality item embeddings have become increas-
≥0
ingly available, for example from foundation models pre-trained on internet-scale data (e.g., Radford et al.,
2021), naively equipping these representations with the Euclidean distance may not accurately capture the
semanticrelationsbetweenitemsasperceivedbyhumans,andthereforemaynotalignwithhumanvaluesor
preferences (Yu et al., 2014; Canal et al., 2022). Meanwhile, people often agree on their perception of item
similarities (Colucci et al., 2016). In this work, we study when and how a shared Mahalanobis distance can
be learned from a large crowd, where each user answers a few queries of the form: “Do you prefer x or x′?”
The line of work on simultaneous metric and preference learning was recently introduced by Xu and
Davenport(2020),whostudieditundertheidealpointmodelforasingleuser. Theyproposedanalternating
minimization algorithm to recover both the Mahalanobis distance and user ideal point. After, Canal et al.
(2022) introduced a convex formulation of the problem, providing the first theoretical guarantees while
extending the results to crowdsourced data. They showed that the cost of learning a Mahalanobis distance
can be amortized among users; it is possible to jointly learn the metric and ideal points in Rd so long as
sufficiently many users each provides Θ(d) preference comparisons.
However,whentherepresentationsofdataareveryhigh-dimensional,obtainingΩ(d)preferencecompar-
isons from each user can be practically infeasible. It can be expensive to ask a user more than a few queries
(Cohen et al., 2005) both in terms of cost and cognitive overload, and users may have concerns over their
1
4202
raM
82
]GL.sc[
1v92691.3042:viXraFigure 1: In our divide-and-conquer approach, users help us recover the metric Q restricted to subspaces
λ
V . We stitch these together to recover the metric M on Rd. The ellipses visualize the low-dimensional unit
λ
spheres, which are ‘slices’ of the full metric.
privacy (Jeckmans et al., 2013). Fortunately, through crowdsourcing, we often have access to preference
comparisons from a large pool of users. In this paper, we ask the fundamental question:
Can we learn an unknown Mahalanobis distance metric in Rd
from o(d) preference comparisons per user?
Weprovideatwofoldanswertothisquestion. First,weshowanegativeresult: evenwithinfinitelymany
users,itisgenerallyimpossibletolearnanythingatallabouttheunderlyingmetricwheneachuserprovides
fewer than d preference comparisons. In general, there is no hope for recovering the unknown metric from
preference comparisons without learning individual preference points as well.
Second, we show that the negative result does not rule out the possibility of learning the metric when
the set of items are subspace-clusterable (Definition 12); that is, when they lie in a union of low-dimensional
subspaces (Parsons et al., 2004; Ma et al., 2008; Elhamifar and Vidal, 2013). These subspaces may capture,
for instance, different categories or classes of items; such structure has also been studied extensively in
compressed sensing (Lu and Do, 2008; Eldar and Mishali, 2009) and computer vision (Ho et al., 2003),
among others. Given items with subspace-clusterable structure, we show that we can learn the Mahalanobis
distanceusingadivide-and-conquerapproach(Figure1). Thisinvolveslearningthemetricrestrictedtoeach
subspace,whichisfeasibleusingveryfewcomparisonsperuser,andthenreconstructingthefullmetricfrom
these subspace metrics.
Contributions We study the fundamental problem of learning an unknown metric with limited pair-
wise comparison queries, i.e, whether it is possible to learn a shared unknown metric without learning the
individual preference points. Our main contributions are as follows:
1. Weprovideanimpossibilityresult: nothingcanbelearnediftheitemsareingeneralposition(Section3);
2. Wedefinethenotionofsubspace-clusterableitemsandproposeadivide-and-conquerapproach,suchthat:
• Givennoiseless,unquantizedcomparisonsthatindicatehowmuchauserprefersoneitemoveranother,
we show that subspace-clusterability is necessary and sufficient for identifying the unknown metric
(Section 4);
• Givennoisy,quantizedcomparisonsintheformofbinaryresponsesoversubspace-clusterableitems,we
present recovery guarantees in terms of identification errors for our approach (Section 5);
3. We implement our proposed algorithm and validate our findings using synthetic data (Section 6).
21.1 Related work
Thereisarichliteratureonmetriclearning;see(Kulisetal.,2013)forasurvey. Alineofmetriclearningfrom
humanfeedbackfocusesonlearningMahalanobisdistancesfromtripletcomparisons(SchultzandJoachims,
2003; Verma and Branson, 2015; Mason et al., 2017), in which users are asked “is u closer to x or x′?”
However, triplet comparisons are a specific type of feedback that is not always practical to obtain. And so,
an important extension of these works is metric learning from preference comparisons, which can be seen as
avariantoftripletcomparisonswithanunknownlatentcomparatoru. Eventhoughpreferencecomparisons
are a weaker form of feedback, they are also much more prevalent. For example, they can be inferred from
user behavior, assuming users tend to engage more with items perceived to be more ideal.
Specifically, in this paper, we consider the ideal point model (Coombs, 1950), where the latent compara-
tor u in a preference comparison represents a user’s ideal item. Note that, if the ideal points are known
beforehand, one can simply treat this as a problem of metric learning from triplet comparisons. Conversely,
if the metric is known, one can also localize user ideal points using techniques from (Jamieson and Nowak,
2011; Massimino and Davenport, 2021; Tatli et al., 2023).
Our paper builds upon recent research that studies simultaneous metric and preference learning (Xu
and Davenport, 2020; Canal et al., 2022). In a single user setting, Xu and Davenport (2020) developed an
algorithm that iteratively alternate between estimating the metric and the user ideal point. Canal et al.
(2022) generalized the setting to involve multiple users. They established identifiability guarantees when
users provide unquantized measurements, and presented generalization bounds and recovery guarantees
when users provide binary responses. While Canal et al. (2022) showed that it is possible to jointly recover
a metric and user ideal points when each user answers Θ(d) queries, we address the fundamental question
of learning Mahalanobis distances when we have a much limited budget of o(d) preference comparisons per
user. The o(d) budget is more realistic especially when items are embedded in higher dimensions, but also
poses interesting new challenges as learning user ideal points is no longer possible.
Several other works in the broader literature are related. For example, learning ordinal embeddings
or kernel functions from triplet comparisons has been studied. Tamuz et al. (2011) developed an active
multi-dimensional scaling algorithm to learn item embeddings, with the goal of capturing item similarities
perceived by humans. See also (Van Der Maaten and Weinberger, 2012; Jain et al., 2016; Kleindessner and
von Luxburg, 2017), among other works. Hsieh et al. (2017) introduced a collaborative metric learning
algorithm, which uses matrix factorization to learn user and item embeddings such that the Euclidean
distance reflects user preferences and item/user similarities. A divide-and-conquer approach for deep metric
learning has been studied by Sanakoyeu et al. (2019), who use k-means to cluster items and learn separate
metrics for each cluster before concatenating them together; they performed an extensive empirical study
based on image data. In comparison, we consider the ideal point model to study the fundamental problem
of metric learning from limited preference comparisons. As we build directly on this line of work by Xu and
Davenport (2020) and Canal et al. (2022), we now present background and existing results in greater detail.
2 Preliminaries
Theidealpointmodel LetX beasetofitemsembeddedintoRd withanunknownMahalanobisdistance
ρ. Let M be its matrix representation in Rd×d. That is, M is a positive-definite (symmetric) matrix and for
all x,x′ ∈Rd,
(cid:113)
ρ(x,x′):= (x−x′)⊤M(x−x′)=∥x−x′∥ .
M
Suppose there is a large pool of users, and each user is associated with an unknown ideal point in Rd. A
user with ideal point u prefers an item x over another x′ if and only if ρ(x,u) < ρ(x′,u); or whenever
ψ(x,x′;u)<0, where:
ψ (x,x′;u):=∥x−u∥2 −∥x′−u∥2 . (1)
M M M
Each user’s ideal point may be distinct, but we assume that the metric ρ is a shared. We aim to recover ρ
when each user provides very few preference comparisons.
3Weconsidertwotypesofuserpreferencecomparisonsforlearningthemetric: unquantized andquantized
measurements. From a user with ideal point u, these are of the form:
(x,x′,ψ) and (x,x′,y),
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
unquantized quantized
where ψ =ψ(x,x′;u) is a real number that indicates the difference between the squared distances, and y is
binary, taking values in {−1,+1}. When y =−1, x is preferred over x′, and y =+1 indicates otherwise.
Metric learning from preference measurements We now review the existing algorithmic ideas for
recovering the metric from preference feedback under the ideal point model. Suppose that we are given
unquantizedmeasurementsfromasingleuserwithanidealpointu∈Rd. Withalittlealgebra(Canaletal.,
2022), the measurement in Eq. (1) becomes:
ψ (x,x′;u)=(cid:10) xx⊤−x′x′⊤ , M(cid:11) +(cid:10) x−x′,v(cid:11) , (2)
M
where v := −2Mu. The first inner product is the trace inner product for matrices, while the second inner
productistheusualinnerproductonRd. There-parametrizationv ofuissometimescalledthepseudo-ideal
point. Thus,unquantizedmeasurementsarelinearoverthejointvariables(M,v). Givenasetofunquantized
measurementsfromauser,onecanjustsolvealinearsystemofequationstorecoverthematrixrepresentation
M of ρ, as described in Algorithm 3 of Appendix B. Since M has full rank and therefore invertible, we can
then recover u from M and v (Canal et al., 2022).
As there are d(d+1) +d degrees of freedom in (M,v), to recover the metric in this way requires at least
2
that many measurements from a single user; the first term corresponds to the dimension of symmetric d×d
matrices representing Mahalanobis distances, the second for the user ideal point.
Whend2 isverylarge,wemaywanttoamortizelearningthemetricovermanyusers. Canaletal.(2022)
show that this is possible. Let the users be indexed by elements in [K]. We can construct a larger linear
regression problem, where each user has a separate covariate corresponding to their ideal point. Now, the
jointvariableis(M,v ,v ,...,v ),whichhas d(d+1)+dK degreesoffreedom. Whenthepopulationislarge,
1 2 K 2
it suffices to ask each user Θ(d+d2/K) preference queries, which can be much closer to d than d2. This
procedure is given in Algorithm 4 of Appendix B.
However,modernrepresentationsofdatamaybeextremelyhigh-dimensional,anditwouldbetooonerous
for any single user to provide d measurements. In this paper, we tackle this question: If we have access to
many users but can only ask each user a much more limited number m ≪ d of preferences queries, can we
stillrecoverρ? Wenotethatwitho(d)pairwisequeries, itisimpossibletolocalizetheidealpreferencepoint
of a user even with a known metric (Jamieson and Nowak, 2011; Massimino and Davenport, 2021). So, our
goal here is to address the open question of whether it is possible to learn an unknown metric with such
limited queries per users given a sufficiently large pool of users.
Notation Let Sym(Rd) denote the symmetric d×d matrices equipped with the trace inner product, and
let Sym+(Rd) be the positive-definite matrices. For readability, we often make abbreviations of the form
∆∈Sym(Rd) and δ ∈Rd:
∆≡xx⊤ −x′x′⊤ and δ ≡x−x′.
Then, ∆⊕δ is an element of Sym(Rd)⊕Rd, the direct sum of inner product spaces, and we can shorten
Eq. (2) to:
ψ
(x,x′;u)=(cid:10)
∆⊕δ,M
⊕v(cid:11)
.
M
Following the experimental design literature, let us call a collection of such elements a design matrix:
Definition 1. Let {(x ,x )} be a collection of item pairs. It induces the linear map D : Sym(Rd)×
i0 i1 i∈[m]
Rd →Rm,
(cid:10) (cid:11)
D(A,w) = ∆ ⊕δ ,A⊕w ,
i i i
where ∆ = x x⊤ −x x⊤ and δ = x −x for i ∈ [m]. As a slight abuse of language, we call D the
i i0 i0 i1 i1 i i0 i1
induced design matrix. If item pairs are drawn from a distribution P over (Rd×Rd)m, we say that D is
m
a random design and write D ∼P . We also define σ2 (P )= 1 ·σ (cid:0)E[D∗D](cid:1).
m min m m min
For additional background and notation, see Appendix B.
4Figure 2: A set of points X ⊂R2 that has generic pairwise relations.
3 An impossibility result
Consider the mathematically simplified setting in which users provide unquantized responses. We show a
negative result stating that when users provide fewer than d comparisons, we fundamentally cannot learn
anything about M if the items are in general position in the following sense:
Definition 2. A set X ⊂Rd has genericpairwiserelations if for any acyclic graph G=(X,E) with at most
d edges, the set {x−x′ :(x,x′)∈E} is linearly independent.
Thegeometricmeaningofhavinggenericpairwiserelationsissimple: ifanydpairsofpointsareconnected
by lines, then those lines are linearly independent (unless they form cycles; see Figure 2 for an illustration).
Proposition C.3 shows that almost all finite subsets of Euclidean space have generic pairwise relations with
respect to the Lebesgue measure1.
Thefollowingtheoremshowsthatifitemshavegenericpairwiserelations,thensetsofm≤dunquantized
measurementsfromasingleuserprovidenoinformationabouttheunderlyingmetric. Inparticular,suppose
that M and v are the underlying matrix representation and user’s pseudo-ideal point, both unknown to us.
Then, for any other Mahalanobis matrix M′, we can find a pseudo-ideal point v′ that is also consistent with
the data. In fact, the negative result holds even with infintely many users:
Theorem 3. Fix M ∈ Sym+(Rd) and v k ∈ Rd for each k ∈ N. Let (D k) k∈N be a collection of design
matrices, each for a set of m≤d pairwise comparisons. If each set of compared items has generic pairwise
relations, then for all M′ ∈Sym+(Rd), there exists (v′) ⊂Rd such that:
k k∈N
D (M,v )=D (M′,v′), ∀k ∈N.
k k k k
See Appendix C for a proof of Theorem 3. This theorem shows that when items have generic pairwise
relations itisnotjustthatwecannotrecoverρ,butthatwecannotgleananythingatallaboutρwhenusers
each provide d or fewer comparisons, for every matrix in Sym+(Rd) is consistent with D. While each user
provides us with more data, each also introduces new degrees of freedom—the unknown ideal points. When
learning from crowds, more data does not necessarily lead to more usable information.
4 Exact recovery with low-rank subspace structure
The above negative result applies to almost all finite sets of items. It seems to tell a pessimistic story
for metric learning when data is embedded into high dimensions and when it is infeasible to obtain Ω(d)
preference comparisons per user.
However, the story is not closed and shut yet. Real-world data often exhibit additional structure that
could help us recover the metric, such as low intrinsic dimension (Fefferman et al., 2016). In particular, we
assume that many items of X lie on a union of subspaces. The approximate validity of this assumption is
the basis of work in manifold learning (Roweis and Saul, 2000; Tenenbaum et al., 2000; Belkin and Niyogi,
2003), compressed sensing (Donoho, 2006), and sparse coding (Olshausen and Field, 1997), among others.
1InAppendixC,wediscusstheconnectionbetweengenericpairwiserelationsandgenerallinearposition,astandardnotion
fromgeometry.
5In this case, we can take a divide-and-conquer approach to metric learning by identifying the metric
restricted to those subspaces, before stitching them back together to recover the full metric. Let’s define
subspace Mahalanobis distances:
Definition 4. Let V be a subspace of Rd. A metric on V is a subspace Mahalanobis distance if it is a
subspace metric of some Mahalanobis distance ρ on Rd. In that case, we denote the subspace metric by ρ(cid:12) (cid:12) ,
V
where for all x,x′ ∈V,
ρ(cid:12)
(cid:12)
(x,x′)=ρ(x,x′).
V
In general, we cannot hope to identify an arbitrary metric from a finite number of its subspace metrics.
However, Mahalanobis distances have much more structure than arbitrary metrics on Rd. A Mahalanobis
distance on Rd can be fully specified using d(d+1)/2 numbers. By recovering its subspace metrics, we can
hope to chip away at the degrees of freedom of Mahalanobis distances. As another way of intuition, each
Mahalanobis distance may be identified with its unit sphere—points that are unit distance away from the
origin. These points form a (d−1)-dimensional ellipsoid in Rd. To recover a subspace Mahalanobis distance
on V means that we are able to determine which points of V intersect this ellipsoid (see Figure 1). If we do
this for sufficiently many subspaces, we can determine the whole ellipsoid. To formalize this intuition, we
now linear-algebraically relate a Mahalanobis distance with its subspace metrics.
4.1 A linear parametrization of Mahalanobis distances
To describe the linear relationship between a Mahalanobis distance and its subspace metrics, we need to
parametrize the subspace metrics. To do so, we first need to fix a choice of coordinates on each V ⊂Rd. In
the following, let V be an r-dimensional subspace of Rd and let B ∈ Rd×r be an orthonormal basis of V,
where r ≪d.
Definition 5. We say V has a canonicalrepresentation if it is equipped with an orthonormal basis B, where
the canonical representation of a vector x∈V is given by B⊤x∈Rr.2
Definition 6. Let Sym(V) and Sym+(V) respectively denote the pairs (Sym(Rr),B) and (Sym+(Rr),B),
where V has a canonical representation given by B.
Let Q∈Sym(V) mean that Q∈Sym(Rr), and that it carries the basis information B along with it.
JustasMahalanobisdistancesonRd areinone-to-onecorrespondencewithpositive-definitematrices, so
too are Mahalanobis distances on V in correspondence with Sym+(V). Furthermore, Proposition D.1 shows
that the matrix representations of a Mahalanobis distance and its restriction to a subspace is given by the
following linear map.
Definition 7. Let V and B be as before. Define the linear map Π :Sym(Rd)→Sym(V) by:
V
Π (A)=B⊤AB. (3)
V
Thus, if a Mahalanobis distance ρ on Rd and its restriction ρ(cid:12) (cid:12) to a subspace V have representations
V
M ∈Sym+(Rd) and Q∈Sym+(V), respectively, then:
Q=Π (M)=B⊤MB.
V
4.2 Learning with low-rank subspaces
To see how low-dimensional structure can help us make progress in learning the metric, consider a simple
setting where all items lie in some low-dimensional subspace V. Instead of learning the full metric ρ, we
could aim for a more modest goal of learning the subspace metric ρ(cid:12)
(cid:12)
.
V
As before, let V be an r-dimensional subspace of Rd with a canonical representation. If all items and
ideal points lie in V, then learning ρ(cid:12)
(cid:12)
immediately reduces to the usual setting of learning a Mahalanobis
V
distance, since we can simply ignore the remaining dimensions and reparametrize the problem. But when
2WeshallalwaysequipRd withthestandardbasis,sothatavectorisitsowncanonicalrepresentationinRd.
6the ideal points are not assumed to lie on V, it is not evident a priori that we can ignore the dimensions
extending beyond the set of items. However, it turns out that for Mahalanobis distances, we may.
The next lemma shows that even if a user’s ideal point u∈Rd falls outside of V, for items in V, there is
a phantom ideal point u ∈V such that preference comparisons for items in V generated by u and u are
V V
equivalent.
Lemma 8. Let V be an r-dimensional subspace of Rd with a canonical representation given by B ∈ Rd×r.
Fix any Mahalanobis distance M ∈Sym+(Rd), any pair of items x,x′ ∈Rd, and ideal point u∈Rd. Suppose
that x and x′ are contained in V with canonical representation x =B⊤x and x′ =B⊤x′ in Rr. Then:
V V
ψ (cid:0) x,x′;u(cid:1) =ψ (cid:0) x ,x′ ;u (cid:1) ,
M Q V V V
where the phantom ideal point u of u on V satisfies (B⊤MB)u =B⊤Mu, and Q=Π (M) is the matrix
V V V
representation in Sym+(V) of the subspace metric ρ(cid:12)
(cid:12)
.
V
Consequently, learning a subspace metric ρ(cid:12)
(cid:12)
turns into a problem of metric learning from preference
V
comparisons in Rr. From here, we can simply use existing algorithms to recover the matrix representation
of the subspace metric. By Canal et al. (2022), it is possible to identify the subspace metric so long as users
can each provide m ≥ Ω(r) preference comparisons. For this easier problem of learning ρ(cid:12) (cid:12) , when r ≪ d,
V
we can do with o(d) responses per user.
In the remainder of this section, we give a simple characterization for when a Mahalanobis distance on
V can be learned from preference comparisons of items on V. The set of items needs to be sufficiently rich
so that all degrees of freedom of Sym(V)⊕V can be captured. We define:
Definition 9. Let V be a subspace of Rd with canonical representation given by B. A subset X ⊂ V
V
quadratically spans V if Sym(V)⊕V is linearly spanned by the set:
(cid:8) (x x⊤−x′ x′⊤)⊕(x−x′):x,x′ ∈X (cid:9) ,
V V V V V
where x =B⊤x and x′ =B⊤x′ denote the canonical representations of x and x′ in V.
V V
If we have no restriction on how many queries we can ask a user, then it is straightforward to see that
quadratic spanning is a sufficient condition for recovering the underlying metric. For simplicity, let V =Rd.
If X quadratically spans Rd, then we can detect all dimensions of M ⊕v corresponding to the Mahalanobis
matrix and a user’s pseudo-ideal point. To do so, choose any design matrix D :Sym(Rd)⊕Rd →Rm whose
rows {∆ ⊕δ :i∈[m]} span Sym(Rd)⊕Rd.
i i
When the number of queries is limited per user, the following result shows that the quadratic spanning
condition is still sufficient for recovering ρ(cid:12) (cid:12) , provided we can ask many users m≥dim(V)+1 unquantized
V
preference queries.
Proposition10. LetX quadraticallyspanasubspaceV ofdimensionr. ThereexistsacollectionD ,...,D
1 K
of design matrices, each over m pairs of items in X, such that given a (distinct) user’s response to each
design, ρ(cid:12) (cid:12) can be identified when m≥r+1 and K ≥r(r+1)/2.
V
To complement this sufficient condition, the next result shows that if X does not quadratically span V,
then the subspace metric ρ(cid:12) (cid:12) cannot be recovered from only preference comparisons of items in X ∩V.
V
Proposition 11. Let (D k) k∈N be a set of design matrices over items in X ⊂V. If X does not quadratically
span V, then infinitely many Mahalanobis distances on V are consistent with any set of user responses to
the design matrices.
Proofs for the above results are deferred to Appendix D.2.
4.3 Learning with subspace-clusters
We’ve seen how to partially learn a Mahalanobis distance given many items within a subspace. We now
consider how to fully recover the metric when many items lie in a union of subspaces (V ) . In this case,
λ λ∈Λ
adivide-and-conquerapproachisintuitive: (i)recovereachsubspacemetric,then(ii)reconstructρfromthe
7Algorithm 1: Metric learning from subspace clusters
Input: Unquantized measurements over items that lie in a union of subspaces V ,λ∈Λ
λ
// Stage 1: learning subspace metrics
1 for each subspace λ∈Λ do
2 Recover Qˆ λ ∈Sym(Rrλ) with respect to B λ via reduction to Algorithm 4 (Canal et al., 2022)
// Stage 2: reconstruction
3
Solve the linear equations over A∈Sym(Rd):
B⊤AB =Qˆ , λ∈Λ
λ λ λ
Output: Aˆ, the solution to the above linear equations.
learned subspace metrics. Recall that each subspace metric ρ(cid:12) (cid:12) is related to the full metric ρ by the linear
V
map Π from Definition 7. Therefore, we can reconstruct ρ from its subspace metrics by solving a system
V
of linear equations. Algorithm 1 summarizes this approach.
In order to characterize when a Mahalanobis distance can be reconstructed from its subspace metrics,
we introduce the notion of subspace-clusterability. A set of items X is subspace-clusterable when many of
its items lie on sufficiently many item-rich subspaces. Formally:
Definition 12. A set X ⊂Rd is subspace-clusterable over subspaces V ⊂Rd indexed by λ∈Λ whenever:
λ
1. each subset X ∩V quadratically spans V .
λ λ
2. (cid:8) xx⊤ :x∈V ,λ∈Λ(cid:9) linearly spans Sym(Rd).
λ
By Propositions 10 and 11, the first condition is necessary and sufficient for recovering each subspace
metric ρ(cid:12) (cid:12) . Proposition 13 shows that the second condition is necessary and sufficient for recovering the ρ
from subsV pλace metrics.
Proposition 13. Let ρ be a Mahalanobis distance on Rd. Let (V ) be a collection of subspaces with
λ λ∈Λ
canonical representations given by the orthonormal bases (B ) . The following are equivalent:
λ λ∈Λ
1. (cid:8) xx⊤ :x∈V ,λ∈Λ(cid:9) spans Sym(Rd).
λ
(cid:77)
2. Let Π be given by Equation (3). The linear map Π:Sym(Rd)→ Sym(V ) is injective, where:
Vλ λ
λ∈Λ
(cid:77)
Π(A)= Π (A).
Vλ
λ∈Λ
3. If ρˆis a Mahalanobis distance such that ρˆ(cid:12)
(cid:12)
=ρ(cid:12)
(cid:12)
for all λ∈Λ, then ρˆ=ρ.
Vλ Vλ
See Appendix D.3 for the proof. This proposition verifies the correctness of Algorithm 1. Let Q ∈
λ
tS hy em o+ n( lV y) mr ae tp rr ie xse Ant ∈ρ S(cid:12) (cid:12) V yλm. (RT dh )en c, onst se isp te3 ntof wt ih the a thlg eo sr yit sh tm emsp oe fc li ifi ne es arth ea qt uaΠ tV ioλ n( sA i) s= thQ eλ o. neIf tΠ hatis ri en pj re ec st eiv ne t, st ρh .en
Remark 14. We can compute the number of subspaces required to identify ρ using Proposition 13. For
example,whendim(V )=1foreachλ∈Λ,eachsubspacecapturesonedegreeoffreedomofρ,so|Λ|≥ d(d+1)
λ 2
is necessary. See Figure D.1 in Appendix D for geometric intuition.
5 Approximate recovery from binary responses
Previously, we studied metric learning from unquantized preference comparisons of the form (x,x′,ψ). We
now consider a more realistic setting where we obtain binary responses of the form (x,x′,y), where y ∈
8Algorithm 2: Metric learning from binary responses
Input: Quantized measurements over items that lie in a union of subspaces V ,λ∈Λ
λ
// Stage 1: learning subspace metrics
1 for each λ∈Λ do
2 Recover Qˆ λ ∈Sym(Rrλ) with respect to B λ via reduction to Algorithm 5 (Canal et al., 2022)
// Stage 2: reconstruction
3
Use ordinary least squares to solve the linear regression problem over A∈Sym(Rd):
Mˆ ← argmin
(cid:88)(cid:13)
(cid:13)Qˆ −B⊤AB
(cid:13) (cid:13)2
(4)
LS (cid:13) λ λ λ(cid:13)
A∈Sym(Rd)
λ∈Λ
F
4 Project Mˆ LS onto the set of positive semidefinite d×d matrices by solving the convex optimization
problem:
Mˆ ←argmin (cid:13) (cid:13)A−Mˆ LS(cid:13) (cid:13)2
F
(5)
A⪰0
Output: Mˆ.
{−1,+1}. Furthermore, we assume that responses are quantized and noisy, where noise can depend on the
user and items, as in (Mason et al., 2017; Xu and Davenport, 2020; Canal et al., 2022).
For our divide-and-conquer approach, due to the inexactness of the responses, we can no longer expect
to exactly identify each subspace metric. However, we show that as long as each subspace metric can
be recovered approximately, then they can be stitched together to approximately recover the full metric
(Theorem15). Andindeed,approximaterecoveryineachsubspaceisknowntobepossible. InProposition18,
we present a version of Theorem 4.1 of Canal et al. (2022) adapted to subspaces; this gaurantee is provided
under a probabilistic noise model that we describe shortly.
Divide-and-conquer algorithm Algorithm 2 generalizes our earlier algorithm for unquantized measure-
ments. As before, say we’ve obtained measurements for a set of items subspace-clusterable over (V ) . In
λ λ
the first stage, we recover the subspace metrics on each V . Lemma 8 reduces metric learning on subspaces
λ
to metric learning on Rr, where r is the dimension of the subspace, so we can call existing methods for
metric learning from binary responses across users (Canal et al. (2022) or Algorithm 5). Thus, we obtain an
estimator Qˆ for each subspace metric Q .
λ λ
In the second stage, we approximately reconstruct the Mahalanobis matrix M from the estimators Qˆ .
λ
When each Qˆ was exact, we could just solve the linear system of equations Π (Mˆ) = Qˆ . As this
is no longer
tλ
he case, we instead compute the ordinary least squares
estimatorV Mλˆ
,
whichλ
minimizes
LS
(cid:80) ∥Qˆ − Π (A)∥2 over A ∈ Sym(Rd) in Eq. (4) of Algorithm 2. Finally, we ensure that the recon-
strλ uctedλ matrV iλ x corresponds to a pseudo-metric by solving a linear program to project Mˆ onto the cone
LS
of positive semi-definite matrices (Boyd and Vandenberghe, 2004).
5.1 Recovery guarantees
Reconstruction guarantee The following theorem gives a recovery guarantee on the full metric, given
approximaterecoveryforeachsubspacemetric,∥Qˆ −Q ∥ ≤εforsomeε>0. SeeAppendixE.1forproof.
λ λ F
Theorem 15. Let Rd have a Mahalanobis distance with matrix representation M ∈Sym+(Rd). Let X ⊂Rd
be subspace-clusterable over subspaces V indexed by λ ∈ Λ, where |Λ| = n. Let Mˆ be the estimator of M
λ
and let Qˆ be the estimator of the subspace metric Q for each λ learned from Algorithm 2. Suppose there
λ λ
exist γ ≤ε such that (cid:13) (cid:13)E(cid:2) Qˆ λ(cid:3) −Q λ(cid:13) (cid:13)
F
≤γ and (cid:13) (cid:13)Qˆ λ−Q λ(cid:13) (cid:13)
F
≤ε for each λ. Fix p∈(0,1]. Then, there is a
9universal constant c>0 such that with probability at least 1−p,
 (cid:115) 
(cid:13) (cid:13)Mˆ −M(cid:13) (cid:13) ≤c· 1 γ√ n+εd log2d ,
F σ (Π) p
min
where σ >0 is the least singular value of Π.
min
Remark 16. This recovery guarantee depends on three parameters: (1) σ (Π) captures how well-spread
min
the set {xx⊤ :x∈V ,λ∈Λ} is across Sym(Rd). (2) ε bounds the recovery error for each subspace metric;
λ
it decreases as the number of pairwise comparisons per user increases (Remark 19). (3) γ bounds the bias
of the estimator Qˆ . It can be the dominating term in the recovery bound, for example when σ (Π) ≫ d.
λ min
While this bias term γ ≤ε can be made arbitrarily small with enough comparisons per user, for data-starved
regimes, bias reduction can also be applied in practice (e.g. Firth (1993)).
Recovery guarantee for subspace metrics For completeness, we adapt the setting and results of
(Canal et al., 2022) to provide a recovery guarantee for learning each subspace metric. We assume the same
probabilistic model:
Assumption17(Probabilisticmodel). LetM ∈Sym+(Rd)bethematrixrepresentationoftheMahalanobis
distance, let v ,...,v ∈Rd be the pseudo-ideal points for a collection of users, and let X ⊂Rd be a set of
1 K
items. We assume:
∥M∥ ≤ζ , ∥v ∥≤ζ , sup∥x∥≤1,
F M k v
x∈X
for ζ ,ζ >0. When asked to compare two items x and x′, the kth user provides a binary response Y with:
M v
Pr[Y =y]=f(cid:0) y·ψ (x,x′;u )(cid:1) ,
M k
where f : R → [0,1] is a strictly increasing link function such that f(z) = 1−f(−z), and where u is the
k
corresponding ideal point. On the domain |z| ≤ 2(ζ +ζ ), let f have lower bounded derivative f′(z) ≥ c
M v f
and let the map z (cid:55)→−logf(z) have Lipschitz constant L.
Algorithm 5 estimates (M,v ,...,v ) by using the users’ measurements to construct an optimization
1 K
program over the parameters; when the loss function supplied to the algorithm is ℓ(z) = −logf(z), the
procedure is equivalent to maximum likehlihood estimation. As noted above, it suffices to consider learning
Mahalanobis distances on Rr. The following proposition proves correctness of Algorithm 5.
Proposition 18 (Theorem 4.1, Canal et al. (2022)). Suppose that Rr has a Mahalanobis distance with
representation Q ∈ Sym+(Rr) where ∥Q∥ ≤ ζ . Let each user k ∈ [K] have pseudo-ideal point v ∈ Rr
F M k
where v ≤ ζ . Let P be a distribution over designs of size m over Rr (Definition 1). For each user,
k v m
let D ∼ P be an i.i.d. random design, and let D = {(x ,x ,y )} be the user’s responses under
k m k i0 i1 i;k i∈[m]
Assumption 17. Fix p ∈ (0,1]. Given loss function ℓ(z) = −logf(z), Algorithm 5 returns Qˆ ∈ Sym+(Rr),
where with probability at least 1−p,
(cid:115)
∥Qˆ−Q∥2 ≤
16L (ζ M2 +Kζ v2)log p4
.
F c2 ·σ2 (P ) mK
f min m
The proof of Proposition 18 is deferred to Appendix E.2
Remark 19. We can simplify the bound if we assume that M has bounded entries, say ∥M∥ ≤ 1. Let’s
∞
also assume that user ideal points are contained in the unit ball, so that ∥u ∥ ≤ 1 for each user. Then,
√ k 2
we can set ζ ≤ r and ζ ≤ 2 r since v = −2Mu . Remark E.2 shows that given access to a subspace-
M v k k
clusterable set of items, we can construct a sequence of random designs (P ) over those items such that
m m
σ2 (P )=Ω(1). Suppressing the confidence parameter p, we obtain the recovery guarantee:
min m
(cid:32)(cid:114) (cid:33)
r2+Kr
∥Qˆ−Q∥2 =O .
F mK
106 Empirical Validation
In this section, we empirically validate our findings using synthetic data3. We aim to address the following
questions:
1. Givenlimitednoisy,quantizedpreferencecomparisonsonsubspace-clusterableitems,canourproposed
divide-and-conquer algorithm recover an unknown metric M?
2. Does the performance of our algorithm improve if we have access to more subspace-clusters, users or
preference comparisons per user?
3. When items in X lie approximately in a union of subspaces, can we still recover M?
Experimental setup For each run, we generate a random ground-truth metric M ∈Sym+(Rd) from the
standardWishartdistributionW(I ,d),acollectionofuniform-at-randomr-dimensionalsubspaces(Stewart,
d
1980), and a set of user ideal points drawn i.i.d. from the Gaussian N(0,1I ). Within each subspace, items
d d
are drawn i.i.d. from N(0,1BB⊤), where B ∈Rd×r is an orthonormal basis of that subspace. Given a user
r
and a pair of items, a binary response is sampled according to the probabilistic model in Assumption 17,
(cid:0) (cid:1)
Pr[Y =y]=f y·ψ ,
where f is a link function. We select the logistic sigmoid link function, f(z;β) = 1/(cid:0) 1+exp(−βz)(cid:1) in our
experiments. Byvaryingβ, wecangeneratebinaryresponsesthatinterpolatebetweenuncorrelatedrandom
noise (β =0) and noiseless quantized measurments (β =∞). Further details can be found in Appendix F.1.
Weuseourdivide-and-conquerapproachtolearnthemetric(Algorithm2). Toapproximatethesubspace
metrics, this method calls Algorithm 5 (Stage 1, line 2). It does so by performing maximum likelihood esti-
mation on the correctly-specified probabilistic model. When stitching the subspaces together, we observed
that Huber regression (Huber, 1964; Pedregosa et al., 2011) generally leads to better performance over least
squaresregressionwithinAlgorithm2(Stage2,line3). Inthefollowing,wereportresultsobtainedusingthis
robustvariantoflinearregression. WeevaluatethelearnedmetricMˆ byitsrelativeerror,∥Mˆ−M∥ /∥M∥ .
F F
We ran three experiments each for 30 runs, where we set the subspace dimension to r = 1 and we set
β =1 in the logistic sigmoid link function.
Experiment1: Relativeerrorvsnumberofcomparisons Inthefirstexperiment,wesettheambient
dimension to d = 10 and generated data that lie in a union of 80 subspaces (by Remark 14, at least
dim(Sym(R10))= 55 subspaces are needed for recovery). We ran Algorithm 2 for different combinations of
K and m, where K is the number of users per subspace and m is the number of preference comparisons
per user. Figure 3a compares the average relative errors for varying K and m. This experiment shows that
with more preference comparisons, recovery within each subspace improves and we achieve better recovery
ofthefullmetric; thissupportsTheorem15andProposition18. Thisexperimentalsosuggeststhatgiven1-
dimensionalsubspaces,evenaskingforonlytwomeasurementsperuserissufficienttoachievegoodempirical
performance for metric recovery.
Experiment 2: Relative error vs number of subspaces In the second experiment, we set K = 60
and m = 4. For ambient dimensions d = 3,4,...,10, we consider the relative error for reconstructing Mˆ
using an increasing number of subspaces, n = 5,6,...,80. Figure 3b shows the average relative errors. For
each d, average relative error decreases as n increases. Furthermore, even in thisnon-idealized setting where
users provide noisy, binary responses, we can obtain non-trivial relative error when the number of subspaces
n exceeds the information-theoretic bound d(d+1)/2. This corroborates the dimension-counting argument
in Remark 14 beyond unquantized measurements.
3Ourcodeisavailableathttps://github.com/zhiwang123/metric-learning-lazy-crowds.
11(a) (b) (c)
Figure 3: (a) shows the average relative errors for varying numbers of users per subspace and preference
comparisonsperuser,whereitemslieinaunionof801-dimensionalsubspacesofR10. (b)showstheaverage
relative errors given increasing numbers of 1-dimensional subspaces to reconstruct Mˆ; for each subspace,
60 users each provides 4 preference comparisons. The dotted red curve illustrates the dimension-counting
argument in Remark 14. (c) shows the average relative errors for varying subspace noise levels, where
items lie approximately in a union of 80 1-dimensional subspaces of R10; each user provides 8 preference
comparisons. The error bars in (a) and (c) represent one standard deviation from the mean.
Experiment 3: Recovery when items approximately lie in subspaces In the third experiment, we
empirically study how our approach works when the subspace clusterable assumption only approximately
holds. For a subspace V, we sample items near V from N(0,1BB⊤+ σ2 B B⊤), where σ > 0 is a given
r d−r ⊥ ⊥
noise level, B ∈ Rd×r and B ∈ Rd×(d−r) are orthonormal bases of V and its orthogonal complement V⊥,
⊥
respectively. Thewayuserpreferenceresponsesaregeneratedremainsthesameasbefore. Foreachsubspace
V, we preprocess the items by running singular value decomposition on the nearby items to recover an r-
dimensional subspace Vˆ. We project these items to Vˆ, before running Algorithm 2 with these approximate
representations. We set d = 10 and m = 8. For each subspace noise level σ, we ran our approach on items
that lieapproximately in 80 subspaces forvarying K; Figure 3cshowsthe average relative errors. Whenthe
noise level σ is low, we can still recover the metric well. This may break down as σ increases; indeed, when
σ =1, there is no subspace structure at all.
Learning with a misspecified response model Recall that the subspace metrics are learned via max-
imum likelihood estimation (Algorithm 5, line 2). In this section, we investigate how sensitive this approach
is to a misspecified response model. To do so, we repeated the above three experiments with a fixed model
that assumes β = 1. However, we generated the data from mis-matched response noise levels: (a) β = 4,
corresponding to the “medium” setting in (Canal et al., 2022), and (b) β =∞, which is the noiseless setting
where y =−1 if the corresponding unquantized measurement is less than 0 and y =+1 otherwise. Figure 4
showsthatthelearnerperformswellevenwithoutthecorrectknowledgeofβ. InAppendixF,weshowthat
the approach still achieves reasonable performance when the negative log loss (to compute the maximum
likelihood) is replaced with the hinge loss in Algorithm 5 (Figure F.1). This shows that the learner may not
need to know that probabilistic model under which user binary responses are generated.
See also Appendix F.2 for additional experimental results with subspace dimension r =2.
12(a) Medium noise (β =4)
(b) Noiseless (β =∞)
Figure 4: shows the results obtained from the three experiments with a misspecified response model. The
learner is agnostic to the response noise level used to generated the data (β = 4 and β = ∞) and assumes
β =1 when recovering subspace metrics before stitching them together.
137 Conclusion and future work
We studied crowd-based metric learning from very few preference comparisons per user. In general, we
showednothingcanbelearned. However,whentheitemsexhibitlow-ranksubspace-clusterablestructure,we
proposedadivide-and-conquerapproachandprovidedrecoveryguarantees. Interestingly,thisworksuggests
that when training of foundation models, there is reason to favor learning general-purpose representations
with low-rank structures, as this may reduce the cost of downstream fine-tuning and alignment.
Our experiments show that even when the items do not exactly lie on the subspaces, but instead only
exhibit approximate subspace structure, our method can still recover the metric. We leave establishing
theoretical gurantees for this setting for future work. Our results has implications for alignment of repre-
sentations from foundation models to human preferences and we defer building an algorithmic framework
that finds subspace clusters before learning metrics, and evaluating it with real-world item embeddings and
human preference feedback for future work.
8 Acknowledgements
ZW thanks Kamalika Chaudhuri for helpful discussions and the National Science Foundation under IIS
1915734 for support. This work was also partially supported by NSF grants NCS-FO 2219903 and NSF
CAREERAwardCCF2238876. Additionally,thisworkwaspartiallysupportedbytheNSFawards: SCALE
MoDL-2134209,CCF-2112665(TILOS).ItwasalsosupportedinpartbytheDARPAAIEprogram,theU.S.
Department of Energy, Office of Science, the Facebook Research Award, as well as CDC-RFA-FT-23-0069
from the CDC’s Center for Forecasting and Outbreak Analytics.
References
M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural
computation, 15(6):1373–1396, 2003.
S. P. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.
G. Canal, B. Mason, R. Korlakai Vinayak, and R. Nowak. One for all: Simultaneous metric and preference
learning over multiple users. Advances in Neural Information Processing Systems, 35:4943–4956, 2022.
R. Cohen, M. Y. Cheng, and M. W. Fleming. Why bother about bother: Is it worth it to ask the user. In
Proceedings of AAAI Fall Symposium, 2005.
L.Colucci,P.Doshi,K.-L.Lee,J.Liang,Y.Lin,I.Vashishtha,J.Zhang,andA.Jude. Evaluatingitem-item
similarity algorithms for movies. In Proceedings of the 2016 CHI conference extended abstracts on human
factors in computing systems, pages 2141–2147, 2016.
C. H. Coombs. Psychological scaling without a unit of measurement. Psychological review, 57(3):145, 1950.
D. L. Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289–1306, 2006.
Y. C. Eldar and M. Mishali. Robust recovery of signals from a structured union of subspaces. IEEE
Transactions on Information Theory, 55(11):5302–5316, 2009.
E. Elhamifar and R. Vidal. Sparse subspace clustering: Algorithm, theory, and applications. IEEE transac-
tions on pattern analysis and machine intelligence, 35(11):2765–2781, 2013.
C. Fefferman, S. Mitter, and H. Narayanan. Testing the manifold hypothesis. Journal of the American
Mathematical Society, 29(4):983–1049, 2016.
D. Firth. Bias reduction of maximum likelihood estimates. Biometrika, 80(1):27–38, 1993.
J. Ho, M.-H. Yang, J. Lim, K.-C. Lee, and D. Kriegman. Clustering appearances of objects under varying
illumination conditions. In 2003 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, 2003. Proceedings., volume 1, pages I–I. IEEE, 2003.
14C.-K. Hsieh, L. Yang, Y. Cui, T.-Y. Lin, S. Belongie, and D. Estrin. Collaborative metric learning. In
Proceedings of the 26th international conference on world wide web, pages 193–201, 2017.
P. J. Huber. Robust estimation of a location parameter. The Annals of Mathematical Statistics, pages
73–101, 1964.
L.Jain,K.G.Jamieson,andR.Nowak. Finitesamplepredictionandrecoveryboundsforordinalembedding.
Advances in neural information processing systems, 29, 2016.
K. G. Jamieson and R. Nowak. Active ranking using pairwise comparisons. Advances in neural information
processing systems, 24, 2011.
A. J. Jeckmans, M. Beye, Z. Erkin, P. Hartel, R. L. Lagendijk, and Q. Tang. Privacy in recommender
systems. Social media retrieval, pages 263–281, 2013.
M. Kleindessner and U. von Luxburg. Kernel functions based on triplet comparisons. Advances in neural
information processing systems, 30, 2017.
B. Kulis et al. Metric learning: A survey. Foundations and Trends® in Machine Learning, 5(4):287–364,
2013.
Y. M. Lu and M. N. Do. A theory for sampling signals from a union of subspaces. IEEE transactions on
signal processing, 56(6):2334–2345, 2008.
Y. Ma, A. Y. Yang, H. Derksen, and R. Fossum. Estimation of subspace arrangements with applications in
modeling and segmenting mixed data. SIAM review, 50(3):413–458, 2008.
B. Mason, L. Jain, and R. Nowak. Learning low-dimensional metrics. Advances in neural information
processing systems, 30, 2017.
A. K. Massimino and M. A. Davenport. As you like it: Localization via paired comparisons. The Journal of
Machine Learning Research, 22(1):8357–8395, 2021.
B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by
V1? Vision research, 37(23):3311–3325, 1997.
L. Parsons, E. Haque, and H. Liu. Subspace clustering for high dimensional data: a review. Acm sigkdd
explorations newsletter, 6(1):90–105, 2004.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,andE.Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,
J. Clark, et al. Learning transferable visual models from natural language supervision. In International
conference on machine learning, pages 8748–8763. PMLR, 2021.
S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. science, 290
(5500):2323–2326, 2000.
A. Sanakoyeu, V. Tschernezki, U. Buchler, and B. Ommer. Divide and conquer the embedding space for
metric learning. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition,
pages 471–480, 2019.
M. Schultz and T. Joachims. Learning a distance metric from relative comparisons. Advances in neural
information processing systems, 16, 2003.
G. W. Stewart. The efficient generation of random orthogonal matrices with an application to condition
estimators. SIAM Journal on Numerical Analysis, 17(3):403–409, 1980.
15O. Tamuz, C. Liu, S. Belongie, O. Shamir, and A. T. Kalai. Adaptively learning the crowd kernel. In
Proceedings of the 28th International Conference on Machine Learning, pages 673–680, 2011.
G. Tatli, Y. Chen, and R. K. Vinayak. Learning populations of preferences via pairwise comparison queries.
In ICML 2023 Workshop The Many Facets of Preference-Based Learning, 2023.
J.B.Tenenbaum,V.d.Silva,andJ.C.Langford. Aglobalgeometricframeworkfornonlineardimensionality
reduction. science, 290(5500):2319–2323, 2000.
L.VanDerMaatenandK.Weinberger. Stochastictripletembedding. In2012IEEEInternationalWorkshop
on Machine Learning for Signal Processing, pages 1–6. IEEE, 2012.
N.VermaandK.Branson. Samplecomplexityoflearningmahalanobisdistancemetrics. Advances in neural
information processing systems, 28, 2015.
A. Xu and M. Davenport. Simultaneous preference and metric learning from paired comparisons. Advances
in Neural Information Processing Systems, 33:454–465, 2020.
J. Yu, D. Tao, J. Li, and J. Cheng. Semantic preserving distance metric learning and applications. Infor-
mation Sciences, 281:674–686, 2014.
16Supplementary Material for
Metric Learning from Limited Pairwise Preference Comparisons
Outline of the supplementary material
Appendix A Additional algorithms from existing work 18
Appendix B Direct sums of inner product spaces 18
Appendix C Proofs and additional results for Section 3 19
C.1 Generic pairwise relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Appendix D Proofs and additional results for Section 4 22
D.1 An additional result for Section 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D.2 Proofs for Section 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.3 Proof of Proposition 13 from Section 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Appendix E Proofs and additional results for Section 5 26
E.1 Proofs and additional remarks for Theorem 15 . . . . . . . . . . . . . . . . . . . . . . . . . . 26
E.2 Proofs and additional remarks for Proposition 18 . . . . . . . . . . . . . . . . . . . . . . . . . 28
E.3 Auxiliary lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
Appendix F Details and additional results for Section 6 34
F.1 Experimental details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
F.2 Additional experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
17A Additional algorithms from existing work
Algorithm 3 and Algorithm 4 describe the procedures for learning an unknown Mahalanobis distance using
unquantizedmeasurementsfromasingleuserandalargepoolofusers, respectively. SeeSection2of(Canal
et al., 2022).
Algorithm 5 describes the convex optimization problem introduced in Section 3 of (Canal et al., 2022)
for simultaneous metric and preference learning using quantized measurements from multiple users. Here,
ℓ : R → R can be any convex loss function that is L-Lipschitz-continuous. In particular, to achieve the
0+
recoveryguaranteeinProposition18,weassumetheprobabilisticmodelinAssumption17withlinkfunction
f and use the loss function ℓ(z)=−logf(z).
Algorithm 3: Metric learning using unquantized measurements from a single user (Canal et al.,
2022)
Input: A set D =(cid:8) (x ,x ,ψ )(cid:9)m of unquantized measurements from a single user.
i0 i1 i i=1
1 Solve the system of linear equations over symmetric matrices A∈Rd×d and vectors w ∈Rd:
(cid:10) x x⊤ −x x⊤, A(cid:11) +(cid:10) x −x ,w(cid:11) =ψ .
i0 i0 i1 i1 i0 i1 i
Output: Aˆ, the solution to the above linear equations.
Algorithm 4: Metric learning using unquantized measurements from multiple users (Canal et al.,
2022)
Input: A family of D =(cid:8) (x ,x ,ψ )(cid:9)mk of unquantized measurements from users k ∈[K].
k i0;k i1;k i;k i=1
1
Solve the system of linear equations over symmetric matrices A∈Rd×d and vectors
w ,w ,...,w ∈Rd:
1 2 K
(cid:10) x x⊤ −x x⊤ , A(cid:11) +(cid:10) x −x ,w (cid:11) =ψ .
i0;k i0;k i1;k i1;k i0;k i1;k k i;k
Output: Aˆ, the solution to the above linear equations.
Algorithm 5: Metric learning using quantized measurements from multiple users (Canal et al.,
2022)
Input: A family of D =(cid:8) (x ,x ,y )(cid:9)mk of quantized measurements from users k ∈[K];
k i0;k i1;k i;k i=1
hyperparameters ζ ,ζ >0.
M v
1
Solve the convex optimization problem over symmetric matrices A∈Rd×d and vectors
w ,w ,...w ∈Rd:
1 2 K
Mˆ,{vˆ } ← min (cid:88)(cid:88) ℓ(cid:32) y (cid:18)(cid:68) x x⊤ −x x⊤ ,A(cid:69) +(cid:10) x −x ,w (cid:11)(cid:19)(cid:33) (6)
k k
A,{wk}
k k Dk
i;k i0 i0 i1;k i1;k i0;k i1;k k
s.t. A⪰0, ∥A∥ ≤ζ , ∥w ∥ ≤ζ ∀k
F M k 2 v
Output: Mˆ.
B Direct sums of inner product spaces
Inthepaper, wehaveliberallymadeuseofdirectsumsofinnerproductspaces, forexample, Sym(Rd)⊕Rd,
which we treat as an inner product space. It allows us ready access to well-established machinery including
18innerproducts,norms,singularvalues,andpseudoinverses. Thedirectsumofinnerproductspacesisdefined:
Definition B.1. Let (cid:0) V,⟨·,·⟩ (cid:1) and (cid:0) W,⟨·,·⟩ (cid:1) be two inner product spaces. Their directsum is the vector
V W
space V ⊕W equipped with the inner product:
⟨v ⊕w ,v ⊕w ⟩ =⟨v ,v ⟩ +⟨w ,w ⟩ .
1 1 2 2 V⊕W 1 2 V 1 2 W
In particular, this induces the norm on V ⊕W satisfying ∥v⊕w∥2 =∥v∥2 +∥w∥2 .
V⊕W V W
Moore-Penrose pseudoinverse The pseudoinverse can be defined for any map between inner product
spaces:
Definition B.2. Let A:V →W be a linear map between inner product spaces V and W. Let K =ker(A)
and let K⊥ be its orthogonal complement. Let A : K⊥ → Im(A) be the restriction of A to K⊥ and let
K⊥
Π : W → Im(A) be the orthogonal projection onto Im(A). The Moore-Penrose pseudoinverse of A is
Im(A)
the map A+ :W →V given by:
A+ =A−1 ◦Π .
K⊥ Im(A)
Note that A−1 exists by the first isomorphism theorem of algebra.
K⊥
Universal property The following property of direct sum allows us to decompose a linear map A :
V ⊕V →V, which we use in the proof of Theorem 15, when decomposing Π+ :(cid:76) Sym(V )→Sym(Rd).
1 2 λ λ
PropositionB.3(Universalpropertyofthedirectsum,MacLaneandBirkhoff(1999)). LetA:V ⊕V →V
1 2
be a linear map. Then, there exists A :V →V for i=1,2 such that for all v ⊕v ∈V ⊕V ,
i i 1 2 1 2
A(v ⊕v )=A (v )+A (v ).
1 2 1 1 2 2
Schatten norm The Frobenius norm over matrices can be generalized to linear maps between inner
product spaces:
Definition B.4. Let A : V → W be a linear map between finite-dimensional inner product spaces of rank
r. Let σ ≥···≥σ be its nonzero singular values. The 2-Schatten norm ∥A∥ is given by:
1 r 2
r
(cid:88)
∥A∥2 = σ2.
2 i
i=1
In particular, this implies ∥A∥ ≤σ (A)·(cid:112) rank(A).
2 max
Proposition B.5. Let A : V ⊕V → V be a linear map between finite-dimensional inner product spaces.
1 2
Let A :V →V for i=1,2 be given as in Proposition B.3. Then:
i i
∥A∥2 =∥A ∥2+∥A ∥2,
2 1 2 2 2
where ∥·∥ denotes the 2-Schatten norm.
2
C Proofs and additional results for Section 3
For the proof of Theorem 3, we will make use of the notion of a comparison graph over a set of items. Given
preference comparisons from a user, the induced comparison graph is simply the directed graph over items
where two items are connected by an edge if the user has compared them:
Definition C.1. A comparison graph G = (V,E) is a graph whose vertices V = {x ,...,x } is a set
1 N
of items and whose edges E = {(x ,x )}m is a set of item pairs. Its edge-vertex incidence matrix
i0 i1 i=1
19S ∈{−1,0,+1}m×N is defined by:

1 j =i
0
S = −1 j =i
ij 1
0 o.w.
Theorem 3. Fix M ∈ Sym+(Rd) and v k ∈ Rd for each k ∈ N. Let (D k) k∈N be a collection of design
matrices, each for a set of m≤d pairwise comparisons. If each set of compared items has generic pairwise
relations, then for all M′ ∈Sym+(Rd), there exists (v′) ⊂Rd such that:
k k∈N
D (M,v )=D (M′,v′), ∀k ∈N.
k k k k
Proof of Theorem 3. FixM′ ∈Sym+(Rd). Itsufficestoprovetheresultforasingleuser,sincethecovariates
v ’s impose no constraints on each other. Fix a pseudo-ideal point v ∈ Rd. Let D be a design matrix
k
induced by the collection of pairs {(x ,x )}m from a set of items X ={x ,...,x }. We show that when
i0 i1 i=1 1 N
X has generic pairwise relations, then there exists v′ ∈ Rd such D(M,v) = D(M′,v′). By expanding and
rearranging this equation, we obtain a linear system of equations Av′ = b, where A ∈ Rm×d and b ∈ Rm,
and where the ith set of equations is given by:
(x −x )⊤v′ =(cid:10) x x⊤ −x x⊤,M −M′(cid:11) +(cid:10) x −x ,v(cid:11) .
i0 i1 i0 i0 i1 i1 i0 i1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ithrowofA ithentryofb
The Rouché–Capelli theorem states that the system Auˆ = b has a solution if the rank of the augmented
matrix [A|b] is equal to the rank of the design matrix A. If this is the case, then, there is a solution v′ for
any choice of M′ ∈Sym+(Rd).
Tofinishtheproof,weshowthattheranksofAand[A|b]areequal. Tothisend,letS betheedge-vertex
incidence matrix induced by {(x ,x )}m . Define the matrix X ∈ RN×d and vector b′ ∈ RN so that the
i0 i1 i=1
jth row of each is:
X =x⊤ and b′ =(cid:10) x x⊤,M −M′(cid:11) +(cid:10) x −v(cid:11) ,
j j j j j j
so that A = SX and b = Sb′. The items have generic pairwise relations, and so rank(S) = rank(SX) by
Lemma C.2. The augmented matrix [A|b] has the decomposition S[X|b′], so its rank is upper bounded by
rank(S). And because the rank([A|b]) is at least rank(A), we obtain equality, as claimed.
Lemma C.2. Let V ={x ,...,x } be a set of items in Rd, and let X ∈RN×d be its matrix representation,
1 N
so that the jth row is X =x⊤. Let G=(V,E) be a comparison graph with |E|≤d. Let S be its edge-vertex
j j
incidence matrix. If the items have generic pairwise relations, then:
rank(SX)=rank(S).
Proof. Let G′ =(V,E′) be a maximal acyclic subgraph G′ ⊂G, say with m′ edges, and let S′ ∈Rm′×N be
its corresponding edge-vertex incidence matrix. On the one hand, we have:
rank(S′)≥rank(S′X).
On the other, because X has pairwise generic relations and m′ ≤d, we have:
rank(S′X)=dim(cid:0) span(cid:0) {x−x′ :(x,x′)∈E′}(cid:1)(cid:1) =m′ ≥rank(S′).
The first equality is obtained by the definition of rank applied to S′X. The second equality follows from
pairwise genericity. Thus, we have rank(S′)=rank(S′X)≤rank(SX). Furthermore, we claim that:
rank(S)=rank(S′).
It would follow that rank(S)≤rank(SX)≤rank(S), which implies the result.
We prove the claim by showing that for any e ∈ E\E′, the row S is a linear combination of rows S
e e′
where e′ ∈E′. Let e=(x,x′). By the maximality of G′, a cycle containing e is created by including e into
20G′. Thus, there is an undirected path P from x to x′ in G′, where P =(x ,...,x ) satisfies:
0 k
• x =x and x =x′,
0 k
• either (x ,x ) or its reversal (x ,x ) is contained in E′.
i−1 i i i−1
For each i, let e ∈ E′ be one of these edges (x ,x ) or (x ,x ) and let r ∈ {−1,+1} indicate whether
i i−1 i i i−1 i
e was the reversal of (x ,x ). It follows that indeed S is a linear combination of the rows of S′,
i i−1 i e
k
(cid:88)
S = r S .
e i ei
i=1
C.1 Generic pairwise relations
In the next proposition, we show that our notion of generic pairwise relations is a notion of points being in
general position (Matousek, 2013); almost all finite subsets of Rd have pairwise generic relations. Recall:
Definition 2. A set X ⊂Rd has genericpairwiserelations if for any acyclic graph G=(X,E) with at most
d edges, the set {x−x′ :(x,x′)∈E} is linearly independent.
Proposition C.3. Fix N ∈ N. We say that X ∈ RN×d has generic pairwise relations if its rows have
generic pairwise relations. The following set has Lesbegue measure zero:
(cid:8) X ∈RN×d :X is not pairwise generic(cid:9) .
Proof. Let S be the finite collection of all edge-vertex incidence matrices S for acyclic comparison graphs
with at most d edges on N items. Notice that if X ∈ RN×d is not pairwise generic, then there exists some
S ∈S such that SX ∈Rm×d is not full rank. It follows that:
(cid:8) X not pairwise generic(cid:9) = (cid:91) (cid:8) det(SXX⊤S⊤)=0(cid:9) .
S∈S
The zero set {det(SXX⊤S⊤)=0} of a non-zero polynomial has Lebesgue measure zero, by Sard’s theorem.
The finite union of measure zero sets also has measure zero.
The concept of general linear position is a standard notion of general position. We present the definition
in a way to highlight its relationship to pairwise genericity. Recall that a star graph is a tree with a root
vertex connected to all other vertices.
Definition C.4. Let X be a subset of Rd. We say that X is in general linear position if for any star graph
G=(V,E) with at most d edges on V ⊂X, the set {x−x′ :(x,x′)∈E} is linearly independent.
Because star graphs are acyclic graphs, the following is immediate:
Proposition C.5. If X has generic pairwise relations, then X is in general linear position.
On the other hand, the converse is not necessarily true. As we can see from the following example,
having pairwise generic relations is a strictly stronger condition than being in general linear position; see
also Figure C.1 for an illustration.
Example C.6. Consider the following points in R2:
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
0 1 1 2
x = x = x = x = .
1 0 2 0 3 1 4 1
This collection of points is in general linear position, since no three points are collinear. However, these
points do not have generic pairwise relations. We have:
x −x =x −x .
2 1 4 3
21(1,1) (2,1) (2,1)
(0,0.8)
(0,0) (1,0) (0,0) (1,0)
(a) (b)
FigureC.1: (a)IllustrationofExampleC.6. Thesetoffourpointsisingenerallinearposition, butdoesnot
have generic pairwise relations. (b) A set of four points that has generic pairwise relations; it must also be
in general linear position.
D Proofs and additional results for Section 4
D.1 An additional result for Section 4.1
Proposition D.1. There is a one-to-one correspondence between Sym+(V) and Mahalanobis distances on
V. In particular, ρ :V ×V →R is a Mahalanobis distance if and only if there exists some Q∈Sym+(V)
V
such that:
(cid:113)
ρ (x,x′)= (x−x′)BQB⊤(x−x′).
V
Moreover, Q is unique. We say that Q is the matrix representation of the Mahalanobis distance ρ . If ρ
V V
is the subspace metric on V of a Mahalanobis distance ρ on Rd with representation M ∈Sym+(Rd), then:
Q=Π (M).
V
Proof. (=⇒). Suppose that ρ is a Mahalanobis distance on V. We show that it has a representation in
V
Sym+(V). By definition, there exists a Mahalanobis distance ρ on Rd such that:
(cid:12)
ρ
V
=ρ(cid:12) V.
Let M be the matrix representation of ρ and let Q=Π (M)∈Sym+(V). Then:
V
(cid:113)
ρ (x,x′)= (x−x′)⊤M(x−x′)
V
(cid:113)
= (x−x′)⊤BB⊤MBB⊤(x−x′)
(cid:113)
= (x−x′)⊤BQB⊤(x−x′),
where the first equality expands the equality ρ (x,x′) = ρ(x,x′), the second uses the fact that BB⊤x = x
V
for all x∈V since B ∈Rd×r is an orthonormal basis, and the third equality is uses the definition of Π .
V
Toproveuniqueness,supposethatQ,Q′ ∈Sym+(V)representρ . WeclaimthatQ=Q′. Toshowthis,
V
it suffices to prove that for all z ∈Rr,
(cid:10) Q−Q′,zz⊤(cid:11)
=0.
This is because the collection {zz⊤ :z ∈Rr} spans all (r×r)-symmetric matrices. To this end, fix z ∈Rr.
We take x,x′ ∈V ⊂ by setting x=Bz and x′ =0. We have:
(cid:113) (cid:112)
ρ (x,x′)= (x−x′)⊤BQB⊤(x−x′)= z⊤Qz.
V
ThesameequationholdsforQ′ sincebothrepresentρ . Squaringbothequationsandtakingtheirdifference
V
22shows that ⟨Q−Q′,zz⊤⟩=0, as desired. Thus, Q=Q′ and the matrix representation of ρ is unique.
V
(⇐=). Let Q∈Sym+(V). We can extend the orthonormal basis B of V to an orthonormal basis of Rd.
In particular, let B ∈Rd×(d−r) be an orthonormal basis of the orthogonal complement of V. Set:
⊥
M =B B⊤+BQB⊤,
⊥ ⊥
so that M ∈ Sym+(Rd) is positive-definite. Let ρ be the Mahalanobis distance on Rd represented by M.
Then, the Mahalanobis distance ρ(cid:12) (cid:12) on V has representation:
V
Π (M)=B⊤MB =B⊤B B⊤B+B⊤BQB⊤B =Q,
V ⊥ ⊥
which shows that each Q∈Sym+(Rd) corresponds to a Mahalanobis distance on V.
D.2 Proofs for Section 4.2
Lemma 8. Let V be an r-dimensional subspace of Rd with a canonical representation given by B ∈ Rd×r.
Fix any Mahalanobis distance M ∈Sym+(Rd), any pair of items x,x′ ∈Rd, and ideal point u∈Rd. Suppose
that x and x′ are contained in V with canonical representation x =B⊤x and x′ =B⊤x′ in Rr. Then:
V V
ψ (cid:0) x,x′;u(cid:1) =ψ (cid:0) x ,x′ ;u (cid:1) ,
M Q V V V
where the phantom ideal point u of u on V satisfies (B⊤MB)u =B⊤Mu, and Q=Π (M) is the matrix
V V V
representation in Sym+(V) of the subspace metric ρ(cid:12)
(cid:12)
.
V
Proof. Let v = −2Mu and v = −2Qu be the pseudo-ideal user points for u and u , respectively. The
V V V
following shows that v is given by the canonical representation of the orthogonal projection of v to V,
V
v =−2B⊤MB(B⊤MB)−1B⊤Mu=−2B⊤Mu=B⊤v.
V
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
Q uV
We now expand the definitions of ψ and ψ ,
M Q
ψ
(x,x′;u)( =i)(cid:68) xx⊤−x′x′⊤ ,M(cid:69) +(cid:68) x−x′,v(cid:69)
M
( =ii)(cid:68) BB⊤(xx⊤−x′x′⊤ )BB⊤,M(cid:69) +(cid:68) BB⊤(x−x′),v(cid:69)
(i =ii)(cid:68) B⊤xx⊤B−B⊤x′x′⊤ B,B⊤MB(cid:69) +(cid:68) B⊤x−B⊤x′,B⊤v(cid:69)
( =iv)(cid:68) x x ⊤ −x′ x′ ⊤ ,Q(cid:69) +(cid:10) x −x′ ,v (cid:11)
V V V V V V V
( =v) ψ (x ,x′ ;u ),
Q V V V
where(i)and(v)followbydefinition,(ii)usesthefactthatasB ∈Rd×r isanorthonormalbasis,BB⊤v =v
for all v ∈ V, (iii) applies the following property for the trace inner product ⟨BA,C⟩ = tr(C⊤BA) =
⟨A,B⊤C⟩, and (iv) rewrites the equation in terms of the canonical representations.
Proposition10. LetX quadraticallyspanasubspaceV ofdimensionr. ThereexistsacollectionD ,...,D
1 K
of design matrices, each over m pairs of items in X, such that given a (distinct) user’s response to each
design, ρ(cid:12) (cid:12) can be identified when m≥r+1 and K ≥r(r+1)/2.
V
Proof. By Lemma 8, it suffices to prove the result for V = Rd. We show that if X quadratically spans Rd,
then we can construct an (m,K)-experimental design where m=d+1 and K =d(d+1)/2 such that there
isauniquematrixconsistentwithalluserresponses. LetD =d+d(d+1) bethedimensionofSym(Rd)⊕Rd.
2
Since X quadratically spans V, there exists a collection of pairs {(x ,x )}D such that:
i0 i1 i=1
span(cid:0)(cid:8) ∆ ⊕δ :i∈[D](cid:9)(cid:1) = Sym(Rd)⊕Rd, (7)
i i
23where we let ∆ =x x⊤ −x x⊤ and δ =x −x . In particular, the collection {∆ ⊕δ } is linearly
independent. Wi ithoui0 t i l0oss ofi1 gei n1eralityi , we mi0 ay si e1 lect these so that the first d pairwi ise i dii ff∈ e[D re] nces δ are
i
also linearly independent:
span(cid:0) {δ :i∈[d]}(cid:1) =Rd.
i
We will ask all users to compare the first d pairs and one additional pair, unique to the user. In particular,
set the kth collection of preference comparison queries by:
D =(cid:8) (x ,x ):i∈I (cid:9) , where I =[d]∪{d+k}.
k i0 i1 k k
First, we show that the responses from a single user must reveal at least one dimension of Sym(Rd). To
see this, let’s fix a user k ∈[K]. From Equation (7), we can define the vector (α :i∈I ) so that:
i,k k
(cid:88)
α =1 and α δ =0.
d+k;k i;k i
i∈Ik
Therefore,fromthepreferencemeasurements,wededucethatatleastonedegreeoffreedomofM isrevealed:
(cid:42) (cid:43)
(cid:88) α ψ = (cid:88) α (cid:10) ∆ ,M(cid:11) + (cid:88) α (cid:10) δ ,v (cid:11) = (cid:88) α ∆ ,M . (8)
i;k i;k i;k i i;k i k i;k i
i∈Ik i∈Ik i∈Ik i∈Ik
(cid:124) (cid:123)(cid:122) (cid:125)
⟨0,vk⟩
We now claim that each user reveals a different degree of freedom of M. In particular, it suffices to show
that the following collection of matrices spans Sym(Rd),
 
(cid:88) 
α ∆ :k ∈[K] .
i;k i
 
i∈Ik
Suppose otherwise. Since K = d(d+1), this means that this collection of matrices are linearly dependent,
2
and that there exists a non-zero vector (µ :k ∈[K]) such that 0∈Sym(Rd) is the linear combination:
k
(cid:88) (cid:88)
µ α ∆ =0.
k i;k i
k∈[K] i∈Ik
Becausewechoseα =1foreachuserk ∈[K],thisimpliesthatzeroinSym(Rd)⊕Rd isalsoanon-trivial
d+k;k
linear combination of the collection ∆ ⊕δ , where:
i i
 
d D
(cid:88) (cid:88) (cid:88)
 µ kα i;k∆ i⊕δ i+ µ i−d·∆ i⊕δ i =0.
i=1 k∈[K] i=d+1
But then this collection is not full rank and cannot span Sym(Rd)⊕Rd, as assumed in Equation (7). It
follows that M is the unique solution to the system of linear equations corresponding to Equation (8).
Proposition 11. Let (D k) k∈N be a set of design matrices over items in X ⊂V. If X does not quadratically
span V, then infinitely many Mahalanobis distances on V are consistent with any set of user responses to
the design matrices.
Proof. Because X does not quadratically span V, there exists an element Q ⊕v ∈ Sym(V)⊕V such
V ⊥ ⊥
that:
(cid:68)(cid:0) x x⊤−x′ x′⊤(cid:1) ⊕(x−x′),Q ⊕v (cid:69) =0,
V V V V ⊥ ⊥
for all x,x′ ∈X , where x =B⊤x and x′ =B⊤x′. Let M =BQB⊤, so that:
V V V ⊥
(cid:68)(cid:0) xx⊤−x′x′⊤(cid:1) ⊕(x−x′),M ⊕v (cid:69) =0, for all x,x′ ∈X . (9)
⊥ ⊥ V
24(a) (b)
Figure D.1: (a) Illustrates the number of subspaces needed to reconstruct a high-dimensional ellipsoid from
itsintersectionswithlow-dimensionalsubspaces. InR2,weneed3pointsondistinct1-dimensionalsubspaces
to possibly recover an ellipse centered at the origin. (b) When we cannot exactly identify where the high-
dimensionalellipsoidintersectswitheachsubspace,wemaystillfitanellipsoidfromapproximateestimations
using least squares (Gander et al., 1994).
We claim that if M ∈ Sym+(Rd) is consistent with the kth user’s responses D = {(x ,x ,ψ )}m ,
k i0;k i1;k i;k i=1
then the matrix M +λM is also consistent, provided that M +λM remains in Sym+(Rd). In particular,
⊥ ⊥
if M is consistent, there exists an ideal point u so that for all i∈[m]:
k
ψ =ψ (x ,x ;u )( =i)(cid:68)(cid:0) x x⊤ −x x⊤(cid:1) ⊕(x −x ),M ⊕v (cid:69)
i;k M i0 i1 k i0 i0 i1 i1 i0 i1 k
( =ii)(cid:68)(cid:0) x x⊤ −x x⊤(cid:1) ⊕(x −x ),M ⊕v +λM ⊕v (cid:69)
i0 i0 i1 i1 i0 i1 k ⊥ ⊥
(iii)
= ψ (x ,x ;λu˜ ),
M+λM⊥ i0 i1 k
where (i) expands the definition of ψ while setting the pseudo-ideal point to v = −2Mu , (ii) applies
M k k
Equation (9), and (iii) applies the definition of ψ while setting u˜ =−1M−1(v +v ).
M+M⊥ k 2 k ⊥
Thus, if M is the matrix representation of the underlying Mahalanobis distance, the following matrices
are also consistent:
(cid:26) (cid:27)
σ (M)
M +λM :0≤λ< min ,
⊥ σ (M )
max ⊥
where σ (M ) is the maximum singular value of M while σ (M) is the minimum singular value of M;
max ⊥ ⊥ min
this implies that M +λM is positive-definite. Infinitely such λ’s exist because (a) σ (M )<∞ is finite
⊥ max ⊥
and (b) σ (M)>0 is bounded away from zero because M is positive-definite.
min
D.3 Proof of Proposition 13 from Section 4.3
Proposition 13. Let ρ be a Mahalanobis distance on Rd. Let (V ) be a collection of subspaces with
λ λ∈Λ
canonical representations given by the orthonormal bases (B ) . The following are equivalent:
λ λ∈Λ
1. (cid:8) xx⊤ :x∈V ,λ∈Λ(cid:9) spans Sym(Rd).
λ
(cid:77)
2. Let Π be given by Equation (3). The linear map Π:Sym(Rd)→ Sym(V ) is injective, where:
Vλ λ
λ∈Λ
(cid:77)
Π(A)= Π (A).
Vλ
λ∈Λ
3. If ρˆis a Mahalanobis distance such that ρˆ(cid:12)
(cid:12)
=ρ(cid:12)
(cid:12)
for all λ∈Λ, then ρˆ=ρ.
Vλ Vλ
Proof. (1=⇒2). Suppose span(cid:8) xx⊤ :x∈V ,λ∈Λ(cid:9) =Sym(Rd). To show that Π is injective, it suffices to
λ
show that its kernel is trivial. Let M ∈ker(Π). We claim that for any λ∈Λ and x∈V , we have:
λ
(cid:10) xx⊤,M(cid:11) =0. (10)
25Assume this for now. Then, M ∈Sym(Rd)=span(cid:8) xx⊤ :x∈V ,λ∈Λ(cid:9), so that ⟨M,M⟩=0. This implies
λ
that M =0, so the kernel is trivial. We now show Eq. (10). Using the definition of Π , when M ∈ker(Π),
Vλ
we have:
Π (M)=B⊤MB =0. (11)
Vλ λ λ
Say that dim(V λ)=r
λ
and x∈V λ. As B
λ
∈Rd×rλ is a basis of V λ, there exists z ∈Rrλ such that x=B λz.
By Eq. (11),
(cid:10) xx⊤,M(cid:11) =z⊤B⊤MB z =0.
λ λ
(2 =⇒ 1). We prove the contrapositive. Suppose that S = span(cid:8) xx⊤ :x∈V ,λ∈Λ(cid:9) does not span
λ
Sym(Rd). Then, there exists some nonzero A ∈ S⊥ in its orthogonal complement. To show that Π is not
injective, we show that A∈ker(Π). That is, for all λ∈Λ, that B⊤AB =0. We do this by proving that all
λ λ
eigenvalues of B⊤AB are zero.
λ λ
Let v ∈Rrλ be any unit eigenvector of B λ⊤AB
λ
and α be the corresponding eigenvalue, so that:
α=v⊤B⊤AB v =(cid:10) xx⊤,A(cid:11) ,
λ λ
where x=B v is an element of V . But because A∈S⊥, this implies that the eigenvalue is zero, α=0.
λ λ
(2 =⇒ 3). Let M and Mˆ be the matrix representations of ρ and ρˆ, respectively. By assumption, their
subspace metrics coincide over (V ) , so Proposition D.1 implies:
λ λ
Π (M)=Π (Mˆ).
Vλ Vλ
And as Π is injective, we must have M =Mˆ, so that ρ=ρˆ.
(3 =⇒ 2). We prove that Π is injective by showing that its kernel is trivial. Let A ∈ ker(Π). Then, let
c,cˆ>∥A∥ and define M =c−1A+I and Mˆ =cˆ−1A+I, which are positive-definite by construction. Let
op
ρ and ρˆ be their corresponding Mahalanobis distances. Their subspace metrics on all V ’s coincide, since
λ
A∈ker(Π),
Π(M)=Π(c−1A+I)=Π(I)=Π(cˆ−1A+I)=Π(Mˆ).
And so, by assumption ρ = ρˆ. But as the matrix representation of a Mahalanobis distance is unique
(Proposition D.1), this implies that M =Mˆ, proving that A=0.
E Proofs and additional results for Section 5
E.1 Proofs and additional remarks for Theorem 15
Theorem 15. Let Rd have a Mahalanobis distance with matrix representation M ∈Sym+(Rd). Let X ⊂Rd
be subspace-clusterable over subspaces V indexed by λ ∈ Λ, where |Λ| = n. Let Mˆ be the estimator of M
λ
and let Qˆ be the estimator of the subspace metric Q for each λ learned from Algorithm 2. Suppose there
λ λ
exist γ ≤ε such that (cid:13) (cid:13)E(cid:2) Qˆ λ(cid:3) −Q λ(cid:13) (cid:13)
F
≤γ and (cid:13) (cid:13)Qˆ λ−Q λ(cid:13) (cid:13)
F
≤ε for each λ. Fix p∈(0,1]. Then, there is a
universal constant c>0 such that with probability at least 1−p,
 (cid:115) 
(cid:13) (cid:13)Mˆ −M(cid:13) (cid:13) ≤c· 1 γ√ n+εd log2d ,
F σ (Π) p
min
where σ >0 is the least singular value of Π.
min
Proof of Theorem 15. Let c = 2c where c is a universal constant to be defined later. Recall from Eq. (5)
0 0
that Mˆ minimizes ∥A−Mˆ ∥ over all A∈Sym+(Rd). Since M is also contained in Sym+(Rd), we have:
LS F
(cid:13) (cid:13)Mˆ −Mˆ LS(cid:13) (cid:13)
F
≤(cid:13) (cid:13)M −Mˆ LS(cid:13) (cid:13) F.
By the triangle inequality,
(cid:13) (cid:13)Mˆ −M(cid:13) (cid:13)
F
≤(cid:13) (cid:13)Mˆ −Mˆ LS(cid:13) (cid:13) F+(cid:13) (cid:13)Mˆ LS−M(cid:13) (cid:13)
F
≤2(cid:13) (cid:13)Mˆ LS−M(cid:13) (cid:13) F.
26Therefore, it suffices to show that, with probability 1−δ,
(cid:32) (cid:114) (cid:33)
(cid:13) (cid:13)Mˆ LS−M(cid:13) (cid:13)
F
≤c 0·
σ
1
(Π)
γ√ m+εd log2 δd . (12)
min
Before proving Eq. (12), we introduce some notation.
Notation and facts For each subspace V , we denote the recovery error by:
λ
(cid:16) (cid:17) (cid:16) (cid:17)
E =Qˆ −Q = E[Qˆ ]−Q + Qˆ −E[Qˆ ] ,
λ λ λ λ λ λ λ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Hλ (bias) ξλ (noise)
whichwedecomposeintoabiastermH :=E[Qˆ ]−Q andanoisetermξ :=Qˆ −E[Qˆ ]. Byassumption,
λ λ λ λ λ λ
∥H ∥ ≤γ and E[ξ ]=0, ∥ξ ∥ ≤∥E ∥ ≤ε.
λ F λ λ F λ F
(cid:16) (cid:17)
Let H :=(cid:76) H , ξ :=(cid:76) ξ , and E :=H+ξ. Thus, E =(cid:76) Qˆ −Q , by the above bias/noise
λ∈Λ λ λ∈Λ λ λ∈Λ λ λ
(cid:113) √
decomposition. In addition, since ∥H ∥ ≤γ, we have ∥H∥= (cid:80) ∥H ∥2 ≤ mγ.
λ F λ∈Λ λ F
We now prove Eq. (12). Recall from Eq. (4) that Mˆ is the least-squares solution, so that:
LS
Mˆ −M =Π+(E)
LS
=Π+(H +ξ), (13)
where Π+ : (cid:76) Sym(V ) → Sym(Rd) denotes the Moore–Penrose inverse of Π (see Definition B.2). It
λ∈Λ λ
then follows from Eq. (13) and the triangle inequality that
(cid:13) (cid:13)Mˆ LS−M(cid:13) (cid:13)
F
≤(cid:13) (cid:13)Π+(H)(cid:13) (cid:13) F+(cid:13) (cid:13)Π+(ξ)(cid:13) (cid:13) F. (14)
By Proposition 13, the map Π is injective since X is subspace-clusterable. Thus, σ (Π)>0, and:
min
(cid:13) (cid:13)Π+(H)(cid:13) (cid:13) F ≤ σ 1 (Π)∥H∥ F ≤ σ 1 (Π)γ√ m. (15)
min min
It then follows from Eq. (14) and Eq. (15) that, to prove Eq. (12), it suffices to show that, with probability
at least 1−δ,
(cid:32) (cid:114) (cid:33)
(cid:13) (cid:13)Π+(ξ)(cid:13) (cid:13)
F
≤c 0·
σ
1
(Π)
εd log2 δd . (16)
min
By the universal property of the direct sum (see Proposition B.3), there exist Π+ :Sym(V )→Sym(Rd)
λ λ
for each λ∈Λ, such that
(cid:88)
Π+(ξ)= Π+(ξ ).
λ λ
λ∈Λ
Observe that
1. Each ξ is from subspace V ; and thus, ξ ’s and Π+(ξ )’s across subspaces are independent,
λ λ λ λ λ
2. E(cid:104) Π+(ξ )(cid:105) =Π+(cid:0)E[ξ ](cid:1) =0; and,
λ λ λ λ
3. (cid:13) (cid:13)Π+ λ(ξ λ)(cid:13) (cid:13)
F
≤(cid:13) (cid:13)Π+ λ(cid:13) (cid:13) op·(cid:13) (cid:13)ξ λ(cid:13) (cid:13)
F
≤(cid:13) (cid:13)Π+ λ(cid:13) (cid:13) 2·ε,
where ∥·∥ denotes the 2-Schatten norm (see Definition B.4).
2
27Corollary E.4 gives a Hoeffding-style concentration inequality for independent sub-Gaussian random
matrices. Applied here, it states that there exists a universal constant c such that, with probability 1−δ,
0
(cid:13) (cid:13) (cid:13)(cid:88) (cid:13)
(cid:13)Π+(ξ)(cid:13) =(cid:13) Π+(ξ )(cid:13)
(cid:13) (cid:13) (cid:13) λ λ (cid:13)
F F
λ∈Λ
(cid:115)
( ≤i) c 0· (cid:88)(cid:13) (cid:13)Π+ λ(cid:13) (cid:13)2 2·ε2log2 δd
λ∈Λ
(cid:114)
( =ii)
c
0·(cid:13) (cid:13)Π+(cid:13)
(cid:13) 2·ε
log2 δd
(cid:114)
(iii) 1 2d
≤ c · ·εd log , (17)
0 σ (Π) δ
min
where(i)appliesthethirdobservationfromabove,(ii)appliesPropositionB.5about2-Schattennorms, and
(iii) uses the following facts:
• ∥Π+∥ ≤σ (Π+)·(cid:112) rank(Π+), (see Definition B.4),
2 max
1
• σ (Π+)= ,
max σ (Π)
min
• rank(Π+)≤ d(d+1) ≤d2.
2
E.2 Proofs and additional remarks for Proposition 18
Proposition 18 (Theorem 4.1, Canal et al. (2022)). Suppose that Rr has a Mahalanobis distance with
representation Q ∈ Sym+(Rr) where ∥Q∥ ≤ ζ . Let each user k ∈ [K] have pseudo-ideal point v ∈ Rr
F M k
where v ≤ ζ . Let P be a distribution over designs of size m over Rr (Definition 1). For each user,
k v m
let D ∼ P be an i.i.d. random design, and let D = {(x ,x ,y )} be the user’s responses under
k m k i0 i1 i;k i∈[m]
Assumption 17. Fix p ∈ (0,1]. Given loss function ℓ(z) = −logf(z), Algorithm 5 returns Qˆ ∈ Sym+(Rr),
where with probability at least 1−p,
(cid:115)
∥Qˆ−Q∥2 ≤
16L (ζ M2 +Kζ v2)log p4
.
F c2 ·σ2 (P ) mK
f min m
Proof. The objective over which the parameters (A,w ,...,w ) is optimized in Eq. (6) of Algorithm 5 can
1 K
be written as:
Rˆ(A,w ,...,w )= (cid:88) (cid:88) −logf(cid:0) y ·D (A,w )(cid:1) .
1 K i;k i;k k
k∈[K]i∈[m]
Let(Qˆ,vˆ ,...,vˆ )bethesolutionrecoveredinthisstepofAlgorithm5. Theexcessriskoftheseparameters
1 K
isdefinedtobehowmuchworseinexpectationtheparametersareatexplainingobserveddatacomparedto
28the true parameters (Q,v ,...,v ) that generated the data. The excess risk leads to a bound on ∥Qˆ−Q∥2,
1 K F
E(cid:2) Rˆ(Qˆ,vˆ ,...,vˆ )(cid:3) −E(cid:2) Rˆ(Q,v ,...,v )(cid:3)
1 K 1 K
 
( =a) (cid:88) E (cid:88) KL(cid:16) f(cid:0) D i;k(Q,v k)(cid:1)(cid:13) (cid:13) (cid:13)f(cid:0) D i;k(Qˆ,vˆ k)(cid:1)(cid:17) 
Dk∼Pm
k∈[K] i∈[m]
( ≥b) 2c2 (cid:88) E (cid:13) (cid:13)D (cid:0) Qˆ−Q,vˆ −v (cid:1)(cid:13) (cid:13)2
f (cid:13) k k k (cid:13)
Dk∼Pm
k∈[K]
( ≥c) 2c2 (cid:88) m·σ2 (P )·(cid:16) ∥Qˆ−Q∥2 +∥vˆ −v ∥2(cid:17)
f min m F k k
k∈[K]
≥ 2mKc2 ·σ2 (P )·∥Qˆ−Q∥2, (18)
f min m F
where each inequality is justified below. We just need to show that the excess risk of Qˆ returned by the
algorithmhassmallexcessrisk. LemmaE.1approachesthisviaastandardgeneralizationargument,showing
that with probability at least 1−δ,
E(cid:2) Rˆ(Qˆ,vˆ ,...,vˆ )(cid:3) −E(cid:2) Rˆ(Q,v ,...,v )(cid:3)
1 K 1 K
(cid:114)
4
≤Rˆ(Qˆ,vˆ ,...,vˆ )−Rˆ(Q,v ,...,v ) +32L mK(ζ2 +Kζ2)log , (19)
1 K 1 K M v δ
(cid:124) (cid:123)(cid:122) (cid:125)
≤0
where the indicated difference is less than zero because (Qˆ,vˆ ,...,vˆ ) is the minimizer of Rˆ. The result is
1 k
obtained by combining Eqs. (18) and (19). To finish the prove, we justify the above inequalities:
(a) Recall that Pr[Y =+1]=f(D (Q,v )). Because f(z)=1−f(−z), we also have that:
i;k i;k k
Pr[Y =−1]=1−f(D (Q,v ))=f(−D (Q,v )).
i;k i;k k i;k k
Therefore, Pr[Y =y]=f(y·D (Q,v )). It follows that the excess risk is equal to:
i;k i;k k
E(cid:2) Rˆ(Qˆ,vˆ ,...,vˆ )(cid:3) −E(cid:2) Rˆ(Q,v ,...,v )(cid:3)
1 K 1 K
 (cid:32) (cid:33)
=
(cid:88)
E
(cid:88)
−log
f(Y i;k·D i;k(Q,v k))

k∈[K]Dk,Y
i∈[m]
f(Y i;k·D i;k(Qˆ,vˆ k))
 (cid:32) (cid:33)
= k(cid:88) ∈[K]DE k i(cid:88) ∈[m]y∈{(cid:88) −1,+1}−f(y·D i;k(Q,v k))log f f( (y y· ·D Di i; ;k k( (QQ ˆ,, vv
ˆ
kk )) )) ,
where we obtain the equality (a) by applying the definition KL(p∥q),
p 1−p
KL(p∥q)=plog +(1−p)log .
q 1−q
29(b) The following is the same argument used in (Canal et al., 2022, Proposition E.3).
(cid:88) KL(cid:16) f(cid:0) D (Q,v )(cid:1)(cid:13) (cid:13)f(cid:0) D (Qˆ,vˆ )(cid:1)(cid:17) ≥2 (cid:88) (cid:16) f(cid:0) D (Q,v )(cid:1) −f(cid:0) D (Qˆ,vˆ )(cid:1)(cid:17)2
i;k k (cid:13) i;k k i;k k i;k k
i∈[m] i∈[m]
≥2c2 (cid:88) (cid:16) D (Q,v )−D (Qˆ,vˆ )(cid:17)2
f i;k k i;k k
i∈[m]
=2c2 (cid:88) (cid:16) D (Qˆ−Q,vˆ −v )(cid:17)2
f i;k k k
i∈[m]
(cid:13) (cid:13)2
=2c2(cid:13)D (Qˆ−Q,vˆ −v )(cid:13) ,
f(cid:13) k k k (cid:13)
where the first inequality comes from KL(p∥q) ≥ 2(p−q)2, see (Mason et al., 2017, Lemma 5.2), the
second uses the monotonicity of f and the lower bound of f′, the third applies linearity of D , and
i;k
the fourth just rewrites the sum in terms of the squared ℓ -norm over Rm.
2
(c) Recall that σ2(P )= 1 ·σ (E[D∗D]) when D ∼P . Let X =(Qˆ−Q)⊕(vˆ −v ) for short. Then,
m m min m k k
E(cid:13) (cid:13)D (Qˆ−Q,vˆ −v )(cid:13) (cid:13)2 =E(cid:10) D X,D X(cid:11)
(cid:13) k k k (cid:13) k k
=X⊤E[D∗D ]X
k k
≥σ (E[D∗D ])·∥X∥2
min k k
=m·σ2 (P )·(cid:0) ∥Qˆ−Q∥2 +∥vˆ −v ∥2(cid:1) ,
min m F k k
where the inequality applies the variational characterization of the minimum singular value.
Lemma E.1. Let δ ∈ (0,1). Given the assumptions of Proposition 18, Eq. (19) holds with probability at
least 1−δ.
Proof. For short, let Θ ⊂ Sym+(Rr)⊕Rr×K denote the set of parameters θ ≡ (A,w ,...,w ) such that
1 K
A∈Sym+(Rr)with∥A∥ ≤ζ andw ∈Rr with∥w ∥≤ζ . Weclaimthatwithprobabilityatleast1−δ,
F M k k v
we have uniform convergence:
(cid:114)
sup (cid:12) (cid:12)Rˆ(θ)−E(cid:2) Rˆ(θ)(cid:3)(cid:12) (cid:12)≤16L mK(ζ2 +Kζ2)log4 . (20)
(cid:12) (cid:12) M v δ
θ∈Θ
Before proving this, notice that this implies Eq. 19. In particular, let θˆ correspond to the parameters
(Qˆ,vˆ ,...,vˆ ) and let θ correspond to (Q,v ,...,v ). Then we have that with probability at least 1−δ,
1 K 1 K
both Rˆ(θˆ) and Rˆ(θ) are close to their expected values, each contributing at most the right-hand side of
Eq. (20):
(cid:114)
E(cid:2) Rˆ(θˆ)(cid:3) −E(cid:2) Rˆ(θ)(cid:3) ≤Rˆ(θˆ)−Rˆ(θ)+32L mK(ζ2 +Kζ2)log4 .
M v δ
In the remainder of the proof, we show Eq. (20). For any θ ∈ Θ, consider the empirical risk Rˆ(θ). We
claim that the risk contribution by the ith comparison by the kth user is a bounded random variable,
(cid:12) (cid:12)
(cid:12) (cid:0) (cid:0) (cid:1)(cid:1) 1(cid:12)(a)
(cid:12) (cid:12)−log f Y i;k·D i;k(A,w k) +log 2(cid:12)
(cid:12)
≤ 2L(ζ M +ζ v).
Let us verify this claim later. For now, the bounded difference inequality (reproduced below as Lemma E.5)
30implies that with probability at least 1−δ,
(cid:34) (cid:35) (cid:114)
sup (cid:12) (cid:12)Rˆ(θ)−E(cid:2) Rˆ(θ)(cid:3)(cid:12) (cid:12)≤E sup (cid:12) (cid:12)Rˆ(θ)−E(cid:2) Rˆ(θ)(cid:3)(cid:12) (cid:12) +4L(ζ +ζ ) 2mKlog2 . (21)
(cid:12) (cid:12) (cid:12) (cid:12) M v δ
θ∈Θ θ∈Θ
To bound the expectation term, let us combine each user’s random design matrix D into a single (m,K)-
k
experimental design matrix D :Sym(Rr)⊕Rr×K →Rm×K, so that it is the following linear map:
D(A,w ,...,w ) =D (A,w ).
1 K i;k i;k k
Let D∗ : Rm×K → Sym(Rr)⊕Rm×K be its adjoint. Let ϵ ∈ {−1,+1}m×K be an array of independent
R
Rademacher random variables, so that ϵ is equal to −1 or +1 uniformly at random. Then:
i;k
 (cid:12) (cid:12)
E(cid:34) sup (cid:12) (cid:12) (cid:12)Rˆ(θ)−E(cid:2) Rˆ(θ)(cid:3)(cid:12) (cid:12) (cid:12)(cid:35) ( ≤b) 2E

sup (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:88) (cid:88) ϵ i;k(cid:16) −logf(cid:0) Y i;k·D i;k(A,w k)(cid:1)(cid:17)(cid:12) (cid:12) (cid:12) (cid:12)

θ∈Θ A,w1,...,wK(cid:12)k∈[K]i∈[m] (cid:12)
 (cid:12) (cid:12)
(cid:12) (cid:12)
( ≤c) 2L·E

sup (cid:12) (cid:12)
(cid:12)
(cid:88) (cid:88) ϵ i;k(cid:0) Y i;k·D i;k(A,w k)(cid:1)(cid:12) (cid:12) (cid:12)

A,w1,...,wK(cid:12)k∈[K]i∈[m] (cid:12)
(cid:34) (cid:35)
(cid:12) (cid:12)
( =d) 2L·E sup (cid:12)(cid:10) ϵ,D(θ)(cid:11)(cid:12)
(cid:12) (cid:12)
θ∈Θ
( ≤e) 2L·E(cid:13) (cid:13)D∗ϵ(cid:13)
(cid:13)·sup∥θ∥
θ∈Θ
(f) (cid:113)
≤ 4L 2mK(ζ2 +Kζ2), (22)
M v
where we justify each step below. We obtain Eq. (20) by combining Eqs. (21) and (22),
(cid:114)
sup
(cid:12) (cid:12)Rˆ(θ)−E(cid:2) Rˆ(θ)(cid:3)(cid:12) (cid:12)≤4L(cid:113)
2mK(ζ2 +Kζ2)+4L(ζ +ζ )
2mKlog2
(cid:12) (cid:12) M v M v δ
θ∈Θ
(cid:115)
(cid:18) (cid:19)
(i) 2
≤ 4L 2 2mK(ζ2 +Kζ2)+2mK(ζ +ζ )2log
M v m v δ
(cid:115)
(cid:18) (cid:19)
(ii) 2
≤ 8L mK·(ζ2 +Kζ2)· 1+3log
m v δ
(cid:114)
(iii) 4
≤ 16L mK·(ζ2 +Kζ2)log ,
M v δ
where (i) applies a variant of the AM-GM inequality √ a+√ b ≤ (cid:112) 2(a+b), (ii) uses the following upper
bound (ζ +ζ )2 ≤ 3(ζ2 +Kζ2), which holds whenever ζ ,ζ ≥ 0 and K ≥ 1, and (iii) uses 1 < 3log2
√ M v M v M v
and 8 3<16. Finally, we prove the remaining inequalities:
(a) Because we have assumed that items lie in the unit ball and that the parameters satisfy ∥A∥ ≤ ζ
F M
and ∥w ∥≤ζ , the unquantized measurements are bounded:
i v
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)D (A,v )(cid:12)≤ sup (cid:12)(cid:10) xx⊤ −x′x′⊤ ,A(cid:11) +(cid:10) x−x′,v (cid:11)(cid:12)≤2∥A∥ +2∥v ∥≤2(ζ +ζ ),
(cid:12) i;k k (cid:12) (cid:12) k (cid:12) F k M v
x,x′∈B(0,1)
where we have used triangle inequality for (cid:13) (cid:13)xx⊤−x′x′⊤(cid:13) (cid:13) ≤ 2 and ∥x−x′∥ ≤ 2. Because −logf(·)
F
31is L-Lipschitz on this domain, whenever |z|≤2(ζ +ζ ), we have:
M v
(cid:12) (cid:12)
(cid:12) 1(cid:12) (cid:12) (cid:12)
(cid:12)−logf(z)+log (cid:12)=(cid:12)−logf(z)+logf(0)(cid:12)≤L|z|.
(cid:12) 2(cid:12)
(b) This inequality follows from a standard symmetrization argument. Let H be a set of N-tuples of
functions, where h ≡ (h ,...,h ). Given a set of i.i.d. random variables Z ,...,Z ,Z′,...,Z′ and
1 N 1 N 1 N
a set of Rademacher random variables ϵ ,...,ϵ ∈{−1,+1}, we have:
1 N
 (cid:12)  (cid:12)  (cid:12) (cid:12)
(cid:12) N N (cid:12) (cid:12) N N (cid:12)
E sup (cid:12) (cid:12) (cid:12)(cid:88) h i(Z i)−E (cid:88) h i(Z i)(cid:12) (cid:12) (cid:12) =E sup (cid:12) (cid:12) (cid:12)(cid:88) ϵ ih i(Z i)−(cid:88) ϵ ih i(Z i′)(cid:12) (cid:12) (cid:12)

h∈H (cid:12)i=1 i=1 (cid:12) h∈H (cid:12)i=1 i=1 (cid:12)
 (cid:12) (cid:12)  (cid:12) (cid:12)
(cid:12) N (cid:12) (cid:12) N (cid:12)
≤E sup (cid:12) (cid:12) (cid:12)(cid:88) ϵ ih(Z i)(cid:12) (cid:12) (cid:12) +E sup (cid:12) (cid:12) (cid:12)(cid:88) ϵ ih i(Z i′)(cid:12) (cid:12) (cid:12)

h∈H (cid:12)i=1 (cid:12) h∈H (cid:12)i=1 (cid:12)
 (cid:12) (cid:12)
(cid:12) N (cid:12)
=2E sup (cid:12) (cid:12) (cid:12)(cid:88) ϵ ih i(Z i)(cid:12) (cid:12) (cid:12) .
h∈H (cid:12)i=1 (cid:12)
In our setting, we have an index set (i,k)∈[m]×[K] and h :Z (cid:55)→−logf(cid:0) Z·D (A,w )(cid:1).
i;k i;k k
(c) We use the fact that the function −logf(z) is L-Lipschitz over the domain |z| ≤ 2(ζ +ζ ). We
M v
can move the Lipschitz constant out of the expectation by applying (Zhang, 2023, Theorem 6.28),
reproduced below.
(d) This step first makes use of the fact that the random variables ϵ Y =d ϵ are equal in distribution.
i;k i;k i;k
Then, it consolidates everything using the trace inner product on Rm×K.
(e) This step uses the property of the adjoint ⟨ϵ,D(θ)⟩ = ⟨D∗(ϵ),θ⟩ ≤ ∥D∗(ϵ)∥·∥θ∥. The first inner
product is over Rm×K, the second inner product and norm are over Sym(Rr)⊗Rr×K.
(cid:113)
(f) We apply the bound on the parameters sup ∥θ∥≤ ζ2 +Kζ2 along with the following:
M v
θ∈Θ
E(cid:13) (cid:13)D∗ϵ∥( ≤i)(cid:113) E(cid:10) DD∗,ϵϵ⊤(cid:11)
(cid:113)
( =ii) (cid:10)E[DD∗],E[ϵϵ⊤](cid:11)
(iii)(cid:115) (cid:88) (cid:13)2 (iv) √
= ∥∆ i;k⊕δ i;k(cid:13) ≤ 2 2mK.
i,k
The (i) uses Jensen’s inequality, (ii) uses the independence of the randomness over the design matrices
andtheRademacherrandomvariables,(iii)usesthefactthatE[ϵϵ⊤]istheidentityonRm×K,and(iv)
uses the fact that items are contained in the unit Euclidean ball, so that:
∥∆ ⊕δ ∥2 =∥∆ ∥2+∥δ ∥2 ≤22+22.
i;k i;k i;k i;k
RemarkE.2. ToshowthatthereexistsP suchthatσ2 (P )=Ω(1),assumethespaceRr isquadratically
m min m
spanned by X. In particular, there exists a collection of items (x ,x )n such that its design matrix D is
i0 i1 i=1
32full rank. Define X ∈Sym(Rr)⊕Rr for i=1,...,n by X =∆ ⊕δ . Then, D∗D corresponds to:
i i i i
n
(cid:88)
D∗D = X X⊤,
i i
i=1
where σ (D∗D) > 0. Let P be constructed by drawing m pairs uniformly at random. Let D be the
min m m
random design matrix. Let I ∼Unif([n]) for j =1,...,m be the index of the jth random pair, so that:
j
 
m
(cid:88)
E[D m∗ D m]=E  X IjX I⊤ j
i=1
m n
(cid:88) 1 (cid:88)
= X X⊤
n i i
j=1 i=1
m
= D∗D.
n
It follows that for this choice of random design, we have σ2 (P )=σ (D∗D), which is a constant.
min m min
E.3 Auxiliary lemmas
Lemma E.3 (Hoeffding-style inequality for independent bounded random vectors, (Jin et al., 2019), Corol-
lary 7). There exists a universal constant c such that for any random vectors X ,X ,...,X ∈Rd that are
1 2 m
independent and satisfy E[X ] = 0 and ∥X ∥ ≤ κ for i ∈ [m], we have, for any δ ∈ (0,1], with probability
i i 2 i
at least 1−δ,
(cid:118)
(cid:13) m (cid:13) (cid:117) m
(cid:13)(cid:88) (cid:13) (cid:117)(cid:88) 2d
(cid:13)
(cid:13)
X i(cid:13)
(cid:13)
≤c·(cid:116) κ2
i
log
δ
.
i=1 2 i=1
Corollary E.4 (Matrixversion,(Jinetal.,2019),Corollary7). Thereexistsauniversalconstantcsuchthat
for any random matrices X ,X ,...,X ∈Rd×d that are independent and satisfy E[X ]=0 and ∥X ∥ ≤κ
1 2 m i i F i
for i∈[m], we have, for any δ ∈(0,1], with probability at least 1−δ,
(cid:118)
(cid:13) m (cid:13) (cid:117) m
(cid:13)(cid:88) (cid:13) (cid:117)(cid:88) 2d
(cid:13)
(cid:13)
X i(cid:13)
(cid:13)
≤c·(cid:116) κ2
i
log
δ
.
i=1 F i=1
(cid:16) (cid:17) (cid:16) (cid:17)
Proof. Since log 2d2 ≤2log 2d for δ ≤1, the corollary follows directly from Lemma E.3.
δ δ
Lemma E.5 (Bounded difference inequality). Let f :XN →R satisfy the bounded difference property,
sup (cid:12) (cid:12)f(x 1,...,x N)−f(x 1,...,x′ i,...,x N)(cid:12) (cid:12)≤C, ∀i∈[N].
x1,...,xN,x′
i
Let X ,...,X be i.i.d. random variables. Then, with probability at least 1−δ,
1 N
(cid:114)
(cid:12) (cid:12)f(X 1,...,X N)−E[f(X 1,...,X N)(cid:12) (cid:12)≤C 2Nlog2 δ.
This theorem is also known as McDiarmid’s inequality; as reference, see for example (Zhang, 2023,
Theorem 6.16).
Lemma E.6 (Theorem 6.28, (Zhang, 2023)). Let h be an L-Lipschitz function h : R → R. Let F be a
function class with functions f :Z → R. Let z ,...,z ∈Z and let ϵ ,...,ϵ be independent Rademacher
1 N 1 N
33random variables. Then:
 (cid:12) (cid:12)  (cid:12) (cid:12)
(cid:12) N (cid:12) (cid:12) N (cid:12)
E sup (cid:12) (cid:12) (cid:12)(cid:88) ϵ ih(cid:0) f(z i))(cid:12) (cid:12) (cid:12) ≤L·E sup (cid:12) (cid:12) (cid:12)(cid:88) ϵ if(z i)(cid:12) (cid:12) (cid:12) .
f∈F (cid:12)i=1 (cid:12) f∈F (cid:12)i=1 (cid:12)
F Details and additional results for Section 6
Our experimental setup and implementation are inspired by and adapted from (Canal et al., 2022). In
Section F.1, we provide further details to our experimental setup. In Section F.2, we present additional
experimental results.
F.1 Experimental details
Each simulation run is defined by several parameters: the ambient dimension d, the number of subspaces
n, the dimension of each subspace r, the number of users per subspace K, and the number of preference
comparisons per user m.
Data generation In each simulation run, we generate a symmetric positive definite matrix M from the
Wishart distribution W(d,I ) and normalize it so that ∥M∥ = d, as in (Canal et al., 2022, Section F.3).
d F
We generate n uniform-at-random r-dimensional subspaces (Stewart, 1980): for each subspace, we draw r
independent random vectors from N(0,1I ) and use QR decomposition to find an orthonormal basis. For
d d
each subspace V equipped with orthonormal basis B, we generate K user ideal points from N(0,1I ). For
d d
each user, we generate 2m items (m pairs), where each item is a fresh draw from N(0,1BB⊤).
r
For Experiment 3, given V and B generated in this way, we generate 2mK items that approximately
lie on a subspace V by sampling from N(0,1BB⊤+ σ2 B B⊤), where B is an orthonormal basis of V⊥,
r d−r ⊥ ⊥ ⊥
the orthogonal complement of V. We generate user responses as before (see Section 6; namely, we use the
sigmoid link function with varying choices of noise levels, β =1,4,∞).
To learn the metric using these approximately subspace-clusterable items, Algorithm 2 needs to be
modified since it expects that the items lie exactly on a union of subspaces. We do so by constructing new
representations for the items, where for each set of 2mK items X ∈ Rd×2mK that approximately lie on V,
we use singular value decomposition to construct a rank-r approximation Xˆ of the items, minimizing:
min ∥Xˆ −X∥ .
F
rank(Xˆ)≤r
See the Eckart–Young–Mirsky theorem (Golub et al., 1987, for reference). Algorithm 2 can then be run
directlyonthelow-rankrepresentationXˆ (thisproceduredoesnotaffecthowuserresponsesaregenerated).
Algorithm implementation We provide additional details on the implementation of Algorithm 2. In
Stage 1 (learning subspace metrics), we use Algorithm 5 and set constraints based on oracle knowledge
of optimal hyperparameters ζ and ζ (also called the best-case hyperparameter setting in (Canal et al.,
M v
2022)). We use ℓ(z;β) = log(1+exp(−βz)) as the loss function, where β is assumed known and given by
the logistic link function above. We use the Splitting Conic Solver (SCS) in CVXPy with hyperparameters
eps = 1e4 and max_iters = 1e5 to solve the convex optimization problem.
In Stage 2 of our practical implementation (reconstruction from subspace metrics), we note that least
squares can be sensitive to outliers, and therefore we use the Huber loss for robust regression (Huber,
1964). In particular, we use the HuberRegressor from scikit-learn (Pedregosa et al., 2011) with default
hyperparameters,exceptforsettingmax_iters = 1e4. Toreconstructafullmetric,weusesubspacemetrics
learned in Stage 1. We note that we do not include a subspace (and the learned subspace metric) into
our reconstruction step if CVXPy/SCS does not solve the corresponding optimization problem in Stage 1
successfully, that is, prob.status != OPTIMAL. Nevertheless, given n subspaces, if CVXPy/SCS does not
successfully solve any of them, we use the n-th subspace alone for reconstruction.
34(a) High noise (β =1)
(b) Medium noise (β =4)
(c) Noiseless (β =∞)
Figure F.1: shows the results obtained using the same data in the three experiments (Section 6), wherein
the learner now uses the hinge loss to recover subspace metrics (Algorithm 5). Note that the y-axis scales
in the plots for Experiment 3 have been slightly adjusted to enhance clarity.
35(a) (b) (c)
FigureF.2: (a)showstheaveragerelativeerrorsoveritemsthatlieinaunionof402-dimensionalsubspaces.
(b) shows the average relative errors for reconstructing Mˆ from increasing numbers of 2-dimensional sub-
spaces;foreachsubspace,80userseachprovides10preferencecomparisons. Thedottedredcurveillustrates
thecountingargumentinRemark14; here, each2-dimensionalsubspacecancontributeatmost3degreesof
freedom. (c)showstheaveragerelativeerrorsforvaryingsubspacenoiselevels;here,itemslieapproximately
in a union of 40 2-dimensional subspaces and each user provides 10 preference comparisons.
F.2 Additional experimental results
Wefurtherstudywhetherourtwo-stageapproachrequiresexactknowledgeoftheprobabilisticmodelunder
which user binary responses are generated. To this end, we repeated the three experiments in Section 6
where userresponses aresampled accordingto thelogistic sigmoidlink functionwith varying response noise
levels, β = 1, β = 4, and β = ∞ (noiseless). Given the same data used for the experiments discussed in
Section 6, we now set the learner to use the hinge loss,
ℓ(z)=max(0,1−z),
instead of the negative log loss, to learn subspace metrics in Stage 1 of Algorithm 5. Figure F.1 shows the
performance of the learner. When compared with the results in Figures 3 and 4, the learner still recovers
the full metric reasonably well. This further validates the effectiveness of our divide-and-conquer approach.
We also ran the three experiments in Section 6 for subspace dimension r = 2, with slightly different
parameters. Theresponsenoiselevelwassettoβ =1andwasknowntothelearner,andeachexperimentwas
run30times. FigureF.2acomparestheaveragerelativeerrorsforvaryingK andm,whereitemslieinaunion
of 40 subspaces. Figure F.2b shows the average errors given increasing numbers of subspaces, where K =80
and m = 10. Note that by the dimension-counting argument in Remark 14, each 2-dimensional subspace
(cid:108) (cid:109)
contributes at most 2(2+1) = 3 degrees of freedom, and therefore a minimum of d(d+1) subspaces are
2 6
needed. FigureF.2bshowstheaveragerecoveryerrorsforvaryingsubspacenoiselevels,σ ∈{0,0.1,0.2,0.3},
and varying K, where items lie in a union of 40 subspaces and we set m=10.
References
G. Canal, B. Mason, R. Korlakai Vinayak, and R. Nowak. One for all: Simultaneous metric and preference
learning over multiple users. Advances in Neural Information Processing Systems, 35:4943–4956, 2022.
W. Gander, G. H. Golub, and R. Strebel. Least-squares fitting of circles and ellipses. BIT Numerical
Mathematics, 34:558–578, 1994.
G. H. Golub, A. Hoffman, and G. W. Stewart. A generalization of the eckart-young-mirsky matrix approxi-
mation theorem. Linear Algebra and its applications, 88:317–327, 1987.
36P. J. Huber. Robust estimation of a location parameter. The Annals of Mathematical Statistics, pages
73–101, 1964.
C. Jin, P. Netrapalli, R. Ge, S. M. Kakade, and M. I. Jordan. A short note on concentration inequalities for
random vectors with subgaussian norm. arXiv preprint arXiv:1902.03736, 2019.
S. Mac Lane and G. Birkhoff. Algebra, volume 330. American Mathematical Soc., 1999.
B. Mason, L. Jain, and R. Nowak. Learning low-dimensional metrics. Advances in neural information
processing systems, 30, 2017.
J. Matousek. Lectures on discrete geometry, volume 212. Springer Science & Business Media, 2013.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,andE.Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
G. W. Stewart. The efficient generation of random orthogonal matrices with an application to condition
estimators. SIAM Journal on Numerical Analysis, 17(3):403–409, 1980.
T. Zhang. Mathematical analysis of machine learning algorithms. Cambridge University Press, 2023.
37