TENSOR NETWORK-CONSTRAINED KERNEL MACHINES AS
GAUSSIAN PROCESSES
APREPRINT
FrederiekWesel KimBatselier
DelftCenterforSystemsandControl DelftCenterforSystemsandControl
DelftUniversityofTechnology DelftUniversityofTechnology
TheNetherlands TheNetherlands
f.wesel@tudelft.nl k.batselier@tudelft.nl
March29,2024
ABSTRACT
TensorNetworks(TNs)haverecentlybeenusedtospeedupkernelmachinesbyconstrainingthe
modelweights,yieldingexponentialcomputationalandstoragesavings. Inthispaperweprovethat
theoutputsofCanonicalPolyadicDecomposition(CPD)andTensorTrain(TT)-constrainedkernel
machinesrecoveraGaussianProcess(GP),whichwefullycharacterize,whenplacingi.i.d. priors
overtheirparameters. Weanalyzetheconvergenceofboth CPD and TT-constrainedmodels,and
showhowTTyieldsmodelsexhibitingmoreGPbehaviorcomparedtoCPD,forthesamenumber
ofmodelparameters. Weempiricallyobservethisbehaviorintwonumericalexperimentswhere
werespectivelyanalyzetheconvergencetotheGPandtheperformanceatprediction. Wethereby
establishaconnectionbetweenTN-constrainedkernelmachinesandGPs.
1 Introduction
TensorNetworks[TNs,Cichocki,2014;Cichockietal.,2016,2017],atoolfrommultilinearalgebra,extendtheconcept
ofrankfrommatricestotensorsallowingtorepresentanexponentiallylargeobjectwithalinearnumberofparameters.
Assuch,TNshavebeenusedtoreducethestorageandcomputationalcomplexitiesbycompressingthemodelparameters
ofarangeofmodelssuchasDeepNeuralNetworks(DNNs)[Novikovetal.,2015],ConvolutionalNeuralNetworks
(CNNs)[Jaderbergetal.,2014;Lebedevetal.,2015], RecurrentNeuralNetworks(RNNs)[Yeetal.,2018], Graph
NeuralNetworks(GNNs)[Huaetal.,2022]andtransformers[Maetal.,2019].
Similarly,TNshavealsofoundapplicationinthecontextofkernelmachines[StoudenmireandSchwab,2016;Novikov
etal.,2018;WeselandBatselier,2021]. Suchmodelslearnamultilinear(i.e. nonlinear)data-dependentrepresentation
from an exponentially large number of fixed features by means of a linear number of parameters, and are as such
characterizedbyanimplicitsourceofregularization. Furthermore,storageandtheevaluationofthemodelandits
gradientrequirealinearcomplexityinthenumberofparameters,renderingthesemethodspromisingcandidatesfor
applicationsrequiringbothgoodgeneralizationandscalability. Howevertheirmultilinearityprecludesclosed-form
Bayesianinference,andhasrestrictedthetrainingofthesemodelstothemaximumlikelihood(ML)andmaximuma
posteriori(MAP)framework.
Incontrast,GaussianProcesses[GPs,RasmussenandWilliams,2006]areanestablishedframeworkformodeling
functions which naturally allow the practitioner to incorporate prior knowledge. Importantly, when considering
i.i.d. observationsandGaussianlikelihoods,GPsallowforthedeterminationoftheposteriorinclosed-form,which
considerablyfacilitatestaskssuchasinference,samplingandtheconstructionofsparseapproximationsamongmany
others. Themaindrawbackofhavingaclosed-formposteriorlieshoweverintheirinabilitytoautonomouslylearn
features,whichhasarguablyfavoredtheuseofdeeplearningmodels.
InthispaperweestablishaconnectionbetweenTN-constrainedkernelmachinesandGPs,thussolvinganopenproblem
consideredbyWeselandBatselier[2021,2023]. WeprovethattheoutputsofCanonicalPolyadicDecomposition
4202
raM
82
]GL.sc[
1v00591.3042:viXraTensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
(CPD)andTensorTrain(TT)-constrainedkernelmachinesconvergetoafullycharacterizedGPwhenspecifyingi.i.d.
priorsacrosstheircomponents. Thisresultallowsustoderivethatthatforthesamenumberofmodelparameters,
TT-constrainedmodelsachievefasterconvergencetotheGPcomparedtotheirCPDcounterpartsandthusaremore
pronetoexhibit GP behavior. Weanalyzetheconsequencesofthesefindingsinthecontextof MAP estimationand
finallyempiricallyobserveGPconvergenceandGPbehaviorintwonumericalexperiments.
Therestofthispaperisorganizedasfollows.Insection2weprovideabriefintroductiontoGPsandtheirapproximations,
TNsand TN-constrainedkernelmachines. Insection3wepresentourmainresult,i.e. theequivalenceinthelimit
betweenTN-constrainedkernelmachinesandGPs. Insection4wediscussournumericalresultswhichillustratethe
foundresults. Wethenprovideareviewofrelatedwork(section5)andaconclusion(section6). Wediscussthenotation
usedthroughoutthepaperinappendixA.
2 Background
GPs are a collection of random variables such that any finite subset has a joint Gaussian distribution [Rasmussen
and Williams, 2006]. They provide a flexible formalism for modeling functions which inherently allows for the
incorporationofpriorknowledgeandtheproductionofuncertaintyestimatesintheformofapredictivedistribution.
Morespecifically,aGPisfullyspecifiedbyameanfunctionµ∈R,typicallychosenaszero,andacovarianceorkernel
functionk(·,·):RD×RD →R:
f(x)∼GP(µ,k(x,·)).
Givenalabeleddataset{(x n,y n)}N n=1consistingofinputsx
n
∈RD andi.i.d. noisyobservationsy
n
∈R,GPscanbe
usedformodelingtheunderlyingfunctionf inclassificationorregressiontasksbyspecifyingalikelihoodfunction.
Forexamplethelikelihood
p(y |f(x ))=N(f(x ),σ2), (1)
n n n
yieldsa GP posteriorwhichcanbeobtainedinclosed-formbyconditioningtheprior GP onthenoisyobservations.
Calculatingthemeanandcovarianceofsuchaposteriorcruciallyrequiresinstantiatingandformallyinvertingthe
kernelmatrixK suchthatk :=k(x ,x ). TheseoperationsrespectivelyincuracomputationalcostofO(N2)and
n,m n m
O(N3)andthereforeprohibittheprocessingoflarge-sampleddatasets.
2.1 BasisFunctionApproximation
TheprevailingapproachinliteraturetocircumventtheO(N3)computationalbottleneckistoprojecttheGPontoa
finitenumberofBasisFunctions(BFs)[e.g.,RasmussenandWilliams,2006;Quin˜onero-CandelaandRasmussen,
2005]. Thisisachievedbyapproximatingthekernelas
k(x,x′)≈φ(x)TΛφ(x′), (2)
where here φ(x) : RD → RM are (nonlinear) basis functions and Λ ∈ RM×M are the BF weights. This finite-
dimensionalkernelapproximationensuresadegeneratekernel[RasmussenandWilliams,2006],asitcharacterizedby
afinitenumberofnon-zeroeigenvalues. ItsassociatedGPcanbecharacterizedequivalentlyas
f(x)=⟨φ(x),w⟩, w ∼N(0,Λ), (3)
whereinw ∈RM arethemodelweightsandΛistheassociatedpriorcovariance. OncemoreconsideringaGaussian
likelihood(equation(1))yieldsaclosed-formposteriorGPwhosemeanandcovariancerequireonlytheinversionof
thematrix(cid:80)N φ(x )φ(x )T. ThisyieldsacomputationalcomplexityofO(NM2+M3),whichallowstotackle
n=1 n n
large-sampleddatawhenN ≫M.
2.2 ProductKernels
IntheremainderofthispaperweconsiderGPswithproductkernels.
Definition2.1(Productkernel[RasmussenandWilliams,2006]). Akernelk(x,x′)isaproductkernelif
D
(cid:89)
k(x,x′)= k(d)(x ,x′), (4)
d d
d=1
whereeachk(d)(·,·):R×R→Risavalidkernel.
2TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
Whilemanycommonlyusedkernelsareproductkernelse.g. theGaussiankernelandthepolynomialskernel,product
kernelsprovideastraightforwardstrategytoextendone-dimensionalkernelstothehigher-dimensionalcase[Rasmussen
andWilliams,2006;Hensmanetal.,2017]. Thebasisfunctionsandpriorcovarianceofproductkernelscanthenbe
determinedbasedonthebasisfunctionexpansionoftheirconstituentsasfollows.
Lemma2.2(Basisfunctionsandpriorcovariancesofproductkernels). Considertheproductkernelofdefinition2.1.
Denotethebasisfunctionsandpriorcovarianceofeachfactork(d)(x d,x′ d)asφ(d)(x d)∈RMd andΛ(d) ∈RMd×Md
respectively,thenthebasisfunctionsandpriorcovarianceofk(x,x′)are
φ(x)=⊗D φ(d)(x ), (5)
d=1 d
and
Λ=⊗D Λ(d), (6)
d=1
The inherent challenge in this approach stems from the exponential increase of the number of basis functions M
andthusofmodelparametersasafunctionofthedimensionalityoftheinputdata,therebyrestrictingtheirutilityto
low-dimensionaldatasets.
SuchstructurearisesforinstancewhendealingwithMercerexpansionsofproductkernels,inthestructuredkernel
interpolation framework [Wilson and Nickisch, 2015; Yadav et al., 2021] variational Fourier features framework
[Hensmanetal.,2017]andHilbert-GPframework[SolinandSa¨rkka¨,2020]. Alternativeimportantapproximation
strategieswhichavoidthisexponentialscalingarerandomfeatures[RahimiandRecht,2007;La´zaro-Gredillaetal.,
2010], inducing features [Csato´ and Opper, 2002; Seeger et al., 2003; Quin˜onero-Candela and Rasmussen, 2005;
SnelsonandGhahramani,2006;Hensmanetal.,2013,2015]andadditiveGPs[Duvenaudetal.,2011;Luetal.,2022]
whichcircumventtheoutlinedcomputationalissue. AllthoseapproachescanbeinterpretedasprojectingtheGPona
setofBFs.
The performance of these methods however tends to deteriorate in higher dimensions, as they need to cover an
exponentiallylargedomainwithalinearnumberofrandomsamplesorinducingpoints. Theseissuesaresomeofthe
computationalaspectsofthecurseofdimensionality,whichrendersitdifficulttooperateinhigh-dimensionalfeature
spaces[Hastieetal.,2001].
2.3 TensorNetworks
Arecentalternativeapproachtoremedysaidcurseofdimensionalityaffectingtheexponentiallyincreasingweightsof
thelinearmodelinequation(3)consistsinconstrainingthemodelsweightswtobealow-ranktensornetwork. TNs
expressaD-dimensionaltensorW asamulti-linearfunctionofanumberofcoretensors. TwocommonlyusedTNsare
theCPDandTT,definedasfollows.
Definition2.3(CPD[Hitchcock,1927]). AD-dimensionaltensorW ∈RM1×M2×···×MD hasarank-RCPDif
R D
(cid:88)(cid:89)
w = w(d) . (7)
m1,m2,...,mD md,r
r=1d=1
ThecoresofaCPDarethematricesW(d) ∈RMd×R. SinceaCPDtensorcanbeexpressedsolelyintermsofitscores,
itsstoragerequiresP
=R(cid:80)D
M
parametersasopposedto(cid:81)D
M .
CPD d=1 d d=1 d
Definition 2.4 (TT [Oseledets, 2011]). A D-dimensional tensor W ∈ RM1×M2×···×MD admits a rank-
(R :=1,R ,...,R :=1)tensortrainif
0 1 D
(cid:88)R0 (cid:88)R1 (cid:88)RD (cid:89)D
w = ··· w(d) . (8)
m1,m2,...,mD rd−1,md,rd
r0=1r1=1 rD=1d=1
ThecoresofatensortrainareD3-dimensionaltensorsW(d) ∈RRd−1×M×Rd whichyieldP
TT
=(cid:80)D d=1R D−1M DR
D
parameters.
InthefollowingwedenotebyTN(W)atensorwhichadmitsageneralTNformat,byCPD(W)atensorwhichadmitsa
rank-RCPformandbyTT(W)atensorinrank-(R 0:=1,R 1,...,R D:=1)TTform. Lastly,wedenotebyR 1(W)a
tensorwhichisinrank-1CPformorrank-(1,1,...,1)TT,asbothareequivalent.
Importantly,werefertoatensoringeneral TN formatTN(W) ∈ RM1×M2×·×MD asunderparametrized ifitsrank
hyperparameters,e.g.
RincaseofCPD,arechosensuchthatitsstoragecostislessthan(cid:81)D
d=1M d. Thisiscrucialin
ordertoobtainstorage,andaswewillsee,computationalbenefits.
3TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
2.4 TensorNetwork-ConstrainedKernelMachines
TNshavebeenusedtoreducethenumberofmodelparametersinkernelmachines(equation(3))bytensorizingtheBFs
φ(x)andmodelweightswandbyconstrainingbothtobeunderparameterizedTNs. Thisapproachlaysitsfoundations
onthefactthattheFrobeniusinnerproductofatensorizedvectorisisometricwithrespecttotheEuclideaninner
product,i.e.
f(x)=⟨φ(x),w⟩=⟨ten(φ(x)),ten(w)⟩ . (9)
F
ThisisometryallowsthentoconstraintheBFsandthemodelweightstobeanunderparameterizedTN. Sinceproduct
kernelsyieldanexpansionintermsofKronecker-productBFs(equation(5)),theyarearank-1TNbydefinitionafter
tensorization. Embeddingtheserelationsyieldsanapproximatemodel
f(x)≈f (x):=⟨R (ten(φ(x))),TN(ten(w))⟩ , (10)
TN 1 F
characterizedbylowerstorageandcomputationalcomplexities. Thisapproachhasbeenproposedmostlyforweights
modeledasCPD[KargasandSidiropoulos,2021;WeselandBatselier,2021,2023]orTT[Wahlsetal.,2014;Stouden-
mireandSchwab,2016;Batselieretal.,2017;Novikovetal.,2018;Chenetal.,2018]astheyarguablyintroducefewer
hyperparameters(onlyoneincaseofCPD)andthusareinpracticeeasiertoworkwithcomparedtootherTNssuchas
theMulti-ScaleEntanglementRenormalizationAnsatz(MERA)[ReyesandStoudenmire,2021].
Wedefinesuchmodelsaswewillneedthemindetailinthenextsection,wherewepresentourmaincontribution.
Definition2.5(CPD-constrainedkernelmachine). TheCPD-constrainedkernelmachineisdefinedas
f (x):=⟨R (ten(φ(x))),CPD(ten(w))⟩ (11)
CPD 1 F
R
(cid:88)
= h (x), (12)
r
r=1
wheretheintermediatevariablesh ∈Raredefinedas
r
D
h (x):=(cid:89) φ(d)(x )T w(d) . (13)
r d :,r
d=1
Similarly,weprovideadefinitionfortheTT-constrainedkernelmachine.
Definition2.6(TT-constrainedkernelmachine). TheTT-constrainedkernelmachineisdefinedas
f (x):=⟨R (ten(φ(x))),TT(ten(w))⟩ (14)
TT 1 F
(cid:88)RD R (cid:88)D−1 (cid:88)R0 (cid:89)D
= ··· z(d) (x ), (15)
rd−1,rd d
rD=1rD−1=1 r0=1d=1
wheretheintermediatevariablesZ(d) ∈RRd−1×Rd aredefinedelement-wiseas
(cid:88)Md
z(d) (x ):= φ(d)(x )w(d) . (16)
rd−1,rd d md d rd−1,md,rd
md=1
Evaluating CPD and TT-constrained kernel machines (equation (11), equation (14)) and their gradients can be ac-
complished with O(P ) and O(P ) computations, respectively. This allows the practitioner to tune the rank
CPD TT
hyperparameter in order to achieve a model that fits in the computational budget at hand and that learns from the
specifiedBFs.
Fromanoptimizationpoint-of-view,modelsintheformofequation(10)havebeentrainedbothintheML[Stoudenmire
andSchwab,2016;Batselieretal.,2017]andintheMAPsetting[Wahlsetal.,2014;Novikovetal.,2018;Chenetal.,
2018;KargasandSidiropoulos,2021;WeselandBatselier,2021,2023]andinthecontextofGPvariationalinference
[Izmailovetal.,2018]whereTTsareusedtoparameterizethevariationaldistribution. Itishowevernotclearifandhow
thesemodelsrelatetotheweight-spaceGPequation(3).
Inthefollowingsectionwepresentthemaincontributionofourwork: weshowhowwhenplacingi.i.d. priorsonthe
coresoftheseapproximatemodels,theyconvergetoaGPwhichwefullycharacterize.
4TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
P =1600 P =1600000 P =1320 P =1605000
CPD CPD TT TT
Figure1: Histogramsoftheempirical PDF of CPD (blue)and TT (orange)modelsspecifiedintheorem3.1and3.2
evaluatedatarandompointasafunctionofmodelparametersP forD = 16. Theblacklineisthe PDF ofthe GP.
NoticehowTTconvergesfastertotheGPforthesamenumberofmodelparametersP.
3 TN-ConstrainedKernelMachinesas GPs
WecommencetooutlinethecorrespondencebetweenTN-constrainedkernelmachineandGPs,whichmakesuseofthe
CentralLimitTheorem(CLT). Webeginbyelucidatingthesimplestcase,i.e. theCPD.
Theorem3.1(GPlimitofCPD-constrainedkernelmachine). ConsidertheCPD-constrainedkernelmachine
f (x):=⟨R (ten(φ(x))),CPD(ten(w))⟩ .
CPD 1 F
IfeachoftheRcolumnsw(d)
:,r
∈RMd ofeachCPDcoreisani.i.d. randomvariablesuchthat
(cid:104) (cid:105)
E w(d) =0,
:,r
E(cid:104)
w :( ,d r)w :( ,d
r)T(cid:105)
=R− D1Λ(d),
thenf (x)convergesindistributionasR→∞totheGP
CPD
(cid:32) D (cid:33)
f (x)∼GP 0,(cid:89) φ(d)(x )T Λ(d)φ(d)(·) .
CPD d
d=1
Proof. SeeappendixB.1.
AsimilarresultcanbeconstructedfortheTTcase.
Theorem3.2(GPlimitofTT-constrainedkernelmachine). ConsidertheTT-constrainedkernelmachine
f (x):=⟨R (ten(φ(x))),TT(ten(w))⟩
TT 1 F
IfeachoftheR d−1R dfibersW(d)
rd−1,:,rd
∈RMd ofeachTTcoreisani.i.d. randomvariablesuchthat
(cid:104) (cid:105)
E W(d) =0,
rd−1,:,rd
E(cid:104)
W(d) W(d)
T(cid:105)
=
1
Λ(d),
rd−1,:,rd rd−1,:,rd (cid:112)
R R
d−1 d
thenf (x)convergesindistributionasR →∞,R →∞,...,R →∞totheGaussianprocess
TT 1 2 D−1
(cid:32) D (cid:33)
f (x)∼GP 0,(cid:89) φ(d)(x )T Λ(d)φ(d)(·) .
TT d
d=1
Proof. SeeappendixB.2.
5TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
Theorem3.2guaranteestheconvergenceindistributionoff (x)totheGPofequation(3)bytakingsuccessivelimits
TT
ofeachTTrank. Importantly,thesameconvergenceresultsalsoholdstrueiftheTTranksgrowsimultaneously,see
appendixB.3.
Boththeorem3.1and3.2areremarkable,astheyimplythatadegenerateGPwhichcanbedefinedwithanexponential
numberof(cid:81)D
d=1M dweightswcanbealsoobtainedwithaninfinitenumberofmodelparametersP usingtheCPD-
constrainedmodelofdefinition2.5ortheTT-constrainedmodelofdefinition2.6. Further,theorem3.1and3.2suggest
thatCPDandTT-basedmodelsexhibitGPbehaviorintheoverparameterizedregime. GPbehaviorischaracterizedby
afixedlearningrepresentation,whichincaseofthekernelintheorem3.1and3.2isfullydefinedbytheBFsandis
hencedata-independent. Onthecontrary,inthefiniterankregime,bothCPDandTTmodelsareabletocraftnonlinear
featuresfromtheprovidedBFs,potentiallylearningmorelatentpatternsinthedata.
3.1 ConvergenceRatestotheGP
Whileboththeorem3.1andtheorem3.2guaranteeconvergenceindistributiontotheGPofequation(3),theydosoat
ratesthatdifferintermsofthenumberofmodelparameters. Letusassume,forsimplicity,thatthenumberofbasis
functionsisthesamealongeachdimension,i.e.,M,andthattheD−1TTranksequalR.Itfollowsthenthatthenumber
ofCPDmodelparametersP =MDR andthenumberofTTmodelparametersP =M(D−2)R2 +2MR =
CPD CPD TT TT TT
O(MDR2 ). Given the convergence rate of the CLT for the expression in equation (11) to the GP in equation (3),
TT
denotedasO(1/(cid:113) R CPD)withrespecttothevariableP CPD,wecanestablishthefollowingcorollarybysubstitutingR CPD
asafunctionofP .
CPD
Corollary3.3(ConvergencerateforCPD). Undertheconditionsoftheorem3.1,thefunctionf (x)convergesin
CPD
distributiontotheGPdefinedbyequation(3). Theconvergencerateisgivenby:
(cid:32)(cid:18) (cid:19)1(cid:33)
MD 2
f (x)→O .
CPD P
CPD
Due to their hierarchical structure, TT models are a composition of RD−1 variables, but can be represented in a
TT
quadraticnumberofmodelparametersinR ,sinceP =O(MDR2 ). ExpressingthentheCLTconvergencerateof
TT TT TT
O(1/(cid:113) R TTD−1)asafunctionofP TTyieldsthefollowingcorollary.
Corollary 3.4 (Convergence rate for TT). Under the conditions of theorem 3.2, the function f (x) converges in
TT
distributiontotheGPdefinedbyequation(3). Theconvergencerateisgivenby:
(cid:32)(cid:18) (cid:19)D−1(cid:33)
MD 4
f (x)→O .
TT P
TT
Therefore,whendealingwithidenticalmodelsintermsofthenumberofbasisfunctions(M),dimensionalityofthe
inputs(D),andthenumberofmodelparameters(P =P ),f (x)willconvergeatapolynomiallyfasterratethan
CPD TT TT
f (x),thusexhibitingGPbehaviorwithareducednumberofmodelparameters. Inparticular,basedoncorollaries3.3
CPD
and3.4weexpecttheGPconvergencerateofTTmodelstobefasterforD ≥3.
TheseinsightsarerelevantforpractitionersengagedwithTN-constrainedkernelmachines,astheyshedlightonthe
balancebetweentheGPand(deep)neuralnetworkbehaviorinherentinthesemodels. Notably,CPDandTT-constrained
models,akintoshallowandDNNsrespectively,havethecapacitytocraftadditionalnonlinearitiesbeyondtheprovided
basis functions. This characteristic can result in superior generalization when dealing with a limited number of
parameters. However,astheparametercountincreases,weexpectthesemodelstotransitiontowards GP behavior,
characterizedbyafixedfeaturerepresentationandstaticincomparison.
3.2 ConsequencesforMAPEstimation
Asdiscussedinsection2.4, TN-constrainedkernelmachinesaretypicallytrainedinthe ML or MAP frameworkby
constraining the weights w in the log-likelihood or log-posterior to be a TN. In said MAP context, and e.g. when
specifyinganormalprioronthemodelweightsw ∼N(0,Λ),theresultingregularizationtermΩisapproximatedby
Ω as
TN
(cid:12)(cid:12) (cid:12)(cid:12)2 (cid:12)(cid:12) (cid:16) (cid:17) (cid:12)(cid:12)2
Ω:=(cid:12) (cid:12)(cid:12) (cid:12)Λ−1 2w(cid:12) (cid:12)(cid:12)
(cid:12)
≈Ω TN:=(cid:12) (cid:12)(cid:12) (cid:12)TN(ten Λ− 21w )(cid:12) (cid:12)(cid:12)
(cid:12)
,
F F
6TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
 '     '     '     '    
 & 3 '       î   
     7 7
  î   
          î   
   
   
  î   
   
                                                             
 3  3  3  3
Figure2: MeanandstandarddeviationoftheCrame´r–vonMisesstatisticW2 evaluatedbetweentheempiricalCDF
ofCPDandTTmodelsspecifiedintheorem3.1and3.2evaluatedatN = 10randompointsasafunctionofmodel
parametersP forD =2,4,8,16. ThetwomodelsareequivalentforD =2. NoticehowTTconvergesfastertotheGP
asthedimensionalityoftheinputsDincreases.
whereΛ=⊗D Λ(d). Forexample,incaseofCPD-constrainedmodelswehave
d=1
Ω =(cid:12) (cid:12)(cid:12) (cid:12)⊙D (cid:16) W(d)T Λ(d)−1 W(d)(cid:17)(cid:12) (cid:12)(cid:12) (cid:12)2 . (17)
CPD (cid:12)(cid:12) d=1 (cid:12)(cid:12)
F
ThisformofregularizationisconsideredforTTbyWahlsetal.[2014];Novikovetal.[2018];Chenetal.[2018]andfor
CPDbyWeselandBatselier[2021,2023]. ItprovidesaFrobeniusnormapproximationoftheregularizationtermwhich
recoverstheoriginalMAPestimateasthehyperparametersofTN(ten(Λw))arechosensuchthatTN(ten(Λw))=
ten(Λw). Ifwenowconsidertheregularizationterminthelog-posterioroftheorem3.1weendupwith
D (cid:12)(cid:12) (cid:12)(cid:12)2
Ω CPD:=RD1 (cid:88)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)Λ(d)−1 2W(d)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12)
(cid:12)
. (18)
d=1 F
ThisregularizationhasbeenappliedwithoutthescalingfactorRD1 andwithΛ(d) = I Md,asobservedinthework
ofKargasandSidiropoulos[2021],whomaynothavebeenawareoftheunderlyingconnectionatthattime. Itdoes
notaccountforthemodelinteractionsacrossthedimensionalityduetothei.d.d. assumptionsonthecores. Contrary
totheregularizationΩ inequation(17),itprovidesanapproximationwhichrecoversthelog-priorΩandthusthe
TN
MAP inthelimit. Theseconsiderationspointtothefactthatifthepractitionerisinterestedonlyina MAP estimate
whichgivesweighttothefullpriorw ∼N(0,Λ)aswellaspossible,hemightbemoreinterestedintheregularization
ofequation(17). Furthermore,thepriorsintheorem3.1and3.2provideasensibleinitialguessforgradient-based
optimizationwhichisinvariantw.r.t. thedimensionalityoftheinputsandthechoiceofrankhyperparameters. Wehence
directlyaddresstheinitializationissuesaffectingTN-constrainedkernelmachines[Barrattetal.,2021]byprovidinga
sensibleinitializationstrategyandregularizationwhichdoesnotsufferfromvanishingorexplodinggradients.
4 NumericalExperiments
Wesetuptwonumericalexperimentsinordertorespectivelyempiricallyobservetheclaimsintheorem3.1and3.2by
evaluatingtheconvergencetothepriorGPinequation(3),andtoevaluatetheGPbehaviorofsuchmodelsatpredictionin
thefiniterankcase.InallexperimentswemadeuseoftheHilbert-GPSolinandSa¨rkka¨[2020]toprovideaBFexpansion
fortheGaussiankernelandoptforM =10basisfunctionsperdimension. Whensamplingthecoresofthemodelsin
theorem3.1and3.2weresortedtonormallydistributedrandomvariables. WeimplementedbothmodelsinPyMC
[Abril-Plaetal.,2023]. ThefullyanonymizedPythonimplementationisavailableatgithub.com/fwesel/tensorGP.
4.1 GPConvergence
In order to empirically verify the convergence to the GP of equation (3) we sample 10000 instances of the CPD
andTTmodelsspecifiedintheorem3.1and3.2forincreasingCPDandTTranksyieldinguptoP = 10000model
parameters. SincethetargetdistributionisGaussianwithknownmoments,werecordtheCrame´r–vonMisesstatistic
W2[D’AgostinoandStephens,1986]whichgivesametricofclosenessbetweenthetargetandoursampledempirical
CDF. We repeat this for N = 10 randomly sampled data points and for D = 2,4,8,16 and report the mean and
standarddeviationoftheresultsinfigure2. Thereinitcanbeobservedthatforthesamenumberofmodelparameters,
7
   :TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
 \ D F K W  H Q H U J \
 1        '     1        '   
      & 3 '
    
 7 7
    
 * 3
    
    
         
    
    
    
                                                                         
 3  3
Figure 3: Mean and standard deviation of the test RMSE evaluated of CPD and TT models as a function of model
parametersP aswellastheirtargetGP. NoticehowintheyachtdatasetTTexhibitsmoreGPbehaviorcomparedtoCPD
asP increases. OntheenergydatasetbothmethodsexhibitGPbehavioralreadyforranksdifferentthanone,which
explainswhyTTappearstobeslower.
TT converges more rapidly than CPD as the dimensionality of the inputs grows. Both approaches however need
exponentiallymoreparameterstoconvergeatthesamerateforincreasingdimensionalityoftheinputs. Notethatfor
D =4CPD,contrarytowhatstatedinsection3.1stillconvergesfasterduetotheapproximationmadewhenconsidering
P =DMR2. HistogramsoftheempiricalCDFforonedatapointareshowninfigure1. Thisbehaviorstemsfrom
TT
thefactthatforafixedcombinationofD,M andP,TTcapturesanexponentialRD−1rangeofmodelinteractionsin
contrasttotheRlinearinteractionsexhibitedbyCPD. ThisfactrendersthechoiceofTTmoresuitablewithrespect
toCPDifonewishesthemodeltoexhibitmoreGPbehavior,whichischaracterizedbyaweight-independentfeature
representationandislesslikelytooverfit.
4.2 GPBehavioratPrediction
Toinvestigatewhether CPD and TT-constrainedkernel machinesexhibit GP behavior asthenumber ofparameters
increaseswetackletwosmallUCIregressionproblems,yachtandenergy[DuaandGraff,2017]. Weconsider70%
of the data for training an the remaining for test and model our observations as having i.i.d. Gaussian likelihood
(equation(1)).WethenconsidertheGPintheorem3.1and3.2whichwetrainbymaximizingthemarginalizedlikelihood
(10randominitializations). InordertocomparemodelswiththetargetGP,wefixtheobtainedhyperparameters{σ,Λ}
andsample4chainsof2000instancesfromtheposteriorsp(CPD(ten(w))|y)andp(TT(ten(w))|y)forarange
of model parameters P using the No U-Turn Sampling Hamiltonian Monte Carlo scheme with default parameters.
After discarding the first 1000 samples (burn-in), we obtain posterior predictive distributions p(f (x) | y) and
CPD
p(f (x) | y). WethencomparethetwomodelsintermsoftheRootMeanSquaredError(RMSE)oftheposterior
TT
meanonthetestdata. WeplotthemeanandstandarddeviationoftheRMSEoverthechainsinfigure3.
Infigure3onecanobservethatthepredictionofbothCPDandTTmodelstendstowardstheGPasthenumberofmodel
parametersincreases. Incaseoftheyachtdatasetthishappenswithsmallererrors,i.e. bothmodelsgeneralizebetter.
Asexpected,theTT-constrainedkernelmachineexhibitsmoreGPbehaviorandinthiscaseworsegeneralization. In
caseoftheenergydatasetbothmethodshaveverysimilarperformancecomparedtotheGPandconvergewithlarger
testerrorsassoonastheirranksaredifferentthanone. Whilethebehavioronbothdatasetscanbeexplainedintermof
theGPunderfittingontheyachtdataset,itisworthnotingthatbothTTandCPD-basedmodelsperformatleastaswell
astheGPforrankshigherthanone,renderingthemcomputationallyadvantageousalternativestotheGPinthiscontext.
5 RelatedWork
OurcontributioniscloselytiedtothelinksbetweenBayesianneuralnetworksandGPs,firstestablishedforsingle-layer
single-outputneuralnetworks[Neal,1996a,b]havingsigmoidal[Williams,1996], Gaussian[Williams,1997]and
rectifiedlinearunit[ChoandSaul,2009]asactivationfunction.
ThisideawasextendedtoDNNsbyLeeetal.[2018]andMatthewsetal.[2018]fortheallthemostcommonactivation
functions. ItisimportanttonotethatthederivationofLeeetal.[2018]makesuseofrecursiveapplicationoftheCLT
8
 ( 6 0 5TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
toeachlayerofthenetwork,whileMatthewsetal.[2018]considersthecasewherethewidthofeachlayergrows
simultaneouslytotheothers,whichisarguablymorevaluableinpractice.
FurtherextensionshavebeenproposedtoCNNswherethenumberofchannelstendstoinfinity[Novaketal.,2018;
Garriga-Alonsoetal.,2018],toRNNs[Sunetal.,2022]andtoDNNshavinglow-rankconstraintsontheweightmatrices
[Nait-Saadaetal.,2023].
Inparticulartheorem3.1resemblestheresultsofNeal[1996a,b];Williams[1997]whichrelateinfinite-widthsingle
layer neural networks to GPs. The CPD rank corresponds exactly to the width of the neural network. The crucial
differencelieshoweverintheproductstructure,whichisnotpresentinneuralnetworksandintroducesanonlinearityof
differentkindthantheoneinducedbytheactivationfunction,whichislinearintheCPDandmoreingeneralforany
TN. TTsontheotherhandresembleDNNsastheymaptheoutputofeachcoretothenextone. However,incontrastto
DNNs,theinputsareprocessedoverthedepthofthenetwork. Foramoreindepthdiscussionwereferthereaderto
[Cohenetal.,2016].
Likewisetheorem3.2istheTNcounterparttotheworksofLeeetal.[2018];Matthewsetal.[2018]whichrelatefinite
depthneuralnetworkstoGPs. TheTTranksarethenequivalenttothewidthofeachlayerandtheactivationfunctionis
linear. IncontrasttotheDNNscase,theinducedGPsaredegenerate.
6 Conclusion
In this paper we proved that CPD and TT-constrained kernel machines are GPs in the limit or large TN ranks. We
characterizedthetarget GP, analyzedtheconvergencebehaviorofbothmodelsandshowedthatcomparedto CPD,
TT-basedmodelsconvergefastertotheGPwhendealingwithhigher-dimensionalinputs. Weempiricallydemonstrated
thesepropertiesbymeansofnumericalexperiments.
WhiletheGPconvergenceistooslowtowarrantthesemodelsasGPpriorapproximations,theinsightswederivedare
usefulinpracticeastheyshedlightontheeffectsofthechoiceofTNandregularizationinthesemodels.
References
O.Abril-Pla,V.Andreani,C.Carroll,L.Dong,C.J.Fonnesbeck,M.Kochurov,R.Kumar,J.Lao,C.C.Luhmann,
O.A.Martin,M.Osthege,R.Vieira,T.Wiecki,andR.Zinkov. PyMC:Amodern,andcomprehensiveprobabilistic
programming framework in Python. PeerJ Computer Science, 9:e1516, Sept. 2023. ISSN 2376-5992. doi:
10.7717/peerj-cs.1516.
F.Barratt,J.Dborin,andL.Wright. ImprovementstoGradientDescentMethodsforQuantumTensorNetworkMachine
Learning. InSecondWorkshoponQuantumTensorNetworksinMachineLearning,May2021.
K.Batselier,Z.Chen,andN.Wong. TensorNetworkalternatinglinearschemeforMIMOVolterrasystemidentification.
Automatica,84:26–35,Oct.2017. ISSN0005-1098. doi: 10.1016/j.automatica.2017.06.033.
Z.Chen,K.Batselier,J.A.K.Suykens,andN.Wong. ParallelizedTensorTrainLearningofPolynomialClassifiers.
IEEETransactionsonNeuralNetworksandLearningSystems,29(10):4621–4632,Oct.2018. ISSN2162-2388. doi:
10.1109/TNNLS.2017.2771264.
Y. Cho and L. Saul. Kernel Methods for Deep Learning. In Advances in Neural Information Processing Systems,
volume22.CurranAssociates,Inc.,2009.
A. Cichocki. Era of Big Data Processing: A New Approach via Tensor Networks and Tensor Decompositions.
arXiv:1403.2048[cs],Aug.2014.
A. Cichocki, N. Lee, I. Oseledets, A.-H. Phan, Q. Zhao, and D. P. Mandic. Tensor Networks for Dimensionality
ReductionandLarge-ScaleOptimization: Part1Low-RankTensorDecompositions. FoundationsandTrends®in
MachineLearning,9(4-5):249–429,2016. ISSN1935-8237,1935-8245. doi: 10.1561/2200000059.
A. Cichocki, A.-H. Phan, Q. Zhao, N. Lee, I. V. Oseledets, M. Sugiyama, and D. Mandic. Tensor Networks for
DimensionalityReductionandLarge-ScaleOptimizations.Part2ApplicationsandFuturePerspectives. Foundations
andTrends®inMachineLearning,9(6):249–429,2017. ISSN1935-8237,1935-8245. doi: 10.1561/2200000067.
N.Cohen,O.Sharir,andA.Shashua. OntheExpressivePowerofDeepLearning: ATensorAnalysis. InConference
onLearningTheory,pages698–728.PMLR,June2016.
L.Csato´ andM.Opper. SparseOn-LineGaussianProcesses. NeuralComputation,14(3):641–668,Mar.2002. ISSN
0899-7667. doi: 10.1162/089976602317250933.
R.B.D’AgostinoandM.A.Stephens. Goodness-of-FitTechniques. CRCPress,Jan.1986.
9TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
D.DuaandC.Graff. UCIMachineLearningRepository,2017.
D.K.Duvenaud, H.Nickisch, andC.Rasmussen. AdditiveGaussianProcesses. AdvancesinNeuralInformation
ProcessingSystems,24:226–234,2011.
A.Garriga-Alonso,C.E.Rasmussen,andL.Aitchison. DeepConvolutionalNetworksasshallowGaussianProcesses.
InInternationalConferenceonLearningRepresentations,Sept.2018.
T.Hastie,J.Friedman,andR.Tibshirani. TheElementsofStatisticalLearning. SpringerSeriesinStatistics.Springer,
NewYork,NY,2001. ISBN978-1-4899-0519-2978-0-387-21606-5. doi: 10.1007/978-0-387-21606-5.
J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for Big data. In Proceedings of the Twenty-Ninth
ConferenceonUncertaintyinArtificialIntelligence,UAI’13,pages282–290,Arlington,Virginia,USA,Aug.2013.
AUAIPress.
J.Hensman, A.Matthews, andZ.Ghahramani. ScalableVariationalGaussianProcessClassification. InArtificial
IntelligenceandStatistics,pages351–360.PMLR,Feb.2015.
J.Hensman,N.Durrande,andA.Solin. VariationalFourierfeaturesforGaussianprocesses. TheJournalofMachine
LearningResearch,18(1):5537–5588,Jan.2017. ISSN1532-4435.
F.L.Hitchcock. TheExpressionofaTensororaPolyadicasaSumofProducts. JournalofMathematicsandPhysics,
6(1-4):164–189,1927. ISSN1467-9590. doi: 10.1002/sapm192761164.
C. Hua, G. Rabusseau, and J. Tang. High-Order Pooling for Graph Neural Networks with Tensor Decomposition.
AdvancesinNeuralInformationProcessingSystems,35:6021–6033,Dec.2022.
P.Izmailov,A.Novikov,andD.Kropotov. ScalableGaussianProcesseswithBillionsofInducingInputsviaTensor
TrainDecomposition. InInternationalConferenceonArtificialIntelligenceandStatistics,pages726–735.PMLR,
Mar.2018.
M.Jaderberg,A.Vedaldi,andA.Zisserman. SpeedingupConvolutionalNeuralNetworkswithLowRankExpansions.
ProceedingsoftheBritishMachineVisionConference2014,2014. doi: 10.5244/c.28.88.
N.KargasandN.D.Sidiropoulos. SupervisedLearningandCanonicalDecompositionofMultivariateFunctions. IEEE
TransactionsonSignalProcessing,pages1–1,2021. ISSN1941-0476. doi: 10.1109/TSP.2021.3055000.
M.La´zaro-Gredilla,J.Quin˜nero-Candela,C.E.Rasmussen,andb.R.Figueiras-Vidal. SparseSpectrumGaussian
ProcessRegression. JournalofMachineLearningResearch,11(63):1865–1881,2010. ISSN1533-7928.
V.Lebedev,Y.Ganin,M.Rakhuba,I.V.Oseledets,andV.S.Lempitsky. Speeding-upConvolutionalNeuralNetworks
UsingFine-tunedCP-Decomposition. InInternationalConferenceonLearningRepresentations,Jan.2015.
J.Lee,Y.Bahri,R.Novak,S.S.Schoenholz,J.Pennington,andJ.Sohl-Dickstein. DeepNeuralNetworksasGaussian
Processes. InInternationalConferenceonLearningRepresentations,Feb.2018.
X. Lu, A. Boukouvalas, and J. Hensman. Additive Gaussian Processes Revisited. In Proceedings of the 39th
InternationalConferenceonMachineLearning,pages14358–14383.PMLR,June2022.
X.Ma,P.Zhang,S.Zhang,N.Duan,Y.Hou,M.Zhou,andD.Song. ATensorizedTransformerforLanguageModeling.
InAdvancesinNeuralInformationProcessingSystems,volume32.CurranAssociates,Inc.,2019.
A.G.d.G.Matthews,M.Rowland,J.Hron,R.E.Turner,andZ.Ghahramani. GaussianProcessBehaviourinWide
DeepNeuralNetworks,Aug.2018.
T. Nait-Saada, A. Naderi, and J. Tanner. Beyond IID weights: Sparse and low-rank deep Neural Networks are
alsoGaussianProcesses. InTheTwelfthInternationalConferenceonLearningRepresentations,Oct.2023. doi:
10.48550/arXiv.2310.16597.
R. M. Neal. Bayesian Learning for Neural Networks. Springer Science & Business Media, Jan. 1996a. ISBN
978-1-4612-0745-0.
R.M.Neal. PriorsforInfiniteNetworks. InR.M.Neal, editor, BayesianLearningforNeuralNetworks, Lecture
Notes in Statistics, pages 29–53. Springer, New York, NY, 1996b. ISBN 978-1-4612-0745-0. doi: 10.1007/
978-1-4612-0745-0 2.
R.Novak,L.Xiao,Y.Bahri,J.Lee,G.Yang,J.Hron,D.A.Abolafia,J.Pennington,andJ.Sohl-dickstein. Bayesian
DeepConvolutionalNetworkswithManyChannelsareGaussianProcesses. InInternationalConferenceonLearning
Representations,Sept.2018.
A.Novikov,D.Podoprikhin,A.Osokin,andD.P.Vetrov. Tensorizingneuralnetworks. InC.Cortes,N.Lawrence,
D.Lee, M.Sugiyama, andR.Garnett, editors, AdvancesinNeuralInformationProcessingSystems, volume28.
CurranAssociates,Inc.,2015.
10TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
A. Novikov, I. Oseledets, and M. Trofimov. Exponential machines. Bulletin of the Polish Academy of Sciences:
TechnicalSciences;2018;66;No6(SpecialSectiononDeepLearning: TheoryandPractice);789-797,2018. ISSN
2300-1917.
I.V.Oseledets. Tensor-TrainDecomposition. SIAMJournalonScientificComputing,33(5):2295–2317,Jan.2011.
ISSN1064-8275,1095-7197. doi: 10.1137/090752286.
J.Quin˜onero-CandelaandC.E.Rasmussen. AUnifyingViewofSparseApproximateGaussianProcessRegression.
JournalofMachineLearningResearch,6(65):1939–1959,2005. ISSN1533-7928.
A.RahimiandB.Recht. Randomfeaturesforlarge-scalekernelmachines. InProceedingsofthe20thInternational
ConferenceonNeuralInformationProcessingSystems,NIPS’07,pages1177–1184,RedHook,NY,USA,Dec.2007.
CurranAssociatesInc. ISBN978-1-60560-352-0.
C.E.RasmussenandC.K.I.Williams.GaussianProcessesforMachineLearning.AdaptiveComputationandMachine
Learning.MITPress,Cambridge,Mass,2006. ISBN978-0-262-18253-9.
J.A.ReyesandE.M.Stoudenmire. Multi-scaletensornetworkarchitectureformachinelearning. MachineLearning:
ScienceandTechnology,2(3):035036,July2021. ISSN2632-2153. doi: 10.1088/2632-2153/abffe8.
M.W.Seeger,C.K.I.Williams,andN.D.Lawrence. FastForwardSelectiontoSpeedUpSparseGaussianProcess
Regression. InInternationalWorkshoponArtificialIntelligenceandStatistics,pages254–261.PMLR,Jan.2003.
E.SnelsonandZ.Ghahramani. SparseGaussianProcessesusingPseudo-inputs. InAdvancesinNeuralInformation
ProcessingSystems,volume18.MITPress,2006.
A.SolinandS.Sa¨rkka¨. Hilbertspacemethodsforreduced-rankGaussianprocessregression. StatisticsandComputing,
30(2):419–446,Mar.2020. ISSN1573-1375. doi: 10.1007/s11222-019-09886-w.
E.M.StoudenmireandD.J.Schwab.Supervisedlearningwithtensornetworks.InProceedingsofthe30thInternational
ConferenceonNeuralInformationProcessingSystems,NIPS’16,pages4806–4814,RedHook,NY,USA,Dec.2016.
CurranAssociatesInc. ISBN978-1-5108-3881-9.
X.Sun,S.Kim,andJ.-I.Choi. Recurrentneuralnetwork-inducedGaussianprocess. Neurocomputing,509:75–84,Oct.
2022. ISSN0925-2312. doi: 10.1016/j.neucom.2022.07.066.
S.Wahls,V.Koivunen,H.V.Poor,andM.Verhaegen. LearningmultidimensionalFourierserieswithtensortrains. In
2014IEEEGlobalConferenceonSignalandInformationProcessing(GlobalSIP),pages394–398,Dec.2014. doi:
10.1109/GlobalSIP.2014.7032146.
F.WeselandK.Batselier. Large-ScaleLearningwithFourierFeaturesandTensorDecompositions. InAdvancesin
NeuralInformationProcessingSystems,May2021.
F. Wesel and K. Batselier. Tensor-based Kernel Machines with Structured Inducing Points for Large and High-
DimensionalData. InProceedingsofThe26thInternationalConferenceonArtificialIntelligenceandStatistics,
pages8308–8320.PMLR,Apr.2023.
C.Williams. ComputingwithInfiniteNetworks. InAdvancesinNeuralInformationProcessingSystems,volume9.
MITPress,1996.
C.Williams. ComputingwithInfiniteNetworks. AdvancesinNeuralInformationProcessingSystems9,pages295–301,
1997.
A.WilsonandH.Nickisch.KernelInterpolationforScalableStructuredGaussianProcesses(KISS-GP).InProceedings
ofthe32ndInternationalConferenceonMachineLearning,pages1775–1784.PMLR,June2015.
M.Yadav,D.Sheldon,andC.Musco. FasterKernelInterpolationforGaussianProcesses. InProceedingsofThe24th
InternationalConferenceonArtificialIntelligenceandStatistics,pages2971–2979.PMLR,Mar.2021.
J. Ye, L. Wang, G. Li, D. Chen, S. Zhe, X. Chu, and Z. Xu. Learning Compact Recurrent Neural Networks With
Block-Term Tensor Decomposition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition,pages9378–9387,2018.
11TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
A Notation
Throughout this paper we denote scalars in both capital and non-capital italics w,W, vectors in non-capital bold
w, matrices in capital bold W and tensors in capital italic bold font W. The m-th entry of a vector w ∈ RM is
indicatedasw
m
andthem 1m 2...m D-thentryofaD-dimensionaltensorW ∈RM1×M2×···×MD asw m1,m2,...mD.
We employ the column notation to indicate a set of elements of tensor given a set of indices, e.g. W and
m1,:,m2
W represent respectively all elements and the first three elements along the second dimension of tensor
m1,1:3,m2
W withfixedindicesm andm . TheKroneckerproductisdenotedby⊗andtheHadamard(elementwise)by⊙.
1 2
We employ one-based indexing for all tensors. The Frobenius inner product between two D-dimensional tensors
V,W ∈RM1×M2×···×MD is
(cid:88)M1 (cid:88)M2 (cid:88)MD
⟨V,W⟩ := ··· v w ,
F m1,m2,...,mD m1,m2,...,mD
m1=1m2=1 mD=1
andtheFrobeniusnormofW ∈RM1×M2×···×MD isdenotedanddefinedas
||W||2 :=⟨W,W⟩ .
F F
Wedefinethevectorizationoperatorasvec(·):RM1×M2×···×MD →RM1M2···MD suchthat
vec(W) =w ,
m m1,m2,...,mD
withm=m 1+(cid:80)D d=2(m d−1)(cid:81)d k− =1 1M k. Likewise,itsinverse,thetensorizationoperatorten(·):RM1M2···MD →
CM1×M2×...MD isdefinedsuchthat
ten(w) =w .
m1,m2,···,mD m
B Proofs
B.1 GPofCPD-ConstrainedKernelMachine
TheoremB.1(GPlimitofCPD-constrainedkernelmachine). ConsidertheCPD-constrainedkernelmachine
f (x):=⟨R (ten(φ(x))),CPD(ten(w))⟩ .
CPD 1 F
IfeachoftheRcolumnsofeachCPDcorew(d)
:,r
∈RMd isani.i.d. randomvariablesuchthat
(cid:104) (cid:105)
E w(d) =0,
:,r
E(cid:104) w(d)w(d)T(cid:105)
=
1
Λ(d),
:,r :,r RD1
thenf (x)convergesindistributionasR→∞totheGaussianprocess
CPD
(cid:32) D (cid:33)
f (x)∼GP 0,(cid:89) φ(d)(x )T Λ(d)φ(d)(·) .
CPD d
d=1
Proof. Consider the R intermediate functions h
r
of equation (13) which constitute the CPD-constrained model of
equation(11). Duetothei.i.d. assumptiononw(d) eachaddendisthesamefunctionofi.i.d. randomvariablesand
:,r
thusisitselfi.i.d.. Themeanofeachaddendis
(cid:34) D (cid:35)
E[h (x)]=E (cid:89) φ(d)(x )T w(d) =0, (19)
r d :,r
d=1
12TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
duetothei.i.dassumptionandthelinearityofexpectation. Itscovarianceis
E[h (x)h (x′)] (20a)
r r
(cid:34) D D (cid:35)
=E (cid:89) φ(d)(x )T w(d) (cid:89) φ(d)(x′)T w(d) (20b)
d :,r d :,r
d=1 d=1
(cid:34) D (cid:35)
=E (cid:89) φ(d)(x )T w(d) w(d) T φ(d)(x′) (20c)
d :,r :,r d
d=1
D
=(cid:89)
φ(d)(x
)TE(cid:104)
w(d) w(d)
T(cid:105)
φ(d)(x′) (20d)
d :,r :,r d
d=1
D
=1 (cid:89) φ(d)(x )T Λ(d)φ(d)(x′).
R d d
d=1
Herethestepfromequation(20b)toequation(20c)exploitsthefactthatthetransposeofascalarisequaltoitself,the
stepfromequation(20c)toequation(20d)isduetothelinearityofexpectation. Asthevariancesofeachintermediate
functionh areappropriatelyscaled,bythemultivariatecentrallimittheoremf (x)convergesindistributiontoa
r CPD
multivariatenormaldistribution,whichisfullyspecifiedbyitsfirsttwomoments
E(cid:2)
f
(x)(cid:3)
=0,
CPD
D
E(cid:2) f (x)f (x′)(cid:3) = (cid:89) φ(x )TΛ(d)φ(x ′).
CPD CPD d d
d=1
Since any finite collection of {f (x),...,f (x′)} will have a joint multivariate normal distribution with the
CPD CPD
aforementionedfirsttwomoments,weconcludethatf (x)istheGaussianprocess
CPD
(cid:32) D (cid:33)
f (x)∼GP 0,(cid:89) φ(d)(x )T Λ(d)φ(d)(·) .
CPD d
d=1
B.2 GPofTT-ConstrainedKernelMachineintheSequentialLimitoftheTTRanks
TheoremB.2(GPinthelimitofTT-constrainedkernelmachine). ConsidertheTT-constrainedkernelmachine
f (x):=⟨R (ten(φ(x))),TT(ten(w))⟩
TT 1 F
IfeachoftheR d−1R dfibersofeachTTcoreW(d)
rd−1,:,rd
∈RMd isani.i.d. randomvariablesuchthat
(cid:104) (cid:105)
E W(d) =0,
rd−1,:,rd
E(cid:104)
W(d) W(d)
T(cid:105)
=
1
Λ(d),
rd−1,:,rd rd−1,:,rd (cid:112)
R R
d−1 d
thenf (x)convergesindistributionasR →∞,R →∞,...,R →∞totheGaussianprocess
TT 1 2 D−1
(cid:32) D (cid:33)
f (x)∼GP 0,(cid:89) φ(d)(x )T Λ(d)φ(d)(·) .
TT d
d=1
Proof. Definethevectorofintermediatefunctionh(d+1) ∈RRd+1 recursivelyas
(cid:88)Rd
h(d+1):= z(d+1) (x )h(d),
rd+1 rd,rd+1 d+1 rd
rd=1
13TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
withh(0):=1. Notethatthefirsttwomomentsofintermediatevariablez(d+1) (x )are
rd,rd+1 d+1
(cid:104) (cid:105)
E z(d+1) (x ) =0,
rd,rd+1 d+1
(cid:104) (cid:105)
E z(d+1) (x )z(d+1) (x′ )
rd,rd+1 d+1 rd,rd+1 d+1
= 1 φ(d)(x )T Λ(d)φ(d)(x′)
(cid:112) d d
R R
d d+1
Weproceedbyinduction. Fortheinductionstepsupposethath r(d d)isaGP,identicalandindependentforeveryr dsuch
that
(cid:32) d (cid:33)
h(d) ∼GP 0,√1 (cid:89) φ(p)(x )T Λ(p)φ(p)(·) .
rd R p
d p=1
Thescalarh(d+1)isthesumofR i.i.d. termshavingmean
rd+1 d
(cid:104) (cid:105) (cid:104) (cid:105)
E h(d+1) =E z(d+1) (x )h(d) =0,
rd+1 rd,rd+1 d+1 rd
andcovariance
(cid:104) (cid:105)
E h(d+1)h(d+1)
rd+1 rd+1
(cid:104) (cid:105)
=E z(d+1) (x )h(d)z(d+1) (x′ )h(d)
rd,rd+1 d+1 rd rd,rd+1 d+1 rd
(cid:104) (cid:105) (cid:104) (cid:105)
=E z(d+1) (x )z(d+1) (x′ ) E h(d)h(d)
rd,rd+1 d+1 rd,rd+1 d+1 rd rd
d+1
= 1 (cid:89) φ(p)(x )T Λ(p)φ(p)(x′).
(cid:112) p p
R
d+1 p=1
SincetheassumptionsoftheCLTaresatisfiedh r(d d+ +11)convergesindistributiontothenormaldistribution,fullyspecified
by the above mentioned first two moments. Since any finite collection of {h(d+1)(x ),...,h(d+1)(x′ )}
rd+1 1:d+1 rd+1 1:d+1
will have a joint multivariate normal distribution with the aforementioned first two moments, we conclude that
h( rd d+ +11)(x 1:d+1)istheGP
(cid:32) d+1 (cid:33)
h(d+1) ∼GP 0, 1 (cid:89) φ(p)(x )T Λ(p)φ(p)(·) .
rd+1 (cid:112)
R
p
d+1 p=1
Forthebasecase,considertheR outputsofthefirsthiddenfunctionh(1). Theyarei.i.d. withmean
1 r1
(cid:104) (cid:105)
E h(1)(x ) =0.
r1 1
andcovariance
E(cid:104) h(1)(x )h(1)(x′)(cid:105) = √1 φ(1)(x )T Λ(1)φ(1)(x ).
r1 1 r1 1 R 1 1
1
WenowconsidertheR outputsofthesecondhiddenfunctionh(2)
2 r2
(cid:88)R1
h(2) = z(2) (x )h(1),
r2 r1,r2 2 r1
r1=1
whicharei.i.d. astheyarethesamefunctionoftheR i.i.d. outputsofh(1)(x ). Morespecifically,theirmeanand
1 r1 1
covarianceare
(cid:104) (cid:105)
E h(2) =0,
r2
(cid:104) (cid:105)
E h(2)(x )h(2)(x′)
r2 2 r2 2
2
=√1 (cid:89) φ(d)(x )T Λ(d)φ(d)(x ).
d d
R
2
d=1
14TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
Once more by the CLT h( r2 2) converges in distribution to the normal distribution with the above first two moments.
Sinceanyfinitecollectionof{h(2)(x ),...,h(2)(x′ }willhaveajointmultivariatenormaldistributionwiththe
r2 1:2 r2 1:2
aforementionedfirsttwomoments,weconcludethath( 22)(x 1:2)istheGP
(cid:32) 2 (cid:33)
1 (cid:89)
h(2) ∼GP 0,√ φ(d)(x )TΛ(d)φ(d)(·) ,
r2 R d
2
d=1
which is our base case. Hence by induction f (x) = h(D) converges in distribution as R → ∞, R → ∞, ...,
TT 1 2
R
D−1
→∞totheGP
(cid:32) D (cid:33)
f (x)∼GP 0,(cid:89) φ(d)(x )T Λ(d)φ(d)(·) .
TT d
d=1
B.3 GPofTT-ConstrainedKernelMachineintheSimultaneousLimitoftheTTRanks
Intheorem3.2weprovebyinductionthattheTT-constrainedkernelmachineconvergestoaGPbytakingsuccessive
limitsoftheTTranks. ThisresultisanalogoustotheworkofLeeetal.[2018],whoprovethatfortheDNNs,taking
sequentiallythelimitofeachlayer. Amorepracticallyusefulresultconsistsintheconvergenceinthesimultaneous
limitofTTranks.
IndeeplearningMatthewsetal.[2018,theorem4]proveconvergenceinthecontextofDNNsoverthewidthsofall
layerssimultaneously. SaidtheoremhasbeenemployedtoproveGPconvergenceinthecontextofconvolutionalneural
networks[Garriga-Alonsoetal.,2018]andinthecontextofDNNswhereeachweightmatrixisoflowrank[Nait-Saada
etal.,2023].
SeeingthesimilaritybetweenTT-constrainedkernelmachines(equation(14))andDNNsandthetechnicalityofthe
proof, similarly to [Garriga-Alonso et al., 2018; Nait-Saada et al., 2023] we draw a one-to-one map between the
TT-constrainedkernelmachinesandtheDNNsconsideredinMatthewsetal.[2018,theorem4]. Convergenceinthe
simultaneouslimitisthenguaranteedbyMatthewsetal.[2018,theorem4].
Webeginbyrestatingthedefinitionsoflinearenvelopeproperty,DNNs,linearenvelopepropertyandnormalrecursion
asfoundinMatthewsetal.[2018]. Tomakethecomparisoneasierforthereader,wechangetheindexingnotationto
matchtheoneinthispaper.
DefinitionB.3(Linearenvelopepropertyfornonlinearities[Matthewsetal.,2018]). Anonlinearityt:R→Rissaid
toobeythelinearenvelopepropertyifthereexistc,l≥0suchthatthefollowinginequalityholds
|t(u)|<c+l|u|∀u∈R. (25)
DefinitionB.4(FullyconnectedDNN[Matthewsetal.,2018]). Afullyconnecteddeepneuralwithone-dimensional
outputandinputsx∈RR0 isdefinedrecursivelysuchthattheinitialstepis
(cid:88)R0
h(1)(x)= z(1) x +b(1), (26)
r1 r1,r0 r0 r1
r0=1
theactivationstepbynonlinearactivationfunctiontisgivenby
g(d) =t(f(d)), (27)
rd rd
andthesubsequentlayersaredefinedbytherecursion
(cid:88)Rd
h(d+1) = z(d+1) g(d)+bd+1, (28)
rd+1 rd+1,rd rd rd+1
rd=1
sothath(D)istheoutputofthenetwork. Intheabove,Z(d) ∈RRd−1×Rd andb(d) ∈RRd arerespectivelytheweights
andbiasesofthed-thlayer.
DefinitionB.5(WidthfunctionMatthewsetal.[2018]). Foragivenfixedinputn∈N,awidthfunctionv(d) :N→N
atdepthdspecifiesthenumberofhiddenunitsR atdepthd.
d
15TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
LemmaB.6(Normalrecursion[Matthewsetal.,2018]). Considerz(d) ∼N(0,C(d))andb(d) ∼N(0,C(d)). If
rd−1,rd w rd b
theactivationsofthed-thlayerarenormallydistributedwithmoments
(cid:104) (cid:105)
E h(d) =0 (29)
rd
(cid:104) (cid:105)
E h(d)h(d) =K(x,x′), (30)
rd rd
thenunderrecursionequations(27)and(28),asR →∞,theactivationsofthenextlayerconvergeindistribution
d−1
toanormaldistributionwithmoments
(cid:104) (cid:105)
E h(d+1) =0 (31)
rd+1
(cid:104) (cid:105)
E h(d+1)h(d+1) =C(d+1)E [t(ϵ )t(ϵ )]+C(d+1). (32)
rd+1 rd+1 w (ϵ1,ϵ2)∼N(0,K) 1 2 b
WecannowstatethemajorresultinMatthewsetal.[2018].
TheoremB.7(GPinthesimultaneouslimitoffullyconnectedDNNs[Matthewsetal.,2018]). Considerarandom
DNNoftheformofdefinitionB.4obeyingthelinearenvelopeconditionofdefinitionB.3. Thenforallsetsofstrictly
increasingwidthfunctionsv(d)andforanycountableinputset{x,...,x′},thedistributionoftheoutputofthenetwork
convergesindistributiontoa GP asn → ∞. The GP hasmeanandcovariancefunctionsgivenbytherecursionin
lemmaB.6.
CorollaryB.8(GPinthesimultaneouslimitofTT-constrainedkernelmachines). ConsiderarandomTT-constrained
kernelmachineoftheformofdefinition2.6obeyingthelinearenvelopeconditionofdefinitionB.3. Thenforallsetsof
strictlyincreasingwidthfunctionsv(d)andforanycountableinputset{x,...,x′},thedistributionoftheoutputof
thenetworkconvergesindistributiontoaGPasP → ∞. TheGPhasmeanandcovariancefunctionsgivenbythe
recursioninlemmaB.6andstatedintheorem3.2.
Proof. When examining definition B.4 and comparing it with definition 2.6 it becomes clear that both models are
similar. Inthespecialcaseofinvolvinglinearactivationfunctionandzerobiases,themodelsarestructurallyidenticalif
oneconsidersunitinputsx=1inequation(26). ThenormalrecursioninlemmaB.6issatisfiedbyTT-constrained
kernelmachines,aswehavethat
t(u):=u∀u∈R,
C(d+1):=0,
b
C(d+1):= 1 φ(d)(x )T Λ(d)φ(d)(x′),
(cid:112) d d
R R
d d+1
d
K:=√1 (cid:89) φ(p)(x )T Λ(p)φ(p)(x′)
R p p
d p=1
E [t(ϵ )t(ϵ )]:=K.
(ϵ1,ϵ2)∼N(0,K) 1 2
HencebytheoremB.7,forallsetsofstrictlyincreasingwidthfunctionsv(d)andforanycountableinputset{x,...,x′},
thedistributionoftheoutputofthenetworkconvergesindistributiontoaGP,fullyspecifiedbytheoutputofthenormal
recurisioninlemmaB.6,whichequalstheGPintheorem3.2.
16