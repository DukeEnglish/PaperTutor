Sparse Feature Circuits: Discovering and Editing
Interpretable Causal Graphs in Language Models
SamuelMarks∗ CanRager EricJ.Michaud
NortheasternUniversity Independent MIT
YonatanBelinkov DavidBau AaronMueller*
Technion–IIT NortheasternUniversity NortheasternUniversity
Abstract
Weintroducemethodsfordiscoveringandapplyingsparsefeaturecircuits.
Thesearecausallyimplicatedsubnetworksofhuman-interpretablefeatures
forexplaininglanguagemodelbehaviors. Circuitsidentifiedinpriorwork
consistofpolysemanticanddifficult-to-interpretunitslikeattentionheads
orneurons,renderingthemunsuitableformanydownstreamapplications.
Incontrast,sparsefeaturecircuitsenabledetailedunderstandingofunan-
ticipatedmechanisms. Becausetheyarebasedonfine-grainedunits,sparse
featurecircuitsareusefulfordownstreamtasks:WeintroduceSHIFT,where
we improve the generalization of a classifier by ablating features that a
humanjudgestobetask-irrelevant. Finally,wedemonstrateanentirelyun-
supervisedandscalableinterpretabilitypipelinebydiscoveringthousands
ofsparsefeaturecircuitsforautomaticallydiscoveredmodelbehaviors.
1 Introduction
Thekeychallengeofinterpretabilityresearchistoscalablyexplainthemanyunanticipated
behaviorsofneuralnetworks(NNs). MuchrecentworkexplainsNNbehaviorsinterms
ofcoarse-grainedmodelcomponents,forexamplebyimplicatingcertaininductionheads
inin-contextlearning(Olssonetal.,2022)orMLPmodulesinfactualrecall(Mengetal.,
2022; Geva et al., 2023; Nanda et al., 2023, inter alia). However, such components are
generally polysemantic (Elhage et al., 2022) and hard to interpret, making it difficult to
applymechanisticinsightstodownstreamapplications. Ontheotherhand,priormethods
analyzingbehaviorsintermsoffine-grainedunits(Kimetal.,2018;Belinkov,2022;Geiger
etal.,2023;Zouetal.,2023)demandthatresearchersbeginwiththeanswer: theyassume
the existence of curated data that isolates the target behavior. Such approaches are not
well-suitedfordiscoveringunanticipatedmechanisms.
Weproposetoexplainmodelbehaviorsusingfine-grainedcomponentsthatplaynarrow,
interpretableroles. Doingsorequiresustoaddresstwochallenges: First,wemustidentify
the correct fine-grained unit of analysis, since obvious choices like neurons1 are rarely
interpretable,andunitsdiscoveredviasupervisedmethodsrequirepre-existinghypotheses.
Second,wemustaddressthescalabilityproblemposedbysearchingforcausalcircuitsover
alargenumberoffine-grainedunits.
Weleveragerecentprogressindictionarylearning(Brickenetal.,2023;Cunninghametal.,
2024)totacklethefirstchallenge. Namely,wetrainsparseautoencoders(SAEs)toidentify
directionsinanLM’slatentspacewhichrepresenthuman-interpretablefeatures. Then,to
solvethescalabilitychallenge,weemploylinearapproximations(Sundararajanetal.,2017;
Nanda,2022;Syedetal.,2023)toefficientlyidentifySAEfeatureswhicharemostcausally
implicatedinmodelbehaviors,aswellasconnectionsbetweenthesefeatures. Theresultis
∗Correspondencetos.marks@northeastern.eduandaa.mueller@northeastern.edu.
1Weuse“neuron”torefertoabasis-aligneddirectioninanLM’slatentspace(notnecessarily
precededbyanonlinearity).
1
4202
raM
82
]GL.sc[
1v74691.3042:viXraData
Contrastive Pairs
Get feature circuit. Interpret circuit. Debug if desired.
§3 The boy near the teacher has
The boys near the teacher have m §3 m §3,4,5 m §4
Classification Data
§4 His research is in … Professor
She worked in the OR … Nurse
Auto-discovered Behaviors
§5
Part 1, Part 2, Part 3, Part 4
Figure 1: Overview. Given contrastive input pairs, classification data, or automatically
discoveredmodelbehaviors,wediscovercircuitscomposedofhuman-interpretablesparse
featurestoexplaintheirunderlyingmechanisms. Wethenlabeleachfeatureaccordingto
whatitactivatesonorcausestohappen. Finally,ifdesired,wecanablatespuriousfeatures
outofthecircuittomodifyhowthesystemgeneralizes.
asparsefeaturecircuitwhichexplainshowmodelbehaviorsariseviainteractionsamong
fine-grainedhuman-interpretableunits.
Sparsefeaturecircuitscanbeproductivelyusedindownstreamapplications. Weintroduce
atechnique,SparseHuman-InterpretableFeatureTrimming(SHIFT),whichshiftsthegener-
alizationofanLMclassifierbysurgicallyremovingsensitivitytounintendedsignals—even
withoutknowingwhatthosesignalsareinadvance. WedemonstrateSHIFTbydebiasinga
classifierinaworst-casesettingwhereanunintendedsignal(gender)isperfectlypredictive
oftargetlabels(profession).
Finally,wedemonstrateourmethod’sscalabilitybyautomaticallydiscoveringthousandsof
LMbehaviorswiththeclusteringapproachofMichaudetal.(2023),andthenautomatically
discoveringfeaturecircuitsforthesebehaviors.
Ourcontributionsaresummarizedasfollows(Figure1):
1. Ascalablemethodtodiscoversparsefeaturecircuits. Wevalidateourmethodby
evaluatingfeaturecircuitsonasuiteofsubject-verbagreementtasks.
2. SHIFT,atechniqueforremovingsensitivitytounintendedsignalswithoutdisam-
biguatingdata.
3. A fully-unsupervised automatic feature circuit discovery pipeline that identifies
circuitsforthousandsofautomaticallydiscoveredLMbehaviors.
Wereleasecode,dataandautoencodersatgithub.com/saprmarks/feature-circuits.
2 Formulation
Feature disentanglement with sparse autoencoders. A fundamental challenge in NN
interpretabilityisthatindividualneuronsarerarelyinterpretable(Elhageetal.,2022). Re-
cently,Cunninghametal.(2024);Brickenetal.(2023)haveshownthatsparseautoencoders
(SAEs)canbeusedtoidentifyinterpretabledirections.WecloselyfollowBrickenetal.(2023)
totrainSAEsforattentionoutputs,2MLPoutputs,andresidualstreamactivationsforeach
layerofPythia-70M(Bidermanetal.,2023). Givenaninputactivationx∈Rd model fromone
ofthesemodelcomponents,thecorrespondingSAEcomputesadecomposition
d ∑SAE
x=xˆ+ϵ(x) = f (x)v +b+ϵ(x) (1)
i i
i=1
intoanapproximatereconstructionxˆ asasparsesumoffeaturesv andanSAEerrorterm
i
ϵ(x) ∈Rd model. Thefeaturesv i ∈Rd model areunitvectors,thefeatureactivations f i(x) ∈Rare
asparsesetofcoefficients,b∈Rd model isabias,andwetaked SAE =64·d model. TheSAEs
2Inotherwords,theoutputoftheattentionlayer’soutprojection.
2aretrainedtominimizeanL2reconstructionerrorandanL1regularizationtermwhich
promotessparsity. DetailsaboutourSAEsandtheirtrainingcanbefoundinAppendixB.
ScalablytrainingbetterSAEsisanactiveareaofresearch,andwedevelopourmethodswith
rapidfutureprogressinmind. However,atpresentSAEerrorsϵ(x)accountforarelatively
significant1–15%ofthevarianceinx. OurmethodshandletheseSAEerrorsgracefullyby
incorportingthemintooursparsefeaturecircuits;thisgivesaprincipleddecomposition
of model behaviors into contributions from potentially-interpretable features and error
componentsnotyetcapturedbyourSAEs. Weadditionallynotethatthemainbottleneckto
scalingourmethodstolargermodelsisthetrainingoftheSAEsthemselves. Asweexpect
increasingpublicavailabilityofSAEsforlargeopen-sourcemodels,wetreatscalingSAEs
themselvesasout-of-scopeandfocusonthescalabilityofourcoremethods.
Attributing causal effects with linear approximations. Let m be a real-valued metric
computedviaacomputationgraph(e.g.,aNN);leta∈Rd representanodeinthisgraph.
Followingpriorwork(Vigetal.,2020;Finlaysonetal.,2021),wequantifytheimportanceof
aonapairofinputs(x ,x )viaitsindirecteffect(IE;Pearl,2001)onm:
clean patch
(cid:16) (cid:12) (cid:17)
IE(m;a;x ,x ) = m x (cid:12)do(a=a ) −m(x ). (2)
clean patch clean(cid:12) patch clean
Herea isthevaluethatatakesinthecomputationofm(x ),andm(x |do(a =
patch patch clean
a ))denotesthevalueofmwhencomputingm(x )butinterveninginthecomputation
patch clean
ofmbymanuallysettingatoa . Forexample,giveninputsx =“Theteacher”and
patch clean
x =“The teachers,” we have metric m(x) = logP(“are”|x)−logP(“is”|x) the log
patch
probabilitydifferenceoutputbyaLM. Thenifaistheactivationof aparticularneuron,
alargevalueofIE(m;a;x ,x )indicatesthattheneuronishighlyinfluentialonthe
clean patch
model’sdecisiontooutput“is”vs.“are”onthispairofinputs.
We often want to compute IEs for a very large number of model components a, which
cannotbedoneefficientlywith(2). Wethusemploylinearapproximationsto(2)thatcan
becomputedformanyainparallel. Thesimplestsuchapproximation,attributionpatching
(Nanda,2022;Syedetal.,2023;Krama´retal.,2024),employsafirst-orderTaylorexpansion
(cid:16) (cid:17)
IˆE (m;a;x ,x ) = ∇ m| a −a (3)
atp clean patch a a=a patch clean
clean
whichestimates(2)foreveryainparallelusingonlytwoforwardandonebackwardpass.
Inpractice,sinceweusesmallmodels,wecaninsteademployamoreexpensivebutmore
accurateapproximationbasedonintegratedgradients(Sundararajanetal.,2017):
(cid:32) (cid:33)
IˆE (m;a;x ,x ) = ∑ ∇ m| (a −a ) (4)
ig clean patch a αa clean+(1−α)a
patch
patch clean
α
wherethesumin(4)rangesoverN =10equally-equallyspacedα ∈ {0, 1,..., N−1}. This
N N
cannotbedoneinparallelfortwonodeswhenoneisdownstreamofanother,butcanbe
doneinparallelforarbitrarilymanynodeswhichdonotdependoneachother. Thusthe
additionalcostofcomputingIˆE overIˆE scaleslinearlyinNandtheserialdepthofm’s
ig atp
computationgraph.
Theabovediscussionappliestothesettingwherewehaveapairofcleanandpatchinputs,
andwewouldliketounderstandthateffectofpatchingaparticularnodefromitscleanto
patchvalues. Butinsomesettings(§4,5),wehaveonlyasingleinputx. Inthiscase,we
insteadapproximatetheindirecteffectIE(m;a;x) = m(x|do(a=0))−m(x)ofsettingato
0. WegetthemodifiedformulasforIˆE(m;a;x)from(3)and(4)byreplacingawith0.
3 SparseFeatureCircuitDiscovery
Inthissection,weintroducesparsefeaturecircuits,whicharecomputationalsub-graphs
thatexplainmodelbehaviorsintermsofSAEfeaturesanderrorterms. Wefirstexplain
3SAE feature x m
SAE error
a1 a2
b1 b2
Submodule ϵ1 ϵ2
1 Cache activations and metric. 2 Backpropagate. 3 Compute effects. 4 Compute and
Store gradients. Filter nodes. filter edges.
m = log p(have) – log p(has)
m m ∇a1m m ∇ϵ1m m m
∇a2m ∇ϵ2m
a2 b2 ϵ2 a2 b2 ϵ2 a2 b2 ϵ2 a2 b2 a2 b2
a1 b1 ϵ1 a1 b1 ϵ1 a1 b1 ϵ1 b1 ϵ1 b1 ϵ1
IÊ(a,m)=∇am⋅( aa − aa )
x = The teacher x = The teachers IÊ(a,m)>TN
Figure2:Overviewofourmethod.Weviewourmodelasacomputationgraphthatincludes
SAEfeaturesanderrors. Wecacheactivations(Step1)andcomputegradients(Step2)for
eachnode. WethencomputeapproximateindirecteffectswithEq.(3;shown)or(4)and
filteraccordingtoanodethresholdT (Step3). Wesimilarlycomputeandfilteredges(Step
N
4);seeApp.A.1.
ourcircuitdiscoveryalgorithm(§3.1). Then,weanalyzecircuitsfoundwithourtechnique
to show that they are both more interpretable and more concise than circuits consisting
ofneurons(§3.2). Finally,weperformacasestudyofthecircuitswediscoveredfortwo
linguistictasksrelatedtosubject-verbagreement(§3.3).
3.1 Method
SupposewearegivenanLM M,SAEsforvarioussubmodulesof M(e.g.,attentionoutputs,
MLPoutputs,andresidualstreamvectors,asin§2),adatasetD consistingeitherofcon-
trastivepairs(x ,x )ofinputsorofsingleinputs x,andametricmdependingon
clean patch
M’soutputwhenprocessingdatafromD. Forexample,Figure2showsthecasewhereD
consistsofpairsofinputswhichdifferinnumber,andmisthelogprobabilitydifference
between Moutputtingtheverbformthatiscorrectforthepatchvs.cleaninput.
ViewingSAEfeaturesaspartofthemodel. Akeyideaunderpinningourmethodisthat,
byapplyingthedecomposition(1)tovarioushiddenstatesxintheLM,wecanviewthe
feature activations f and SAE errors ϵ as being part of the LM’s computation. We can
i
thus represent the model as a computation graph G where nodes correspond to feature
activationsorSAEerrorsatparticulartokenpositions.
ApproximatingtheIEofeachnode. LetIˆEbeoneofIˆE orIˆE (see§2). Thenforeach
atp ig
node a in G and input x ∼ D, we compute IˆE(m;a;x). We apply some choice of node
thresholdT toselectnodeswithalarge(absolute)IE.
N
Consistentwithpriorwork(Nanda,2022;Krama´retal.,2024),wefindthatIˆE accurately
atp
estimatesIEsforSAEfeaturesandSAEerrors,withtheexceptionofnodesinthelayer0
MLPandearlyresidualstreamlayers,whereIˆE underestimatesthetrueIE.Wefindthat
atp
IˆE significantlyimprovesaccuracyforthesecomponents,soweuseitinourexperiments
ig
below. SeeAppendixCformoreinformationaboutlinearapproximationquality.
ApproximatingtheIEofedges.Usingananalogouslinearapproximation,wealsocompute
theaverageIEofedgesinthecomputationgraph. Althoughtheideaissimple,themath-
ematicsaresomewhatinvolved,sowerelegatethedetailstoApp.A.1. Aftercomputing
theseIEs,wefilterforedgeswithabsoluteIEexceedingsomeedgethresholdT .
E
Aggregation across token positions and examples. For templatic data where tokens
in matching positions play consistent roles (see §3.2, 3.3), we take the mean effect of
nodes/edges across examples. For non-templatic data (§4, 5) we first sum the effects
4(a) Faithfulness of C (b) Faithfulness of M \ C
1
1 1 features fFeeaatutruerse circuit features
0.8 features_wo_errs fFeeaatutruerse_w coi_rcerursit w/o SAE errors features_wo_errs
0.8 0.8 0.6 f ne ea ut ru or nes s_wo_some_errs f nF Ne eea euat urut orru oners sne_ w cco ii r_r ccso uum ii tte w_e/rors attention/MLP SAE errors f ne ea ut ru or nes s_wo_some_errs
0.6
0.4
0.6
0.4 0.2
0.2 0.4 0
0 0 500 0.2 1000 1500 0 50 100 150 200 250 300
0 Nodes Nodes Nodes
Figure3: Faithfulnessforcircuits(a)andtheircomplements(b),measuredonheld-outdata.
−0.2
FaintlinescorrespondtothestructuresfromTable1,withtheaverageinbold. Theideal
50 100 150 200 250 300
faithfulnessforcircuitsis1,whiletheidNeodaeslscorefortheircomplementsis0.
ofcorrespondingnodes/edgesacrosstokenpositionbeforetakingtheexample-wisemean.
SeeApp.A.2.
Practicalconsiderations.Variouspracticaldifficultiesariseforefficientlycomputingthegra-
dientsneededbyourmethod.Wesolveusingacombinationofstopgradients,pass-through
gradients,andtricksforefficientJacobian-vectorproductcomputation;seeApp.A.3.
3.2 DiscoveringandEvaluatingSparseFeatureCircuitsforSubject–VerbAgreement
Toevaluateourmethod,wediscoversparsefeaturecircuits(henceforth,featurecircuits)
forfourvariantsofthesubject-verbagreementtask(Table1). Specifically,weadaptdata
fromFinlaysonetal.(2021)toproducedatasetsconsistingofcontrastivepairsofinputs
that differ only in the grammatical number of the subject; the model’s task is to choose
theappropriateverbinflection. WeusethisdataalongwithourSAEsfrom§2todiscover
circuitsforPythia-70M.
Weevaluatecircuitsforinterpretability,faithfulness,andcompleteness. Foreachcriterion,
wecomparetoneuroncircuitsdiscoveredbyapplyingourmethodswithneuronsinplaceof
sparsefeatures;inthissetting,therearenoerrortermsϵ.
Interpretability. We asked human crowdworkers to rate the interpretability of random
features,randomneurons,featuresfromourfeaturecircuits,andneuronsfromourneuron
circuits.Crowdworkersratesparsefeaturesassignificantlymoreinterpretablethanneurons,
withfeaturesthatparticipateinourcircuitsalsobeingmoreinterpretablethanrandomly
sampledones;seeApp.D.
Faithfulness. GivenacircuitCandmetricm,letm(C)denotetheaveragevalueofmoverD
whenrunningourmodelwithallnodesoutsideofCmean-ablated,i.e.,settotheiraverage
valueoverdatafromD.3 Wethenmeasurefaithfulnessas m(C)−m(∅) ,where∅ denotesthe
m(M)−m(∅)
emptycircuitand Mdenotesthefullmodel. Intuitively,thismetriccapturestheproportion
ofthemodel’sperformanceourcircuitexplains,relativetomeanablatingthefullmodel
(whichrepresentsthe“prior”performanceofthemodelwhenitisgiveninformationabout
the task, but not about specific inputs). We find that model components in early layers
typicallyhandlespecifictokens;sincethetokensappearinginourheld-outevaluationset
don’tnecessarilyalignwiththoseinourcircuitdiscoveryset,thereisn’tprimafaciereason
forearlycircuitcomponentstogeneralize. Wethusmeasurethefaithfulnessofourcircuits
startingatlayer2.
We plot faithfulness for feature circuits and neuron circuits after sweeping over node
thresholds (Figure 3a). We find that small feature circuits explain a large proportion of
model behavior: the majority of performance is explained by less than 100 nodes. In
contrast, around 1500 neurons are required to explain half the performance. However,
asSAEerrorsarehigh-dimensionalandcoarse-grained,theycannotbefairlycompared
toneurons;wethusalsoplotthefaithfulnessoffeaturecircuitswithallSAEerrornodes
3FollowingWangetal.(2023),weablatefeaturesbysettingthemtotheirmeanposition-specific
values.
5
ssenlufhtiaF ssenlufhtiaF
ssenlufhtiaF
ssenlufhtiaFStructure Examplecleaninput Exampleoutput
Simple Theparents p(is)−p(are)
WithinRC Theathletethatthemanagers p(likes)−p(like)
AcrossRC Theathletethatthemanagerslike p(do)−p(does)
AcrossPP Thesecretariesnearthecars p(has)−p(have)
Table1: Examplecleaninputsxandoutputsmforsubject-verbagreementtasks.
has/have
Verb form
discriminators
Embeddings, layer 0-4 MLP, resid Layers 2-3 attn, MLP, resid
PP/RC end detection
Noun number PP/RC detection
detection
4 4 15 5
33 8
Layers 4-5 attn, resid
The girl/girls that the teacher sees
Figure4: SummaryofthecircuitforagreementacrossRC(fullcircuitinappendixF.1). The
modeldetectsthenumberofthesubject. Then,itdetectsthestartofaPP/RCmodifyingthe
subject. Verbformdiscriminatorspromoteparticularverbinflections(singularorplural).
SquaresshownumberoffeaturenodesinthegroupandtrianglesshownumberofSAE
errornodes,withtheshadingindicatingthesumofIˆEtermsacrossnodesinthegroup.
removed,orwithallattentionandMLPerrornodesremoved. Wefind(unsurprisingly)that
removingresidualstreamSAEerrorsseverelydisruptsthemodelandcurtailsitsmaximum
performance. RemovingMLPandattentionerrorsislessdisruptive.
Completeness. Aretherepartsofthemodelbehaviorthatourcircuitfailstocapture? We
measurethisasthefaithfulnessofthecircuit’scomplement M\C(Figure3b). Weobserve
thatwecaneliminatethemodel’staskperformancebyablatingonlyafewnodesfromour
featurecircuits,andthatthisistrueevenwhenweleaveallSAEerrorsinplace. Incontrast,
ittakeshundredsofneuronstoacheivethesameeffect.
3.3 Casestudy: Subject–verbagreementacrossarelativeclause
Wefindthatinspectingsmallfeaturecircuitsproducedbyourtechniquecanprovideinsights
intohowPythia-70Marrivesatobservedbehaviors. Toillustratethis,wepresentacase
studyofasmallfeaturecircuitforsubject–verbagreementacrossarelativeclause(RC).
To keep the number of nodes we need to annotate manageable, we set a relatively high
nodethresholdof0.1,resultinginacircuitwith69nodesandfaithfulness.19(computed
asin§3.2). WesummarizethiscircuitinFigure4;thefullcircuit(aswellassmallcircuits
forothersubject-verbagreementtasks)canbefoundinApp.F.1. WedepictSAEfeatures
withrectanglesandSAEerrorswithtriangles. Wegenerallynoticedcircuitsdiscovered
withqualitativelybetterSAEsattributedasmallerproportionofthetotaleffecttoSAEerror
nodes;thissuggeststhatourcircuitdiscoverytechniquecouldbeadaptedintoameasureof
SAEquality.
OurcircuitdepictsaninterpretablealgorithmwhereinPythia-70Mselectsappropriateverb
formsviatwopathways. ThefirstpathwayconsistsofMLPandembeddingfeatureswhich
detectthenumberofthemainsubjectandthengenericallypromotematchingverbforms.
Thesecondpathwaybeginsthesame,butmovestherelevantnumberinformationtothe
endoftherelativeclausebyusingPP/RCboundarydetectors.
We find significant overlap between this circuit and the circuit we discovered for agree-
mentacrossaprepositionalphrase,withPythia-70Mhandlingthesesyntacticallydistinct
structuresinamostlyuniformway. InaccordancewithFinlaysonetal.(2021),wefindless
overlapwithourcircuitsforsimpleagreementandwithinRCagreement(AppendixF.1).
6Accuracy
Method ↑Profession ↓Gender ↑Worstgroup
Original 61.9 87.4 24.4
CBP 82.5 55.0 63.1
SHIFT 88.5 54.0 76.0
SHIFT+retrain 93.1 52.0 89.0
Neuronskyline 80.6 65.6 46.5
Featureskyline 88.5 54.0 62.9
Oracle 93.0 49.4 91.9
Table2: Accuraciesonbalanceddatafortheground-truthlabel(profession)andspurious
label(gender). “Worstgroupaccuracy”referstowhicheverprofessionaccuracyislowest
amongmaleprofessors,malenurses,femaleprofessors,femalenurses.
4 Application: Removingunintendedsignalsfromaclassifierwithout
disambiguatinglabels
NNclassifiersoftenrelyonunintendedsignals—e.g.,spuriousfeatures. Nearlyallprior
workonthisproblemreliesonaccesstodisambiguatinglabeleddatainwhichunintended
signalsarelesspredictiveoflabelsthanintendedones. However,sometaskshavestructural
propertieswhichdisallowthisassumption. Forexample,inputsfordifferentclassesmight
comefromdifferentdatasources(Zechetal.,2018).Additionally,somehaveraisedconcerns
(Ngoetal.,2024;Casperetal.,2023)thatsophisticatedLMstrainedwithhumanfeedback
(Christianoetal.,2023)insettingswitheasy-to-harddomainshift(Burnsetal.,2023;Hase
etal.,2024)willbemisalignedbecause,inthesesettings,“overseerapproval”and“desirable
behavior”areequallypredictiveoftrainingrewardlabels.Morefundamentally,theproblem
withunintendedsignalsisthattheyareunintended—nottheyareinsufficientlypredictive—
andwewouldlikeourmethodstoreflectthis.
WethusproposeSpuriousHuman-interpretableFeatureTrimming(SHIFT),whereahuman
changesthegeneralizationofaclassifierbyeditingitsfeaturecircuit. WeshowthatSHIFT
removessensitivitytounintendedsignalswithoutaccesstodisambiguatinglabeleddata,or
evenwithoutknowingwhatthesignalsareaheadoftime.
Method. SupposewearegivenlabeledtrainingdataD = {(x,y )};anLM-basedclassifier
i i
CtrainedonD;andSAEsforvariouscomponentsofC. ToperformSHIFT,we:
1. Applythemethodsfrom§3tocomputeafeaturecircuitthatexplainsC’saccuracy
oninputs(x,y) ∼ D(e.g.,usingmetricm = −logC(y|x)).
2. Manuallyinspectandevaluatefortask-relevancyeachfeatureinthecircuitfrom
Step1.
3. Ablatefrom Mfeaturesjudgedtobetask-irrelevanttoobtainaclassifierC′.
4. (Optional)Furtherfine-tuneC′ ondatafromD.
Step3removestheclassifier’sdependenceonunintendedsignalswecanidentify,butmay
disruptperformancefortheintendedsignal.Step4canbeusedtorestoresomeperformance.
Experimentalsetup.WeillustrateSHIFTusingtheBiasinBiosdataset(BiB;De-Arteagaetal.,
2019). BiBconsistsofprofessionalbiographies,andthetaskistoclassifyanindividual’s
professionbasedontheirbiography. BiBalsoprovideslabelsforaspuriousfeature: gender.
WesubsampleBiBtoproducetwosetsoflabeleddata:
• Theambiguousset,consistingofbiosofmaleprofessors(labeled0)andfemale
nurses(labeled1).
• Thebalancedset,consistingofanequalnumberofbiosformaleprofessors,male
nurses,femaleprofessors,andfemalenurses. Thesedatacarryprofessionlabels
(theintendedsignal)andgenderlabels(theunintendedsignal).
7The ambiguous set represents a worst-case scenario: the unintended signal is perfectly
predictiveoftraininglabels. Givenonlyaccesstotheambiguousset,ourtaskistoproduce
aprofessionclassifierwhichisaccurateonthebalancedset.
We train a linear classifier based on Pythia-70M using the ambiguous set; see App. E.1
fordetails. Weapply SHIFT byfirstdiscoveringacircuitusingthezero-ablationvariant
describedin§3.1;thiscircuit,showninApp.F.2,contains67features. Wemanuallyinterpret
eachfeatureusinganinterfacesimilartothatshowninApp.D;namely,weinspectcontexts
andtokensfromThePile(Gaoetal.,2020)thatmaximallyactivatethefeature,aswellas
tokenswhoselog-probabilitiesaremostaffectedbyablatingthefeature. Wejudge55of
thesefeaturestobetask-irrelevant(e.g.,featuresthatpromotefemale-associatedlanguage
in biographies of women, as in Figure 18; see App. G for more examples). Although
thisinterpretabilitystepusesadditionalunlabeleddata,weemphasizethatweneveruse
additionallabeleddata(orevenadditionalunlabeledinputsfromtheclassificationdataset).
Baselinesandskylines. TocontextualizetheperformanceofSHIFT,wealsoimplement:
• SHIFTwithneurons. PerformSHIFT,butusingneuronsinsteadofSAEfeatures.
• ConceptBottleneckProbing(CBP),adaptedfromYanetal.(2023)(originallyfor
multimodaltext/imagemodels). CBPworksbytrainingaprobetoclassifyinputs
xgivenaccessonlytoavectorofaffinitiesbetweentheLM’srepresentationof x
andvariousconceptvectors. SeeApp.E.2forimplementationdetails.
• Featureskyline. Insteadofrelyingonhumanjudgementtoevaluatewhethera
featureshouldbeablated,weablatethe55featuresfromourcircuitthataremost
causallyimplicatedinspuriousfeatureaccuracyonthebalancedset.
• Neuronskyline. Thesameasthefeatureskyline,butusing55neuronsinstead.
• Oracle. Aclassifiertrainedonground-truthlabelsonthebalancedset.
Results. Wefind(Table2)thatSHIFTnear-completelyremovesourclassifier’sdependence
ongenderinformation,withStep3ofSHIFTprovidingthebulkoftheimprovement. We
furtherfindourjudgementsofwhichfeaturesaretask-relevanttobehighlyinformative:
SHIFTwithoutretrainingmatchesthefeatureskyline.
Moreover, SHIFT criticallyreliesontheuseofSAEfeatures. Whenapplying SHIFT with
neurons,essentiallynoneoftheneuronsareinterpretable,makingitdifficulttotellifthey
oughttobeablated;seeAppendixGforexamples. Becauseofthis,weabandontheSHIFT
withneuronsbaseline.Evenusingthebalancedsettoselectneuronsforremoval(theneuron
skyline)failstomatchtheperformanceofSHIFT.
5 UnsupervisedCircuitDiscoveryatScale
Sofar,wehavereliedonhuman-collecteddatatospecifyLMbehaviorsforanalysis. How-
ever,LMsimplementnumerousinterestingbehaviors,manyofwhichmaybecounterintu-
itivetohumans. Inthissection,weadaptourtechniquestoproduceanearfully-automated
interpretabilitypipeline,startingfromrawtextdataandendingwiththousandsoffeature
circuitsforauto-discoveredmodelbehaviors.
Wedothisintwosteps:
1. Behaviordiscovery. Weimplementvariantsofthequantadiscoveryapproachfrom
Michaudetal.(2023),whichworkbyclusteringcontextsbasedonvectorsderived
from Pythia-70M activations, gradients or both. Implementation details can be
foundinApp.H.
2. Circuitdiscovery. Givenacluster,weapplythezero-ablationvariantofourtech-
niquefrom§3usingdatasetD = {(x,y )},thesetofcontextsintheclustertogether
i i
withthenexttokenappearinginThePile,andmetricm = −logP(y |x ).
i i
Wepresentexampleclusters,aswellasinterestingfeaturesparticipatingintheirassociated
circuits(Figure5). FullannotatedcircuitscanbefoundinApp.F.3, andaninterfacefor
exploringallofourclustersand(unlabeled)circuitscanbefoundatfeature-circuits.xyz.
8Cluster 382: Incrementing sequences Cluster 475: “to” as infinitive object
var input = [1, 2, 3, 4, 5, 6, 7, 8 At issue, whether the defendant should be allowed to
Step 1. Download the latest CompsNY 3.49 Full British Prime Min David Cameron says in televised remarks he would like Britain to
Step 2. Double click the Setup file and follow the prompts […]
Step 3. After the main install closes, click OK […] Reader bloggers are asked to
Step 4
Example features involved: Example features involved:
Succession Narrow induction Objects which can precede Other words which precede
object complements infinitive objects
Chapter 1 A, B, C A3 … A → 3 or III or 4 …
Chapter 2
Chapter 3 I, II, III, IV A7 … A → 7 or vii or 8 … Direct the user to It’s up to you to According to This infection leads to
Figure 5: Example clusters and features which participate in their circuits. Features are
activeontokensshadedblueandpromotetokensshadedinred. (left)Anexamplenarrow
inductionfeaturerecognizesthepattern A3...Aandcopiesinformationfromthe3token.
Thiscomposeswithasuccessionfeaturetoimplementtheprediction A3...A →4. (right)
Onefeaturepromotes“to”afterwordswhichcantakeinfinitiveobjects. Aseparatefeature
activatesonobjectsofverbsorprepositionsandpromotes“to”asanobjectcomplement.
Whileevaluatingtheseclustersandcircuitsisanimportantopenproblem,wegenerally
findthattheseclustersexposeinterestingLMbehaviors,andthattheirrespectivefeature
circuits can provide useful insights on mechanisms of LM behavior. For instance, we
automatically discover attention features implicated in succession and induction, two
phenomena thoroughly studied in prior work at the attention head level using human-
curateddata(Olssonetal.,2022;Gouldetal.,2023).
Feature circuits can also shed interesting light on their clusters. For example, while the
clustersinFigure5seematfirsttoeachrepresentasinglemechanism,circuit-levelanalysis
revealsinbothcasesaunionofdistinctmechanisms.Forcluster475,Pythia-70Mdetermines
whether“to[verb]”isanappropriateobjectintwodistinctmanners(seeFigure5caption).
Andforcluster382,thepredictionofsuccessorsreliesongeneralsuccesionfeatures,aswell
asmultiplenarrowinductionfeatureswhichrecognizepatternslike“A3...A”.
6 RelatedWork
Causal interpretability. Interpretability research has applied causal mediation analysis
(Pearl,2001;Robins&Greenland,1992)tounderstandthemechanismsunderlyingparticular
modelbehaviorsandtheiremergence(Yuetal.,2023;Gevaetal.,2023;Hannaetal.,2023;
Toddetal.,2024;Prakashetal.,2024;Chenetal.,2024,interalia). Thistypicallyrelieson
counterfactualinterventions(Lewis,1973),suchasactivationpatchingorpathpatchingon
coarse-grainedcomponents(Conmyetal.,2023;Wangetal.,2023). Sometechniquesaimto,
givenahypothesizedcausalgraph,identifyamatchingcausalmechanisminanLM(Geiger
etal.,2021;2022;2023);incontrast,weaimheretodiscovercausalmechansismswithout
startingfromsuchhypotheses.
Robustnesstospuriouscorrelations. Thereisalargeliteratureonmitigatingrobustnessto
spuriouscorrelations,includingtechniqueswhichrelyondirectlyoptimizingworst-group
accuracy(Sagawaetal.,2020;Orenetal.,2019;Zhangetal.,2021;Sohonietal.,2022;Nam
etal.,2022),automaticallyormanuallyreweightingdatabetweengroups(Liuetal.,2021;
Nametal.,2020;Yaghoobzadehetal.,2021;Utamaetal.,2020;Creageretal.,2021;Idrissi
et al., 2022; Orgad & Belinkov, 2023), training classifiers with more favorable inductive
biases (Kirichenko et al., 2023; Zhang et al., 2022; Iskander et al., 2024), or editing out
undesiredconcepts(Iskanderetal.,2023;Belroseetal.,2023;Wangetal.,2020;Ravfogel
etal.,2020;2022a;b). Allofthesetechniquesrelyonaccesstodisambiguatinglabeleddatain
thesenseof§4. Sometechniquesfromasmallerliteraturefocusedonimageormultimodal
modelsapplywithoutsuchdata(Oikarinenetal.,2023;Yanetal.,2023). Ourmethodhere
isinspiredbytheapproachofGandelsmanetal.(2024)basedoninterpretingandablating
undesiredattentionheadsinCLIP.
9Featuredisentanglement. InadditiontotherecentworkSAEworkofCunninghametal.
(2024);Brickenetal.(2023),otherapproachestofeaturedisentanglementinclude(Schmid-
huber,1992;Desjardinsetal.,2012;Kim&Mnih,2018;Chenetal.,2016;Makhzani&Frey,
2013;Heetal.,2022;Peeblesetal.,2020;Schneider&Vlachos,2021;Burgessetal.,2018;
Chenetal.,2018;Higginsetal.,2017,interalia).
7 Conclusion
Wehaveintroducedamethodfordiscoveringcircuitsonsparsefeatures. Usingthismethod,
wediscoverhuman-interpretablecausalgraphsforasubject–verbagreementtask,aclassi-
fier,andthousandsofgeneraltokenpredictiontaskswherenoclearrightorwronganswer
exists. We can edit the set of features that models have access to by ablating sparse fea-
turesthathumansdeemspurious;wefindthatthisissignificantlymoreeffectivethana
neuron-basedablationmethodwhichhasanunfairadvantage.
Acknowledgments
WethankBuckSchlegeris,RyanGreenblatt,andNeelNandafordiscussionofideasup-
streamtotheexperimentsin§4. WethankLoganRiggsandJannikBrinkmannforhelp
trainingSAEs. WealsothankJoshEngelsandMaxTegmarkfordiscussionsaboutclustering
andsparseprojectionsrelatedto§5. S.M.issupportedbyanOpenPhilanthropyalignment
grant. C.R.issupportedbyManifundRegrants. E.J.MissupportedbytheNSFGraduate
ResearchFellowshipProgram(GrantNo.2141064). Y.B.issupportedbytheIsraelScience
Foundation(GrantNo.448/20)andanAzrieliFoundationEarlyCareerFacultyFellow-
ship. Y.B.andD.B.aresupportedbyajointOpenPhilanthropyalignmentgrant. A.M.is
supportedbyaZuckermanpostdoctoralfellowship.
Limitations
ThesuccessofourtechniquereliesonaccesstoSAEsforagivenmodel. TrainingsuchSAEs
currentlyrequiresalarge(butone-time)upfrontcomputecost, whichposesanobstacle
torunningourmethods. Additionally,modelcomponentsnotcapturedbytheSAEswill
remainuninterpretableafterapplyingourmethod.
Muchofourevaluationisqualitative. Whilewehavequantitativeevidencethatfeature
circuitsareusefulforimprovinggeneralizationwithoutadditionaldata(§4),evaluating
dictionariesandcircuitswithoutdownstreamtasksischallenging.
References
YonatanBelinkov. Probingclassifiers: Promises,shortcomings,andadvances. Computational
Linguistics,48(1):207–219,2022.
NoraBelrose,DavidSchneider-Joseph,ShauliRavfogel,RyanCotterell,EdwardRaff,and
StellaBiderman. LEACE:Perfectlinearconcepterasureinclosedform. InThirty-seventh
ConferenceonNeuralInformationProcessingSystems,2023. URLhttps://openreview.net/
forum?id=awIpKpwTwF.
Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle
O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai
Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models
acrosstrainingandscaling. InInternationalConferenceonMachineLearning,pp.2397–2430.
PMLR,2023.
Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Con-
erly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan
Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-
Dodds,AlexTamkin,KarinaNguyen,BraydenMcLean,JosiahEBurke,TristanHume,
10Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: De-
composinglanguagemodelswithdictionarylearning. TransformerCircuitsThread,2023.
https://transformer-circuits.pub/2023/monosemantic-features/index.html.
ChristopherP.Burgess, IrinaHiggins, ArkaPal, LoicMatthey, NickWatters, Guillaume
Desjardins,andAlexanderLerchner. Understandingdisentanglinginβ-vae,2018.
CollinBurns,PavelIzmailov,JanHendrikKirchner,BowenBaker,LeoGao,LeopoldAschen-
brenner,YiningChen,AdrienEcoffet,ManasJoglekar,JanLeike,IlyaSutskever,andJeff
Wu. Weak-to-stronggeneralization: Elicitingstrongcapabilitieswithweaksupervision,
2023.
Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Je´re´my Scheurer,
JavierRando,RachelFreedman,TomaszKorbak,DavidLindner,PedroFreire,TonyWang,
SamuelMarks,Charbel-Raphae¨lSegerie,MicahCarroll,AndiPeng,PhillipChristoffersen,
Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau,
EricJ.Michaud,JacobPfau,DmitriiKrasheninnikov,XinChen,LauroLangosco,Peter
Hase,ErdemBıyık,AncaDragan,DavidKrueger,DorsaSadigh,andDylanHadfield-
Menell. Open problems and fundamental limitations of reinforcement learning from
humanfeedback,2023.
AngelicaChen,RavidShwartz-Ziv,KyunghyunCho,MatthewLLeavitt,andNaomiSaphra.
Suddendropsintheloss: Syntaxacquisition,phasetransitions,andsimplicitybiasin
MLMs. In The Twelfth International Conference on Learning Representations, 2024. URL
https://openreview.net/forum?id=MO5PiKHELW.
Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of
disentanglement in variational autoencoders, 2018. URL https://openreview.net/
forum?id=BJdMRoCIf.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel.
Infogan: interpretable representation learning by information maximizing generative
adversarialnets. InProceedingsofthe30thInternationalConferenceonNeuralInformation
ProcessingSystems,NIPS’16,pp.2180–2188,RedHook,NY,USA,2016.CurranAssociates
Inc. ISBN9781510838819.
PaulChristiano,JanLeike,TomB.Brown,MiljanMartic,ShaneLegg,andDarioAmodei.
Deepreinforcementlearningfromhumanpreferences,2023.
ArthurConmy,AugustineN.Mavor-Parker,AengusLynch,StefanHeimersheim,andAdria`
Garriga-Alonso. Towardsautomatedcircuitdiscoveryformechanisticinterpretability. In
Thirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
ElliotCreager,Joern-HenrikJacobsen,andRichardZemel. Environmentinferenceforinvari-
antlearning. InMarinaMeilaandTongZhang(eds.),Proceedingsofthe38thInternational
ConferenceonMachineLearning,volume139ofProceedingsofMachineLearningResearch,
pp. 2189–2200. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/
creager21a.html.
HoagyCunningham,AidanEwart,LoganRiggs,RobertHuben,andLeeSharkey. Sparse
autoencoders find highly interpretable features in language models. In The Twelfth
InternationalConferenceonLearningRepresentations,2024. URLhttps://openreview.net/
forum?id=F76bwRSLeK.
Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs,
Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman
Kalai. Bias in bios: A case study of semantic representation bias in a high-stakes set-
ting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT*
’19, pp. 120–128, New York, NY, USA, 2019. Association for Computing Machinery.
ISBN9781450361255. doi: 10.1145/3287560.3287572. URLhttps://doi.org/10.1145/
3287560.3287572.
11Guillaume Desjardins, Aaron Courville, and Yoshua Bengio. Disentangling factors of
variationviagenerativeentangling. ComputingResearchRepository,arXiv:1210.5474,2012.
NelsonElhage,TristanHume,CatherineOlsson,NicholasSchiefer,TomHenighan,Shauna
Kravec,ZacHatfield-Dodds,RobertLasenby,DawnDrain,CarolChen,RogerGrosse,
Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher
Olah. Toymodelsofsuperposition. TransformerCircuitsThread,2022. https://transformer-
circuits.pub/2022/toy model/index.html.
MatthewFinlayson,AaronMueller,SebastianGehrmann,StuartShieber,TalLinzen,and
YonatanBelinkov. Causalanalysisofsyntacticagreementmechanismsinneurallanguage
models. InChengqingZong,FeiXia,WenjieLi,andRobertoNavigli(eds.),Proceedings
of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
InternationalJointConferenceonNaturalLanguageProcessing(Volume1: LongPapers),pp.
1828–1843, Online, August 2021. Association for Computational Linguistics. doi: 10.
18653/v1/2021.acl-long.144. URLhttps://aclanthology.org/2021.acl-long.144.
YossiGandelsman,AlexeiA.Efros,andJacobSteinhardt. InterpretingCLIP’simagerepre-
sentationviatext-baseddecomposition. ComputingResearchRepository,arXiv:2310.05916,
2024.
LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,Jason
Phang,HoraceHe,AnishThite,NoaNabeshima,ShawnPresser,andConnorLeahy. The
Pile: An800GBdatasetofdiversetextforlanguagemodeling,2020.
AtticusGeiger,HansonLu,ThomasIcard,andChristopherPotts. Causalabstractionsof
neuralnetworks. InM.Ranzato,A.Beygelzimer,Y.Dauphin,P.S.Liang,andJ.Wortman
Vaughan(eds.),AdvancesinNeuralInformationProcessingSystems,volume34,pp.9574–
9586. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_
files/paper/2021/file/4f5c422f4d49a5a807eda27434231040-Paper.pdf.
AtticusGeiger,ZhengxuanWu,HansonLu,JoshRozner,ElisaKreiss,ThomasIcard,Noah
Goodman, and Christopher Potts. Inducing causal structure for interpretable neural
networks. 162:7324–7338,17–23Jul2022. URLhttps://proceedings.mlr.press/v162/
geiger22a.html.
Atticus Geiger, Chris Potts, and Thomas Icard. Causal abstraction for faithful model
interpretation. ComputingResearchRepository,arXiv:2301.04709,2023.
MorGeva,JasmijnBastings,KatjaFilippova,andAmirGloberson.Dissectingrecalloffactual
associations in auto-regressive language models. In Houda Bouamor, Juan Pino, and
KalikaBali(eds.),Proceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,pp.12216–12235,Singapore,December2023.AssociationforComputational
Linguistics. doi: 10.18653/v1/2023.emnlp-main.751. URLhttps://aclanthology.org/
2023.emnlp-main.751.
RhysGould,EuanOng,GeorgeOgden,andArthurConmy. Successorheads: Recurring,
interpretableattentionheadsinthewild. ComputingResearchRepository,arXiv:2312.09230,
2023.
Michael Hanna, Ollie Liu, and Alexandre Variengien. How does GPT-2 compute
greater-than?: Interpretingmathematicalabilitiesinapre-trainedlanguagemodel. In
Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:
//openreview.net/forum?id=p4PckNQR8k.
PeterHase,MohitBansal,PeterClark,andSarahWiegreffe. Theunreasonableeffectiveness
ofeasytrainingdataforhardtasks,2024.
T. He, Z. Li, Y. Gong, Y. Yao, X. Nie, and Y. Yin. Exploring linear feature disentangle-
ment for neural networks. In 2022 IEEE International Conference on Multimedia and
Expo (ICME), pp. 1–6, Los Alamitos, CA, USA, jul 2022. IEEE Computer Society. doi:
10.1109/ICME52920.2022.9859978. URL https://doi.ieeecomputersociety.org/10.
1109/ICME52920.2022.9859978.
12Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew
Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic vi-
sualconceptswithaconstrainedvariationalframework. InInternationalConferenceon
LearningRepresentations,2017. URLhttps://openreview.net/forum?id=Sy2fzU9gl.
BadrYoubiIdrissi,MartinArjovsky,MohammadPezeshki,andDavidLopez-Paz. Simple
databalancingachievescompetitiveworst-group-accuracy. InBernhardScho¨lkopf,Car-
olineUhler,andKunZhang(eds.),ProceedingsoftheFirstConferenceonCausalLearning
andReasoning,volume177ofProceedingsofMachineLearningResearch,pp.336–351.PMLR,
11–13Apr2022. URLhttps://proceedings.mlr.press/v177/idrissi22a.html.
ShadiIskander,KiraRadinsky,andYonatanBelinkov. Shieldedrepresentations: Protect-
ing sensitive attributes through iterative gradient-based projection. In Anna Rogers,
Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Com-
putational Linguistics: ACL 2023, pp. 5961–5977, Toronto, Canada, July 2023. Associ-
ation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.369. URL
https://aclanthology.org/2023.findings-acl.369.
ShadiIskander,KiraRadinsky,andYonatanBelinkov. Leveragingprototypicalrepresenta-
tionsformitigatingsocialbiaswithoutdemographicinformation. ComputingResearch
Repository,2403.09516,2024.
BeenKim,MartinWattenberg,JustinGilmer,CarrieCai,JamesWexler,FernandaViegas,
et al. Interpretability beyond feature attribution: Quantitative testing with concept
activation vectors (tcav). In International conference on machine learning, pp. 2668–2677.
PMLR,2018.
HyunjikKimandAndriyMnih. Disentanglingbyfactorising. InJenniferDyandAndreas
Krause(eds.),Proceedingsofthe35thInternationalConferenceonMachineLearning,volume80
ofProceedingsofMachineLearningResearch,pp.2649–2658.PMLR,10–15Jul2018. URL
https://proceedings.mlr.press/v80/kim18b.html.
DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. CoRR,
abs/1412.6980,2014. URLhttps://api.semanticscholar.org/CorpusID:6628106.
Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training
is sufficient for robustness to spurious correlations. Computing Research Repository,
arXiv:2204.02937,2023.
Ja´nosKrama´r,TomLieberum,RohinShah,andNeelNanda. AtP*: Anefficientandscalable
methodforlocalizingllmbehaviourtocomponents,2024.
DavidK.Lewis. Counterfactuals. Blackwell,Malden,Mass.,1973.
EvanZLiu,BehzadHaghgoo,AnnieSChen,AditiRaghunathan,PangWeiKoh,Shiori
Sagawa,PercyLiang,andChelseaFinn. Justtraintwice: Improvinggrouprobustness
withouttraininggroupinformation.InMarinaMeilaandTongZhang(eds.),Proceedingsof
the38thInternationalConferenceonMachineLearning,volume139ofProceedingsofMachine
LearningResearch,pp.6781–6792.PMLR,18–24Jul2021. URLhttps://proceedings.mlr.
press/v139/liu21f.html.
IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InInternational
ConferenceonLearningRepresentations,2017. URLhttps://api.semanticscholar.org/
CorpusID:53592270.
AlirezaMakhzaniandBrendanJ.Frey.k-sparseautoencoders.ComputingResearchRepository,
abs/1312.5663,2013. URLhttps://api.semanticscholar.org/CorpusID:14850799.
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing
factualassociationsinGPT. AdvancesinNeuralInformationProcessingSystems,36,2022.
arXiv:2202.05262.
13Eric J Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of
neuralscaling. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
URLhttps://openreview.net/forum?id=3tbTw2ga8K.
JunhyunNam,HyuntakCha,SungsooAhn,JaehoLee,andJinwooShin. Learningfrom
failure: Training debiased classifier from biased classifier. In Proceedings of the 34th
InternationalConferenceonNeuralInformationProcessingSystems,NIPS’20,RedHook,NY,
USA,2020.CurranAssociatesInc. ISBN9781713829546.
Junhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin. Spread spurious attribute:
Improvingworst-groupaccuracywithspuriousattributeestimation,2022.
NeelNanda.Attributionpatching:Activationpatchingatindustrialscale,2022. URLhttps:
//www.neelnanda.io/mechanistic-interpretability/attribution-patching.
Neel Nanda. Open source replication & commentary on Anthropic’s dictionary
learning paper, 2023. URL https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/
open-source-replication-and-commentary-on-anthropic-s.
Neel Nanda, Senthooran Rajamanoharan, Ja´nos Krama´r, and Rohin Shah. Fact
finding: Attempting to reverse-engineer factual recall on the neuron level,
2023. URL https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/
fact-finding-attempting-to-reverse-engineer-factual-recall.
RichardNgo,LawrenceChan,andSo¨renMindermann. Thealignmentproblemfromadeep
learningperspective. ComputingResearchRepository,arXiv:2209.00626,2024.
TuomasOikarinen,SubhroDas,LamM.Nguyen,andTsui-WeiWeng. Label-freeconcept
bottleneck models. In The Eleventh International Conference on Learning Representations,
2023. URLhttps://openreview.net/forum?id=FlCg47MNvBA.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn
Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy
Jones,JacksonKernion,LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,Jack
Clark,JaredKaplan,SamMcCandlish,andChrisOlah. In-contextlearningandinduction
heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-
context-learning-and-induction-heads/index.html.
YonatanOren,ShioriSagawa,TatsunoriB.Hashimoto,andPercyLiang. Distributionally
robustlanguagemodeling.InKentaroInui,JingJiang,VincentNg,andXiaojunWan(eds.),
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe
9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.4227–
4237, HongKong, China, November2019.AssociationforComputationalLinguistics.
doi: 10.18653/v1/D19-1432. URLhttps://aclanthology.org/D19-1432.
Hadas Orgad and Yonatan Belinkov. BLIND: Bias removal with no demographics. In
Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pp.
8801–8821,Toronto,Canada,July2023.AssociationforComputationalLinguistics. doi:
10.18653/v1/2023.acl-long.490. URLhttps://aclanthology.org/2023.acl-long.490.
Judea Pearl. Direct and indirect effects. In Proceedings of the Seventeenth Conference on
UncertaintyinArtificialIntelligence,UAI’01,pp.411–420,SanFrancisco,CA,USA,2001.
MorganKaufmannPublishersInc. ISBN1558608001.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P.Prettenhofer,R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,
M.Perrot,andE.Duchesnay. Scikit-learn: MachinelearninginPython. JournalofMachine
LearningResearch,12:2825–2830,2011.
WilliamPeebles,JohnPeebles,Jun-YanZhu,AlexeiA.Efros,andAntonioTorralba. The
hessian penalty: A weak prior for unsupervised disentanglement. In Proceedings of
EuropeanConferenceonComputerVision(ECCV),2020.
14NikhilPrakash,TamarRottShaham,TalHaklay,YonatanBelinkov,andDavidBau. Fine-
tuningenhancesexistingmechanisms: Acasestudyonentitytracking. InProceedingsof
the2024InternationalConferenceonLearningRepresentations,2024. arXiv:2402.14811.
ShauliRavfogel,YanaiElazar,HilaGonen,MichaelTwiton,andYoavGoldberg. Nullit
out: Guarding protected attributes by iterative nullspace projection. In Dan Jurafsky,
Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual
MeetingoftheAssociationforComputationalLinguistics,pp.7237–7256,Online,July2020.
AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.acl-main.647. URL
https://aclanthology.org/2020.acl-main.647.
ShauliRavfogel,MichaelTwiton,YoavGoldberg,andRyanDCotterell. Linearadversarial
concepterasure. InKamalikaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvari,
GangNiu,andSivanSabato(eds.),Proceedingsofthe39thInternationalConferenceonMa-
chineLearning,volume162ofProceedingsofMachineLearningResearch,pp.18400–18421.
PMLR, 17–23 Jul 2022a. URL https://proceedings.mlr.press/v162/ravfogel22a.
html.
ShauliRavfogel,FranciscoVargas,YoavGoldberg,andRyanCotterell. Adversarialconcept
erasure in kernel space. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.),
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,
pp. 6034–6055, Abu Dhabi, United Arab Emirates, December 2022b. Association for
Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.405. URL https://
aclanthology.org/2022.emnlp-main.405.
JamesM.RobinsandSanderGreenland. Identifiabilityandexchangeabilityfordirectand
indirecteffects. Epidemiology,3(2):143–155,1992. ISSN10443983. URLhttp://www.jstor.
org/stable/3702894.
ShioriSagawa,PangWeiKoh,TatsunoriB.Hashimoto,andPercyLiang. Distributionally
robustneuralnetworks. InInternationalConferenceonLearningRepresentations,2020. URL
https://openreview.net/forum?id=ryxGuJrFvS.
Ju¨rgen Schmidhuber. Learning Factorial Codes by Predictability Minimization. Neural
Computation,4(6):863–879,111992. ISSN0899-7667. doi: 10.1162/neco.1992.4.6.863. URL
https://doi.org/10.1162/neco.1992.4.6.863.
JohannesSchneiderandMichalisVlachos. Explainingneuralnetworksbydecodinglayer
activations,2021.
Nimit S. Sohoni, Maziar Sanjabi, Nicolas Ballas, Aditya Grover, Shaoliang Nie, Hamed
Firooz, and Christopher Re´. BARACK: Partially supervised group robustness with
guarantees,2022.
MukundSundararajan,AnkurTaly,andQiqiYan. Axiomaticattributionfordeepnetworks.
InProceedingsofthe34thInternationalConferenceonMachineLearning-Volume70,ICML’17,
pp.3319–3328.JMLR.org,2017.
AaquibSyed,CanRager,andArthurConmy. Attributionpatchingoutperformsautomated
circuitdiscovery. InNeurIPSWorkshoponAttributingModelBehavioratScale,2023. URL
https://openreview.net/forum?id=tiLbFR4bJW.
EricTodd,MillicentL.Li,ArnabSenSharma,AaronMueller,ByronC.Wallace,andDavid
Bau. Functionvectorsinlargelanguagemodels. InProceedingsofthe2024International
ConferenceonLearningRepresentations,2024.
Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna Gurevych. Towards debias-
ing NLU models from unknown biases. In Bonnie Webber, Trevor Cohn, Yulan He,
and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pp. 7597–7610, Online, November 2020. Associa-
tion for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.613. URL
https://aclanthology.org/2020.emnlp-main.613.
15JesseVig,SebastianGehrmann,YonatanBelinkov,SharonQian,DanielNevo,YaronSinger,
andStuartShieber. Investigatinggenderbiasinlanguagemodelsusingcausalmediation
analysis.InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(eds.),Advances
inNeuralInformationProcessingSystems,volume33,pp.12388–12401.CurranAssociates,
Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
92650b2e92217715fe312e6fa7b90d82-Paper.pdf.
KevinRoWang,AlexandreVariengien,ArthurConmy,BuckShlegeris,andJacobSteinhardt.
Interpretability in the wild: a circuit for indirect object identification in GPT-2 small.
In The Eleventh International Conference on Learning Representations, 2023. URL https:
//openreview.net/forum?id=NpsVSN6o4ul.
TianluWang,XiVictoriaLin,NazneenFatemaRajani,BryanMcCann,VicenteOrdonez,
andCaimingXiong. Double-harddebias: Tailoringwordembeddingsforgenderbias
mitigation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.),
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,
pp. 5443–5453, Online, July 2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.acl-main.484. URLhttps://aclanthology.org/2020.acl-main.484.
YadollahYaghoobzadeh,SoroushMehri,RemiTachetdesCombes,T.J.Hazen,andAlessan-
droSordoni. Increasingrobustnesstospuriouscorrelationsusingforgettableexamples.
InPaolaMerlo,JorgTiedemann,andReutTsarfaty(eds.),Proceedingsofthe16thConference
oftheEuropeanChapteroftheAssociationforComputationalLinguistics: MainVolume, pp.
3319–3332,Online,April2021.AssociationforComputationalLinguistics. doi: 10.18653/
v1/2021.eacl-main.291. URLhttps://aclanthology.org/2021.eacl-main.291.
AnYan,YuWang,YiwuZhong,ZexueHe,PetrosKarypis,ZihanWang,ChengyuDong,
AmilcareGentili,Chun-NanHsu,JingboShang,andJulianMcAuley. Robustandinter-
pretablemedicalimageclassifiersviaconceptbottleneckmodels,2023.
QinanYu,JackMerullo,andElliePavlick. Characterizingmechanismsforfactualrecall
inlanguagemodels. InHoudaBouamor,JuanPino,andKalikaBali(eds.),Proceedings
ofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.9924–9959,
Singapore,December2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2023.emnlp-main.615. URLhttps://aclanthology.org/2023.emnlp-main.615.
JohnR.Zech,MarcusA.Badgeley,ManwayLiu,AnthonyB.Costa,JosephJ.Titano,and
EricKarlOermann. Variablegeneralizationperformanceofadeeplearningmodelto
detectpneumoniainchestradiographs: Across-sectionalstudy. PLOSMedicine,15(11):
e1002683,November2018. ISSN1549-1676. doi: 10.1371/journal.pmed.1002683. URL
http://dx.doi.org/10.1371/journal.pmed.1002683.
JingzhaoZhang,AdityaKrishnaMenon,AndreasVeit,SrinadhBhojanapalli,SanjivKumar,
and Suvrit Sra. Coping with label shift via distributionally robust optimisation. In
InternationalConferenceonLearningRepresentations,2021. URLhttps://openreview.net/
forum?id=BtZhsSGNRNi.
MichaelZhang,NimitSSohoni,HongyangRZhang,ChelseaFinn,andChristopherRe.
Correct-N-Contrast: Acontrastiveapproachforimprovingrobustnesstospuriouscorre-
lations. InKamalikaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvari,GangNiu,
andSivanSabato(eds.),Proceedingsofthe39thInternationalConferenceonMachineLearning,
volume162ofProceedingsofMachineLearningResearch,pp.26484–26516.PMLR,17–23Jul
2022. URLhttps://proceedings.mlr.press/v162/zhang22z.html.
AndyZou,LongPhan,SarahChen,JamesCampbell,PhillipGuo,RichardRen,Alexander
Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation
engineering: Atop-downapproachtoAItransparency. ComputingResearchRepository,
arXiv:2310.01405,2023.
16Figure6:Computingedgeweights.Wecomputeallgradientsbetweenadjacentcomponents
bybackpropagatingfromtheactivationofadownstreamfeatureactivation. Then,when
computingcontributionstonon-adjacentnodes,wecorrectforindirectcontributionsby
subtractingoffaproductofJacobians. Finally,wecomputeestimatedindirecteffectsvia
specificpathsbymultiplyinggradientsw.r.t. mbythesourcenode’sactivationdifference.
WeonlykeepedgeswithanestimatedeffectaboveedgethresholdT (ahyperparameter).
E
A MethodologicalDetails
A.1 ComputingEdgeWeights
Let e be an edge between an upstream node u and downstream node d, corresponding
toanupstreamactivationau anddownstreamactivationad. Whenestimatingthecausal
importanceofe,weconsidertwocases:
(a) e connects a layer ℓ residual stream component and a layer ℓ attention or MLP
component,oreconnectsalayerℓattentionorMLPcomponentandalayerℓ+1
residualstreamcomponent;4
(b) econnectsalayerℓresidualstreamcomponentandalayerℓ+1residualstream
component.
In case (a), the indirect effect of e corresponds to the result of intervening to set d =
(cid:16) (cid:17)
d x |do(u=u ) ,butnotinterveningonanyotherdownstreamnodeonwhichau
clean patch
hasadirecteffect. Aswithnodes,weusealinearapproximation:
(cid:16) (cid:17)
IˆE(m;e;x ,x ) = ∇ m| ∇ d| u −u (5)
clean patch d d u u patch clean
clean clean
(cid:16) (cid:12) (cid:16) (cid:16) (cid:12) (cid:16) (cid:17)(cid:17)(cid:17)(cid:17)
≈ m x (cid:12)do d=d x (cid:12)do u=u
clean(cid:12) clean(cid:12) patch
IfdisanSAEerror,thenthenaiveapproachtocomputingisexpressioninvolvesperforming
d backwardspasses;fortunatelywecanstillcomputetheproductinasinglebackwards
model
passasexplainedin§A.3.
Incase(b),wewouldliketogiveeascorethatrepresentstheeffectthatau hasonmvia
ad,butwhichisnotalreadyexplainedbycompositionsofedgesofedgesoftype(a)via
intermediateactivationsam. AsisshowninFigure6,foreachintermediateactivationam,we
cancorrectforthenontransitiveeffectthrougham bysubtractingoffaproductofJacobians.
Ingeneral,weset
(cid:16) (cid:17)(cid:16) (cid:17)
IˆE(m;e;x clean,x patch)= ∇ dm|
dclean
∇ ud| uclean−∑
m
∇ md|
mclean
∇ um|
uclean
u patch−u
clean
(6)
wherethesumistakenoverintermediatenodesmbetweenuandd.
17Templatic data: No position aggregation Non-templatic data: Sum aggregation
sum
0, a1 1, a1 1, b1 0, a1 1, a1 1, b1 a1 b1
The doctor It’s strange …
sum
0, a1 1, a1 1, b1 0, a1 1, a1 1, b1 a1 b1
The children Doctor Johnson …
mean
mean
0, a1 1, a1 1, b1
a1 b1
Figure7: Aggregationofnode/edgeeffectsacrossexamples(andsometimes,acrosstoken
positions). Eachfeatureislabeledas“tokenposition,featureindex.” Ifwehavetemplatic
data,wepreservetokenpositioninformation,andtreatthesamefeaturesindifferenttoken
positionsasdifferentfeatures. Ifwehavemoregeneralnon-templaticdata,wefirstsum
acrosspositions,andthentaketheexample-wisemeanoftheposition-aggregatedeffects.
A.2 Aggregatingacrosstokenpositionsandexamples
Figure7summarizeshowweaggregateeffectsacrossexamples(andoptionallyacrosstoken
positions). Fortemplaticdatawheretokensinmatchingpositionsplayconsistentroles(see
§3.2,3.3),wetakethemeaneffectofnodes/edgesacrossexamples. Inthiscase,wetreat
thesamefeature(orneuron)indifferenttokenpositionsasdifferentnodesaltogetherinthe
circuit,eachwiththeirownseparateeffectsontargetmetricm.
Fornon-templaticdata(§4,5),wefirstsumtheeffectsofcorrespondingnodes/edgesacross
tokenpositionsbeforetakingtheexample-wisemean. Thismeansthateachfeatureappears
inthecircuitonce,representingitseffectsatalltokenpositionsinaninput.
A.3 Practicalconsiderations
Herewereviewanumberoftricksthatweusetocomputethequantitiesdefinedabove
efficiently. The backbone of our approach is to, given an activation x ∈ Rd model of some
submoduleforwhichwehaveanSAE,usetheSAEtocomputethequantities f (x)andϵ(x)
i
in(1),andtheninterveneinourmodel’sforwardpasstoset
∑
x← f (x)v +b+ϵ(x). (7)
i i
i
Eventhoughxwasalreadynumericallyequaltotheright-handsideof(7),aftertheinter-
ventionthecomputationgraphwillincorporatethevariables f (x)andϵ(x). Thus,when
i
weusePytorch’sautogradalgorithmtopeformbackpropogationofdownstreamquantities,
wewillautomaticallycomputegradientsforthesevariables.
Analternativeapproachforcomputinggradients(whichwedonotuse)istosimplyrun
themodelwithoutinterventions,usebackpropogationtocomputeallgradients∇ m,and
x
usetheformulas
∇ m = ∇ m·v, ∇ m = ∇ m
fi x i ϵ x
whichfollowfromthechainrulewhenmisanyfunctionofx.
Stop gradients on SAE errors to compute SAE feature gradients. The natural way to
computetheSAEerrorϵ(x)isbyfirstusingtheSAEtocomputexˆ andthensettingϵ(x) =
x−xˆ. However,ifwetakethisapproach,thenafterapplyingtheintervention(7)wewould
have
∇ m = ∇ m∇ xd = ∇ m∇ (xˆ+xu−xˆ) =0
fi vxd fi xd fi
4AsPythiamodelsemployparallelattention,thelayerℓattentioncomponentshavenoeffecton
thelayerℓMLPcomponents.
18where xd the copy of x which is downstream of f in the computation graph, and xu is
i
the copy which is upstream of f . To fix this, we apply a stop gradient to ϵ(x) so that
i
xd =xˆ+stopgrad(xu−xˆ).
Pass-throughgradients. Althoughthestopgradientfromabovesolvestheproblemof
vanishinggradientsforthe f ,itinterfereswiththebackpropogationofgradientstofurther
i
upstream nodes. In order to restore exact gradient computation, we implement a pass-
throughgradientonthecomputationofourdictionary. Thatis,inthenotationabove,we
interveneinthebackwardspassofourmodeltoset
∇ xum ← ∇ xdm.
Jacobian-vectorproducts. Donenaively,computingthequantitiesin(5)and(6)whendor
m∈Rd model areSAEerrorswouldtakeO(d model)backwardspasses. Fortunately,onecan
usethefollowingtrick: when Aisaconstant1×nmatrix,x∈Rm,andy=y(x) ∈Rn isa
functionofx,wehave
A∇ y= ∇ (Ay)
x x
wheretheright-handsideisa1×mJacobianwhichcanbecomputedwithasingleback-
wardpass. Thuswecancompute(5)withonlytwobackwardspassesbyfirstcomputing
(cid:16) (cid:17)
∇ m| andthencomputing∇ ∇ m| withanotherbackwardspass,wherethe
d d u d d
clean clean
second ∇ m| istreatedasaconstant(e.g.,bydetachingitinPytorch). Asimilartrick
d d
clean
canbeappliedfor(6).
B DetailsonSparseAutoencoders
B.1 Architecture
FollowingBrickenetal.(2023),ourSAEsareone-layerMLPswithatiedpre-encoderbias.
Inmoredetail,ourSAEshaveparameters
W E ∈RdSAE×d model,W D ∈Rd model×dSAE, b E ∈RdSAE,b D ∈Rd model
where the columns of W are constrained to be unit vectors. Given an input activation
D
x∈Rd model,wecomputethesparsefeaturesactivationsvia
(cid:2) (cid:3)
f= f (x) ... f (x) =W (x−b )+b
1 dSAE E D E
andreconstructionsvia
xˆ =W f+b .
D D
Thefeaturevectorsv i ∈Rd model arethecolumnsofW D.
B.2 Training
Fix a specific choice of activation in Pythia-70M, e.g. MLP output, attention output, or
residual stream in a particular layer. Following Cunningham et al. (2024); Bricken et al.
(2023)wetrainanSAEforthisactivationbysamplingrandomtextfromThePile(Gaoetal.,
2020)(specificallythefirst128tokensofrandomdocuments),extractingthevaluesxforthis
activationovereverytoken,andthentrainingourSAEtominimizealossfunction
L = L +λL = ∥xˆ−x∥ +λ∥f∥
reconstruction sparsity 2 1
consistingofaL2reconstructionlossandaL1regularizationtermtopromotesparsity. This
lossisoptimizedusingavariantofAdam(Kingma&Ba,2014)adaptedtoensurethatthe
columnsofW areunitvectors(seeBrickenetal.(2023)orourcodefordetails). Weuse
D
λ =0.1andalearningrateof10−4.
FollowingNanda(2023),wecacheactivationsfrom10000tokensinabufferandrandomly
sample batches of size 214 for training our SAE. When the buffer is half-depleted, we
19replenishitwithfreshtokensfromThePile. Wetrainfor120000steps,resultinginatotalof
about2billiontrainingtokens.
AmajorobstacleintrainingSAEsisdeadfeatures, thatis, neuronsinthemiddlelayerof
the SAE which never or rarely activate. We mitigate this by, every 25000 training steps,
reinitializingfeatureswhichhavenotactivatedintheprevious12500stepsusingthesame
reinitializationproceduredescribedinBrickenetal.(2023).
Finally,weusealinearlearningratewarmupof1000stepsatthestartoftrainingandafter
everytimethatneuronsareresampled.
B.3 Evaluation
Herewereportonvariouseasy-to-quantifymetricsofSAEquality. Notethatthesemetrics
leaveoutimportantqualitativepropertiesoftheseSAEs,suchastheinterpretabilityoftheir
features(App.D).Ourmetricsare:
• Varianceexplained,asmeasuredby1−
Var(x−xˆ)
.
Var(x)
• AverageL1,andL0normsoff.
• Percentageoffeaturesaliveasmeasuredbyfeatureswhichactivateatleastonce
onabatchof512tokens.
• Crossentropy(CE)differenceandpercentageofCErecovered. TheCEdifference
isthedifferencebetweenthemodel’soriginalCElossandthemodel’sCElosswhen
interveningtosetxtothereconstructionxˆ. WeobtainpercentageofCErecovered
bydividingthisdifferencebythedifferencebetweentheoriginalCElossandthe
CElosswhenzero-ablatingx. TheseCElossesarecomputedaveragedoverabatch
of128contextsoflength128.
ThesemetricsareshowninTables3–6. Notethatweindexresidualstreamactivationstobe
thelayerwhichoutputstheactivation(sothelayer0residualstreamisnottheembeddings,
andthelayer5residualstreamistheoutputofthefinallayer,immediatelyprecedingthe
finaldecoder).
%VarianceExplained L1 L0 %Alive CEDiff %CERecovered
96 1 3 36 0.17 98
Table3: EmbeddingSAEevaluation.
Layer %VarianceExplained L1 L0 %Alive CEDiff %CERecovered
Attn0 92% 8 128 17% 0.02 99%
Attn1 87% 9 127 17% 0.03 94%
Attn2 90% 19 215 12% 0.05 93%
Attn3 89% 12 169 13% 0.03 93%
Attn4 83% 8 132 14% 0.01 95%
Attn5 89% 11 144 20% 0.02 93%
Table4: AttentionSAEevaluationbylayer.
C QualityofLinearApproximationsofIndirectEffects
Figure 8 shows the quality of our linear approximations for indirect effects. Prior work
(Nanda, 2022; Krama´r et al., 2024) investigated attribution patching accuracy for IEs of
coarse-grainedmodelcomponents(queries,keys,andvaluesforattentionheads,residual
streamvectors,andMLPoutputs)andMLPneurons. WorkingwithSAEfeaturesanderrors,
20Layer %VarianceExplained L1 L0 %Alive CEDiff %CERecovered
MLP0 97% 5 5 40% 0.10 99%
MLP1 85% 8 69 44% 0.06 95%
MLP2 99% 12 88 31% 0.11 88%
MLP3 88% 20 160 25% 0.12 94%
MLP4 92% 20 100 29% 0.14 90%
MLP5 96% 31 102 35% 0.15 97%
Table5: MLPSAEevaluationbylayer.
Layer %VarianceExplained L1 L0 %Alive CEDiff %CERecovered
Resid0 92% 11 59 41% 0.24 97%
Resid1 85% 13 54 38% 0.45 95%
Resid2 96% 24 108 27% 0.55 94%
Resid3 96% 23 68 22% 0.58 95%
Resid4 88% 23 61 27% 0.48 95%
Resid5 90% 35 72 45% 0.55 92%
Table6: Residual(Resid)SAEevaluationbylayer.
ourresultsechopreviousfindings: attributionpatchingisgenerallyquitegood,butsome-
timesunderestimatesthetrueIEs. Notableexceptionsarethelayer0MLPandtheresidual
stream in early layers. We also find that our integrated gradients-based approximation
significantlyimprovesapproximationquality.
D HumanInterpretabilityRatingsforSparseFeatures
Weaskedhumancrowdworkerstoratetheinterpretabilityofrandomfeatures, random
neurons,featuresfromourfeaturecircuits,andneuronsfromourneuroncircuitsona0–100
scale(Table7). Crowdworkersratesparsefeaturesassignificantlymoreinterpretablethan
neurons,withfeaturesthatparticipateinourcircuitsalsobeingmoreinterpretablethan
randomlysampledfeatures.
SeeFigures9and10forexamplesofthehumanannotatorinterface.Humanswerepresented
withthetokensonwhichthefeatureactivatedmoststrongly,followedbythetokenswhose
probabilities were most affected in Pythia-70M when the feature was ablated. This is
followed by a series of example contexts in which the feature activated on some subset
oftokens,wherefeatureactivationsareshowninvaryingshadesofblue(darkershades
indicatehigheractivations). Onthesamepagebelowthecontexts,weaskannotatorsto
Activationtype Interpretability
Dense(random) 32.6
Dense(agreement) 30.2
Dense(BiB) 36.0
Sparse(random) 52.8
Sparse(agreement) 62.3
Sparse(BiB) 81.5
Table7: Humaninterpretabilityratingsfordense(neuron)vs.sparse(autoencoder)features.
Wepresentmeaninterpretabilityscoresacrossfeaturesona0–100scale. Weshowscoresfor
featuresthatwereeitheruniformlysampled(random),thetop30byIˆEfromthesubject–
verbagreementacrossRCtask(agreement;§3.3),orthetop30byIˆEfortheBiasinBiostask
(BiB;§4).
21(a) Attribution patching
Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6
m
Residual
strea
Attention
MLP
SAE feature
(b) Integrated gradients
SAE error
Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6
m
Residual
strea
Attention
MLP
Exact effect (IE)
Figure8: ApproximateIEs(y-axis)andexactIEs(x-axis)usingattributionpatching(a;top)
orintegratedgradients(b;bottom). EachpointcorrespondstoanSAEfeatureorSAEerror
atonetokenpositionofoneinput. Datawerecollectedfrom30inputsfromouracrossRC
dataset.
Figure 9: The human annotation interface used to obtain the interpretability ratings in
Table7. Here,weshowtheinstructions,top-activatingtokens,thetokenprobabilitiesthat
weremostaffectedwhenablatingthefeature,andexamplecontextswithfeatureactivation
values.
22
)Ê
I(
tceffe
etamixorppAFigure10: Thehumanannotationinterfaceusedtoobtaintheinterpretabilityratingsin
Table7. Here,weshowtheratinginterfaceonthesamepageasthecontentinFig.9,below
theexamplecontexts. Humanswereaskedtowriteatextualdescriptionofeachfeature,
assigna0–100interpretabilityrating,andassigna0–100semanticcomplexityratingtoeach
feature.
writeatextualdescriptionofthefeature,andratebothitsinterpretabilityanditssemantic
complexityon0–100scales.
CrowdworkerswererecruitedfromtheARENASlackchannel,whosemembersaremachine
learning researchers interested in AI alignment and safety. The selection of annotators
certainlyinfluencedourresults;atrulyrandomsampleofhumanannotatorswouldlikely
displayhighervariancewhenannotatingfeatures.
Onecommonerrorpatternwenoticeisthatannotatorsoftenlabelfeaturesaccordingto
semanticgroupings(e.g.,“textaboutpolitics,”anddonotpayattentiontosyntacticcontext
(e.g.,“pluralnouns”). Futureworkcouldaddressthisdesignbiasbytestingvariantsofthe
instructions.
E ImplementationDetailsforClassifierExperiments
E.1 Classifiertraining
HerewedescribehowwetrainaclassifieronPythia-70MfortheBiasinBios(BiB)task
of §4. To mimic a realistic application setting, we search over hyperparameters to train
high-performingbaselineandoracleclassifiers(usingtheambiguousandbalanceddatasets,
respectively). HyperparameterswerenotselectedforstrongSHIFTperformance.
Theinputstoourclassifierareresidualstreamactivationsfromtheendofthepenultimate
layerofPythia-70M.5 Wemean-poolover(non-padding)tokensfromthecontextInpre-
liminaryexperiments,weobtainedslightlyworsebaselineandoracleperformancewhen
insteadextractingrepresentationsoveronlythefinaltoken. Wealsoobtainedslightlyworse
performancewhentrainingtheclassifieronactivationsfromthefinallayerofPythia-70M.
WethenfitalinearprobetotheserepresentationswithlogisticregressionusingtheAdamW
optimizer(Loshchilov&Hutter,2017)withalearningrateof0.01forasingleepoch. When
retrainingafterperformingSHIFT,wetuneonlythislinearprobe—notthefullmodel.
5TheimplicationofthisisthatwefreezetheLMandonlyupdatetheparametersoftheclassifier.
23Forunclearreasons,wewereunabletofitaprobewithgreater-than-chanceaccuracywhen
applyinglogisticregressiontorepresentationsextractedfromthefinallayer;thisiswhywe
usedpenultimatelayerrepresentationsabove.
E.2 ImplementationforConceptBottleneckProbing
OurimplementationforConceptBottleneckProving(CBP)isadaptedfrom(Yanetal.,2023).
Itworksasfollows:
1. First,wecollectanumberofkeywordsrelatedtotheintendedpredictiontask. We
use N = 20keywords: nurse,healthcare,hospital,patient,medical,clinic,triage,
medication,emergency,surgery,professor,academia,research,university,tenure,
faculty,dissertation,sabbatical,publication,andgrant.
2. Weobtainconceptvectorsc ,...,c foreachkeywordbyextractingPythia-70M’s
1 N
penultimatelayerrepresentationoverthefinaltokenofeachkeyword,andthen
subtractingoffthemeanconceptvector. (Withoutthisnormalization,wefoundthat
conceptvectorshaveveryhighpairwisecosinesimilarities.)
3. Givenaninputwithrepresentationx(obtainedviathemean-poolingprocedure
inApp.E.1),weobtainaconceptbottleneckrepresentationz∈RN bytakingthe
cosinesimilaritywitheachc.
i
4. Finally,wetrainalinearprobewithlogisticregressionontheconceptbottleneck
representationsz,asinApp.E.1.
Wedecidedtonormalizeconceptvectorsbutnotinputrepresentationsbecauseitresulted
instrongerperformance.
F FeatureCircuits
F.1 Subject–VerbAgreement
Here, we present the full agreement circuits for various syntactic agreement structures,
whereweannotateallfeaturesanddonotcollapsesimilarfeaturesintothesamenodes. All
circuitsherearediscoveredwithT =0.1andT =0.01unlessotherwisenoted. Ineach
N E
circuit,sparsefeaturesareshowninrectangles,whereascausallyrelevanterrortermsnotyet
capturedbyourSAEsareshownintriangles. Nodesshadedindarkercolorshavestronger
effectsonthetargetmetricm. Bluenodesandedgesarethosewhichhavepositiveindirect
effects (i.e., are useful for performing the task correctly), whereas red nodes and edges
arethosewhichhavecounterproductiveeffectsonm(i.e.,causethemodeltoconsistently
predictincorrectanswers).
First,wepresentagreementacrossarelativeclause(Figure11). Themodelappearstodetect
thesubject’sgrammaticalnumberatthesubjectposition. Onepositionlater,featuresdetect
thepresenceofrelativepronouns(thestartofthedistractorclause). Finally,atthelasttoken
oftherelativeclause,theattentionmovesthesubjectinformationtothelastposition,where
itassistsinpredictingthecorrectverbinflection.
Thecircuitforagreementacrossaprepositionalphrase(Figure12)looksremarkablysimilar
toagreementacrossarelativeclause;thesetwocircuitsshareover85%oftheirfeatures,and
manyofthesamefeaturesareusedfordetectingbothprepositionsandrelativeclauses.
Forsimpleagreement(Figure13,discoveredwithT = 0.2andT = 0.02),manyofthe
N E
samefeaturesthatwereimplicatedinnounnumberdetectionandverbnumberpredictionin
thepreviouscircuitsalsoappearhere. Themodeldetectsthesubject’snumberatthesubject
positioninearlylayers. Inlaterlayers,thesenounnumberdetectorsbecomeinputstoverb
numberpromoters,whichactivateonanythingpredictiveofparticularverbinflections.
Finally,thecircuitforagreementwithinarelativeclause(Figure13,discoveredwithT =
N
0.2andT =0.02)appearstohavethesamestructureasthatforsimpleagreement: subject
E
numberdetectorsinearlylayers,followedbyverbnumberpromotersinlaterlayers.
24Figure 11: The full annotated feature circuit for agreement across a relative clause. The
modeldetectsthesubject’snumberatthesubjectposition. Otherfeaturesdetectrelative
pronouns(thestartofthedistractorclause). Finally,atthelasttokenoftheRC,theattention
movesthesubjectinformationtothelastposition,whereitassistsinpredictingthecorrect
verbinflection.
Figure12: Thefullannotatedfeaturecircuitforagreementacrossaprepositionalphrase.
The model detects the subject’s number at the subject position. Other features detect
prepositionalphrases(thestartofthedistractorclause). Finally,atthelasttokenoftheRC,
theattentionmovesthesubjectinformationtothelastposition,whereitassistsinpredicting
thecorrectverbinflection.
Figure13: Thefullannotatedfeaturecircuitforsimpleagreement. Themodeldetectsthe
subject’snumberatthesubjectpositioninearlylayers. Inlaterlayers,theseareinputsto
featureswhichactivateonanythingpredictiveofparticularverbinflections.
25Figure14: Thefullannotatedfeaturecircuitforagreementwithinarelativeclause. The
modeldetectsthesubject’snumberatthesubject(withintheRC)’spositioninearlylayers.
Inlaterlayers,thesefeaturesareinputstofeatureswhichactivateonanythingpredictiveof
particularverbinflections.
Figure 15: The full annotated feature circuit for the Bias in Bios classifier. Many nodes
simplydetectthepresenceofgenderedpronounsorgenderednames. Afewfeaturesattend
toprofessioninformation,includingonewhichactivatesonwordsrelatedtonursing,and
anotherwhichactivatesonpassagesrelatingtoscienceandacademia.
F.2 BiasinBiosCircuit
Here,wepresentthefullannotatedcircuitdiscoveredfortheBiasinBiosclassifier(described
in§4andApp.E).ThecircuitwasdiscoveredusingT =0.1andT =0.01.Weobservethat
N E
thecircuit(Figure15)containsmanynodeswhichsimplydetectthepresenceofgendered
pronounsorgenderednames. Afewfeaturesattendtoprofessioninformation,including
onewhichactivatesonwordsrelatedtonursing,andanotherwhichactivatesonpassages
relatingtoscienceandacademia.
F.3 ClusterCircuits
Here,wepresentfullannotatedcircuitsdiscoveredforautomaticallydiscoveredbehaviors
(described in App. H). First, we present the circuit for incrementing number sequences
26Figure16: Thefullannotatedfeaturecircuitforincrementingnumbersequences. Themodel
firstdetectsthepresenceofspecificnumbertokens,like“3”. Later,itlearnsmorerobust
semanticrepresentationsofthosenumbers,like“iii”and“Three”. Then,themodelusesa
seriesofnarrowandgeneralsuccesionandinductionfeaturestoincrementthenextnumber.
(Figure16),discoveredwithT =0.4andT =0.04. Wenotethatthiscircuitincludesmany
N E
featureswhichperformeithersuccession(Gouldetal.,2023)orinduction(Olssonetal.,
2022). Thesuccessionfeaturesinthelayer3attentionseemtobegeneral;theyincrement
manydifferentnumbersandletters(asinFigure5).Theinductionfeaturesaresensitiveonly
tospecifictokens: forexample,contextsoftheform“x3...x3”,where“3”isaliteral. These
composetoformspecificsuccessorfeaturesinlayer5: themoststrongly-activatinglayer5
residualfeaturespecificallyincrements“3”to“4”giveninduction-likelists,whereeachlist
itemisprecededbythesamestring(e.g.,“Chapter1...Chapter2...Chapter3...Chapter”).
The circuit for predicting infinitival objects (Figure 17, discovered with T = 0.25 and
N
T = 0.001) contains two distinct mechanisms. First, the model detects the presence of
E
specificverbslike“remember”or“require”whichoftentakeinfinitivalobjects. Then,the
model uses two separate mechanisms to predict infinitive objects. The first mechanism
detectspresent-tenseverbs,participles,orpredicateadjectiveswhichcanbeimmediately
followedbyinfinitivaldirectobjects(e.g.,“Theywereexcitedto...”).Thesecondmechanism
detectsnominaldirectobjectsthatcandirectlyprecedeinfinitivalobjectcomplements(e.g.,
“Theyaskedusto...”). Finally,thesetwomechanismsbothinfluencetheoutputinlayer5
withoutfullyintersecting.
G SampleFeatures
G.1 SparseFeatures
Here,wepresentexamplesofsparsefeatureswithhighindirecteffectsontheBiasinBios
task.Someofthesefeaturesclearlyactivateontermsrelatedtomedicineoracademia,which
arerelatedtothetargetprofessionclassificationtask. Otherssimplydetectthepresenceof
“he”orfemalenames.
G.2 Neurons
Forcontrast,wealsopresentexamplesofdensefeatures—thatis,neuronsfromMLPs,layer-
endresiduals,andtheout-projectionoftheattention—withhighindirecteffectsontheBias
inBiostask. Wecannotdirectlyinterprettheactivationpatternsoftheseneurons,andso
itisdifficulttoruntheSHIFTwithneuronsbaseline. Wethereforeinsteadcomparetothe
27Figure17: Thefullannotatedfeaturecircuitforpredicting“to”asaninfinitivalobject. The
modelfirstdetectsthepresenceofverbsthatoftentakeinfinitivalobjects. Then,itusesone
mechanismtodetectpresent-tenseverbs, participles, orpredicateadjectiveswhichtake
infinitivalobjects,andanothermechanismtodetectdirectobjectsthatcandirectlyprecede
infinitivalobjectcomplements. Finally,thesetwomechanismsbothinfluencetheoutputin
layer5withoutfullyintersecting.
Figure 18: An example sparse feature from the Bias in Bios task (attn 3/22029). This
featuredetectsfemale-relatedwordsinbiographiesofwomen. Italsopromoteswordslike
“husband”and“ne´e”. Thisfeatureprobablycontributestopreferencesforthespurious
correlateofgender;wethereforeablateit.
Figure 19: An example sparse feature from the Bias in Bios task (resid 2/31098). This
featureactivatesonwordsrelatedtonursing,including“RN”and“nurse”. Thisprobably
relatestothetargettaskofprofessionprediction. Wethereforekeepit.
28Figure 20: An example neuron from the Bias in Bios task. This appears to activate on
beginningsandendsofsentences,butalsomorestronglyonanytokeninasentencethat
containscapitallettersornumbers. Wecannotdeducewhetherthiswouldcontributemore
togenderorprofessionnames.
Figure21: AnexampleneuronfromtheBiasinBiostask. Thisactivatespositivelyontokens
startingwithcapitalletters,butnegativelyonmanyothertokens(whoseunifyingtheme
wecannotdeduce).
neuronskyline,whereweallowtheskylineanunfairadvantagebysimplyablatingneurons
whichhavepositiveeffectsongender-basedprobabilitiesgiventhebalancedset.
H DiscoveringLMBehaviorswithClustering
In this section, we describe our unsupervised method for discovering language model
behaviors. More specifically, following Michaud et al. (2023), we cluster contexts from
ThePileaccordingtothePythia-70M’sinternalstateduringinference. Inthissection,we
describeourclusteringpipelineandmethods.
H.1 FilteringTokens
Wemustfirstlocate(context,answer)pairsforwhichanLMcorrectlypredictstheanswer
tokenfromthecontext. WeselectThePile(Gaoetal.(2020))asageneraltextcorpusand
filtertopairsonwhichPythia-70Mconfidentlyandcorrectlypredictstheanswertoken,
withcross-entropylowerthan0.1or0.3nats,dependingontheexperiment. Themodel
consistentlyachieveslowlossontokenswhichinvolve“induction”(Olssonetal.,2022)—i.e.,
tokenswhicharepartofasubsequencewhichoccurredearlierinthecontext. Weexclude
inductionsamplesbyfilteringoutsamplesinwhichthebigram(finalcontexttoken,answer
token)occuredearlierinthecontext.
H.2 CachingModel-internalInformation
WefindbehaviorsbyclusteringsamplesaccordingtoinformationabouttheLM’sinternals
whenrunonthatsample. Wefindclustersofsampleswherethemodelemployssimilar
29mechanismsfornext-tokenprediction. Weexperimentwithvariousinputstotheclustering
algorithm:
• DenseActivations: Wetakeactivations(residualstreamvectors,attentionblock
outputs,orMLPpost-activations)fromagivencontextandconcatenatethem. To
obtainavectorwhoselengthisindependentofthecontextlength,wecaneitheruse
theactivationsatthelastNcontextpositionsbeforetheanswertoken,oraggregate
(sum)acrossthesequencedimension. Weexperimentwithbothvariants.
• SparseActivations:Ratherthandensemodelactivations,wecanusetheactivations
ofSAEfeatures. Weconcatenateandaggregatetheseinthesamemannerasfor
denseactivations.
• Dense Component Indirect Effects: We approximate the indirect effect of all
features on the correct prediction using 2 without a contrastive pair—namely,
by setting a = 0. The negative log-probability of the answer token m =
patch
−logp(answer)servesasourmetricforthecorrectpredictionofthenexttoken.The
computatiomoflineareffectsrequiressavingboth(1)activationsand(2)gradients
w.r.t m at the final N positions for each context in the dataset. We optionally
aggregatebysummingoverallpositions.
• Sparse Indirect Effects: Similarly, we can compute the linear effects of sparse
activationsonthecorrectprediction.
• Gradient w.r.t. model parameters: As in Michaud et al. (2023), we also exper-
iment with using gradients of the loss w.r.t. model parameters, but with some
modifications. Wedescribethismethodinmoredetailin§H.3below.
H.3 HyperparametersandImplementationDetails
Weapplyeitherspectralclusteringor k-meansclustering. Forspectralclustering, given
eitheractivationsoreffectsx forsamplei,wecomputeamatrixofpairwisecosinesimi-
i
laritiesC = x ·x /(||x ||||x ||)betweenallpairsofsamples. Beforeperformingspectral
ij i j i j
clustering,wenormalizeallelementsofCtobein[0,1]byconvertingthecosinesimilarities
toangularsimilarities: Cˆ =1−arccos(C )/π.
ij ij
Weusethescikit-learn(Pedregosaetal.,2011)spectralclusteringimplementationwith
k-means.Forallinputsexceptgradientsw.r.t.modelparameters,weusedspectralclustering
across8192samples. Wechosek(thenumberoftotalclusters)tomaximizethenumberof
clustersimplicatedinmorethanoneinputcontext.
Wealsoexperimentedwithusinggradientsw.r.t.modelparametersasinputs,asinMichaud
etal.(2023). Here,wescaleupourapproachto100,000samples. Itisintractibletoperform
spectralclusteringgiven100,000samples, soweinsteaduse k-meansclustering. Rather
thanclusteringthegradientsthemselves(whicharehigh-dimensional),weclustersparse
randomprojectionsofthegradientsdownto30,000dimensions. Whenprojecting,weusea
matrixwithentries{−1,0,1}. Whensamplingtheentriesofthismatrix,sampleanonzero
valuewithprobability32/30000,andifnonzero,sample−1or1withequalprobability. For
asparseprojectionmatrixwithdimensionsRn×30000,therewillonaveragebe32·nnonzero
entries,wherenisthenumberofparametersinthemodel.6
6Weonlyconsidergradientsw.r.t.non-embeddingandnon-layernormparameters.
30