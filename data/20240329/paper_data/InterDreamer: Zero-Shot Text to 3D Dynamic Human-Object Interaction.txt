InterDreamer: Zero-Shot Text to 3D Dynamic
Human-Object Interaction
Sirui Xu1†, Ziyin Wang2†, Yu-Xiong Wang1‡, and Liang-Yan Gui1‡
1 University of Illinois at Urbana-Champaign
2 Fudan University
† Equal Contribution ‡ Equal Advising
https://sirui-xu.github.io/InterDreamer/
Abstract. Text-conditionedhumanmotiongenerationhasexperienced
significantadvancementswithdiffusionmodelstrainedonextensivemo-
tion capture data and corresponding textual annotations. However, ex-
tendingsuchsuccessto3Ddynamichuman-objectinteraction(HOI)gen-
eration faces notable challenges, primarily due to the lack of large-scale
interaction data and comprehensive descriptions that align with these
interactions.Thispapertakestheinitiativeandshowcasesthepotential
of generating human-object interactions without direct training on text-
interactionpairdata.Ourkeyinsight inachievingthisisthatinteraction
semantics and dynamics can be decoupled. Being unable to learn inter-
action semantics through supervised training, we instead leverage pre-
trainedlargemodels,synergizingknowledgefromalargelanguagemodel
andatext-to-motionmodel.Whilesuchknowledgeoffershigh-levelcon-
trol over interaction semantics, it cannot grasp the intricacies of low-
level interaction dynamics. To overcome this issue, we further introduce
aworldmodeldesignedtocomprehendsimplephysics,modelinghowhu-
man actions influence object motion. By integrating these components,
our novel framework, InterDreamer, is able to generate text-aligned 3D
HOI sequences in a zero-shot manner. We apply InterDreamer to the
BEHAVE and CHAIRS datasets, and our comprehensive experimental
analysis demonstrates its capability to generate realistic and coherent
interaction sequences that seamlessly align with the text directives.
1 Introduction
Text-guided human motion generation [94] has made unprecedented progress
through advancements in diffusion models [31,85,86], leading to synthesis out-
comes that are more realistic, diverse, and controllable. This progress has fur-
therignitedincreasedinterestinexploringexpandedtasksrelatedtotext-guided
human interaction generation, such as social interaction [54] and human-scene
interaction [33]. However, many of these explorations are limited in that the
dynamics of objects are not involved or cannot be controlled by text. Aiming
to bridge such a gap, this paper undertakes the initiative to tackle a more chal-
lengingtask–generating versatile 3D human-object interactions (HOIs) through
language guidance, as illustrated in Fig. 1.
4202
raM
82
]VC.sc[
1v25691.3042:viXra2 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
Semantics Dynamics
A person picks up a
backpack, puts it on, and
then walks forward
clockwise.
How do humans
carry backpacks?
Fig.1:InterDreamercangeneratevivid3Dhuman-objectinteractionsequencesguided
by textual descriptions. Its zero-shot ability is achieved by integrating semantics and
dynamics knowledge from large-scale text-motion data (upper left), a large language
model(LLM)(bottomleft),3Dhuman-objectinteractiondatabase(uppermiddle),and
interaction prior (bottom middle). We visualize the generated text-guided interaction
sequence (upper right), with the beginning of the sequence unfolded (bottom right).
More details are available in https://sirui-xu.github.io/InterDreamer/.
While a direct solution, as suggested by concurrent work [21,50,71,109],
would be replicating the success observed in human motion generation and
adopting a similar supervised approach for learning text-driven HOIs, it is not
scalable. Indeed, even generating social or scene interactions heavily relies on
extensive collections of text-interaction pair data [25,54,62,104]. Scaling these
methodstoaddressthemorecomplexHOIsoutlinedinourstudycouldnecessi-
tatedatasetsofcomparablemagnitude.Achievingthisgoalappearsunattainable
bymerelyannotatingexisting3DHOIdatasets[7,22,34,36,41,51,129,136],which
are relatively limited in size. Although recent studies [21,50,71] have annotated
some of these datasets, the volume of text-motion pairs still lags significantly
behind that available for existing text-driven motion generation efforts.
An intriguing question naturally emerges: what is the potential of zero-shot
learning for text-conditioned HOI generation, which is the main focus of this
paper. However, formulating the task in a zero-shot setting presents significant
challenges,primarilyduetotheinabilitytodirectlylearnthealignmentbetween
text and HOI dynamics. Our key observation then is that interaction semantics
and dynamics can be decoupled. That is, the high-level semantics of an inter-
action, aligned with its textual description, can be informed by human motion
and the initial object pose. Meanwhile, the low-level dynamics of the interaction
– specifically, the subsequent behavior of the object – are governed by the forces
exerted by the human, within the constraints of physical laws.
Motivatedbytheseinsights,weintroduceInterDreamer–anovelframework
that synergizes knowledge of interaction semantics and dynamics. As shown in
Fig. 1, both of them do not necessarily need to be learned from text-interaction
pairs, if they are decoupled, thus leading to the ability for zero-shot generation.InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 3
The semantics of interaction, although not available through direct super-
vised training, can be harnessed from a variety of prior knowledge that is in-
dependent of text-interaction pair datasets. Specifically, to acquire semantically
aligned human motion and initial interaction, we first consult a large language
model (LLM), such as GPT-4 [67] and Llama 2 [95], to provide understand-
ings including how humans typically use specific body parts in interactions
with particular objects, by exploiting its in-context learning capability with
few-shot prompting [10] and chain-of-thought prompting [107]. Both interme-
diatethoughtsandthefinalthoughtarethenusedto(i)generateasemantically
alignedhumanmotionwithapre-trainedtext-to-motionmodel;and(ii)identify
an initial object pose that is harmonious with the generated initial human pose
and text description through a retrieval-based algorithm.
Whiletheselargemodelscanofferhigh-levelmotionsemanticsmodeling,they
lack crucial low-level dynamics knowledge. Nevertheless, by decoupling interac-
tion dynamics from semantics, a key advantage emerges in our InterDreamer
framework:interactiondynamicscanbelearnedfrommotioncapturedatawith-
out the necessity of text annotations. We instantiate this idea by developing a
novel world model, which predicts the subsequent state of an object affected by
the interaction. The key here is to attain generalizable control signals. To do so,
we exert control over the object through the motion of vertices on the human
body.Theseverticesaresolelysampledinregionswherecontactoccurs,agnostic
to the overall object shape and whole-body motion. Such abstraction empowers
the model to learn the fundamental physics from a publicly available 3D HOI
dataset [7]. The plausibility of generated interaction is further enhanced by a
subsequent optimization procedure on synthesized human and object motion.
Tosummarize,ourcontributionsare:(i)Weinitiateanoveltaskofsynthesiz-
ingwhole-bodyinteractionswithdynamicobjectsguidedbytextualcommands,
without access to text-interaction pair data, which is the first to our knowledge.
(ii) We introduce a novel framework that decomposes semantics and dynamics,
and they can be integrated effortlessly. (iii) Our methodology harnesses knowl-
edgefromalargelanguagemodel(LLM)andatext-to-motionmodelasexternal
resources,alongsideourproposednovelworldmodel.Remarkably,theonlycom-
ponent that requires training is the world model, underscoring the ease of use
of our framework. Experimental results demonstrate that our zero-shot frame-
work, InterDreamer, is capable of producing semantically aligned and realistic
human-object interactions, and generalizes beyond existing HOI datasets.
2 Related Work
Text-Conditioned Human Motion Generation. Significant progress has
beenwitnessedinhumanmotionsynthesistasks,givendifferentkindsofexternal
conditions, including action categories [2,27,49,73], past motions [5,14,65,117,
118,125], trajectories [38,39,80,97,113], scene context [11,29,33,91,99–101,
104,132,137,138], and unconditional generation [76]. Recently, human motion
synthesis guided by textual descriptions [1,6,15,20,25,26,42,45,59,60,74,75,4 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
77,84,92,106,123,128,130,134,135,141] is popular and extended to various
applications, including the text-conditioned generation of multiple-person [24,
57,105] and human-scene interaction [17,33,37]. Our goal is to model human
and object dynamics concurrently guided by text in a zero-shot setting.
Human-Object Interaction Generation. Synthesizing hand-object interac-
tions[53,124,126,139,140]andsingle-framehuman-objectinteractions[32,44,72,
102,112,127] are popular topics and extended to zero-shot settings [40,52,120].
Recently, researchers explore whole-body dynamic interaction generation, in
kinematic-based approaches [18,23,46–48,51,63,64,79,87–89,98,109,110,116,
119,133]andphysics-basedapproaches[4,8,13,19,30,56,66,68,103,114,115,121].
Current methods in HOI synthesis are often restricted by a narrow scope of ac-
tions, the use of non-dynamic objects, and a lack of comprehensive whole-body
motion. Our work aims to generate diverse whole-body interactions with vari-
ous objects, and enables control through language input. Recent datasets [7,22,
34,36,41,51,90,129,136] provide the groundwork for research in this area, and
concurrent efforts [21,50,71] demonstrate the feasibility of applying supervised
learning methods through annotating datasets. However, the amount of data
currently available falls short when compared to more extensive text-motion
datasets [25,55,62]. This discrepancy in data volume limits the capability of
supervised methods to capture the complexity of human-object interactions,
motivating us to investigate the potential of zero-shot generation.
External Knowledge from LLMs. Large language models (LLMs) are be-
ing used for advanced visual tasks, such as editing images based on instruc-
tions [9]. In digital humans, they are used to reconstruct 3D human-object in-
teractions [102] and generate human motions [3,35,122,134] as well as human-
scene interactions [111]. Our approach is inspired by [102], which uses LLMs
to infer contact body parts with a given object for reconstructing 3D human-
objectinteractions–ataskdifferentfromours.Also,ourapproachutilizesmore
advanced LLMs, such as GPT-4 [67] or Llama 2 [95], not only to understand
contact body parts but also to further narrow the distribution gap between the
text descriptions and models in our pipeline. This is achieved by leveraging the
in-context learning (ICL) of LLMs with the chain-of-thought prompting [107].
3 Methodology
Problem Formulation. Our goal is to synthesize a sequence of 3D human-
objectinteractionsxthatcorrespondstoadescriptivetextp.Thissequenceisa
seriesoftuples[(h ,o ),(h ,o ),...,(h ,o )],whereh representsthehuman
1 1 2 2 M M i
poseparametersdefinedbytheSMPLmodel[58],ando definestheobjectpose
i
in terms of its 3D spatial location and orientation. The sequence length M is
variable and is dynamically determined by our text-to-motion model based on
the input text p. We do not require text-interaction paired data for training.
Overview. Our framework, illustrated in Fig. 2, can be conceptualized as a
Markovdecisionprocess(MDP).Webeginbydividingthemotionsequenceinto
T segments, each with m frames, where M = T ×m. Object motion {o }M
i i=1InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 5
High-Level Planning Low-Level Control World Model
You will be given a sentence that describes the interaction between a person Interaction Retrieval Dynamics Modeling
and an object. You will need to extract information from the description
and answer three questions based on the following rules and examples.
1. Extract the category of the object with which the person interacts.
at,st,at+1 st+1
A ston os lw , be or xs s msh alo l,u bld ac kb pe a s cke ,l e bc ot xe lod n f gr ,o pm las t tih ce c of no tl al io new r,i tn ag b leo sb qj ue ac rt es , : y [ ot gra as bh ab lli ,n y, om go an mit ao tr , , t ob oo lx bl oar xg , e, Abstracting states (object) and actions (human)
chairwood, chairblack, boxmedium, boxtiny, suitcase, tablesmall].
[start of rules]
•Pick the most similar object from the list if the description is not in the list.
[end of rules] s
1
2. Infer the body part that contacts with the object at the beginning of the
interaction.
Answers should be selected from the following body parts: [right hand, left hand,
arm, back of the hip, upper leg, leg, upper back, left foot, right foot, front body, back, shoulder,
no contact].
[start of rules]
•If the description does not specify which hand or foot, randomly choose from
“left” or “right”. Actions as spatial conditioning
… controls to the dynamics model
[end of rules]
3 [ • … [s e. t nTS a dr hi t m e o o fsp f u r l r ui buf ljy l eee sca s ]t]n od f m tho e d si ef ny t eth nce e d se hs oc uri lp d t bio en ". a person". aT 1e ,…xt- ,t ao t-Ma o1 tio atn
+1
P a F a A ab b ba u cs s ss t tt t tut ir r r ors a a ae nt c c ca t t tst i i ite o o oa n n nte D b C atl r ty o eon c nska s tm is oi ncs
[start of examples]
A person uses their feet to apply force to the medium-sized box. Answer:
boxmedium|left foot|A person kicks the box with left foot.
…
[end of examples] Optimization
T S Po lh eme
a
sed eoe n os ec
u
r i ti s pp upti h to y tn hs i eo c f a
a
lh nlyu
s
wm pu ea rln l i f- n oo g lb
l
otj he wec
i
t nc i h gn a t tie hr r eoa n fc
o
t t ri ho men af: l to oo fr to adjust its position. at+1, st+1 a t* +1, s t*
+1
"Answer: Answer 1|Answer 2|Answer 3"
Fig.2:AnoverviewofourInterDreamer.(i)Ourhigh-levelplanninganalyzesthe
description using LLMs and provides guidance to the low-level control. (ii) Our low-
level control includes a text-to-motion model that translates text into human actions
a , and an interaction retrieval model for extracting the object’s initial pose as the
t+1
first state s . (iii) Our world model executes the actions and outputs the next state
1
s throughdynamicsmodeling.Anoptimizationprocessiscoupledwiththedynamics
t+1
model, projecting the state and action onto valid counterparts s∗ and a∗ . Solid
t+1 t+1
arrows mean that the process is performed iteratively.
is formulated as a sequence of environmental states {s }T , and human mo-
t t=1
tion {h }M is described as a sequence of actions {a }T that interact with the
i i=1 t t=1
environment.UndersuchanMDPsetup,ourpipelinestartswithhigh-levelplan-
ningL,which,giventhetextualinteractiondescriptionp,deciphersthedetailed
context g = L(p) (Sec. 3.1). Then, a text-to-motion model π translates con-
text g into human actions iteratively, modeled as a ∼ π(a |s ,{a }t ,g)
t+1 t+1 t i i=1
(Sec. 3.2). An interaction retrieval model R proposes an initial object state s ,
1
basedontheinitialactiona andcontextg (Sec.3.2).Afterthat,aworldmodel
1
P is trained to predict future states s from the current action and state
t+1
(Sec. 3.3). Our world model further incorporates an optimization process with
prior knowledge on human interactions encoded, for both state and action re-
finement (Sec. 3.4). Notably, the text-to-motion and world models are executed
iteratively until text-to-motion generates an end frame.
3.1 High-Level Planning
Powered by LLMs’ strong reasoning capabilities as well as their common sense,
our high-level planning L yields interaction details g =L(p) that cannot be di-
rectly extracted in textual descriptions p. The process undertaken by L encom-
passesthreesteps:(i)Determiningtheobject:TheLLMisemployedtotranslate
the described objects into corresponding categories from a pre-defined list used6 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
in the world model. (ii) Determining initial human-object contact: The LLM
infers the body parts involved in the interaction, drawing from a list defined in
theSMPLmodel[58].(iii)Reducing the distribution gap:TheLLMbridges
the distribution gap between the freeform textual input and the language used
within the text-to-motion model training dataset [25]. This involves standardiz-
ing syntax and content according to specific guidelines we design.
InFig.2,wedemonstratethepromptusedbytheLLM,aswellasexamples,
following the few-shot prompting [10]. We define the sequence of intermediate
thoughts and the final thought, i.e., the answers to the three questions, as the
detailed information g =L(p), which guides the subsequent procedure.
Our high-level planning operates indirectly in the generation of interactions.
Nonetheless, it plays a key role in effectively bridging the distribution gap be-
tweentextualdescriptionsandsubsequentinteractiongeneration.Inotherwords,
it narrows the vast range of possible interactions into a more manageable dis-
tribution within the capabilities of our framework. As detailed in Sec. 4.1, our
high-level planning incorporates GPT-4 [67] or Llama-2 [95] for evaluation.
3.2 Low-Level Control
Withtheinformationgderivedfromthedescriptionp,thelow-levelcontrolaims
to create an initial state s and a sequence of human actions {a }T , such that
1 t t=1
they correspond with the objectives outlined by g. In this section, we detail the
text-to-motionmodelandinteractionretrievalsystemdesignedtofulfillthetask.
Text-to-Motion.Weconstructthetext-to-motionmodelπdesignedtodevelop
actions to be executed in the world model. At each timestep t, π receives the
sequence of previous actions {a }t and the text tokens extracted from g =
i i=1
L(p), and produces a preliminary next action a , which will be adjusted to
t+1
theultimateactiona∗ throughanoptimizationprocessthatintertwinesitwith
t+1
the object state, introduced in Sec. 3.4. Thus, the overall process coupled with
the optimization can be formally defined as a ∼ π(a |s ,{a }t ,g). The
t+1 t+1 t i i=1
initialactiona ∼π(a |g)isinfluencedbythetextfromg withoutprioractions
1 1
or states, which will be used in the interaction retrieval. As detailed in Sec. 4.1,
π is capable of leveraging existing text-to-motion models, including MDM [93],
MotionDiffuse [130], ReMoDiffuse [131], and MotionGPT [35].
InteractionRetrieval.TheinteractionretrievalcomponentRsetsupaninitial
state s ∼R(a ,g), given the initial action a generated by the text-to-motion
1 1 1
model. We present here a user-friendly pipeline that is based on handcraft rules
and a learning-based approach in Sec. B.1 of supplementary. First, we build a
databasebycollectingHOIframesfromthetargetdataset,e.g.,theBEHAVE[7]
or CHAIRS [36] dataset. The indexing key for retrieval is a tuple consisting of
the body part in contact and the category of the object involved. The value for
retrieval is a per-frame contact map, i.e., a list of K vertex pairs {(di,di)}K .
h o i=1
Here,di indexesthecontactvertexonthehuman’ssurface,whiledi indexesthe
h o
correspondingcontactvertexontheobject’ssurface.Thiscontactmapislinked
with their key, thus establishing a searchable record of interactions. During the
inferencestage,equippedwiththebodypartandobjectinformationprovidedbyInterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 7
thehigh-levelplanning(Sec.3.1),weutilizethemasakeytoretrieveallrelevant
contactmapsfromourdatabase.Wesampleonemap{(di,di)}K accordingto
h o i=1
a pre-defined metric, and employ it to ascertain an object state s ∼ R(a ,g),
1 1
harmoniouswiththeinitialhumanactiona ,astheinitiationoftheinteraction.
1
More details are provided in Sec. B.1 of supplementary.
3.3 World Model
Ourworldmodelcombinesadynamicsmodelandtheoptimizationprocess,ded-
icatedtosimulatingstatetransitionsaffectedbyappliedactions.Whiledrawing
inspiration from similar concepts utilized in robotics [83,108] and autonomous
drivingsystems[43],weuseithereforrolloutwithoutmodel-basedlearning.This
model, trained on the 3D HOI dataset, serves a role similar to a physics simu-
lator but is much simpler – it takes the preceding object state s along with a
t
pairofconsecutiveactionsa anda ,andpredictsthesubsequentobjectstate
t t+1
s .Theinterplaybetweenthelow-levelcontrolandtheworldmodelultimately
t+1
produces a coherent interaction rollout.
In designing the dynamics model, a naïve method would be directly taking
the raw actions and past state as its input. This method, however, suffers from
a severe generalization problem during inference: the dynamics model is likely
to encounter some human actions that do not exist in the training set, since our
text-to-motion model is not trained on the HOI dataset. We provide ablation
studies as evidence for this claim in Sec. 4. Thus, instead of directly modeling
the complex distribution of interactions, we approach the interactions from the
contact vertices on the object, as shown in Fig. 2. This locality ensures that the
dynamics model remains focused on interactions in the contact region, without
being distracted by the motion of body parts that are irrelevant to the object.
Input Representation. Specifically, at each time step t, we abstract the past
action as H historical vertex trajectories {{vj}N }H , and the future action
i j=1 i=1
as F future vertex trajectories {{vj}N }H+F , where non-fixed variable N is
i j=1 i=H+1
thenumberofsampledcontactvertices.Notethatwetrainourdynamicsmodel
to forecast over a longer duration than the past motion (F > H), while only
the foremost future action will be used for autoregressive generation during the
inference, to facilitate the long-term generation process, as suggested in [16].
To determine these N vertices, we start with object’s signed distance fields
{sdf }H overthepastH frames,derivedfromthepaststates .Wethensample
i i=1 t
vertices that meet the following criteria:
|sdf (vj)|≤δ , ∀i=1,...,H, ∀j, (1)
i i 1
∥vj −vk∥≥δ , ∀j ̸=k, (2)
i i 2
where δ and δ are two hyperparameters. The objective is to sparsely sample
1 2
contact vertices,whileensuring theyare sufficientto encompass theinteraction.
Wecharacterizeeachvertextrajectory{vj}H+F withafeaturefj toprovide
i i=1
information in addition to motion, which includes (i) vertex coordinates at T-
pose,providinginformationaboutthepositionofthehumanvertexonthebody8 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
surface; (ii) the vertex-to-object surface distance, indicating vertex’s impact on
the object; and (iii) the vertex’s velocity relative to its nearest object vertex.
Architecture. An overview of the architecture is demonstrated in Fig. 2. An
unconditional dynamics block can be initiated as G(x ,Θ), mapping the input
k
feature map x at the k-th layer to another feature map, with Θ denoting the
k
dynamicsmodelparameters.Toincorporatehumanvertexcontrols,weintroduce
a secondary network F(yj,Θ ) operating on N vertex features {yj}N , where
k v k j=1
Θ isitsparameters.Withacross-attentionlayerAttn,thefinaldynamicsblock
v
is formulated as:
x ,{yj }N =Attn(G(x ,Θ),{F(yj,Θ )}N ). (3)
k+1 k+1 j=1 k k v j=1
We stack multiple blocks to form the dynamics model. The initial input, x ,
0
corresponds to the previous state s , while each yj represents the feature of the
t 0
vertex trajectory, containing both the trajectory {vj}H+F and its associated
i i=1
feature vector fj. The output of this model, s , is preliminary and subject to
t+1
furtheroptimizationasdetailedinSec.3.4,whichwillyieldthefinalfuturestate
s∗ . We utilize the Mean Squared Error loss to train the dynamics model. For
t+1
more details, please refer to Sec. B.2 of supplementary.
3.4 Optimization
Optimization serves as a role to introduce prior knowledge and avoid the accu-
mulation of errors. During inference, we input the initial action a and state
t+1
s and refine them into the fine-grained action a∗ and state s∗ . This re-
t+1 t+1 t+1
finement is achieved through gradient descent on the human and object pose
parameters. Our optimization includes several loss terms: a fitting loss to align
a∗ and s∗ with their preliminary counterparts, a velocity loss for temporal
t+1 t+1
smoothness, a contact loss to promote occurring contacts, and a collision loss to
reduce penetration in the interaction. For efficiency, we perform optimization
only if the loss is above a threshold. Specifically, given the reference interaction
sequence{h }L and{o }L ofarbitrarylengthL,derivedfromprevioussteps,
i i=1 i i=1
weapplygradientdescenttooptimizehumanposesequence{h∗}L andobject
i i=1
pose sequence {o∗}L , using the loss function,
i i=1
E =λ E +λ E +λ E +λ E , (4)
opt fit fit vel vel cont cont pene pene
where λ , λ , λ , and λ are hyperparameters.
fit vel cont pene
FittingLoss.WeminimizetheL1distancebetweentheinputandthereference,
L
(cid:88)
E = (∥h∗−h ∥ +∥o∗−o ∥ ). (5)
fit i i 1 i i 1
i=1
Velocity Loss. We leverage a velocity loss to smooth the interaction sequence,
L−1
(cid:88)
E = (∥h∗ −h∗∥ +∥o∗ −o∗∥ ). (6)
vel i+1 i 1 i+1 i 1
i=1InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 9
A person is seated in a chair. A person holds a large box up with their right hand,
lowers their right arm, and pulls the box with left
hand towards them.
A person is walking forward,
while carrying a backpack on his back. A person walks clockwise while holding an a small box with left hand. A person throws a yoga ball towards the ground.
Fig.3: Qualitative results on the BEHAVE dataset [7]. The interaction sequences
are presented through a time-series visualization where color changes denote progres-
sion through frames. Frames are separately visualized when the pelvis remains nearly
static. Here, our synergized knowledge comes from GPT-4 [67] and MotionGPT [35].
Contact Loss. We leverage a contact loss to encourage body part to contact
the object surface, if they are close to each other in the initial interaction,
L
(cid:88) (cid:88)
E = min∥v [d ]−v [d ]∥ . (7)
cont o∗ o h∗ h 2
i=1dh∈Ti
do i i
wherev [d ]denotesthevertexonthehumanbodysurface,andv [d ]repre-
h∗ h o∗ o
i i
sentsthecorrespondingvertexonthesurfaceoftheobject.AndT ={d |min
i h do
∥v [d ]−v [d ]∥ ≤ ϵ} includes the index of reference human vertex v [d ]
oi o hi h 2 hi h
that is close to the reference object vertex v [d ], where ϵ is a hyperparameter,
oi o
d and d are vertex indices for human mesh and object mesh, respectively.
h o
Penetration Loss. Given the signed-distance field of the human pose sdf ,
h∗
i
we employ a penetration loss to penalize the body-object interpenetration,
L
(cid:88)(cid:88)
E =− min(sdf (v [d ]),0). (8)
pene h∗ o∗ o
i i
i=1 do
4 Experiments
ExtensivecomparisonsevaluatetheperformanceofourInterDreameracrosstwo
motion-relevanttasks.DetailsoftheevaluationsettingsareprovidedinSec.4.1.
We present both quantitative (Sec. 4.2) and qualitative results (Sec. 4.3) for
our approach. Additionally, we perform ablation studies to verify the efficacy of
each component within our framework. These studies also cover the interaction
prediction task [116] to evaluate our dynamics model. Additional details and
results are presented Sec. C and Sec. D of the supplementary.10 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
Fig.4:Qualitativeresultsinmorechallengescenarioswithfree-forminput notfrom
our annotations, showing the ability of our InterDreamer to fit different object sizes
and handle complex and long sequences. Here, our synergized knowledge comes from
GPT-4 [67] and MotionGPT [35].
Table 1: Quantitative results on human motion quality on the BEHAVE dataset
with our annotation. We show that our high-level planning narrows the distributional
gap andeffectivelyadaptssinglehumangeneratorsintozero-shothuman-objectinter-
action generation. To evaluate R-Precision, a batch size of 16 is selected.
Methods Planning R-Precision↑ FID↓ MMDist↓ Multimodality↑ Diversity→
(Ours)
Top1 Top2 Top3
GroundTruth - 0.237±0.004 0.392±0.004 0.496±0.005 0.024±0.000 4.259±0.006 - 6.510±0.227
× 0.153±0.016 0.279±0.026 0.398±0.016 12.279±0.217 5.351±0.057 7.604±0.190 7.598±0.334
MDM[93] ✓ 0.163±0.0100.307±0.0430.402±0.01910.374±0.3045.303±0.117 7.281±0.083 7.471±0.427
× 0.205±0.011 0.351±0.002 0.458±0.021 10.208±0.500 4.837±0.064 4.520±0.163 7.323±0.412
MotionDiffuse[130] ✓ 0.216±0.0320.369±0.0230.472±0.027 9.015±0.403 4.649±0.029 4.991±0.172 7.295±0.501
× 0.196±0.009 0.338±0.011 0.448±0.012 6.385±0.201 4.855±0.029 5.889±0.524 7.160±0.306
ReMoDiffuse[131] ✓ 0.223±0.0060.368±0.0150.482±0.011 5.237±0.174 4.784±0.053 6.350±0.411 7.201±0.318
× 0.233±0.003 0.344±0.004 0.457±0.005 5.497±0.106 5.205±0.027 1.062±0.211 8.316±0.204
MotionGPT[35] ✓ 0.234±0.0040.387±0.0030.471±0.007 4.751±0.121 4.995±0.003 1.337±0.193 7.106±0.487
4.1 Experimental Setup
Datasets. We evaluate our model on the BEHAVE dataset [7], which includes
recordings of 8 individuals interacting with 20 everyday objects. Our analysis
focuses on 18 objects for which interaction sequences are available at 30 Hz.
The human pose is modeled using SMPL-H [58,82], with hand poses set to an
average pose due to the absence of detailed hand pose in the dataset. Object
poses are rotation matrices and translations. We manually segment the long
interaction sequences in the test set, and annotate them with descriptions as
well as their starting and ending indices, leading to 532 subsequences for eval-
uation. The CHAIRS [36] dataset encompasses the capture of 46 subjects, rep-
resented via SMPL-X [70] bodies, engaging with 81 distinct types of chairs and
sofas. Without whole dataset annotations, we use the CHAIRS dataset solely
for qualitative evaluation. We further use GPT-4 [67] to rephrase and diver-
sify annotations: 1)less complexity:someoneholdsabackpackandstepsleft;2)our
annotation: a person holds a backpack in front of them with both hands and takesInterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 11
A seated person on chair 63 adjusts the sitting position repeatedly. Someone can be seen on chair 96 and adjusts their sitting position.
Fig.5:QualitativeresultsontheCHAIRSdataset[36].Ourdynamicsmodeltrained
ontheBEHAVEdataset[7]generalizeswellontheCHAIRSdatasetunseenintraining.
Interaction sequences are visualized through a time-series style where color changes
denote progression through frames. Frames are separately visualized. Here, high-level
planning and low-level control use GPT-4 [67] and MotionGPT [35], respectively.
Table2:Quantitativeresultsonthezero-shottext-to-interactiontask(left)andon
theinteractionpredictiontask(right).Ourdynamicsmodelwithvertex-basedcontrol
generates interactions with the best quality.
Zero-shottext-to-interaction Interactionprediction[116]
Methods
CMD↓ Pene.(10−2%)↓ Trans.Err.(mm)↓Rot.Err.(10−3rad)↓Pene.(10−2%)↓
w/ocontrol 0.424 533 123 256 228
w/markercontrol(InterDiff[116]) 0.219 484 123 226 164
w/rawcontrol 0.325 957 129 265 218
w/vertexcontrol(ours) 0.151 443 119 221 156
a step to the left; 3) more complexity: with both hands, a person clutches a heavy
backpack firmly and brings it close to their body, then steps to the left with their
left leg. Metrics. The evaluation metrics are divided into three categories:
(i) Human motion quality: The Fréchet Inception Distance (FID) measures the
distributional distance between the generated motions and ground truth. The
MultiModality (Multimodality) and Diversity metrics assess the variance in
generated human motion. R-Precision evaluates the consistency between the
text and the generated human motion within the latent feature space. Multi-
Modal distance (MM Dist) is the distance between the motion feature and the
textfeature.Wefollow[25]togeneratemotionandtextfeatures.(ii)Interaction
quality: We propose a metric to measure the distance between contact maps of
realinteractionsandthosegenerated(CMD).Theper-sequencecontactmapis
definedbythepercentageoftimethateachbodypartisactivelyincontact.The
detailed formulation is provided in Sec. C of supplementary. We measure the
collision (Pene. [116]), which calculates the average percentage of object ver-
ticesthathavenon-negativevaluesinthehumansigneddistancefields[69].(iii)
Object motion accuracy: The dynamics model’s performance in the interaction
predictiontask[116]isevaluatedbytheaccuracyofpredictedobjectmotions,in-
cluding Trans. Err., the average distance between predicted and ground truth,
and Rot. Err., the average distance between the predicted and ground truth.
Baselines. As we are introducing a new task, there is no established base-
line available at the current stage. Note that it is unfair to compare our work
with concurrent supervised learning approaches [21,50,71], and their code is
not publicly available. To facilitate our comparisons, we develop various base-
lines to evaluate both our overall pipeline and its individual components. In theApplying great forces,
A person lifts an object with force with left hand
the man lifts the chair with one hand.
12 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
Someone can be seen sitting on a yogaball. A person is seated on an object.
(a) Motion w/ planning v.s. w/o planning (b) Text feature w/ planning v.s. w/o planning
Positioned in front of the bag,
A person is holding an object with both hands.
the individual is holding onto it.
Fig.6: (a) Ablation study on the high-level planning. On the left are results from
MotionGPT [35] based on raw descriptions (“w/o planning”); on the right are results
withourplanning(“w/planning”).Free-form descriptionswiththeout-of-distribution
object name lead to results that bear little resemblance to the description. (b) We vi-
sualizeCLIP[78]featuresofdescriptionsonHumanML3D[25],ourinitialrawannota-
tions(“w/oplanning”),andtheannotationsprocessedthroughourhigh-levelplanning
framework (“w/ planning”). The CLIP features are processed via t-SNE [61].
Table 3: Ablation study on the high-level planning. Q1 and Q2 ask to identify the
object category and the contact body part, respectively. We assess the accuracy by
comparing the LLM’s responses with labels we annotate. Note that the text input to
LLMsmaycontainambiguities;forexample,theannotationis“hand” whenthemotion
uses “right hand.” We include Q1 Acc∗ and Q2 Acc∗ excluding ambiguous text.
LLM(#ofparameters) Q1Acc↑Q1Acc∗↑Q2Acc↑Q2Acc∗↑
GPT-4[67] 0.801 0.997 0.703 0.964
Llama-2(7B)[95] 0.073 0.147 0.436 0.689
Llama-2(13B)[95] 0.232 0.319 0.662 0.853
Llama-2(70B)[95] 0.722 0.967 0.798 0.907
context of high-level planning, we utilize GPT-4 [67] and Llama-2 [95], illus-
trating the effectiveness of our prompts across different language models. For
low-level motion generation control, our baselines include MDM [93], Motion-
Diffuse [130], ReMoDiffuse [131], and MotionGPT [35], which span a range of
text-to-motionapproachestrainedonHumanML3D[25]andshowthegeneraliz-
ability of our framework. Our dynamics model baselines are varied, comprising:
InterDreamer with InterDiff [116], which adopts their Interaction Correction
module as the dynamics model; InterDreamer without control, which operates
object dynamics independently of human motion; and InterDreamer with raw
control, utilizing unprocessed human motion to guide the dynamics.
4.2 Quantitative Results
Table 1 presents a comparative analysis of our approach, InterDreamer adopt-
ingvarioustext-to-motionmodelsagainstfourcounterpartsontheBEHAVE[7]
dataset. Our approach generalizes across various models and consistently out-
performs baselines. Specifically, InterDreamer exhibits superior motion quality,
reflected by a significantly lower FID, higher R-Precision, and better diversity,
highlightingthebenefitsofincorporatingourplanningtoreducethedistribution
gap in the zero-shot setting.InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 13
Fig.7: Results from the interaction retrieval. We demonstrate that our proposed re-
trievalapproachbasedonhandcraftrulescanextractdiverseandrealisticinteractions.
Ofthese,oneinteractionissampledbasedonapre-definedmetricforsubsequentsteps.
(a) raw control (b) vertex-based control
Fig.8: Ablation study on the dynamics modeling. Given the text description of “A
personwalksclockwisewhileholdingasmallboxwithlefthand,” our(b)vertex-based
control can synthesize consistent contacts, which (a) the baseline fails to do.
In Table 2, comparing our whole pipeline to baselines without explicit con-
trol, InterDreamer achieves better interaction quality in terms of CMD and
penetration scores, showing the importance of human influence on object mo-
tion.Againstmethodsthatutilizedirectrawhumanmotionormarkers[116]for
control, our full method demonstrates enhanced performance by offering more
fine-grained guidance and extracting generalizable features.
4.3 Qualitative Results
Fig. 3 displays several results guided by the text that we annotate on the BE-
HAVE dataset [7]. Our method exhibits proficiency in interpreting the textual
input and synthesizing dynamic, realistic interactions, despite the absence of
training with text-interaction paired data. More importantly, our method can
process free-form language descriptions of motion that deviate from our anno-
tation, exhibiting zero-shot generalizability in its performance, as illustrated in
Fig. 4, where we selectively use more complex sequences of interactive descrip-
tions that are beyond the scope of the original dataset. Fig. 5 further exemplifies
the zero-shot ability of our method that is able to generalize effectively to the
CHAIRSdataset[36],despiteourdynamicsmodelnotbeingtrainedonit.Fig.7
depicts the retrieval procedure, resulting in a diverse set of interactions that are
both high-quality and semantically aligned. More experimental results and the
user study are presented in Sec. D of supplementary.
4.4 Ablation Study
Adaptability of high-level planning. Is our framework adaptable across dif-
ferent large language models (LLMs)? As illustrated in Table 3, our analysis14 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
contains two types of language models: GPT-4 [67], which is accessible through
APIs and operates as a black box model; and Llama-2 [95], which is an open-
source model. We demonstrate that language models with large parameters ex-
hibit very high accuracy in responding to questions tailored to our prompts,
thereby validating the framework’s adaptability.
Effectiveness of text-to-motion with high-level planning. In consistency
withTable1,Fig.6offersaqualitativecomparisonoftext-to-motionresults,con-
trasting outputs with and without augmentation by LLM-revised text descrip-
tions. The comparison shows that motions generated without LLM-enhanced
descriptions often fail to correspond with the intended text, if the text with the
objectdescriptionisnotinthedistributionoftrainingdataonHumanML3D[25].
This underscores the LLM’s critical role in bridging the gap in a zero-shot way.
On the right-hand side of Fig. 6, we see that planning not only reduces the
distributional gap in motion, but also directly reduces it in text. We visualize
the CLIP [78] features of descriptions on HumanML3D, our raw annotations,
andtheannotationsprocessedbyhigh-levelplanning.Thetextprocessedbythe
planning shows more similarity to the in-distributional text, where the average
cosine similarity is 0.932 over 0.913 from the raw annotation.
Effectiveness of world model. In the quantitative evaluation, we show that
the performance of our pipeline is enhanced by the tailored design of our world
model. Table 2 provides additional evidence of this effectiveness by integrating
the proposed world model, as interaction correction within the InterDiff frame-
work[116]intheinteractionpredictiontask.Thisimplementationdemonstrates
enhanced conditionality in the object dynamics modeling across various tasks,
attributed to the vertex-level control. Doing so effectively removes the whole-
body complexity, most of which tends to be irrelevant to the interaction. Fig. 8
further indicates that our vertex-based control is able to establish consistent
interactions over time, while guidance from motion features is not robust.
5 Conclusion
We introduce the novel task of text-guided 3D human-object interaction gen-
eration, and we aim to achieve this without reliance on text-interaction pair
data.Tothisend,wepresentInterDreamerthatdecouplesinteractiondynamics
from semantics, where high-level planning and low-level control are introduced
to generate semantically aligned human motion and initial object pose, while a
world model is responsible for the object dynamics guided by the interaction.
Ourapproachdemonstratespromisingeffectivenessinthisnoveltask,suggesting
its considerable potential for various real-world applications.
Limitations. The current utilization of dynamics modeling could be enhanced.
A prospective improvement involves incorporating model-based learning tech-
niques, which empower the agent to more effectively interact with the environ-
ment and learn a broader range of skills. The generated results may not be
physicallyplausibleandhandposesareroughbecausetheyaremissingfromthe
dataset, but could be improved by integrating a physics simulator.InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 15
References
1. Ahuja, C., Morency, L.P.: Language2pose: Natural language grounded pose fore-
casting. In: 3DV (2019)
2. Athanasiou, N., Petrovich, M., Black, M.J., Varol, G.: Teach: Temporal action
composition for 3d humans. In: 3DV (2022)
3. Athanasiou,N.,Petrovich,M.,Black,M.J.,Varol,G.:SINC:Spatialcomposition
of 3d human motions for simultaneous action generation. In: ICCV (2023)
4. Bae, J., Won, J., Lim, D., Min, C.H., Kim, Y.M.: Pmp: Learning to physically
interactwithenvironmentsusingpart-wisemotionpriors.In:SIGGRAPH(2023)
5. Barquero,G.,Escalera,S.,Palmero,C.:BeLFusion:Latentdiffusionforbehavior-
driven human motion prediction. In: ICCV (2023)
6. Barquero, G., Escalera, S., Palmero, C.: Seamless human motion composition
with blended positional encodings. In: CVPR (2024)
7. Bhatnagar, B.L., Xie, X., Petrov, I., Sminchisescu, C., Theobalt, C., Pons-Moll,
G.: BEHAVE: Dataset and method for tracking human object interactions. In:
CVPR (2022)
8. Braun, J., Christen, S., Kocabas, M., Aksan, E., Hilliges, O.: Physically plausi-
ble full-body hand-object interaction synthesis. arXiv preprint arXiv:2309.07907
(2023)
9. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image
editing instructions. In: CVPR (2023)
10. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-
shot learners. In: NeurIPS (2020)
11. Cao,Z.,Gao,H.,Mangalam,K.,Cai,Q.Z.,Vo,M.,Malik,J.:Long-termhuman
motion prediction with scene context. In: ECCV (2020)
12. Casas, D., Comino-Trinidad, M.: SMPLitex: A generative model and dataset for
3d human texture estimation from single image. In: BMVC (2023)
13. Chao,Y.W.,Yang,J.,Chen,W.,Deng,J.:Learningtosit:Synthesizinghuman-
chair interactions via hierarchical control. In: AAAI (2021)
14. Chen, L.H., Zhang, J., Li, Y., Pang, Y., Xia, X., Liu, T.: HumanMAC: Masked
motion completion for human motion prediction. In: ICCV (2023)
15. Chen,X.,Jiang,B.,Liu,W.,Huang,Z.,Fu,B.,Chen,T.,Yu,G.:Executingyour
commands via motion diffusion in latent space. In: CVPR (2023)
16. Chi,C.,Feng,S.,Du,Y.,Xu,Z.,Cousineau,E.,Burchfiel,B.,Song,S.:Diffusion
policy: Visuomotor policy learning via action diffusion. In: RSS (2023)
17. Cong, P., Dou, Z.W., Ren, Y., Yin, W., Cheng, K., Sun, Y., Long, X., Zhu, X.,
Ma,Y.:LaserHuman:Language-guidedscene-awarehumanmotiongenerationin
free environment. arXiv preprint arXiv:2403.13307 (2024)
18. Corona,E.,Pumarola,A.,Alenya,G.,Moreno-Noguer,F.:Context-awarehuman
motion prediction. In: CVPR (2020)
19. Cui, J., Liu, T., Liu, N., Yang, Y., Zhu, Y., Huang, S.: AnySkill: Learning open-
vocabulary physical skill for interactive agents. In: CVPR (2024)
20. Dabral, R., Mughal, M.H., Golyanik, V., Theobalt, C.: MoFusion: A framework
for denoising-diffusion-based motion synthesis. In: CVPR. pp. 9760–9770 (2023)
21. Diller, C., Dai, A.: CG-HOI: Contact-guided 3d human-object interaction gener-
ation. In: CVPR (2024)
22. Fan, Z., Taheri, O., Tzionas, D., Kocabas, M., Kaufmann, M., Black, M.J.,
Hilliges, O.: ARCTIC: A dataset for dexterous bimanual hand-object manipu-
lation. In: CVPR (2023)16 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
23. Ghosh, A., Dabral, R., Golyanik, V., Theobalt, C., Slusallek, P.: IMoS: Intent-
driven full-body motion synthesis for human-object interactions. arXiv preprint
arXiv:2212.07555 (2022)
24. Ghosh,A.,Dabral,R.,Golyanik,V.,Theobalt,C.,Slusallek,P.:ReMoS:Reactive
3dmotionsynthesisfortwo-personinteractions.arXivpreprintarXiv:2311.17057
(2023)
25. Guo,C.,Zou,S.,Zuo,X.,Wang,S.,Ji,W.,Li,X.,Cheng,L.:Generatingdiverse
and natural 3d human motions from text. In: CVPR (2022)
26. Guo,C.,Zuo,X.,Wang,S.,Cheng,L.:Tm2t:Stochasticandtokenizedmodeling
for the reciprocal generation of 3d human motions and texts. In: ECCV (2022)
27. Guo,C.,Zuo,X.,Wang,S.,Zou,S.,Sun,Q.,Deng,A.,Gong,M.,Cheng,L.:Ac-
tion2motion: Conditioned generation of 3d human motions. In: ACMMM (2020)
28. Han, S., Joo, H.: CHORUS: Learning canonicalized 3d human-object spatial re-
lations from unbounded synthesized images. In: ICCV (2023)
29. Hassan, M., Ghosh, P., Tesch, J., Tzionas, D., Black, M.J.: Populating 3d scenes
by learning human-scene interaction. In: CVPR (2021)
30. Hassan, M., Guo, Y., Wang, T., Black, M., Fidler, S., Peng, X.B.: Synthesizing
physical character-scene interactions. In: SIGGRAPH (2023)
31. Ho,J.,Jain,A.,Abbeel,P.:Denoisingdiffusionprobabilisticmodels.In:NeurIPS
(2020)
32. Hou,Z.,Yu,B.,Tao,D.:Compositional3dhuman-objectneuralanimation.arXiv
preprint arXiv:2304.14070 (2023)
33. Huang, S., Wang, Z., Li, P., Jia, B., Liu, T., Zhu, Y., Liang, W., Zhu, S.C.:
Diffusion-based generation, optimization, and planning in 3d scenes. In: CVPR
(2023)
34. Huang, Y., Taheri, O., Black, M.J., Tzionas, D.: InterCap: Joint markerless 3D
tracking of humans and objects in interaction. In: GCPR (2022)
35. Jiang, B., Chen, X., Liu, W., Yu, J., Yu, G., Chen, T.: MotionGPT: Human
motion as a foreign language. In: NeurIPS (2023)
36. Jiang, N., Liu, T., Cao, Z., Cui, J., Chen, Y., Wang, H., Zhu, Y., Huang,
S.: CHAIRS: Towards full-body articulated human-object interaction. In: ICCV
(2023)
37. Jiang,N.,Zhang,Z.,Li,H.,Ma,X.,Wang,Z.,Chen,Y.,Liu,T.,Zhu,Y.,Huang,
S.: Scaling up dynamic human-scene interaction modeling. In: CVPR (2024)
38. Karunratanakul,K.,Preechakul,K.,Suwajanakorn,S.,Tang,S.:GMD:Control-
lable human motion synthesis via guided diffusion models. In: ICCV (2023)
39. Kaufmann, M., Aksan, E., Song, J., Pece, F., Ziegler, R., Hilliges, O.: Convolu-
tional autoencoders for human motion infilling. In: 3DV (2020)
40. Kim, H., Han, S., Kwon, P., Joo, H.: Zero-shot learning for the primitives of 3d
affordance in general objects. arXiv preprint arXiv:2401.12978 (2024)
41. Kim, J., Kim, J., Na, J., Joo, H.: ParaHome: Parameterizing everyday home
activities towards 3d generative modeling of human-object interactions. arXiv
preprint arXiv:2401.10232 (2024)
42. Kim, J., Kim, J., Choi, S.: Flame: Free-form language-based motion synthesis &
editing. In: AAAI (2023)
43. Kim, S.W., Zhou, Y., Philion, J., Torralba, A., Fidler, S.: Learning to simulate
dynamic environments with gamegan. In: CVPR (2020)
44. Kim, T., Saito, S., Joo, H.: NCHO: Unsupervised learning for neural 3d compo-
sition of humans and objects. In: ICCV (2023)
45. Kong,H.,Gong,K.,Lian,D.,Mi,M.B.,Wang,X.:Priority-centrichumanmotion
generation in discrete latent space. In: ICCV (2023)InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 17
46. Krebs, F., Meixner, A., Patzer, I., Asfour, T.: The kit bimanual manipulation
dataset. In: Humanoids (2021)
47. Kulkarni,N.,Rempe,D.,Genova,K.,Kundu,A.,Johnson,J.,Fouhey,D.,Guibas,
L.: NIFTY: Neural object interaction fields for guided human motion synthesis.
arXiv preprint arXiv:2307.07511 (2023)
48. Lee,J.,Joo,H.:Locomotion-Action-Manipulation:Synthesizinghuman-scenein-
teractions in complex 3d environments. In: ICCV (2023)
49. Lee,T.,Moon,G.,Lee,K.M.:Multiact:Long-term3dhumanmotiongeneration
from multiple action labels. In: AAAI (2023)
50. Li,J.,Clegg,A.,Mottaghi,R.,Wu,J.,Puig,X.,Liu,C.K.:Controllablehuman-
object interaction synthesis. arXiv preprint arXiv:2312.03913 (2023)
51. Li, J., Wu, J., Liu, C.K.: Object motion guided human motion synthesis. ACM
Transactions on Graphics (TOG) 42(6), 1–11 (2023)
52. Li, L., Dai, A.: GenZI: Zero-shot 3d human-scene interaction generation. In:
CVPR (2024)
53. Li, Q., Wang, J., Loy, C.C., Dai, B.: Task-oriented human-object interactions
generation with implicit neural representations. arXiv preprint arXiv:2303.13129
(2023)
54. Liang, H., Zhang, W., Li, W., Yu, J., Xu, L.: InterGen: Diffusion-based
multi-human motion generation under complex interactions. arXiv preprint
arXiv:2304.05684 (2023)
55. Lin, J., Zeng, A., Lu, S., Cai, Y., Zhang, R., Wang, H., Zhang, L.: Motion-X: A
large-scale 3d expressive whole-body human motion dataset. In: NeurIPS (2023)
56. Liu, L., Hodgins, J.: Learning basketball dribbling skills using trajectory opti-
mizationanddeepreinforcementlearning.ACMTransactionsonGraphics(TOG)
37(4), 1–14 (2018)
57. Liu,Y.,Chen,C.,Yi,L.:Interactivehumanoid:Onlinefull-bodymotionreaction
synthesis with social affordance canonicalization and forecasting. arXiv preprint
arXiv:2312.08983 (2023)
58. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A
skinned multi-person linear model. ACM transactions on graphics (2015)
59. Lu, S., Chen, L.H., Zeng, A., Lin, J., Zhang, R., Zhang, L., Shum, H.Y.:
HumanTOMATO: Text-aligned whole-body motion generation. arXiv preprint
arXiv:2310.12978 (2023)
60. Ma, S., Cao, Q., Zhang, J., Tao, D.: Contact-aware human motion generation
from textual descriptions. arXiv preprint arXiv:2403.15709 (2024)
61. VanderMaaten,L.,Hinton,G.:Visualizingdatausingt-sne.Journalofmachine
learning research 9(11) (2008)
62. Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: AMASS:
Archive of motion capture as surface shapes. In: ICCV (2019)
63. Mandery, C., Terlemez, O., Do, M., Vahrenkamp, N., Asfour, T.: The kit whole-
body human motion database. In: ICAR (2015)
64. Mandery, C., Terlemez, O., Do, M., Vahrenkamp, N., Asfour, T.: Unifying rep-
resentations and large-scale whole-body motion databases for studying human
motion. IEEE Transactions on Robotics 32(4), 796–809 (2016)
65. Mao, W., Liu, M., Salzmann, M.: Generating smooth pose sequences for diverse
human motion prediction. In: CVPR (2021)
66. Merel,J.,Tunyasuvunakool,S.,Ahuja,A.,Tassa,Y.,Hasenclever,L.,Pham,V.,
Erez, T., Wayne, G., Heess, N.: Catch & carry: reusable neural controllers for
vision-guided whole-body tasks. ACM Transactions on Graphics (TOG) 39(4),
39–1 (2020)18 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
67. OpenAI: ChatGPT. https://chat.openai.com/ (2023)
68. Pan, L., Wang, J., Huang, B., Zhang, J., Wang, H., Tang, X., Wang, Y.:
Synthesizing physically plausible human motions in 3d scenes. arXiv preprint
arXiv:2308.09036 (2023)
69. Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: DeepSDF:
Learningcontinuoussigneddistancefunctionsforshaperepresentation.In:CVPR
(2019)
70. Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A.A., Tzionas,
D.,Black,M.J.:Expressivebodycapture:3Dhands,face,andbodyfromasingle
image. In: CVPR (2019)
71. Peng,X.,Xie,Y.,Wu,Z.,Jampani,V.,Sun,D.,Jiang,H.:HOI-Diff:Text-driven
synthesis of 3d human-object interactions using diffusion models. arXiv preprint
arXiv:2312.06553 (2023)
72. Petrov,I.A.,Marin,R.,Chibane,J.,Pons-Moll,G.:Objectpop-up:Canweinfer
3d objects and their poses from human interactions alone? In: CVPR (2023)
73. Petrovich,M.,Black,M.J.,Varol,G.:Action-conditioned3dhumanmotionsyn-
thesis with transformer vae. In: ICCV (2021)
74. Petrovich, M., Black, M.J., Varol, G.: TEMOS: Generating diverse human mo-
tions from textual descriptions. In: ECCV (2022)
75. Petrovich, M., Black, M.J., Varol, G.: TMR: Text-to-motion retrieval using con-
trastive 3d human motion synthesis. In: ICCV (2023)
76. Raab, S., Leibovitch, I., Li, P., Aberman, K., Sorkine-Hornung, O., Cohen-Or,
D.: MoDi: Unconditional motion synthesis from diverse data. In: CVPR (2023)
77. Raab,S.,Leibovitch,I.,Tevet,G.,Arar,M.,Bermano,A.H.,Cohen-Or,D.:Single
motion diffusion. arXiv preprint arXiv:2302.05905 (2023)
78. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021)
79. Razali,H.,Demiris,Y.:Action-conditionedgenerationofbimanualobjectmanip-
ulation sequences. In: AAAI (2023)
80. Rempe, D., Luo, Z., Bin Peng, X., Yuan, Y., Kitani, K., Kreis, K., Fidler, S.,
Litany,O.:Traceandpace:Controllablepedestriananimationviaguidedtrajec-
tory diffusion. In: CVPR (2023)
81. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022)
82. Romero, J., Tzionas, D., Black, M.J.: Embodied hands: Modeling and capturing
hands and bodies together. ACM Transactions on Graphics 36(6) (2017)
83. Seo,Y.,Hafner,D.,Liu,H.,Liu,F.,James,S.,Lee,K.,Abbeel,P.:Maskedworld
models for visual control. In: CoRL (2023)
84. Shafir, Y., Tevet, G., Kapon, R., Bermano, A.H.: Human motion diffusion as a
generative prior. arXiv preprint arXiv:2303.01418 (2023)
85. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-
vised learning using nonequilibrium thermodynamics. In: ICML (2015)
86. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.arXivpreprint
arXiv:2010.02502 (2020)
87. Starke, S., Zhang, H., Komura, T., Saito, J.: Neural state machine for character-
scene interactions. ACM Trans. Graph. 38(6), 209–1 (2019)
88. Starke, S., Zhao, Y., Komura, T., Zaman, K.: Local motion phases for learn-
ing multi-contact character movements. ACM Transactions on Graphics (TOG)
39(4), 54–1 (2020)InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 19
89. Taheri, O., Choutas, V., Black, M.J., Tzionas, D.: GOAL: Generating 4d whole-
body motion for hand-object grasping. In: CVPR (2022)
90. Taheri, O., Ghorbani, N., Black, M.J., Tzionas, D.: GRAB: A dataset of whole-
body human grasping of objects. In: ECCV (2020)
91. Tendulkar, P., Surís, D., Vondrick, C.: FLEX: Full-body grasping without full-
body grasps. In: CVPR (2023)
92. Tevet, G., Gordon, B., Hertz, A., Bermano, A.H., Cohen-Or, D.: Motionclip:
Exposing human motion generation to clip space. In: ECCV (2022)
93. Tevet,G.,Raab,S.,Gordon,B.,Shafir,Y.,Cohen-Or,D.,Bermano,A.H.:Human
motion diffusion model. arXiv preprint arXiv:2209.14916 (2022)
94. Tevet,G.,Raab,S.,Gordon,B.,Shafir,Y.,Cohen-or,D.,Bermano,A.H.:Human
motion diffusion model. In: ICLR (2023)
95. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)
96. Turk, A.M.: Amazon mechanical turk. Retrieved August 17, 2012 (2012)
97. Wan, W., Dou, Z., Komura, T., Wang, W., Jayaraman, D., Liu, L.: Tlcontrol:
Trajectory and language control for human motion synthesis. arXiv preprint
arXiv:2311.17135 (2023)
98. Wan, W., Yang, L., Liu, L., Zhang, Z., Jia, R., Choi, Y.K., Pan, J., Theobalt,
C.,Komura,T.,Wang,W.:Learntopredicthowhumansmanipulatelarge-sized
objectsfrominteractivemotions.IEEERoboticsandAutomationLetters(2022)
99. Wang, J., Xu, H., Xu, J., Liu, S., Wang, X.: Synthesizing long-term 3d human
motion and interaction in 3d scenes. In: CVPR (2021)
100. Wang,J.,Rong,Y.,Liu,J.,Yan,S.,Lin,D.,Dai,B.:Towardsdiverseandnatural
scene-aware 3d human motion synthesis. In: CVPR (2022)
101. Wang, J., Yan, S., Dai, B., Lin, D.: Scene-aware generative network for human
motion synthesis. In: CVPR (2021)
102. Wang, X., Li, G., Kuo, Y.L., Kocabas, M., Aksan, E., Hilliges, O.: Reconstruct-
ing action-conditioned human-object interactions using commonsense knowledge
priors. In: 3DV (2022)
103. Wang,Y.,Lin,J.,Zeng,A.,Luo,Z.,Zhang,J.,Zhang,L.:PhysHOI:Physics-based
imitationofdynamichuman-objectinteraction.arXivpreprintarXiv:2312.04393
(2023)
104. Wang, Z., Chen, Y., Liu, T., Zhu, Y., Liang, W., Huang, S.: HUMANISE:
Language-conditionedhumanmotiongenerationin3dscenes.In:NeurIPS(2022)
105. Wang, Z., Wang, J., Lin, D., Dai, B.: InterControl: Generate human motion in-
teractions by controlling every joint. arXiv preprint arXiv:2311.15864 (2023)
106. Wei,D.,Sun,X.,Sun,H.,Li,B.,Hu,S.,Li,W.,Lu,J.:Understandingtext-driven
motionsynthesiswithkeyframecollaborationviadiffusionmodels.arXivpreprint
arXiv:2305.13773 (2023)
107. Wei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,Xia,F.,Chi,E.,Le,Q.V.,Zhou,
D.,etal.:Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.
In: NeurIPS (2022)
108. Wu,P.,Escontrela,A.,Hafner,D.,Abbeel,P.,Goldberg,K.:Daydreamer:World
models for physical robot learning. In: CoRL (2023)
109. Wu,Q.,Shi,Y.,Huang,X.,Yu,J.,Xu,L.,Wang,J.:THOR:Texttohuman-object
interaction diffusion via relation intervention. arXiv preprint arXiv:2403.11208
(2024)
110. Wu, Y., Wang, J., Zhang, Y., Zhang, S., Hilliges, O., Yu, F., Tang, S.: SAGA:
Stochastic whole-body grasping with contact. In: ECCV (2022)20 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
111. Xiao, Z., Wang, T., Wang, J., Cao, J., Zhang, W., Dai, B., Lin, D., Pang, J.:
Unified human-scene interaction via prompted chain-of-contacts. arXiv preprint
arXiv:2309.07918 (2023)
112. Xie, X., Bhatnagar, B.L., Pons-Moll, G.: Chore: Contact, human and object re-
construction from a single rgb image. In: ECCV (2022)
113. Xie, Y., Jampani, V., Zhong, L., Sun, D., Jiang, H.: OmniControl: Control any
joint at any time for human motion generation. arXiv preprint arXiv:2310.08580
(2023)
114. Xie, Z., Starke, S., Ling, H.Y., van de Panne, M.: Learning soccer juggling skills
with layer-wise mixture-of-experts. In: SIGGRAPH (2022)
115. Xie,Z.,Tseng,J.,Starke,S.,vandePanne,M.,Liu,C.K.:Hierarchicalplanning
and control for box loco-manipulation. arXiv preprint arXiv:2306.09532 (2023)
116. Xu, S., Li, Z., Wang, Y.X., Gui, L.Y.: InterDiff: Generating 3d human-object
interactions with physics-informed diffusion. In: ICCV (2023)
117. Xu,S.,Wang,Y.X.,Gui,L.Y.:Diversehumanmotionpredictionguidedbymulti-
level spatial-temporal anchors. In: ECCV (2022)
118. Xu, S., Wang, Y.X., Gui, L.: Stochastic multi-person 3d motion forecasting. In:
ICLR (2023)
119. Xu, X., Joo, H., Mori, G., Savva, M.: D3D-HOI: Dynamic 3d human-object in-
teractions from videos. arXiv preprint arXiv:2108.08420 (2021)
120. Yang, Y., Zhai, W., Luo, H., Cao, Y., Zha, Z.J.: LEMON: Learning 3d human-
object interaction relation from 2d images. In: CVPR (2024)
121. Yang, Z., Yin, K., Liu, L.: Learning to use chopsticks in diverse gripping styles.
ACM Transactions on Graphics (TOG) 41(4), 1–17 (2022)
122. Yao, H., Song, Z., Zhou, Y., Ao, T., Chen, B., Liu, L.: MoConVQ: Unified
physics-basedmotioncontrolviascalablediscreterepresentations.arXivpreprint
arXiv:2310.10198 (2023)
123. Yazdian, P.J., Liu, E., Cheng, L., Lim, A.: MotionScript: Natural language de-
scriptions for expressive 3d human motions. arXiv preprint arXiv:2312.12634
(2023)
124. Ye, Y., Li, X., Gupta, A., De Mello, S., Birchfield, S., Song, J., Tulsiani, S., Liu,
S.:Affordancediffusion:Synthesizinghand-objectinteractions.In:CVPR(2023)
125. Yuan, Y., Kitani, K.: DLow: Diversifying latent flows for diverse human motion
prediction. In: ECCV (2020)
126. Zhang, H., Christen, S., Fan, Z., Zheng, L., Hwangbo, J., Song, J., Hilliges, O.:
ArtiGrasp: Physically plausible synthesis of bi-manual dexterous grasping and
articulation. arXiv preprint arXiv:2309.03891 (2023)
127. Zhang,J.Y.,Pepose,S.,Joo,H.,Ramanan,D.,Malik,J.,Kanazawa,A.:Perceiv-
ing 3d human-object spatial arrangements from a single image in the wild. In:
ECCV (2020)
128. Zhang, J., Zhang, Y., Cun, X., Zhang, Y., Zhao, H., Lu, H., Shen, X., Shan, Y.:
Generatinghumanmotionfromtextualdescriptionswithdiscreterepresentations.
In: CVPR (2023)
129. Zhang, J., Luo, H., Yang, H., Xu, X., Wu, Q., Shi, Y., Yu, J., Xu, L., Wang,
J.: NeuralDome: A neural modeling pipeline on multi-view human-object inter-
actions. In: CVPR (2023)
130. Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., Liu, Z.: MotionDif-
fuse: Text-driven human motion generation with diffusion model. arXiv preprint
arXiv:2208.15001 (2022)
131. Zhang,M.,Guo,X.,Pan,L.,Cai,Z.,Hong,F.,Li,H.,Yang,L.,Liu,Z.:ReMoD-
iffuse: Retrieval-augmented motion diffusion model. In: ICCV (2023)InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 21
132. Zhang,W.,Dabral,R.,Leimkühler,T.,Golyanik,V.,Habermann,M.,Theobalt,
C.: ROAM: Robust and object-aware motion generation using neural pose de-
scriptors. arXiv preprint arXiv:2308.12969 (2023)
133. Zhang, X., Bhatnagar, B.L., Starke, S., Guzov, V., Pons-Moll, G.: COUCH: To-
wards controllable human-chair interactions. In: ECCV (2022)
134. Zhang,Y.,Huang,D.,Liu,B.,Tang,S.,Lu,Y.,Chen,L.,Bai,L.,Chu,Q.,Yu,N.,
Ouyang, W.: Motiongpt: Finetuned llms are general-purpose motion generators.
arXiv preprint arXiv:2306.10900 (2023)
135. Zhang, Z., Liu, R., Aberman, K., Hanocka, R.: TEDi: Temporally-entangled dif-
fusion for long-term motion synthesis. arXiv preprint arXiv:2307.15042 (2023)
136. Zhao, C., Zhang, J., Du, J., Shan, Z., Wang, J., Yu, J., Wang, J., Xu, L.: I’M
HOI:Inertia-awaremonocularcaptureof3dhuman-objectinteractions.In:CVPR
(2024)
137. Zhao,K.,Wang,S.,Zhang,Y.,Beeler,T.,Tang,S.:Compositionalhuman-scene
interaction synthesis with semantic control. In: ECCV (2022)
138. Zhao, K., Zhang, Y., Wang, S., Beeler, T., Tang, S.: Synthesizing diverse human
motions in 3d indoor scenes. In: ICCV (2023)
139. Zheng, J., Zheng, Q., Fang, L., Liu, Y., Yi, L.: CAMS: Canonicalized manipula-
tion spaces for category-level functional hand-object manipulation synthesis. In:
CVPR (2023)
140. Zhou, K., Bhatnagar, B.L., Lenssen, J.E., Pons-Moll, G.: Toch: Spatio-temporal
object-to-hand correspondence for motion refinement. In: ECCV (2022)
141. Zhou, W., Dou, Z., Cao, Z., Liao, Z., Wang, J., Wang, W., Liu, Y., Komura, T.,
Wang,W.,Liu,L.:EMDM:Efficientmotiondiffusionmodelforfast,high-quality
motion generation. arXiv preprint arXiv:2312.02256 (2023)22 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
Supplementary Material
In this supplementary material, we include additional method details and ex-
perimental results: (i) We provide a demo video, explained in Sec. A. (ii) We
presentadditionaldetailsofinteractionretrieval,worldmodel,andoptimization
in Sec. B. (iii) We provide implementation details and additional information
on the experimental setup in Sec. C. (iv) We provide additional qualitative ex-
periments in Sec. D.
A Visualization Video
Beyond the qualitative results presented in the main paper, we include a video
ontheprojectwebsitethatoffersmoredetailedvisualizationsofthetask,further
illustrating the efficacy of our approach. These demos highlight (i) We conduct
aqualitativecomparisonofourapproachwithexistingtext-to-HOIwork[21,71]
within the framework of supervised learning. Note that as our setting is the
zero-shotgeneration,itisunfairtocompareourworkwiththeseapproaches;we
include the comparison here for additional reference. We evaluate our method
bydirectlytestingourtrainedmodelontheannotateddataavailablefromtheir
websites,specificallyretrievingtheirgeneratedvideosfordirectcomparison.Re-
markably, even without training on these datasets, our method generates results
thatdemonstratehigh-qualityinteractions.(ii)Thesmoothnessoftheinteraction
sequences generated by our method. (iii) Remarkably, our method is proficient
in creating sequences that maintain consistent contact where contact remains
largely unchanged during interactions, such as carrying a box and sitting in
a chair. (iv) It is even capable of synthesizing complex interactions involving
dynamically-changing contact, such as the handover and throwing of objects.
(v) We contrast our framework, which incorporates vertex control, against gen-
erationprocessesthatrelyonrawmotioncontrol.Theresultsdemonstratestrong
generalizability of our method.
B Additional Details of Methodology
B.1 Low-Level Control
Handcraft Interaction Retrieval. In Sec. 3.2 of the main paper, we detail
the construction of the interaction database and emphasize the use of body
parts and object categories as keys to fetch semantically-aligned contact maps.
Same as the main paper, we define a contact map as a list of K index pairs of
vertices {(di,di)}K . This section delves into the methodology for outlining an
h o i=1
optimization process to generate the object initial pose s given contact maps
1
andtheinitialhumanposea ,andchooseoneposebasedonapredefinedmetric.
1
Let v [d ] denote the vertex on the surface of the object, and v [d ] rep-
h1 o o1 h
resent the corresponding vertex on the human body surface, where d and d
o hInterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 23
are the indices of vertices. Specifically, to optimize s , the overall optimization
1
objective is given by,
E =λ E +λ E +λ E , (9)
opt fit fit cont cont pene pene
where λ , λ , and λ are hyperparameters.
fit cont pene
Fitting Loss. To project a contact map to a object pose, we minimize the L2
distance between the human vertices and the object vertices indicated by the
contact map,
K
(cid:88)
E = ∥v [di]−v [di]∥ . (10)
fit o1 o h1 h 2
i=1
Contact Loss. We leverage a contact loss to encourage body part to contact
the object surface,
(cid:88)
E = min∥v [d ]−v [d ]∥ . (11)
cont
dh
do
o1 o h1 h 2
where d here indicates the vertices index in the certain body part.
h
Penetration Loss. Given the signed-distance field of the human pose sdf ,
h1
we employ a penetration loss to penalize the body-object interpenetration,
(cid:88)
E =− min(sdf (v [d ]),0). (12)
pene h1 o1 o
do
Themetricfordeterminingthefinalposeselectionisgivenbytheexpression
1(E =0)/E . We sample one pose from the set of poses generated by all
pene cont
contact maps, based on the metric.
Learning-based Interaction Retrieval. Our interaction retrieval can also
be achieved by integrating knowledge from several learning-based algorithms.
Although being more complicated, the retrieval can be done without handcraft
rules. Our pipeline can be divided into followings. (i) Given the text prompt t
and the initial human pose a , we synthesize corresponding images via Stable
1
Diffusion[81].(ii)Wefollow[28]filteroutimageswithlowqualityininteraction
(iii)Anoff-the-shelfmodelLEMON[120]isemployedtoobtainobjectaffordance
and human contact, given the generated image paired with human pose a and
1
objecttemplate.Theoutput{(li)M ,(li)N }indicatesthecontactvertindexes
h i=1 o i=1
of human and object respectively, and the output T indicates the estimated
1
object translation, which is used for initialization in the optimization. (iv) To
acquire the object pose, we utilize the optimization to minimize the Chamfer
distance between the human vertices and the object vertices, indicated by the
contact vertices obtained in the last step.
(cid:88)
E = min∥v [lk]−v [lj]∥ . (13)
fit
k
o1 o h1 h 2
j24 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
B.2 World Model
In Sec. 3.3 of the main paper, we introduce the input representation for the
dynamics model. The architecture of the world is also introduced in detail. In
this section, we introduce detail with particular emphasis on scenarios in which
the initial state s is presented.
1
In the particular instance where the time step t = 1, the state vector s
1
encapsulates a single frame. Consequently, we employ two distinct models for
dynamics prediction. For predictions originating from the initial state, the his-
tory motion encompasses a single time step H = 1. In contrast, for predictions
for subsequent states, the historical interval covering m time steps, where m
denoting the frame count per segment.
C Additional Details of Experimental Setup
Fig.A: We use Amazon Mechanical Turk [96] to build an annotation platform. We
provide instructions to guide the annotator to split a long sequence into several short
sub-sequences with their start and end frames, and then annotate each sub-sequence.
Weinformannotatorsthatourcollecteddataareusedfortext-motiongenerationwhen
they accept this work.
Datasets. We have included a screenshot of our annotation platform in Fig. A.
Upon acceptance of our work, we plan to update the Acknowledgements section
with the identities of the annotators. Our annotations are further diversified by
the GPT-4 [67]. The prompt used for this purpose is: I’m going to give you
an essay, and I want you to give me three sentences in English of
varying degrees of complexity, like the following example: “A
person lifts something with both hands, dumps it to the right, and
then puts it back down.” A person reaches to the left for
something, then reaches to the right as if to dump it, and thenInterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction 25
puts it back to the left. A person picks up something with both
hands, tilts it, and then puts it down. To make the text more
complex, you can add more detailed adjectives and adverbs and
complicate sentence structure and verbs. The input text is “A man
leaned his upper right thigh against a small table and stretched
his left hand and left foot outward.” Please give me three texts
that vary in complexity but keep the meaning of the sentence the
same.
Metrics. In Sec. 4.1, we introduce the metrics employed in this paper. This
section details the formula for the metric CMD proposed. The formulations for
othermetricsareavailableintheexistingliterature[25,116].CMDquantifiesthe
discrepancy between the contact maps of ground truth interactions and those
synthesizedone.Inthiscontext,acontactmapischaracterizedbytheproportion
of time each body part {p }P maintains active contact. Here, p denotes the
i i=1 i
percentageoftimeduringwhichthebodypartiislessthanathresholddistance
from the object. And the metric is defined as,
P
1 (cid:88)
CMD = ∥p −pGT∥ , (14)
P i i 1
i=1
where pGT is from the ground truth contact map, P is the number of the body
i
parts defined in the SMPL [58], and we define the distance threshold as 0.03 m.
Implementation Details. The segment in the MDP contains m = 4 frames.
Thedynamicsmodel,whichincludes2dynamicsblocksasdescribedinthemain
paper,istrainedontheBEHAVEtrainingset[7],withabatchsizeof32,alatent
dimension of 64, and for 500 epochs. For rollout after the initial step t>1, our
dynamicsmodelistrainedtopredictoveralongertimeframe(F =3×m=12),
exceeding the past motion duration (H =m=4). For the initial step t=1, we
trainaseparatedynamicsmodeltoforecastadurationofF =15giventhepast
motion over H =1 frame, consistent with Sec. B.2. The optimization process is
conducted over 300 epochs, utilizing a learning rate of 0.01.
D Additional Qualtitative Results
User Study.Weconductadouble-blinduserstudy.Giventwoannotations,we
useourdynamicsmodelwithInterDiff[116]andwithourproposedvertexcontrol
to generate 8 samples respectively. We design pairwise evaluations. Considering
one pair from ours and another one from InterDiff, human judges are asked to
determine which interaction has the better interaction quality. From the results
of 11 human evaluations in Table A, our approach has a success rate of 61.4%
against the baseline.
Interaction Retrieval. In addition to the results in the main paper and the
demo video, we here visualize the intermediate retrieval results. Fig. B depicts
both the handcraft retrieval and learning-based retrieval procedure, resulting in
adiversesetofinteractionsthatarebothhigh-qualityandsemanticallyaligned.26 S. Xu, Z. Wang, Y.-X. Wang, L.-Y. Gui
1. yogamat; 2. shoulder; 1. yogaball; 2. right hand;
3. a person is carrying an object on the shoulder. 3. a person throws an object with right hand towards the ground.
Text-to-Motion Interaction Retrieval Text-to-Motion Interaction Retrieval
Fig.B: Qualitative resultsfromtheinteractionretrieval.Wedemonstratethatour
proposed handcraft interaction retrieval (unified color person) and learning-based in-
teraction retrieval (textured person, where textures are from [12]) can extract diverse
and realistic interactions.
TableA:Userstudyonthezero-shottext-to-interactiontask.Underpairwisehuman
voting results, our generated results significantly outperform the baseline considering
the interaction fidelity.
Method w/InterDiff[116]vs.w/vertexcontrol(ours)
w/InterDiff[116]vs. N/A 38.6%
w/vertexcontrol(ours) 61.4% N/A
E Potential Negative Societal Impact
Some potential negative societal impacts include: (i) Our approach can be used
tosynthesizerealistichumanmotioninteractingwithobjects,whichcouldpoten-
tiallyleadtothecreationofmisinformation.(ii)Ourapproachevaluatesonreal
behavioral information, which may raise privacy concerns. However, our model
utilizes a processed representation (SMPL [58]) of the human motion that re-
tains minimal identifying details, in contrast to raw data or images. This aspect
can be positively regarded as a feature that enhances privacy.