Is Continual Learning Ready for Real-world Challenges?
TheodoraKontogianni1 YuanwenYue1 SiyuTang1 KonradSchindler1
Abstract tion (Aljundi et al., 2018; Kirkpatrick et al., 2017; Li &
Despite continual learning’s long and well- Hoiem, 2017; Chaudhry et al., 2019; Lopez-Paz & Ran-
established academic history its application in zato,2017),2Dsemanticsegmentation(Michieli&Zanut-
real-worldscenariosremainsratherlimited. This tigh,2019;Cermellietal.,2020;Maracanietal.,2021),and
paper contends that this gap is attributable to a recently, 3D semantic segmentation (Camuffo & Milani,
misalignment between the actual challenges of 2023; Yang et al., 2023), its practical application in real-
continuallearningandtheevaluationprotocolsin world scenarios remains limited (Gonzalez et al., 2020).
use,renderingproposedsolutionsineffectivefor This paper contends that this gap is attributable to a mis-
addressingthecomplexitiesofreal-worldsetups. alignmentbetweentheactualchallengesofcontinuallearn-
We validate our hypothesis and assess progress ing and the evaluation protocols in use, rendering pro-
to date, using a new 3D semantic segmentation posedsolutionsineffectiveforaddressingthecomplexities
benchmark, OCL-3DSS. We investigate various ofreal-worldsetups.
continual learning schemes from the literature
This paper underscores the importance of recognizing the
by utilizing more realistic protocols that neces-
limitations of current evaluation protocols and advocates
sitateonlineandcontinuallearningfordynamic,
forexploringCLmethodsthroughnewexperimentalproto-
real-worldscenarios(e.g.,inroboticsand3Dvi-
colsthatmirrorreal-worldsetups. Theresultsaresurpris-
sion applications). The outcomes are sobering: ing: All examined methods exhibit significant failure with
all considered methods perform poorly, signifi- near-zero intersection-over-union(IoU) even in medium-
cantly deviating from the upper bound of joint sizedtasksequences(20tasks)andfallconsiderablyshort
offline training. This raises questions about the oftheupperboundachievablebyjointofflinetraining.
applicabilityofexistingmethodsinrealisticset-
tings.Ourpaperaimstoinitiateaparadigmshift, While CL has traditionally focused on offline image clas-
advocating for the adoption of continual learn- sification (Aljundi et al., 2018; Kirkpatrick et al., 2017;
ingmethodsthroughnewexperimentalprotocols Li & Hoiem, 2017; Chaudhry et al., 2019; Lopez-Paz &
thatbetteremulatereal-worldconditionstofacil- Ranzato, 2017), the demand for foundation models in au-
itatebreakthroughsinthefield. tonomousagentsandrobotics,capableofdynamicadapta-
tioninreal-time3Denvironmentsunderscores3Dseman-
ticsegmentationasamoresuitabletaskforbenchmarking
1.Introduction continuallearningmethods.
Continual learning(CL)(Ring et al., 1994) is one of the Regardlessofthetask,CLmethodsadheretosimilareval-
fundamental machine learning fields. Its methodology uationscenarios.Thesescenariostypicallyinvolve(i)mul-
emerged as a solution for incorporating new knowledge tipletrainingepochsoverthecurrenttask,allowingforthe
in existing models to adapt to new data distribution such repetitiveshufflingofdatatofacilitatetasklearningbutnot
as new labels and tasks (Ring et al., 1994). Furthermore, well-suited for streaming data. Additionally, (ii) reliance
in scenarios where re-training a model from scratch with on large pre-trained models, while introducing only a few
the entire dataset is impractical or resource-intensive (e.g. additional tasks in a CL scenario, often leading to trivial
foundation models), incremental updates on new data can solutions. Lastly, (iii)apreferenceforadisjoint scenario,
maketheprocessmoreefficient. whereinputdata(pixelsor3Dpoints)onlyfromprevious
andcurrentclassesareincluded,anunrealisticassumption,
Despitecontinuallearning’swell-establishedacademichis-
particularlyinthecontextofautonomousagents.
tory, spanningdecadesandtaskssuchasimageclassifica-
To address some of these limitations, there has been re-
1ETH Zurich, Switzerland. Correspondence to: Theodora cently a growing interest in Online Continual Learning
Kontogianni<kontogianni.theodora@gmail.com>.
(OCL) (Ghunaim et al., 2023) that studies the problem of
learning from an online stream of data and tasks. Such a
1
4202
beF
51
]GL.sc[
1v03101.2042:viXraRethinkingContinualLearningforReal-worldImpact
setup is well-suited for dynamic environments where the We observe that all methods exhibit comparable results
learning process is ongoing such as in robotics but OCL when the number of tasks is small (≤ 5), the common
methodsprimarilyfocusonimageclassification,asiscom- setupinexistingbenchmarks. However,theirperformance
mon in most continual learning studies, without delving sharplydeclinestoalmostzeroafterafewmoretaskseven
intoothertaskslikesemanticsegmentation. for the recently popular methods with language guidance
and transformer architecture. Notably, the replay-based
In a similar spirit, our setup features a single online data
method ER (Chaudhry et al., 2019) stands out as the sole
stream.Ateachtimestep,asmallbatchofdataarrives,pro-
approach to maintain competitiveness in an OCL-3DSS
viding the model with an opportunity for learning before
scenario. However, with a mIoU of 42.1% (20 tasks) and
transitioningtothenextincomingsetofdata(single-pass).
utilizing 33% of the dataset in memory for replay, it still
Trainingunfoldsoveraprolongedsequenceoftasks(long
lags significantly behind the 66.4% achieved on ScanNet
tasksequence),whereallclassesarelearnedincrementally,
byjointtraining.
starting immediately from the first task without any batch
pre-training. Starting with a randomly initialized model, Given the streaming nature of the task, we noticed that
rather than one pre-trained on a set of tasks, may initially many classes are difficult to learn in an online continual
seemlessrealistic. However,itallowsforthesimulationof learningsetupwithoutconstantrepetitionoftheclasssam-
a more extended sequence of tasks in practice. Addition- ples as in joint training. Positive backward and positive
ally, in contrast to previous methods emphasizing the dis- forward transfer has not been witnessed with the excep-
jointscenario,weprioritizetheoverlappedscenario. Here, tion of positive backward transfer for the replay methods
datapointsfromfutureclassesarealsoaccessible,although as it is expected when replaying previous samples. While
withouttheircorrespondinggroundtruth. replay-basedmethodsmayprovideapracticalsolution,we
contend that they do not fundamentally address the core
To study online continual learning in 3D semantic seg-
aspects of continual learning and adaptation to new expe-
mentation, we modified various well-known 3D seman-
riences. Detailedexperimentssupportingourassertionare
tic segmentation datasets,ScanNet(Dai et al., 2017a),
presentedinthesubsequentsectionsofthepaper.
S3DIS(Armeni et al., 2016) and SemanticKITTI(Behley
etal.,2019)tosuitouronlinecontinuallearningframework Weencouragefutureworkstoadoptourexperimentalpro-
and used them to evaluate the performance of numerous tocolforassessing2Dand3Dsemanticsegmentationina
widely-used continual learning approaches. Furthermore, continuallearningframework. Despitethechallengingna-
inspiredbyrecentapproachesforcontinuallearninginim- ture of our real-life setup, where most continual learning
ages we evaluated the use of a (i)vision-language model methodsproduceunfavorableresults,webelievethisisthe
anda(ii)transformerdecoder. waytowardsignificantadvancementsinthefield.
CLsuffersfromthepervasiveissueknownascatastrophic
forgetting (French, 1999; McCloskey & Cohen, 1989). 2.RelatedWork
Thisreferstotheneuralnetworks’tendencytoabruptlyand 2.1.OfflineContinualLearning
entirely erase previously acquired knowledge when learn-
ingnewinformation. Onlinecontinuallearning,whichas- The simplest and most common methods in continual
sumes single-pass data, is strictly harder than offline, due learning have been focusing on two main approaches:
to the combined challenges of catastrophic forgetting and replay-basedandregularization-basedmethods.
underfittingwithinasingletrainingepoch. Replay-based methods store input samples or represen-
tations of the current task while the model learns from
Due to continual learning’s long history, there are sev-
them and later replays them while the model learns new
eral works on continual learning from where we select
tasks(Lopez-Paz&Ranzato,2017;Maracanietal.,2021;
a combination of seminal and state-of-the-art methods
Chaudhry et al., 2019). Replay-based methods overcome
to fairly compare in our OCL-3DSS setup:MAS(Aljundi
catastrophic forgetting by simply re-training on the stored
et al., 2018) as a regularization-based approach, LwF(Li
dataoftasksalreadyexperiencedsofar. Someworksstore
&Hoiem,2017)asadistillation-basedandER(Chaudhry
past experiences based on knowledge distillation (Hinton
etal.,2019)asareplay-basedapproach. Furthermore, we
et al., 2015) especially when privacy is a concern but
assessedthemethodproposedbyYangetal.(2023),which
the simple storing of a set of raw samples from previous
currentlystandsasthesoleexistingapproachincontinual
tasks(Chaudhryetal.,2019;Lopez-Paz&Ranzato,2017)
learning for 3D semantic segmentation. The method pri-
worksremarkablywell. Replay-basedmethodsareconsis-
marily relies on the utilization of confident pseudolabels,
tentlythebest-performingonesincontinuallearning.How-
asiscommoninmanyotherCLmethods. Weadditionally
ever,theyareboundbythesizeofthereplaybufferandthe
incorporated and evaluated language guidance and differ-
re-trainingtimewitheveryadditionaltask. Itcouldbear-
entbackbonearchitectures.
guedthattheseapproachesdon’tfundamentallytacklecon-
2RethinkingContinualLearningforReal-worldImpact
tinuallearningbutratherrepresentanalternativemanifes- Segmentationsharethesamefoundationalassumptionsas
tationofjointtraining. traditional offline CL methods. These involve initiating
Regularization Methods. Several previous benchmarks with a pre-trained model on a substantial subset of tasks
oncontinuallearningimposerestrictionsbyprohibitingac- andgraduallyincorporatingafewnewtasks. Trainingoc-
cess to previously seen data. The rationale behind such curs over multiple epochs for each task, allowing the re-
constraints is often attributed to concerns related to stor- peated shuffling of data. The assumption is that the data
agespaceandprivacy. Themajorityofthesemethodscon- distribution for each task remains stationary and is drawn
centrate on addressing the issue of catastrophic forgetting fromani.i.d. distribution. Incontrast,ourapproachintro-
through regularization, imposing penalties during training duces CL in 3D Semantic Segmentation by (i) adopting a
basedonfactorssuchastheimportanceofweightsforspe- fullyincrementalonlinesettingand(ii)deviatingfromthe
cific tasks (Aljundi et al., 2018; Kirkpatrick et al., 2017). i.i.d. assumption,makingitmoresuitablefortasksinvolv-
Theobjectiveistoidentifycrucialparametersforeachtask, ingautonomousagents.
restrictingtheiralterationinsubsequenttasksaccordingto
theirperformance. Alternatively,someapproachesinvolve
3.ProblemFormulation
knowledge distillation from a prior model (Li & Hoiem,
2017). Notably,thesemethodsoperatewithouttheneedto Before we introduce our Online Continual Learning for
storepreviouslylabeledexamples. Whiletheyexhibitsat- 3DSemanticSegmentation(OCL-3DSS)setup(Sec.3.3),
isfactoryperformanceinsmalldatasetsandstraightforward wepresentthesetupofsemanticsegmentationin3Dpoint
classificationtasks,theirefficacydiminishesinchallenging clouds(Sec.3.1)andofflinecontinuallearning(Sec.3.2).
andcomplexscenarios(Ghunaimetal.,2023),ascorrobo-
ratedbyourexperiments. 3.1.Standard3DSemanticSegmentation
KeyIssues. ThemajorityofCLevaluationsareconducted
in small-scale datasets on classification tasks. Further-
Let x ∈ RN×Cin be a 3D point cloud consisting of N
pointsandC inputchannels(typicallyC =3forXYZ
more, traditional continual learning methods start with a in in
coordinates or C = 6 for XYZ and RGB if color infor-
pre-trained model on a large subset of the tasks and in- in
mation is available). The segmentation map, denoted as
crementally introduce a handful of new tasks (Fig. 7, Ap-
y ∈ RN×|C|, represents semantic classes within the point
pendix). They allow the model to be trained over multi-
cloud,whereC isthesetofsemanticclasses.
pleepochsoneachtaskwithrepeatedshufflingofdataand
wherethedatadistributionofeachtaskisstationary. Given a training set, the objective of semantic segmenta-
tionistolearnamodelf (x):X →Y thatmapstheinput
θ
2.2.OnlineContinualLearning spaceX toasetofclassprobabilityvectorsX →RN×|C|.
The final prediction for each 3D point is determined by
To alleviate these limitations, recently OCL methods
y = argmaxf (x)[i,c]N ,wheref (x)[i,c]represents
started focusing on the cases where data arrive one tiny pred θ i=1 θ
c∈C
batchatatimeandrequirethemodeltolearnfromasingle theprobabilityof3Dpointibelongingtoclassc.
passofdata(Ghunaimetal.,2023).
KeyIssues. Imageclassificationremainsthesolefocusof 3.2.OfflineContinualLearning
all these methods as in most CL studies. In contrast, we
In contrast to conventional training approaches, continual
introduceonlinelearningfor3Dsemanticsegmentation,a
learning (Ring et al., 1994) involves training a machine
morepragmaticsetupforautonomousagenttasks.
learning model on a sequential stream of data from var-
ious tasks. Instead of learning from the entire dataset
2.3.ContinualLearninginSemanticSegmentation
T ⊂ X ×Y atonce,learningoccursinmultiplestepsde-
Recently,increasedattentionhasbeendevotedtocontinual noted as t = 1,...,T, where only a subset of the classes
learningforsemanticsegmentationin2Dimages(Michieli C ⊂ C isavailableateachstept. Thisnewsetofclasses
t
& Zanuttigh, 2019; Cermelli et al., 2020). The prob- C ∩C = ∅ is used as the ground truth for updating
t 1...t−1
lem was first introduced in (Michieli & Zanuttigh, 2019). the model parameters. The model is expected to perform
Reguralization-based PLOP (Cermelli et al., 2020) alle- optimallynotonlyonthecurrenttasktbutalsotopredict
viates forgetting of old knowledge by distilling multi- andperformwellonallclasseslearneduptothatpoint,
scale features. Replay-based RECALL (Maracani et al.,
Akeychallengeincontinuallearningistheoccurrenceof
2021) obtains more additional data via GAN or web. Re-
catastrophic forgetting (French, 1999; McCloskey & Co-
cently (Camuffo & Milani, 2023) has presented a first at-
hen, 1989). This phenomenon arises when a network,
tempt at using CL for semantic segmentation on LiDAR
trained with data from a new distribution corresponding
pointcloudsinoutdoorscenes.
to task t, tends to forget previously acquired knowledge,
Key Issues. All approaches addressing CL in Semantic
3RethinkingContinualLearningforReal-worldImpact
resulting in a substantial decline in performance for tasks C =1inScanNet,apre-trainedmodelwithjointtrain-
novel
1,...,t − 1. From a broader CL perspective, this chal- ing for C = 19 classes is updatedwithasingleclass.
base
lenge reflects the stability-plasticity dilemma (Grossberg ClassesarealsosplitintoC andC basedonlyonthe
base novel
&Grossberg,1982),wherestabilityentailspreservingpast original class label order of the dataset (S ) or alphabeti-
0
knowledge, whileplasticityinvolvestheabilitytoquickly cally (S ), without taking into consideration the difficulty
1
adapttonewinformation. ofeachclass. Furthermore,3Dsemanticsegmentationhas
witnessed tremendous improvement based on sparse con-
3.3.TowardsARealisticEvaluation volutionalarchitectures(Choyetal.,2019)andtransform-
ers (Schult et al., 2023), which have enhanced the joint
Continual Semantic Segmentation (CSS) aims to train a
training upper bound even further. Notably, these archi-
model f over T steps, where at each step t, the model
θ tectureshavenotbeenconsideredinexistingprotocols.
encounters a dataset Dt = {xt,y˜t}. Here, xt represents
theinputimageorpointcloud, andy˜t denotestheground We understand that the existing protocols in continual
truth segmentation at time t ∈ [1,...,T]. The segmen- learning for 3DSS reflect existing 2D CSS benchmarks
tation map at each step includes only the current classes however we propose a more realistic setup that can re-
C ,withlabelsfrompreviousstepsC andfuturesteps flect the progress in continual learning more accurately.
t 1:t−1
C collapsedintoabackgroundclassorignored. Inthe Existing 3D semantic segmentation datasets can be mod-
t+1:T
commondisjointsetupduringthecurrenttaskt,inputpix- ifiedtoourOCL-3DSSprotocol. Tothatendweleverage
elsorpointsicorrespondingtofuturetasksxtwithy˜t+1:T three popular public 3D semantic segmentation datasets:
i i
areexcluded. Nevertheless,themodelatsteptisexpected (i) ScanNet (Dai et al., 2017b), (ii) S3DIS (Armeni et al.,
topredictallclassesencounteredovertime,denotedasC . 2016)and(iii)SemanticKITTI(Behleyetal.,2019))toval-
1:t
Thesemanticsegmentationtaskattimetisformulatedas: idateourevaluationsetup.
θ =argmin E L(f (xt),y˜t)
t (xt,y˜t) θ
θt 4.2.Datasets
OnlineContinual3DSemanticSegmentation. Toestab-
We modify the following datasets for online continual 3D
lishafoundationforsubsequentdiscussions,weintroduce
semanticsegmentation:
astraightforwardcontinuallearningprocessforsequential
ScanNet(Daietal.,2017b)isoneofthelargestreal-world
fine-tuningonastreamof3Dsemanticsegmentationtasks
3D datasets on the task of 3D semantic segmentation. It
(1,2,...). Startingwitharandomlyinitializedmodelf ,
θ0 contains 1210 training scenes and 312 scenes for valida-
weiterativelyadapttomodelsf byincorporatingy˜t into
θt tion with 20 semantic labels. For training and validation,
thecurrentmodelf .
θt−1 we follow the standard split for 3D semantic segmenta-
In contrast to prior works, we define our continual learn- tion. We split the training set into 20 tasks, each one of
ing scenario in a more challenging setup, introducing key them representing the learning of one label. Therefore,
differencesfromprevioussetupsincontinualsemanticseg- eventhougheach3Dsceneremainsunchangedweuseonly
mentation(CSS):(i)Themodel,f ,isinitializedwithran- thegroundtruthlabelsofthecurrentclass. Wewouldlike
θ0
domweights,implyingtheabsenceofoldtasks,onlyprevi- to process each scene a single time in a true online setup
ousones. (ii)InDt ={xt,y˜t},wheret∈{{1},...,{T}} howeversinceScanNetisahighlyimbalanceddataset(see
and t is a singleton set containing the labels of a single Fig. 8 in Appendix) some object classes are present only
class. (iii)Thenumberoftasks, T, isnotablylonger. (iv) in a few scenes. To alleviate the online learning problem
EachdatasetDtisprocessedonlyonce. anddisentanglethetaskdifficultyfromthedataavailability
we re-use scenes of the underrepresented classes by sam-
4.ProposedBenchmark
plingwithreplacementuntilwehavethemaximumof1201
4.1.ACloserLookAtExistingProtocols scenesforeachclass. However, pleasenotethatthisdoes
noteventhenumberofpointslabeledforeachtask(forex-
Inthissection,wereflectontheexistingevaluationproto-
ampleclasseslikewallremainoneofthemostrepresented
cols (Yang et al., 2023) and observe that they operate un-
onesmakingthelearningstilleasier). Afterlearningeach
der the common disjoint setting of 2D class incremental
classt,weevaluateallclassesuptoandincludingtonthe
segmentation,wheretheincrementaltrainingincludesonly
test set. When we reach class 20, the evaluation is identi-
theoldandcurrentclassesofthepointcloud,excludingthe
cal to that in joint training. This protocol stands in direct
futureclasses. Wefindthatexistingsetupsadoptthetwo-
contrast to existing setups, such as 15 − 5, 17 − 3, and
setparadigm,oneforC andoneforC classes. C
base novel novel 19−1 (Yang et al., 2023). Our setup can be denoted as
containsonly5,3,and1newclassesforbothScanNet(Dai
1−1−1···1 = 1×20. Notably,wenotonlyincremen-
etal.,2017b)andS3DIS(Armenietal.,2016).
tally add classes, but this approach also results in a much
To put these numbers into perspective, in the case of longersequenceoftasks.
4RethinkingContinualLearningforReal-worldImpact
S3DIS (Armeni et al., 2016) contains point clouds of 272 150
roomsin6indoorareaswith13semanticclasses. Weuse 100 shower curtainwindow refrigerator 35
sink toilet counter sofa 30
t o eh xne cli ec n ph e ta c cll o le unn tt tg i eni rn u .g alA ler ae ra n5 ina gs st ee ts ut pa an sd at bh oe vr ee .s Wta es utr sa ein ali ln cg lain ssa en s 55 00 0 pi tc at bu lr ee c f bla o ab o ti r hn te ut
b
d dw o ea o sl krl boc ou kc rt sh a ha i enir
lf
bedotherfur. 122 505
100 10
SemanticKITTI(Behleyetal.,2019)isanoutdoordataset 5
150
without RGB information (compared to ScanNet and 100 50 0 50 100 150 0
FT MAS LwF ER-200
S3DIS).Itisaverybigdatasetsotoalignwiththeonline Figure1.T-SNEofCLIPfea-
setup we selected sequence 0 for training and sequence 8 turesonScanNet(Daietal., Figure2.Performance vari-
fortesting. Weuse9classespresentinthosescenesforour 2017a) classes, evenly dis- ance on the task ordering in
onlinecontinuallearningsetup: car,road,sidewalk,build- tributedin2Dspace. ScanNet.
ing,fence,vegetation,trunk,terrainandpole. thefeaturecorrespondingtotheoutputtoken,representing
Et ∈ RC, the language representation of class t with
lang
4.3.TaskDifficultyScenarios embeddingdimensionC.
Since the order of arrival of each task could influence the WeaimtoconstrainthefeatureencodingsEt ofclasstin
l
performance we organize three different learning scenar- the model at layer l to be close to the language represen-
ios: 1.standard, 2.difficult→easy, 3.easy→difficult. The tation of the same class. We introduce language guidance
standard refers to the existing order from the dataset cre- throughthisfeatureusingcosinesimilarity:
ators,difficult→easyintroducestasksfromtheraresttothe
most common class based on the number of 3D points of Et ·Et
eachclassandeasy→difficultusestheinverseorder. S(E lt ang,E lt)= ∥Etlan ∥g ·∥El
t∥
(1)
lang l
4.4.DecoderArchitectures WedefineS(Et ,Et′)similarly. Weoptimizethecosine
lang l
We explore two decoder architectures. One is a standard losstoincentivizethemodeltoaligntheper3Dpointfea-
convolutional head that predicts semantic logits. In addi- tures of the network close to the language representation
tion, we adapt the recent Transformer decoders (Strudel oftheirrespectiveclassE lt ang. Simultaneously,wemitigate
et al., 2021; Cheng et al., 2022) for image semantic seg- theforgettingofprevioustasksbykeepingthefeaturescor-
mentation to our online continual 3D semantic segmenta- respondingtoallotherclassest′ farawayfromE lt ang. This
tion. We show our findings are independent from specific leveragesthefactthatthetextfeaturesarealreadysepara-
decoderarchitectures. ble(Fig.1),thusalleviatingforgetting.
Given the task t, the model only has access to the current
5.ClassSupervisionUsingLanguageModels tasklabelsandnoaccesstolabelsfromothertasks. There-
fore, we optimize the following triplet loss based on the
Recently, prompt tuning has surfaced as an alternative to
cosinesimilaritydescribedabove:
rehearsal buffers in continual learning for 2D semantic
segmentation. Approaches in this domain (Wang et al., L(A,P,N)=−S(A,P)+S(A,N)+margin (2)
2022b;a; Khan et al., 2023) leverage a pre-trained vision
Thetotaloptimizationisdefinedas:
encoder and employ prompt learning to facilitate the con-
M
tinuous learning of tasks, eschewing the need to replay (cid:88)
Jt = L(At,Pi,Ni) (3)
samplesfromprecedingtasks.
i=1
Integratinglanguageintocontinuallearningfor3Dseman-
Here,wedefinetheanchorpointAtasthefeatureencoding
ticsegmentationpresentschallengesduetothescarcityof
Et ,P = Et asthefeaturevectorofpointsbelongingto
images typically found in such datasets. In the absence lang l
class t, and N = Et′ as the feature vector of points not
of readily available images, our reliance is solely on text l
belonging to class t. We sample an equal number M of
promptsforguidanceandinstruction.
pointsiforboththenegativeandpositivepoints.
For a given training sample (x,y ) consisting of a point
t
Duringinference,weassignalabeltoeachpointbasedon
cloudxwithpointlabelsy ,weexpresstheclassnamefor
t
themaximumsimilaritytothelanguagefeatures. Thisway
classtinlanguageasthefollowingprompt:
we require no adaptive decoder and we can add as many
classesasneeded.
“Aphotoofclassname”
Thepromptisinputtoapre-trainedtextencodertoextract
5
UoImRethinkingContinualLearningforReal-worldImpact
70 70
60 60 80
50 50 70
40 40
30 30 60
20 20 50
10 10
0 1 5 10 15 20 0 1 5 10 15 20 40
LR=103 T La Rs =k 104 LR=105 MAS-1 T Ma As Sk -20 MAS-200 30
MAS-10 MAS-100 20
Figure3.Learning rate in FT Figure4.MASweightdoesnot 10
hasminimalinfluence. significantlyimpactforgetting. 0
0 5 10 15 20
6.Experiments Task
JT MAS SOTA ER-100 ER-400
FT LwF ER-50 ER-200
ExistingApproachesandBaselines. Weconsiderthefol- Figure5.Overall performance of CL methods on our OCL-
lowing state-of-the-art approaches for our empirical stud- 3DSSsetup(ScanNet-v2).AllCLmethodsexceptERconvergeto
ies: MAS(Aljundietal.,2018),LwF(Li&Hoiem,2017) almostzerobythe20 task. Despitethesizablememorybuffer,
th
andER(Chaudhryetal.,2019). Wechoosethemascanon- evenERisstillfarfromJT.ERismoreofapracticalsolution.
ical examples for regularization-based, distillation-based
classesappearonlyinalimitednumberofscenes, wead-
and memory replay approaches, discussed in Section 2.
dressthisbyresamplingscenesuntilthecountalignswith
LwFregularizestheoutputofthenetworkintasktbasedon
themaximumavailablescenesofthemostprevalenttask.
theoutputsofthenetworkoftaskt−1byalwayssavingthe
Backbones. We use the MinkowskiEngine(ME) (Choy
modelparametersoftheprevioustask. MASpenalizesthe
etal.,2019)witha5cmvoxelsizeasabackbonethatwe
change of the important parameters for the previous tasks
train from scratch starting on task 1. We use SGD with
basing their importance on the sensitivity of the predicted
a fixed common learning rate for all tasks. Besides the
outputtothechangeintheparameter. ERasin(Ghunaim
standard ME decoder, we also modify a Transformer de-
et al., 2023) performs one gradient step on a batch, half
coder(Strudeletal.,2021;Chengetal.,2022)toourtask.
ofwhichisrandomlysampledfromthememory. Wealso
For the language guidance LG-CLIP, we use CLIP (Rad-
evaluate Yang et al. (2023) as the only baseline in contin-
fordetal.,2021).
ual learning for 3D semantic segmentation that combines
Supervision. Allmethodsarefullysupervisedwithallthe
pseudo-labels created by the model from previous tasks
3D points with the label of the current class. When lan-
withadistillationloss.
guageguidanceisusedsupervisionrequiresonly10points
Finally, we also compare with a reference model learned per class per scene and we follow that scenario in all our
in a traditional semantic segmentation joint training (JT) tableentriesthatreportCLIP.Thesupervisionoftheother
without continual learning, which may constitute an up- methods is at the level of at least hundreds of points per
per bound. Our lower bound consists of fine-tuning with scene(dependingontheclasspopularity,eventhousands).
new task data (FT) without taking any measures to allevi-
atecatastrophicforgetting.
7.AnalysisandDiscussion
Metrics. WereportmeanIntersectionoverUnion(mIoU)
bothafterthefinalincrementalstepandattheintermediate Thechallenge: DifferencesbetweenJTandFT.Wefirst
stages. Notably, while many methods demonstrate satis- studytheimpactofincrementalfine-tuning,inarelatively
factoryperformanceforuptothreeadditionaltasks, there long sequence of tasks 20, 12, and 9 for ScanNet, S3DIS
is a tendency to completely forget earlier tasks in longer and SemanticKITTI respectively. There is a big discrep-
sequences. Weadditionallyreport: ancybetweentheJointTraining(JT)ofasetoftasksand
Finetuning (FT) in each task sequentially as shown in Ta-
IoU −IoUT
forgetting = t t ×100% bles (4, 5, 6) and Fig. 5. For example, as it can be seen
t IoU
t inTab.4,JTonthe20classesoftheScanNetdatasetwith
whereIoU isthecurrenttasktperformanceatsteptand a batch size of 10 for 25 epochs returns an mIoU of 66.4
t
IoUT is the performance of task t after all the steps. The in the validation set. This can be considered the upper
t
smaller the value, the better. Negative values mean im- bound for the task on such a backbone. To understand
provedlearninginfuturetasks. the scale of the catastrophic forgetting issue the sequen-
Implementation Details. During each training step t, a tialtrainingoftaskswithoutanycountermeasurestomiti-
batchconsistsof103Dscenes. InthecaseofER,5scenes gate the catastrophic forgetting yields 0.4 mIoU (Fig. 5).
arerandomlysampledfrommemory. Thisprocessrepeats Besides catastrophic forgetting, online learning of tasks
until all scenes for each task are utilized. To handle the sequentially encounters an additional challenge: learning
long-tailnatureof3Dsemanticsegmentation,wheresome withlimiteddata(Tab.7).
6
UoIm UoIm
UoImRethinkingContinualLearningforReal-worldImpact
Table1.OCL-3DSSonScanNet-val(Daietal.,2017b)after3,5, Table4.OCL-3DSSonScanNet-val(Daietal.,2017b)after3,5,
10and20(final)tasks.Augmentingthepointcloudsdidnothave 10and20(final)tasks.BestmethodisER-400.Mostmethodsof-
aninfluenceonothermethodsbesidestheFT.Usingaugmenta- fercomparableresultsuptothe5 taskbutonlyreplaymethods
th
tionfortrainingincaseswherethei.i.dassumptiondoesnothold maintainperformanceafter. LG-CLIPisweaklysupervisedwith
(eg. multiple scans of the same scene) might be comparable to just103Dpointsperclass.
MASandLwFforfewtasks(≤5).
Method Mem.Size Mem./Data mIoUafterttasks
slots·|C| (percent) 3 5 10 20
Method mIoUafterttasks
FT(lower bound) - - 24.9 3.9 2.1 0.4
3 5 10 20
MAS(Aljundi et al., 2018) - - 33.7 22.6 10.8 4.9
FT (lower bound) 24.9 3.9 2.1 0.4 LwF(Li & Hoiem, 2017) - - 41.2 34.1 17.4 7.8
FT + Aug. 27.3 17.4 9.1 4.1 ER-50(Chaudhry et al., 2019) 50·20 4.1 29.6 30.3 17.4 17.0
ER-200(Chaudhry et al., 2019) 200·20 16.4 42.2 36.6 26.8 35.7
ER-400(Chaudhry et al., 2019) 400·20 32.8 43.6 36.4 39.0 42.1
Table2.OCL-3DSS on ScanNet-val (Dai et al., 2017b) after 3, Yang et al.(2023) 40.7 24.3 12.1 7.0
5,10and20(final)taskswithdicelossinsteadofcrossentropy. LG-Clip-weak sup. - - 37.5 18.5 10.4 3.2
OnlyER(Chaudhryetal.,2019)improved. Theothermethods JT(upper bound) - - - - - 66.4
hadcomparableresults(seeTab.8inAppendix).
Table5.OCLin3DsemanticsegmentationonS3DIS-A5(Ar-
menietal.,2016)after3,6and12(final)tasks.
Method mIoUafterttasks
3 5 10 20
ER-200(Chaudhry et al., 2019) 42.2 36.6 26.8 35.7 Method Mem.Size Mem./Data mIoUafterttasks
ER-200(Chaudhry et al., 2019)+ DiceLoss 47.1 41.7 43.8 36.1 slots·|C| (percent) 3 6 12
FT(lower bound) - - 25.1 16.3 9.1
Table3.OCL-3DSS on ScanNet-val (Dai et al., 2017b) after 3, MAS(Aljundi et al., 2018) - - 29.5 14.7 8.6
LwF(Li & Hoiem, 2017) - - 30.8 8.3 6.4
5, 10 and 20 (final) tasks with limited use of memory. Just a
ER-20(Chaudhry et al., 2019) 20·12 9.8 33.2 17.4 8.6
singlestoredexamplefromeachclassperformsalmostaswellas ER-50(Chaudhry et al., 2019) 50·12 24.5 33.7 18.3 13.7
ER-100(Chaudhry et al., 2019) 100·12 49.0 34.3 19.4 16.5
50. Withtheadditionallanguageguidance,itcansurpassthe50
LG-Clip-weak sup. - - 34.2 11.6 4.4
examplesinthe5and10classes.Thesizeofthememorymatters
JT(upper bound) - - - - 65.4
actuallyatthe20classes. Sostoringseveralexamplespertaskis
importantforlargetasksequencesbutfewexamplesneedtobe bles 4,5,6 the best performing method is ER (Chaudhry
storedforamoderatenumberoftasks. et al., 2019). It is a simple method where half of the
training batch consists of samples of previous tasks ran-
Method mIoUafterttasks
3 5 10 20 domly selected. In our case, we store the raw labels of
ER-50(Chaudhryetal.,2019) 29.6 30.3 17.4 17.0
the 3D points of previous tasks. As anticipated, the size
ER-2(Chaudhryetal.,2019) 37.0 29.9 18.3 14.1
ER-1(Chaudhryetal.,2019) 31.7 24.8 18.5 8.8 of the memory is a relevant factor, at least up to a certain
ER-1+LG-Clip 30.5 34.1 20.7 8.1
threshold,especiallyasthenumberoftasksincreases. It’s
IsOCLpurelyanoptimizationproblem? Adjustingthe
crucialtohighlightthatdespitestoringnearlyone-thirdof
learningrate(Fig.3)doesnotalleviateforgettingnordoes
our dataset, the performance at the conclusion of 20 tasks
augmentation(Tab.1).Diceloss(Tab.2)alleviatestheclass
reaches, atbest, approximately60%oftheupperlimitfor
imbalanceandfacilitateslearningnewclassesonlyinER.
ScanNet (Dai et al., 2017a) (Tab. 4). Comparable find-
Are regularization methods a solution? We have eval-
ings are observed in S3DIS (Armeni et al., 2016) (Tab. 5)
uated one of the most popular regularization-based meth-
and SemanticKITTI (Behley et al., 2019) (Tab. 6). The
ods MAS (Aljundi et al., 2018). MAS has shown good
second-best approach is LwF (Li & Hoiem, 2017), which
resultsinclassificationtaskswhenusingapre-trainednet-
enforcesregularizationonthenewmodelpredictions,com-
work on ImageNet where new tasks are added (a popular
pelling them to align closely with the predictions of the
setup in offline CL on classification tasks). However, in
modelfromtheprevioustask. Followingcloselyinperfor-
ourchallengingsetupofincrementallearningfromscratch
manceisthemethodproposedbyYangetal.(2023),which
themethodperformspoorly(Tables4,5,6andFig.4). Dif-
derives much of its effectiveness from utilizing pseudola-
ferentweightsfortheMASlosscomparedtothesegmen-
belsgeneratedfromconfidentpredictionsinprevioustasks.
tation loss do not seem to have a significant effect on the
finaloutcome(Fig.4). Largeweightvaluesjustreducethe
Table6.OCL-3DSSonSemanticKITTI(Behleyetal.,2019)af-
plasticity. Contrary to (Aljundi et al., 2018), in our case,
ter3,5and9(final)tasks.
LwF(Li&Hoiem,2017)outperformsMASshowingthat
inourmorechallengingsetup,theexistingtakeawaysfrom Method Mem.Size Mem./Data mIoUafterttasks
slots·|C| (percent) 3 5 9
theCLfieldofimageclassificationdonotnecessarilyhold. FT(lower bound) - - 6.4 4.9 3.4
Itisveryimportanttopointoutthatbothmethodsexperi- MAS(Aljundi et al., 2018) - - 7.4 3.3 7.1
LwF(Li & Hoiem, 2017) - - 41.1 44.5 49.1
ence catastrophic forgetting almost after the 3 task that
rd ER-200(Chaudhry et al., 2019) 200·9 4.4 29.0 33.5 48.5
cannot be mitigated both in ScanNet and S3DIS. In Se- ER-500(Chaudhry et al., 2019) 500·9 11.0 29.0 35.2 52.5
ER-1000(Chaudhry et al., 2019) 1000·9 22.0 31.0 34.4 52.3
manticKITTI(Tab.6)LwFseemstoperformverywell. LG-Clip-weak sup. - - 3.2 8.1 2.2
Is it possible to CL without memory? As shown in Ta- JT(upper bound) - - - - 67.4
7RethinkingContinualLearningforReal-worldImpact
Table7.ForgettingperclassonScanNet-val(Daietal.,2017a). AllmethodsexceptERconvergetoalmostzeroIoU.Methodslike
LwF(Li&Hoiem,2017)andYangetal.(2023)trytoalleviateitbuttheyjustrememberthepopulousclasseslikewallandfloorandfail
tolearnnewones.With-werepresentunderfitting(zeroIoUwhilelearningthatclass).Negativescoresaregood→mIoUisincreasing
withtheprogressionoftasks.Possibleonlywithreplayorpseudolabels.
Forgetting↓
Method
FT 100 100 100 99 99 95 100 98 100 100 100 100 100 100 100 100 100 100 100 0.4
MAS(Aljundietal.,2018) -52 3 72 11 - 52 - - - - - - - - - - - - - 4.9
LwF(Li&Hoiem,2017) -32 3 - - - - - - - - - - - - - - - - 7.8
ER-50(Chaudhryetal.,2019) -58 -43 100 -1467 -116 -282 -18 67 -75 -367 100 -67 -47 -225 100 -200 -533 100 -140 17.0
Yangetal.(2023) -0.3 -4 - - -0.1 - - - - - - - - - - - - - - 7.0
LG-CLIP-weaksup. 30 52 95 79 96 96 -1 92 95 62 5 -884 -142 -115 -9 46 -200 42 -1 3.2
MAS ER-50 tails). Our scenario is much more challenging: we do not
80 80
70 70 use images and the classes for regularization appear sep-
60 60
50 50 arately. But with only 10 points per class for supervision
40 40 language guidance offers comparable results to the other
30 30
20 20 methods(Tables4,5,6)thatarefullysupervised.Itcanalso
10 10 beusedwithaverysmallmemorybuffertoimprovemem-
0 0
1 5 10 15 20 1 5 10 15 20
Task Task oryreplay(Tab.3).
New Classes Old Classes New Classes Old Classes
Can we learn new classes? Our setup presents signif-
Figure6.OldvsnewclassperformancewithMAS(Aljundietal.,
icant challenges. Firstly, it is highly imbalanced, with
2018) and ER (Chaudhry et al., 2019). This provides a clearer
manyclassesappearinginonlyafewscenesandwithfew
understandingoftheplasticityandrigidityofthemethod. MAS
points. Secondly, in our online configuration, the model
sacrifices plasticity (learning of new classes) for stability. But
encountersthedataonlyonce. Toensureafaircomparison
eveninthebestperformingERthelearningofthenewclassesis
limitedinanonlinescenario. and prevent performance bias towards classes with more
instances, we reused scenes containing underrepresented
However, if objects from the previous class are absent in classes. This approach ensures that our model encounters
the scene, the method struggles, resulting in the retention an equal number of scenes for each class, aligning with
of only ubiquitous classes like walls and floors that are the second point of our setup, which aims to simulate a
presentineveryscene(Tab.7). Itisnoteworthytoreiterate challenging but fair online scenario. Mastering new tasks
thatbothmethodsexperienceasignificantperformancede- with such limited data proves to be as challenging as mit-
clineafterthe5thtask,reinforcingourassertionthatonline igatingforgetting, eveninthebest-performingERscenar-
CLandCL,ingeneral,necessitatemuchlongersequences ios(Fig.6,Tab.7).
of tasks for a comprehensive evaluation. SemanticKITTI Task order influence. The overall performance is im-
stands out as an exception, given that the majority of its pactedbytheordertheclassesappear. Therefore,wecon-
classes are present in every scene. While memory replay duct our experiments using three distinct class orderings:
methods appear to be at the forefront of continual learn- theoriginaltaskorderfromthedatasetsetup,easy-to-hard,
ing, with discussions extending to the concept of infinite andhard-to-easy.Taskdifficultyisdeterminedbythenum-
memory (Prabhu et al., 2023), we would like to point out berofpointsofeachclass(morepoints→easiertask).Stan-
that this is not solving the fundamental issues of learning dardisclosetodifficultandweingeneralwardagainstthe
continuouslyandadaptingwithfewdatabutinsteadrefor- easyasoptimistic. WeshowmeanandvarianceinFig.2.
mulatedtheproblemasanotherjointtrainingsetup. Decoder architecture influence. We empirically find the
How much memory do we actually need? As seen Ta- challenge of OCL-3DSS cannot be solved simply with an
bles4,5,6andFig.5thememorysizeisarelevantfactor,at advancedTransformerdecoder. DetailsintheAppendix.
leastuptoacertainthreshold,especiallyasthenumberof
tasksincreases. However,inTab.3weseethatstoringjust
8.Conclusion
asingleexampleforeachclassER-1(20examplesintotal
forScanNet-v2),iscomparabletoER-50(50examplesper
Inthiswork,weexposetheissuesofthecommonpractices
class)inthelowtaskregimeof≤5tasks.
oncontinuallearningmethods. Weprovideinitialattempts
Can large-scale language models help? Inspired by the
towards characterizing the difficulty of continual learning
recent advances in open-world segmentation with the use eveninmoderate-sizedtasksequences(≥ 5)andlearning
of image-language models like CLIP (Peng et al., 2023)
sequentiallytaskswithlimiteddata. Werecommendfuture
we co-embedded our 3D point features with text from the
works to evaluate their proposed continual learning meth-
CLIPfeaturespacetoalleviatecatastrophicforgettingsince
odsonsuchmorechallengingclosertoreal-worldsetups.
CLIP features are already easily separated (Sec. 5 for de-
8
UoIm
↓llaw
UoIm
↓roofl ↓tenibac ↓deb ↓riahc afos ↓elbat rood ↓wodniw ↑flehskoob ↑erutcip ↓retnuoc ↓ksed ↓niatruc ↓egdirf ↓niatruc.hs ↓teliot ↓knis ↓buthtab ↑UoImRethinkingContinualLearningforReal-worldImpact
References Ghunaim, Y., Bibi, A., Alhamoud, K., Alfarra, M.,
Al Kader Hammoud, H. A., Prabhu, A., Torr, P. H., and
Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., and
Ghanem,B. Real-TimeEvaluationinOnlineContinualLearn-
Tuytelaars, T. Memory Aware Synapses: Learning What
ing: A New Hope. In IEEE/CVF Conference on Computer
(Not)ToForget. InEuropeanConferenceonComputerVision
VisionandPatternRecognition(CVPR),2023. 1,3,6,13
(ECCV),2018. 1,2,3,6,7,8,12,13
Gonzalez,C.,Sakas,G.,andMukhopadhyay,A. Whatiswrong
Armeni, I., Sener, O., Zamir, A. R., Jiang, H., Brilakis, I., Fis- withcontinuallearninginmedicalimagesegmentation? arXiv
cher,M.,andSavarese,S.3DSemanticParsingofLarge-Scale preprintarXiv:2010.11008,2020. 1
IndoorSpaces. InIEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR),2016. 2,4,5,7,11,12,13 Grossberg,S.andGrossberg,S. HowDoesaBrainBuildaCog-
nitiveCode? Studiesofmindandbrain: Neuralprinciplesof
Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., learning,perception,development,cognition,andmotorcon-
Stachniss,C.,andGall,J. SemanticKITTI:ADatasetforSe- trol,1982. 4
manticSceneUnderstandingofLidarSequences. InInterna-
tionalConferenceonComputerVision(ICCV),2019. 2,4,5, Hinton,G.,Vinyals,O.,andDean,J. DistillingtheKnowledgein
7,11,13 ANeuralNetwork. arXivpreprintarXiv:1503.02531,2015. 2
Khan, M. G. Z. A., Naeem, M. F., Gool, L. V., Stricker, D.,
Camuffo, E.andMilani, S. ContinualLearningforLiDARSe-
Tombari,F.,andAfzal,M.Z. IntroducingLanguageGuidance
mantic Segmentation: Class-Incremental and Coarse-to-Fine
Strategies on Sparse Data. In IEEE/CVF Conference on inPrompt-basedContinualLearning. InInternationalConfer-
ComputerVisionandPatternRecognition(CVPR)Workshops, enceonComputerVision(ICCV),2023. 5
2023. 1,3
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des-
jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T.,
Cermelli, F., Mancini, M., Bulo, S. R., Ricci, E., and Caputo,
Grabska-Barwinska,A.,etal. OvercomingCatastrophicFor-
B. ModelingtheBackgroundforIncrementalLearninginSe-
getting in Neural Networks. Proceedings of the National
manticSegmentation. InIEEE/CVFConferenceonComputer
AcademyofSciences,2017. 1,3
VisionandPatternRecognition(CVPR),2020. 1,3
Li,Z.andHoiem,D. LearningWithoutForgetting. Transactions
Cermelli, F., Cord, M., and Douillard, A. CoMFormer: Con-
onPatternAnalysisandMachineIntelligence(PAMI),2017.1,
tinual Learning in Semantic and Panoptic Segmentation. In
2,3,6,7,8,11,12,13
IEEE/CVFConferenceonComputerVisionandPatternRecog-
nition(CVPR),2023. 12 Lopez-Paz, D.andRanzato, M. GradientEpisodicMemoryfor
ContinualLearning. AdvancesinNeuralInformationProcess-
Chaudhry,A.,Rohrbach,M.,Elhoseiny,M.,Ajanthan,T.,Doka- ingSystems(NeurIPS),2017. 1,2
nia,P.,Torr,P.,andRanzato,M.ContinualLearningWithTiny
EpisodicMemories. InWorkshoponMulti-TaskandLifelong Loshchilov,I.andHutter,F. DecoupledWeightDecayRegular-
ReinforcementLearning,2019. 1,2,6,7,8,12,13,14 ization. InInternationalConferenceonLearningRepresenta-
tions,2017. 13
Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., andGirdhar,
R. Masked-attention Mask Transformer for Universal Image Maracani,A.,Michieli,U.,Toldo,M.,andZanuttigh,P. Recall:
Segmentation. InIEEE/CVFConferenceonComputerVision Replay-BasedContinualLearninginSemanticSegmentation.
andPatternRecognition(CVPR),2022. 5,6 InInternationalConferenceonComputerVision(ICCV),2021.
1,2,3
Choy, C., Gwak, J., and Savarese, S. 4D Spatio-Temporal
McCloskey, M. and Cohen, N. J. Catastrophic Interference in
ConvNets: Minkowski Convolutional Neural Networks. In
ConnectionistNetworks:TheSequentialLearningProblem.In
IEEE/CVFConferenceonComputerVisionandPatternRecog-
Psychologyoflearningandmotivation.1989. 2,3
nition(CVPR),2019. 4,6,12,13
Michieli,U.andZanuttigh,P. IncrementalLearningTechniques
Dai,A.,Chang,A.X.,Savva,M.,Halber,M.,Funkhouser,T.,and
for Semantic Segmentation. In International Conference on
Nießner,M.ScanNet:Richly-annotated3DReconstructionsof
ComputerVision(ICCV)Workshops,2019. 1,3
IndoorScenes. InIEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR),2017a. 2,5,7,8,11 Peng, S., Genova, K., Jiang, C. M., Tagliasacchi, A., Pollefeys,
M.,andFunkhouser,T. OpenScene: 3DSceneUnderstanding
Dai,A.,Chang,A.X.,Savva,M.,Halber,M.,Funkhouser,T.,and
with Open Vocabularies. In IEEE/CVF Conference on Com-
Nießner,M.Scannet:Richly-Annotated3DReconstructionsof
puterVisionandPatternRecognition(CVPR),2023. 8
IndoorScenes. InIEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR),2017b. 4,7,12,13 Prabhu,A.,Cai,Z.,Dokania,P.,Torr,P.,Koltun,V.,andSener,
O. OnlineContinualLearningWithouttheStorageConstraint.
Deng, R., Shen, C., Liu, S., Wang, H., and Liu, X. Learning arXivpreprintarXiv:2305.09253,2023. 8
to predict crisp boundaries. In Proceedings of the European
conference on computer vision (ECCV), pp. 562–578, 2018. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agar-
11 wal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.
LearningTransferableVisualModelsFromNaturalLanguage
French, R. M. Catastrophic Forgetting in Connectionist Net- Supervision. InInternationalConferenceonMachineLearn-
works. TrendsinCognitiveSciences,1999. 2,3 ing(ICML),2021. 6,13
9RethinkingContinualLearningforReal-worldImpact
Ring,M.B.etal. ContinualLearninginReinforcementEnviron-
ments. 1994. 1,3
Schult,J.,Engelmann,F.,Hermans,A.,Litany,O.,Tang,S.,and
Leibe,B.Mask3Dfor3DSemanticInstanceSegmentation.In
InternationalConferenceonRoboticsandAutomation(ICRA),
2023. 4
Shang,C.,Li,H.,Meng,F.,Wu,Q.,Qiu,H.,andWang,L.Incre-
menter:TransformerforClass-IncrementalSemanticSegmen-
tationWithKnowledgeDistillationFocusingonOldClass. In
IEEE/CVFConferenceonComputerVisionandPatternRecog-
nition(CVPR),2023. 12
Strudel, R., Garcia, R., Laptev, I., and Schmid, C. Segmenter:
Transformerforsemanticsegmentation. InInternationalCon-
ferenceonComputerVision(ICCV),2021. 5,6
Vitter,J.S.Randomsamplingwithareservoir.ACMTransactions
onMathematicalSoftware(TOMS),11(1):37–57,1985. 14
Wang,Z.,Zhang,Z.,Ebrahimi,S.,Sun,R.,Zhang,H.,Lee,C.-Y.,
Ren,X.,Su,G.,Perot,V.,Dy,J.,etal. Dualprompt: Comple-
mentaryPromptingforRehearsal-FreeContinualLearning. In
EuropeanConferenceonComputerVision(ECCV),2022a. 5
Wang, Z., Zhang, Z., Lee, C.-Y., Zhang, H., Sun, R., Ren, X.,
Su,G.,Perot,V.,Dy,J.,andPfister,T. EarningtoPromptfor
Continual Learning. In IEEE/CVF Conference on Computer
VisionandPatternRecognition(CVPR),2022b. 5
Yang,Y.,Hayat,M.,Jin,Z.,Ren,C.,andLei,Y. Geometryand
Uncertainty-Aware3DPointCloudClass-IncrementalSeman-
ticSegmentation. InIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition(CVPR),2023. 1, 2, 4, 6, 7, 8,
11,13
10RethinkingContinualLearningforReal-worldImpact
Task 1 Task 2 Task 3
Training Evaluate Training Evaluate Training Evaluate
…
Floor Floor Chair Floor Chair Desk Floor Chair Desk
Old Tasks New Tasks
Joint Training Training Evaluate
Future Classes Points Do not exist
Desk Door Wall Floor Chair Desk Door Wall Floor
Window Devices Window Devices Chair
Figure7.Anillustrationofourcontinuallearningframework. Oursuggestedprotocol(toprow)vs.theexistingprotocols(bottom
row)(i)learnclassesincrementallyinsteadofjointlytrainingforthemajorityofclasses(ii)evaluatesonlongsequencesoftasksinstead
ofatwotrainingstepapproach(iii)containspointsbelongingtofutureclasses(floorandchairaremissingin2 row,1 col.)
nd st
11 02 00 00 34 50 4000 120
30 100 800 25 3000 80 600 20 15 2000 60 400 10 40
200 5 1000
0 20 0 0 0
Figure8.ScanNet-v2 (Dai et al., 2017a). Number of scenes a Figure9.SemanticKITTI(Behleyetal.,2019). Distributionof
class is present (left) and number of 3D points labeled for each classpresenceacrossscenes(left)andthenumberoflabeled3D
class (right). The number of points determines the easyness of pointsforeachclass(right). OurSemanticKITTIsetupnotably
atask. Themorethepointstheeasierthetask. Therightfigure lacks the typical long tail problem (left) encountered in online
representsthetaskorderingofeasy→difficultscenariodescribed continual learning, as nearly all 9 classes are present in every
inSec.4.3inthemainpaper. scene. When benchmarking OCL, it is crucial to consider not
onlythepointcount(right)butalsothefrequencyofscenesfea-
A.DatasetStatistics turingeachobject(left).
ScanNet-v2 (Dai et al., 2017a) stands out as one of the continual learning, as demonstrated in our evaluation (see
mostextensiveindoor3Ddatasets,characterizedbyalong- Tab.6inthemainpaper). However,italsohighlightsthat
taildistribution(seeFig.8,left). Thisdistribution,reflect- certain datasets, despite their vast size in the number of
ing real-world scenarios, is not only evident in the points scenes, may not be ideal benchmarks for evaluating con-
per class but also in the frequency of scenes featuring an tinuallearningmethods.
object class. Real-world occurrences of certain objects S3DIS (Armeni et al., 2016) occupies a middle ground
arelessfrequent,makingbothlearningandretainingthem between the more diverse datasets of ScanNet and Se-
challenging. Thisdifficultyiscompoundedbythefactthat manticKITTI,asillustratedinFig.10. Ourtrainingseten-
theseobjectsdon’tappearineveryscene,evenunlabeledin compassesatotalof204scenesforAreas1-4,6,withArea
differenttasks,renderingmethodslikepseudolabels(Yang 5 reserved for evaluation, following standard practices in
etal.,2023)orLwF(Li&Hoiem,2017)lesseffective.Due thefield.
toitsalignmentwithreal-worlddistributionandhavingthe
highestnumberofclasses(20),theScanNetdatasetischo- B.LossFunctions
senforallourablationstudies.
SemanticKITTI (Behley et al., 2019), on the other hand, In addition to the conventional cross-entropy loss, we
isalarge-scaleoutdoorLiDARdatasetthatweemployfor assessed our configuration by incorporating the Dice
trainingusing4541scenesfromsequence-0. Asillustrated loss (Deng et al., 2018) (Tab. 8). While the Dice loss
inFig.9, SemanticKITTIdoesn’texhibitalong-tailprob- was originally crafted to enhance the definition of bound-
lem; most labeled outdoor classes are nearly ubiquitous aries, it also proved beneficial in addressing class imbal-
acrossscenes.Thischaracteristicmitigatesissuesrelatedto anceswithinastandardjointtrainingframework.Itdemon-
11
SSD3-LCO
gnitsixE
)sruO(
slocotorP
senecs fo rebmuN
roolf llaw .nruf
rehto
rood riahc elbat wodniw tenibac ksed knis erutcip afos deb niatruc teliot retnuoc egdirf flehskoob buthtab .truc
rewohs
]601[
stniop fo rebmuN
llaw roolf riahc rood tenibac wodniw elbat erutinrufrehto deb afos flehskoob niatruc ksed rotaregirfer erutcip retnuoc buthtab niatruc
rewohs
teliot knis
senecs fo rebmuN
rac daor klawedis gnidliub noitategev elop ecnef knurt niarret
]601[
stniop fo rebmuN
noitategev gnidliub daor klawedis rac niarret ecnef knurt elopRethinkingContinualLearningforReal-worldImpact
200 50
175
150 40
125 30
100
75 20
50 10
25
0 0 Prediction
3D Scene
Figure10.S3DIS (Armeni et al., 2016). Number of scenes a
class is present (left) and number of 3D points labeled for each
class(right).
3D Scene Task1 Floor
Table8.OCL-3DSSonScanNet-val(Daietal.,2017b)after3,5, Task2 Chair Prediction
10and20(final)tasksincorporatinganadditionaldicelossalong- Task 3 Desk
Class Tokens
sidecross-entropy. Notably,onlyER(Chaudhryetal.,2019)ex-
hibitedimprovement.
Figure11. Decoderarchitectures. Weselectedtwomodernde-
coderarchitecturesforourstudy:(i)aconventionalconvolutional
Method mIoUafterttasks
3 5 10 20 decode (top) and a Transformer decoder that has demonstrated
FT(lower bound) 24.9 3.9 2.1 0.4
FT + DiceLoss 23.0 1.6 0.7 0.9 state-of-the-artresultsinthesamefield(bottom).
MAS(Aljundi et al., 2018) 33.7 22.6 10.8 4.9
MAS(Aljundi et al., 2018) + DiceLoss 34.4 19.2 8.9 4.4
LwF(Li & Hoiem, 2017) 41.2 34.1 17.4 7.8 Notably,recentefforts(Cermellietal.,2023;Shangetal.,
LwF(Li & Hoiem, 2017) + DiceLoss 31.5 25.8 16.3 7.1
2023) have explored leveraging the attention mechanism
ER-200(Chaudhry et al., 2019) 42.2 36.6 26.8 35.7
ER-200(Chaudhry et al., 2019)+ DiceLoss 47.1 41.7 43.8 36.1 ofTransformerstoenhancethestability-plasticitytrade-off
LG-Clip-weak sup. 37.5 18.5 10.4 3.2
LG-Clip + DiceLoss 25.2 20.9 8.6 5.8 in continual learning. In the Transformer architecture, we
JT (upper bound) - - - 66.4 learnasetofclasstokensbutwedynamicallyaddthemto
strates a modest improvement in scenarios akin to joint thenetworkonetaskatatime. Thoseclasstokensinteract
learning, such as ER (Chaudhry et al., 2019). Neverthe- withencoderfeaturesinaTransformerdecoder,whichwill
less, it imposes penalties on prevalent, easily identifiable predicttheclasssegmentationmask.
classes,contrastingwithothermethodologieslikeLwF(Li
However,despitethewidespreaduseofTransformerarchi-
& Hoiem, 2017) that rely on them for their performance
tectures,weillustrateinTab.9thatcontinuallearningchal-
metrics.
lenges persist even when employing such powerful archi-
tectures.
C.TheImportanceoftheDecoder
The challenges associated with FT are considerably more
Architecture
pronounced in an architecture that demands a substantial
We selected two modern decoder architectures for our amount of data like the Transformer architecture. MAS
study: (i) a conventional convolutional decoder (Choy alsoencounterssignificantdifficultiesinalleviatingforget-
etal.,2019)frequentlyemployedin3Dsemanticsegmen- ting in the context of such an architecture. On the other
tation, and (ii) a Transformer decoder that has demon- hand, ER exhibits superior performance with the Trans-
strated state-of-the-art results in the same field (Fig. 11). former decoder, however only evident at most up to the
10 task. Surprisingly, ER with a transformer decoder
th
Table9.Online continual learning in 3D semantic segmenta- appears to face more substantial challenges compared to
tionwithTransformerdecoder(TD)vstheConvolutionalde- theconvolutionaldecoderinlatertasks,notablyinthe20 th
coder(CD)onScanNet-val(Daietal.,2017b)after3,5,10and task(lastcolumn).Thisobservationunderscorestheimpor-
20(final)tasks. tanceofevaluatingasufficientnumberoftaskstomakeac-
curate assessments about the effectiveness of different se-
Method Mem.Size Mem./Data mIoUafterttasks
slots·|C| (percent) 3 5 10 20 tups in continual learning. Given that the evaluations for
F FT T+ +T CD D( (l lo ow we er rb bo ou un nd d) ) - - - - 21 4.4 .9 1 3. .6 9 0 2. .4 1 0 0. .2 4 3-5tasksmightnotbeindicativeofperformancetrendsex-
MAS+TD(Aljundietal.,2018) - - 1.4 1.6 0.8 0.4 tendingtothe20 taskandbeyond.
MAS+CD(Aljundietal.,2018) - - 33.7 22.6 10.8 4.9 th
LwF+TD(Li&Hoiem,2017) - - 34.3 19.3 9.3 4.8
LwF+CD(Li&Hoiem,2017) - - 41.2 34.1 17.4 7.8
ER-50+TD(Chaudhryetal.,2019) 50·20 4.1 40.0 35.2 22.9 11.5 D.ImplementationDetails
ER-50+CD(Chaudhryetal.,2019) 50·20 4.1 29.6 30.3 17.4 17.0
ER-200+TD(Chaudhryetal.,2019) 200·20 16.4 50.9 36.6 28.6 20.9
ER-200+CD(Chaudhryetal.,2019) 200·20 16.4 42.2 36.6 26.8 35.7 ModelEncoder. Foralldatasetsandmethods,weemploy
ER-400+TD(Chaudhryetal.,2019) 400·20 32.8 46.2 40.3 33.0 28.1
ER-400+CD(Chaudhryetal.,2019) 400·20 32.8 43.6 36.4 39.0 42.1 aRes16UNet34A(Choyetal.,2019)astheencoder.Thefi-
LG-Clip+TD-weaksup. - - 36.1 20.8 7.7 5.2
LG-Clip+CD-weaksup. - - 37.5 18.5 10.4 3.2 nalfeaturelayerhasbeenadjustedtogeneratefeatureswith
12
senecs
fo
rebmuN
gniliec roolf llaw rood riahc elbat esackoob maeb nmuloc wodniw draob afos
]601[
stniop
fo
rebmuN
llaw gniliec roolf rood esackoob riahc elbat maeb nmuloc wodniw draob afos
remrofsnarT
redoceD
NNC
redoceDRethinkingContinualLearningforReal-worldImpact
Table10.Hyperparametersforourexperiments.LRrepresentsthelearningrateandCEthecross-entropyloss.λ istheweight
method
oftheregularizationlossdependingonthemethod(e.g.λ fortheMASlossexplainedinEq.4).Wewillpublishourcodetofacilitate
mas
reproducibility.
Methods
Dataset
MAS(Aljundi et al., 2018)LwF(Li & Hoiem, 2017)ER(Chaudhry et al., 2019)LG-Clip
LREncoder 10−4 5×10−3 10−3 10−3
LRDecoder 10−5 10−4 10−4 10−4
ScanNet(Daietal.,2017b)
λCE 1 1 1 -
λmethod 50 1 - 1
LREncoder 10−4 5×10−3 10−3 10−4
LRDecoder 10−4 10−4 10−4 10−4
S3DIS(Armenietal.,2016)
λCE 1 1 1 -
λmethod 50 1 - 1
LREncoder 5×10−3 5×10−3 10−3 10−4
LRDecoder 10−4 10−4 10−4 10−4
SemanticKITTI(Behleyetal.,2019)
λCE 1 1 1 -
λmethod 1 1 - 1
adimensionalityof512,deviatingfromtheoriginal128in D.1.ComparedMethods
ME (Choy et al., 2019) to align with the CLIP (Radford
In addition to the Joint Training (JT) and Finetuning (FT)
et al., 2021) feature dimensionality. Joint Training (JT)
baselinesoutlinedinthemainpaper,weprovideamorede-
was conducted with the 512 dimensionality, showing no
tailed description of the seminal continual learning works
significant performance change compared to the original
analyzedinoursetup.
ME(Choyetal.,2019).
Memory Aware Synapses (MAS). MAS (Aljundi et al.,
Model Decoder. The conventional decoder consists of a
2018) incorporates a penalty to regularize the update of
1 × 1 × 1 convolutional layer with stride 1 that projects
modelparametersthatareimportanttopasttasks. During
encoder features to semantic logits. The Transformer de-
eachtrainingstep,MASoptimizesthefollowingloss:
coderconsistsof3layerswith512dimensions. Eachlayer
consistsofself-attention,cross-attention,andfeed-forward
network. Theattentionineachlayerhas8heads. L=λ CE ·L CE(F(x k;θ),y˜ t)+
Other Details. For the joint training and (Yang et al., λ ·(cid:88) Ω (θt −θt−1)2 (4)
2023), the model was trained according to the established mas ij ij ij
i,j
pipelinesoftheoriginalmethods. Forthecontinuallearn-
ing setup, the network was trained using cross-entropy withθt−1theparametersofthemodelfortheprevioustask
loss (except in cases where Dice loss was incorporated), ij
andθt thecurrentmodelparameters. Itallowsthechange
employing a stochastic gradient descent optimizer with ij
ofparametersthatarenotimportantforprevioustasks(low
a mini-batch size of 10. We train the model with the
Ω )andpenalizesthechangeofimportantones(highΩ ).
TransformerdecoderusingAdamW(Loshchilov&Hutter, ij ij
Theparameterimportanceiscomputedas:
2017). Methodparametersindetail:
• ER: The mini-batch size was reduced to 5, and the 1 (cid:88)N
mini-batchsizeretrievedfromthememorybufferwas Ω ij = N ||g ij(x k)|| (5)
alsosetto5,resultinginatotalof10,consistentwith k=1
othermethods.
∂[l2(F(x ;θ))]
• LwF: We set the temperature factor T = 2 as per where g ij(x k) = 2
∂θ
k the gradient of the
ij
the original paper and other continual learning (CL)
learned function with respect to parameter θ at the data
papers.
pointx . N isthetotalnumberofpointsatagiventask.
k
LearningwithoutForgetting(LwF).LwF(Li&Hoiem,
• LG-Clip: Usesonly10pointspersceneforsupervi-
2017) utilizes knowledge distillation from a teacher to a
sion.
studentmodeltopreserveknowledgefrompasttasks. The
• Diceloss: Weightedwithλ
dice
=50andλ
CE
=1. teachermodelisthemodelafterlearningthelasttaskt−1,
andthestudentmodelisthemodeltobetrainedonthecur-
For the hyperparameter tuning, we followed (Ghunaim renttaskt. Foranewtasktwith(x ,y )theinputx and
k t k
etal.,2023). FordetailspleaseseeTab.10. Wewillpublish groundtruthy ,LwFcomputesF(x ;θt−1),theoutputof
t k
our code upon acceptance to facilitate the reproducibility thepreviousmodelθt−1forthenewdatax . Duringtrain-
k
ofourresults. ing,LwFoptimizesthefollowingloss:
13RethinkingContinualLearningforReal-worldImpact
L=λ ·L (F(x ;θ),y˜)+
CE CE k t
λ ·L (F(x ;θt−1),F(x ;θt)) (6)
lwf lwf k k
where F(x ;θt−1),F(x ;θt) are the predicted values of
k k
theprevious andcurrentmodelusingthe samex . Theλ
k
controlsthefavoringoftheoldtasksoverthecurrenttask.
Experience Replay (ER). ER (Chaudhry et al., 2019) is
a straightforward yet effective replay method. It employs
reservoir sampling (Vitter, 1985) for updating the mem-
ory with new examples and random sampling for retriev-
ing examples from memory. Reservoir sampling ensures
that each incoming data point has an equal probability of
(memorysize)/n to be stored in the memory buffer. n is
thenumberofdatapointsobservedforeachtaskuptothe
present. In our configuration, all tasks have the same size
memory buffer. ER trains the model by combining in the
mini-batchthecurrenttaskdatawiththedatafrommemory
usingthecross-entropyloss.
14