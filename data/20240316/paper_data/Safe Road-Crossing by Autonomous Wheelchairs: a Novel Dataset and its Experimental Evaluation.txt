Safe Road-Crossing by Autonomous Wheelchairs:
a Novel Dataset and its Experimental Evaluation
Carlo Grigioni[0009−0002−0794−0612], Franca Corradini[0009−0009−5867−9478],
Alessandro Antonucci[0000−0001−7915−2768], Jérôme Guzzi[0000−0002−1263−4110],
and Francesco Flammini[0000−0002−2833−7196]
name.surname@supsi.ch
IDSIA USI-SUPSI, University of Applied Sciences and Arts of Southern Switzerland,
Lugano, Switzerland
Abstract. Safe road-crossing by self-driving vehicles is a crucial prob-
lemtoaddressinsmart-cities.Inthispaper,weintroduceamulti-sensor
fusionapproachtosupportroad-crossingdecisionsinasystemcomposed
byanautonomouswheelchairandaflyingdronefeaturingarobustsen-
sory system made of diverse and redundant components. To that aim,
we designed an analytical danger function based on explainable physi-
calconditionsevaluatedbysinglesensors,includingthoseusingmachine
learningandartificialvision.Asaproof-of-concept,weprovideanexper-
imentalevaluationinalaboratoryenvironment,showingtheadvantages
ofusingmultiplesensors,whichcanimprovedecisionaccuracyandeffec-
tively support safety assessment. We made the dataset available to the
scientificcommunityforfurtherexperimentation.Theworkhasbeende-
veloped in the context of an European project named REXASI-PRO,
which aims to develop trustworthy artificial intelligence for social navi-
gation of people with reduced mobility.
1 Introduction
Self-driving vehicles have attracted a huge interest in the intelligent transporta-
tion systems industry. Those vehicles are equipped with advanced sensors pro-
vidingdatatocomputingsystemsrunningArtificialIntelligence (AI)algorithms
thatenablethemtooperateandnavigatewithouthumanintervention.Theyof-
fer the promise of increased safety and performance. These vehicles rely on sev-
eraltechnologies,includingLiDARs,radars,cameras,andsophisticatedsoftware
to perceive their surroundings, make decisions, and control their movements [5].
However, all those technological innovations based on AI and Machine Learning
(ML)comewithbigchallengesintermsoftrustworthiness,duetolimitedtrans-
parencyandfailurespossiblyhavingfatalconsequencesinsafety-criticalsystems
[8]. This is also the case of Autonomous Wheelchairs (AWs) that are meant to
support motion-impaired persons in safe door-to-door navigation.
Therehavebeenseveraleffortstoimprovethetechnologyofpoweredwheelchairs
since their first appearance during the 19th century [21]. Modern AWs can be
4202
raM
31
]OR.sc[
1v48980.3042:viXra2 C. Grigioni et al.
very complex including sophisticated components, such as autonomous naviga-
tion system, and aim to accommodate several disabilities by means of multi-
modal interfaces. Safety assessment and certifications of AWs are paramount
aspects to address [6]. In fact, in presence of components based on AI, together
withspecificregulationssuchas“ISO7176-14:2022Wheelchairs”,1 otherrequire-
ments and guidelines must be considered such as the ones included in the EU
Artificial Intelligence Act.2
The work described in this paper has been developed within the European
project REXASI-PRO (REliable & eXplAinable Swarm Intelligence for Peo-
ple with Reduced mObility),3 which addresses indoor and outdoor use cases to
demonstrate trustworthy social navigation of autonomous wheelchairs together
withflyingdronesinreal-worldenvironments.Dronesareusedtoprovideaview
of the environment from different perspectives and thus collect additional data
to scan and map the surrounding environment.
Inthispaper,wedealwitharoad-crossingscenariousingmultiplesensorsand
with no support of traffic lights. We adopt multi-sensor fusion for run-time risk-
baseddecisionsupport.Tothisaim,weuselaboratorywheeledrobots,equipped
withartificialvisionanddistancesensors,asmock-upmodelsofactualAWsand
drones, for which we perform run-time safety evaluation using an analytical
danger function based on physical considerations.
The main original contributions of this paper can be summarized as follows:
– We provide a novel reference scenario for road-crossing by AWs as well as
hints and details about laboratory experimentation and performance evalu-
ation.
– Wesharewiththescientificcommunityaspecificdatasetwithdatarecorded
from multiple sensors that can be used for further experimentation and per-
formance evaluation.
– Weintroduceanapproachforroad-crossingdecisionmakingbasedonanan-
alytical and hence explainable danger function highlighting the importance
of multi-sensor obstacle detection.
– As a proof-of-concept, we report the results of a preliminary experimental
evaluation demonstrating the advantages of using diverse redundant sen-
sors, which can support safety certification when used in combination with
probabilistic safety models.
The remaining of this paper is structured as follows. Sec. 2 provides an
overview of the related literature. Sec. 3 introduces the reference road-crossing
scenario. Sec. 4 describes the design of the danger function. Sec. 5 addresses
dataset generation in the laboratory environment. Sec. 6 reports the results of
theexperimentalevaluation.Finally,Sec.7providesconclusionsandhintsabout
future developments.
1 https://www.iso.org/standard/72408.html
2 https://artificialintelligenceact.eu
3 https://rexasi-pro.spindoxlabs.com/Safe Road-Crossing by Autonomous Wheelchairs 3
2 Related works
Pedestrians and wheelchair users are among the most vulnerable group type
in transport systems and traffic environments [10,13,20]. Being road-crossing
especially dangerous, it is one of the most investigated situation in terms of
pedestrian–vehicle conflict to prevent accidents [7]. In [17], authors propose a
crosswalk safety warning system where both pedestrian and vehicle behaviour
is detected and evaluated by multiple sensors; warning alerts are communicated
to both parts through lights integrated in the cross path infrastructure. In [3],
a smart traffic light control system equipped with a camera is used to man-
age pedestrian crossing considering the traffic condition and type of pedestrian;
crossing is improved for the elderly or people with disabilities without worsen-
ing traffic. In [14], authors propose a vision-based approach to generate safety
messagesinreal-timeusingvideostreamsfromroadsidetrafficcameras;themes-
sagescanbeusedbyconnectedautonomousvehiclesandsmartphonesequipped
with pedestrian safety applications. Given the costs of such approaches, virtual
reality is a useful tool to simulate smart pedestrian crossing [11], particularly if
designed for wheelchair users [2].
Although those solutions represent an aid for wheelchair users, they need a
specific and possibly expensive infrastructure. In [16], ML is used to elaborate
thevideosofon-boardcamerasandtosupportAWdecisions.Asimilarapproach
is applied in [4], where a convolutional neural network is suitably designed to
evaluate the risk of road-crossing on Indian roads. Compared to our approach,
those works are based on single ML technologies lacking sufficient redundancy,
diversity and transparency, and as such less suitable for safety assessment.
3 Reference scenario for safe road-crossing
When crossing a road, especially where there are no traffic lights, humans con-
sider a variety of factors to ensure both safety and convenience. Primarily, they
assess the flow of traffic, considering variables like the speed and quantity of ve-
hicles. They also account for the presence of road-crossings, traffic signals, and
zebra crossings, as these demarcate safe areas for traversing. The distance to
the nearest crossing point and the estimated time required to reach it are taken
intoconsideration,asarepotentialtrafficdelaysthatmightofferasafercrossing
opportunity. Additionally, individuals evaluate their own physical capabilities
for quick crossing and gauge the visibility of oncoming vehicles. Factors such
as weather conditions, the presence of children or elderly, and the accessibility
of nearby sidewalks also affect the decision. Ultimately, the act of crossing a
roadinvolves amultifaceted interplay ofthese considerations,striking abalance
between personal safety and the need to reach the destination efficiently.
In our analysis, we assume that pedestrians account for distance, speed and
acceleration([1]and[15])aspivotalcuesintheirassessmentofapproachingvehi-
clesbeforedeterminingwhetheritissafetocrosstheroad.Thesefactorsprovide
vital insights into how rapidly a vehicle is closing in and whether adequate time
exists for a safe crossing.4 C. Grigioni et al.
(a)
(b)
Fig.1:Road-crossingscenario:droneandAWcooperatetosupportsafedecisions,
e.g., when a car surpasses the crossing location (a), or when a car stops before
the crossing location (b).
Distance. Thedistanceofanapproachingvehicleisacentralindicatoronwhich
pedestrians heavily rely. Distance perception is related to the personal experi-
ence of each single pedestrian. For example it might be affected by the physical
condition of the pedestrian, the weather condition or other elements that can
affect the time required to perform crossing.
Speed. People gauge speed through visual cues, such as changes in the vehicle’s
apparent size and the interval it takes for the vehicle to traverse a specific dis-
tance between reference points on the road. A vehicle that is swiftly shortening
the gap is generally perceived as moving at a higher speed, while one with a
more gradual progression may be perceived as moving slower.
Acceleration. Pedestrians also consider acceleration, denoting how swiftly an
objectaltersitsspeedovertime.Accelerationcontributestothedecision-making
process by signaling how quickly a vehicle may reach the pedestrian’s location.
Consequently, this can influence the choice to wait for the vehicle to pass before
attempting to cross.
We apply a similar reasoning to the road-crossing scenario for AWs, where the
system must take a safe road-crossing decision. Following the principles of re-
dundancy and diversity, a safe decision must be based on multiple sensors using
different technologies whose accuracy is affected by mostly independent factors
[9]. This can be achieved, for instance, by combining cameras featuring artifi-
cial vision with LiDARs or other distance sensors. Additional information canSafe Road-Crossing by Autonomous Wheelchairs 5
be obtained by connecting the AW with drones or specific infrastructures [14],
providing information from diverse sources or different perspectives.
The road-crossing scenario we consider consists of an AW on the side of a
one-way road aiming to reach the other side, a drone connected to the AW that
provides further information and a vehicle approaching the specific section of
the road. We assume that there are no traffic lights and the driver’s behaviour
is not predictable; he/she might slow down to allow the crossing or move ahead,
maintaining its motion. Hence, the AW must use its sensing capabilities to esti-
mate the potential danger: the crossing is safe if the danger is estimated under
a certain threshold. If the danger is too high, crossing is not recommended un-
til the vehicle slows down or surpasses the crossing location. This is why it is
important to design a danger function, as described in the next section. Fig. 1
provides a schematic representation of the scenario.
4 Design of the danger function
In this section, we address the problem of assessing the risk associated with
road-crossingbasedonwhatdiscussedintheprevioussection.Theroad-crossing
scenarioincludesanAWattemptingtocrosstheroadandanoncomingobstacle
(e.g., a car). As shown in Fig. 2, the two bodies are assumed to have orthogonal
motions, with the pedestrian crossing the road from one side to the other while
the vehicle proceeds along the driveway.
Inourapproach,theriskevaluationisbasedonacontinuousfunctiong,called
Danger Function (DF). The function is intended to reflect a kinematic analysis
of the road-crossing scenario in a particular moment t. Accordingly, the value
of the DF reflects the danger level associated with the kinematic configuration
of the system at a specific moment. For decision-making, we summarise the
assessments provided by the DF g by tagging them as potentially dangerous if
the corresponding DF values exceed a given threshold g∗.
Fig.2: Kinematics involved in the design of the danger function.6 C. Grigioni et al.
To derive the DF, we assume that the AW can proceed with uniform linear
motionandtheobstacleapproachesitwithauniformlyacceleratedmotion.Both
assumptions are often unrealistic, but might reflect an over-cautious modelling
providingasaferevaluation:inrealscenarios,incaseofdanger,thevehiclewould
likely slow down and the AW would likely accelerate.
We consequently model the road-crossing scenario as follows. Assuming the
AW crossing the road to move on the x-axis and starting the crossing from
position x=0 with constant speed v , we have:
w
x (t)=v ·t. (1)
w w
Similarly,assumingtheobstacle,i.e.,thecar,movingonthey-axiswithconstant
acceleration a and initial speed v :
c c
a
y (t)=v ·t+ c ·t2. (2)
c c 2
FromEq.(1),wecancomputethetimet requiredbytheAWtocrossaroad
cross
of width l, i.e., t :=l/v . During the interval of time [0,t ], the obstacle
cross w cross
shouldbefarenoughtonotimpactwiththeAW,i.e.,ifd isthedistanceofthe
c
obstacle:
a
d >v ·t + c ·t2 , (3)
c c cross 2 cross
and hence:
l ·v + l2 ·a
vw c 2·v w2 c
<1. (4)
d
c
WecanregardtheconstraintinEq.(4)asasafetyconditionforakinematics-
basedDFwiththresholdone.Asexpected,thedangerincreasesforhighervalues
of speed and acceleration of the car, while decreasing for increasing distances.
Assessing all the parameters of such a DF might be critical when coping with
different road-crossing scenarios. For this reason, we heuristically define a DF g,
with similar relations with d , v and a as in Eq. (4), whose parameters might
c c c
be reasonably assessed for a generic scenario.
Regarding the inverse dependence with respect to d , we assume that close
c
objects are significantly more dangerous than distant objects, even if they are
moving at lower speed. Consequently, to enhance the impact of the distance
on the function in specific situations where the two bodies are close, a loga-
rithm smoothing is applied to d . Moreover, a small threshold ϵ is also added
c
to always obtain finite values. Concerning the linear combination of speed and
acceleration in the numerator of the left-hand side of Eq. (4), we perform lin-
ear transformations with threshold on both v and a , and denote the output of
c c
thesetransformationsasvˆ andaˆ anduseacoefficientk toevaluatetherelative
c c
contribution of the speed with respect to the acceleration. Therefore the DF is
described as:
vˆ +k·aˆ
g(d ,v ,a ):= c c , (5)
c c c log(d +ϵ)
cSafe Road-Crossing by Autonomous Wheelchairs 7
wherek =0.1andϵ=0.6aresetfollowingheuristics.Thelineartransformation
of the speed is:

0 if v ≤v ,
 c c
vˆ c := 1vvc c− −v vc
c
i if
f
v vc >< vv c ,≤v c, (6)
c c
and, for the acceleration:

−1 if a ≤−a
aa
cc −+a ac
c
if
−c
a c
<ac
c ≤−a c
aˆ := 0 if −a <a ≤a (7)
c c c c
1aa cc −− aa
c
i if
f
a
ac
< >aa
c
.≤a
c
c c
The above transformations are intended to prevent contributions to the DF by
low speeds (i.e., v ≤ v ) and low accelerations (i.e., |a | ≤ a ), while also
c c c c
putting a normalised upper bound to the contributions of high speeds (v ≥v )
c c
andaccelerations(|a |≥a ).Fig.3showsthechoiceofthethresholdparameters
c c
v , v , a , and a , we considered for the experimental setup (Sec. 6).
c c c c
Fig. 4 shows an example of the kinematics (for clarity, only distance and
speed on the left) and the corresponding values of the DF (right).
vˆ
1 c
1 aˆ
c
a (m)
c s2
a =1 a =10
c c
v(m)
s
v c=0.05 v c=0.65 −1
Fig.3: Linear transformations with threshold used for v (left) and a (right).
c c
5 Dataset generation
In this section, we provide the specification of the laboratory environment —
including all the equipment and hardware components — and describe the pro-
cedureofdatacollection,datapre-processinganddataelaboration.Intheexper-
iments,wetargetasimilarscenarioasdescribedinSec.3.Weadoptasimplified
experimental setup, where three ground robots equipped with vision and dis-
tance sensors represent the AW, drone and vehicle, as depicted in Fig. 5. In8 C. Grigioni et al.
3 0.6 1.5
distance
speed
2 0.4 g∗
1 0.2
0 0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
t(s) t(s)
Fig.4: Distance, speed (left) and DF values (right) on the experiment in Sec. 6.
our lab, we can access ground truth poses from a very accurate motion tracking
system.
5.1 Laboratory environment
The experiments have been performed in the IDSIA Autonomous Robotics Lab-
oratory.4
Robots. Forourexperimentsweusethreewheeledomni-directionalrobots:each
one is a RoboMaster EP (RM), a commercial education platform from DJI,5
whose specifications are summarized in Tab. 1. Each RM is customised for its
role as “car”, “AW”, or “drone”.
RM simulates the vehicle and does not require additional sensors, as we are
c
interested just on its movement.
RM representstheAWandisequippedwithacameraandfourinfraredrange
w
sensors.GiventheirnarrowFieldOfView (FOV)(seeTab.1),rangesensors
are located on the RM so that their outputs can be combined to obtain
informationforalargerFOV.TheRM anditssensorsareshowninFig.5b.
w
Conservatively, we consider the minimum distance reading returned by the
fourrangesensors,whichfromnowonwerefertoasthesinglesensorRange
Sensors Unit (RSU).
RM acts as the drone and has a camera positioned on top of the robotic arm.
d
MotionTracker. ThelabusedfortheexperimentsisequippedwithanOptiTrack
motiontracker6 composedofeighteeninfraredcamerascoveringanareaof6m×
6m×2m to track the pose of the three robots at 30Hz with sub-millimetre
accuracy.
4 https://idsia-robotics.github.io
5 https://www.dji.com/ch/robomaster-ep
6 https://docs.optitrack.com/v/v2.3
)m(decnatsid )s/m(vdeeps glevelregnadSafe Road-Crossing by Autonomous Wheelchairs 9
size 32cm×24cm×27cm
Chassis maximal speed 3.5m/s
maximal angular speed600◦/s
FOV 120◦
Camera video resolution 1280 x 720
video fps 30Hz
maximal range 10m
Range sensorsFOV 20◦
accuracy 5%
Table 1: Technical specifications of RoboMaster EP.
5.2 Data collection and preprocessing
AsinthescenariodescribedinSec.3,RM andRM aremotionlessandoriented
w d
towards the approaching RM , which is remotely controlled.
c
Data collection. Tocontroltherobotsandcollectdatafromthemotiontracker,
we used the Robot Operating System (ROS2 [18]), executing a ROS2 driver for
eachrobot.7 Foreachexperimentalrun,werecordeddatafromthetwocameras,
the RSU and the motion tracker in bag files as detailed in Tab. 2.
(a) (b)
Fig.5: (a) Example of setup for data collection: in red is the trajectory of RM
c
and in blue the crossing path of RM . (b) RM and its components.
w w
Overall,werecorded15runsfortwodifferentsetups(thefirstoneisdepicted
in Fig. 5a), for which runs last approximately 6s and resp. 9s. In the second
setup,theinitialdistanceoftheRM withrespecttotheRM islarger,allowing
c w
for more complex patterns in movement of the obstacle during experiments.
7 https://github.com/jeguzzi/robomaster_ros10 C. Grigioni et al.
Name Source Type Frequency
Poses of RM , RM and RM Motion Tracker2D poses 30Hz
w d c
Video RM 1080×720 RGB images 30Hz
w
Video RM 1080×720 RGB images 30Hz
d
Range sensors RM 4 distances to nearest object10Hz
w
Table 2: Raw data recorded during the experiments.
Data pre-processing. Raw data from the bag files were processed to synchronise
allstreams.RelativedistancesbetweenRM andRM werecomputedforground
w c
truth poses and from RSU. Raw data from cameras were split using OpenCV8
into frames. These were processed with You Only Look Once (YOLO)9 [19], a
commonlyusedMachineLearningmodel,todetectRM .RMsdonothaveaspe-
c
cific class in YOLO, but are consistently recognised as motorcycles. YOLO was
used to identify their bounding boxes in image space, from which we computed
thedistanceusingtrianglesimilarity.10 Finally,thethreedistancemeasuresfrom
thesensorsonboard,i.e.closedRSUandthetwocameras,weresmoothed,taking
an average of the previous 2 (RSU) or 5 (cameras) samples .
5.3 Data elaboration and sensor fusion
Speedandaccelerationmeasureswerecomputedfromthefourdistancemeasures
obtained from the RSU, the two cameras and the tracker. Then, danger was
estimatedforeachsensorusingthefunctiondescribedinSec.4.Whenthedanger
value exceeded the given threshold, a dangerous situation was detected and
crossing was not recommended. An example of the results is shown in Fig. 6:
threeframesfromtheRM cameraoutputaredisplayedwiththecorresponding
w
DF value calculated by the tracker. A dot in green (resp. red) defines a safe
(resp. dangerous) situation as a result of the threshold criterion.
Since three values can be obtained from the RSU and the two cameras, a
fusionprocedureisrequired.Rawdatafromsensorsarealreadyaligned,buthave
differentsamplingtimes.Allmeasureswerethereforeresampledat100Hzbefore
merging. We considered three different fusion architectures. A straightforward
fusion procedure consists of taking the average of the distances measured by
thethreesensors.Wecallthisproceduredistance fusion.Wecallinsteaddanger
fusion the fusion procedure where the average of the DFs computed by the
distancesmeasuredbythethreesensorsisconsidered.Finally,voting fusion was
appliedtothevaluesobtainedafterthethresholdoperationontheDFsthrough
amajorityvote.Allthedatacollectedareavailable,togetherwiththecodeused
for processing and analysis, in a freely available repository.11.
8 https://docs.opencv.org/4.x/index.html
9 https://github.com/ultralytics/yolov5
10 https://pyimagesearch.com/2015/01/19/find-distance-camera-objectmarker-using-
python-opencv/
11 https://github.com/CarloGrigioni/safe_roadcrossing_awSafe Road-Crossing by Autonomous Wheelchairs 11
Fig.6: Example of the results: RM camera frames number 6, 60 and 90 with
w
corresponding DF calculated by ground truth (tracker) measures.
2 2
rangesensors
1 1 distancefusion
AWcamera
tracker
dronecamera
tracker
0 0
1 2 3 4 1 2 3 4
t (s) t (s)
Fig.7: Distance as recorded by the different sensors (left), by the ground truth
(tracker) and after the distance fusion (right).
6 Experimental evaluation
In this section, we present the results of the three fusion approaches described
inSec.5.3foroneoftheexperiments.Fig.7depictsthecomparisonbetweenthe
three pre-processed distance measures with the “ground truth” from the motion
tracker (left), and the DF based on the distance fusion (right).
From Fig. 7 (left), it can be observed that RSU output is always available,
whilecamerasgeneratesignalsonlywhenRM isalreadyneartotheotherRMs.
c
When the output of the cameras is not available, the distance fusion is only
computed based on the RSU. This is due to the computer vision system, which
is unable to adequately detect the RM since it is confused with the laboratory
c
background when it is far from cameras.
Similar considerations can be done for the danger fusion. The DF values
computed by the three sensors are displayed and compared with the tracker
data in Fig. 8 (left). Those values are very noisy and unstable, while the fusion
offers a smoother output that is more suitable to danger evaluation, as depicted
by Fig. 8 (right).
)m(
d
ecnatsid12 C. Grigioni et al.
Foraquantitativeevaluation,wecomputedaRootMeanSquareError (RMSE)
by comparing the DFs based on the tracker, with those obtained from the sin-
gle sensors, and with the distance and danger fusion outputs. Such a descriptor
cannotbecomputedforthevoting fusion,whichreturnsabinaryoutput.When
coping with such outputs, we quantify the classification performance of the dif-
ferent methods in terms of accuracy, precision and recall. To interpret those
measures, note that a false positive is a case where a safe output is recognised
as a dangerous situation. The overall results are displayed in Tab. 3.
In our experimentation with single technologies, the range sensors provided
the best performance in terms of RMSE, accuracy, and precision. However, the
AW camera obtained the best recall. At the fusion level, the worst performance
was given by the voting fusion. In spite of a higher RMSE compared to the
danger fusion, the accuracy of the distance fusion was significantly higher. This
confirms the idea that the best results are obtained by performing the fusion
before any further signal processing [12].
rangesensors distancefusion
AWcamera dangerfusion
dronecamera tracker
tracker
g∗ g∗
1 2 3 4 1 2 3 4
t(s) t(s)
Fig.8: DF computed from distances recorded by the sensors (left) and as ob-
tained by danger and distance fusion (right).
Source/Fusion Technique RMSE AccuracyRecallPrecision
Range sensors 0.582 0.92 0.89 0.99
AW camera 0.948 0.77 1.0 0.59
Drone camera 1.923 0.51 0.63 0.32
Distance fusion 0.447 0.92 0.91 0.95
Danger fusion 0.361 0.83 0.78 0.96
Voting fusion - 0.67 0.50 0.38
Table 3: Performance evaluation of sensors and fusion techniques.
gregnadSafe Road-Crossing by Autonomous Wheelchairs 13
7 Conclusions
In this paper, we have addressed the problem of safe road-crossing by au-
tonomouswheelchairssupportedbyflyingdrones,usingmulti-sensorfusion.We
have focused on the generation of a relevant laboratory dataset from multi-
ple artificial vision and distance sensors installed on RoboMasters operating in
ROSandOptiTrackenviroments.Wemadethedatasetpubliclyavailabletothe
scientific community for further experimentation and performance evaluation.
We also designed an analytical danger function to enable run-time risk assess-
ment for road-crossing decision support. We have experimentally evaluated the
danger function to provide some preliminary results as a proof-of-concept. The
function is based on physical conditions and therefore represents a case of ex-
plainable decision fusion, whereas the output of some sensors, especially those
based on artificial vision, is affected by a variable degree of opacity and uncer-
taintythatneedtobequantifiedtosupportprobabilisticsafetyevaluation.Such
an evaluation can be performed using an approach similar to the ones already
introduced in [9] and [6], and will be developed in future works. As future de-
velopments, we also plan to extend the dataset and experimentation with more
complex real-world situations to be recognised and managed, such as multiple
vehicles/obstacles from different directions, as well as with interference and dis-
turbances due to vibrations, dirty camera lenses, glares, darkness, obstructions,
weatherconditions(rain,fog,etc.),someofwhichcan besimulatedbyapplying
appropriate software filters. Although we focused on a specific application, we
believe our contribution can be extended to other crossing scenarios, e.g., to
support vision impaired people with similar assistive technologies.
Acknowledgments. ThisworkwassupportedbytheSwissStateSecretariatforEd-
ucation, Research and lnnovation (SERI). The project has been selected within the
European Union’s Horizon Europe research and innovation programme under grant
agreement: HORIZON-CL4-2021-HUMAN-01-01. Views and opinions expressed are
however those of the authors only and do not necessarily reflect those of the fund-
ing agencies, which cannot be held responsible for them.
References
1. Underestimation tendencies of vehicle speed by pedestrians when crossing un-
marked roadway. Accident Analysis & Prevention 143, 105586 (2020)
2. Asha, A.Z., Smith, C., Freeman, G., Crump, S., Somanath, S., Oehlberg, L.,
Sharlin, E.: Co-designing interactions between pedestrians in wheelchairs and au-
tonomousvehicles.In:Proceedingsofthe2021ACMDesigningInteractiveSystems
Conference. p. 339–351. DIS ’21, ACM, New York, NY, USA (2021)
3. Banu, A.S., Lakchida, S.A.S., Shanthini, V.S., Stinsha, S.L.: Smart traffic light
controlsystemusingimageprocessing.In:2022InternationalConferenceonAug-
mented Intelligence and Sustainable Systems (ICAISS). pp. 700–706 (2022)14 C. Grigioni et al.
4. Brahmbhatt, S.: A dataset and model for crossing Indian roads. In: Proceedings
of the Thirteenth Indian Conference on Computer Vision, Graphics and Image
Processing. ICVGIP ’22, ACM, New York, NY, USA (2023)
5. Chen, L., Li, Y., Huang, C., Li, B., Xing, Y., Tian, D., Li, L., Hu, Z., Na, X.,
Li, Z., Teng, S., Lv, C., Wang, J., Cao, D., Zheng, N., Wang, F.Y.: Milestones in
autonomousdrivingandintelligentvehicles:Surveyofsurveys.IEEETransactions
on Intelligent Vehicles 8(2), 1046–1056 (2023)
6. Corradini,F.,Flammini,F.,Antonucci,A.:Probabilisticmodellingfortrustworthy
artificialintelligenceindrone-supportedautonomouswheelchairs.In:Proc.1stInt.
Symp.onTrustworthyAutonomousSystems.TAS’23,ACM,NewYork,US(2023)
7. ElHamdani,S.,Benamar,N.,Younis,M.:Pedestriansupportinintelligenttrans-
portationsystems:Challenges,solutionsandopenissues.TransportationResearch
Part C: Emerging Technologies 121, 102856 (2020)
8. Flammini, F., Alcaraz, C., Bellini, E., Marrone, S., Lopez, J., Bondavalli, A.:
Towards trustworthy autonomous systems: Taxonomies and future perspectives.
IEEE Transactions on Emerging Topics in Computing (01), 1–13 (5555)
9. Flammini, F., Marrone, S., Nardone, R., Caporuscio, M., D’Angelo, M.: Safety
integrity through self-adaptation for multi-sensor event detection: Methodology
and case-study. Future Generation Computer Systems 112, 965–981 (2020)
10. Guo,W.,Ma,X.,Li,Y.,Tan,J.,Zhang,Y.:Studyoncharacteristicsofpedestrian
crossingthestreetinshijingshan.In:Proc.2015Int.ConferenceonArchitectural,
Civil and Hydraulics Engineering. pp. 396–402. Atlantis Press (2015/11)
11. Guo, X., Shen, Z., et al.: Smart pedestrian crossing design by using smart devices
to improve pedestrian safety. Reviews of Adhesion and Adhesives 11(3) (2023)
12. Hall,D.,Llinas,J.:Anintroductiontomultisensordatafusion.Proceedingsofthe
IEEE 85(1), 6–23 (1997)
13. Henje, C., Stenberg, G., Lundälv, J., Carlsson, A.: Obstacles and risks in the
trafficenvironmentforusersofpoweredwheelchairsinSweden.AccidentAnalysis
& Prevention 159, 106259 (2021)
14. Islam,M.,Rahman,M.,Chowdhury,M.,Comert,G.,Sood,E.D.,Apon,A.:Vision-
based personal safety messages (PSMs) generation for connected vehicles. IEEE
Transactions on Vehicular Technology 69(9), 9402–9416 (2020)
15. Li, F., Pan, W., Xiang, J.: Effect of vehicle external acceleration signal light on
pedestrian-vehicle interaction. Scientific Reports 13 (09 2023)
16. Nakayama, Y., Lu, H., Tan, J.K., Kim, H.: Environment recognition for naviga-
tion of autonomous wheelchair from a video image. In: 2017 17th International
Conference on Control, Automation and Systems (ICCAS). pp. 1439–1443 (2017)
17. Qu,D.,Li,H.,Liu,H.,Wang,S.,Zhang,K.:Crosswalksafetywarningsystemfor
pedestrians to cross the street intelligently. Sustainability 14, 10223 (08 2022)
18. Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R.,
Ng, A.: ROS: an open-source robot operating system. vol. 3 (01 2009)
19. Redmon,J.,Divvala,S.,Girshick,R.,Farhadi,A.:YouOnlyLookOnce:Unified,
real-time object detection (2016)
20. Statistics, N.: Analysis: 2018 fatal motor vehicle crashes: Overview. Traffic Safety
Facts Research Note. Report No. DOT HS 812 p. 826 (2019)
21. Yukselir, M., Scarrow, K., Wilson, P., Cowan, T.: The brains behind the electric
wheelchair, one of Canada’s ‘great artifacts’. The Globe and Mail 27 (2012)