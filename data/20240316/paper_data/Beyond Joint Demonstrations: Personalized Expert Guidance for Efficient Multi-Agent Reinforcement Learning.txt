Beyond Joint Demonstrations:
Personalized Expert Guidance for Efficient Multi-
Agent Reinforcement Learning
Peihong Yu Manav Mishra Alec Koppel
peihong@umd.edu mishra20@iiserb.ac.in alec.koppel@jpmchase.com
University of Maryland IISER Bhopal JP Morgan Chase & Co.
Carl Busart Priya Narayan
carl.e.busart.civ@army.mil priya.narayanan.civ@army.mil
DEVCOM Army Research Laboratory DEVCOM Army Research Laboratory
Dinesh Manocha Amrit Bedi Pratap Tokekar
dmanocha@umd.edu amritbedi@ucf.edu tokekar@umd.edu
University of Maryland University of Central Florida University of Maryland
Abstract
Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of
efficientexplorationduetotheexponentialincreaseinthesizeofthejointstate-action
space. While demonstration-guided learning has proven beneficial in single-agent
settings, its direct applicability to MARL is hindered by the practical difficulty
of obtaining joint expert demonstrations. In this work, we introduce a novel
concept of personalized expert demonstrations, tailored for each individual agent or,
more broadly, each individual type of agent within a heterogeneous team. These
demonstrations solely pertain to single-agent behaviors and how each agent can
achieve personal goals without encompassing any cooperative elements, thus naively
imitating them will not achieve cooperation due to potential conflicts. To this end,
we propose an approach that selectively utilizes personalized expert demonstrations
as guidance and allows agents to learn to cooperate, namely personalized expert-
guided MARL (PegMARL). This algorithm utilizes two discriminators: the first
provides incentives based on the alignment of policy behavior with demonstrations,
and the second regulates incentives based on whether the behavior leads to the
desiredobjective. WeevaluatePegMARLusingpersonalizeddemonstrationsinboth
discrete and continuous environments. The results demonstrate that PegMARL
learnsnear-optimalpoliciesevenwhenprovidedwithsuboptimaldemonstrations,and
outperformsstate-of-the-artMARLalgorithmsinsolvingcoordinatedtasks. Wealso
showcase PegMARL’s capability to leverage joint demonstrations in the StarCraft
scenario and converge effectively even with demonstrations from non-co-trained
policies.
1 Introduction
The use of expert demonstrations has been proven effective in accelerating learning in single-agent
reinforcement learning, as evidenced by studies such as Kang et al. (2018); Chen & Xu (2022);
Rengarajan et al. (2022). This approach has since been extended to Multi-Agent Reinforcement
Learning (MARL) (Lee & Lee, 2019; Qiu et al., 2022), which typically assumes the availability of
high-quality collaborative joint demonstrations. However, from a practical standpoint, collecting
1
4202
raM
31
]AM.sc[
1v63980.3042:viXrajoint demonstrations can be labor-intensive, demanding one user per agent in cooperative scenarios.
Furthermore, these demonstrations are not scalable. If we change the number of agents or introduce
new types of agents, we will need to gather a new set of demonstrations to learn from.
In contrast, it is much easier to obtain demonstrations for individual agents, or even better, for each
typeofagentsinaheterogeneoussetting. Wethusaskthefollowingresearchquestion: couldweleverage
individual-wise task demonstrations instead? In this work, we refer to such expert demonstrations
thataddresssingle-agentbehaviorsforpersonalobjectivesaspersonalized demonstrations(Figure
1). Since the personalized demonstrations will not necessarily reflect how the agents can collaborate
and may even conflict with each other in the joint setting, naively mimicking the demonstrations
will not achieve cooperation. Therefore, purely imitation learning-based approaches would not be
effective. Weneedanapproachthatselectivelyutilizessuitablepersonalizedexpertdemonstrationsas
guidance and allows agents to learn to cooperate via collecting reward signals from the environments.
To this end, we present our
algorithm, Personalized Expert-
Guided MARL (PegMARL), Existing Methods utilize Joint Ours utilizes Personalized Single‐Agent
Demonstrationsfor Joint‐Policy Learning Demonstrationsfor Joint‐Policy Learning
which carries out reward-shaping
 Demonstrations need to be recollected if  Agents of the same type can utilize the
as a form of guidance. We imple-
the agent configurations changes same set of demonstrations
ment this via two discriminators.
Thefirst,apersonalizedbehavior
discriminator, evaluates local
state-action pairs, providing pos-
itive incentives for actions that
alignwiththedemonstrationand
negative incentives for divergent
ones. The second, a personalized
Top row: demonstrations; Bottom row: learning tasks Agents of different types Goals
dynamics discriminator, assesses
whether a local state-action Figure 1: Joint demonstrations are costly to collect but offer rich
pair induces a desired change information on collaborative behaviors. Personalized demonstra-
in dynamics similar to that tions are easier to collect, but solely focus on individual agent
observed in the demonstration, goals, so they lack cooperative elements.
adjusting the incentive weight
accordingly. We prove the effectiveness of PegMARL on both discrete gridworld and continuous
multi-agent particle environments (Lowe et al., 2017; Mordatch & Abbeel, 2017). The main
contributions of this paper are as follows:
(1) Our approach is the first to enable utilizing personalized demonstrations for policy learning
in heterogeneous MARL environments, regardless of the number and type of agents involved.
(2) We propose PegMARL that dynamically and selectively reshapes the original reward to aid
exploration. It is worth mentioning that this algorithm is general and compatible with most MARL
policy gradient methods.
(3) We evaluate PegMARL with personalized demonstrations in both discrete and continuous
environments. Our algorithm outperforms state-of-the-art decentralized MARL algorithms, pure
multi-agentimitationlearning, andreward-shapingtechniquesintermsofscalabilityandconvergence
speed, and obtains near-optimal policies even when provided with suboptimal demonstrations.
(4)WeshowcasePegMARL’scapabilitytoalsoleveragejoint demonstrations,regardlessofwhether
they are sampled from co-trained or non-co-trained policies1. Experimental results on the StarCraft
environment (Samvelyan et al. (2019)) demonstrate that PegMARL converges effectively even with
demonstrations from non-co-trained policies.
1“Co-trained policies” refers to policies that have been trained together in the same environment with shared
experiences. WereferthereaderstoSection6.2ofWangetal.(2023)formoreinformation.
22 Related Works
Imitation Learning (IL). IL methods seek to replicate an expert policy from demonstration data
generated by that policy. Behavior Cloning (BC) is a commonly employed IL technique (Pomerleau
(1991); Bojarski et al. (2016)), where the expert policy is estimated through supervised learning on
demonstration data. However, BC is susceptible to compounding errors resulting from distribution
shifts(Rossetal.(2011)). AnotherthreadofILresearchisInverseReinforcementLearning(IRL)(Ng
et al. (2000); Ziebart et al. (2008)), in which the underlying reward function is estimated from the
demonstration data and then used for policy learning. To alleviate the computational overhead
of IRL, Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon (2016)) was introduced,
allowing direct policy learning from observed data without intermediate IRL steps. Song et al.
(2018) extended this approach to introduce Multi-Agent GAIL (MAGAIL), which adapts GAIL
to high-dimensional environments featuring multiple cooperative or competing agents. In general,
IL approaches can rarely perform better than demonstrations. Therefore, they are not directly
suitable for scenarios where only personalized expert demonstrations that do not demonstrate how
to collaborate are available.
Learning from Demonstration (LfD). In contrast to IL methods, LfD aims to leverage demon-
stration data to facilitate learning rather than simply mimicking expert behavior. Existing LfD
works incorporate demonstration data into a replay buffer with a prioritized replay mechanism to
acceleratethelearningprocess. Duetotheoff-policynatureofthedemonstrationdata,mostmethods
are value-based Hester et al. (2018); Vecerik et al. (2017). There has been some recent work where
the demonstrations are used to aid exploration, especially in environments with large state-action
spaces (Kang et al. (2018); Chen & Xu (2022); Rengarajan et al. (2022)). For instance, POfD (Kang
et al. (2018)) learns an implicit reward from the demonstration data using a discriminator and incor-
porates it into the original sparse reward. LOGO (Rengarajan et al. (2022)) uses the demonstration
data to directly guide the policy update: during each update iteration, the algorithm seeks a policy
that closely resembles the behavior policy within a trust region. However, these approaches are
primarily focused on single-agent settings.
In multi-agent settings, Qiu et al. (2022) suggests using demonstrations to pretrain agents through
imitation learning as a warm start, followed by optimization of the pretrained policies using standard
MARL algorithms. Lee & Lee (2019) augment the experience buffer with demonstration trajectories
and gradually decrease the mixing of demonstration samples during training to prevent the learned
policy from being overly influenced by demonstrations. Similar to POfD, DM2 (Wang et al. (2023))
enables agents to enhance their task-specific rewards by training discriminators as well. Each agent
matches toward a target distribution of concurrently sampled trajectories from a joint expert policy
to facilitate coordination. These approaches, however, require joint demonstrations of the same team
configurations, which can be cumbersome (as we mentioned in section 1). Our approach differs in
that we leverage only personalized expert demonstrations to learn a cooperative policy.
3 Preliminaries
WestartbyconsideringaMarkovDecisionProcess(S,A,P,r,γ)foracooperativemulti-agentsetting
with N agents. Here, S denotes the global state across all the agents, which can be decomposed as
the product of N local spaces S as S = S ×S ×···×S . By noting the local state of agent i
i 1 2 N
as s ∈ S , the global state is s = (s ,s ,··· ,s ). Similarly, we define the global action space A
i i 1 2 N
as A=A ×A ×···×A , meaning that for any a∈A, we may write a=(a ,a ,··· ,a ) with
1 2 N 1 2 N
a ∈ A . The transition probability from state s to s′ after taking a joint action a is denoted by
i i
P(s′|s,a)=Q P (s′|s,a). Allagentsshareacommonrewardfunctionr,andγ ∈[0,1]isthediscount
i i i
factor. In this work, we focus on decentralized learning and define the global policy as π (a|s), where
θ
θ ∈Θ are the policy parameters. Specifically, we have θ =(θ ,θ ,··· ,θ ) as the factorized global
1 2 N
policy parameters, and we can write π (a|s)=Q π (a |s) using policy factorization.
θ i θi i
3Objective: The green agent
Personalized Demonstrations Our algorithm learnscooperative policies
opens the door, allowing the red
Nocooperating behaviors are demonstrated guidedby personalized demonstrations
agent to retrieve the key.
Green agent
for the red agent:
MAPPO failed to learn Red agent enters the middle stepsaway
meaningful policies Navigating to the key location room oncethe doorisopen 3 from the green
(the door is open) 2 square for the
red agent to
pass
for the green agent:
Navigating to the green square 1
to open the door Green agent moves on to the 4
(note the door shutsif the green green square to open and hold Red agent navigates to the
due to the sparsenessof the agent moves away) the door goal after the green agent
environmental reward signals steps aside
Figure 2: An example of utilizing personalized demonstrations to learn cooperative multi-
agent policies. To learn successful cooperation, the agents are required not only to imitate the
demonstrations to achieve personal goals but also to learn how to avoid conflicts and collaborate.
We visualize the state visitation frequency of the personalized demonstrations and the joint policies
learned by our algorithm and MAPPO, where a darker color means a higher value. We observe that
the demonstrations guide the agents in exploring the state space more efficiently than in MAPPO.
Personalized Tasks and MDPs. To introduce the notion of personalized demonstration, we
need to extract the individual tasks of each agent from the collective tasks of multiple agents. For
example, in Figure 2, the green agent’s personalized task is to open the door and the red agent’s
personalized task is to reach the key without the other’s presence. We then define Personalized
Markov Decision Processes (PerMDPs) (S ,A ,Q ,r ,γ) for each agent or, more generally, for each
i i i i
type of agents that share the same objective. Here, a type refers to behaviorally identical agents
using the formalism from Bettini et al. (2023), and we provide the PerMDP definition for each agent
for simplicity. We assume that the state space S and action space A of the personalized task T
i i i
are the same as the local state and action spaces of agent i from the joint task. The transition
probability from state s to s′ after taking a action a is represented as Q (s′|s ,a ). By following an
i i i i i i i
arbitrarypolicyπ (a |s )forthispersonalizedtask,wecancollectasetofpersonalizeddemonstrations
i i i
B ={(st,at)}H , where H is the episode horizon. We’d like to emphasize that while we assume the
i i i t=0
personalized tasks and the joint task are conducted in the same environment map, the underlying
transition dynamics are different. Additionally, the joint reward r may not necessarily equal the
summation of rewards r for each personalized task.
i
Occupancy Measures. For a joint policy π =(π ,π ,...,π ), we can write the global state action
1 2 N
occupancy measure λπ(s,a) as
X∞ (cid:16) (cid:12) (cid:17)
λπ(s,a)= γt·P st =s,at =a (cid:12) π
(cid:12)
t=0
and write the corresponding local cumulative state-action occupancy measure as
X∞ (cid:16) (cid:12) (cid:17)
λπ(s ,a )= γt·P st =s ,at =a (cid:12) π
i i i i i i i (cid:12)
t=0
for∀a ∈A ,s ∈S . Aninterestingobservationtonotehereisthatwecanwritethelocaloccupancy
i i i i
measure as the marginalization of the global occupancy measure with respect to all other agents.
Mathematically, it holds that
X X
λπ(s ,a )= λπ(s,a)
i i i
a∈{ai}×A−is∈{si}×S−i
with A =Π A and S =Π S .
−i j̸=i j −i j̸=i j
4Objective: Co-trained Joint Demonstrations Personalized Demonstrations Personalized demos can
agent 𝑎and 𝑏swap Agent behaviors are compatible Agent behaviors can be conflicting elicit both compatibleand
positions conflictingjoint strategies
𝑃𝑎𝑡ℎ 1 𝑏 𝑃𝑎𝑡ℎ 1 𝑏 𝑃𝑎𝑡ℎ 1 𝑏 𝑃𝑎𝑡ℎ 1 𝑏
𝑃𝑎𝑡ℎ 1 𝑏
𝑎 𝑏
𝑃𝑎𝑡ℎ 1 𝑃𝑎𝑡ℎ 2
Danger Danger Danger Danger
Danger
𝑎 𝑏
𝑎 𝑃𝑎𝑡ℎ 2 𝑎 𝑃𝑎𝑡ℎ 2 𝑎 𝑃𝑎𝑡ℎ 2 𝑎 𝑃𝑎𝑡ℎ 2
𝑎 𝑃𝑎𝑡ℎ 2 𝑃𝑎𝑡ℎ 2 𝑃𝑎𝑡ℎ 1
Figure 3: When joint demonstrations are sampled from co-trained policies, the agents’ behaviors
exhibit compatibility. In contrast, personalized demonstrations solely focus on how each agent
achieves its individual goal and lack cooperative elements, potentially leading to conflicts.
4 MARL with Personalized Expert Demonstrations
Now, we are ready to present the main problem we are interested in solving in this work. Assume
each agent i is associated with a personalized task T . We collect one set of expert demonstrations
i
for each agent i or, equivalently, each personalized task T . By letting an expert user perform each
i
personalizedtaskintherespectivepersonalizedMDP,weobtainacollectionofexpertdemonstrations
denoted by {B ,B ,...,B }. We assume that the underlying expert policy associated with B is
E1 E2 EN Ei
π Ei(a i|s i), and λπEi is the occupancy measure following the expert’s policy π
Ei
for agent i.
4.1 Formulating Personalized Expert-Guided MARL
In standard Multi-Agent Reinforcement Learning, agents
aim to discover optimal joint policies that maximize the
Personalized Demonstration
long-term return R(π θ) := ⟨λπθ,r⟩ = N1 PN i=1⟨λπ iθ,r⟩.
Typically, the learning process commences with random
exploration, which is often inefficient due to MARL’s ex-
ponentially growing exploration spaces, especially when
rewards are sparse. Beginning with the intuition of lever- Imitate in multi-agent Env
aging personalized demonstrations as guidance for how
each agent should accomplish their personalized tasks to
Succeed
promote more effective exploration, akin to Kang et al.
(2018)forsingle-agentcases,wecandefinetheobjectivefor
learning from personalized demonstrations in multi-agent
settings as follows:
Fail
m θ∈a Θx N1 XN (cid:16) ⟨λπ iθ,r⟩−ηD JS(cid:0) λπ iθ || λπEi(cid:1)(cid:17) ,
i=1 Figure4: Amotivatingexampleillustrat-
where η is a weighting term balancing the long-term re- ing the imitation of personalized demon-
ward and the personalized policy similarity. The Jensen- strations in a multi-agent environment.
Shannon (JS) Divergence terms enable individual agents The primary technical challenge lies in
to align their actions with their respective personalized the discrepancy between the transition
demonstrations and facilitate the achievement of their spe- dynamics in the personalized MDP and
cific objectives. the local transition dynamics for each
agent in the multi-agent environment.
However,blindlyimitatingthepersonalizeddemonstration
may not always yield favorable outcomes and can even
impede the learning process. Previous works in MARL have predominantly utilized joint demonstra-
tions as guidance. As demonstrated by DM2 (Wang et al. (2023)), trajectories must be sampled from
a co-trained joint expert policy for their joint action-matching objective to converge. The crucial
aspect that joint demonstrations offer, which personalized demonstrations lack, is compatibility.
5To illustrate, consider the example depicted in Figure 3, where two agents aim to swap positions
without entering the danger region. Two sets of optimal joint policies exist for this task: agent a
takes path 1 and agent b takes path 2, or vice versa. When sampling joint demonstrations from a set
of co-trained policies, the agents’ behaviors will be naturally compatible. In contrast, if personalized
demonstrations were provided, agents could indifferently choose between both paths to reach their
goals, potentially resulting in conflicting strategies through naive imitation.
Hence, we pose the following question: How can we leverage the benefits of using personalized
demonstrations to enhance exploration efficiency while circumventing the imitation of conflicting
behaviors that might hinder joint-policy training?
Consider the scenario depicted in Figure 4, where the blue agent seeks to replicate its personalized
demonstration within a multi-agent environment. The blue agent can successfully transition to
the neighboring grid on the right if it’s unoccupied; otherwise, the transition fails. Viewing the
blue agent as the focal agent and the green agent as part of the environment, successful imitation
hinges on the agent’s “local transition dynamics” matching those of the environment where the
personalized demonstrations originated. We define P(s′|s ,a ) as an approximation for the i-th
i i i
agent’s local transition dynamics in the multi-agent environment, which is governed by the true
transition dynamics P(s′|s,a) and the policy π . With this insight, we further refine the objective
θ
function as follows:
m θ∈a Θx N1 XN (cid:18) ⟨λπ iθ,r⟩−ηD JS(cid:16) λˆπ iθ || λˆπEi(cid:17)(cid:19) , (1)
i=1
where λˆ iπθ and λˆ πEi have the same definition as the original λπ iθ and λπEi but with a restricted
domain:
dom =dom ={(s ,a )∈S ×A |D (cid:0) P (s′|s ,a ) || Q (s′|s ,a )(cid:1) ≤ϵ}. (2)
λˆπθ λˆπEi i i i i JS i i i i i i i i
i
where Q(s′|s ,a ) represents the true transition dynamics of the personalized MDP from which
i i i
demonstrations are collected. By adjusting the learning objective in this manner, the occupancy
matching only happens on those state-action pairs that guide us toward the desired next local state
under the current policy.
4.2 Solving Personalized Expert-Guided MARL
Now, we can begin the development of a practical algorithm. By adopting Theorem 2 from Kang
et al. (2018), we can substitute the JS divergence between occupancy measures with
D JS(cid:0) λπ iθ || λπEi(cid:1) ≈s Du ip(cid:16) E πθ[log(1−D i(s i,a i))]+E πEi[log(D i(s i,a i))](cid:17) ,
where D(s ,a ):S ×A →(0,1) is a discriminative classifier to discern if the (s ,a ) pair is from the
i i i i i i
demonstration or the current policy. However, this doesn’t account for the constraint in Equation
(4.1). Therefore, by introducing an indicator function 1(s ,a ) for whether (s ,a ) belongs to the
i i i i
domain of λˆπ iθ and λˆ πEi, we can obtain
D JS(cid:16) λˆ iπθ || λˆπEi(cid:17) ≈s Du ip(cid:16) E πθ(cid:2)1(s i,a i)·log(1−D i(s i,a i))(cid:3)+E
πEi
(cid:2)1(s i,a i)·log(D i(s i,a i))(cid:3)(cid:17) .
Since direct access to the transition distributions is unavailable, verifying whether each (s ,a ) pair
i i
is within the domain is not directly feasible. To address this, we further approximate the indicator
function using another discriminative classifier D (s ,a ,s′) : S ×A ×S → (0,1), estimating
i i i i i i i
the likelihood of a (s ,a ,s′) tuple being from the demonstrations. In essence, it quantifies the
i i i
likelihood that the corresponding (s ,a ) induces a desired change in dynamics as observed in the
i i
demonstrations.
After approximating the inner JS divergence term in Equation (4.1) with two discriminators, we
integrate the discriminative rewards derived from these discriminators into the environmental
6Algorithm 1 Personalized Expert-Guided MARL (PegMARL)
Input: Number of agents N; environment env; personalized expert trajectories B ,...,B ; batch
E1 EN
size M; weight parameters {ηk}.
Initialize: Policies {π }, discriminators {D } and {D }, where i=1,2,...,N.
Output: Learned poθ lii cies {π }. ϕi ϕ¯ i
θi
1: for iteration k =0,1,2,... do
2: Gather trajectories of multi-agent rollouts from env, Bk =ROLLOUT(π,env).
3: for agent i=0,1,...,N −1 do
4: Sample M tuples of (s i,a i,s′ i) from demonstration BEi and Bk;
5: Update personalized behavior discriminator D ϕi:
max(cid:16) E (cid:2)log(1−D (s ,a ))(cid:3)+E (cid:2)logD (s ,a )(cid:3)(cid:17) .
ϕi
Bk ϕi i i BEi ϕi i i
6: Update personalized dynamics discriminator D ϕ¯ i:
(cid:18) h i h i(cid:19)
max E log(1−D (s ,a ,s′)) +E logD (s ,a ,s′) .
ϕ¯
i
Bk ϕ¯ i i i i BEi ϕ¯ i i i i
7 8:
:
E Us pt dim ata ete agt eh ne tr pes oh lia cp ye πd θir .eward as rˆ ik =r−ηkD ϕ¯ i(s i,a i,s′ i)log(1−D ϕi(s i,a i)).
9: end for
10: end for
reward for the outer problem. Specifically, we estimate the reshaped reward rˆ as rˆ = r −
i i
ηD (s ,a ,s′)log(1−D (s ,a )), where ϕ and ϕ¯ are parameters for the two discriminators. The
perϕ s¯ oinai lizei dbi
ehavior
discϕ ri imi inai
torD
evalui atesloci
alstate-actionpairs,providingpositiveincentives
ϕi
for actions that align with the demonstration and negative incentives for divergent ones, while the
personalized dynamics discriminator D assesses if a local state-action pair induces a desired change
in dynamics akin to that observed in tϕ h¯ ie demonstration, adjusting the incentive weight accordingly.
Subsequently,policyoptimizationisconductedbymaximizingthelong-termreturnwiththereshaped
reward. Our PegMARL algorithm, detailed in Algorithm 1, is compatible with any policy gradient
methods, with MAPPO (Yu et al. (2021)) adopted in our implementation.
5 Experiments
In this section, we empirically evaluate the performance of PegMARL, focusing on the following
questions: (1) How does PegMARL, which leverages personalized demonstrations, compare to state-
of-the-art MARL techniques? (2) How does PegMARL scale with an increasing number of agents
and in the case of continuous state-action spaces? (3) How does the sub-optimality of personalized
expert demonstrations affect the performance of PegMARL? and (4) How does PegMARL perform
when trained with joint demonstrations sampled from co-trained or non-co-trained expert policies?
5.1 Main Results with Personalized Demonstrations
We evaluate PegMARL with personalized demonstrations on both discrete gridworld environments
(Figure 5) and a continuous multi-agent particle environment (Figure 8a). Our comparison includes
fourstrongbaselines: MAPPO(Yuetal.(2021)),aleadingdecentralizedMARLalgorithm;MAGAIL
(Song et al. (2018)), a state-of-the-art multi-agent imitation learning algorithm; DM2 (Wang et al.
(2023)), which combines distribution matching reward with environmental reward; and ATA (She
et al. (2022)), one of the best multi-agent reward-shaping methods. Notably, MAGAIL and DM2
were not originally designed for personalized demonstrations. Hence, we have adapted them for use
in personalized demonstration settings through necessary modifications.
7easy hard
2 agents 3 agents 4 agents
(b) The door scenario: the agents are heteroge-
(a)Thelavascenario: theagentsarehomoge- neous, the assistant agent (green) must reach the
neous, aiming to reach corresponding diagonal greensquareandremaintheretoholdthedooropen,
positions without entering the lava. allowing the other agent (red) to reach the goal.
Figure 5: The discrete gridworld environments. Circles indicate the starting locations of the agents,
while squares of the same color denote their respective goal locations. More details can be found in
Appendix A.1.
PegMARL PegMARL* DM2 DM2* MAGAIL MAGAIL* ATA MAPPO Oracle
2 agents 3 agents 4 agents
10 10 10
8 8 8
6 6 6
4 4 4
2 2 2
0
0 -10 -10 0
-10 -20 -20
-20 -30 -- 43 00
-40 -50
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
Number of Timesteps 1e6 Number of Timesteps 1e6 Number of Timesteps 1e6
Figure 6: Learning curves of PegMARL versus other baseline methods under the lava scenario.
PegMARL converges to higher rewards and generalizes better to larger numbers of agents. The star
symbols (*) in the legend indicate that suboptimal personalized demonstrations are adopted.
easy hard
10 10
8 8
6
6 4
4 2
0
2 -2
0
0 1 2 3 4 5 0 1 2 3 4 5
Number of Timesteps 1e6 Number of Timesteps 1e6
Figure 7: Learning curves of PegMARL versus other baseline methods under the door scenario.
PegMARL shows better robustness in terms of convergence and generalizability.
For each scenario, we collect two sets of personalized demonstrations for each agent: optimal and
suboptimal. The average episodic rewards of suboptimal demonstrations are approximately half of
their optimal counterparts (more details can be found in Appendix B). We execute each method in
every environment with 10 distinct random initializations and plot the mean and variance across all
runs. Oracle denotes the best possible return achievable with optimal policies.
How does PegMARL scale with an increasing number of agents? The lava scenario (Figure
5a) contains three variations, each involving different quantities of agents and escalating levels of
complexity. The agents receive a positive reward only upon reaching the goal, incur penalties for
collisions, and instantly die and terminate the episodes if they step into the lava, making learning in
this environment challenging. As we observe from Figure 6, MAPPO struggles to develop meaningful
behavior across all scenarios due to the sparse reward structure. MAGAIL performs worse, primarily
due to the absence of environmental reward signals. The naive imitation of personalized agent
demonstrations leads to frequent collisions among agents. While ATA can learn suboptimal policies
8
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE20
40
60
80
PegMARL
DM2
100
MAPPO
0.0 0.5 1.0 1.5
Number of Timesteps 1e7
(a) The modified cooperative navigation scenario:
(b)LearningcurvesofPegMARLversusMAPPO
the agents need to navigate around the wall to
and DM2 under the modified cooperative naviga-
occupy both landmarks.
tion scenario
Figure 8: (a) The modified cooperative navigation scenario. (b) The learning curves demonstrate
that PegMARL is effective in continuous environments.
inthe2-agentsetting, itslearningefficacydeclinesdrasticallyasthenumberofagentsincreases. This
underscores the challenges of training as the complexity of the multi-agent environment increases.
In contrast, PegMARL demonstrates superior generalizability across scenarios with more agents,
maintaining stable performance despite suboptimal demonstrations. As DM2 has been adapted to
accommodate personalized demonstrations, it closely resembles PegMARL but lacks the personalized
dynamicsdiscriminatorcomponent. Asthenumberofagentsincreases,wewouldexpectthenumberof
waysforagentstointeracttoincrease,andthereforeforthepersonaldemonstrationstobepotentially
misleading. Consequently, DM2 experiences a decrease in convergence speed as the number of agents
increases. This highlights the importance of the personalized dynamics discriminator in PegMARL,
enablingeffectivehandlingofinter-agentinteractionsinpersonalizeddemonstrationscenarios,thereby
ensuring its efficacy across varying agent counts.
How does PegMARL perform under the heterogeneous setting? The door scenario (Figure
5b) contains two variants with varying levels of difficulty. The easy case is fairly straightforward:
success should be achievable by adhering to personalized demonstrations, which illustrate how each
agent navigates to their respective goal locations. As shown in Figure 7, most algorithms, except for
MAGAIL, showcase proficient performance. Notably, PegMARL exhibits the swiftest convergence.
We attribute MAGAIL’s failure in this case to its inability to direct the green agent to remain
positioned at the green square — a behavior not explicitly demonstrated. This underscores the
importance of environmental reward signals when integrating personalized demonstrations into multi-
agent learning paradigms. The hard case necessitates a higher degree of agent cooperation: once the
red agent gains entry to the middle room, the green agent must move aside from the green square to
enable the red agent’s passage into the right room. In this complex setting, only PegMARL and
DM2 demonstrate commendable convergence. PegMARL, equipped with the personalized dynamics
discriminator, maintains faster convergence compared to DM2.
How does PegMARL perform in the continuous setting? We modified the cooperative
navigation task from the multi-agent particle environment (Lowe et al. (2017); Mordatch & Abbeel
(2017)) to evaluate the performance of our algorithm in a continuous environment (more details
can be found in Appendix A.2). Figure 8b demonstrates the learning curves of PegMARL versus
MAPPO and DM2. This validates the ability of PegMARL in continuous environments.
5.2 Results with Joint Demonstrations
While PegMARL is primarily designed to leverage personalized demonstrations, it can also be
extended to utilize joint demonstrations. Unlike other MARL algorithms like DM2 (Wang et al.
(2023)) that require compatible joint demonstrations sampled from co-trained policies to achieve
convergence, PegMARL has the flexibility to utilize joint demonstrations even from non-co-trained
9
sdraweR
edosipEPegMARL DM2 demo PegMARL DM2 demo
PegMARL(diff) DM2(diff) PegMARL(diff) DM2(diff)
5m_vs_6m 3s_vs_4z
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7 Timesteps 1e7
Figure 9: Learning curves of PegMARL versus DM2 in two tasks under the SMAC scenarios. The
suffix“diff”inthelegendindicatesthatthejointdemonstrationsusedaresampledfromnon-co-trained
policies. Otherwise, the demonstrations are sampled from co-trained policies.
policies. To demonstrate, we conduct comparative experiments within the StarCraft Multi-Agent
Challenge (SMAC) environment (Samvelyan et al. (2019)). We employed the same two tasks that
were used in DM2: 5mv6m, which involves 5 Marines (allies) against 6 Marines (enemies), and 3sv4z,
which features 3 Stalkers (allies) versus 4 Zealots (enemies). The win rates of the demonstrations in
both cases are approximately 30%.
How does PegMARL scale with joint demonstrations? Figure9presentsthelearningcurvesof
PegMARLalongsidetheDM2baselineinboththe5mv6mand3sv4zconfigurationswithintheSMAC
environment. When the joint demonstrations are sampled from co-trained policies, PegMARL
achieves a comparable, and in some cases even higher, success rate compared to DM2 in both tasks.
When the joint demonstrations are sampled from policies that are non-co-trained, the success
rate of DM2 exhibits a significant decline in both tasks. In the 5mv6m task, DM2’s success rate
drops to nearly 0, indicating a failure to learn. In contrast, PegMARL maintains a similar level
of performance compared to results from co-trained demonstrations, with only a slight decrease
in convergence speed. These findings underscore the versatility and effectiveness of PegMARL in
scenarios requiring collaboration among diverse agents, demonstrating its robustness across various
multi-agent environments.
6 Conclusions
In this work, we introduce PegMARL, a novel approach for Multi-Agent Reinforcement Learning,
which adopts personalized expert demonstrations as guidance and allows agents to learn to cooperate.
Specifically,thealgorithmutilizestwodiscriminatorstodynamicallyreshapetherewardfunction: one
providesincentivestoencouragethealignmentbetweenpolicybehaviorwithprovideddemonstrations,
and the other regulates incentives based on whether the behavior leads to the desired objective. We
demonstrate PegMARL’s effectiveness, scalability, and robustness in both discrete and continuous
environments, where it outperforms state-of-the-art decentralized MARL algorithms, pure imitation
learning, and reward-shaping techniques. We observe that PegMARL can achieve near-optimal
policies even with suboptimal demonstrations. Furthermore, We showcase PegMARL’s capability to
leverage joint demonstrations and converge successfully, regardless of whether they are sampled from
co-trained or non-co-trained policies.
References
Matteo Bettini, Ajay Shankar, and Amanda Prorok. Heterogeneous multi-robot reinforcement
learning. arXiv preprint arXiv:2301.07137, 2023.
10
etaR
noW
elttaB
naeM
etaR
noW
elttaB
naeMMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning
for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.
Jie Chen and Wenjun Xu. Policy gradient from demonstration and curiosity. IEEE Transactions on
Cybernetics, 2022.
Tim Franzmeyer, Mateusz Malinowski, and Joao F. Henriques. Learning altruistic behaviours
in reinforcement learning without external rewards. In International Conference on Learning
Representations, 2022. URL https://openreview.net/forum?id=KxbhdyiPHE.
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan,
John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural
information processing systems, 29, 2016.
Bingyi Kang, Zequn Jie, and Jiashi Feng. Policy optimization with demonstrations. In International
conference on machine learning, pp. 2469–2478. PMLR, 2018.
Hyun-Rok Lee and Taesik Lee. Improved cooperative multi-agent reinforcement learning algo-
rithm augmented by mixing demonstrations from centralized policy. In Proceedings of the 18th
International Conference on Autonomous Agents and MultiAgent Systems, pp. 1089–1098, 2019.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. Neural Information Processing Systems
(NIPS), 2017.
Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent
populations. arXiv preprint arXiv:1703.04908, 2017.
Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml,
volume 1, pp. 2, 2000.
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural
computation, 3(1):88–97, 1991.
Yunbo Qiu, Yuzhu Zhan, Yue Jin, Jian Wang, and Xudong Zhang. Sample-efficient multi-agent
reinforcement learning with demonstrations for flocking control. arXiv preprint arXiv:2209.08351,
2022.
Desik Rengarajan, Gargi Vaidya, Akshay Sarvesh, Dileep Kalathil, and Srinivas Shakkottai. Rein-
forcement learning with sparse rewards using guidance from offline demonstration. In Interna-
tional Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=
YJ1WzgMVsMt.
StéphaneRoss,GeoffreyGordon,andDrewBagnell. Areductionofimitationlearningandstructured
prediction to no-regret online learning. In Proceedings of the fourteenth international conference
on artificial intelligence and statistics, pp. 627–635. JMLR Workshop and Conference Proceedings,
2011.
MikayelSamvelyan,TabishRashid,ChristianSchroederDeWitt,GregoryFarquhar,NantasNardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.
Jennifer She, Jayesh K Gupta, and Mykel J Kochenderfer. Agent-time attention for sparse rewards
multi-agent reinforcement learning. arXiv preprint arXiv:2210.17540, 2022.
11Jiaming Song, Hongyu Ren, Dorsa Sadigh, and Stefano Ermon. Multi-agent generative adversarial
imitation learning. Advances in neural information processing systems, 31, 2018.
Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess,
Thomas Rothörl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep
reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817,
2017.
Caroline Wang, Ishan Durugkar, Elad Liebman, and Peter Stone. Dm2: Decentralized multi-agent
reinforcement learning via distribution matching. 2023.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pp. 1433–1438. Chicago, IL, USA, 2008.
12A Environment Details
A.1 Discrete Gridworld Environments
We use two gridworld environments with discrete state and action space: the lava scenario and the
door scenario (Figure 5). The agents in the lava scenario are homogeneous because they have the
same objective: navigating to their corresponding goal location. The door scenario is heterogeneous:
the assistant agent (green) must open the door while the other agent (red) must reach the goal.
• The lava scenario: This environment has a 6-by-6 lava pond in the center (Figure 5a). We
provide three variations of this scenario, each with varying numbers of agents and increasing
levels of complexity. The main goal for the agents is to efficiently navigate to their assigned goal
locations while avoiding stepping into the lava. An episode terminates when all agents reach their
respective goals (succeed), or if any agents step into the lava or the maximum episode length is
reached (fail).
• The door scenario: This environment is adapted from Franzmeyer et al. (2022) (see Figure
5b). In this scenario, the green agent must navigate to a designated green square and maintain its
presence there to sustain the open state of the green door, thereby enabling the entry of a red
agent into the right side room. An episode ends when the red agent reaches the red goal location
(succeed) or the maximum episode length is reached (fail).
Eachagent’slocalstatespaceisits{x,y}coordinatesinthemap. Weconcatenateallthelocalstates
together to form the global state and assume all agents have access to the global state, which has a
dimension of Rn×2 (n is the agent number). The local action space includes five actions: left, right,
up, down, and stay. A sparse reward is granted when an episode succeeds, while a small penalty
will be subtracted according to the steps taken (10−step_count/max_step). Agents will receive a
penalty of −1 if they collide with each other.
A.2 Cooperative Navigation
We modified the cooperative navigation task of multi-agent particle environment Lowe et al. (2017);
Mordatch&Abbeel(2017)toevaluatetheperformanceofouralgorithminacontinuousenvironment.
The modified environment consists of 2 agents, 2 goal landmarks and 1 wall between the agents
and the goals. The agents need to navigate around the wall to occupy both landmarks. The state
definition is the same as the original. We sparsify their original reward as follows:
X X
r = min(0.3,mind(a,g))+ min(0.3,mind(a,g)).
g∈G a∈A
a∈A g∈G
B Additional Experiments Details and Visualizations
Figure10showsanexampleofpersonalizeddemonstrationsforthelavascenario. Table1summarizes
the details of the suboptimal demonstrations, whose average episodic rewards are around 4.5 and
about half of their optimal counterparts.
We provide visual representations of the policy and state occupancy measure corresponding to
suboptimal demonstrations in Figure 11 for the lava scenarios and in Figure 12 for the door scenario.
The red square symbolizes the agent’s initial position in these visualizations, while the green square
designates its respective goal location. Arrows within the figures denote available actions at each
state, with arrow length indicating the probability associated with each action.
We additionally depict the state visitation frequencies of the joint policies learned by PegMARL with
suboptimal demonstrations and MAPPO for both the lava (Figure 13) and the door scenario (Figure
14).
13Goal AgentId S A Samples AverageEpisodicReward
1 R2 5 300 4.42
2 R2 5 440 4.79
3 R2 5 344 4.3
4 R2 5 360 4.41
Start (a) The lava scenario
Case Agent S A Samples Average Episodic Reward
Figure 10: An example of person- red R2 5 643 4.01
easy
alized demonstrations for the lava green R2 5 612 4.39
scenario (we did not visualize all red R2 5 607 4.34
hard
the optimal paths). There is only green R2 5 593 4.36
one agent in the environment. The
(b) The door scenario
agent may take both the left and
the right path toward the goal. Table 1: The details of suboptimal demonstrations.
(a) Agent 1 (b) Agent 2
(c) Agent 3 (d) Agent 4
Figure11: Agentpoliciesandstateoccupancymeasuresestimatedfromthesuboptimaldemonstrations
for the lava scenario.
(a) The easy case (b) The hard case
Figure12: Agentpoliciesandstateoccupancymeasuresestimatedfromthesuboptimaldemonstrations
for the door scenario. The top row is for the red agent, and the bottom row is for the green agent.
14PegMARL MAPPO PegMARL MAPPO PegMARL MAPPO
(a) 2-agent case (b) 3-agent case (c) 4-agent case
Figure 13: State visitation frequency of the joint policies learned by PegMARL (with suboptimal
demonstrations)andMAPPOforthelavascenario. Thedarkercolormeansahighervalue. MAPPO
failed to learn any meaningful policies in all three settings.
PegMARL MAPPO PegMARL MAPPO
(a) The easy case (b) The hard case
Figure 14: State visitation frequency of the joint policies learned by PegMARL (with suboptimal
demonstrations) and MAPPO for the door scenario. The darker color means a higher value.
MAPPO converges to a suboptimal policy in the easy case, while failing to learn in the hard case.
15