GroupContrast: Semantic-aware Self-supervised Representation Learning
for 3D Understanding
ChengyaoWang1 LiJiang3 XiaoyangWu2 ZhuotaoTian4
BohaoPeng1 HengshuangZhao2* JiayaJia1,4
1CUHK 2HKU 3CUHK(SZ) 4SmartMore
https://github.com/dvlab-research/GroupContrast
Abstract
Scene
Self-supervised3Drepresentationlearningaimstolearn
effective representations from large-scale unlabeled point
clouds. Most existing approaches adopt point discrimina-
CSC
tion as the pretext task, which assigns matched points in
two distinct views as positive pairs and unmatched points
as negative pairs. However, this approach often results in
GC (ours)
semantically identical points having dissimilar represen-
tations, leading to a high number of false negatives and
introducing a “semantic conflict” problem. To address
Figure1. Visualizationofactivationmapsdepictingcosinesimi-
this issue, we propose GroupContrast, a novel approach
laritytothequerypoint(indicatedbyayellowcross)inthescene.
that combines segment grouping and semantic-aware con- Ourapproachdemonstratessuperioreffectivenessindiscriminat-
trastive learning. Segment grouping partitions points into ingsemanticallysimilarpointscomparedtoCSC[21].
semanticallymeaningfulregions,whichenhancessemantic
coherence and provides semantic guidance for the subse-
quentcontrastiverepresentationlearning. Semantic-aware supervisedlearning,wheremodelsaretrainedfromscratch
contrastivelearningaugmentsthesemanticinformationex- onspecificdatasetsandtasks.
tracted from segment grouping and helps to alleviate the Recent studies [21, 44, 48] have explored the applica-
issueof“semanticconflict”.Weconductedextensiveexper- tion of point discrimination as a pretext task for 3D self-
iments on multiple 3D scene understanding tasks. The re- supervisedrepresentationlearning.Theseapproachestrans-
sults demonstrate that GroupContrast learns semantically form each scene into two distinct views. They consider
meaningful representations and achieves promising trans- matchedpointsbetweenthetwoviewsaspositivepairsand
ferlearningperformance. unmatched points as negative pairs. Despite decent per-
formance gains observed in downstream tasks fine-tuning,
thereisasignificantissueinpreviouspointdiscrimination
works: theyfocusonthegeometricandstructuralrelation-
1.Introduction
ships,disregardingtheinherentsemanticcorrelations.Con-
Self-supervisedvisualrepresentationlearningaimstolearn sequently, theystruggletogeneratesimilarrepresentations
effective representations from large-scale unlabeled data. for semantically similar points within the 3D scene. To
The learned representation can boost large amounts of illustrate this issue, we visualize the activation map of a
downstream applications, such as object detection and se- previous point discrimination model, CSC [21], as shown
mantic segmentation. Despite self-supervised learning has in Figure 1. The visualization reveals that previous meth-
achieved remarkable results in 2D dense prediction tasks, ods fail to effectively capture semantic similarity. High-
3Drepresentationlearningremainsanemergingfield. The response points are scattered throughout the scene despite
predominant approaches for 3D scene recognition rely on not necessarily possessing semantic similarity with query
points. Conversely, points that are semantically similar to
*Correspondingauthor. thequerypointmayexhibitlowcorrelations.
4202
raM
41
]VC.sc[
1v93690.3042:viXraThis observation motivates us to improve the point dis- Net [11] and 30.0% mIoU on ScanNet200 [33] semantic
crimination pretext task. Merely designating unmatched segmentation using a SparseUNet [9] pre-trained by our
points as negative pairs in the pretext task may result in a method. These results outperform current state-of-the-art
high number of false negatives. This is because elements self-supervised3Drepresentationlearningapproaches.The
thatshouldbesemanticallyidenticalarecompelledtohave contributionofourworkcanbesummarizedasfollows:
dissimilar representations, which we refer to as the “se- • We examine the representations generated by the cur-
manticconflict”. Asaconsequence,theconflictmaycom- rent unsupervised point cloud representation learning
promise the semantic consistency that is crucial for down- method and observe the presence of semantic conflict,
stream dense prediction tasks where different individuals whichcanpotentiallyimpedetheperformanceofdown-
are required to be assigned to their corresponding seman- streamapplications.
ticlabels. Toaddressthisissue,wepresentGroupContrast • We propose GroupContrast, consisting of Segment
whichconsistsoftwoessentialparts: 1)SegmentGrouping GroupingandSemantic-awareContrastiveLearning,to
and2)Semantic-AwareContrastiveLearning. address the semantic conflict by preserving the cross-
view geometric consistency while avoiding negative
Segment grouping aims to enhance the semantic coher-
pairswithsimilarsemantics.
ence among points within a scene and provide semantic
• ExtensiveexperimentsdemonstratethatGroupContrast
guidanceforthefollowingcontrastivelearning. Itachieves
achievesstate-of-the-arttransferlearningresultsinvar-
this by partitioning the point cloud into semantically simi-
ious3Dsceneperceptiontasks.
lar groups via a segment-level deep clustering process. In
particular,wefirstgenerateinitialsegmentsviaagraphcut
2.RelatedWork
methodbasedonlow-levelgeometricinformation[13]and
getthesegmentfeaturesviasegment-wisepooling. Weem-
2Dself-supervisedrepresentationlearning. Instancedis-
ploy a set of learnable prototypes as cluster centers. Cor-
crimination [12] as a pretext task for self-supervised vi-
relationsbetweensegmentrepresentationsandtheseproto-
sualrepresentationlearninghasmaderemarkableprogress
types are then computed, and an informative-aware distil-
in recent years. By leveraging InfoNCE loss [27] as an
lationlossisappliedtoencourageconsistencybetweenthe
optimization objective for contrastive representation learn-
segment-prototype correlations across two views with dif-
ing, a number of studies [5, 7, 18] have shown impressive
ferent augmentations. Segment grouping holds significant
transfer learning performance. More recently, modern ap-
potentialforeffectivelygroupingsemanticallysimilarseg-
proaches[2,3,6,8,16,28]furtherimprovethisparadigm
ments,therebyservingasarobustfoundationforadvancing
by removing negative pairs. Despite the impressive trans-
pointdiscriminationandaddressing“semanticconflict”.
fer learning performance on image classification tasks, in-
Semantic-aware contrastive learning. Based on the re- stancediscriminationtreatsanimageasawhole,migrating
sultsofsegmentgrouping,wecanimprovethepretexttask complexstructuresinnaturalimages. Toaddressthisissue,
ofpointdiscriminationbyintegratingthepositivepairsob- some studies explore pixel discrimination [25, 39, 49] and
tainedwithinthesamegroupandthenegativepairsderived objectdiscrimination[19,20,41,42,47]asapretexttask,
fromdifferentgroups. Thisapproachhelpstoalleviatethe whichenhancestheintrinsicstructureoftheimageandfur-
issue of “semantic conflict” by ensuring that the elements ther improves the transfer learning performance on dense
innegativepairshavedistinctgeometricrepresentationsin predictiontasks. Inthiswork,weattempttoconductvisual
therepresentationspace. AnInfoNCEloss[27]isthenap- representationlearningoncomplicated3Dscenes,whichis
plied to aggregate positive pairs and scatter negative pairs morecorrelatedwiththislineofwork.
intherepresentationspace. Besides,theconfidenceweight 3Dself-superviserepresentationlearning. Unlikethe2D
isfoundconducivetocontrastivelearningbymitigatingthe counterpart, self-supervised representation learning on 3D
adverse impacts of incorrect segment assignments yielded point clouds is still an emerging area. Earlier works [17,
bysegmentgrouping. 34,35,40]conductself-supervisedrepresentationlearning
AsshownintheactivationmapdepictedinFigure1,our onobject-centricpointclouds[4].Experimentally,theseap-
method effectively recognizes semantically similar points proachesareunabletobenefit3Dsceneunderstanding[48].
inthesceneforthequerypoint,incontrasttotheconfusion Recent works [21, 22, 44, 48, 50, 52, 56] start to build
observedintheCSCmodel[21].Thishighlightstheemerg- self-supervised3Drepresentationlearningonscene-centric
ing capacity of GroupContrast in semantic-level recogni- data [11] and found sufficient performance improvement
tion. Extensiveexperimentson3Dsemanticsegmentation, when transferred to a diverse set of 3D scene perception
instancesegmentation,andobjectdetectiondemonstratethe tasks. As a pioneer work, PointContrast [48] adopts point
promisingtransferlearningperformanceofGroupContrast. discriminationasapretexttask.CSC[21]explorepointdis-
Forinstance,ourapproachachieves75.7%mIoUonScan- criminationwithscenecontextdescriptors.MSC[44]intro-teacher Θ
Segment
aug1 𝑔
Θ Grouping
𝑓
Θ
ℎ
Θ assign
view 𝑉
𝑘
𝑆
Θ
EMA EMA
point cloud 𝑋 student 𝜃 𝑆 𝜃 group labels 𝑌෠
prototypes
𝑔
𝜃
𝑓 Semantic-aware
aug2 𝜃
ℎ ℎ′ Contrastive
𝜃 𝜃
encoder Learning
view 𝑉 𝑞 projectors predictor
Figure2.OverviewofourproposedGroupContrastframework.Ourframeworkusestwoneuralnetworks,eachcomprisingabackbone
andtwoprojectorsforsegmentgroupingandcontrastivelearning. Theparametersoftheteachernetworkareupdatedasanexponential
movingaverage(EMA)oftheparametersofthestudentnetwork. Thestudentnetworkincludesanadditionalasymmetricpredictorfor
contrastivelearning.TheSegmentGroupingmoduleassignseachpointtooneofnprototypes,andthisclusteringresultservesasaguide
foreffectivecontrastiverepresentationlearning.
ducesmaskedreconstructionlearningtoenforcethepretext points. GroupContrast consists of two key components:
and alleviate the mode collapse problem. However, these SegmentGroupingandSemantic-awareContrastiveLearn-
approaches treat matched points as positive pairs and un- ing. Firstly, we present the overall framework of our
matchedpointsasnegativepairs,leadingtoalargenumber GroupContrast in Section 3.1. Then, we delve into Seg-
of false negatives. In contrast, we attempt to discover se- mentGroupinginSection3.2,whichenablesthediscovery
mantic meaningful regions to avoid the model being con- of semantic meaningful regions in unlabeled point clouds.
fused by false negatives. Cluster3Dseg [14] group points Following that, we introduce Semantic-aware Contrastive
with identical labels into subclass and learn a better repre- Learning in Section 3.3, which leverages the regions dis-
sentationspaceviacontrastivelearning. Buttheyfocuson coveredinSegmentGroupingforeffectivecontrastiverep-
supervisedlearning,whilewefocusonunsupervisedrepre- resentationlearning.
sentationlearning.
3Dsceneunderstanding. Therearetwoprimaryarchitec- 3.1.OverallFramework
tures for 3D scene understanding: point-based and voxel-
InGroupContrast,weemployadual-networkstructure,in-
based methods. Point-based methods [30, 31, 43, 45, 51,
cludingateachernetworkandastudentnetwork,toensure
53–55] directly operate on the points, making them well-
a stable and consistent contrastive learning process. As il-
suited for learning point clouds. However, directly oper-
lustrated in Figure 2, the student network θ consists of an
ating on the points makes this line of work computation-
encoder f , two projectors g and h , an asymmetric pre-
allyexpensive. Incontrast,voxel-basedmethodstransform θ θ θ
dictorh′,andasetofnlearnableprototypesS ∈ Rn×D,
pointsintoregularvoxelstoapply3Dconvolutions[26,37]. θ θ
whereD indicatesthefeaturedimension. Theteachernet-
Drivenbyhighlyoptimizedsparseconvolution[9,15],this
workΘsharesthesamearchitectureasthestudentnetwork,
line of work achieves excellent efficiency. Following pre-
exceptfortheasymmetricpredictionlayerh′. Theteacher
viousworkson3Drepresentationlearning[44,46,48],we θ
networkhasadifferentsetofparameters,whichareformed
conduct representation learning and downstream task fine-
bytakingtheexponentialmovingaverage(EMA)ofthestu-
tuningonavoxel-basedmethodSparseUNet[9].
dentnetworkθ.
Given a point cloud X, two augmented views V and
3.Method k
V derived from X are fed into the teacher network and
q
In this section, we introduce a novel method, GroupCon- thestudentnetwork,respectively. Then,wetakethepoint-
trast, for 3D self-supervised representation learning to en- levelfeaturesproducedbytheprojectorsg andhasthein-
hance the feature alignment among semantically similar putsfortheSegmentGroupingmoduleandSemantic-awareScene Geometry Segment Group Results Ground Truth
view𝑉𝑘 segments 𝑆Θ
EMA Distillation
𝑆𝜃
informative
weight 𝐻
view 𝑉𝑞 segments
Figure3.SegmentGroupingisoptimizedbydistillingtheassign-
mentscoresbetweeneachsegmentandthenprototypesfromthe
teachernetworktothestudentnetwork. Aninformativeweightis Figure 4. The result of Segment Grouping. We compare the
employedtomakethestudentnetworkfocusonmorechallenging groupingresultswithoriginalgeometrysegments[13]andseman-
segments. ticgroundtruth. Segmentgroupingeffectivelygroupspointsinto
semanticallymeaningfulregionswithouthumansupervision.
ContrastiveLearningmodule,respectively. IntheSegment
Groupingprocess, asetoflearnableprototypesS areused segment grouping. A cross-entropy loss is applied to en-
as cluster centers for identifying the meaningful semantic couragetheassignmentscoresofthestudentnetworkcon-
groups within the 3D scene. These groups are then em- sistentwiththeteachernetwork. Theoverallgroupingloss
ployedintheSemantic-awareContrastiveLearningmodule iscomputedastheaveragecross-entropylossacrossP seg-
tomitigatethesemanticconflictproblemandassisttherep- ments:
1 (cid:88)
resentationlearning. Lgroup =− K ·log(Q ). (2)
P i i
i∈[0,P)
3.2.SegmentGrouping
Preventionofcollapse.Directlyapplyingthisoptimization
The Segment Grouping module is illustrated in Figure 3.
objectivewillleadtocollapse[3].InspiredbyDINO[3],we
We first utilize geometric information of the points (e.g.,
apply centring and sharpening for the momentum teacher
normal) to generate P segments for the overlapped region
outputstoavoidmodelcollapse. Forsharpening,wemake
of the two augmented views V and V using a graph cut
q k
the teacher temperature τ lower than student temperature
method [13]. Then, segment-wise average pooling is ap- t
τ toproduceasharpertargettoavoiduniformassignments.
pliedonthel2-normalizedpoint-levelfeaturesproducedby s
Forcentring,weuseabiastermctotheteacherandreduce
the projector g for both augmented views, resulting in the
it from the prototypes when producing the teacher assign-
segment-levelfeaturesz ∈ RP×D andz ∈ RP×D. Af-
q k
ments. ThebiastermcisformedbytakingtheEMAofthe
ter that, we calculate the prototype assignment scores for
outputproducedbytheteachernetwork:
each segment by measuring the cosine similarity between
the segment-level features and the n learnable prototypes.
1 (cid:88)
Specifically,withsegment-levelfeaturesz q,z kandnlearn- c=λ c·c+(1−λ c)·
P
z k[i]S ΘT, (3)
ableprototypesS θ,S Θ whicharealll2-normalized,theas- i∈[0,P)
signmentscoresQ ∈ RP×n andK ∈ RP×n forsegments
where P stands for the number of segments, and λ refers
ineachviewcanbewrittenas c
to the momentum value. Intuitively, centring prevents one
prototype from dominating the prototype assignment pro-
Q=softmax(z ST/τ ), K =softmax(z (S −c)T/τ ). cess.
q θ s k Θ t
n n
(1) Informative-aware distillation. The approach described
Here, the temperature parameters τ and τ control the above can be regarded as a knowledge distillation proce-
t s
sharpnessoftheoutputdistributionfortheteachernetwork dure from teacher network Θ to student network θ. How-
andstudentnetwork,respectively. Additionally,abiasterm ever,treatingallsegmentsequallyindistillationcanleadto
cisintroducedtoavoidcollapse,whichwillbefurtherdis- the model overlooking more informative segments. These
cussedlater. segments are typically more difficult to assign prototypes,
The teacher network is an average of consecutive stu- i.e., with higher entropy, and should be paid with extra at-
dentnetworks.Averagingmodelweightsovertrainingsteps tention during distillation. Therefore, we use the entropy
tends to produce a more accurate model [29, 38]. We can ofteacherassignmentscoresK tomeasureeachsegment’s
take advantage of this to set the optimization objective of “amountofinformation”andincorporatetheentropymaskview𝑉
onthesemanticgroupingresultYˆ.Pointsinthesamegroup
𝑘
aresetaspositivepairs,whilepointsindifferentgroupsare
treatedasnegativepairs. Formally, forthetwoaugmented
viewsV andV ,wesampleN pointsfromtheiroverlapped
q k
refer group labels 𝑌෠ region and set the point indices of these sampled points in
InfoNCE Yˆ as I q ∈ RN and I k ∈ RN, respectively. The positive
pairsetisthendefinedas
P ={(i,j)|i∈I ,j ∈I ,Yˆ =Yˆ }. (6)
q k i j
Confidenceweight 𝐶
Confidence-awarelearning.Intheearlystagesoftraining,
view 𝑉 𝑞 thegrouplabelsYˆ maynotalwaysbereliable. Usingnoisy
group labels for contrastive representation learning may
Figure5.ContrastiveLearning.WeuseanInfoNCEloss[27]to confuse the model. Therefore, we evaluate the confidence
aggregatepointswithinthesamegroupandscatterpointsacross
of each positive pair and incorporate confidence weights
different groups, as indicated by the Segment Grouping result.
in contrastive representation learning, to alleviate the ad-
Here the red point in view V serves as a query, the red points
q verseeffectsbroughtbytheuncertainelements.Concretely,
in view V are positive samples, and the blue points in view V
k k weleveragetheteacherassignmentscoresK inconfidence
arenegativesamples. Bothmodulesareconductedonoverlapped
evaluation.Foreachpositivepairi,jwithgroupinglabelk,
regionsofthetwoaugmentedviewsonly, whicharehighlighted
withdarkercolorsinthefigure. theconfidenceweightC i,j iscalculatedas
C =K ×K , (7)
i,j si,k sj,k
todistillationloss. TheentropymaskH foreachsegmenti
canbeconcludedasH = −(cid:80)n Kj ·log(Kj),andthe
wheres iands
j
indicatethesegmentindicesofpointsiand
i j=1 i i j,respectively.
groupinglossinEq.2canbeupdatedas
Improved contrastive loss. For the teacher network, we
1 (cid:88) add a projector h Θ after the encoder to extract feature
Lgroup =− (cid:80)
H
H i·K i·log(Q i). (4) v k. For the student network, inspired by previous ap-
i∈[0,P) i i∈[0,P) proaches [3, 5, 16], a projector h together with an extra
θ
asymmetricpredictorh′ isappliedafterencodertoextract
θ
Grouping result. With the assignment scores, we group featurev . Bothv andv arel2-normalizedforcontrastive
q q k
the P segments into n clusters by assigning each segment representationlearning. TheInfoNCEloss[27]isadopted
totheprototypewiththehighestassignmentscore. Weuse toaggregatepositivepairsandscatternegativepairsinthe
theteacherassignmentscoresKtoextractgroupingresults. representationspace. Byincorporatingtheaforementioned
Formally, the segment-level grouping result Yˆ seg ∈ RP is confidenceweightC i,j,givenasetofpositivepairsP anda
calculatedas temperatureparameterτ,theimprovedcontrastivelosscan
Yˆ seg =argmaxK. (5) bewrittenas
n
WethenprojectYˆ toeachpointtoobtainthepoint-level
grouplabelsYˆ forse tg heoverlappedregionoftwoviews. As Lcon = |P1
|
(cid:88) −C i,j·log exp(vi·vj/e τx )p +(v qi (cid:80)·v kj/ eτ x) p(vi·vk/τ).
illustratedinFigure4,oursegmentgroupingprocesseffec- i,j∈P q k q k
i,k∈/P
tivelygroupsinitialsegmentsintosemanticallymeaningful (8)
regions. Wesetτ to0.4,followingpreviousapproaches[21,48].
3.3.Semantic-awareContrastiveLearning 3.4.OverallOptimizationObjective
As discussed in Section 1, the issue of “semantic conflict” Wejointlyoptimizesegmentgroupingandcontrastiverep-
existsinpreviouscontrastive-basedrepresentationlearning resentationlearningforpre-training. Theoveralloptimiza-
methodswherethesemanticallyidenticalelementsmayer- tionobjectiveisaweightedsumofEq.4andEq.8,which
roneouslyhavedistinctrepresentations. Toaddressthisis- canbewrittenas
sue for achieving a better agreement between the semanti-
callysimilarpoints,weusethegrouplabelsYˆ astheseman- Loverall =λ gLgroup+λ cLcon, (9)
ticguidancetoenhancecontrastiverepresentationlearning. whereλ andλ arescalefactors. Weempiricallysetλ =
g c g
Semantic-awarepositivepairs. AsillustratedinFigure5, λ = 1,asourexperimentssuggestthattheperformanceis
c
we define the positive pairs for contrastive learning based robusttodifferentscalefactors.4.Experiments No Centering
No Centering No Sharpening GroupContrast
No Sharpening
Weconductextensiveexperimentstovalidatetheeffective-
ness of our proposed GroupContrast framework. We first
perform ablation studies in Section 4.1 to demonstrate the
efficacy of each proposed component, then compare our
approach with previous state-of-the-art self-supervised 3D
representationlearningapproachesinSection4.2.
4.1.MainProperties low high
To assess the effectiveness and analyze the key properties Figure6.Grouplabels(top)andConfidenceweight(down)gen-
of our GroupContrast, we conduct ablation experiments eratedfromthepre-trainedmodelwithoutcentringand/orsharpen-
on its core design choices. As a default setting, we first ing.Withoutcentring,pointsaregroupedintooneortworegions.
Withoutsharpening,theassignmentscoresbecomeuniformvec-
self-supervisedpre-trainaSparseUNet[9]onScanNet[11]
tors,leadingtolowconfidenceweight.
dataset for 600 epochs. We then utilize ScanNet seman-
tic segmentation as the downstream task and evaluate the
performance using the mIoU (%) metric. The results of
isrobusttothenumberofsampledpoints. Fortrainingeffi-
ablation studies are concluded in Table 1. Please refer to
ciency,wesample2048pointsforeach3Dscene.
thesupplementarymaterialformoreimplementationdetails
Informative-awaredistillation. InTable1d,westudythe
aboutthepre-trainingandfine-tuning.
effect of informative-aware distillation. Introducing infor-
Positive pair construction. In Table 1a, we compare the
mative weight prevents the model from overlooking more
semantic-awarepositivepairsgeneratedbasedonSegment
informative segments during distillation, leading to better
Grouping results with several baselines to validate the ef-
transferlearningperformance.
fectivenessofSegmentGroupingoncontrastiverepresenta-
Teachertemperatureτ .InTable1e,westudythetemper-
tionlearning. Thebaselineapproachesinclude(1)Matched t
atureparameterinSegmentGrouping. Wefollowprevious
Points, which uses matched points in two views as posi-
work[3]tosetstudenttemperatureτ =0.1andablatedif-
tive pairs and unmatched points as negative pairs, similar s
ferent values of teacher temperature τ . The results show
to PointContrast[48]; (2) Spatial Grid, which divides the t
thatasofterteacherleadstobettertransferlearningperfor-
point cloud into multiple spatial grids and assigns points
mance.
within the same grid as positive pairs. In this case, we
set the grid size to 1m×1m; and (3) Geometry Segment, Predictor. In Table 1f, we study the effect of the asym-
which generates segments using a normal-based graph cut metric predictor. Similar to previous 2D self-supervised
method[13]andassignspointswithinthesamesegmentas representationlearningapproaches[3,5,16,42],introduc-
positive pairs. As shown, simply assigning points in the inganasymmetricpredictormakesthecontrastiveobjective
same spatial grid as positive pairs leads to a decrease in more challenging, resulting in better transfer learning per-
transferlearningperformance,evenworsethantheMatched formance.
Points baseline. Although Geometry Segments incorpo- Semantic-awarecontrastiverepresentationlearning. In
rate geometric priors into the network, the improvement Table1g,westudythedesignofsemantic-awarecontrastive
in transfer learning is only marginal. By employing Seg- representation learning. Introducing semantic-aware posi-
ment Grouping based on Geometry Segments, we observe tive pairs for contrastive learning helps alleviate the issue
anoteworthyimprovementof1.1pointsintransferlearning of“semanticconflict”andresultsinbettertransferlearning
performancecomparedtotheMatchedPointsbaseline, af- performance. Moreover, incorporating confidence weight
firmingtheeffectivenessofourproposedsegmentgrouping foreachpositivepairalleviatestheadverseeffectsbrought
approach. by the uncertain elements, further improving the transfer
Numberofprototypes. InTable1b,weinvestigatetheim- learningperformance.
pact of the number of prototypes n in Segment Grouping. Collapseproblem. InTable1h,westudytheeffectiveness
Our observations indicate that n = 32 yields the best per-
ofcentringandsharpeninginavoidingcollapse. Asshown
formanceforScanNetpre-training. Toofewprototypescan
in the table, both centring and sharpening effectively im-
lead to excessive feature aggregation, while too many can
prove the transfer learning performance on semantic seg-
causethemodeltolearnoverlyfine-grainedfeatures.
mentation. Furthermore, tofurtherverifytheeffectiveness
Numberofsampledpoints.InTable1c,westudythenum- ofcenteringandsharpening, wevisualizethegroupingre-
ber of points sampled from the overlapped region for con- sults and corresponding assignment scores for GroupCon-
trastiverepresentationlearning. Weobservethatthemodel trastwithoutsharpeningandcentringinFigure6. WithoutPositivePairs FTmIoU(%) Prototypesn FTmIoU(%) SamplePoints FTmIoU(%)
MatchedPoints 74.6 16 75.2 1024 75.5
SpatialGrid 74.2 32 75.7 2048 75.7
GeometrySegment 74.8 64 75.3 4096 75.5
SegmentGrouping 75.7 128 74.8 8192 75.4
(a)PositivePairs. Positivepairsconstructedbasedon (b) Number of prototypes. 32 prototypes (c) Number of sampled points. Our ap-
SegmentGroupingworkbestindownstreamtransfer. workbestforSegmentGrouping. proachisrobusttonumberofsampledpoints.
Infomative-aware FTmIoU(%) Temperatureτt FTmIoU(%) Predictor FTmIoU(%)
75.4 0.04 75.3 75.3
✓ 75.7 0.07 75.7 ✓ 75.7
(d) Informative-aware. Incorporating informative (e)Teachertemperature. Asofterteacher (f)Predictor.Includingasymmetricpredic-
weightfordistillationboostsdownstreamperformance. leadstobetterdownstreamperformance. torboostsdownstreamperformance.
Semantic-aware Confidence-aware FTmIoU(%) Centering Sharpening FTmIoU(%) Epochs ScanNet ScanNet(20%)
72.7
74.8 300 74.8 65.0
✓ 73.1
✓ 75.1 ✓ 74.1 600 75.7 65.8
✓ ✓ 75.7 ✓ ✓ 75.7 1200 75.7 66.5
(g) Semantic-aware Contrastive Learning. Incor- (h)AvoidCollapse. Incorporatingbothcen- (i) Pre-training epochs. Scaling up the
porating semantic-aware positive pairs and confidence teringandsharpeninghelpsourapproachto number of pre-train epochs makes the
weightscanimprovedownstreamperformance. alleviatingthecollapseproblem. modelmoredata-efficient.
Table1. AblationStudy. Withoutfurtherexplanation,wepre-trainaSparseUNet[9]for600epochsonScanNet[11]datasettoanalyse
ourmaindesignchoicesandproperties. Wereportfine-tuningmIoUresults(%)onScanNetSemanticSegmentation. Defaultsettingsare
markedin gray.
centring,allpointsinascenearegroupedintotworegions, the backbone and adopt a longer training schedule (1200
resultinginmultiplefalsepositivesforcontrastiverepresen- epochs). ThetransferlearningresultsareconcludedinTa-
tationlearning. Withoutsharpening,theassignmentscores ble2. Pleaserefertosupplementarymaterialsformoreim-
Q and K become uniform vectors, resulting in low confi- plementationdetailsaboutdownstreamtaskfine-tuning.
dencescoresandnoisygrouplabels,whichmakeithardfor Semanticsegmentation.InTable2a,wereporttheSeman-
contrastivelosstoconverge. Theresultisevenworsewith- ticSegmentationresultsonScanNet[11],ScanNet200[33]
out both techniques, where all points are grouped into an and S3DIS [1] benchmark with a SparseUNet backbone.
identical region, and the corresponding confidence scores Forin-domaintransferlearningwherepre-trainingandfine-
foreachpointarealsolow. tuning are both conducted on the ScanNet [11] dataset,
Pre-training epochs. In Table 1i, we study the number we achieve 75.7% mIoU on the ScanNet validation set
ofpre-trainingepochs. Resultsindicatethatincreasingthe and 30.0% mIoU on the ScanNet200 validation set, out-
pre-trainingepochfrom600to1200doesnotenhanceper- performingthecurrentstate-of-the-artapproachesby0.7%
formancewhenfine-tuningthefullScanNettrainingsetfor mIoU and 1.2% mIoU, respectively. Moreover, the model
semantic segmentation. However, in a data-efficient sce- pre-trained with our approach on ScanNet achieves 72.0%
nario where only 20% of reconstructed point clouds are mIoU when transferred to S3DIS semantic segmentation,
used for fine-tuning, scaling up the pre-training epoch ef- demonstrating that GroupContrast is also effective for
fectivelyimprovesperformance. cross-domaintransferlearning.
Instancesegmentation.InTable2b,wereporttheinstance
4.2.ResultsComparison
segmentationresultsonScanNet[11],ScanNet200[33]and
In this section, we evaluate the effectiveness of our pro- S3DIS[1]benchmarkwithPointGroup[23]astheinstance
posed GroupContrast by comparing it with previous self- segmentation head and SparseUNet as the backbone. We
supervised 3D representation learning approaches [21, 44, stillfindconsistenttransferlearningperformanceimprove-
48]. Experiments are conducted on various downstream mentcomparedwithpreviousresults. Specifically, ourap-
tasks, including 3D semantic segmentation, instance seg- proachachieves62.3mAP@0.5ontheScanNetvalidation
mentation, and object detection. Additionally, we evalu- set,whichis2.7pointshigherthanthepreviousstate-of-the-
ate the data efficiency of GroupContrast on data-efficient art 3D self-supervised pre-training method. Furthermore,
3D semantic segmentation. We apply SparseUNet [9] as our approach achieves 27.5 mAP@0.5 when fine-tuningSemanticSegmentation(mIoU) LR SemanticSegmentation(mIoU)
Datasets
SC PC[48] CSC[21] MSC[44] GC(ours) Pct. SC CSC[21] MSC[44] GC(ours)
ScanNet 72.2 74.1 73.8 75.3 75.7 100% 72.2 73.8 75.0 75.7
ScanNet200 25.0 26.2 26.4 28.8 30.0 1% 26.1 28.9 29.2 30.7
S3DIS 68.2 70.3 72.2 - 72.0 5% 47.8 49.8 50.7 52.9
10% 56.7 59.4 61.0 62.0
(a)SemanticSegmentation. WereportthemIoU(%)resultsonScanNet,
20% 62.9 64.6 64.9 66.5
ScanNet200,andS3DISbenchmarks.
(a)LimitedReconstruction.WecomparethemIoU(%)resultsonScanNet
InstanceSegmentation(mAP@0.5)
Datasets dataefficientsemanticsegmentationbenchmarkwithlimitedscenerecon-
SC PC[48] CSC[21] MSC[44] GC(ours) structionsetting.
ScanNet 56.9 58.0 59.4 59.6 62.3 LA SemanticSegmentation(mIoU)
ScanNet200 24.5 24.9 25.2 26.8 27.5
Pct. SC CSC[21] MSC[44] GC(ours)
S3DIS 59.3 60.5 63.4 - 63.5
Full 72.2 73.8 75.0 75.7
(b)InstanceSegmentation. WereportthemAP@0.5resultsonScanNet,
20 41.9 55.5 61.2 61.2
ScanNet200,andS3DISbenchmarks.
50 53.9 60.5 66.8 67.3
ObjectDetection(mAP@0.5) 100 62.2 65.9 69.7 70.3
Datasets
200 65.5 68.2 70.7 71.8
SC PC[48] CSC[21] MSC[44] GC(ours)
(b)LimitedAnnotation. WecomparethemIoU(%)resultsonScanNet
ScanNet 35.2 38.0 39.3 - 41.1
dataefficientsemanticsegmentationbenchmarkwithlimitedpointannota-
SUNRGB-D 31.7 34.8 36.4 - 37.0 tionsetting.
(c)ObjectDetection.WereportthemAP@0.5resultsonScanNetandSUN
RGB-Dbenchmarks. Table 3. Data Efficiency. We evaluate the data efficiency of
GroupContrast on ScanNet data efficient semantic segmentation
Table2. Resultscomparisonon3DSemanticSegmentation,In- benchmark.Themodelispre-trainedonScanNetpointcloudwith
stanceSegmentationandObjectDetection. Wepre-trainourap- SparseUNet[9] as the backbone. SC denotes train from scratch.
proachonScanNetpointcloudwithSparseUNet[9]astheback- Ourresultsaremarkedin gray.
bone for transfer learning performance comparison. SC denotes
trainfromscratch.Ourresultsaremarkedin gray.
5.Conclusion
on ScanNet200instance segmentationand 63.5 mAP@0.5
This work presents GroupContrast, a self-supervised rep-
when fine-tuning on S3DIS instance segmentation, both
resentation learning framework for 3D scene understand-
outperformingprevious3Dself-supervisedpre-trainingap-
ing, with joint segment grouping and semantic-aware con-
proaches.
trastivelearning. Segmentgroupingdiscoverssemantically
Objectdetection. InTable2c,wereporttheObjectDetec- meaningfulregionsbyassigningeachsegmentaprototype.
tion results on ScanNet [11] and SUN-RGBD [36] bench- Basedonthegroupingresult, acontrastivelearningobjec-
mark with VoteNet [32] as the detection head and Sparse- tiveisthenappliedtoproduceasemantic-awarerepresenta-
UNet as the backbone. As shown in the table, we can tionspace.Ourapproachcaneffectivelydecomposeapoint
alsofindperformanceimprovementinobjectdetectionfine- cloud into multiple semantically meaningful regions with-
tuning. We can achieve 41.1 mAP@0.5 on the Scan- outsupervision,showingtheemergingabilityinsemantic-
Net validation set and 37.0 mAP@0.5 on the SUN-RGBD levelrecognition. Moreover,extensiveexperimentalresults
validation set, surpassing the previous state-of-the-art 3D demonstrate that our approach achieves promising transfer
self-supervisedpre-trainingapproachby1.8pointsand0.6 learning performance on various 3D scene understanding
pointsrespectively. tasks, such as 3D semantic segmentation, object detection
Dataefficiency.Apartfromfulldatasetfine-tuning,wealso andinstancesegmentation.
evaluate the data efficiency of our approach on the Scan- WhileourGroupContrastbringsgreatbenefitstodown-
NetDataEfficientSemanticSegmentationbenchmark. We stream tasks through ScanNet pre-training, it is currently
use the same data split as CSC [21] in both limited scene limited by the relatively small scale of the pre-training
reconstruction and limited points annotation settings. We dataset.Asadirectionforfutureresearch,weaimtoexplore
reportthedataefficientsemanticsegmentationresultinTa- cross-datasetpre-trainingtoenlargethepre-trainingdataset
ble 3. As reported, our approach achieves state-of-the-art sizeandcollaborateourframeworkwithwell-trainedvisual
performance for all cases in both settings. These results foundationmodels. Theseeffortsareexpectedtoovercome
suggest that our proposed approach is effective in improv- thislimitationandenhancethegeneralizabilityandrobust-
ingthedataefficiencyof3Dsceneunderstanding. nessofourframework.References Maaten. 3dsemanticsegmentationwithsubmanifoldsparse
convolutionalnetworks. InCVPR,2018. 3
[1] IroArmeni,OzanSener,AmirR.Zamir,HelenJiang,Ioan-
[16] Jean-Bastien Grill, Florian Strub, Florent Altche´, Corentin
nisBrilakis,MartinFischer,andSilvioSavarese. 3dseman-
Tallec,PierreRichemond,ElenaBuchatskaya,CarlDoersch,
ticparsingoflarge-scaleindoorspaces. InCVPR,2016. 7
Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
[2] MathildeCaron,IshanMisra,JulienMairal,PriyaGoyal,Pi-
laghiAzar,etal. Bootstrapyourownlatent-anewapproach
otrBojanowski,andArmandJoulin. Unsupervisedlearning
toself-supervisedlearning. Advancesinneuralinformation
of visual features by contrasting cluster assignments. Ad-
processingsystems,33:21271–21284,2020. 2,5,6
vancesinneuralinformationprocessingsystems, 33:9912–
[17] Kaveh Hassani and Mike Haley. Unsupervised multi-task
9924,2020. 2
featurelearningonpointclouds. InICCV,2019. 2
[3] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou,
[18] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRoss
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg-
Girshick. Momentumcontrastforunsupervisedvisualrep-
ingpropertiesinself-supervisedvisiontransformers.InPro-
resentationlearning. InCVPR,2020. 2
ceedingsoftheIEEE/CVFinternationalconferenceoncom-
[19] Olivier J He´naff, Skanda Koppula, Jean-Baptiste Alayrac,
putervision,pages9650–9660,2021. 2,4,5,6
Aaron Van den Oord, Oriol Vinyals, and Joao Carreira.
[4] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
Efficient visual pretraining with contrastive detection. In
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
ProceedingsoftheIEEE/CVFInternationalConferenceon
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
ComputerVision,pages10086–10096,2021. 2
An information-rich 3d model repository. arXiv preprint
[20] OlivierJHe´naff,SkandaKoppula,EvanShelhamer,Daniel
arXiv:1512.03012,2015. 2
Zoran, Andrew Jaegle, Andrew Zisserman, Joa˜o Carreira,
[5] TingChen,SimonKornblith,MohammadNorouzi,andGe-
andReljaArandjelovic´.Objectdiscoveryandrepresentation
offreyHinton. Asimpleframeworkforcontrastivelearning
networks. InComputerVision–ECCV2022: 17thEuropean
ofvisualrepresentations. InICML,2020. 2,5,6
Conference,TelAviv,Israel,October23–27,2022,Proceed-
[6] XinleiChenandKaimingHe.Exploringsimplesiameserep- ings,PartXXVII,pages123–143.Springer,2022. 2
resentationlearning. InProceedingsoftheIEEE/CVFcon-
[21] Ji Hou, Benjamin Graham, Matthias Nießner, and Saining
ference on computer vision and pattern recognition, pages
Xie. Exploring data-efficient 3d scene understanding with
15750–15758,2021. 2
contrastivescenecontexts. InCVPR,2021. 1,2,5,7,8,11
[7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
[22] SiyuanHuang,YichenXie,Song-ChunZhu,andYixinZhu.
Improved baselines with momentum contrastive learning.
Spatio-temporal self-supervised representation learning for
arXivpreprintarXiv:2003.04297,2020. 2
3d point clouds. In Proceedings of the IEEE/CVF Inter-
[8] X Chen, S Xie, and K He. An empirical study of training nationalConferenceonComputerVision,pages6535–6545,
self-supervised vision transformers. in 2021 ieee. In CVF 2021. 2
InternationalConferenceonComputerVision(ICCV),pages
[23] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-
9620–9629,2021. 2
WingFu,andJiayaJia.Pointgroup:Dual-setpointgrouping
[9] ChristopherChoy,JunYoungGwak,andSilvioSavarese. 4d for3dinstancesegmentation. CVPR,2020. 7,11
spatio-temporal convnets: Minkowski convolutional neural [24] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
networks. InCVPR,2019. 2,3,6,7,8,11 ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
[10] Pointcept Contributors. Pointcept: A codebase for point head,AlexanderCBerg,Wan-YenLo,etal. Segmentany-
cloudperceptionresearch,2023. 11 thing. InProceedingsoftheIEEE/CVFInternationalCon-
[11] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ferenceonComputerVision,pages4015–4026,2023. 11
ber, Thomas Funkhouser, and Matthias Nießner. Scannet: [25] Songtao Liu, Zeming Li, and Jian Sun. Self-emd: Self-
Richly-annotated 3d reconstructions of indoor scenes. In supervisedobjectdetectionwithoutimagenet.arXivpreprint
ProceedingsoftheIEEEconferenceoncomputervisionand arXiv:2011.13677,2020. 2
patternrecognition,pages5828–5839,2017. 2,6,7,8,11 [26] DanielMaturanaandSebastianScherer. Voxnet: A3dcon-
[12] AlexeyDosovitskiy,JostTobiasSpringenberg,MartinRied- volutionalneuralnetworkforreal-timeobjectrecognition.In
miller,andThomasBrox. Discriminativeunsupervisedfea- IROS,2015. 3
turelearningwithconvolutionalneuralnetworks. Advances [27] AaronvandenOord, YazheLi, andOriolVinyals. Repre-
inneuralinformationprocessingsystems,27,2014. 2 sentationlearningwithcontrastivepredictivecoding. arXiv
[13] PedroFFelzenszwalbandDanielPHuttenlocher. Efficient preprintarXiv:1807.03748,2018. 2,5
graph-based image segmentation. International journal of [28] Maxime Oquab, Timothe´e Darcet, The´o Moutakanni, Huy
computervision,59:167–181,2004. 2,4,6,11 Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
[14] Tuo Feng, Wenguan Wang, Xiaohan Wang, Yi Yang, and DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal.
QinghuaZheng.Clusteringbasedpointcloudrepresentation Dinov2:Learningrobustvisualfeatureswithoutsupervision.
learning for 3d analysis. In Proceedings of the IEEE/CVF arXivpreprintarXiv:2304.07193,2023. 2
InternationalConferenceonComputerVision,pages8283– [29] Boris T Polyak and Anatoli B Juditsky. Acceleration of
8294,2023. 3 stochastic approximation by averaging. SIAM journal on
[15] Benjamin Graham, Martin Engelcke, and Laurens van der controlandoptimization,30(4):838–855,1992. 4[30] CharlesRQi,HaoSu,KaichunMo,andLeonidasJGuibas. [46] XiaoyangWu,ZhuotaoTian,XinWen,BohaoPeng,Xihui
Pointnet: Deep learning on point sets for 3d classification Liu, Kaicheng Yu, and Hengshuang Zhao. Towards large-
andsegmentation. InCVPR,2017. 3 scale 3d representation learning with multi-dataset point
[31] CharlesRQi,LiYi,HaoSu,andLeonidasJGuibas. Point- prompttraining. InCVPR,2024. 3
net++: Deephierarchicalfeaturelearningonpointsetsina [47] JiahaoXie,XiaohangZhan,ZiweiLiu,YewSoonOng,and
metricspace. InNeurIPS,2017. 3 ChenChangeLoy. Unsupervisedobject-levelrepresentation
[32] Charles R Qi, Or Litany, Kaiming He, and Leonidas J learningfromsceneimages.AdvancesinNeuralInformation
Guibas. Deephoughvotingfor3dobjectdetectioninpoint ProcessingSystems,34:28864–28876,2021. 2
clouds.InProceedingsoftheIEEEInternationalConference [48] SainingXie,JiataoGu,DemiGuo,CharlesRQi,Leonidas
onComputerVision,2019. 8,11 Guibas, and Or Litany. Pointcontrast: Unsupervised pre-
[33] DavidRozenberszki,OrLitany,andAngelaDai. Language- trainingfor3dpointcloudunderstanding. InECCV,2020.
grounded indoor 3d semantic segmentation in the wild. In 1,2,3,5,6,7,8,11
ECCV,2022. 2,7 [49] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen
[34] Aditya Sanghi. Info3d: Representation learning on 3d ob- Lin,andHanHu. Propagateyourself: Exploringpixel-level
jectsusingmutualinformationmaximizationandcontrastive consistencyforunsupervisedvisualrepresentationlearning.
learning. InECCV,2020. 2 In Proceedings of the IEEE/CVF Conference on Computer
[35] Jonathan Sauder and Bjarne Sievers. Self-supervised deep VisionandPatternRecognition,pages16684–16693,2021.
learning on point clouds by reconstructing space. In 2
NeurIPS,2019. 2 [50] HonghuiYang,ShaZhang,DiHuang,XiaoyangWu,Haoyi
[36] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Zhu,TongHe,ShixiangTang,HengshuangZhao,QiboQiu,
Sunrgb-d:Argb-dsceneunderstandingbenchmarksuite.In Binbin Lin, Xiaofei He, and Wanli Ouyang. Unipad: A
ProceedingsoftheIEEEconferenceoncomputervisionand universalpre-trainingparadigmforautonomousdriving. In
patternrecognition,pages567–576,2015. 8 CVPR,2024. 2
[37] ShuranSong,FisherYu,AndyZeng,AngelXChang,Mano- [51] Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao,
lisSavva,andThomasFunkhouser.Semanticscenecomple- andXihuiLiu.Sam3d:Segmentanythingin3dscenes.arXiv
tionfromasingledepthimage. InCVPR,2017. 3 preprintarXiv:2306.03908,2023. 3,11
[38] AnttiTarvainenandHarriValpola. Meanteachersarebetter [52] Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan
role models: Weight-averaged consistency targets improve Misra. Self-supervised pretraining of 3d features on any
semi-supervised deep learning results. Advances in neural point-cloud. InProceedingsoftheIEEE/CVFInternational
informationprocessingsystems,30,2017. 4 ConferenceonComputerVision,pages10252–10263,2021.
2
[39] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong,
and Lei Li. Dense contrastive learning for self-supervised [53] Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia.
visual pre-training. In Proceedings of the IEEE/CVF Con- Pointweb: Enhancinglocalneighborhoodfeaturesforpoint
ferenceonComputerVisionandPatternRecognition,pages cloudprocessing. InCVPR,2019. 3
3024–3033,2021. 2 [54] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and
[40] YueWangandJustinMSolomon.Deepclosestpoint:Learn- VladlenKoltun. Pointtransformer. InICCV,2021.
ing representations for point cloud registration. In ICCV, [55] ZhishengZhong,JiequanCui,YiboYang,XiaoyangWu,Xi-
2019. 2 aojuan Qi, Xiangyu Zhang, and Jiaya Jia. Understanding
[41] FangyunWei,YueGao,ZhirongWu,HanHu,andStephen imbalancedsemanticsegmentationthroughneuralcollapse.
Lin. Aligningpretrainingfordetectionviaobject-levelcon- 2023. 3
trastivelearning. AdvancesinNeuralInformationProcess- [56] Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha
ingSystems,34:22682–22694,2021. 2 Zhang, Xianglong He, Tong He, Hengshuang Zhao, Chun-
[42] XinWen,BingchenZhao,AnlinZheng,XiangyuZhang,and huaShen,YuQiao,andWanliOuyang. Ponderv2: Pavethe
XiaojuanQi. Self-supervisedvisualrepresentationlearning way for 3d foundation model with a universal pre-training
withsemanticgrouping. InAdvancesinNeuralInformation paradigm. arXivpreprintarXiv:2310.08586,2023. 2
ProcessingSystems,2022. 2,6
[43] XiaoyangWu, YixingLao, LiJiang, XihuiLiu, andHeng-
shuangZhao. Pointtransformerv2: Groupedvectoratten-
tionandpartition-basedpooling. InNeurIPS,2022. 3
[44] XiaoyangWu,XinWen,XihuiLiu,andHengshuangZhao.
Maskedscenecontrast: Ascalableframeworkforunsuper-
vised3drepresentationlearning. InCVPR,2023. 1,2,3,7,
8,11
[45] XiaoyangWu,LiJiang,Peng-ShuaiWang,ZhijianLiu,Xi-
huiLiu,YuQiao,WanliOuyang,TongHe,andHengshuang
Zhao. Point transformer v3: Simpler, faster, stronger. In
CVPR,2024. 3Appendix dataforfine-tuning.ForScanNetandScanNet200semantic
segmentation, the model is fine-tuned for 800 epochs with
A.ImplementationDetails
a batch size of 48. For S3DIS semantic segmentation, the
modelisfine-tunedfor3000epochswithabatchsizeof12.
Our implementation is mainly based on Pointcept [10], a
The voxel size is set to 0.02 for ScanNet fine-tuning and
codebasefocusingon3Dsceneunderstandingandrepresen-
0.05 for S3DIS fine-tuning. Please refer to Table 5b and
tationlearning. Theimplementationdetailsonpre-training
Table 5c for more details on semantic segmentation fine-
andfine-tuningarelistedbelow.
tuning. For data-efficient semantic segmentation on Scan-
A.1.Pre-training Net, wefollowthesamesettingasfulldatasetfine-tuning,
asillustratedinTable5b.
Backbone architecture. Following previous self-
Instance segmentation. We use SparseUNet [9] as the
supervisedrepresentationlearningapproaches[21,44,48],
backbone and PointGroup [23] as the segmentation head
we adopt SparseUNet34C [9] as a backbone for ablation
forinstancesegmentationfine-tuning.Experimentsarecon-
studiesandresultcomparisons. Theimplementationdetail
ductedonScanNetv2andS3DIS.ForScanNetv2,wefine-
ofthebackbonearchitectureisthesameasinpreviousap-
tune the model on the training set and report the perfor-
proaches.
manceonthevalidationset. ForS3DIS,wereporttheper-
Pre-training dataset. Following previous work [21, 44,
formanceonArea5anduseotherdataforfine-tuning. For
48], we conduct self-supervised pre-training with Group-
ScanNetandScanNet200instancesegmentation,themodel
ContrastonScanNetv2[11]pointclouddata.
is fine-tuned for 800 epochs with a batch size of 48. For
Data augmentation. We follow MSC [44] to set our S3DIS instance segmentation, the model is fine-tuned for
data augmentation pipeline for all experiments, which in- 3000 epochs with a batch size of 12. The voxel size is set
clude Spatial augmentations, photometric augmentations to 0.02 for ScanNet fine-tuning and 0.05 for S3DIS fine-
and sampling augmentations. The data augmentation tuning. PleaserefertoTable5dandTable5eformorede-
pipelineisillustratedinTable4. tailsoninstancesegmentationfine-tuning.
Pre-trainingsetting. Forablationstudiesexperiments,the
Objectdetection. WeuseSparseUNet[9]asthebackbone
number of default pre-training epochs is 600. For trans-
andVoteNet[32]asthedetectionheadforobjectdetection
ferlearningresultscomparison,thenumberofpre-training
fine-tuning. ExperimentsareconductedonScanNetv2and
epochs is 1200. Please refer to Table 5a for more imple-
SUN-RGBD.Wefine-tunethemodelonthetrainingsetand
mentationdetailsatthepre-trainingstage.
reporttheperformanceonthevalidationset. Wereportthe
transferlearningresultsonScanNetandSUN-RGBDobject
Augmentation Value detection. We fine-tune the model for 360 epochs with a
randomrotate angle=[-1,1],axis=‘z’,p=1 batch size of 64 for both datasets. The voxel size is set to
randomrotate angle=[-1/64,1/64],axis=‘x’,p=1 0.02. Please refer to Table 5f for more details on object
randomrotate angle=[-1/64,1/64],axis=‘y’,p=1 detectionfine-tuning.
randomflip p=0.5
randomcoordjitter sigma=0.005,clip=0.02
B.CollaborationwithFoundationModels
randomcolorbrightnessjitter ratio=0.4,p=0.8
randomcolorcontrastjitter ratio=0.4,p=0.8
We further study the potential of collaborating our work
randomcolorsaturationjitter ratio=0.2,p=0.8
randomcolorhuejitter ratio=0.02,p=0.8 with existing visual foundation models, such as Segment
randomcolorgaussianjitter std=0.05,p=0.95 AnythingModels(SAM)[24]. Recently,thereemergesev-
voxelization voxelsize=0.02 eralworksthatleverageSAMtopredict3Dboundingboxes
randomcrop ratio=0.6 or segmentation masks on point clouds. These segmenta-
tion masks can directly replace the GraphCut [13] results
Table4.Dataaugmentationpipeline.
in Segment Grouping. To assess this possibility, we sub-
stitutetheGraphCutresultswiththesegmentationmaskof
A.2.Fine-tuning SAM3D[51]andvalidateitseffectivenesson3Drepresen-
tation learning. As depicted in Figure 7, Segment Group-
Semantic segmentation. We use a SparseUNet [9] to- ingsuccessfullyclustersbothGraphCutmaskandSAM3D
gether with a projection layer for semantic segmentation mask into proper regions. The mIoU result for ScanNet-
fine-tuning. ExperimentsareconductedonScanNetv2and v2 semantic segmentation fine-tuning is 75.9%, which is
S3DIS.ForScanNetv2,wefine-tunethemodelonthetrain- higherthantheresultthatusingGraphCut(75.7%). Incor-
ingsetandreporttheperformanceonthevalidationset.For porating existing visual foundation models is a promising
S3DIS,wereporttheperformanceonArea5anduseother way to mitigate data scarcity for 3D visual representationConfig Value Config Value Config Value
optimizer SGD optimizer SGD optimizer SGD
scheduler cosine scheduler cosine scheduler cosine
learningrate 0.1 learningrate 0.05 learningrate 0.1
weightdecay 1e-4 weightdecay 1e-4 weightdecay 1e-4
optimizermomentum 0.8 optimizermomentum 0.9 optimizermomentum 0.9
batchsize 32 batchsize 48 batchsize 12
warmupepochs 12 warmupepochs 40 warmupepochs 0
epochs 1200 epochs 800 epochs 3000
(a)Self-supervisedpre-trainingonScanNet (b) Semantic Segmentation fine-tuning on (c) Semantic Segmentation fine-tuning on
ScanNet S3DIS
Config Value Config Value Config Value
optimizer SGD optimizer SGD optimizer SGD
scheduler poly scheduler poly scheduler step
learningrate 0.1 learningrate 0.1 learningrate 1e-3
weightdecay 1e-4 weightdecay 1e-4 weightdecay 0
optimizermomentum 0.9 optimizermomentum 0.9 optimizermomentum 0.9
batchsize 48 batchsize 12 batchsize 64
warmupepochs 0 warmupepochs 0 warmupepochs 0
epochs 800 epochs 3000 epochs 180
(d) Instance Segmentation fine-tuning on (e) Instance Segmentation fine-tuning on (f) Object Detection fine-tuning on ScanNet
ScanNet S3DIS andSUN-RGBD
Table5.Experimentsettings.Welistexperimentsettingsforbothupstreampre-traininganddownstreamfine-tuning.
Scene Graph Cut Graph Cut + Grouping SAM3D SAM3D + Grouping
Figure7.SegmentGroupingiscapableofaggregatingbothGraphCutmaskandSAM3Dmaskintosemanticmeaningfulregions.
learning. Weintendtopursuefurtherinourfutureresearch. modellearnabetterrepresentationspaceandbenefitdown-
streamfine-tuning.
C.PrototypeVisualizationandAnalysis
Weattempttovisualizetheregionsassignedtoeachproto-
typetoanalysewhethertherandomlyinitializedprototypes
canlearnsemanticmeanings. AsillustratedinFigure8,the
modelsuccessfullydiscoverssemanticmeaningfulconcepts
fromunlabeled3Dscenes. Theseconceptsincludeseman-
ticcategoriessuchasfloor, table, ceilingandwall, aswell
asobjectpartslikechairbackrestsandsofabackrests. The
visualization results demonstrate that the prototypes have
effectivelylearnedandcapturedsemanticmeaning.
Sincenosupervisionsignalsareprovided,theresultsof
segment grouping are bound to orthogonal to the seman-
tic labels sometimes. For example, assign points with the
same semantic label to different clusters, or group points
withdifferentsemanticlabelsintoidenticalclusters.Webe-
lievediscoveringsemanticmeaningfulsubcategoriesisnot
harmfulattherepresentationlearningstage. ItcanhelptheFigure8. PrototypeVisualization. Eachrowreferstooneprototype,andthegroupregionsarehighlightedwithaspecificcolor. Our
methodcandiscoversemanticmeaningfulconceptsfromunlabeled3Dscenes.