Scalability of Metropolis-within-Gibbs schemes for
high-dimensional Bayesian models
Filippo Ascolani∗, Gareth O. Roberts† and Giacomo Zanella‡
March 15, 2024
Abstract
Westudygeneralcoordinate-wiseMCMCschemes(suchasMetropolis-within-Gibbssam-
plers), which are commonly used to fit Bayesian non-conjugate hierarchical models. We
relate their convergence properties to the ones of the corresponding (potentially not imple-
mentable) Gibbs sampler through the notion of conditional conductance. This allows us
to study the performances of popular Metropolis-within-Gibbs schemes for non-conjugate
hierarchical models, in high-dimensional regimes where both number of datapoints and pa-
rameters increase. Given random data-generating assumptions, we establish dimension-free
convergence results, which are in close accordance with numerical evidence. Applications
to Bayesian models for binary regression with unknown hyperparameters and discretely ob-
served diffusions are also discussed. Motivated by such statistical applications, auxiliary
results of independent interest on approximate conductances and perturbation of Markov
operators are provided.
1 Introduction
Over30yearsago,theGibbssampler(GS)andmoregeneralcoordinate-wisesamplers(often
termedMetropolis-within-Gibbs(MwG)samplers)wereintroducedaspowerfultoolstoenable
Bayesian inference for structured data (including spatial, hierarchical or temporal models),
forexamplesee[21,63,5,12,10]. Thesubstantialimpactofthesemethodshasspreadacross
a wide variety of scientific fields, for example see [22, 26, 41].
∗Department of Statistical Science, Duke University, filippo.ascolani@duke.edu
†Department of Statistics, University of Warwick, gareth.o.roberts@warwick.ac.uk
‡Department of Decision Sciences and BIDSA, Bocconi University, giacomo.zanella@unibocconi.it
1
4202
raM
41
]OC.tats[
1v61490.3042:viXraCoordinate-wise Markov chain Monte Carlo (MCMC) schemes work by partitioning the
vector of parameters in different blocks and updating them one at the time, conditional on
all the others. Working in such a coordinate-wise manner can be computationally beneficial
in many cases (see e.g. Section 1.1) and it has been observed empirically that such samplers
oftenhaveextremelygoodconvergenceproperties. Howevertheoreticalunderstandingofthis
phenomenon has proved difficult, as the analysis of the resulting algorithms is often subtle
and case-specific. Substantial progress has now been made on understanding the pure GS
(see Section 1.3), but implementation of these algorithms is generally restricted to contexts
withspecialisedconditionalconjugacypropertiesandthereforeitisimportanttounderstand
more general coordinate-wise samplers which remain comparatively under-studied.
Inthisworkwestudygeneralcoordinate-wisesamplers(whichincludeMwGasaspecial
case),withaparticularfocusonrelatingtheirconvergencepropertiestotheonesofexactGS.
A key theoretical result (Corollary 1) states that the performances of a generic coordinate-
wise scheme, measured in terms of the conductance of the associated operator, differ from
the ones of GS by a multiplicative factor, which we explicitly control through the goodness
of the conditional updates. As motivated in the next section, we apply such findings to
high-dimensional non-conjugate Bayesian models, where we provide theoretical justification
for the empirically observed good performances of coordinate-wise samplers.
1.1 Motivating example: non-conjugate hierarchical models
Our motivating example is given by classical and widely used Bayesian hierarchical models
[23, 22], where the observed dataset Y = (Y ) is divided into J groups, each fea-
1:J j j=1,...,J
turing some local (i.e. group-specific) parameters θ . Consider for example a hierarchical
j
logistic model defined as
(cid:18) m(cid:19) eyθj
Y |θ ∼f(y |θ )= j =1,...,J,
j j j y (1+eθj)m
θ |µ,τ i ∼id N(θ |µ,τ−1) j =1,...,J, (1)
j
(µ,τ)∼p (·),
0
with y ∈ {0,...,m} and m being a positive integer. Under model (1), the conditional dis-
tribution of θ given (Y ,µ,τ) factorizes as L(dθ|Y ,µ,τ) = ⊗J L(dθ |Y ,µ,τ), where
1:J 1:J j=1 j j
⊗ denotes the product of independent distributions. This makes model (1) particularly
well-suited for coordinate-wise samplers, since the high-dimensional update of θ given (µ,τ)
decouples into J independent low-dimensional updates (see Section 6.4 for more discussion
on the computational implications of this). Figure 1 compares the efficiency of the resulting
samplerswithtwogradient-basedMCMCmethods. Thetargetisthejointposteriordistribu-
tion L(dθ,dµ,dτ|Y ), and we consider the high-dimensional regime where J →∞, so that
1:J
both the number of datapoints and parameters, i.e. n = Jm and p = Jℓ+D respectively,
diverge. Full details on the simulation set-up, including algorithmic and prior specifications,
are postponed to Section 6.5. Note that L(dθ |Y ,µ,τ) is not available in closed form for
j j
2Figure1: Medianoftheintegratedautocorrelationtimesmultipliedbytheaveragenumberoflikelihood
evaluations per iterations (on log-scale) for four MCMC schemes targeting the posterior distribution of
model(1),asafunctionofJ (numberofgroups). Themedianreferstorepetitionsoverdatasetsrandomly
generated according to the model with true parameters µ∗ =τ∗ =1. See Section 6.5 for more details.
model (1). Exact sampling is possible through an Adaptive Rejection sampler [24], so that
exact GS can still be used, but this is potentially hard to implement and its computational
cost scales badly with the dimensionality of θ . Instead, it is much more common and com-
j
putationally convenient to implement a MwG scheme performing a L(dθ |Y ,µ,τ)-invariant
j j
Metropolis update of θ . Both GS and MwG are implemented in Figure 1.
j
AsFigure1suggests, coordinate-wisesamplerscanprovidestateoftheartperformances
forhierarchicalmodels. Inparticular,bothGSandMwGexhibitdimension-freeconvergence
properties (i.e. the number of iterations per effective sample does not grow with J) and
the slowdown of MwG relative to GS seems to be constant with respect to dimensionality.
Intuitively, we attribute the good empirical performances of coordinate-wise schemes to the
sparse conditional independence structure of hierarchical models, which allows to perform
computationally cheap high-dimensional block updates. This is not a peculiar feature of (1)
but rather a common phenomenon occurring in many Bayesian models [49].
Whilst the phenomenon of good convergence properties for coordinate-based samplers
for hierarchical models has been long recognised, see e.g. [63], there has until recently been
very little theory to explain this phenomenon (see Section 1.3). In this paper we reduce this
gap by proving dimension-free convergence of the mixing times associated to MwG schemes
targeting hierarchical models (see Corollary 2 in Section 6.3).
1.2 Objective and structure of the paper
Themaincontributionsofthepapercanbedividedintwoparts. Inthefirstone,weprovide
bounds on the approximate conductance of a generic coordinate-wise scheme in terms of the
3corresponding quantity for the GS (Corollary 1). Working with the approximate version of
theconductanceiscrucialforourpurposesandsubsequentapplications(seee.g.Remark8).
The general theory naturally applies to MwG schemes, such as those targeting condition-
ally log-concave distributions (Proposition 3). In the second part, we analyze performances
of coordinate-wise samplers for relevant statistical applications, combining the bounds dis-
cussedabovewithspecificmodelproperties,statisticalasymptoticsandsomenovelauxiliary
results on approximate conductances and perturbation of Markov operators. Much em-
phasis is placed on coordinate-wise schemes for generic two-levels hierarchical models with
non-conjugatelikelihood,suchas(1),forwhichweareabletoprovedimension-freebehaviour
oftotalvariationmixingtimes,underwarm(Corollary2)andfeasible(Proposition5)starts.
MwGschemesforBayesianlogisticregressionwithunknownhyper-parametersandinference
on discretely observed diffusions are also discussed in some detail.
Thestructureofthepaperisasfollows. Section2brieflyrecallsthenotionofconductance
and its connection to mixing times of Markov chains. Section 3 introduces the class of
coordinate-wise schemes that we consider (which include random-scan GS and MwG) and
provides general results relating the conductance of coordinate-wise schemes to the one of
GS (Theorem 1 and Corollary 1). Section 4 consider the specific case of MwG schemes and
theexampleofconditionallylog-concavetargets. Afterdiscussingindependentresultsabout
approximate conductances and perturbations of Markov operators (Section 5), we move to
statistical applications, where we consider Bayesian hierarchical models (Section 6), logistic
regression with unknown hyperparameters (Section 7) and inference for discretely observed
diffusionprocesses(Section8). Mathematicalproofs,togetherwithsomeadditionalexamples
and results, are postponed to the Appendix.
1.3 Related literature
Compared to other MCMC schemes (such as gradient-based ones), there are relatively few
quantitativetheoreticalresultsforcoordinate-wisesamplingschemes. Moreover,mostworks
focus on the exact Gibbs sampler, e.g. [62, 1, 59, 36, 32, 70, 51, 29, 52, 55, 13], for which
specific techniques exploiting the exact updating structure and the associated alternating
projectionrepresentationcanbeexploited[58,15,16,3]. Resultsforgeneralcoordinate-wise
MCMC, including MwG, are instead quite limited. Exceptions include [56, 42, 30, 31, 53],
whichmostlyfocusongeometricoruniformergodicity,and[67],whichprovidessomeanalysis
of MALA-within-Gibbs schemes.
While working on our manuscript, it came to our attention that [54] independently and
concurrently developed results relating the spectral Gap of coordinate-wise samplers to the
ones of exact GS. Their results are similar in spirit to the ones we develop in Sections 3
and 4. On the other hand, the specific results and the type of applications are significantly
different. For example, [54] study the spectral gap, while in this work we work with the
approximate version of the conductance defined in (4), which is crucial for the subsequent
4applicationsweconsider(seee.g.Remarks8and11). Also,weconsideranddevelopinsome
detail various statistical applications (Sections 6, 7 and 8) where we combine our techniques
with posterior asymptotics, dimensionality reduction and perturbation arguments.
2 Mixing times and conductance
Let P be a π-invariant Markov transition kernel on X, where π ∈ P(X) and P(X) denotes
the collection of probability measures on a space X. When studying the convergence of a
Markov chain to its invariant distribution, a typical object of interest is given by the total
variation mixing times starting from µ, defined as
t
mix(P,ϵ,µ)=inf(cid:8)
t≥0 :
(cid:13) (cid:13)µPt−π(cid:13)
(cid:13)
TV
<ϵ(cid:9)
, ϵ∈[0,1], µ∈P(X),
wherePt denotesthet-thpowerofP,µPt(A)=(cid:82) Pt(x,A)µ(dx)foranyA⊆X and∥·∥
X TV
denotes the total variation norm. By definition, the mixing times quantify the number of
Markov chain’s iterations required to obtain a sample from the target distribution π up to
errorϵ. Wewillfocusonworst-casemixingtimeswithrespecttoM-warmstarts. Thelatter
are starting distributions defined as
N (π,M)={µ∈P(X) : µ(A)≤Mπ(A) for all A⊆X}, M ≥1, π ∈P(X). (2)
The associated worst-case mixing times for P targeting π are
t (P,ϵ,M)= sup t (P,ϵ,µ). (3)
mix mix
µ∈N(π,M)
Inordertogivequantitativeboundson(3)atypicalstrategyistostudythes-conductance
of P, i.e.
(cid:26) P(∂A) 1 (cid:27) (cid:90)
Φ (P)=inf ; s<π(A)≤ ,A⊂X , P(∂A)= P(x,Ac)π(dx) (4)
s π(A) 2
A
with s∈[0,1/2). If s=0, we write Φ(P):=Φ (P) and call it conductance. Also, P(∂A)
0
is sometimes called the flux of P through A and P(∂A)/π(A) coincides with the probability
thattheMarkovchainexitsAinonestep,giventhatitstartsfromπrestrictedtoA. Itiswell
known(seee.g. Corollary1.5in[39])thatastrictlypositiveconductanceimpliesexponential
convergence of Pt to π and thus a bound on the mixing times. This is summarized in the
following lemma.
Lemma 1. Let P be a π-invariant Markov transition kernel. For every s∈[0,1/2), t≥0,
M ≥1 and µ∈N (π,M), it holds
(cid:13) (cid:13)µPt−π(cid:13)
(cid:13)
≤Ms+M(cid:18)
1−
Φ2 s(P)(cid:19)t
.
TV 2
In particular, if s= ϵ we have
2M
log(2M)−log(ϵ)
t (P,ϵ,M)≤ .
mix (cid:16) (cid:17)
−log 1− Φ2 s(P)
2
5Remark 1. Theusualdefinitionofs-conductance(seee.g. [39,18])isslightlydifferentwith
respect to the one given in (4). Appendix B provides results for the alternative definition.
Definition (4) is less tight but leads to neater formulas (see e.g. Theorem 1).
Remark 2. While being common in the literature, see e.g. [14, 18, 65], the warm start
assumption can be quite stringent especially in a high dimensional context. Thus it is often
of interest to provide mixing times bounds for feasible starts, which can be explicitly sampled
from (e.g. [18]). In Section 6 we provide a feasible start for hierarchical models.
3 Coordinate-wise MCMC
Let X = X ×···×X be a product space. Given x = (x ,...,x ) ∈ X, we denote by
1 d 1 d
x = (x ) ∈ X = × X the vector x without the i-th element and by π (dx | x )
−i j j̸=i −i j̸=i j i i −i
the conditional distribution of x given x under x∼π.
i −i
In this paper we focus on coordinate-wise kernels, defined as
d
P = 1(cid:88) P , P (x,dy)=Px−i(x ,dy )δ (cid:0) dy (cid:1) , (5)
d i i i i i x−i −i
i=1
where Px−i is a π (·|x )-invariant Markov kernel on X and thus, by construction, P is
i i −i i i
invariant with respect to π. At each iteration, a Markov chain evolving according to P
picks uniformly at random a coordinate i in {1,...,d} and then updates x sampling from
i
Px−i(x ,·), while leaving x unchanged. We will sometimes assume each Px−i to be re-
i i −i i
versible, which means π (dx |x )Px−i(x ,dy ) = Px−i(y ,dx )π(dy | x ), and positive
i i −i i i i i i i i −i
semi-definite, which means that (cid:82) f(x )Px−if(x )π (dx |x ) ≥ 0 for every square inte-
Xi i i i i i −i
grable f : X → R, where Px−if(x ) = (cid:82) f(y )Px−i(x ,dy ). The latter are common
i i i i i i i
assumptions which are often satisfied: for example every reversible operator Q can be made
positive semi-definite by considering its lazy version, i.e. Q˜ = 1I + 1Q with I being the
2 2
identity operator [39].
An important special case of (5) is the so-called random scan Gibbs sampler, whose
kernel is defined as G =
1(cid:80)d
G where G is the kernel that performs exact sampling
d i=1 i i
from π (·|x ), i.e.
i −i
G (x,dy)=Gx−i(x ,dy )δ (cid:0) y (cid:1) , Gx−i(x ,dy )=π (dy |x ). (6)
i i i i x−i −i i i i i i −i
In order to relate Φ (P) to Φ (G) for general coordinate-wise kernels P, we introduce
s s
the following notion of conditional conductance of P. We define
κ(P,K)= min κ (P ,K), κ (P ,K)= inf
κ(cid:0) Px−i(cid:1)
K ⊆X (7)
i i i i i
i∈{1,...,d} x∈K
where κ(cid:0) Px−i(cid:1) is the conductance of the kernel Px−i on X , defined as
i i i
(cid:40)(cid:82) Px−i(x ,Bc)π (dx |x ) (cid:41)
κ(cid:0) Px−i(cid:1) =inf B i i i i −i ; π (B |x )∈(0,1),B ⊂X x ∈X .
i π (B |x )π (Bc |x ) i −i i −i −i
i −i i −i
(8)
6The latter measures how much the invariant update of Px−i is close to exact sampling as in
i
Gx−i. In particular, one has κ(cid:0) Gx−i(cid:1) = 1 for all i and x , while κ(cid:0) Px−i(cid:1) ≤ 1 for general
i i −i i
reversibleandpositive-definiteupdates(seee.g.Corollary1below). Thus,κ(P,K)measures
how much the coordinate updates of P are close to exact sampling as in G, over the set K.
By construction, κ(G,K)=1 for all K.
Remark 3. κ (P ,X) should not be confused with the conductance of P on X. Indeed the
i i i
latter is always equal to 0, since P leaves x unchanged. Similarly, κ(P,X) is not the
i −i
conductance of P on X, since it only measures the quality of the conditional updates, not
directly the convergence speed of Pt to π.
Remark 4. Equations (8) and (4) consider two slightly different versions of conductance,
whichdifferinthedenominatoroftheinnerformulas(e.g.π(A)insteadofπ(A)π(Ac)in (4)).
This affects the resulting number of at most a factor of 2 and it allows to avoid unnecessary
constants in the theorems to follows, e.g. Corollary 1.
The next theorem provides a connection between the flux of G and the flux of P, for an
arbitrary coordinate-wise operator P, in terms of the conditional conductance.
Theorem 1. Let P be a π-invariant coordinate-wise kernel as in (5). For every A,K ⊂X
and i=1,...,d we have
P (∂A)≥κ (P ,K)(G (∂A)−π(A∩Kc)) .
i i i i
Moreover,ifPx−i isreversibleandpositivesemi-definiteforeveryx ∈X ,thenG (∂A)≥
i −i −i i
P (∂A).
i
Remark 5. When Px−i is reversible and positive semi-definite, Theorem 1 with K = X
i
implies
G (∂A)≥P (∂A)≥κ (P ,X)G (∂A),
i i i i i
so that κ(P ) controls how much flux is lost when passing from exact to invariant updates on
i
thei-thcoordinate. TheextensiontoagenericK ⊆X allowstoignore“bad”setswhichhave
low probability under π (see also Remark 8). This turns out to be crucial in some statistical
applications, see e.g. Section 6 for the case of hierarchical models.
WecanuseTheorem1toboundΦ (P)intermsofΦ (G),asdetailedinthenextcorollary.
s s
Corollary 1. Let P be a π-invariant coordinate-wise kernel as in (5). Then for every
s∈[0,1/2) and K ⊆X we have
π(Kc)(cid:32) 1(cid:88)d (cid:33)
Φ (P)≥κ(P,X)Φ (G) and Φ (P)≥κ(P,K)Φ (G)− κ (P ,K) , (9)
s s s s s d i i
i=1
with κ(P,K) as in (7). Moreover, if Px−i is reversible and positive semidefinite for every
i
i=1,...,d and x ∈X , then Φ (G)≥Φ (P).
−i −i s s
7The inequalities in (9) quantify the loss of efficiency incurred by substituting an exact
Gibbs update with a π (·|x )-invariant one, provided the conditional conductance is uni-
i −i
formly bounded away from zero. Crucially, the bound is informative even if the conditional
conductance is controlled only on a set K, provided π(Kc)/s is small.
Remark 6. The first inequality in (9) is tight, in the sense that for every c ∈ [0,1] there
exists an operator P such that κ(P,X)=c and Φ (P)=cΦ (G). A simple example is given
s s
by P =cG +(1−c)I for all i=1,...,d. See Section A.1 in the Appendix for details.
i i
The bound in (9) shares some similarity with the ones in [40], where they decompose a
general Markov chain in different, easier to analyze, pieces (see in particular their Theorem
1.1). Their context, though, is very different, since they are motivated by multi-modal
problems and do not consider coordinate-wise schemes.
4 Applications to Metropolis-within-Gibbs schemes
Asafirstapplicationofthetheorydevelopedabove,andinparticularofCorollary1,wecon-
sider Metropolis-within-Gibbs (MwG) schemes. MwG are popular MCMC algorithms that
replace the exact conditional updates of Gibbs schemes with π (·|x )-invariant Metropolis-
i −i
Hastings (MH) updates, thus making coordinate-wise MCMC algorithms applicable to gen-
eral models as opposed to only conditionally conjugate ones. Random-scan MwG kernels
take the form of (5), with Px−i defined as
i
(cid:90) (cid:90)
Px−i(x ,A)= αx−i(x ,y )Qx−i(x ,dy )+δ (A) (cid:2) 1−αx−i(x ,y )(cid:3) Qx−i(x ,dy ) A⊂X ,
i i i i i i i i xi i i i i i i i
A
(10)
where αx−i is the MH acceptance probability with target π (·|x ) and proposal Qx−i, i.e.
i i −i i
(cid:26) π (dy |x )Qx−i(y ,dx )(cid:27)
αx−i(x ,y )=min 1, i i −i i i i x ,y ∈X .
i i i π (dx |x )Qx−i(x ,dy ) i i i
i i −i i i i
MwG schemes are an instance of coordinate-wise kernels and thus (9) applies to them.
We now consider two instances of MwG schemes where the conditional conductance can
be controlled, namely independent MH conditional updates and conditionally log-concave
target distributions.
4.1 Conditional updates with independent Metropolis-Hastings
We first consider MwG schemes with conditional updates performed using Independent
Metropolis-Hastings (IMH), meaning that the MH proposal kernel does not depend on x .
i
Thus we assume (with a slight abuse of notation)
Qx−i(x ,dy )=Qx−i(dy ) x∈X, i=1,...,d (11)
i i i i i
forsomeQx−i ∈P(X ). Despiteitssimplicity,MwGwithIMHproposalsisroutinelyusedin
i i
variouscontexts(seee.g.theBayesianinferencefordiffusionsexampleofSection8[61,6,47]).
8ThenextpropositionshowsthatanupperboundontheRadon-Nykodymderivativebetween
π (·|x ) and Qx−i, i.e.
i −i i
π (dx |x )
i i −i ≤M x∈X, i=1,...,d, (12)
Qx−i(dx )
i i
directly implies a lower bound on the conductance of P =
1(cid:80)d
P .
d i=1 i
Proposition 1. Define Px−i and Qx−i as in (10) and (11). If (12) holds, then Φ(P) ≥
i i
1 Φ(G).
M
Proposition 1 is a consequence of Corollary 1 and the fact that (12) implies a bound on
the conditional conductance of the independent Metropolis-Hastings updates.
Whileitiswell-knownthatIMHsuffersfromthecurseofdimensionality(i.e.performances
usually deteriorate exponentially with d if the update is jointly applied to all coordinates),
Proposition1impliesthatMwGschemeswithIMHproposalsonlypayaconstantslowdown
relative to exact Gibbs if the dimensionality of each X is fixed. This result will be used in
i
Section 8 to provide a lower bound on the conductance of a data augmentation scheme for
discretely observed diffusions.
4.2 Conditionally log-concave distributions
Inthissectionweconsiderthecasewheretheconditionaldistributionsπ (·|x )arestrongly
i −i
log-concave and smooth. More specifically, we take X = Rd and assume that the target
distribution admits a density π(x) with respect to the Lebesgue measure such that f(x):=
−logπ(x) satisfies
m (x ) L (x )
i −i ∥x −y ∥2 ≤f(y)−f(x)−∇f(x)⊤(y−x)≤ i −i ∥x −y ∥2 (13)
2 i i 2 2 i i 2
for all x,y∈Rd such that x =y . The ratio c (x )=L (x )/m (x ) is the condition
−i −i i −i i −i i −i
number of the conditional distribution π (·|x ).
i −i
Log-concavityandsmoothnessarecommonassumptionsintheMCMCliterature[14,17],
under which bounds on the conductance of various MCMC algorithms are available, see e.g.
[18, 2]. Combining bounds available in the literature with Corollary 1 one can bound the
conductance of MwG in terms of the Gibbs one.
Consider for example targeting π (·|x ) using the IMH algorithm with proposal distri-
i −i
bution
Qx−i(dy )=N(cid:0) y ;x∗(x ),m (x )−1I (cid:1) dy , (14)
i i i i −i i −i di i
whereN(y;µ,Σ)denotesthedensityofaGaussianwithmeanµandcovarianceΣevaluated
at y, I denotes the d×d identity matrix and x∗(x ) denotes the mode of π (·|x ). The
d i −i i −i
existence and uniqueness of x∗(x ) follows by the log-concavity of π (·|x ).
i −i i −i
Proposition 2. Assume (13). For Px−i and Qx−i defined as in (10) and (14), we have
i i
κ(cid:0) P ix−i(cid:1) ≥c i(x −i)−d 2i x∈Rd, (15)
9which implies
(cid:20) (cid:21)
Φ(P)≥ mininf c i(x −i)−d 2i Φ(G).
i x−i
Remark 7. The dependence of the conductance on the dimensionality of the single block is
the one we expect from the usual theory on Independent Metropolis-Hastings schemes, i.e.
the bound decreases exponentially fast in d . Thus, the best blocking of the variables in terms
i
of conductance is a trade-off between the conditional conductance in (15), which deteriorates
by increasing d , and the conductance Φ(G) of the associated Gibbs sampler, which typically
i
increases by increasing d .
i
WenowconsiderthecaseofMwGwithconditionalupdatesperformedthroughRandom-
Walk Metropolis (RWM).
Proposition 3. Assume (13) for all i=1,...,d. Define Px−i as in (10) with
i
Qx−i(x ,dy )=N(cid:0) y ;x ,σ2(x )I (cid:1) dy , σ2(x )= η , (16)
i i i i i i −i di i i −i d L (x )
i i −i
where η >0 is a fixed constant. Then there exists a constant M =M(η) depending only on
η such that
(cid:115)
1
κ(Px−i)≥M , x∈Rd, (17)
i d c (x )
i i −i
which implies
(cid:115)
1
Φ(P)≥M mininf Φ(G).
i x−i d ic i(x −i)
Alsointhiscasethebestblockingscheme,intermsofconductance,isgivenbyatrade-off
between the conditional conductance and the behaviour of the Gibbs sampler. However, the
dependence on d is polynomial, rather than exponential as in the case of Proposition 2,
i
which makes RWM more robust to the dimensionality of each coordinate.
Note that (13) is implied by global log-concavity and smoothness, but it is a weaker
requirement than that. Thus, Proposition 2 and 3 can in principle apply to cases where
standard MCMC results based on global log-concavity and smoothness do not hold. One
interesting example is given by models whose density is log-concave and smooth conditional
on some low-dimensional hyperparameters but not jointly. In those cases Proposition 2 and
3allowtorelateMwGtoexactGibbs,whichcanthenbeanalyzedwithdifferenttechniques.
Section 7 below considers one such example.
5 Auxiliary results for statistical applications
InordertosuccessfullyapplyTheorem1andCorollary1toMarkovchainsarisingincommon
statisticalapplications,suchasBayesianhierarchicalmodelsconsideredinSection6,weneed
some auxiliary results dealing with approximate conductances and perturbation of Markov
operators,whichcanbeofindependentinterest. ThesearedescribedinSection5.1. Wealso
recall some facts about the conductance of product operators in Section 5.2.
105.1 Approximate conductance and perturbations of Markov opera-
tors
Consider two generic Markov kernels P and P with invariant distributions π and π . We
1 2 1 2
define a notion of discrepancy between P and P as follows
1 2
∆(P ,P ,M)= sup ∥µP −µP ∥ , (18)
1 2 1 2 TV
µ∈N(π1,M)
with N (π ,M) as in (2). The next theorem shows how to relate ∆(P ,P ,M) with the
1 1 2
difference between Φ (P ) and Φ (P ) for s′ close to s.
s 1 s′ 2
Theorem 2. Let P and P be transition kernels with invariant distributions π and π ,
1 2 1 2
and δ =∥π −π ∥ . For s≥δ, we have
1 2 TV
2δ
Φ (P )≥Φ (P )−∆(P ,P ,1/s)− .
s 1 s−δ 2 1 2 s
Therefore, knowledge on Φ (P ) can be used to bound Φ (P ), with two additional
s−δ 2 s 1
termsmeasuringrespectivelythedistancebetweentheoperatorsandbetweentheassociated
invariant distributions. Theorem 2 significantly simplifies for the case of the Gibbs sampler,
as we now show. Let G and G be (random scan) Gibbs sampler kernels targeting π and
1 2 1
π , meaning that
2
d d
1(cid:88) 1(cid:88)
G = G and G = G ,
1 d 1,i 2 d 2,i
i=1 i=1
with G and G defined as G in (6) with π replaced by, respectively, π and π . One can
1,i 2,i i 1 2
control the discrepancy between G and G in terms of δ =∥π −π ∥ as follows. The
1,i 2,i 1 2 TV
proofissimilartotheoneofProposition2.2in[3],wheredeterministic-scanGibbssampleris
considered(seealso[11]forageneralapproachtoboundthetotalvariationdistancebetween
Markov kernels in terms of the stationary distributions).
Lemma 2. For every M ≥1 and i=1,...,d we have
∆(G ,G ,M)≤2Mδ. (19)
1,i 2,i
Combining Lemma 2 and Theorem 2 we obtain the following result.
Theorem 3. For s≥δ, we have
4δ
Φ (G )≥Φ (G )− .
s 1 s−δ 2 s
Remark 8. An interesting byproduct of Theorem 3 is that, if G and G are Gibbs samplers
n
targeting π and π respectively, then ∥π −π∥ →0 as n→∞ implies
n n TV
liminfΦ (G )≥Φ (G)
s n s−δ
n
for every δ > 0. This crucially relies on s > 0, that is on using the approximate ver-
sion of the conductance: for s = 0 it is not true in general that ∥π −π∥ → 0 implies
n TV
liminf Φ(G )greaterthanΦ (G). SectionA.2intheAppendixprovidesasimpleexample
n n s−δ
where ∥π −π∥ →0 and Φ(G)=1, but Φ(G )=0 for every n.
n TV n
11In Section 6 we will use Theorem 3 to prove that, under suitable assumptions, the ap-
proximate conductance of Gibbs samplers targeting Bayesian hierarchical models remains
bounded away from zero as the number of parameters and datapoints diverge (Theorem 5).
5.2 Conductance and independent products
The next lemma, which is a simple consequence of Cheeger inequality, provides a lower
boundtotheconductanceofaproductofindependentkernels. ThiswillbeusefulinSection
6 to bound the conditional conductance of MwG schemes for models with a high degree of
conditional independence.
Lemma 3. Let P be a π -invariant kernel with π ∈ P(X ), for j = 1,...,J; and P =
j j j j
⊗J P be the corresponding product kernel on X =×J X . Then
j=1 j j=1 j
Φ2(P )
Φ(P)≥ min j .
j∈{1,...,J} 4
Remark9. Withtheresultaboveweloseafactorof2,passingfromΦ(P )toΦ2(P ). While
j j
finer results are available under additional assumptions [9], the bound in Lemma 3 suffices
for our purposes.
6 High-dimensional hierarchical models
In this section we analyze the performances of coordinate-wise MCMC targeting Bayesian
hierarchical models, in a high-dimensional regime where both the number of datapoints and
parameters increase.
We consider a general class of hierarchical models, with data divided in J groups, each
having a set of group-specific parameters θ . The latter share a common prior with hyper-
j
parameters ψ. Thus, we assume the following model:
Y |θ ∼f(·|θ ) j =1,...,J,
j j j
θ |ψ i ∼id p(·|ψ) j =1,...,J, (20)
j
ψ ∼p (·).
0
We assume that the prior for θ ∈Rℓ belongs to the exponential family, that is
j
(cid:40) S (cid:41)
(cid:88)
p(θ |ψ)=h(θ)exp η (ψ)T (θ)−A(ψ) , (21)
s s
s=1
where ψ ∈ RD, h : Rℓ → R is a non-negative function and η (ψ), T (θ) and A(ψ) are
+ s s
known real-valued functions with domains RD, Rℓ and RD respectively. We let f(y | θ) be
an arbitrary likelihood function with data y ∈ Y ⊆ Rm and parameters θ ∈ Rℓ, dominated
by a suitable σ-finite measure (usually Lebesgue or counting one).
126.1 Gibbs and MwG kernels
When sampling from the posterior distribution of model (20), it is natural to consider
coordinate-wise MCMC schemes that alternate update from the full conditionals of local
and global parameters. Denoting θ = (θ ,...,θ ), Y = (Y ,...,Y ) and π (dψ,dθ) :=
1 J 1:J 1 J J
L(dψ,dθ |Y ), the transition kernel of the exact two-block GS targeting π (dψ,dθ) is
1:J J
defined as
1 1
G = G + G (22)
J 2 1,J 2 2,J
with
G ((ψ,θ),(dψ′,dθ′))=π (dψ′ |θ)δ (dθ′), G ((ψ,θ),(dψ′,dθ′))=π (dθ′ |ψ)δ (dψ′).
1,J J θ 2,J J ψ
SamplingfromG ,however,requiresπ (dθ |ψ)tobeavailableinclosedformandamenable
J J j
to exact sampling, which is typically feasible only for conditionally conjugate models. For
non-conjugate models, more broadly applicable coordinate-wise MCMC methods (such as
MwG schemes) are typically used. The corresponding kernel is
1 1
P = P + P , (23)
J 2 1,J 2 2,J
where
P ((ψ,θ),(dψ′,dθ′)))=Pθ(ψ,dψ′)δ (dθ′),
1,J θ
P ((ψ,θ),(dψ′,dθ′)))=Pψ,Y (θ,dθ′)δ (dψ′),
2,J ψ
withPθ andPψ,Y being,respectively,π (dψ |θ)andπ (dθ |ψ)-invarianttransitionkernel.
J J
In order to relate the convergence properties of P to the ones of G we need to control
J J
theconditionalconductanceofP . Todothat,wecanleveragetheconditionalindependence
J
of (θ ,...,θ ) given ψ under π , which implies that we can take a factorized Pψ,Y defined
1 J J
as
J
Pψ,Y (θ,dθ′)= (cid:89) Pψ,Yj(cid:0) θ ,dθ′(cid:1) (24)
j j
j=1
with Pψ,Yj being a π J(dθ
j
|ψ)-invariant kernel. Thus, by Lemma 3
κ2(Pψ,Yj)
κ(Pψ,Y)≥ min . (25)
j∈{1,...,J} 4
Condition(C)belowimposesalowerboundonκ(Pψ,Yj)forψbelongingtosomeappropriate
set. We also require a lower bound on κ(Pθ). This is usually less critical since ψ is low-
dimensional(i.e.offixeddimensionalitynotgrowingwithJ)andπ (dψ |θ)isoftenavailable
J
in closed form due to (21), in which case one can perform exact Gibbs updates (i.e. take
Pθ(ψ,dψ′)=π (dψ′ |θ)).
J
Remark10. Beyondbeinginterestingperse,relatingtheconvergencepropertiesofP tothe
J
ones of G is theoretically appealing because the latter is potentially much easier to analyse,
J
using for example the dimensionality reduction approach discussed in [3, Lemma 4.2]. On
the contrary P does not enjoy such dimensionality reduction property and thus performing
J
a direct analysis of it is in principle much harder.
136.2 Statistical assumptions
We now describe the statistical assumptions we require on the data-generation process and
likelihood of model (20), in order to an analyze the performances of P as J →∞.
J
We denote the marginal likelihood of the model, obtained by integrating out the group
specific parameter θ, as
(cid:90)
g(y |ψ)= f(y |θ)p(θ |ψ)dθ (26)
Rℓ
and its Fisher Information matrix as
(cid:20) (cid:21)
[I(ψ)] =E {∂ logg(Y |ψ)} (cid:8) ∂ logg(Y |ψ)(cid:9) , d,d′ =1,...,D.
d,d′ ψd ψ d′
WedenotebyQ ∈P(Rm)theprobabilitymeasurewithdensityg(y |ψ),andtheassociated
ψ
product measures as Q(J) or Q(∞). Following [3] we consider the following assumptions:
ψ ψ
(B1) Thereexistsψ∗ ∈RD suchthatY i ∼id Q forj =1,2,.... Moreoverthemapψ →g(·|
j ψ∗
(cid:112)
ψ) is one-to-one and the map ψ → g(x|ψ) is continuously differentiable for every
x. Finally, the prior density p is continuous and strictly positive in a neighborhood of
0
ψ∗.
(B2) ThereexistacompactneighborhoodΨofψ∗ andasequenceoftestsu : RmJ → [0,1]
j
such that (cid:82) u (y ,...,y )(cid:81)J g(y |ψ∗)dy →0 and
RmJ j 1 J j=1 j 1:J
sup
(cid:82)
[1−u (y ,...,y
)](cid:81)J
g(y |ψ)dy →0, as J →∞.
ψ̸∈Ψ RmJ j 1 J j=1 j 1:J
(B3) The Fisher Information matrix I(ψ) is non-singular and continuous w.r.t. ψ.
Assumptions(B1)-(B3)requiremodel(20)tobewell-specified,inthesensethatthemarginal
likelihood (26) corresponds to the true data-generating mechanism. Moreover, the global
parametersψneedtobeidentifiable,asformalizedin(B2)and(B3): thisguaranteesthatthe
posteriordistributiononψcontractstothetruevalueψ∗atanappropriaterate(thankstothe
Bernstein-vonMisesTheorem,e.g. [68]). Wealsoconsiderasetofmoretechnicalregularity
assumptions on the likelihood f(y | θ ), (B4)-(B6), which are stated for completeness in
j
Appendix C. Notice that assumptions (B1)-(B6) are satisfied by common formulations of
model (20), such as the hierarchical normal model or models for binary and categorical data
(see e.g. Section 5 in [3]).
6.3 Dimension-free mixing of MwG for hierarchical models
ThefollowingconditionrequirestheconditionalconductanceofP tobeboundedawayfrom
J
0 in a neighbourhood S∗ of ψ∗:
(C) There exist a neighborhood S∗ of ψ∗ and κ>0 such that κ(cid:0) P ,S∗×RJℓ(cid:1) ≥κ.
J
Note that the constant κ>0 above should not depend on J or Y . By (25) condition (C)
1:J
amounts to requiring
inf
(ψ,Yj)∈S∗×Yκ(cid:0) Pψ,Yj(cid:1)
≥κ and inf
θ∈RJℓκ(cid:0) Pθ(cid:1)
≥κ. (27)
14The key requirement in (27) is that the kernel Pψ,Yj is well-behaved in a neighbourhood of
ψ∗, uniformly in Y ∈ Y. Section 6.5 provides some examples where these conditions are
j
verified.
The following theorem bounds Φ (P ) in terms of κ and Φ (G ).
s J s J
Theorem 4. Consider model (20), kernels G and P defined in (22)-(23) and s∈(0,1/2).
J J
Then, under assumptions (B1)-(B3) and (C), we have
(cid:18) κ2 (cid:19)
Q(J) Φ (P )≥ Φ (G ) →1 as J →∞.
ψ∗ s J 8 s J
At an intuitive level, Theorem 4 states that two things are sufficient for P to mix fast:
J
firstthatG mixesfastandsecondthattheconditionalconductanceofP aroundψ∗isgood.
J J
Motivated by Theorem 4, the next theorem studies the behaviour of Φ (G ) as J →∞.
s J
Theorem 5. Consider model (20) and G defined in (22). Then, under assumptions (B1)-
J
(B6), for every s∈(0,1/2) there exists a constant g(s)>0 such that
Q(J)(Φ (G )≥g(s))→1, as J →∞. (28)
ψ∗ s J
The inequality in (28) holds with probability converging to one as J → ∞, under the
true data-generating mechanism. Equivalently, (28) states that Φ (G ), which is a random
s J
variable depending on Y , is bounded away from zero in Q(∞)-probability as J →∞.
1:J ψ∗
Remark 11. Theorem 5 crucially relies on the fact that the approximate version of the
conductanceisconsidered, i.e.thats>0. Indeed, theproofofTheorem5exploitsasymptotic
characterizations on the posterior of (ψ,θ), which in general do not provide meaningful
bounds for the case s = 0. This is connected with Remark 8: the exact conductance with
s = 0 is not directly controlled by the convergence of the invariant distributions (π → π),
n
since it is sensitive to the behaviour of kernels over sets with arbitrarily small probability
under π.
We can combine Theorem 4 with Theorem 5 to deduce that the mixing times of P are
J
uniformly bounded in probability as J →∞.
Corollary 2. Consider model (20) and P defined in (23). Under assumptions (B1)-(B6)
J
and (C), for every M ≥1 and ϵ>0 there exists T (ψ∗,ϵ,M)<∞ such that
Q(J)(t (P ,ϵ,M)≤T (ψ∗,ϵ,M))→1 as J →∞.
ψ∗ mix J
Corollary 2 implies that
t (P ,ϵ,M)=O (1) as J →∞,
mix J P
i.e. that the sequence of random variables {t (P ,ϵ,M)} is bounded in probability
mix J J=1,2,...
as J →∞. This is in accordance with numerical evidences (see e.g. Figure 1).
156.4 Computational complexity implications
Sampling from P defined in (23) requires O(J) computational cost. This follows from
J
(24) and the fact that sampling from each
Pψ,Yj(cid:0)
θ j,dθ
j′(cid:1)
involves an O(1) cost, since the
conditional distributions π (dθ | ψ) = L(dθ |ψ,Y ,) depend only on the local observations
J j j j
Y and not on the whole dataset Y . The cost of sampling from Pθ(ψ,·) depends on
j 1:J
the specification of Pθ, but it is typically dominated by the O(J) cost of computing the
conditional density π (dψ | θ) = L(dψ | θ,Y ) once. Under the assumptions of Corollary
J 1:J
2,weobtainthereforethat,withhighprobabilitywithrespecttothedatagenerationprocess,
the MwG algorithm with kernel P produces a sample with ϵ-accuracy in TV distance with
J
O(J) cost when initialized from a warm start. Section 6.6 extends Corollary 2 to the case
where the algorithm is implemented starting from a specific feasible distribution, sampling
from which requires O(Jlog(J)) cost (given access to the maximum marginal likelihood
estimator). Thus, under the above assumption, the total cost required by MwG for each
ϵ-accurate sample, including the initialization, scales as O(Jlog(J)).
It is interesting to compare this cost with the one of standard gradient-based MCMC
methods,suchasMetropolis-AdjustedLangevinAlgorithm(MALA)andHamiltonianMonte
Carlo (HMC). The cost required by a single full likelihood or gradient evaluation for model
(1) is O(J). Available results suggest that the number of gradient evaluations required
by MALA and HMC (as well as other π -invariant or Metropolis-adjusted gradient-based
J
MCMC schemes) to converge to stationarity scales as O(Jα) as J → ∞, for some α > 0
that depends on the setup and type of algorithm [57, 7, 69]. Combining these two facts, we
can expect the cost required by gradient-based MCMC schemes for each posterior sample to
scaleasO(J1+α)forα>0. Thesebrieftheoreticalconsiderationsareinagreementwiththe
numerical results observed in Sections 1.1 and 6.5.1.
6.5 Models with discrete data
The main requirement in Corollary 2 is Assumption (C), which imposes a bound on the
conditionalconductance,uniformlyoverY ∈Y. Asettingwherethelatteriseasilysatisfied
j
is given by models with discrete data, where Y is finite.
Moreformally,letf(y |θ)in(20)beaprobabilitymassfunctionwithsupport{y ,...,y }
0 m
withm<∞, i.e.foreveryθ ∈RK werequirethesupportoff(y |θ)nottodependonθ, i.e.
m
(cid:88)
f(y |θ)=1, f(y |θ)>0, r =0,...,m. (29)
r r
r=0
The assumption in (29) is mild and holds for most likelihoods usually employed with binary
or categorical data, e.g. multinomial logit and probit. For simplicity we consider the case of
a single local parameter with Gaussian prior, i.e.
Y |θ ∼f(y |θ ), θ ,...,θ |µ,τ i ∼id N(µ,τ−1), (µ,τ)∼p (·). (30)
j j j 1 J 0
16For example the case f(y | θ) = (cid:0)m(cid:1) eyθ , with y = 0,...,m, corresponds to the logistic
y (1+eθ)m
hierarchical model with Gaussian random effects. The next proposition shows that, under
mild conditions, MwG schemes targeting model (30) lead to dimension-free mixing times.
Proposition 4. Consider model (30) with likelihood as in (29) and let assumptions (B1)-
(B3) be satisfied. Let the operator P
J
be as in (23), where
κ(cid:0) Pψ,yr(cid:1)
is continuous with
respect to ψ and κ(cid:0) Pψ,yr(cid:1) >0 for every r =0,...,m and ψ ∈S∗, with S∗ neighborhood of
ψ∗. Then for every M ≥1 and ϵ>0 there exists T (ψ∗,ϵ,M)<∞ such that
Q(J)(t (P ,ϵ,M)≤T (ψ∗,ϵ,M))→1 as J →∞.
ψ∗ mix J
Requiring
κ(cid:0) Pψ,yr(cid:1)
>0for fixedψ and y
r
isarguably amildcondition, e.g.it isimplied
by geometric ergodicity [28].
6.5.1 Numerical simulations
First, we provide details on the numerical results illustrated in Figure 1. The model specifi-
cation is as in (1), which is a special case of (30) with the logistic likelihood. The prior on
the hyperparameters ψ = (µ,τ) is set to µ | τ ∼ N(0,103/τ) and τ ∼ Gamma(1,1), while
the data are generated by the same model with true parameters µ∗ = τ∗ = 1 and m = 10
observations per group. Four different algorithms are employed to sample from the associ-
ated posterior distribution: a two-block GS defined as in (22), where the conditional update
of L(θ | ψ,Y ) is performed through adaptive rejection sampling [24]; a MwG algorithm
j j
defined as in (23), where the conditional update is done using MH with Barker proposal as
in Algorithm 2 of [38]; MALA with optimal tuning, in the sense that samples obtained from
a long run of GS were used to optimally tune a diagonal preconditioning matrix [57]; the
No-U-Turn-Sampler (NUTS) [27] implemented in the software Stan [64].
Since mixing times of high-dimensional Markov chains are computationally intractable
and hard to approximate (see e.g. the discussion in Biswas et al. [8]), we consider Integrated
Autocorrelation Times (IATs) as an easier-to-compute empirical measure of convergence
time. Givenaπ-invariantMarkovchain(X(t)) ,theIATassociatedtoasquare-integrable
t≥1
test function f is defined as
∞
(cid:88) (cid:16) (cid:17)
IAT(f)=1+2 Corr f(X(1)),f(X(t)) ,
t=2
anditmeasuresthenumberofdependentsamplesthatisequivalenttoanindependentdraw
(cid:82)
from π in terms of estimation of f(x)π(dx). Intuitively, the higher the IAT the slower
the convergence. In our numerical experiments, we estimate the IAT with the ratio of the
number of iterations and the effective sample size, as described in [25], with the effective
sample size computed with the R package mcmcse [19].
Figure 1 depicts the median (over random data set replications) of the maximum IAT
over all the parameters (both global and group specific) for the four algorithms, with the
17Figure 2: Median IATs (on the log scale) of four MCMC schemes targeting the posterior distribution
of model (31) with ℓ = 5 and m = 30, as a function of the number of groups. The median refers to
repetitionsoverdatasetsrandomlygeneratedaccordingtothemodelwith(τ∗,...,τ∗)=(2,1,1,3,2)and
1 5
µ∗ i ∼id Unif([−1,1]) for every k =1,...,5.
k
numberofgroupsrangingfrom128to4096. SinceeachiterationoftheNUTSkernelinvolves
multiple steps (i.e. multiple gradient evaluations), in order to provide a fair comparison the
associated IAT is multiplied by the average number of gradient evaluations per iteration.
In accordance with Corollary 2, the IATs of coordinate-wise samplers do not increase as
J → ∞. On the contrary, the IATs of gradient-based samplers seem to grow as J → ∞.
The optimally tuned MALA shows better performances than a black-box implementation of
NUTS, illustrating the importance of carefully tuning step-sizes.
In our second set of experiments, we test coordinate-wise samplers in situations where
the dimensionality of θ in model (30) is increased by including the presence of ℓ > 1
j
covariatespergroup,leadingtoJℓlatentparameters. Inparticular,weconsiderthefollowing
specification of the hierarchical logistic regression model
(cid:32) (cid:33)
Y |θ ,X in ∼d. Bernoulli
eX i⊤θj
, θ |µ,τ i ∼id ⊗ℓ N(µ ,τ−1), (µ,τ)∼p (·),
ij j i 1+eX i⊤θj j k=1 k k 0
(31)
where i = 1,...,m, θ = (θ ,...,θ ) for every j = 1,...,J, µ = (µ ,...,µ ) and τ =
j 1j ℓj 1 ℓ
(τ ,...,τ ). The usual conjugate prior is employed for (µ,τ), i.e.
1 ℓ
µ |τ in ∼d. N(0,103/τ ), τ i ∼id Gamma(1,1), k =1,...,ℓ. (32)
k k k k
Inallthesimulationstofollows,thecovariatesusedforthedata-generationprocess(excluding
the intercept) are independently sampled from a uniform distribution between −5 and 5.
Figure 2, which is based on model (31) with ℓ = 5 and m = 30, suggests that the
dimension-free convergence of coordinate-wise methods holds even in the presence of covari-
ates. Indeed all the four schemes, whose kernels are as in (23) and differ only for the choice
of kernel Pψ,Yj for the conditional updates of the local parameters, exhibit roughly constant
IATs as the number of groups grows. Their difference in performance highlights the impact
of the conditional conductance, which increases both going from RWM to Barker as well as
18Figure 3: Median IATs of five MCMC schemes targeting the posterior distribution of model (1), as a
function of the number of covariates (intercept included). The median refers to repetitions over datasets
randomly generated according to model (31) with τ∗ = 0.5 and µ∗ i ∼id Unif([−1,1]) for every k. Some
k k
points for IMH and RWM are omitted due to the difficulty of appropriately estimating high values of
IATs.
running multiple steps of the invariant update at each iteration (see Figure 2).
Figure 3 instead reports the IATs of different coordinate-wise samplers for model (31)
with J = m = 30 and the number of covariates ℓ ranging between 1 and 5. As above, the
samplers’ kernels are as in (23), but they differ in the specific invariant update on the local
parameters. TheobservedIATsincreasewithℓ,whichiscoherentwiththetheorydeveloped
e.g. in Propositions 2 and 3, which suggests that the goodness of the conditional update
typically deteriorates with the dimensionality (equivalently, referring to (9), the conditional
conductance decreases with ℓ). In particular, in accordance with Proposition 2, the IATs
associated to IMH grow very quickly with the dimensionality of θ . This emphasizes that
j
dependingonℓthechoiceoftheconditionalupdatebecomesmoreandmorerelevant. Notice
moreover that IMH requires to compute the mode of π (θ | µ,τ,Y ) for every j at each
J j j
iteration, which becomes infeasible even with small ℓ. Note that in this context also the
IATs of the exact GS are expected to increase with ℓ (see Section 7 for more illustrations
of similar phenomena and discussion about connections with (31) being a so-called centered
parametrization). This suggests that the increase in IATs as ℓ increases observed in Figure
3 is due to a combination of the reduction of the conductance of GS and the conditional
conductance of the updates.
6.6 Feasible start
Corollary 2 proves that the Markov chain defined in (23) yields bounded mixing times,
provided it is initialized from a warm distribution as of (2). In this section we provide a
so-calledfeasiblestartingdistributionwhichprovidessimilarguarantees. Forsimplicity,here
we assume that the update of the global parameters is exact, i.e. P =G in (23).
1,J 1,J
19Let S∗ be the neighborhood of ψ∗ satisfying assumption (C) and (ν )J ⊂P(Rℓ) be a
j,ψ j=1
collection of J distributions such that
ν ∈N (π (dθ |ψ),M), j =1,...,J,
j,ψ J j
for every ψ ∈ S∗ and a fixed constant M ≥ 1. Under (29), any distribution ν ∈
j,ψ
P(Rℓ) with compact support satisfies the requirement above. Denote now with ψˆ =
J
argmax
(cid:81)J
g(Y | ψ) the Maximum marginal Likelihood Estimator (MLE), with g as
ψ j=1 j
in (26). Let µ =µ
∈P(cid:0)RD+ℓJ(cid:1)
, with c>0 fixed constant, be defined as
J J,c
 
µ
J(A)=(cid:90)
Unif
cJ−1/2(cid:16)
ψˆ
J(cid:17) (dψ)(cid:89)J
ν j,ψ(Pψ,Yj)tJ(dθ j) , t
J
=
−lol go (cid:0)g 1(J −)
κ2(cid:1)
(33)
A j=1 2
for every A⊂RD+ℓJ, where κ is the constant satisfying assumption (C). Moreover B (x) is
c
the closed ball of center x and radius c>0, Unif(B)(dψ) denotes the uniform distribution
over B ⊂ RD and (Pψ,Yj)t denotes the t-th power of the kernel defined in (23). Thus,
sampling from µ in (33) can be performed in two steps: the global parameters ψ follow
J
a random perturbation of the MLE and, conditional on ψ, J independent Markov chains
with kernels (Pψ,Yj)J are run for a logarithmic (in J) number of iterations. The next
j=1
proposition shows that mixing times starting from µ enjoy a dimension-free asymptotic
J
behaviour.
Proposition 5. Consider the same setting and assumptions of Corollary 2 and µ as in
J
(33). Then, for every ϵ>0 there exists T (ψ∗,ϵ)<∞ such that
Q(J)(t (P ,ϵ,µ )≤T (ψ∗,ϵ))→1 as J →∞. (34)
ψ∗ mix J J
Remark 12. Notice that µ defined in (33) is not in general a warm start, i.e. we cannot
J
guarantee µ ∈N (π (dψ,dθ),M) for a fixed M that does not grow with J. However in the
J J
proof of Proposition 5 it is shown that µ is “close enough” to a warm distribution in total
J
variation distance, which suffices for our purposes.
Assuming ψˆ can be approximately computed with O(Jlog(J)) computational cost or
J
less, the cost of sampling from µ in (33) is O(Jlog(J)). As already mentioned in Sec-
J
tion 6.4, this implies that the overall computational cost of running the algorithm is again
O (Jlog(J)).
P
7 Bayesian binary regression with unknown prior vari-
ance
Consider a Bayesian logistic regression model with unknown prior precision defined as
(cid:32) (cid:33)
eθ⊤Xi
Y |θ,α∼Bernoulli , i=1,...,n
i 1+eθ⊤Xi
(35)
θ
|α∼N(cid:0) 0,α−1Σ(cid:1)
,
α∼Gamma(a,b),
20where θ = (θ ,...,θ ), X is a n×d matrix with i-th row X , Σ is a d×d positive definite
1 d i
covariancematrixandGamma(a,b)denotestheGammadistributionwithparametersa,b>
0. Let π(dα,dθ) := L(dα,dθ |Y) ∈ P(R1+d) be the joint posterior of α and θ given the
vector of observations Y = (Y ,...,Y ), under model (36). The conditional density of θ
1 n
given α under π, i.e.
(cid:40) n (cid:41)
π(θ |α)∝exp
Y⊤Xθ−(cid:88) log(cid:16) 1+eθ⊤Xi(cid:17)
−
α
θ⊤Σ−1θ , (36)
2
i=1
is strongly log-concave. We denote the condition number of π(θ |α) by c(α). Explicit
bounds on c(α) are available [18], which however diverge to ∞ as α goes to 0. As a result,
convergence properties of MCMC algorithms targeting model (36) with fixed α are well
understood [14, 17, 18]. However, α is usually unknown in applications and it is typically
incorporatedasarandomvariableintheBayesianmodel, asin(36). Insuchcases, thejoint
posterior π is not log-concave on Rd+1 and analyzing the convergence of MCMC algorithms
performing joint updates of (α,θ) can be much harder.
Given model (35), it is natural to consider a coordinate-wise posterior sampling scheme
with π-invariant kernel
1 1
P = G + P , (37)
2 1 2 2
where
G ((α,θ),(dα′,dθ′))=δ (dθ′)π(dα′ |θ),
1 θ
P ((α,θ),(dα′,dθ′))=δ (dα′)Pα,Y (θ,dθ′),
2 α
with Pα,Y being a π(dθ | α)-invariant kernel. Indeed, by strong log-concavity of π(θ |α),
Propositions 2 and 3 can be applied when choosing Pα,Y appropriately. Moreover, sam-
pling from π(dα|θ) is straightforward due to the Normal-Gamma conjugacy. The next
proposition states the resulting bound on the conductance, when Pα,Y is a RWM update.
Proposition 6. Let S ⊂ R+ and P be as in (37) with Pα,Y as in (16). Then for every
s∈(0,1/2) we have
(cid:114)
inf c(α)−1 π(Sc×Rd)
Φ (P)≥M α∈S Φ (G)− ,
s d s s
where G= 1G + 1G is the kernel of a two-component GS targeting π and M =M(η)>0
2 1 2 2
is the same constant of Proposition 3, which does not depend on π.
Intuitively, Proposition 6 implies that the following two conditions are sufficient for P to
mixfast: theposteriordistributionofαconcentratesinaregionwheretheconditionnumber
c(α) is not too high and the exact GS has a good approximate conductance (i.e. Φ (G) is
s
not too close to 0). While providing a lower bound to Φ (G) may seem equally challenging
s
as doing it for Φ (P), an important simplification (in terms of dimensionality reduction) is
s
available for the exact GS. Let T = θ⊤Σ−1θ and let π˜(dα,dT) := L(dα,dT |Y) ∈ P(R2)
be the two-dimensional marginal posterior distribution of (α,T), with associated GS kernel
G˜. The next lemma provides a lower bound on Φ (G) in terms of Φ (G˜).
s s
21Figure 4: Median IATs (on the log scale) of three MCMC schemes targeting the posterior distribution
ofmodel(35)withd=5,asafunctionofthenumberofobservationsn. Themedianreferstorepetitions
over datasets randomly generated according to model (35) with α∗ =1 and Σ= 1I.
5
Lemma 4. Let G and G˜ be kernels of the Gibbs samplers on π and π˜ as above. Then for
every s∈(0,1/2) we have
(cid:18) (cid:19)
Φ2 (G˜)
−log 1− s/8
1 2
Φ (G)≥ .
s 4 log(8)−log(s)
Remark 13. The main intuition underlying Lemma 4 is that the update of α given θ de-
pends only on T, i.e. π(dα|θ) = π(dα|T) for T = θ⊤Σ−1θ, which implies that G and G˜
have closely related convergence properties. See Lemma 8 and the proof of Lemma 4 in the
supplement for more details and [58, 15, 51, 3] for results similar in spirit.
We compare numerically three coordinate-wise samplers with kernels as in (37), with
Pα,Y given by, respectively RWM, Barker and 100 repeated steps of Barker (which we take
as a proxy of the exact GS in this context due to the small value of d and the high number
of steps). We consider model (35) with Σ=d−1I , d=5, a=b=1 and n ranging from 20
d
to 100. The data are generated from the same model with α set to 1. The results, reported
in Figure 4, illustrate that, for all the samplers under consideration, performances improve
as n grows and in particular the IATs decrease as n increases. This is in accordance with
the fact that the parametrization of model (35) is a so-called ‘centered’ one (in the sense of
[20, 46, 48]), so that the performances of the associated coordinate-wise samplers improve
as n/d increases (i.e. as the data became more informative) while they suffer if d/n is large.
Notice that the reduction in IAT with respect to n is mostly due to the behaviour of the
exact GS rather than a change in the conditional conductance: indeed, also the black line in
Figure 4 exhibits a decreasing behaviour.
Next, we consider also the non-centered version of model (35), which can be formulated
as
(cid:32) √ (cid:33)
eβ⊤Xi/ α
Y i |β,α∼Bernoulli 1+eβ⊤Xi/√
α
, β |α∼N(0,Σ), α∼Gamma(a,b). (38)
22Figure 5: Median IATs of four MCMC schemes targeting the posterior distribution of model (35), as a
function of the number of covariates d and n=d/2. The median refers to repetitions over datasets ran-
domly generated according to model (35) with α∗ =1 and Σ= 1I. Full lines: centered parametrization.
d
Dotted lines: non-centered parametrization.
Similarly to (37), coordinate-wise samplers can be used to sample from the posterior distri-
bution π(dα,dβ)=L(dα,dβ |Y), with the caveat that π(dα |β) is not available in closed
form and thus we perform a MH update for it instead of an exact Gibbs one. Given that α
is a one-dimensional parameter, we found a RWM update of it to be sufficient for our pur-
poses. Figure5reportstheIATestimatesforthecoordinate-wisesamplersobtainedwiththe
centered (full lines) and non-centered (dotted lines) parametrizations when d∈{20,60,100}
and n = d/2. In accordance with the discussion above, the non-centered parametrization
behaves significantly better than the centered one in this regime with d/n relatively large.
Note that even the two MwG schemes with 100 Barker steps per iteration exhibit very dif-
ferentbehavioursandinparticularthenon-centeredoneissignificantlymorerobusttolarge
values of d.
8 Data augmentation schemes for discretely-observed
diffusions
Many real-life phenomena of interest, for example in biology, physics and finance, can be
described as a diffusion over [0,T] defined through a Stochastic Differential Equation (SDE)
such as
dX =b(θ,X )dt+σ(X )dB , t∈[0,T], (39)
t t t t
whereB isaBrownianmotiononRandθ ∈Rp isavectorofparametersonwhichwewant
t
to make inference. See e.g. [61, 6, 47] and references therein for a review.
The functions b and σ are assumed to satisfy the basic regularity conditions (e.g. locally
Lipschitz,σ boundedbelow)whichimplytheexistenceofaweaklyuniquesolution. Forease
of exposition in this section, we shall restrict ourselves to the case where the function σ is
23known, although our results could be readily extended to the case of unknown σ through
standard techniques (see e.g. [61]). Multivariate extensions are also possible.
Proceeding in the standard way, we apply the Lamperti transformation x → η(x) to X
t
as in (39), with η(x)=(cid:82)x σ(u)−1du, from which we obtain a diffusion with unit coefficient.
z
Therefore without loss of generality we can consider
dX =b(θ,X )dt+dB , t∈[0,T]. (40)
t t t
Assuming hypothetically we observed the entire trajectory X = {X : t ∈ [0,T]}, the
[0,T] t
likelihood of θ is given by the well-known Girsanov formula, which reads
(cid:40) (cid:41)
G(cid:0) X ,θ(cid:1) =exp
(cid:90) T
b(X ,θ)dX −
1(cid:90) T
b2(X ,θ)dt , (41)
[0,T] t t 2 t
0 0
fromwhichtheposteriordistributionofθ,givenapriordistributionπ(dθ),hasdensitygiven
by
(cid:0) (cid:1) (cid:0) (cid:1)
π θ |X ,θ ∝G X ,θ π(θ), (42)
[0,T] [0,T]
assuming π(dθ) admits a density π(θ) with respect to the Lebesgue measure.
In practice we observe (40) at fixed points T = (t ,t ,...,t ) where we assume for
0 1 N
simplicity t −t = ∆ for every i. If ∆ is small enough typically a discretization of the
i i−1
integrals in (41) can often be employed (as in the Euler-Maruyama method) circumventing
the need for data-augmentation. More realistically, when data is sparser, the likelihood is
given by
N
(cid:89)
f(X ,...,X )=f (X ) f (X ,X ),
t0 tN θ t0 θ ti−1 ti
i=1
where f (x,z) is the transition probability density associated to (40) of passing from x to
θ
z in an interval ∆ of time. However f is in general intractable and not available in closed
θ
form. To circumvent this problem, a data augmentation scheme has been proposed in the
literature ([61, 6, 47]), where at each step the missing data Y ={X t} t̸∈T are imputed. This
can be described as a coordinate-wise scheme on X = Rp ×(cid:81)N R(ti−1,ti) with invariant
i=1
distribution π(dθ,dY |XT). The associated operator P can be formally defined as
1 1
P = G + P , (43)
2 1 2 2
where
G ((θ,Y),(dθ′,dY′))=δ (dY′)π(cid:0) dθ |X (cid:1)
1 Y [0,T]
is the exact step for θ, which we assume to be feasible thanks to the explicit likelihood in
(41), and
N
(cid:89)
P ((θ,Y),(dθ′,dY′))=δ (dθ′) Pθ(Y ,dY′), (44)
2 θ i i i
i=1
with Y denoting the evolution of the diffusion over the interval (t ,t ) and Pθ a Markov
i i−1 i i
operator on R(ti−ti−1) with invariant distribution π(cid:0) dY
i
|θ,X ti−1,X ti(cid:1) . In words, step (44)
requires updating separately the evolution of the diffusion in the N sub-intervals defined by
24T: thiscanbedoneindependentlythankstotheMarkovpropertyof (40). Acommonchoice
for Pθ, see e.g. [61], is an Independent Metropolis-Hastings as in Section 4.1, where the new
i
path Y′ is sampled according to a Brownian bridge on [t −t ] constrained to be equal to
i i i−1
X and X at the endpoints and it is accepted with probability
ti−1 ti
(cid:26) G(Y′,θ)(cid:27)
α(Y,Y′)=min 1, . (45)
G(Y,θ)
For the technical details we refer to [61].
We make the following assumptions:
(C1) The function b(θ,x) is differentiable in x for every θ and continuous in θ for every x.
(C2) The function b2(θ,x)+b′(θ,x) is bounded below for every θ.
(C3) The prior π(dθ) is supported on a compact space S ⊂R.
(C4) There exists c>0 such that X ∈[−c,c] for every i=1,...,N.
ti
Assumptions(C1)−(C2)arecommoninthisliterature(seee.g. [6])andaresatisfiedinmany
interesting cases [50], while (C3)−(C4) allow for technical simplifications: an alternative
would be to consider a compact set K which retains most of the posterior mass, as in (9).
The next proposition shows that we can then provide a lower bound on the conductance of
P in (43) in terms of the corresponding Gibbs sampler.
Proposition 7. Let P be as in (43), with Pθ operators of the Independent Metropolis-
i
Hastings as in (45). Then under assumptions (C1) − (C4) there exists a constant κ =
κ(c)>0 such that
Φ(P)≥κΦ(G), (46)
for every N, T=(t 0,...,t N), with 0≤t
0
<···<t
N
≤T, and XT =(X t0,...,X tN).
Notethat, althoughnotexplicitlyindicatedinournotation, thekernelsP andGdepend
on N (as well as on T and XT). Indeed, the key and non-trivial part in the statement of
Theorem7isthattheconstantκin(46)doesnotdependonN,TandXT. This,forexample,
implies that, under assumptions (C1)−(C4), the decrease of conductance passing from G
to P remains bounded as N → ∞. Extensions of Proposition 7 to the case of approximate
conductance, in order to relax assumption (C3), are direct following the approach of the
previous sections.
Proposition 7 illustrates the applicability of the techniques developed in this paper in
the context of parameters inference for diffusions: in this case the missing data require
the imputation of an infinite-dimensional object, i.e. a diffusion trajectory. Under the
above assumptions, Proposition 7 reduces the problem of determining the performances
of P as in (43) to the task of studying the associated Gibbs sampler. Similarly to the
applications discussed in the Sections 6 and 7, the latter can entail major simplifications
through dimensionality reduction: for example, in the popular setting where b(x,θ) is a
polynomial of degree m in θ, techniques analogous to Lemma 4 and Remark 13 allow to
25reduce the problem to analysing a Gibbs sampler G˜ targeting a ℓ-dimensional target with
ℓ = O(2m). We leave more detailed examples with specific classes of diffusions to future
work.
9 Discussion
TheresultsofSection3showthatperformancesofacoordinate-wiseschemecanbecontrolled
monitoring two factors: (a) whether conditional updates are close enough to exact sampling
(i.e. the conditional conductance is far from zero) and (b) whether the GS itself is a good
scheme for the specific sampling problem. For example, the first inequality in (9) bounds
the slow-down of a general coordinate-wise sampler relative to GS (measured in terms of
reduction of conductance) with a multiplicative term of the form min κ (P ,X),
i∈{1,...,d} i i
definedin(7). Aninterestingaspectofsucha‘slow-down’factoristhatitdependsondonly
through the minimum operation. Thus, if the GS updates are replaced by moderately good
MH conditional updates (e.g. ones with conductance uniformly bounded away from 0) the
resulting MwG sampler will incur in a slow-down that is constant with respect to d. This
observation agrees with the observation that, provided the full-conditional distributions are
well-behaved, MwG tends to perform similarly to the corresponding GS (see e.g. Figure 1),
even when d is large.
Note that the results of Section 3, in their current form, strongly relies on the random-
scanarchitecture,whereateachiterationarandomlychosenblockisupdated. Extensionsto
other popular orderings, e.g. deterministic-scan ones, are less trivial than one might expect.
In particular, naive applications of the techniques employed in this paper would lead to
conductance bounds depending on multiplicative factors of the form
(cid:81)d
κ (P ,K), which
i=1 i i
scaleexponentiallybadlywithd. Developingtightboundsfordeterministic-scancoordinate-
wise samplers is an interesting direction for future work.
Our results provide simple and intuitive guidance for practitioners using MwG-type
schemes,whichistothinkseparatelyofthetwopotentialsourcesofslowmixing: (a)badcon-
ditionalupdatesand(b)strongdependenceamongparameters(whichmightslowdownGS).
Also, they justify applying the various techniques developed in the literature to improve GS
convergence [20, 46, 48, 35, 71, 34] to the broader class of MwG and general coordinate-wise
samplers.
Beyond being interesting per se (in terms of improving our understanding of commonly
used coordinate-wise samplers), the general results of Section 3 are motivated by and ap-
plied to the analysis of MwG schemes targeting high-dimensional non-conjugate Bayesian
hierarchical models, where we manage to derive dimension-free bounds on total variation
mixingtimesasJ →∞. AsillustratedinSection1.1, thesearecomputationallychallenging
(as well as widely used) models, where competing sampling algorithms (including popular
gradient-based MCMC schemes, such as NUTS) exhibit, either empirically, theoretically or
26both, a total computational cost scaling super-linearly with the number of groups J. On
the contrary, our results imply a total computational cost of coordinate-wise samplers that
scaleslinearlywithJ,thusprovidingstate-of-the-artperformancesforhigh-dimensionalnon-
conjugate Bayesian hierarchical models.
Our use of the approximate conductance and of perturbation results (see e.g. Section
5.1) is motivated by statistical applications, where we seek to combine MCMC convergence
analysis with Bayesian posterior asymptotic results (such as the celebrated Bernstein-von
Mises theorem). Note that, despite being a powerful and potentially useful approach in
various contexts, rigorous combinations of MCMC theory and Bayesian asymptotics are
relatively scarce in the literature and, beyond few exceptions [60, 4, 33], mostly recent [44,
43, 66, 3]. We hope that the techniques developed in this paper might serve as starting
pointtodevelopabetterquantitativeunderstandingofpopularcoordinate-wisesamplersfor
otherclassesofhigh-dimensionalstructuredBayesianmodels(e.g.timesseries,factormodels,
Gaussian processes, etc), thus reducing the significant gap between theory and practice still
present in this area.
Funding. GZ acknowledges support from the European Research Council (ERC),
through StG “PrSc-HDBayLe” grant ID 101076564. GOR was supported by EPSRC grants
BayesforHealth(R018561)CoSInES(R034710)PINCODE(EP/X028119/1),EP/V009478/1
and by the UKRI grant, OCEAN, EP/Y014650/1.
References
[1] Amit, Y. (1996). Convergence properties of the Gibbs sampler for perturbations of
Gaussians. The Annals of Statistics 24(1), 122–140.
[2] Andrieu, C., A. Lee, S. Power, and A. Q. Wang (2022). Explicit convergence bounds
for Metropolis Markov chains: isoperimetry, spectral gaps and profiles. arXiv preprint
arXiv:2211.08959.
[3] Ascolani, F. and G. Zanella (2024). Dimension-free mixing times of Gibbs samplers for
Bayesian hierarchical models. Ann. Statist. In press.
[4] Belloni, A. and V. Chernozhukov (2009). On the computational complexity of MCMC-
based estimators in large samples.
[5] Besag, J. and P. J. Green (1993). Spatial statistics and Bayesian computation. Journal
of the Royal Statistical Society Series B: Statistical Methodology 55(1), 25–37.
[6] Beskos, A., O. Papaspiliopoulos, and G. O. Roberts (2006). Retrospective exact simula-
tion of diffusion sample paths with applications. Bernoulli 12(6), 1077–1098.
27[7] Beskos, A., N. Pillai, G. Roberts, J. Sanz-Serna, and A. Stuart (2013). Optimal tuning
of the hybrid Monte Carlo algorithm. Bernoulli 19, 1501–1534.
[8] Biswas,N.,P.E.Jacob,andP.Vanetti(2019). EstimatingconvergenceofMarkovchains
with L-lag couplings. Advances in Neural Information Processing Systems 32.
[9] Bobkov, S. G. and C. Houdr´e (1997). Isoperimetric constants for product probability
measures. The Annals of Probability, 184–205.
[10] Brooks, S., A. Gelman, G. L. Jones, and X. Meng (2011). Handbook of Markov Chain
Monte Carlo. Chapman and Hall.
[11] Caprio,R.andA.Johansen(2023). AcalculusforMarkovchainMonteCarlo: studying
approximations in algorithms. arXiv preprint arXiv:2310.03853.
[12] Casella, G. and E. I. George (1992). Explaining the Gibbs Sampler. Am. Stat. 46,
167–174.
[13] Chlebicka, I., K.Latuszynski, andB.Miasojedow(2023). SolidarityofGibbsSamplers:
the spectral gap. arXiv preprint arXiv:2304.02109.
[14] Dalalyan,A.S.(2017). TheoreticalGuaranteesforApproximateSamplingfromSmooth
and Log-Concave Densities. J. R. Stat. Soc. Ser. B. 79, 651–676.
[15] Diaconis, P., K. Khare, and L. Saloff-Coste (2008). Gibbs Sampling, Exponential Fam-
ilies and Orthogonal Polynomials. Stat. Sci. 23, 151–178.
[16] Diaconis, P., K. Khare, and L. Saloff-Coste (2010). Stochastic alternating projections.
Illinois Journal of Mathematics 54(3), 963–979.
[17] Durmus, A. and E. Moulines (2017). Nonasymptotic convergence analysis for the un-
adjusted Langevin algorithm. Ann. Appl. Probab. 27, 1551–1587.
[18] Dwivedi, R., Y. Chen, M. J. Wainwright, and B. Yu (2019). Log–concave sampling:
Metropolis–Hastings algorithms are fast! J. Mach. Learn. Res. 20, 1–42.
[19] Flegal,J.M.,J.Hughes,D.Vats,K.Gupta,andU.Maji(2021). mcmcse: MonteCarlo
Standard Errors for MCMC. R package.
[20] Gelfand, A. E., S. K. Sahu, and B. P. Carlin (1995). Efficient parametrisations for
normal linear mixed models. Biometrika 82(3), 479–488.
[21] Gelfand, A. E. and A. F. Smith (1990). Sampling-based approaches to calculating
marginal densities. Journal of the American statistical association 85(410), 398–409.
[22] Gelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin
(2013). Bayesian Data Analysis. CRC press.
28[23] Gelman, A. and J. L. Hill (2007). Data Analysis Using Regression and Multi-
level/Hierarchical Models. Cambridge University Press.
[24] Gilks, W. R. and P. Wild (1992). Adaptive Rejection Sampling for Gibbs Sampling. J.
R. Stat. Soc. Ser. C 41, 337–348.
[25] Gong, L. and J. M. Flegal (2015). A Practical Sequential Stopping Rule for High-
Dimensional Markov Chain Monte Carlo. J. Comput. Graph. Stat. 25, 684–700.
[26] Green, P. J., K. Latuszynski, M. Pereyra, and C. P. Robert (2015). Bayesian com-
putation: a summary of the current state, and samples backwards and forwards. Stat.
Comput. 25, 835–862.
[27] Hoffman, M. D. and A. Gelman (2014). The No-U-Turn sampler: adaptively setting
path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res. 15(1), 1593–1623.
[28] Jarner, S. F. and E. Hansen (2000). Geometric ergodicity of Metropolis algorithms.
Stochastic processes and their applications 85(2), 341–361.
[29] Jin, Z. and J. P. Hobert (2022). Dimension free convergence rates for Gibbs samplers
for Bayesian linear mixed models. Stoch. Process. Their Appl. 148, 25–67.
[30] Johnson, A. A., G. L. Jones, and R. C. Neath (2013). Component-wise Markov chain
Monte Carlo: Uniform and geometric ergodicity under mixing and composition.
[31] Jones, G. L., G. O. Roberts, and J. S. Rosenthal (2014). Convergence of conditional
Metropolis-Hastings samplers. Advances in Applied Probability 46(2), 422–445.
[32] Kamatani, K. (2014a). Local consistency of Markov chain Monte Carlo methods. Ann.
Inst. Stat. Math. 66, 63–74.
[33] Kamatani,K.(2014b).LocalconsistencyofMarkovchainMonteCarlomethods.Annals
of the Institute of Statistical Mathematics 66(1), 63–74.
[34] Kastner, G. and S. Fru¨hwirth-Schnatter (2014). Ancillarity-sufficiency interweaving
strategy (ASIS) for boosting MCMC estimation of stochastic volatility models. Compu-
tational Statistics & Data Analysis 76, 408–423.
[35] Khare, K. and J. P. Hobert (2011). A spectral analytic comparison of trace-class data
augmentation algorithms and their sandwich variants. The Annals of Statistics 39(5),
2585–2606.
[36] Khare,K.andH.Zhou(2009). RatesofconvergenceofsomemultivariateMarkovchains
with polynomial eigenfunctions. Ann. Appl. Probab. 2, 737–777.
[37] Levin, D. A. and Y. Peres (2017). Markov chains and mixing times, Volume 107.
American Mathematical Soc.
29[38] Livingstone, S. and G. Zanella (2022). The Barker proposal: combining robustness and
efficiency in gradient-based MCMC. Journal of the Royal Statistical Society Series B:
Statistical Methodology 84(2), 496–523.
[39] Lov´asz,L.andM.Simonovits(1993).RandomWalksinaConvexBodyandanImproved
Volume Algorithm. Random Struct. and Alg. 4, 359–412.
[40] Madras, N. and D. Randall (2002). Markov chain decomposition for convergence rate
analysis. Annals of Applied Probability, 581–606.
[41] Martin, G. M., D. T. Frazier, and C. P. Robert (2023). Computing Bayes: From Then
‘Til Now. Stat. Sci. In press.
[42] Neath,R.C.andG.L.Jones(2009). Variable-at-a-timeimplementationsofMetropolis-
Hastings. arXiv preprint arXiv:0903.0664.
[43] Negrea,J.,J.Yang,H.Feng,D.M.Roy,andJ.H.Huggins(2022). Statisticalinference
with stochastic gradient algorithms. arXiv preprint arXiv 2207.
[44] Nickl, R. and S. Wang (2022). On polynomial-time computation of high-dimensional
posterior measures by Langevin-type algorithms. Journal of the European Mathematical
Society.
[45] Papaspiliopoulos, O., G. Roberts, and G. Zanella (2020). Scalable inference for crossed
random effects models. Biometrika 107, 25–40.
[46] Papaspiliopoulos, O., G. O. Roberts, and M. Sk¨old (2003). Non-Centered Parameter-
izations for Hierarchical Models and Data Augmentation (with discussion). In Bayesian
Statistics (J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P. Dawid, D. Heckerman, A.
F. M. Smith and M. West, eds.), pp. 307–326.
[47] Papaspiliopoulos, O., G. O. Roberts, and O. Stramer (2013). Data augmentation for
diffusions. Journal of Computational and Graphical Statistics 22(3), 665–688.
[48] Papaspiliopoulos, O., G. O. R. Roberts, and M. Sk¨old (2007). A General Framework
for the Parametrization of Hierarchical Models. Statistical Science, 59–73.
[49] Papaspiliopoulos, O., T. Stumpf-F´etizon, and G. Zanella (2023). Scalable computation
for Bayesian hierarchical models. arXiv preprint arXiv:2103.10875.
[50] Polson, N. G. and G. O. Roberts (1994). Bayes factors for discrete observations from
diffusion processes. Biometrika 81(1), 11–26.
[51] Qin, Q.andJ.P.Hobert(2019). ConvergencecomplexityanalysisofAlbertandChib’s
algorithm for Bayesian probit regression. Ann. Statist. 47, 2320–2347.
30[52] Qin,Q.andJ.P.Hobert(2022). Wasserstein-basedmethodsforconvergencecomplexity
analysis of MCMC with applications. Ann. Appl. Prob. 32, 124–166.
[53] Qin,Q.andG.L.Jones(2022). Convergenceratesoftwo-componentMCMCsamplers.
Bernoulli 28(2), 859–885.
[54] Qin, Q., N. Ju, and G. Wang (2023). Spectral gap bounds for reversible hybrid Gibbs
chains. arXiv preprint arXiv:2312.12782.
[55] Qin, Q. and G. Wang (2022). Spectral Telescope: Convergence Rate Bounds for
Random-Scan Gibbs Samplers Based on a Hierarchical Structure. arXiv preprint
arXiv:2208.11299.
[56] Roberts, G. and J. Rosenthal (1997). Geometric ergodicity and hybrid Markov chains.
[57] Roberts, G. O. and J. S. Rosenthal (1998). Optimal scaling of discrete approximations
to Langevin diffusions. J. R. Stat. Soc. Ser. B 60, 255–268.
[58] Roberts,G.O.andJ.S.Rosenthal(2001).MarkovChainsandDe-InitializingProcesses.
Scand. J. Stat. 28, 489–504.
[59] Roberts,G.O.andS.H.Sahu(1997). UpdatingSchemes,CorrelationStructure,Block-
ing and Parameterization for the Gibbs Sampler. J. R. Stat. Soc. Ser. B 59, 291–317.
[60] Roberts, G. O. and S. K. Sahu (2001). Approximate predetermined convergence prop-
erties of the Gibbs sampler. Journal of Computational and Graphical Statistics 10(2),
216–229.
[61] Roberts, G. O. and O. Stramer (2001). On inference for partially observed nonlinear
diffusion models using the Metropolis–Hastings algorithm. Biometrika 88(3), 603–621.
[62] Rosenthal, J. S. (1995). Minorization Conditions and Convergence Rates for Markov
Chain Monte Carlo. J. Am. Stat. Assoc 90, 558–566.
[63] Smith, A. F. and G. O. Roberts (1993). Bayesian computation via the Gibbs sampler
and related Markov chain Monte Carlo methods. Journal of the Royal Statistical Society:
Series B (Methodological) 55(1), 3–23.
[64] Stan Development Team (2024). RStan: the R interface to Stan. R package version
2.32.5.
[65] Tang, R. and Y. Yang (2022a). Computational Complexity of Metropolis-Adjusted
Langevin Algorithms for Bayesian Posterior Sampling. arXiv preprint arXiv:2206.06491.
[66] Tang, R. and Y. Yang (2022b). On the Computational Complexity of Metropolis-
Adjusted Langevin Algorithms for Bayesian Posterior Sampling. arXiv preprint
arXiv:2206.06491.
31[67] Tong, X. T., M. Morzfeld, and Y. M. Marzouk (2020). MALA-within-Gibbs samplers
for high-dimensional distributions with sparse conditional structure. SIAM Journal on
Scientific Computing 42(3), A1765–A1788.
[68] Van der Vaart, A. W. (2000). Asymptotic Statistics. Cambridge University Press.
[69] Wu, K., S. Schmidler, and Y. Chen (2022). Minimax Mixing Time of the Metropolis-
Adjusted Langevin Algorithm for Log-Concave Sampling. J. Mach. Learn. Res. 23, 1–63.
[70] Yang, J. and J. S. Rosenthal (2022). Complexity results for MCMC derived from
quantitative bounds. Ann. Appl. Prob. 33, 1459–1500.
[71] Yu, Y. and X. L. Meng (2011). To center or not to center: That is not the question:
an Ancillarity–Sufficiency Interweaving Strategy (ASIS) for boosting MCMC efficiency.
Journal of Computational and Graphical Statistics 20(3), 531–570.
32Appendix A Additional examples
A.1 Tightness of the bound in Corollary 1
Define
d
1(cid:88)
P = P , P =cG +(1−c)I, (47)
d i i i
i=1
for i = 1,...,d and c ∈ (0,1). It follows that P (∂A) = cG (∂A) for every A ⊆ X, so that
i i
Φ(P)=cΦ(G). Also, by (47), we have
Px−i(x ,dy )=cGx−i(x ,dy )+(1−c)δ (y ),
i i i i i i xi i
so that
(cid:90) (cid:90)
Px−i(x ,Bc)π (dx |x )=c Gx−i(x ,Bc)π (dx |x )=cπ (B |x )π (Bc |x )
i i i i −i i i i i −i i −i i −i
B B
for every B ⊆ X By (8) we conclude κ(Px−i) = c for every x ∈ X, from which it follows
i i
Φ(P)=κ(P)Φ(G) as desired.
A.2 Convergence of the stationary distributions does not imply
convergence of the conductances
LetX =R2andπbeabivariatestandardnormaldistribution. Defineπ tobethetruncation
n
of π on the set C , where
n
(cid:91)
C ={(−∞,n]×(−∞,n]} {[n,+∞)×[n,+∞)}.
n
Let G and G be the operators of the associated Gibbs samplers. Then, it is not difficult to
n
show that ∥π −π∥ → 0 as n → ∞ and Φ(G ) = 0 for every n, since it suffices to take
n TV n
A = [n,+∞)×[n,+∞) in (4). On the other hand Φ(G) = 1, since G is a Gibbs Sampler
(GS) on a distribution with independent components.
Appendix B Alternative definition of s-conductance
An alternative definition relative to (4) is given by
(cid:26) (cid:27)
P(∂A) 1
Φ˜ (P)=inf ; s<π(A)≤ ,A⊂X . (48)
s π(A)−s 2
Note that Φ (P)≥Φ˜ (P) for every s≥0 and Φ˜ (P)=Φ (P)=Φ(P).
s s 0 0
ThenexttheoremshowsthattheconclusionsofCorollary1holdalsofortheconductance
defined as in (48).
Theorem 6. Let P be a coordinate-wise operator as in (5), with target distribution π ∈
P(X), and κ(P,K) as in (7). Then for every 0≤s≤1/2 and K ⊆X we have
Φ˜ (P)≥κ(P,X)Φ˜ (G) (49)
s s
33and
Φ˜
(P)≥κ(P,K)2(s−s′)
Φ˜ (G)−
2π(Kc)(cid:32) 1(cid:88)d
κ (P
,K)(cid:33)
. (50)
s 1−2s s′ 1−2s d i i
i=1
Proof. Let A⊂X. By Theorem 1 we have
d d d
1(cid:88) 1(cid:88) 1(cid:88)
P(∂A)= P (∂A)≥ κ (P ,X)G (∂A)≥κ(P) G (∂A).
d i d i i i d i
i=1 i=1 i=1
The inequality in (49) is obtained from the above by dividing by π(A)−s and taking the
infimum over A⊂X such that 1/2≥π(A)>s.
We now consider (50). Let A ⊂ X be such that 1/2 ≥ π(A) > s. Since π(A)−s ≤
(1−2s)/2, we have
(cid:18) (cid:19) (cid:18) (cid:19)(cid:32) d (cid:33)
P(∂A) 2 2 1(cid:88)
≥ κ(P,K)G(∂A)− κ (P ,K) π(Kc).
π(A)−s 1−2s 1−2s d i i
i=1
Since π(A)>s we also have
π(A)−s′ G(∂A)
G(∂A)= G(∂A)≥(s−s′) .
π(A)−s′ π(A)−s′
The above imply
P(∂A) 2(s−s′) G(∂A) 1(cid:88)d (cid:18) 2 (cid:19)
≥κ(P,K) − κ (P ,K) π(Kc).
π(A)−s 1−2s π(A)−s′ d i i 1−2s
i=1
The desired inequality follows by taking the infimum over A ⊂ X such that 1/2 ≥ π(A) >
s.
Appendix C Regularity assumptions (B4)-(B6) for model
(20)
(cid:16) (cid:17)
Define T =
(cid:80)J
T (θ
),...,(cid:80)J
T (θ ) , with T as in (21), and let
j=1 1 j j=1 S j s
M(p)(ψ |y)=E[Tp(θ )|Y =y,ψ] , (51)
s s j j
M(p)(ψ |y)=E[Tp(θ )Tp(θ )|Y =y,ψ], (52)
s,s′ s j s′ j j
(cid:16) (cid:17)
betheposteriormomentsofT givenψ,denoteM(p)(ψ |y)= M(p)(ψ |y),...,M(p)(ψ |y) ∈
1 S
RS and
(cid:104) (cid:105)
[C(ψ)] =E ∂ M(1)(ψ |Y ) , [V(ψ)] =E [Cov(T (θ ),T (θ )|Y ,ψ)], (53)
s,d Yj ψd s j s,s′ Yj s j s′ j j
with s,s′ = 1,...S and d = 1,...,D. Moreover we write B for the ball of center ψ∗ and
δ
radius δ, and denote expectations with respect to the law of Y as defined in (B1) by E [·].
j Yj
Finally, we define the posterior characteristic function of T(θ ) = (T (θ ),...,T (θ )) and
j 1 j S j
(cid:104) (cid:105)
(cid:80)k j=1T(θ j),givenψ,asφ(t|Y j,ψ)=E eit⊤T(θj) |Y j,ψ fort∈RS. andφ(k)(t|Y 1:k,ψ)=
(cid:81)k
φ(t|Y ,ψ), respectively. Assumptions (B4)−(B6) now read:
j=1 j
34(B4) The expectation M(p)(ψ | y) is well defined for every y and p = 1,...,6. More-
s
over, there exist δ > 0 and C finite constant such that for every ψ ∈ B it holds
4 δ4
(cid:104)(cid:12) (cid:12)(cid:105) (cid:104)(cid:12) (cid:12)(cid:105)
E (cid:12)∂ M(6)(ψ |Y )(cid:12) <C, E (cid:12)∂ ∂ M(1)(ψ |Y )(cid:12) <C,
Yj (cid:12) ψd s j (cid:12) Yj (cid:12) ψd ψ d′ s j (cid:12)
(cid:104)(cid:12) (cid:12)(cid:105) (cid:104)(cid:12) (cid:110) (cid:111)(cid:12)(cid:105)
E (cid:12)∂ M(1)(ψ |Y )(cid:12) < C and E (cid:12)∂ M(1)(ψ |Y )M(1)(ψ |Y ) (cid:12) < C for
Yj (cid:12) ψd s,s′ j (cid:12) Yj (cid:12) ψd s j s′ j (cid:12)
s,s′ = 1,...,S and d,d′ = 1,...,D. Finally, the matrix V(ψ∗) defined in (53) is
non singular.
(B5) There exist k ≥1 and δ >0 such that
5
(cid:90) (cid:12) (cid:12)2
sup (cid:12)φ(k)(t|Y ,ψ)(cid:12) dt<∞,
(cid:12) 1:k (cid:12)
ψ∈Bδ5 RS
iid
for almost every Y ,...,Y ∼ Q .
1 k ψ∗
(B6) There exist k′ ≥1 and δ >0 such that
6
(cid:12) (cid:12)
sup sup(cid:12)φ(k′)(t|Y ,ψ)(cid:12)<ϕ(ϵ),
(cid:12) 1:k′ (cid:12)
ψ∈Bδ6 |t|>ϵ
iid
for almost every Y ,...,Y ∼ Q , with ϕ(ϵ)<1 for every ϵ>0.
1 k ψ∗
Some discussion on the interpretation and applicability of assumptions (B4)-(B6) can be
found in Appendix B of [3].
Appendix D Proofs
D.1 Proof of Theorem 1
First we introduce some notation and a preliminary lemma. For every A ⊆ X, x ∈ X and
i=1,...,d we write
A (x )=A∩{y∈X : y =x ,j ̸=i}, and Ac(x )=Ac∩{y∈X : y =x ,j ̸=i}.
i −i j j i −i j j
For every A⊆X, i=1,...,d and x ∈X we denote
−i −i
A (x )={x ∈X : x∈A}, Ac(x )={x ∈X : x̸∈A}.
i −i i i i −i i i
Notice that A (x )⊆X and Ac(x )⊆X .
i −i i i −i i
Proof of Theorem 1. By (5) we have
(cid:90) (cid:90) (cid:90)
P (∂A)= P (x,Ac)π(dx)= Px−i(x ,Ac(x ))π (dx |x )π(dx )
i i i i i −i i i −i −i
A x−i Ai(xi)
(cid:90) (cid:34)(cid:82) Px−i(x ,Ac(x ))π (dx |x )(cid:35)
= Ai(xi) i i i −i i i −i π (A (x )|x )π (Ac(x )|x )π(dx ),
π (A (x )|x )π (Ac(x )|x ) i i −i −i i i −i −i −i
x−i i i −i −i i i −i i
(54)
where, with a slight abuse of notation, π(dx ) denotes the marginal distribution of X
−i −i
under X ∼π. By (8) we have
(cid:82) Px−i(x ,Ac(x ))π (dx |x )
Ai(xi) i i i −i i i −i ≥κ(cid:0) Px−i(cid:1)
π (A (x )|x )π (Ac(x )|x ) i
i i −i −i i i −i −i
35and thus
(cid:90)
P (∂A)≥ κ(cid:0) Px−i(cid:1) π (A (x )|x )π (Ac(x )|x )π(dx )
i i i i −i −i i i −i −i −i
x−i
(cid:90)
= κ(cid:0) Px−i(cid:1) π (Ac(x )|x )π(dx)
i i i −i −i
x∈A
(cid:90)
≥ κ(cid:0) Px−i(cid:1) π (Ac(x )|x )π(dx)
i i i −i −i
x∈A∩K
(cid:90)
≥κ (P ,K) π (Ac(x )|x )π(dx).
i i i i −i −i
x∈A∩K
Moreover
(cid:90) (cid:90) (cid:90)
π (Ac(x )|x )π(dx)= π (Ac(x )|x )π(dx)− π (Ac(x )|x )π(dx)
i i −i −i i i −i −i i i −i −i
x∈A∩K x∈A x∈A∩Kc
(cid:90)
≥ π (Ac(x )|x )π(dx)−π(A∩Kc).
i i −i −i
x∈A
Combining the two previous inequalities and using π (Ac(x ) | x ) = Gx−i(x ,Ac(x ))
i i −i −i i i i −i
we get
(cid:18)(cid:90) (cid:19)
P (∂A)≥κ (P ,K) π (Ac(x )|x )π(dx)−π(A∩Kc)
i i i i i −i −i
x∈A
(cid:18)(cid:90) (cid:19)
=κ (P ,K) π(dx)G (x,Ac)−π(A∩Kc)
i i i
x∈A
as desired. If Px−i is reversible and positive semi-definite, it is possible to show that
i
(cid:82) Px−i(x ,Ac(x ))π(dx |x )
Ai(xi) i i i −i i −i
≤1. (55)
π (A (x )|x )π (Ac(x )|x )
i i −i −i i i −i −i
Indeed, since Px−i is invariant with respect to π(·|x ) it holds
i −i
(cid:90) (cid:90)
Px−i(x ,Ac(x ))π(dx |x )+ Px−i(x ,Ac(x ))π(dx |x )=π (Ac(x )|x ).
i i i −i i −i i i i −i i −i i i −i −i
Ai(xi) Ac i(xi)
Moreover, since Px−i is positive semi-definite by e.g. Lemma 1.1 in [39] we have
i
(cid:90)
Px−i(x ,Ac(x ))π(dx |x )≥π2(Ac(x )|x ),
i i i −i i −i i i −i −i
Ac i(xi)
from which (55) follows. Combining (54) with (55) we obtain
(cid:90)
P (∂A)≤ π (A (x )|x )π (Ac(x )|x )π(dx )=G (∂A).
i i i −i −i i i −i −i −i i
x−i
D.2 Proof of Corollary 1
Proof. Let A⊂X. By Theorem 1 we have
d d d
1(cid:88) 1(cid:88) 1(cid:88)
P(∂A)= P (∂A)≥ κ (P ,X)G (∂A)≥κ(P.X) G (∂A).
d i d i i i d i
i=1 i=1 i=1
The first inequality in (9) is obtained dividing by π(A) and taking the infimum over A⊂X
such that 1/2 ≥ π(A) > s. The inequalities in (49) follow by dividing by, respectively,
π(A)−s or π(A) and taking the infimum over A⊂X such that 1/2≥π(A)>s.
36As regards the other inequality, again by Theorem 1 we have
d (cid:32) d (cid:33)
1(cid:88) 1(cid:88)
P(∂A)≥ κ (P ,K)G (∂A)− κ (P ,K) π(A∩Kc)
d i i i d i i
i=1 i=1
(cid:32) d (cid:33)
1(cid:88)
≥κ(P,K)G(∂A)− κ (P ,K) π(Kc).
d i i
i=1
Let now A⊂X be such that 1/2≥π(A)>s. Therefore by the above we have
P(∂A) G(∂A) (cid:32) 1(cid:88)d (cid:33) π(Kc)
≥κ(P,K) − κ (P ,K) ,
π(A) π(A) d i i s
i=1
from which the right inequality in (9) follows.
Finally, if Px−i is reversible and positive semi-definite, again by Theorem 1 we have
d d
1(cid:88) 1(cid:88)
G(∂A)= G (∂A)≥ P (∂A)=P(∂A),
d i d i
i=1 i=1
from which we immediately deduce Φ (G)≥Φ (P).
s s
D.3 Proof of Proposition 1
Proof. By definition of M, for every x and y we have
i i
(cid:26) (cid:27)
1 dπ (·|x ) 1 dπ (·|x )
α(x ,y )≥min 1, i −i (y ) = i −i (y ),
i i M dQ i M dQ i
i i
so that by (10) it holds
1 (cid:90) dπ (·|x ) 1
Px−i(x ,Bc)≥ i −i (y )Q (dy )= π (Bc |x ),
i i M dQ i i i M i −i
Bc i
for every B ⊂ X . This implies that κ(Px−i) ≥ 1/M for every i = 1,...,d and the result
i i
follows by Corollary 1.
D.4 Proof of Proposition 2
We need a preliminary lemma.
Lemma 5. Let π be a log-concave distribution on Rd with parameters m and l and mode
x∗. Then for very x∈Rd it holds
(cid:18) (cid:19)d
π(x) L 2
≤ .
N(cid:0) x|x∗, 1I (cid:1) m
m d
Proof. Denoting f(x)=−logπ(x) we have
π(x) m d (cid:16)m(cid:17)
log =−f(x)+ ∥x−x∗∥2− log
N(cid:0) x|x∗, 1I (cid:1) 2 2 2 2π
m d
(cid:104) m (cid:105) d (cid:16)m(cid:17)
=−f(x∗)− f(x)−f(x∗)− ∥x−x∗∥2 − log (56)
2 2 2 2π
d (cid:16)m(cid:17)
≥−f(x∗)− log ,
2 2π
37by (13). Moreover, notice that
(cid:90) (cid:90)
1=
e−f(x)dx=e−f(x∗) e−[f(x)−f(x∗)]dx.
Rd Rd
By (13) it holds f(x)−f(x∗)≤ L∥x−x∗∥2 which implies
2 2
(cid:90) e−[f(x)−f(x∗)]dx≥(cid:90)
e−L 2∥x−x∗∥2
2dx=(cid:18) 2π(cid:19)d
2 ,
L
Rd Rd
so that
(cid:18) (cid:19)d
e−f(x∗)
≤
L 2
.
2π
Combining this with (56) the result follows.
(cid:16) (cid:17)di
Proof of Proposition 2. ByLemma5itispossibletoapplyProposition1withM =min inf m(x−i) 2 ,
i x−i L(x−i)
from which the result follows.
D.5 Proof of Proposition 3
Proof. The first part of the statement follows directly from Corollary 35 in [2], while the
second part is a consequence of Corollary 1.
D.6 Proof of Theorem 2
Proof. For every i=1,2 and A⊂X denote with πA(·)=π (·∩A)/π (A) the restriction of
i i i
π to A. It is clear that πA ∈N (π ,1/π (A)). Fix now A⊂X such that s<π (A)≤A and
i i i i 1
notice that by the triangular inequality we have
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)P π1( (∂ AA )) − P π2( (∂ AA ))(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12)π 1AP 1(Ac)−π 2AP 2(Ac)(cid:12) (cid:12)
1 2
≤(cid:12) (cid:12)π 1AP 1(Ac)−π 1AP 2(Ac)(cid:12) (cid:12)+(cid:12) (cid:12)π 1AP 2(Ac)−π 2AP 2(Ac)(cid:12) (cid:12)
≤∆(P 1,P 2,1/s)+(cid:13) (cid:13)π 1A−π 2A(cid:13) (cid:13)
TV
.
Moreover, for every B ⊂X we have
(cid:12) (cid:12)
(cid:12) (cid:12)πA(B)−πA(B)(cid:12) (cid:12)=(cid:12) (cid:12)π 1(B∩A)
−
π 2(B∩A)(cid:12)
(cid:12)
1 2 (cid:12) π (A) π (A) (cid:12)
1 2
(cid:12) (cid:12) (cid:12) (cid:12)
≤(cid:12) (cid:12)π 1(B∩A)
−
π 2(B∩A)(cid:12) (cid:12)+(cid:12) (cid:12)π 2(B∩A)
−
π 2(B∩A)(cid:12)
(cid:12)
(cid:12) π (A) π (A) (cid:12) (cid:12) π (A) π (A) (cid:12)
1 1 1 2
(cid:12) (cid:12) (cid:12) (cid:12)
≤
δ
s
+(cid:12)
(cid:12)
(cid:12)π
(1
A)
−
π
(1 A)(cid:12)
(cid:12) (cid:12)π 2(A)=
δ
s
+(cid:12)
(cid:12)
(cid:12)π 2(A π)− (Aπ )1(A)(cid:12)
(cid:12)
(cid:12)
1 2 1
2δ
≤ .
s
Therefore we have
(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)P π1( (∂ AA ))
−
P π2( (∂ AA ))(cid:12)
(cid:12) (cid:12)≤∆(P 1,P 2,1/s)+
2 sδ
,
1 2
38for every A⊂X such that s<π (A)≤A, so that it holds
1
(cid:26) (cid:27)
P (∂A) 1
Φ (P )=inf 1 , s<π (A)≤
s 1 π (A) 1 2
1
(cid:26) (cid:27)
P (∂A) 1 2δ
≥inf 2 , s<π (A)≤ +∆(P ,P ,1/s)+
π (A) 1 2 1 2 s
2
(cid:26) (cid:27)
P (∂A) 1 2δ
≥inf 2 , s−δ <π (A)≤ +∆(P ,P ,1/s)+
π (A) 2 2 1 2 s
2
2δ
=Φ (P )−∆(P ,P ,1/s)− ,
s−δ 2 1 2 s
as desired.
D.7 Proof of Lemma 2
It is easy to prove that an equivalent way to define N as in (2) is given by
(cid:26) (cid:27)
dµ
N (π,M)= µ∈P(X) : (x)≤M for all A⊆X , M ≥1, π ∈P(X), (57)
dπ
where dµ denotes the Radon-Nikodym derivative of µ with respect to π.
dπ
Proof of Lemma 2. Recall that, for π ∈P(X) and π ∈P(X), the total variation distance
1 2
is defined as
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
∥π 1−π 2∥
TV
= sup (cid:12)
(cid:12)
f(x)π 1(dx)− f(x)π 2(dx)(cid:12) (cid:12),
f∈H(X) X X
where
H(X)={f |f : X → [0,1] measurable}.
For notational convenience, in the following we denote
f(x ,y )=f(x ,...,x ,y ,x ,...,x ),
−i i 1 i−1 i i+1 d
for every f ∈ H(X) and i = 1,...,d. Moreover, we write π (dx | dx ), with j = 1,2 and
j,i i i
i=1,...,d, to denote the conditional distribution of the i-th coordinate induced by π .
j
Let now f ∈H(X) and µ∈N(π ,M). Then with j =1,2 we have
1
(cid:90) (cid:90) (cid:90)
f(y)µG (dy)= f(x ,y )π (dy |x )µ(dx)
j,i −i i j,i i −i
X X Xi
(cid:90) (cid:90) dµ
= (x)f(x ,y )π (dy |x )π (dx).
dπ −i i j,i i −i 1
X Xi 1
Then we have
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
(cid:12) f(dy)µG 1,i(dy)− f(dy)µG 2,i(dy)(cid:12)
(cid:12) (cid:12)
X X
=M(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:90) (cid:90) dd πµ (x)f(x M−i,y i)
π 1,i(dy i |x −i)π
1(dx)−(cid:90) (cid:90) dd πµ (x)f(x M−i,y i)
π 2,i(dy i |x −i)π
1(dx)(cid:12) (cid:12)
(cid:12)
(cid:12)
X Xi 1 X Xi 1
(cid:12)(cid:90) (cid:90) (cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
=M(cid:12) g(x,y i)π 1,i(dy
i
|x −i)π 1(dx)− g(x,y)π 2,i(dy
i
|x −i)π 1(dx)(cid:12),
(cid:12) (cid:12)
X Xi X Xi
where g(x,y ) = dµ (x)f(dx−i,y) ∈ H(X × X ), by (57). Moreover, notice that we can
i dπ1 M i
disintegrate as follows
π (dy |x )π (dx)=π (dx |x )π (dy |x )π (dx ),
j,i i −i 1 1,i i −i j,i i −i 1 −i
39with j =1,2. Therefore
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
(cid:12) f(y)µG 1,i(dy)− f(y)µG 2,i(dy)(cid:12)
(cid:12) (cid:12)
X X
(cid:12)(cid:90) (cid:90)
(cid:12)
M(cid:12) g(x,y i)π 1,i(dx
i
|x −i)π 1,i(dy
i
|x −i)π 1(dx −i)
(cid:12)
=
X Xi
(cid:90) (cid:90) (cid:12)
(cid:12)
− g(x,y i)π 1,i(dx
i
|x −i)π 2,i(dy
i
|x −i)π 1(dx −i)(cid:12)
(cid:12)
X Xi
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
=M(cid:12) h(x −i,y i)π 1,i(dy
i
|x −i)π 1(dx −i)− h(x −i,y i)π 2,i(dy
i
|x −i)π 1(dx −i)(cid:12),
(cid:12) (cid:12)
X X
(cid:82)
where h(x ,y )= g(x,y)π (dx |x ) ∈H(X). Thus we get that
−i i Xi 1,i i −i
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
∥µG 1,i−µG 2,i∥ TV ≤M sup (cid:12) (cid:12) h(x −i,y i)π 1,i(dy i |x −i)π 1(dx −i)− h(x −i,y i)π 2,i(dy i |x −i)π 1(dx −i)(cid:12) (cid:12).
h∈H(X) X X
By the triangular inequality, for every h∈H(X) we have
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
(cid:12) h(x −i,y i)π 1,i(dy i |x −i)π 1(dx −i)− h(x −i,y i)π 2,i(dy i |x −i)π 1(dx −i)(cid:12)
(cid:12) (cid:12)
X X
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
≤(cid:12) h(x −i,y i)π 1,i(dy
i
|x −i)π 1(dx −i)− h(x −i,y i)π 2,i(dy
i
|x −i)π 2(dx −i)(cid:12)
(cid:12) (cid:12)
X X
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
+(cid:12) h(x −i,y i)π 2,i(dy
i
|x −i)π 2(dx −i)− h(x −i,y i)π 2,i(dy
i
|x −i)π 1(dx −i)(cid:12)
(cid:12) (cid:12)
X X
≤2∥π −π ∥ ,
1 2 TV
as desired.
D.8 Proof of Theorem 3
Proof. Let M =1/s. By G
=d−1(cid:80)d
G , G
=d−1(cid:80)d
G , and the definition of ∆
1 i=1 1,i 2 i=1 2,i
in (18), we have
d
1(cid:88)
∆(G ,G ,1/s)≤ ∆(G ,G ,1/s),
1 2 d 1,i 2,i
i=1
where we used the triangle inequality for the total variation norm and the fact that G and
1
G for i = 1,...,d share the same invariant distribution π . Combining the latter with
1,i 1
Lemma 2 we obtain
2δ
∆(G ,G ,1/s)≤ .
1 2 s
The desired statement follows by combining the above inequality with Theorem 2.
D.9 Proof of Lemma 3
Proof. DenotewithGap(P)thespectralgapoftheoperatorP (seee.g. [2]forthedefinition).
It is well-known (see e.g. Lemma 2 in [45]) that Gap(P) = min Gap(P ). By
j∈{1,...,J} j
Cheeger’s bounds (e.g. Lemma 5 in [2]) we have
Gap(P) Gap(P ) Φ2(P )
Φ(P)≥ = min j ≥ min j ,
2 j∈{1,...,J} 2 j∈{1,...,J} 4
as desired.
40D.10 Proof of Theorem 4
Proof. Fix s>0 and let K∗ =S∗×RℓJ. By Corollary 1 we have
1−π (K∗)
Φ (P )≥κ(P ,K∗)Φ (G )− J .
s j J s J s
By assumption (C) and (25) we have κ(P,K∗)≥κ2/4, so that
κ2 1−π (K∗)
Φ (P )≥ Φ (G )− J .
s j 4 s J s
Moreover, by the Bernstein-von Mises Theorem (e.g. Theorem 10.1 in [68]), whose assump-
tions are met thanks to (B1)−(B3), we can deduce
Q(J)(π (K∗)→1)→1,
ψ∗ J
as J →∞. Thus we conclude
(cid:18) κ2 (cid:19) (cid:18) 1−π (K∗) κ2 (cid:19)
Q(J) Φ (P )≥ Φ (G ) ≤Q(J) J ≤ Φ (G ) →1,
ψ∗ s J 8 s J ψ∗ s 8 s J
as J →∞.
D.11 Proof of Corollary 2
Proof. Combining Theorems 5 and 4 we get
(cid:18) κ2 (cid:19)
Q(J) Φ (P )≥ g(s) →1, (58)
ψ∗ s J 8
as J →∞, with g(s)>0 for every s>0. Let now
log(2M)−log(ϵ)
T(ψ∗,ϵ,M)= ,
(cid:16) (cid:17)
−log 1− κ4g2(ϵ/(2M))
128
then by Lemma 1 and (58) we conclude
Q(J)(t (P ,ϵ,M)≤T(ψ∗,ϵ,M))→1
ψ∗ mix J
as J →∞.
D.12 Proof of Theorem 5
We need a preliminary lemma, which is well-known and whose proof (inspired by the one of
Theorem 7.3 in [37] for discrete Markov chains) is included for completeness.
Lemma 6. Let P be a Markov kernel which is reversible with respect to π. Then it holds
(cid:13)
(cid:13)µ
APt−π(cid:13)
(cid:13)
TV
≥
1
2
−tP π( (∂ AA ))
for every A⊂X such that π(A)≤1/2 and t≥1, with µ (·)=π(·∩A)/π(A).
A
41Proof. Let B ⊂A. Then by reversibility of P we have
1
(cid:20)(cid:90) (cid:21)
(µ P)(B)−µ (B)= P(x,B)π(dx)−π(B)
A A π(A)
A
1
(cid:20)(cid:90) (cid:21)
= P(x,A)π(dx)−π(B) ≤0.
π(A)
B
If instead B ⊂Ac we have (µ P)(B)−µ (B)=(µ P)(B)≥0. Thus
A A A
P(∂A)
∥µ P −µ ∥ =(µ P)(Ac)= .
A A TV A π(A)
Usingrepeatedlythetriangleinequalityandthemonotonicityofthetotalvariationdistance
with respect to transition kernel multiplications, we obtain
(cid:13) (cid:13)µ APt−µ A(cid:13) (cid:13)
TV
≤tP π( (∂ AA )) (59)
for every t≥1. Moreover
1
∥µ −π∥ ≥π(Ac)≥ ,
A TV 2
so that by (59) and the triangle inequality
1
2
≤∥µ A−π∥
TV
≤(cid:13) (cid:13)µ APt−π(cid:13) (cid:13)
TV
+(cid:13) (cid:13)µ APt−µ A(cid:13) (cid:13)
TV
≤(cid:13)
(cid:13)µ
APt−π(cid:13)
(cid:13)
TV
+tP π( (∂ AA ))
,
from which the result follows.
We can use Lemma 6 to provide a lower bound on Φ (P) in terms of the corresponding
s
mixing times.
Lemma 7. Let P be a Markov kernel which is reversible with respect to π. For every
s,ϵ∈(0,1/2) we have
1/2−ϵ
Φ (P)≥ .
s t (P,ϵ,s−1)
mix
Proof. Let M =1/s. For any A⊆X with s<π(A)≤1/2 define
π(B∩A)
µ (B)= , B ⊆X .
A π(A)
By construction µ is a M-warm start. Moreover, by Lemma 6 we have
A
(cid:13)
(cid:13)µ
APt−π(cid:13)
(cid:13)
TV
≥
21 −tP π( (∂ AA ))
.
Taking the infimum with respect to A such that π(A)>s, we get
sup
(cid:13) (cid:13)µPt−π(cid:13)
(cid:13)
TV
≥ sup
(cid:13) (cid:13)µPt−π(cid:13)
(cid:13)
TV
≥
1
2
−tΦ s(P).
µ∈N(π,s−1) s<π(A)<1/2
It follows
(cid:32) (cid:33)
Φ s(P)≥t−1 21 − sup (cid:13) (cid:13)µPt−π(cid:13) (cid:13)
TV
.
µ∈N(π,s−1)
Taking t=t (P,ϵ,s−1) completes the proof.
mix
42We need moreover two other preliminary Lemmas. These can be seen as the analogue
of Lemma 4.1 and Theorem 4.2 in [3] to the setting of random-scan Gibbs sampler, and the
proofs follow similar lines. Let X = X ×X be a product space and π ∈ P(X), whose
1 2
associated Gibbs sampler has operator G as in (22). Let T : X → Xˆ such that
1 1
L(X |X )=L(X |T(X )) under (X ,X )∼π. (60)
2 1 2 1 1 2
(cid:16) (cid:17) (cid:16) (cid:17)
Let T(t),X(t) = T(X(t)),X(t) be the stochastic process obtained as a time-
2 1 2
t≥1 t≥1
(cid:16) (cid:17)
wise mapping of the Markov chain X(t),X(t) , with operator G, under (x ,x ) (cid:55)→
1 2 1 2
t≥1
(T(x ),x ). The latter process contains all the information characterising the convergence
1 2
(cid:16) (cid:17)
of X(t),X(t) , in the sense made precise in the following lemma. Below we denote
1 2
t≥1
by πˆ = L(T(X ),X ) under (X ,X ) ∼ π, i.e. the push-forward of π under (x ,x ) (cid:55)→
1 2 1 2 1 2
(T(x ),x ), by πˆ (dt | x ) and πˆ (dx | t) its conditional distributions and by Gˆ the kernel
1 2 1 2 2 2
of the two-block Gibbs sampler targeting πˆ. Under this notation (60) can be written as
π (dx |x )=πˆ (dx |T(x )).
2 2 1 2 2 1
(cid:16) (cid:17)
Lemma 8. Assume (60) holds. Then, the process T(t),X(t) is a Markov chain, its
2
t≥1
transition kernel coincides with Gˆ, and
t (G,ϵ,M)=t (Gˆ,ϵ,M) (M,ϵ)∈[1,∞)×(0,1).
mix mix
(cid:16) (cid:17) (cid:16) (cid:17)
Proof. The Markovianity of the sequence T(t),X(t) follows by the one of X(t) ,
2 2
t≥1 t≥1
(cid:16) (cid:17)
which is well known [15, 58]. We now show that T(t),X(t) admits Pˆ as kernel. Using
2
t≥1
the definition of G and the law of total probability conditioning on X(t), the conditional
1
(cid:16) (cid:17) (cid:16) (cid:17)
distribution of T(t),X(t) given T(t−1),X(t−1) is given by
2 2
(cid:18) (cid:19)
L dT(t),dX(t) |T(t−1),X(t−1)
2 2
1 (cid:90) (cid:16) (cid:17)
= δ (dX(t)) δ (dT(t))π dx |X(t−1)
2 X(t−1) 2 T(x1) 1 1 2
2
1 (cid:90) (cid:16) (cid:17)
+ δ (dT(t)) π (dX(t) |x )α dx |T(t−1),X(t−1)
2 T(t−1) 2 2 1 1 2
whereα(dx |t,x )denotestheconditionaldistributionofX givenT(X )=tandX =x
1 2 1 1 2 2
when (X ,X ) ∼ π. Note that, by construction, (cid:82) 1(T(x ) = t)α(dx |t,x ) = 1 where 1
1 2 1 1 2
denotes the indicator function. Combining the latter with (60) we have
(cid:90) (cid:16) (cid:17)
π (dX(t) |x )α dx |T(t−1),X(t−1)
2 2 1 1 2
(61)
(cid:90) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
= πˆ dX(t) |T(x ) α dx |T(t−1),X(t−1) =πˆ dX(t) |T(t−1) .
2 2 1 1 2 2 2
Also, by definition of πˆ we have
(cid:90) (cid:16) (cid:17) (cid:16) (cid:17)
δ (dT(t))π dx |X(t−1) =πˆ dT(t) |X(t−1) .
T(x1) 1 1 2 1 2
43Combining the above we obtain
(cid:18) (cid:19)
L dT(t),dX(t) |T(t−1),X(t−1)
2 2
1 (cid:16) (cid:17) 1 (cid:16) (cid:17)
= πˆ dT(t) |X(t−1) δ (X(t))+ πˆ dX(t) |T(t−1) δ (T(t))
2 1 2 X(t−1) 2 2 2 2 T(t−1)
2
(cid:16)(cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
=Gˆ T(t−1),X(t−1) , dT(t),dX(t) ,
2 2
(cid:16) (cid:17) (cid:16) (cid:17)
as desired. From the above one can easily deduce that X(t),X(t) and T(t),X(t)
1 2 2
t≥1 t≥1
are co-deinitializing as in [58] and thus, by Corollary 2 therein, for every µ ∈ P(X ×X )
1 2
we have
(cid:13) (cid:13)µGt−π(cid:13) (cid:13) TV =(cid:13) (cid:13) (cid:13)νGˆt−πˆ(cid:13) (cid:13) (cid:13) TV , (62)
(cid:16) (cid:17)
where ν ∈ P Xˆ ×X is the push forward of µ under (x ,x ) (cid:55)→ (T(x ),x ). Moreover,
1 2 1 2 1 2
by (2) we have that ν ∈ N (πˆ,M) whenever µ ∈ N (π,M). It follows that t (G,ϵ,M) ≤
mix
t (Gˆ,ϵ,M). For the reverse inequality, fix ν ∈N (πˆ,M) and take
mix
(cid:90)
µ(dx ,dx )= α(dx |t,x )ν(dt,dx ).
1 2 1 2 2
By (2) we have µ ∈ N (π,M) and thus (62). It follows t (Gˆ,ϵ,M) ≤ t (G,ϵ,M) as
mix mix
desired.
Lemma 9. Consider model (20) and the Gibbs sampler defined in (22). Then, under as-
sumptions (B1)-(B6), for every ψ∗ ∈ RD, M ≥ 1 and ϵ > 0, there exists T(ψ∗,ϵ,M) such
that
Q(J)(t (G ,ϵ,M)≤T(ψ∗,ϵ,M))→1, as J →∞. (63)
ψ∗ mix J
Proof. Let T be as in Appendix C, πˆ = L(T(θ),ψ) under (θ,ψ) ∼ π and Gˆ be the
J J J
kernel of the Gibbs sampler targeting πˆ . Then, by Lemma 8 it holds t (G ,ϵ,M) =
J mix J
t (Gˆ ,ϵ,M). Let ψ˜ and T˜ be the one-to-one transformations defined in (17) and (19) of
mix J
[3]. Denoting π˜ = L(T˜,ψ˜ | Y ), by an analogous version of Lemma 2.1 in [3] we have
J 1:J
t (Gˆ ,ϵ,M)=t (G˜ ,ϵ,M), where G˜ is the operator of the Gibbs sampler on π˜ . By
mix J mix J J J
Lemma C.18 in [3] we have that
∥π˜ −π˜∥ →0 (64)
J TV
as J → ∞ in Q(∞)-probability, where π˜ is a multivariate Normal distribution with non-
ψ∗
singular covariance matrix. Thus, by Theorem 1 in [1] we have that Φ(G˜) > 0, where G˜ is
the operator of the Gibbs sampler on π˜. By Theorem 3, (64) implies that
liminf Φ (G˜ )≥Φ(G˜)>0
s J
J
in Q(∞)-probability for every s>0. The result then follows by Lemma 1 with
ψ∗
log(2M)−log(ϵ)
T(ψ∗,ϵ,M)= .
(cid:16) (cid:17)
−log 1− Φ2(G)
2
44Proof of Theorem 5. Lemma 7 with ϵ=1/4 implies
1
Φ (G )≥ .
s J 4t (G ,1/4,s−1)
mix J
Lemma 9 with M = 1/s and ϵ = 1/4 implies that there exists T = T(ψ∗,ϵ,M) < ∞ such
that Q(J)(t (G ,ϵ,M)≤T) → 1 as J → ∞. It follows Q(J)(Φ (G )≥g(s)) → 1 as
ψ∗ mix J ψ∗ s J
J →∞ with g(s)=1/(4T)>0.
D.13 Proof of Proposition 4
Proof. Without loss of generality we can assume S∗ to be compact, which by Weierstrass’
Theorem implies that assumption (C) is satisfied. Moreover, assumptions (B4)−(B6) are
satisfied by Lemma 5.3 in [3], so that the result follows by Corollary 2.
D.14 Proof of Proposition 5
Let µ,π ∈P(X) and define
(cid:90) (cid:18) dµ (cid:19)2 (cid:90) dµ
L (µ,π)= (x) π(dx)= (x)µ(dx), (65)
2 dπ dπ
X X
with L (µ,π) = ∞ if µ is not absolutely continuous with respect to π. The next lemma
2
shows that if L (µ,π) is small, then µ is close to a warm start in total variation.
2
Lemma 10. Assume µ ∈ P(X) and L (µ,π) ≤ c. Then, for every r ∈ (0,1) there exists
2
(cid:16) (cid:17)
ν ∈N π, c such that
r(1−r)
2−r
∥µ−ν∥ ≤r .
TV 1−r
(cid:110) (cid:111)
Proof. Fix r ∈ (0,1) and let A = x∈X : dµ(x)≥c/r . Then by definition of L (µ,π)
dπ 2
and A we have
(cid:90) dµ c
c≥L (µ,π)≥ (x)µ(dx)≥ µ(A),
2 dπ r
A
which implies that µ(A)≤r. Define now ν ∈P(X) as ν(·)=µ(·∩Ac)/µ(Ac). By definition
of A and the above, it holds
dν c c
(x)≤ ≤ (66)
dπ rµ(Ac) r(1−r)
for every x∈X. Moreover, for every B ⊂X, we have
(cid:12) (cid:18) (cid:19) (cid:12)
|ν(B)−µ(B)|=(cid:12) (cid:12)µ(B∩Ac) 1 −1 −µ(B∩A)(cid:12) (cid:12)
(cid:12) µ(Ac) (cid:12)
(cid:12) (cid:12)
=(cid:12) (cid:12)µ(B∩Ac) µ(A) −µ(B∩A)(cid:12)
(cid:12)
(cid:12) µ(Ac) (cid:12)
µ(A)
≤µ(B∩Ac) +µ(B∩A)
µ(Ac)
µ(A) r 2−r
≤ +µ(A)≤ +r =r ,
µ(Ac) 1−r 1−r
which implies ∥µ−ν∥ ≤r2−r, as desired.
TV 1−r
45Define π˜
∈P(cid:0)RD+ℓJ(cid:1)
as
J
(cid:18) (cid:19) J
π˜ (dψ,dθ)=N dψ |ψˆ , 1 I−1(ψ∗) (cid:89) L(dθ |ψ,Y ), (67)
J (S∗) J J j J
j=1
where N is the normal distribution truncated on S∗ and I(ψ∗) denotes the Fisher infor-
(S∗)
mation matrix associated to the marginal likelihood of ψ, evaluated at the data-generating
value ψ∗. We need another preliminary lemma.
Lemma 11. Under the same notation and assumptions of Proposition 5, there exists a
constant C ≥1 such that
Q(J)(L (µ ,π˜ )≤C)→1
ψ∗ 2 J J
as J →∞. The constant C depends only on c and M used in the definition of µ .
J
Proof. By Lemma C.45 in [3] there exist a constant R=R(c)∈[1,∞) such that
(cid:16) (cid:17)
Q(J) µ(−2) ∈N(π˜(−2),R) →1,
ψ∗ J J
as J →∞. Thus, with probability converging to 1 as J →∞ under Q(J), we have
ψ∗
 
L 2(µ J,π˜
J)=(cid:90) d dµ
π˜
JJ(ψ,θ)dµ
J(dψ,dθ)=(cid:90) dd πµ ˜(
J
(− −2 2) )(ψ)(cid:89)J d dµ
π
JJ
(( θθ
jj
|| ψψ ))
dµ J(dψ,dθ)
J j=1
 
≤R(cid:90) (cid:89)J (cid:90) d dµ
πJ
(( θθ
j
|| ψψ ))
µ J(dθ
j
|ψ)µ( J−2)(dψ),
S∗
j=1
J j
(68)
by(57)and(67). Moreover,sinceν
j,ψ
∈N (π J(θ
j
|ψ),M),µ J(dθ
j
|ψ)=ν j,ψ(Pψ,Yj)tJ(dθ j),
and using assumption (C) and Lemma 1, we have
(cid:12) (cid:12) (cid:12) (cid:12)(cid:90) d dµ πJ (( θθ j || ψψ )) µ J(dθ j |ψ)−1(cid:12) (cid:12) (cid:12) (cid:12)≤M(cid:13) (cid:13)ν j,ψ(Pψ,Yj)tJ(dθ j)−π J(dθ j |ψ)(cid:13) (cid:13) TV ≤M(cid:18) 1− κ 22(cid:19)tJ .
J j
Using the definition of t we obtain
J
(cid:89)J (cid:90) dµ J(θ
j
|ψ)
µ (dθ
|ψ)≤(cid:18)
1+
M(cid:19)J
≤eM, (69)
dπ (θ |ψ) J j J
J j
j=1
for every ψ ∈ S∗ and J ≥ 1. By (68) and (69) we obtain the desired statement with
C =ReM.
Proof of Proposition 5. Since the conditional distribution of θ given ψ under π and π˜
J J
coincide, we have
(cid:13) (cid:13)
δ :=∥π −π˜ ∥ =(cid:13)π(−2)−π˜(−2)(cid:13) , (70)
J J J TV (cid:13) J J (cid:13)
TV
where π(−2) and π˜(−2) are the marginal distributions of ψ under π and π˜ , respectively.
J J J J
Thus, by the Bernstein-von Mises Theorem (e.g. Theorem 10.1 in [68]), whose requirements
are met thanks to assumptions (B1)-(B3), we have
δ →0, (71)
J
46as J →∞ in Q(∞)-probability. Consider now the kernel P˜ targeting π˜ , defined as
ψ∗ J J
1 1
P˜ = G˜ + P ,
J 2 1,J 2 2,J
with P as in (23) and G˜ Gibbs update on π˜ (ψ,θ). By construction and Lemma 2, for
2,J 1,J J
every J and N ≥1 we have
(cid:16) (cid:17) (cid:13) (cid:13)
∆ P˜ ,P ,N = sup (cid:13)µP˜ −µP (cid:13)
J J (cid:13) J J(cid:13)
µ∈N(π˜J,N) TV
(72)
1 (cid:13) (cid:13)
= sup (cid:13)µG˜ −µG (cid:13) ≤Nδ .
2 µ∈N(π˜J,N)(cid:13) 1,J 1,J(cid:13) TV J
Combining (72) with Theorem 2, for every s≥δ we get
J
(cid:16) (cid:17) 2 3
Φ (P˜ )≥Φ (P )−∆ P˜ ,P ,1/s − δ ≥Φ (P )− δ . (73)
s J s−δJ J J J s J s−δJ J s J
Therefore, by (73), (71) and Theorems 4 and 5, for every s > 0 there exists a constant
h(s)>0 such that
(cid:16) (cid:17)
Q(J) Φ (P˜ )≥h(s) →1, (74)
ψ∗ s J
as J → ∞. Note that the value h(s) depends on the model and data generating process
under consideration, and thus in particular also on ψ∗. Now, for every ν ∈N(π˜ ,N) with
J J
N ≥1ands>0, applyingthetriangularinequalityfollowedby (72)andLemma1, wehave
(cid:13) (cid:13)ν JP Jt −π J(cid:13) (cid:13) TV ≤(cid:13) (cid:13) (cid:13)ν JP Jt −ν JP˜ Jt(cid:13) (cid:13) (cid:13) TV +(cid:13) (cid:13) (cid:13)ν JP˜ Jt −π˜ J(cid:13) (cid:13) (cid:13) TV +∥π˜ J −π J∥ TV
(cid:16) (cid:17) (cid:13) (cid:13)
≤∆ P˜ ,P ,N +(cid:13)ν P˜t −π˜ (cid:13) +δ
J J (cid:13) J J J(cid:13) J
TV
(cid:32) (cid:33)t
Φ2(P˜ )
≤(N +1)δ +Ns+N 1− s J .
J 2
ByLemma11thereexistsaconstantC ≥1suchthatL (µ ,π˜ )≤C withQ(J)-probability
2 J J ψ∗
going to 1 as J → ∞. Thus, for every r ∈ (0,1), by Lemma 10, there exist ν ∈
J
(cid:16) (cid:17)
N π˜ , C such that ∥µ −ν ∥ ≤ r2−r as J → ∞ in Q(∞)-probability. Thus, for
J r(1−r) J J TV 1−r ψ∗
every fixed t∈N and s,r ∈(0,1) we have
(cid:13) (cid:13)µ JP Jt −π J(cid:13) (cid:13) TV ≤∥µ J −ν J∥ TV +(cid:13) (cid:13)ν JP Jt −π J(cid:13) (cid:13) TV
2−r
(cid:18)
C
(cid:19)
Cs C
(cid:18) h(s)2(cid:19)t
≤r + +1 δ + + 1−
1−r r(1−r) J r(1−r) r(1−r) 2
asJ →∞inQ(∞)-probability,withC beingtheconstantofLemma11. Fixϵ>0. Thenone
ψ∗
can that r = r(ϵ) ∈ (0,1), s = s(ϵ) ∈ (0,1) and t = t(ϵ) ∈ N, all depending on ϵ, such that
r2−r <ϵ/4, Cs <ϵ/4 and C
(cid:16)
1−
h(s)2(cid:17)t
<ϵ/4. Combining with δ →0 as J →∞
1−r r(1−r) r(1−r) 2 J
(cid:13) (cid:13)
in Q(∞)-probability it follows that (cid:13)µ Pt(ϵ)−π (cid:13) < ϵ as J → ∞ in Q(∞)-probability
ψ∗ (cid:13) J J J(cid:13) ψ∗
TV
and thus Q(J)(t (P ,ϵ,µ )≤t(ϵ)) → 1 as J → ∞ as desired. The results follows with
ψ∗ mix J J
T (ψ∗,ϵ)=t(ϵ) where the dependence on ψ∗ comes from h(s).
D.15 Proof of Proposition 6
Proof. TheresultfollowsimmediatelybycombiningProposition3withCorollary1,choosing
K =S×Rd.
47D.16 Proof of Lemma 4
Proof. Notice that by definition of T =θ⊤Σ−1θ we have
π(dα|θ)=π˜(dα|T).
Thus, for every ϵ>0 and M >0 by Lemma 8 it holds t (G˜,ϵ,M)=t (G,ϵ,M). Thus,
mix mix
Lemma 7 with ϵ=1/4 implies
1 1
Φ (G)≥ = . (75)
s 4t mix(G,1/4,s−1) 4t mix(G˜,1/4,s−1)
Moreover, by Lemma 1 we have
log(8)−log(s)
t (G˜,1/4,s−1)≤ . (76)
mix (cid:18) Φ2 s(G˜)(cid:19)
−log 1− 8
2
The result follows by combining (75) and (76).
D.17 Proof of Proposition 7
Proof. Denote with P the law of Y induced by (40) for a fixed θ and with W(X ,X )
θ i ti−1 ti
the law of the corresponding Brownian bridge. By equation (10) in [61] we have
dP g(X ,X ) 1
dW(X
ti−θ
1,X
ti)(Y i)=G(Y i,θ)
f
θ(Xt ti i− −1 1,Xt ti i), g(x,y)= √ 2πe− 21 ∆(y−x)2 . (77)
By equation (4) in [6] thanks to (C1) we have
(cid:40) (cid:41)
G(Y ,θ)=exp
−1(cid:90) ti
(cid:2) b2(θ,Y )−b′(θ,Y )(cid:3) dt . (78)
i 2 i,t i,t
ti−1
Combining(77)and(78),thenforeveryθby(C2)thereexistsM(θ)>0suchthatG(Y ,θ)≤
i
M(θ)foreveryY . Thanksto(C3)andcontinuityofbwithrespecttoθwecanfindaconstant
i
M >0 such that M(θ)≤M for every θ ∈K. Finally, thanks to (C3) and (C4) we have
g(X ,X )
ti−1 ti
≤R,
f (X ,X )
θ ti−1 ti
for a suitable R=R(c)>0, for every θ ∈S and i=1,...,N. Thus, in the end we get
dP
θ (Y )≤MR
dW(X ,X ) i
ti−1 ti
Thus, by Proposition 1 we have κ(Pθ)≥m/M for every θ ∈K, which by Lemma 3 implies
i
1
κ(P )≥ .
2 4M2R2
The result then follows by Corollary 1 with κ= 1 .
4M2R2
48