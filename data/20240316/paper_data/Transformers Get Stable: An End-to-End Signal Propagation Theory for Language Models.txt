Transformers Get Stable: An End-to-End
Signal Propagation Theory for Language Models
AkhilKedia*1 MohdAbbasZaidi*1 SushilKhyalia*2 JunghoJung1 HarshithGoka1 HaejunLee1
Abstract 2019a;Xiongetal.,2020;Bachlechneretal.,2021;Wang
etal.,2022a;Dehghanietal.,2023).
Inspiteoftheirhugesuccess,transformermodels
remain difficult to scale in depth. In this work, Theoretical analysis via signal propagation, kernel meth-
we develop a unified signal propagation theory odshasledtoanimprovedunderstandingoftheseissues.
and provide formulae that govern the moments Several works in the signal propagation domain (Glorot
oftheforwardandbackwardsignalthroughthe &Bengio,2010;Arpitetal.,2016;Xuetal.,2019;Dong
transformermodel. Ourframeworkcanbeused et al., 2021; Davis et al., 2021; Wang et al., 2022b) have
tounderstandandmitigatevanishing/exploding analysedthepropagationofmomentsforsomecomponents
gradients,rankcollapse,andinstabilityassociated ofdeeptransformers,butoftenmakesimplifyingassump-
withhighattentionscores. WealsoproposeDeep- tionsofIIDinputs,uncorrelatedoutputs,ignoringeffectof
ScaleLM,aninitializationandscalingschemethat query/keyinitialization,simplifyingnon-linearity,etc. We
conservesunitoutput/gradientmomentsthrough- observedbreakdownofeachoftheseassumptionswithreal
outthemodel,enablingthetrainingofverydeep worlddata,adverselyaffectingmodelstability.
models with 100s of layers. We find that trans-
These issues highlight the need for a holistic theoretical
formermodelscouldbemuchdeeper–ourdeep
frameworkthatcanfullyexplainsignalpropagationthrough
modelswithfewerparametersoutperformshallow
transformermodelswithrealdata. Inthiswork,weprovide
modelsinLanguageModeling,SpeechTransla-
a framework to fully explain signal propagation through
tion, and Image Classification, across Encoder-
transformermodels,byderivingclosed-formexpressions
only, Decoder-only and Encoder-Decoder vari-
forthefirstandsecond-ordermoments(meanandvariance)
ants,forbothPre-LNandPost-LNtransformers,
oftheoutputsandgradientsofeachofthecomponentsof
formultipledatasetsandmodelsizes. Theseim-
the transformer model (Embeddings, FFN, ReLU/GeLU,
provements also translate into improved perfor-
LayerNorm, Dropout, Softmax, Single-Head Attention),
manceondownstreamQuestionAnsweringtasks
Attention and FFN blocks, and through the entire model.
andimprovedrobustnessforimageclassification.
Ourderivedequationsareempiricallyverifiedwithinstrict
errorboundswithrealworlddata(codeinsupplementary).
We apply this framework to understand and mitigate in-
1 Introduction stabilityissueswithdeeptransformers–vanishing/explod-
inggradients,rankcollapse,andinstabilitycausedbyhigh
Transformermodelsareextremelypopularacrossdifferent
QKvalues. Toharnesstheimprovedcomplexityofdeeper
domains of machine learning, however, deep transform-
models (Montúfar et al., 2014; Poole et al., 2016; Raghu
ers are plagued with issues of gradient explosion/vanish-
et al., 2017), we propose DeepScaleLM, a novel initial-
ing(Shleiferetal.,2021;Takaseetal.,2022;Smithetal.,
izationschemethataugmentsresidual/outputscaling,and
2022; Chowdhery et al., 2023) and rank collapse (Zhou
ensuresthemomentsofoutputsandgradientsremainfully
etal.,2021;Nocietal.,2022)thatadverselyaffecttraining
conservedthroughoutthemodel. DSLMenablesustobreak
stability. Proposedremediesincluderesidualscaling,chang-
thedepthbarrierandtrainmodelswith100soflayerswhich
inginitialization,orextra/modifiedlayernorms(Zhangetal.,
consistently outperform shallow models for BERT, GPT,
Encoder-Decoder style models on Question-Answering,
*Equalcontribution 1LanguageIntelligenceTeam,Samsung
Research, Seoul, South Korea. 2Department of Computer Speech-to-text translation and Image Classification tasks
Science, Carnegie Melon University, Pittsburgh, USA (work whilereducingparameters.
done while at Samsung). Correspondence to: Akhil Kedia
<akhil.kedia@samsung.com>.
1
4202
raM
41
]LC.sc[
1v53690.3042:viXraTransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
2 RelatedWorks etal.,2022). Ourcontributiongoesbeyondprovidinganop-
timalscalingscheme–ourtheoryenablesinformedchoices
Fordetaileddiscussionofpriorworks,refertoAppendixA.
about these initialization/scaling schemes based on their
expressivity-trainabilitytrade-off.
Initialization Severalworks(Glorot&Bengio,2010;He
etal.,2015;Brocketal.,2021;Pooleetal.,2016;Schoen-
OtherNetworkmodificationsforDeepNetworks Ar-
holz et al., 2017) improved the initialization of ResNet-
chitecturalmodificationssuchasZhaietal.(2023);Zhou
s/ReLUnetworks. Theseworksdonotconsidertransform-
et al. (2021); Shleifer et al. (2021) can only stabilize the
ers, and are unable to handle Softmax/Attention. Others,
modellaterduringtrainingandnotatinitialization. They
such as ADMIN (Liu et al., 2020a), Mishkin & Matas
areorthogonaltoourapproach,andourequationscanbe
(2016);Liuetal.(2020b)achieveunitvarianceforfaster
easilyextendedtocoverthese. SomeworkssuchasDeep-
convergencebyscalingtheweightsand/oroutputsbasedon
Net,ADMINshowperformanceimprovementsbymaking
empiricalprofilingofaforwardpass. Blakeetal.(2023)
the model deeper, but much larger. In this work, we ex-
also tries to achieve this, but does not completely handle
ploreastrictersettingofkeepingtransformerparameters
correlationandnon-zeromeanofReLU.Wedemonstrate
andcomputeconstantwhilemakingthemodeldeeper.
thatthisprofilingisunnecessary,andcaninsteadbedone
theoreticallyinourwork.
3 MomentsofTransformerModels
Signal Propagation Signal propagation in Neural Net-
3.1 MomentsofTransformerComponents
works(Neal,1995;LeCunetal.,1996)hasalonghistory,
such as for ResNets (He et al., 2015; De & Smith, 2020; Following an analysis similar to that of Xavier initializa-
Brock et al., 2021; Schoenholz et al., 2017; Hoedt et al., tion (Glorot & Bengio, 2010), we derive closed-form ex-
2022;Labatieetal.,2021;Marionetal.,2022;Klambauer pressions for the mean and variance of the output and of
et al., 2017; Balduzzi et al., 2017), and for transformers thebackpropagatedgradientforallthecomponentsofthe
in (Xu et al., 2019; Dong et al., 2021; Davis et al., 2021; transformermodelinTable1.
Noci et al., 2022; Martens et al., 2021; He et al., 2023;
Here µ , σ2 , µ , σ2 are the mean and variance of
Shietal.,2022;Wangetal.,2022b). Ourworkconsiders xin xin xout xout
theinput/outputs,σ2 ,σ2 arethevarianceofthegradient
previouslyoftenneglectedeffectsofdropout,inputcorrela- gout gin
back-propagatedto/fromthecomponent,andrl,rdarethe
tion,activationnon-linearity,QK initialization,providing
correlationsacrosssequencelengthandhiddendimension.
closedformswithverifiablecorrectnessofsignalpropaga-
p is the dropout probability, L sequence length, d ,d
tion. Thisallowsustoconstraintheoutputandgradientto in out
input/output dimensions of Linear layer, σ2, σ2 vari-
almostexactlyunitvariance. w wembd
ancesoftheweightsoftheLinearlayerandtheEmbeddings
table. At the input side, rl originates from repeated to-
Moment Control & Residual Scaling Bounded gradi- xin
kens. Fortext,weestimateinputcorrelationtheoretically
ents have been shown to results in better/faster conver-
byassumingthatinputtokensfollowZipf(Kingsley,1935)
gence(Shenetal.,2020;Yuetal.,2017;Youetal.,2017;
distribution. DetailedproofsareprovidedinAppendixB,
2020;Takaseetal.,2022;Shleiferetal.,2021;Hayouetal.,
andallassumptionsaresummarizedinAppendixM.2.
2019). Different scaling schemes for residual networks
(λ for skip connections and β for residual output) have
3.2 MomentsofTransformerBlocks
been explored by prior works, such as λ2 +β2 = 1 for
ResNets(Balduzzietal.,2017;Szegedyetal.,2017;Hanin CombiningtheexpressionsreportedinTable1,wederive
& Rolnick, 2018; Arpit et al., 2019; Zhang et al., 2019b; closed-formexpressionsforthemomenttransformationdur-
Hoedt et al., 2022). Learnable β 0 was used in Skip- ingtheforwardandbackwardpassofthetransformerAt-
≈
Init(De&Smith,2020),ReZero(Bachlechneretal.,2021), tentionandFFNblocks. TheAttentionblockreferstothe
LayerScale (Touvron et al., 2021b), Value-SkipInit (He Q,K,V projection,followedbyMulti-HeadAttentionand
et al., 2023). Others proposed β2 = (1), where N is Output-ProjectionLayer.TheFFNblockreferstotheLinear
O N
max/current layer was used in Arpit et al. (2019); Brock layerfollowedbynon-linearity(ReLU)andoutputLinear
etal.(2021);Marionetal.(2022);Zhangetal.(2022);He layer. Table 2 provides our derived equations for these,
etal.(2023);Nocietal.(2022);De&Smith(2020);Liu where σ2, σ2, σ2 , σ2 are the variances for V weights,
v o w1 w2
et al. (2020a;b); Davis et al. (2021); Blake et al. (2023), Output-Projectionweights,andweightsofFFNblockLin-
whileDSInit(Zhangetal.,2019a),T-Fixup(Huangetal., ear layers, and d is model the hidden size. These results
2020), DeepNorm(Wangetal.,2022a)usedβ2 < (1). showthatconsideringcorrelationrl,dropoutpandeffects
O N
However,theoptimalinitialization/scalingcanvarybased ofnon-linearityarecrucialforcorrectlymodellingsignal
ondata/modelcharacteristics(Zhangetal.,2022;Marion propagationthroughTransformerblocks.
2TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Table1: Signalpropagationforforwardandbackwardpassesthroughcomponentsofatransformer(GeLUinAppendixB.5). The
expressionshereareillustrativesimplificationoffullclosedformformulaeinAppendicesBandD.
Component µxout σ x2
out
σ g2
in
rl
xout
rl
gin
Embeddings 0 (cid:80) σ2 - π2 + 2 -
wembd 18 log(V )2 9
∗ | |
rl +µ2 /σ2
Linear(d d ) 0 d σ2(σ2 +µ2 ) d σ2σ2 xin xin xin rl
in → out in w xin xin out w gout 1+µ2 /σ2 gout
xin xin
ReLU (cid:112)σ (x 2in
π)
(π (2− π)1) σ x2
in
1 2σ g2
out
0.7r xl in+0.3r xl in2 (1
2
+ sin−1 π(r xl in) )rl
gout
σ2
LayerNorm(d) 0 1 gout rl rl
σ2 xin gout
xin
σ2 +pµ2 1 rl (1 p)
Dropout(p) µ xin xin σ2 xin − (1 p)rl
xin 1 p 1 p gout 1+pµ2 /σ2 − gout
− − xin xin
SHA-withoutV 0 rl σ2 rl σ2 1 1
xin xin gout gout
Softmax 1 e(1 −rxd in)σx2 in −1 e(1 −rxd in)σx2 in σ2 - -
L L2 L2 gout
Table2:MomentPropagationthroughtheblocksofatransformerlayer.Exactclosedforms/proofsareprovidedinAppendicesCandD.
Component σ2 rl σ2 rl
xout xout gin gin
d2σ2σ2σ2 rl d2σ2σ2 σ2
AttentionBlock o v xin∗ xin 1 p o v∗ goutrl 1 p
(1 p) − (1 p) gout −
− −
FFNBlock 2d2σ w2 1σ w2 2σ x2 in (1 p)(1 +r xl in +(1 1 )rl 2 ) σ2 σ2 (1 p)(1 +sin−1(rxl in) )rl
(1 p) − π 2 2−π xin xout∗ gout − 2 π gout
−
3.3 MomentsofEntireTransformerModel MLMdata,asshowninFigures1,2,3and4(Reproducible
insupplementary),andusingImageNetdata(asshownin
ByrepeatedlyapplyingtheexpressionsinTable2foreach
AppendixI.1). Ourformulaepredicttheobservedgradient
layer,wecalculatethepropagationofmomentsofoutputs
and forward/backward norms with remarkable accuracy,
and gradients through the entire transformer model. We
with mean and median relative errors of 6.8% and 5.2%
do this for both Pre-LN style transformers, in which the
respectively,andanR2of0.998. Wefurtherverifythatfor
skip connection bypasses the LayerNorm, and for Post-
model depths in range [1 768], and model dimensions
LNstyletransformers,inwhichtheLayernormisapplied −
[128 6096],thereportedformulaearewithin10%error,
beforetheskip-connection. Themethodisfullydetailedin −
evenacross768layersofthetransformermodel.
Appendices G.1 and G.2. Figures 1, 2 and 3 provide the
forward (left to right) and backward (right to left) signal
3.5 ValidityofTheoreticalPredictionsevenafter
propagation at initialization through the layers of a very
Training
deep192-layermodelwithXavierinitialization.
Interestingly,ourtheoreticalestimatesholdapproximately
3.4 NumericalValidationofTheoreticalResults evenafterthemodelshavebeentrainedforalargenumber
ofsteps. Themodelstaysintheregimeitisinitializedwith
Weverifythetheoreticalformulaeoftransformercompo-
(ashasalsobeenshowninLi&Liang(2018);Aroraetal.
nentsandblocksbyrunningsimulationswithreal/synthetic
(2019a);Leeetal.(2019);Jesusetal.(2020);Aroraetal.
data,(detailedinAppendixE,codereleasedassupplemen-
(2019b)),highlightingtheimportanceofcorrectinitializa-
tarymaterial). Evenat99percentile,noerror(otherthan
tion. Further,weanalyzeforwardexplosionina48-layer
SHAgradientσ2)islargerthan10%,verifyingourassump-
PreLNmodel(after100ktrainingsteps)andgradientexplo-
tions.
sionina64-layerPreLNmodel(after150ktrainingsteps)
All our derivations are modality-agnostic. We verify our anduseourtheorytopredictthemoments. Ourlineares-
formulaefortheentiretransformermodelusingrealtextual timationfortheforwardgrowthandhyperbolicestimation
3TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
100 Empirical 0.015 Empirical Empirical 2 ForwardPreLN 0.004
80 Theoretical 0.01 Theoretical 10−3 Predicted 1.5 BB aa cc kk ww aa rr dd PP or se tL LN N 0.003
60
1 0.002
40
0.005 10−4
20 0.5 0.001
0 0 0 0
0 48 96 144 192 0 48 96 144 192 0 48 96 144 192 0 48 96 144 192
LayerNumber-> LayerNumber-> LayerNumber-> LayerNumber->
Figure 1: Pre-LN: Forward Figure 2: Pre-LN: Backward Figure3: Post-LN:Backward Figure 4: DeepScaleLM: The vari-
variance increases linearly gradientvarianceincreaseshy- gradientvanishesexponentially ances remain conserved for both
withnumberoflayersN. perbolicallywithN. withN (log-scale). backwardandforwardpass.
forthegradientexplosionmatchcloselywiththeobserved ization(Wangetal.,2022a)orsuggestthatthebackpropa-
momentsasshowninAppendixinFigures7and8. gatedgradientshavealinearrelationtoQKvariance(Noci
etal.,2022). Critically,notefromourderivationsofsoft-
max(Appendix B.7), the backwards gradients from Q/K
4 Applications
areexponentiallyrelatedtotheirvariance,highlightingthe
4.1 ExplainingVarianceExplosioninTransformer critical significance of correct initialization of Q/K. For
e.g.,byinitializingthemtoonly2xthexaviervalues(keep
Ourapproachtheoreticallyprovesthegradientvanishing/ex-
allotherinitializationsthesame),backwardsgradientsex-
plosion(Table3)forbothPre-LNandPost-LNtransformers.
ploded10000xthrougha192layermodel. Ourtheoryex-
plainstheseempiricalobservationsandsuggestssimpleini-
ExplodingOutputandGradientinPre-LN Theforward tializationstrategytofixthisproblem,achievingthesame
output for Pre-LN transformer increases linearly with in- varianceonQKwithouttheoverheadofLayerNorm.
creasingdepthN (AppendixG.1)sinceeachlayer’soutput
isdirectlyaddedtotheskipconnection,asseeninFigure1. 4.3 ExplainingandMitigatingRankCollapse
Forthebackwardpass,thegradientincreaseshyperbolically
Similartoourwork,Nocietal.(2022)alsoanalyzemoment
withincreasingN,asseeninFigure2. Intuitively,thisis
propagationthroughthetransformer,andobservedtherank
becausethegradientincreasesineverylayerwhenablock’s
collapseofthetoken’srepresentationsatinitializationafter
gradientisaddedtotheskipconnection,andthefractional
justafewlayers,i.e.,allthetokenrepresentationsbecame
increaseingradientisinverselyproportionaltotheforward
thesame(rl 1afterjust12layers)atinitialization. This
variance(whichincreasesbyN)becauseofLayerNorm. x ≈
hasalsobeenreportedinShietal.(2022);Zhouetal.(2021);
Wang et al. (2022b); He et al. (2023); Bachlechner et al.
Vanishing/ExplodingGradientinPost-LN Whilelayer- (2021);Zhaietal.(2023),andsuggestedmodificationssuch
normsolvestheexplosionintheforwardpassofnetworks asaddingaskipconnectiononattentionscores,initializing
withresidualconnections(De&Smith,2020),ithastheop- Q/otherweightsto0,ornormalizingallFFNweights.
positeimpactonthegradient. AsprovedinAppendixG.2,
thegradientinaPost-LNtransformergrows/decaysexpo- 1
nentiallywiththenumberoflayers(Figure3). 1.2
0.8 DeepScaleLM Xavier
1 Xavier,λ=1,β= N2
Intuitively,thegradientisfirsttransformedwithinthelayer 0.6 0.8
andthenattheLayerNormplacedbeforethelayer.Themul- 0.6
0.4
tiplicativefactorisappliedrepeatedly,andcausesgradient 0.4
0.2 Attenoutputrl
tovanishorexplodeexponentially,aswasalsoobservedin FFNoutputrl 0.2
Schoenholzetal.(2017). ThisexplainswhyPost-LNmod- 0 0 0.2 0.4 0.6 0.8 1 0 0 20 40 60 80 100
els are more challenging to train than Pre-LN for deeper InputCorrelation-> LayerNumber->
networks(Wangetal.,2022a;Shleiferetal.,2021;Takase
etal.,2022). Figure 5: Forward r xl
out
for Figure 6: No rank collapse is
FFNandAttentionblockswith observed with Xavier init and
p = 0.1. FFN reduces rl dropout. rl increases slower
4.2 ExplainingimpactoflargeQKvalues forr xl
in
> 0.65,andattenx tio ou nt withβ2 = N2 orforDSLM.
InDehghanietal.(2023),theauthorsobservedlargeQK alwayshasr xl out <1.
valuesdestabilizedthetraining,andsolvedthisempirically
byaddingalayernormafterattentionscores. Priorworkon Ourtheorysuggestsaverysimplesolution–Dropout. As
signalpropagationeitherignoredtheeffectofQKinitial- ourclosedformexpressionssuggest,bothFFNblock(be-
4
>-ecnairaVdrawroF >-ecnairaVdrawkcaB >-ecnairaVdrawkcaB
>-noitalerroCtuptuO
>-ecnairaVdrawroF
>-noitalerroCtuptuO
>-ecnairaVdrawkcaBTransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
(cid:113)
c ua reus 5e ).of WR ite hL dU r) opa on ud t,d oro up ro mu et tr he od duc se hot whe sc tho arr te sl uat ci hon a( rF anig k- 2. Wesetσ w2
2
= σ w2
1
= d1
∗
1 −2p,tomaketheoutput
oftheFFNblock1.
collapsewillnotoccur,andrl willquicklyreachastable
x 3. Weiterativelycalculatelayer-by-layerrl ,rl using
value<1(AppendixF),asverifiedempiricallyinFigure6. xin xout
expressionsfromTable2,andcalculatetheinitialvari-
Alternatively, scaling the block output by β = 1 , or anceoftheattentionblockweightstomaketheoutput
√N
equivalentlyinitializingtheweightsverysmallinPost-LN variance1.
willalsopreventrankcollapse,evenwithoutDropout. For
This initialization of transformer blocks, combined with
Pre-LN, λ = 1 slows down increase in rl compared to
λ2 = 1 1 (butthesameslowdowncanbeachievedby thescalingoftheskipconnectionandresidual,andcorrect
− N initialization of the embeddings, results σ2 = 1, irre-
decreasingβ). WhilesimilartoNocietal.(2022),wehigh- model
spectiveofthenumberoflayerN. Thisinitializationalso
lightsomeissuesinNocietal.(2022)inAppendixF.This
preservesthebackwardgradient,asprovedforPre-LNand
highlights the criticality of correct initialization, dropout
Post-LN,inAppendicesG.3andG.4. Empirically,weshow
and scaling for deep transformer models, as well as the
the backward gradient being preserved for both Pre-LN
explainabilitypowerofourtheoreticalframework.
andPost-LNevenacross192layersatthestartoftraining
(Figure4).
Table3:Comparisonofmaximumtheoreticalforward(Fwd)pass
andbackward(Bck)passgrowthinvariancefortheentiretrans-
formermodelacrossmethods(SeeAppendixGforproofs).Here ChoiceofScalingParameters Whileanychoiceofβwill
βistheinitialvalueofresidualscalingforLayerScale.
workatinitialization,highervaluesofβ,forexampleβ2 =
Method Post-LN Pre-LN 0.5causesgradientstovanish(Figure9,Table4). Thisis
becausecovariancebetweenresidualandskipconnection
Bck Sensitivity Fwd Bck Sensitivity
increasestheforwardvariance,whichcausesnormalization
DSLM O(1) (1) 1 O(1) (1)
O O todecreasebackwardgradient(De&Smith,2020).
Vanilla O(c±N) O(N) O(N) O(N) O(logN)
DSInit O(1) O(N−1) O(1) O(1) O(N−1) Similartootherpriorworks(AppendixA.3),weuseβ2 =
LayerScale (1) (βN) (1) (1) (βN) k inallourexperiments,wherekissomesmallconstant.
O O O O O N
DeepNet O(1) O(N−0.5) - - - T foh ris Pre en -a Lb Nle .s Fu os rt Po ob so t-u Ln Nd ,th βe 2falli kng ir sa td hi ee on rt e( tA icp ap lle yn rd ei qx uG ir. e3 d)
≤ N2
to bound the gradient (Appendix G.6). In practice, with
4.4 DeepScaleLM:EnablingDeepTransformers β2 = 2, even with 768 layers, we empirically observed
N
thefinaloutputvariancefromthemodeldoesnotexceed
WeproposeDeepScaleLM(DSLM),anewinitialization/
30, and all our models converge. We hence use β2 = k
scalingschemethatalleviatestheissuesdiscussedabove. N
(Figure10),butapractitionermaychooseβ2 = k ,with
Nα
α>1ifmorestabilityisrequiredattheexpenseofperfor-
Residual/Skip-Connection Scaling Let σ2 , σ2 ,
skip block mance/“sensitivity”(RefertoSection5.6andcomparison
σ2 be the variances of the skip connection, the block,
model to prior works in Section 5.5). While the above analysis
andtheoutputofthefinallayerofthemodel,respectively.
assumes positive covariance (which we always observed
Let σ2 = σ2 , and we scale them by scalars λ and β
skip block experimentally),negativecovariancefollowsasimilarrea-
respectively. Then,ashasbeenproveninnumerousworks
soning,andwillcausegradientexplosioninstead.
(AppendixA.3),ifλ2+β2 =1,thisscalingwillmaintain
thevarianceafteradditionoftheresidual.
PreventingRankCollapse ForDSLM,applyingblock
equationsiterativelyshowsthatrl <1 1 afterN layers.
Initialization Howeverwhileensuringσ2 =σ2 (and x − e2
skip block
equal to the variance of model input) has been done for
SimplerInitialization Anotheravenuetohandletheco-
ResNets(AppendixA.1),itisdifficulttoachievetheoreti-
variancebetweenresidualandskipconnectioncouldbeto
callyfortransformers. ByleveragingtheequationsinTa-
set λ2 + β2 < 1. We therefore also consider a simpler
ble2,ourtheoryprovidesusthetoolstoachievethis. We
initializationmethod(AppendixN),inwhichwemodifythe
modify the initialization of the components of the trans-
initialization of attention value and output matrices to be
formerFFNandAttentionblockssuchthatthevarianceof
the same as those of FFN block. This decreases the "ef-
theiroutputis1,asfurtherdetailedinAppendixN–
fective"β oftheattentionblock,butastheattentionblock
1. We set the variance of embedding weights as σ2 = has2xfewerparamsthanFFN,thischangeinweightage
e
nu1
m−
ep mbd,wherenum embdisthenumberofembeddings seemsreasonable. AsweshowinAppendicesG.5andG.6
types. Asembeddingsarefollowedbyadropout,this whilevariancesarenolongerunitatinitialization,theyare
ensurestheinputvariancetothemodelis1. still bounded. This change does not impact performance
5TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
significantly,asweshowinTable13. 2300Linearand768attentionlayers)converges.
Ourmethodiscomparabletothebaselineforshallowmod-
FoldingScalingintoWeightsforInference Thescaling
elsbutstartstooutperformasthemodelgetsdeeper. Our
parametersintroducedherecanbefullyabsorbedintothe
192-layermodeloutperformsthevanilla12-layer,andour
modelcheckpointweightsbyscalinglayernormgainand
96layeroutperformsthevanilla24-layermodel. The160M
outputlinearweights,henceanddonotrequireanychanges
192-layer model outperforms the vanilla 24-layer 336M
tovanillatransformersinferencecode.
modelwithmorethan2 theparams.
×
DeepScaleLM enables training deeper-narrower models
ReadingTable4vertically,wecancomparetheperformance
with100soflayers,outperformingstandardmodelsacross
ofourapproachwiththebaselineaswevarythemodeldepth
transformervariants,tasksandmodalities.
(N)whilekeepingthehiddendimension(d)constant. The
baselinemodelsoftendivergeatlargerdepths. Bystabiliz-
5 DeepScaleLMResults ingthetraining,DSLMallowstraininglargermodelswith
betterperformance,withperformanceconsistentlyincreas-
5.1 ImprovementsonEncoder-only(BERT)Models ingwithlargerdepth.
Implementation Details We test our method on the
Table4:Performance(perplexity)ofBERTmodelswithdifferent
MaskedLanguageModellingtaskwiththeBERT(Devlin
shapes.Deep-Thinmodelsprovidelargeimprovementswithfewer
et al., 2019) model. We use Pile-CC dataset (Gao et al., parameters.
2021) to pre-train our model. We use k = 2 for β, and
ModelN/D 12/1024 48/512 192/256 768/128
weusealloriginalhyper-parametersofBERT,exceptfor
(#Params) (185M) (168M) (160M) (156M)
learning rate (LR). We find that higher LR is needed for
Baseline 14.2 14.8 17.2 diverge
ourdeeper-narrowermodels(similartoYangetal.(2021)).
DSLM 15.5 13.1 12.9 18.4
Hence, we search for LR for all the models. The train-
ing steps were decided based on Chinchilla (Hoffmann ModelN/D 24/1024 96/512 384/256 -
etal.,2022),at6.6Btokens. Table25providesallhyper- (#Params) (336M) (319M) (311M) -
parameterdetails. WhenusingDSLM,modeloutputwas Baseline 13.2 diverge diverge -
down-scaledby√dbeforeLM-head. DSLM 14.0 11.7 12.3 -
Wetraindifferentlanguagemodelswiththesamenumber
of parameters and compute – while increasing the depth Pre-training Performance Improvements for Pre-LN
(N), we reduce the hidden dimension d keeping number WealsoappliedDSLMtothedeepPre-LNmodels,trained
of transformer parameters (Nd2) constant. When chang- for3.3Btokens. Table5showthatDSLMsignificantlyim-
ingfrom12-layer1024-dmodelto192-layer256-dmodel, provestheperformanceofthePre-LNmodelacrossarange
computenegligiblyincreasesbyonly6.6%whenkeeping ofmodeldepths.
Nd2 constant(Table24),whilethenumberofparameters
Table5:DSLMwithPre-LNModels.
decreasesby5 15%duetodecreasingembeddingsize.
−
ModelN/D 12/512 96/512 192/256 768/128
EvaluationMetrics Pre-trainingPerplexity(exponential
Baseline 29.4 20.6 19.8 26.9
ofpre-trainingtest-setloss)isoftenusedtomeasureMLM DSLM 26.0 15.4 17.0 25.9
pre-training performance (RoBERTa (Liu et al., 2019),
Megatron-LM(Shoeybietal.,2019),Tayetal.(2023),or
PerformanceImprovementsafterLongerPre-training
similarvariantsinSalazaretal.(2020);Luetal.(2023)),
Duetocomputelimitations,ourexperimentsweretrained
andiswell-correlatedwithdownstreamperformance(Geip-
forChinchillaoptimalsteps. Toensurereproducibilityof
ing&Goldstein,2023). Weusetheperplexityasreported
ourwork(scriptsprovidedinsupplementary),anddemon-
byMegatron-LMhere. Callingthismeasure“perplexity”
strate sustained improvements for standard models, we
isaslightabuseofnotation(aspreviouswordswhichare
trainedtheBERT-basemodelusingpublicWikipediadata
maskedarenotavailable,andfuturewordsare). Fordown-
for 64B tokens (30x chinchilla tokens). We train a 4x
streamfine-tuning,weuseaccuracy.
deeper,10%smallermodelusingDSLM(N/d=48/384).
WefinetunethesemodelsonthepublicRACE-M,RACE-
Pre-TrainingPerformanceImprovements InTable4,
H(Laietal.,2017),MNLI(Williamsetal.,2018)andQQP1
weprovidetheresultsobtainedonscalingmodeldepthafter
datasets.AsshowninTable6,ourmodelprovidesbetterpre-
applyingDSLMtoPost-LN.PostLNmodelsoftendiverge
trainingperformancewhichistranslatedintodownstream
whilescalingmodeldepth. DSLMstabilizesthetrainingof
Post-LNmodels,andevena768layerPost-LNmodel(with 1QuoraQuestionPairsdataset
6TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Question-Answeringtasks’performanceacrossalldatasets. ClassificationtaskusingViTbaselinesprovidedbyBeyer
et al. (2022), and trained a 4x deeper model with same
Table 6: BERT-base (trained for 64B tokens) pre-training and params. ThedeeperDSLMmodeloutperformsthebaseline
finetuningresults(meanaccuracyacross5runswithstderr).
ViTbothinboth90and300epochsettings. Theimprove-
Dataset Baseline DSLM mentsalsotranslatetoimprovedrobustnessonImageNet-
v2(Rechtetal.,2019),ImageNet-R(Hendrycksetal.,2021)
PretrainingPerformance
andImageNet-Sketch(Wangetal.,2019).
ValidationPPL 8.3 7.8
FinetuningAccuracy
Table9:ApplyingDSLMtoImageclassificationusingViT.
MNLI 82.4±0.1 83.7±0.1
QQP 90.8±0.03 91.1±0.05 EvalSet 90-epoch 300-epoch
RACE-Middle 71.1±0.2 74.0±0.3 Baseline DSLM Baseline DSLM
RACE-High 63.7±0.1 65.7±0.2
ImageNet 76.5 77.2 79.8 80.3
ImageNet-Real 83.2 83.8 85.4 85.5
5.2 ImprovementsonDecoder-onlyModels(GPT) ImageNet-v2 63.7 65.2 67.9 68.3
ImageNet-R 23.9 24.4 27.8 28.3
WeappliedDSLMtothedecoder-onlyGPTmodel,trained ImageNet-Sketch 24.4 25.5 28.7 29.9
for8Btokens(slightlymorethanChinchilla-optimal). Sim-
ilar to BERT, increasing model depth by 4x with DSLM
whilekeepingtheparametersconstantresultsinimproved 5.5 ComparisonwithPriorMethods
performance(Table7).
In Table 10, we compare DSLMwith severalprior meth-
ods for deep transformers. DSInit and DeepNet stabilize
Table 7: Application of DSLM to Decoder-only model (GPT),
whileincreasingmodeldepthto4x(token-levelPPL). themodeltrainingattheexpenseofreduced“sensitivity”
(Section 5.6) by using smaller effective values of β2, at
Type ModelShape(N/d) LMPPL (N 2)and (N 1.5)respectively. Interesting,96-layer
− −
Baseline DSLM Baseline DSLM O O
modeldivergeswithDSInit,despiteDSInitusingasmaller
Pre-LN 12/1024 48/512 11.6 11.2 β asymptotically – this is because the constants hidden
Pre-LN 24/1024 96/512 10.4 10.1 in (N 2) are much larger for DSInit. Our method, by
−
O
Post-LN 12/1024 48/512 12.7 11.7 analysingsignalpropagation,hasconstantsexactly1.
Post-LN 24/1024 96/512 11.6 10.8
Bamboo method is a vanilla Pre-LN transformer, which
ourmethodout-performs. SkipInit,ReZero,LayerScaleand
5.3 ImprovementsonSpeech(Encoder-Decoder) Value-Skipinitallinitializeβtozero/verysmallvalues–this
choicemayslowdownlearninginitiallybyreducingback-
WeapplyDSLMonencoder/decoderstyletransformerfor
propagated gradients, and a learnable β under-performs
Speech-to-Text translation task. The baseline has been
comparedtofixed(Table12). Nocietal.(2022)initializes
trainedontheMuST-C(DiGangietal.,2019)datasetus-
Query and Key matrices to a large value, causing diver-
ingthefairseq(Ottetal.,2019)library. UsingDSLM,we
gence (Section 4.2). ADMIN requires an extra profiling
successfullytraina4xdeepermodelwhichoutperformsthe
passthroughthemodel,andmoreimportantly,cannotstop
18-layer(12-encoder,6-decoderlayers)baselinewith9%
vanishinggradients(AppendixA.1),causingthe192-Layer
lessparametersasseeninTable8.
modeltodiverge.
Table8:ApplicationofDSLMtoSpeech-to-Texttranslation.
Table10:ComparisonwithpriormethodsfordeepTransformers.
Model ModelSize BLEUScore
N –N /d Pre-LN Post-LN Method 192/256 96/512
enc dec
Baseline 12–6/256(31.1M) 24.9 21.9 DSInit(Zhangetal.,2019a) 15.9 diverge
DSLM 48–24/128(28.4M) 25.6 23.8 ADMIN(Liuetal.,2020a) diverge 25.2
SkipInit(De&Smith,2020) 15.1 13.1
ReZero(Bachlechneretal.,2021) diverge diverge
5.4 ImprovementsonVisionModality
LayerScale(Touvronetal.,2021b) 13.2 14.4
DeepNorm(Wangetal.,2022a) 14.4 13.4
WealsoapplyourmethodtoVisionTransformers. Aswe
Nocietal.(2022) diverge diverge
show in Appendix I.1 using ImageNet-1k (Russakovsky
Bamboo(Xueetal.,2023) 17.1 diverge
etal.,2015)datawithViT(Dosovitskiyetal.,2021)model,
Value-SkipInit(Heetal.,2023) 18.8 17.1
our method can also constrain the growth of moments in DeepScaleLM(ours) 12.9 11.7
Vision Transformers. We train our models on the Image
7TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
5.6 AnalysisofDSLM Table13:Ablationoftheinitializations.
ModelQuantization SimilartoUnitScaling(Blakeetal., Model ModelSize(N/d) Perf
2023),conservingunitactivationsandgradientsfromour Xavier 192/256(160M) 38.2
methodresultsinmodelswhichlosemuchlessperformance DSLM 192/256(160M) 17.0
whenquantized(viadirectcasting)toFP8precisioncom- DSLM(simple) 192/256(160M) 17.9
pared to original models. We apply 8-bit quantization to
Fixedσ=0.02 96/512(319M) 20.5
the48-Layer512-dimBertbaselinemodelandthemodel DSLM 96/512(319M) 17.9
trained with DSLM. Table 11 provides the performance
correspondingtothefullprecisioninferenceandFP8infer-
ences(correspondingtotwodifferentFP8standards,E5M2 itmanagestooutperformthe24-layer1024-dmodel,that
andE4M3). DSLMmodelcanbecompressedto25%of has62.5%moreparameters,atequalwall-clocktimeandat
theoriginalsizewithsignificantlylowerperformanceloss. equalnumberoftokens.
Table11:ModelperformanceondirectcastingtoFP8
DiscussionofRelativeStrength Ingeneral,foraβofthe
Model FP32 E5M2 E4M3 formβ2 = k ,wecanchoosefromawiderangeofvalues
Nα
Baseline 14.8 42.5(∆27.7) 16.5(∆1.7)
fortheconstantkandexponentα. Thereisanexpressivity-
DSLM 13.1 21.4(∆ 8.3) 13.9(∆0.8) trainability trade-off in training deep networks (Yang &
Schoenholz,2017)–havinglowerβ (smallerk orhigher
α)willresultinnetworkswhereobservedissues(forward
AblationofResidualScaling Table12providesthere- growthorgradientexplosion/vanishing)aremitigated,but
sultscorrespondingtothedifferentcomponentsofourpro- theymayconvergeslowly/sub-optimally.
posed DSLM scheme for training 96-layer 512-d model
Davisetal.(2021)defines“sensitivity”asthevarianceof
Post-LNmodel. Themodelfailstoconvergewithoutthe
relativechangeinoutputforsmallperturbationsinparame-
proposedresidualscaling. β mayalsobesetaslearnable
(similartoBatchNorm (Ioffe&Szegedy,2015)),afterini-
ters,averagedacrossallparameters. Ifσ s2
kip
=1,sensitivity
tializingitwithβ2 = N2. Wefindthatthisdoesnotsignifi- canbeshowntobemeanacrosslayersofN ∗(1/σ b2 lock)=
cantlyimpactperformance,andβremainswithin[0.2 5] N β2.Meanisnotrobusttooutliers,andhencewesuggest
ofitsinitializedvalues. − × me∗ dianmayprovideamorerobustmeasure. Fore.g.,for
vanillapre-LN,Davisetal.(2021)’sdefinitiongivessensi-
Table12:AblationofvariousDeepScaleLMcomponents. tivityas (log(N)),whereasusingmedianprovidesamore
O
robustmeasureas (1).ButonlythefirstN/10layershave
Model Perf O
(log(N))sensitivity,andthelast9N/10layershave (1)
VanillaXavier(withorw/oβ2 =0.5) diverge sO ensitivity. WewillusemedianinthediscussionbelowO .
DSLM-Init(withorw/oβ2 =0.5) diverge
DSLM-Init+β2 = 2 (learnableβ) 12.2 InAppendixL,weshowthatthefallingradientforbothpre-
DSLM-Init+β2 = N 2 (fixedβ) 11.7 LNandpost-LNforβ2 =k/Nαis (ekN1−α ). Thesensi-
N O
tivityishencekN1 α. ForDSLM,wechoseα=1,thatis
−
thesweetspotonthestability-expressivitycurvewhereboth
AblationofInitialization Table13providesablationre- thegradientfallboundandsensitivityexpressionsbecome
sults for our proposed initialization. All experiments in independentofmodeldepth. Forhighervaluesofαsuch
Table 13 were conducted for the Pre-LN model with our asα = 2(DS-Init)and,α = 1.5(DeepNet),thegradient
proposedscaling(λ,β),sincethePost-LNmodeldiverged becomes stable using but the model expressivity reduces
with Xavier initialization. Xavier initialization performs withdepth,asshowninTable3. Suchmodelsmightnotbe
significantlyworseforverydeepmodels,duetohigherQK abletoextractbetterresultswhengoingdeeper,aswein-
initialization. BERT default initialization with σ = 0.02 deedverifyempiricallyinthecomparisonwithpriorworks
alsoperformsworse. Finally,DSLMsimplerinitialization paragraphinSection5.5.
performscomparablytoDSLM.
6 Conclusion
Compute AppendixKprovidesdetailedtheoreticaland
wall-clockcomputeoverheadsformakingmodelsdeeper. Wetheoreticallyderiveclosedformsforthegrowthofvari-
Weobservethatupto200layers,thetheoreticalcompute ances for forward and backward pass through individual
iswithin6 7%andwall-clocktimesiswithin15%ofthe transformercomponentsaswellastheentiretransformer
−
originalshallowmodel. Whileour192-layer256-dmodel model. Theseformulaeenableustoidentifyandsolvethe
requires6%extracomputethanthe12-layer1024-dmodel, keyreasonsforvanishing/explodinggradientsandrankcol-
8TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
lapse in very deep transformers. Via scaling and correct Arpit, D., Campos, V., and Bengio, Y. How to initialize
initialization,wealsoenabletrainingverydeeptransform- your network? robust initialization for weightnorm i&
ersupto768layers. Ourexperimentssuggestthatdeeper resnets. In Wallach, H. M., Larochelle, H., Beygelz-
transformersshouldbeexplored–usingourmethod,mod- imer, A., d’Alché-Buc, F., Fox, E. B., and Garnett,
elswith100soflayersoutperformlargerstandardmodels R. (eds.), Advances in Neural Information Processing
acrossmultiplemodalities,tasks,andtransformervariants. Systems32: AnnualConferenceonNeuralInformation
Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada, pp. 10900–
EthicsStatement
10909, 2019. URL https://proceedings.
Thispaperpresentsworkwhosegoalistoadvancethefield neurips.cc/paper/2019/hash/
of Machine Learning. There are many potential societal e520f70ac3930490458892665cda6620-
consequences of our work, some which we feel must be Abstract.html.
specificallyhighlightedhere. Usingcrawledwebdatafor
Bachlechner, T., Majumder, B. P., Mao, H. H., Cottrell,
pre-training language models is questionable, something
G., and McAuley, J. J. Rezero is all you need: fast
which society has yet to finalize its views on. Language
convergence at large depth. In de Campos, C. P.,
modellinginparticularsuffersfromhallucinations,andmay
Maathuis, M. H., and Quaeghebeur, E. (eds.), Pro-
beusedformisinformation.
ceedings of the Thirty-Seventh Conference on Uncer-
taintyinArtificialIntelligence,UAI2021,VirtualEvent,
References
27-30 July 2021, volume 161 of Proceedings of Ma-
chineLearningResearch,pp.1352–1361.AUAIPress,
Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R.,
2021.URLhttps://proceedings.mlr.press/
andWang, R. Onexactcomputationwithaninfinitely
v161/bachlechner21a.html.
wide neural net. In Wallach, H. M., Larochelle, H.,
Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and Balduzzi, D., Frean, M., Leary, L., Lewis, J. P., Ma,
Garnett, R. (eds.), Advances in Neural Information K. W., and McWilliams, B. The shattered gradients
Processing Systems 32: Annual Conference on Neural problem: If resnets are the answer, then what is the
Information Processing Systems 2019, NeurIPS 2019, question? In Precup, D. and Teh, Y. W. (eds.),
December8-14,2019,Vancouver,BC,Canada,pp.8139– Proceedings of the 34th International Conference on
8148, 2019a. URL https://proceedings. Machine Learning, ICML 2017, Sydney, NSW, Aus-
neurips.cc/paper/2019/hash/ tralia, 6-11 August 2017, volume 70 of Proceedings
dbc4d84bfcfe2284ba11beffb853a8c4- of Machine Learning Research, pp. 342–350. PMLR,
Abstract.html. 2017. URL http://proceedings.mlr.press/
v70/balduzzi17b.html.
Arora, S., Du, S. S., Hu, W., Li, Z., and Wang, R.
Beyer, L., Zhai, X., and Kolesnikov, A. Better plain vit
Fine-grained analysis of optimization and generaliza-
baselinesforimagenet-1k,2022.
tion for overparameterized two-layer neural networks.
In Chaudhuri, K. and Salakhutdinov, R. (eds.), Pro- Blake, C., Orr, D., and Luschi, C. Unit scaling: Out-of-
ceedings of the 36th International Conference on Ma- the-box low-precision training. In Krause, A., Brun-
chine Learning, ICML 2019, 9-15 June 2019, Long skill, E., Cho, K., Engelhardt, B., Sabato, S., and
Beach, California, USA, volume 97 of Proceedings Scarlett, J. (eds.), International Conference on Ma-
of Machine Learning Research, pp. 322–332. PMLR, chine Learning, ICML 2023, 23-29 July 2023, Hon-
2019b. URLhttp://proceedings.mlr.press/ olulu, Hawaii, USA, volume 202 of Proceedings of
v97/arora19a.html. Machine Learning Research, pp. 2548–2576. PMLR,
2023.URLhttps://proceedings.mlr.press/
Arpit, D., Zhou, Y., Kota, B. U., and Govindaraju, V. v202/blake23a.html.
Normalization propagation: A parametric technique
Brock,A.,De,S.,andSmith,S.L. Characterizingsignal
for removing internal covariate shift in deep networks.
propagationtoclosetheperformancegapinunnormal-
In Balcan, M. and Weinberger, K. Q. (eds.), Pro-
izedresnets. In9thInternationalConferenceonLearn-
ceedings of the 33nd International Conference on Ma-
ingRepresentations,ICLR2021,VirtualEvent,Austria,
chine Learning, ICML 2016, New York City, NY, USA,
May 3-7, 2021. OpenReview.net, 2021. URL https:
June 19-24, 2016, volume 48 of JMLR Workshop and
//openreview.net/forum?id=IX3Nnir2omJ.
Conference Proceedings, pp. 1168–1176. JMLR.org,
2016. URL http://proceedings.mlr.press/ Chen,M.,Wei,Z.,Huang,Z.,Ding,B.,andLi,Y. Simple
v48/arpitb16.html. anddeepgraphconvolutionalnetworks.InProceedingsof
9TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
the37thInternationalConferenceonMachineLearning, Heek,J.,Gilmer,J.,Steiner,A.P.,Caron,M.,Geirhos,
ICML2020,13-18July2020,VirtualEvent,volume119 R., Alabdulmohsin, I., Jenatton, R., Beyer, L., Tschan-
ofProceedingsofMachineLearningResearch,pp.1725– nen, M., Arnab, A., Wang, X., Ruiz, C. R., Minderer,
1735. PMLR, 2020. URL http://proceedings. M.,Puigcerver,J.,Evci,U.,Kumar,M.,vanSteenkiste,
mlr.press/v119/chen20v.html. S., Elsayed, G. F., Mahendran, A., Yu, F., Oliver, A.,
Huot, F., Bastings, J., Collier, M., Gritsenko, A. A.,
Chowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,Mishra,
Birodkar, V., Vasconcelos, C.N., Tay, Y., Mensink, T.,
G., Roberts, A., Barham, P., Chung, H. W., Sutton,
Kolesnikov, A., Pavetic, F., Tran, D., Kipf, T., Lucic,
C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
M.,Zhai,X.,Keysers,D.,Harmsen,J.J.,andHoulsby,
S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
N. Scalingvisiontransformersto22billionparameters.
N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,
In Krause, A., Brunskill, E., Cho, K., Engelhardt, B.,
Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,
Sabato,S.,andScarlett,J.(eds.),InternationalConfer-
G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S.,
enceonMachineLearning,ICML2023,23-29July2023,
Dev,S.,Michalewski,H.,Garcia,X.,Misra,V.,Robin-
Honolulu, Hawaii, USA, volume 202 of Proceedings
son, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D.,
ofMachineLearningResearch,pp.7480–7512.PMLR,
Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Do-
2023.URLhttps://proceedings.mlr.press/
han,D.,Agrawal,S.,Omernick,M.,Dai,A.M.,Pillai,
v202/dehghani23a.html.
T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child,
R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, Dettmers,T.,Pagnoni,A.,Holtzman,A.,andZettlemoyer,
B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier- L. QLoRA:EfficientfinetuningofquantizedLLMs. In
Hellstern, K., Eck, D., Dean, J., Petrov, S., andFiedel, Thirty-seventh Conference on Neural Information Pro-
N. Palm: Scalinglanguagemodelingwithpathways. J. cessingSystems,2023. URLhttps://openreview.
Mach.Learn.Res.,24:240:1–240:113,2023.URLhttp: net/forum?id=OUIFPHEgJU.
//jmlr.org/papers/v24/22-1144.html.
Devlin,J.,Chang,M.,Lee,K.,andToutanova,K. BERT:
Clark, K., Luong, M., Le, Q. V., and Manning, C. D. pre-training of deep bidirectional transformers for lan-
ELECTRA:pre-trainingtextencodersasdiscriminators guage understanding. In Burstein, J., Doran, C., and
rather than generators. In 8th International Confer- Solorio,T.(eds.),Proceedingsofthe2019Conferenceof
ence on Learning Representations, ICLR 2020, Addis theNorthAmericanChapteroftheAssociationforCom-
Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, putationalLinguistics: HumanLanguageTechnologies,
2020. URLhttps://openreview.net/forum? NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,
id=r1xMH1BtvB. 2019,Volume1(LongandShortPapers),pp.4171–4186.
Association for Computational Linguistics, 2019. doi:
Davis, J. Q., Gu, A., Choromanski, K., Dao, T., Ré, C., 10.18653/V1/N19-1423. URL https://doi.org/
Finn, C., and Liang, P. Catformer: Designing stable 10.18653/v1/n19-1423.
transformersviasensitivityanalysis. InMeila, M.and
Zhang,T.(eds.),Proceedingsofthe38thInternational Di Gangi, M. A., Cattoni, R., Bentivogli, L., Negri, M.,
Conference on Machine Learning, ICML 2021, 18-24 andTurchi,M. MuST-C:aMultilingualSpeechTransla-
July 2021, Virtual Event, volume 139 of Proceedings tion Corpus. In Burstein, J., Doran, C., and Solorio,
ofMachineLearningResearch,pp.2489–2499.PMLR, T. (eds.), Proceedings of the 2019 Conference of the
2021. URL http://proceedings.mlr.press/ North American Chapter of the Association for Com-
v139/davis21a.html. putationalLinguistics: HumanLanguageTechnologies,
Volume1(LongandShortPapers),pp.2012–2017,Min-
De, S. and Smith, S. L. Batch normalization biases neapolis,Minnesota,June2019.AssociationforCompu-
residual blocks towards the identity function in deep tationalLinguistics. doi: 10.18653/v1/N19-1202. URL
networks. In Larochelle, H., Ranzato, M., Hadsell, https://aclanthology.org/N19-1202.
R., Balcan, M., and Lin, H. (eds.), Advances in
Dong, Y., Cordonnier, J., and Loukas, A. Attention is
Neural Information Processing Systems 33: Annual
not all you need: pure attention loses rank doubly ex-
Conference on Neural Information Processing Sys-
ponentially with depth. In Meila, M. and Zhang, T.
tems 2020, NeurIPS 2020, December 6-12, 2020,
(eds.), Proceedings of the 38th International Confer-
virtual, 2020. URL https://proceedings.
ence on Machine Learning, ICML 2021, 18-24 July
neurips.cc/paper/2020/hash/
2021,VirtualEvent,volume139ofProceedingsofMa-
e6b738eca0e6792ba8a9cbcba6c1881d-
chineLearningResearch,pp.2793–2803.PMLR,2021.
Abstract.html.
URLhttp://proceedings.mlr.press/v139/
Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., dong21a.html.
10TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, 2019. URL http://proceedings.mlr.press/
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, v97/hayou19a.html.
M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,
He,B.,Martens,J.,Zhang,G.,Botev,A.,Brock,A.,Smith,
N. An image is worth 16x16 words: Transformers for
S.L.,andTeh,Y.W. Deeptransformerswithoutshort-
image recognition at scale. In 9th International Con-
cuts: Modifying self-attention for faithful signal prop-
ference on Learning Representations, ICLR 2021, Vir-
agation. In The Eleventh International Conference on
tual Event, Austria, May 3-7, 2021. OpenReview.net,
LearningRepresentations,ICLR2023,Kigali,Rwanda,
2021. URLhttps://openreview.net/forum?
May 1-5, 2023. OpenReview.net, 2023. URL https:
id=YicbFdNTTy.
//openreview.net/pdf?id=NPrsUQgMjKK.
Gao,L.,Biderman,S.,Black,S.,Golding,L.,Hoppe,T.,
He, K., Zhang, X., Ren, S., and Sun, J. Delving deep
Foster, C., Phang, J., He, H., Thite, A., Nabeshima,
intorectifiers: Surpassinghuman-levelperformanceon
N., Presser, S., and Leahy, C. The pile: An 800gb
imagenet classification. In 2015 IEEE International
dataset of diverse text for language modeling. CoRR,
ConferenceonComputerVision,ICCV2015,Santiago,
abs/2101.00027,2021. URLhttps://arxiv.org/
Chile,December7-13,2015,pp.1026–1034.IEEECom-
abs/2101.00027.
puterSociety,2015. doi: 10.1109/ICCV.2015.123. URL
Geiping, J. and Goldstein, T. Cramming: Training a lan- https://doi.org/10.1109/ICCV.2015.123.
guage model on a single GPU in one day. In Krause,
Hendrycks,D.,Basart,S.,Mu,N.,Kadavath,S.,Wang,F.,
A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,
andScarlett,J.(eds.),InternationalConferenceonMa-
Song,D.,Steinhardt,J.,andGilmer,J. Themanyfacesof
chine Learning, ICML 2023, 23-29 July 2023, Hon-
robustness: Acriticalanalysisofout-of-distributiongen-
olulu, Hawaii, USA, volume 202 of Proceedings of
eralization. In2021IEEE/CVFInternationalConference
MachineLearningResearch,pp.11117–11143.PMLR,
onComputerVision,ICCV2021,Montreal,QC,Canada,
2023.URLhttps://proceedings.mlr.press/
October10-17,2021,pp.8320–8329.IEEE,2021. doi:
v202/geiping23a.html.
10.1109/ICCV48922.2021.00823.URLhttps://doi.
Glorot, X. and Bengio, Y. Understanding the difficulty org/10.1109/ICCV48922.2021.00823.
of training deep feedforward neural networks. In Teh,
Hoedt, P.-J., Hochreiter, S., and Klambauer, G. Nor-
Y. W. and Titterington, D. M. (eds.), Proceedings of
malisation is dead, long live normalisation! In
the Thirteenth International Conference on Artificial
ICLR Blog Track, 2022. URL https://iclr-
Intelligence and Statistics, AISTATS 2010, Chia La-
blog-track.github.io/2022/03/25/
guna Resort, Sardinia, Italy, May 13-15, 2010, vol-
unnormalized-resnets/. https://iclr-blog-
ume 9 of JMLR Proceedings, pp. 249–256. JMLR.org,
track.github.io/2022/03/25/unnormalized-resnets/.
2010. URL http://proceedings.mlr.press/
v9/glorot10a.html. Hoffmann,J.,Borgeaud,S.,Mensch,A.,Buchatskaya,E.,
Cai, T., Rutherford, E., de Las Casas, D., Hendricks,
Hanin,B.andRolnick,D. Howtostarttraining: Theeffect
L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E.,
ofinitializationandarchitecture. InBengio,S.,Wallach,
Millican, K., van den Driessche, G., Damoc, B., Guy,
H. M., Larochelle, H., Grauman, K., Cesa-Bianchi,
A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W.,
N., and Garnett, R. (eds.), Advances in Neural Infor-
Vinyals,O.,andSifre,L. Trainingcompute-optimallarge
mation Processing Systems 31: Annual Conference on
language models. CoRR, abs/2203.15556, 2022. doi:
NeuralInformationProcessingSystems2018,NeurIPS
10.48550/ARXIV.2203.15556. URL https://doi.
2018, December 3-8, 2018, Montréal, Canada, pp.
org/10.48550/arXiv.2203.15556.
569–579, 2018. URL https://proceedings.
neurips.cc/paper/2018/hash/ Huang,X.S.,Perez,F.,Ba,J.,andVolkovs,M. Improving
d81f9c1be2e08964bf9f24b15f0e4900- TransformerOptimizationThroughBetterInitialization.
Abstract.html. InProceedingsofthe37thInternationalConferenceon
Machine Learning, pp. 4475–4483. PMLR, November
Hayou,S.,Doucet,A.,andRousseau,J. Ontheimpactof
2020.URLhttps://proceedings.mlr.press/
the activation function on deep neural networks train-
v119/huang20f.html.
ing. In Chaudhuri, K. and Salakhutdinov, R. (eds.),
Proceedings of the 36th International Conference on Ioffe,S.andSzegedy,C. Batchnormalization:Accelerating
MachineLearning, ICML2019, 9-15June2019, Long deepnetworktrainingbyreducinginternalcovariateshift.
Beach, California, USA, volume 97 of Proceedings of In Bach, F. R. and Blei, D. M. (eds.), Proceedings of
Machine Learning Research, pp. 2672–2680. PMLR, the32ndInternationalConferenceonMachineLearning,
11TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
ICML2015,Lille,France,6-11July2015,volume37of Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. H.
JMLRWorkshopandConferenceProceedings,pp.448– RACE:large-scalereadingcomprehensiondatasetfrom
456.JMLR.org,2015. URLhttp://proceedings. examinations. In Palmer, M., Hwa, R., and Riedel, S.
mlr.press/v37/ioffe15.html. (eds.), Proceedings of the 2017 Conference on Empiri-
calMethodsinNaturalLanguageProcessing,EMNLP
Jesus, R. J., Antunes, M. L. P., da Costa, R. A. F., Doro-
2017, Copenhagen, Denmark, September 9-11, 2017,
govtsev, S. N., Mendes, J. F. F., and Aguiar, R. L. Ef-
pp. 785–794. Association for Computational Linguis-
fectofthe initialconfigurationofweightsonthe train- tics,2017. doi: 10.18653/V1/D17-1082. URLhttps:
ing and function of artificial neural networks. CoRR, //doi.org/10.18653/v1/d17-1082.
abs/2012.02550,2020. URLhttps://arxiv.org/
abs/2012.02550. LeCun,Y.,Bottou,L.,Orr,G.B.,andMüller,K. Effiicient
backprop. In Orr, G. B. and Müller, K. (eds.), Neural
Kim, H., Papamakarios, G., and Mnih, A. The lipschitz Networks: TricksoftheTrade,volume1524ofLecture
constant of self-attention. In Meila, M. and Zhang, Notes in Computer Science, pp. 9–50. Springer, 1996.
T. (eds.), Proceedings of the 38th International Con- doi: 10.1007/3-540-49430-8\_2. URLhttps://doi.
ference on Machine Learning, ICML 2021, 18-24 July org/10.1007/3-540-49430-8_2.
2021,VirtualEvent,volume139ofProceedingsofMa-
chineLearningResearch,pp.5562–5571.PMLR,2021. Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Novak,
URLhttp://proceedings.mlr.press/v139/ R.,Sohl-Dickstein,J.,andPennington,J. Wideneural
kim21i.html. networks of any depth evolve as linear models under
gradient descent. In Wallach, H. M., Larochelle, H.,
Kingsley,Z.G. Thepsycho-biologyoflanguage: anintro- Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and
ductiontodynamicphilology. HoughtonMifflin,1935. Garnett, R. (eds.), Advances in Neural Information
Processing Systems 32: Annual Conference on Neural
Klambauer,G.,Unterthiner,T.,Mayr,A.,andHochreiter,
Information Processing Systems 2019, NeurIPS 2019,
S. Self-normalizingneuralnetworks. InGuyon,I.,von
December8-14,2019,Vancouver,BC,Canada,pp.8570–
Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R.,
8581, 2019. URL https://proceedings.
Vishwanathan,S.V.N.,andGarnett,R.(eds.),Advances
neurips.cc/paper/2019/hash/
in Neural Information Processing Systems 30: Annual
0d1a9651497a38d8b1c3871c84528bd4-
ConferenceonNeuralInformationProcessingSystems
Abstract.html.
2017, December 4-9, 2017, Long Beach, CA, USA,
pp. 971–980, 2017. URL https://proceedings. Li, Y. and Liang, Y. Learning overparameterized neural
neurips.cc/paper/2017/hash/ networks via stochastic gradient descent on structured
5d44ee6f2c3f71b73125876103c8f6c4- data. In Bengio, S., Wallach, H. M., Larochelle,
Abstract.html. H., Grauman, K., Cesa-Bianchi, N., and Garnett, R.
(eds.), Advances in Neural Information Processing
Korotkov,N.E.andKorotkov,A.N. Integralsrelatedto
Systems 31: Annual Conference on Neural Infor-
the error function. Chapman & Hall/CRC, Philadel-
mation Processing Systems 2018, NeurIPS 2018,
phia, PA, March 2020. ISBN 9780367408206. URL
December 3-8, 2018, Montréal, Canada, pp. 8168–
https://www.taylorfrancis.com/books/
8177, 2018. URL https://proceedings.
mono/10.1201/9780367809232/integrals-
neurips.cc/paper/2018/hash/
related-error-function-nikolai-
54fe976ba170c19ebae453679b362263-
korotkov-alexander-korotkov.
Abstract.html.
Labatie, A., Masters, D., Eaton-Rosen, Z., and Luschi,
Liu,L.,Liu,X.,Gao,J.,Chen,W.,andHan,J. Understand-
C. Proxy-normalizing activations to match batch
ingthedifficultyoftrainingtransformers. InWebber,B.,
normalization while removing batch dependence. In
Cohn,T.,He,Y.,andLiu,Y.(eds.),Proceedingsofthe
Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang,
2020ConferenceonEmpiricalMethodsinNaturalLan-
P., and Vaughan, J. W. (eds.), Advances in Neural
guageProcessing,EMNLP2020,Online,November16-
InformationProcessingSystems34: AnnualConference
20,2020,pp.5747–5763.AssociationforComputational
on Neural Information Processing Systems 2021,
Linguistics, 2020a. doi: 10.18653/V1/2020.EMNLP-
NeurIPS2021,December6-14,2021,virtual,pp.16990–
MAIN.463. URLhttps://doi.org/10.18653/
17006, 2021. URL https://proceedings.
v1/2020.emnlp-main.463.
neurips.cc/paper/2021/hash/
8d2a5f7d4afa5d0530789d3066945330- Liu, X., Duh, K., Liu, L., and Gao, J. Very deep
Abstract.html. transformers for neural machine translation. CoRR,
12TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
abs/2008.07772,2020b.URLhttps://arxiv.org/ Neal, R. M. Bayesian learning for neural networks.
abs/2008.07772. PhD thesis, University of Toronto, Canada, 1995.
URL https://librarysearch.library.
Liu,Y.,Ott,M.,Goyal,N.,Du,J.,Joshi,M.,Chen,D.,Levy,
utoronto.ca/permalink/01UTORONTO_
O.,Lewis,M.,Zettlemoyer,L.,andStoyanov,V. Roberta:
INST/14bjeso/alma991106438365706196.
ArobustlyoptimizedBERTpretrainingapproach. CoRR,
abs/1907.11692, 2019. URL http://arxiv.org/ Ng, E. W. and Geller, M. A table of integrals of the Er-
abs/1907.11692. ror functions. Journal of Research of the National Bu-
reau of Standards, Section B: Mathematical Sciences,
Lo, C. F. WKB approximation for the sum of
73B(1):1, 1969. ISSN 0098-8979. doi: 10/gdtk9p.
two correlated lognormal random variables. Ap-
URLhttps://nvlpubs.nist.gov/nistpubs/
plied Mathematical Sciences, 7:6355–6367, 2013.
jres/73B/jresv73Bn1p1_A1b.pdf.
ISSN 13147552. doi: 10.12988/ams.2013.39511.
URL http://www.m-hikari.com/ams/ams- Nguyen,T.Q.andSalazar,J. Transformerswithouttears:
2013/ams-125-128-2013/39511.html.
Improvingthenormalizationofself-attention.InNiehues,
J., Cattoni, R., Stüker, S., Negri, M., Turchi, M., Ha,
Lu, J., Zhu, D., Han, W., Zhao, R., Namee, B. M., and
T., Salesky, E., Sanabria, R., Barrault, L., Specia, L.,
Tan,F. Whatmakespre-trainedlanguagemodelsbetter
andFederico, M. (eds.), Proceedingsof the16th Inter-
zero-shotlearners? InRogers, A., Boyd-Graber, J.L.,
national Conference on Spoken Language Translation,
andOkazaki,N.(eds.),Proceedingsofthe61stAnnual
IWSLT2019,HongKong,November2-3,2019.Associa-
MeetingoftheAssociationforComputationalLinguistics
tionforComputationalLinguistics,2019. URLhttps:
(Volume1: LongPapers),ACL2023,Toronto,Canada,
//aclanthology.org/2019.iwslt-1.17.
July9-14,2023,pp.2288–2303.AssociationforCompu-
tationalLinguistics,2023. doi: 10.18653/V1/2023.ACL-
Noci,L.,Anagnostidis,S.,Biggio,L.,Orvieto,A.,Singh,
LONG.128. URLhttps://doi.org/10.18653/
S. P., and Lucchi, A. Signal propagation in trans-
v1/2023.acl-long.128.
formers: Theoretical perspectives and the role of rank
collapse. In NeurIPS, 2022. URL http://papers.
Marion,P.,Fermanian,A.,Biau,G.,andVert,J. Scaling
nips.cc/paper\_files/paper/2022/hash/
resnetsinthelarge-depthregime. CoRR,abs/2206.06929,
ae0cba715b60c4052359b3d52a2cff7f-
2022. doi: 10.48550/ARXIV.2206.06929. URLhttps:
Abstract-Conference.html.
//doi.org/10.48550/arXiv.2206.06929.
Martens, J., Ballard, A., Desjardins, G., Swirszcz, G., Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,
Dalibard, V., Sohl-Dickstein, J., and Schoenholz, S. S. N.,Grangier,D.,andAuli,M. fairseq: Afast,extensible
Rapid training of deep neural networks without skip toolkitforsequencemodeling,2019.
connections or normalization layers using deep kernel
Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., and
shaping. CoRR, abs/2110.01765, 2021. URL https:
Ganguli,S. Exponentialexpressivityindeepneuralnet-
//arxiv.org/abs/2110.01765.
worksthroughtransientchaos. InLee,D.D.,Sugiyama,
Mishkin, D. and Matas, J. All you need is a good init. M.,vonLuxburg,U.,Guyon,I.,andGarnett,R.(eds.),
In Bengio, Y. and LeCun, Y. (eds.), 4th International AdvancesinNeuralInformationProcessingSystems29:
Conference on Learning Representations, ICLR 2016, Annual Conference on Neural Information Processing
SanJuan,PuertoRico,May2-4,2016,ConferenceTrack Systems2016,December5-10,2016,Barcelona,Spain,
Proceedings,2016. URLhttp://arxiv.org/abs/ pp.3360–3368,2016.URLhttps://proceedings.
1511.06422. neurips.cc/paper/2016/hash/
148510031349642de5ca0c544f31b2ef-
Montúfar,G.F.,Pascanu,R.,Cho,K.,andBengio,Y. On Abstract.html.
the number of linear regions of deep neural networks.
InGhahramani,Z.,Welling,M.,Cortes,C.,Lawrence, Raghu, M., Poole, B., Kleinberg, J. M., Ganguli, S.,
N.D.,andWeinberger,K.Q.(eds.),AdvancesinNeural and Sohl-Dickstein, J. On the expressive power of
InformationProcessingSystems27: AnnualConference deep neural networks. In Precup, D. and Teh, Y. W.
on Neural Information Processing Systems 2014, (eds.),Proceedingsofthe34thInternationalConference
December 8-13 2014, Montreal, Quebec, Canada, pp. on Machine Learning, ICML 2017, Sydney, NSW, Aus-
2924–2932, 2014. URL https://proceedings. tralia, 6-11 August 2017, volume 70 of Proceedings
neurips.cc/paper/2014/hash/ ofMachineLearningResearch,pp.2847–2854.PMLR,
109d2dd3608f669ca17920c511c2a41e- 2017. URL http://proceedings.mlr.press/
Abstract.html. v70/raghu17a.html.
13TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do abs/2110.09456,2021. URLhttps://arxiv.org/
imagenet classifiers generalize to imagenet? CoRR, abs/2110.09456.
abs/1902.10811, 2019. URL http://arxiv.org/
Shoeybi,M.,Patwary,M.,Puri,R.,LeGresley,P.,Casper,
abs/1902.10811.
J., and Catanzaro, B. Megatron-lm: Training multi-
Rong, Y., Huang, W., Xu, T., and Huang, J. Dropedge: billion parameter language models using model paral-
Towardsdeepgraphconvolutionalnetworksonnodeclas- lelism. CoRR, abs/1909.08053, 2019. URL http:
sification. In 8th International Conference on Learn- //arxiv.org/abs/1909.08053.
ingRepresentations,ICLR2020,AddisAbaba,Ethiopia,
Smith,S.,Patwary,M.,Norick,B.,LeGresley,P.,Rajbhan-
April26-30,2020.OpenReview.net,2020. URLhttps:
dari,S.,Casper,J.,Liu,Z.,Prabhumoye,S.,Zerveas,G.,
//openreview.net/forum?id=Hkx1qkrKPr.
Korthikanti,V.,Zheng,E.,Child,R.,Aminabadi,R.Y.,
Bernauer,J.,Song,X.,Shoeybi,M.,He,Y.,Houston,M.,
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,
Tiwary,S.,andCatanzaro,B. Usingdeepspeedandmega-
S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bern-
trontotrainmegatron-turingNLG530b,Alarge-scale
stein,M.S.,Berg,A.C.,andFei-Fei,L. Imagenetlarge
generativelanguagemodel.CoRR,abs/2201.11990,2022.
scalevisualrecognitionchallenge. Int.J.Comput.Vis.,
URLhttps://arxiv.org/abs/2201.11990.
115(3):211–252,2015. doi: 10.1007/S11263-015-0816-
Y. URL https://doi.org/10.1007/s11263-
Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A. A.
015-0816-y.
Inception-v4,inception-resnetandtheimpactofresidual
connectionsonlearning. InSingh,S.P.andMarkovitch,
Salazar, J., Liang, D., Nguyen, T. Q., and Kirchhoff, K.
S.(eds.), ProceedingsoftheThirty-FirstAAAIConfer-
Maskedlanguagemodelscoring. InJurafsky,D.,Chai,
enceonArtificialIntelligence,February4-9,2017,San
J., Schluter, N., and Tetreault, J. R. (eds.), Proceed-
Francisco,California,USA,pp.4278–4284.AAAIPress,
ingsofthe58thAnnualMeetingoftheAssociationfor
2017.URLhttp://aaai.org/ocs/index.php/
Computational Linguistics, ACL 2020, Online, July 5-
AAAI/AAAI17/paper/view/14806.
10, 2020, pp. 2699–2712. Association for Computa-
tionalLinguistics,2020. doi: 10.18653/V1/2020.ACL- Takase,S.,Kiyono,S.,Kobayashi,S.,andSuzuki,J. On
MAIN.240. URLhttps://doi.org/10.18653/ layer normalizations and residual connections in trans-
v1/2020.acl-main.240. formers. CoRR,abs/2206.00330,2022. doi: 10.48550/
ARXIV.2206.00330. URL https://doi.org/10.
Schoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl-
48550/arXiv.2206.00330.
Dickstein,J. Deepinformationpropagation. In5thInter-
nationalConferenceonLearningRepresentations,ICLR Tay, Y., Dehghani, M., Abnar, S., Chung, H. W., Fe-
2017, Toulon, France, April 24-26, 2017, Conference dus, W., Rao, J., Narang, S., Tran, V. Q., Yogatama,
TrackProceedings.OpenReview.net,2017.URLhttps: D., and Metzler, D. Scaling laws vs model archi-
//openreview.net/forum?id=H1W1UN9gg. tectures: How does inductive bias influence scaling?
In Bouamor, H., Pino, J., and Bali, K. (eds.), Find-
Shen, S., Yao, Z., Gholami, A., Mahoney, M. W., and
ings of the Association for Computational Linguistics:
Keutzer,K. Powernorm: Rethinkingbatchnormalization
EMNLP 2023, Singapore, December 6-10, 2023, pp.
intransformers. InProceedingsofthe37thInternational
12342–12364. Association for Computational Linguis-
Conference on Machine Learning, ICML 2020, 13-18 tics, 2023. URL https://aclanthology.org/
July 2020, Virtual Event, volume 119 of Proceedings 2023.findings-emnlp.825.
ofMachineLearningResearch,pp.8741–8751.PMLR,
2020. URL http://proceedings.mlr.press/ Touvron,H.,Cord,M.,Douze,M.,Massa,F.,Sablayrolles,
v119/shen20e.html. A., and Jégou, H. Training data-efficient image trans-
formers&distillationthroughattention. InMeila,M.and
Shi,H.,Gao,J.,Xu,H.,Liang,X.,Li,Z.,Kong,L.,Lee, Zhang,T.(eds.),Proceedingsofthe38thInternational
S.M.S.,andKwok,J.T. Revisitingover-smoothingin Conference on Machine Learning, ICML 2021, 18-24
BERTfromtheperspectiveofgraph. InTheTenthInter- July2021,VirtualEvent,volume139ofProceedingsof
nationalConferenceonLearningRepresentations,ICLR MachineLearningResearch,pp.10347–10357.PMLR,
2022,VirtualEvent,April25-29,2022.OpenReview.net, 2021a. URLhttp://proceedings.mlr.press/
2022. URLhttps://openreview.net/forum? v139/touvron21a.html.
id=dUV91uaXm3.
Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G.,
Shleifer,S.,Weston,J.,andOtt,M. Normformer: Improved and Jégou, H. Going deeper with image transform-
transformerpretrainingwithextranormalization. CoRR, ers. In 2021 IEEE/CVF International Conference on
14TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Computer Vision, ICCV 2021, Montreal, QC, Canada, H.M.,Larochelle,H.,Beygelzimer,A.,d’Alché-Buc,F.,
October 10-17, 2021, pp. 32–42. IEEE, 2021b. doi: Fox,E.B.,andGarnett,R.(eds.),AdvancesinNeuralIn-
10.1109/ICCV48922.2021.00010.URLhttps://doi. formationProcessingSystems32: AnnualConferenceon
org/10.1109/ICCV48922.2021.00010. NeuralInformationProcessingSystems2019,NeurIPS
2019,December8-14,2019,Vancouver,BC,Canada,pp.
Wang,H.,Ge,S.,Lipton,Z.C.,andXing,E.P. Learning 4383–4393, 2019. URL https://proceedings.
robust global representations by penalizing local neurips.cc/paper/2019/hash/
predictive power. In Wallach, H. M., Larochelle, H., 2f4fe03d77724a7217006e5d16728874-
Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and Abstract.html.
Garnett, R. (eds.), Advances in Neural Information
Processing Systems 32: Annual Conference on Neural Xue, F., Chen, J., Sun, A., Ren, X., Zheng, Z., He, X.,
Information Processing Systems 2019, NeurIPS 2019, Chen, Y., Jiang, X., and You, Y. A study on trans-
December 8-14, 2019, Vancouver, BC, Canada, pp. formerconfigurationandtrainingobjective. InKrause,
10506–10518,2019. URLhttps://proceedings. A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
neurips.cc/paper/2019/hash/ andScarlett,J.(eds.),InternationalConferenceonMa-
3eefceb8087e964f89c2d59e8a249915- chine Learning, ICML 2023, 23-29 July 2023, Hon-
Abstract.html. olulu, Hawaii, USA, volume 202 of Proceedings of
MachineLearningResearch,pp.38913–38925.PMLR,
Wang, H., Ma, S., Dong, L., Huang, S., Zhang, D., and 2023.URLhttps://proceedings.mlr.press/
Wei,F. Deepnet: Scalingtransformersto1,000layers. v202/xue23b.html.
CoRR,abs/2203.00555,2022a. doi: 10.48550/ARXIV.
2203.00555. URLhttps://doi.org/10.48550/ Yang, G. and Schoenholz, S. S. Mean field residual
arXiv.2203.00555. networks: On the edge of chaos. In Guyon, I., von
Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R.,
Wang, P., Zheng, W., Chen, T., and Wang, Z. Anti- Vishwanathan,S.V.N.,andGarnett,R.(eds.),Advances
oversmoothingindeepvisiontransformersviathefourier in Neural Information Processing Systems 30: Annual
domainanalysis: Fromtheorytopractice. InTheTenth ConferenceonNeuralInformationProcessingSystems
InternationalConferenceonLearningRepresentations, 2017, December4-9, 2017, LongBeach, CA,USA,pp.
ICLR2022,VirtualEvent,April25-29,2022.OpenRe- 7103–7114, 2017. URL https://proceedings.
view.net,2022b.URLhttps://openreview.net/ neurips.cc/paper/2017/hash/
forum?id=O476oWmiNNp. 81c650caac28cdefce4de5ddc18befa0-
Abstract.html.
Williams, A., Nangia, N., and Bowman, S. R. A broad-
coverage challenge corpus for sentence understanding Yang,G.,Hu,E.J.,Babuschkin,I.,Sidor,S.,Liu,X.,Farhi,
throughinference. InWalker,M.A.,Ji,H.,andStent,A. D.,Ryder,N.,Pachocki,J.,Chen,W.,andGao,J. Tuning
(eds.),Proceedingsofthe2018ConferenceoftheNorth large neural networks via zero-shot hyperparameter
AmericanChapteroftheAssociationforComputational transfer. In Ranzato, M., Beygelzimer, A., Dauphin,
Linguistics: Human Language Technologies, NAACL- Y.N.,Liang,P.,andVaughan,J.W.(eds.),Advancesin
HLT2018,NewOrleans,Louisiana,USA,June1-6,2018, NeuralInformationProcessingSystems34: AnnualCon-
Volume 1 (Long Papers), pp. 1112–1122. Association ferenceonNeuralInformationProcessingSystems2021,
forComputationalLinguistics,2018. doi: 10.18653/V1/ NeurIPS2021,December6-14,2021,virtual,pp.17084–
N18-1101. URL https://doi.org/10.18653/ 17097, 2021. URL https://proceedings.
v1/n18-1101. neurips.cc/paper/2021/hash/
8df7c2e3c3c3be098ef7b382bd2c37ba-
Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing,
Abstract.html.
C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On
layer normalization in the transformer architecture. In You, Y., Gitman, I., and Ginsburg, B. Scaling SGD
Proceedings of the 37th International Conference on batch size to 32k for imagenet training. CoRR,
Machine Learning, ICML 2020, 13-18 July 2020, Vir- abs/1708.03888, 2017. URL http://arxiv.org/
tual Event, volume 119 of Proceedings of Machine abs/1708.03888.
Learning Research, pp. 10524–10533. PMLR, 2020.
URLhttp://proceedings.mlr.press/v119/ You, Y., Li, J., Reddi, S. J., Hseu, J., Kumar, S., Bho-
xiong20b.html. janapalli, S., Song, X., Demmel, J., Keutzer, K., and
Hsieh, C. Large batch optimization for deep learning:
Xu, J., Sun, X., Zhang, Z., Zhao, G., and Lin, J. Under- TrainingBERTin76minutes. In8thInternationalCon-
standingandimprovinglayernormalization. InWallach, ferenceonLearningRepresentations,ICLR2020,Addis
15TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,
2020. URLhttps://openreview.net/forum?
id=Syx4wnEtvH.
Yu, A. W., Lin, Q., Salakhutdinov, R., and Carbonell,
J. G. Normalized gradient with adaptive stepsize
method for deep neural network training. CoRR,
abs/1707.04822, 2017. URL http://arxiv.org/
abs/1707.04822.
Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,
Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M.
Stabilizingtransformertrainingbypreventingattention
entropycollapse. InKrause,A.,Brunskill,E.,Cho,K.,
Engelhardt,B.,Sabato,S.,andScarlett,J.(eds.),Inter-
nationalConferenceonMachineLearning,ICML2023,
23-29July2023,Honolulu,Hawaii,USA,volume202of
ProceedingsofMachineLearningResearch,pp.40770–
40803.PMLR,2023. URLhttps://proceedings.
mlr.press/v202/zhai23a.html.
Zhang, B., Titov, I., and Sennrich, R. Improving deep
transformerwithdepth-scaledinitializationandmerged
attention. InInui,K.,Jiang,J.,Ng,V.,andWan,X.(eds.),
Proceedingsofthe2019ConferenceonEmpiricalMeth-
odsinNaturalLanguageProcessingandthe9thInterna-
tionalJointConferenceonNaturalLanguageProcessing,
EMNLP-IJCNLP 2019, Hong Kong, China, November
3-7,2019,pp.898–909.AssociationforComputational
Linguistics,2019a. doi: 10.18653/V1/D19-1083. URL
https://doi.org/10.18653/v1/D19-1083.
Zhang,H.,Dauphin,Y.N.,andMa,T. Fixupinitialization:
Residual learning without normalization. In 7th Inter-
nationalConferenceonLearningRepresentations,ICLR
2019,NewOrleans,LA,USA,May6-9,2019.OpenRe-
view.net,2019b.URLhttps://openreview.net/
forum?id=H1gsz30cKX.
Zhang, H., Yu, D., Yi, M., Chen, W., and Liu, T. Stabi-
lize deep resnet with a sharp scaling factor τ. Mach.
Learn.,111(9):3359–3392,2022. doi: 10.1007/S10994-
022-06192-X. URLhttps://doi.org/10.1007/
s10994-022-06192-x.
Zhao,H.,Ma,S.,Zhang,D.,Deng,Z.,andWei,F.Aremore
layersbeneficialtographtransformers? InTheEleventh
InternationalConferenceonLearningRepresentations,
ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenRe-
view.net,2023. URLhttps://openreview.net/
pdf?id=uagC-X9XMi8.
Zhou, D., Kang, B., Jin, X., Yang, L., Lian, X., Hou, Q.,
andFeng,J. Deepvit: Towardsdeepervisiontransformer.
CoRR,abs/2103.11886,2021. URLhttps://arxiv.
org/abs/2103.11886.
16TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Contents
A RelatedWorks 18
A.1 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.2 SignalPropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.3 MomentControl&ResidualScaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.4 OtherNetworkmodificationsforDeepNetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B MomentPropagationthroughTransformerComponents 20
B.1 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.2 Linear . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.3 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
B.4 ReLU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.5 GeLU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.6 LayerNorm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
B.7 Softmax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
B.8 ScaledDot-ProductAttention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
C MomentPropagationthroughTransformerBlocks 55
C.1 TransformerAttentionBlock . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
C.2 TransformerFFNBlock. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
D SummaryTableofMomentPropagationthroughTransformerComponents 56
E NumericalVerification 60
F RankCollapseandCorrelationAnalysis 61
G MomentPropagationthroughtheEntireTransformerModel 62
G.1 VanillaPre-LN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
G.1.1 ForwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
G.1.2 BackwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
G.2 VanillaPost-LN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
G.2.1 ForwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
G.2.2 BackwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
G.3 DeepScaleLMPre-LN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
G.3.1 ForwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
G.3.2 BackwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
G.4 DeepScaleLMPost-LN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
17TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
G.4.1 ForwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
G.4.2 BackwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
G.5 DeepScaleLM(Simplified)Pre-LN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
G.5.1 ForwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
G.5.2 BackwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
G.6 DeepScaleLM(Simplified)Post-LN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
G.6.1 ForwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
G.6.2 BackwardPass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
H ModelMomentFigures 70
H.1 ValidityofTheoreticalPredictionsandExplodingGradientsevenafterTraining . . . . . . . . . . . . . . 70
H.2 ImportanceofResidualScaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
I OtherModalities: VisionandSpeech 71
I.1 VisionTransformers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
I.2 TransformersforSpeechModality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
J StatisticalSignificance 72
J.1 ErrorBarsforPre-TrainingExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
J.2 StatisticalSignificanceforFine-tuningExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
K Compute 72
K.1 Theoreticalcompute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
K.2 WallClocktimes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
L DiscussionofRelativeStrength 73
M DiscussionofApproximationsandAssumptions 74
M.1 IllustrativeApproximationsofFullFormulaeinMainPaper . . . . . . . . . . . . . . . . . . . . . . . . 74
M.2 AssumptionsandApproximationsinDerivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
N DeepScaleLMPseudocode 75
O Hyper-parameters 76
A RelatedWorks
A.1 Initialization
Several works, such as Glorot & Bengio (2010); He et al. (2015); Brock et al. (2021) improved the initialization of
ResNets/ReLUnetworks,butcruciallytheseworksdonotconsidertheimpactofcorrelationintheinput,whichislargein
Transformermodels. Pooleetal.(2016)takescorrelationintoaccount,andSchoenholzetal.(2017)initializesweightsfor
networkswithboundedactivationssothatcorrelationreaches1asymptotically.
18TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Someworks,suchasMishkin&Matas(2016),sequentiallyprofileeachlayerempiricallybyrunningforwardpassesthrough
themodel,andscalingtheweightsand/oroutputtoachieveunitvariance,andLiuetal.(2020a;b)appliedthesamemethod
forTransformers. Blakeetal.(2023)alsotriestoachieveunitvariance,butdoesnotconsidercorrelationininputoracross
tokens,andignoresthenon-zeromeanofReLU,resultinginincorrectscale. Bachlechneretal.(2021)showsunitvariance
leadstofasterconvergenceatthestartofthetraining.
Wedemonstratethatthisprofilingisunnecessary,andcaninsteadbedonetheoreticallyinDeepScaleLM.Furthermore,
whereoutputorgradientincreasesinsomepriorworkswithmorelayers(eg. forADMIN(Liuetal.,2020a),graddecreases
by (N)(increasesby (log(N))forPre-LN)),ourmethodallowsmaintainingbothunitoutputandequalgradientacross
O O
alllayersatinitialization,andboundedduringtraining.
A.2 SignalPropagation
SignalpropagationinNeuralNetworkshasalonghistory,suchasNeal(1995);LeCunetal.(1996). Morerecently,several
workshavefocusedonsignalpropagationforResNets,suchasHeetal.(2015);De&Smith(2020);Brocketal.(2021);
Schoenholzetal.(2017);Hoedtetal.(2022);Labatieetal.(2021);Marionetal.(2022);Klambaueretal.(2017);Balduzzi
etal.(2017).
Fortransformers,signalpropagationwasstudiedinXuetal.(2019);Dongetal.(2021);Davisetal.(2021);Nocietal.
(2022). Ourworkalsoconsiderspreviouslyneglectedeffectsofdropout,inputcorrelationbetweentokens,non-linearity,
QK initialization,andprovidesclosedformswithverifiablecorrectnessofthissignalpropagation. Oursisthefirstworkto
theoreticallyconstraintheoutputandgradienttoalmostexactlyunitwithoutanyprofilingpasses,showingthevalidityof
ourformulaeandofourassumptions. SeethesectionSection4.3formorediscussiononNocietal.(2022)specifically.
Heetal.(2023)extendsneuralkernelmethodsofDKS(Martensetal.,2021)toTransformerstomodelnetworkbehaviour,
assumingtheMLPtobelinearinitseffectonattentionwithrespecttocorrelation. Q/C mapsinkernelmethodsaresimilar
tosignalpropagation,asexpectedmomentsareequivalenttoqandmvaluesofkernels(Martensetal.,2021). Ourmethod
relaxestheseassumptions,andweshowthatconsideringtheimpactofReLU/GeLUoncorrelationiscriticaltocorrectly
modellingattention. Inparticular,ourformulaeshowthatanMLPblockwithGeLUwillalsoincreasecorrelationinthe
absenceofdropout(thesamesettingasusedinHeetal.(2023)). Atlargedepths,Heetal.(2023)’smethodsuffersfrom
rankcollapse(withtheirdeepermodelsunder-performingshallowerones),whichourmethodsuccessfullyprevents.
Wealsoaccountforcaseswithnon-IIDinputsthatmayoccurduetosegment/positionembeddingsorduetonon-uniform
token distributions in real data (that are distributed approximately per Zipf’s law Kingsley (1935)) – and find that this
stronglyaffectsoutputvarianceoftheattentionblock.
A.3 MomentControl&ResidualScaling
Boundedgradients,ornormalizingper-layergradients,havebeenshowntoresultsinbetter/fasterconvergence(Shenetal.,
2020;Yuetal.,2017;Youetal.,2017;2020). WokssuchasTakaseetal.(2022);Shleiferetal.(2021);Hayouetal.(2019)
alsoachievedimprovedtrainingbyempiricallymitigatingthegradientexplosion.
Scalingwithλ2+β2 =1tocontrolmomentshaveoftenbeenusedforResNets(Balduzzietal.,2017;Szegedyetal.,2017;
Hanin&Rolnick,2018;Arpitetal.,2019;Zhangetal.,2019b;Hoedtetal.,2022). Szegedyetal.(2017)proposedtouse
anysmallβ,Balduzzietal.(2017)proposedtosetβ2 = 0.5,Bachlechneretal.(2021)setsβ = 0andlearnable. De&
Smith(2020)showedthatλ2 =0.5isnotsufficienttosolvevanishinggradients.
β2 = k wasusedtocontrolgrowthofmomentsinArpitetal.(2019);Brocketal.(2021);Marionetal.(2022);Zhang
N
etal.(2022);Heetal.(2023);Nocietal.(2022)). β2 = k,wherenisthecurrentlayer,wasusedinDe&Smith(2020);
n
Liuetal.(2020a;b);Davisetal.(2021);Blakeetal.(2023),butthisresultsinlogarithmicboundsinsteadofconstantfor
forwardpropagationifλ=1isused,andvanishinggradientforbackwardpropagationotherwise.
Valuesofβ2 < k,suchas(effectively) 1 forDSInit(Zhangetal.,2019a)or 1 forDeepNet(Wangetal.,2022a)
N N2 N1.5
decrease sensitivity of the model, and may result in the model becoming “too linear”. DeepNet shows performance
improvementsbymakingthemodeldeeper,butkeepingthehiddendimensionconstant. Oursettingismuchmorestrict
-wekeepthenumberofparameters(andhencecompute)constant,andourmethodstillshowperformanceimproveson
makingthemodeldeeper. Forexample,DeepNet‘s200layermodelis3.2Bparams,whereasour192layermodelis160M
params(20xsmaller).
19TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Sometimes,theseβ valuesareusedinconjunctionwithλ = 1,suchasinLiuetal.(2020a;b),butasshowninHeetal.
(2023), fully normalized residual connections with λ2 +β2 = 1 often perform better than those with λ = 1. We also
observedlowerperformancewithλ=1inourinitialexperiments,andhencewefullynormalizetheresidualconnections.
Ourcontributiongoesbeyondprovidinganoptimalscalingscheme. Usingthetheoreticalframeworkandclosed-form
expressionsformomentpropagationthroughbothPre-LNandPost-LNdevelopedinthiswork,practitionerscanmake
informedchoicesaboutusinganyofthescalingfactorsabovebasedonthestability-performancetradeoffs,suchasusinga
lowerβ forscenarioswithhighcorrelation,orusinghigherβ withuncorrelatedinputs.
A.4 OtherNetworkmodificationsforDeepNetworks
Shietal.(2022);Zhouetal.(2021);Wangetal.(2022b);Dongetal.(2021)showedthatattentioncausesrankcollapsein
deepermodels,andChenetal.(2020);Zhaoetal.(2023)showedthesameforgraphs. Takaseetal.(2022)addedsome
extraskipconnectionsfromtheinputofthemodel,Nguyen&Salazar(2019)modifiedlayernormslightly,Zhaietal.(2023)
normalizedalllinearlayersbytheirspectralnorm, andShleiferetal.(2021)addedextralayernorms. Someworksin
particular,suchasZhaietal.(2023);Zhouetal.(2021)canonlypreventattentionentropycollapselaterduringtraining,
but our work will also prevent rank collapse at initialization caused by the very structure of the transformer model, in
particularincreaseincorrelationcausedbybothattentionandReLU/GeLU.Themethodsintheseworksareorthogonalto
ourapproach,andourequationscanbeeasilyextendedtocoverthearchitecturalmodificationssuggestedinthese.
B MomentPropagationthroughTransformerComponents
Weprovidedetailedproofsoftheclosed-formexpressionforeachofthetransformercomponent–Linearlayer,Dropout,
ReLU,GeLU,LayerNorm,andSoftmax.
Foranycomponent,inputisrepresentedasx andx istheoutput. Thegradientflowinginintothecomponentfromthe
in out
outputsideisrepresentedasg andthebackpropagatedgradienttowardstheinputisg . Weswitchfromvectortomatrix
out in
notation(X ,X )wheneverneeded. Weassumethattheinputisdistributednormally (0,σ ). Noassumptionsare
in out
N
xin
maderegardingthecovarianceoftheinput–itisnotassumedtobeIID,anditmay/may-nothavecovariancebothalongthe
sequencelengthandhiddendimension. Additionalassumptionsneededtoderivetheproofsforsoftmaxandattentioncanbe
foundintherespectiveproofs.
B.1 Embeddings
The BERT model’s embedding componentconsists of 3 look up tables -tokenembeddings, position embeddings, and
segmentembeddings. Foragiveninputtoken,eachofthese3embeddingsareaddedbeforebeingpassedtothetransformer
model. Othertransformermodels,suchasdecoder-onlyGPTlacksome(eg. segment)ofthese,butthederivationsremain
similar. Wederiveformulaeforeachoftheseembeddingtypesbelow.
TokenEmbeddings WedonotassumetheinputembeddingstobeIID.Repetitionofsametokenintroducescorrelation
acrossthesequencelength. Weassumethattheinputtokenshavebeensampledfromamultinomialdistribution. Thewords
/tokenidsaredistributedalmostaccordingtoZipf’slaw(Kingsley,1935). Assumingweinitializealltheembeddingswith
varianceσ2 ,therelevantstatisticsforwordembeddingsoutputx areasfollows
wembd outwe
µ =0
xoutwe
σ2 =σ2
xoutwe wembd
Covl(x
)=(cid:88)N
i
∗(N
i
−1)
σ2
outwe L (L 1) ∗ wembd
∗ −
rl(x
)=(cid:88)N
i
∗(N
i
−1)
outwe
L (L 1)
∗ −
Covd(x )=0
outwe
20TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
AssumeithwordoccursN
i
times,itcontributes N Li∗ (( LNi− 1)1 )) tothecovariancealongsequencelength. Similarly,wecan
calculatethecorrelationforsegment-typeembedding∗so−utputx . Zipf’slawstatesthattheprobabilityforeachtokenis
outse
inverselyproportionaltoitsrank. Forthewordwithranki,p = c,wherec= 1 = 1 ,whereγ 0.58isthe
i i (cid:80) i 1 i γ+log( |V |) ≈
Euler’sconstant.
ForasentenceoflengthL,thetokenwithprobabilityp isexpectedtooccurp .Ltimes. Hence,foragivenvocabularysize
i i
V ,wecancalculatethecorrelationasfollows
| |
rl(x
)=(cid:88)N
i
∗(N
i
−1)
outwe
L (L 1)
∗ −
V
(cid:88)| | p iL (p iL 1)
= ∗ −
L (L 1)
i ∗ −
(cid:80) p2 L 1
= i i ∗ −
L 1
−
(cid:80) c2
L 1
= i i2 ∗ −
L 1
−
Lπ2
1
6.(γ+log(V ))2 −
| |
≈ L 1
−
π2
,assumingγ 0.58<<log(V ) 10.4,L>>1
≈ 6.log(V )2 ≈ | | ≈
| |
SegmentTypeEmbeddings Similarly,thesegmenttypeembeddingshavetwopossiblevaluesdenotingthesentence
order. Iffirstsentencehaslengthx,wecanconsiderthisasaspecialcaseoftheanalysisperformedabovewithtwopossible
tokens,whereN = xandN = L x. Assumingxisdistributeduniformlybetween0toL,L xalsohasthesame
1 2
− −
distribution. Hence,
N2+N2 L
rl(x ,N ,N )= 1 2 −
outse 1 2
L (L 1)
∗ −
Takingexpectation,weget
2 L2 L
rl(x )= 3 ∗ −
outse
L (L 1)
∗ −
2
≈ 3
PositionEmbeddings Sincelearntpositionembeddingsarelookuptableswithuniqueinputs,thecorrelationfromposition
embeddingsis0.
FinalModelInputEmbeddings Eachoftheaboveembeddingsareaddedbeforebeingpassedtothetransformermodel.
Sincethevarianceissameforallembeddingtypes,thefinalcorrelationistheaverageofthethree. Hence:
1
rl(x )= (rl(x )+rl(x ))
out
3
outwe outse
π2 2
= +
18 log(V )2 9
∗ | |
Forourcase, V = 32000andsequencelengthL = 256,thetheoreticallypredictedcorrelationrl = 0.227whichis
| | xin
within3%oftheempiricallyobservedcorrelation(0.221).
21TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Hence,thefinalmomentsfortheembeddingoutputare
µ =0
xout
σ2 =3 σ2
xout ∗ wembd
π2 2
Covl =( + )σ2
xout 18 log(V )2 9 xout
∗ | |
Covd =0
xout
B.2 Linear
Forlinearlayerwithd dimensionalinputx ,andd dimensionaloutputx ,wecandefinetheforwardpassmathemat-
in in out out
icallyas,
x =x W
out in
(cid:88)din
= x = x W
⇒
outj ini i,j
i=1
Similarly,wedefinethebackwardpassas,
g =g WT
in out
(cid:88)dout
= g = g W
⇒
inj outi j,i
i=1
Forexpectationofoutputwehave,
(cid:88)din (cid:88)din
E[x ]=E[ x W ]= E[x W ]
outj ini i,j ini i,j
i=1 i=1
(cid:88)din
= E[x ]E[W ]=µ µ
ini i,j xin w
i=1
(Asweightsandinputareindependentofeachother)
µ =0 ( j)
xout
∀
Togetvarianceoftheoutputofforwardpasswehave,
(cid:88)din
Var(x )=Var( x W )
outj ini i,j
i=1
Astheweightsareinitializedindependentlyeachterminsummationisindependentofeachother
(cid:88)din
= (Var(x W ))
ini i,j
i=1
(cid:88)din
= ((σ2 +µ2 )(σ2 +µ2) µ2 µ2)
xin xin w w − xin w
i=1
(Asweightsandinputareindependentofeachother)
(cid:88)din
= (σ2 +µ2 )σ2
xin xin w
i=1
22TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Var(x )=d (σ2 +µ2 )σ2 ( j)
outj in xin xin w ∀
σ2 =d (σ2 +µ2 )σ2
xout in xin xin w
Ifwehavetwoinputsx andy suchthatforalliwehaveCorr(x ,y ) = rl ,andx = x Wandy = y W.
in in ini ini xin out in out in
Thenforanyj wehave
E[x y ] E[x ]E[y ]
Corr(x ,y )=
outj outj
−
outj outj
outj outj (cid:112)
Var(x )Var(y )
outj outj
E[x y ]
=
outj outj
(cid:112)
σ2 σ2
xout xout
E[(cid:80)din
x W
(cid:80)din
y W ]
= i=1 ini i,j k=1 ink k,j
σ2
xout
E[(cid:80)din x y W2 +(cid:80)din (cid:80)din x y W W ]
= i=1 ini ini i,j k=1,k ̸=i i=1 ini ink i,j k,j
σ2
xout
Insecondsummationalltermsareindependentofeachotherandastheexpectationofweightsis0wehave
E[(cid:80)din x y W2 ]
Corr(x ,y )= i=1 ini ini i,j
outj outj σ2
xout
(cid:80)din E[x y W2 ]
= i=1 ini ini i,j (Independenceofweightinitialization)
σ2
xout
(cid:80)din E[x y ]E[W2 ]
= i=1 ini ini i,j
σ2
xout
(cid:80)din (rl σ2 +µ2 )σ2
= i=1 xin xin xin w (Definitionofcorrelation)
σ2
xout
d (rl σ2 +µ2 )σ2
= in xin xin xin w
d (σ2 +µ2 )σ2
in xin xin w
rl σ2 +µ2
Corr(x ,y )= xin xin xin
outj outj σ2 +µ2
xin xin
rl σ2 +µ2
rl = xin xin xin
xout σ2 +µ2
xin xin
Asthebackwardpasshassimilarstructure,assumingµ =0wecanusethesameanalysistoget,
gout
µ =0
gin
σ2 =d σ2 σ2
gin out gout w
B.3 Dropout
WecandefineDropoutmathematicallyas,
x =Dropout(x )
out in
(cid:40) xini
withprobability1 p
= x = (1 p) −
⇒
outi
0
−
else
Tocalculateexpectationofdropout,
x
E[x ]=0 p+(1 p) E[ ini ]
outi
∗ − ∗ (1 p)
−
23TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
µ =µ
xout xin
Forvariance,
Var(x )=E[x2 ] E[x ]2
outi outi − outi
x2
=0 p+(1 p) E[ ini ] µ2
∗ − ∗ (1 p)2 − xin
−
E[x2 ]
= ini µ2
(1 p) − x
−
σ2 +µ2
= xin xin µ2
(1 p) − xin
−
σ2 +pµ2
σ2 = xin xin
xout (1 p)
−
If we have two inputs x and y such that for all i we have Corr(x ,y ) = rl , and x = Dropout(x ) and
in in ini ini xin out in
y =Dropout(y ). Thenforanyj wehave
out in
E[x y ] E[x ]E[y ]
Corr(x ,y )=
outj outj
−
outj outj
outj outj (cid:112)
Var(x )Var(y )
outj outj
E[x y ] µ µ
=
outj outj
−
xout xout
(cid:112)
σ2 σ2
xout xout
p2 0+2 p (1 p) 0+(1 p)2 E[ xinjyinj ] µ2
= ∗ ∗ ∗ − ∗ − ∗ (1 −p) ∗(1 −p) − xout
σ2
xout
E[x y ] µ2
= inj inj − xout
σ2
xout
(rl σ2 )(1 p)
Corr(x ,y )= xin xin − =rl
outj outj σ2 +pµ2 xout
xin xin
WecandefinethebackwardpassofDropoutas,
(cid:40) gouti
ifx isn’tdroppedout(whichhasprobability(1 p))
g = (1 p) i −
ini
0
−
else
Again wecan see that backwardhas similar definition tothat of forward pass. Assuming µ = 0 and usingsimilar
gxout
analysisweget,
µ =0
gin
σ2
σ2 = gout
gin (1 p)
−
B.4 ReLU
Formualefunctionallyequivalenttooursforµ ,σ2,andσ2havealsobeenderivedinArpitetal.(2016).
x x g
WecandefineReLUmathematicallyas,
x =ReLU(x )
out in
(cid:40)
x ifx >0
= x =
ini ini
⇒
outi
0 else
24TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
ForgettingexpectationofoutputofReLUfornormallydistributedinputwehave,
x2
E[x
]=(cid:90)
∞
ReLU(x ini)exp(−
2σ
x2in ini)
dx
outi
√2πσ
ini
−∞
xin
x2 x2
=(cid:90) 0 0 ∗exp(−
2σ
x2in ini)
dx
+(cid:90)
∞
x iniexp(−
2σ
x2in ini)
dx
√2πσ
ini
√2πσ
ini
−∞
xin 0 xin
x2
=(cid:90)
∞
x iniexp(−
2σ
x2in ini)
dx
√2πσ
ini
0 xin
x2 x dx
Substitutingt= ini wehavedt= ini ini weget,
2σ2 σ2
xin xin
(cid:90)
E[x ]= ∞ σ xinexp( −t)dt
outi
√2π
0
σ σ
=
xin
[ exp( t)] =
xin
√2π − −
∞0
√2π
Hence,themeanofoutput
σ
µ = xin (1)
xout
√2π
Varianceofoutputcanbecalculatedby,
Var(x )=E[x 2] E[x ]2
outi outi
−
outi
x2
=(cid:90) ∞ (ReLU(x ini))2exp(− 2σ x2in ini)
dx
σ x2
in
√2πσ
ini
− 2π
−∞
xin
x2 x2
=(cid:90) 0 0 ∗exp(− 2σ x2in ini)
dx
+(cid:90) ∞ x2 iniexp(− 2σ x2in ini)
dx
σ x2
in
√2πσ
ini
√2πσ
ini
− 2π
−∞
xin 0 xin
x2
=(cid:90) ∞ x2 iniexp(− 2σ x2in ini)
dx
σ x2
in
√2πσ
ini
− 2π
0 xin
x2
(cid:90) x2 exp(− ini)
LetI =
∞ ini 2σ x2
in dx ,thensubstitutingt= x wehave,
√2πσ
ini
−
ini
0 xin
I
=(cid:90)
−∞
−t2exp( 2−σt x22 in)
dt
√2πσ
0 xin
(cid:90) 0 t2exp( 2−σt 22 )
= xin dt
√2πσ
−∞
xin
= I+I
=(cid:90) 0 t2exp( 2−σt x22 in) dt+(cid:90)
∞
x2 iniexp(− 2σx x22 in ini)
dx
⇒ √2πσ √2πσ
ini
−∞
xin 0 xin
x2
(cid:90) x2 exp(− ini)
2I = ∞ ini 2σ x2 in dx =σ2
√2πσ ini xin
−∞
xin
σ2 σ2 σ2 1
= Var(x )= xin xin = xin(1 )
⇒
outi
2 − 2π 2 − π
25TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
σ2 1
σ2 = xin(1 )
xout 2 − π
Now for two inputs x and y such that for all i we have Corr(x ,y ) = rl , and x = ReLU(x ) and y =
in in ini ini xin out in out
ReLU(y ). Thenforanyj wehave,
in
E[x y ] E[x ]E[y ]
Corr(x ,y )=
outj outj
−
outj outj
outj outj (cid:112)
Var(x )Var(y )
outj outj
E[x y
]=(cid:90) ∞(cid:90)
∞
x injy
inj
exp(−(x2
inj
+y i2
nj
−2r xl inx injy inj)
)dx dy
outj outj (cid:113) 2σ2 (1 (rl )2) inj inj
0 0 2πσ x2
in
1 −(r xl in)2 xin − xin
(cid:90) (cid:90) x y (x rl y )2 y2
= ∞ ∞ inj inj exp(− inj − xin inj )exp(− inj)dx dy
(cid:113) 2σ2 (1 (rl )2) 2σ2 inj inj
0 0 2πσ x2
in
1 −(r xl in)2 xin − xin xin
Substitutingt=x rl y ,andassumingy isconstantfortheinnerintegral,dx =dt
inj − xin inj inj inj
E[x y ]=
outj outj
y2
=(cid:90) 0∞ y inj √ex 2p πσ(− x2σ inx2in ij n)(cid:90) −∞
r xl inyinj
√2πσt x+ in(cid:113)r xl 1iny −in (j
r xl in)2
exp(
2σ x2
in(1− −t2
(r xl
in)2))dtdy
inj
=(cid:90) 0∞ √2y πin σj
xin
exp(− 2σy x2i2 n inj)(cid:90) −∞
r xl inyinj √2πσ
xin(cid:113)t
1 −(r xl in)2
exp(
2σ x2
in(1− −t2
(r xl
in)2))dtdy
inj
+(cid:90)
0∞
√y
2i πnj
σ x
exp(−
2σy x2i2
n
inj)(cid:90)
−∞
r xl inyinj √2πσ
xir nxl (cid:113)iny
1in −j
(r xl in)2
exp(
2σ x2
in(1−
−t2
(r xl
in)2))dtdy
inj
LetusfirstdefineI andI as:
1 2
I
1
=(cid:90) 0∞ √2y πin σj
xin
exp(− 2σy x2i2 n inj)(cid:90) −∞
r xl inyinj √2πσ
xin(cid:113)t
1 −(r xl in)2
exp(
2σ x2
in(1− −t2
(r xl
in)2))dtdy
inj
I
2
=(cid:90)
0∞
√y
2i πnj
σ x
exp(−
2σy x2i2
n
inj)(cid:90)
−∞
r xl inyinj √2πσ
xir nxl (cid:113)iny
1in −j
(r xl in)2
exp(
2σ x2
in(1−
−t2
(r xl
in)2))dtdy
inj
I
1
=(cid:90) 0∞ √2y πin σj
xin
exp(− 2σy x2i2 n inj)(cid:90) −∞
r xl inyinj √2πσ
xin(cid:113)t
1 −(r xl in)2
exp(
2σ x2
in(1− −t2
(r xl
in)2))dtdy
inj
t2 tdt
Substitutingp= wehavedp=
2σ2 (1 (rl )2) σ2 (1 (rl )2)
xin − xin xin − xin
(cid:113)
I
=(cid:90) ∞ y inj exp(−y i2 nj)(cid:90) ∞ σ xin (1 −(r xl in)2)
exp( p)dpdy
1 0 √2πσ xin 2σ x2 in 2σx2( ir nxl (1in −y (in rj xl) i2 n)2) √2π − inj
(cid:113)
=(cid:90) ∞ y inj exp(−y i2 nj)σ xin (1 −(r xl in)2)
exp(
−(r xl iny inj)2
)dy
√2πσ 2σ2 √2π 2σ2 (1 (rl )2) inj
0 xin xin xin − xin
(cid:113)
=(cid:90) ∞ y inj (1 −(r xl in)2)
exp(
−y i2 nj
)dy
2π 2σ2 (1 (rl )2) inj
0 xin − xin
26TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
y2 y dy
Substitutingm=
inj
,dm=
inj inj
,
2σ2 (1 (rl )2) σ2 (1 (rl )2)
xin − xin xin − xin
(cid:113)
(cid:90) (1 (rl )2)
I = ∞ − xin (1 (rl )2)σ2 exp( m)dm
1 2π − xin xin −
0
=
(1 −(r xl in)2)3 2σ x2
in
2π
I
2
=(cid:90)
0∞
√2y
πin σj
xin
exp(−
2σy x2i2
n
inj)(cid:90)
−∞
r xl inyinj √2πσ
xir nxl (cid:113)iny
1in −j
(r xl in)2
exp(
2σ x2
in(1−
−t2
(r xl
in)2))dtdy
inj
=(cid:90) 0∞ √r xl 2i πny σi2 n xj
in
exp(− 2σy x2i2 n inj)(cid:90) −∞
r xl inyinj √2πσ
xin(cid:113)1
1 −(r xl in)2
exp(
2σ x2
in(1− −t2
(r xl
in)2))dtdy
inj
Substitutingp= t,whereΦisCDFofStandardNormalDistribution
−
I
2
=(cid:90) 0∞ √r xl 2i πny σi2 n xj
in
exp(− 2σy x2i2 n inj)(cid:90)
r
xl− in∞
yinj √2πσ
xin(cid:113)− 11
−(r xl in)2
exp(
2σ x2
in(1− −p2
(r xl
in)2))dpdy
inj
=(cid:90) 0∞ √r xl 2i πny σi2 n xj
in
exp(− 2σy x2i2 n inj)(cid:90) −r ∞xl inyinj
√2πσ
xin(cid:113)1
1 −(r xl in)2
exp(
2σ x2
in(1− −p2
(r xl
in)2))dpdy
inj
(cid:90) rl y2 y2 rl y
= ∞ xin inj exp(− inj)Φ( xin inj )dy
0
√2πσ
xin
2σ x2
in σ
xin(cid:113)
1 −(r xl in)2
inj
=(cid:90) ∞ r xl iny i2 nj exp(−y i2 nj)[1
(1+erf(
r xl iny inj
))]dy
0
√2πσ
xin
2σ x2
in
2
σ
xin(cid:113)
2(1 −(r xl in)2)
inj
rl (cid:90) y2 y2
= xin ∞ inj exp(− inj)dy +
2 √2πσ 2σ2 inj
0 xin xin
rl (cid:90) y2 rl y
xin ∞ y2 exp(− inj)erf( xin inj )dy
2√2πσ
xin 0
inj 2σ x2
in σ
xin(cid:113)
2(1 −(r xl in)2)
inj
LetusdefineI andI as
2,1 2,2
rl (cid:90) y2 y2
I = xin ∞ inj exp(− inj)dy
2,1 2 √2πσ 2σ2 inj
0 xin xin
rl (cid:90) y2 rl y
I = xin ∞ y2 exp(− inj)erf( xin inj )dy
2,2 2√2πσ
xin 0
inj 2σ x2
in σ
xin(cid:113)
2(1 −(r xl in)2)
inj
rl (cid:90) y2 y2
I = xin ∞ inj exp(− inj)dy
2,1 2 √2πσ 2σ2 inj
0 xin xin
rl σ2
I = xin xin (Sameintegralasinvariancecalculation)
2,1
4
27TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
FromNg&Geller(1969)wehave(cid:90) ∞ x2exp( b2x2)erf(ax)dx= √π tan −1( ab) + a .
− 4b3 − 2√πb3 2√πb2(a2+b2)
0
rl 1
Hence,puttinga= xin andb= weget,
(cid:113)
σ
xin
2(1 −(r xl in)2) σ xin√2
√(1 (rl )2) (cid:113)
I = r xl in [2√2σ x3 in tan −1( − r xl inxin )2√2σ x3 in + √2r xl inσ x3 in (1 −(r xl in)2) ]
2,2
2√2πσ 4 − 2√π √π
xin
(cid:113)
=
r xl inσ x2
in
r xl incos −1(r xl in)σ x2
in +
(r xl in)2 (1 −(r xl in)2)σ x2
in
4 − 2π 2π
E[x y ]=I +I +I
outj outj 1 2,1 2,2
(cid:113)
=
(1 −(r xl in)2)3 2σ x2
in +2
r xl inσ x2
in
r xl incos −1(r xl in)σ x2
in +
(r xl in)2 (1 −(r xl in)2)σ x2
in
2π ∗ 4 − 2π 2π
(cid:113)
=
r xl inσ x2
in
r xl incos −1(r xl in)σ x2
+
(1 −(r xl in)2 )σ x2
in
2 − 2π 2π
E[x y ] E[x ]E[y ]
Corr(x ,y )=
outj outj
−
outj outj
outj outj (cid:112)
Var(x )Var(y )
outj outj
(cid:113)
r xl inσ x2
in
r xl incos −1(r xl in)σ x2
in +
(1 −(r xl in)2)σ x2 σ x2
in
2 − 2π 2π − 2π
=
σ2 1
xin(1 )
2 − π
rl =
πr 2xl in +r xl insin−1(r xl in)+(cid:113) (1 −(r xl in)2) −1
xout π 1
−
BackwardpassonReLUcanbedefinedas,
(cid:40)
g ifx >0(whichhasprobability 1)
g = outi ini 2
ini
0 else
Assumingµ =0,
gout
1 1
E[g ]= 0+ E[g ]
ini
2 ∗ 2 ∗
outi
µ =0
gin
Var(g )=E[g2 ] E[g ]2 =E[g2 ]
ini ini − ini ini
1 1
= 0+ E[g2 ]
2 ∗ 2 ∗ out
σ2
σ2 = gout
gin 2
Iffortwoinputsx andy foralliwehaveCorr(g ,g )=rl ,andg ,g bethegradientafterpassingthrough
in in outxi outyi gout inxi inyi
ReLUlayer. Thenwehave,
E[g g ]=P(x >0,y >0)E[g g ]
inxi inyi ini ini outxi outyi
=P(x >0,y >0)rl σ2
ini ini gout gout
28TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
P(x >0,y >0)=
ini ini
=(cid:90) ∞(cid:90) ∞ x iniy ini exp(−(x2 ini +y i2 ni −2r xl inx iniy ini) )dx dy
(cid:113) 2σ2 (1 (rl )2) ini ini
0 0 2πσ x2
in
1 −(r xl in)2 xin − xin
=(cid:90) ∞(cid:90) ∞ x iniy ini exp(−(x ini −r xl iny ini)2 )exp(−y i2 ni)dx dy
(cid:113) 2σ2 (1 (rl )2) 2σ2 ini ini
0 0 2πσ x2
in
1 −(r xl in)2 xin − xin xin
Substitutingt=x rl y ,andassumingy isconstantfortheinnerintegral,dx =dt
ini − xin ini ini ini
P(x >0,y >0)=
ini ini
(cid:90) 0∞ √2π1
σ xin
exp(− 2σy x2i2 n ini)(cid:90) −∞
r xl inyini √2πσ
xin(cid:113)1
1 −(r xl in)2
exp(
2σ x2
in(1− −t2
(r xl
in)2))dtdy
ini
Substitutingp= t,whereΦisCDFofStandardNormalDistribution
−
P(x >0,y >0)=
ini ini
=(cid:90) 0∞ √2π1
σ xin
exp(− 2σy x2i2 n ini)(cid:90)
r
xl− in∞
yini √2πσ
xin(cid:113)− 11
−(r xl in)2
exp(
2σ x2
in(1− −p2
(r xl
in)2))dpdy
ini
=(cid:90) 0∞ √2π1
σ xin
exp(− 2σy x2i2 n ini)(cid:90) −r ∞xl inyini
√2πσ
xin(cid:113)1
1 −(r xl in)2
exp(
2σ x2
in(1− −p2
(r xl
in)2))dpdy
ini
=(cid:90) ∞ 1 exp(−y i2 ni)Φ( r xl iny ini )dy
0
√2πσ
xin
2σ x2
in σ
xin(cid:113)
1 −(r xl in)2
ini
=(cid:90) ∞ 1 exp(−y i2 ni)[1
(1+erf(
r xl iny ini
))]dy
0
√2πσ
xin
2σ x2
in
2
σ
xin(cid:113)
2(1 −(r xl in)2)
ini
= 1(cid:90) ∞ 1 exp(−y i2 ni)dy + 1 (cid:90) ∞ exp(−y i2 ni)erf( r xl iny ini )dy
2
0
√2πσ
xin
2σ x2
in
ini 2√2πσ
xin 0
2σ x2
in σ
xin(cid:113)
2(1 −(r xl in)2)
ini
= 1 + 1 (cid:90) ∞ exp(−y i2 ni)erf( r xl iny ini )dy
4 2√2πσ
xin 0
2σ x2
in σ
xin(cid:113)
2(1 −(r xl in)2)
ini
(cid:90)
FromNg&Geller(1969)wehave ∞ exp( b2x2)erf(ax)dx= √π 1 tan 1(b )
−
− 2b − b√π a
0
rl 1
Puttinga= xin andb= weget,
(cid:113)
σ
xin
2(1 −(r xl in)2) σ xin√2
(cid:113)
P(x >0,y >0)=
1
+
1 [√πσ xin√2 σ xin√2
tan 1(
(1 −(r xl in)2)
)]
ini ini 4 2√2πσ 2 − √π − rl
xin xin
1 1 π
= + [ cos 1(rl )]
4 2π 2 − − xin
=
1
+
sin−1(r xl in)
4 2π
= E[g g
]=(1
+
sin−1(r xl in)
)rl σ2
⇒ inxi inyi 4 2π gout gout
29TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
(1 + sin−1(r xl in) )rl σ2
Corr(g ,g )= 4 2π gout gout
inxi inyi σ g2
out
2
rl
=(1
+
sin−1(r xl in)
)rl
gout 2 π gout
B.5 GeLU
ForwardpassthroughGeLUisdefinedas,
x =GeLU(x )
out in
= x =x Φ(x )
⇒
outi ini ini
whereΦ(x)isCDFofStandardNormalDistributionatx
(cid:18) (cid:19)
x x
=
ini
1+erf(
ini)
2 √2
TogetthemeanofoutputofGeLU,wehave
E[x ]=(cid:90) ∞ x outi exp(−x2 ini)dx
outi √2πσ 2σ2 ini
−∞
xin xin
=(cid:90)
∞
x ini(1+erf(x √in 2i)) exp(−x2
ini)dx
2√2πσ 2σ2 ini
−∞
xin xin
=(cid:90) ∞ x ini exp(−x2 ini)dx +(cid:90) ∞ x inierf(x √in 2i) exp(−x2 ini)dx
2√2πσ 2σ2 ini 2√2πσ 2σ2 ini
−∞
xin xin
−∞
xin xin
=(cid:90)
∞
x inierf(x √in 2i) exp(−x2
ini)dx (Integralofoddfunction)
2√2πσ 2σ2 ini
−∞
xin xin
= 1 (cid:90) ∞ x erf(x ini)exp(−x2 ini)dx
2√2πσ ini √2 2σ2 ini
xin
−∞
xin
(cid:90)
From2.6.1.4ofKorotkov&Korotkov(2020), ∞ zerf(az)exp( a z2)dz = a
1
− a √a2+a
1 1
−∞
1 1
Substituting,a= ,a = ,wehave
√2 1 2σ2
xin
1
1
E[x ]= √2
outi
2√2πσ xin 2σ1 2
(cid:113)
1 2 + 2σ1 2
xin xin
1 2σ3
= xin
(cid:112)
2√2πσ σ2 +1
xin xin
σ2
µ = xin
xout (cid:112)
2π(σ2 +1)
xin
Forcalculatingvarianceofoutput,
(cid:90) x2 x2
E[x2 ]= ∞ outi exp(− ini)dx
outi √2πσ 2σ2 ini
−∞
xin xin
30TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
=(cid:90)
∞
x2 ini(1+erf(x √in 2i))2 exp(−x2
ini)dx
4√2πσ 2σ2 ini
−∞
xin xin
(cid:90) x2 x2
= ∞ ini exp(− ini)dx
4√2πσ 2σ2 ini
−∞
xin xin
+(cid:90)
∞
x2 inierf(x √in 2i) exp(−x2
ini)dx
+(cid:90)
∞
x2 inierf2(x √in 2i) exp(−x2
ini)dx
2√2πσ 2σ2 ini 4√2πσ 2σ2 ini
−∞
xin xin
−∞
xin xin
=
σ x2
in
+(cid:90)
∞
x2 inierf2(x √in 2i) exp(−x2
ini)dx (Definitionofvariance,andintegralofoddfunction)
4 4√2πσ 2σ2 ini
−∞
xin xin
= σ x2 in + 1 (cid:90) ∞ x2 erf2(x ini)exp(−x2 ini)dx
4 4√2πσ ini √2 2σ2 ini
xin
−∞
xin
From2.7.3.3ofKorotkov&Korotkov(2020)
(cid:90)
∞ z2exp( az2)erf(a z)erf(a z)=
1 2
−
−∞
1 1 a a a a (2a+a2+a2)
( tan 1( 1 2 )+ 1 2 1 2 )
− (cid:112) (cid:112)
√π a√a a2+aa2+aa2 a a+a2+a2(a2+aa2+aa2+a2a2)
1 2 1 2 1 2 1 2
Substitutinga= 1 ,a =a = 1
2σ x2
in
1 2 √2
(cid:90) ∞ x2 erf2(x ini)exp(−x2 ini)dx
ini √2 2σ2 ini
−∞
xin
1 1 1 2( σ1 2 +1)
= (2√2σ3 tan 1( 2 )+ xin )
√π xin − (cid:113) 1 + 1 1 (cid:113) 1 +1( 1 + 1 + 1)
4σ4 2σ2 2σ2 2σ2 4σ4 2σ2 4
xin xin xin xin xin xin
1 σ2 4√2σ5 (σ2 +1)
= (2√2σ3 tan 1( xin )+ xin xin )
√π xin − (cid:112) (σ2 +1)2 σ4 (cid:112) 2σ2 +1(σ4 +2σ2 +1)
xin − xin xin xin xin
1 σ2 4√2σ5
= √π(2√2σ x3 insin−1(
σ2
x +in 1)+
(cid:112)
2σ2
+1(x σin
2
+1))
xin xin xin
2√2σ3 σ2 2σ2
= xin(sin−1( xin )+
(cid:112)
xin ))
√π σ2 +1 2σ2 +1(σ2 +1)
xin xin xin
E[x2 ]= σ x2 in + 1 (cid:90) ∞ x2 erf2(x ini)exp(−x2 ini)dx
outi 4 4√2πσ ini √2 2σ2 ini
xin
−∞
xin
σ2 1 2√2σ3 σ2 2σ2
= xin + xin(sin−1( xin )+
(cid:112)
xin ))
4 4√2πσ √π σ2 +1 2σ2 +1(σ2 +1)
xin xin xin xin
σ2 σ2 σ2 2σ2
E[x2 outi]= 4xin + 2x πin(sin−1(
σ2
x +in 1)+
(cid:112)
2σ2
+1x (in
σ2
+1)))
xin xin xin
Var(x )=E[x2 ] (E[x ])2
outi outi − outi
σ2 π σ2 σ2 2σ2
σ x2
out
= 2x πin(
2 −
1+x σin
2
+sin−1( 1+x σin
2
)+
(1+σ2
)(cid:112)xin
1+2σ2
)
xin xin xin xin
Nowifwehavetwoinputsx andy suchthatforallvaluesofi,wehaveCorr(x ,y )=rl ,thenwecancalculate
in in ini ini xin
thecovarianceCov(x ,y )foranyj as,
outj outj
Cov(x ,y )=E[x y ] E[x ]E[y ]
outj outj outj outj
−
outj outj
31TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
E[x y ]
outj outj
=(cid:90)(cid:90) ∞ x outjy outj exp(−x2 inj +2r xl inx injy inj −y i2 nj)dx
dy =I
(cid:113) 2σ2 (1 (rl )2) inj inj
−∞ 2πσ x2 in (1 −(r xl in)2) xin − xin
=(cid:90)(cid:90)
∞
x inj(1+erf(x √in 2j))y inj(1+erf(y √in 2j)) exp(−x2
inj
+2r xl inx injy
inj
−y i2
nj)dx
dy
(cid:113) 2σ2 (1 (rl )2) inj inj
−∞ 8πσ x2 in (1 −(r xl in)2) xin − xin
=(cid:90)
∞
y inj(1+erf(y √in 2j))
exp(
−y i2
nj
)I dy
(cid:113) 2σ2 (1 (rl )2) X inj
−∞ 8πσ x2 in (1 −(r xl in)2) xin − xin
WhereI
=(cid:90) ∞
x
(1+erf(x inj))exp(−x2 inj +2r xl inx injy inj)dx
X inj √2 2σ2 (1 (rl )2) inj
−∞ xin − xin
I
=(cid:90) ∞
x
(1+erf(x inj))exp(−x2 inj +2r xl inx injy inj)dx
X inj √2 2σ2 (1 (rl )2) inj
−∞ xin − xin
(cid:90) x2 +2rl x y
= ∞ x exp(− inj xin inj inj)dx +
inj 2σ2 (1 (rl )2) inj
−∞ xin − xin
(cid:90) ∞
x
erf(x inj)exp(−x2 inj +2r xl inx injy inj)dx
inj √2 2σ2 (1 (rl )2) inj
−∞ xin − xin
(cid:90) x2 +2rl x y
Let,I = ∞ x exp(− inj xin inj inj)dx
X,1 inj 2σ2 (1 (rl )2) inj
−∞ xin − xin
I
=(cid:90) ∞
x
erf(x inj)exp(−x2 inj +2r xl inx injy inj)dx
X,2 inj √2 2σ2 (1 (rl )2) inj
−∞ xin − xin
(cid:90) x2 +2rl x y
I = ∞ x exp(− inj xin inj inj)dx
X,1 inj 2σ2 (1 (rl )2) inj
−∞ xin − xin
(cid:90) x2 +2rl x y (rl )2y2 (rl )2y2
= ∞ x exp(− inj xin inj inj)exp( − xin inj )exp( xin inj )dx
inj 2σ2 (1 (rl )2) 2σ2 (1 (rl )2) 2σ2 (1 (rl )2) inj
−∞ xin − xin xin − xin xin − xin
(rl )2y2 (cid:90) (x rl y )2
=exp( xin inj ) ∞ x exp(− inj − xin inj )dx
2σ2 (1 (rl )2) inj 2σ2 (1 (rl )2) inj
xin − xin −∞ xin − xin
(cid:90) x (x rl y )2
= ∞ inj exp(− inj − xin inj )dx
−∞
√2πσ xin(cid:113) (1 −(r xl in)2) 2σ x2 in(1 −(r xl in)2) inj
(cid:113) (rl )2y2
=rl y √2πσ (1 (rl )2)exp( xin inj )
xin inj xin − xin 2σ2 (1 (rl )2)
xin − xin
I
=(cid:90) ∞
x
erf(x inj)exp(−x2 inj +2r xl inx injy inj)dx
X,2 inj √2 2σ2 (1 (rl )2) inj
−∞ xin − xin
From2.7.2.4ofKorotkov&Korotkov(2020),
(cid:90)
∞ zerf(a z)exp( az2+bz)dz =
1
−
−∞
√πb b2 a b a b2
1 1
= exp( )erf( )+ exp( )
2a√a 4a 2(cid:112) a2+aa2 a(cid:112) a+a2 4a+4a2
1 1 1
32TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Substitutinga = 1 ,a= 1 ,b= r xl inyinj ,weget
1 √2 2σ x2 in(1 −(r xl in)2) σ x2 in(1 −(r xl in)2)
√π r xl inyinj (r xl in)2y i2 nj r xl inyinj
σ2 (1 (rl )2) σ4 (1 (rl )2)2 √2σ2 (1 (rl )2)
I = xin − xin exp( xin − xin )erf( xin − xin )
X,2 2 2√2σ x3 in(11 −(r xl in)2)3 2 4 2σ x2 in(1 −1 (r xl in)2) 2(cid:113) 4σ x4 in(1 −1 (r xl in)2)2 + 4σ x2 in(1 −1 (r xl in)2)
(rl )2y2
1 xin inj
+
√2
exp(
σ x4 in(1 −(r xl in)2)2
)
1 (cid:113) 1 + 1 4 1 + 4
2σ x2 in(1 −(r xl in)2) 2σ x2 in(1 −(r xl in)2) 2 2σ x2 in(1 −(r xl in)2) 2
(cid:113) (rl )2y2 rl y
=rl y √2πσ (1 (rl )2)exp( xin inj )erf( xin inj )
xin inj xin − xin 2σ2 (1 (rl )2) (cid:113)
xin − xin 2(σ x2 in(1 −(r xl in)2)+1)
+
2σ x3 in(1 −(r xl in)2)23
exp(
(r xl in)2y i2
nj
)
(cid:113)
2(σ2 (1 (rl )2)+1)σ2 (1 (rl )2)
σ x2 in(1 −(r xl in)2)+1 xin − xin xin − xin
LetusdefineI andI as:
X,2,1 X,2,2
(cid:113) (rl )2y2 rl y
I =rl y √2πσ (1 (rl )2)exp( xin inj )erf( xin inj )
X,2,1 xin inj xin − xin 2σ2 (1 (rl )2) (cid:113)
xin − xin 2(σ x2 in(1 −(r xl in)2)+1)
I =
2σ x3 in(1 −(r xl in)2)3 2
exp(
(r xl in)2y i2 nj
)
X,2,2 (cid:113) 2(σ2 (1 (rl )2)+1)σ2 (1 (rl )2)
σ x2 in(1 −(r xl in)2)+1 xin − xin xin − xin
I
=(cid:90)
∞
y inj(1+erf(y √in 2j))
exp(
−y i2
nj
)I dy
(cid:113) 2σ2 (1 (rl )2) X inj
−∞ 8πσ x2 in (1 −(r xl in)2) xin − xin
=(cid:90)
∞
y inj(1+erf(y √in 2j))
exp(
−y i2
nj
)(I +I +I )dy
(cid:113) 2σ2 (1 (rl )2) X,1 X,2,1 X,2,2 inj
−∞ 8πσ x2 in (1 −(r xl in)2) xin − xin
I
=(cid:90)
∞
y inj(1+erf(y √in 2j))
exp(
−y i2
nj
)I dy
1 (cid:113) 2σ2 (1 (rl )2) X,1 inj
−∞ 8πσ x2 in (1 −(r xl in)2) xin − xin
I
=(cid:90)
∞
y inj(1+erf(y √in 2j))
exp(
−y i2
nj
)I dy
2 (cid:113) 2σ2 (1 (rl )2) X,2,1 inj
−∞ 8πσ x2 in (1 −(r xl in)2) xin − xin
I
=(cid:90)
∞
y inj(1+erf(y √in 2j))
exp(
−y i2
nj
)I dy
3 (cid:113) 2σ2 (1 (rl )2) X,2,2 inj
−∞ 8πσ x2 in (1 −(r xl in)2) xin − xin
WehaveI =I +I +I
1 2 3
I
=(cid:90)
∞
y inj(1+erf(y √in 2j))
exp(
−y i2
nj )rl y
1 (cid:113) 2σ2 (1 (rl )2) xin inj
−∞ 8πσ x2 in (1 −(r xl in)2) xin − xin
(cid:113) (rl )2y2
√2πσ (1 (rl )2)exp( xin inj )dy
xin − xin 2σ2 (1 (rl )2) inj
xin − xin
=
r xl
in
(cid:90)
∞
y i2 nj(1+erf(y √in 2j)) exp(−y i2
nj)dy
4 √2πσ2 2σ2 inj
−∞
xin xin
33TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
=
r xl
in
(cid:90)
∞
y i2
nj
exp(−y i2
nj)dy
+
r xl
in
(cid:90)
∞
y i2 njerf(y √in 2j) exp(−y i2
nj)dy
4 √2πσ2 2σ2 inj 4 √2πσ2 2σ2 inj
−∞
xin xin
−∞
xin xin
rl σ2
= xin xin (Definitionofvariance,andintegralofoddfunction)
4
I
=(cid:90)
∞
y inj(1+erf(y √in 2j))
exp(
−y i2
nj )rl y
2 (cid:113) 2σ2 (1 (rl )2) xin inj
−∞ 8πσ x2 in (1 −(r xl in)2) xin − xin
(cid:113) (rl )2y2 rl y
√2πσ (1 (rl )2)exp( xin inj )erf( xin inj )dy
xin − xin 2σ2 (1 (rl )2) (cid:113) inj
xin − xin 2(σ x2 in(1 −(r xl in)2)+1)
rl (cid:90) y y2 rl y
= xin ∞ y2 (1+erf( inj))exp(− inj)erf( xin inj )dy
4√2πσ
xin −∞
inj √2 2σ x2
in
(cid:113)
2(σ x2 in(1 −(r xl in)2)+1)
inj
rl (cid:90) y2 rl y
= xin ∞ y2 exp(− inj)erf( xin inj )dy
4√2πσ
xin −∞
inj 2σ x2
in
(cid:113)
2(σ x2 in(1 −(r xl in)2)+1)
inj
rl (cid:90) y y2 rl y
+ xin ∞ y2 erf( inj)exp(− inj)erf( xin inj )dy
4√2πσ
xin −∞
inj √2 2σ x2
in
(cid:113)
2(σ x2 in(1 −(r xl in)2)+1)
inj
rl (cid:90) y y2 rl y
= xin ∞ y2 erf( inj)exp(− inj)erf( xin inj )dy (IntegralofOddfunction)
4√2πσ
xin −∞
inj √2 2σ x2
in
(cid:113)
2(σ x2 in(1 −(r xl in)2)+1)
inj
From2.7.3.3ofKorotkov&Korotkov(2020),
(cid:90)
∞ z2exp( az2)erf(a z)erf(a z)=
1 2
−
−∞
1 1 a a a a (2a+a2+a2)
( tan 1( 1 2 )+ 1 2 1 2 )
− (cid:112) (cid:112)
√π a√a a2+aa2+aa2 a a+a2+a2(a2+aa2+aa2+a2a2)
1 2 1 2 1 2 1 2
Substitutinga= 1 ,a = 1 ,a = r xl in
2σ x2
in
1 √2 2 √2(σ x2 in(1 −(r xl in)2)+1)
rl
a a = xin
1 2 (cid:113)
2 (σ2 (1 (rl )2)+1)
xin − xin
1 1 (rl )2
a2+aa2+aa2 = + + xin
1 2 4σ4 4σ2 4σ2 (σ2 (1 (rl )2)+1)
xin xin xin xin − xin
σ2 (1 (rl )2)+1+σ4 (1 (rl )2)+σ2 +(rl )2σ2
= xin − xin xin − xin xin xin xin
4σ4 (σ2 (1 (rl )2)+1)
xin xin − xin
σ4 +2σ2 +1 (rl )2σ4 (σ2 +1)2 (rl σ2 )2
= xin xin − xin xin = xin − xin xin
4σ4 (σ2 (1 (rl )2)+1) 4σ4 (σ2 (1 (rl )2)+1)
xin xin − xin xin xin − xin
a2+aa2+aa2 (σ2 +1)2 (rl σ2 )2
a+a2+a2 = 1 2 = xin − xin xin 2σ2
1 2 a 4σ4 (σ2 (1 (rl )2)+1) ∗ xin
xin xin − xin
(σ2 +1)2 (rl σ2 )2
= xin − xin xin
2σ2 (σ2 (1 (rl )2)+1)
xin xin − xin
(σ2 +1)2 (rl σ2 )2 (rl )2
a2+aa2+aa2+a2a2 = xin − xin xin + xin
1 2 1 2 4σ4 (σ2 (1 (rl )2)+1) 4(σ2 (1 (rl )2)+1)
xin xin − xin xin − xin
(σ2 +1)2 (rl σ2 )2+(rl )2σ4 (σ2 +1)2
= xin − xin xin xin xin = xin
4σ4 (σ2 (1 (rl )2)+1) 4σ4 (σ2 (1 (rl )2)+1)
xin xin − xin xin xin − xin
34TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
1 (σ2 +1)2 (rl σ2 )2
2a+a2+a2 = + xin − xin xin
1 2 2σ2 2σ2 (σ2 (1 (rl )2)+1)
xin xin xin − xin
(σ2 +1)2 (rl σ2 )2+σ2 (1 (rl )2)+1
= xin − xin xin xin − xin
2σ2 (σ2 (1 (rl )2)+1)
xin xin − xin
(σ2 +1)2+σ2 +1 (rl σ2 )2 σ2 (rl )2
= xin xin − xin xin − xin xin
2σ2 (σ2 (1 (rl )2)+1)
xin xin − xin
(σ2 +1)(σ2 +2) (rl )2σ2 (σ2 +1)
= xin xin − xin xin xin
2σ2 (σ2 (1 (rl )2)+1)
xin xin − xin
(σ2 +1)(σ2 (1 (rl )2)+2)
= xin xin − xin
2σ2 (σ2 (1 (rl )2)+1)
xin xin − xin
rl
xin
I =
r xl
in (2√2σ3 tan 1(
2√(σ x2 in(1 −(r xl in)2)+1)
))
2 4√2πσ xin xin − (cid:114) (σ x2 in+1)2 −(r xl inσ x2 in)2
4σ4 (σ2 (1 (rl )2)+1)
xin xin − xin
rl (σ2 +1)(σ2 (1 (rl )2)+2)
xin xin xin − xin
+
r xl
in (
2√(σ x2 in(1 −(r xl in)2)+1) 2σ x2 in(σ x2 in(1 −(r xl in)2)+1)
)
(cid:114)
4√2πσ xin 1 (σ x2 in+1)2 −(r xl inσ x2 in)2 (σ x2 in+1)2
2σ2 2σ2 (σ2 (1 (rl )2)+1)4σ4 (σ2 (1 (rl )2)+1)
xin xin xin − xin xin xin − xin
rl rl σ2
= xin (2√2σ3 tan 1( xin xin ))
4√2πσ
xin
xin − (cid:113)
(σ x2
in
+1)2 −(r xl inσ x2 in)2
rl 2√2rl σ5 (σ2 (1 (rl )2)+2)
+ xin ( xin xin xin − xin )
(cid:113)
4√2πσ
xin (σ x2
in
+1) (σ x2
in
+1)2 −(r xl inσ x2 in)2
rl σ2 rl σ2 rl σ2 (σ2 (1 (rl )2)+2)
I
2
= x 2in πxin(sin−1( σ2xin +xin 1)+ xin xin (cid:113)xin − xin )
xin (σ x2
in
+1) (σ x2
in
+1)2 −(r xl inσ x2 in)2
I
=(cid:90)
∞
y inj(1+erf(y √in 2j))
exp(
−y i2
nj
)
3 (cid:113) 2σ2 (1 (rl )2)
−∞ 8πσ x2 in (1 −(r xl in)2) xin − xin
2σ x3 in(1 −(r xl in)2)23
exp(
(r xl in)2y i2
nj
)dy
(cid:113) 2(σ2 (1 (rl )2)+1)σ2 (1 (rl )2) inj
σ x2 in(1 −(r xl in)2)+1 xin − xin xin − xin
=(cid:90)
∞
σ xin(1 −(r xl in)2)y inj(1+erf(y √in 2j))
exp(
−y i2 nj(σ x2 in(1 −(r xl in)2)+1 −(r xl in)2)
)dy
(cid:113) 2(σ2 (1 (rl )2)+1)σ2 (1 (rl )2) inj
−∞ 4π σ x2 in(1 −(r xl in)2)+1 xin − xin xin − xin
=(cid:90)
∞
σ xin(1 −(r xl in)2)y inj(1+erf(y √in 2j))
exp(
−y i2 nj(σ x2
in
+1)(1 −(r xl in)2)
)dy
(cid:113) 2(σ2 (1 (rl )2)+1)σ2 (1 (rl )2) inj
−∞ 4π σ x2 in(1 −(r xl in)2)+1 xin − xin xin − xin
σ (1 (rl )2) (cid:90) y y2 (σ2 +1)
= xin − xin ∞ y (1+erf( inj))exp( − inj xin )dy
4π(cid:113)
σ x2 in(1 −(r xl in)2)+1
−∞
inj √2 2(σ x2 in(1 −(r xl in)2)+1)σ x2
in
inj
σ (1 (rl )2) (cid:90) y2 (σ2 +1)
= xin − xin ∞ y exp( − inj xin )dy
(cid:113) inj 2(σ2 (1 (rl )2)+1)σ2 inj
4π σ x2 in(1 −(r xl in)2)+1 −∞ xin − xin xin
35TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
σ (1 (rl )2) (cid:90) y y2 (σ2 +1)
+ xin − xin ∞ y erf( inj)exp( − inj xin )dy
4π(cid:113)
σ x2 in(1 −(r xl in)2)+1
−∞
inj √2 2(σ x2 in(1 −(r xl in)2)+1)σ x2
in
inj
σ (1 (rl )2) (cid:90) y y2 (σ2 +1)
= xin − xin ∞ y erf( inj)exp( − inj xin )dy (IntegralofOddfunction)
4π(cid:113)
σ x2 in(1 −(r xl in)2)+1
−∞
inj √2 2(σ x2 in(1 −(r xl in)2)+1)σ x2
in
inj
(cid:90)
From2.6.1.4ofKorotkov&Korotkov(2020), ∞ zerf(az)exp( a z2)dz = a
1
− a √a2+a
1 1
−∞
1 (σ2 +1)
Substituting,a= ,a = xin ,wehave
√2 1 2σ2 (σ2 (1 (rl )2)+1)
xin xin − xin
σ (1 (rl )2) 1
I = xin − xin ( √2 )
3 (cid:113) (cid:114)
4π σ x2 in(1 −(r xl in)2)+1
2σ2
(σ2(σ (x2 1in+ (r1 l)
)2)+1)
1
2
+
2σ2
(σ2(σ (x2 1in+ (r1 l)
)2)+1)
xin xin − xin xin xin − xin
=
σ xin(1 −(r xl in)2) 2σ x3 in(σ x2 in(1 −(r xl in)2)+1)3 2
(cid:113) (cid:113)
4π σ2 (1 (rl )2)+1(σ2 +1) σ4 (1 (rl )2)+σ2 +σ2 +1
xin − xin xin xin − xin xin xin
σ4 (σ2 (1 (rl )2)+1)(1 (rl )2)
I = xin xin − xin − xin
3 (cid:113)
2π(σ2 +1) (σ2 +1)2 (rl σ2 )2
xin xin − xin xin
Finallywehave,
I =I +I +I
1 2 3
rl σ2 rl σ2 rl σ2 rl σ2 (σ2 (1 (rl )2)+2)
= xin xin + xin xin(sin−1( xin xin )+ xin xin (cid:113)xin − xin )
4 2π σ2 +1
xin (σ x2
in
+1) (σ x2
in
+1)2 −(r xl inσ x2 in)2
σ4 (σ2 (1 (rl )2)+1)(1 (rl )2)
+ xin xin − xin − xin
(cid:113)
2π(σ2 +1) (σ2 +1)2 (rl σ2 )2
xin xin − xin xin
rl σ2 rl σ2 rl σ2 σ4 (σ2 (1 (rl )2)+1+(rl )2)
I = xin xin + xin xin sin−1( xin xin )+ xin xin (cid:113)− xin xin
4 2π σ2 +1
xin 2π(σ x2
in
+1) (σ x2
in
+1)2 −(r xl inσ x2 in)2
 
σ2 2rl rl σ2 2σ2 (σ2 (1 (rl )2)+1+(rl )2)
I = 4xin r xl
in
+ πxin sin−1( σ2xin +xin 1)+ xin xin (cid:113)− xin xin 
xin π(σ x2
in
+1) (σ x2
in
+1)2 −(r xl inσ x2 in)2
Wehave,
Cov(x ,y )=I E[x ]E[y ]
outj outj
−
outj outj
σ4
Cov(x ,y )=I xin
outj outj − 2π(σ2 +1)
xin
σ2 rl σ2
Cov(x outj,y outj)= 4x πin(πr xl
in
+2r xl insin−1( σ2xin +xin 1)
xin
2σ2 (σ2 (1 (rl )2)+1+(rl )2) 2σ2
+ xin xin − xin xin xin )
(cid:113)
− (σ2 +1)
(σ x2
in
+1) (σ x2
in
+1)2 −(r xl inσ x2 in)2 xin
36TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
ThebackwardpassthroughGeLUisdefinedas,
x x2
g =(Φ(x )+ ini exp(− ini))g
ini ini
√2π 2
outi
1 x x x2
=( (1+erf( ini))+ ini exp(− ini))g
2 √2 √2π 2
outi
Sothemeanofgradientisobtainedasfollowing,
1 x x x2
E[g ]=E[( (1+erf( ini))+ ini exp(− ini))g ]
ini
2 √2 √2π 2
outi
1 x x x2
=E[( (1+erf( ini))+ ini exp(− ini))]E[g ]=0
2 √2 √2π 2
outi
µ =0
gin
Similarlyforvariance,
1 x x x2
E[g2 ]=E[( (1+erf( ini))+ ini exp(− ini))2g2 ]
ini 2 √2 √2π 2 outi
1 x x x2
=E[( (1+erf( ini))+ ini exp(− ini))2]E[g2 ]
2 √2 √2π 2 outi
1 x x x2
=E[( (1+erf( ini))+ ini exp(− ini))2]σ2
2 √2 √2π 2 gout
1 x x x2
I =E[( (1+erf( ini))+ ini exp(− ini))2]
2 √2 √2π 2
x2
=(cid:90) ∞ (1 (1+erf(x ini))+ x ini exp(−x2 ini))2exp(− 2σ x2in ini)
dx
2 √2 √2π 2 √2πσ
ini
−∞
xin
I
=(cid:90) ∞ (1
+
erf2(x √in 2i)
+
x2 iniexp( −x2 ini)
+
erf(x √in 2i)
+
4 4 2π 2
−∞
x iniexp(−x 22 ini)
+
x iniexp(−x 22 ini)erf(x √in 2i) )exp(− 2σx x22 in ini)
dx
√2π √2π √2πσ
ini
xin
x2
(cid:90) exp(− ini)
I =
∞ 1 2σ x2
in dx
1
4 √2πσ
ini
−∞
xin
1
I =
1
4
I
=(cid:90)
∞
erf2(x √in 2i)exp(− 2σx x22 in ini)
dx
2
4 √2πσ
ini
−∞
xin
= 1 (cid:90) ∞ erf2(x ini)exp(−x2 ini)dx
4√2πσ √2 2σ2 ini
xin
−∞
xin
From2.7.1.3ofKorotkov&Korotkov(2020),
(cid:90)
∞ erf(a z)erf(a z)exp( az2)dz = 2 tan 1( a 1a 2 )
1 2 − (cid:112)
− √πa a2+aa2+aa2
−∞ 1 2
37TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Substitutinga= 1 ,a =a = 1
2σ x2
in
1 2 √2
1 2 1
I = tan 1( 2 )
2 (cid:113) − (cid:113)
4√2πσ xin π 2σ1 2 4σ1 4 + 4σ1 2 + 4σ1 2
xin xin xin xin
1 σ2 1 σ2
= tan 1( xin )= tan 1( xin )
− (cid:112) − (cid:112)
2π 2σ2 +1 2π (σ2 +1)2 σ4
xin xin − xin
1 σ2
I
2
=
2π
sin−1(
σ2
x +in 1)
xin
x2
I
=(cid:90) ∞ x2 iniexp( −x2 ini)exp(− 2σ x2in ini)
dx
3
2π √2πσ
ini
−∞
xin
= 1 (cid:90) ∞ x2 ini exp(−x2 ini(2σ x2 in +1) )dx
2πσ
xin
−∞
√2π 2σ x2
in
ini
= 1 σ xin (cid:90) ∞ x2 ini exp(−x2 ini(2σ x2 in +1) )dx
2πσ xin (cid:112) (2σ x2
in
+1)
−∞
√2π √(2σ σx x2i in
n+1)
2σ x2
in
ini
1 σ σ2
= xin xin (Definitionofvariance)
(cid:112)
2πσ xin (2σ x2
in
+1)(2σ x2
in
+1)
σ2
I = xin
3 2π(2σ x2
in
+1)3 2
I
=(cid:90)
∞
erf(x √in 2i)exp(− 2σx x22 in ini)
dx =0 (Integralofoddfunction)
4
2 √2πσ
ini
−∞
xin
I
=(cid:90)
∞
x iniexp(−x 22 ini)exp(− 2σx x22 in ini)
dx =0 (Integralofoddfunction)
5
√2π √2πσ
ini
−∞
xin
I
=(cid:90)
∞
x iniexp(−x 22 ini)erf(x √in 2i)exp(− 2σx x22 in ini)
dx
6
√2π √2πσ
ini
−∞
xin
=
1 (cid:90) ∞
x
erf(x ini)exp(−x2 ini(σ x2
in
+1)
)dx
2πσ
xin
−∞
ini √2 2σ x2
in
ini
From2.6.1.4ofKorotkov&Korotkov(2020),(cid:82) −∞ ∞zerf(az)exp( −a 1z2)dz = a1√aa
2+a1
Substituting,a= 1 ,a = (σ x2 in+1) ,wehave
√2 1 2σ x2
in
1
1
I =
√2
6 (cid:114)
2πσ xin (σ x2 in+1) 1 + (σ x2 in+1)
2σ2 2 2σ2
xin xin
1 2σ3
= xin
(cid:112)
2πσ xin (σ x2
in
+1) 2σ x2
in
+1
σ2
I = xin
6 (cid:112)
π(σ2 +1) 2σ2 +1
xin xin
I =I +I +I +I +I +I
1 2 3 4 5 6
1 1 σ2 σ2 σ2
=
4
+
2π
sin−1(
σ x2
inx +in 1)+
2π(2σ
x2x inin
+1)3 2
+
π(σ x2
in
+1)x (cid:112)in
2σ x2
in
+1
38TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
1 1 σ2 σ2 (4σ2 +2+σ2 +1)
=
4
+
2π
sin−1(
σ x2
inx +in 1)+ 2x πin
(σ x2
inx +in
1)(2σ x2
inx +in
1)3 2
1 1 σ2 σ2 (5σ2 +3)
I =
4
+
2π
sin−1(
σ x2
inx +in 1)+
2π(σ x2
inxi +n 1)x (2in
σ x2
in
+1)3 2
SothevarianceofgradientofinputofGeLUcomesouttobe
E[g2 ]=Iσ2
ini gout
(cid:34) (cid:35)
1 1 σ2 σ2 (5σ2 +3)
σ g2
in
=
4
+
2π
sin−1(
σ x2
inx +in 1)+
2π(σ x2
inxi +n 1)x (2in
σ x2
in
+1)3 2
σ g2
out
Iffortwoinputsx andy foralliwehaveCorr(g ,g )=rl ,andg ,g bethegradientafterpassingthrough
in in outxi outyi gout inxi inyi
GeLUlayer. Thenwehave,
E[g g ]=
inxi inyi
1 x x x2 1 y y y2
=E[( (1+erf( ini))+ ini exp(− ini))g ( (1+erf( ini))+ ini exp(− ini))g ]
2 √2 √2π 2
outxi
2 √2 √2π 2
outyi
1 x
E[g g ]=E[( (1+erf( ini))+
inxi inyi
2 √2
x x2 1 y y y2
ini exp(− ini))( (1+erf( ini))+ ini exp(− ini))]E[g g ]
√2π 2 2 √2 √2π 2
outxi outyi
1 x
=E[( (1+erf( ini))+
2 √2
x x2 1 y y y2
ini exp(− ini))( (1+erf( ini))+ ini exp(− ini))]rl σ2
√2π 2 2 √2 √2π 2 gout gout
1 x
I =E[( (1+erf( ini))+
2 √2
x x2 1 y y y2
ini exp(− ini))( (1+erf( ini))+ ini exp(− ini))]
√2π 2 2 √2 √2π 2
(cid:90)
=
∞ (1 (1+erf(x ini))+
2 √2
−∞
x x2 1 y y y2
ini exp(− ini))( (1+erf( ini))+ ini exp(− ini))p dx dy
√2π 2 2 √2 √2π 2
xini,yini ini ini
Wherep = 1 exp(−x2 ini +2r xl inx iniy ini −y i2 ni)
xini,yini
2πσ x2
in(cid:113)
(1 −(r xl in)2)
2σ x2 in(1 −(r xl in)2)
(cid:90) (1(1+erf(yini))+ yini exp(−y i2 ni)) y2
I = ∞ 2 √2 √2π 2 exp( − ini )I dy
(cid:113) 2σ2 (1 (rl )2) X ini
−∞ 2πσ x2 in (1 −(r xl in)2) xin − xin
Where,
I =(cid:90) ∞ (1 (1+erf(x ini))+ x ini exp(−x2 ini))exp(−x2 ini +2r xl inx iniy ini)dx
X 2 √2 √2π 2 2σ2 (1 (rl )2) ini
−∞ xin − xin
I =(cid:90) ∞ 1 exp(−x2 ini +2r xl inx iniy ini)dx
X,1 2 2σ2 (1 (rl )2) ini
−∞ xin − xin
39TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
=
1(cid:90)
∞
exp(−x2
ini
+2r xl inx iniy
ini)exp(
−(r xl in)2y i2
ni )exp(
(r xl in)2y i2
ni )dx
2 2σ2 (1 (rl )2) 2σ2 (1 (rl )2) 2σ2 (1 (rl )2) ini
−∞ xin − xin xin − xin xin − xin
= 1 exp( (r xl in)2y i2 ni )(cid:90) ∞ exp(−(x ini −r xl iny ini)2 )dx
2 2σ2 (1 (rl )2) 2σ2 (1 (rl )2) ini
xin − xin −∞ xin − xin
= 1 exp( (r xl in)2y i2 ni )√2πσ (cid:113) (1 (rl )2)(cid:90) ∞ exp(− 2σ(x x2 ii nni (− 1 −r xl (rin xly inin )i 2) )2 ) dx
2 2σ x2 in(1 −(r xl in)2) xin − xin
−∞
√2πσ xin(cid:113) (1 −(r xl in)2) ini
(cid:113)
I =
√2πσ xin (1 −(r xl in)2)
exp(
(r xl in)2y i2
ni )
X,1 2 2σ2 (1 (rl )2)
xin − xin
(cid:90) erf(xini) x2 +2rl x y
I = ∞ √2 exp(− ini xin ini ini)dx
X,2 2 2σ2 (1 (rl )2) ini
−∞ xin − xin
= 1(cid:90) ∞ erf(x ini)exp(−x2 ini +2r xl inx iniy ini)dx
2 √2 2σ2 (1 (rl )2) ini
−∞ xin − xin
From2.7.1.6ofKorotkov&Korotkov(2020),
(cid:90) ∞ erf(a z)exp( az2+bz)dz =(cid:114) π exp(b2 )erf( a 1b )
1 (cid:112)
− a 4a 2 a2+aa2
−∞ 1
Substitutinga = 1 ,a= 1 ,b= r xl inyini
1 √2 2σ x2 in(1 −(r xl in)2) σ x2 in(1 −(r xl in)2)
(cid:115)
(r xl in)2y i2
ni
r xl inyini
1 π σ4 (1 (rl )2)2 √2σ2 (1 (rl )2)
I = exp( xin − xin )erf( xin − xin )
X,2 2 1 4 1 2(cid:113) 1 + 1
2σ x2 in(1 −(r xl in)2) 2σ x2 in(1 −(r xl in)2) 4σ x4 in(1 −(r xl in)2)2 4σ x2 in(1 −(r xl in)2)
(cid:113)
I = √2πσ xin (1 −(r xl in)2) exp( (r xl in)2y i2 ni )erf( r xl iny ini )
X,2 2 2σ2 (1 (rl )2) (cid:113)
xin − xin 2(σ x2 in(1 −(r xl in)2)+1)
I =(cid:90) ∞ x ini exp(−x2 ini)exp(−x2 ini +2r xl inx iniy ini)dx
X,3 √2π 2 2σ2 (1 (rl )2) ini
−∞ xin − xin
=(cid:90) ∞ x ini exp(−x2 ini(σ x2 in(1 −(r xl in)2)+1)+2r xl inx iniy ini)dx
√2π 2σ2 (1 (rl )2) ini
−∞ xin − xin
(cid:90) x2 +
2r xl inxiniyini
=
∞ x ini exp(− ini (σ x2 in(1 −(r xl in)2)+1)
)dx
√2π 2σ x2 in(1 −(r xl in)2) ini
−∞ (σ2 (1 (rl )2)+1)
xin − xin
(cid:90) x2 +
2r xl inxiniyini −(r xl in)2y i2
ni
=
∞ x ini exp(− ini (σ x2 in(1 −(r xl in)2)+1) )exp((σ x2 in(1 −(r xl in)2)+1)2
)
√2π 2σ x2 in(1 −(r xl in)2) 2σ x2 in(1 −(r xl in)2) ∗
−∞ (σ2 (1 (rl )2)+1) (σ2 (1 (rl )2)+1)
xin − xin xin − xin
(rl )2y2
xin ini
(σ2 (1 (rl )2)+1)2
exp( xin − xin )dx
2σ2 (1 (rl )2) ini
xin − xin
(σ2 (1 (rl )2)+1)
xin − xin
(rl )2y2
=exp( xin ini )
2σ2 (1 (rl )2)(σ2 (1 (rl )2)+1) ∗
xin − xin xin − xin
40TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
(cid:90) ∞ x ini exp(−(x ini − (σ x2 in(1r −xl (in ry xlin ini )2)+1))2
)dx
√2π 2σ x2 in(1 −(r xl in)2) ini
−∞ (σ2 (1 (rl )2)+1)
xin − xin
(cid:113)
=exp(
(r xl in)2y i2
ni )
σ xin 1 −(r xl in)2
(cid:113)
2σ2 (1 (rl )2)(σ2 (1 (rl )2)+1)
xin − xin xin − xin (σ x2 in(1 −(r xl in)2)+1)
(cid:90) ∞ x ini exp(−(x ini − (σ x2 in(1r −xl (in ry xlin ini )2)+1))2
)dx
√2π
σxin√1 −(r xl in)2 2σ x2 in(1 −(r xl in)2) ini
−∞ √(σ x2 in(1 −(r xl in)2)+1) (σ x2 in(1 −(r xl in)2)+1)
(rl )2y2
=exp( xin ini ).
2σ2 (1 (rl )2)(σ2 (1 (rl )2)+1)
xin − xin xin − xin
(cid:113)
σ xin 1 −(r xl in)2 r xl iny ini
(cid:113)
(σ2 (1 (rl )2)+1)
(σ x2 in(1 −(r xl in)2)+1) xin − xin
(cid:113)
I =
r xl iny iniσ xin 1 −(r xl in)2
exp(
(r xl in)2y i2
ni )
X,3 (σ x2 in(1 −(r xl in)2)+1)3 2 2σ x2 in(1 −(r xl in)2)(σ x2 in(1 −(r xl in)2)+1)
I =
(cid:90) (1(1+erf(yini))+ yini exp(−y i2 ni)) y2
∞ 2 √2 √2π 2 exp( − ini )(I +I +I )dy
(cid:113) 2σ2 (1 (rl )2) X,1 X,2 X,3 ini
−∞ 2πσ x2 in (1 −(r xl in)2) xin − xin
(cid:90) (1(1+erf(yini))+ yini exp(−y i2 ni)) y2
I = ∞ 2 √2 √2π 2 exp( − ini )I dy
1 (cid:113) 2σ2 (1 (rl )2) X,1 ini
−∞ 2πσ x2 in (1 −(r xl in)2) xin − xin
(cid:90) (1(1+erf(yini))+ yini exp(−y i2 ni)) y2
= ∞ 2 √2 √2π 2 exp( − ini )
(cid:113)
2σ2 (1 (rl )2)
−∞ 2πσ x2 in (1 −(r xl in)2) xin − xin
(cid:113)
√2πσ xin (1 −(r xl in)2)
exp(
(r xl in)2y i2
ni )dy
2 2σ2 (1 (rl )2) ini
xin − xin
=
1(cid:90)
∞
(1 2(1+erf(y √in 2i))+ √yi 2ni
π
exp(−y 2i2 ni)) exp(−y i2
ni)dy
2 √2πσ 2σ2 ini
−∞
xin xin
I = 1(cid:90) ∞ 1 exp(−y i2 ni)dy = 1
1,1 4 √2πσ 2σ2 ini 4
−∞
xin xin
I =
1(cid:90)
∞
erf(y √in 2i) exp(−y i2
ni)dy =0 (Integralofoddfunction)
1,2 4 √2πσ 2σ2 ini
−∞
xin xin
I =
1(cid:90)
∞
y iniexp(−y 2i2 ni) exp(−y i2
ni)dy =0 (Integralofoddfunction)
1,3 2 2πσ 2σ2 ini
−∞
xin xin
41TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
(cid:90) (1(1+erf(yini))+ yini exp(−y i2 ni)) y2
I = ∞ 2 √2 √2π 2 exp( − ini )I dy
2 (cid:113) 2σ2 (1 (rl )2) X,2 ini
−∞ 2πσ x2 in (1 −(r xl in)2) xin − xin
(cid:90) (1(1+erf(yini))+ yini exp(−y i2 ni)) y2
= ∞ 2 √2 √2π 2 exp( − ini )
(cid:113)
2σ2 (1 (rl )2)
−∞ 2πσ x2 in (1 −(r xl in)2) xin − xin
(cid:113)
√2πσ xin (1 −(r xl in)2) exp( (r xl in)2y i2 ni )erf( r xl iny ini )dy
2 2σ2 (1 (rl )2) (cid:113) ini
xin − xin 2(σ x2 in(1 −(r xl in)2)+1)
=
1(cid:90)
∞
(1 2(1+erf(y √in 2i))+ √yi 2ni
π
exp(−y 2i2 ni)) exp(−y i2
ni).
2 √2πσ 2σ2
−∞
xin xin
rl y
erf( xin ini )dy
(cid:113) ini
2(σ2 (1 (rl )2)+1)
xin − xin
I = 1(cid:90) ∞ 1 exp(−y i2 ni)erf( r xl iny ini )dy =0 (Integralofoddfunction)
2,1 4
−∞
√2πσ
xin
2σ x2
in
(cid:113)
2(σ x2 in(1 −(r xl in)2)+1)
ini
I = 1 (cid:90) ∞ erf(y ini)exp(−y i2 ni)erf( r xl iny ini )dy
2,2 4√2πσ
xin −∞
√2 2σ x2
in
(cid:113)
2(σ x2 in(1 −(r xl in)2)+1)
ini
From2.7.1.3ofKorotkov&Korotkov(2020),
(cid:82) −∞ ∞erf(a 1z)erf(a 2z)exp( −az2)dz = √2 πatan −1( √a2+a a1a a2
2 1+aa2
2)
Substitutinga= 1 ,a = 1 ,a = r xl in
2σ x2
in
1 √2 2 √2(σ x2 in(1 −(r xl in)2)+1)
rl
xin
1 2 2√(σ2 (1 (rl )2)+1)
I = tan 1( xin − xin )
2,2 (cid:113) − (cid:114)
4√2πσ xin π 2σ1 2 1 + 1 + (r xl in)2
xin 4σ4 4σ2 4σ2 (σ2 (1 (rl )2)+1)
xin xin xin xin − xin
1 rl σ2 1 rl σ2
I = tan 1( xin xin )= tan 1( xin xin )
2,2 − (cid:113) − (cid:113)
2π 2π
σ4 +2σ2 +1 (rl )2σ4 (σ2 +1)2 (rl σ2 )2
xin xin − xin xin xin − xin xin
1 rl σ2
I
2,2
=
2π
sin−1( σ2xin +xin 1)
xin
I = 1 (cid:90) ∞ y exp(−y i2 ni)exp(−y i2 ni)erf( r xl iny ini )dy
2,3 4πσ ini 2 2σ2 (cid:113) ini
xin
−∞
xin 2(σ x2 in(1 −(r xl in)2)+1)
=
1 (cid:90) ∞
y
exp(−y i2 ni(σ x2
in
+1)
)erf(
r xl iny ini
)dy
4πσ ini 2σ2 (cid:113) ini
xin
−∞
xin 2(σ x2 in(1 −(r xl in)2)+1)
From2.6.1.4ofKorotkov&Korotkov(2020),(cid:82) −∞ ∞zerf(az)exp( −a 1z2)dz = a1√aa
2+a1
rl (σ2 +1)
Substituting,a= xin ,a = xin ,wehave
√2(σ x2 in(1 −(r xl in)2)+1) 1 2σ x2
in
rl
xin
1 √2(σ2 (1 (rl )2)+1)
I = xin − xin
2,3 (cid:114)
4πσ xin (σ x2 in+1) (r xl in)2
+
(σ x2 in+1)
2σ2 2(σ2 (1 (rl )2)+1) 2σ2
xin xin − xin xin
42TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
rl σ2
= xin xin
(cid:113)
2π(σ2 +1) σ4 +2σ2 +1 (rl )2σ4
xin xin xin − xin xin
rl σ2
I = xin xin
2,3 (cid:113)
2π(σ2 +1) (σ2 +1)2 (rl σ2 )2
xin xin − xin xin
(cid:90) (1(1+erf(yini))+ yini exp(−y i2 ni)) y2
I = ∞ 2 √2 √2π 2 exp( − ini )I dy
3 (cid:113) 2σ2 (1 (rl )2) X,3 ini
−∞ 2πσ x2 in (1 −(r xl in)2) xin − xin
(cid:90) (1(1+erf(yini))+ yini exp(−y i2 ni)) y2
= ∞ 2 √2 √2π 2 exp( − ini )
(cid:113)
2σ2 (1 (rl )2)
−∞ 2πσ x2 in (1 −(r xl in)2) xin − xin
(cid:113)
r xl iny iniσ xin 1 −(r xl in)2
exp(
(r xl in)2y i2
ni )dy
(σ x2 in(1 −(r xl in)2)+1)3 2 2σ x2 in(1 −(r xl in)2)(σ x2 in(1 −(r xl in)2)+1) ini
= r xl in (cid:90) ∞ y (1 (1+erf(y ini))+ y ini exp(−y i2 ni))
2πσ xin(σ x2 in(1 −(r xl in)2)+1)3 2
−∞
ini 2 √2 √2π 2
y2 (rl )2y2
exp( − ini )exp( xin ini )dy
2σ2 (1 (rl )2) 2σ2 (1 (rl )2)(σ2 (1 (rl )2)+1) ini
xin − xin xin − xin xin − xin
= r xl in (cid:90) ∞ y (1 (1+erf(y ini))+ y ini exp(−y i2 ni))
2πσ xin(σ x2 in(1 −(r xl in)2)+1)3 2
−∞
ini 2 √2 √2π 2
y2 (σ2 (1 (rl )2)+1 (rl )2)
exp( − ini xin − xin − xin )dy
2(σ2 (1 (rl )2)+1)σ2 (1 (rl )2) ini
xin − xin xin − xin
= r xl in (cid:90) ∞ y (1 (1+erf(y ini))+ y ini exp(−y i2 ni))
2πσ xin(σ x2 in(1 −(r xl in)2)+1)3 2
−∞
ini 2 √2 √2π 2
y2 (σ2 +1)
exp( − ini xin )dy
2σ2 (σ2 (1 (rl )2)+1) ini
xin xin − xin
rl (cid:90) y2 (σ2 +1)
I = xin ∞ y exp( − ini xin )dy =0 (Integralofoddfunction)
3,1 4πσ xin(σ x2 in(1 −(r xl in)2)+1)3 2
−∞
ini 2σ x2 in(σ x2 in(1 −(r xl in)2)+1) ini
I = r xl in (cid:90) ∞ y erf(y ini)exp( −y i2 ni(σ x2 in +1) )dy
3,2 4πσ xin(σ x2 in(1 −(r xl in)2)+1)3 2
−∞
ini √2 2σ x2 in(σ x2 in(1 −(r xl in)2)+1) ini
From2.6.1.4ofKorotkov&Korotkov(2020),(cid:82) −∞ ∞zerf(az)exp( −a 1z2)dz = a1√aa
2+a1
Substituting,a= 1 ,a = (σ x2 in+1) ,wehave
√2 1 2σ x2 in(σ x2 in(1 −(r xl in)2)+1)
rl 1
I = xin √2
3,2 4πσ xin(σ x2 in(1 −(r xl in)2)+1)3 2 (σ x2 in+1) (cid:114) 1 + (σ x2 in+1)
2σ2 (σ2 (1 (rl )2)+1) 2 2σ2 (σ2 (1 (rl )2)+1)
xin xin − xin xin xin − xin
rl σ2
= xin xin
(cid:113)
2π(σ2 +1) σ4 +2σ2 +1 (rl )2σ4
xin xin xin − xin xin
rl σ2
I = xin xin
3,2 (cid:113)
2π(σ2 +1) (σ2 +1)2 (rl σ2 )2
xin xin − xin xin
rl
I = xin .
3,3 2πσ xin(σ x2 in(1 −(r xl in)2)+1)3 2
43TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
(cid:90) y2 y2 y2 (σ2 +1)
∞ ini exp(− ini)exp( − ini xin )dy
√2π 2 2σ2 (σ2 (1 (rl )2)+1) ini
−∞ xin xin − xin
rl
= xin .
2πσ xin(σ x2 in(1 −(r xl in)2)+1)3 2
(cid:90) y2 y2 (σ4 +2σ2 +1 (rl )2σ4 )
∞ ini exp(− ini xin xin − xin xin )dy
√2π 2σ2 (σ2 (1 (rl )2)+1) ini
−∞ xin xin − xin
rl (cid:90) y2 y2 ((σ2 +1)2 (rl σ2 )2)
= xin ∞ ini exp(− ini xin − xin xin )dy
2πσ xin(σ x2 in(1 −(r xl in)2)+1)3 2
−∞
√2π 2σ x2 in(σ x2 in(1 −(r xl in)2)+1) ini
(cid:113)
=
r xl
in
σ xin (σ x2 in(1 −(r xl in)2)+1)
2πσ xin(σ x2 in(1 −(r xl in)2)+1)3 2 (cid:113) (σ x2
in
+1)2 −(r xl inσ x2 in)2
(cid:90) y2 y2 ((σ2 +1)2 (rl σ2 )2)
∞ ini exp(− ini xin − xin xin )dy
−∞
√2πσ √xin (√ σ2(σ +x2 i 1n )( 21 − (( rr lxl in σ)2 2)+ )1 2) 2σ x2 in(σ x2 in(1 −(r xl in)2)+1) ini
xin − xin xin
=
r xl
in
σ x3 in(σ x2 in(1 −(r xl in)2)+1)3 2
2πσ xin(σ x2 in(1 −(r xl in)2)+1)3 2 ((σ x2
in
+1)2 −(r xl inσ x2 in)2)3 2
rl σ2
I = xin xin
3,3 2π((σ x2
in
+1)2 −(r xl inσ x2 in)2)3 2
I =I +I +I
1 2 3
=I +I +I +I +I +I +I +I +I
1,1 1,2 1,3 2,1 2,2 2,3 3,1 3,2 3,3
1 1 rl σ2
I = + sin−1( xin xin )+
4 2π σ2 +1
xin
2rl σ2 rl σ2
xin xin + xin xin
2π(σ x2
in
+1)(cid:113) (σ x2
in
+1)2 −(r xl inσ x2 in)2 2π((σ x2
in
+1)2 −(r xl inσ x2 in)2)23
1 1 rl σ2 rl σ2 ((2σ2 +3)(σ2 +1) 2(rl σ2 )2)
I =
4
+
2π
sin−1(
σ
x2x ii nn +xin 1)+ x 2in πx (σin
x2
in
+x 1in
)((σ x2
in
+xin
1)2
−−
(r xl
inσx x2i in n)2x )in
3 2
WedefinedCov(g ,g ),as
inxi inyi
Cov(g ,g )=Irl σ2
inxi inyi gout gout
Cov(g ,g )=
inxi inyi
(cid:34) (cid:35)
1 1 rl σ2 rl σ2 ((2σ2 +3)(σ2 +1) 2(rl σ2 )2)
4
+
2π
sin−1(
σ
x2x ii nn +xin 1)+ x 2in πx (σin
x2
in
+x 1in
)((σ x2
in
+xin
1)2
−−
(r xl
inσx x2i in n)2x )in
3 2
r gl outσ g2
out
B.6 LayerNorm
Theaffinetransformationforlayernormaretypicallyinitializedwith1scaleand0bias,sotheydonotchangeanyofour
derivationsbelowandareignoredhenceforth. Foraninputx theforwardpassofLayerNormis,
in
x =LayerNorm(x )
out in
x x¯
= x =
ini
−
in
⇒
outi
σˆ
xin
44TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Where
(cid:80)din
x
x¯ = i=1 ini
in
d
in
(cid:115)
(cid:80)din
(x x¯ )2
σˆ = i=1 ini − in
xin
d
in
TogetexpectationofoutputofLayerNorm,
x x¯
E[x ]=E[ ini − in ]
outi
σˆ
xin
(cid:88)din
E[x
]=(cid:88)din
E[x
ini
−x¯
in ]
outi
σˆ
i=1 i=1
xin
=E[(cid:88)din
x
ini
−x¯
in
]
σˆ
i=1
xin
(cid:80)din
(x x¯ )
=E[ i=1 ini − in ]
σˆ
xin
(cid:88)din
E[x ]=0
outi
i=1
Bysymmetryforanyi,j andi=j wehaveE[x ]=E[x ]=µ
̸
outi outj xout
= d µ =0
⇒
in xout
µ =0
xout
Similarlywecalculatevarianceofoutputby,
Var(x )=E[x2 ] E[x ]2 =E[x2 ]
outi outi − outi outi
(x x¯ )2
E[x2 ]=E[ ini − in ]
outi σˆ2
xin
(cid:88)din
E[x2
]=(cid:88)din E[(x
ini
−x¯ in)2
]
outi σˆ2
i=1 i=1 xin
=E[(cid:88)din (x
ini
−x¯ in)2
]
σˆ2
i=1 xin
(cid:80)din (x x¯ )2
=E[ i=1 ini − in ]
σˆ2
xin
(cid:88)din
E[x2 ]=d
outi in
i=1
Bysymmetryforanyi,j andi=j wehaveE[x2 ]=E[x2 ]=σ2
̸ outi outj xout
= d σ2 =d
⇒ in xout in
σ2 =1
xout
a.s
Nowwehaveσˆ σ forlarged . Soforlargevaluesofd wecantreatσˆ asaconstantwhichhasvalueσ . We
xin
−→
xin in in xin xin
usethisapproximationtogetthefollowingresults. Fortwoinputsx andy suchthatforalli,Corr(x ,y )=rl . For
in in ini ini xin
45TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
allj wehave,
E[x y ] E[x ]E[y ]
Corr(x ,y )=
outj outj
−
outj outj
outj outj (cid:112)
Var(x )Var(y )
outj outj
E[x y ] µ µ
=
outj outj
−
xout xout
(cid:112)
σ2 σ2
xout xout
E[x y ] 0
=
outj outj
−
√1
=E[x y ]
outj outj
(x x¯ )(y y¯ )
=E[ inj − in inj − in ]
σˆ σˆ
xin yin
(x x¯ )(y y¯ )
E[ inj − in inj − in ]
≈ σ σ
xin xin
E[(x x¯ )(y y¯ )]
=
inj
−
in inj
−
in
σ2
xin
E[(x (cid:80)d ki =n 1xink)(y (cid:80)d l=in 1yinl)]
= inj − din inj − din
σ2
xin
E[x y y (cid:80)d ki =n 1xink x (cid:80)d l=in 1yinl + (cid:80)d ki =n 1xink (cid:80)d l=in 1yinl]
= inj inj − inj din − inj din din din
σ2
xin
Elementsbelongingtodifferentdimensionsfromx andy areindependentofeachotherandhencefori,j andi=j we
in in
haveE[x y ]=µ2 . ̸
ini inj xin
E[x y ] E[y (cid:80)d ki =n 1xink] E[x (cid:80)d l=in 1yinl]+E[(cid:80)d ki =n 1xink (cid:80)d l=in 1yinl]
= inj inj − inj din − inj din din din
σ2
xin
rl σ2 +µ2
r xl inσ x2 in+dinµ2
xin
r xl inσ x2 in+dinµ2
xin +
r xl indinσ x2 in+d2 inµ2
xin
= xin xin xin − din − din d2 in
σ2
xin
rl σ2 (1 1 )
= xin xin − din
σ2
xin
1
Corr(x ,y )=rl (1 ) rl =rl
outj outj xin − d ≈ xin xout
in
FromXuetal.(2019)(Eq. 17),thebackwardpassthroughLayerNormis,
g = g out (I 1T din1 din +xT outx out )
in
σˆ
din
− d
xin in
g out (I 1T din1 din +xT outx out )
≈ σ
din
− d
xin in
1T 1 +xT x
Wehave lim din din out out =O whereO iszeromatrixwithshaped d
din→∞ d in
din,din din,din in
×
in
g
g out (I )
in
≈ σ
din
xin
46TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
g
out
=
σ
xin
g
= g =
outi
⇒
ini
σ
xin
Ifµ =0,
gout
µ =0
gin
σ2
σ2 = gout
gin σ2
xin
B.7 Softmax
Assumption: Otherthanassumingnormallydistributedinputs,wealsoassumethatLislargeL>>1toderivesoftmax
variance.
TheforwardpassofSoftmaxcanbedefinedas
x =Softmax(x )
out in
exini
x =
outi (cid:80)L exinj
j=1
Forcalculatingmeanwecaneasilyseethat,
L
(cid:88)
x =1
outi
i=1
Takingexpectationbothsides,weget
L
(cid:88)
E[ x ]=1
outi
i=1
L
(cid:88)
E[x ]=1
outi
i=1
Bysymmetrywecanassumethatforanyi,j,i=j,wehaveE[x ]=E[x ]
̸
outi outj
LE[x ]=1
outi
1
µ =
xout
L
Letusdefinez =(cid:80) jeyj wherey
j
=x
j
−x iisnormallydistributed N(0,σ j). Hence,eacheyj islog-normallydistributed,
andzisasumofcorrelatedlog-normals. Following(Lo,2013),thissumoflog-normalscanbeapproximatedasanother
log-normalrandomvariable,Log (µ ,σ ),whereµ andσ areasfollows-
z z z z
N
(cid:88) (cid:88) σj2
S + =E[ y j]= e 2
j j
σ z2 = S1
2
(cid:88) corr j,kσ jσ ke1 2(σ j2+σ k2)
+ j,k
σ2
µ =ln(S ) z
z +
− 2
47TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Since the difference of two normals x and x is also normal, from the M.G.F. of normal distribution, we have σ2 =
j i j
2σ2 (1 r )ifj =i,andσ2 =0ifj =i.
xin − xin ̸ j
Also,corr =0ifj =iork =i,elsecorr = 1.
j,k j,k 2
Wecansubstitutethesevaluesintheaboveequations,toget
S
+
=(L 1)eσ x2 in(1 −rxin)+1
−
L
σ2 =σ2 (1 r )
z xin − xin L 1
−
σ2
µ =ln(S ) z
z +
− 2
Sincezislog-normal,x = 1 isalsolog-normalwithLog ( µ ,σ ). Thevarianceoflog-normaldistributioncanbe
out z N − z z
obtainedfromstandardformulaeforlog-normaldistributionas(eσ z2 1)eσ z2 −2µz.
−
Substitutingthevaluesofµ andσ fromabove,weget
z z
σ2 =
(eσ z2 −1)e2 ∗σ z2
xout S2
+
(eσ x2 in(1 −rxin) LL
−1
1)e2σ x2 in(1 −rxin) LL
−1
= −
((L 1)eσ x2 in(1 −rxin)+1)2
−
ForlargeL,wecanignorethe1inthedenominator-
σ2 =
(eσ x2 in(1 −rxin) LL −1 −1)
xout (L 1)2
−
IfL>>1andσ2 issmall,wegetthemoresimplifiedformulaas-
xin
σ2 (e(1 −r xd in)σ x2 in −1) (AssumingL>>1)
xout ≈ L2
Usingthemeanandvariances,wecancalculatethescaleofsoftmaxoutputasfollows-
E[x2 ]=σ2 +µ2
out xout xout
(e(1 −r xd in)σ x2 in)
=
L2
TheJacobianofSoftmaxcanbecalculatedas((Kimetal.,2021)):
(cid:40)
x (1 x ) ifi=j
J =
outi
−
outi
i,j
x x else
−
outi outj
48TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
ForlargevaluesofLthisapproximatelybecomes
J diag(x )
out
≈
g =g J
in out
g g x
ini
≈
outi outi
E[g ] E[g x ]
ini
≈
outi outi
=E[g ]E[x ]=0=µ
outi outi gin
E[g2 ] E[g2 x2 ]
ini ≈ outi outi
=E[g2 ]E[x2 ]
outi outi
σ2 =σ2
(e(1 −r xd in)σ x2 in)
gin gout L2
B.8 ScaledDot-ProductAttention
InapplicabilityofDirectUsageofSoftmaxDerivationsforSHA:Onemaybetemptedtoassumeattentionscorestobe
independentofvalues. ThisthenenablestheuseofourpreviousLogNormal-basedsoftmaxderivation,toeasilyderivethe
forwardvariances.
Butthetheoreticallycalculatedmomentsstronglydisagreewithempiricalsimulations. ThisisbecauseSHAisX =
out
X W W TXT
Dropout(SoftMax( in Q K in))X W ,andtheW TXTtermcannotbetreatedindependentlyoftheX W
√d in V K in in V
k
term. Asimpleverificationofthiscanbecheckedbysimplysimulating(XWT)X,andverifyingthatthevariancesofthe
resultsdonotmatchthatofL σ2((XW)),butdoifthesecondXisreplacedbyanotherrandomtensor.
∗
ThisnecessitatesanalternatemethodologytoderiveSHA,wherethecomponentsaretreatedasaunifiedwhole.
Assumption: WeassumethatLandd areverylargewhencomparedtoscaleofscoresbeingpassedtotheSoftmax. These
in
approximationsholdtrueforsmallvaluesofσ andσ ,andtheresultingformulaearefairlyaccurate,asshowninthe
q k
numericalverificationsection.
TheforwardpassofScaledDot-ProductAttentionis
QKT
X =Dropout(SoftMax( ))V
out (cid:112)
d
i,k
Where,
Q=X W
in Q
K=X W
in K
V=X W
in V
X W W TXT
X =Dropout(SoftMax( in Q K in))X W
out (cid:112) in V
d
i,k
Let,
X W W TXT
O=Dropout(SoftMax( in Q K in))X
(cid:112) in
d
i,k
X W W T
W= in Q K
(cid:112)
d
i,k
O=Dropout(SoftMax(WXT))X
in in
UsingresultsfromLinearLayerwehaveσ2 =d σ2 σ2σ2 =d σ2 σ2
w in xin q k in xin qk
L
(cid:88)
O = Dropout(SoftMax(WXT)) X
i,j in i,k ink,j
k=1
49TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
=(cid:88)L
Dropout(
exp((WXT in) i,k)
)X
L
ink,j
(cid:88)
k=1 exp((WXT) )
in i,m
m=1
=(cid:88)L Dropout(exp((WXT in) i,k))
X
L
ink,j
(cid:88)
k=1 exp((WXT) )
in i,m
m=1
L
(cid:88)
Dropout(exp((WXT) ))X
in i,k ink,j
= k=1
L
(cid:88)
exp((WXT) )
in i,m
m=1
(cid:88)L (cid:88)din
Dropout(exp( W X ))X
i,l ink,l ink,j
= k=1 l=1
(cid:88)L (cid:88)din
exp( W X )
i,n inm,n
m=1 n=1
(cid:88)L (cid:88)din
Dropout(exp( W X )X )
i,l ink,l ink,j
= k=1 l=1
(cid:88)L (cid:88)din
exp( W X )
i,n inm,n
m=1 n=1
EachX canbewrittenas:
ini,j
X =ϵ +δ
ini,j j i,j
Whereϵ andδ areallindependentanddefinedas
j i,j
ϵ (0,rl σ2 )
j ∼N xin xin
δ (0,(1 rl )σ2 )
i,j ∼N − xin xin
(cid:88)L (cid:88)din
Dropout(exp( W X )X )
i,l ink,l ink,j
O = k=1 l=1
i,j
(cid:88)L (cid:88)din
exp( W X )
i,l ink,l
k=1 l=1
(cid:88)L (cid:88)din
(1 d )(exp( W X )X )
−
i,k i,l ink,l ink,j
= k=1 l=1
(cid:88)L (cid:88)din
(1 p) exp( W X )
−
i,l ink,l
k=1 l=1
Whered isBernoullirandomvariablewhichis1withprobabilityp
i,k
(cid:80)L
(1 d
)exp((cid:80)din
W (ϵ +δ ))(ϵ +δ )
= k=1 − i,k l=1 i,l l k,l j k,j
(1
p)(cid:80)L exp((cid:80)din
W (ϵ +δ ))
− k=1 l=1 i,l l k,l
(cid:80)L
(1 d
)exp((cid:80)din
W ϵ
)exp((cid:80)din
W δ )
=ϵ k=1 − i,k l=1 i,l l l=1 i,l k,l
j
(1
p)(cid:80)L exp((cid:80)din
W ϵ
)exp((cid:80)din
W δ )
− k=1 l=1 i,l l l=1 i,l k,l
50TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
(cid:80)L
(1 d
)exp((cid:80)din
W ϵ
)exp((cid:80)din
W δ )δ
+ k=1 − i,k l=1 i,l l l=1 i,l k,l k,j
(1
p)(cid:80)L exp((cid:80)din
W ϵ
)exp((cid:80)din
W δ )
− k=1 l=1 i,l l l=1 i,l k,l
(cid:80)L
(1 d
)exp((cid:80)din
W δ )
(cid:80)L
(1 d
)exp((cid:80)din
W δ )δ
=ϵ k=1 − i,k l=1 i,l k,l + k=1 − i,k l=1 i,l k,l k,j
j
(1
p)(cid:80)L exp((cid:80)din
W δ ) (1
p)(cid:80)L exp((cid:80)din
W δ )
− k=1 l=1 i,l k,l − k=1 l=1 i,l k,l
Letv 1 =ϵ j(cid:80) (1L k −= p1( )1 (cid:80)− L kd =i, 1k) exex pp ((cid:80)((cid:80) d l=id l n= 1in 1 WW i,i l, δl kδ ,k l, )l) andv 2 = (cid:80)L k (= 11 −( p1 )− (cid:80)di L k,k =) 1e ex xp p( ((cid:80) (cid:80)d l= d lin =i1 n 1W Wi, il ,lδ δk k,l ,l) )δk,j. Wehave,
O =v +v
i,j 1 2
Givenafixedϵ,W,wehave
(cid:80)L
(1 d
)exp((cid:80)din
W δ )
v ϵ,W =ϵ k=1 − i,k l=1 i,l k,l
1 | j (1 p)(cid:80)L exp((cid:80)din W δ )
− k=1 l=1 i,l k,l
(cid:80)L k=1(1 −di,k)exp((cid:80)d l=in 1Wi,lδk,l)
=ϵ L
j
(1
p)(cid:80)L k=1exp((cid:80)d l=in 1Wi,lδk,l)
− L
ByWLLN, (cid:80)L k=1(1 −di,k)ex Lp((cid:80)d l=in 1Wi,lδk,l) →p (1 −p)E δ[exp((cid:80)d l=in 1W i,lδ k,l)],and
(1 p)(cid:80)L k=1exp((cid:80)d l=in 1Wi,lδk,l) p (1 p)E [exp((cid:80)din W δ )]
− L → − δ l=1 i,l k,l
p
Thus,wehavev ϵ,W ϵ
1 j
| →
(cid:80)L
(1 d
)exp((cid:80)din
W δ )δ
v ϵ,W = k=1 − i,k l=1 i,l k,l k,j
2 | (1 p)(cid:80)L exp((cid:80)din W δ )
− k=1 l=1 i,l k,l
1 √L(cid:80)L (1 −di,k)exp((cid:80)d l=in 1Wi,lδk,l)δk,j
=
√L k=1 L
(1
p)(cid:80)L exp((cid:80)d l=in 1Wi,lδk,l)
− k=1 L
Letµ =E [(1 d )exp((cid:80)din W δ )δ ],σ2 =Var ((1 d )exp((cid:80)din W δ )δ ). Bycentrallimit
num δ,d − i,k l=1 i,l k,l k,j num δ,d − i,k l=1 i,l k,l k,j
theoremforlargeL,
(cid:80)L
(1 d
)exp((cid:80)din
W δ )δ
(cid:80)L
(1 d
)(exp((cid:80)din
W δ )δ µ )
√L k=1 − i,k l=1 i,l k,l k,j =√L k=1 − i,k l=1 i,l k,l k,j − num +√Lµ
num
L L
(cid:80)L
(1 d
)exp((cid:80)din
W δ )δ
√L k=1 − i,k l=1 i,l k,l k,j d (0,σ2 )+√Lµ
L →N num num
(cid:80)L (1 d )exp((cid:80)din W δ )δ σ2
k=1 − i,k l=1 i,l k,l k,j d (µ , num)
num
L →N L
l=d
(cid:89)
µ =E [1 d ]( E [exp(W δ )])E [exp(W δ )δ ]
num d i,k δ i,l k,l δ i,j k,j k,j
−
l=1,l=j
̸
W2σ2
E [exp(W δ )]=exp( i,l δ) (MGFofgaussian)
δ i,l k,l
2
E [exp(W δ )δ ]=(cid:90) ∞ exp(W i,jδ k,j)δ k,j exp( δ k2 ,j)dδ
δ i,j k,j k,j √2πσ −2σ2 k,j
δ δ
−∞
=(cid:90) ∞ exp(W i2 ,jσ δ2
)
δ k,j
exp(
(δ k,j −W i,jσ δ2)2
)dδ
2 √2πσ − 2σ2 k,j
δ δ
−∞
=exp(W i2 ,jσ δ2 )(cid:90) ∞ δ k,j
exp(
(δ k,j −W i,jσ δ2)2
)dδ
2 √2πσ − 2σ2 k,j
δ δ
−∞
51TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
W2 σ2
=exp( i,j δ)W σ2
2 i,j δ
(cid:80)din W2σ2
µ =(1 p)exp( l=1 i,l δ )W σ2
num − 2 i,j δ
l=d
(cid:89)
σ2 =E [(1 d )2]( E [exp(2W δ )])E [exp(2W δ )δ2 ] µ2
num d − i,k δ i,l k,l δ i,j k,j k,j − num
l=1,l=j
̸
E [exp(2W δ )]=exp(2W2σ2) (MGFofgaussian)
δk,l i,l k,l i,l δ
(cid:90) exp(2W δ )δ2 δ2
E [exp(2W δ )δ2 ]= ∞ i,j k,j k,j exp( k,j )dδ
δk,j i,j k,j k,j √2πσ −2σ2 k,j
δ δ
−∞
=(cid:90) ∞ exp(2W2 σ2) δ k2 ,j exp( (δ k,j −2W i,jσ δ2)2 )dδ
i,j δ √2πσ − 2σ2 k,j
δ δ
−∞
=exp(2W2 σ2)(cid:90) ∞ δ k2 ,j exp( (δ k,j −2W i,jσ δ2)2 )dδ
i,j δ √2πσ − 2σ2 k,j
δ δ
−∞
=exp(2W2 σ2)(4W2 σ4+σ2)
i,j δ i,j δ δ
(cid:88)din (cid:88)din
σ2 =(1 p)exp(2 W2σ2)(4W2 σ4+σ2) (1 p)2exp( W2σ2)W2 σ4
num − i,l δ i,j δ δ − − i,l δ i,j δ
l=1 l=1
Similarly,(cid:80)L exp((cid:80)din
W δ )isalsoasumofLi.i.d. randomvariablesforfixedW. ByWLLNwehave,
k=1 l=1 i,l k,l
(1
p)(cid:80)L k=1exp((cid:80)d
l=in 1W i,lδ k,l) p (1 p)E
[exp((cid:88)din
W δ )]
δ i,l k,l
− L → −
l=1
l=d
p (1 p)((cid:89) E [exp(W δ )])
δ i,l k,l
→ −
l=1
(1
p)(cid:80)L k=1exp((cid:80)d l=in 1W i,lδ k,l) p
(1
p)exp((cid:80)d l=in 1W i2 ,lσ δ2
)
− L → − 2
(cid:80)L k=1exp((cid:80)d l=in 1Wi,lδk,l)δk,j
v = L
2 (cid:80)L k=1exp((cid:80)d l=in 1Wi,lδk,l)
L
As for a given W,ϵ, both the numerator and denominator converge in distribution and denominator is converging to a
constantbySlutskystheorem,
µ σ2
v W,ϵd ( num , num )
2 | →N (cid:80)din W2 σ2 L(1 p)2exp((cid:80)din W2σ2)
(1 −p)exp( l=1 2 i,l δ) − l=1 i,l δ
exp((cid:80)d l=in 1W i2 ,lσ δ2)(4W i2 ,jσ δ4+σ δ2) W2 σ4
v
2
|W,ϵ →d N(W i,jσ δ2, (1 −p)
L
− i,j δ )
Thuswehave,
exp((cid:80)d l=in 1W i2 ,lσ δ2)(4W i2 ,jσ δ4+σ δ2) W2 σ4
O
i,j
|W,ϵ ∼N(W i,jσ δ2, (1 −p)
L
− i,j δ )+ϵ
j
Wehave,
E[O W]=W σ2+0=W σ2
i,j | i,j δ i,j δ
52TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
exp((cid:80)d l=in 1W i2 ,lσ δ2)(4W i2 ,jσ δ4+σ δ2) W2 σ4
E[O2 W]= (1 −p) − i,j δ +σ2
i,j| L ϵ
E[O ]=E [O W]=E [W σ2]=0
i,j W i,j | W i,j δ
E[O2 ]=E [O2 W]
i,j W i,j|
exp((cid:80)d l=in 1W i2 ,lσ δ2)(4W i2 ,jσ δ4+σ δ2) W2 σ4
=E W[W i2 ,jσ δ4+ (1 −p)
L
− i,j δ +σ ϵ2]
Forlarged byWLLNandcontinuousmappingtheoremexp((cid:80)din W2σ2) exp(d σ2σ2)
in l=1 i,l δ ≈ in w δ
(L 1)σ2σ4+ exp(dinσ w2σ δ2)(4σ w2σ δ4+σ δ2)
= − w δ (1 −p) +σ2
L ϵ
(1 rl )2(L 1)d σ6 σ2 +
exp((1 −r xl in)d2 inσ x4 inσ q2 k)(4(1 −r xl in)2dinσ x6 inσ q2 k+(1 −r xl in)σ x2 in)
= − xin − in xin qk (1 −p) +rl σ2
L xin xin
Hence,
µ =0
xout
(1 rl )2(L 1)d σ6 σ2 +
exp((1 −r xl in)d2 inσ x4 inσ q2 k)(4(1 −r xl in)2dinσ x6 inσ q2 k+(1 −r xl in)σ x2 in)
σ2 = − xin − in xin qk (1 −p) +rl σ2
xout L xin xin
Now to get covariance we make two approximations. As the term (cid:80)L k=1(1 −di,k)exp((cid:80)d l=in 1Wi,lδk,l) converges to 1, we
(1 −p)(cid:80)L k=1exp((cid:80)d l=in 1Wi,lδk,l)
approximatev ϵ .
Alsowewilltreat(cid:80)L exp((cid:80)din
W δ )
exp((cid:80)d l=in 1W i2 ,lσ δ2
). Then,wehave
1i,j ≈ j k=1 l=1 i,l k,l ≈ 2
v ϵ
1i,j
≈
j
(cid:80)L (1 −di,k)exp((cid:80)d l=in 1Wi,lδk,l)δk,j
v k=1 L
2i,j ≈ (cid:80)din W2 σ2
(1 p)exp( l=1 i,l δ)
− 2
Thismakesv andv independent. Forcovariance
1i,j 2i,j
E[O O ]=E [E[O O W]]
i,j m,j W i,j m,j
|
O O W =(v +v )(v +v )
i,j m,j
|
1i,j 2i,j 1m,j 2m,j
=v v +v v +v v +v v
1i,j 1m,j 1i,j 2m,j 2i,j 1m,j 2i,j 2m,j
v v =ϵ2
1i,j 1m,j j
E[v v W]=σ2
1i,j 1m,j| ϵ
Asv =v =ϵ ,v v +v v =ϵ (v +v ),andϵ isindependentof(v +v ). Thus,wehave
1i,j 1m,j j 1i,j 2m,j 2i,j 1m,j j 2i,j 2m,j j 2i,j 2m,j
E[v v +v v W]=E[ϵ W]E[(v +v )W]=0 E[(v +v )W]=0
1i,j 2m,j 2i,j 1m,j| j
|
2i,j 2m,j
| ∗
2i,j 2m,j
|
(cid:80)L (1 −di,k1)exp((cid:80)d l=in 1Wi,lδk1,l)δk1,j (cid:80)L (1 −dm,k2)exp((cid:80)d l=in 1Wm,lδk2,l)δk2,j
v v = k1=1 L k2=1 L
2i,j 2m,j (cid:80)din (W2 +W2 )σ2
(1 p)2exp( l=1 i,l m,l δ)
− 2
E[(cid:80)L
(1 d
)exp((cid:80)din
W δ )δ
(cid:80)L
(1 d
)exp((cid:80)din
W δ )δ ]
E[v v W]= k1=1 − i,k1 l=1 i,l k1,l k1,j k2=1 − m,k2 l=1 m,l k2,l k2,j
2i,j 2m,j| (cid:80)din (W2 +W2 )σ2
L2(1 p)2exp( l=1 i,l m,l δ)
− 2
53TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Breakingsummationintotwoparts: k =k =kandk =k ,weget
1 2 1 2
̸
E[(cid:80)L (1 d )(1 d )exp((cid:80)din (W +W )δ )δ2 ]
= k=1 − i,k − m,k l=1 i,l m,l k,l k,j
(cid:80)din (W2 +W2 )σ2
L2(1 p)2exp( l=1 i,l m,l δ)
− 2
E[(cid:80)L (cid:80)L
(1 d )(1 d
)exp((cid:80)din
W δ
)exp((cid:80)din
W δ )δ δ ]
+ k1=1 k2=1,k2̸=k1 − i,k1 − m,k2 l=1 i,l k1,l l=1 m,l k2,l k1,j k2,j
(cid:80)din (W2 +W2 )σ2
L2(1 p)2exp( l=1 i,l m,l δ)
− 2
(cid:80)L E[(1 d )(1 d )exp((cid:80)din (W +W )δ )δ2 ]
= k=1 − i,k − m,k l=1 i,l m,l k,l k,j
(cid:80)din (W2 +W2 )σ2
L2(1 p)2exp( l=1 i,l m,l δ)
− 2
(cid:80)L (cid:80)L E[(1 d )(1 d )exp((cid:80)din W δ )exp((cid:80)din W δ )δ δ ]
+ k1=1 k2=1,k2̸=k1 − i,k1 − m,k2 l=1 i,l k1,l l=1 m,l k2,l k1,j k2,j
(cid:80)din (W2 +W2 )σ2
L2(1 p)2exp( l=1 i,l m,l δ)
− 2
(cid:88)din (cid:88)din
E[(1 d )(1 d )exp( (W +W )δ )δ2 ]=E[(1 d )]E[(1 d )]E[exp( (W +W )δ )δ2 ]
− i,k − m,k i,l m,l k,l k,j − i,k − m,k i,l m,l k,l k,j
l=1 l=1
(cid:80)din (W +W )2σ2
=(1 p)2exp( l=1 i,l m,l δ)((W +W )2σ4+σ2)
− 2 i,j m,j δ δ
(cid:88)din (cid:88)din
E[(1 d )(1 d )exp( W δ )exp( W δ )δ δ ]=E[(1 d )]E[(1 d )]
−
i,k1
−
m,k2 i,l k1,l m,l k2,l k1,j k2,j
−
i,k1
−
m,k2
l=1 l=1
(cid:88)din (cid:88)din
E[exp( W δ )δ ]E[exp( W δ )δ ]
i,l k1,l k1,j m,l k2,l k2,j
l=1 l=1
(cid:80)din (W2 +W2 )σ2
=(1 p)2exp( l=1 i,l m,l δ)W W σ4
− 2 i,j m,j δ
exp((cid:80)d l=in 1(Wi,l+Wm,l)2σ δ2 )((W +W )2σ4+σ2)
E[v v W]= 2 i,j m,j δ δ
2i,j 2m,j| (cid:80)din (W2 +W2 )σ2
Lexp( l=1 i,l m,l δ)
2
(L 1)exp((cid:80)d l=in 1(W i2 ,l+W m2 ,l)σ δ2 )W W σ4
+ − 2 i,j m,j δ
(cid:80)din (W2 +W2 )σ2
Lexp( l=1 i,l m,l δ)
2
exp((cid:80)din W W σ2)((W +W )2σ4+σ2) (L 1)W W σ4
= l=1 i,l m,l δ i,j m,j δ δ + − i,j m,j δ
L L
So,wehave
exp((cid:80)din W W σ2)((W +W )2σ4+σ2) (L 1)W W σ4
E[O O W]=σ2+ l=1 i,l m,l δ i,j m,j δ δ + − i,j m,j δ
i,j m,j | ϵ L L
E[O O ]=E [E[O O W]]
i,j m,j W i,j m,j
|
exp((cid:80)din W W σ2)((W +W )2σ4+σ2) (L 1)W W σ4
=E [σ2+ l=1 i,l m,l δ i,j m,j δ δ + − i,j m,j δ]
W ϵ L L
exp((cid:80)din W W σ2)((W +W )2σ4+σ2)
=σ2+E [ l=1 i,l m,l δ i,j m,j δ δ ]
ϵ W L
54TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Forlargevaluesofd byWLLNandcontinuousmappingtheoremwehaveexp((cid:80)din W W σ2) 1. Thus,wehave
in l=1 i,l m,l δ ≈
(2σ2σ4+σ2)
E[O O ]=σ2+ w δ δ
i,j m,j ϵ L
(2(1 rl )2d σ6 σ2 +(1 rl )σ2 )
E[O O ]=rl σ2 + − xin in xin qk − xin xin
i,j m,j xin xin L
TheconvergenceargumentswehavemaderequirethescaleofthevariablestobesmallwhencomparedtoLandd . The
in
growthinscalecanbecontrolledeasilybycontrollingσ ,andweobservethatifweletσ becomearbitrarilylargethe
qk qk
scorespassedtoSoftmaxdivergeleadingtodegenerateattentiononlyattendingtoonetokenwhichhasthehighestscore. To
avoidthisdegenerateattention,wechoosesmallervaluesofσ ,σ andinthatscenario,theapproximatevalueforvariance
q k
andcovarianceare,
σ2 rl σ2
xout ≈ xin xin
Covl rl σ2
xout ≈ xin xin
TogetthefinalvarianceandcovariancewecanuseresultsofLinearlayertoaccountforW . Ifweinitializeσ andσ
V q k
tobesmall,ininitialphaseoftrainingtheoutputofSoftmaxlayercanbetreatedasbeingaconstant= 1T L1L. Usingthis
L
assumptionwehave,
1T1
X Dropout( L L )X W
out ≈ L in V
1T1
= g Dropout( L L )Tg W T
⇒ Xin ≈ L Xout V
1T1
=Dropout( L L )g W T
L Xout V
µ =0
gin
σ2 dσ2
σ2 = gout v (1+(L 1)rl (1 p))
gin L(1 p) − gout −
−
σ2 dσ2
Covl = gout v(1+(L 1)rl )
gin L − gout
C MomentPropagationthroughTransformerBlocks
C.1 TransformerAttentionBlock
AforwardpassthroughattentionblockconsistsofLayerNorm,followedbyScaledDot-ProductAttention,followedbyan
outputprojectionlayer(aLinearLayer),andfinallyaDropout. Usingtheresultsfromaboveweget,
µ =0 0 0 0=0
xout
∗ ∗ ∗
σ2
xout
=
(1 −r xl in)2(L −1)d inσ x6 inσ q2σ k2+
exp((1 −r xl
in
L)d2 inσ x4 inσ q2σ k2)(4(1
(−
1
−r xl
pi
)n)2dinσ x6 inσ q2σ k2+(1 −r xl in)σ x2 in)
+r xl inσ x2
in
.d inσ v2.
(1d inσ
o
p2
)
−
=
d2 inσ o2σ v2σ x2
in

(1 −r xl in)2(L −1)d inσ x4 inσ q2σ k2+
exp((1 −r xl in)d2 inσ x4 inσ q2σ k2)(4 (( 11
−−
pr )xl in)2dinσ x4 inσ q2σ k2+(1 −r xl in))
+rl


(1 p) L xin
−
Covl
xout
55TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
(cid:32) (cid:33)
(2(1 rl )2d σ6 σ2σ2+(1 rl )σ2 )
= rl σ2 + − xin in xin q k − xin xin .d σ2.d σ2.1
xin xin L in v in o
(cid:32) (cid:33)
(2(1 rl )2d σ4 σ2σ2+(1 rl ))
=d2σ2σ2σ2 rl + − xin in xin q k − xin
in o v xin xin L
1 d σ2
σ2 =σ2 d σ2 in v (1+(L 1)rl (1 p))
gin gout ∗ (1 p) ∗ in o ∗ L(1 p) − gout −
− −
d2σ2 σ2σ2
= in gout v o(1+(L 1)rl (1 p))
L(1 p)2 − gout −
−
d σ2
Covl =σ2 1 d σ2 in v(1+(L 1)rl )
gin gout ∗ ∗ in o ∗ L − gout
d2σ2 σ2σ2
= in gout v o(1+(L 1)rl )
L − gout
C.2 TransformerFFNBlock
AforwardpassthroughtheFFNblockofatransferhasaLayerNorm,thenaLinearlayerfromdto4d,whichisthenpassed
throughaReLUgate,theoutputofwhichistheprojectedbacktoddimensionusinganotherLinearlayer,andeventually
passedthroughaDropout. Againusingtheresultsfromaboveweget,
µ =0 (LastLinearLayermakesit0)
xout
π 1 1 1
σ2 =1 d σ2 ( − + ) 4d σ2 σ2
xout ∗ in w1 ∗ 2π 2π ∗ in w2 ∗ (1 p) ∗ xin
−
2d2σ2 σ2
= in w1 w2σ2
(1 p) xin
−
Covl =d σ2
(r xl
in +
(1 −(r xl in)2)0.5
+
r xl insin−1(r xl in) 1
+
1
) 4d σ2 σ2
xout in w1 ∗ 4 2π 2π − 2π 2π ∗ in w2 ∗ xin
=4d2σ2 σ2 σ2
(r xl
in +
(1 −(r xl in)2)0.5
+
r xl insin−1(r xl in)
)
in w1 w2 xin 4 2π 2π
rl =2 (1 p)
(r xl
in +
(1 −(r xl in)2)0.5
+
r xl insin−1(r xl in)
)
xout ∗ − ∗ 4 2π 2π
(1 p) (r xl in + 1 +(1 1 )rl 2 ) (Fittinga2-ndorderpolynomial)
≈ − ∗ 2 π 2 − π xin
1 1
σ2 =σ2 d σ2 4d σ2
gin gout ∗ (1 p) ∗ in w2 ∗ 2 ∗ in w1
−
2d2σ2 σ2 σ2
= in w1 w2 gout
(1 p)
−
Covl =Covl 1 d σ2
(1
+
sin−1(r xl in)
) 4d σ2
gin gout ∗ ∗ in w2 ∗ 4 2π ∗ in w1
=4d2σ2 σ2 Covl
(1
+
sin−1(r xl in)
)
in w1 w2 gout 4 2π
D SummaryTableofMomentPropagationthroughTransformerComponents
InTable14,Table15,Table16,Table17,Table18andTable19,wesummarizethesignalpropagationformulaeforallthe
transformercomponents.
56TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Table14:MomentPropagation(mean)duringforwardpassthroughcomponentsoftransformermodel.
Component µ
xout
Embeddings 0
FFN(d .d ) 0
1 2
σ
ReLU xin
(cid:112)
(2π)
σ2
GeLU xin
(cid:112)
2π(σ2 +1)
xin
LayerNorm(d) 0
Dropout(p) µ
xin
Softmax 1
L
SHABlock(withoutV) 0
AttnBlock 0
FFNBlock 0
Table15:MomentPropagation(variance)duringforwardpassthroughcomponentsoftransformermodel.
Component σ2
xout
Embeddings 0
FFN(d .d ) d σ2(σ2 +µ2 )
1 2 1 w xin xin
(π 1)
ReLU − σ2
(2π) xin
GeLU σ 2x2 πin(π
2 −
1+σ x2 σi x2n
in
+sin−1( 1+σ x2 σi x2n in)+
(1+σ x2
in2 )σ √x2 in
1+2σ x2
in)
LayerNorm(d) 1
σ2 +pµ2
Dropout(p) xin xin
1 p
−
Softmax (eσx2 in(1− ((r Lxl in) 1L )eL − σ1 x2− in(1 1) −e2 rσ xl ix2 ni )n +(1 1− )r 2xl in)LL −1
−
SHA(withoutV)
d inσ x2
in
(cid:32) (1 −r xl in)2(L −1)dinσ x4 inσ q2σ k2+exp((1−rxl in)d2 inσx4 inσq2σk2)(4 (( 11 −− pr )xl in)2dinσx4 inσq2σk2+(1−rxl in))
+rl
(cid:33)
(1 p) L xin
−
AttnBlock(Approx)
d2 inσ o2σ v2σ x2
in
(cid:32) (1 −r xl in)2(L −1)dinσ x4 inσ q2σ k2+exp((1−rxl in)d2 inσx4 inσq2σk2)(4 (( 11 −− pr )xl in)2dinσx4 inσq2σk2+(1−rxl in))
+rl
(cid:33)
(1 p) L xin
−
2d2σ2 σ2 σ2
FFNBlock in w1 w2 xin
(1 p)
−
57TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Table16:MomentPropagation(variance)duringbackwardspassthroughcomponentsoftransformermodel.
Component σ2
gin
Embeddings -
FFN(d .d ) d σ2σ2
1 2 2 w gout
1
ReLU σ2
2 gout
(cid:20) (cid:21)
GeLU 41 + 21
π
sin−1(
σ
x2σ ix2 n+in 1)+ 2π(σσ x2x i2 ni +n( 15 )σ (x2 2i σn+
x2
in3 +)
1)23
σ g2
out
σ2
LayerNorm(d) gout
σ2
xin
1
Dropout(p) σ2
1 p gout
−
Softmax ((eσx2 in(1− ((r Lxl in) 1L )eL − σ1 x2− in(1 1) −e2 rσ xl ix2 ni )n +(1 1− )r 2xl in)LL −1 + L1 2)σ g2 out
−
d σ2
SHABlock(withoutV) in gout (1+(L 1)rl (1 p))
L(1 p)2 − gout −
−
d2σ2 σ2σ2
AttnBlock(Approx) in gout v o(1+(L 1)rl (1 p))
L(1 p)2 − gout −
−
2d2σ2 σ2 σ2
FFNBlock in w1 w2 gout
(1 p)
−
58TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Table17:Covariance(alongsequencelength)propagationthroughthecomponentsoftransformermodel.
Component Covl
xout
Embeddings
(cid:80)N
i
∗(N
i
−1)
σ2
L (L 1)) ∗ wembd
∗ −
FFN(d .d ) d σ2(Covl +µ2 )
1 2 1 w xin xin
ReLU
(1
+
sin−1(r xl in)
)Covl (1
(cid:113)
(1 (rl
)2))σ x2
in
4 2π xin − − − xin 2π
GeLU
σ 4x2
πin(πr xl
in
+2r xl
insin−1(r σxl
x2in
inσ +x2
1in)+
2 (σσ x2x2
ii nn
+(σ 1x2
)i
√n(1
(−
σ
x2( ir nxl
+in
1) )2 2) −+ (1 r+
xl
i( nr σxl
x2in
in) )2 2)
−
(σ2 x2σ inx2
+in 1))
1 Covl
LayerNorm(d) (1 ) xin
− d σ2
xin
Dropout(p) Covl
xin
(cid:18) (cid:19)
SHA(withoutV) d σ2 rl +
(2(1 −r xl in)2dinσ x4 inσ q2σ k2+(1 −r xl in))
in xin xin L
(cid:18) (cid:19)
AttnBlock(Approx) d2σ2σ2σ2 rl +
(2(1 −r xl in)2dinσ x4 inσ q2σ k2+(1 −r xl in))
in o v xin xin L
(cid:113)
FFNBlock 4d σ2 σ2 σ2
(r xl
in +
(1 −(r xl in)2
+
r xl insin−1(r xl in)
)
in w1 w2 xin 4 2π 2π
Table18:Covariance(hiddendimension)propagationthroughthecomponentsoftransformermodel.
Component Covd
xout
Embeddings 0
FFN(d .d ) 0
1 2
ReLU
(1
+
sin−1(r xd in)
)Covd (1
(cid:113)
(1 (rd
)2))σ x2
in
4 2π xin − − − xin 2π
GeLU
1
LayerNorm(d)
−d 1
−
Dropout(p) Covd
xin
SHABlock(withoutV) 0
AttnBlock 0
FFNBlock 0
59TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Table19:Gradientcovariance(alongsequencelength)propagationthroughthecomponentsoftransformermodel.
Component Covl
gin
Embeddings -
FFN(d .d ) d σ2Covl
1 2 2 w gout
ReLU
(1
+
sin−1(r xl in)
)Covl
4 2π gout
(cid:20) (cid:21)
GeLU 1
4
+ 21
π
sin−1(r σxl x2in inσ +x2 1in)+ r xl 2in πσ (x2 σin x2( i( n2 +σ 1x2 )i (n (+ σ3 x2) in( +σ x2 1i )n 2+ −1 () r− xl2 in( σr x2xl ii nn )σ 2x2 )in 23)2) r gl outσ g2
out
Covl
LayerNorm(d) gout
σ2
xin
Dropout(p) Covl
gout
d σ2
SHABlock(withoutV) in gout(1+(L 1)rl )
L − gout
d2σ2 σ2σ2
AttnBlock(Approx) in gout v o(1+(L 1)rl )
L − gout
FFNBlock 4d2σ2 σ2 Covl
(1
+
sin−1(r xl in)
)
in w1 w2 gout 4 2π
E NumericalVerification
WeperformnumericalverificationfortheformulaereportedinTable14,Table15,Table16,Table17,Table18andTable19.
TheparameterrangeshavebeenprovidedinTable21. Foreachparameter,3-5valuesweresampleduniformly(orlog
uniformly)acrosstherangefornumericalsimulation. Table20providesthepercentageerrorcorrespondingtothe50 ,90
th th
and99 percentile. Thesesimulationresultsareallfullyreproducibleusingourcodereleasedassupplementarymaterial.
th
Evenat99percentile,noerror(otherthanSHAbackwards)islargerthan10%,verifyingourassumptions.
Table20:PercentageErrors[50th,90th,99thpercentile]forthetheoreticalformulascorrespondingtoforwardandbackwardpassthrough
componentsofthetransformermodel.
Component µ σ2 σ2 Covl Covl
xout xout gin xout gin
FFN [0.0,0.4,1.3] [0.4,1.4,2.8] [0.2,1.0,2.2] [0.4,1.4,2.8] [0.2,1.0,2.2]
ReLU [0.3,1.3,2.3] [0.5,1.9,3.4] [0.6,1.5,2.6] [0.3,1.6,3.1] [0.2,1.1,2.3]
GeLU [0.1,1.0,2.4] [0.2,0.6,1.3] [0.2,0.6,1.1] [0.1,0.5,1.2] [0.1,0.4,0.9]
LayerNorm [0.0,0.0,0.0] [0.0,0.0,0.0] [0.4,1.5,3.2] [0.1,0.5,1.0] [0.2,0.9,2.2]
Dropout [0.0,0.1,0.5] [0.1,0.5,1.5] [0.1,0.7,1.5] [0.0,0.4,1.3] [0.1,0.5,1.2]
Softmax [0.0,0.0,0.0] [0.2,0.9,4.0] [0.1,0.6,4.5] - -
Single-HeadAtten. [0.2,1.0,2.5] [1.4,4.1,7.8] [2.2,13.3,44.5] [1.3,3.9,7.4] [1.6,4.5,8.2]
60TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Table21:Rangeofinputvariance/correlationsusedfortheoreticalformulaverificationreportedinTable20forthetheoreticalformulas
correspondingtoforwardandbackwardpassthroughcomponentsofthetransformermodel.Thedropoutprobabilityrangewas[0,1)for
DropoutandSingle-HeadAttention,andσ2 forFFNwas[10−2,102]/d .
w in
Component µ σ2 σ2 Corrl Corrl d d L
xin xin gout xin gout in out
FFN [-10,10] [0.1,10] [0.1,10] [0,1.0) [0,1.0) [101,103] [101,103] [102,103]
ReLU [0] [0.1,10] [0.1,10] [0,1.0) [0,1.0) - - [102,103]
GeLU [0] [0.1,10] [0.1,10] [0,1.0) [0,1.0) - - [102,103]
LayerNorm [-10,10] [0.1,10] [0.1,10] [0,1.0) [0,1.0) [102,103] - [102,103]
Dropout [-10,10] [0.1,10] [0.1,10] [0,1.0) [0,1.0) [102,103] - [102,103]
Softmax [0] [10 4,1] [0.1,10] [0,1.0) - - - [300,104]
−
Single-HeadAtten. [0] [1] [0.1,10] [0,1.0) [0,1.0) [102,103] [32,64,128,256] [300,104]
F RankCollapseandCorrelationAnalysis
Intheprevioussections,wederivedtheformulasthatdeterminehowthecorrelationwillchangethroughtheAttentionand
FFNblocksbothforforwardandbackwardpass. BothattentionandFFNblocksmodifythecorrelationasshowninthe
Table2.
Simplifyingtheformulaeinthetableabove,werewritetheoutputvariancefortheattentionblockasσ2 =C rl σ2 ,
xattn 1 ∗ xin∗ xin
andtheoutputoftheFFNblockisσ2 =C σ2 ,whereC andC aredefinedasfollows.
xffn 2 ∗ xin 1 2
d2σ2σ2 2d2σ2 σ2
C = o v,C = w1 w2,
1 2
(1 p) (1 p)
− −
Thisalsohelpsustorewritethebackwardpassastheσ2 =C rl σ2 andσ2 =C σ2 .
gattn 1 ∗ gout ∗ gout gffn 2 ∗ gout
SpecificallyincaseofXavierinitializationwith0.1dropout,C =2.2,C =0.4.
1 2
Assuming a dropout of 0.1, the FFN block (with the ReLU) will reduce the correlation if it rises above 0.64 (where
rl < rl forFFNblock). Andtheattentionblockwillneveroutputacorrelationhigherthan0.9. Hencecorrelation
xout xin
willneverreach1,butratherasteady,stablevaluebetweenReLU’smaximumcorrelationandthatoftheattentionblock.
Dropout’seffectinpreventingrankcollapsewasalsoobservedin(Rongetal.,2020).
Wecanapproximatethestablevalueofcorrelationaftermanylayersbasedontheweightageaverageofthecorrelationin
theAttentionoutputandFFNoutput. Whentheattentionoutputisaddedtotheskipconnection,thenewcorrelationwillbe
aweighted(byvariance)averageofthecorrelationamongthetokensofattentionoutputandamongthetokensintheskip
connection. AndthesamewillhappenaftertheFFNblock.
AweightedaverageofthecorrelationsofFFNandattentionblocksgivesthestableasymptoticcorrelationrl
xmax
C (1 p)+C (1 p)(1 + r xl max +(1 1 )rl 2 )
rl = 1 ∗ − 2 ∗ − π 2 2 − π xmax
xmax C +C
1 2
Specificallyforthecaseofxavierinitialization,solvingtheaboveequationwithC =2.2,C =0.4,givesrl 0.88,as
1 2 xmax ≈
empiricallyverifiedinFigure6.
Similarly,thecorrelationforbackwardgradientwillalsoconvergeatastablevaluerl ,obtainedbysolvingthebelow
gmax
equation-
61TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
C (1 p)+C (1 p)(1 + sin−1(r xl max) )rl
rl = 1 ∗ − 2 ∗ − 2 π gmax
gmax C +C
1 2
Specificallyforthecaseofxavierinitialization,thisgivesrl =0.87. Notehowrl rl .
gmax gmax ≈ xmax
Discussionon(Nocietal.,2022) (Nocietal.,2022)focusesprimarilyonlinearactivation,wetheoreticallyanalyzethe
changeinoutputcorrelationcausedbyReLU.WefindthatReLU(oranyasymmetricnon-linearityingeneral)critically
affectscorrelation. Asourclosedformexpressionssuggest,bothFFNblock(becauseofReLU)anddropoutreducethe
correlation. While(Nocietal.,2022)mentionstheuseofdropout,asweshowaboveandobserveempiricallyinFigure6,
rankwillnotcollapsewithdropout,andperhaps(Nocietal.,2022)didnotusedropout. Further,weobservedthatFigure10
ofthesupplementaryof(Nocietal.,2022)showsacorrelationabove1,whichisimpossible.
Wereplicatedtheexperimentalsettingsof(Nocietal.,2022)withoutdropout,andobservedthattherankcollapseoccurs
duetoincorrectinitialization. Theyusearathernon-standardversionofxavierinitialization-insteadof 2 ,they
fanin+fanout
use 1 . Hence,theyinitializeamuchhighervalueforVasfan ismuchgreaterthanfan (“Numberofheads”
fanout in out
timesgreater),andthisresultsinvarianceoftheoutputoftheattentionblockC1beingmuchhigherthanFFNC2. As
attentionblockoutputsamuchhighercorrelationthantheFFNblock,increasingitsoutputvariancewithoutusingdropout
willresultinrankcollapse. Thishighlightsthecriticalityofcorrectinitialization,aswellastheexplainabilitypowerofour
theoreticalframeworkproposedinthepaper.
G MomentPropagationthroughtheEntireTransformerModel
G.1 VanillaPre-LN
WewillusetheapproximationslistedinTable2here.
G.1.1 FORWARDPASS
Forforwardpass,aTransformerPre-LNhasLayerNormfollowedbytheAttentionblock,residualconnection,LayerNorm,
andthentheFFNblock. Letσ2 betheoutputvarianceafter1suchlayer,andσ2 betheoutputvarianceaftertheentire
layer model
modelofN layers.
d2σ2σ2 rl
σ2 = o v ∗ xin
xattn (1 p)
−
2d2σ2 σ2
σ2 = w1 w2
xffn (1 p)
−
σ2 =σ2 +σ2 +σ2
xlayer xin xattn xffn
d2σ2σ2 rl 2d2σ2 σ2
=σ2 + o v ∗ xin + w1 w2
xin (1 p) (1 p)
− −
d2σ2σ2 2d2σ2 σ2
Let,C = o v,C = w1 w2,
1 2
(1 p) (1 p)bu
− −
Then,σ2 =σ2 +C rl +C
xlayer xin 1 ∗ xin 2
AswediscussinSection4.3,thecorrelationrl quicklyreachesastableconstantmaximumvaluerl ,whichcanbefound
xin xmax
usingthecalculationsinAppendixF.Letrl >0betheminimumvalueofthiscorrelation,letC =C rl +C ,and
xmin 3 1 ∗ xmax 2
C =C rl +C . Then,
4 1 ∗ xmin 2
σ2 +C σ2 σ2 +C
xin 4 ≤ xlayer ≤ xxin 3
62TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
HenceafterNlayers,
σ2 +N C σ2 σ2 +N C
xin ∗ 4 ≤ xmodel ≤ xin ∗ 3
= σ2 =Θ(N) (2)
⇒ xmodel
ThisshowsthatoutputvarianceofPre-LNwillincreaselinearlywithnumberoflayersN.
Inpractice,becausethecorrelationquicklyreachesrl ,thevarianceoftheentiremodelσ2 σ2 +N C .
xmax xmodel ≈ xin ∗ 3
Discussion: Thishastheeffectthattransformerblocksneartheoutputcanaffectthemodeloutputmuchless,astheskip
connectionvarianceincreasesbutblockoutputvarianceisconstant. Weconjecturethatparametersinthesearehencenot
beingutilizedtotheirfullpotential. SpecificallyincaseofXavierinitialization,C = 2.2,C = 0.4,rl = 0.85. For
1 2 xmax
larged,σ2 willbenegligiblysmallcomparedtoσ2 ,sowehave-
xin xlayer
σ2 C N (2.2 0.85+0.4)N 2.2N
xmodel ≈ 3 ∗ ≈ ∗ ≈
G.1.2 BACKWARDPASS
Forthebackwardpass,aTransformerPre-LNgradientwillfirstbackpropagatethroughtheFFNblock,thengetsrescaled
byLayernorm,andaddedwiththeskipconnection. ItthenbackpropagatesthroughtheAttentionblock,getsrescaledby
Layernorm,andfinallyaddedwiththeskipconnection. Letσ2 bethegradientvariancebackpropagatingfromthenth
g,n
layer,andσ2 bethegradientvarianceaftertheentiremodelofN layers.
gmodel
FortheAttentionblock,letσ2 bethegradientbackpropagatingfromtheblock. ThenforlongsequencelengthLwe
have-
gattn,n −1
d2σ2σ2 σ2
σ2 = o v ∗ gout,n(1+(L 1)rl )
gattn,n −1 L(1 p) − gout,n
−
d2σ2σ2 rl σ2
o v ∗ gout,l∗ gout,n
≈ (1 p)
−
σ2 isthenrescaledbytheLayernormtogiveσ2 . AsLayernormscalesgradientbytheinverseoftheinput
vag rat itn a, nn c− e1 σ2 ,whichfromthesectionabove,weg kat ntn- olay werno ir sm, an p− p1 roximatelyσ2 =C (n 1). Then
xin,n −1 xin,n −1 3 ∗ −
σ2 =C rl σ2
gattn,n −1 1 ∗ gout,n∗ gout,n
C rl σ2
σ2 = 1 ∗ gout,n∗ gout,n
gattn-layernorm,n −1 σ x2
in,n −1
C rl σ2
1 ∗ gout,n∗ gout,n
≈ C (n 1)
3
∗ −
Therefore,thefinalgradientσ2 afteradditionwiththeskipconnectionis
gattn-layer,n −1
C rl
σ2 =(1+ 1 ∗ gout,n )σ2
gattn-layer,n −1 C
3
(n 1) gout,n
∗ −
63TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Similarly,wecangetσ2 fortheffnblock.Thentogetthegradientbackpropagatedthroughtheentirelayerσ2 ,
wehave,
gffn-layer,n −1 gout,n −1
C
σ2 =(1+ 2 )σ2
gffn-layer,n −1 C
3
(n 1) gout,n
∗ −
σ2 =(1+ C 1 ∗r gl out,n )(1+ C 2 )σ2
gout,n −1 C
3
(n 1) C
3
(n 1) gout,n
∗ − ∗ −
σ2 (1+ C 1 ∗r gl out,n + C 2 )σ2
gout,n −1 ≈ C
3
(n 1) C
3
(n 1) gout,n
∗ − ∗ −
C rl +C
=(1+ 1 ∗ gout,n 2 )σ2
C (n 1) gout,n
3
∗ −
C rl +C
=(1+ 1 ∗ gout,n 2 )σ2
(C rl +C ) (n 1) gout,n
1 ∗ xin,n 2 ∗ −
C
=(1+ gpre,n )σ2
n 1 gout,n
−
Whereweignorehigherordertermsforlargen,anddefineC gpre,n = C C1 1∗ ∗r rgl xlo iu nt ,, nn ++ CC 22.
SinceC >0,wewillwitnessanincreaseingradientgoingbackward,andthisincreaseisinverselyproportionaltothe
gpre,n
currentlayern,matchingwithempiricallyobservedgrowth(Figure2).
LetC = C2 =0.15betheminimumvalueofC ,andC = C1+C2 =6.5bethemaximum. Then
gpre,min C1+C2 gpre,n gpre,max C2
theaboveequationisboundedby:
C C
(1+ gpre,min )σ2 σ2 (1+ gpre,min )σ2
n 1 gout,n ≤ gout,n −1 ≤ n 1 gout,max
− −
ApplyingtheaboveequationrepeatedlyuntilthefinallayerN,thisrecurrencecanbeapproximatelysolvedbytreating
σ2 asacontinuousfunctionofn,takinglogarithmofbothsides,andintegrating. Thisgivesthefollowingsolutionfor
gout,n
σ2 :
gout,n
N Cgpre,min N Cgpre,max
σ2 ( ) σ2 σ2 ( )
gout,N ∗ n ≤ gout,n ≤ gout,N ∗ n
Ifthecorrelationrl quicklyreachesastableconstantmaximumvaluerl (approximatelyequaltobutslightlylessthan
gout,n gmax
rl (AppendixF)),C 1,andwegetexactlyhyperbolicgrowthasshownbelow:
xmax gpre ≈
N
σ2 =σ2 ( )
gout,n gout,N ∗ n
ThegradientvariancewillincreasehyberbolicallywithnumberoflayersN whilegoingbackwards.
Discussion: Thishastheeffectthatmuchlowerlearningrateisrequiredfortheentiremodel,becausethegradientsnear
theinputlayersaremuchhigher,slowingdownlearningandmakingthemodelunstable.
G.2 VanillaPost-LN
G.2.1 FORWARDPASS
TheforwardpassofPost-LNistriviallyalways1atinitialization,becausetheskipconnectiondoesnotcrosstheLayerNorm.
64TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
G.2.2 BACKWARDPASS
FollowingananalysissimilartothatforPre-LN,weget
1+C
σ2 = 2 σ2
gffn-layer,n −1 1+C
1
∗r xl
out,n −1
gout,n
1+C rl
σ2 = 1 ∗ gout,nσ2
gattn-layer,n −1 1+C
2
gout,n
σ2 = 1+C 1 ∗r gl out,n 1+C 2 σ2
gout,n −1 1+C
2
∗ 1+C
1
∗r xl
out,n −1
∗ gout,n
1+C rl
= 1 ∗ gout,n σ2
1+C rl gout,n
1 ∗ xout,n −1
LetC =
1+C1∗r gl
out,n . AswediscussinAppendixF,thecorrelationsbothquicklyreachamaximumstablevalue. But
5,n 1+C1∗r xl
out,n−1
ther gl out,n’smaximumvaluer gl
max
isslightlydifferentthanr xl max. LetC 5 = 11 ++ CC 11 ∗∗ rr xlgl m ma ax x, thenC 5 canbeeithergreateror
smallerthan1. Hence,weget
σ2 =C σ2
gattn-layer,n −1 5,n gout,n
N
(cid:89)
= C σ2
5,i gout,N
i=n
≈C 5(N −n)σ g2
out,N
σ g2
attn-layer,n −1
=C 5(N −n)σ g2
out,N
(3)
ThisshowsthatgradientvarianceofPost-LNwilldecrease/increaseexponentiallywithnumberoflayersN whilegoing
backwards. EvenveryslightlydifferentvalueofC from1,suchas0.96,willcausea2000xfallingradientafter200layers.
5
Discussion: ThisshowswhyPost-LNtransformerismuchmoredifficulttotrainfordeepermodelsthanPre-LN.While
forPre-LNthebackwardsgradientincreaseshyber-bolicallytoamaximumofN,inPost-LNthegradientcanincreaseor
decreaseexponentially,stoppingthemodelfromconverging.
G.3 DeepScaleLMPre-LN
G.3.1 FORWARDPASS
InDeepScaleLM,theweightinitializationarechosenspecificallysothatσ2 andσ2 arebothequalto1foralllayers,by
xattn xffn
iterativelycalculatingrl asdetailedinAppendixN.Also,theembeddingsareinitializedsothatσ2 isalso1. Hence,
xin xin
σ2 =λ2 σ2 +β2 σ2
layer ∗ skip ∗ block
=λ2+β2 =1
Hencetheforwardpassvarianceremains1throughoutthemodel.
G.3.2 BACKWARDPASS
FortheFFN-block,wehaveσ2 =σ2 =1,asperequationsinTable2ofthemainpaper.
xin,n −1 xout,n −1
65TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
SimilartoVanilla-PreLN,wearriveat
C rl σ2
σ2 = 1 ∗ gout,n∗ gout,n
gattn-layernorm,n −1 σ x2
in,n −1
Here,σ2 =1asshownabove,andsinceweightsareinitializedsothatC1 rl =1. LetC = r gl out,n :
xin,n −1 ∗ xin 6,n r xl out,n−1
rl
σ2 = gout,n σ2
gattn-layernorm,n −1 r xl
in,n−1
∗ gout,n
=C σ2
6,n ∗ gout,n
Therefore,assumingnocovariancebetweenblockgradientsandskipconnection(whichwillbetrueatinitialization),the
finalgradientσ2 afteradditionwiththeskipconnectionis
gattn-layer,n −1
σ2 =λ2σ2 +β2σ2
gattn-layer,n −1 gout,n gattn-layernorm,n −1
=λ2σ2 +β2C σ2
gout,n 6,n gout,n
=(λ2+β2C ) σ2
6,n ∗ gout,n
C 1
=(1+ 6,n − ) σ2
N ∗ gout,n
SimilarlyfortheFFNlayer,σ2 =σ2 ,asσ2 =σ2 =1.
gffn-layer,n −1 gout,n xin,n −1 xout,n −1
Hence,
C 1
σ2 =(1+ 6,n − ) σ2 ,
gout,n −1 N ∗ gout,n
N
σ2 =(cid:89) (1+ C 6,n −1 ) σ2 ,
gout,1 N ∗ gout,N
i=1
N
(cid:89) (1+ C 6 −1 ) σ2 ,
≈ N ∗ gout,N
i=1
(1+
C
6
−1 )N −1
σ2 ,
≈ N ∗ gout,N
=eC6−1 ∗σ g2
out,N
σ2
≈ gout,N
,whereweapplied(1 k)N e k,andC 1.
− N ≈ − 6 ≈
Discussion: HenceforDeepScaleLM,thebackwardvarianceofgradientremainsconstant(boundedbyaconstant)across
alllayers.
G.4 DeepScaleLMPost-LN
G.4.1 FORWARDPASS
SameasvanillaPost-LN,thiswillremainpreservedat1.
66TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
G.4.2 BACKWARDPASS
FollowingananalysissimilartothatforVanillaPost-LN,weget
σ2 =σ2
gffn-layer,n −1 gout,n
σ2 =(λ2 1+β2 C rl )σ2
gattn-layer,n −1 ∗ ∗ 1 ∗ gout,n gout,n
rl
=(λ2+β2 gout,n )σ2
∗ rl gout,n
xin,n
rl
σ2 =(λ2+β2 gout,n)σ2
gout,n −1 ∗ r xl
in,n
gout,n
SimilartoPre-LN,weusethemaximumvalueofthesecorrelations,andassumeC =1. Weget
6
rl
σ2 =(λ2+β2 gmax)σ2
gout,n −1 ∗ r xl
max
gout,n
=(λ2+β2C )σ2
6 gout,n
(λ2+β2)σ2
≈ gout,n
=σ2
gout,n
HenceforDeepScaleLM,thebackwardvarianceofgradientremainsconstantacrossalllayers.
Discussion: SimilartoDeepScale-LMPre-LN,theassumptionC =1isnotrequired,andyieldsthesameconstantbound
6
ifwedonotassumeittobe1.
G.5 DeepScaleLM(Simplified)Pre-LN
G.5.1 FORWARDPASS
For simplified DeepScaleLM, the initialization for the FFN block does not change, so its output remains 1 same as
DeepScaleLM.FortheAttentionblock,wechangeditsinitializationtomimicthatoftheFFNblock. Wewillshowthat
initially,simplifiedDeepScaleLM’sforwardpassisbounded.
rl
σ2 =1asDeepScaleLM,σ2 = xin. Therefore,theoutputvarianceafterlayernwillbe
xffn xattn 2
σ2 =λ2 σ2 +β2 σ2
xattn-skip,n ∗ xlayer,n−1 ∗ xattn
2 1
=(1 ) σ2 + rl
− N ∗ xlayer,n−1 N ∗ xin
SimilarlyaftertheFFNblock,theoutputskipwillbe-
σ2 =λ2 σ2 +β2 σ2
xlayer,n ∗ xattn-skip,n ∗ xffn
2 2 1 2
=(1 ) ((1 ) σ2 + rl )+ 1
− N ∗ − N ∗ xlayer,n−1 N ∗ xin N ∗
2 2 1 2
=(1 )2 σ2 +(1 ) rl +
− N ∗ xlayer,n−1 − N ∗ N ∗ xin N
Ascorrelationcoefficientrl 1,weget,
xin ≤
2 2 1 2
σ2 (1 )2 σ2 +(1 ) 1+
xlayer,n ≤ − N ∗ xlayer,n−1 − N ∗ N ∗ N
67TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
2 3 2
=(1 )2 σ2 +
− N ∗ xlayer,n−1 N − N2
2 3
(1 )2 σ2 +
≤ − N ∗ xlayer,n−1 N
ApplyingtheaboverecurrenceequationN times,weget
N
2 3 (cid:88) 2
σ2 (1 )2N σ2 + (1 )2i
xlayer,N ≤ − N ∗ xlayer,0 N ∗ − N
i=0
2 3 1 (1 2)2N
=(1 )2N σ2 + − − N
− N ∗ xlayer,0 N ∗ 1 (1 2)2
− − N
Sinceλ2+β2 =1andβ2issmallforlargeN.Wecanrewritetheaboveequationscompletelyintermsofβ asfollows
3 1 (1 β2)2N
σ2 =(1 β2)2N σ2 + β2 − − (4)
xlayer,N − ∗ xlayer,0 2 ∗ 1 (1 β2)2
− −
3
(1 β2)2N σ2 + (1 (1 β2)2N) (5)
≈ − ∗ xlayer,0 4 − −
ForlargeN,weknow(1 k)N e k. Sotheabovebecomes-
− N ≈ −
3 1 e 4
σ x2
layer,N
≈e −4 ∗σ x2
layer,0
+
N ∗
4− −
4
N − N2
3 1 e 4
≤e −4 ∗σ x2
layer,0
+
N ∗
−
4
−
N
3
=e 4 1+ (1 e 4)
− −
∗ 4 ∗ −
3 1
= +
4 4e4
ThisgivesusanupperboundontheoutputvarianceafterN layers. Bysettingrl =0insteadof1intheequationabove,
xin
andproceedingsimilarly,wecanalsoarriveatalowerboundof 1 + 1 .
2 2e4
1 1 3 1
+ σ2 + (6)
2 2e4 ≤ xlayer,N ≤ 4 4e4
Discussion Informally,thisisbecausetheattentionblockoutputvariancewillbebetween0and0.5,andffnblockoutput
always1. Becauseofourλ,β scaling,theoutputwillslowlyconvergetobeinbetweenthetwooutputs.
Notethattheabovederivationassumesnocorrelationbetweentheblockoutputandtheskipconnection. Aswementioned
inourmainpaper,wedoobservecorrelationbetweentheinputandtheoutput. Assuch,theoretically,aftereveryblock,
(cid:113)
thevarianceσ2 canincreasebyσ2 + σ2 . Thiswillcausethefinaloutputvariancetoincreasebyfactorsof
xlayer,n xblock xlayer,n
2 √N. Inpracticehowever,weobservetheoutputvariancestonotgrowtoolarge.
∗
G.5.2 BACKWARDPASS
SimilartoDeepScaleLMPre-LN,wearriveat
C rl σ2
σ2 = 1 ∗ gout,n∗ gout,n
gattn-layernorm,n −1 σ x2
in,n −1
68TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
0.5 C
∗ 6 σ2
≈ σ2 ∗ gout,n
xin,n −1
σ2 =λ2σ2 +β2σ2
gattn-layer,n −1 gout,n gattn-layernorm,n −1
0.5 C
=(λ2+β2 ∗ 6 ) σ2
∗ σ2 ∗ gout,n
xin,n −1
2 0.5 C
=(1+ ( ∗ 6 1)) σ2
N ∗ σ2 − ∗ gout,n
xin,n −1
Similarly,fortheFFNlayer,weget
2 1
σ2 =(1+ ( 1)) σ2
gffn-layer,n −1 N ∗ σ x2
in,n −1
− ∗ gout,n
Multiplyingthese,weget
2 0.5 C 2 1
σ2 =(1+ ( ∗ 6 1)) (1+ ( 1)) σ2
gout,n −1 N ∗ σ x2
in,n −1
− ∗ N ∗ σ x2
in,n −1
− ∗ gout,n
2 0.5 C 1
(1+ ( ∗ 6 + 2)) σ2
≈ N ∗ σ2 σ2 − ∗ gout,n
xin,n −1 xin,n −1
As0.5 σ2 ,weget 4 ( C6 + 2 4) 2C +2. Hence,onapplyingtheaboverecurrenceNtimes,
weget≤ xin,n −1 − ≤ σ x2 in,n−1 σ x2 in,n−1 − ≤ 6
e −4 ∗σ g2
out,N
≤σ g2
out,n −1
≤e2C6+2 ∗σ g2
out,N
Hence,weshowthatevenforsimplifiedDeepScaleLMPre-LN,themaximumrelativeincrease/fallingradientvarianceis
boundedacrosslayers.
Discussion: Theabovederivationswillalsobevalidifthereiscorrelationintheinput. Correlationwillcauseσ2
toincrease,effectivelydecreasingthebackpropagatedgradientthroughtheblocktodecrease(asLayernormwillscx ain l, en −by1
inverseofσ2 ). However,eveninthatcase,ourgradientwillstillbeboundedbytheabovelower-bound.
xin,n −1
Intuitively,asthegradientcanflowfreelythroughtheskipconnection,hence,σ2 λ4 σ2 ,whichwhenapplied
N times,yieldsσ2 e 4 σ2
gout,n −1 ≥ ∗ gout,n
gout,1 ≥ − ∗ gout,N
G.6 DeepScaleLM(Simplified)Post-LN
G.6.1 FORWARDPASS
TheforwardpassvarianceforPost-LNistriviallybounded.
G.6.2 BACKWARDPASS
FollowingananalysissimilartothatforDeepScaleLMPost-LN,weget
λ2+0.5 β2 rl
σ2 = ∗ ∗ gout,n σ2
gout,n −1 λ2+0.5 ∗β2 ∗r xl
in,n
gout,n
1+ 2(0.5rl 1)
= N gout,n− σ2
1+ 2(0.5rl 1) gout,n
N xin,n−
69TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Applyingtaylorexpansion,weget,
2
σ2 (1+ ((0.5rl 1) (0.5rl 1)))σ2
gout,n −1 ≈ N gout,n− − xin,n− gout,n
1
=(1+ (rl rl ))σ2
N gout,n− xin,n gout,n
Theaboveequationcanberewrittenintermsofβ asfollows
β2
σ2 =(1+ (rl rl ))σ2 (7)
gout,n −1 2 gout,n− xin,n gout,n
As 2 (rl rl ) 2,applyingtheaboverecurrenceN timesweget
− ≤ gout,n− xin,n ≤
e 2 σ2 σ2 e2 σ2
− ∗ gout,N ≤ gout,n −1 ≤ ∗ gout,N
Discussion: Theabovederivationsassumenocorrelationintheinput,andhenceisonlycorrectatinitialization. However,
ifthereiscorrelationbetweentheblockoutputandskipconnection(r ),thelayernormwillcauseσ2 tobedown-scaled
byafactorof1+ 2
√∗
Nrx,wherecissomeconstant,asopposedto1+x N2 above. However,iftherego iu st,n a− ls1 ocorrelationinthe
gradientsoftheblockandskipconnection(r ),thenumeratorintheequationsaboveforσ2 willalsobeincreased,by
afactorof1+ 2 ∗rg. Henceifthecorrelatiog nsamongthegradientsandamongtheoutpg uo tut, an r− e1 similar,theabovebounds
√N
willremain. Ifβ2 issetas 1 ,thenevenifinputcorrelationsexist,thebackwardgradientwillbebounded,followinga
N2
similarderivationasabove. However,weconjecturethatthisdecreasestheabilityofthetransformerlayerstomodifythe
skipconnectiontoostrongly,decreasingthe“expressivity”ofthemodel. ThisissimilartotheapproachofDSInit,whichwe
showinourmainpaperdoesindeeddecreasemodelperformance.
H ModelMomentFigures
H.1 ValidityofTheoreticalPredictionsandExplodingGradientsevenafterTraining
Whentrainingavanilla64-layerLanguageModel,weobservedrepeatedgradientexplosionscausingmodeldivergence.
Also, our 64-layer model initially performed worse than a smaller 48-layer model. We observed that (1) The norm of
themodeloutputwasincreasinggoingforwards(2)Thebackpropagatedgradientwasincreasinggoingbackward. These
issuesweremorepronouncedinthedeeper64-layermodelcomparedtothe48-layermodel. Weappliedourformulaeto
understandtheseinstabilities.
InFigure8,wecanseetheobservedgrowthintheoutputofa48-layerPreLNmodelafter100ktrainingsteps. Theobserved
growthacross48layersremainsveryclosetoourpredictedvalues. Weobserveasimilartrendforthebackwardpassofour
64-layerPreLNmodelafteritwastrainedfor150ksteps. Figure7showstheobservedgradientexplosionhowstheobserved
gradientexplosionvs. ourhyperbolicgrowthestimation. Interestingly,ourtheoreticalestimatesholdapproximatelyeven
afterthemodelshavebeentrainedforalargenumberofsteps. Themodelstaysintheregimeitisinitializedwith(this
normality of trained neural network weights have also been observed in other works, such as (Dettmers et al., 2023)),
highlightingtheimportanceofcorrectinitialization.
H.2 ImportanceofResidualScaling
Anyvalueforλorβ suchthatλ2+β2 =1issufficienttopreservetheoutput(similarto(Liuetal.,2020b))andgradients
atinitialization. However,weobservethatforhighβ,astrainingprogresses,thegradientstartstovanish,asshownin
Figure9. ThisisbecauseCov(residual,block)isnolongerzero,whichcausestheforwardoutputtogrowacrosslayerand
thegradienttovanish.
Onthecontrarywhenwechooseβ2 = 1,gradientisconservedevenafter50kstepsoftrainingasshowninFigure10.
N
70TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Backward(Empirical)
Backward(Theoretical)
20 Forward(Empirical) − 40 Forward(Theoretical)
25
− 30
−30 20
35 10
−
0 10 20 30 40 50 60
LayerNumber-> 0 10 20 30 40 50
Figure7: Gradientexplodesbackwardfor64-layerpre-LN,
LayerNumber->
increasinghyperbolicallywithnumberoflayersN,after150k Figure8: Lineargrowthintheforwardpassfora48-layer
trainingsteps. Ourtheoreticalmodelsstillhold-themodel 1024-dPreLNmodelaftertrainingfor100ksteps.Ourtheo-
neverescapestheregimeitisinitializedin. reticalmodelsstillholdwell.
·10−5
8 Backward(Empirical) Backward(Empirical)
Backward(Theoretical)
6 0.1
4
0.05
2
0 0
0 20 40 60 80 100 0 20 40 60 80 100
LayerNumber-> LayerNumber->
Figure 9: Gradient vanishes for DeepScaleLM, decreasing Figure10:GradientdoesnotvanishforDeepScaleLM,using
exponentiallywithlayersN,usingfixedλ2 =0.9andβ2 = fixedλ2 =1− 1 andβ2 = 1,after50ktrainingsteps.
N N
0.1,after50ktrainingsteps.
I OtherModalities: VisionandSpeech
I.1 VisionTransformers
Applyingourmethodtovisiontransformers(foreg. ViT(Dosovitskiyetal.,2021)orDeiT(Touvronetal.,2021a))will
only require handling the input embeddings section Appendix B.1 - For ViT, this is a simple linear projection. Given
normalizedimageinputs,ourLinearsectionAppendixB.2providesformulaetocalculatethevarianceandcorrelationofthe
embeddingswhichareinputtothemodel.
WeempiricallyverifiedthatforimagesfromImageNet,theembeddingsafterthelinearprojectiondoindeedfollowthe
normal distribution, with an R2 of 0.95. Furthermore, normalizing images to have approximately unit variance, given
(cid:113)
linearweightsinitializedby 1,theoutputvariancewasobservedas1.02(within2%error). WhileweusedZipf’slawto
d
estimateinputembeddingcorrelationfortext,thiscouldsimplybeempiricallymeasuredforvisionaftertheembedding
layer–wemeasuredthistobe0.46usingthecodeprovidedbyBeyeretal.(2022).
Usingthismeasuredvalueofinputcorrelation,wecanapplyourDSLMmethodtoViT.AsweshowinFigure11,our
methodsuccessfullycontrolsboththeforwardandbackwardmomentsfortheViTmodelwith100soflayers.
I.2 TransformersforSpeechModality
SimilartoVision,applyingourmethodtotransformersforspeechmodality(foreg. Fairseq(Ottetal.,2019)alsoonly
requireshandlingtheinputembeddingssectionAppendixB.1,andcanbereplacedbytheempiricallyobservedvalues. The
inputvariancewasobservedas2.2,andcorrelationas0.29.
AdditionalResultsonTransformersforSpeechModalityformoreLanguagePairs Wealsoconductsomepreliminary
experimentscomparingourmethodDSLMwithbaselinemodelsonmorelanguagepairs. Fortheseexperiments,welimited
themaximumsourcesequencelengthto1024duetocomputelimitations. Examplesinbothtrainingandtestinglongerthan
thislengthwerediscarded. TheresultsareshowninTable22.
71
>-)goL(tneidarGfoecnairaV
>-tneidarGfoecnairaV
>-tuptuOledoMfoecnairaV
>-tneidarGfoecnairaVTransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
2 0.12
ForwardViT(Empirical)
BackwardViT(Empirical) 0.1
1.5
0.08
1 0.06
0.04
0.5
0.02
0 0
0 50 100 150
LayerNumber->
Figure11:DeepScaleLM:ThevariancesremainconservedforbothbackwardandforwardsforViT,usingImageNetdata,aftereven192
transformerlayers
Table22:Comparisonofourmethodtobaselinemodelsonmorelanguagepairs.
Model Language ModelSize BLEUScore
N –N /d
enc dec
Baseline English Spanish 12–6/256(31.1M) 21.61
→
DSLM English Spanish 48–24/128(28.4M) 23.03
→
Baseline English French 12–6/256(31.1M) 23.74
→
DSLM English French 48–24/128(28.4M) 26.30
→
J StatisticalSignificance
J.1 ErrorBarsforPre-TrainingExperiments
Inourinitialexperiments,weobservedverylittlevariationinperformanceacrossdifferentruns–weconjecturethatthe
modelistrainedonalargeenoughnumberoftokensfordifferencesininitialization/dataseedtonotmatter. Weprovide
meanandstandarderrorforthe12L-1024DPost-LNandDSLMmodelsfromTable4below:
Table23:Standarderroracrossrunsforpre-training.
Model Mean StandardError
Post-LNBaseline 14.33 0.14
DSLM 15.56 0.08
Asthevariationwassosmall,andduetocomputelimitations,wedidnotrunmultiplerunsforotherexperimentsthereafter.
WealsoreportedthebestscoreforBaselinePost-LN,andtheworstscoreforDSLMforthe12L-1024DmodelsTable4for
aconservativecomparison.
J.2 StatisticalSignificanceforFine-tuningExperiments
Mean and standard errors for all downstream fine-tuning experiments were reported in Table 6. The differences are
statisticallysignificantatp<5%foralldatasetsexceptQQP.
K Compute
K.1 Theoreticalcompute
Table24providestheexactcomputeforthemodelsreportedinTable4. WefollowthecodeprovidedbyElectra(Clark
etal.,2020)tocalculatetheeachmodel’scompute(FLOPs). Weobservethatupto200layers,theextracomputeiswithin
72
>-tuptuOledoMfoecnairaV
>-tneidarGfoecnairaVTransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
6 7%oftheoriginalshallowmodel.
−
Table24:Modelcomputewithincreasingdepth(keepingNd2constant).
Layers(N) HiddenDim(d) Params Compute(Flops) %Extra
12 1024 185M 1.06e20 -
48 512 168M 1.03e20 -2.5%
192 256 160M 1.12e20 6.3%
784 128 156M 1.38e20 30.6%
24 1024 336M 1.92e20 -
96 512 319M 1.96e20 2.3%
384 128 311M 2.19e20 14.5%
K.2 WallClocktimes
Wealsocomparedwallclocktimeoverheads,andfoundthemtonotbetoolarge. Forexample,the48-layer-512-dmodel
hasonly9.8%overheadinwallclocktimecomparedto12-layer-1024-dmodel. Evenwhenlargernumberoflayers,suchas
96-layer-512-d,theoverheadisonly14.9%comparedto24-layer-1024-dmodel. Profilingrevealedmajorityoftheoverhead
wasduetoextralatencyofaddedGPUkernellaunches. Hence,approachessuchascudaGraphs(whichbatcheskernel
launchestogether)orgraphcompilationtechniquesmaydecreasethisoverheadfurther.
Thisoverheadwilldecreasethebiggertheoriginalmodelsize,andbecomemuchsmaller. Forexample,fora5Bparams
modelwith24-Layers-4096d(areasonableshapeincontemporarymodels,forexample,LLaMA7Bhas32L-4096D)has
muchlesscomputeoverhead-only6.6%overheadat96layers,and13.6%overheadat192layers.
Despitethiswall-clocktimeoverhead,duetolargeperformancegainsfromincreasingdepth,the160Mparams192-Lmodel
fromTable4outperformsthevanilla336MBERT-large24-Lmodelwith2xmoreparams,evenatequalwalltimes.
Furthermore,alargefractionoftheperformanceimprovementsmentionedhappenwhenincreasingthenumberofmodel
layersby4x-andasshownabove,thewallclocktimeoverheadisminimal. Makingstandardmodels4xmoredeepto
50 100layers,willprovidealargefractionofperformancegainswithoutmuchoverhead.
−
L DiscussionofRelativeStrength
InEquation4,wediscussedthatthebackwardrecurrenceequationforPreLNcanbewrittenas
3
σ2 (1 β2)2N σ2 + (1 (1 β2)2N)
xlayer,N ≈ − ∗ xlayer,0 4 − −
Replacingβ2 = k andusing(1+ k )N =ekN1−α,weget
Nα Nα
3
σ2 e2cN1−α σ2 + (1 e2cN1−α )
xlayer,N ≈ ∗ xlayer,0 4 −
3 3
=e2cN1−α (σ2 )+
∗ xlayer,0 − 4 4
Hence,thefallingradientforβ2 = k is (ekN1−α ).
Nα O
SimilarlyforPostLN,wecanuseEquation7
β2
σ2 =(1+ (rl rl ))σ2
gout,n −1 2 gout,n− xin,n gout,n
(1 β2) σ2 σ2 (1+β2) σ2
− ∗ gout,N ≤ gout,n −1 ≤ ∗ gout,N
73TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Hence,forNlayers,thegradientfall/growthisagain (e kN1−α ).
±
O
M DiscussionofApproximationsandAssumptions
M.1 IllustrativeApproximationsofFullFormulaeinMainPaper
SomevalueslistedinTable1areapproximations/illustrativesimplificationsoftheirfullclosedformsinAppendixDand
AppendixB.Wediscussallofthesebelow.
• For ReLU forward correlation, we used a simple polynomial regression of the closed form formula. This simple
regressionisaremarkablygoodfit,asshowninfigureFigure12,andcanbereproducedinoursupplementary.
• For layernorm, we ignored the factor of 1 compared to d, or 1/d compared to 1, assuming large enough hidden
dimensiond.
• ForSHAwithoutV,weusedthefinalsimplifiedformulaeforσ2 andoutputcorrelationfromAppendixB.8. Forthe
xout
gradient,wefurthersimplifiedtheformulaeinAppendixB.8,assumingL L 1.
≈ −
1
1 Inputcorrelation
Inputcorrelation FFNblockcorrelation
0.8 ReLUoutputcorrelation 1+r+(1 1)r2
0.8 π 2 2−π 0.7r+0.3r2
0.6
0.6
0.4
0.4
0.2
0.2
0
0 0.2 0.4 0.6 0.8 1
0
0 0.2 0.4 0.6 0.8 1 InputCorrelation->
InputCorrelation-> Figure 13: Approximation of the FFN forward correlation
Figure 12: Approximation of the Relu forward correlation formula, without dropout. Dropout will reduce the above
formula correlationby1−p.
Furthermore,theformulaeprovidedinTable2areapproximateversionsofthefullformulaeprovidedinAppendixD.In
Table2,weappliedasimilarapproximationasdoneinTable1forReLU,fromthefullformulainAppendixDforoutput
correlation. Thispolynomialapproximationisalsoaverygoodfit,asshowninFigure13,andcanbereproducedinour
supplementary.
OurexactformulaeforblocksandcomponentsalsoaccountforIIDcases-ascanbeverifiedbyoursimulations,inwhich
wedocovercasesIIDinputswithexactly0correlation,asnotedinCorrl columninTable21. Inthesimplifiedformulae,
xin
andinDeepScaleLMinitializationandmodel,wesimplifiedourformulaesothattheyonlyremainaccuratefornon-IID
inputs. Thiswasbecauseofthreeconsiderations:
1. InNLPdomain,mosttextwillinevitablybenon-IIDduetorepeatedcommonwords. Thiswasencounteredinallour
experiments.
2. InVisiondomain,forViTinparticular,therewillbecorrelationamongpixelintensitiesacrosspatchembeddings,as
discussedincommonresponsesection.
3. InSpeechdomain,similartotext,mostspeechwillinevitablybenon-IIDduetorepeatedcommonsounds.
4. Lastly,evenifthereisexactly0correlationininput,theveryfirstattentionlayerandthefirstFFNlayerinparticular,
willaddcorrelationstotheoutput,ensuringoursimplifiedformulaeholdreasonablyaccurately.
74
>-noitalerroCtuptuO
>-noitalerroCtuptuOTransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
M.2 AssumptionsandApproximationsinDerivations
• Exceptforattention,softmaxandLayerNormallotherderivationsoftransformercomponents–Embeddings,FFN,
ReLU/GeLU,Dropout,FFNBlockarefullyexact,assumingonlynormaldistributionofinputs,weightsandgradients.
Wejustifythisnormalityassumptionbelow:
1. Inputs: Astheembeddingsarelookuptablesoftoken-ids,andembeddingweightsareinitializedfromNormal
distributioninXavier,theinputstothetransformerarenormallydistributed.
2. Gradients: AsthemodeloutputsareNormal,thesoftmaxoftheclassificationheadresultsinaLog-Normal
distributionforprobabilitiesp,asshowninAppendixB.7. Sincethecross-entropylossis log(p),weexpectthe
−
loss(andhencethefinalgradientbeingback-propagated)beinglog(Log-Normaldistribution),tobeaNormal
distribution. Wealsoverifythisempiricallybycheckingthenormalityofthebackpropagatedgradientstothe
deepesttransformerlayer,andthegradientsmatchthebest-fitNormaldistributionwithanR2of0.999,showing
thatthegradientsareindeedNormallydistributed.
3. Weights: WeightsareinitializedfromNormaldistributioninXavier,andarehenceNormal.
• Forattention,softmaxandLayerNorm,weassumethesequencelengthLandthehiddendimensiondarelarge.
• Forembeddings,weassumedZipf’slawtocalculateinitialinputcorrelationintokens,aswellasassumeduniform
distributionforsegmentlengthsfornextsentencepredictiontaskofBERT.Notethatthisassumptionisnotstrictly
required,andcanalsobeempiricallyobservedandgivenasinputtoourmethod.
N DeepScaleLMPseudocode
## Define constants of DeepScaleLM
λ2 = 1− 2 ; β2 = 2
N N (cid:113)
σ2 = 1−p ; σ2 = 1 ; σ2 = 1 ∗ 1−p
e 3 qk d f d 2
## Scale skip connection and block output
def add_skip(x, f(x)):
return λ ∗ x + β ∗ f(x)
## Stable initialization of weights
def init(w):
if w is ['ffn', 'v_proj', 'out_proj']:
nn.init.normal_(w, gain = σ )
f
elif w is ['q_proj', 'k_proj']:
nn.init.normal_(w, gain = σ )
qk
elif w is ['embd']:
nn.init.normal_(w, gain = σ )
e
Figure14:Pseudo-codeforsimplifiedversionofourDeepScaleLMmethod.
75TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
## Define constants for scaling residual and output
λ2 = 1− 2 ; β2 = 2
## DefineNconstantsNfor embedding and FFN block
(cid:113)
σ2 = 1−p ; σ2 = 1 ∗ 1−p
e 3 f d 2
## Scale skip connection and block output
def add_skip(x, f(x)):
return λ ∗ x + β ∗ f(x)
## Find layerwise input correlation upto N layers
def corr_input_layerwise(r, N):
r = []
N
for i in range(N):
r = λ2 . r + β2(1−p)
(1−(rl )2)0.5 rl cos−1(rl )
r = λ2 . r + β2(1−p)(rl + xin − xin xin )
xin π π
r .append(r)
N
return r
N
## Define constants for attention block
(cid:114)
σ2 = 1 ∗ 1−p; σ2 = 1 ; r=rl
l,o d rxl, in
n
qk d xin
where rl,n = corr_input_layerwise(r, N)[n]
xin
## Stable initialization of weights
def dslm_init(w, l):
if w is ['ffn']:
nn.init.normal_(w, gain = σ )
f
elif w is ['v_proj', 'out_proj']:
nn.init.normal_(w, gain = σ )
l,o
elif w is ['q_proj', 'k_proj']:
nn.init.normal_(w, gain = σ )
qk
elif w is ['embd']:
nn.init.normal_(w, gain = σ )
e
Figure15:Pseudo-codeforourproposedmethodDeepScaleLM:Wescaletheblockoutputandtheskipconnectionbeforeadding,and
keeptrackofcorrelationacrosslayers.Weappropriatelyinitializetheweights.(N:numoflayers,d:modelhiddendimension,p:dropout
probability,rl iscalculatedbasedonexpressionsprovidedinsubsectionB.1.)
xin
O Hyper-parameters
BERTPretraining WeusedMegatron-LM’sdefaultBertWordPieceLowerCasetokenizer,withtheoriginalBERTlower-
casedvocab,andwithtrainablepositionembeddings. Thesamehyper-parameters(includingLRschedule,warmup)were
usedforallmodels,andLRsearchovertherangebelowwasperformedforallmodels. Thefinalbestmodelsalwayshad
optimalLRwithintherangeandnotattheboundaryoftheLRrangeforallofourexperiments. Detailedhyper-paramsare
providedinTable25.
76TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
Table25:TrainingHyper-Parameters.Weusealloriginalhyper-parametersofBERT,exceptforlearning-rate(LR).
Parameters Values
Optimizer Adam
β ,β 0.9,0.999
1 2
EffectiveBatchSize 256
Drop-out(p) 0.1
SequenceLength 256
TrainIters 100,000
NumGPUs 8
Learningrate [1,3,5,7,10]*10 4
−
Schedule Linear
LRDecayIterations 98%
Warmupsteps 1%
MinLR 1 10 5
−
∗
Gradientclipping 1.0
BatchSize/GPU 2
GradAccumSteps 16
ReproducibleLongerPre-trainingandFinetuning Ourcodereleasedinthesupplementaryprovidesexactscriptsfor
bothpre-trainingandallfine-tuning. Weusedalloriginal/officialhyper-paramsofBERT,exceptLRwasincreasedfor
DSLMasmentionedpreviously.
VisionViTTraining WeusedViT-SBaselinefrom(Beyeretal.,2022)forImageNet-1kalongwithitsdefaulthyper-
parameters.ItusesanMLPhead,aGlobalAvgPoolandafixed2Dsin-cospositionembeddings.Thesamehyper-parameters
wereusedforallthemodels. Detailedhyper-parametersareprovidedinTable26
Table26:TrainingHyper-ParametersforViTTraining.Weusealloriginalhyper-parametersof(Beyeretal.,2022),exceptforlearning-rate
LR.
Parameters Values
Optimizer Adam
β ,β 0.9,0.999
1 2
WeightDecay 10 4
−
EffectiveBatchSize 1024
Drop-out(p) 0.0
PatchSize 16
TrainingImageSize 224x224
EvaluationImageSize 224x224
TrainEpochs [90,300]
NumGPUs 8
Learningrate [1,2,3.5,4]*10 3
−
Schedule Linear
LRDecaySchedule Cosine
Warmupsteps 10000
MinLR 0.0
Gradientclipping 1.0
BatchSize/GPU 128
Augmentation RandAug(n=2,mag=10)+MixUp(p=0.2)
SpeechFairseqTraining Table27providesthehyperparametersusedtotraintheSpeechtranslationmodels,following
thoseofofficialfairseq. Thesamehyper-parameterswereusedforallthemodels. WereporttheBLEUbyaveragingthe
77TransformersGetStable:AnEnd-to-EndSignalPropagationTheoryforLanguageModels
weightsofthelast10checkpointsattheendoftraining.
Table27:TrainingHyper-Parametersforspeech-to-texttranslation.Weusealloriginalhyper-parametersinFairseq,exceptforeffective
batchsizeandlearning-rate(LR).
Parameters Values
Optimizer Adam
β ,β 0.9,0.999
1 2
SourcetokensperBatch [30k,40k]
Drop-out(p) 0.1
TextSequenceLength 1024
SpeechSequenceLength 6000
TrainIters [66k,100k]
NumGPUs 1
Learningrate [3,5]*10 4,[1,2,3,4]*10 3
− −
Schedule InverseSquare-root
Warmupsteps 20%
Gradientclipping 10.0
BatchSize/GPU [52,80]
GradAccumSteps 8,16
78