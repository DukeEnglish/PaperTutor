Trust AI Regulation? Discerning users are vital to build trust and effective AI
regulation
Zainab Alalawi1, Paolo Bova1, Theodor Cimpeanu2, Alessandro Di Stefano1,
Manh Hong Duong3, Elias Fern´andez Domingos4,5, The Anh Han1,∗,
Marcus Krellner2, Bianca Ogbo1, Simon T. Powers6, and Filippo Zimmaro7,8
1 SchoolComputing,EngineeringandDigitalTechnologies,TeessideUniversity
2 SchoolofMathematicsandStatistics,UniversityofStAndrews
3 SchoolofMathematics,UniversityofBirmingham
4 MachineLearningGroup,Universit´elibredeBruxelles
5 AILab,VrijeUniversiteitBrussel
6 SchoolofComputing,EngineeringandtheBuiltEnvironment,EdinburghNapierUniversity
7 DepartmentofMathematics,UniversityofBologna
8 DepartmentofComputerScience,UniversityofPisa
⋆ Correspondingauthor: TheAnhHan(T.Han@tees.ac.uk)
ABSTRACT
There is general agreement that some form of regulation is necessary both for AI creators to be incentivised
to develop trustworthy systems, and for users to actually trust those systems. But there is much debate about
whatformtheseregulationsshouldtakeandhowtheyshouldbeimplemented. Mostworkinthisareahasbeen
qualitative, andhasnotbeenabletomakeformalpredictions. Here, weproposethatevolutionarygametheory
can be used to quantitatively model the dilemmas faced by users, AI creators, and regulators, and provide
insights into the possible effects of different regulatory regimes. We show that creating trustworthy AI and
user trust requires regulators to be incentivised to regulate effectively. We demonstrate the effectiveness of two
mechanisms that can achieve this. The first is where governments can recognise and reward regulators that do
a good job. In that case, if the AI system is not too risky for users then some level of trustworthy development
and user trust evolves. We then consider an alternative solution, where users can condition their trust decision
on the effectiveness of the regulators. This leads to effective regulation, and consequently the development of
trustworthy AI and user trust, provided that the cost of implementing regulations is not too high. Our findings
highlight the importance of considering the effect of different regulatory regimes from an evolutionary game
theoretic perspective.
4202
raM
41
]IA.sc[
1v01590.3042:viXra2
FIG. 1: Core features. The figure schematically illustrates the core features of the three-population model of
AI governance. Users can either trust (T) or not trust (N) the AI system, in which case they do not adopt the
system and get zero benefit. Creators can either defect by creating unsafe AI products (D) or cooperate by
creating safe ones (C), which entails additional costs. Applying regulations (C) also comes with some costs,
while punishing defecting creators requires further costs.
I. INTRODUCTION
Debates are taking place across the world about what kind of regulation should apply to the development of
artificial intelligence (AI) systems. Governments want trustworthy AI systems to be developed, and for users
to actually trust these systems – this is exemplified by the aims of the EU AI act [1]. Regulation is typically
assumed to be the way to achieve this [2]. Economic history supports this assumption; safety critical systems
suchasautomotivevehiclesandmedicaldevicesareindeedregulatedinthisway. Butitislessclearwhatforms
the regulations should take and who should implement them. See, for example, proposals in the EU compared
to the USA on the amount of restrictions that should be placed on AI creators [3, 4]. Moreover, there is the
key question of who should create and enforce the regulations? Should the regulations be public or private [5]?
That is, should they be created by government bodies, or by industrial organisations and accrediting bodies
suchastheIEEE?Andoncetheyhavebeencreated,whowillenforcethem? Enforcementcouldinvolveactions
such as auditing source code, training runs, training data, and monitoring for consumer complaints. This could
be done by government sector organisations, but it could also be done by third-party auditors [6].
To help governments choose the most suitable kind of regulatory framework, we need to be able to predict
the effects of different regulatory systems. Most of the discourse around this at the moment is qualitative and
does not lead to formal predictions [7–10]. This limits the ability of governments, technology companies, and
citizens to foresee what the effects of different regulatory systems might be [11]. There is a small amount of
literature on AI race modelling [12–14], including from the perspective of (evolutionary) game theory [15–18],
but this has not considered how different regulatory mechanisms influence both user trust and the compliance
of companies with AI safety considerations. To address this, we propose that evolutionary game theory [19]
can be used to formally model the effects of different regulatory systems in terms of their incentives on tech
companies, end users, and regulators.
We present a framework for formalising the strategic interaction between users, AI system creators, and
regulators as a game (Fig. 1). This game captures three key decisions that these actors face:
1. Users: do they trust and hence use an AI system or not?
2. AI creators: do they follow a safety optimal development path in compliance with regulations, or do
they pursue a competitive development path that violates regulations in a race to the market?
3. Regulators: dotheyinvestinmonitoringAIsystemcreatorsandenforcingtheregulationseffectively, or
do they cut corners to save the costs of doing this?3
Thishighlightsthedilemmasfacingusers,creators,andregulators. UserscanbenefitfromusinganAIsystem,
but also run the risk that the system may not act in their best interest, i.e. may not be trustworthy [20]. This
follows from the fact that AI creators are themselves in competition with each other, as highlighted by the
current “AI race” to develop artificial general intelligence [8, 12, 21–23]. Consequently, we cannot assume that
creatorswillalwaysactinthebestinterestsoftheirusersbycomplyingwithregulationsanddevelopingsystems
worthy of user trust. Finally, regulators themselves may be self-interested. This may occur when governments
delegate the enforcement of regulations to other actors, such as private audit firms [6, 9, 24]. This kind of
delegation may reduce government costs, but it also introduces a principal agent problem [25]: regulators are
themselves agents with their own profit maximising goals.
Toanalysethemodel,weusethemethodsofevolutionarygametheory. Evolutionarygametheoryisbasedon
the idea that agents can learn behaviours that benefit them from social learning, i.e. by copying the behaviour
of other agents in their population that are doing better than themselves. This avoids the need to assume
that the agents are fully rational and have complete information. In our model, we consider three populations
corresponding to the three actors: users, creators, and regulators.
Our analysis demonstrates how incentives for regulators are important. Governments desire that all creators
producetrustworthyAIsystems, andalluserstrustthesesystems. Suchastatecannotbereachedifregulators
that do their job properly cannot be distinguished from regulators that cut corners. This holds regardless of
the severity of punishment for defecting creators.
We consider two possible institutional solutions to this problem. First, we show that if governments can
provide rewards to regulators that do a good job, and use of the AI system is not too risky for users, then some
level of trustworthy development and trust by users occurs. We then consider an alternative solution, where
users may condition their trust decision on the effectiveness of the regulators, for example, where information
about the past performance of the regulators is available. This leads to effective regulation, and consequently
the development of trustworthy AI and user trust, provided that the cost of implementing regulations is not
too high.
II. MODELS AND METHODS
A. Three population model of AI governance
WecreateabaselinemodelofanAIdevelopmentecosystem[2]. Itissimilartocurrentmodelsofregulations
forinformationsystems,e.g. theGeneralDataProtectionRegulations(GDPR)andtheAIActoftheEuropean
Union. Themodelinvolvesthreepopulationsrepresentingthethreeactorsintheecosystem: AIusers,AIsystem
creators, and regulators. In each population, individuals can choose different options (also called strategies).
A user can decide to trust (T) or not (N) an AI system: This is a combination of trust in the regulator and
the creator. The creator either complies (C) with the rules set out by the regulators or not (D). The regulator
chooses whether to create rules and enforce compliance (C) or not (D).
The individual payoff earned in any one encounter (also called a game) depends on the strategy of the
participatingindividuals. Ineachgame,oneuser,onedeveloper,andoneregulatorparticipate. Iftheusertrusts
and adopts an AI system when both the developer and the regulator cooperate, the user benefits significantly
from AI adoption, denoted by b . However, if AI adoption occurs when the developer defects, the user is
U
affected by unsafe AI, gaining a reduced or even negative benefit, denoted by ϵ×b , where ϵ ∈ [−∞,1]. This
U
parameter,ϵ,alsorepresentsarisk factor thatuserstakewhenadoptingtheAIsystem. Forthesystemcreator,
since it takes more time and effort to comply with the precautionary requirements, we assume that playing C
requiresanextracostc ,comparedtoplayingD(thecostforthisisnormalisedto0). Regardingtheregulators,
P
they earn the benefit b when the user trusts and adopts AI. Since it is costly to create rules and development
R
technologies to capture unsafe development, we assume that playing C requires a extra cost c , compared to
R
playing D (the cost for this is normalised to 0). The model strategies and parameters are summarised in Table
I.
As shown in our analysis below, in this system, regulators are always better off not cooperating, and trust is
rarely encouraged. Thus, below we consider two extended models that might enable investment of high quality
regulation, safe development, and users’ trust.
1. First, we consider that the cooperative regulator is rewarded an amount of b if they catch defective
fo
creators (when users trust and adopt AI). The payoff matrix is summarised in Table II.
2. Second, we assume that regulators’ reputation is publicly available and users can act conditionally on
whether the regulators’ reputation is good or not; see Table III. Namely, the CT strategy means users do
not trust when the regulator’s reputation is bad.
Inthegeneralcase, thisleadstoafrequency-dependentdynamicsineachofthethreepopulations, wherethe
dynamicalprocessofstrategyadoptionassumesthatthoseindividualsthataremorefitaremoreoftenimitated
by their peers, an adaptive scheme akin to social learning (see Methods below).4
TABLE I: AI Governance model (User Us., Creator Cr and Regulator Re).
Parameters'description Symbol
Users trust (T) or not (N) trust an AI system – this is a combination of trust T, N
in regulators and trust in creators
Creators comply with the rules (C) or not (D) as set out by the Regulators C, D
Regulators create rules and ensure compliance (C) or not (D) C, D
Benefit users get from trust and adopt the AI system b
U
Fraction of user benefit when creators play D, where ε in [-∞,1], also referred ε
to as the (inverse) risk factor users take when adopting an AI
Benefit the creator gets from selling the product b
P
AdditionalcostofcreatingsafeAI(thecostofcreatingunsafeAIisnormalised c
P
to 0)
Funding for regulators (which is only generated upon AI adoption) b
R
The cost of developing rules and unsafe capture technologies (the cost of not c
R
doing this is normalised to 0)
The cost of institutional punishment v
The impact of institutional punishment u
Strategies Payoffs
Us. Cr Re User Creator Regulator
T C C b b −c b −c
U P P R R
T C D b b −c b
U P P R
T D C εb b −u b −c −v
U P R R
T D D εb b b
U P R
N C C 0 −c −c
P R
N C D 0 −c 0
P
N D C 0 0 -c
R
N D D 0 0 0
TABLE II: AI Governance extended model (User Us., Creator Cr and Regulator Re), where regulators are
rewarded for capturing unsafe creators (when users trust and adopt).
Strategies Payoffs
Us. Cr Re User Creator Regulator
T C C b b −c b −c
U P P R R
T C D b b −c b
U P P R
T D C εb b −u b −c −v+b
U P R R fo
T D D εb b b
U P R
N C C 0 −c −c
P R
N C D 0 −c 0
P
N D C 0 0 -c
R
N D D 0 0 0
B. Methods
1. Stochastic dynamics for finite populations
The baseline model. We consider three different well-mixed populations of Users (U), Creators (C) and
Regulators (R) of sizes, respectively N , N and N . Let x be the fraction of users that trust the AI system.
U C R
Let y and z be respectively the fraction of Creators and Regulators that cooperate. Each game involves an
individualrandomlydrawnfromeachpopulation. Thefitnessthatauser, regulatorandcreatorobtainsineach5
FIG. 2: Reward for regulators. The figure shows the changes to the original model. Regulators gain extra
reward for capturing unsafe development.
FIG. 3: Conditional trust. The figure shows how the state of regulation changes the behaviour of users,
which impacts the whole system.
game is respectively given by
fU =yzPU +(1−y)zPU +y(1−z)PU +(1−y)(1−z)PU , (1)
X∈{T,N} XCC XDC XCD XDD
fC =xzPC +(1−x)zPC +x(1−z)PC +(1−x)(1−z)PC , (2)
Y∈{C,D} TYC NYC TYD NYD
fR =xyPR +(1−x)zPR +x(1−y)PR +(1−x)(1−y)PR . (3)
Z∈{C,D} TCZ NCZ TDZ NDZ6
TABLE III: AI Governance extended model—conditional trust (User Us., Creator Cr and Regulator Re). In
this model, regulators’ reputation are assumed to be publicly available before the game, and users can act
conditionally on it: CT does not trust if regulator has a bad reputation.
Strategies Payoffs
Us. Cr Re User Creator Regulator
CT C C b b −c b −c
U P P R R
CT C D 0 −c 0
P
CT D C εb b −u b −c −v+b
U P R R fo
CT D D 0 0 0
N C C 0 −c −c
P R
N C D 0 −c 0
P
N D C 0 0 -c
R
N D D 0 0 0
We now derive the fitness/average payoffs for each model. First, for the baseline model, from (1) and the
payoff table I, we calculate explicitly the difference of the fitness between two strategies in users:
fU −fU =yz(PU −PU )+(1−y)z(PU −PU )+y(1−z)(PU −PU )+(1−y)(1−z)(PU −PU )
T N TCC NCC TDC TDC TCD NCD TDD NDD
=yzb +(1−y)z(εb )+y(1−z)b +(1−y)(1−z)(εb )
U U U U
(cid:16) (cid:17)
=b yz+ε(1−y)z+y(1−z)+ε(1−y)(1−z)
U
=b (y+ε(1−y)). (4)
U
Similarly, the difference of the fitness between two strategies in creators is
fC −fC =xz(PC −PC )+(1−x)z(PC −PC )+x(1−z)(PC −PC )+(1−x)(1−z)(PC −PC )
C D TCC TDC NCC NDC TCD TDD NCD NDD
(cid:16) (cid:17)
=xz b −c −(b −u) +(1−x)z(−c )+x(1−z)(b −c −b )+(1−x)(1−z)(−c )
P P P P P P P P
=xz(u−c )+(1−x)z(−c )+x(1−z)(−c )+(1−x)(1−z)(−c )
P P P P
=−c +uxz. (5)
P
Finally, the difference of the fitness between two strategies in regulators is
fR−fR =xy(PR −PR )+(1−x)y(PR −PR )+x(1−y)(PR −PR )+(1−x)(1−y)(PR −PR )
C D TCC TCD NCC NCD TDC TDD NDC NDD
(cid:16) (cid:17) (cid:16) (cid:17)
=xy (b −c )−b +(1−x)y(−c )+x(1−y) b −c −v−b +(1−x)(1−y)(−c )
R R R R R R R R
=xy(−c )+(1−x)y(−c )+x(1−y)(−c −v)+(1−x)(1−y)(−c )
R R R R
=−c −x(1−y)v. (6)
R
We notice that fR−fR is always negative. Thus, it is always better off for the regulator to defect, that is, to
C D
provide regulator rules.
Now,fortheextendedmodelwheretherewardb isprovidedtoregulatorsforcapturingunsafecreatorsand
fo
if they catch defective creators (when users adopt only). Then only the payoff difference fR−fR is changed to
C D
fR−fR =−c +x(1−y)(b −v). (7)
C D R fo
Finally, when users can make a conditional decision based on the regulator’s reputation, we have
fU −fU =b z(y+ϵ(1−y))
T N U
fC −fC =−c +uxz (8)
C D P
fR−fR =−c +b x+(b −v)x(1−y)
C D R R fo
For a finite population setting, at each time step, a randomly selected individual A, with fitness f , may
A
adopt a different strategy by imitating a randomly chosen individual B from the same population (with fitness
f ) with probability given by the Fermi distribution
B
p=[1+e−β(fB−fA)]−1,7
where β ≥ 0 is the strength of selection. β = 0 corresponds to neutral drift where imitation decisions are
random, while for large β →∞, the imitation decision becomes increasingly deterministic.
In the absence of mutations or exploration, the end states of evolution are inevitably monomorphic: once
such a state is reached, it cannot be escaped through imitation. We thus further assume that with a certain
mutation probability, an agent switches randomly to a different strategy without imitating another agent. In
thelimitofsmallmutationrates,thedynamicswillproceedwith,atmost,twostrategiesinthepopulation,such
that the behavioural dynamics can be conveniently described by a Markov chain, where each state represents a
monomorphic population, whereas the transition probabilities are given by the fixation probability of a single
mutant[26–28]. TheresultingMarkovchainhasastationarydistribution, whichcharacterisestheaveragetime
the population spends in each of these monomorphic end states.
Now, the probability to change the number k of agents using strategy A by ± one in each time step can be
written as (Z is the population size) [29]
Z−k k (cid:104) (cid:105)−1
T±(k)= 1+e∓β[fA(k)−fB(k)] . (9)
Z Z
The fixation probability of a single mutant with a strategy A in a population of (Z−1) agents using B is given
by [27, 29]
 −1
Z (cid:88)−1 (cid:89)i T−(j)
ρ B,A =1+ T+(j) . (10)
i=1 j=1
The transition matrix Λ corresponding to the set of {1,...,s} strategies is given by:
s
Λ = ρ ji and Λ =1− (cid:88) Λ . (11)
ij,j̸=i 3 ii ij
j=1,j̸=i
Fixation probability ρ denotes the likelihood that a population transitions from a state i to a different state
ij
j when a mutant of one of the populations adopts an alternate strategy s. The fixation probability is divided
by the number of populations (3) representing the interaction of three players at a time [30, 31].
2. Population dynamics for infinite populations: The multi-population replicator dynamics
Inthissection, werecalltheframeworkofthereplicatordynamicsformulti-populations[32,33]. Todescribe
thedynamics,weconsiderasetofmdifferentpopulations(missomepositiveinteger),whichareinfinitelylarge
and well-mixed. Each population i, i = 1,...m, consists of n (n is some positive integer) different strategies
i i
(types). Let x ,1 ≤ i ≤ m,1 ≤ j ≤ n , be the frequency of the strategy j in the population i. We denote by
ij i
x = (x )ni , which is the collection of all strategies in the population i, and x = (x ,...,x ), which is the
i ij j=1 1 m
collection of all strategies in all populations.
Foreachi∈{1,...,m}andj ∈{1,...,n },letf (x)bethefitness(reproductiverate)ofthestrategyj inthe
i ij
population i. This fitness is obtained when the strategy j interacts with all other strategies in all populations;
thus, it depends on all the strategies in the populations. The average fitness of the population i is defined by
f¯(x)=(cid:88)ni
x f (x).
i ij ij
j=1
The multi-population replicator dynamics is then given by
x˙ =x (f (x)−f¯(x)), 1≤i≤m, 1≤j ≤n . (12)
ij ij ij i i
This is in general an ODE system of
(cid:80)m
n equations. Noting, however that since
(cid:80)ni
x = 1 for all
i=1 i j=1 ij
i=1,...,m, we can reduce the above system to a system of
(cid:80)m
n −m equations.
i=1 i
Now we focus on the case when there are two strategies in each population (which is the case for our models
of AI governance and trust in the present paper), that is n =2 for all i=1,...,n . Let η be the frequency of
i i i
the first strategy in the population i, i = 1,...,m (thus 1−η will be the frequency of the second strategy in
i
the population i), let η = (η ,...,η ). Let f (η) and f (η) be the fitness of the first and second strategy in
1 m 1i 2i
the population i. Since
f¯(η)=η f (η)+(1−η )f (η),
i i 1i i 2i8
we have
f (η)−f¯(η)=f (η)−(η f (η)+(1−η )f (η))=(1−η )(f (η)−f (η)).
1i i 1i i 1i i 2i i 1i 2i
Thus we obtain the following system of equations
η˙ =η (1−η )(f (η)−f (η)), i=1,...,m. (13)
i i i 1i 2i
This is a system of m coupled nonlinear ordinary differential equations for m variables.
In the subsequent sections, we employ (13) to our models of AI governance trust, where the fitness are
computed from the payoff matrix constructed in the models, see Tables I-II-III, where we assume that the
payoffs are directly translated to the biological fitnesses (infinite strength of selection).
III. EQUILIBRIUM ANALYSIS IN INFINITE POPULATIONS
1. The baseline model
We consider three different well-mixed populations of Users (U), Creators (C) and Regulators (R). Let x be
the frequency of users that trust the AI system. Let y and z be respectively the frequency of Creators and
Regulators that cooperate.
The replicator dynamics is
x˙ =x(1−x)(fU −fU)=x(1−x)b (y+ε(1−y)), x(0)=x (14a)
T N U 0
y˙ =y(1−y)(fC −fC)=y(1−y)(−c +uxz), y(0)=y (14b)
C D P 0
z˙ =z(1−z)(fR−fR)=z(1−z)(−c −x(1−y)v), z(0)=z (14c)
C D R 0
where (x ,y ,z ) ∈ [0,1]3 is the initial data. This is a complex, nonlinear system of three coupled ordinary
0 0 0
differential equations (ODEs) for three variables. In general, the dynamical solutions x(t),y(t),z(t) may show
differentbehaviourdependingonthevaluesoftheparametersandtheinitialdata. Wenowfocusonstationary
solutions (equilibria), which describe the long-time behaviour of the dynamics.
a. Stationarysolutions(equilibriumpoints) Stationarysolutionsof (14)arepoints(x∗,y∗,z∗)∈[0,1]3 that
make the right hand side of (14) vanish. It is clear that the vertices of the unit cube (x,y,z) ∈ {0,1}3 are
stationary solutions (we call them vertex equilibria).
There may exist other, non-vertex, stationary solutions depending on the sign of ε and the relation between
c and u.
P
If ε>0, then y+ε(1−y)>0 for all y ∈[0,1]. It implies that there are no other stationary solutions (except
in the very specific case where u=c , in which (x∗,y∗,z∗)∈{0,1}×[0,1]×{0,1} is a stationary solution).
p
If ε<0, and 0<c ≤u, then other, non-vertex, stationary solutions are given by
P
ε
y∗ = , −c +ux∗z∗ =0, z∗ =1,
ε−1 P
which is
c ε
x∗ = P, y∗ = , z∗ =1. (15)
u ε−1
b. Stability Analysis Next, we study the stability of the equilibria. To this end, we define
F(x,y,z)=(F (x,y,z),F (x,y,z),F (x,y,z)), F (x,y,z)=x(1−x)b (y+ε(1−y)),
1 2 3 1 U
F (x,y,z)=y(1−y)(−c +uxz), F (x,y,z)=z(1−z)(−c −x(1−y)v).
2 P 3 R
Then (14) can be reformulated as
x˙ =F (x,y,z), y˙ =F (x,y,z), z˙ =F (x,y,z).
1 2 39
The Jacobian matrix is
∂F1 ∂F1 ∂F1
∂x ∂y ∂z
DF(x,y,z)=∂F2 ∂F2 ∂F2
 ∂x ∂y ∂z 
∂F3 ∂F3 ∂F3
∂x ∂y ∂z
 
b (1−2x)(y+ε(1−y)) b x(1−x)(1−ε) 0
U U
= uy(1−y)z (1−2y)(−c +uxz) uy(1−y)x .
 P 
−z(1−z)(1−y)v xvz(1−z) (1−2z)(−c −x(1−y)v)
R
We recall that an equilibrium (x∗,y∗,z∗) is asymptotically stable if all eigenvalues of DF(x∗,y∗,z∗) have
negative real parts.
c. StabilityAnalysisforvertexequilibria Wenowdeterminethestabilityforvertex-equilibria,(z∗,y∗,z∗)∈
{0,1}3. For these equilibria, DF(x∗,y∗,z∗) is always a diagonal matrix and is explicitly given by
 
b (1−2x∗)(y∗+ε(1−y∗)) 0 0
U
DF(x∗,y∗,z∗)= 0 (1−2y∗)(−c +ux∗z∗) 0 .
 P 
0 0 (1−2z∗)(−c −x∗(1−y∗)v)
R
For instance at the origin (x∗,y∗,z∗)=(0,0,0) we have
 
εb 0 0
U
DF(x,y,z)= 0 −c 0 .
 P 
0 0 −c
R
For another example at (x∗,y∗,z∗)=(1,0,0) then
 
−εb 0 0
U
DF(x,y,z)= 0 −c 0 .
 P 
0 0 −c
R
Since for vertex equilibria, DF(x∗,y∗,z∗) is diagonal and its eigenvalues read
λ =b (1−2x∗)(y∗+ε(1−y∗)), λ =(1−2y∗)(−c +ux∗z∗), λ =(1−2z∗)(−c −x∗(1−y∗)v).
1 U 2 P 3 R
Thus (x∗,y∗,z∗) is stable only if
λ <0, λ <0, λ <0.
1 2 3
Since (−c −x∗(1−y∗)v)<0, it follows that λ <0 iff z∗ =0. For z∗ =0, λ =−c (1−2y∗)<0 iff y∗ =0.
R 3 2 P
Then λ =εb (1−2x∗).
1 U
For ε>0, then λ <0 iff x∗ =1. On the other hand, if ε<0, then λ <0 iff x∗ =0.
1 1
In conclusion, for vertex equilibria, (x∗,y∗,z∗)∈{0,1}3:
• when ε > 0, the equilibrium (x∗,y∗,z∗) = (1,0,0) is stable, thus the users will fully trust (even if the
creators and the regulators defect).
• when ε<0, the origin (x∗,y∗,z∗)=(0,0,0) is stable, both regulators and creators defect and there is no
trust from users.
d. Stability analysis for the non-vertex equilibrium . For the non-vertex equilibrium (15), the Jacobian
matrix is of the form
 0 b Uc uP(1− c up)(1−ε) 0 
DF =−u ε 0 −u ε cP .
 (ε−1)2 (ε−1)2 u 
0 0 c R+ c uP 1−1 εv
Thus it has one positive (recalling that ε<0 in this case) real root, which is the last diagonal entry. Therefore,
the non-vertex equilibrium is not stable.10
2. Extended model: incentive for regulator to coperate
For the extended model, using fR−fR calculated in (7) in (14), the replicator dynamics become
C D
x˙ =x(1−x)(fU −fU)=x(1−x)b (y+ε(1−y)), x(0)=x (16a)
T N U 0
y˙ =y(1−y)(fC −fC)=y(1−y)(−c +uxz), y(0)=y (16b)
C D P 0
z˙ =z(1−z)(fR−fR)=z(1−z)(−c +x(1−y)(b −v)), z(0)=z (16c)
C D R fo 0
where (x ,y ,z )∈[0,1]3 is the initial data. Practically, compared to the baseline model, the only difference is
0 0 0
that we allowed v to be negative. This is again a complex nonlinear coupled system of 3 ordinary differential
equations. Itssolutionsx(t),y(t),z(t)mayexhibitdifferentbehaviourdependingonthevaluesoftheparameters
and the initial data, see Figure 4.
The vertex equilibria in the baseline model are obviously still equilibria for this extended model. In addition,
due to the presence of the incentive b there may be another internal equilibrium
fo
c (1−ε) ε c (b −v)
x∗ = R , y∗ = , z∗ = P fo , (17)
b −v ε−1 u(1−ε)
fo
if ε < 0 and the parameters c ,c ,(b −v) are such that 0 < x∗,y∗,z∗ < 1 (in particular b < v). The
P R fo fo
Jacobian matrix is
 
b (1−2x)(y+ε(1−y)) b x(1−x)(1−ε) 0
U U
DF(x,y,z)= uy(1−y)z (1−2y)(−c +uxz) uy(1−y)x 
 P 
−z(1−z)(1−y)(v−b ) x(v−b )z(1−z) (1−2z)(−c −x(1−y)(v−b )).
fo fo R fo
Wearenowinterestedindeterminingthestabilityforvertexequilibria,(z∗,y∗,z∗)∈{0,1}3. Fortheseequilibria,
DF(x∗,y∗,z∗) is always a diagonal matrix and is explicitly given by
 
b (1−2x∗)(y∗+ε(1−y∗)) 0 0
U
DF(x∗,y∗,z∗)= 0 (1−2y∗)(−c +ux∗z∗) 0 
 P 
0 0 (1−2z∗)(−c −x∗(1−y∗)(v−b )).
R fo
DF(x∗,y∗,z∗) still has three real eigenvalues, corresponding to the diagonal entries:
λ =b (1−2x∗)(y∗+ε(1−y∗)), λ =(1−2y∗)(−c +ux∗z∗), λ =(1−2z∗)(−c −x∗(1−y∗)(v−b )).
1 U 2 P 3 R fo
Thus (x∗,y∗,z∗) is stable only if
λ <0, λ <0, λ <0.
1 2 3
If v > b then (−c −x∗(1−y∗)(v−b ) < 0, the stability for vertex equilibria is the same as in the case
fo R fo
where b =0.
fo
We consider now v <b .
fo
When ε>0 then λ <0 iff x∗ =1, in which case λ and λ reduce to
1 2 3
λ =(1−2y∗)(−c +uz∗),λ =(1−2z∗)(−c −(1−y∗)(v−b ).
2 p 3 R fo
• (y∗,z∗) = (0,0). Then λ = −c < 0, λ = −c −(v−b ). Thus the equilibrium (1,0,0) is stable if
2 P 3 R fo
−c <v−b <0 and unstable otherwise.
R fo
• (y∗,z∗) = (0,1). Then λ = −c +u, λ = c +(v−b ). Thus (1,0,1) is stable only if u < c and
2 P 3 R fo P
c +(v−b )<0 and is unstable otherwise.
R fo
• (y∗,z∗)=(1,0). Then λ =c >0. Thus the equilibrium (1,1,0) is unstable.
2 P
• (y∗,z∗)=(1,1). Then λ =c −u, λ =c >0. Thus (1,1,1) is unstable.
2 P 3 R
When ε<0
• y∗ =0. Then λ =b (1−2x∗)ε. Hence λ <0 for x∗ =0. Then λ =−c <0 and λ =−c (1−2z∗).
1 U 1 2 P 3 R
Hence λ <0 for z∗ =0. In this case, (0,0,0) is a stable equilibrium.
3
• y∗ =1. Then λ =b (1−2x∗). Hence λ <0 for x∗ =1. Then λ =c −uz∗. Thus λ <0 for z∗ =0
1 U 1 2 P 2
and c <u. Thus λ =−c <0. If c <u then (1,1,0) is a stable equilibrium.
P 3 R P11
(a) x =y =z =0.1, ϵ=0.01 (b) x =y =z =0.5, ϵ=0.01
0 0 0 0 0 0
(c) x =y =z =0.1, ϵ=−0.5 (d) x =y =z =0.5, ϵ=−0.5
0 0 0 0 0 0
(e) x =y =z =0.1, ϵ=−1 (f) x =y =z =0.5, ϵ=−1
0 0 0 0 0 0
FIG. 4: Numerical integration of the evolution equation for the extended model (16), describing
the evolution of the density of trusting users x(t), cooperating creators y(t) and cooperating regulators z(t).
In all these simulations: b =4, c =0.5, u=1.5, c =0.5, b −v =1.5. For the initial conditions and the
U P R fo
other parameters, see the captions.
3. Model with conditional trust
For the model with conditional trust, the replicator dynamics reads
x˙ =x(1−x)b z(y+ϵ(1−y)), x(0)=x (18a)
U 0
y˙ =y(1−y)[−c +uxz], y(0)=y (18b)
P 0
z˙ =z(1−z)[−c +b x+(b −v)x(1−y)], z(0)=z (18c)
R R fo 0
The stability analysis for this model is similar to the extended model (incentive for the regulator to cooperate).
Hence we omit it here.12
FIG. 5: Low regulation cost (c =0.5). Conditional trust can lead to full trust, cooperative regulation and
R
safe development. Parameters set to: b =b =b =4, u=1.5, v =0.5, c =0.5, β =0.1,
U R P P
N =N =N =100.
U P R
4. Numerical results for replicator dynamics
Wepresentherenumericalresultsfortheinfinitepopulationsetting. First,Figure4showsdifferentoutcomes
in the evolutionary dynamics when varying initial conditions (fractions of strategists x,y,z in the population).
In figure 4a), the system first reaches trust and regulation (TCC), which leads to profitability for regulators to
stop enforcing safety standards (TCD). In panels 4b) and d) we find a stable limit cycle between regulation
and unsafe development. Users fully trust, but regulators mirror unsafe AI development. Safe AI reduces
regulation, whereas unsafe AI prompts them to reinstate incentives. Thus, we could simplify the equations
(16), taking x(t)=1 constantly and explaining mathematically the existence of the cycle. Qualitatively, when
defecting creators emerge, regulators enforce standards (because b > v+c ), thus it is profitable to comply
fo R
when meeting a defecting creator. Conversely, when cooperating regulators emerge, for creators it is better to
cooperate as well (as u > c ); but, if creators cooperate, the enforcing standards becomes unprofitable and
P
regulators defect. In the absence of regulation, unsafe developers re-emerge and the limit cycle restarts. In 4c),
e) and f), trust is never established in the users’ population, there is no incentive to regulate as there is no
adoption, and creators ignore safety standards - NDD is reached.
IV. STOCHASTIC ANALYSIS
We present here numerical results for the finite population setting (see Methods). Figure 5 shows results
for varying b for both proposed models when the cost of regulation c is sufficiently small. We identify two
fo R
regimes,basedonthesignoftheriskfactorϵ. Weassumethatdefectioninthecreatorpopulation(i.e.,strategies
oftheformxDz)isthemostdeleteriousoutcomeforusers,buttherestillexistsapotentialbenefitwhenthesign
of ϵ is positive. In such cases, we observe the evolution of trust regardless of the success of regulators. Given
low incentives for capturing unsafe developers (b ), regulators fail to adequately enforce safety. Increasing
fo
incentives compels regulators to act, thus giving rise to a mixed outcome between TCC,TCD and TDC (refer
to Figure 7 for an in-depth view of the dominance cycles that govern these states). Whether the population13
FIG. 6: High regulation cost (c =5). Conditional trust is not sufficient to enable safe development and
R
AI adoption when the cost of regulation is high, as this discourages regulators to cooperate. Parameters set
to: b =b =b =4, u=1.5, v =0.5, c =0.5, β =0.1, N =N =N =100
U R P P U P R
evolves to TDD (for low b ) or the mixed state, we note that for this regime, it is always beneficial to adopt
fo
even unsafe AI (see discussion). If ϵ < 0, no trust can evolve, and technology is never adopted. Importantly,
When regulation is cheaply implementable (low c ), conditional trust solves these issues for all incentives and
R
in both risk regimes.
Figure 6 shows results for varying b for different models, when the cost of regulation c is large compared
fo R
to the benefit, e.g., when the technology is highly sophisticated and it is difficult to develop technology to
capture unsafe behaviour. In the regime of low risk ϵ < 0, we observe defection because users do not trust
the technology and the regulators. For very low values of b , there is no incentive to deviate from defection,
fo
and this remains the main regime even increasing the b values. With conditional trust, illustrated in Fig.
fo
6 (b), even in the absence of trust in regulations, other strategies start emerging, due to cyclic dominance
between TCC →TCD →TDD →TDC (see Figure 7). This cycle only emerges if regulators are given a high
enough incentive to enforce safety standards (b ). With a high risk factor ϵ < 0, trust, regulation and safety
fo
never emerge. Interestingly, extending this model to conditional trust, we see a decrease in trust and adoption.
Previously, low risk (ϵ > 0) provided enough of a benefit for users to adopt AI even in the case of rapid, risky
development,yetregulatorspreventthisfromoccurring. Regulationistoocostly,thusregulatorsrestrictaccess
to AI, deeming it unsafe. If we consider conditional trust in such cases, users cannot elect to adopt technology
unless regulators approve of creators, which hinges on the cost of regulation.
V. DISCUSSION
A. Key Takewaways
Our conceptual model of an AI governance ecosystem highlights several key factors relevant to both current
and future regulatory landscapes for AI:
I. AIcompaniesaremotivatedtoimplementsafetyprotocolsonlywhenregulatorybodieshavethecapacity
to enforce compliance,14
FIG. 7: Stationary distribution and transitions amongst the strategies (with conditional trust
and without conditional trust) (b =4). Only arrows where the transition probability is larger than the
R
opposite one, are shown. Dashed lines represent neutral transitions. Parameters set to: b =b =4, u=1.5,
U P
v =0.5, c =0.5, β =0.1. Population sizes: N = N = N =100.
P U P R
II. InscenarioswherethepotentialnegativeconsequencesofunsafeAIdevelopmentaresignificant,usertrust
in the system hinges on the dependability of regulatory authorities,
III. For regulatorybodiesto consistently enforcesafety measures, the costassociated withregulation mustbe
manageable, and their capacity to identify non-compliant AI creators should be high,
IV. Critical users, namely those whose trust in the regulatory system is conditional on its perceived effective-
ness, are essential to enable a stable and long-lasting trust in the system, where creators comply with
regulations and regulators continue to enforce them.
We found that trust can only emerge when regulators have an incentive to regulate. Moreover, the presence
of critical users, that is, users that place their trust conditionally, are able to break the cyclic dynamics that
we observe in Fig. 4a. Figs. 7b and d show how the presence of these users who trust conditionally breaks the
cycle in the dimension of the regulator and creator populations, by making TCC dominant over TCD. That is,
when users trust conditionally, enforcing regulation risk dominates not enforcing it.
B. Implications for AI Governance
WefocusonthepolicyimplicationsofourmainfindingsIIIandIV:highcostsinprovidingeffectiveregulation
pose an obstacle to building trust in future AI systems, and that it is vital to help users discern which systems
have undergone effective regulation.15
Scenarios where regulators face large costs in assessing the risks of AI systems are plausible. Existing AI
evaluators already experience relatively high costs in providing effective regulation for fronteir AI systems.
Foundation models like Open AI’s GPT-4 require large amounts of computational hardware and access to the
top AI talent [22]. Evaluations of these systems are likewise expensive and demand teams with expertise in AI.
Academia and public institutions who wish to assess the risks of new AI systems therefore compete with AI
creators in acquiring these resources.
To overcome these barriers to entry, policy makers should consider (i) investing in building up government
capacity to assess the risks of large AI systems [34], and (ii) subsidising organisations that show promise in
providing services that improve the effectiveness of AI regulation. Governments may therefore wish to take
heed of the UK’s investment in their AI Safety Institute, (author?) [35].
Governments and regulators can benefit from communicating information that allows users to discern which
methods that regulators use are reliable (so that, in turn, they can discern which AI systems are safe and
trustworthy). Doing so requires investing in the expertise needed to follow the recommendations on how to
assesstheriskspresentedbyAIsystems[7]. Thiscouldmeanconductingmachinelearningmodelevaluationsor
other tests as part of an audit or risk assessment of new frontier AI systems [36, 37]. The role of academia and
the commentariat should also not be underestimated here [2]. Academia and commentators can also play a key
role in communicating to users information about the effectiveness of regulations. Our model can be extended
to capture this by including academics and commentators as an additional evolving population of agents.
In the case where the regulator is a public institution with greater powers, they may wish to monitor the
size of training runs for frontier AI systems or to impose fines on companies whose AI systems cause significant
economic harm [38]. The governance of computing hardware (shortened to compute) has also been noted as
a point of potentially high leverage, given that large amounts of physical infrastructure are much easier to
monitor, track, and limit in principle than software, data, or model weights [39].
One more avenue regulators can pursue to help demonstrate to users that trustworthy AI systems will be
developed and deployed safely is to encourage international coordination in the governance of AI systems,
perhapsbyproposingdesignsforinternationalinstitutionstogovernnewAIbreakthroughsorAIsafetyprojects
[40,41]. WorldpowershaveexpressedanawarenessthatAIcouldposeenormousriskswhensigningtheBletchley
Declaration [42], highlighting that international coordination of some form is tractable and desired.
Finally,publicinstitutionswillbenecessarytoensurethatcompaniesandusers,particularlylargeenterprises
that serve a wide number of additional users, can form reliable impressions of the reputation of different
regulators [35]. Model registration is an essential prerequisite for such a reputation system; Users need to know
which regulatory evaluations match which AI systems [43]. Moreover, governments may wish those providing
regulatory services to preregister a set of experiments they plan to run to test the model. Regulators could
later add details about how they dealt with unforeseen findings in their experiments. This would help them
to demonstrate vigilance (i.e. that they have a high likelihood of finding early signs of dangerous capabilities
when they in fact exist), while ensuring that their insights can be shared with other organisations quickly and
safely [34, 44]. Keep in mind that, as our results show, this information is useful whenever we have multiple
organisations evaluating frontier AI systems, including at the international level [41].
C. Limitations and Areas for Future Research
Above, we discussed how decision makers can act on the insights from our results. Now, we turn to the
limitationsofthepresentmodel. Animportantsimplificationwemadewastolimiteachactorinourmodeltoa
smallsetofstrategicchoices. Futureresearchcouldaddresstheselimitationsbyincorporatingpartnerselection
into these models between populations. In a more realistic setting, users would choose which AI system they
want to use. Companies too may choose to relocate to avoid especially burdensome regulation.
We also think it is essential to model more explicitly competition between different regulatory blocs. One
appropriate formalism for achieving this would be to model each regulatory bloc as an island network that
is connected loosely to other islands. These islands of regulation would be in competition with each other
through a process of cultural group selection [45–47]. For example, creators may move to regulatory regimes
more favourable to themselves, while regulators may imitate the regulatory regimes of other groups that are
more successful than themselves.
OurmodelalsodoesnotfullycapturetheracedynamicsbetweenAIcompanies. OpenAI’sinitialcommercial
breakthroughwithChatGPThassparkedinterestinbuildingmorecapablefoundationmodelsamongtheworld’s
leading tech companies [22]. As predicted by other game-theoretic models of tech race dynamics, remaining
at the frontier of AI capabilities appears to take precedence over pursuing AI safety research agendas [12, 15].
It is noteworthy that to date, companies have had relatively limited success making generative AI systems
fundamentally safer.
Scaling AI systems quickly encounters a brisk trade-off with safety research. It allows less time to investigate
and understand the impacts of smaller models, particularly when new algorithms are used to pretrain or fine
tune those models. More talent and computational resources must also be allocated to these large experiments,
allowing fewer resources to pursue research agendas on AI safety. Therefore, we cannot expect even relatively16
safety-mindedAIlabstochoosetheoptimaltrade-off,giventhehugecompetitivedynamicsatplay. Forafurther
discussionofwhatcanbedonetodirectlykerbracedynamics,consultthefollowingreferences[14,21,38,44,48].
While we recognise the limitations of our simpler scenario, we find our approach useful as a tool for thinking
through what assumptions policymakers must make to assume that different regulatory regimes are likely to
achieve their policy objectives.
Furtheravenuesofresearchshouldtestthepredictionsofourmodel,providingreal-worldevidenceofstrategic
interactionsbetweenusers,creators,andregulatorsintheAIdomain. Exploringadditionalincentivestructures
for regulators could further enhance our understanding of effective AI governance. A useful incentive structure
to consider is that AI companies could be mandated to pay for regulatory services that governments consider
essential to demonstrate the safety of the model [6, 9].
Our findings offer valuable insights into the dynamics at play in the AI regulation landscape and highlight
the indispensable role of reliable regulators, incentivised by either governmental rewards or the maintenance of
a prestigious reputation. They highlight the value of taking a game-theoretic approach to formally model the
effects of different kinds of regulatory system on the behaviour of both AI creators and users.
VI. APPENDIX
ACKNOWLEDGEMENT
Thisworkisproducedduringtheworkshop”AIGovernanceModelling”,fundedthroughthegeneroussupport
from the Future of Life institute (TAH).
[1] J.Laux,S.Wachteret al.,“TrustworthyartificialintelligenceandtheEuropeanUnionAIact: Ontheconflationof
trustworthiness and acceptability of risk,” Regulation & Governance, vol. 18, no. 1, pp. 3–32, 2024.
[2] S.T.Powers,O.Linnyketal.,“TheStuffWeSwimin: RegulationAloneWillNotLeadtoJustifiableTrustinAI,”
IEEE Technology and Society Magazine, vol. 42, no. 4, pp. 95–106, 2023.
[3] C. Siegmann and M. Anderljung, “The Brussels Effect and Artificial Intelligence,” Oct. 2022.
[4] T.Baker,“TheExecutiveOrderonSafe,Secure,andTrustworthyAI:DecodingBiden’sAIPolicyRoadmap,”Nov.
2023.
[5] J.Tallberg,E.Ermanetal.,“Theglobalgovernanceofartificialintelligence: Nextstepsforempiricalandnormative
research,” International Studies Review, vol. 25, no. 3, p. viad040, 2023, private vs public regulation.
[6] J. Clark and G. K. Hadfield, “Regulatory Markets for AI Safety,” arXiv, Dec. 2019.
[7] A. Dafoe, “AI Governance: Overview and Theoretical Lenses,” in The Oxford Handbook of AI Governance, J. B.
Bullock, Y.-C. Chen, J. Himmelreich, V. M. Hudson, A. Korinek, M. M. Young, and B. Zhang, Eds. Oxford
University Press, 2023, p. 0.
[8] M.Anderljung,J.Barnhartet al.,“FrontierAIRegulation: ManagingEmergingRiskstoPublicSafety,”Jul.2023.
[9] G. K. Hadfield and J. Clark, “Regulatory Markets: The Future of AI Governance,” Apr. 2023.
[10] J. Alaga and J. Schuett, “Coordinated Pausing: An Evaluation-Based Coordination Scheme for Frontier AI Devel-
opers,” Sep. 2023.
[11] J. Pitt, “Chatsh*t and other conversations (that we should be having, but mostly are not),” IEEE Technology and
Society Magazine, vol. 42, no. 3, pp. 7–13, 2023.
[12] S.Armstrong,N.Bostromet al.,“RacingtothePrecipice: AModelofArtificialIntelligenceDevelopment,”Ai &
Society, vol. 31, no. 2, pp. 201–206, May 2016.
[13] T. LaCroix and A. Mohseni, “The Tragedy of the AI Commons,” Synthese. An International Journal for Episte-
mology, Methodology and Philosophy of Science, vol. 200, no. 4, p. 289, 2022.
[14] M. Jensen, N. Emery-Xu et al., “ Industrial Policy for Advanced AI: Compute Pricing and the Safety Tax ,” 2023.
[15] T. A. Han, L. M. Pereira et al., “ To Regulate or Not: A Social Dynamics Analysis of an Idealised AI Race ,”
Journal of Artificial Intelligence Research, vol. 69, pp. 881–921, Nov. 2020.
[16] T. A. Han, “ Institutional Incentives for the Evolution of Committed Cooperation: Ensuring Participation Is as
ImportantasEnhancingCompliance,” Journal of The Royal Society Interface, vol.19, no.188, p.20220036, 2022.
[17] T. Cimpeanu, F. Santos et al., “Artificial Intelligence Development Races in Heterogeneous Settings,” Scientific
Reports, vol. 12, no. 1, p. 1723, 2022.
[18] P.Bova,A.DiStefanoetal.,“ATaleofTwoRegulatoryMarkets: TheRoleofInstitutionalIncentivesinSupporting
Sustainable Regulatory Markets for Future AI Systems ,” in ALIFE 2023: Ghost in the Machine: Proceedings of
the 2023 Artificial Life Conference. MIT Press, Jul. 2023.
[19] J. Hofbauer and K. Sigmund, Evolutionary games and population dynamics. Cambridge university press, 1998.
[20] T.A.Han,C.Perret,andS.T.Powers,“Whento(ornotto)trustintelligentmachines: Insightsfromanevolutionary
game theory analysis of trust in repeated games,” Cognitive Systems Research, vol. 68, pp. 111–124, 2021.
[21] A. Askell, M. Brundage et al., “The Role of Cooperation in Responsible AI Development,” arXiv, Jul. 2019.
[22] B. Cottier, T. Besiroglu et al., “Who Is Leading in AI? An Analysis of Industry AI Research,” 2024.
[23] T. D. a. C. Grant, “How Microsoft Catapulted to $3 Trillion on the Back of AI,”
https://www.wsj.com/tech/microsoft-closes-above-3-trillion-as-its-big-ai-play-generates-excitement-e37ea2a4,
2024.17
(a) x =y =z =0.1, ϵ=−1.2, b =b =4 (b) x =y =z =0.5, ϵ=−1.2, b =b =4
0 0 0 U R 0 0 0 U R
(c) x =y =z =0.1, ϵ=−0.5, b =b =4 (d) x =y =z =0.5, ϵ=−0.5, b =b =4
0 0 0 U R 0 0 0 U R
(e) x =y =z =0.1, ϵ=−1.2, b =b =1 (f) x =y =z =0.5, ϵ=−1.2, b =b =1
0 0 0 U R 0 0 0 U R
FIG. 8: Numerical integration of the evolution equation for the model with conditional trust (18), describing
the evolution of the density of trusting users x(t), cooperating creators y(t) and cooperating regulators z(t).
In all these simulations: c =0.5, u=1.5, c =0.5, b −v =5. For the initial conditions and the other
P R fo
parameters, see the captions.
[24] P. Cihon, M. J. Kleinaltenkamp et al., “ AI Certification: Advancing Ethical Practice by Reducing Information
Asymmetries ,” IEEE Transactions on Technology and Society, vol. 2, no. 4, pp. 200–209, Dec. 2021.
[25] D. C. North, Institutions, institutional change and economic performance. Cambridge university press, 1990.
[26] L.A.Imhof,D.Fudenberg,andM.A.Nowak,“Evolutionarycyclesofcooperationanddefection,”Proc.Natl.Acad.
Sci. U.S.A., vol. 102, pp. 10797–10800, 2005.
[27] M. A. Nowak, A. Sasaki, C. Taylor, and D. Fudenberg, “Emergence of cooperation and evolutionary stability in
finite populations,” Nature, vol. 428, pp. 646–650, 2004.
[28] E. F. Domingos, F. C. Santos, and T. Lenaerts, “Egttools: Evolutionary game dynamics in python,” Iscience,
vol. 26, no. 4, 2023.
[29] A.Traulsen,M.A.Nowak,andJ.M.Pacheco,“Stochasticdynamicsofinvasionandfixation,”Phys.Rev.E,vol.74,
p. 11909, 2006.
[30] S. Encarnac¸a˜o, F. P. Santos, F. C. Santos, V. Blass, J. M. Pacheco, and J. Portugali, “Paradigm shifts and the
interplay between state, business and civil sectors,” Royal Society open science, vol. 3, no. 12, p. 160753, 2016.
[31] Z.Alalawi,T.A.Han,Y.Zeng,andA.Elragig,“Pathwaystogoodhealthcareservicesandpatientsatisfaction: An
evolutionarygametheoreticalapproach,”inArtificial Life Conference Proceedings. MITPressOneRogersStreet,18
Cambridge, MA 02142-1209, USA journals-info ..., 2019, pp. 135–142.
[32] P.D.Taylor,“Evolutionarilystablestrategieswithtwotypesofplayer,”Journalofappliedprobability,vol.16,no.1,
pp. 76–83, 1979.
[33] J. Bauer, M. Broom, and E. Alonso, “The stabilization of equilibria in evolutionary game dynamics through muta-
tion: mutation limits in evolutionary games,” Proceedings of the Royal Society A, vol. 475, no. 2231, p. 20190355,
2019.
[34] J. Whittlestone and J. Clark, “Why and How Governments Should Monitor AI Development,” arXiv, Aug. 2021.
[35] GOV.UK, “Introducing the AI Safety Institute,” https://www.gov.uk/government/publications/ai-safety-institute-
overview/introducing-the-ai-safety-institute, 2023.
[36] T. Shevlane, S. Farquhar et al., “Model Evaluation for Extreme Risks,” May 2023.
[37] L.KoesslerandJ.Schuett,“RiskAssessmentatAGICompanies: AReviewofPopularRiskAssessmentTechniques
from Other Safety-Critical Industries ,” Jul. 2023.
[38] Y. Shavit, “ What Does It Take to Catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training
via Compute Monitoring ,” Mar. 2023.
[39] G. Sastry, L. Heim, H. Belfield, M. Anderljung, M. Brundage et al., “Computing Power and the Governance of
Artificial Intelligence,” 2024.
[40] R.F.Trager,B.Haracketal.,“InternationalGovernanceofCivilianAI:AJurisdictionalCertificationApproach,”
2023. [Online]. Available: https://dx.doi.org/10.2139/ssrn.4579899
[41] L. Ho, J. Barnhart et al., “International Institutions for Advanced AI,” Jul. 2023.
[42] Gov.uk, “ The Bletchley Declaration by Countries Attending the AI Safety Summit ,”
https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-
declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023, 2023.
[43] G. Hadfield, M.-F. Cu´ellar, and T. O’Reilly, “It’s Time to Create a National Registry for Large AI
Models,” https://carnegieendowment.org/2023/07/12/it-s-time-to-create-national-registry-for-large-ai-models-pub-
90180, 2023.
[44] P. Bova, A. Di Stefano et al., “ Both Eyes Open: Vigilant Incentives Help Regulatory Markets Improve AI Safety
,” Mar. 2023.
[45] K. Binmore, Natural justice. Oxford University Press, 2005.
[46] J. C. van den Bergh and J. M. Gowdy, “A group selection perspective on economic behavior, institutions and
organizations,” Journal of Economic Behavior & Organization, vol. 72, no. 1, pp. 1–20, 2009.
[47] P. Richerson, R. Baldini, A. V. Bell, K. Demps, K. Frost, V. Hillis, S. Mathew, E. K. Newton, N. Naar, L. Newson
et al.,“Culturalgroupselectionplaysanessentialroleinexplaininghumancooperation: Asketchoftheevidence,”
Behavioral and Brain Sciences, vol. 39, p. e30, 2016.
[48] T. A. Han, T. Lenaerts et al., “ Voluntary Safety Commitments Provide an Escape from Over-Regulation in AI
Development ,” Technology in Society, vol. 68, p. 101843, 2022.