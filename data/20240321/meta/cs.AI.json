[
    {
        "title": "On Pretraining Data Diversity for Self-Supervised Learning",
        "authors": "Hasan Abed Al Kader HammoudTuhin DasFabio PizzatiPhilip TorrAdel BibiBernard Ghanem",
        "links": "http://arxiv.org/abs/2403.13808v1",
        "entry_id": "http://arxiv.org/abs/2403.13808v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13808v1",
        "summary": "We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models will be available at\nhttps://github.com/hammoudhasan/DiversitySSL .",
        "updated": "2024-03-20 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13808v1"
    },
    {
        "title": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition",
        "authors": "Ziyu LiuZeyi SunYuhang ZangWei LiPan ZhangXiaoyi DongYuanjun XiongDahua LinJiaqi Wang",
        "links": "http://arxiv.org/abs/2403.13805v1",
        "entry_id": "http://arxiv.org/abs/2403.13805v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13805v1",
        "summary": "CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.",
        "updated": "2024-03-20 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13805v1"
    },
    {
        "title": "ZigMa: Zigzag Mamba Diffusion Model",
        "authors": "Vincent Tao HuStefan Andreas BaumannMing GuiOlga GrebenkovaPingchuan MaJohannes FischerBjorn Ommer",
        "links": "http://arxiv.org/abs/2403.13802v1",
        "entry_id": "http://arxiv.org/abs/2403.13802v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13802v1",
        "summary": "The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$. Code will be released at https://taohu.me/zigma/",
        "updated": "2024-03-20 17:59:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13802v1"
    },
    {
        "title": "Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs",
        "authors": "Yusuke MikamiAndrew MelnikJun MiuraVille Hautamäki",
        "links": "http://arxiv.org/abs/2403.13801v1",
        "entry_id": "http://arxiv.org/abs/2403.13801v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13801v1",
        "summary": "We demonstrate experimental results with LLMs that address robotics action\nplanning problems. Recently, LLMs have been applied in robotics action\nplanning, particularly using a code generation approach that converts complex\nhigh-level instructions into mid-level policy codes. In contrast, our approach\nacquires text descriptions of the task and scene objects, then formulates\naction planning through natural language reasoning, and outputs coordinate\nlevel control commands, thus reducing the necessity for intermediate\nrepresentation code as policies. Our approach is evaluated on a multi-modal\nprompt simulation benchmark, demonstrating that our prompt engineering\nexperiments with natural language reasoning significantly enhance success rates\ncompared to its absence. Furthermore, our approach illustrates the potential\nfor natural language descriptions to transfer robotics skills from known tasks\nto previously unseen tasks.",
        "updated": "2024-03-20 17:58:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13801v1"
    },
    {
        "title": "Reverse Training to Nurse the Reversal Curse",
        "authors": "Olga GolovnevaZeyuan Allen-ZhuJason WestonSainbayar Sukhbaatar",
        "links": "http://arxiv.org/abs/2403.13799v1",
        "entry_id": "http://arxiv.org/abs/2403.13799v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13799v1",
        "summary": "Large language models (LLMs) have a surprising failure: when trained on \"A\nhas a feature B\", they do not generalize to \"B is a feature of A\", which is\ntermed the Reversal Curse. Even when training with trillions of tokens this\nissue still appears due to Zipf's law - hence even if we train on the entire\ninternet. This work proposes an alternative training scheme, called reverse\ntraining, whereby all words are used twice, doubling the amount of available\ntokens. The LLM is trained in both forward and reverse directions by reversing\nthe training strings while preserving (i.e., not reversing) chosen substrings,\nsuch as entities. We show that data-matched reverse-trained models provide\nsuperior performance to standard models on standard tasks, and compute-matched\nreverse-trained models provide far superior performance on reversal tasks,\nhelping resolve the reversal curse issue.",
        "updated": "2024-03-20 17:55:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13799v1"
    }
]