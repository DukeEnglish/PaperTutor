[
    {
        "title": "MotorEase: Automated Detection of Motor Impairment Accessibility Issues in Mobile App UIs",
        "authors": "Arun KrishnavajjalaSM Hasan MansurJustin JoseKevin Moran",
        "links": "http://dx.doi.org/10.1145/3597503.3639167",
        "entry_id": "http://arxiv.org/abs/2403.13690v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13690v1",
        "summary": "Recent research has begun to examine the potential of automatically finding\nand fixing accessibility issues that manifest in software. However, while\nrecent work makes important progress, it has generally been skewed toward\nidentifying issues that affect users with certain disabilities, such as those\nwith visual or hearing impairments. However, there are other groups of users\nwith different types of disabilities that also need software tooling support to\nimprove their experience. As such, this paper aims to automatically identify\naccessibility issues that affect users with motor-impairments.\n  To move toward this goal, this paper introduces a novel approach, called\nMotorEase, capable of identifying accessibility issues in mobile app UIs that\nimpact motor-impaired users. Motor-impaired users often have limited ability to\ninteract with touch-based devices, and instead may make use of a switch or\nother assistive mechanism -- hence UIs must be designed to support both limited\ntouch gestures and the use of assistive devices. MotorEase adapts computer\nvision and text processing techniques to enable a semantic understanding of app\nUI screens, enabling the detection of violations related to four popular,\npreviously unexplored UI design guidelines that support motor-impaired users,\nincluding: (i) visual touch target size, (ii) expanding sections, (iii)\npersisting elements, and (iv) adjacent icon visual distance. We evaluate\nMotorEase on a newly derived benchmark, called MotorCheck, that contains 555\nmanually annotated examples of violations to the above accessibility\nguidelines, across 1599 screens collected from 70 applications via a mobile app\ntesting tool. Our experiments illustrate that MotorEase is able to identify\nviolations with an average accuracy of ~90%, and a false positive rate of less\nthan 9%, outperforming baseline techniques.",
        "updated": "2024-03-20 15:53:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13690v1"
    },
    {
        "title": "Learning User Embeddings from Human Gaze for Personalised Saliency Prediction",
        "authors": "Florian StrohmMihai BâceAndreas Bulling",
        "links": "http://arxiv.org/abs/2403.13653v1",
        "entry_id": "http://arxiv.org/abs/2403.13653v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13653v1",
        "summary": "Reusable embeddings of user behaviour have shown significant performance\nimprovements for the personalised saliency prediction task. However, prior\nworks require explicit user characteristics and preferences as input, which are\noften difficult to obtain. We present a novel method to extract user embeddings\nfrom pairs of natural images and corresponding saliency maps generated from a\nsmall amount of user-specific eye tracking data. At the core of our method is a\nSiamese convolutional neural encoder that learns the user embeddings by\ncontrasting the image and personal saliency map pairs of different users.\nEvaluations on two public saliency datasets show that the generated embeddings\nhave high discriminative power, are effective at refining universal saliency\nmaps to the individual users, and generalise well across users and images.\nFinally, based on our model's ability to encode individual user\ncharacteristics, our work points towards other applications that can benefit\nfrom reusable embeddings of gaze behaviour.",
        "updated": "2024-03-20 14:58:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13653v1"
    },
    {
        "title": "VCounselor: A Psychological Intervention Chat Agent Based on a Knowledge-Enhanced Large Language Model",
        "authors": "H. ZhangZ. QiaoH. WangB. DuanJ. Yin",
        "links": "http://arxiv.org/abs/2403.13553v1",
        "entry_id": "http://arxiv.org/abs/2403.13553v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13553v1",
        "summary": "Conversational artificial intelligence can already independently engage in\nbrief conversations with clients with psychological problems and provide\nevidence-based psychological interventions. The main objective of this study is\nto improve the effectiveness and credibility of the large language model in\npsychological intervention by creating a specialized agent, the VCounselor, to\naddress the limitations observed in popular large language models such as\nChatGPT in domain applications. We achieved this goal by proposing a new\naffective interaction structure and knowledge-enhancement structure. In order\nto evaluate VCounselor, this study compared the general large language model,\nthe fine-tuned large language model, and VCounselor's knowledge-enhanced large\nlanguage model. At the same time, the general large language model and the\nfine-tuned large language model will also be provided with an avatar to compare\nthem as an agent with VCounselor. The comparison results indicated that the\naffective interaction structure and knowledge-enhancement structure of\nVCounselor significantly improved the effectiveness and credibility of the\npsychological intervention, and VCounselor significantly provided positive\ntendencies for clients' emotions. The conclusion of this study strongly\nsupports that VConselor has a significant advantage in providing psychological\nsupport to clients by being able to analyze the patient's problems with\nrelative accuracy and provide professional-level advice that enhances support\nfor clients.",
        "updated": "2024-03-20 12:46:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13553v1"
    },
    {
        "title": "The Tribal Theater Model: Social Regulation for Dynamic User Adaptation in Virtual Interactive Environments",
        "authors": "H. ZhangB. DuanH. WangZ. QiaoJ. Yin",
        "links": "http://arxiv.org/abs/2403.13550v1",
        "entry_id": "http://arxiv.org/abs/2403.13550v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13550v1",
        "summary": "This paper proposes a social regulation model for dynamic adaptation\naccording to user characteristics in virtual interactive environments, namely\nthe tribal theater model. The model focuses on organizational regulation and\nbuilds an interaction scheme with more resilient user performance by improving\nthe subjectivity of the user. This paper discusses the sociological theoretical\nbasis of this model and how it was migrated to an engineering implementation of\na virtual interactive environment. The model defines user interactions within a\nfield that are regulated by a matrix through the allocation of resources. To\nverify the effectiveness of the tribal theater model, we designed an\nexperimental scene using a chatroom as an example. We trained the matrix as an\nAI model using a temporal transformer and compared it with an interaction field\nwith different levels of control. The experimental results showed that the\ntribal theater model can improve users' interactive experience, enhance\nresilient user performance, and effectively complete environmental interaction\ntasks under rule-based interaction.",
        "updated": "2024-03-20 12:40:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13550v1"
    },
    {
        "title": "From One to Many: How Active Robot Swarm Sizes Influence Human Cognitive Processes",
        "authors": "Julian KadukMüge CavdanKnut DrewingHeiko Hamann",
        "links": "http://arxiv.org/abs/2403.13541v1",
        "entry_id": "http://arxiv.org/abs/2403.13541v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13541v1",
        "summary": "In robotics, understanding human interaction with autonomous systems is\ncrucial for enhancing collaborative technologies. We focus on human-swarm\ninteraction (HSI), exploring how differently sized groups of active robots\naffect operators' cognitive and perceptual reactions over different durations.\nWe analyze the impact of different numbers of active robots within a 15-robot\nswarm on operators' time perception, emotional state, flow experience, and task\ndifficulty perception. Our findings indicate that managing multiple active\nrobots when compared to one active robot significantly alters time perception\nand flow experience, leading to a faster passage of time and increased flow.\nMore active robots and extended durations cause increased emotional arousal and\nperceived task difficulty, highlighting the interaction between robot the\nnumber of active robots and human cognitive processes. These insights inform\nthe creation of intuitive human-swarm interfaces and aid in developing swarm\nrobotic systems aligned with human cognitive structures, enhancing human-robot\ncollaboration.",
        "updated": "2024-03-20 12:26:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13541v1"
    }
]