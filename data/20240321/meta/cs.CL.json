[
    {
        "title": "Learning from Models and Data for Visual Grounding",
        "authors": "Ruozhen HePaola Cascante-BonillaZiyan YangAlexander C. BergVicente Ordonez",
        "links": "http://arxiv.org/abs/2403.13804v1",
        "entry_id": "http://arxiv.org/abs/2403.13804v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13804v1",
        "summary": "We introduce SynGround, a novel framework that combines data-driven learning\nand knowledge transfer from various large-scale pretrained models to enhance\nthe visual grounding capabilities of a pretrained vision-and-language model.\nThe knowledge transfer from the models initiates the generation of image\ndescriptions through an image description generator. These descriptions serve\ndual purposes: they act as prompts for synthesizing images through a\ntext-to-image generator, and as queries for synthesizing text, from which\nphrases are extracted using a large language model. Finally, we leverage an\nopen-vocabulary object detector to generate synthetic bounding boxes for the\nsynthetic images and texts. We finetune a pretrained vision-and-language model\non this dataset by optimizing a mask-attention consistency objective that\naligns region annotations with gradient-based model explanations. The resulting\nmodel improves the grounding capabilities of an off-the-shelf\nvision-and-language model. Particularly, SynGround improves the pointing game\naccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on\nRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to\n63.67%.",
        "updated": "2024-03-20 17:59:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13804v1"
    },
    {
        "title": "ZigMa: Zigzag Mamba Diffusion Model",
        "authors": "Vincent Tao HuStefan Andreas BaumannMing GuiOlga GrebenkovaPingchuan MaJohannes FischerBjorn Ommer",
        "links": "http://arxiv.org/abs/2403.13802v1",
        "entry_id": "http://arxiv.org/abs/2403.13802v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13802v1",
        "summary": "The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$. Code will be released at https://taohu.me/zigma/",
        "updated": "2024-03-20 17:59:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13802v1"
    },
    {
        "title": "Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs",
        "authors": "Yusuke MikamiAndrew MelnikJun MiuraVille Hautamäki",
        "links": "http://arxiv.org/abs/2403.13801v1",
        "entry_id": "http://arxiv.org/abs/2403.13801v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13801v1",
        "summary": "We demonstrate experimental results with LLMs that address robotics action\nplanning problems. Recently, LLMs have been applied in robotics action\nplanning, particularly using a code generation approach that converts complex\nhigh-level instructions into mid-level policy codes. In contrast, our approach\nacquires text descriptions of the task and scene objects, then formulates\naction planning through natural language reasoning, and outputs coordinate\nlevel control commands, thus reducing the necessity for intermediate\nrepresentation code as policies. Our approach is evaluated on a multi-modal\nprompt simulation benchmark, demonstrating that our prompt engineering\nexperiments with natural language reasoning significantly enhance success rates\ncompared to its absence. Furthermore, our approach illustrates the potential\nfor natural language descriptions to transfer robotics skills from known tasks\nto previously unseen tasks.",
        "updated": "2024-03-20 17:58:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13801v1"
    },
    {
        "title": "Reverse Training to Nurse the Reversal Curse",
        "authors": "Olga GolovnevaZeyuan Allen-ZhuJason WestonSainbayar Sukhbaatar",
        "links": "http://arxiv.org/abs/2403.13799v1",
        "entry_id": "http://arxiv.org/abs/2403.13799v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13799v1",
        "summary": "Large language models (LLMs) have a surprising failure: when trained on \"A\nhas a feature B\", they do not generalize to \"B is a feature of A\", which is\ntermed the Reversal Curse. Even when training with trillions of tokens this\nissue still appears due to Zipf's law - hence even if we train on the entire\ninternet. This work proposes an alternative training scheme, called reverse\ntraining, whereby all words are used twice, doubling the amount of available\ntokens. The LLM is trained in both forward and reverse directions by reversing\nthe training strings while preserving (i.e., not reversing) chosen substrings,\nsuch as entities. We show that data-matched reverse-trained models provide\nsuperior performance to standard models on standard tasks, and compute-matched\nreverse-trained models provide far superior performance on reversal tasks,\nhelping resolve the reversal curse issue.",
        "updated": "2024-03-20 17:55:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13799v1"
    },
    {
        "title": "Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts",
        "authors": "Guangzeng HanWeisi LiuXiaolei HuangBrian Borsari",
        "links": "http://arxiv.org/abs/2403.13786v1",
        "entry_id": "http://arxiv.org/abs/2403.13786v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13786v1",
        "summary": "Automatic coding patient behaviors is essential to support decision making\nfor psychotherapists during the motivational interviewing (MI), a collaborative\ncommunication intervention approach to address psychiatric issues, such as\nalcohol and drug addiction. While the behavior coding task has rapidly adapted\nmachine learning to predict patient states during the MI sessions, lacking of\ndomain-specific knowledge and overlooking patient-therapist interactions are\nmajor challenges in developing and deploying those models in real practice. To\nencounter those challenges, we introduce the Chain-of-Interaction (CoI)\nprompting method aiming to contextualize large language models (LLMs) for\npsychiatric decision support by the dyadic interactions. The CoI prompting\napproach systematically breaks down the coding task into three key reasoning\nsteps, extract patient engagement, learn therapist question strategies, and\nintegrates dyadic interactions between patients and therapists. This approach\nenables large language models to leverage the coding scheme, patient state, and\ndomain knowledge for patient behavioral coding. Experiments on real-world\ndatasets can prove the effectiveness and flexibility of our prompting method\nwith multiple state-of-the-art LLMs over existing prompting baselines. We have\nconducted extensive ablation analysis and demonstrate the critical role of\ndyadic interactions in applying LLMs for psychotherapy behavior understanding.",
        "updated": "2024-03-20 17:47:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13786v1"
    }
]