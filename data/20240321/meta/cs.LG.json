[
    {
        "title": "On Pretraining Data Diversity for Self-Supervised Learning",
        "authors": "Hasan Abed Al Kader HammoudTuhin DasFabio PizzatiPhilip TorrAdel BibiBernard Ghanem",
        "links": "http://arxiv.org/abs/2403.13808v1",
        "entry_id": "http://arxiv.org/abs/2403.13808v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13808v1",
        "summary": "We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models will be available at\nhttps://github.com/hammoudhasan/DiversitySSL .",
        "updated": "2024-03-20 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13808v1"
    },
    {
        "title": "Editing Massive Concepts in Text-to-Image Diffusion Models",
        "authors": "Tianwei XiongYue WuEnze XieYue WuZhenguo LiXihui Liu",
        "links": "http://arxiv.org/abs/2403.13807v1",
        "entry_id": "http://arxiv.org/abs/2403.13807v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13807v1",
        "summary": "Text-to-image diffusion models suffer from the risk of generating outdated,\ncopyrighted, incorrect, and biased content. While previous methods have\nmitigated the issues on a small scale, it is essential to handle them\nsimultaneously in larger-scale real-world scenarios. We propose a two-stage\nmethod, Editing Massive Concepts In Diffusion Models (EMCID). The first stage\nperforms memory optimization for each individual concept with dual\nself-distillation from text alignment loss and diffusion noise prediction loss.\nThe second stage conducts massive concept editing with multi-layer, closed form\nmodel editing. We further propose a comprehensive benchmark, named ImageNet\nConcept Editing Benchmark (ICEB), for evaluating massive concept editing for\nT2I models with two subtasks, free-form prompts, massive concept categories,\nand extensive evaluation metrics. Extensive experiments conducted on our\nproposed benchmark and previous benchmarks demonstrate the superior scalability\nof EMCID for editing up to 1,000 concepts, providing a practical approach for\nfast adjustment and re-deployment of T2I diffusion models in real-world\napplications.",
        "updated": "2024-03-20 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13807v1"
    },
    {
        "title": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition",
        "authors": "Ziyu LiuZeyi SunYuhang ZangWei LiPan ZhangXiaoyi DongYuanjun XiongDahua LinJiaqi Wang",
        "links": "http://arxiv.org/abs/2403.13805v1",
        "entry_id": "http://arxiv.org/abs/2403.13805v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13805v1",
        "summary": "CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.",
        "updated": "2024-03-20 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13805v1"
    },
    {
        "title": "Learning from Models and Data for Visual Grounding",
        "authors": "Ruozhen HePaola Cascante-BonillaZiyan YangAlexander C. BergVicente Ordonez",
        "links": "http://arxiv.org/abs/2403.13804v1",
        "entry_id": "http://arxiv.org/abs/2403.13804v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13804v1",
        "summary": "We introduce SynGround, a novel framework that combines data-driven learning\nand knowledge transfer from various large-scale pretrained models to enhance\nthe visual grounding capabilities of a pretrained vision-and-language model.\nThe knowledge transfer from the models initiates the generation of image\ndescriptions through an image description generator. These descriptions serve\ndual purposes: they act as prompts for synthesizing images through a\ntext-to-image generator, and as queries for synthesizing text, from which\nphrases are extracted using a large language model. Finally, we leverage an\nopen-vocabulary object detector to generate synthetic bounding boxes for the\nsynthetic images and texts. We finetune a pretrained vision-and-language model\non this dataset by optimizing a mask-attention consistency objective that\naligns region annotations with gradient-based model explanations. The resulting\nmodel improves the grounding capabilities of an off-the-shelf\nvision-and-language model. Particularly, SynGround improves the pointing game\naccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on\nRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to\n63.67%.",
        "updated": "2024-03-20 17:59:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13804v1"
    },
    {
        "title": "ZigMa: Zigzag Mamba Diffusion Model",
        "authors": "Vincent Tao HuStefan Andreas BaumannMing GuiOlga GrebenkovaPingchuan MaJohannes FischerBjorn Ommer",
        "links": "http://arxiv.org/abs/2403.13802v1",
        "entry_id": "http://arxiv.org/abs/2403.13802v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13802v1",
        "summary": "The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$. Code will be released at https://taohu.me/zigma/",
        "updated": "2024-03-20 17:59:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13802v1"
    }
]