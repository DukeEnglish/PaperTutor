[
    {
        "title": "On Pretraining Data Diversity for Self-Supervised Learning",
        "authors": "Hasan Abed Al Kader HammoudTuhin DasFabio PizzatiPhilip TorrAdel BibiBernard Ghanem",
        "links": "http://arxiv.org/abs/2403.13808v1",
        "entry_id": "http://arxiv.org/abs/2403.13808v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13808v1",
        "summary": "We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models will be available at\nhttps://github.com/hammoudhasan/DiversitySSL .",
        "updated": "2024-03-20 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13808v1"
    },
    {
        "title": "Editing Massive Concepts in Text-to-Image Diffusion Models",
        "authors": "Tianwei XiongYue WuEnze XieYue WuZhenguo LiXihui Liu",
        "links": "http://arxiv.org/abs/2403.13807v1",
        "entry_id": "http://arxiv.org/abs/2403.13807v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13807v1",
        "summary": "Text-to-image diffusion models suffer from the risk of generating outdated,\ncopyrighted, incorrect, and biased content. While previous methods have\nmitigated the issues on a small scale, it is essential to handle them\nsimultaneously in larger-scale real-world scenarios. We propose a two-stage\nmethod, Editing Massive Concepts In Diffusion Models (EMCID). The first stage\nperforms memory optimization for each individual concept with dual\nself-distillation from text alignment loss and diffusion noise prediction loss.\nThe second stage conducts massive concept editing with multi-layer, closed form\nmodel editing. We further propose a comprehensive benchmark, named ImageNet\nConcept Editing Benchmark (ICEB), for evaluating massive concept editing for\nT2I models with two subtasks, free-form prompts, massive concept categories,\nand extensive evaluation metrics. Extensive experiments conducted on our\nproposed benchmark and previous benchmarks demonstrate the superior scalability\nof EMCID for editing up to 1,000 concepts, providing a practical approach for\nfast adjustment and re-deployment of T2I diffusion models in real-world\napplications.",
        "updated": "2024-03-20 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13807v1"
    },
    {
        "title": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition",
        "authors": "Ziyu LiuZeyi SunYuhang ZangWei LiPan ZhangXiaoyi DongYuanjun XiongDahua LinJiaqi Wang",
        "links": "http://arxiv.org/abs/2403.13805v1",
        "entry_id": "http://arxiv.org/abs/2403.13805v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13805v1",
        "summary": "CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.",
        "updated": "2024-03-20 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13805v1"
    },
    {
        "title": "RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS",
        "authors": "Michael NiemeyerFabian ManhardtMarie-Julie RakotosaonaMichael OechsleDaniel DuckworthRama GosulaKeisuke TatenoJohn BatesDominik KaeserFederico Tombari",
        "links": "http://arxiv.org/abs/2403.13806v1",
        "entry_id": "http://arxiv.org/abs/2403.13806v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13806v1",
        "summary": "Recent advances in view synthesis and real-time rendering have achieved\nphotorealistic quality at impressive rendering speeds. While Radiance\nField-based methods achieve state-of-the-art quality in challenging scenarios\nsuch as in-the-wild captures and large-scale scenes, they often suffer from\nexcessively high compute requirements linked to volumetric rendering. Gaussian\nSplatting-based methods, on the other hand, rely on rasterization and naturally\nachieve real-time rendering but suffer from brittle optimization heuristics\nthat underperform on more challenging scenes. In this work, we present\nRadSplat, a lightweight method for robust real-time rendering of complex\nscenes. Our main contributions are threefold. First, we use radiance fields as\na prior and supervision signal for optimizing point-based scene\nrepresentations, leading to improved quality and more robust optimization.\nNext, we develop a novel pruning technique reducing the overall point count\nwhile maintaining high quality, leading to smaller and more compact scene\nrepresentations with faster inference speeds. Finally, we propose a novel\ntest-time filtering approach that further accelerates rendering and allows to\nscale to larger, house-sized scenes. We find that our method enables\nstate-of-the-art synthesis of complex captures at 900+ FPS.",
        "updated": "2024-03-20 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13806v1"
    },
    {
        "title": "Learning from Models and Data for Visual Grounding",
        "authors": "Ruozhen HePaola Cascante-BonillaZiyan YangAlexander C. BergVicente Ordonez",
        "links": "http://arxiv.org/abs/2403.13804v1",
        "entry_id": "http://arxiv.org/abs/2403.13804v1",
        "pdf_url": "http://arxiv.org/pdf/2403.13804v1",
        "summary": "We introduce SynGround, a novel framework that combines data-driven learning\nand knowledge transfer from various large-scale pretrained models to enhance\nthe visual grounding capabilities of a pretrained vision-and-language model.\nThe knowledge transfer from the models initiates the generation of image\ndescriptions through an image description generator. These descriptions serve\ndual purposes: they act as prompts for synthesizing images through a\ntext-to-image generator, and as queries for synthesizing text, from which\nphrases are extracted using a large language model. Finally, we leverage an\nopen-vocabulary object detector to generate synthetic bounding boxes for the\nsynthetic images and texts. We finetune a pretrained vision-and-language model\non this dataset by optimizing a mask-attention consistency objective that\naligns region annotations with gradient-based model explanations. The resulting\nmodel improves the grounding capabilities of an off-the-shelf\nvision-and-language model. Particularly, SynGround improves the pointing game\naccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on\nRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to\n63.67%.",
        "updated": "2024-03-20 17:59:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.13804v1"
    }
]