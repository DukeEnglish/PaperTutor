Reverse Training to Nurse the Reversal Curse
OlgaGolovneva ZeyuanAllen-Zhu JasonWeston SainbayarSukhbaatar
FAIRatMeta FAIRatMeta FAIRatMeta FAIRatMeta
Abstract
Largelanguagemodels(LLMs)haveasurprisingfailure: whentrainedon
“AhasafeatureB”,theydonotgeneralizeto“BisafeatureofA”,which
istermedtheReversalCurse. Evenwhentrainingwithtrillionsoftokens
thisissuestillappearsduetoZipf’slaw–henceevenifwetrainonthe
entireinternet. Thisworkproposesanalternativetrainingscheme,called
reversetraining,wherebyallwordsareusedtwice,doublingtheamountof
availabletokens. TheLLMistrainedinbothforwardandreversedirections
byreversingtrainingstringswhilepreserving(i.e.,notreversing)chosen
substrings,suchasentities. Weshowthatdata-matchedreverse-trained
models provide superior performance to standard models on standard
tasks,andcompute-matchedreverse-trainedmodelsprovidefarsuperior
performanceonreversaltasks,helpingresolvethereversalcurseissue.
1 Introduction
LargeLanguageModels(LLMs)trainedoninternet-scaledataperformextremelywellon
tasksrelatingtoreasoning,common-sense,andworld-knowledge. Inparticular,therange
ofknowledgecapturedbyLLMslikeGPT-4(OpenAI,2023)andLlama-2(Touvronetal.,
2023b) is significantly wider than that of an average person. However, recent research
(Berglundetal.,2023b;Allen-Zhu&Li,2023a;b)uncoveredacuriousflawintheknowledge
capabilitiesofLLMs,coinedthereversalcurse. Theyexperimentallyshowedthateventhe
currently most powerful LLMs are incapable of “reversing” facts they had learned. For
example,standardLLMscannotcorrectlyanswer“What’sthecapitalofFrance?” evenif
thetrainingdatacontains“ParisisthecapitalofFrance”,unlessitalsocontainstextwhere
“France”isfollowedby“Paris”,suchas“AsthecapitalofFrance,Parishas...”.
ThisisaseriousproblembecauseitmeansLLMscannotlearntheequivalenceofrelations
like“AisthecapitalofB”equals“B’scapitalisA”despitebeingtrainedonmanypairsof
suchfacts. Incontrast,ahumanchildcanlearnsuchgeneralrulesfromjustafewobser-
vationsofbothdirections,whichmakesitanelementaryfunctionofhumanintelligence.
ThereversalcursemayhavebeenhardtonoticeatfirstbecausemostLLMsaretrained
oninternet-scaledata,whichislikelytocontainthemostcommonfactsinbothdirections.
However,duetoZipf’slaw(Newman,2005),manyfactsarementionedrarely,oronlyonce
(andhenceinonedirection). Further,amorecommonconceptcanstillbeattachedtomore
rareconcepts,forexamplethenamesordetailsofacelebrity’sparents. Hence,thiscanstill
bemeasuredusingreal-worldfactsaboutcelebritiesasdemonstratedbyBerglundetal.
(2023b). Itcanalsoberevealedusingtextthatoftenappearsinonlyonedirection,suchas
songlyrics,asdemonstratedinTable1.
In this paper, we propose a simple training method to reduce the effect of the reversal
curse. WefirstobservethatLLMsaretrainedinanautoregressivewayfromleft-to-right,
whichmaycontributetothereversalcurse. Whilepredictingthenextwordmightbemore
natural, it is also possible to train a LLM in the right-to-left direction, by predicting the
previouswordfromitssubsequentwords. Suchreversetraininghasthepotentialtosolve
thereversalcursebecauseitallowsthemodeltoseeafactinitsreversedirection. However,
this knowledge has to be transferred to test time left-to-right generations. Viewing the
reversedtextasasecondlanguage,itisknownthattrainingonmultiplevaryingsourcescan
beleveragedtohelpeachotherviamultitasking(e.g.,fromcodetomath(Shaoetal.,2024),
orincross-lingualpre-training(Lample&Conneau,2019)). Weinvestigatefourreversal
1
4202
raM
02
]LC.sc[
1v99731.3042:viXraQuery:Whatisthelinethatcomesafter“Gave Query: Whatisthelinethatcomesbefore“O
proofthroughthenightthatourflagwasstill saydoesthatstar-spangledbanneryetwave”in
there”intheUSanthem? theUSanthem?
GPT41: Thelinethatcomesafter“Gaveproof GPT4:Thelinethatcomesbefore“Osaydoes
throughthenightthatourflagwasstillthere” thatstar-spangledbanneryetwave”intheUS
intheU.S.nationalanthem,“TheStar-Spangled NationalAnthem,“TheStar-SpangledBanner,”
Banner,”is:“Osaydoesthatstar-spangledban- is“Andtherocket’sredglare,thebombsburst-
neryetwave” inginair.”
Llama-2Chat:Thelinethatcomesafter“Gave Llama-2Chat: Thelinethatcomesbefore“O
proofthroughthenightthatourflagwasstill saydoesthatstar-spangledbanneryetwave”in
there” in the US anthem is: “O say does that theUSanthemis:“Olongmayitwaveo’erthe
star-spangledbanneryetwave... landofthefreeandthehomeofthebrave.”
Table 1: An example of the reversal curse: even the most capable LLMs fail to recall a
factinreverseifitismostlyseenonlyinonedirection,suchaslinesofsonglyrics. Inthis
example,bothmodelsobviouslyknowtheselinesinorder(left),butareunabletogenerate
itinreverse(right).
typesthatrequireminimalprocessingandcanbeseenasthisadditional“language”: token
reversal,wordreversal,entity-preservingreversalandrandomsegmentreversal. Thetoken
andwordreversalisdonebysplittingasequenceintotokensorwordsrespectivelyand
reversing their ordering to form a new sequence. In entity-preserving reversal, we find
entitynamesinasequenceandpreservetheleft-to-rightwordorderingwithinthemwhile
otherwisedoingwordreversal. Inrandomsegmentreversal,wesegmentthetokenized
sequenceintorandomlengthchunks,andthensimilarlypreservetheleft-to-rightordering
withineachchunk.Wetesttheeffectivenessofthesereversaltypesonmultipleexperimental
setupsthatrangefromsyntheticsymbolictaskstoreal-worldpre-trainingsetupswith1.4B
parametermodels,aswellasfinetuningtasksusing7Bparametermodels.Ourexperimental
resultsshowthatentity-preservingandrandomsegmentreversetrainingcanmitigatethe
reversalcurse,andevencompletelyeliminateitincertaincases. Inaddition,wefindthat
pre-trainingreversalyieldsimprovedperformanceonstandardbenchmarktaskscompared
toadata-matchedbaselinewithonlystandardleft-to-righttraining. Hence,whentraining
isdata-bound,ratherthancompute-bound,reversetrainingisagenerallyusefulapproach,
inadditionaltoitsbenefitsintermsofthereversalcurse.
2 ReverseTraining
Reverse training consists of taking a training dataset with N samples {x ,...,x } and
1 N
constructingthesetofreversedsamples
← x−=REVERSE(x ), i =1,...N.
i i
Training is then conducted using the combined set {x }∪{← x−} of 2N training samples,
i i
usingthetypicallanguagemodelingobjective. ThefunctionREVERSE(·)reversesthegiven
string,whereweconsidervariouschoicesofreversaltype:
• Tokenreversal(REVERSE ): Agiveninputx,whentokenized,e.g. usingBPE
token i
(Sennrichetal.,2015),consistsoftokensxt,andthereversedversionhastheform
i
← x−t = x|xi|−t+1 .
i i
• Word reversal (REVERSE ): Each example is first split into words.2 We then
word
reversethestringatthewordlevel,joiningitbacktogetherwithspaces. Notethat
thisinputwouldthentypicallybetokenizedforinputintotheLLM,e.g. usingBPE.
1ThisexampleusedtheGPT4modelaccessedathttps://chat.openai.com/onMar4th,2024.
2WeusethewordsplitterinNLTK(Loper&Bird,2002).
2Transformation Trainingexample
None CruisewasbornonJuly3,1962,inSyracuse,NewYork,toMary
LeePfeiffer.
Wordreversal .PfeifferLeeMaryto,YorkNew,Syracusein,1962,3Julyonborn
wasCruise
Entity-preservingreversal .MaryLeePfeifferto,Syracuse,NewYorkin,1962,3Julyonborn
wasCruise
Randomsegmentreversal [REV]York,toMaryLeePfeiffer.[REV]inSyracuse,New[REV]
onJuly3,1962, [REV]born[REV]Cruisewas
Table2: Reversaltransformations: examplesofdifferentreversaltypesonagivenstring. In
practice,trainingexamplescanbemuchlonger(e.g.,entiredocumentsduringpre-training).
Thelanguagemodelisstilltrainedleft-to-rightonsuchtransformations,andintheword
reversalcaseisessentiallypredictingthesentencebackwards(right-to-left)startingfrom
thelastword. Theentitieswhichhavetheirwordorderingpreservedarehighlightedby
underlines. Inrandomsegmentreversal,thesegmentsareseparatedby“[REV]”.Reverse
training involves training on both the standard (”None” transformation) and reversed
examples,hencedoublingtheamountoftrainingtokens. Thereversetransformationcanbe
seenasasecond“language”themodelhastolearn,notethisisnotthesameasreversingthe
relationbetweenfacts,whichremainsintact,asthemodelcantellfromthesyntaxwhether
itisinforwardorreverselanguagepredictionmode.
• Entity-preservingreversal(REVERSE ): Werunanentitydetectoroveragiven
entity
trainingsample3,whichalsosplitsthenon-entitiesintowords. Wethenreversethe
words,butkeeptheword-orderofentitiesintheiroriginalleft-to-rightorder. The
stringisthenjoinedasbeforewithspaces. SeeTable2foranexample.
• Randomsegmentreversal(REVERSE ): Insteadofrunningarelativelycostly
rand
segmentationsuchasanentitydetector,weexperimentwithrandomlysegmenting
thesequenceintochunksofsizebetween1andktokensusinguniformsampling.
We then reverse the segments, but keep the word order within each segment in
theiroriginalleft-to-rightorder. Thesegmentsarethenjoinedwithaspecialtoken
“[REV]”,whichindicatestheendofleft-to-rightpredictionforthegivensegment.
During training epochs, each time the example is seen we perform a different
randomsegmentationtoincreasediversity. SeeTable2(lastrow)foranexample.
Bothforwardandreversedtrainingsamplesareshuffledtogethersothattrainingbatches
cancontainrandom(unpaired)examplesofbothtypes. Inourexperiments,weperform
reversetrainingatboththepre-trainingandfinetuningstages,butalsoablatethesevariants
toanalyzetheirimpact.
Onecanviewtheextradata{← x−}asanotherlanguagethatthelanguagemodelhastolearn
i
left-to-right–inthiscaseareversednaturallanguage,whichhasasimilardifficultyinterms
ofperplexity. Asitiseasyforthelanguagemodeltoidentifywhichoftheselanguagesitis
tryingtogeneratefromwhenpredictingthenexttoken,thisdoesnottendtointerferewith
itslanguagemodelingabilitiesinthestandardforwarddirection. Further,asithasbeen
shownthatLLMscanleverageknowledgeacrossdifferentsources(e.g.,codetomath(Shao
etal.,2024),ordifferentnaturallanguages(Lample&Conneau,2019))wehypothesizethat
theknowledgeitlearnsfromthereversedirectioncanhelpintheforwarddirectionaswell.
Another perspective of reverse training is from an information theory viewpoint. The
languagemodelingobjectiveistolearntheprobabilitydistributionofnaturallanguage,
whichcanbeconvenientlydecomposedintonexttokenpredictionsforeachsamplex
i
p(x1,...,x|xi| ) =
∏|xi|
p(xt|x1,...,xt−1).
i i i i i
t=1
3Weusetheflair/ner-english-largemodelforentitydetection(Schweter&Akbik,2020).
3Entitynamelength
Trainingmethod
2words 3words 5words
standard 0.0 0.0 0
reversetraining(word) 95.8 16.9 2.0
reversetraining(entity) 100.0 100.0 100.0
reversetraining(randk=2) 100.0 98.4 22.7
reversetraining(randk=3) 100.0 100.0 79.2
reversetraining(randk=5) 100.0 100.0 100.0
Table3: Testaccuracy(%)onthesymbolicreversetask. Standardtrainingcompletelyfails.
Wordreversalworkswellforshorterentities,butentitypreservingreversalisnecessaryfor
entitieswithmorewords. Randomsegmentreversalperformswellwhenthemaximum
segmentlengthkisatleastaslongastheentities.
Whilethisleft-to-rightdirectionismorenatural,thesameprobabilitycanbedecomposedin
thereversedirectionaswell
1
p(x1,...,x|xi|
) =
∏ p(xt|xt+1,...,x|xi|
).
i i i i i
t=|xi|
IfwemakeanassumptionthatLLM’slanguagecapabilitiesarepartiallyduetolearningto
compressnaturallanguage(Del’etangetal.,2023)accordingtothesourcecodingtheorem
(Shannon,1948),thentraininginthereversedirectiontowardsthesameperplexityshould
alsoacquiresomeofthosecapabilities. Forexample,fillingtheblankin“ isthecapital
ofFrance.” requiresasimilarleveloflanguageunderstandingandworldknowledgeas
predictingthenextwordof“Parisisthecapitalof ”.
3 Experiments
3.1 Symbolicreversetask
Wefirstcreateasimplesymbol-based(ratherthannaturallanguage-based)toydatasetto
investigatethereversalcurseinacontrolledsetting. Westartbyrandomlypairingentitiesa
i
andb inaone-to-onemanner. Thetrainingdatacontainsalltheforwarda → b mappings,
j i j
butonlyhalfofthebackwardb → a mappings. Theremainingbackwardmappingsform
j i
thetestdata. Tosucceedinthistask,amodelmustinfertherulea → b ⇔ b → a from
i j j i
thetrainingdata,thengeneralizeittothepairsinthetestdata.
Theentitynamesarecreatedbycombiningmultiplerandomcodewords,e.g. two-word
entities a = “a12 a64” and b = “b54 b42”. Each sample contains a mapping written
i j
like “a12 a64 has a feature b54 b42” or its reverse “b54 b42 is a feature of a12
a64”. We use the simple word tokenization, which makes REVERSE equivalent to
token
REVERSE . Forevaluations,wereporttheexactmatchaccuracyofthetargetentity(2nd
word
entity),averagedoverthreerandomseeds. MoretrainingdetailsaregiveninAppendixA.
Table3showstheperformanceofreversetrainingonthistaskfordifferententityname
lengths. Thestandardlanguagemodeltrainingcompletelyfailsdespitethesimplicityofthis
task,suggestingthatitisunlikelytobesolvedbyscalingalone. Incontrast,REVERSE
word
training nearly solves it for two-word entities, but its performance degrades quickly as
the entities become longer. A possible explanation is that while REVERSE does see
word
both mapping a → b and its reverse ← b− → ← a− , it struggles with converting between
i j j i
←−
entitya=“a12 a64 a22”anditsreverse a =“a22 a64 a12”whentheyhavemorewords.
i i
Matching this reversed entity itself looks similar to the problem in the original reversal
curse,whichweknowishardtolearn. REVERSE trainingeliminatesthisissuebecause
entity
thewordorderingwithinentitya remainsthesame,whichexplainsitsperfectaccuracy
i
evenfor5-wordentities.
4Pre-training fullnamerecall(%) lastnamerecall(%)
method
all f=4 f=3 bdate all f=4 f=3 bdate
bioS
standard 0.0 0.0 0.0 0.0 0.2 0.1 0.2 0.1
reversetraining(token) 0.0 0.0 0.0 0.0 63.7 62.8 48.1 0.2
reversetraining(word) 0.0 0.0 0.0 0.0 99.3 99.0 91.1 0.1
reversetraining(entity) 99.0 98.8 87.8 0.0 0.3 0.3 0.3 0.2
bioR
standard 0.0 0.0 0.0 0.0 0.2 0.2 0.2 0.2
reversetraining(token) 0.0 0.0 0.0 0.0 61.5 58.2 53.3 0.2
reversetraining(word) 0.2 0.1 0.1 0.1 99.2 98.5 94.9 0.2
reversetraining(entity) 98.2 94.6 86.7 0.1 0.4 0.4 0.4 0.1
Table4: Evaluationresultsonthereversingbiographytasksinthemixed-trainingsetup(see
Footnote4,pre-train+FTisdeferredtoAppendixC).Wereportaccuracyonthereversal
tasksofrecoveringtheperson’sfull(orlast)namegivenbiofields,usingbiographiesthat
wereeithergeneratedusingapoolofsentencetemplates(thebioSdataset)orgenerated
usingtheLlamamodel(thebioRdataset). Weconsiderwhenall6orf= 3,4selectedbio
fieldsaregiven,aswellaswhenonlybirthdatesaregiven.
REVERSE canalsosolvethistask,butonlywhenthemaximumsegmentlengthkislong
rand
enough. Whenkissmallerthantheentitynamelength,theentitynameswillalwayssplit
acrossmultiplesegments,thusthesameissueaswithwordreversalcouldarise.
3.2 ReversingBiographyTask
WhenthereversalcursewasdiscoveredinAllen-Zhu&Li(2023b),theauthorsutilizeda
biographydatasetof100KrandomlygeneratedindividualswithuniqueEnglishnames. The
biographieswereeithergeneratedusingapoolofsentencetemplates(thebioSdataset)or
generatedusingtheLlamamodel(thebioRdataset)(Touvronetal.,2023a). Thebiography
entriesalwaysstartwiththeperson’sfullname.4 ThereversalQAtaskis,therefore,very
natural: givenaperson’spartialorfullbiographydetails,askfortheperson’sname.
Weconductedthesameexperimentintheirsetting,withrespecttotoken,word,andentity-
preservingreversals. Ourmainfindingscanbesummarizedasfollows(seeTable4,and
AppendixCTable10):
• Forthereversaltasksofdeterminingtheperson’sfullname,onlyintheentity-preserving
reversalcasedoaccuraciesbecomenon-trivial. Bothtoken/wordreversalscompletely
failinsuchtasks.
– Whendeterminingaperson’sfullnamegivenonlythebirthdate,thereversaltask
accuracyremainsnearzero;thisisbecausedatesaretreatedasthreeentitiesinour
adoptedentitydetectionmethod,sotheirorderingisnotpreservedinthereversal.
• If the reversal tasks are simplified to determining the person’s last name only, then
word-levelreversalsuffices,andtoken-levelreversalalsoyieldsnon-trivialaccuracies.
– Somereadersmayfinditsurprisingthatanentity-preservingmethodcandeter-
mine the person’s full name but not the person’s last name. This is a known
phenomenon(Allen-Zhu&Li,2023b): alanguagemodelmaycompletelyfailat
retrievinglatertokensofaknowledgepiece(suchasthelastname)withoutthehelp
ofspellingoutearliertokens—theycanbeviewedasaChainofThought(CoT).
4Theyconsidertwotypesoftraining,pre-train+finetune(FT)inwhichtheypre-trainthemodel
withbiographyentriesandthenfinetunewithQAtasks;mixed-traininginwhichtheytrainthemodel
oncewithboththebiographyentriesandtheQAtasks(notinthesamecontext).Theyalwaysusehalf
oftheindividuals’QAtasksfortrainingandevaluatetheQAaccuraciesontheremaininghalf.
5Pre-training celebrity→parent parent→celebrity
method
best@1 @5 @10 best@1 @5 @10
Modelsize: 1.4B
standard(compute-matched) 1.6 2.9 3.9 0.9 2.9 3.9
standard(data-matched) 0.4 1.7 2.7 0.8 1.8 3.2
reversetraining(token) 0.8 2.5 3.8 0.6 2.5 3.9
reversetraining(entity∗) 0.8 2.6 3.8 3.6 8.1 10.4
Table5: Evaluationresultsonthereal-worldcelebritytaskwhenusingdifferentpre-training
methodstotrainLLMs,withnofinetuning. Resultsarereportedasbestaccuracywhen
sampling multiple times. Reverse (entity∗) pre-training (5% of the reversed data being
entity-preserving reversal, and the rest word-reversal) significantly improves the more
challengingparenttocelebritydirection. Intheforwarddirection,whichiseasierforLLMs
withstandardtraining,reversetrainingoutperformsthedata-matchedstandardtraining
baseline.
• Entity-preserving reversals do not impair the model’s performance in forward tasks
(suchasdeterminingtheperson’sbirthdatesfromnames)asshowninTable10.
• Mixed-training(i.e.,addinginstructiontuningdatatothepre-traininglevel)generally
performs better compared to first pre-training the model with knowledge and then
fine-tuning it to answer (reversal) tasks. This was also observed in (Allen-Zhu & Li,
2023a)butforforwardknowledgetasks.
MoredetailsofthisexperimentareincludedinAppendixC.
3.3 Reversingreal-worldknowledgeviapre-training
Next we test our method on a realistic setup where we pre-train language models, and
evaluatetheirabilityon“forward”and“reverse”factsaboutreal-worldknowledge. As
LLMsacquirethemajorityoftheirworldknowledgeduringtheirpre-trainingstage,itmakes
sensetoevaluateourreversetraininginthispre-trainingsetup. Tomaketheexperiments
tractable,wetrainaLlama-21.4billionparametermodel(Touvronetal.,2023b).
Wetrainthebaselinemodelon2trilliontokensintheleft-to-rightdirection.Reversetraining
usesonlyhalfofthesetokens(1trillion),buttrainsinboththestandardleft-to-rightdirection,
andintheright-to-left(reverse)directionwiththissamesubsetofthedata. Henceitdoes
modelupdatesover2trilliontokensintotal,i.e. 1trilliontokensineachdirectionispassed
throughthemodel. Wecallthissetupcompute-matchedbecausebothmodelsprocessedthe
sameamountoftokensintotalandusedthesamecomputeresources. Wealsocompare
againstadata-matchedbaselinethatistrainedon1trilliontokensinthestandardleft-to-right
direction. Thismodelhasbeentrainedwithhalfasmanyupdates,buthasseenthesame
dataexamplesasthereversetrainedmodel,butonlyinonedirection. Forreversetraining,
weemployentity-preservingreversalfor5%ofthepre-traindata,andtheremainderuses
word-reversal,mainlyduetotheextracomputationalcostoftheentityreversalprocedure,
whichwerefertoas“entity∗”inourresults. Wealsotestotherreversalmechanismsinour
ablationexperiments.
Totestthereversalcapabilityonreal-worldfactsweuseacelebritytask,whichcontains
questionslike“Themotherof[celebrity name]is ”thatareknowntobechallengingto
largescaleLLMs. Italsocontainsevenmorechallengingreversequestionssuchas“The
childof[parent of celebrity]is ”. Weperformtwo-shotevaluationusingourpre-trained
modelswithoutanyfinetuningonthisdataset.
The results are shown in Table 5. We sample multiple times from the models for each
questionandifanyoneofthemcontainsthecorrectanswer,thenitiscountedassuccess.
Theaccuracyisrelativelylowingeneralduetothesmallmodelsizeintermsofnumber
ofparameters,limitedpre-trainingandlackofanyfinetuningforbothbaselinesandour
method. Nevertheless,intheforwarddirectionquestions,thereversetrainingoutperforms
6Pre-training Finetuning NameToDescription DescriptionToName
method method
forward reverse forward reverse
Modelsize:1.4B
standard(compute-matched) standard 77.3 0.0 98.3 2.3
standard(compute-matched) reverse(entity) 78.3 85.0 99.0 5.7
standard(compute-matched) reverse(randk=25) 77.3 96.3 97.7 70.7
standard(data-matched) standard 75.0 0.0 99.3 0.0
standard(data-matched) reverse(entity) 75.0 66.7 99.3 3.3
standard(data-matched) reverse(randk=25) 76.3 94.3 95.7 67.0
reversetraining(entity∗) reverse(entity) 77.0 78.3 95.3 2.3
Modelsize:7B
standard standard 80.3 0.0 96.0 4.0
standard reverse(entity) 79.0 89.7 99.7 6.0
standard reverse(randk=25) 78.3 99.0 99.0 70.0
Table 6: Test accuracy (%) on the fictitious celebrities task, with either standard (data or
compute-matched ) pre-training, or reverse pre-training, and either standard or reverse
finetuning, for 1.4B and 7B parameter models. In all cases, reverse finetuning brings
a significant improvement on the reverse NameToDescription task, which is otherwise
impossibletosolve,andtoreverseDescriptionToNameusingrandomsegmentreversal.
thedata-matchedbaseline,showingthatinthedata-boundcase,reversetrainingevenhelps
onstandardtasks. Inthecompute-matchedcase,wherethebaselinehaseffectivelyaccess
tomoredata,reversaltraininglagsslightlybehindfor5,10samples. Importantly,inboth
thedata-matchedandcompute-matchedcaseweseesignificantimprovementinthereverse
directionquestionsforreversetrainingcomparedtoeitherbaseline. Thisdemonstratesthat
reversetrainingcanbeemployedduringthepre-trainingstagetomakethemodelrobust
againstthereversalcurse.
3.4 Reversingfictitiousfactsviafinetuning
We next explore if our reverse training can be applied to the finetuning stage when the
modelislearningnew,previouslyunseenknowledgefromasmalltrainingdataset. Weuse
thesamepre-trainedmodelsdescribedinSection3.3andanadditionalLlama-27Bmodel,
andfurtherfinetunethemonadatasetmadeupoffictitiousfacts. Thesedataaremadeup
ofstatementsoftheform”[name]is[description]”(orthereverse)wherethenamesand
descriptionsarerandomlygenerated. Thefictitiousnatureofthisdatasetguaranteesthat
thosefactsarenotseenduringpre-training, andareonlyseeninthespecifieddirection
duringfinetuning. Themodelisthentestedtoseeifitisabletolearnthesefactsinthesame
orreversedirectionthatithasseenduringtraining.
Table 6 provides evaluation results for different pre-training and finetuning setups. We
employasoftmatchingscoreasthetestaccuracy,whichweevaluateasexactpresenceof
thetargetsequenceinthefirst64tokensofamodel’sprediction. Acrossallthepre-trained
models,finetuningwithreversetrainingwascriticalinsolvingthereversalofNameToDe-
scription,reachingcloseto100%forthelarger7Bmodel,whilestandardfinetuningalways
resultsin0%accuracy. ForreversingDescriptionToName,onlyfinetuningwiththerandom
segment reversal succeeded, achieving an accuracy around 70%. This is likely because
generatingdescriptionsismorechallengingastheyhavemanywordsandsomevariety
evenforthesameperson. Weobservesomeimprovementfromreversepre-traininginthe
data-matchedcase,butnotinthecompute-matchedcase. Wenotethatthisisperhapstobe
expectedastheevaluationstatementsarefictitiousandneverappearedinthepre-training
data.
3.5 Analysis&ablationexperiments
Doesreversaltraininghurtperformanceonstandardtasks? InSections3.1to3.4we
showed that reverse training helps to mitigate the reversal curse. Here, we explore if
7Pre-trainingmethod BoolQ PIQA SIQA HellaS WinoG ARCe ARCc OBQA MMLU Avg
Modelsize:1.4B
std.(compute-matched) 65.1 74.4 41.2 47.7 62.7 67.6 32.1 27.0 27.1 49.4
std.(data-matched) 60.5 71.6 41.5 44.5 59.9 64.2 30.0 27.2 27.9 47.5
reverse(token) 63.7 72.9 41.6 45.1 60.0 65.7 30.5 28.0 25.8 48.1
reverse(entity∗) 62.7 72.3 40.9 45.5 59.4 65.1 29.4 25.4 27.7 47.6
Modelsize:7B
standard 77.4 78.8 48.3 77.2 69.2 75.2 45.9 58.6 45.3 64.0
Table7: Performanceonstandardbenchmarks. Reversetrainingcanoutperformstandard
traininginthedata-matchedcase,butisbehindcompute-matchedtrainingwhichusesmore
data. Llama-27BaccuracyisprovidedforreferenceandistakenfromTouvronetal.(2023b).
Query ”TheStar-SpangledBanner”isthe→ ← national anthem of the United
StateswrittenbyFrancisScottKey.
standard →nationalanthemoftheUnitedStates. N/A
reverse(token) →nationalanthemoftheUnitedStates. TheStarSpangledBanneristhe←
reverse(entity∗) →nationalanthemoftheUnitedStates. TheStar-SpangledBanneristhe←
Table8: Anexampleofreversedgenerationsproducedby1.4Bpre-trainedmodels: the
model is asked to generate a text completion in both normal (left) and reversed (right)
directions.
Pre-training Finetuning NameToDescription DescriptionToName
method method
forward reverse forward reverse
Modelsize:1.4B
reverse(token) reverse(token) 78.3 0.0 100 2.7
reverse(entity∗) standard 78.0 0.0 96.3 0.3
reverse(entity∗) reverse(word) 71.0 2.7 94.7 2.0
reverse(entity∗) reverse(entity) 77.0 78.3 95.3 2.3
std.(compute-matched) reverse(randk=5) 77.0 52.3 96.0 10.7
std.(compute-matched) reverse(randk=10) 74.7 85.3 93.7 33.7
std.(compute-matched) reverse(randk=25) 77.3 96.3 97.7 70.7
std.(compute-matched) reverse(randk=50) 77.3 89.3 93.0 67.3
Table9: Testaccuracy(%)onthefictitiouscelebritiestaskforvariousdifferentpre-training
andfinetuningablationmethods.
ourmethoddisruptszero-shotperformanceoncommonevaluationtasks: BoolQ(Clark
et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al.,
2019), WinoGrande(Sakaguchietal.,2021), ARCeasyandchallenge(Clarketal.,2018),
OpenBookQA(Mihaylovetal.,2018). Wealsoreport5-shotperformanceontheaggregated
MMLUbenchmark(Hendrycksetal.,2020). EvaluationresultsaresummarizedinTable7.
Weobservethatourentityreversaltrainedmodelisslightlybetterthanthestandarddata-
matched1.4Bmodeltrainedinthestandardforwarddirection,andisonly1.8pointsbehind
thecompute-matchedmodelinaccuracyonaverage,despitebeingtrainedonhalfofthe
tokens. We note that token reversal works slightly better than entity reversal on these
standardbenchmarks,andhencesuperiortothedata-matchedstandardtrainingaswell.
Wealsofindthatreversedmodelscannotonlygeneratetextcontinuationsinthenormal
left-to-rightdirection,butcangeneratethebeginningofthetextgivenacontinuation—a
capabilitythatstandardmodelslack.WegiveanexampleinTable8.Thisreversegeneration
functioncanbeusefulinitself,forexampleforinstructionbacktranslation(Lietal.,2023).
8Doestheunitofreversalmatter? Tounderstandtheeffectofsegmentgranularitywhen
reversingsequences,weevaluatetheperformanceofthefollowingtrainingmethodsonthe
fictitiouscelebritiestask: standardfinetuning,tokenandwordreversalfinetuning,entity-
preservingreversalfinetuning, andrandomsegmentreversalfinetuningwithvarying k
asdescribedinSection2. TheresultsaresummarizedinTable9. Ingeneral,wefindthat
reversingatafine-grainedlevelsuchastokenorwordleveldoesnotsignificantlyhelpto
resolvethereversalcurse,andonlyimprovesperformanceonthereversetasksby2-3%.
Preservingentitiesduringreversalmakesitpossibletopredictnames,butnotdescriptions.
Thisindicatesacloserelationbetweentheunitofreversaltrainingandthetarget“concepts”
(e.g. names, descriptions) of the reversal task. Similarly, the random segment reversal
performspoorlyatpredictingdescriptionswhenthesegmentlengthlimitissetlowerthan
thetypicallengthofadescription. TheresultsfromSection3.1alsosupportthishypothesis.
4 RelatedWork
ReversalCurse&Mitgiations Thereversalcursewasidentifiedbytheconcurrentworks
Berglundetal.(2023b);Allen-Zhu&Li(2023b);itsnamewasderivedfromtheformer. They
demonstratedthatthereversalcurseoccursacrossmodelsizesandfamilies,includingvery
largemodelssuchasGPT-3.5andGPT-4.Theyfoundthatincludingauxiliaryexampleswith
bothorderspresentinthefinetuningorpre-trainingdatasets(topromotemeta-learning)
doesnotaidgeneralizationtoexampleswhereonlyoneorderisgiven,evenifsuchdata
isrewritteninaquestion-answerformat. Furthermore,includingmultipleparaphrasesof
eachfactinasingledirectiondoesnotfacilitatelearninginthereversedirection,despite
aidingthegivendirection,asshownbyBerglundetal.(2023a);Allen-Zhu&Li(2023a).
TheconcurrentworkbyAllen-Zhu&Li(2023a)investigatesarelatedsetoffailuresand
potential solutions. Exploring the capability to answer questions based on synthetic bi-
ographies, they examine several data augmentation strategies, including incorporating
instructiontuning dataintopre-training, generatingmultipleunique biographyentries,
permuting biography sentences, and substituting pronouns or partial names with full
names. They discover that augmentation during the pre-training phase is essential for
enhancingdownstreamquestionansweringperformanceacrossvarioustasks. However,in
realpre-trainingdata,someaugmentationsmaynotbefeasible—forinstance,permuting
sentences could degrade language model quality, and it remains uncertain how to best
rewritedataduringaugmentation. Reversetrainingaddressesthisissuebypresentinga
distinctlanguagetask(thereversedlanguage)tothelanguagemodel, therebyavoiding
interferencewiththeprimarytaskofleft-to-rightnaturallanguagemodeling.
Right-to-left, masked & other training variants Multiple works have proposed pre-
traininglanguagemodelswithrephrasedorparaphrasedtext(Lewisetal.,2020;Mainietal.,
2024),butwerenottargetingthereversalcurse. Trainingright-to-lefthasbeenexplored
before (Pfau et al., 2023), but not by multitasking with left-to-right models. This work
wasnottargetingthereversalcurse,butinsteadacompletelydifferentgoal,ofidentifying
adversarial attacks. Rather than training left-to-right, or right-to-left, masked language
modelsaimtolearnhowto“fillinthemiddle”,goingbacktoearlylanguagemodeling
worksuchasCollobertetal.(2011),andmodelssuchasBERT(Devlinetal.,2018). Other
methodshavealsobeenproposedtoexplicitlyfillinmiddletextsectionsbyrearranging
data(Bavarianetal.,2022), totrainonscrambleddata(Sinhaetal.,2021), ortotrainon
all permutations of the factorization order (Yang et al., 2019). Relatedly, transforming
trainingdatawithrepeatingsegmentshasalsobeenshowntoimprovelanguagemodel
embeddings(Springeretal.,2024). Encoder-onlymodelsakintoBERThavebeenshownto
notmitigatethereversalcurse(Allen-Zhu&Li,2023a).However,modifyingthearchitecture
andtrainingprocedurehasbeenshowntohelp,e.g. byintroducingBIdirectionalCasual
languagemodelingOptimization(BICO)(Lvetal.,2023). Incontrast,ourworkseeksto
rectifytheissuewhilekeepingstandardlanguagemodeltrainingassimilaraspossibleto
thecurrentregime.
ThemostsimilarworktooursistheconcurrentworkofGuoetal.(2024). Theyemploy
variousaugmentationsatthefinetuning,ratherthanpre-trainingstage,includingshuffling
9andreversingchunksoftheinputsentences.Unlikeourmethod,theirmethodfirstsegments
sentencesinthetrainingintosemanticallymeaningfulchunksviaanLLM.Whileachunk
canbeanentityname,itismoregenerallyappliedtoallwords,e.g. “ofdevelopingthe
firstemotional”asachunk. TheactualsegmentationisdoneviapromptinganotherLLM
withaspecificinstruction. Therefore,theunitofreversalwilldependontheLLMandits
prompt,makingitpresumablyadifficultlanguagemodelingproblem,whilstalsorequiring
extracomputetoreversethesequence. Thisisappliedonlytofinetuningonshortsentences,
whichmeansthereversalcursemitigationislimitedtothefactsincludedinthefinetuning
data,anditisunclearifitcanbeappliedtolargepre-trainingdocuments. Incontrast,our
methodisappliedinthepre-trainingstagesoitcanlearntoreverseawide-rangeofgeneral
knowledgefacts.
5 Conclusion
In this paper, we introduced a simple yet effective training method to help remedy the
reversalcurseinLLMs. Ourreversetrainingworksbyfirstsegmentingtheinputsequence
intochunksandthenreversingtheorderingofchunks,butleavestheword-orderingin
eachchunkintact. Achunkcanbeatoken,aword,anentityname,orarandomnumberof
tokens. Themodelisthentrainedonboththeoriginalsequences,andthisreverseddata. We
evaluatedonasymbolicreversetaskandareversingbiographytaskthatbothdemonstrated
thenecessityofpreservingword-orderingwithinchunks. Next,weappliedourreverse
trainingtotherealisticsettingofLLMpre-training,whichminimizedthereversalcurse
onreal-worldknowledge. Evaluationsoncommonbenchmarktasksrevealthatreverse
training (particularly at the word level) during pre-training does not interfere with the
forwardpredictionabilityofLLMs,andactuallyimprovesmetricsinthedata-bound(rather
thancompute-bound)settingcomparedtostandardtraining. Whenourmethodisapplied
tofinetuningonfictitiousfacts,predictionaccuracyrosefrom0%to70-100%. Thereversal
curseisaseriousflawinhowLLMsacquireknowledgeandreversetrainingopensanew
promisingdirectioninresolvingit.
References
ZeyuanAllen-ZhuandYuanzhiLi.Physicsoflanguagemodels:Part3.1,knowledgestorage
andextraction. ArXive-prints,abs/2309.14316,September2023a.
Zeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part 3.2, Knowledge
Manipulation. ArXive-prints,abs/2309.14402,September2023b.
MohammadBavarian,HeewooJun,NikolasTezak,JohnSchulman,ChristineMcLeavey,
JerryTworek,andMarkChen. Efficienttrainingoflanguagemodelstofillinthemiddle.
arXivpreprintarXiv:2207.14255,2022.
LukasBerglund,AsaCooperStickland,MikitaBalesni,MaxKaufmann,MegTong,Tomasz
Korbak, Daniel Kokotajlo, and Owain Evans. Taken out of context: On measuring
situationalawarenessinllms. arXivpreprintarXiv:2309.00667,2023a.
LukasBerglund,MegTong,MaxKaufmann,MikitaBalesni,AsaCooperStickland,Tomasz
Korbak,andOwainEvans. Thereversalcurse: Llmstrainedon”aisb”failtolearn”bis
a”. arXivpreprintarXiv:2309.12288,2023b.
YonatanBisk,RowanZellers,JianfengGao,YejinChoi,etal. Piqa: Reasoningaboutphysical
commonsense in natural language. In Proceedings of the AAAI conference on artificial
intelligence,volume34,pp.7432–7439,2020.
ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,and
KristinaToutanova.Boolq:Exploringthesurprisingdifficultyofnaturalyes/noquestions.
arXivpreprintarXiv:1905.10044,2019.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,
andOyvindTafjord.Thinkyouhavesolvedquestionanswering?tryarc,theai2reasoning
challenge. arXivpreprintarXiv:1803.05457,2018.
10RonanCollobert, JasonWeston, Le´onBottou, MichaelKarlen, KorayKavukcuoglu, and
Pavel Kuksa. Natural language processing (almost) from scratch. Journal of machine
learningresearch,12(ARTICLE):2493–2537,2011.
Gr’egoireDel’etang,AnianRuoss,Paul-AmbroiseDuquenne,ElliotCatt,TimGenewein,
ChristopherMattern,JordiGrau-Moya,WenliangKevinLi,MatthewAitchison,Laurent
Orseau, Marcus Hutter, and Joel Veness. Language modeling is compression. ArXiv,
abs/2309.10668,2023. URLhttps://api.semanticscholar.org/CorpusID:262054258.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
trainingofdeepbidirectionaltransformersforlanguageunderstanding. arXivpreprint
arXiv:1810.04805,2018.
QingyanGuo,RuiWang,JunliangGuo,XuTan,JiangBian,andYujiuYang. Mitigating
reversalcurseviasemantic-awarepermutationtraining,2024.
DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,and
JacobSteinhardt. Measuringmassivemultitasklanguageunderstanding. arXivpreprint
arXiv:2009.03300,2020.
GuillaumeLampleandAlexisConneau. Cross-linguallanguagemodelpretraining. arXiv
preprintarXiv:1901.07291,2019.
MikeLewis,MarjanGhazvininejad,GargiGhosh,ArmenAghajanyan,SidaWang,andLuke
Zettlemoyer. Pre-trainingviaparaphrasing. AdvancesinNeuralInformationProcessing
Systems,33:18470–18481,2020.
XianLi,PingYu,ChuntingZhou,TimoSchick,LukeZettlemoyer,OmerLevy,JasonWeston,
andMikeLewis. Self-alignmentwithinstructionbacktranslation. ArXiv,abs/2308.06259,
2023. URLhttps://api.semanticscholar.org/CorpusID:260866107.
EdwardLoperandStevenBird.Nltk:Thenaturallanguagetoolkit.arXivpreprintcs/0205028,
2002.
AngLv,KaiyiZhang,ShufangXie,QuanTu,YuhanChen,Ji-RongWen,andRuiYan. Are
wefallinginamiddle-intelligencetrap? ananalysisandmitigationofthereversalcurse.
arXivpreprintarXiv:2311.07468,2023.
Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly.
Rephrasingtheweb: Arecipeforcomputeanddata-efficientlanguagemodeling. arXiv
preprintarXiv:2401.16380,2024.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor
conduct electricity? a new dataset for open book question answering. arXiv preprint
arXiv:1809.02789,2018.
MarkEJNewman. Powerlaws,paretodistributionsandzipf’slaw. Contemporaryphysics,46
(5):323–351,2005.
OpenAI. Gpt-4technicalreport,2023.
Jacob Pfau, Alex Infanger, Abhay Sheshadri, Ayush Panda, Julian Michael, and Curtis
Huebner. Elicitinglanguagemodelbehaviorsusingreverselanguagemodels. InSocially
ResponsibleLanguageModellingResearch,2023.
KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi. Winogrande: An
adversarialwinogradschemachallengeatscale. CommunicationsoftheACM,64(9):99–106,
2021.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa:
Commonsensereasoningaboutsocialinteractions. arXivpreprintarXiv:1904.09728,2019.
StefanSchweterandAlanAkbik. Flert: Document-levelfeaturesfornamedentityrecogni-
tion,2020.
11RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrare
wordswithsubwordunits. arXivpreprintarXiv:1508.07909,2015.
C.E.Shannon. Amathematicaltheoryofcommunication. TheBellSystemTechnicalJournal,
27(3):379–423,1948. doi: 10.1002/j.1538-7305.1948.tb01338.x.
Zhihong Shao, Peiyi Wang, Qihao Zhu, R. X. Xu, Jun-Mei Song, Mingchuan Zhang,
Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathemati-
cal reasoning in open language models. ArXiv, abs/2402.03300, 2024. URL https:
//api.semanticscholar.org/CorpusID:267412607.
KoustuvSinha,RobinJia,DieuwkeHupkes,JoellePineau,AdinaWilliams,andDouwe
Kiela. Maskedlanguagemodelingandthedistributionalhypothesis: Orderwordmatters
pre-trainingforlittle. arXivpreprintarXiv:2104.06644,2021.
JacobMitchellSpringer,SuhasKotha,DanielFried,GrahamNeubig,andAditiRaghunathan.
Repetitionimproveslanguagemodelembeddings. arXivpreprintarXiv:2402.15449,2024.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timothe´e Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Openandefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,
2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,Lukas
Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude
Fernandes,JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,Naman
Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,
ViktorKerkez,MadianKhabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Marie-AnneLachaux,ThibautLavril,JenyaLee,DianaLiskovich,YinghaiLu,Yuning
Mao,XavierMartinet,TodorMihaylov,PushkarMishra,IgorMolybog,YixinNie,Andrew
Poulton, JeremyReizenstein, RashiRungta, KalyanSaladi, AlanSchelten, RuanSilva,
EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang,RossTaylor,
AdinaWilliams, Jian XiangKuan, Puxin Xu, Zheng Yan, IliyanZarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
SergeyEdunov,andThomasScialom. Llama2: Openfoundationandfine-tunedchat
models,2023b.
ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RussRSalakhutdinov,andQuocV
Le. Xlnet: Generalizedautoregressivepretrainingforlanguageunderstanding. Advances
inneuralinformationprocessingsystems,32,2019.
RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Cana
machinereallyfinishyoursentence? arXivpreprintarXiv:1905.07830,2019.
A Symbolicreversetaskdetails
Thevocabularyisbuiltbygenerating100differentwordsperposition,e.g. a200toa299
forthesecondwordofentities a. Thenwecreate10,000entities a and10,000entitiesb
i i j
byconcatenatingrandomwordsspecifictoeachposition. Finally,a entitiesarerandomly
i
mappedtob ,resultingin10,000pairs. Themodelisan8-layerTransformerwithahidden
j
sizeof512. Thetrainingiscontinuedfor500epochswithbatchsize=1024,learningrate
=0.0003,anddropoutrate=0.1.
B Transformermodelpre-training
Wetrainatransformermodelwithdim =2048,n layers =24,andn heads =16,resultingin
1.4Bparameters. TrainingdataandhyperparametersetupmostlyrepeatstheonefromTou-
vronetal.(2023b). Toadaptfortherelativelysmallermodelsize,weincreasethelearning
12rateto4.0e−4,andtheglobalbatch-sizewascappedat2Mduetothelimitednumberof
GPUs. During training, we observe a fixed gap in training perplexity between baseline
modelsandreversetraining(Figure1). Thelossofthebaselinemodelismeasuredondata
inthestandarddirection,whilethereversetraininglosscoversdatainbothdirections. We
positthatthereversetrainingdoesn’tinterferewithforwardlearning—thus,themodel’s
performance does not degrade on standard benchmarks in data-match conditions, and
becauseweobserveamatchintheconvergencerateofthereversetrainedmodelswiththe
baselinemodelwhenit’strainedonabout50%ofthedata.
Figure1:Traininglossfor1.4Bmodelsinthepre-trainingstage.Onthex-axiswedisplaythe
totalnumberoftokensmodelhasbeentrainedon,includingbothinstandardandreverse
direction.
InFigure2,weevaluateperformanceonthereal-worldknowledgetaskformultiplecheck-
pointsduringpre-training,whereaccuracyisreportedusingbest@1sampling. Wenoticean
upwardtrendinperformanceonthereversetaskwithnosaturationatthelastcheckpoint.
Hence,weassumethatifwecontinuepre-trainingwewouldseefurtherimprovement.
Figure2: Evaluationresultsduringtrainingonthereal-worldcelebritytaskwhenusing
differentpre-trainingmethodsforLLMs.
C BiographyDataExperimentDetails
Inourbiographydataexperiments,weutilizethebioSmulti5+permutedatasetfromAllen-
Zhu&Li(2023a;b)asourbioSdataset,whichgenerates5biographyentriesperpersonusing
randomly chosen sentence templates and permutations. We use the bioR multi5 dataset
13fromthemasourbioRdataset,whichgenerates5biographyentriesperpersonbyinvoking
Llamafivetimes.
FollowingAllen-Zhu&Li(2023b),weemployGPT2-small(12layers,12heads,and768
dimensions)forthebioSdatasetandGPT2with12layers,20heads,and1280dimensions
forthebioRdataset. WealsoutilizethesameAdamWoptimizerwithcosinelearningrate
decay(β =0.9,β =0.98,ε =10−6).
1 2
• ForthebioSdataset,wetrainfor80,000stepswithabatchsizeof192,whichistwice
theirbatchsize.
• ForthebioRdataset,wetrainfor150,000stepswithabatchsizeof192,whichistwice
theirbatchsize.
Duringpre-training(ormixed-training),weuseaweightdecayof0.03andselectthebest
among three learning rates: 0.0005, 0.001, 0.002; we also employ 1000 steps of learning
ratewarmup. Duringfinetuning(FT),weuseaweightdecayof0.01,andselectthebest
amongtwolearningrates: 0.0003or0.0005;wedonotuselearningratewarmup. During
mixed-training, we use QA = 0.3 which means 30% of the training tokens come from
r
instructionfinetunedata.
ReversalQAtasks. WeconsiderfourreversaltasksfromAllen-Zhu&Li(2023b):
• Givemethe[last/full]nameofthepersonbornonOctober2,1996? (bdatetolast,bdatetofull)
• Givemethe[last/full]nameofthepersonwhostudiedCommunicationsatMassachusettsInstituteofTechnologyandworked
forMetaPlatforms? (threetolast,threetofull)
• Givemethe[last/full]nameofthepersonwhostudiedCommunicationsatMassachusettsInstituteofTechnology,wasborn
inPrinceton,NJ,andworkedforMetaPlatforms? (fourtolast,fourtofull)
• Givemethe[last/full]nameofthepersonwhostudiedCommunicationsatMassachusettsInstituteofTechnology,wasborn
onOctober2,1996inPrinceton,NJ,andworkedforMetaPlatformsatMenloPark,CA? (alltolast,alltofull)
ForwardQAtasks. WeconsiderthesamesixforwardtasksfromAllen-Zhu&Li(2023a):
• WhatisthebirthdateofAnyaBriarForger? • WhatmajordidAnyaBriarForgerstudy?
Answer:October2,1996. Answer:Communications.
• WhatisthebirthcityofAnyaBriarForger? • WhichcompanydidAnyaBriarForgerworkfor?
Answer:Princeton,NJ. Answer:MetaPlatforms.
• WhichuniversitydidAnyaBriarForgerstudy? • WheredidAnyaBriarForgerwork?
Answer:MassachusettsInstituteofTechnology. Answer:MenloPark,CA.
FullresultsaresummarisedinTable10. Onemaynoticethatourreportedforwardtask
accuraciesareslightlyhigherthanthosereportedinAllen-Zhu&Li(2023a). Thisimprove-
mentisattributedtoouruseofalargerbatchsize,smallerweightdecay,andthebestresult
amongthreeruns.
14baseline 0.0 0.5 0.3 1.0 0.4 13.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2
bioS 100 100 100 100 99.9 99.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.2 0.2 0.1 0.2 0.2 0.2 0.1
bioS (token reversal) 100 100 100 100 99.6 99.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 37.5 63.7 29.5 62.8 6.6 48.1 0.2 0.2
bioS (word reversal) 100 100 100 100 99.7 99.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 84.2 99.3 77.8 99.0 19.3 91.1 0.2 0.1
bioS (entity reversal) 100 100 100 100 99.8 99.5 83.9 99.0 74.9 98.8 23.9 87.8 0.0 0.0 0.2 0.3 0.2 0.3 0.2 0.3 0.2 0.2
bioR 99.8 99.6 99.7 99.6 99.6 88.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2
bioR (token reversal) 99.8 99.5 99.6 99.5 99.5 86.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 53.6 61.5 23.2 58.2 10.0 53.3 0.2 0.2
bioR (word reversal) 99.8 99.5 99.6 99.4 99.3 84.3 0.0 0.2 0.0 0.1 0.0 0.1 0.0 0.1 97.3 99.2 63.6 98.5 27.1 94.9 0.3 0.2
bioR (entity reversal) 99.9 99.7 99.8 99.7 99.7 90.6 96.7 98.2 57.0 94.6 23.1 86.7 0.0 0.1 0.3 0.4 0.3 0.4 0.3 0.4 0.2 0.1
FT bdate FT bcity FT uni Fv T maj Fo Tr cname FT ccity FT all_ Mt Io X_ f au ll ll _ Ft To f_f ou ul Mrl I_t Xo f_f ou ul Frl _ T t to h_ rf Meul Iel X_ t to h_ rf eu Fl el T_ t bo d_f a Mtu Il el X_ t bo d_f atul el _to_full FT all_ Mt Io X_l a als lt _ Ft To f_l oa us Mrt I_t Xo f_l oa us Frt _ T t to h_l r Mea Is et X_ t to h_l rea Fs e Tt _ t bo d_l a Ma t Is e Xt _ t bo d_l aa ts et _to_last
Table10: Forwardvs. reversaltaskaccuracyforthedatabioS,bioR(Allen-Zhu&Li,2023b).
Leftblock=forwardQAaccuracy(askforfieldsgivenpersonnames).
Middleblock=reversalQAaccuracy(askforperson’sfullnamegivenselectedbiofields):
“bdate”=givenbirthdatesonly,“all’=givenallfields,etc,seeAppendixC).
Rightblock=reversalQAaccuracy(askforlastnamegivenselectedbiographyfields).
FT=pre-trainfollowedbyinstructionfinetune;MIX=addinstructionFTdatatopre-train.
15