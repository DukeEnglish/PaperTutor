An Ordering of Divergences for Variational Inference
with Factorized Gaussian Approximations
Charles C. Margossian1, Loucas Pillaud-Vivien2, and Lawrence K. Saul1
1Center for Computational Mathematics, Flatiron Institute
2CERMICS Laboratory, Ecole des Ponts ParisTech
Abstract
Given an intractable distribution p, the problem of variational inference (VI) is to
compute the best approximation q from some more tractable family . Most com-
Q
monly the approximation is found by minimizing a Kullback-Leibler (KL) divergence.
However,thereexistothervalidchoicesofdivergences,andwhen doesnotcontainp,
Q
each divergence champions a different solution. We analyze how the choice of diver-
gence affects the outcome of VI when a Gaussian with a dense covariance matrix is
approximated by a Gaussian with a diagonal covariance matrix. In this setting we
show that different divergences can be ordered by the amount that their variational
approximationsmisestimate variousmeasuresofuncertainty, suchas the variance, pre-
cision, and entropy. We also derive an impossibility theorem showing that no two of
these measures can be simultaneously matched by a factorized approximation; hence,
the choice of divergence informs which measure, if any, is correctly estimated. Our
analysiscoverstheKLdivergence, theR´enyidivergences, andascore-baseddivergence
that compares logp and logq. We empirically evaluate whether these orderings
∇ ∇
hold when VI is used to approximate non-Gaussian distributions.
Keywords: variational inference, Kullback-Leibler divergence, R´enyi divergence, Fisher di-
vergence, score matching
1 Introduction
In many statistical analyses, it is necessary to approximate an intractable distribution, p,
by a more tractable distribution, q. This problem can be posed as an optimization, one
whosegoalistodiscoverthebest-matchingdistributionq withinsomelarger,parameterized
family of approximating distributions. Optimizations of this sort are the subject of a
Q
large literature on variational inference (VI; Jordan et al., 1999; Wainwright and Jordan,
2008; Blei et al., 2017). VI is not the only way to approximate an intractable distribution,
but with a finite computational budget, it often achieves a smaller error than Markov chain
Monte Carlo methods, even if the latter possess better asymptotic properties (Robert and
Casella, 2004). In this sense, VI is well suited to the challenges posed by large data sets
and high-dimensional models. It is now widely applied to problems in Bayesian inference
and machine learning.
4202
raM
02
]LM.tats[
1v84731.3042:viXraDivergence Notation Definition
(cid:104) (cid:105)
(Reverse) Kullback-Leibler KL(q p) E log q
|| q p
(cid:104) (cid:105)
(Forward) Kullback-Lebiler KL(p q) E log p
|| p q
(cid:104) (cid:105)
R´enyi of order α (0,1) R (p q) 1 E pα 1
∈ α || α(α−1) q qα −
(Reverse) Score-based S(q ||p) E q(cid:104)(cid:13) (cid:13) ∇log pq(cid:13) (cid:13)2 Cov(q)(cid:105)
(Forward) Score-based S(p ||q) E p(cid:104)(cid:13) (cid:13) ∇log pq(cid:13) (cid:13)2 Cov(p)(cid:105)
Table1: Divergences that we consider in this paper for VI. The score-based divergences (Cai
et al., 2024) are defined here for the special case that the base distribution is Gaussian.
Despite its widespread adoption, many aspects of VI remain poorly or incompletely
understood. At the heart of VI is the question of what constitutes a useful approxima-
tion of p. Suppose we wish to approximate a target distribution p over latent variables
z = z , and let be the family of approximating distributions. VI attempts to solve
1:n
∈ Z Q
an optimization of the form
q∗ = argmin D(q,p), (1)
q∈Q
where D is a divergence satisfying (i) D(q,p) 0 for all q and (ii) D(q,p) = 0 if and
≥ ∈ Q
only if p = q.
InprincipleanyproperdivergenceD(q,p)maybeusedforVIineq.(1). Byfarthemost
popular choice is the Kullback-Leibler divergence, KL(q p), which measures the disagree-
||
ment between logp and logq, but it is not the only one that can be considered. Other valid
choices include the “forward” KL(p q), which arises in expectation propagation (Vehtari
||
etal.,2020), theR´enyiorα-divergencesR (p q)(Minka,2005;LiandTurner,2016), which
α
||
interpolate between KL(q p) and KL(p q) when α (0,1), and score-based (or Fisher) di-
|| || ∈
vergences(Courtade,2016;Caietal.,2024),whichmeasurethediscrepancybetween logp
∇
and logq. Table 1 provides an overview of the divergences we analyze in this paper.
∇
All of these divergences vanish if and only if q=p, but when p / Q, as is typically the
∈
case in VI, the solution of eq. (1) can vary dramatically from one divergence to the other.
We illustrate this when p is a two-dimensional Gaussian with correlation ε = 0.75 and
Q
is the family of Gaussians with diagonal covariance matrices; see Fig. 1. The goal of this
paper is to understand which divergence works best for a given application.
For VI to be useful, these divergences must align with inferential goals. These goals
may vary from one application to the next. In Bayesian statistics, the aim is often to
calculate the posterior mean, variance, and quantiles of interpretable univariate quantities
(Gelman et al., 2013); in certain applications, it may also or alternatively be of interest
to calculate the posterior precision (or inverse covariance) matrix (Bernardo and Smith,
2000). In statistical physics, the aim is generally to compute the free energy (Parisi, 1988),
which is itself equal to the difference of the internal energy and the entropy. In this paper,
we consider three distinct measures of uncertainty—(i) the marginal variances, (ii) the
marginal precisions, and (iii) the multivariate entropy—and for each of these measures, we
analyze how the fidelity of VI depends on the divergence that it attempts to minimize.Divergence Match
p
S(p q)
||
KL(p q) Variance
||
R (q p) Entropy
α
||
KL(q p) Precision
||
S(q p)
||
Q Target p
a. b. z 1
Figure1: (a) The optimal projection of p onto depends on the divergence used to measure
Q
disagreement between p and q . (b) Optimal factorized approximations q to a two-
∈ Q
dimensional Gaussian p with off-diagonal correlation, as measured by different divergences.
The marginal variances are matched by minimizing KL(p q), the marginal precisions by
||
minimizing KL(q p), and the entropy (in this example) by minimizing R (p q) with α=0.5.
α
|| ||
Our work is in line with several efforts to evaluate VI through the lens of downstream
inferential tasks. The mean and variances of p are of most interest for Bayesian statistics,
but Huggins et al. (2020) caution that a low KL(q p) or R (p q) divergence does not
α
|| ||
guaranteethatq accuratelyestimatesthesequantities. Insteadtheyrecommendtoestimate
the Wasserstein distance, which can be used to bound the difference in moments of q and p;
see also Biswas and Mackey (2024). In latent variable models, VI is used to bound a
marginallikelihoodwhichisthenmaximizedwithrespecttothemodelparameters(e.g.,the
weights of a neural network (Tomczak, 2022)). Several works examine the tightness of this
bound (e.g Li and Turner, 2016; Dieng et al., 2017; Daudel et al., 2023) and the statistical
properties of the resulting maximum likelihood estimator (Wang and Blei, 2018). VI is also
usedintandemwithotherinferenceschemes—forexample,toinitializeMarkovchainMonte
Carlo (Zhang et al., 2022) or to estimate a proposal distribution for importance sampling.
Forthelatter, thequalityofVIcanbeassessedfromthedistributionofimportanceweights
(Yaoetal.,2018;Vehtarietal.,2024). Insum,therearemanydifferentwaystoevaluateVI,
and one lesson of our paper is that different inferential goals may be incompatible—may,
in fact, compete against one another.
InmostapplicationsofVI,thevariationalfamilydoesnotincludethetargetdistribution
(i.e., p ), and the resulting inference must be compromised in some way. The simplest
̸∈ Q
variational families are those that only contain factorized distributions,
n
(cid:89)
q(z) = q (z ). (2)
i i
i=1
Variational families of this form originated in the mean-field approximations for physical
models of ferromagnetism (Parisi, 1988). In practice we do not expect p to factorize; the
assumption is made out of computational convenience. When z Rn, a common choice
∈
for q is that of a Gaussian density (e.g Kucukelbir et al., 2017). The resulting algorithm
i
is called factorized Gaussian VI (FG-VI). By its very nature, FG-VI cannot estimate the
z 2correlations between the different components of z. Still, at least in theory, it is possible
to construct a factorized Gaussian that matches the variance, precision, or entropy of p
and, in this sense, construct an approximation that (however misleading it may be in other
respects) achieves its primary inferential goal to estimate one of these quantities.
Inthispaper,weanalyzehowthechoiceofdivergenceineq.(1)affectstheabilityofFG-
VI to meet these goals. We focus on the case where p is a Gaussian distribution with a non-
diagonal covariance matrix. This setting, though simple, is already rich enough to provide
several counter-intuitive results, and it reveals in a stark way that different inferential goals
are best served by different divergences. Our analysis is a natural first step in the study
of the inherent trade-offs of factorized VI. There has been previous work (MacKay, 2003;
TurnerandSahani,2011;MargossianandSaul,2023)inthesettingwherepisGaussian,but
here we provide a more thorough treatment, formalizing existing intuitions and extending
many earlier results. Our contributions are as follows:
• In Section 2, we present an impossibility theorem: for any choice of the divergence
in eq. (1), the resulting approximation q from FG-VI cannot simultaneously match
any two of the variance, precision, and entropy of the target distribution p, and hence
the choice of divergence determines which (if any) of these measures of uncertainty is
correctly estimated.
• In Section 3, we study the optimizations that arise in FG-VI when p is Gaussian,
and we analyze the solutions to these optimizations for all of the divergences that
appear in Table 1. It has been shown previously that FG-VI matches the marginal
precisionsofpwhenminimizingKL(q p)andthemarginalvarianceswhenminimizing
||
KL(p q) (MacKay, 2003; Turner and Sahani, 2011); we review these results in light
||
of the impossibility theorem of Section 2. Later, with the additional machinery of
Section 4, we show that there always exists a unique α (0,1), determined by the
∈
covarianceofp, suchthatFG-VImatchestheentropyofpwhenminimizingtheR´enyi
divergence R (p q). In this section, we also show that FG-VI can wildly misestimate
α
||
the marginal variances when minimizing the score-based divergences; in particular,
we derive conditions under which it estimates a vanishing variance when minimizing
S(q p) and an infinite variance when minimizing S(p q). We refer to such cases of
|| ||
FG-VI as instances of variational collapse, where the variational approximation is not
well-defined because arginf D(q,p) .
q∈Q
̸∈ Q
• Ourmainresult,presentedinSection4,isanordering ofallthedivergencesinTable1
basedontheestimatestheyyieldofthemarginalvariances. Thisorderingalsoinduces
corresponding orderings based on the marginal precisions and the entropy.
• In Section 5, we experiment with FG-VI on several data sets and investigate numeri-
cally whether these orderings hold for target distributions that are not Gaussian.
2 Impossibility theorem for FG-VI
First we explore various trade-offs when one normal distribution, p, whose covariance ma-
trix has off-diagonal elements, is approximated by another normal distribution, q, whosecovariance matrix is diagonal. Let
p = normal(µ,Σ), (3)
q = normal(ν,Ψ), (4)
where Ψ is diagonal but Σ is not; here we have in mind the setting of FG-VI where p is the
target distribution and q is the variational approximation. Nevertheless we emphasize that
the results in this section are not tied to any particular choice of the divergence D(q,p) in
eq. (1). Rather, these trade-offs are inherent to the use of a factorized approximation, and
they arise regardless of how q is chosen.
The main result of this section is an impossibility theorem: the theorem considers
three measures of p’s uncertainty—its marginal variances, its marginal precisions, and its
entropy—and states that if p has a non-diagonal covariance matrix, then no factorized
approximationqcancorrectlyestimateanytwoofthesemeasuresatthesametime. Tostate
the theorem, the following notation is useful. We denote the entropy of a distribution by
(p) = E [logp], (5)
p
H −
and we remind the reader that the precision matrix is given by the inverse of the covariance
matrix. Also, for any square matrix, we use diag( ) to denote the vector formed from its
·
diagonal elements, and we use vector inequalities, such as diag(Ψ) diag(Σ), to denote
≤
that Ψ Σ for all i. With this notation, we have the following result.
ii ii
≤
Theorem 2.1 (Impossibility theorem for FG-VI). Let p and q be normal distributions
overRN withcovariancesΣandΨ, respectively, whereΨisdiagonalbutΣisnot.Then
1. (Variance matching)
If diag(Ψ)=diag(Σ), then (q)> (p) and diag(Ψ−1) diag(Σ−1), and this last
H H ≤
inequality is strict for at least one component along the diagonal.
2. (Precision matching)
If diag(Ψ−1)=diag(Σ−1), then (q)< (p) and diag(Ψ) diag(Σ), and this last
H H ≤
inequality is strict for at least one component along the diagonal.
3. (Entropy matching)
If (q)= (p), then Ψ <Σ and Ψ−1<Σ−1 for at least one i and j.
H H ii ii jj jj
The theorem states that when q matches p in any one of the above ways, it cannot also
match p in any of the others (except in the trivial case that Σ is diagonal). We prove the
theorem with three lemmas, each of which examines the trade-off in FG-VI between two
different measures of uncertainty.
2.1 Variance-entropy tradeoff
The trade-off between variance and entropy has been previously studied in the classical
setting of FG-VI where q is chosen to minimize KL(q p). There it was noted that the en-
||
tropy of q is governed by a shrinkage-delinkage trade-off (Margossian and Saul, 2023): theentropy of q is decreased when q underestimates the marginal variances of p (the shrink-
age effect), but it is increased when q ignores the correlations in p between off-diagonal
components (the delinkage effect). We use similar tools to prove the following result.
Lemma2.2(Variance-entropytrade-off). Ifpineq.(3)hasanon-diagonalcovariance,
then q in eq. (4) with a diagonal covariance cannot correctly match both its marginal
variancesandentropy: i.e., ifdiag(Ψ)=diag(Σ), then (q)> (p), andif (q)= (p),
H H H H
then Ψ <Σ for some i.
ii ii
Proof. We begin by defining two auxiliary matrices whose elements are dimensionless (un-
like those of Σ and Ψ). The first of these is the correlation matrix C Rn×n of the target
∈
distribution p, with elements
Σ
ij
C = , (6)
ij (cid:112)
Σ Σ
ii jj
which is positive-definite and has unit diagonal entries, C =1. The second is the diagonal
ii
matrix R Rn×n that records the ratios of marginal variances in q and p; i.e.,
∈
Ψ
ii
R = . (7)
ii
Σ
ii
Next we consider the difference in entropy of p and q. From basic properties of Gaussian
distributions (Cover and Thomas, 2006) and the definitions in eqs. (6–7), it follows that
(cid:104) (cid:105) (cid:104) (cid:105)
(p) (q) = 1 log Σ (2πe)n 1 log Ψ (2πe)n ,
H −H 2 | | − 2 | |
= 1 log Σ 1 log Ψ ,
2 | |− 2 | |
(cid:12) (cid:12)
= 1
2
log(cid:12) (cid:12)Ψ−1 2ΣΨ−1 2(cid:12) (cid:12),
(cid:12) (cid:12)
= 21 log(cid:12) (cid:12)R−1 2CR−1 2(cid:12) (cid:12), (8)
= 1 log R −1 1 log C −1. (9)
2 | | − 2 | |
The first determinant in eq. (9) is easy to compute because the matrix R, as defined in
eq. (7), is purely diagonal. Notably, we can bound the second determinant in eq. (9) by the
Hadamard inequality (Horn and Johnson, 2012, Theorem 7.8.1), which states that
n
(cid:89)
C < C = 1. (10)
ii
| |
i=1
We note that the inequality in eq. (10) is strict under the assumption of the lemma that Σ
(and hence also C) is not diagonal. Combining the last two results, we see that
(cid:18) (cid:19)
1 (cid:88) Σ ii
(p) (q) < log . (11)
H −H 2 Ψ
ii
i
Now suppose that diag(Ψ)=diag(Σ); then from eq. (11) we see that (p)< (q), proving
H H
the first claim of the lemma. Conversely, suppose that (p)= (q); then from eq. (11)
(cid:80) (cid:80) H H
we see that logΣ > logΨ , and this can only be true if Σ >Ψ for some i. This
i ii i ii ii ii
proves the second claim of the lemma.2.2 Precision-entropy trade-off
Next we derive a similar trade-off between precision and entropy. This trade-off is captured
by the following lemma.
Lemma 2.3 (Precision-entropy trade-off). If p in eq. (3) has a non-diagonal covari-
ance, then q in eq. (4) with a diagonal covariance cannot correctly match both its
marginal precisions and entropy: i.e., if diag(Ψ−1)=diag(Σ−1), then (q)< (p),
H H
and if (q)= (p), then Ψ−1<Σ−1 for some i.
H H ii ii
Proof. Theprooffollowsthesamestructureastheproofoftheprecedinglemma. Wedefine
auxiliary matrices C˜ and R˜ with the dimensionless elements
Σ−1 Ψ−1
C˜ = ij , R˜ = ii , (12)
ij (cid:113)
Σ−1Σ−1
ii Σ− ii1
ii jj
where C˜ has all ones along the diagonal and R˜ has all zeroes off the diagonal. Again we
examine the difference in entropy between p and q. Proceeding from eq. (8), we find that
(cid:12) (cid:12) (cid:12) (cid:12)
H(p) −H(q) = 21 log(cid:12) (cid:12)R−1 2CR−1 2(cid:12)
(cid:12)
= 1
2
log(cid:12) (cid:12)R˜ 1 2C˜−1R˜ 21(cid:12)
(cid:12)
= 1
2
log |R˜
|−
1
2
log |C˜ |. (13)
As before, the difference in entropy is expressed in terms of two determinants. The deter-
minant of R˜ is easy to compute because R˜, as defined in eq. (12), is a diagonal matrix.
Again we deduce from the Hadamard inequality that C˜ < (cid:81) C˜ = 1. Thus we see that
| | i ii
1 (cid:88)
(cid:18) Ψ−1(cid:19)
(p) (q) > log ii . (14)
H −H 2 Σ−1
i ii
Note that the direction of inequality in eq. (14) is reversed from the direction of inequality
in eq. (11). Now suppose that diag(Ψ−1) = diag(Σ−1); then from eq. (14) we see that
(p)> (q), proving the first claim of the lemma. Conversely, suppose that (p)= (q);
H then froH m eq. (14) we see that (cid:80) logΣ−1 > (cid:80) logΨ−1, and this can onlH y be trH ue if
i ii i ii
Σ−1>Ψ−1 for some i. This proves the second claim of the lemma.
ii ii
2.3 Variance-precision trade-off
Finally, we derive a similar trade-off between variance and precision. This trade-off is
captured by the following lemma.
Lemma 2.4 (Variance-precision trade-off). If p in eq. (3) has a non-diagonal co-
variance, then q in eq. (4) with a diagonal covariance cannot correctly match both its
marginalvariancesandprecisions: i.e., ifdiag(Ψ)=diag(Σ), thendiag(Ψ−1) diag(Σ−1)
≤
and this inequality is strict for at least one component along the diagonal; likewise, if
diag(Ψ−1)=diag(Σ−1), then diag(Ψ) diag(Σ) and this inequalitiy is strict for at least
≤
one component along the diagonal.Proof. Let C denote the correlation matrix in eq. (6), and let λ ,λ ,...,λ denote its
1 2 n
eigenvalues. Also, let e denote the unit vector along the ith axis. Since C =1, we have
i ii
C−1 = e⊤(C−1+C)e 1, (15)
ii i i −
(cid:104) (cid:105)
min e⊤(C−1+C)e 1 , (16)
≥ ∥e∥=1 −
(cid:16) (cid:17)
= min λ−1+λ 1 , (17)
j j j −
min(cid:0) λ−1+λ 1(cid:1) , (18)
≥ λ>0 −
= 1, (19)
where in eq. (17) we have exploited the properties of Rayleigh quotients. Next, starting
from eq. (6), we can verify by direct calculation that C−1 = Σ−1(cid:112) Σ Σ , and in particular
ij ij ii jj
along the diagonal (when i=j), we have
C−1 = Σ−1Σ . (20)
ii ii ii
Now suppose that the variances of p and q match; then combining eq. (20) and the previous
inequality, we see that
Σ−1
1 C−1 = Σ−1Σ = Σ−1Ψ = ii , (21)
≤ ii ii ii ii ii Ψ−1
ii
and we conclude (since i was arbitrary) that diag(Ψ−1) diag(Σ−1). Conversely, suppose
≤
that the precisions of p and q match; then by the same reasoning we see that
Σ
1 C−1 = Σ−1Σ = Ψ−1Σ = ii , (22)
≤ ii ii ii ii ii Ψ
ii
and again we conclude (since i was arbitrary) that diag(Ψ) diag(Σ). Finally, we recall
≤
that by assumption Σ (and hence C) is not diagonal; furthermore, if C = 0, then the unit
ij
̸
vector e is not an eigenvector of C. In this case, eq. (16) becomes a strict inequality, and
j
for the jth component along the diagonal we see that C−1>1. But then eqs. (21–22) also
jj
becomestrictinequalities, showingthatΨ−1<Σ−1 whenthevariancesmatchandΨ <Σ
jj jj jj jj
when the precisions match. This proves the lemma.
Remark 2.5. The proof of Lemma 2.4 does not use the fact that p and q are Gaussian,
and so the result extends to any factorized distribution q and non-factorized distribu-
tion p with finite second moments.
From the preceding lemma, we can also extract a more general result of linear algebra
(Horn and Johnson, 2012, Theorem 7.17) that will prove especially useful in Section 4.
Note that for any covariance matrix Σ and correlation matrix C, we have shown above
that diag(C−1) 1; also, this inequality is strict for at least one component along the
≥
diagonal when Σ is not itself diagonal. Since C−1=Σ Σ−1, we have therefore shown the
ii ii ii
following.Proposition 2.6. Let A Rn×n. If A 0, then
∈ ≻
A A−1 1 for all i, (23)
ii ii ≥
A A−1 > 1 for some j; (24)
jj jj
in particular, the inequality is strict along the jth component whenever (cid:80) A > 0.
k̸=j| jk |
To conclude the section, we note the following: we obtain the Impossibility Theorem
for FG-VI merely by piecing together the results of the preceding lemmas. In particular,
Lemma 2.2 proves the first claims of Theorem 2.1 under the conditions for variance and
entropymatching,Lemma2.4provesthesecondclaimsofTheorem2.1undertheconditions
for variance and precision matching, and Lemma 2.3 proves the remaining claims.
3 Divergences for FG-VI
In this section we derive the solutions for FG-VI that are obtained from eq. (1) by mini-
mizing the divergences in Table 1. We also highlight various properties of these solutions.
As in the previous section, we assume everywhere that p and q are given by eqs. (3–4),
where Ψ is diagonal but Σ is not.
3.1 KL divergence
Most VI is based on minimizing the reverse KL divergence in Table 1, and the implications
of this choice for FG-VI have been extensively studied. For completeness, we review certain
keyresultsinthenotationofthispaper. Forpandq ineqs.(3–4),thereverseKLdivergence
is given by (Cover and Thomas, 2006)
(cid:20) (cid:21)
1 Σ
KL(q p) = (ν µ)⊤Σ−1(ν µ)+log | | +Tr(Σ−1Ψ) n . (25)
|| 2 − − Ψ −
| |
Withrespecttothisdivergence,thebestfactorizedapproximationq isobtainedbychoosing
the parameters ν and Ψ that minimize eq. (25). The variational mean appears only in the
first term of eq. (25), which is minimized by setting ν = µ, and the remaining terms are
minimized by setting
diag(Ψ−1) = diag(Σ−1). (26)
Further details of these calculations can be found in the references (Turner and Sahani,
2011; Margossian and Saul, 2023).
It has been widely observed that VI based on the reverse KL divergence tends to un-
derestimate uncertainty (MacKay, 2003; Minka, 2005; Turner and Sahani, 2011; Blei et al.,
2017; Giordano et al., 2018). We can now refine and qualify this statement in the setting
of FG-VI where both p and q are Gaussian. In this case we have the following:
Proposition 3.1 (Precision matching). If q in eq. (4) has a diagonal covariance but p
in eq. (3) does not, then the reverse KL divergence in eq. (25) is minimized when qmatches the mean and marginal precisions of p but strictly underestimates the joint
entropy and underestimates all of the marginal variances (at least one of them strictly).
The proof is immediate: the matching of precisions follows from eqs. (26), and the underes-
timationofvariancesandentropyfollowsfromtheImpossibilityTheoreminSection2. This
is the first of several results showing that the “uncertainty deficit” of FG-VI depends on
the measure that is used to quantify uncertainty. Indeed, when p is Gaussian, the marginal
precisions are correctly estimated by minimizing the reverse KL divergence; moreover, it
has been shown that when Σ has constant off-diagonal entries, the entropy gap between
p and q only grows as (logn), while the entropy itself grows as (n), meaning that the
O O
fractional entropy gap tends to 0 as n (Margossian and Saul, 2023, Theorem 3.6).
→∞
A similar result is obtained in the thermodynamic limit of Ising models with constant,
long-range interactions, where the mean-field approximation has its roots (Parisi, 1988).
NextweconsidertheforwardKLdivergenceinTable1. Thisdivergenceisnotgenerally
minimized for VI because it involves an expectation, namely E [log p] with respect to
p q
the target distribution p. When p is not tractable, it is not possible to compute this
expectation analytically, and it may be too expensive to estimate it by sampling. However,
it is sometimes possible, as is done in expectation propagation (Vehtari et al., 2020), to
match moments between q and an intermediate distribution q , which equals q save for
\i
its ith factor, which is replaced by the ith factor of p. This is equivalent to minimizing
KL(q q). It is not true, however, that we minimize KL(p q) by iterating through all the
\i
|| ||
factors in this way.
It is illuminating to contrast the properties of the reverse and forward KL divergences
for FG-VI. For p and q in eqs. (3–4), the forward KL divergence can be obtained from
eq. (25) simply by swapping the parameters of these distributions. In this way we see that
(cid:20) (cid:21)
1 Ψ
KL(p q) = (µ ν)⊤Ψ−1(µ ν)+log | | +Tr(Ψ−1Σ) n . (27)
|| 2 − − Σ −
| |
This divergence is similarly minimized by setting ν = µ, but now we obtain a different
solution when the remaining terms are minimized with respect to Ψ. In particular, the
best factorized approximation minimizing eq. (27) is obtained when
diag(Ψ) = diag(Σ); (28)
namely, the forward KL divergence is minimized by matching the marginal distributions of
p and q; a similar observation is made in MacKay (2003). By combining this observation
with the Impossibility Theorem, we obtain a counterpart to our previous result:
Proposition 3.2 (Variance matching). If q in eq. (4) has a diagonal covariance but p
in eq. (3) does not, then the forward KL divergence in eq. (27) is minimized when q
matches the mean and marginal variances of p but strictly overestimates the joint en-
tropy and underestimates all of the marginal precisions (at least one of them strictly).3.2 R´enyi divergence
Next we consider the R´enyi divergence of order α (or the α-divergence) in Table 1,
1 (cid:90) (cid:18) pα(z) (cid:19)
R (p q) = 1 q(z)dz, (29)
α || α(α 1) qα(z) −
−
The α-divergence has also been studied in the context of VI (Li and Turner, 2016). It can
be defined in various ways, but these various definitions have in common that they all yield
the same underlying optimization for VI. We use the above definition (Cichocki and Amari,
2010) to recover the KL divergences in the previous sections as limiting cases:
lim R (p q) = KL(q p), (30)
α
α→0 || ||
lim R (p q) = KL(p q). (31)
α
α→1 || ||
Thus for α (0,1), the R´enyi divergences provide a one-parameter family of divergences
∈
interpolating between KL(q p) and KL(p q). Likewise, for α = 2, eq. (29) recovers the
|| ||
χ2-divergence, which can also be minimized for approximate inference (Dieng et al., 2017).
Next we consider FG-VI based on minimizing the α-divergence in eq. (29). We restrict
our attention to the case α (0,1), and as shorthand, we define the matrices
∈
Σ = αΨ+(1 α)Σ, (32)
α
−
Φ = αΣ−1+(1 α)Ψ−1, (33)
α
−
which, for α (0,1), are convex combinations of the covariance and precision matrices of p
∈
and q in eqs. (3-4). Note that these matrices are also related by the identity Σ = ΣΦ Ψ.
α α
In terms of these matrices, the α-divergence in eq. (29) is given by
R α(p q) =
1 (cid:104) e−α(1 2−α)(µ−ν)⊤Σ− α1(µ−ν)
Σ α
−1
2 Ψ
α
2 Σ
1− 2α 1(cid:105)
, (34)
|| α(α 1) | | | | | | −
−
anditisonlydefinedforvaluesofαsuchthatΦ 0(Burbea,1984;LieseandVajda,1987;
α
≻
Hobza et al., 2009; Gil et al., 2013). (If Φ is not positive definite, then the integral in
α
eq. (29) does not exist, and the α-divergence is undefined; when α (0,1), it is always the
∈
case that Φ 0 by convexity of the positive-definite matrix cone.) With this notation,
α
≻
we have the following result.
Proposition 3.3 (R´enyi fixed-point equations). Let α (0,1), and let p and q be given
∈
by eqs. (3–4) where Ψ is diagonal. Then the R´enyi divergence in eq. (29) is minimized
when ν = µ and the estimated precisions from Ψ satisfy the fixed-point equation
diag(cid:0) Ψ−1(cid:1)
=
diag(cid:0) Σ−1(cid:1)
, (35)
α
or equivalently when the estimated variances from Ψ satisfy the fixed-point equation
diag(Ψ) =
diag(cid:0) Φ−1(cid:1)
. (36)
αProof. When α (0,1), the prefactor 1/α(α 1) in eq. (34) is negative, and the expression
∈ −
asawholeisminimizedbymaximizing thefirstterminbrackets. Equivalently, upontaking
logarithms, we see that
(cid:104) (cid:105) (cid:104) (cid:105)
argmin R (p q) = argmin α(1 α)(µ ν)⊤Σ−1(µ ν)+log Σ αlog Ψ . (37)
α α α
|| − − − | |− | |
q∈Q ν,Ψ
The first term on the right is minimized (and zeroed) by setting ν=µ, thus matching the
means of q and p. Eliminating this term, and substituting for Σ from eq. (32), we see that
α
(cid:104) (cid:105) (cid:104) (cid:105)
argmin R (p q) = argmin log αΨ+(1 α)Σ αlog Ψ . (38)
α
|| | − |− | |
q∈Q Ψ
Theexpressionontherightofeq.(38)divergesifanydiagonalelementΨ vanishesor goes
ii
to infinity, so the minimum must exist at some Ψ 0. Solving for the minimum, we find
≻
∂ (cid:105)
0 = [log αΨ+(1 α)Σ αlog Ψ , (39)
∂Ψ | − |− | |
ii
= α[αΨ+(1 α)Σ]−1 αΨ−1, (40)
− ii − ii
= α(cid:2) Σ−1 Ψ−1(cid:3) , (41)
α − ii
thus proving eq. (35). Finally, we observe that as α 0, we recover the precision-matching
→
solution obtained when minimizing KL(q p).
||
To derive the fixed-point equation in eq. (36), we rewrite eq. (38) as
(cid:104) (cid:105) (cid:104) (cid:105)
argmin R (p q) = argmin log αΨ+(1 α)Σ αlog Ψ +log Ψ log Ψ , (42)
α
|| | − |− | | | |− | |
q∈Q Ψ
=
argmin(cid:104) log(cid:12)
(cid:12)α+(1
α)ΣΨ−1(cid:12)
(cid:12) (1
α)log(cid:12) (cid:12)Ψ−1(cid:12) (cid:12)(cid:105)
, (43)
− − −
Ψ
= argmin(cid:104) log(cid:12) (cid:12)αΣ−1+(1 α)Ψ−1(cid:12) (cid:12) (1 α)log(cid:12) (cid:12)Ψ−1(cid:12) (cid:12)(cid:105) , (44)
− − −
Ψ−1
where in the last step we have exploited the fact that additive constants, such as
log(cid:12) (cid:12)Σ−1(cid:12)
(cid:12),
do not change the location of the minimum with respect to Ψ (or equivalently, with respect
to Ψ−1). Solving for the minimum, we find that
0 = ∂ (cid:104) log(cid:12) (cid:12)αΣ−1+(1 α)Ψ−1(cid:12) (cid:12) (1 α)log(cid:12) (cid:12)Ψ−1(cid:12) (cid:12)(cid:105) = (1 α)(cid:2) Φ−1 Ψ(cid:3) , (45)
∂Ψ−1 − − − − α − ii
ii
which is equivalent to the claim that diag(Ψ) =
diag(cid:0) Φ−1(cid:1)
in eq. (36). Finally, we observe
α
that,asα 1,thissolutionrecoversthevariance-matchingsolutionobtainedbyminimizing
→
KL(p q).
||
Proposition 3.3 provides two (equivalent) fixed-point equations for the variational pa-
rameter Ψ, and it will prove useful to work with both in Section 4. In componentwise
notation, these fixed-point equations take the form
Ψ−1 = (cid:2) αΨ+(1 α)Σ(cid:3)−1 , (46)
ii − ii
Ψ = (cid:2) αΣ−1+(1 α)Ψ−1(cid:3)−1 , (47)
ii − ii
and again we see that eq. (46) reduces to the precision-matching solution Ψ−1 = Σ−1 as
ii ii
α 0 and eq. (47) reduces to the variance-matching solution Ψ = Σ as α 1.
ii ii
→ →3.3 Score-based divergence
Next we consider the score-based divergences presented in Table 1. These divergences are
similar to the relative Fisher information (Courtade, 2016, eq. (7)) except that they use
a weighted norm (Cai et al., 2024, Appendix A) to measure the difference between the
gradients of logp and logq. When p and q are Gaussian, these score-based divergences are
given by
(cid:90)
(cid:13) (cid:13)2
S(q p) = dzq(z)(cid:13) logq(z) logp(z)(cid:13) , (48)
|| ∇ −∇ Cov(q)
(cid:90)
(cid:13) (cid:13)2
S(p q) = dzq(z)(cid:13) logq(z) logp(z)(cid:13) , (49)
|| ∇ −∇ Cov(p)
where as shorthand we use v 2 = v⊤Av to denote the weighted norm for any A 0.
∥ ∥A ≻
The score-based divergences in eqs. (48-49) are dimensionless measures of the discrepancy
between p and q, and in particular, like the KL and R´enyi divergences in the previous
sections,theyareinvarianttoaffinereparameterizationsofthesupportofthesedistributions
(Cai et al., 2024, Theorem A.4). For p and q in eqs. (3–4), these divergences are given by
(cid:104) (cid:105)
S(q p) = tr (cid:0) I ΨΣ−1(cid:1)2 +(ν µ)⊤Σ−1ΨΣ−1(ν µ), (50)
|| − − −
(cid:104) (cid:105)
S(p q) = tr (cid:0) I ΣΨ−1(cid:1)2 +(µ ν)⊤Ψ−1ΣΨ−1(µ ν), (51)
|| − − −
as also shown in Cai et al. (2024). The rest of this section examines the solutions that min-
imize these expressions with respect to the variational mean ν and diagonal covariance Ψ.
First we consider the minimization of the reverse score-based divergence, S(q p). This
||
minimization can be formulated as a nonnegative quadratic program (NQP)—that is, a
convex instance of quadratic programming with nonnegativity constraints.
Proposition 3.4 (NQP for minimizing S(q p)). Let p and q be given by eqs. (3–4)
||
where Ψ is diagonal. Then the reverse score-based divergence in eq. (50) is minimized
by setting ν=µ and solving the quadratic program
(cid:104) (cid:105)
min 1s⊤Hs 1⊤s , (52)
s≥0 2 −
where s is constrained to lie in the nonnegative orthant, 1 is the vector of all ones, H
is the positive-definite matrix with elements
(cid:16) (cid:17)2
Σ−1
ij
H = , (53)
ij Σ−1Σ−1
ii jj
and Ψ is obtained from the solution of eq. (52) by identifying s = Ψ Σ−1.
i ii ii
Proof. It is clear that eq. (50) is minimized by setting ν=µ in the rightmost term. For the
remaining term, we note that
1 tr(cid:104) (cid:0) I ΨΣ−1(cid:1)2(cid:105) = n (cid:88) Ψ Σ−1 + 1 (cid:88) Ψ Ψ (cid:16) Σ−1(cid:17)2 , (54)
2 − 2 − ii ii 2 ii jj ij
i ijand the NQP in eq. (52) is obtained by defining s = Ψ Σ−1. Finally, we observe that
i ii ii
the matrix H in eq. (53) is always positive-definite. Indeed, H is the Hadamard (i.e.,
component-wise) square of the correlation matrix built from the precision matrix Σ−1.
Positive-definiteness of H follows then from the Schur product theorem that the Hadamard
product of two positive-definite matrices is also positive-definite.
Given the symmetry of eqs. (50-51), it is not surprising that we obtain an analogous
optimization when minimizing the forward score-based divergence, S(p q), in terms of the
||
variational parameters ν and Ψ. In particular, we have the following result.
Proposition 3.5 (NQP for minimizing S(q p)). Let p and q be given by eqs. (3–4)
||
where Ψ is diagonal. Then the forward score-based divergence in eq. (50) is minimized
by setting ν=µ and solving the quadratic program
(cid:104) (cid:105)
min 1t⊤Jt 1⊤t , (55)
t≥0 2 −
where t is constrained to lie in the nonnegative orthant, J is the positive-definite matrix
with elements
(Σ )2
ij
J = , (56)
ij
Σ Σ
ii jj
and Ψ is obtained from the solution of eq. (55) by identifying t = Ψ−1Σ .
i ii ii
Proof. The proof follows the same steps as in the previous proposition, but with the ma-
trices Ψ and Σ playing the reverse roles as they did in eq. (54).
We conclude this section by noting a peculiar property of the score-based divergences in
Table 1. It is possible for the solutions of the NQPs in Propositions 3.4 and 3.5 to lie on the
boundary of the nonnegative orthant. That is, there may exist some ith component along
the diagonal of Ψ such that S(q p) is minimized by setting Ψ =0 (in Proposition 3.4) or
ii
||
such that S(p q) is minimized by setting Ψ−1=0 (in Proposition 3.5). Strictly speaking,
|| ii
solutions of this form—with zero or infinite marginal variances—do not define proper dis-
tributions that lie in the family of factorized approximations. Such solutions never arise
Q
with FG-VI based on the KL or R´enyi divergences for α 0,1 . This phenomenon gives
∈ { }
rise to the following definition.
Definition 3.6. Let denote the family of factorized Gaussian approximations in
Q
eq. (2) that are proper distributions (i.e., with 0< (q)< ). For some target distri-
H ∞
bution p and divergence D, we say that FG-VI undergoes variational collapse when
arginf D(q,p) .
q∈Q
̸∈ Q
Fig. 2 illustrates this collapse when FG-VI is used to approximate a multivariate Gaus-
sian distribution in three dimensions with varying degrees of correlation; the amount of
correlation is determined by the magnitude of the off-diagonal elements C , C , and C
12 23 13
in eq. (6). These off-diagonal elements are constrained by the fact that the correlationFigure 2: When FG-VI is based on minimizing the score-based divergences in Table 1, it
may estimate zero or infinite values for the marginal variances. The red areas indicate
these occurrences of variational collapse when FG-VI with a score-based divergence is used
to approximate a three-dimensional Gaussian with a non-diagonal correlation matrix C.
matrix as a whole must be positive definite. Each panel in the figure visualizes a two-
dimensional slice of the three-dimensional (convex) set of positive-definite correlation ma-
trices for a fixed value of C . The blue regions of each slice indicate the areas where the
23
NQPs in Propositions 3.4 and 3.5 yield finite and nonzero estimates of the variances; the
red regions indicate areas of variational collapse. (It can be shown, in three dimensions,
that the forward and reverse score-based divergences share the same areas of variational
collapse.) Interestingly, the leftmost panel shows that variational collapse does not occur in
twodimensionsorarisepurelyfrompairwisecorrelations. Ontheotherhand,theremaining
panels show that variational collapse occurs in three dimensions whenever the correlation
matrix is dense and has at least one off-diagonal element of sufficiently large magnitude.
4 Ordering of divergences for FG-VI
We have seen that different divergences D(q,p) yield different solutions to the problem of
variationalinferenceineq.(1); wehavealsoseen,inturn,thatthesedifferentsolutionsyield
different estimates of the marginal variances, precisions, and entropy. When it is possible
to order these different estimates of uncertainty, it is also natural to define a corresponding
ordering of the divergences from which they were derived. In this section, we derive such
an ordering over all the divergences listed in Table 1.
We begin with the following definition, which bases this ordering on the marginal vari-
ances that are estimated by FG-VI.
Definition 4.1. Let and denote the families of multivariate Gaussian distribu-
P Q
tions in eqs. (3–4), where Ψ is diagonal but Σ is not. Consider two divergences D
a
and D , and for any p , let Ψ(a) and Ψ(b) denote the covariances of the factor-
b
∈P
ized approximations obtained, respectively, by minimizing D (q,p) and D (q,p) over all
a b
q . We say D dominates D , written D D , if for any p we have that
a b a b
∈Q ≻ ∈ P
i, Ψ (a) Ψ (b), (57)
ii ii
∀ ≥
j, Ψ (a) > Ψ (b). (58)
jj jj
∃
From the above definition, we see intuitively that D dominates D in the following sense:
a b2.0 2.0 4
1.5 1.5 2 divergence
SM(p||q)
1.0 1.0 0 RK aL (( pp || || qq ))
KL(q||p)
0.5 0.5 −2 SM(q||p)
0.0 0.0 −4
1.00 0.75 0.50 0.25 0.00 1.00 0.75 0.50 0.25 0.00 1.00 0.75 0.50 0.25 0.00
|C| |C| |C|
Figure 3: Comparison of variances, precisions, and entropy estimated by FG-VI with dif-
ferent divergences. Here FG-VI was used to approximate a 2-dimensional Gaussian with a
non-diagonal correlation matrix C. The R´enyi divergence was computed for α=0.5. The
ordering of the curves matches the predictions of Theorem 4.3 and Proposition 4.2.
it yields no smaller estimates of the marginal variances—and at least one larger estimate—
when D and D are used for FG-VI with a Gaussian target distribution.
a b
TheabovedefinitionwasbasedontheestimatesofmarginalvariancesbyFG-VI,andwe
shall use it throughout the rest of the paper. But it is not the only possible definition. Be-
cause FG-VI estimates a strictly diagonal covariance matrix, the ordering in Definition 4.1
also immediately implies an ordering of the marginal precisions and entropies.
Proposition 4.2. Suppose D D . Then for any p :
a b
≻ ∈P
(i) the marginal precisions are ordered, with diag(Ψ−1(a)) diag(Ψ−1(b)), and
≤
this inequality is strict for at least one component along the diagonal.
(ii) the entropies are ordered, with (q ) < (q ), where q and q are the solu-
a b a b
H H
tions obtained by minimizing D (q,p) and D (q,p) over all q .
a b
∈ Q
Theseimplicationsmaybeusefulforapplicationswhichrequiretheestimationofprecisions
and/or entropies rather than variances.
With Definition 4.1 in hand, we are now able to state the main result of this paper; it
is an ordering over all the divergences listed in Table 1.
Theorem 4.3 (Ordering of divergences). For any (α ,α ) satisfying 0<α <α <1,
1 2 1 2
S(q p) KL(q p) R (p q) R (p q) KL(p q) S(p q). (59)
|| ≺ || ≺
α1
|| ≺
α2
|| ≺ || ≺ ||
Figure 3 illustrates the consequences of this ordering when FG-VI is used to approximate
a 2-dimensional Gaussian with varying degrees of correlation. The amount of correlation is
measured by the determinant of the correlation matrix, C ; as this determinant decreases
| |
from one to zero—from no correlation to perfect correlation—the target Gaussian more
starkly violates the assumption of factorizability in FG-VI. From left to right, the panels
plot the estimates of variance, precision, and entropy that are obtained from FG-VI with
different divergences, and we see that for all values of C , these estimates are ordered
| |
exactlyaspredictedbyTheorem4.3andProposition4.2. Therestofthissectionisdevoted
to proving this theorem.
ecnairav
evitaleR
noisicerp
evitaleR
pag
yportnEIn fact, we have already proven one of these orderings. Recall that when FG-VI is
based on minimizing KL(q p), it underestimates the marginal variances, at least one of
||
them strictly; this is the content of Lemma 2.4 and Proposition 3.1. On the other hand,
when FG-VI is based on minimizing KL(p q), it correctly estimates the marginal variances
||
(MacKay, 2003); this is the content of Proposition 3.2. Thus we have already shown
KL(q p) KL(p q).
|| ≺ ||
4.1 Ordering of score-based divergences
In what follows we prove the two outermost orderings in Theorem 4.3; specifically, we show
that S(q p) KL(q p) and KL(p q) S(p q). At a high level, these proofs are obtained
|| ≺ || || ≺ ||
by analyzing the Karush-Kuhn-Tucker (KKT) conditions of the quadratic programs in
Propositions 3.4 and 3.5.
Proof of S(q p) KL(q p) . Consider the diagonal covariance matrix Ψ that FG-VI esti-
|| ≺ ||
mates by minimizing S(q p) on the left side of this ordering. This matrix is obtained from
||
the solution of the NQP in eq. (52) for the unknown nonnegative variables s = Ψ Σ−1.
i ii ii
The solution to this NQP must satisfy the KKT conditions: namely, for each component
of s, we have either that (i) s =0 and Hs >1 or (ii) s >0 and Hs =1. We examine each
i i i i
of these cases in turn:
s =0 ThisisacaseofvariationalcollapsewherebyminimizingS(q p)weestimate
i
||
that Ψ = 0.
ii
s >0 In this case we observe that (Hs) =1 from the KKT condition and also that
i i
H 0 and H = 1 from eq. (53). Thus we see that
ij ii
≥
(cid:88)
s = H s = 1 H s 1, (60)
i ii i ij j
− ≤
j̸=i
or equivalently, that Ψ Σ−1 1.
ii ii ≤
We have thus shown that Ψ Σ−1 1 for all of the marginal variances estimated by mini-
ii ii ≤
mizing the score-based divergence S(q p). In fact, we claim that this inequality must be
||
strict for at least one component. The claim is trivially true if any s =0, since this implies
i
Ψ Σ−1=0. Suppose to the contrary that every element of s is strictly positive. Since by
ii ii
assumption Σ is not diagonal, it must also be true that Σ−1 is not diagonal, and therefore
from eq. (53) we can pick some i and k such that H > 0. But then, continuing from
ik
eq. (60), we see that
(cid:88)
s = 1 H s 1 H s < 1, (61)
i ij j ik k
− ≤ −
j̸=i
or equivalently that Ψ Σ−1 < 1, thus proving the claim. In sum we have shown that
ii ii
Ψ 1/Σ−1 for all of the marginal variances estimated by minimizing S(q p), and also
ii ≤ ii ||
that this inequality is strict for at least one component. Finally, we recall from eq. (26)
that 1/Σ−1 is the marginal variance estimated by minimizing KL(q p). It follows from
ii ||
Definition 4.1 that S(q p) KL(q p).
|| ≺ ||Proof of KL(p q) S(q p) . Consider the diagonal covariance matrix Ψ that FG-VI es-
|| ≺ ||
timates by minimizing S(p q). This matrix is obtained from the solution of the NQP in
||
eq. (55) for the unknown nonnegative variables t = Ψ−1Σ . The rest of the proof follows
i ii ii
exactly the same steps as the previous one, but with t playing the role of s . In this way
i i
we can show that Ψ−1Σ 1 (or equivalently Σ Ψ ) for all of the marginal variances
ii ii ≤ ii ≤ ii
estimated by minimizing S(p q), and also that this inequality is strict for at least one
||
component. Finally, we recall from eq. (28) that Σ is the marginal variance estimated by
ii
minimizing KL(p q), and it follows from Definition 4.1 that KL(p q) S(p q).
|| || ≺ ||
4.2 Ordering of KL divergences
NextweprovethetwointermediateorderingsinTheorem4.3; specifically, foranyα (0,1),
∈
we show that KL(q p) R (p q) and R (p q) KL(p q). At a high level, these proofs
α α
|| ≺ || || ≺ ||
are obtained by applying the inequalities in Proposition 2.6 to the fixed-point equations for
the marginal variances in eqs. (46–47); recall that these were the marginal variances from
FG-VI estimated by minimizing the R´enyi divergence of order α.
Proof of KL(q p) R (p q) for α (0,1) . Let Ψ denote the diagonal covariance matrix
α
|| ≺ || ∈
estimatedbyminimizingR (p q),andconsiderthematrix[αΣ−1+(1 α)Ψ]−1 thatappears
α
|| −
on the right side of its fixed-point equation in eq. (47). This matrix is positive-definite
and non-diagonal for all α (0,1), and thus we can apply the inequality in eq. (23) of
∈
Proposition 2.6 to its diagonal elements. In this way, we find:
αΨ Σ−1 = Ψ (cid:2) αΣ−1+(1 α)Ψ−1(cid:3) (1 α), (62)
ii ii ii Ψ − ii − −
ii
(1 α), (63)
≥ (cid:2) αΣ−1+(1 α)Ψ−1(cid:3)−1 − −
− ii
= (Ψ /Ψ ) (1 α), (64)
ii ii
− −
= α. (65)
Dividing both sides by αΣ−1, we see that Ψ 1/Σ−1 for all i. Next we appeal to the
ii ii ≥ ii
strict inequality of Proposition 2.6 and conclude that Ψ > 1/Σ−1 for some j. On the left
jj jj
and right sides of these inequalities appear, respectively, the marginal variances estimated
by minimizing R (p,q) and KL(q,p). Thus we have shown R (p q) KL(q,p).
α α
|| ≻
Proof of R (p q) KL(p q) for α (0,1) . The proof follows the same steps as the pre-
α
|| ≺ || ∈
vious one, but now we combine the inequalities of Proposition 2.6 with the fixed-point
equation in eq. (46). In this way we find:
Ψ−1
(1 α)Ψ−1Σ = Ψ−1[αΨ+(1 α)Σ] α ii α = 1 α. (66)
− ii ii ii − ii− ≥ [αΨ+(1 α)Σ]−1− −
− ii
Dividing both sides by (1 α)Ψ−1, we see that Σ Ψ for all i, and we know from the
− ii ii ≥ ii
strict inequality in eq. (24) of Proposition 2.6 that Σ > Ψ for some j. On the left and
jj jj
right sides of these inequalities appear, respectively, the marginal variances estimated by
minimizing KL(p q) and R (p q). Thus we have shown KL(p q) R (p q).
α α
|| || || ≻ ||4.3 Ordering of R´enyi divergences
Finally we prove the ordering of R´enyi divergences in Theorem 4.3. This proof is more
technical, relying on a detailed analysis of the fixed-point equation in eq. (47).
Proof of R (p q) R (p q) for 0< α <α <1 . Let Ψ(α) denote the diagonal covari-
α1
|| ≺
α2
||
1 2
ance matrix that FG-VI estimates by minimizing R (p q); here, we regard this matrix as a
α
||
functionovertheinterval(0,1). Thefunctioniscontinuous, andweshallprovetheordering
by showing that the estimated variance Ψ (α) is strictly increasing whenever Σ Σ−1>1.
ii ii ii
Let us review what we have already shown. For α (0,1), we know from the orderings
∈
of Section 4.2 that Ψ (α) is sandwiched between its limiting values at zero and one:
ii
(cid:0) Σ−1(cid:1)−1
= lim Ψ (α) Ψ (α) lim Ψ (α) = Σ . (67)
ii α→0+ ii ≤ ii ≤ α→1− ii ii
We also know that Σ Σ−1 > 1 whenever (cid:80) Σ > 0, and that in this case, the above
ii ii j̸=i| ij |
inequalities are strict. A useful picture of this situation is shown in the left panel of Fig. 4.
Consider any component Ψ (α) of the estimated variances that is not constant on the unit
ii
interval. (As shown previously, there must be at least one such component.) In this case,
thereareonlytwopossibilities: eitherΨ (α)isstrictlyincreasing, oritisnot. Iftheformer
ii
is always true, then it follows that R (p q) R (p q) whenever 0<α <α <1. We
α1
|| ≺
α2
||
1 2
shall prove by contradiction that this is indeed the case.
Assume that there are one or more diagonal components, Ψ (α), that are neither con-
ii
stantnorstrictlyincreasingovertheunitinterval;also,let denotethesetthatcontainsthe
I
indices of these components. Since each such component must have at least one stationary
point, we can also write:
(cid:110) (cid:12) (cid:111)
= i(cid:12)Σ Σ−1>1 and Ψ′ (α) = 0 for some α (0,1) . (68)
I (cid:12) ii ii ii ∈
For each such component, there must also exist some minimal point τ (0,1) where its
i
∈
derivative vanishes. We define τ = min τ and j = argmin τ , so that by definition
i∈I i i∈I i
Ψ′ (τ) 0 for all i, (69)
ii
≥
Ψ′ (τ) = 0. (70)
jj
Toobtainthedesiredcontradiction, weshallprovethatthereexistsnopointτ (asimagined
in Fig. 4) with these properties.
We start by rewriting the fixed-point equation in ?? and eq. (47) in a slightly different
form,
Ψ (α) = [Φ(α)−1] = (cid:2) αΣ−1+(1 α)Ψ−1(α)(cid:3)−1 , (71)
ii ii − ii
where on the right side we have explicitly indicated all sources of dependence on α. Next,
we differentiate both sides of eq. (71) for i=j and at α=τ:
(cid:26) (cid:27)(cid:12)
Ψ′ jj(τ) = e⊤
j
dd α(cid:104) τΣ−1+(1 −τ)Ψ−1(α)(cid:105)−1 + dd α(cid:104) αΣ−1+(1 −α)Ψ−1(τ)(cid:105)−1 (cid:12) (cid:12)
(cid:12)
e j, (72)
α=τ
where on the right side we have separated out the different sources of dependence on α.
Evaluating the first term on the right side, we find that
(cid:26) (cid:27)(cid:12)
e⊤ j dd α(cid:104) τΣ−1+(1 −τ)Ψ−1(α)(cid:105)−1 (cid:12) (cid:12)
(cid:12)
e j = e⊤ j Φ−1(τ)Ψ−1(τ)Ψ′(τ)Ψ−1(τ)Φ−1(τ)e j, (73)
α=τΨ (α) f(α)
jj
Σ
jj
f(0) = f(τ) = Ψ (τ)
jj
1/Σ−1
jj
α α
0 τ 1 0 τ 1
Figure 4: (Left) Either Ψ (α) is strictly increasing over α (0,1), or it is not, with some
jj
∈
minimal point τ of vanishing derivative. We prove the former by showing that no such
point τ exists. (Right) The proof is based on properties of the function f(α) in eq. (75).
The function is convex; it also satisfies f(0)=f(τ)=Ψ (τ) and f′(0)f′(τ) < 0.
jj
and now we recognize that this term is nonnegative: of the matrices on the right side of
eq. (73), note that Φ(α) and Ψ(α) are positive definite for all α (0,1), and Ψ′(τ) is
∈
positive semidefinite by virtue of the defining properties of τ in eqs. (69–70). Dropping this
first term from the right side of eq. (72), we obtain the inequality
0 d (cid:110) e⊤(cid:2) αΣ−1+(1 α)Ψ−1(τ)(cid:3)−1 e (cid:111)(cid:12) (cid:12) , (74)
≥ dα j − j (cid:12) α=τ
where we have exploited that the left side of eq. (72) vanishes by the second defining
property of τ in eq. (70).
Nextweshowthateq.(74)containsacontradiction. Todoso,weexaminetheproperties
of the function defined on [0,1] by
f(α) = e⊤(cid:2) αΣ−1+(1 α)Ψ−1(τ)(cid:3)−1 e . (75)
j j
−
This is, of course, exactly the function that is differentiated in eq. (74), whose right side is
equal to f′(τ). This function possesses several key properties. First, we note that
f(0) = e⊤(cid:2) Ψ−1(τ)(cid:3)−1 e = Ψ (τ) = e⊤(cid:2) τΣ−1+(1 τ)Ψ−1(τ)(cid:3)−1 e = f(τ). (76)
j j jj j j
−
Second, we note that f is convex on the unit interval; here, we are observing the convexity
of the matrix inverse (Nordstr¨om, 2011). Third, we note that
f′(0) = e⊤Ψ(τ)(cid:2) Σ−1 Ψ−1(τ)(cid:3) Ψ(τ)e = Ψ (τ)(cid:2) Σ−1 Ψ−1(τ)(cid:3) < 0. (77)
− j − j jj − jj
Finally, we note that the sign of f′(τ) must oppose the sign of f′(0); as shown in the right
panel of Fig. (4), this follows from the preceding properties that f(0) = f(τ), that the
function f is convex on [0,1], and that f′(0)<0. Thus we have shown that f′(τ)>0. But
this is directly in contradiction with eq. (74), which states that 0 f′(τ), and we are forced
≥
to conclude that no such point τ exists. This completes the proof.
From the above, we can also prove the existence of an entropy-matching solution for
FG-VI analogous to the precision-matching and variance-matching solutions of Section 3.1.0.7 n
10
8
6
0.6 4
2
0.5
0.25 0.50 0.75
e
Figure 5: When p is Gaussian over Rn, there always exists a unique α (0,1) such that
∈
the factorized approximation q minimizing R (p q) matches the entropy of p. However,
α
||
the entropy-matching value of α depends on the dimension and covariance structure of p.
Above we vary the dimension n and constant correlation ε.
Corollary 4.4. Let q = argmin R (p q). There exists a unique value α (0,1)
α q∈Q α
|| ∈
such that (q ) = (p).
α
H H
Proof. Let Ψ(α) denote the covariance of q for α (0,1). Since Ψ (α) is continuous
α ii
(cid:80) ∈
with respect to α, so is log Ψ(α) = logΨ (α), and hence so is the entropy (q ).
| | i ii H α
Moreover, from the ordering of R´enyi divergences, we see that log Ψ(α) and (q ) are not
α
| | H
only continuous, but strictly increasing over the unit interval α (0,1). In addition, we
∈
have
1/Σ−1 = Ψ (0) Ψ (α) Ψ (1) = Σ , i. (78)
ii ii ≤ ii ≤ ii ii ∀
with q interpolating smoothly between the precision-matching and the variance-matching
α
approximations of FG-VI. From the impossibility result in Theorem 2.1, the precision-
matching approximation (at α=0) underestimates the entropy of p, while the variance-
matching approximation (at α=1) overestimates it. From continuity and strict monotonic-
ity, it follows that there exists a unique α (0,1) such that (q ) = (p).
α
∈ H H
Theaboveprovestheexistenceanentropy-matchingsolution,butitisnotaconstructive
proof. There does not exist a single value of α for which (q ) = (p), even when p is
α
H H
Gaussian. We show this numerically for the case where the target covariance Σ has unit
diagonal elements and constant off-diagonal elements ε (0,1). As seen in Figure 5, the
∈
entropy-matching value of α (obtained by a grid search) changes as we vary the amount of
correlation and the dimension of the problem.
5 Numerical experiments
Does the ordering of divergences hold when FG-VI is applied to non-Gaussian targets? We
study this question empirically on a range of models; in most of these models, the target p
is only known up to a normalizing constant.
WhenpisnotGaussian,somedivergencesinTable1arehardertominimizenumerically
than others. We use the following approaches. We minimize KL(q p) by (equivalently)
||
maximizing the evidence lower bound (ELBO), and we estimate the ELBO via Monte
agnihctam−yportnEModel n Details Variances Entropy
Gaussian 10 Full-rank covariance matrix. ✓ ✓
Rosenbrock 2 High curvature. ✓ ✓
Eight Schools 10 Small hierarchical linear model. ✗ ✓
German Credit 25 Logistic regression. ✓ ✓
Radon Effect 91 Large hierarchical linear model. ✗ ✓
Stochastic Volatility 103 Stochastic volatility model. ✗ ✓
Table 2: Target distributions p with dimension n. The two right-most columns indicate
whether the divergences are ordered according to the marginal variances and the entropy.
Carlo with draws from q, a procedure at the core of modern implementations of “black-
box”VI(Kucukelbiretal.,2017). Weuseasimilarproceduretooptimizetheα-divergence;
however, thisprocedureismorefraught1 asMonteCarloestimatorsoftheα-divergenceand
itsgradientssufferfromlargevariance, especiallyinlargedimensionsandasαapproaches1
(Geffner and Domke, 2021). We minimize S(q p) by adapting a recent batch-and-match
||
(BaM)methodinspiredbyproximalpointalgorithms(Caietal.,2024); BaMwasoriginally
developed for Gaussian variational families with dense covariance matrices, but here we
experimentwithBaMupdatesforFG-VI(derivedinAppendixA.1). WeminimizeKL(p q)
||
by estimating the means and marginal variances of p from long runs of Markov chain
Monte Carlo; the factorized approximation minimizing KL(q p) is the one that matches
||
these moments (MacKay, 2003). We do not empirically evaluate the forward score-based
divergence for FG-VI as we do not have a reliable method to minimize S(p q).
||
We experiment with the above methods on a diverse collection of target distributions
from the inference gym, a curated library of models for the study of inference algorithms
(Sountsovetal.,2020). WedescribethesetargetdistributionsbrieflyinTable2andprovide
moredetailsinAppendixA.2. WeimplementeachmodelinPythonanduseJAXtocalculate
derivatives (Bradbury et al., 2018). We optimize the α-divergences for α=0.1 and α=0.5,
but only on low-dimensional targets (D<10) where the optimizers are numerically stable.
We run each VI algorithm for 1,000 iterations, and at each iteration, we use B = 10,000
draws from q to estimate the divergence and its gradient. This large number of draws is
overkill for many problems, but it helps in practice to stabilize the optimizations of α-
divergences. In addition, we parallelize many calculations on a GPU, keeping run times
short. We provide code to run these experiments at https://github.com/charlesm93/
VI-ordering.
The ordering of divergences that we proved for Gaussian targets is not guaranteed to
hold for non-Gaussian targets, and empirically we observe that it is more likely to fail in
high-dimensional problems; see the penultimate column of Table 2. Intuitively, this is to be
expected because the ordering in Theorem 4.3 is voided by a violation in any component
of the marginal variance. This is illustrated in Fig. 6 for the Eight Schools models, where
some marginal variances are ordered in the same way as predicted for Gaussian targets,
1For more discussion on implementing VI with α-divergences, see e.g., Hernandez-Lobato et al. (2016);
Li and Turner (2016); Dieng et al. (2017); Daudel et al. (2021, 2023).EightSchools
1.5
S(q p) KL(q p) α=0.1 α=0.5 KL(p q)
|| || ||
1.0
0.5
0.0
z[1] z[2] z[3] z[4] z[5] z[6] z[7] z[8] z[9] z[10]
Figure 6: Variance when targeting the posterior distribution of the Eight Schools model.
Gaussian Rosenbrock
EightSchools
0.5
0.5
0.0 1
0.0
−0.5 0 −0.5
−1.0 −1.0
S(q|p |) KL(q|p |) α=0.1 α=0.5 KL(p|q |) S(q|p |) KL(q|p |) α=0.1 α=0.5 KL(p|q |) S(q|p |) KL(q|p |) α=0.1 α=0.5 KL(p|q |)
GermanCredit RadonEffect StochasticVolatility
4 5
50
2
0
0 0
−2 −5 −50
S(q|p |) KL(q|p |) KL(p|q |) S(q|p |) KL(q|p |) KL(p|q |) S(q|p |) KL(q|p |) KL(p|q |)
Figure 7: Ordering of entropy across targets. The zero on each vertical axis corresponds
to the average of the entropies estimated by the approximations in each panel.
but most are not. The true marginal variances in this model were estimated by long runs
of Markov chain Monte Carlo.
WhenpisnotGaussian,analternativeorderingofdivergencesforFG-VIissuggestedby
theestimatestheyyieldofthejointentropy. AsshowninthefinalcolumnofTable2andin
Fig.7, theorderinginTheorem4.3does correspond—acrossallofthenon-Gaussiantargets
inourexperiments—totheorderingofentropiesestimatedbyFG-VI.Intheseexperiments,
however, we are not able to compare the entropies estimated by FG-VI to the actual
entropies. Tocomputetheentropyofnon-Gaussiandistributions,itisnecessarytoestimate
their normalizing constants; this can be done by invoking a Gaussian-like approximation,
as in bridge sampling (Meng and Schilling, 2002; Gronau et al., 2020), however we do not
wish to compare a theory for Gaussian targets to an empirical benchmark which relies on
a Gaussian approximation.
6 Discussion
Divergences over probability spaces are ubiquitous in the statistics and machine learning
literature, but it remains challenging to reason about them. This challenge is salient in
VI, where the divergence does not serve merely as an object for theoretical study but also
bears directly on the algorithmic implementation of the method. Our work reveals how
)dertnec(yportnE
)dertnec(yportnE
ecnairaVdezilamroN
)dertnec(yportnE
)dertnec(yportnE
)dertnec(yportnE
)dertnec(yportnEthe choice of divergence can align (or misalign) with intelligible inferential goals, and it is
in line with a rich and recent body of work on assessing the quality of VI (e.g, Yao et al.,
2018; Wang and Blei, 2018; Huggins et al., 2020; Dhaka et al., 2021; Biswas and Mackey,
2024). A distinguishing feature of our paper is that it analyzes multiple and competing
inferentialgoals—goalswhichcannotbesimultaneouslymetandwhichareeachbestserved
by a different divergence. Also, our work focuses on a setting that is especially conducive
to analysis, one in which the inherent trade-offs of factorized VI can be completely charac-
terized, as opposed to more general treatments of the problem. The intuitions developed
from this analysis suggest useful directions, conjectures, and insights for studying broader
classes of distributions.
There exists previous work comparing divergences, notably in the context of latent
variable models, where VI is used to approximate a marginal likelihood. In this context,
the KL and R´enyi divergences have been ordered based on the (lower or upper) bound they
provide on the marginal likelihood (Li and Turner, 2016; Dieng et al., 2017). (Interestingly,
this ordering agrees with the one provided in this paper, even though it is based on a
different criterion.) This type of ordering cannot be extended to score-based divergences,
however, as they do not provide bounds on the marginal likelihood.
We have seen that the results of VI can be improved, for certain inferential goals, by
choosing a particular divergence. But this is not the only way to improve the results of VI;
another is to use a richer family of variational approximations (e.g, Hoffman and Blei,
Q
2015; Johnson et al., 2016; Dhaka et al., 2021). The deficits of FG-VI may be addressed,
for instance, by Gaussian variational approximations with full covariance matrices (Opper
and Archambeau, 2009; Cai et al., 2024). In high dimensions, Dhaka et al. (2021) argue
that the results of VI can be improved more easily by optimizing KL(q p) over a richer
||
variational family than by optimizing certain alternative divergences. Nevertheless, even as
the variational family becomes richer, it is not likely to contain the target distribution p,
and therefore the approximating distribution q must in some way be compromised. For
example, if is a family of symmetric distributions and the target p is asymmetric, then
Q
an approximation may only be able to match the mode or the mean of the target. It stands
to reason that further impossibility theorems, in the spirit of Theorem 2.1, can be derived
to elucidate these compromises.
Aknowledgments
We thank David Blei, Diana Cai, Robert Gower, Chirag Modi, and Yuling Yao for helpful
discussions. We are also grateful to anonymous reviewers from the conference on Uncer-
tainty in Artificial Intelligence for their comments on a previous paper (Margossian and
Saul, 2023), which provided some of the motivation for this manuscript.
References
Bernardo, J. M. and A. F. M. Smith (2000). Bayesian Theory. Wiley.
Biswas, N. and L. Mackey (2024). Bounding Wasserstein distance with couplings. Journal
of the American Statistical Association. DOI: 10.1080/01621459.2023.2287773.Blei, D. M., A. Kucukelbir, and J. D. McAuliffe (2017). Variational inference: A review for
statisticians. Journal of the American Statistical Association 112, 859–877.
Bradbury, J., R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula,
A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang (2018). JAX: composable
transformations of Python+NumPy programs.
Burbea, J. (1984). The convexity with respect to Gaussian distributions of divergences of
order α. Utilitas Mathematica 26, 171–192.
Cai, D., C. Modi, L. Pillaud-Vivien, C. C. Margossian, R. M. Gower, D. M. Blei, and
L. K. Saul (2024). Batch and match: black-box variational inference with a score-based
divergence. arXiv:2402.14758.
Cichocki, A. and S.-i. Amari (2010). Families of alpha- beta- and gamma-divergences:
Flexible and robust measures of similarities. Entropy 12, 1532–1568.
Courtade, T. A. (2016). Links between the logarithmic Sobolev inequality and the convo-
lution inequalities for entropy and Fisher information. arXiv:1608.05431.
Cover, T. M. and J. A. Thomas (2006). Elements of Information Theory. John Wiley &
Sons, Inc.
Daudel, K., J. Benton, and A. Doucet (2023). Alpha-divergence variational inference meets
importance weighted auto-encoders: Methodology and asymptotics. Journal of Machine
Learning Research 24, 1–83.
Daudel, K., R. Douc, and F. Roueff (2021). Infinite-dimensional gradient-based descent for
alpha-divergence minimisation. The Annals of Statistics 49, 2250–2270.
Dhaka, A. K., A. Catalina, M. Welandawe, M. R. Andersen, J. H. Huggins, and A. Ve-
htari (2021). Challenges and opportunities in high-dimensional variational inference. In
Advances in Neural Information Processing Systems 34, pp. 7787–7798.
Dieng,A.B.,D.Tran,R.Ranganath,J.Paisly,andD.M.Blei(2017). Variationalinference
via χ upper bound minimization. In Advances in Neural Information Processing Systems
30, pp. 2732–2741.
Dua, D. and C. Graff (2017). UCL machine learning repository.
Geffner, T. and J. Domke (2021). On the difficulty of unbiased alpha divergence mini-
mization. In Proceedings of the 38th International Conference on Machine Learning, pp.
3650–3659.
Gelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin (2013).
Bayesian Data Analysis. Chapman & Hall/CRC Texts in Statistical Science.
Gelman, A. and J. Hill (2007). Data Analysis Using Regression and Multilevel-Hierarchical
Models. Cambridge University Press.Gil, M., F. Alajaji, and T. Linder (2013). R´enyi divergence measures for commonly used
univariate continuous distributions. Information Sciences 249, 124–131.
Giordano, R., T. Broderick, and M. I. Jordan (2018). Covariances, robustness, and varia-
tional bayes. Journal of Machine Learning Research 19, 1–49.
Gronau, Q. F., H. Singmann, and E.-J. Wagenmakers (2020). bridgesampling: An R
package for estimating normalizing constants. Journal of Statistical Software 92.
Hernandez-Lobato, J., Y. Li, M. Rowland, T. Bui, D. Hern´andez-Lobato, and R. Turner
(2016).Black-boxalphadivergenceminimization.InProceedingsofthe33rdInternational
Conference on Machine Learning, pp. 1511–1520.
Hobza, T., D. Morales, and L. Pardo (2009). Statistical Methodology 6, 424–436.
Hoffman, M. D. and D. M. Blei (2015). Stochastic structured variational inference. In
Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,
pp. 361–369.
Horn, R. A. and C. R. Johnson (2012). Matrix analysis. Cambridge University Press.
Huggins, J. H., M. Kasprzak, T. Campbell, and T. Broderick (2020). Validated variational
inference via practical posterior error bounds. In Proceedings of the 23rd International
Conference on Artificial Intelligence and Statistics, pp. 1792–1802.
Johnson, M. J., D. K. Duvenaud, A. Wiltschko, R. P. Adams, and S. R. Datta (2016).
Composing graphical models with neural networks for structured representations and
fast inference. In Advances in neural information processing systems 29, pp. 2946–2954.
Jordan, M. I., Z. Ghahramani, T. S. Jaakkola, and L. K. Saul (1999). An introduction to
variational methods for graphical models. Machine Learning 37, 183–233.
Kim, S., N. Shepard, and S. Chib (1998). Stochastic volatility: likelihood inference and
comparison with arch models. The Review of Economic Studies 65, 361–393.
Kucukelbir, A., D. Tran, R. Ranganath, A. Gelman, and D. Blei (2017). Automatic differ-
entiation variational inference. Journal of Machine Learning Research 18, 1–45.
Li, Y. and R. E. Turner (2016). R´enyi divergence variational inference. In Advances in
Neural Information Processing Systems 29, pp. 1073–1081.
Liese, F. and I. Vajda (1987). Convex Statistical Distances. Leipzig: Teubner.
MacKay, D. J. (2003). Information theory, inference, and learning algorithms.
Margossian, C. C. and L. K. Saul (2023). The shrinkage-delinkage trade-off: An analysis of
factorized gaussian approximations for variational inference. In Proceedings of the 39th
Conference on Uncertainty in Artificial Intelligence, pp. 1358–1367.
Meng, X. and S. Schilling (2002). Warp bridge sampling. Journal of Computational and
Graphical Statistics 11, 552–586.Minka, T. (2005). Divergence measures and message passing. Technical Report MSR-TR-
2005-173.
Nordstr¨om, K. (2011). Convexity of the inverse and Moore-Penrose inverse. Linear Algebra
and Its Applications 434, 1489–1512.
Opper, M. and C. Archambeau (2009). The variational gaussian approximation revisited.
Neural Computation 21(3), 786–792.
Parisi, G. (1988). Statistical Field Theory. Addison-Wesley.
Robert, C. P. and G. Casella (2004). Monte Carlo Statistical Methods. Springer.
Rosenbrock, H. H. (1960). An automatic method for finding the greatest or least value of
a function. Computer Journal 3, 175–184.
Rubin, D. B. (1981). Estimation in parallelized randomized experiments. Journal of Edu-
cational Statistics 6, 377–400.
Sountsov, P., A. Radul, and contributors (2020). Inference gym.
Tomczak, J. M. (2022). Deep generative modeling. Springer.
Turner, R. E. and M. Sahani (2011). Two problems with variational expectation maximisa-
tion for time-series models. In D. Barber, A. T. Cemgil, and S. Chiappa (Eds.), Bayesian
Time series models, Chapter 5, pp. 109–130. Cambridge University Press.
Vehtari, A., A. Gelman, T. Sivula, P. Jylanki, D. Tran, S. Sahai, P. Blomstedt, J. P.
Cunningham, Schiminovich, and C. P. Robert (2020). Expectation propagation as a way
of life: A framework for bayesian inference on partitioned data. Journal of Machine
Learning 21, 1–53.
Vehtari, A., D. Simpson, A. Gelman, Y. Yao, and J. Gabry (2024). Pareto smoothed
importance sampling. Journal of Machine Learning Research. To appear.
Wainwright, M. J. and M. I. Jordan (2008). Foundations and Trends in Machine Learn-
ing 1(1–2), 1–305.
Wang, Y. and D. M. Blei (2018). Frequentist consistency of variational bayes. Journal of
the American Statistical Association 114, 1147–1161.
Yao, Y., A. Vehtari, D. Simpson, and A. Gelman (2018). Yes, but did it work?: Evaluating
variational inference. In Proceedings of the 35th International Conference on Machine
Learning, pp. 5577–5586.
Zhang, L., B. Carpenter, A. Gelman, and A. Vehtari (2022). Pathfinder: Parallel quasi-
Newton variational inference. Journal of Machine Learning Research 23(306), 1–49.A Additional details for experiments
A.1 Batch and Match algorithm for FG-VI
Batch and Match (BaM) (Cai et al., 2024, BaM) is an iterative algorithm for VI that
attempts to estimate and minimize the reverse score-based divergence, S(q p). At each
||
iteration, BaM minimizes a regularized objective function
2
LBaM(q) = D(cid:98)qt(q ||p)+
λ
KL(q ||q t), (79)
t
where
• q is a current approximation,
t
∈ Q
• ˆ (q p) is a Monte Carlo estimator of the score-based divergence
Dqt
||
B
ˆ (q p) = 1 (cid:88) logq(cid:0) z(b)(cid:1) logp(cid:0) z(b)(cid:1) 2 , (80)
Dqt || B ||∇ −∇ ||Ψ
b=1
using draws z(b) q .
t
∼
• λ > 0 is the learning rate, or step size, at the tth iteration.
t
Caietal.(2024,AppendixC)derivetheBaMupdateswhichminimizeeq.(79)when isthe
Q
familyofGaussiandistributionswithdense covariancematrices. Inthisappendix,wederive
the analogous updates to minimize eq. (79) when is the family of Gaussian distributions
Q
with diagonal covariance matrices. These were the updates used for the experiments in
Section 5 of the paper.
We follow closely the derivation in Cai et al. (2024), noting only the essential differences
for the case of FG-VI. We use g(b) = logp(cid:0) z(b)(cid:1) as shorthand for the score at the bth
∇
sample. As before, the following averages need to be computed at each iteration:
B B
1 (cid:88) 1 (cid:88)
z¯ = z(b), g¯ = g(b). (81)
B B
b=1 b=1
For FG-VI, we instead compute diagonal matrices C and Γ with nonnegative elements
B B
1 (cid:88)(cid:16)
(b)
(cid:17)2 1 (cid:88)(cid:16)
(b)
(cid:17)2
C = z z¯ , Γ = g g¯ . (82)
ii B i − i ii B i − i
b=1 b=1
Let q denote the minimizer of eq. (79), with mean ν and diagonal covariance ma-
t+1 t+1
trix Ψt+1. The update for the mean takes the same form as in the original formulation
of BaM:
νt+1 = λ 1 (cid:0) z¯+Ψt+1g¯(cid:1) + 1 νt. (83)
1+λ 1+λ
t t
To obtain the covariance update, we substitute this result into eq. (79) and differentiate
with respect to the diagonal elements of Ψt+1. For FG-VI, we obtain a simple quadratic
equation for each updated variance:
(cid:18) Γ + 1 g¯2(cid:19) (cid:0) Ψt+1(cid:1)2 + 1 Ψt+1 (cid:18) C + 1 Ψt + (ν it −z¯ i)2(cid:19) = 0. (84)
ii 1+λ i ii λ ii − ii λ ii 1+λ
t t t tEq. (84) always admits a positive root, and for FG-VI, this positive root is the BaM update
for the ith diagonal element of Ψt+1.
At a high level, BaM works the same way with diagonal covariance matrices as it does
with full covariance matrices. Each iteration involves a batch step which draws the B
samples from q and a match step that performs the calculations in eqs. (81-84). We find
t
a constant learning rate λ =1 to work well for the experiments in Section 5.
t
A.2 Models from the inference gym
In this appendix, we provide details about the models and data sets from the inference
gym used in Section 5.
Rosenbrock Distribution. (n=2)Atransformationofanormaldistribution, withnon-
trivial correlations (Rosenbrock, 1960). The contour plots of the density function have the
shape of a crescent moon. The joint distribution is given by:
p(z ) = normal(0,10) (85)
1
p(z z ) = normal(0.03(z2 100),1). (86)
2 1 1
| −
We use FG-VI to approximate p(z ,z ).
1 2
Eight Schools. (n=10)ABayesianhierarchicalmodeloftheeffectsofatestpreparation
program across 8 schools (Rubin, 1981):
p(µ) = normal(5,32) (87)
p(τ) = normal+(0,10) (88)
p(θ µ,σ) = normal(µ,τ2) (89)
i
|
p(y θ ) = normal(θ ,σ2), (90)
i i i i
|
wherenormal+ isanormaldistributiontruncatedat0. Theobservationsarex = (y ,σ )
1:8 1:8
and the latent variables are z = (µ,τ,θ ). We use VI to approximate the posterior
1:8
distribution p(z x).
|
German Credit. (n=25) A logistic regression applied to a credit data set (Dua and
Graff, 2017), with covariates x = x , observations y = y , coefficients z = z and an
1:24 1:24 1:24
intercept z :
0
p(z) = normal(0,I ), (91)
n
(cid:18) (cid:19)
1
p(y z,x) = Bernoulli (92)
i | e−zTxi−z0
We use FG-VI to approximate p(z y ,x).
i
|Radon Effect. (n=91) A hierarchical model to measure the Radon level in households
(Gelman and Hill, 2007). Here, we restrict ourselves to data from Minnesota. For each
household, we have three covariates, x = x , with corresponding coefficients w = w ,
i i,1:3 1:3
and a county level covariate, θ , where j[i] denotes the county of the ith household. The
j[i]
model is:
p(µ) = normal(0,1) (93)
p(τ) = Uniform(0,100) (94)
p(θ µ,τ) = normal(µ,τ2) (95)
j
|
p(w ) = normal(0,1) (96)
k
p(σ) = Uniform(0,100) (97)
p(logy ) = normal(wTx +θ ,σ2). (98)
i i j[i]
We use FG-VI to approximate p(µ,τ,θ,w,σ x,y).
|
Stochastic volatility. (n=103) A time series model with 100 observations. The original
model by Kim et al. (1998) is given by:
p(σ) = Cauchy+(0,2) (99)
p(µ) = exponential(1) (100)
p((ϕ+1)/2) = Beta(20,1.5) (101)
p(z ) = normal(0,1) (102)
i
(cid:112)
h = µ+σz / 1 ϕ2 (103)
1 1
−
h = µ+σz +ϕ(h µ) (104)
i>1 i i−1
−
p(y h ) = normal(0,exp(h /2)). (105)
i i i
|
The original model was developed for a time series of 3000 observations, but we only
work with the first 100 observations. For these observations, we use FG-VI to model
p(σ,µ,ϕ,z y).
|