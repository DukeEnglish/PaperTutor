ADATRANS: FEATURE-WISE AND SAMPLE-WISE ADAPTIVE
TRANSFER LEARNING FOR HIGH-DIMENSIONAL REGRESSION
ZelinHe∗ YingSun† JingyuanLiu‡§ RunzeLi∗
ABSTRACT
We consider the transfer learning problem in the high dimensional setting, where the feature di-
mensionislargerthanthesamplesize. Tolearntransferableinformation,whichmayvaryacross
featuresorthesourcesamples,weproposeanadaptivetransferlearningmethodthatcandetectand
aggregatethefeature-wise(F-AdaTrans)orsample-wise(S-AdaTrans)transferablestructures. We
achievethisbyemployinganovelfused-penalty,coupledwithweightsthatcanadaptaccordingto
thetransferablestructure. Tochoosetheweight,weproposeatheoreticallyinformed,data-driven
procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while
filteringoutnon-transferablesignals,andS-AdaTranstoobtaintheoptimalcombinationofinforma-
tiontransferredfromeachsourcesample. Thenon-asymptoticratesareestablished,whichrecover
existingnear-minimaxoptimalratesinspecialcases. Theeffectivenessoftheproposedmethodis
validatedusingbothsyntheticandrealdata.
Keywords TransferLearning High-dimensionalRegression FeatureSelection Non-asymptoticTheory
· · ·
1 Introduction
Transferlearningisatechniquethatincorporatesknowledgefromvarioussourcetasksintoatargettask[Torreyand
Shavlik,2010]. Thisapproachisespeciallyusefulinhigh-dimensionaldataanalysis,suchasmedicalimagingand
geneticdataanalysis,wherethenumberoffeaturesissignificantlylargerthanthetargetsamplesize. Afundamental
challengeintransferlearning,especiallyunderthehigh-dimensionalsetting,isaddressingthenegativetransferissue: If
differentsourcesamplesareequallyaggregatedintothetrainingprocedure,itcouldleadtoasignificantdeterioration
of the estimation accuracy [Li et al., 2022, 2023a,b]. Although several studies have explored the identification of
transferablesamples,theyoftenpre-selectthesesamplespriortotrainingviasomead-hocfittingmethods[Tianand
Feng,2022,Liu,2023]. Thesemethodstypicallyresultinabinary“all-or-none"decisionregardingthetransferof
information,whichmaynotbeoptimalinpractice.
Infact,wenoticethatinhigh-dimensionalsetting,thetransferablestructureoftenvariesacrossfeatureswithinthe
samesourcesample. Onesuchexampleliesintheanalysisofbrainfunctionalconnectivitypatterns[Lietal.,2018],
whereeachsourcemayhaveadistinctsetofnon-transferablefeaturesduetovariationsinbrainconditions,asdepicted
inFig. 1(a). Inotherapplications,theoverallsourceinformativelevelvariesacrosssourcesamples,affectedbyfactors
suchasdataquality,reliability,andrelevancetothetargettask. Forinstance,ingenetic-diseaseassociationstudies,
duetopatientdemographicsandmeasurementmethodologies,source-tasksimilaritywiththetargetoftenvariesina
whole-samplemanner[TangandSong,2016,Wahlstenetal.,2003]. Fig. 1(b)illustratessuchascenario,wherethe
darkerorangecolorinthefirstsourceindicatesthatthissourceismorerelevanttothetargettaskwhilethesecond
sourceislesstransferable.
Theabovefeature-specificandsample-specifictransferablestructuresinspireustoproposeanadaptivetransferlearning
frameworkunderthehigh-dimensionalsetting,toadaptivelyadjusttheinformationtransferredfromthesourcestothe
∗DepartmentofStatistics,PennsylvaniaStateUniversity.
†SchoolofElectricalEngineeringandComputerScience,PennsylvaniaStateUniversity.
‡MOEKeyLaboratoryofEconometrics, DepartmentofStatisticsandDataScienceinSchoolofEconomics,WangYanan
InstituteforStudiesinEconomics,andFujianKeyLabofStatistics,XiamenUniversity.
§CorrespondingAuthor.
4202
raM
02
]LM.tats[
1v56531.3042:viXraFeature-wiseandSample-wiseAdaptiveTransferLearning
Figure1: (a)Feature-specificand(b)Sample-specifictransferablestructure. Targetsignalsarehighlightedinblue. The
shadeoforangedepictsamountoftransferableinformation.
targetwhileestimatingparameters. Specifically,wepresentasolutionforachievingfeature-andsample-wiseadaptive
transferlearning,calledAdaTrans,inhigh-dimensionallinearregressioninvolvingK sourcetasks. Thetargetmodelis
p-dimensionalwithsparsitylevels. Themethodisbasedonanovelfused-penaltythatextractsthenon-transferable
signalsfromthesourcesandmergesthetransferablepartwiththetarget. Wethentailorthemethodtoaddressthe
uniquechallengesoffeature-wiseandsample-wiseadaptationrespectively. Ourcontributionsareasfollows.
Feature-wiseAdaptiveTransfer. Forscenarioswherethesourcesamplescontaindifferentfeature-wisetransferable
structures,weproposeF-AdaTrans,whichcandetectnon-transferablefeaturesforeachsource. Weachievethisby
assigningfeature-wiseweightstothefused-penaltyintheobjectivefunction. Undermildconditions, F-AdaTrans
convergestoan“oracle”estimator,whicheffectivelycapturestheoverlapbetweenthetargetsignalsetandtransferable
featuresetswithineachsource.
(cid:112)
Theconvergencerateoftheproposedestimatorisshowntobeoftheorderκ slogs/N,whereN =n +Kn is
F T S
thefullsamplesize,n andn arerespectivelythetargetandsourcesamplesize. Thefactorκ ,rangingfromO(1)to
T S F
(cid:112)
O( N/n ),quantifiesthefeaturetransferability,revealingtheenhancedaccuracybeyondsingle-taskbenchmarks.
T
Sample-wiseAdaptiveTransfer. Forscenarioswherethenon-transferablesignalsaredense,weproposeS-AdaTrans
wheresample-wiseweightswaregiventothepenaltyterm. Weprovethattheestimatorattainsanon-asymptoticrate
O(cid:0) κ (w) slogp/N+h¯(w)(cid:112) logp/n (cid:1) . Thefactorsκ (w)andh¯(w),dependentonw,revealatradeoffbetweenthe
S T S
·
lossofeffectivesamplesizeandthesuppressionofnegativetransfer. Theboundrecoversnearminimaxoptimalratesin
specialcases,showingthetightnessofouranalysis. Theresultfurtherprovidesanaturalguidelinefordeterminingthe
weights: weshowwiththeoptimalweight,S-AdaTransprovablyimprovesoverthesingle-taskbenchmark,achieving
robustnessagainstnegativetransfer.
Data-drivenWeightSelection. Weproposedata-drivenmethodsfordeterminingweightsforbothF-AdaTransand
S-AdaTrans.ForF-AdaTrans,weintroduceaweightingprocedurebasedonthefoldedconcavepenalization[FanandLi,
2001]. Weprovethat,whenthenon-transferablesignalisdetectable,thismethodidentifiesthetransferablestructureand
obtainstheoracleestimatorwithoverwhelmingprobability. ForS-AdaTrans,weimplementacross-validation-based
weightselectionprocedure,withitseffectivenessdemonstratedacrossdiversesimulationsettings.
Inaddition,AdaTransisdesignedtojointlearnthetargetandallsourceparameters,ratherthanpoolingthesamples
alltogetherwithaunifiedparametervector. Thus,itcanautomaticallyaccountforthecovariate-shiftissue,whichis
anothertypicalchallengeforhigh-dimensionaltransferlearning.
RelatedWorks. Thereareseveraltransferlearningmethodsforhigh-dimensionalregressionproblems(e.g. Lietal.
[2022],TianandFeng[2022],Lietal.[2023b],Liu[2023]). Despitethegrowinginterest,noneofexistingworkshave
consideredlearningfeaturesadaptivelyfromthesource. Asforthesampleadaptivelearning,theseworksusuallyadopt
adiscontinuousdecisionstrategy,whereasourcesampleiseitherincludedorcompletelydiscarded. Thisfundamentally
differsfromoursample-adaptivemethod,wheretheweightscantakecontinuousvalues. Consequently,ourmethod
includes and continuously interpolates the binary decision options, offering the opportunity to achieving a better
performancebyexploringalargersearchspace.
Inthelow-dimensionalsetting,therearesomeexistingworksfocusingonadaptivelylearningfromheterogeneous
sources. Forinstance,samplereweighingtechniquehasbeenstudiedindomainadaptationtoalignthesourceand
2Feature-wiseandSample-wiseAdaptiveTransferLearning
target distributions [Shimodaira, 2000, Cortes et al., 2010, Fang et al., 2020, Zhang et al., 2020, Lu et al., 2022].
Whenconsideringmultiplesources,generalizationerrorofthemodelparameterobtainedbyminimizingempiricalrisk
weightedacrossthetaskshasbeenderived[Ben-Davidetal.,2010,Zhangetal.,2012,KonstantinovandLampert,2019,
Wenetal.,2020]. Thenweightcanbechosenbyminimizingthebound. Theproblemhasalsobeenstudiedinfederated
learning [Hanzely et al., 2021]. Therein, adaptive methods choose weights by solving hierarchical optimization
problems[Lietal.,2021,Dengetal.,2023]. Whilenon-exhaustivelylisted,methodsfallintotheabovecategoriesare
eitherpurelycomputationalorrequireestimatingthesourceandtargetdataprobabilitydistributionortheirdiscrepancies.
Consequently,theyarenotapplicableinthehighdimensionalsettingwherefeaturedimensionfarexceedsthesample
size.
Comparedtosample-adaptivemethods,theworksrelatedtofeature-wiseadaptivelearningisscarce. Onelineisvertical
federatedlearning[Yangetal.,2019],wherethedataispartitionedbyfeatures. Thestudythereinfocusoncomputation
methodsratherthanthestatisticalperformance.
Notation. Boldupper-andlower-caselettersdenotematricesandvectors, respectively. ForamatrixA Rm×n,
∈
we denote the i-th row of A as A , the column-submatrix indexed by a set S as A , A as the operator norm
i· S q
∥ ∥
induced by the q-norm and Λ (A) and Λ (A) as its maximum and minimum eigenvalues, respectively. Set
max min
[K] := 1,...,K . We let a b := max a,b and a b := min a,b . We use c,c ,c ,... to denote generic
0 1
constants{ independe} ntofn,pan∨ dK. Leta { =O} (b )and∧ a ≲b den{ ote} a /b cforsomeconstantcwhennis
n n n n n n
| |≤
largeenough;a =o(b )orb a ifa =O(c b )forsomec 0.
n n n n n n n n
≫ →
2 Preliminaries
Weconsiderahigh-dimensionaltransferlearningprobleminvolvingonetargettaskandK sourcetasks. Forthetarget
task,weobserveasample(X(0),y(0))generatedfromthelinearmodel
y(0) =X(0)β(0)+ϵ(0), (1)
wherey(0) RnT istheresponsesample,X(0) RnT×p isthefeaturematrix,β(0) Rp isthetargetparameterof
∈ ∈ ∈
primaryinterestandϵ(0) RnT istheobservationnoise. Thetargetparameterissparsewiths:= β(0)
0
nonzero
∈ ∥ ∥
elements,andtheambientdimensionpisallowedtobelargerthann .
T
In addition to the primary target sample, suppose we have access to K sets of auxiliary source samples
(X(k),y(k)) K ,eachgeneratedfrom
{ }k=1
y(k) =X(k)(β(0)+δ(k))+ϵ(k), (2)
Thesourceparameterdiffersfromthatofthetargetbyδ(k) Rp, whichwetermedas“target-sourcecontrast"or
∈
“non-transferablesignal". Forsimplicity,weassumethesourcesamplesshareacommonsize,i.e.,n =n , k [K],
k S
∀ ∈
whichistypicallylargerthann .
T
Ourgoalistoestimatethetargetparameterβ(0) usingbothtargetandsourcesampleswhereonly(a)partofsource
featuresineachsourcesamplearetransferableor(b)partofsourcesamplesaretransferable,whichwillbediscussedin
Section3andSection4,respectively.
ThissectionendswithtwostandardassumptionsonfeaturematrixX(k)andobservationerrorϵ(k).
Assumption 2.1. For any 0 k K, 0 i n , X(k)’s are independent sub-Gaussian random vectors with
≤ ≤ ≤ ≤ k i·
meanzeroandcovarianceΣ(k). Furthermore,thereexistssomeconstantcsuchthat1/c min Λ (Σ(k))
0≤k≤K min
≤ ≤
max Λ (Σ(k)) c.
0≤k≤K max
≤
Assumption2.2. Forany0 k K,0 i n ,theϵ(k)’sareindependentGaussianrandomvariableswithzero
≤ ≤ ≤ ≤ k i
meanandvarianceσ suchthatmax σ cforsomeconstantcandϵ(k)isindependentofX(k).
k 0≤k≤K k
≤
3 Feature-wiseAdaptiveTransferLearning
Intransferlearning,eachsourcetaskmayhaveitsuniquetransferablestructure: someofitsfeaturesaretransferable,
whereasothersarenot. Thetransferabilityofthejthfeatureinthek-thsourcetaskcanbeassessedbythemagnitudeof
target-sourcecontrastδ(k): Ifδ(k)isnegligible,indicatingthejthfeaturehasasimilareffectbetweenthek-thsource
j j
andthetarget,itistransferable. Conversely,ifδ(k)isnon-negligible,thecorrespondingfeatureshouldbetreatedas
j
non-transferable.
3Feature-wiseandSample-wiseAdaptiveTransferLearning
Let S = j : β(0) = 0 , depicting the sparsity structure of the primary parameter β(0). For k [K], define
0 { j ̸ } ∈
S = j : δ(k) = 0 ; then the features in the k-th source can be divided into two sets - the transferable set
k { j ̸ }
Sc = j :δ(k) =0 ,withzerosource-targetcontrasts,andthenon-transferablesetS withnon-negligiblecontrasts.
k { j } k
Note that we assume zero contrasts for the transferable sets for technical simplicity, and they can be relaxed to
sufficientlysmallcontrastsinpractice.
Nextweintroduceafeature-wiseadaptivetransferlearningframeworkthatauto-detectstransferablefeaturesineach
source,byassigningdifferentpenaltystrengthsbasedonthenon-transferablesignalδ(k)’s. Settingδ(0) =0,consider
j
theproblem
K p K p
min (cid:110) 1 (cid:88) y(k) X(k)(β(0)+δ(k)) 2+λ (cid:88) w(0) β(0) +λ (cid:88)(cid:88) w(k) δ(k) (cid:111) , (3)
β(0),δ(1),...,δ(K) N ∥ − ∥2 0 j | j | 1 j | j |
k=0 j=1 k=1j=1
whereN =Kn +n isthetotalsamplesize,λ andλ aretuningparametersandw(k)saretheweightsassigned
S T 0 1 j
for different non-transferable signals. The first term of (3) measures the overall fit of the models with parameters
β(0) +δ(k) K , and the second term imposes a penalty to achieve sparsity of β(0). The third term of (3) is a
{ }k=0
fused-penaltywithweightsthatadaptivelyshrinksδ(k)’s: Ideally,itappliesstrongerpenaltiestotransferablefeatures
j
withnegligibleδ(k),shrinksthemtozero,sothattheinformationfromthejthfeatureofthek-thsourceissuccessfully
j
transferredtohelpestimateβ(0);meanwhile,itdoesnotimposeexcessivepenaltiestonon-transferablefeatureswith
relativelylargeδ(k)’s,topreventintroducingbias. Problem(3)canbewrittenasastandardLassoregressionproblem
j
andcanbeefficientlysolved.
Weremarkthatthedefinitionoftransferable/informativefeaturesinvertstheusuallogicofpenalizedregression. Recall
thatinthepenalizedregression,non-informativefeaturesarepenalizedtozerowhileinformativefeaturesarepreserved
inthemodel. However,forthepenaltiesimposedonsource-targetcontrastsδ(k)’s,“shrinkingtozero"indeedrepresents
j
a transferable feature that is “informative" to the model, while δ(k)’s that are not penalized to zero correspond to
j
“non-informative"andnon-transferablefeatures.
3.1 WeightChoicewithKnownTransferableStructure
Anaturalquestioninfeature-wiseadaptivetransferlearningishowtodeterminetheweightsw(k)’s. Toanswerthis,we
j
firstconsideraidealscenariowherethesparsityandtransferablestructureareknown. Inthissetting,wecandefine
“oracleestimators"βˆ(0) ,δˆ(1) ,...,δˆ(K)
via
ora ora ora
K
1 (cid:88)
min y(k) X(k)(β(0)+δ(k)) 2
β(0),{δ(k)}K N ∥ − ∥2 (4)
k=1 k=0
s.t. β(0) =0, δ(k) =0, k [K].
S 0c S kc ∀ ∈
Basedontheabovedefinition,theoptimalweightsinthisoraclescenarioshouldbechosensuchthatsolvingproblem
(3)wouldyieldtheoracleestimatorin(4). Thechoiceofsuchoracleweightisprovidedinthefollowingtheorem.
(cid:113)
Theorem3.1. GivenAssumptions2.1and2.2,andprovidedthatn
S
≳logp,ifchoosew j(k) =1
{j∈S
kc},λ
0
≳ lo Ngp
(cid:113)
andλ 1 ≳ n NS lo ng Sp,thenwithprobabilitylargerthan1 −exp( −c 1logp),thesolutionofproblem(3)satisfies(4).
Theorem3.1statesthatifwechoosetheweightsuchthatonlythetransferablesetoffeaturesarepenalized,thenwitha
sufficientlystrongpenalty,solvingtheproblem(3)couldyieldanoracleestimatorthatfullyutilizesthetransferable
structure. Tobetterunderstandheoracleestimator, wedenoteX˜ = ((X(0) )⊤,(X˜(1) )⊤,...,(X˜(K) )⊤)⊤, where
S0 S0 S0 S0
X˜(k) = (I H(k) )X(k) and H(k) := X(k) [(X(k) )⊤X(k) ]−1(X(k) )⊤ represents the projection matrix onto the
S0 − Sk S0 Sk Sk Sk Sk Sk
columnspaceofX(k). Next,weprovidetheexplicitformoftheoracleestimatorofβ(0).
Sk
Proposition3.2. If S <n andmax S <n ,thesolutiontoproblem(4)satisfies
0 T 1≤k≤K k S
| | | |
βˆ(0) =[X˜⊤ X˜ ]−1X˜⊤ y and βˆ(0) =0, (5)
ora,S0 S0 S0 S0 ora,S 0c
4Feature-wiseandSample-wiseAdaptiveTransferLearning
NotethatX˜(k) istheprojectionofX(k),ourfeaturematrixofinterest,ontothenullspaceofX(k),thenon-transferable
S0 S0 Sk
feature matrix. The projection essentially filters out the non-transferable components, and the “overlap” between
X(k)andX(k)representsthetransferableproportionofthek-thsource: ifthenon-transferablefeaturematrixX(k)is
S0 Sk Sk
orthogonaltothetargetfeaturematrixX(k),X˜(k) =X(k);inthiscase,thek-thsourcesampleisfullytransferable. On
S0 S0 S0
theotherhand,ifX(k)islinearlyrelatedtoX(k),aswouldbethecasewhenS S ,thenX˜(k) =0,indicatingthat
Sk S0 0 ⊂ k S0
theentirek-thsourceisnon-transferable. Thus,X˜ isindeedthecollectionofthe“transferablepart”ofeachsource
sample,alongwiththefulltargetsample.
Inthefollowingproposition,westudytheestimationerroroftheoracleestimator.
Proposition3.3. UnderAssumptions2.1and2.2,if S < n ,max S < n andN logp,theerrorof
0 T 1≤k≤K k S
| | | | ≥
βˆ(0)
satisfiestheℓ bound
ora 2
∥βˆ(0) −β(0)∥ ≲κ (cid:13) (cid:13) (cid:13)(cid:32) X⊤ S0X S0(cid:33)−1(cid:13) (cid:13) (cid:13) (cid:114) slogs , (6)
ora 2 F (cid:13) N (cid:13) N
(cid:13) (cid:13)
∞
withprobabilitylargerthan1 exp(c logp),whereX iscolumn-submatrixindexedbyS ofthefull-sampledesign
−
1 S0 0
matrixX,and
(cid:13) (cid:13)
(cid:13)[X˜⊤ X˜ ]−1X˜⊤ ϵ(cid:13)
κ F := (cid:13) (cid:13) (cid:13)[XS ⊤ S0 0X SS 00 ]−1X⊤ SS 00 ϵ(cid:13) (cid:13)(cid:13) ∞∞. (7)
Theterm(slogs/N)1/2 in(6)isnearlyoptimalforestimatingβ(0) usingthefullsamplesofsizeN -itrevealsthe
benefitoftransferringthesourceinformationwhenestimatingβ(0). (cid:0) X⊤ X /N(cid:1)−1 isafull-sampleestimateofthe
S0 S0
precisionmatrixΣ−1andhasaboundedℓ norm. Thetermκ in(7)measuresthetransferabilityofsourcedatasets.
S0 ∞ F
IfallsourcedatasetsaretransferableinthesensethatX(k) X(k)forallk [K],thenκ =1. Inthisidealcase,we
Sk ⊥ S0 ∈ F
canutilizeallinformationfromsize-N fullsamplewhenestimatingβ(0). Otherwise,ifallsourcetasksarecompletely
(cid:112)
non-transferable in the sense that S S for all k [K], we have κ N/n . In this case, the estimation
0 k F T
(cid:112) ⊂ ∈ ≍
ratebecomes slogs/n ,whichisactuallythesingle-taskbenchmarkuptoalogarithmorder. Betweenthesetwo
T
extremes,eachsourcecontainsbothtransferableandnon-transferablefeatures,henceκ reflectstheproportionof
F
transferableinformationamongthefullsample.
3.2 WeightChoicewithUnknownTransferableStructure
We now introduce a data-driven approach to determine the weights in (3) when the non-transferable signals δ(k)s
j
areunknown,whichisoftenthecaseinpractice. Recallthatalargerpenaltyshouldbeimposedonnegligibleδ(k)
j
andasmallerpenaltyonnon-negligibleδ(k). Asuitablecandidateforthispurposeisthefamilyoffolded-concave
j
penalties,denotedby (t),includingSCAD[FanandLi,2001],MCP[Zhang,2010],etc. Seeaformaldefinitionofa
λ
R
folded-concavepenaltyinAppendixC.1. Furthermore,inspiredbythelocallinearapproximation(LLA)algorithm
[ZouandLi,2008,Fanetal.,2014],weconsiderthefirst-orderTaylorapproximationofthefolded-concavepenalties
onsomeinitialestimators:
(β(0) ) ′(β(0) )(β(0) β(0) ),
Rλ j ≈Rλ init,j − init,j
(δ(k) ) ′(δ(k) )(δ(k) δ(k) ), k [K],
Rλ j ≈Rλ init,j − init,j ∈
where ′(t)isthefirst-orderderivativeof (t). Basedonthis,wesettheweightsin(3)asλ w(0) = ′ (β(0) )
Rλ Rλ 0 j Rλ0 init,j
andλ w(k) = ′ (δ(k) ). Fig. 2depictstheSCADpenaltyfunctionalongwithitsderivative,whichdemonstratesa
1 j Rλ1 init,j
strongerpenaltytosmallentriestopromotesparsesolutions,andaconstantpenaltytolargeentriestoavoidbias.
Inthenexttheorem,weshowthatwithanappropriatechoiceofinitialestimatorandtuningparameters,solvingthe
problem(3)withtheproposedweightyieldstheoracleestimatorin(4),orequivalently,Proposition3.2. Tobeginwith,
weconsidertheparameterspace
(cid:110) (cid:111)
Θ = δ(k) h∧, δ(k) =0,k [K], β(0) h∧, β(0) =0 , (8)
1 ∥ Sk∥min ≥ k ∥ S kc ∥max ∈ ∥ S0∥min ≥ 0 ∥ S 0c ∥max
5Feature-wiseandSample-wiseAdaptiveTransferLearning
2 1
0.8
1.5
0.6
1
0.4
0.5
0.2
x x
2 1 1 2 2 1 1 2
− − − −
Figure2: PenaltyFunctions. SCAD(blue)andLasso(green)shownontheleft;theabsolutevalueoftheirfirst-order
derivativesareontheright.
whereh∧’sandh∧areminimumsignals,whicharecrucialforidentifyingthetransferableandsparsitystructureusing
k 0
thelimitedsamples. Incaseswherethisminimumsignalassumptionistoorestrictive,onemayconsiderthealternatives
discussedinSection4.
Theorem3.4. UnderAssumption2.1and2.2,considerinitialestimatorssatisfyingforallk [K],
∈
(cid:13) (cid:13)βˆ(0) β(0)(cid:13) (cid:13) a 2 λ , (cid:13) (cid:13)δˆ(k) δ(k)(cid:13) (cid:13) a 2 λ . (9)
(cid:13) init − (cid:13) ∞ ≤ 2 0 (cid:13) init − (cid:13) ∞ ≤ 2 1
Further assume n ≳ logp, h∧ 3aλ ≳ n /N(cid:112) logp/n , k [K] and h∧ 3aλ ≳ (cid:112) logp/N, where
S k ≥ 2 1 S S ∈ 0 ≥ 2 0
a > a 0 and a 0 are constants specified in Appendix C.1. If we choose w(0) = (λ )−1 ′ (β(0) ) and
2 ≥ 1 ≥ j 0 Rλ0 init,j
w(k) =(λ )−1 ′ (δ(k) ),thenwithprobabilitylargerthan1 exp( c logp),thesolutionofproblem(3)satisfies
j 1 Rλ1 init,j − − 1
(4).
Theorem3.4suggeststhatundercertainconditions,solvingtheproblem(3)yieldstheoracleestimator. Theminimum
signalstrengthconditiononh∧requiresthatthecontrastδ(k)withinthetransferablesetSc andthenon-transferableset
k k
S areseparable. Condition(9)requiresthattheunderlyingproblemislocalizable. Thatis,itshouldbepossibleto
k
identifyaninitialestimatorthatisaccurateenoughtodifferentiatetheseparatedδ(k)’s. PropositionC.1inAppendixC
providesaconcreteexampleofsuchaninitialestimator.
Insum,Theorem3.4indicatesthatundercertainconditions,thefoldedconcavepenalization-basedtransferlearning
procedurecanfullyutilizethesparsenatureofβ(0)andthetransferablestructureofsourcetasksforestimatingβ(0).
4 Sample-wiseAdaptiveTransferLearning
Thesuccessofthefeature-wiseadaptivetransferlearningreliesontheassumptionthatthetargetsignalissufficiently
strongandthesource-targetcontrastpossessesatransferablestructure,i.e.,δ(k)isasparsevectorwithh∧ sufficiently
k
large. Inapplicationswherethesourcesamples’qualityisloworunreliable,thesourceparametercandeviatefromthat
ofthetargetsignificantly,yieldingdensesource-targetcontrasts. Inthissetting,theoreticalguaranteesofthefeature
adaptivemethod(3)nolongerholdduetothehardnessofidentifyingthetransferableelementsofthesources. To
addresstheproblemandlimittheimpactoflessinformativesources,weintroduceasample-wiseadaptivetransfer
learningmethodthatautomaticallyweightsthesourcesamplesaccordingtotheirinformativelevel.
Letw =[w ,...,w ]withw 0beingtheweightassignedtothek-thsource,andrecallthatn =n fork [K]
0 K k k S
≥ ∈
andn =n fork =0.
Weproposetoestimatethetargetparameterβˆ(0)
bysolving:
k T
(cid:118)
(cid:40) K (cid:117) K K (cid:41)
min 1 (cid:88) w y(k) X(k)(β(0)+δ(k)) 2+λ (cid:117) (cid:116)(cid:88)n k w2 β(0) +λ (cid:88) w δ(k) , (10a)
β(0),{δ(k)}K N k ∥ − ∥2 0 N k∥ ∥1 1 k ∥ ∥1
k=1 k=0 k=0 k=1
1
s.t. (X(0))⊤(y(0) X(0)β(0)) λ , (10b)
∞ T
n ∥ − ∥ ≤
T
whereλ ,λ andλ arenonnegativetuningparameters.
0 1 T
6Feature-wiseandSample-wiseAdaptiveTransferLearning
Problem(10)isdesignedbasedonthesamefused-penaltyas(3). However,twokeydifferencesaremadetoachieve
sample-wiseadaptivetransfer. First,unlikeinproblem(3)wheredifferentweightsareappliedelement-wisetoβ(0)
andδ(k),asinglescalarw isassignedtoboththelosstermandparametersassociatedtothek-thsource,adjustingits
k
importanceinestimatingthetargetparameter. Second,weintroduceconstraint(10b)toidentifyβ(0): Thelefthand
sideof(10b)correspondstothegradientoftargetlossfunctionandisequaltozeroatthegroundtruthβ(0) inthe
noiselesscaseϵ(0) =0;theslacknessλ isintroducedtoaccountforthepresenceofnoise. Intuitively,constraint(10b)
i T
narrowsdownthesearchspaceforβ(0)usingprimarytargetdata,andtheobjectivefunction(10a)proceedstousethe
fullsample,improvingtheestimationaccuracy.
Wenotethatthesolutionofproblem(10)isinvariantunderpositivescalingofthew ’s. Toremovesuchambiguity,we
k
restricttheweightstosatisfythenormalizationconstraint
K
(cid:88)n
k
w =1, w 0, k =0,...,K. (11)
k k
N ≥
k=0
Tobetterunderstandtheformulation(10),weexaminetwocases. Ifwesetthetargetweightw = N/n andthe
0 T
sourceweightw =0forallk [K],problem(10)reducestoasingle-tasklassoregressionproblemwhereonlythe
k
∈
targetdataisusedtoestimateβ(0). Inthiscase,thereisnoinformationtransferfromthesourcesamplesandshouldbe
chosenwhentheδ(k)’sarelarge. Ontheotherhand,ifwesetallw =1forallk =0,...,K,thenequalimportance
k
isassignedtoallsamplesinestimatingβ(0). Suchweightassignmentisidealifalltheδ(k)’sarezero. Byadjustingthe
w ’s,weinterpolatethetwoextremesandaimtoachievetheoptimalbalancebetweenthesourceandtargetsamples.
k
4.1 WeightChoicewithKnownInformativeLevel
Wefirstdiscussthechoiceofw withaknowninformativelevel,characterizedbythefollowingparameterspace:
k
(cid:110) (cid:111)
Θ := β : β(0) s, δ(k) h ,k [K] . (12)
2 0 1 k
∥ ∥ ≤ ∥ ∥ ≤ ∈
Thetargetparameterβ(0)isstillassumedtobes-sparse. However,theinformativelevelofthek-thsourceisquantified
bytheℓ normofthewholesource-targetcontrastδ(k),allowingfordenseδ(k)swithsmallelements. Alargerh
1 k
indicatesthatthek-thsourceislesssimilartothetarget.
Thefollowingresultestablishestheestimationerrorofβˆ(0)
assumingbothsandh ’sareknown.
k
(cid:113)
Theorem 4.1. Under Assumption 2.1 and 2.2, assume n > n , slogp +Kh¯(w) logp = o(1) and w = 0 or
S T nT nT k
w wforsomesmallconstantw >0. Ifwechoose
k
≥
λ
0
=c
0 (cid:32) (cid:88)K n
Nkw
k2(cid:33)−1 2(cid:32) h¯(w s) 22 nlogp(cid:33) 41 +(cid:32) lo Ngp(cid:33)1 2
,
T
k=0
(cid:32) (cid:33)1 (cid:32) (cid:33)1
n logp 2 logp 2
λ =c S , andλ =c ,
1 0N n T 1 n
T T
forsomeappropriateconstantsc andc ,thenthesolutionof(10)satisfies
0 1
(cid:114)
βˆ(0) β(0) 2 ≲κ (w)slogp +c h¯(w) logp , (13)
∥ − ∥2 S N Σ n
T
withprobabilitylargerthan1 exp(c logp),where
2
−
κ
(w):=(cid:88)K n kw k2
,
h¯(w):=(cid:88)K n kw
k
h , (14)
S k
N N
k=0 k=1
andc isauniversalconstant.
Σ
Termh¯(w)istheweightednon-transferablelevelandc isauniversalconstantdependingonthelargestandsmallest
Σ
eigenvaluesof Σ(k) K . Tointerpretκ (w),wenoticethatunderthenormalizationconstraint(11),bysolvingthe
{ }k=0 S
KKTsystemwecanconcludeκ (w)attainsminimumvalue1withthecorrespondingw =1fork =0,...,K. In
S k
7Feature-wiseandSample-wiseAdaptiveTransferLearning
suchacase,thefirsttermisslogp/N andcorrespondstoanearminimaxoptimalrateusingN targetsamples[Raskutti
et al., 2011] . Therefore, in the same way as κ defined in (7), κ (w) can be viewed as a metric quantifying the
F S
transferabilityofthesourcedatasets.
To further understand the result, we revisit the two extreme cases previously discussed. Under the weight choice
w =N/n andw =0forallk [K],problem(10)reducestothesingle-tasklassoregressionproblemand (13)
0 T k
∈
correspondingly degenerates to the near minimax optimal bound slogp/n . When setting w = 1, on the other
T k
hand,(13)becomesslogp/N +h˜(cid:112) logp/n withh˜ := 1 (cid:80)K h beingtheaverageofallh s. Inthiscase,the
T K k=1 k k
firsttermof(13)achievesitsminimum,atthecostofintroducinganonzerosecondtermaccountingforthenegative
impactofnon-transferablesource-targetcontrastsδ(k). Sucharateisagainnearminimaxoptimalformulti-source
sparseregressionproblemsundersmallh˜ [Lietal.,2022].
Withaflexiblechoiceoftheweightsw ’s,theupperbound(13)representsamiddlegroundbetweenthetwoextremes.
k
Asκ (w) 1,thefirstterm,κ (w)slogp/N,reflectsalossintheeffectivesamplesizewhenthesourcesamplesare
S S
notfullyut≥
ilized.
Thesecondterm,h¯(w)(cid:112)
logp/n ,alsodependingontheweightw ,showsthepotentialreduction
T k
ofnegativetransferifh islargebutthecorrespondingsourcesampleisdown-weighted. Consequently,theweights
k
shouldbecarefullychosentobalancethetradeoffbetweenthesetwotermstoachievetheoptimalestimationrate.
Togaintheoreticalinsights,weconsiderthesimplecasewithK =1,andinvestigatetheimpactofthesamplesizen ,
S
n ,andtheinformativelevelh ontheoptimalweights.
T k
Corollary4.2. SupposeK = 1,theweightundercondition(11)thatminimizestheestimationerrorbound(13)is
w =(N/n )w′ andw =(N/n )w′,with
0 S 0 1 T 1
(cid:40) (cid:41)
w′ =max
slogp
−
c 2Σ√n
T
·h 1√logp
,0 ,
1 slogp+ nT slogp
nS ·
andw′ =1 w′.
0 − 1
Corollary4.2revealsthefollowingfacts. Whenh = 0,wehavew = w = 1,corroboratingthefactthatsource
1 0 1
andtargetsamplesshouldbeequallyweightedwhensharingthesamegroundtruthparameter. Ash increases,the
1
weightw assignedtothesourcesampledecreases,indicatingweshoulddown-weightthesourcewhenitbecomesless
1
informative. Consideringtheimpactofthesamplesize,weseethatthenormalizedweightw′ decreaseseitherwhen
1
n decreasesorn increases. Theimplicationisintuitive,showingthatunderthesetwocases,thetargetbecomes
S T
relativelymorereliableandmoreweightsshouldbeallocated.
WefinishbydiscussingtheassumptionsofTheorem4.1. Theconditionslogp/n =o(1)showsthatthedimension
T
pcangrowuptoanexponentialorderwithrespecttothetargetsamplesize. Suchasample-dimensiondependency
isstandardinthehigh-dimensionalsetting.
TherequirementthatKh¯(w)(cid:112)
logp/n isboundedcanbefulfilledif
T
eitherh¯(w)diminishesorn growswiththetasknumberK. Thelowerboundonw preventstheweightstobeo(1)
T k
aspgrows. Inpracticeitcanbesetasasmallnumericalconstant. Thesetworequirementsaretechnicalconditions
imposedtodealwiththehigherordertermsariseduetothenon-strongconvexityandsmoothnessoftheempirical
loss,challengesuniquetothehighdimensionalsetting(cf. LemmaA.5andSec.4.1). Notably,ourbounddoesnot
dependonthewholecovariancematricesΣ(k)’sbutonlytheireigenvaluebounds,demonstratingtherobustnessofour
methodtocovariateshifts. Atthetechnicallevel,theproofofthetheoreminvolvesanon-asymptoticanalysisofthe
coupledstructureofβ(0)andδ(k)inproblem(10),togetherwithafinetreatmentofboththenon-strongconvexityand
smoothness,whichmaybeofindependentinterest.
4.2 WeightChoicewithUnknownInformativeLevel
Obtainingoptimalweightsbyminimizingtheerrorboundin(13)requiresknowingthesparsitylevelsofβ(0),the
informativelevelh ,andtheuniversalconstantc. Inthissection,weprovideadata-drivenapproachtocomputethe
k
weightsbysolvingthefollowingoptimizationproblem:
min
(cid:40) (cid:88)K ∥βˆ( in0 i)
t∥0logp (w′)2+λ
(cid:88)K
δˆ(k)
(cid:114)
logp
w′(cid:41)
{w′}K n S k W ∥ init∥1 n T k
k k=0 k=0 k=1
K
(cid:88)
s.t. w′ =1andw′ 0, k =0,1,...,K. (15)
k k ≥
k=0
8Feature-wiseandSample-wiseAdaptiveTransferLearning
In(15),weestimatesandh respectivelyby
βˆ(0)
and
δˆ(k)
usingsomeinitialestimators,suchastheLasso
k
∥
init∥0
∥
init∥1
estimatordescribedinCorollaryC.1. Theunknownconstantc ishandledasahyperparameterλ ,whoseoptimal
1 W
choiceisfoundthroughcross-validationonthetargetdataset. Theoptimizationproblem(15)isconvexandcanbe
solvedefficiently. Withtheweightcomputedby(15),wecanthenproceedtosolveproblem(10)toobtainanestimator
for β(0). Note that although problem (10) involves three hyperparameters, namely λ , λ , and λ , their relative
0 1 T
magnitudesarefixedandthuscanbejointlytuned.
5 EmpericalExperiments
Weevaluatetheempiricalperformanceofourproposedmethods, F-AdaTransandS-AdaTrans, andcomparewith
existing high-dimensional transfer learning method TransGLM [Tian and Feng, 2022]. Specifically, the following
methodsareimplemented:
Ora-Est : Oracle estimator with known transferable structure and Ora-F-AdaTrans : F-AdaTrans with known
•
transferablestructure(Section3.1);F-AdaTrans: F-AdaTranswithdata-drivenweightsselection(Section3.2);
Ora-S-AdaTrans: S-AdaTranswithknownsourceinformativelevel(Section4.1);S-AdaTrans: S-AdaTranswith
•
data-drivenweightsselection(Section4.2);
TransGLM[TianandFeng,2022]: atransferlearningalgorithmwithtransferablesourcedetection,eachsourceis
•
eitherincludedordiscardedbasedonsomedecisionrule;
Lasso(baseline): LASSOregressiononthetargettask.
•
Eachsimulationsettingisreplicatedwith100independenttrials,andwereporttheaverageestimationerrorofthe
targetparameter,i.e., βˆ(0) β(0) 2. ImplementationdetailscanbefoundinAppendixD.1.
∥ − ∥2
Problemsettings. Weconsiderahigh-dimensionallinearregressionproblemwithdimensionp=500andsparsity
levels = 8. Thetargetcoefficientissetasβ(0) = 0.3for1 j sandβ(0) = 0otherwise. Thetargetsampleis
j ≤ ≤ j
generatedaccordingto(1)withn =50andthesourcesamplesaregeneratedaccordingto(2). Wesetn =250and
T S
K =2ifnototherwisespecified. Weconsiderthefollowingtwoparameterconfigurationsofδ(k).
Setting1: feature-wiseadaptivetransfer. Wegeneratetwotypesofsourcesampleswithnon-overlappingtransferable
featuresasdepictedinFig.1(a). Fork [K],ifkisoddthenon-transferableδ(k)isnonzeroforthefirsts/2elements;
∈
otherwiseithasnonzeroelementsrangingfroms/2+1tos . Thestrengthofδ(k) iscontrolledbyaparameterh∧.
k
Weseth∧ =0.6ands =25ifnototherwisespecified.
k
Setting2:sample-wiseadaptivetransfer.Wegeneratethetwotypesofsourcescorruptedbyweakdensenon-transferable
signals,asdepictedinFig.1(b). Thenon-transferablesignalδ(k)isnonzeroforthefirsts elemenetsforallk [K].
k
∈
Foroddkthestrengthofδ(k) iscontrolledbyh∧/10,andforevenkitiscontrolledbyh∧. Weseth∧ =0.024and
s =450ifnototherwisespecified.
k
ThedetaileddescriptionoftheproblemsettingislefttoAppendixD.2. Weinvestigatetheimpactoffourkeyfactorson
theperformanceofF-AdaTransinSetting1andthatofS-AdaTransinSetting2,namely,the(i)non-transferablesignal
strengthh∧,(ii)numberofnon-transferablefeaturess ,(iii)numberofsourcetasksK,and(iv)sourcesamplesizen ,
k S
reportedrespectivelyinFig.3andFig.4. FordetailsonthecomparativeperformanceofF-AdaTransandS-AdaTrans,
pleaseseeAppendixD.4.
F-AdaTrans(Setting1). Fig.3(i)illustratesasthestrengthofnon-transferablesignalsh∧increases,theestimation
errorofF-AdaTransslightlyincreasesthendecreases. Thistrenddemonstratesthemethod’sincreasedcapabilityto
identifythetransferablestructurewithincreasingsignalstrength. Oncesomenon-transferablefeaturesarepinpointed,
theyaresubsequentlyfilteredout,andtherebypreventingfurthernegativetransfer. Thisfindingisfurthersupported
byFig.7inAppendixD.4,showingthedetectionaccuracyofthenon-transferablefeaturesincreasesash∧increases.
Incontrast,theTransGLMmethodshowsarapidlyincreasingerror,duetothefactthatitcanonlyeitherincludeor
completelydiscardasourcesample,butfailstotransferusefulfeatures.
Fig.3(ii)showsass increases,TransGLMhasanerrorclosetoLasso(baseline),primarilybecausethesourcesamples
k
areexcludedintheestimation. TheerrorofF-AdaTransincreasessincetheinitialestimatorgetsworseasthecardinality
ofδ(k)grows,makingithardertoidentifythetransferablestructureinthesource. However,itstillachievesabetter
performancethanthebenchmarkmethodsduetothepartialinclusionoftransferablefeatures.
9Feature-wiseandSample-wiseAdaptiveTransferLearning
Figure3: Theaverageestimationerrorunderfeature-wiseadaptivetransfersetting(Setting1)
Figure4: Theaverageestimationerrorundersample-wiseadaptivetransfersetting(Setting2)
10Feature-wiseandSample-wiseAdaptiveTransferLearning
Fig.3(iii)and(iv)showsasthetotalnumberofsourcesamplesincreases,eitherintheformofanincreasingtasknumber
K orthesamplesn pertask,F-AdaTransachievesasmallerestimationerror. WecanalsoobservethatF-AdaTrans
S
hasafavourablescalabilitywithrespecttoK.
Sample-AdaTrans(Setting2). Fig.4(i)showsS-AdaTransisrobusttoincreasingh∧. Itisinterestingtoobservethat
TransGLMinthiscaseshowssignificantinstabilityintheestimationerror. Thisisbecauseforsmallh∧,TransGLM
includesbothsourcestoestimateβ(0) formostofthetrails. ThisleadstoanerrorevenlargerthanLasso(baseline)
duetonegativetransfer. Theerrorthendecreasesforlargeh∧,sinceinsomereplicationsthenon-informativesource
isdetectedandexcluded. S-AdaTrans,ontheotherhand,isabletoadjusttheimportanceofthesourcesamplesby
assigningcontinuousvaluesofweights,andthusmorestablewithrespecttothechangeofsourceinformativelevel.
SeeFig.8inAppendixD.4forthenumericalevidence. SimilarbehaviorofthemethodsalsoappearinFig.4(ii)ass
k
increases,asbothtwocasesessentiallyimplyanincreaseinh [cf.(12)].
k
Forcases(iii)and(iv),weobservetheerrorofS-AdaTransdecreases. Further,itgetsclosertoOra-S-AdaTranswith
knownsourceinformativelevelsincetheinitialestimatorsbecomemoreaccurate.
In addition to the synthetic data study, we also apply F-AdaTrans and S-AdaTrans to analyzing the impact of the
high-dimensionalfinancialmetricsonthestockrecoveryabilityofS&P500companiesaftertheCOVID-19pandemic.
TheresultsareprovidedinAppendixD.3.
6 Conclusion
Inthispaper,weintroduceanadaptivetransferlearningmethodAdaTransenablingfeature-andsample-specifictransfer
inhigh-dimensionalregression. Ifthetransferableinformationisknown,wederivetheoptimalweightsandtherate
oftheseoracleestimators. Whenthetransferableinformationisunknown,wefurtherdevelopdata-drivenmethods
to compute the weights. We show both theoretically and empirically that by adaptively choosing the transferable
information,AdaTransoutperformsexistingmethods.
References
ShaiBen-David,JohnBlitzer,KobyCrammer,AlexKulesza,FernandoPereira,andJenniferWortmanVaughan. A
theoryoflearningfromdifferentdomains. Machinelearning,79:151–175,2010.
CorinnaCortes,YishayMansour,andMehryarMohri. Learningboundsforimportanceweighting. Advancesinneural
informationprocessingsystems,23,2010.
Yuyang Deng, Mohammad Mahdi Kamani, Pouria Mahdavinia, and Mehrdad Mahdavi. Distributed personalized
empiricalriskminimization. InInternationalWorkshoponFederatedLearningforDistributedDataMining,2023.
JianqingFanandRunzeLi. Variableselectionvianonconcavepenalizedlikelihoodanditsoracleproperties. Journalof
theAmericanstatisticalAssociation,pages1348–1360,2001.
JianqingFan,LingzhouXue,andHuiZou. Strongoracleoptimalityoffoldedconcavepenalizedestimation. Annalsof
statistics,42(3):819,2014.
TongtongFang,NanLu,GangNiu,andMasashiSugiyama. Rethinkingimportanceweightingfordeeplearningunder
distributionshift. Advancesinneuralinformationprocessingsystems,33:11996–12007,2020.
FilipHanzely,BoxinZhao,andMladenKolar. Personalizedfederatedlearning: Aunifiedframeworkanduniversal
optimizationtechniques. arXivpreprintarXiv:2102.09743,2021.
NikolaKonstantinovandChristophLampert. Robustlearningfromuntrustedsources. InInternationalconferenceon
machinelearning,pages3488–3498.PMLR,2019.
Hailong Li, Nehal A Parikh, and Lili He. A novel transfer learning approach to enhance deep neural network
classificationofbrainfunctionalconnectomes. Frontiersinneuroscience,12:491,2018.
SaiLi,TTonyCai,andHongzheLi. Transferlearningforhigh-dimensionallinearregression: Prediction,estimation
andminimaxoptimality. JournaloftheRoyalStatisticalSocietySeriesB:StatisticalMethodology,84(1):149–173,
2022.
SaiLi,TTonyCai,andHongzheLi. Transferlearninginlarge-scalegaussiangraphicalmodelswithfalsediscovery
ratecontrol. JournaloftheAmericanStatisticalAssociation,118(543):2171–2183,2023a.
SaiLi,LinjunZhang,TTonyCai,andHongzheLi. Estimationandinferenceforhigh-dimensionalgeneralizedlinear
modelswithknowledgetransfer. JournaloftheAmericanStatisticalAssociation,pages1–12,2023b.
11Feature-wiseandSample-wiseAdaptiveTransferLearning
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through
personalization. InInternationalConferenceonMachineLearning,pages6357–6368.PMLR,2021.
Shuo Shuo Liu. Unified transfer learning models for high-dimensional linear regression. arXiv preprint
arXiv:2307.00238,2023.
NanLu,TianyiZhang,TongtongFang,TakeshiTeshima,andMasashiSugiyama. Rethinkingimportanceweightingfor
transferlearning. InFederatedandTransferLearning,pages185–231.Springer,2022.
GarveshRaskutti,MartinJWainwright,andBinYu. Minimaxratesofestimationforhigh-dimensionallinearregression
overℓ -balls. IEEETransactionsonInformationTheory,57(10):6976–6994,2011.
q
HidetoshiShimodaira. Improvingpredictiveinferenceundercovariateshiftbyweightingthelog-likelihoodfunction.
JournalofStatisticalPlanningandInference,90(2):227–244,2000.
LuTangandPeterXKSong.Fusedlassoapproachinregressioncoefficientsclustering:learningparameterheterogeneity
indataintegration. TheJournalofMachineLearningResearch,17(1):3915–3937,2016.
YeTianandYangFeng. Transferlearningunderhigh-dimensionalgeneralizedlinearmodels. JournaloftheAmerican
StatisticalAssociation,pages1–14,2022.
LisaTorreyandJudeShavlik. Transferlearning. InHandbookofresearchonmachinelearningapplicationsandtrends:
algorithms,methods,andtechniques,pages242–264.IGIglobal,2010.
DouglasWahlsten,PamelaMetten,TamaraJPhillips,StephenLBoehm,SueBurkhart-Kasch,JanetDorow,Sharon
Doerksen,ChrisDowning,JenniferFogarty,KristinaRodd-Henricks,etal. Differentdatafromdifferentlabs: lessons
fromstudiesofgene–environmentinteraction. Journalofneurobiology,54(1):283–311,2003.
JunfengWen,RussellGreiner,andDaleSchuurmans.Domainaggregationnetworksformulti-sourcedomainadaptation.
InInternationalconferenceonmachinelearning,pages10214–10224.PMLR,2020.
QiangYang,YangLiu,TianjianChen,andYongxinTong. Federatedmachinelearning: Conceptandapplications. ACM
TransactionsonIntelligentSystemsandTechnology(TIST),10(2):1–19,2019.
ChaoZhang,LeiZhang,andJiepingYe. Generalizationboundsfordomainadaptation. Advancesinneuralinformation
processingsystems,25,2012.
Cun-HuiZhang. Nearlyunbiasedvariableselectionunderminimaxconcavepenalty. TheAnnalsofStatistics,38(2):
894–942,2010.
TianyiZhang,IkkoYamane,NanLu,andMasashiSugiyama. Aone-stepapproachtocovariateshiftadaptation. In
AsianConferenceonMachineLearning,pages65–80.PMLR,2020.
HuiZouandRunzeLi. One-stepsparseestimatesinnonconcavepenalizedlikelihoodmodels. Annalsofstatistics,36
(4):1509,2008.
12