ZigMa: Zigzag Mamba Diffusion Model
Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui,
Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer
LMU Munich
https://taohu.me/zigma/
Abstract Thediffusionmodelhaslongbeenplaguedbyscalabilityand
quadratic complexity issues, especially within transformer-based struc-
tures. In this study, we aim to leverage the long sequence modeling
capability of a State-Space Model called Mamba to extend its appli-
cability to visual data generation. Firstly, we identify a critical over-
sight in most current Mamba-based vision methods, namely the lack
of consideration for spatial continuity in the scan scheme of Mamba.
Secondly, building upon this insight, we introduce a simple, plug-and-
play, zero-parameter method named Zigzag Mamba, which outperforms
Mamba-based baselines and demonstrates improved speed and memory
utilizationcomparedtotransformer-basedbaselines.Lastly,weintegrate
ZigzagMambawiththeStochasticInterpolantframeworktoinvestigate
the scalability of the model on large-resolution visual datasets, such as
FacesHQ 1024×1024 and UCF101, MultiModal-CelebA-HQ, and MS
COCO 256×256. Code will be released at https://taohu.me/zigma/
Keywords: Diffusion Model · State-Space Model · Stochastic Inter-
polant
1 Introduction
Diffusionmodelshavedemonstratedsignificantadvancementsacrossvariousap-
plications, including image processing [68], video analysis [39], point cloud pro-
cessing [88], and human pose estimation [28]. Many of these models are built
uponLatentDiffusionModels(LDM)[68],whicharetypicallybasedontheUNet
backbone.However,scalabilityremainsasignificantchallengeinLDMs[44].Re-
cently,transformer-basedstructureshavegainedpopularityduetotheirscalabil-
ity[8,66]andeffectivenessinmulti-modaltraining[9].Notably,thetransformer-
based structure DiT [66] has even contributed to enhancing the high-fidelity
video generation model SORA [65] by OpenAI. Despite efforts to alleviate the
quadratic complexity of the attention mechanism through techniques such as
windowing [60], sliding [11], sparsification [16,49], and hashing [17,75], it re-
mains a bottleneck for diffusion models.
Ontheotherhand,State-SpaceModels[30,31,34]haveshowngreatpotential
forlongsequencemodeling,competingwithtransformer-basedmethods.Several
methods[26,29,31,70]havebeenproposedtoenhancetherobustness[93],scala-
bility [29], and efficiency [31,32] of State-Space Models. Among these, a method
4202
raM
02
]VC.sc[
1v20831.3042:viXra2 Hu et al.
20k iterations training 30k iterations training
Continuity is broken
Sweep without considering Spatial Continuity
Single Direction Zigzag Mamba
Multi Direction Zigzag Mamba
Figure1: Motivation. Our Zigzag Mamba method improves the network’s position-
awarenessbyarrangingandrearrangingthescanpathofMambainaheuristicmanner.
called Mamba [29] aims to alleviate these issues through work-efficient paral-
lel scanning and other data-dependent innovations. However, the advantage of
Mambaliesin1Dsequencemodeling,andextendingitto2Dimagesisachalleng-
ingquestion.Previousworks[59,98]haveproposedflattening2Dtokensdirectly
by computer hierarchy such as row-and-column-major order, but this approach
neglects Spatial Continuity, as shown in Figure 1. Other works [56,62] consider
various directions in a single Mamba block, but this introduces additional pa-
rameters and GPU memory burden. In this paper, we aim to emphasize the
importance of Spatial Continuity in Mamba and propose several intuitive and
simple methods to enable the application of Mamba blocks to 2D images by in-
corporatingcontinuity-basedinductivebiasesinimages.Wealsogeneralizethese
methods to 3D with spatial-temporal factorization on 3D sequence.
Intheend,StochasticInterpolant[3]providesamoregeneralizedframework
that can uniform various generative models including, Normalizing Flow [14],
diffusionmodel[38,71,73],Flowmatching[4,54,58],andSchrödingerBridge[55].
Previously,someworks[63]exploretheStochasticInterpolantonrelativelysmall
resolutions,e.g.,256×256,512×512.Inthiswork,weaimtoexploreitinfurther
more complex scenarios e.g., 1024×1024 resolution and even in videos.ZigMa 3
In summary, our contributions are as follows: Firstly, we identify the critical
issue of Spatial Continuity in generalizing the Mamba block from 1D sequence
modeling to 2D image and 3D video modeling. Building on this insight, we pro-
pose a simple, plug-and-play, zero-parameter paradigm named Zigzag Mamba
(ZigMa) that leverages spatial continuity to maximally incorporate the induc-
tive bias from visual data. Secondly, we extend the methodology from 2D to 3D
byfactorizingthespatialandtemporalsequencestooptimizeperformance.Sec-
ondly, we provide comprehensive analysis surrounding the Mamba block within
the regime of diffusion models. Lastly, we demonstrate that our designed Zigzag
Mamba outperforms related Mamba-based baselines, representing the first ex-
plorationofStochasticInterpolantsonlarge-scaleimagedata(1024×1024)and
videos.
2 Related Works
Mamba. Several works [83,84] have demonstrated that the State-Space Model
possesses universal approximation ability under certain conditions. Mamba, as
a new State-Space Model, has superior potential for modeling long sequences
efficiently, which has been explored in various fields such as medical imag-
ing [62,69,87,90], image restoration [33,97], graphs [10], NLP word byte [81],
tabular data [2], point clouds [52], and image generation [24]. Among them, the
mostrelatedtousareVisionMamba[59,98],S4ND[64]andMamba-ND[51].Vi-
sionMamba[59,98]usesabidirectionalSSMindiscriminativetaskswhichincurs
ahighcomputationalcost.Ourmethodappliesasimplealternativemambadiffu-
sion in generative models. S4ND [64] introduces local convolution into Mamba’s
reasoningprocess,movingbeyondtheuseofonly1Ddata.Mamba-ND[51]takes
multi-dimensionality into account in discriminative tasks, making use of various
scans within a single block. In contrast, our focus is on distributing scan com-
plexity across every layer of the network, thus maximizing the incorporation of
inductive bias from visual data with zero parameter burden.
Backbones in Diffusion Models. Diffusion models primarily employ UNet-
based [38,68] and ViT-based [8,66] backbones. While UNet is known for high
memory demands [68], ViT benefits from scalability [15,21] and multi-modal
learning[9].However,ViT’squadraticcomplexitylimitsvisualtokenprocessing,
prompting studies towards mitigating this issue [11,19,85]. Our work, inspired
by Mamba [29], explores an SSM-based model as a generic diffusion backbone,
retaining ViT’s modality-agnostic and sequential modeling advantages. Concur-
rently,DiffSSM[91]concentratesonunconditionalandclassconditioningwithin
theS4model[31].DIS[24]mainlyexploresthestate-spacemodelonarelatively
small scale, which is not the exact focus of our work. Our work significantly dif-
fersfromtheirsasitprimarilyfocusesonthebackbonedesignusingtheMamba
blockandextendsittotextconditioning.Furthermore,weapplyourmethodto
more complex visual data.
SDE and ODE in Diffusion models. The realm of Score-based Genera-
tive Models encompasses significant contributions from foundational works such
as Score Matching with Langevin Dynamics (SMLD) by Song et al. [72] and4 Hu et al.
the advent of Denoising Diffusion Probabilistic Models(DDPM) [38,71]. These
methodologies operate within the framework of Stochastic Differential Equa-
tions(SDEs),aconceptfurtherrefinedintheresearchofSongetal.[73].Recent
researchstrides,asexemplifiedbyKarrasetal.[45]andLeeetal.[50],haveshow-
cased the efficacy of employing Ordinary Differential Equation (ODE) samplers
for diffusion SDEs, offering significant reductions in sampling costs compared
to traditional approaches that entail discretizing diffusion SDEs. Furthermore,
within the domain of Flow Matching methodologies [3,4,54,57], both SMLD
and DDPMs emerge as specialized instances under distinct paths of the Prob-
ability Flow ODE framework [73]. These models typically utilize velocity field
parameterizationsemployingthelinearinterpolant,aconceptthatfindsbroader
applications in the Stochastic Interpolant framework [3], with subsequent gen-
eralizations extending to manifold settings [12]. Explored in various topics such
as image generation [40,54], editing [43], video generation [20], motion gener-
ation [42], point cloud generation [88], Reinforcement Learning [96] and text
generation [41]. The SiT model [63] scrutinizes the interplay between interpola-
tion methods in both sampling and training contexts, albeit in the context of
smallerresolutionssuchas512×512.Ourresearchendeavorstoextendthesein-
sights to a larger scale, focusing on the generalization capabilities for 2D images
of 1024×1024 and 3D video data.
3 Method
In this section, we begin by providing background information on State-Space
Models[30,31,34],withaparticularfocusonaspecialcaseknownasMamba[29].
We then highlight the critical issue of Spatial Continuity within the Mamba
framework, and based on this insight, we propose the Zigzag Mamba. This en-
hancement aims to improve the efficiency of 2D data modeling by incorporating
the continuity inductive bias inherent in 2D data. Furthermore, we design a ba-
siccross-attentionblockuponMambablocktoachievetext-conditioning.Subse-
quently, we suggest extending this approach to 3D video data by factorizing the
model into spatial and temporal dimensions, thereby facilitating the modeling
process. Finally, we introduce the theoretical aspects of stochastic interpolation
for training and sampling, which underpin our network architecture.
3.1 Background: State-Space Models
State Space Models (SSMs) [30,31,34] have been proven to handle long-range
dependenciestheoreticallyandempirically[32]withlinearscalingw.r.tsequence
length.Intheirgeneralform,alinearstatespacemodelcanbewrittenasfollows:
x′(t)=A(t)x(t)+B(t)u(t)
y(t)=C(t)x(t)+D(t)u(t),
mapping a 1-D input sequence u(t) ∈ R to a 1-D output sequence y(t) ∈ R
throughanimplicitN-Dlatentstatesequencex(t)∈Rn.Concretely,deepSSMsZigMa 5
seek to use stacks of this simple model in a neural sequence modeling architec-
ture, where the parameters A,B,C and D for each layer can be learned via
gradient descent.
Recently, Mamba [29] largely improved the flexibility of SSMs by relaxing
the time-invariance constraint on SSM parameters, while maintaining computa-
tional efficiency. By employing a work-efficient parallel scan, Mamba mitigates
the impact of the sequential nature of recurrence, whereas fusing GPU opera-
tions removes the requirement to materialize the expanded state. In this paper,
we focus on exploring the scanning scheme of Mamba in diffusion models to
maximize the use of inductive-bias from multi-dimensional visual data.
MLP & Prediction
Forward Forward
Conv1d Scan
Vision Mamba Encoder
Input Norm + +
Image
0 1 2 3 4 5 6 7 8 *9
Flatten & Linear Projection Activation
Mamba Scan
Figure2: ZigMa. Our backbone is structured in L layers, mirroring the style of
DiT [66]. We use the single-scan Mamba block as the primary reasoning module
across different patches. To ensure the network is positionally aware, we’ve designed
an arrange-rearrange scheme based on the single-scan Mamba. Different layers follow
pairsofuniquerearrangeoperationΩandreverserearrangeΩ¯,optimizingtheposition-
awareness of the method.
3.2 Diffusion Backbone: Zigzag Mamba
DiT-StyleNetwork.WeopttousetheframeworkofViTbyAdaLN[66]rather
than the skip-layer focused U-ViT structure [8], as ViT has been validated as a
scalablestructureinliterature[9,15,65].Consideringtheaforementionedpoints,
itinformsourMambanetworkdesigndepictedinFigure4.Thecorecomponent
of this design is the Zigzag Scanning, which will be explained in the following
paragraph.
Zigzag Scanning in Mamba.Previousstudies[82,91]haveusedbidirectional
scanning within the SSM framework. This approach has been expanded to in-
cludeadditionalscanningdirections[56,59,92]toaccountforthecharacteristics
of 2D image data. These approaches unfold image patches along four directions,
resultinginfourdistinctsequences.Eachofthesesequencesissubsequentlypro-
cessed together through every SSM. However, since each direction may have
different SSM parameters (A, B, C, and D), scaling up the number of direc-
tions could potentially lead to memory issues. In this work, we investigate the
Embedded
Patches6 Hu et al.
potential for amortizing the complexity of the Mamba into each layer of the
network.
Ourapproachcentersaroundtheconceptoftokenrearrangementbeforefeed-
ing them into the Forward Scan block. For a given input feature z from layer i,
i
the output feature z of the Forward Scan block after the rearrangement can
i+1
be expressed as:
z =arrange(z ,Ω ), (1)
Ωi i i
z¯ =scan(z ), (2)
Ωi Ω
z =arrange(z¯ ,Ω¯ ), (3)
i+1 Ωi i
Ω represents the 1D permutation of layer i, which rearranges the order of the
i
patchtokensbyΩ ,andΩ andΩ representthereverseoperation.Thisensures
i i i
that both z and z maintain the sample order of the original image tokens.
i i+1
(a) sweep-scan (b) zigzag-scan
(c) zigzag-scanwith8schemes
Figure3: The 2D Image Scan.Ourmambascandesignisbasedonthesweep-scan
schemeshowninsubfigure(a).Fromthis,wedevelopedazigzag-scanschemedisplayed
in subfigure (b) to enhance the continuity of the patches, thereby maximizing the
potential of the Mamba block. Since there are several possible arrangements for these
continuous scans, we have listed the eight most common zigzag-scans in subfigure (c).
NowweexplorethedesignoftheΩ operation,consideringadditionalinduc-
i
tive biases from 2D images. We propose one key properties: Spatial Continuity.
RegardingSpatialContinuity,currentinnovationsofMambainimages[56,59,98]
often squeeze 2D patch tokens directly following the computer hierarchy, such
as row-and-column-major order. However, this approach may not be optimal
for incorporating the inductive bias with neighboring tokens, as illustrated in
Figure 3. To address this, we introduce a novel scanning scheme designed to
maintain spatial continuity during the scan process. Additionally, we consider
space-filling, which entails that for a patch of size N ×N, the length of the 1D
continuous scanning scheme should be N2. This helps to efficiently incorporate
tokens to maximize the potential of long sequence modeling within the Mamba
block.
Toachievetheaforementionedproperty,weheuristicallydesigneightpossible
space-filling continuous schemes, denoted as S (where j ∈[0,7]), as illustrated
j
in Figure 3. While there may be other conceivable schemes, for simplicity, we
limit our usage to these eight. Consequently, the scheme for each layer can be
represented as Ω =S , where % denotes the modulo operator.
i {i%8}ZigMa 7
Optional
Noise +
Scale
Linear and Reshape Cross Attention
Layer Norm Scale, Shift
Layer Norm
Mamba Block +
Scale
Patchify Embed Mamba Scan
Scale, Shift
Noised
Latent Timestep t Layer Norm MLP
Label y
Input Tokens Conditioning
Figure4: The Detail of our Zigzag Mamba block. The detail of Mamba Scan
is shown in Figure 2. The condition can include a timestep and a text prompt. These
are fed into an MLP, which separately modulates the Mamba scan for long sequence
modeling and cross attention for multi-modal reasoning.
Deploying text-condition on Zigzag Mamba. While Mamba offers the ad-
vantage of efficient long sequence modeling, it does so at the expense of the
attention mechanism. As a result, there has been limited exploration into in-
corporatingtext-conditioninginMamba-baseddiffusionmodels.Toaddressthis
gap, we propose a straightforward cross-attention block with skip layers built
upon the Mamba block, as illustrated in Figure 4. This design not only en-
ables long sequence modeling but also facilitates multi-token conditioning, such
as text-conditioning. Furthermore, it has the potential to provide interpretabil-
ity [13,37,76], as cross-attention has been utilized in diffusion models.
Generalizingto3Dvideosbyfactorizingspatialandtemporalinforma-
tion. In previous sections, our focus has been on the spatial 2D Mamba, where
we designed several spatially continuous, space-filling 2D scanning schemes. In
this section, we aim to leverage this experience to aid in designing correspond-
ing mechanisms for 3D video processing. We commence our design process by
extrapolatingfromtheconventionaldirectionalMamba,asdepictedinFigure5.
Given a video feature input z ∈ RB×T×C×W×H, we propose three variants of
the Video Mamba Block for facilitating 3D video generation.
(a)Sweep-scan:Inthisapproach,wedirectlyflattenthe3Dfeaturezwithout
considering spatial or temporal continuity. It’s worth noting that the flattening
process follows the computer hierarchy order, meaning that no continuity is
preserved in the flattened representation.
(b) 3D Zigzag: Compared with we formulate the 2D zigzag in previous sub-
sections, we follow the similar design to generalize it to 3D Zigzag to keep the
continuity in2D and 3D simultaneously. Potentially, the scheme has much more
complexity.Weheuristicallylist8schemesaswell.However,weempiricallyfind
that this scheme will lead to suboptimal optimization.
(c) Factorized 3D Zigzag = 2D Zigzag + 1D Sweep: To address the sub-
optimal optimization issue, we propose to factorize the spatial and temporal
correlations as separate Mamba blocks. The order of their application can be8 Hu et al.
adjusted as desired, for example, "sstt" or "ststst", where "s" represents the
spatial-zigzag Mamba and "t" represents the temporal-zigzag Mamba.
Computation Analysis.ForavisualsequenceT∈R1×M×D,thecomputation
complexityofglobalself-attentionandk-directionmambaandourzigzagmamba
are as follows:
ζ(self-attention)=4MD2+2M2D, (4)
ζ(k-mamba)=k×[3M(2D)N+M(2D)N2], (5)
ζ(zigzag)=3M(2D)N+M(2D)N2, (6)
whereself-attentionexhibitsquadraticcomplexitywithrespecttosequencelength
M, while Mamba exhibits linear complexity (N is a fixed parameter, set to 16
by default). Here, k represents the number of scan directions in a single Mamba
block. Therefore, k-mamba and zigzag share linear complexity with respect to
self-attention. Moreover, our zigzag method can eliminate the k series, further
reducing the overall complexity.
Upon completing the design of the Zigzag Mamba network for improved
visual inductive-bias integration, we proceed to combine it with a new diffusion
framework, as illustrated below.
Bi-directional Mamba 3D Zig-zag Mamba Spatial 2D Scan Temporal 1D Scan
( a ). sweep-scan ( b ). 3D zigzag-scan ( c ). 2D zigzag-scan + 1D normal scan
Figure5: The 3D Video Scan.(a)WeillustratethebidirectionalMambawiththe
sweep scan, where the spatial and temporal information is treated as a set of tokens
withacomputer-hierarchyorder.(b)Forthe3Dzigzag-scan,weaimtomaximizethe
potential of Mamba by employing a spatial continuous scan scheme and adopting the
optimalzigzagscansolution,asdepictedinFigure3.(c)Wefurtherseparatetherea-
soningbetweenspatialandtemporalinformation,resultinginafactorizedcombination
of 2D spatial scan (Ω) plus a 1D temporal scan (Ω′) scheme.ZigMa 9
3.3 Diffusion Framework: Stochastic Interpolant
Sampling based on vector v and score s. Following [3,77], the time-
dependentprobabilitydistributionp (x)ofx alsocoincideswiththedistribution
t t
of the reverse-time SDE [6]:
1 √
dX =v(X ,t)dt+ w s(X ,t)dt+ w dW¯ , (7)
t t 2 t t t t
whereW¯ isareverse-timeWienerprocess,w >0isanarbitrarytime-dependent
t t
diffusion coefficient, s(x,t)=∇logp (x) is the score, and v(x,t) is given by the
t
conditional expectation
v(x,t)=E[x˙ |x =x],
t t
(8)
=α˙ E[x |x =x]+σ˙ E[ε|x =x],
t ∗ t t t
whereα isadecreasingfunctionoft,andσ isanincreasingfunctionoft.Here,
t t
α˙ and σ˙ denote the time derivatives of α and σ , respectively.
t t t t
As long as we can estimate the velocity v(x,t) and/or score s(x,t) fields,
we can utilize it for the sampling process either by probability flow ODE [73]
or the reverse-time SDE (7). Solving the reverse SDE (7) backwards in time
fromX =ε∼N(0,I)enablesgeneratingsamplesfromtheapproximateddata
T
distribution p (x) ∼ p(x). During sampling, we can perform direct sampling
0
from either ODE or SDEs to balance between sampling speed and fidelity. If
we choose to conduct ODE sampling, we can achieve this simply by setting the
noise term s to zero.
In [3], it shows that one of the two quantities s (x,t) and v (x,t) needs to
θ θ
be estimated in practice. This follows directly from the constraint
x=E[x |x =x],
t t
(9)
=α E[x |x =x]+σ E[ε|x =x],
t ∗ t t t
which can be used to re-express the score s(x,t) in terms of the velocity v(x,t)
as
α v(x,t)−α˙ x
s(x,t)=σ−1 t t . (10)
t α˙ σ −α σ˙
t t t t
Thus, v(x,t) and s(x,t) can be mutually conversed. We illustrate how to com-
pute them in the following.
Estimatingthescoresandthevelocityv.Ithasbeenshowninscore-based
diffusion models [73] that the score can be estimated parametrically as s (x,t)
θ
using the loss
(cid:90) T
L (θ)= E[∥σ s (x ,t)+ε∥2]dt. (11)
s t θ t
0
Similarly,thevelocityv(x,t)canbeestimatedparametricallyasv (x,t)viathe
θ
loss
(cid:90) T
L (θ)= E[∥v (x ,t)−α˙ x −σ˙ ε∥2]dt, (12)
v θ t t ∗ t
010 Hu et al.
whereθ representstheZigzagMambanetworkthatwedescribedintheprevious
section,weadoptthelinearpathfortraining,duetoitssimplicityandrelatively
straight trajectory:
α =1−t, σ =t. (13)
t t
Wenotethatanytime-dependentweightcanbeincludedundertheintegrals
in both (11) and (12). These weight factors play a crucial role in score-based
models when T becomes large [47,48]. Thus, they provide a general form that
considers both the time-dependent weight and the stochasticity.
4 Experiment
In this section, we begin by detailing the experimental setup concerning image
and video datasets, as well as our training details. Subsequently, we delve into
several in-depth analyses aimed at elucidating the rationale behind our method
design across various resolutions. Finally, we present our results obtained from
higher-resolution and more complex datasets.
4.1 Dataset and Training Detail
ImageDataset.Toexplorethescalabilityinhighresolution,weconductexper-
imentsontheFacesHQ1024×1024.Thegeneraldatasetthatweusefortraining
and ablations is FacesHQ, a compilation of CelebA-HQ [89] and FFHQ [46], as
employed in previous work such as [23,25].
Fortext-conditionedgeneration,weconducttheexperimentsontheMultiModal-
CelebA2562,5122 [89]andMSCOCO256×256[53]datasets.Bothdatasetsare
composed of text-image pairs for training. Typically, there are 5 to 10 captions
per image in COCO and MultiModal-CelebA. We convert discrete texts to a
sequence of embeddings using a CLIP text encoder [67] following Stable Dif-
fusion [68]. Then these embeddings are fed into the network as a sequence of
tokens.
Video dataset. UCF101 dataset is consists of 13,320 video clips, which are
classified into 101 categories. The total length of these video clips is over 27
hours. All these videos are collected from YouTube and have a fixed frame rate
of 25 FPS with the resolution of 320×240. We randomly sample continuous 16
frames and resize the frames to 256×256.
TrainingDetails.WeuniformlyuseAdamW[61]optimizerwith1e−4learning
rate. For extracting latent features, we employ off-the-shelf VAE encoders. To
mitigate computational costs, we adopted a mixed-precision training approach.
Additionally, we applied gradient clipping with a threshold of 2.0 and a weight
decay of 0.01 to prevent NaN occurrences during Mamba training. Most of our
experiments were conducted on 4 A100 GPUs, with scalability exploration ex-
tendedto16and32A100GPUs.Forsampling,weadopttheODEsamplingfor
speed consideration. For further details, please refer to the Appendix 9.3.ZigMa 11
Table 1: Ablation of Scanning Scheme Number. We evaluate various zigzag
scanning schemes. Starting from a simple “Sweep” baseline, we consistently observe
improvements as more schemes are implemented.
MultiModal-CelebA256 MultiModal-CelebA512
FID5k FDD5k KID5k FID5k FDD5k KID5k
Sweep 158.1 75.9 0.169 162.3 103.2 0.203
Zigzag-1 65.7 47.8 0.051 121.0 78.0 0.113
Zigzag-2 54.7 45.5 0.041 96.0 59.5 0.079
Zigzag-8 45.5 26.4 0.011 34.9 29.5 0.023
4.2 Ablation Study
Scan Scheme Ablation. We provide several important findings based on our
ablationstudiesonMultiModal-CelebAdatasetinvariousresolutionsinTable1.
Firstly, switching the scanning scheme from sweep to zigzag led to some gains.
Secondly, as we increased the zigzag scheme from 1 to 8, we saw consistent
gains. This indicates that alternating the scanning scheme in various blocks can
be beneficial. Finally, the relative gain between Zigzag-1 and Zigzag-8 is more
prominent at higher resolutions (512×512, or longer sequence token number)
compared to lower resolutions (256×256, or shorter sequence token number),
this shows the great potential and more efficient inductive-bias incorporation in
longer sequence number.
Spatial Continuity is Critical. We first explore the importance of spatial
continuityinMambadesignbygroupingpatchesofsizeN×N intovarioussizes:
2×2, 4×4, 8×8, and 16×16, resulting in groups of patch sizes N/2×N/2,
N/4×N/4, N/8×N/8, and N/16×N/16, respectively. Then, we apply our
designed Zigzag-8 scheme at the group level instead of the patch level. Figure 6
illustrates that with increased spatial continuity, notably improved performance
is achieved. Furthermore, we compare our approach with random shuffling of
N ×N patches, revealing notably inferior performance under random shuffling
conditions. All of these results collectively indicate that spatial continuity is a
critical requirement when applying Mamba in 2D sequences.
AblationstudyabouttheNetworkandFPS/GPU-Memory.InFigure7
(a,b),we analyze the forward speed and GPU memory usage while varying the
global patch dimensions from 32 × 32 to 196 × 196. For the speed analysis,
we report Frame Per Second (FPS) instead of FLOPS, as FPS provides a more
explicitandappropriateevaluationofspeed1.Forsimplicity,weuniformlyapply
the zigzag-1 Mamba scan scheme and use batch size=1 and patch size=1 on
an A100 GPU with 80GB memory. It’s worth noting that all methods share
nearly identical parameter numbers for fair comparison. We primarily compare
ourmethodwithtwopopulartransformer-basedDiffusionbackbones,U-ViT[8]
and DiT [66]. It is evident that our method achieves the best FPS and GPU
utilizationwhengraduallyincreasingthepatchingnumber.U-ViTdemonstrates
1 https://github.com/state-spaces/mamba/issues/110#issuecomment-191646401212 Hu et al.
Patch Size 32x32
Patch Size 64x64
102
101
random 2 4 8 16
Patch Group Size
Figure6: Spatial Continuity Analysis. As we incrementally enlarge the patch
group size, the continuous segment of the patch also expands. This enhances spatial
continuity, which we find improves FID on MultiModal-CelebA 256, 512 dataset.
theworstperformance,evenexceedsthememoryboundswhenthepatchnumber
is196.Surprisingly,DiT’sGPUutilizationisclosetoourmethod,whichsupports
our backbone choice of DiT from a practical perspective.
Furthermore, in Figure 7 (c) we conduct an ablation study on GPU memory
andFPSamongdifferentvariantsofourmethod.Wefindthatourmethodincurs
nearlyzeroFPSandGPUmemoryburdenwhengraduallyincreasingtheMamba
ScanSchemes.Thisanalysisprovidesinsightsintothecomparativeperformance
andefficiencyofdifferentdiffusionbackbones,highlightingtheadvantagesofour
proposed method.
Order Receptive Field.WeproposeanewconceptinMamba-basedstructure
fornon-1Ddata.Giventhatvariousspatially-continuouszigzagpathsmayexist
in non-1D data, we introduce the term Order Receptive Field which denotes the
number of zigzag paths explicitly employed in the network design.
AblationstudyabouttheOrderReceptiveFieldandFPS/GPU-Memory.
As depicted in Fig. 10, Zigzag Mamba consistently maintains its GPU memory
consumption and FPS rate, even with a gradually increasing Order Receptive
Field. In contrast, our primary baseline, Parallel Mamba, along with variants
like Bidirectional Mamba and Vision Mamba [59,98], experience a consistent
decrease in FPS due to increased parameters. Notably, Zigzag Mamba, with an
Order Receptive Field of 8, can perform faster without altering parameters.
Patch size. We conducted an ablation study on patch sizes ranging from 1,
2, 4, to 8 in Figure 7 (d), aiming to explore their behaviors under the frame-
work of Mamba. The results reveal that the FID deteriorates as the patch size
increases,aligningwiththecommonunderstandingobservedinthefieldoftrans-
formers [22,79]. This suggests that smaller patch sizes are crucial for optimal
performance.
4.3 Main Result
Main Result on 1024×1024 FacesHQ.Toelaborateonthescalabilityofour
method within the Mamba and Stochastic Interpolant framework, we provide
comparisonsonahigh-resolutiondataset(1024×1024FacesHQ)inTable3.Our
DIFZigMa 13
Out Of Memory
Out Of Memory
(a) FPSv.s.PatchNumber. (b) GPUMemoryv.s.PatchNumber.
40 4
30 3
20 2
10 1
0 0
Sweep Zigzag-1 Zigzag-2 Zigzag-8
(c) GPUusageofvariantsofourmethod. (d) FPSv.s.PatchSize.
Figure7: (a, b).GPU Memory usage and FPS between our method and transformer-
based methods(U-VIT [8] and DiT [66]). (c). GPU memory usage and FPS among
various variants of our method. (d). The relationship between patch size and FID
under various resolutions.
Zigzag-8 Mamba (Our)
40 30 Parallel Mamba
30 Zigzag-8 Mamba (Our) 20
20 Parallel Mamba
10
10
0 0
1 2 4 8 16 32 1 2 4 8 16 32
(a) OrderReceptiveFieldv.s.FPS. (b) OrderReceptiveFieldv.s.GPUMemory.
Figure8:TheablationstudyaboutOrderReceptiveField,FPS,GPUMem-
ory.
Table 2: Main Results on MS-COCO dataset. Our method consistently out-
performs the baseline and can achieve even better results when the training scale is
increased.
Variants FID5k
Sweep 195.1
Zigzag-1 73.1
BidirectionMamba[98] 60.2
Zigzag-8 41.8
Zigzag-8×16GPU 33.8
primary comparison is against Bidirectional Mamba, a commonly used solution
for applying Mamba to 2D image data [59,98]. With the aim of investigating
Mamba’s scalability in large resolutions up to 1,024, we employ the diffusion
)bG(egasU
yromeM
SPF
SPF
)G(
yromeM
UPG14 Hu et al.
Table 3: Main result on FacesHQ-1024 dataset with 4,094 tokens in latent
space. Our method can outperform the baseline and can achieve even better results
when the training scale is increased.
Method FID5k FDD5k
BidirectionMamba-16GPU[98] 51.1 66.3
Zigzag-Mamba-16GPU 37.8 50.5
Zigzag-Mamba-32GPU 26.6 31.2
Table 4: Video Scan Scheme on UCF101 dataset.Ourmethodoutperformsthe
baseline and can achieve even better results when the training scale is increased.
Method Frame-FID5k FVD5k
BidirectionMamba[98]-4GPU 256.1 320.2
3DZigzagMamba-4GPU 238.1 282.3
Factorized3DZigzagMamba-4GPU 216.1 210.2
BidirectionMamba[98]-16GPU 146.2 201.1
Factorized3DZigzagMamba-16GPU 121.2 140.1
model on the latent space of 128×128 with a patch size of 2, resulting in 4,096
tokens. The network is trained on 16 A100 GPUs. Notably, our method demon-
stratessuperiorresultscomparedtoBidirectionalMamba.Detailsregardingloss
and FID curves can be found in Appendix 9.2. While constrained by GPU re-
source limitations, preventing longer training duration, we anticipate consistent
outperformance of Bidirectional Mamba with extended training duration.
COCO dataset. To further compare the performance of our method, we also
evaluate it on the more complex and common dataset MS COCO. We compare
with the Bidirection Mamba as the baseline in Table 2. It should be noted that
all methods share nearly identical parameter numbers for fair comparison. We
trainedallmethodsusing16A100GPUs.pleasecheckAppendix9.3fordetails.
As depicted in Table 2, our Zigzag-8 method outperforms Bidirectional Mamba
as well as Zigzag-1. This suggests that amortizing various scanning schemes
can yield significant improvements, attributed to better incorporation of the
inductive bias for 2D images in Mamba.
UCF101 dataset. In Table 4, we present our results on the UCF101 dataset,
training all methods using 4 A100 GPUs, with further scalability exploration
conducted using 16 A100 GPUs. We mainly compare our method consistantly
with Bidirection Mamba [98]. For the choice of the 3D Zigzag Mamba, please
refer to Appendix 9.3. For Factorized 3D Zigzag Mamba in video processing, we
deploythesst schemeforfactorizingspatialandtemporalmodeling.Thisscheme
prioritizes spatial information complexity over temporal information, hypothe-
sizing that redundancy exists in the temporal domain. Our results consistently
demonstrate the superior performance of our method across various scenarios,
underscoring the intricacy and effectiveness of our approach.ZigMa 15
FacesHQ 1024 × 1024 MultiModal-CelebA 512 × 512
Figure9: Visualization of various resolutions on FacesHQ 1024×1024 and
MultiModal-CelebA 512×512. Our generated samples present high fidelity across
various resolutions.
Visualization. We demonstrate the image visualization of our best results on
FacesHQ 1024 and MultiModal-CelebA 512 in Figure 9. For the visualization of
videos,pleaserefertoAppendix9.1.Itisevidentthatthevisualizationisvisually
pleasing across various resolutions, indicating the efficacy of our methods.
5 Conclusion
In this paper, we present the Zigzag Mamba Diffusion Model, developed within
the Stochastic Interpolant framework. Our initial focus is on addressing the
critical issue of spatial continuity. We then devise a Zigzag Mamba block to
better utilize the inductive bias in 2D images. Further, we factorize the 3D
Mambainto2Dand1DZigzagMambatofacilitateoptimization.Weempirically
designvariousablationstudiestoexaminedifferentfactors.Thisapproachallows
for a more in-depth exploration of the Stochastic Interpolant theory. We hope
our endeavor can inspire further exploration in the Mamba network design. In
the future, we plan to examine this network design for discriminative tasks.
6 Acknowledgement
We would like to thank Timy Phan, Yunlu Chen, Di Wu, Divin Yan for the
extensive proofreading.16 Hu et al.
References
1. Agarwal, N., Suo, D., Chen, X., Hazan, E.: Spectral state space models. arXiv
(2023) 23
2. Ahamed,M.A.,Cheng,Q.:Mambatab:Asimpleyeteffectiveapproachforhandling
tabular data. arXiv (2024) 3, 23
3. Albergo,M.S.,Boffi,N.M.,Vanden-Eijnden,E.:Stochasticinterpolants:Aunifying
framework for flows and diffusions. arXiv (2023) 2, 4, 9
4. Albergo, M.S., Vanden-Eijnden, E.: Building normalizing flows with stochastic
interpolants. arXiv (2022) 2, 4
5. Ali, A., Zimerman, I., Wolf, L.: The hidden attention of mamba models. arXiv
(2024) 23
6. Anderson,B.D.:Reverse-timediffusionequationmodels.StochasticProcessesand
their Applications (1982) 9
7. Anthony, Q., Tokpanov, Y., Glorioso, P., Millidge, B.: Blackmamba: Mixture of
experts for state-space models. arXiv (2024) 23
8. Bao,F.,Li,C.,Cao,Y.,Zhu,J.:Allareworthwords:avitbackboneforscore-based
diffusion models. CVPR (2023) 1, 3, 5, 11, 13, 22
9. Bao, F., Nie, S., Xue, K., Li, C., Pu, S., Wang, Y., Yue, G., Cao, Y., Su, H., Zhu,
J.: One transformer fits all distributions in multi-modal diffusion at scale. arXiv
(2023) 1, 3, 5
10. Behrouz, A., Hashemi, F.: Graph mamba: Towards learning on graphs with state
space models. arXiv (2024) 3, 23
11. Beltagy,I.,Peters,M.E.,Cohan,A.:Longformer:Thelong-documenttransformer.
arXiv (2020) 1, 3
12. Ben-Hamu,H.,Cohen,S.,Bose,J.,Amos,B.,Grover,A.,Nickel,M.,Chen,R.T.,
Lipman, Y.: Matching normalizing flows and probability paths on manifolds. In:
ICML (2022) 4
13. Chefer, H., Gur, S., Wolf, L.: Transformer interpretability beyond attention visu-
alization. In: CVPR (2021) 7
14. Chen,R.T.,Rubanova,Y.,Bettencourt,J.,Duvenaud,D.K.:Neuralordinarydif-
ferential equations. NeurIPS (2018) 2
15. Chen,S.,Xu,M.,Ren,J.,Cong,Y.,He,S.,Xie,Y.,Sinha,A.,Luo,P.,Xiang,T.,
Perez-Rua,J.M.:Gentron:Delvingdeepintodiffusiontransformersforimageand
video generation. arXiv (2023) 3, 5
16. Child, R., Gray, S., Radford, A., Sutskever, I.: Generating long sequences with
sparse transformers. arXiv (2019) 1
17. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,
Hawkins,P.,Davis,J.,Mohiuddin,A.,Kaiser,L.,etal.:Rethinkingattentionwith
performers. arXiv (2020) 1
18. Crowson, K., Baumann, S.A., Birch, A., Abraham, T.M., Kaplan, D.Z., Shippole,
E.: Scalable high-resolution pixel-space image synthesis with hourglass diffusion
transformers. arXiv (2024) 24
19. Dao, T., Fu, D., Ermon, S., Rudra, A., Ré, C.: Flashattention: Fast and memory-
efficient exact attention with io-awareness. NeurIPS (2022) 3
20. Davtyan, A., Sameni, S., Favaro, P.: Randomized conditional flow matching for
video prediction. arXiv (2022) 4
21. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J.,
Steiner, A.P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al.: Scaling vision
transformers to 22 billion parameters. In: ICML (2023) 3ZigMa 17
22. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth
16x16 words: Transformers for image recognition at scale. In: ICLR (2021) 12, 22
23. Esser,P.,Rombach,R.,Ommer,B.:Tamingtransformersforhigh-resolutionimage
synthesis. In: CVPR (2021) 10
24. Fei, Z., Fan, M., Yu, C., Huang, J.: Scalable diffusion models with state space
backbone. arXiv (2024) 3, 23
25. Fischer, J.S., Gui, M., Ma, P., Stracke, N., Baumann, S.A., Ommer, B.: Boosting
latent diffusion with flow matching. arXiv (2023) 10
26. Fu,D.Y.,Dao,T.,Saab,K.K.,Thomas,A.W.,Rudra,A.,Ré,C.:Hungryhungry
hippos: Towards language modeling with state space models. arXiv (2022) 1
27. Gong, H., Kang, L., Wang, Y., Wan, X., Li, H.: nnmamba: 3d biomedical image
segmentation,classificationandlandmarkdetectionwithstatespacemodel.arXiv
(2024) 23
28. Gong,J.,Foo,L.G.,Fan,Z.,Ke,Q.,Rahmani,H.,Liu,J.:Diffpose:Towardmore
reliable 3d pose estimation. In: CVPR (2023) 1
29. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state
spaces. arXiv (2023) 1, 2, 3, 4, 5
30. Gu,A.,Goel,K.,Gupta,A.,Ré,C.:Ontheparameterizationandinitializationof
diagonal state space models. NeurIPS (2022) 1, 4
31. Gu,A.,Goel,K.,Ré,C.:Efficientlymodelinglongsequenceswithstructuredstate
spaces (2021) 1, 3, 4
32. Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., Ré, C.: Combining
recurrent,convolutional,andcontinuous-timemodelswithlinearstatespacelayers.
NeurIPS (2021) 1, 4
33. Guo,H.,Li,J.,Dai,T.,Ouyang,Z.,Ren,X.,Xia,S.T.:Mambair:Asimplebaseline
for image restoration with state-space model. arXiv (2024) 3, 23
34. Gupta, A.,Gu,A.,Berant,J.:Diagonal state spacesareaseffectiveasstructured
state spaces. NeurIPS (2022) 1, 4
35. He,W.,Han,K.,Tang,Y.,Wang,C.,Yang,Y.,Guo,T.,Wang,Y.:Densemamba:
Statespacemodelswithdensehiddenconnectionforefficientlargelanguagemod-
els. arXiv (2024) 23
36. He,X.,Cao,K.,Yan,K.,Li,R.,Xie,C.,Zhang,J.,Zhou,M.:Pan-mamba:Effective
pan-sharpening with state space model. arXiv (2024) 23
37. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or, D.:
Prompt-to-prompt image editing with cross attention control. arXiv (2022) 7
38. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS
(2020) 2, 3, 4
39. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video
diffusion models. In: ARXIV (2022) 1
40. Hu, V.T., Chen, Y., Caron, M., Asano, Y.M., Snoek, C.G., Ommer, B.: Guided
diffusion from self-supervised diffusion features. In: ARXIV (2023) 4
41. Hu, V.T., Wu, D., Asano, Y.M., Mettes, P., Fernando, B., Ommer, B., Snoek,
C.G.: Flow matching for conditional text generation in a few sampling steps. In:
EACL (2024), flow Matching for text generation 4
42. Hu, V.T., Yin, W., Ma, P., Chen, Y., Fernando, B., Asano, Y.M., Gavves, E.,
Mettes, P., Ommer, B., Snoek, C.G.: Motion flow matching for human motion
synthesis and editing. In: ARXIV (2023) 4
43. Hu, V.T., Zhang, D.W., Mettes, P., Tang, M., Zhao, D., Snoek, C.G.: Latent
spaceeditingintransformer-basedflowmatching.In:ICML2023Workshop,New
Frontiers in Learning, Control, and Dynamical Systems (2023) 418 Hu et al.
44. Huang, Z., Zhou, P., Yan, S., Lin, L.: Scalelong: Towards more stable training of
diffusion model via scaling network long skip connection. NeurIPS (2024) 1
45. Karras,T.,Aittala,M.,Aila,T.,Laine,S.:Elucidatingthedesignspaceofdiffusion-
based generative models. In: NeurIPS (2022) 4
46. Karras,T.,Laine,S.,Aila,T.:Astyle-basedgeneratorarchitectureforgenerative
adversarial networks. In: CVPR (2019) 10
47. Kingma, D., Salimans, T., Poole, B., Ho, J.: Variational diffusion models. In:
NeurIPS (2021) 10
48. Kingma,D.P.,Gao,R.:Understandingthediffusionobjectiveasaweightedintegral
of elbos. arXiv (2023) 10
49. Kitaev, N., Kaiser, Ł., Levskaya, A.: Reformer: The efficient transformer. arXiv
(2020) 1
50. Lee,S.,Kim,B.,Ye,J.C.:Minimizingtrajectorycurvatureofode-basedgenerative
models. ICML (2023) 4
51. Li,S.,Singh,H.,Grover,A.:Mamba-nd:Selectivestatespacemodelingformulti-
dimensional data. arXiv (2024) 3, 23, 24
52. Liang, D., Zhou, X., Wang, X., Zhu, X., Xu, W., Zou, Z., Ye, X., Bai, X.: Point-
mamba: A simple state space model for point cloud analysis. arXiv (2024) 3, 23
53. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014) 10
54. Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M., Le, M.: Flow matching for
generative modeling. ICLR (2023) 2, 4
55. Liu, G.H., Chen, T., So, O., Theodorou, E.: Deep generalized schrödinger bridge.
NeurIPS (2022) 2
56. Liu, J., Yang, H., Zhou, H.Y., Xi, Y., Yu, L., Yu, Y., Liang, Y., Shi, G., Zhang,
S., Zheng, H., et al.: Swin-umamba: Mamba-based unet with imagenet-based pre-
training. arXiv (2024) 2, 5, 6
57. Liu,X.,Gong,C.,Liu,Q.:Flowstraightandfast:Learningtogenerateandtransfer
data with rectified flow. arXiv (2022) 4
58. Liu,X.,Gong,C.,Liu,Q.:Flowstraightandfast:Learningtogenerateandtransfer
data with rectified flow. ICLR (2023) 2
59. Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba:
Visual state space model. arXiv (2024) 2, 3, 5, 6, 12, 13, 23
60. Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,S.,Guo,B.:Swintrans-
former: Hierarchical vision transformer using shifted windows. In: ICCV (2021)
1
61. Loshchilov,I.,Hutter,F.:Decoupledweightdecayregularization.In:ICLR(2019)
10
62. Ma,J.,Li,F.,Wang,B.:U-mamba:Enhancinglong-rangedependencyforbiomed-
ical image segmentation. arXiv (2024) 2, 3, 23
63. Ma, N., Goldstein, M., Albergo, M.S., Boffi, N.M., Vanden-Eijnden, E., Xie, S.:
Sit:Exploringflowanddiffusion-basedgenerativemodelswithscalableinterpolant
transformers. arXiv (2024) 2, 4
64. Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., Ré, C.:
S4nd: Modeling images and videos as multidimensional signals with state spaces.
NeurIPS (2022) 3, 23, 24
65. OpenAI: Sora: Creating video from text (2024), https://openai.com/sora 1, 5
66. Peebles, W., Xie, S.: Scalable diffusion models with transformers. arXiv (2022) 1,
3, 5, 11, 13, 22ZigMa 19
67. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021) 10
68. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022) 1, 3, 10
69. Ruan,J.,Xiang,S.:Vm-unet:Visionmambaunetformedicalimagesegmentation.
arXiv (2024) 3, 23
70. Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for
sequence modeling. arXiv (2022) 1
71. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-
vised learning using nonequilibrium thermodynamics. In: ICML (2015) 2, 4
72. Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data
distribution. arXiv (2019) 3
73. Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Ermon,S.,Poole,B.:Score-
based generative modeling through stochastic differential equations. In: ICLR
(2021) 2, 4, 9
74. Stein,G.,Cresswell,J.,Hosseinzadeh,R.,Sui,Y.,Ross,B.,Villecroze,V.,Liu,Z.,
Caterini, A.L., Taylor, E., Loaiza-Ganem, G.: Exposing flaws of generative model
evaluationmetricsandtheirunfairtreatmentofdiffusionmodels.NeurIPS(2023)
24
75. Sun,Z.,Yang,Y.,Yoo,S.:Sparseattentionwithlearningtohash.In:ICLR(2021)
1
76. Tang, R., Liu, L., Pandey, A., Jiang, Z., Yang, G., Kumar, K., Stenetorp, P., Lin,
J., Ture, F.: What the daam: Interpreting stable diffusion using cross attention.
arXiv (2022) 7
77. Tong, A., Malkin, N., Fatras, K., Atanackovic, L., Zhang, Y., Huguet, G., Wolf,
G.,Bengio,Y.:Simulation-freeschr\"odingerbridgesviascoreandflowmatching.
arXiv (2023) 9
78. Unterthiner,T.,vanSteenkiste,S.,Kurach,K.,Marinier,R.,Michalski,M.,Gelly,
S.: Fvd: A new metric for video generation. ICLR Workshop (2019) 24
79. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017) 12
80. Wang,C.,Tsepa,O.,Ma,J.,Wang,B.:Graph-mamba:Towardslong-rangegraph
sequence modeling with selective state spaces. arXiv (2024) 23
81. Wang,J.,Gangavarapu,T.,Yan,J.N.,Rush,A.M.:Mambabyte:Token-freeselec-
tive state space model. arXiv (2024) 3, 23
82. Wang, J., Yan, J.N., Gu, A., Rush, A.M.: Pretraining without attention. arXiv
(2022) 5
83. Wang,S.,Li,Q.:Stablessm:Alleviatingthecurseofmemoryinstate-spacemodels
through stable reparameterization. arXiv (2023) 3, 23
84. Wang, S., Xue, B.: State-space models with layer-wise nonlinearity are universal
approximators with exponential decaying memory. NeurIPS (2024) 3, 23
85. Wang, W., Ma, S., Xu, H., Usuyama, N., Ding, J., Poon, H., Wei, F.: When an
image is worth 1,024 x 1,024 words: A case study in computational pathology.
arXiv (2023) 3
86. Wang,Z.,Ma,C.:Semi-mamba-unet:Pixel-levelcontrastivecross-supervisedvisual
mamba-basedunetforsemi-supervisedmedicalimagesegmentation.arXiv(2024)
23
87. Wang, Z., Zheng, J.Q., Zhang, Y., Cui, G., Li, L.: Mamba-unet: Unet-like pure
visual mamba for medical image segmentation. arXiv (2024) 3, 2320 Hu et al.
88. Wu, L., Wang, D., Gong, C., Liu, X., Xiong, Y., Ranjan, R., Krishnamoorthi, R.,
Chandra, V., Liu, Q.: Fast point cloud generation with straight flows. In: CVPR
(2023) 1, 4
89. Xia, W., Yang, Y., Xue, J.H., Wu, B.: Tedigan: Text-guided diverse face image
generation and manipulation. In: CVPR (2021) 10
90. Xing, Z., Ye, T., Yang, Y., Liu, G., Zhu, L.: Segmamba: Long-range sequential
modeling mamba for 3d medical image segmentation. arXiv (2024) 3, 23
91. Yan, J.N., Gu, J., Rush, A.M.: Diffusion models without attention. arXiv (2023)
3, 5
92. Yang,Y.,Xing,Z.,Zhu,L.:Vivim:avideovisionmambaformedicalvideoobject
segmentation. arXiv (2024) 5
93. Yu, A., Nigmetov, A., Morozov, D., Mahoney, M.W., Erichson, N.B.: Robustify-
ing state-space models for long sequences via approximate diagonalization. arXiv
(2023) 1
94. Zhang,T.,Li,X.,Yuan,H.,Ji,S.,Yan,S.:Pointcouldmamba:Pointcloudlearning
via state space model. arXiv (2024) 23
95. Zhang, Z., Liu, A., Reid, I., Hartley, R., Zhuang, B., Tang, H.: Motion mamba:
Efficient and long sequence motion generation with hierarchical and bidirectional
selective ssm. arXiv (2024) 23
96. Zheng, Q., Le, M., Shaul, N., Lipman, Y., Grover, A., Chen, R.T.: Guided flows
for generative modeling and decision making. arXiv (2023) 4
97. Zheng,Z.,Wu,C.:U-shapedvisionmambaforsingleimagedehazing.arXiv(2024)
3, 23
98. Zhu,L.,Liao,B.,Zhang,Q.,Wang,X.,Liu,W.,Wang,X.:Visionmamba:Efficient
visual representation learning with bidirectional state space model. arXiv (2024)
2, 3, 6, 12, 13, 14, 23ZigMa 21
7 Limitations and Future Work
Our method relies solely on the Mamba Block with a DiT-style layout and con-
ditioning manner. However, a potential limitation of our work is that we cannot
exhaustively list all possible spatial continuous zigzag scanning schemes given a
specific global patch size. Currently, we set these scanning schemes empirically,
whichmayleadtosub-optimalperformance.Additionally,duetoGPUresource
constraints, we were unable to explore longer training durations, although we
anticipate similar conclusions.
For future work, we aim to delve into various applications of the Zigzag
Mamba, leveraging its scalability for long-sequence modeling. This exploration
may lead to improved utilization of the Mamba framework across different do-
mains and applications.
8 Impact Statement
ThisworkaimstoenhancethescalabilityandunlockthepotentialoftheMamba
algorithm within the framework of diffusion models, enabling the generation of
large images with high-fidelity. By incorporating our cross-attention mechanism
into the Mamba block, our method can also facilitate text-to-image generation.
However, like other endeavors aimed at enhancing the capabilities and control
of large-scale image synthesis models, our approach carries the risk of enabling
thegenerationofharmfulordeceptivecontent.Therefore,ethicalconsiderations
and safeguards must be implemented to mitigate these risks.
9 Appendix
Table 5: Various methods for text-to-image generation on the MultiModal-
CelebA 256 dataset.
Method FID5k FDD5k KID5k
In-Context 61.1 39.1 0.061
Cross-Attention 45.5 26.4 0.011
9.1 Visualization
FacesHQ 1024×1024 uncurated visualization in Fig. 14.
MS-COCO uncurated visualization. We visualize the samples in Fig. 13.22 Hu et al.
Table 6: Details of ZigMa Model Variants. We follow previous works [8,22,
66] model configurations for the Small (S), Base (B) and Large (L) variants; we also
introduceanXLarge(XL)configasourlargestmodel.CAdenotesthecross-attention
for text-to-image conditioning.
Model Layers N Hidden size d#params
ZigMa-S 12 384 31.3M
ZigMa-B 12 768 133.8M
ZigMa-L 24 1024 472.5M
ZigMa-XL 28 1152 1058.7M
CA-ZigMa-S 12 384 59.2M
CA-ZigMa-B 12 768 214.1M
CA-ZigMa-L 24 1024 724.4M
CA-ZigMa-XL 28 1152 1549.8M
Zigzag Mamba (Our) Zigzag Mamba (Our) 40 Parallel Mamba 15 Parallel Mamba
30
10
20
5
10
0 0
Model-S Model-B Model-L Model-H Model-S Model-B Model-L Model-H
(a) ModelComplexityv.s.FPS. (b) ModelComplexityv.s.GPUMemory.
Figure10: The ablation study about Model Complexity, FPS, GPU Mem-
ory.
9.2 New result
The variants of our ZigMa Models. We list the variants of our model
inTab.6.WeusetheBase(B)modelasthedefault.Applyingthecross-attention
model is optional, as this module can introduce some parameter burdens.
AblationstudyabouttheModelComplexityandFPS/GPU-Memory.
As shown in Fig. 10. Our method can achieve much better parameter efficiency
whenincorporatingthereceptiveorder.Thereceptiveorderreferstothecumula-
tivespatial-continuouszigzagscanpathin2Dimages,whichwe’veincorporated
into the Mamba as an inductive bias. We list the parameter consumption when
we gradually increase the receptive order in Fig. 10. The receptive order refers
tothecumulativespatial-continuouszigzagscanpathin2Dimages,whichwe’ve
incorporated into the Mamba as an inductive bias.
Loss and FID curve. The training loss curve and the FID curve are demon-
strated in Fig. 11. The loss and FID show the same trend, with our Zigzag
Mamba consistently outperforming other baselines like Sweep-1 and Sweep-2.
In-context v.s. Cross Attention We compare our cross-attention with in-
context attention in Tab. 5. For in-context attention, we concatenate the text
tokens with the image tokens and feed them into the Mamba block. Our results
demonstrate that in-context attention performs worse than our cross-attention.
SPF
)G(
yromeM
UPGZigMa 23
(a) LosstrendoftheMultiModal-CelebA256. (b) FIDtrendoftheMultiModal-CelebA256.
(c) LosstrendoftheMultiModal-CelebA512. (d) FIDtrendoftheMultiModal-CelebA512.
Figure11: The loss and FID trend under various resolutions on dataset
MultiModal-CelebA.Sweep-1andSweep-2aretheMambascanswithoutspatialconti-
nuity,whileZigzag-8representsourmethod.Thisisthedirectlogfromweight-and-bias
(wandb).
Wehypothesizethatthisisduetothediscontinuitybetweenthetexttokensand
the image patch tokens.
The choice of the 3D Zigzag Mamba. For Factorized 3D Zigzag Mamba
in video processing, we deploy the sst scheme for factorizing spatial and tem-
poral modeling. This scheme prioritizes spatial information (ss)complexity over
temporal information (t), hypothesizing that redundancy exists in the temporal
domain. There are numerous other possible combinations of s and t to explore,
which we leave for future work.
9.3 More Details
More related works Several works [83,84] have demonstrated that the State-
SpaceModelpossessesuniversalapproximationabilityundercertainconditions.
Mamba, as a new State-Space Model, has superior potential for modeling long
sequences efficiently, which has been explored in various fields such as medical
imaging [27,62,69,87,90], image restoration [33,97], graphs [10,80], NLP word
byte [81], tabular data [2], human motion synthesis [95], point clouds [52,94],
image generation [24], semi-supervised learning [86],interpretability [5], image
dehazing [97] and pan sharpening [36]. It has been extended to Mixture of
Experts [7], spectral space [1], multi-dimension [51,59,64,98] and dense con-
nection [35]. Among them, the most related to us are VisionMamba [59,98],
S4ND [64] and Mamba-ND [51]. VisionMamba [59,98] uses a bidirectional SSM24 Hu et al.
in discriminative tasks which incurs a high computational cost. Our method
applies a simple alternative mamba diffusion in generative models. S4ND [64]
introduces local convolution into Mamba’s reasoning process, moving beyond
the use of only 1D data. Mamba-ND [51] takes multi-dimensionality into ac-
count in discriminative tasks, making use of various scans within a single block.
Incontrast,ourfocusisondistributingscancomplexityacrosseverylayerofthe
network, thus maximizing the incorporation of inductive bias from visual data
with zero parameter burden.
Double-Indexing Issue for Ω . As shown in Fig. 2. We need to arrange
i
andrearrange operationthatneedstoconductindexingalongthetokennumber
dimension to achieve spatial-continuous mamba reasoning, as the indexing can
betime-consumingwhenconsideringthelargetokennumbers,Wecanformulate
the arrange and rearrange operation as follows:
Ω′ =Ω¯ ·Ω , (14)
i i−1 i
z =scan(z ), (15)
i+1 Ω′
i
(16)
where Ω¯ = I, this assumes that the Mamba-based networks are permuta-
−1
tion equivariant to the order of the tokens. They require 50% fewer indexing
operations, a point which we reiterate here for clearer comparison:
z =arrange(z ,Ω ), (17)
Ωi i i
z¯ =scan(z ), (18)
Ωi Ω
z =arrange(z¯ ,Ω¯ ), (19)
i+1 Ωi i
EvaluationMetrics.Forimage-levelfidelity,weuseestablishedmetricssuchas
Fréchet Inception Distance (FID) and Kernel Inception Distance (KID), follow-
ingpreviousworks.However,sincestudies[18,74]haveshownthatFIDdoesnot
fully reflect human-based opinions, we also adopt the Fréchet DINOv2 Distance
(FDD) using the official repository 2. Our method primarily involves sampling
5,000 real and 5,000 fake images to compute the related metrics.
We primarily consider two metrics for video fidelity evaluation: framewise
FIDandFréchetVideoDistance(FVD)[78].Wesample200videosandcompute
the respective metrics based on these samples.
The training parameters of various datasets are listed in Tab. 7. For the
COCO dataset, a weight decay of 0.01 can contribute to marginal FID gains
(approximately 0.8).
The conditioning of timestep and prompt. The conditioning process is
illustrated in Algorithm 1. The detailed structure of the block can be found
in Fig. 12.
2 https://github.com/layer6ai-labs/dgm-evalZigMa 25
Table 7: Hyperparameters and number of parameters for our network in
variousdatasets.AllmodelsaretrainedonasingleA100with40GBofVRAMusing
a bfloat16 of accelerator package.
FacesHQ1024 MS-COCO256 MultiModal-CelebA512 UCF-101
Autoencoderf 8 8 8 8
z-shape 4×128×128 4×32×32 4×64×64 16×4×32×32
Modelsize 133.8M 133.8M 133.8M 133.8M
Patchsize 2 1 1 2
Channels 768 768 768 768
Depth 12 12 12 12
Optimizer AdamW AdamW AdamW AdamW
Batchsize/GPU 8 8 4 8
GPUnum 32 32 16 16
Learningrate 1e-4 1e-4 1e-4 1e-4
weightdecay 0 0 0 0
EMArate 0.9999 0.9999 0.9999 0.9999
Warmupsteps 0 0 0 0
A100-hours 768 768 384 384
Optional
Noise +
Scale
Linear and Reshape
Cross Attention
Layer Norm Scale, Shift
Layer Norm
Mamba Block +
Scale
Patchify Embed Mamba Scan
Scale, Shift
Noised
Latent Timestep t Layer Norm MLP MLP
Label y
Input Tokens time prompt
Figure12: We separate the condition of timestep and prompt embedding
in two ways.
Algorithm 1 Mamba Block.
defmamba_block(x,t,c=None):
#x:inputdata,shape[B,(WxH),C]or[B,(TxWxH),C]
#t:timestep,(B,C)
#c:condition,(B,D,C)
x=reshape(x)#(B,K,C)
def_mamba(x):
x=rearrange(x)#rearrangebyazigzagmanner
x=mamba(x)
x=rearrange_back(x)#rearrangebackbyazigzagmanner
m,n=AdaLN(t)
x=_mamba(x*m+n)+x
ifcisnotNone:
p,q=AdaLN(c)
x=cross_attention(x*p+q)+x
returnx26 Hu et al.
Figure13: The Uncurated Visualization of MS-COCO dataset. The first row
is illustrated with pairs of images and their captions, while the remaining rows only
images.ZigMa 27
Figure14: Uncurated Visualization of FacesHQ dataset.