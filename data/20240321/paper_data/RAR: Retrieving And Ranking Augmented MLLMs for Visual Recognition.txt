RAR: Retrieving And Ranking Augmented
MLLMs for Visual Recognition
Ziyu Liu∗1,4, Zeyi Sun∗2,4, Yuhang Zang4, Wei Li6, Pan Zhang4,
Xiaoyi Dong4, Yuanjun Xiong5, Dahua Lin3,4, Jiaqi Wang†4
1Wuhan University 2Shanghai Jiao Tong University 3The Chinese
University of Hong Kong 4Shanghai AI Laboratory 5MThreads, Inc. 6
Nanyang Technological University
2020302121195@whu.edu.cn, szy2023@sjtu.edu.cn, {zangyuhang, zhangpan,
dongxiaoyi, wangjiaqi}@pjlab.org.cn
https://github.com/Liuziyu77/RAR
Abstract. CLIP (Contrastive Language–Image Pre-training) uses con-
trastive learning from noise image-text pairs to excel at recognizing a
widearrayofcandidates,yetitsfocusonbroadassociationshindersthe
precision in distinguishing subtle differences among fine-grained items.
Conversely,MultimodalLargeLanguageModels(MLLMs)excelatclas-
sifying fine-grained categories, thanks to their substantial knowledge
from pre-training on web-level corpora. However, the performance of
MLLMs declines with an increase in category numbers, primarily due
to growing complexity and constraints of limited context window size.
To synergize the strengths of both approaches and enhance the few-
shot/zero-shot recognition abilities for datasets characterized by exten-
siveandfine-grainedvocabularies,thispaperintroducesRAR,aRetrieving
And Ranking augmented method for MLLMs. We initially establish a
multi-modalretrieverbasedonCLIPtocreateandstoreexplicitmemory
fordifferentcategoriesbeyondtheimmediatecontextwindow.Duringin-
ference, RAR retrieves the top-k similar results from the memory and
uses MLLMs to rank and make the final predictions. Our proposed ap-
proachnotonlyaddressestheinherentlimitationsinfine-grainedrecog-
nition but also preserves the model’s comprehensive knowledge base,
significantly boosting accuracy across a range of vision-language recog-
nition tasks. Notably, our approach demonstrates a significant improve-
mentinperformanceon5fine-grainedvisualrecognitionbenchmarks,11
few-shotimagerecognitiondatasets,andthe2objectdetectiondatasets
under the zero-shot recognition setting.
Keywords: MLLM · Fine-Grained · Few-shot · Zero-shot Recognition
1 Introduction
TheCLIP(ContrastiveLanguage–ImagePre-training)[41]modelanditsdiverse
variants [8,26,45] provide flexible and robust performance across a wide array
of visual-language understanding tasks. Despite its successes, we observe that
4202
raM
02
]VC.sc[
1v50831.3042:viXra2 Ziyu Liu et al.
Motivation Classification+RAR
VLM, like CLIP, fails in fine-grained classification. Classification Datasets
BAE 146-300 31.03 Fine-grained Common
ATR-42 28.84
Yak-42 28.27 ··· ···
DHC-8-100 28.61
Dornier 328 28.41 Food101Flowers102Oxford pets Cars196 Caltech101 UCF101 Imagenet
MLLM, like LLaVa, fails in large vocabulary classification. Wrong prediction Accuracy significantly improved.
Accuracy on three classification datasets 100 Number of categories CLIP Azalea 75 Average Accuracy on 11 Datasets
80 f fo or r L CL La IPVa 86 80 101 70 f fo or r C RL AI RP+k-NN 69.8 6 4 20 0
0
2528 2244 29 246 000
7
47 0 Correc Ct p lr ee mdi act ti io sn 66 505
5
5763.2 63
0 RAF-DB DTD Food101 0RAF-DBDTD Food101 Memory 50 4-shot 8-shot
Detection+RAR
Large Vocabulary top1: pastry Significantly improved detection ability.
Detection Datasets 1 CLIP Inference t to op p2 3: : p di on uw gh he ne ul t 60 57.1 14
VL 3V DIS
et 1
2
2
InR fe er ra en nk
ce
t
t
t. o
o
o.. p
p
p1
2
3:
: :
d
p
po
a
insu wtg ryh hn eeu lt 34455 50505 48.7 50.7 9.8 11.3 11 246802
GT：Doughnut Memory ... 30 CLIP LS VOT ISAOurs CL VI 3P DO eu trs 0
Fig.1: Upper left: our motivation about the drawbacks of CLIP and MLLM. Our
RARcanseamlesslyintegrateintoMLLMstoimprovethefew-shot/zero-shotabilities
on classification (upper right) and detection (bottom) datasets.
CLIP’s performance begins to wane when faced with datasets characterized by
vastvocabulariesorfine-grainedcategories.AsshownintheupperleftofFig.1,
thedeclineislargelyattributabletotheinherentambiguityoflanguagedescrip-
tions and the challenges posed by synonyms, which can confound the model’s
ability to distinguish between closely related but distinct classes.
Paralleltothesedevelopments,Multi-modalLargeLanguageModels(MLLMs)
haveemergedasapowerfulclassofgenerativemodels,exemplifiedbythelikesof
GPT-4V [38] and analogous advancements [1–3,6,29,40,49,56,58,62]. MLLMs,
pre-trained on extensive corpora with substantial knowledge, demonstrate re-
markable proficiency in identifying fine-grained categories when the total num-
ber of candidates remains manageable. Nevertheless, MLLMs’ efficacy is simi-
larlycompromisedinscenariosinvolvingextensivevocabulariesandfine-grained
categorizations (upper left of Fig. 1). The core of the issue lies in MLLMs fac-
ing significant challenges in managing large context windows (e.g., maximum
2k tokens for LLaVA1.5 [28]), a critical requirement for accurate processing and
interpreting tasks that demand a nuanced understanding of vast vocabularies
and subtle distinctions.
To address these challenges, we propose augmenting standard MLLMs with
our RAR, a retrieving-and-ranking augmented technique. Our RAR enables
models to dynamically incorporate external knowledge into the processing and
generation workflows. By augmenting MLLMs with external knowledge sources,
weaddresschallengesrelatedtolanguageambiguity,synonymhandling,andthe
limitations imposed by limited context windows when dealing with vast vocab-
ularies. Our method uses the inherent strength of MLLMs in generalizing from

ycaruccARAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 3
existing knowledge while addressing their limitations in visual recognition. We
first construct a multi-modal retriever that creates and stores multimodal em-
beddingsforvisualimagesandtextdescriptions.AsshowninFig.1,uponreceiv-
ing an input image at the inference stage, our approach retrieves the top-k class
namesmostsimilartotheimage.Subsequently,theMLLMsranktheseretrieved
candidate results as the final prediction results. To bolster the MLLMs’ rank-
ing performance, we explore fine-tuning with ranking format data or in-context
learning examples without training. By integrating our retrieval-augmented de-
sign, our approach seeks to bridge the gap between the broad generalization
capabilities of MLLMs and the need for precise, fine-grained categorization, of-
feringapathforwardthatpreservesthemodel’sextensiveknowledgebasewhile
significantly boosting its performance on downstream tasks.
To evaluate our method’s efficacy, we conducted benchmarks in three ar-
eas: (1) fine-grained visual recognition across 5 benchmarks, (2) few-shot image
recognition across 11 datasets, and (3) zero-shot object recognition on 2 object
detection datasets with vast vocabularies (e.g., 13204 classes of V3Det [48]). As
presented in the right part of Fig. 1, our findings reveal that our approach no-
tably enhances few-shot learning abilities, yielding an average improvement of
6.2%over11imageclassificationdatasetsunderthe4-shotsetting.Furthermore,
our method achieves a 6.4% improvement on the LVIS dataset and a 1.5% gain
on the V3Det dataset in zero-shot object recognition performance.
In summary, our key contributions are outlined as follows: (1) We conduct
an in-depth analysis of the strengths and weaknesses of VLMs and MLLMs in
processing fine-grained datasets. (2) To enhance the fine-grained few-shot and
zero-shot perception capabilities of MLLMs, we introduce RAR with a multi-
modal retriever and the inference pipeline based on retrieving and ranking. (3)
Our RAR can be seamlessly integrated into various MLLMs in a plug-and-play
manner. (4) Through rigorous testing across 11 classification datasets and 2 ob-
ject detection datasets, we demonstrate that our method outperforms baselines
on a variety of visual recognition tasks.
2 Related Work
Contrastive Language-Image Pre-training (CLIP) [41] understands im-
ages and texts by contrastive learning from a vast amount of visual data paired
withnaturallanguagedescriptions.CLIPhasrobustcapabilitiesindownstream
tasks including image-text retrieval [55], zero-shot classification [12,60], and
open-vocabularyperception[13,57,61].FollowingCLIP,manysubsequentvision-
languagemodels[8,10,18,23,24,26,33,46,54,59]areproposedtofurtherimprove
the vision-language understanding abilities. There are also works done to im-
prove CLIP in zero-shot perception tasks [27,42,44,52,53]. However, simple
dot-product between two unimodality features can lead to sub-optimal results
for fine-grained classification. In this paper, we demonstrate that CLIP faces
challenges in making accurate zero-shot predictions for fine-grained classes, and4 Ziyu Liu et al.
how our proposed method can effectively re-rank these predictions to improve
the accuracy.
Multimodal Large Language Models(MLLMs)suchasGPT4V[38],repre-
sent a significant evolution in the landscape of Large Language Models (LLMs)
by integrating visual images as input tokens alongside textual information. The
integration is facilitated through the use of an additional vision encoder [41]
and a bridging mechanism [1–3,6,29,40,49,56,58,62]. MLLMs significantly en-
hance the interaction between humans and AI in more natural and intuitive
ways and demonstrate remarkable capabilities in understanding and generating
multi-modal content. Despite their prowess, our research uncovers a nuanced
limitation: MLLMs tend to underperform in tasks requiring vast vocabularies,
wheredistinguishingsubtledifferencesamongdifferentcategoriesiscrucial.How-
ever, we prove that MLLMs exhibit a strong ability to excel in the re-ranking of
top results obtained through vision-language models such as CLIP. Fine-R [31]
first delves into leveraging MLLMs for fine-grained perception tasks by prompt
design for better descriptions and attributes. We find a new way to prompt it
with possible candidates to help screening and achieve better performance.
Retrieval-Augmented Generation (RAG) [21] refers to the solution of in-
corporating knowledge from external databases for LLMs, which helps reduce
hallucination,continuousknowledgeupdates,andintegrationofdomain-specific
information.Specifically,RAGmodelsfirstretrievetherelevantknowledgetothe
given text query from the external knowledge base and then augment the LLMs
with the retrieved knowledge. In computer vision, some previous works explore
retrieval-augmentedapproacheswithVLMsforlong-tailedclassification[17,32],
image-textretrieval[30]orimagegeneration[55].Differentfrompreviousworks,
our paper first designs a retrieval-augmented solution for MLLMs. Our research
investigates how incorporating image-image retrieval, image-text retrieval, and
an explicit memory component can enhance the zero-shot and few-shot capabil-
ities of MLLMs.
3 Methodology
We first provide the background information on CLIP, MLLMs, and retrieval-
augmentation in LLMs (Sec. 3.1). Then we present the multi-modal retriever
(Sec. 3.2) module of RAR and how to apply RAR on downstream tasks via
retrieving and ranking (Sec. 3.3).
3.1 Preliminaries
CLIPisamodelcombininganimageencoderΦ andatextencoderΦ that
img txt
uses contrastive learning to understand and align images and text by training
on a vast dataset gathered from the web. The core mechanism of CLIP involves
mapping an input image I to its most semantically similar category c∈C:
p(y =c|x)=argmaxcos(Φ (I),Φ (c)), (1)
img txt
c∈CRAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 5
where y represents the predicted category, C refers to the whole categories list
and cos(·,·) denotes to the cosine similarity.
MultimodalLargeLanguageModelssuchasGPT4V[38]learningtogener-
ate predictions over sequences of tokens that span both image and text modali-
ties.TheMLLMmodelf,parameterizedbyweightsθ,conditionedontheinput
sequences x = (x ,...,x ) of length L , which consist of both text tokens
1 Lin in
x and visual tokens x . The x are extracted from the input image I
txt img img
via the image encoder Φ . MLLM model forecast a sequence of output tokens
img
y=(y ,...,y ) of length L as follows:
1 Lout out
p
(y|x)=(cid:89)Loutp
(y |x,y
)=(cid:89)Loutsoftmax(f(x,y
;θ)) , (2)
θ l=1 θ l ≤l−1 l=1 ≤l−1 yl
where y := (y ,...,y ) refers to the mechanism that predicts the distri-
≤l−1 1 l−1
bution of the next token considering all previously generated tokens.
Retrieval-AugmentationinLargeLanguageModelsintroducesaretrieval
moduleRwiththeLLMparameterizedbyθforgeneration.Theretrievalmodule
R is designed to process an input sequence x against an external memory of
documents M, efficiently selecting a subset of documents M ⊆ M. The subset
M is then fed along with the original input sequence x into the LLM θ, which
uses both the input and the context provided by retrieved results to generate
the target output y:
p
(y|x,M)=(cid:89)Loutp
(y |x,M,y ). (3)
θ θ l ≤l−1
l=1
3.2 Multimodal Retriever
The multimodal retriever is essentially responsible for querying a large multi-
modal external memory or database to find information relevant to the input
query or context. In the process of multimodal retriever, the main challenge lies
inefficientlyencodingandstoringalargevolumeofimages/textembeddingsfor
quick,accurateretrieval.Recognizingthemainchallenge,asshowninFig.2,we
have developed a multi-modal retriever that creates and stores multimodal em-
beddings, with a focus on optimizing retrieval speed through index construction
techniques.
Extracting the Multi-modal Embeddings. We use the CLIP model dis-
cussed in Sec. 3.1 to extract the multi-modal embeddings. Given a data sample
(x ,c )fromthedatasetD containingtheimagex andclassnamec ,weusethe
i i i i
CLIP image encoder Φ to extract the image embedding e ∈ Rd and the
img img
CLIP text encoder Φ to extract the text embedding e ∈Rd. The symbol
text text
d refers to the feature dimension (e.g., d=576 for CLIP ViT-B/16). The image
and text embeddings are stored in the memory M for retrieval (will discuss in
Sec. 3.3). In some zero-shot settings, the image embedding is not available and
we merely store the text embedding into the memory.
Fast Retrieval Optimization. The brute force search is the common method
fordesigningtheretriever,whichrequiresiterationoverallvectorsinthememory6 Ziyu Liu et al.
(a) Multimodal Retriever
Database Image Feature Index
Feature Embeddings Memory
· · ·

· · · Labels · · · Image-Image k-NN
Retrieving
Image-Text k-NN
(b) Retrieving & Ranking
Retrieved Top-K Categories Predicted
Label：Monarch
butterfly
· · ·   Ranking
· · · MLLM
Embeddings           Combining
Fig.2: Pipeline of RAR. (a) We design a multimodal retriever that extracts
the image or text embeddings and stores embeddings in an external memory M. (b)
For theinferencestage ofdownstream recognitiontasks, weretrieve top-k categories
fromthememoryanduseMLLMstorefinetheretrievedresultsasthefinalprediction
through ranking.
Mtocomputesimilarityscores(e.g.,cosinesimilarity)andsubsequentlyidentify
thetop-kresults.Althoughthebruteforcemethodisinherentlystraightforward,
its efficiency markedly diminishes as the dataset escalates to the magnitude of
millionsofembeddings.Toenhancethespeedofretrieval,weimplementanindex
systemthatusestheHNSW(HierarchicalNavigableSmallWorld)algorithm[35].
The adoption of the HNSW methodology facilitates a significant dimensionality
reduction, thereby enabling the construction of a more condensed index. Specif-
ically, vectors in a Rd space of dimension d are transformed into a reduced d
9
dimensional space. This reduction in dimensionality plays a pivotal role in en-
hancing the speed of the retrieval process.
Pre-processing for Detection Datasets. In object detection datasets, our
methodology for extracting image embeddings e is slightly different from the
img
approach discussed previously. As presented in Fig. 3, we apply two additional
pre-processingsteps:croppingandblurring.Somepreviousworkshaveproposed
similar methods in CLIP like [33,54]. In the object detection dataset, an image
typically contains multiple objects of varying sizes. Some objects may dominate
alargeportionoftheimage,whereasothersoccupyminimalspace.Accordingly,
ourobjectdetectionprocedurebeginswithcroppingtheimageregionsbasedon
proposal bounding box coordinates, subsequently resizing the cropped region
to a fixed proportion. Moreover, unlike image classification tasks the objects
of interest generally appear large and centrally positioned, the objects within
objectdetectiondatasetsaresmallerandtheirpositionsmorevaried.Tohelpthe
MLLMsunderstandtheobjectstobedetected,weemployablurringtechnique
onthenon-targetareassurroundingtheobjectsofinterest.Theblurringstrategy
Encoder Image
Encoder Image
· · ·
IndexRAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 7
(a) Pre-process (b) Embedding & Retrieve
bbox1 k-NN Memory 𝑴
Index
Bbox1: carnation, bouquet,flower_arrangement, ...
bbox2 Bbox2: pepper_mill,saltshaker, chopping_board, ...
k-NN Bbox3: flowerpot,vase, glass_(drink_container), ...
①Bbox1: flower_arrangement bbox3
②Bbox2: saltshaker
k-NN ③Bbox3: vase
Fig.3:Extendingourmultimodalretrievertozero-shotrecognitiononobjectdetec-
tiondatasetssuchasLVIS[14]andV3Det[48].Comparedtotheclassificationdatasets,
we apply the additional pre-processing techniques such as cropping and resizing to
extract the image embeddings.
is designed to direct the MLLMs’ focus toward the relevant objects, thereby
facilitating their identification in object detection tasks.
3.3 Inference with Retrieving And Ranking
After successfully constructing memory M by using our multimodal retriever,
our next step is to integrate the memory with the retrieval process and use
MLLMstoranktheretrievalresultsandenhancetheperformanceinfew-shot/zero-
shot perception tasks.
For example, in the inference stage of the few-shot image classification task,
we first use the visual encoder Φ to process the input image and obtain the
img
correspondingimageembeddingeˆ.Thevisualencoderisidenticaltotheencoder
used in our multi-modal retriever. The image embedding eˆ is then navigated
through the previously constructed memory index and ranked by similarity to
identifythetop-k relatedimages.Consequently,memoryMyieldsthenamesof
the retrieved top-k categories, denoted as {c ,c ,c ,...,c }. The top-k retrieved
1 2 3 k
results serve as a preliminary filter, narrowing down the vast possibilities to
those most likely relevant, based on historical data and the semantic closeness
of stored labels to the image content.
Since these cropped sub-images are usually small, CLIP’s ability to extract
featuresfromtheselow-resolutionimagesislimited.Therefore,intheobjectde-
tectiontask,wedonotperformimage-to-imageretrievalbutuseCLIP’sinherent
image-textinteractioncapabilitiestoconductimage-to-textretrieval.Finally,we
also obtain the top-k category information with the highest similarity.
Following the retrieval phase, the retrieved category labels alongside image
embeddingeˆareintegratedandsenttotheMLLMsthroughourrankingprompt.
The MLLMs, combining the internal knowledge and the retrieved information,
makethefinalpredictionoftheimagecategory.Ourproposedinferenceprocess,
using both the retrieval results from our memory bank and subsequent ranking
by the MLLM, ensures a more accurate and contextually aware classification
prediction. Our design represents a significant advancement in few-shot image
Encoder
Encoder
Encoder
Image
Image
Image
· · ·
· · ·
· · ·8 Ziyu Liu et al.
Ranking Prompt Example
Retrieve
Mercedes-Benz Mercedes-Benz Mercedes-Benz Mercedes-Benz 2010 BMW Mercedes-Benz
E-Class Sedan S-Class Sedan C-Class Sedan E-Class Sedan M5 Sedan SL-Class Coupe
Sorted these categories:[ Mercedes-Benz S-Class Sedan, Mercedes-Benz C-Class Sedan, Mercedes-Benz E-Class
Sedan, 2010 BMW M5 Sedan, Mercedes-Benz SL-Class Coupe].
Top-k for high to low: [ Mercedes-Benz E-Class Sedan, Mercedes-Benz S-Class Sedan,
Mercedes-Benz C-Class Sedan, Mercedes-Benz SL-Class Coupe, 2010 BMW M5 Sedan]
Fig.4: Ranking Prompt examples for few-shot image classification. The fine-
grained image examples are from Stanford Cars [20]. We incorporate the initial top-k
retrieved results (e.g., k = 5) into our ranking prompts and use the MLLMs to rank
the retrieved results and make the final prediction.
classification, enabling our system to handle a wide variety of images and cate-
gories with high precision and flexibility.
Ranking Prompt Format. Fig. 4 presents our ranking prompt format. The
processbeginswiththeprompt‘Sort the optional categories: [class a,
class b, class c, class d, class e]’, which is dynamically generated to
include the top-k class names retrieved from our multimodal retriever. Our
method uses the MLLM’s ability to rank these retrieved class names. Unlike
traditional approaches that might rely solely on the initial retrieval order, our
MLLM employs advanced linguistic and semantic analysis to assess the contex-
tual appropriateness of each class name with the input image.
Fine-tuning for Ranking. When directly applying MLLMs to ranking the
retrieved results, MLLMs may predict some errors such as beyond the given list
oroccasionalmisalignment.TofullyexploittherankingpotentialofMLLMsfor
downstream tasks, while avoiding the consumption of extensive computational
resources for training MLLMs, we selected a small-scale classification dataset to
fine-tune the MLLMs. The primary goal of fine-tuning was to enable MLLMs
to improve their ranking ability such as following the format of prompts and
returning results as required.
To create our fine-tuning data, we use the CLIP image encoder Φ to
img
extracttheembeddingsoftwodisjointsubsetsofimagesD andD ,bothdrawn
a b
from the FGVC-Aircraft dataset. We provide the ablation studies in Sec. 4.5
aboutusingdifferentdatasetstoconstructthefine-tuningdata.Ourobservation
reveals that the MLLM demonstrates robustness to the choice of fine-tuning
datasets, with only marginal differences in performance outcomes.
ForeachimageinD ,weapplythek-NNclusteringalgorithmtofindthetop
b
20 most similar images in D including their categories. Afterward, we select 16
a
setsfromthese20images,eachsetcomprisingk images,andretainthosegroups
that contain images of the same category as D . We then shuffled the category
b
labels for these sets. Using the prompts shown in Fig. 4, we create a dataset
comprising roughly 30,000 entries, with the original sequence of categories serv-
ingastheground-truthlabel.Insummary,webuildthefine-tuningdataaiming
to bolster the MLLM’s ranking performance.RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 9
In-Context Learning for Ranking. In-context learning presents a valuable
alternativetofine-tuningwithrankingexamples,particularlyduetoitsflexibility
and lower requirement for specialized data preparation. While fine-tuning with
ranking examples has proven to be highly effective, it necessitates a substantial
amountofcurateddataandcomputationalresourcesfortraining.Incontrast,in-
contextlearningusesthemodel’sexistingknowledgebyprovidingitwithspecific
examplesdirectlywithintheinputprompt,guidingthemodeltounderstandand
execute the task of ranking without the need for explicit re-training. Here we
elaborate on the application of in-context learning with MLLMs to rank the
retrievedresults.ToeffectivelyguidetheMLLMsincomprehendingtheranking
task,weusethepromptformatsimilartoFig.4andintegrateaspecificranking
example into the prompts. Please refer to the Appendix B for our structured
in-context learning prompt. Please refer to Sec. 4.5 for the ablation studies of
discussing the difference between using fine-tuning or in-context learning for
ranking.
4 Experiments
In this section, we present our experiment step (Sec. 4.1) and conduct experi-
ments on different tasks such as fine-grained visual recognition (Sec. 4.2), few-
shotimagerecognition(Sec.4.3)andzero-shotobjectrecognition(Sec.4.4).We
also provide the ablation studies about our design choices (Sec. 4.5).
4.1 Experimental Setup
Datasets and Evaluation Metrics. We follow previous work [31] to choose
5 datasets for fine-grained visual recognition (Bird-200 [47], Cars-196 [20],
Dog-120[19],Flower-102[37],andPet-37[39])andreporttheclusteringaccuracy
(cACC) and semantic similarity accuracy (sACC) as evaluation metrics.
For few-shot image recognition, we select 11 datasets including gen-
eral objects (ImageNet [7], Caltech101 [11]), textual (DTD [4]), scene objects
(SUN397[51]),satelliteimages(EuroSAT[15]),facialexpressions(RAF-DB[25]),
car types (Stanford Cars [20]) and fine-grained datasets (FGVC-Aircraft [34],
Oxford Flowers [37], Food101 [37] and Oxford Pets [39]). We report the top-1
accuracy (%) for all these classification datasets.
Additionally,wealsoselecttwobenchmarksforourzero-shotobjectrecog-
nition setting:(1) TheLVIS[14] datasetthatencompassesover 164,000images
and 1,203 categories. We report the AP , AP , AP, and AP metrics for rare,
r c f all
common, frequent, and all categories. (2) V3Det [48] dataset encompasses an
immensenumberof13204categoriesofreal-worldimages.ForV3Det,wereport
the standard mAP metric of the object detection task.
Implementation Details. We employ a frozen CLIP ViT B/16 model as the
visual encoder Φ to encode the input images and extract the corresponding
img
imageembeddings.Fortheretrievalprocess,wesearchthestoredembeddingsin
memory M using the HNSW algorithm [35]. We use k =5 for the top-k results,10 Ziyu Liu et al.
Table1:Fine-grainedvisualrecognitionacross5datasets.Wefollow[31]toreportthe
averaged clustering accuracy (cACC, %) and semantic similarity accuracy (sACC, %)
resultsover10runs.Thebestandsecond-bestresultsarecolored Green and Red,
respectively.
Bird-200 Car-196 Dog-120 Flower-102 Pet-37 Average
cACC sACC cACC sACC cACC sACC cACC sACC cACC sACC cACC sACC
WordNet+CLIP 39.3 57.7 18.3 33.3 53.9 70.6 42.1 49.8 55.4 61.9 41.8 54.7
BLIP-2 30.9 56.8 43.1 57.9 39.0 58.6 61.9 59.1 61.3 60.5 47.2 58.6
CaSED 25.6 50.1 26.9 41.4 38.0 55.9 67.2 52.3 60.9 63.6 43.7 52.6
FineR 51.1 69.5 49.2 63.5 48.1 64.9 63.8 51.3 72.9 72.4 57.0 64.3
RAR(Ours) 51.6 69.5 53.2 63.6 50.0 65.2 63.7 53.2 74.1 74.8 58.5 65.3
withasoloexceptionk =4inthe4-shotfew-shotsetting.Toimprovetheranking
ability of MLLMs, we prepare 30k fine-tuning data from the FGVC-Aircraft
dataset. In the fine-tuning process, we train the model with one epoch with
a learning rate of 1e−5 on our fine-tuning data and subsequently evaluate the
performance across additional datasets. We present the ablation studies about
the hyper-parameters such as the value of k and the fine-tuning data source in
the Sec. 4.5.
4.2 Fine-Grained Visual Recognition
We first evaluate our RAR on the fine-grained visual recognition setting defined
inpreviouswork[31].Weuseonly3unlabelledimagespercategorytobuildour
memory M for retrieving. Please refer to Appendix C for more implementation
details.
Baselines.Wefollow[31]toselectfourrepresentativemethodsasourbaselines
to compare with: WordNet [36]+CLIP, BLIP-2 [22], CaSED [5], and FineR [31].
Averaged Results over 5 Datasets. Tab. 1 summarizes the results and our
RARachievesthetopperformanceonboththecACC(58.5%)andsACC(65.3%)
metrics.TheWordNet+CLIPandCaSEDbaselinesrelysolelyonCLIPforclass
name retrieval, yet often yield inaccurate predictions. In contrast, our method
addstheadditionalrankingprocesswithMLLMs,whichincreasesthelikelihood
of correctly predicting those accurate yet initially lower-ranked candidates and
therebyboostingtheperformance.Besides,FineRusesMLLM(e.g.,BLIP-2)for
fine-grained recognition via multi-round questioning-answering processes, which
maydemandmorecomputationalresourcesandstruggletoscaleefficientlywith
large vocabulary datasets. Conversely, our approach first retrieves candidates
and then lets MLLMs make predictions on the candidates, optimizing both ac-
curacy and efficiency.
4.3 Few-Shot Image Recognition
The few-shot setting aims to enable a model to recognize new objects with only
a few examples for each new category. Few-shot learning faces substantial chal-RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 11
Table2:Few-shotimageclassificationacross11datasets.Wereportthetop-1accuracy
(%)underthe4-shotand8-shotsettings.HereourRARusestheLLaVA1.5[28]asthe
MLLMtoranktheretrievedresults.Thesymbol‘-’denotestotheLLaVAmodelfails
to make the predictions due to the limited window size.
Method Common Fine-Grained
4-shot
CLIP+KNN 42.1 87.9 14.2 51.4 67.6 47.5 64.6 84.5 49.2 62.6 55.6 57.0
LLaVA1.5Finetuning - 88.4 24.9 - 48.2 46.6 58.9 13.2 - 66.4 28.9 -
RAR(LLaVA1.5) 51.0 92.1 27.7 58.8 74.8 53.9 69.6 80.4 54.4 71.4 60.9 63.2
∆ +9.9 +4.2 +13.5 +7.4 +7.2 +6.4 +5.0 -4.1 +5.2 +8.8 +5.3 +6.2
8-shot
CLIP+KNN 47.6 90.6 28.2 56.8 72.8 53.2 68.3 89.5 56.1 68.3 61.8 63.0
LLaVA1.5Finetuning - 92.1 24.9 - 48.2 54.7 66.5 30.1 - 72.5 46.1 -
RAR(LLaVA1.5) 56.5 93.5 46.9 63.4 81.5 59.3 74.3 87.3 61.2 76.6 67.7 69.8
∆ +8.9 +2.9 +18.7 +6.6 +8.7 +6.1 +6.0 -2.2 +5.1 +8.3 +5.9 +6.8
lenges when applied to fine-grained datasets, which consist of numerous highly
similar classes yet are accompanied by only a minimal amount of training data.
Baselines.Forfew-shot image recognition,weintroducetwobaselinesincluding
CLIP and MLLMs. The first is the CLIP [41] model combined with k-NN to
retrievepredictionsbasedonfew-shotexamples.ThesecondistheLLaVAmodel
directly fine-tuning with LoRA [16] on few-shot examples.
Averaged Results on 11 Datasets. Tab. 2 summarizes the few-shot results
on 11 datasets, including 4 fine-grained datasets. Compared to the CLIP initial
retrievalresults(toprow),ourRAR(thirdrow)withrankingfacilitatesanotable
increase in classification accuracy. On average, our approach boosts the top-1
accuracyfrom57.0to63.2(%)onthe4-shotsetting,andfrom63.0to69.8(%)on
the 8-shot setting. Such improvements illustrate the ranking process of MLLMs
effectively uses a nuanced understanding of context and detail to better align
predictions with ground truth. Additionally, we observe that LLaVA1.5 + fine-
tuning (second row) baseline underperforms in datasets with large vocabularies
suchasImageNetduetotheconstraintofLLMs’contextwindow.Thankstothe
retrieved candidates, our RAR works for datasets with a vast of categories and
is a potent tool in refining classification decisions, proving particularly useful in
handling the diverse and challenging landscape of image classification tasks.
4.4 Zero-Shot Object Recognition
Given the pre-existing object proposals such as ground-truth box annotations,
thezero-shotobjectrecognitiontaskmeasuresthemodel’scapabilityofaligning
regions with textual class descriptions.
Baselines.WeselecttworepresentativepapersCLIP[41]andRegionCLIP[59]
and report their performances as the baseline results. Besides, we apply our
teNegamI 101hcetlaC BD-FAR 793NUS TASoruE
DTD
101-FCU 201rewolF
sraCdrofnatS
101dooF stePdrofxO egarevA12 Ziyu Liu et al.
Table 3: Zero-shot object recognition on Table 4: Zero-shotobjectrecognitionon
LVIS [14] v1.0 validation set. V3Det[48]validation setwith13,204cat-
egories.
AP AP AP AP
r c f all
CLIPw/box 40.6 53.1 59.2 48.7 AP AP AP AP
s m l all
CLIPw/mask 40.8 53.5 59.6 49.2 CLIPw/box 7.2 12.9 12.8 9.8
RegionCLIP 50.1 50.1 51.7 50.7
RAR(LLaVA1.5) 58.7 57.9 54.4 56.2 RAR(LLaVA1.5) 9.9 13.2 13.9 11.1
∆ +8.6 +7.8 +2.7 +5.5 ∆ +2.7 +0.3 +1.1 +1.3
RAR(Qwen-VL) 9.6 12.7 13.7 10.8
RAR(Qwen-VL) 59.6 57.5 53.7 56.4
∆ +2.4 -0.2 +0.9 +1.0
∆ +9.5 +7.4 +2.0 +5.7
RAR(InternLM-XC2) 60.2 58.0 54.3 57.1 RAR(InternLM-XC2) 10.1 13.1 14.5 11.3
∆ +2.9 +0.2 +1.7 +1.5
∆ +10.1+7.9 +2.6 +6.4
methodonarangeofcutting-edgeopen-sourceMLLMs,includingLLaVA1.5[28],
QWen-VL [2] and InternLM-XC2 [9].
Main Results on LVIS. Tab. 3 presents the results that reveal notable im-
provements in all the metrics when applying our RAR. Specifically, when comb-
ing with the recent InternLM-XC2 [9] model, our approach yielded an 8.4 (%)
pointincreaseovertheCLIPbaselineanda6.4(%)enhancementrelativetoRe-
gionCLIP [59]. These advancements underscore the efficacy of using an external
memory for retrieval assistance coupled with the ranking prowess of MLLMs.
Comparison with Rare Classes Results (AP ). We find an interesting ob-
r
servationfromtheexperimentalresultspresentedinTab.3.FortheCLIPmodel,
weobserveaprogressiveincreaseinperformancefromAP throughAP toAP,
r c f
which indicates a gradation in precision across varying class frequencies. How-
ever,employingourmethodyieldsadifferenttrend,wherethe peak performance
is achieved on AP , surpassing the CLIP model by as much as 19.6 percentage
r
points. This significant leap in performance suggests a substantial advantage
of our method when it comes to rare categories. The integration of our RAR
to MLLMs plays a pivotal role here, as it demonstrates a heightened ability
to discriminate among the rare classes. Our observation could be attributed to
the fact that our retrieving and reranking mechanism effectively pools relevant
informationfromtheexternalmemory,providingtheMLLMswitharichercon-
text for rare class identification. Moreover, the ranking capability of MLLMs
ensures that even the lesser-represented classes receive adequate attention dur-
ing the classification process. Our RAR achieves a robust enhancement in the
model’sabilitytodiscernandaccuratelyclassifyobjectsthatareinfrequentlyen-
countered,addressingoneofthesignificantchallengesinlong-taileddistribution
datasets.
Main Results on V3Det.TofurthertesttheeffectivenessofusingMLLMsfor
ranking in scenarios with an extremely large number of fine-grained categories,
weconductedadditionalexperimentsonV3Det[49].Theexperimentalresultsin
Tab.4revealthatourRARhasachievedacommendableimprovementinperfor-
mance,surpassingtheCLIPbaselineby1.5percentagepointsinoverallaverage
precision (AP ) with InternLM-XC2. Such an improvement is particularly sig-
all
nificantgiventhecomplexityoftheV3Detdataset,whichpresentsachallenging
array of 13,204 distinct classes. The MLLMs, with the aid of our retrieving andRAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 13
Objects Retrieved Reranked
1
1 ['pin_(non_jewelry)', 'pennant', 'mail_slot', 'earring', 'scrubbing_brush'] earring
2 2 ['slipper_(footwear)', 'flipper_(footwear)', 'glove', 'ski_boot', 'sock'] glove
3 ['sportswear', 'tennis_racket', 'racket', 'polo_shirt', 'tank_top_(clothing)'] polo_shirt
4
3 4 ['tennis_racket', 'short_pants', 'sportswear', 'tennis_ball', 'knee_pad'] short_pants
Fig.5: Visualization of the ranking examplesforzero-shotobjectrecognitionon
LVIS[14]validation set.Giventhetopretrievedpredictions,ourRARusesMLLMsto
select the correct class names accurately.
rankingmechanisms,haveonceagaindemonstratedtheirrobustperformancein
thedomainofobjectdetectiondatasets.Usingourretrieval-augmentedapproach
allows MLLMs to navigate the extensive and fine-grained category landscape of
V3Det effectively.
Qualitative Results. Fig. 5 presents the visualization results about ranking
examples of our approach on LVIS validation set. The CLIP&K-NN approach
provides an extensive list of object predictions, albeit with the caveat that the
most accurate label might not always emerge as the top-1 choice. The incor-
poration of MLLMs in our RAR significantly streamlines the prediction pro-
cess, yielding more precise and relevant object labels. The visualization results
demonstratethatourRARmeetstheneedforfine-grainedandlargevocabulary
recognition.
4.5 Ablation Experiments
Effects of the parameter k.Wedelveintotheimpactofthehyper-parameter
k on few-shot image recognition setting, as detailed in Tab. 5. We report the
results of RAR with the LLaVA1.5 as the MLLM. Our findings reveal that
our RAR demonstrates a remarkable robustness to variations in k, with only
minordifferencesobservedacrossabroadspectrumofvaluesfrom3to7.Sucha
consistencysuggeststhatRAR’sabilitytogeneralizefromafewexamplesisnot
significantly influenced by the choice of k. Consequently, based on the averaged
results, we select k =5 as the default choice.
Different Fine-tuning data. We study the importance of using different fine-
tuningdatasetsforranking.Weselecttworepresentativedatasets:FGVC-Aircraft
andStanford-Carsasthedatasourcesforconstructingthefine-tuningdata.Our
selection is motivated by their diverse characteristics and relevance in visual
recognitiontasks,providingacomprehensivebasisforfine-tuning.Subsequently,
wefine-tunetheRARwithdifferentMLLMs(QWen-VLandInternLM-XC2)on
thesetwodatasets,aimingtoinvestigatehowdifferentdatasourcesinfluenceper-
formance.Tothoroughlyassesstheimpactofusingdifferentfine-tuningdatasets,
we evaluate the fine-tuned RAR across a diverse set of 10 additional datasets.
Tab. 6 presents the results. We observe that RAR is not sensitive to changes
in the fine-tuning dataset for ranking, thereby confirming its viability as a gen-14 Ziyu Liu et al.
k=3 k=4 k=5 k=6 k=7
DTD 70.27 71.34 71.93 71.93 71.99 Table 5: Ablation studies
Flowers102 96.18 95.57 95.62 95.66 95.57 about the selection of the hyper-
Oxford-pets 80.21 80.38 79.91 79.72 79.42 parameter k.
Eurosat 92.38 92.48 93.28 92.52 92.59
Average 84.76 84.96 85.19 84.96 84.90
Table 6: Ablation studiesabout(1)usingdifferentdatasetsforfine-tuningand(2)
fine-tuning vs. in-context learning. The symbols ‘F’ and ‘S’ stand for fine-tuning on
the FGVC-Aircraft or Stanford-Cars datasets.
Method Strategy Common Fine-Grained
Fine-tune In-Context
RAR F ✗ 75.8 95.5 66.0 72.7 90.7 72.5 81.4 97.5 88.1 87.2 82.7
S ✗ 75.3 94.9 65.1 73.1 88.1 71.0 81.1 95.8 88.3 87.0 82.0
(QWen-VL) ✗ ✓ 72.0 93.4 63.6 65.6 86.2 66.8 76.5 95.6 84.7 82.3 78.7
F ✗ 71.5 94.4 72.7 69.7 91.7 69.9 77.6 93.2 83.9 79.3 80.4
RAR S ✗ 71.5 94.7 71.2 69.7 90.3 69.9 77.5 92.0 83.6 79.7 80.0
(InternLM-XC2) ✗ ✓ 69.2 94.1 66.0 69.7 91.8 68.9 66.1 95.7 85.7 79.2 78.6
eralizable and reliable method for enhancing the performance of MLLMs. The
consistency in results, irrespective of the fine-tuning data source, underlines the
robustness of our fine-tuning strategy. Despite these minor variations, the over-
all performance of using FGVC-Aircrafts (82.7%, top row) is higher than using
StanfordCars(82.0%,secondrow)forQWen-VL,andweobservethesametrend
for InternLM-XC2. Based on our findings, we adopt the FGVC-Aircraft dataset
as our preferred choice for fine-tuning.
Fine-tuning vs. In-Context Learning. We validate the effectiveness of fine-
tuning the MLLM or just in-context learning (training-free) for ranking. The
results are illustrated in Tab. 6. We select two distinct groups for comparison.
The first group (top and fourth rows) involves models that are fine-tuned using
the FGVC-Aircraft dataset, while the second group (third and bottom rows)
consistsofmodelswithin-contextlearningpromptsforranking.Theresultsshow
a consistent improvement in accuracy for the fine-tuned model across almost all
datasets for both QWen-VL and InternLM-XC2. The notable enhancement in
performance across a diverse range of datasets highlights the efficacy of our
fine-tuning strategy. The results substantiate that fine-tuning the MLLM with
target datasets like FGVC-Aircraft significantly bolsters the model’s ranking
capabilities.
5 Conclusion
In this paper, we highlight the potential of combining retrieving and ranking
withmulti-modallargelanguagemodelstorevolutionizeperceptiontaskssuchas
fine-grainedrecognition,zero-shotimagerecognition,andfew-shotobjectrecog-
nition. Motivated by the limited zero-shot/few-shot of CLIP and MLLMs on
teNegamI 101hcetlaC BD-FAR 793NUS TASoruE
DTD
101FCU 201rewolF 101dooF stePdrofxO egarevARAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 15
fine-grained datasets, our RAR designs the pipeline that uses MLLM to rank
the retrieved results. Our proposed approach can be seamlessly integrated into
variousMLLMsforreal-worldapplicationswherethevarietyandvolumeofcat-
egories continuously expand. Our method opens up new avenues for research in
augmenting the MLLM’s abilities with the retrieving-augmented solution and
could be beneficial for other tasks such as reasoning and generation in future
works.
References
1. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K.,
Bitton,Y.,Gadre,S.,Sagawa,S.,Jitsev,J.,Kornblith,S.,Koh,P.W.,Ilharco,G.,
Wortsman,M.,Schmidt,L.:Openflamingo:Anopen-sourceframeworkfortraining
large autoregressive vision-language models. arXiv.org (2023) 2, 4
2. Bai,J.,Bai,S.,Yang,S.,Wang,S.,Tan,S.,Wang,P.,Lin,J.,Zhou,C.,Zhou,J.:
Qwen-VL:Afrontierlargevision-languagemodelwithversatileabilities.arXiv.org
(2023) 2, 4, 12
3. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.:
Sharegpt4v: Improving large multi-modal models with better captions. arXiv
preprint arXiv:2311.12793 (2023) 2, 4
4. Cimpoi,M.,Maji,S.,Kokkinos,I.,Mohamed,S.,Vedaldi,A.:Describingtextures
in the wild. In: CVPR (2014) 9, 19
5. Conti, A., Fini, E., Mancini, M., Rota, P., Wang, Y., Ricci, E.: Vocabulary-free
image classification. In: NeurIPS (2024) 10
6. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi,
S.:Instructblip:Towardsgeneral-purposevision-languagemodelswithinstruction
tuning (2023) 2, 4
7. Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:ImageNet:Alarge-scale
hierarchical image database. In: CVPR (2009) 9, 19
8. Dong,X.,Bao,J.,Zheng,Y.,Zhang,T.,Chen,D.,Yang,H.,Zeng,M.,Zhang,W.,
Yuan, L., Chen, D., Wen, F., Yu, N.: Maskclip: Masked self-distillation advances
contrastive language-image pretraining. In: Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR). pp. 10995–11005
(June 2023) 1, 3
9. Dong,X.,Zhang,P.,Zang,Y.,Cao,Y.,Wang,B.,Ouyang,L.,Wei,X.,Zhang,S.,
Duan,H.,Cao,M.,etal.:InternLM-XComposer2:Masteringfree-formtext-image
composition and comprehension in vision-language large model. arXiv preprint
arXiv:2401.16420 (2024) 12
10. Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X.,
Cao, Y.: EVA: Exploring the limits of masked visual representation learning at
scale. In: CVPR (2023) 3
11. Fei-Fei, L., Fergus, R., Perona, P.: Learning generative visual models from few
training examples: An incremental bayesian approach tested on 101 object cate-
gories. In: CVPR workshop (2004) 9, 19
12. Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., Li, H., Qiao, Y.: Clip-
Adapter: Better vision-language models with feature adapters. IJCV (2023) 3
13. Gu,X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision
and language knowledge distillation. In: ICLR (2022) 316 Ziyu Liu et al.
14. Gupta, A., Dollar, P., Girshick, R.: LVIS: A dataset for large vocabulary instance
segmentation. In: CVPR (2019) 7, 9, 12, 13, 26, 27, 28
15. Helber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: A novel dataset and deep
learning benchmark for land use and land cover classification. IEEE J. Sel. Top.
Appl. Earth Obs. Remote Sens. (2019) 9, 19
16. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
Chen, W.: LoRA: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 (2021) 11
17. Iscen, A., Fathi, A., Schmid, C.: Improving image recognition by retrieving from
web-scale image-text data. In: CVPR (2023) 4
18. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H.,
Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning
with noisy text supervision. In: ICML (2021) 3
19. Khosla, A., Jayadevaprakash, N., Yao, B., Li, F.F.: Novel dataset for fine-grained
image categorization: Stanford dogs. In: CVPR workshop (2011) 9, 19
20. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for fine-
grained categorization. In: ICCV workshops (2013) 8, 9, 19
21. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler,
H., Lewis, M., Yih, W.t., Rocktäschel, T., et al.: Retrieval-Augmented generation
for knowledge-intensive nlp tasks. NeurIPS (2020) 4
22. Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. In: ICML (2023)
10
23. Li,J.,Li,D.,Xiong,C.,Hoi,S.:BLIP:Bootstrappinglanguage-imagepre-training
for unified vision-language understanding and generation. In: ICML (2022) 3
24. Li, L.H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L.,
Zhang, L., Hwang, J.N., et al.: Grounded language-image pre-training. In: CVPR
(2022) 3
25. Li,S.,Deng,W.,Du,J.:Reliablecrowdsourcinganddeeplocality-preservinglearn-
ing for expression recognition in the wild. In: CVPR (2017) 9, 19
26. Li, Y., Fan, H., Hu, R., Feichtenhofer, C., He, K.: Scaling language-image pre-
trainingviamasking.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition (CVPR). pp. 23390–23400 (June 2023) 1, 3
27. Liang, F., Wu, B., Dai, X., Li, K., Zhao, Y., Zhang, H., Zhang, P., Vajda, P.,
Marculescu, D.: Open-vocabulary semantic segmentation with mask-adapted clip.
In: CVPR (2023) 3
28. Liu,H.,Li,C.,Li,Y.,Lee,Y.J.:Improvedbaselineswithvisualinstructiontuning.
arXiv preprint arXiv:2310.03744 (2023) 2, 11, 12, 24
29. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: NeurIPS (2024)
2, 4
30. Liu,H.,Son,K.,Yang,J.,Liu,C.,Gao,J.,Lee,Y.J.,Li,C.:Learningcustomized
visual models with retrieval-augmented knowledge. In: CVPR (2023) 4
31. Liu,M.,Roy,S.,Li,W.,Zhong,Z.,Sebe,N.,Ricci,E.:Democratizingfine-grained
visual recognition with large language models. In: ICLR (2024) 4, 9, 10, 22, 23
32. Long, A., Yin, W., Ajanthan, T., Nguyen, V., Purkait, P., Garg, R., Blair, A.,
Shen,C.,vandenHengel,A.:Retrievalaugmentedclassificationforlong-tailvisual
recognition. In: CVPR (2022) 4
33. Lüddecke, T., Ecker, A.: Image segmentation using text and image prompts. In:
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecog-
nition (CVPR). pp. 7086–7096 (June 2022) 3, 6RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 17
34. Maji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-grained visual
classification of aircraft. arXiv preprint arXiv:1306.5151 (2013) 9, 19
35. Malkov,Y.A.,Yashunin,D.A.:Efficientandrobustapproximatenearestneighbor
search using hierarchical navigable small world graphs. TPAMI (2018) 6, 9
36. Miller,G.A.:WordNet:alexicaldatabaseforenglish.CommunicationsoftheACM
(1995) 10
37. Nilsback,M.E.,Zisserman,A.:Automatedflowerclassificationoveralargenumber
of classes. In: ICVGIP (2008) 9, 19
38. OpenAI: GPT-4V(ision) system card (2023), https://openai.com/research/
gpt-4v-system-card 2, 4, 5, 25
39. Parkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.: Cats and dogs. In: CVPR
(2012) 9, 19
40. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-2:
Grounding multimodal large language models to the world. arXiv.org (2023) 2, 4
41. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021) 1, 3, 4, 11, 26, 27
42. Shtedritski,A.,Rupprecht,C.,Vedaldi,A.:Whatdoesclipknowaboutaredcircle?
visual prompt engineering for vlms. arXiv preprint arXiv:2304.06712 (2023) 3
43. Soomro, K., Zamir, A.R., Shah, M.: UCF101: A dataset of 101 human actions
classes from videos in the wild. arXiv preprint arXiv:1212.0402 (2012) 19
44. Subramanian, S., Merrill, W., Darrell, T., Gardner, M., Singh, S., Rohrbach, A.:
Reclip:Astrongzero-shotbaselineforreferringexpressioncomprehension.In:Pro-
ceedings of the 60th Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). pp. 5198–5215 (2022) 3
45. Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: Eva-clip: Improved training tech-
niques for clip at scale. arXiv preprint arXiv:2303.15389 (2023) 1
46. Sun, Z., Fang, Y., Wu, T., Zhang, P., Zang, Y., Kong, S., Xiong, Y., Lin, D.,
Wang,J.:Alpha-CLIP:Aclipmodelfocusingonwhereveryouwant.arXivpreprint
arXiv:2312.03818 (2023) 3
47. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: Caltech-ucsd birds-
200-2011 (2011) 9, 19
48. Wang, J., Zhang, P., Chu, T., Cao, Y., Zhou, Y., Wu, T., Wang, B., He, C., Lin,
D.: V3Det: Vast vocabulary visual detection dataset. In: ICCV (2023) 3, 7, 9, 12,
28
49. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao,
L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., Tang, J.: Cogvlm: Visual
expert for pretrained language models (2023) 2, 4, 12
50. Wu,W.,Yao,H.,Zhang,M.,Song,Y.,Ouyang,W.,Wang,J.:GPT4Vis:Whatcan
gpt-4 do for zero-shot visual recognition? arXiv preprint arXiv:2311.15732 (2023)
25
51. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: SUN database: Large-
scale scene recognition from abbey to zoo. In: CVPR (2010) 9, 19
52. Xu, X., Xiong, T., Ding, Z., Tu, Z.: Masqclip for open-vocabulary universal im-
age segmentation. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 887–898 (2023) 3
53. Yang, H., Ma, C., Wen, B., Jiang, Y., Yuan, Z., Zhu, X.: Recognize any regions.
arXiv preprint arXiv:2311.01373 (2023) 3
54. Yang, L., Wang, Y., Li, X., Wang, X., Yang, J.: Fine-grained visual prompting
(2023) 3, 618 Ziyu Liu et al.
55. Yasunaga,M.,Aghajanyan,A.,Shi,W.,James,R.,Leskovec,J.,Liang,P.,Lewis,
M., Zettlemoyer, L., Yih, W.t.: Retrieval-augmented multimodal language model-
ing. In: ICML (2023) 3, 4
56. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P.,
Shi, Y., et al.: mplug-owl: Modularization empowers large language models with
multimodality. arXiv.org (2023) 2, 4
57. Zang, Y., Li, W., Zhou, K., Huang, C., Loy, C.C.: Open-vocabulary detr with
conditional matching. In: ECCV (2022) 3
58. Zhang, P., Wang, X.D.B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S.,
Zhang,S.,Duan,H.,Yan,H.,etal.:Internlm-Xcomposer:Avision-languagelarge
model for advanced text-image comprehension and composition. arXiv preprint
arXiv:2309.15112 (2023) 2, 4
59. Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L.H., Zhou, L., Dai, X.,
Yuan,L.,Li,Y.,etal.:RegionCLIP:Region-basedlanguage-imagepretraining.In:
CVPR (2022) 3, 11, 12
60. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language
models. IJCV (2022) 3
61. Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., Misra, I.: Detecting twenty-
thousand classes using image-level supervision. In: ECCV (2022) 3
62. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: MiniGPT-4: Enhancing vision-
language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 (2023) 2, 4RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 19
Appendix
In this appendix, we provide a series of detailed supporting materials to aid
in a deeper understanding of our work. Firstly, in Appendix A, we introduce
thefourteen imageclassification datasetsinvolvedinour experiments,including
seven common datasets and seven fine-grained datasets, as well as two large-
scale vocabulary detection datasets. Following that, in Appendix B, we provide
detailed information on the prompts used in our RAR, as well as the prompts
used in corresponding ablation studies. In Appendix C, we supplement details
on the structure and experimental aspects of RAR, dividing the content into
threesections:Fine-GrainedVisualRecognition,Few-ShotImageClassification,
and Zero-Shot Region Recognition.
A Dataset Statistics
In this section, we delve deeper into the specifics of the fourteen classification
andtwodetectiondatasetsemployedinourresearch.Theclassificationdatasets
encompass a wide range, from general categories that cover a broad spectrum
of common objects to fine-grained types that focus on more specific, detailed
distinctions within a particular category. The detection datasets, on the other
hand, are extensive, encompassing tens of thousands of object categories. These
datasets are designed to challenge the model’s ability to identify and categorize
objectsfromavastarrayofpossibleclasses.Thelong-tailnatureofthesedatasets
poses a significant challenge for our RAR model.
A.1 Classification Datasets
Intheexperimentalpart,weuseatotaloffourteenimageclassificationdatasets,
including seven fine-grained classification datasets and seven common classifica-
tion datasets. Fine-grained image classification datasets include: Bird-200 [47],
Stanford Cars [20], Dog-120 [19], Oxford Flowers [37], Oxford Pets [39], FGVC-
Aircraft [34], and Food101 [37]. Common image classification datasets include:
ImageNet[7],Caltech101[11],RAF-DB[25],Sun397[51],Eurosat[15],DTD[4],
andUCF-101[43].WepresentalltheutilizeddatasetsinFigure1.Andin Tab.7,
we list the statistics and sources of these datasets in detail.
In our fine-grained visual recognition experiments, we employed the follow-
ing datasets: Bird-200, Stanford Cars, Dog-120, Flowers-102, and Oxford pets.
Ineachdataset,weselected3imagesfromthetrainingsettoconstructourmem-
ory and conducted tests on the corresponding validation sets. In our few-shot
image classification experiments, we used the FGVC-Aircraft dataset to build
fine-tune data and tested our RAR model across eleven classification datasets:
StanfordCars,Flower-102,OxfordPets,Food101,ImageNet,Caltech101,RAF-
DB,Sun397,Eurosat,DTD,andUCF-101.Weselectedeither4or8imagesfrom
the training set of each dataset to place into memory, corresponding to 4-shot
and 8-shot settings, respectively, and conducted tests across all validation sets.20 Ziyu Liu et al.
Datasets
Fine-Grained Classification Dataset Detection Dataset
Food101 Stanford Cars Flower102 FGVC-Aircraft Oxford pets Dog-120 Bird-200
LVIS
Common Classification Dataset
SUN397 Eurosat Caltech101 UCF101 DTD Imagenet RAF-DB V3Det
Fig.6: Datsets used in our experiments. We select 14 classification datasets (7 fine-
grained and 7 common) and 2 object detection datasets as our benchmarks.
Table 7: Statistics for the classification and detection datasets used in our three set-
tings:fine-grainedvisualrecognition,few-shotimagerecognition,andzero-shotregion
recognition.
Settings Dataset CategoriesEvaluationMetrics Sourcelink
Bird-200 200 cACC,sACC Birdwebsite
Fine-Grained Car-196 196 cACC,sACC Kaggle
VisualRecog. Dog-120 120 cACC,sACC Tensorflow
Flower-102 102 cACC,sACC Tensorflow
Pet-37 37 cACC,sACC Tensorflow
RAF-DB 7 Accuracy RAF-DBwebsite
Eurosat 10 Accuracy Tensorflow
DTD 47 Accuracy Tensorflow
Few-Shot Image FGVCAircraft 100 Accuracy FGVCwebsite
Caltech101 101 Accuracy Tensorflow
Recog. Food101 101 Accuracy Tensorflow
UCF-101 101 Accuracy Tensorflow
SUN397 397 Accuracy Tensorflow
ImageNet 1000 Accuracy Tensorflow
Zero-Shot LVIS 1203 mAP LVISwebsite
RegionRecog. V3Det 13204 mAP Github
A.2 Detection Datasets
In our Zero-Shot Region Recognition experiments, we utilized two large-scale
vocabulary detection datasets, namely LVIS and V3Det. The LVIS dataset, de-
veloped by Facebook AI researchers, stands out with its extensive coverage,
including 164,000 images and about 2,000,000 high-quality instance segmenta-
tionannotationsthatspanover1,000objectclasses.Thisdatasetisparticularly
notable for its long-tail distribution, which means it includes a large number of
infrequent or rare object classes in addition to the common ones. This diver-
sity challenges our model to recognize and differentiate between a wide array
of objects, including those that are less common and hence more challenging to
identify accurately.
The V3Det dataset complements LVIS by offering an even broader scope.
With its 245,000 images distributed across an impressive 13,204 categories,RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 21
Please play the role of a classification expert, and Please play the role of a classification expert, and
sort the provided categories from high to low sort the provided categories from high to low
according to the top 5 similarity with the input according to the top 5 similarity with the input
image. Here are the optional categories:['2012 Aston image. Here are the optional categories: ['737-800',
Martin V8 Vantage Convertible', '2012 Aston Martin '757-300', '767-200', 'A340-300', 'MD-90']
Virage Coupe', '2012 Aston Martin Virage
Convertible', '2012 Audi TTS Coupe', '2012 Aston
Martin V8 Vantage Coupe'] Based on the input image and the provided
categories, the sorting from high to low similarity
with the aircraft in the image is as follows:
Based on the input image and the provided 1. 757-300
categories, the sorting from high to low similarity 2. 767-200
with the car in the image is as follows: 3. MD-90
1. 2012 Aston Martin V8 Vantage Coupe 4. 737-800
2. 2012 Aston Martin Virage Coupe 5. A340-300
3. 2012 Aston Martin V8 Vantage Convertible
4. 2012 Aston Martin Virage Convertible
5. 2012 Audi TTS Coupe Why did you give this order?
Why did you give this order? The aircraft in the image is a Boeing 757, which is
identifiable by its long fuselage, narrow-body, and
characteristic nose shape. The -300 series has an
The car in the image is an Aston Martin, and its extended fuselage compared to other variants, which
styling most closely resembles the Vantage model appears to match the aircraft pictured. The 767-200
line. It's a coupe, not a convertible, so coupes have and MD-90 have some similarities but differ in
been ranked higher than convertibles. The Audi TTS specific design elements such as winglets and
Coupe, while also a coupe, is from a different engine placement. The 737-800 is a shorter aircraft
manufacturer and thus has distinct styling cues that with a different wing design. The A340-300 is a
are less similar to the vehicle in the image. wide-body aircraft with four engines, making it
quite distinct from the 757-300.
Fig.7: GPT4V Example for Stanford Cars and FGVC Aircraft. Green for ground
truth, blue for characteristics analyzed by GPT-4V.
V3Det brings an unprecedented level of diversity to the table. The dataset in-
cludes1,753,000meticulouslyannotatedboundingboxes,makingitaninvaluable
resource for developing and testing detection algorithms capable of handling a
wide variety of object types. Its large number of categories ensures that the
dataset has a comprehensive representation of the visual world, making it an
ideal testing ground for our Zero-Shot Region Recognition experiments.
B Prompt Formats
Inthissection,wedelveintothedetaileddesignofourprompts.Wehavecrafted
distinct prompts for various tasks to test the capabilities of the baseline model
and our RAR model in visual recognition.
In our RAR pipeline, the prompt primarily serves to merge the input image
withthecategoryinformationretrievedfrommemory.ItguidesMLLMstorank
theretrievedcandidateobjectcategoriesbasedonsimilarity.Ourpromptformat
is as follows:
Pleaseplaytheroleofaclassificationexpert,andsorttheprovided
categoriesfromhightolowaccordingtothe{top-k}similaritywiththe
input image. Here are the optional categories:{categories}.
Here, ‘{top-k}’ is replaced with the number of categories input. And ‘{cate-
gories}’ is replaced with the top-k categories retrieved from memory.22 Ziyu Liu et al.
Please play the role of a Please play the role of a Please play the role of a
classification expert, and sort classification expert, and sort classification expert, and sort
the provided categories from the provided categories from the provided categories from
high to low according to the high to low according to the high to low according to the top
top 5 similarity with the input top 5 similarity with the input 5 similarity with the input
image. Here are the optional image. Here are the optional image. Here are the optional
categories:['lenten rose', categories: ['yorkshire_terrier', categories:
'japanese anemone', 'tree 'chihuahua','miniature_pinsche ['donuts', 'breakfast_burrito',
poppy', 'columbine', 'camellia'] r','havanese','beagle'] 'falafel', 'hummus', 'beet_salad']
Based on the input image and Based on the input image and Based on the input image and
the provided categories, here the provided categories, the the provided categories, the
is the sorting of the categories sorting from high to low sorting from high to low
from high to low according to similarity with the dog in the similarity with the food items
their similarity with the flower image is: in the image is:
in the image: 1. Chihuahua 1. Falafel
1. Japanese anemone 2. Miniature Pinscher 2. Hummus
2. Tree poppy 3. Yorkshire Terrier 3. Beet Salad
3. Camellia 4. Havanese 4. Breakfast Burrito
4. Lenten rose 5. Beagle 5. Donuts
5. Columbine
Why did you give this order? Why did you give this order? Why did you give this order?
The flower in the image The dog in the image is a The dish in the image appears
appears to be a Japanese Chihuahua, identifiable by its to include falafel balls and a
anemone, which typically has small size, prominent ears, and side of hummus, which are
white petals surrounding a facial features. Miniature typically found in Middle
yellow center of stamens and a Pinschers are somewhat similar Eastern cuisine. The other
greenish pistil, matching the in size and ear shape, while items, such as beet salad,
one shown. The other flowers Yorkshire Terriers, Havanese, breakfast burrito, and donuts,
listed have different and Beagles have distinct do not seem to be present in the
characteristics that do not differences in coat, size, and image or resemble the food
match as closely. facial structure compared to the shown.
Chihuahua.
Fig.8: GPT4V Example for Flowers102, Pets37 and Food101. Green for ground
truth, blue for characteristics analyzed by GPT-4V.
Additionally, to assess the visual recognition and ranking capabilities of
MLLMsthemselves,wehavepreparedapromptwithexamplestoserveasinput
for the model. Our structured in-context learning prompt is as follows:
“Pleaseplaytheroleofaclassificationexpert,andsorttheprovided
categories from high to low according to the top 5 similarity with
the input image. Here are the optional categories:{categories}. Your
answershouldfollowthefollowingformat,like:[‘categoryA’,‘category
B’,‘categoryC’,‘categoryD’,‘categoryE’].Onlychoosefivecategories,
and no further information.”
WhentestingtheRARpipelinewithMLLMs,‘{categories}’isreplacedwith
all the category names of each dataset.
C More Implemented Details and Experiments
C.1 Fine-Grained Visual Recognition
In the fine-grained visual recognition section, we first evaluate our RAR on the
setting defined in previous work [31]. For each category in the five datasets,RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 23
Top-k Top-k Top-k
Top-k Top-k Top-k
Fig.9: Evaluation on CLIP+KNN for Caltech101, Flowers102, RAF-DB, Pets37,
DTD and UCF101. We report the top-1, 5, 10, 15, 20 accuracy (%) under the 4-shot
settings.
Fig.10: Evaluation on MLLMs for Caltech101, Flowers102. We report the test
results using 10, 15, 20, 25, and 30 category names as inputs.
we select three unlabeled images to form a 3-shot setting. Then, we extract
embeddings using the CLIP B/16 model and store them in memory. The labels
foreachimage correspond tothepredictionsin[31].Wethentestthe validation
set using the RAR pipeline and measure the results with Clustering Accuracy
(cACC) and Semantic Similarity (sACC).
Evaluation Metrics. In the fine-grained visual recognition section, we use
two synergistic metrics: Clustering Accuracy (cACC) and Semantic Similarity
(sACC) to evaluate our method, following [31]. Clustering Accuracy (cACC)
mainly assesses the accuracy of clustering images within the same category,
withoutconsideringthesemanticrelatednessofcategorylabels.Complementing
this, Semantic Similarity (sACC) measures the similarity between the names of
categories in the clusters and the ground truth.24 Ziyu Liu et al.
Table8:Few-shotimageclassificationacross11datasets.Wereportthetop-1accuracy
(%) under the 1-shot, 2-shot, 4-shot, 8-shot and 16-shot settings. The CLIP+KNN
methoddoesnotutilizethetextencoderofCLIP.Instead,weemploythevisualencoder
to extract image features, and then apply the KNN algorithm to these features. Here
our RAR uses the LLaVA1.5 [28] as the MLLM to rank the retrieved results. The
symbol‘-’denotestotheLLaVAmodelfailstomakethepredictionsduetothelimited
window size.
Method Common Fine-Grained
1-shot
CLIP+KNN 29.2 75.9 11.3 37.7 53.9 35.1 47.8 66.7 32.6 45.3 41.3 43.3
LLaVA1.5Finetuning - 84.1 24.9 - 48.2 22.3 35.4 4.59 - 39.2 16.3 -
RAR(LLaVA1.5) 40.3 85.2 34.8 46.5 62.4 38.1 57.4 50.4 38.3 57.6 47.0 50.7
∆ +10.5 +9.3 +23.5 +8.8 +8.5 +3.0 +9.6 -16.3 +5.7 +12.3 +5.7 +7.4
2-shot
CLIP+KNN 36.1 82.9 11.7 44.6 58.7 41.2 58.5 78.9 40.9 54.1 49.0 50.6
LLaVA1.5Finetuning - 53.1 24.9 - 48.2 22.3 38.7 10.03 - 38.2 16.3 -
RAR(LLaVA1.5) 46.8 89.2 27.9 53.1 68.6 47.9 66.5 54.7 45.9 65.4 54.7 57.4
∆ +10.7 +6.3 +16.2 +8.5 +9.9 +6.7 +8.0 -24.2 +5.0 +11.3 +5.7 +6.8
4-shot
CLIP+KNN 42.1 87.9 14.2 51.4 67.6 47.5 64.6 84.5 49.2 62.6 55.6 57.0
LLaVA1.5Finetuning - 88.4 24.9 - 48.2 46.6 58.9 13.2 - 66.4 28.9 -
RAR(LLaVA1.5) 51.0 92.1 27.7 58.8 74.8 53.9 69.6 80.4 54.4 71.4 60.9 63.2
∆ +9.9 +4.2 +13.5 +7.4 +7.2 +6.4 +5.0 -4.1 +5.2 +8.8 +5.3 +6.2
8-shot
CLIP+KNN 47.6 90.6 28.2 56.8 72.8 53.2 68.3 89.5 56.1 68.3 61.8 63.0
LLaVA1.5Finetuning - 92.1 24.9 - 48.2 54.7 66.5 30.1 - 72.5 46.1 -
RAR(LLaVA1.5) 56.5 93.5 46.9 63.4 81.5 59.3 74.3 87.3 61.2 76.6 67.7 69.8
∆ +8.9 +2.9 +18.7 +6.6 +8.7 +6.1 +6.0 -2.2 +5.1 +8.3 +5.9 +6.8
16-shot
CLIP+KNN 52.0 92.4 35.0 61.2 78.7 57.5 70.6 92.1 63.2 71.8 68.3 67.5
LLaVA1.5Finetuning - 94.1 24.9 - 50.6 63 74.7 59.0 - 62.4 -
RAR(LLaVA1.5) 60.3 94.1 53.1 68.0 84.8 63.7 75.9 92.1 67.8 79.4 72.7 73.8
∆ +8.3 +1.7 +18.1 +6.8 +6.1 +6.2 +5.3 +0.0 +4.6 +7.6 +4.4 +6.3
C.2 Few-Shot Image Classification
Inthissection,wedelvedeeperintosomeintriguingobservationsandmotivations
behind our study. Additionally, we have included an array of expanded test
results in this part, encompassing classification tests from 1-shot to 16-shot,
tests for top-5 accuracy, and we have further expanded our memory to explore
the potential capabilities of RAR.
More Discussion about Motivation. In the field of image classification, es-
pecially when facing the challenges of fine-grained image categorization, can
MLLMs prove competent and effective? To further explore the potential of
MLLMsinimageclassificationtasks,weemployedtheGPT-4Vmodeltotestse-
lected images from our fine-grained datasets. Initially, we used the CLIP+KNN
method to select 5 candidate images and their categories for a single image, en-
suring that these candidates are at the top-5 in similarity among all images in
memory, thus guaranteeing minimal differences between the chosen categories.
Additionally,weintentionallyselectedexamplesthatCLIPfailedtoclassifycor-
teNegamI 101hcetlaC BD-FAR 793NUS TASoruE
DTD
101-FCU 201rewolF
sraCdrofnatS
101dooF stePdrofxO egarevARAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 25
Table 9: Evaluation on 11 datasets, reporting the top-5 accuracy. We use the 4-shot
setting.
Method Common Fine-Grained
CLIP+KNN 67.1 97.6 48.0 78.9 91.5 70.5 85.4 96.5 79.1 86.2 87.6 80.8
RAR(LLaVA1.5) 69.7 97.7 53.8 80.1 92.5 71.9 86.2 96.5 79.1 87.7 88.1 82.1
∆ +2.6 +0.1 +5.8 +1.2 +1.0 +1.4 +0.8 +0.0 +0.0 +1.5 +0.5 +1.3
Table 10: Evaluation on 11 datasets, reporting the top-1 accuracy. The GPT4V [38]
results are copied from [50].
Method Common Fine-Grained
GPT-4V 62.0 95.5 58.5 57.7 36.2 59.1 81.6 70.6 58.3 80.1 92.6 68.4
RAR(LLaVA1.5) 73.4 94.6 73.8 70.6 93.3 71.9 79.1 95.6 72.6 86.2 79.9 81.0
∆ +11.4 -0.9 +15.3 +12.9 +57.1 +12.8 -2.5 +25.0 +14.3 +6.1 -12.7 +12.6
RAR(Intern-IXC2) 71.5 94.4 72.7 69.7 91.7 69.9 77.6 93.2 65.4 83.9 79.3 79.0
∆ +9.5 -1.1 +14.2 +12.0 +55.5 +10.8 -4.0 +22.6 +7.1 +3.8 -13.3 +10.6
RAR(Qwen-VL) 75.8 95.5 66.0 72.7 90.7 72.5 81.4 97.5 81.6 87.2 88.1 82.6
∆ +13.8 +0.0 +7.5 +5.0 +54.5 +13.4 -0.2 +26.9 +23.3 +7.1 -4.5 +14.2
rectly, increasing the complexity of the task. Subsequently, we presented these
imagesandcategoriestoGPT-4V,utilizingthepromptdescribedinAppendixB,
prompting GPT-4V to rank all categories by similarity. During this process, we
also requested GPT-4V to provide the rationale for its classifications, allowing
us to analyze the specific role of MLLMs in classification tasks based on the
reasons provided by GPT-4V. Fig. 7 and Fig. 8 presents several examples of
five fine-grained classification datasets.
From the examples in Fig. 7 and Fig. 8, it is evident that GPT-4V is ca-
pable of effectively analyzing the main feature information of objects in images
duringfine-grainedimageclassificationtasks.Forinstance,itidentifieskeychar-
acteristics such as “coupe” (a two-door car), “long fuselage” (long body of an
aircraft),and“prominent ears” (noticeablyprotrudingears),whicharecrucial
for distinguishing between similar categories. Sometimes, these detailed aspects
maybeoverlookedbytheCLIPmodel,leadingtoclassificationerrors.Therefore,
adopting a method of initial retrieval followed by deeper analysis, firstly filter-
ing through the numerous fine-grained categories and then using MLLMs for
furtherexaminationtoselectthemostaccurateanswer,provestobeaneffective
approach for fine-grained image classification tasks.
Simultaneously, we assessed CLIP’s accuracy in handling a variety of clas-
sification datasets. We selected six datasets: Caltech101, Flower102, RAF-DB,
teNegamI
teNegamI
101hcetlaC
101hcetlaC
BD-FAR
BD-FAR
793NUS
793NUS
TASoruE
TASoruE
DTD
DTD
101-FCU
101-FCU
201rewolF
201rewolF
sraCdrofnatS
sraCdrofnatS
101dooF
101dooF
stePdrofxO
stePdrofxO
egarevA
egarevA26 Ziyu Liu et al.
Table11:CroppingablationofCLIP[41]zero-shotclassificationonLVIS[14]
withgroundtruthproposals.Differentbehaviorscanbeseenbeforeandafterblurring
with respect to different object scales.
CropscaleBlurring 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0
✗ 46.7 47.0 46.6 46.4 43.4 43.040.940.7 37.7 37.1 36.2
AP
✓ 47.9 51.3 52.2 53.9 53.3 52.952.651.8 51.2 50.3 49.8
✗ 39.5 40.9 44.6 44.8 44.4 44.242.943.3 41.2 40.5 39.8
AP
s ✓ 33.6 35.2 41.4 43.2 45.6 46.346.746.9 47.4 47.4 47.3
✗ 61.5 61.3 56.4 55.2 49.5 48.644.443.7 39.9 39.0 38.5
AP
m ✓ 63.5 64.2 66.1 68.3 65.2 64.263.462.2 61.0 59.2 58.6
✗ 59.1 57.2 51.1 50.1 45.6 44.441.440.9 38.0 37.8 37.2
AP
l ✓ 72.4 71.3 69.5 69.6 67.0 65.262.960.7 59.6 57.4 55.2
Pets37,DTD,andUCF101,andtestedtheCLIP+KNNmethodfortop1,5,10,
15, and 20 accuracy, with results presented in Fig. 9. We observed that as the
top-kvalueincreased,theclassificationaccuracyimprovedrapidly,reachingover
90%infourofthesixdatasetswhentop-kreached10.ThisindicatesthatCLIP
shows significant advantages as the number of predicted categories increases,
complementing MLLMs’ ability to discern among similar categories.
Following the experimental design in Fig. 9, we used MLLMs to rank cate-
gories when expanding the number of categories. We chose two datasets, Cal-
tech101 and Flowers102, and used 10, 15, 20, 25, 30 categories as input to
MLLMs, ensuring these included the correct category. As shown in Fig. 10, the
distinction ability of MLLMs gradually decreased as the number of categories
input into MLLMs increased.
Hence, we found that MLLMs and CLIP have complementary advantages in
classification tasks. CLIP initially narrows down the correct answer to a smaller
setthroughpreliminaryscreening,whileMLLMscanfinelyselectthecorrectan-
swerfromthisset.OurRARcombinesthestrengthsofbothCLIPandMLLMs,
first finding likely correct candidates through CLIP and retrieval, and then ac-
curately selecting the correct answer through MLLMs’ ranking, thus achieving
outstanding results across multiple classification datasets.
More Evaluation Results. In our few-shot image classification experiments,
weemployedtheCLIPB/16modeltoextractembeddingsfromnimagesineach
category, which were then stored in memory for testing the accuracy of n-shot
experiments.Toaccelerateretrievalspeed,weinitiallyusetheHNSWalgorithm
to transform the original 576-dimensional vectors into 64-dimensional indices
before storing the image embeddings in memory. HNSW is a commonly used
Approximate Nearest Neighbor (ANN) algorithm, primarily aimed at quickly
findingtheknearestelementstoaqueryinalargesetofcandidates.Todemon-
strate the effectiveness of our method, we included results from 1-shot, 2-shot,RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 27
70
65
60
55
50
45
40
35
1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00
Crop Scale
Fig.11: Metric curve visualization of CLIP [41] zero-shot classification on
LVIS[14]withgroundtruthproposals.Differentbehaviorscanbeseenbeforeandafter
blurring with respect to different object’s scales.
and 16-shot experiments in the supplementary materials, alongside the results
of 4-shot and 8-shot experiments, all of which are presented in Tab. 8.
From the 1-shot to 16-shot experiments, RAR’s results showed an improve-
mentovertheCLIP+KNNmethodby7.4%,6.8%,6.2%,6.8%,and6.3%re-
spectively,averaginga6.7%percentagepointincrease,andsignificantlyoutper-
forming the performance of the LLaVa model itself. This outcome demonstrates
theexcellenceofRARinimageclassificationtasks(includingfine-grainedimage
classification), achieved by integrating the strengths of MLLMs and retrieval
techniques.
Top-5 Accuracy Results. Moreover, in the experiments conducted for our
paper, we selected the top 5 retrieved results for ranking. To test the scalability
ofthismethod,weconductedanewexperimentusingthetop10retrievedresults,
rankingthesetencategoriesandthenassessingtheaccuracyofthetop5.Inthis
experiment, we utilized a 4-shot setting, the result is shown in Tab. 9.
The final results demonstrate that although the top 5 accuracy achieved by
CLIP+KNNwasalreadyhigh,ourRARmethodstillmanagedtomakecompre-
hensive improvements on this basis. The average top 5 accuracy across eleven
datasets increased by 1.3%.
Extension to the whole Training Set. To further explore the potential of
RAR, we expanded the memory size to include all images from the training set
stored in memory. We then compared the performance of RAR under this setup
PA28 Ziyu Liu et al.
with that of GPT-4V across multiple image classification datasets. The results
are presented in Tab. 10.
The results in Tab. 10 show that, regardless of whether the base model is
LLaVa, Intern-IXC2, or Qwen-VL, RAR significantly outperforms GPT-4V in
terms of accuracy. Across eleven datasets, the average precision of RAR exceeds
thatofGPT-4Vby12.5percentagepoints.Itisobservedthateven7BMLLMs,
whenintegratedintotheRARpipeline,farsurpasstheclassificationcapabilities
of GPT-4V across multiple image classification datasets.
C.3 Zero-Shot Region Recognition
We carefully study how to adapt CLIP and MLLMs pretrained on full images
to region-level recognition tasks. Zero-shot LVIS [14] AP metric under different
crop scales and object scales are reported in Fig. 11 and Tab. 11. Based on this
experiment, we conclude with two major observations: Firstly, a proper amount
of blurring can significantly improve classification accuracy. This trick can help
leave enough context information while keeping the foreground object promi-
nent. Secondly, for objects with different scales, different crop scales should be
adapted to maximize classification accuracy. As shown in Fig. 11, after blur-
ring, Different object scale AP curves behave differently with respect to crop
scale. We contribute this phenomenon to the resolution shift of CLIP input im-
ages.Therefore,wemaketwoadaptationsforCLIPandMLLMsforregion-level
recognition: Gaussian blurring and adaptive crop scale. We adopt the hyperpa-
rameters of these two tricks on the LVIS training set and find these adaptions
not only fit for the LVIS validation set but also other detection datasets like
V3Det [48].