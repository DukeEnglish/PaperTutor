1
Multi-agent Reinforcement Traffic Signal Control
based on Interpretable Influence Mechanism
and Biased ReLU Approximation
Zhiyue Luo , Jun Xu, Fanglin Chen
Abstract—Traffic signal control is important in intelligent three main types, including fixed time control[3], actuated
transportation system, of which cooperative control is difficult control[4] and adaptive control[5],[6],[7]. However, neither
to realize but yet vital. Many methods model multi-intersection
fixedtimecontrolnoractuatedcontrolmethodsconsiderlong-
traffic networks as grids and address the problem using multi-
term traffic conditions, thus cannot optimize the traffic signal
agentreinforcementlearning(RL).Despitetheseexistingstudies,
there is an opportunity to further enhance our understanding phases adaptively based on real-time traffic flow. In contrast,
of the connectivity and globality of the traffic networks by adaptivecontrolcaneffectivelymitigatetrafficcongestionand
capturing the spatiotemporal traffic information with efficient enhancetransportationefficiency,whichiscurrentlyaresearch
neural networks in deep RL.
hotspot. With the development of artificial intelligence, data-
In this paper, we propose a novel multi-agent actor-critic
driven control methods play an increasingly important role
frameworkbasedonaninterpretableinfluencemechanismwitha
centralizedlearninganddecentralizedexecutionmethod.Specif- in intelligent transportation systems. Traffic signal control
ically, we first construct an actor-critic framework, for which is a sequential decision problem, which can be modeled as
the piecewise linear neural network (PWLNN), named biased MarkovDecisionProcess(MDP)andsolvedbyreinforcement
ReLU(BReLU),isusedasthefunctionapproximatortoobtaina
learning (RL). RL is a powerful dynamic control paradigm,
more accurate and theoretically grounded approximation. Then,
making no additional assumptions on the underlying traffic
to model the relationships among agents in multi-intersection
scenarios, we introduce an interpretable influence mechanism transition model. Rather than computing explicit traffic flow
basedonefficienthinginghyperplanesneuralnetwork(EHHNN), dynamic equations, RL learns the optimal strategy based on
which derives weights by ANOVA decomposition among agents its experience interacting with the traffic environment. The
and extracts spatiotemporal dependencies of the traffic features.
objective of traffic signal control is to minimize the total
Finally, our proposed framework is validated on two synthetic
waitingtimewithinthetrafficnetworkbycontrollingthephase
traffic networks to coordinate signal control between intersec-
tions, achieving lower traffic delays across the entire traffic of traffic signals at intersections.
network compared to state-of-the-art (SOTA) performance. Several RL methods have been applied to isolated traffic
signal control [8], [9], [10], significantly impacting the field
Index Terms—Multi-agent reinforcement learning, biased
ReLU neural network, efficient hinging hyperplanes neural oftrafficsignalcontrolproblem.However,inrealworld,traffic
network, traffic signal control. networks are interrelated, and controlling a specific intersec-
tion signal will inevitably affect the traffic condition of the
I. INTRODUCTION upstream and downstream intersections, leading to the chain
TRANSPORTATIONisthekeydrivingforceforeconomic reaction of the surrounding intersections. Multi-intersection
traffic network is a complex and nonlinear system that can
and social growth and is one of the manifestations of
be quite challenging to model. The complexity arises from
urban competitiveness. With the rapid development of urban-
the need to process intricate spatiotemporal traffic flow data
ization, traffic congestion has become a major challenge for
and address cooperative problems among agents, which are
cities around the world[1]. The increasing number of vehicles
difficult to solve using a centralized approach. Consequently,
on the roads has led to longer travel times, increased fuel
many studies have explored the application of multi-agent
consumption, and higher levels of air pollution [2].
RL (MARL) in cooperative traffic signal control problems.
Traffic signal control is usually regarded as the most sig-
These studies aim to find a balance between centralized
nificant and effective method for quick and safe transporta-
and decentralized training models, thus reaching an optimum
tion, which reduces traffic congestion in the urban network
strategy for all agents and minimizing the waiting time of
by adjusting the signal phase at intersections[1]. Generally,
vehicles within the traffic network. To explore further the
traffic signal control methods can be mainly divided into
spatial structural dependencies among different intersections,
This work was supported in part by the National Natural Sci- manyresearchersappliedgraphRLinmulti-intersectiontraffic
ence Foundation of China under Grant 62173113, and in part by the signal control, which uses graph neural networks to learn and
Science and Technology Innovation Committee of Shenzhen Munici-
exploits representations of each agent and its neighborhood in
pality under Grant GXWD20231129101652001, and in part by Natu-
ral Science Foundation of Guangdong Province of China under Grant the form of node embedding.
2022A1515011584.(Correspondingauthor:JunXu.) Although the works mentioned above deal with the multi-
ZhiyueLuo,JunXu,andFanglinChenarewiththeSchoolofMechanical
intersection traffic signal control problem through cooperative
Engineering and Automation, Harbin Institute of Technology (Shenzhen),
Shenzhen518055,China(email:xujunqgy@hit.edu.cn). RL, the effect of coupled agents on the global performance of
4202
raM
02
]AM.sc[
1v93631.3042:viXra2
the traffic signal has yet to be considered explicitly. Hence, in dependency among different agents. Section V demonstrate
this paper, we introduce a novel interpretable influence mech- the effectiveness of the EHHNN through traffic forecasting
anism using efficient hinging hyperplanes neural networks experiments, while also comparing our proposed multi-agent
(EHHNN) [11], which aims to capture the spatiotemporal actor-critic framework with traditional fixed time control and
dependencies of traffic information to better build the rela- SOTA MARL algorithm in two simulated environments of
tionships among neighboring intersections. Then, we propose multi-intersection traffic signals control task. Finally, Section
a multi-agent actor-critic framework with a centralized critic VI concludes the paper and outlines the main results.
and decentralized actors. The main idea is to learn the critic
using our interpretable influence mechanism to coordinate II. RELATEDWORK
the interaction between different agents. Besides, we improve In this section, we briefly introduce the related work of
the function approximator used in the deep RL, which is MARL and graph neural networks in traffic signal control.
a piecewise linear neural network (PWLNN), named biased Additionally, we introduce the development of the EHHNN
ReLU (BReLU) neural network. This BReLU neural net- and its application.
work can obtain superior performance than rectified linear
units (ReLU) neural network in function approximation when
A. Deep Graph Convolutional RL
reasonably dividing the piecewise linear (PWL) region [12].
In order to tackle the challenges of large-scale traffic signal
We approximate both the value function and policy function
controlandaddresstheissueofthecurseofdimensionalityin
with BReLU neural network, and thus construct the PWL-
MARL,manystudieshaveexploredtheapplicationofMARL
actor-criticframework.Thisalsocoincideswiththeconclusion
methods to traffic signal control problems, which includes
thatminimizingPWLfunctionsoverpolyhedronsyieldsPWL
approachesemployingindependentdeepQ-network[13],[14],
solutions.
multi-agent deep deterministic policy gradient [15], multi-
The technical contribution of this paper can be summarized
agent advantage actor-critic [16], and large-scale decomposi-
as follows:
tion method [17]. However, not all data in the real world can
• A novel MARL framework is proposed, in which the be represented as a sequence or a grid. To explore further the
BReLU neural network is used as the approximator for spatial structural dependencies among different intersections,
both the value function and policy function to construct manyresearchersappliedgraphRLinmulti-intersectiontraffic
the PWL-actor-critic framework. signal control, which uses graph neural networks [18], [19],
• We propose a novel influence mechanism using the [20] to learn and exploits representations of each agent and
EHHNN and combine it with MARL. Compared to the its neighborhood in the form of node embedding. Then,
graph-based approaches, our proposed mechanism does researchers introduced structured perception and relational
notrequirethepre-definedadjacencymatrixofthetraffic reasoning based on graph networks in MARL [21], [22],
networkandexhibitsexcellentcapabilityincapturingthe whichaimstomodeltherelationshipinmulti-agentscenarios.
spatiotemporal dependencies of traffic flow data. Instead, Following this line, graph-based RL methods have been used
it models the relationships by analyzing the impact of for traffic signal control to attain a deeper understanding of
input variables on the output variables. Compared to interactionsamongdifferentintersections,e.g.,graphconvolu-
the attention mechanism, the EHHNN-based influence tionalnetworkandgraphattentionnetworkwereappliedinRL
mechanism has fewer parameters due to the sparsity of toprocessgraph-structuredtrafficflowdata[23],[24].Several
theEHHHNNanddoesnotincludeanonlinearactivation studies combined deep Q network method with graph neural
function, which better explains the contribution of input networks [25], [26]. In another study, researchers introduced
features to a specific output variable. To the best of our multi-agent advantage actor-critic algorithm to graph-based
knowledge, we are the first to use EHHNN to model RL and proposed a decentralized graph-based method [27].
multi-agent relationships and introduce a novel multi- Furthermore, a spatiotemporal MARL framework was pro-
agent framework using EHHNN-based influence mech- posed, which considers the spatial dependency and temporal
anism. dependency of the traffic information [28].
• Experiments are conducted on both the traffic grid and a
non-Euclideantrafficnetwork.Wecomparetheeffective- B. The EHHNN
ness of our solution with several state-of-the-art (SOTA)
In deep RL, dealing with high-dimensional and intricate
methods and further analyze the relation reasoning given
statespacesisamajorchallenge,somanystudiesuseaneffec-
by our influence mechanism.
tive function approximator to approximate the value function
The rest of this paper is organized as follows: Section and policy function. Neural networks enable the integration
II briefly introduces related work. Section III gives problem of perception, decision-making, and execution phases, facil-
formulation of multi-intersection traffic signal control prob- itating end-to-end learning. PWLNNs are now a successful
lem and models it as Partially-Observable MDP (POMDP). mainstream method in deep learning, and ReLU is a com-
Section IV outlines the detailed implementations of our pro- monly used activation function in PWLNNs. In typical neural
posed multi-agent actor-critic framework, which is based on networks, it is impossible to determine the contribution of
BReLU neural network approximation and employs a novel different input variables to the output through neuron connec-
interpretable influence mechanism to learn the spatiotemporal tions. Therefore, in the multi-agent cooperative control tasks,3
researchers have introduced methods incorporating attention
mechanism and relational inductive biases to determine the
weights of various input features through similarity measure-
ment.However,weaspiretocapturetheinfluenceoftheinputs
on outputs directly through an interpretable neural network.
In 2020, a novel neural network called EHHNN was pro-
posed [11], which strikes a good balance between model flex-
ibility and interpretability. The EHHNN is a kind of PWLNN
derived from the hinging hyperplanes (HH) model [29], in
which the ReLU neural network is a kind of the HH model
[12].WhentheHHmodelinvolvesonlyonelinearhyperplane
passingthroughtheorigin,itbecomesaReLUneuralnetwork.
The EHHNN exhibits a high flexibility in dynamic system
identification due to its PWL characteristics. Furthermore,
the network is interpretable, allowing for determining the
impact of the input layer and hidden layer neurons on the Fig. 1: A non-Euclidean Traffic Network
outputthroughtheanalysisofvariance(ANOVA)[30],thereby
facilitating the analysis of feature variables. In 2021, a new
activation function called BReLU is proposed [12], which is Assumption1:Tosimplifythelarge-scaletrafficproblem,we
similar to the ReLU. The BReLU neural network is also an assumethatthesamplingtimeintervals∆tofallintersections
HH model, i.e. a PWL function. Compared with ReLU, it are the same, then the cycle time can be expressed as
employs multiple bias parameters, which can partition the
T =k·∆t (1)
input spaces into numbers of linear regions, thus resulting
in high flexibility and excellent approximation capabilities,
Assumption 2: We assume that the vehicles from upstream
especially in regression problems.
nodeu enteringedgee willtravelfreelyuntiltheyreachthe
j j
tail of the waiting vehicle queues. And they will be divided
III. MARLFORTRAFFICCONTROLPROBLEM
to join the separated queues of the downstream node d that
j
In multi-intersection traffic signal control problems, each they intend to go to.
signal controller reduces the traffic flow at the intersection Assumption 3: We assume that each intersection block is
by adjusting the phase to minimize the waiting time of the equipped with an individual roadside unit, which can observe
traffic flow in the whole traffic network. In this section, we andcollectthetrafficinformationandtransmitittothecentral
first model an unstructured multi-intersection traffic network controller.
as a directed graph and give the formulation and assumptions Now, the dynamic traffic model can be derived as follows.
used in the optimization problem. Then, we model the multi- Accordingtothevehicleconservationtheorem,thenumberof
intersectiontrafficsignalcontrolasaMDPandgiveadetailed vehicles on edge e at step k is updated by
j
formulation of the state, action, and reward function.
(cid:0) (cid:1)
n (k+1)=n (k)+ I −O ,v ∈N −{u } (2)
j j uj,dj dj,v dj j
A. Traffic Signal Control Problem Formulation
where I denotes the traffic flow entering edge e , while
uj,dj j
We consider a more general traffic network, as shown in O denotesthetrafficflowleavingedgee reachingitsnext
v,uj j
Fig. 1. For the urban traffic network may not be necessarily adjacent target node v, N represents the adjacent nodes of
dj
connected due to the limitation of the area for urban de- the downstream node d .
j
velopment [31]. Similar to related work [13],[25],[32], the And consequently, we can update the density of the j-th
typical intersection shapes are included in the traffic network, edge from node u to node d at time k as
j j
as shown in Fig. 1, v and v are three-way intersections,
3 5 (cid:18) (cid:19)
n (k)·τ
while others are crossroads. Besides, the length of lanes in Φ =min 1, j (3)
thetrafficnetworkvaries.Wedefineintersectionsinthetraffic uj,dj L(e j)
networkasnodesandtheroadbetweeneverytwointersections
whereτ isaconstantrepresentingthevehiculargapandL(e )
as edges, then the multi-intersection traffic network can be j
denotes the length of edge j.
modeled as a directed graph G(V,E,Ψ), where V ={v }|V|
i i=1 The queue length q (k) is the number of vehicles
is the nodes set, |V| = N, refers N nodes (intersections) in uj,dj
waiting on edge e at step k, which is the number of vehicles
the graph. E = {e }|E| is the edges set, |E| is the number j
j j=1 on edge e with a speed of 0. And the total queue length at
j
of edges and there are l lanes on the j-th edge. For each
j intersection i can be expressed as
edge e , there is a corresponding upstream node u ∈V and
j j
downstream node d j ∈V. Ψ represents the global attribute of Q i(k)=(cid:88) q uj,vi(k),u j ∈N vi (4)
the traffic graph.
Beforedescribingthetrafficdynamicsystem,wemakethree The total vehicle waiting time at the intersection i at step k
reasonable assumptions. is denoted as W (k).
i4
Our objective is to minimize the total waiting queue of The global reward on the whole traffic network is a linear
the global traffic network by controlling the phase of the weighted sum of reward r for each agent
i
intersection signals
N
(cid:88)
r(k)= r (k) (9)
T/∆t N i
(cid:88) (cid:88)
Z∗ =min Q (k) (5) i=1
i
k=0 i=1 IV. MULTI-AGENTBRELUACTOR-CRITICWITH
INTERPRETABLEINFLUENCEMECHANISM
B. Multi-intersection Traffic Signal Control as MDP In this section, we propose a multi-agent BReLU actor-
Due to the uncertainty and dynamics of the traffic system, critic framework with an interpretable influence mechanism.
the multi-intersection traffic signal control problem can be We introduce a novel neural network called BReLU neu-
abstracted as a discrete stochastic control problem, and be ral network, which offers improved function approximation
modeledasPOMDP,definedasatuple⟨O,A,R,N,γ⟩,where for RL and constructs a PWL-actor-critic framework. Then,
O = {o ,o ,...,o } is the set of observations, A = we extend the PWL-actor-critic framework to the MARL
1 2 N
{a ,a ,...,a } is the set of actions, R = {r ,r ,...,r } algorithm and employ the interpretable influence mechanism
1 2 N 1 2 N
is the set of reward, and N is the number of agents, also the based on EHHNN to capture the spatiotemporal dependencies
number of intersections and the number of nodes in the graph among different agents. We use a centralized training and
G. decentralizedexecutionmethod,whereajointvaluefunctionis
The observation of each agent i is defined as the vector of learnedfromtheaggregatedinformationandeachactorlearns
neighborhood queue length q of intersection v , phase of its policy function based on the local observations.
uj,vi i
current intersection signal ρ, and road density Φ , which
uj,vi
canwellrepresenttheincomingtrafficflowontheroadandthe A. Overview
queue numbers at the intersection. For agent i with u j ∈N vj The overview framework of our proposed method is shown
neighboring intersection, the local observation at step k is in Fig. 2. We first build a graph that comprises traffic signal
agentsandsubsequentlyproposeanovelinfluencemechanism
o (k)=[ρ (k),q (k),Φ (k)],u ∈N (6)
i i uj,vi uj,vi j vi to extract the spatial dependencies from the input graph. In
The joint state over the traffic network is expressed as S = detail, the observations of each agent (cid:8) oi k(cid:9)N
i=1
go through
o ×o ×...o . a node embedding layer and the EHH-based mechanism to
1 2 N
obtain the node embedding Vin for the actor layer and the
Each intersection has a separate controller giving its signal k
aggregation embedding for the critic layer, respectively. The
(or action). At each time step k, the controller of intersection
moduleinsidetheinfluencemechanismisshownontheright-
i selects a discrete action, denoted as a (k) ∈ A . The joint
action grows exponentially with the nui mber of i agents. We hand side of Fig. 2, the observations (cid:8) oi k(cid:9)N
i=1
is fed into a
consideronlyfeasiblesignconfigurationsintheactionsetand linear transformation layer followed by an EHHNN to obtain
use a four-stage green phase control. the hidden variable H k, then an ANOVA decomposition layer
In traffic signal control, researchers often use characteristic isdesignedforfeatureextractiontoobtaintheimportantcoeffi-
variables such as the total waiting time and the queue length cientσ m.Finally,theaggregationembeddingV kout isobtained
of vehicles to define the reward function. Many studies use through weighted aggregation. We approximate the policy
expressions such as Eq. (7) to define reward function, which function and value function in the actor-critic layers with a
uses changes in traffic characteristic variables X between novel neural network named BReLU and thus construct a
tcv
adjacent time step. PWL-actor-critic framework. Next, we will provide a detailed
description of each module within the proposed framework.
r (k)=X (k−1)−X (k) (7)
tcv tcv tcv
B. Node Embedding
Using the expression above, agents tend to accumulate a
(cid:110) (cid:111)N
certain amount of vehicles at the intersection and then release Firstly, we obtain the node embedding Vin = vin
k i,k
them to obtain larger rewards. This control strategy obviously at the current time step k according to the neighboring edi= g1 e
doesnotconformtothereal-worldtrafficapplicationscenario. information of intersection v , i.e. agent i. As mentioned in
i
Therefore,inthispaper,weproposeanimprovedrewardfunc- Section III, each traffic intersection represents a node, and the
tionbasedonEq.(7).Consideringthecharacteristicsoflarge- node and edge features in the graph network can be repre-
scale traffic network and the optimization objective of traffic sented by the variables in the MDP. The traffic information
control,thenewrewardfunctionisshowninEq.(8),whereκ 1, collected by edge e
j
at time k is defined the same as the
κ
2
and κ
3
are hyperparameters of the reward function under
value of the local observation
(cid:8) oi(cid:9)N
in the MDP
different traffic conditions, ∆Q (k) = Q (k)−Q (k−1) is k i=1
i i i (cid:2) (cid:3)
the changes in queue number between adjacent time step. e j(k)= ρ dj(k),q uj,dj(k),Φ uj,dj(k) , (10)


κ 1, Q i(k)=0 where u j and d j is the upstream node and downstream
r i(k)= −W i(k)/κ 2, ∆Q i(k)>0 (8) node of edge e j, respectly. The traffic information of edge
 −κ 3·∆Q i(k), ∆Q i(k)≤0 e j can represent the interplay among agents, which offers5
Fig. 2: Overview of multi-agent BReLU actor-critic framework.
a more comprehensive understanding of how upstream and directed acyclic graph contribute to the output, including two
downstream traffic flow effects propagate between adjacent types of neurons, source nodes D and intermediate nodes C.
intersections. Then, the node embedding can be expressed as IntheEHHNN,theoutputofsourcenodescanbedescribed
the aggregation of its adjacent edges traffic information as:
vin =fe→v(e (k))=fe→v(cid:0) e (k)(cid:1) ,u ∈N (11) z 1,s =max{0,x m−β m,qm} (12)
i,k j uj,vi j vi
where m represents the dimension of the input variable, and
wherefe→v isaone-layerMultiLayerPerception(MLP)with
β represents the q -th bias parameters on the input
the ReLU activation function. m,qm m
variable x .
m
In the hidden layer of the EHHNN, the intermediate nodes
C. Interpretable Influence Mechanism based on EHHNN
are obtained by minimizing existing neurons of the previous
Thetrafficmodelisanonlinearsystem,whichischallenging layers, which comes from different input dimension
to model for its complex spatiotemporal traffic flow data. (cid:26)
(cid:8) (cid:9)
Therefore, establishing an interpretable influence mechanism z = min max 0,x −β ,
and extracting the impact of neighboring traffic information
p,s
nns1,...,nnsp∈Jp,s
nns1 s1
(13)
(cid:110) (cid:111)(cid:27)
can enhance collaborative control among different intersec-
...max 0,x −β
tions. The EHHNN was first applied in short-term traffic
nnsp sp
flow prediction in 2022, which figured out the spatiotemporal (cid:8) (cid:9)
wherewedefineJ = nn ,...,nn containstheindices
factors influencing the traffic flow using ANOVA decompo-
p,s s1 sp
ofneuronsgeneratedbypreviouslayers,and|J |=p,which
p,s
sition [33]. Compared with this work, we further explore
represents the number of interacting neurons of the p-th layer.
the application of EHHNN in large-scale multi-intersection Finally, the output of the EHHNN H = {h }N is the
control problem. We propose an influence mechanism based k i,k i=1
weighted sum of all neurons in the hidden layer:
on EHHNN, which can not only capture the spatiotemporal
dependencies of traffic flow data, but also illustrate the re- (cid:88)n1 (cid:88)P (cid:88)np
H =α + α z (x˜)+ α z (x˜) (14)
lation representation among different agents, enabling multi- k 0 1,s 1,s p,s p,s
agent cooperative control. The structure of our proposed s=1 p=2s=1
interpretableinfluencemechanismmoduleisshownintheblue where α ,α are the weight of the EHHNN and α is the
1,s p,s 0
box in Fig. 2. constant bias, x˜ = Xin, n and n denotes the number of
k 1 p
Firstly, we reduce the dimension of the local observation neurons in the 1-th and p-th layer, respectively.
(cid:8) oi(cid:9)N
through a linear transformation layer W to obtain In this paper, we employ a two-factor analysis of variance
k i=1
the input variable Xin for the EHH layer. Unlike other neural (ANOVA)todeterminethemaineffectofdifferenttrafficflow
k
networks, the EHHNN possesses interpretability, allowing us information as individual factors, as well as the interaction
to extract the interactions among different input variables effectofbivariatefactorsonintersectioncongestion,i.e.P =2
throughANOVAdecompositionandaninteractionmatrix.The inEq.(14).Thefirstsumrepresentstheinfluenceofindividual
hidden layer in EHHNN can be seen as a directed acyclic variables,thesecondsumrepresentsthejointinfluenceoftwo
graph, as shown in the yellow box in Fig. 2. All nodes in the variables when P = 2. This characteristic of the EHHNN6
providesinsightsintohowdifferentvariablescontributetothe Finally, we derive the aggregation embedding Vout =
k
overall prediction and facilitates a deeper understanding of (cid:110) (cid:111)N
vout , which aggregates information from other inter-
the underlying relationships within the data. Similar to the i,k i=1
sections
relatedworkin[30],[11]and[33],wecanidentifythehidden (cid:88)
vout = σ vin (17)
nodes that influence each output component in the ANOVA i,k m i,k
decomposition, which is calculated by where vin is the node embedding.
i,k
(cid:112) This output graph Vout, which extracts the spatiotemporal
σ = VAR(f (x˜)) k
m m features through ANOVA decomposition of EHHNN, is used
(cid:88) (15)
f (x˜)= α z (x˜) as the input of the centralized critic to learn a joint value
m p,s p,s
Jp,s={m} function and faciliate collaboration among different agents.
While in decentralized execution, each actor learns its policy
where VAR(·) denotes the corresponding variance of the functionbasedonthelocallyobservedembeddingvectorVin.
k
prediction output related to the m-th component of input
variable x˜. The larger the value of σ , the greater the impact
m D. Actor-Critic Framework based on BReLU neural network
ofitscorrespondinginputvariableonthedegreeofcongestion
approximation
of the traffic network.
The RL optimization problem is to obtain the optimal
Remark 1. Due to the physical connection between traffic strategyµ∗ ={u ,u ,...}thatsatisfiestheconstraints,while
0 1
data and road networks, many researchers employ relational
maximizing the total reward, which can be written as
reasoning through graph-based methods or attention mecha-
(cid:110) (cid:111)
nism. However, the graph-based methods require a predefined V˜ k+1(x)= max E g(x,u,w)+γV˜ k(f(x,u,w)) (18)
u∈π(x)
and fixed adjacency matrix to reflect the spatial dependencies
of different nodes, which may not effectively capture the where x denotes the state, w is a random disturbance with a
spatiotemporal dependencies in the dynamic traffic flow data. probability distribution P(·|x,u), V˜ is the value function, π
Theattentionmechanismcomputestheattentioncoefficientby denotes the policy function, g(x,u,w) is the cost per step, γ
performing the dot products for all input vectors and utilizes is the discount factor.
a nonlinear activation function, making it challenging to Lemma 1. When the value function V˜ is a PWL function, the
interpret the relationships between learned weights. Further-
policy function π is also a PWL function.
more, most traditional neural networks lack interpretability,
makingitchallengingtoselectandanalyzetheinputvariables Proof. According to the Bellman’s equation, we have
while accurately predicting the complex spatiotemporal traffic (cid:110) (cid:111)
V˜∗(x)= max E g(x,u,w)+γV˜∗(f(x,u,w)) (19)
flow. The EHHNN can meet the requirements above, which
u∈π∗(x)
employsauniquenetworkstructuretocapturespatiotemporal
this equation can be view as the limit as k →∞ of Eq. (18).
dependencies from the input data. It has fewer parameters
Then the optimal strategy can be derived as
due to the sparsity of the network structure and can extract
(cid:110) (cid:111)
the influence coefficients derived from ANOVA decomposition µ∗ =argmaxE g(x,u,w)+γV˜∗(f(x,u,w))
without knowing the node connectivity of the data, as illus- (20)
u∈π(x)
trated in the framework shown in Fig. 3.
The cost function in Eq. (20) is g(x,u,w)+γV˜(f(x,u,w)),
while g(x,u,w) is aslo known as the reward function in RL.
In this paper, the reward function as shown in Eq. (8) is a
PWL function with respect to state x. If the value function V˜
is a PWL function, the cost function is the sum of two PWL
functions, which is also a PWL function. It has been proved
by bemporad in 2002 [34] that minimizing or maximizing a
PWLcostfunctionV˜ overapolyhedronyieldsaPWLsolution
π. Therefore, if we approximate the value function V˜ using a
PWLNN,thepolicyfunctionπshouldalsobeaPWLsolution,
Fig. 3: Structure of EHH network decomposition
as stated in the conclusion above.
To align with the conclusion obtained from Lemma 1, we
While achieving precise prediction, it processes inter-
employ PWL neural networks to approximate the value func-
pretability through ANOVA decomposition, which obtains the
tionandpolicyfunctioninactor-critic.Theactivationfunction
importantcoefficientsofthehiddenvariablesσ asmentioned
m
plays an important role in neural network approximation, in
inEq.(15).Additionally,sincethereisnononlinearactivation
which the ReLU activation function is prevalent in neural
function after the linear transformation layer, the importance
networksduetoitssimplicityandcomputationefficiency,and
coefficients of input variables can be derived through inverse
it can be defined as:
transformation:
σ =W−1σ (16) z(x)=max{0,x} (21)
in m7
where x = [x ,x ,...,x ]T ∈ R. The prevalent ReLU Each decentralized actor updates its policy function only
1 2 n
network is a kind of HH neural network [12]. based on its local observations, which is updated using the
BasedonReLUneuralnetwork,in2021,LiangandXu[12] proximal policy optimization method. And a clip function is
proposed BReLU neural network, which can be expressed as: used to limit the range of changes in the probability ratio
r (θ,b) of old and new strategies π ,π to avoid large
(cid:8) (cid:9) t i,θ− i,θ
z(x)=max{0,x i−β i1},...,max 0,x i−β iqi (22) variance changes and unstable training.
where q represents the number of linear regions in each
L
(θ)=(cid:88)Nb
min(r (θ,b),clip(r (θ,b),1−ϵ,1+ϵ))·Aˆ (b)
dimension of the input data. i t t i
Different from ReLU neuron, BReLU uses different bias b=1
π (a |vin)
p pa ar ra tim tioe nte ir ns gβ thiq
ei
if no pr uv ta sr pia ab cl ee ss inin tov aari no uu ms bd ei rm oe fns li io nn eas,
r
t rh ee gr ie ob ny
s
r t(θ,b)=
π
i,i θ,θ −(ab b|vi, iib
,n b)
and take advantage of the characteristics of PWL functions. (25)
The input variables after normalization follow an approxi- where
mately normal distribution, and the multiple bias parameters Togiveanoverviewoftheproposedmulti-agentalgorithm,
β are determined by the distribution of the input data: we now summarize it in Algotithm 1 and briefly introduce
iqi
the training process. During a total of N episodes, each
ep
β iqi =[−3ν,−0.824ν,−0.248ν,0.248ν,0.834ν]+η (23) l ta ras nti sn ig tiofo nr {a (odu (r ka )ti ,o an (o kf ),T oi (n kth +e 1tr )a ,i rni (n kg ))p }r Noces is n, tw oe ths eto mre et mhe
-
i i i i i=1
where the parameter ν,η represent the variance and expecta-
ory buffer M. When the recorded data exceeds the batch size
tion of input variable after normalization, respectly. q is the
i N ,weusethepre-trainedEHHNNtocomputetheimportance
b
numberofbiasparametersofthei-thlayerandalsorepresents
coefficientsoftheinputfeaturesandupdateboththeactorand
the number of linear sub-regions. The selection of the value
critic network with constant learning rate.
q is a trade-off between network accuracy and the number
i
of parameters. And the weights in BReLU neural network are Algorithm 1 Multi-agent BReLU actor-critic framework
obtained using the backpropagation method.
Input: A pre-trained EHHNN
The BReLU neural network demonstrates higher flexibility Output: actor θ for i∈N, critic ϕ
i
and better approximation capabilities by effectively dividing 1: Initialization: actor θ i ←0 and critic ϕ i ←0 for i∈N;
the input spaces into numbers of linear regions, even when 2: for ep=1,...,N ep do
the output increases exponentially with the input. Therefore, 3: for k=1,...,T/∆t do
we approximate the value function V˜ and policy function π 4: Reset the environment, o i(k),M=∅
5: Sample a i(k) from π i(k)
in the actor-critic framework using BReLU neural network, 6: receive r i(k) (8) and o i(k+1)
where the functions approximated by BReLU neural network 7: end for
are PWL. The reason for doing this is two-fold. First, it coin- 8: Store transitions M←M∪[o i(k),a i(k),r i(k)]N i=1
cides with the conclusion that minimizing (maximizing) PWL 9: if N(M)>N b then
10: Compute σ using the pre-trained EHHNN (15)
functions over polyhedron yields PWL solutions. Second, the 11: Compute the aggregated result vout using (17)
i,k
approximationofthevaluefunctionandpolicyfunctionsusing 12: for i=1,...,N do
BReLUneuralnetworkprovidesamorepreciseapproximation 13: Calculate the advantage function Aˆ i(b) (25)
thanthatofReLU.TheredboxinFig.2showsthestructureof 14: Update the actor θ i by minimizing L i(θ) (25)
the BReLU neural network used in the proposed PWL-actor- 15: end for
16: Update the global critic ϕ by minimizing L(ϕ) (24)
critic framework.
17: end if
Different from independent proximal policy optimization 18: end for
(IPPO)[35], which lacks collaboration among agents, in our
proposed multi-agent actor-critic algorithm, all critics are
updated together to minimize a joint regression loss function
V. SIMULATIONRESULTS
as shown in Eq. (24), where the joint value function remains
a PWL function. And Aˆ represents the advantage function, In this section, we evaluate our proposed method using
i
the SUMO traffic simulator [37]. Firstly, we employ traffic
which is estimated by the truncated version of the generalized
data of Los Angeles country (METR-LA) dataset [38] for
advantage estimation [36], N denotes the training batch size,
b
b′ is an arbitrary sampling sequence after sample b, γ is the traffic forecasting, comparing existing attention mechanism
and analyzing the effectiveness of the proposed interpretable
discount factor, λ is the regularization parameters and W is
influence mechanism based on EHHNN. Compared with the
the weight of the neural network that approximates the value
function V˜. work in [33], we conduct the result on a different dataset
and validate the performance of EHHNN on a much larger
traffic network. Subsequently, we conduct both quantitative
L(ϕ)=−(cid:88)N (cid:88)Nb
(Aˆ (b))2+λ∥W∥ andqualitativeexperimentsformulti-intersectiontrafficsignal
i 1
control on two synthetic traffic networks, and compare it to
i=1b=1 (24)
Aˆ (b)= (cid:88) γb′−br (b′)−V˜(vout) thetraditionalfixedtimecontrolmethodandtheSOTAMARL
i i i,b controllers.
b′>b8
A. Traffic forecasting reduction are fed into the EHHNN, which outputs the pre-
1) Dataset Description: To validate the effectiveness and diction results. Our proposed model is effectively a two-layer
shallow neural network. Therefore, we compare it with three
interpretability of the proposed influence mechanism, we con-
other shallow networks, including a two-layer fully connected
duct experiments on the METR-LA dataset . The METR-LA
(FC) neural network, a FC long short-term memory (FC-
datasetconsistsoftrafficdatacollectedfromcirculardetectors
LSTM) neural network, and graph attention neural network
onhighwaysinLosAngelesCountry,comprisingatotalof207
(GAT). Besides, we compared the EHHNN with a SOTA
sensors.Forourexperiments,weselecte15adjacentnodes,as
method in traffic forecasting, which is a deep network called
illustrated in Fig. 4. We evaluate the forecasting performance
Spatial-Temporal Graph Convolutional Network (STGCN).
of the EHHNN compared with four baseline neural networks.
All models utilized the same training parameters, undergoing
Furthermore, we conducte the interpretability analysis of the
300epochsoftrainingonthetrainingset,employingavariable
EHH network to facilitate a deeper understanding of the
learning rate optimizer. Subsequently, the optimal parameters
underlying relationships between different nodes.
ofthemodelweredeterminedthroughperformanceevaluation
on the validation set. Finally, the models were evaluated on
the test set. All tests were conducted with a historical time
window of 60 minutes, i.e. 12 sampling points, for predicting
the traffic condition in the subsequent 15, 30, and 45 minutes.
4) Experiment Results: Table I demonstrates the results of
EHHNN and baselines on the dataset METR-LA. Compared
withthethreeshallownetworks,theEHHNNachievedthebest
performance. In comparison with the STGCN deep network,
our model outperformed in both RMSE and R2 metrics,
indicating better fitting to large errors and overall predictions
Fig. 4: The METR-LA traffic network that better align with the actual trends in values.
Besides, due to the sparse connectivity and the neurons-
For the traffic data in METR-LA dataset, information is connected structure of the EHHNN, it can achieve supe-
collected every 5 minutes, with an observation window of 60 rior prediction accuracy with fewer training parameters. The
minutesandamaximumpredictionhorizonof45minutes.We number of hidden layer neurons significantly impacts the
splitthedatasetintothreedistinctsets,includingatrainingset, performance and capability of a neural network. Increasing
a validation set and a test set, with the training set accounting the number of hidden layer neurons enhances the expressive
for 60% of the total samples, and the remaining 40% each ability and complexity of the neural network. However, in
allocated to the validation and test sets. a FC network, adding neurons implies increasing training
2) Measures of effectiveness: To evaluate the accuracy parameters, leading to longer training times and heightened
of different forecasting models, we employ three measures demands on computational resources. Compared with the
of effectiveness, including mean absolute error (MAE), R2 FC network, FC-LSTM network, the EHHNN requires fewer
value[39] and root mean square error (RMSE): training parameters when having the same number of hidden
N T neurons.
1 (cid:88)(cid:88)
MAE= |(x −xˆ )|
N ·T i,t i,t
i=1 t=1 B. Multi-intersection traffic signal control
N T N T
(cid:88)(cid:88) (cid:88)(cid:88) After evaluating the effectiveness and interpretability of the
R2 =1− (x −xˆ )2/ (x −x¯)2
i,t i,t i,t EHHNN in traffic forecasting, in this section, we apply the
i=1 t=1 i=1 t=1
proposedinfluencemechanismbasedonEHHNNtothemulti-
(cid:118)
(cid:117) N T intersection traffic signal control problem, and evaluate the
(cid:117) 1 (cid:88)(cid:88)
RMSE=(cid:116)
N ·T
(x i,t−xˆ i,t)2 (26) algorithm on two different synthetic traffic networks.
i=1 t=1 1) Traffic Signal Control using Synthetic Traffic Networks:
whereN denotesthenumberofpredictednodesofMETR-LA We evaluate our proposed method on two synthetic traffic
dataset, T denotes the historical time window for prediction, networks, including a 5×5 traffic grid and a non-Euclidean
x is the input traffic flow data of the i-th node at time t, traffic network, as shown in Fig. 5. The details are introduced
i,t
and x¯ represents the mean value of the input traffic flow data as follows:
x i,t. • Network 5×5: A 5×5 traffic grid with three bidirectional
3) Experimental Settings: All experiments are compiled lanes in four directions at each intersection. The road
and tested on Linux cluster (CPU: Intel(R) Xeon(R) Silver length is 100m, and the lane speed limit is 13.89m/s.
4216 CPU @ 2.10GHz GPU:NVIDIA GeForce RTX 3090). There are approximately 930 vehicles generated and
The EHHNN is a single hidden layer neural network. In added to the network per episode.
the traffic forecasting experiments of this section, our model • Network NonE: An non-Euclidean traffic network with 8
initially reduces the input dimension through a linear trans- intersections. The intersections are not connected in a
formation layer. Then, the hidden variables after dimension grid-like pattern, and the length of roads varies between9
TABLE I: Comparison of performance on METR-LA
(15/30/60min)
Model
MAE↓ R2 ↓ RMSE↑ Params Neurons
2-layersFC 8.26949/8.42299/8.70810 0.08866/0.07988/0.06103 19.86146/19.96055/20.17158 26029/28954/34804 64
FC-LSTM 4.01271/4.63315/5.66560 0.77434/0.72570/0.64237 9.78062/10.85584/12.69706 59181/93786/162996 64
GAT 5.30224/6.01932/6.77592 0.79210/0.71892/0.63816 9.48623/11.03227/12.52191 14339/15878/18956 64
STGCN 2.83072/3.61224/4.60460 0.83381/0.74521/0.64770 8.48147/10.50363/12.35573 96767/97154/97928 960
EHHNN 3.35360/4.21736/5.24809 0.84352/0.77446/0.66847 8.22992/9.88232/11.98600 42763/62293/101353 370
75m to 150m. There are 10 external road inputs, and • Independent Proximal Policy Optimization (IPPO): a
approximately 250 vehicles are generated and added to decentralizedactor-critic-basedmethodwhereeachagent
the network per episode. We designed this network for learns a truncated advantage function and a policy func-
the various buliding areas of the cities, as described in tion to improve performance.
Subsection III-A. • Graph Convolution RL (DGN): a standard graph-based
RL method that uses graph convolution neural network
andself-attentionmechanismtoextractthetrafficfeature,
thereby learning the Q-values of the agents.
4) Simulation Settings: The simulation runs T = 2500s
per episode, and the traffic signal controllers update every
∆t = 5s. To ensure the safety at the intersection, we set the
yellow phase to 2s, the minimum and maximum green time
g ,g to 5s and 50s, respectively.
min max
In the fixed time control, we set the green phase for each
traffic signal controller as 25s. And for all RL methods, we
(a) 5×5trafficgrid (b) Non-Euclideantrafficnetwork use the same training parameters. All methods are trained for
100 episodes, Table II shows the hyperparameters settings of
Fig. 5: Synthetic Traffic Networks the reward function in MDP and Algotithm 1.
2) Evaluation Metric: To quantitatively measure the con- TABLE II: Hyperparameter settings in MDP
dition of the traffic network, we define two metrics, as shown
Variable Notation Value
in Eq. (27) and Eq. (28). The average waiting time (AVE) is
extrareward κ1 25
used to measure the overall congestion degree of the traffic reducecongestionparameter κ2 5
network, while the traffic flow stability (STA) indicates the increasecongestionparameter κ3 5
frequency of local vehicle accumulation that appears during
Qnetworklearningrata αQ 0.01
criticlearningrate ιc 0.01
the simulation time. actorlearningrate ιA 0.001
AVE=
1
(cid:88)Ts (cid:88)N
W (t) (27)
hyperpar mam ine it be ar tco hfc sl ii zp efunction Nϵ
b
0 3. 22
T i
s
t=0 i=1
1
(cid:88)Ts (cid:34) (cid:88)N (cid:35)2 5) Experimental Results: We compare our proposed
STA= W (t)−E (28) method with the baseline methods on both the 5×5 traffic
T i
s t=0 i=1 grid and non-Euclidean traffic network. Fig. 6 shows the
where T denotes the simulation time, N denotes the number comparisonresultsofourproposedmethodandallthebaseline
s
of intersections, i.e., agents. algorithms on a 5×5 traffic grid. It is evident that, under the
3) Methods Compared with: To evaluate the efffectiveness current parameter settings, the IDQN and MADDPG methods
of our proposed algorithm, we compare it with serval SOTA fail to alleviate traffic congestion, as shown in Fig. 6(b) that
traffic signal control methods, including both tradictional ap- the reward of IDQN and MADDPG have been negative per
proach and RL methods. episode. After excluding these two algorithms, as shown in
• Fixedtimecontrol:apre-definedrule-basedtrafficcontrol Fig. 6, our proposed algorithm achieves similar performance
method, which uses a four-stage phase sequence control. to the two SOTA algorithm IPPO, DGN. Besides, compared
• Independent Deep Q Network (IDQN): a decentralized tofixedtimecontrol,IDQN,MADDPG,ourproposedmehtod
methodwhereeachagentlearnsaQnetworktomaximize achieves reduce the congestion on the traffic network.
its reward independently without interacting with each Subsequently, as illustrated in Fig. 7, we conduct the
other. results on an non-Euclidean traffic network. Our proposed
• Multi-Agent Deep deterministic Policy Gradient (MAD- algorithm exhibits better performance than IPPO, DGN and
DPG): each agent learns a deterministic policy, and this fixed time control method, and achieves reduced delay and
method takes into account the interaction between differ- smoother traffic flow conditions compared to other methods.
ent agents by sharing the experience pool and employing Meanwhile,weconductanablationexperiment,revealingthat
collaborative training methods. the performance obtained using reward function Eq. (7) is10
in traffic information without knowing the pre-defined adja-
cency matrix. The importance of the input variables using
ANOVA decomposition of EHHNN, providing a deeper un-
derstanding of the underlying relationships within the data.
Moreover, the approximation of the global value function and
local policy functions using BReLU neural network not only
provides a more precise approximation but also coincides
with the conclusion that minimizing PWL functions over
polyhedron yields PWL solutions. Simulation experiments
on both the synthetic traffic grid and non-Euclidean traffic
network demonstrate the effectiveness of the proposed multi-
(a) WaitingTimeComparison
agent actor-critic framework, which can effectively extract
important information and coordinate signal control across
500 0 DMGIIDPG PA PQPND ONODPG 11 24105 11 24 00 different intersections, resulting in lower delays in the whole
-500 10 100 traffic network.
8 80
-1000
6 60 --- 221 505 000 000
0 10 20 30 40 epi5s0ode 60 70 80 90 100
024
0 10 20 30 40 epi5s0ode 60 70
IMDIBBD PRRGA PQ GGDNON EED 8HHP 0HHG --owuari-trinegw-ati 9rmd 0e-reward 100024 00
[1] D. Zhao, Y.
Dai,RE anF dER ZE .NC ZE hS
ang. “Computational in-
(b) RewardComparison (c) FlowStabilityComparison
telligence in urban traffic signal control: A survey”.
Fig.6:PerformanceComparisononthe5×5SyntheticTraffic In: IEEE Transactions on Systems, Man, and Cyber-
Grid netics, Part C (Applications and Reviews) 42.4 (2011),
pp. 485–494.
[2] S.Gupta,A.Hamzin,andA.Degbelo.“Alow-costopen
hardware system for collecting traffic data using Wi-Fi
signal strength”. In: Sensors 18.11 (2018), p. 3623.
[3] A. J. Miller. “Settings for fixed-cycle traffic signals”.
In: Journal of the Operational Research Society 14.4
(1963), pp. 373–386.
[4] S.-B. Cools, C. Gershenson, and B. D’Hooghe. “Self-
organizing traffic lights: A realistic simulation”. In:
Advances in applied self-organizing systems (2013),
pp. 45–55.
[5] P. Mannion, J. Duggan, and E. Howley. “An exper-
(a) WaitingTimeComparison imental review of reinforcement learning algorithms
for adaptive traffic signal control”. In: Autonomic road
0 30 14105 140 Transport Support Systems (2016), pp. 47–66.
-500 25 12 120
-1000 20 10 100 [6] A. Haydari and Y. Yılmaz. “Deep reinforcement learn- 15 8 80
-1500 ing for intelligent transportation systems: A survey”. 10 6 60
--- 322 050 000 000
0 10 20 30 40 epi5s0ode 60 70
IMDIBBD PRRGA PQ GGDNON EED 8HHP 0HHG
--owuari-trinegw-ati 9rmd 0e-reward
100-05
5
024
0 10 20 30 40 epi5s0ode 60 70
IMDIBBD PRRGA PQ GGDNON EED 8HHP 0HHG
--owuari-trinegw-ati 9rmd 0e-reward
100024 00 I Sn y: steIE mE sE 23T .1ra (n 2s 0a 2c 0t )io ,n ps p.o 1n 1–I 3n 2te .lligent Transportation
(b) AverageRewardComparison (c) FlowStabilityComparison [7] B. Abdulhai, R. Pringle, and G. J. Karakoulas. “Rein-
forcement learning for true adaptive traffic signal con-
Fig. 7: Performance Comparison on the Non-Euclidean Syn- trol”. In: Journal of Transportation Engineering 129.3
thetic Traffic Network (2003), pp. 278–285.
[8] W.GendersandS.Razavi.“Usingadeepreinforcement
learning agent for traffic signal control”. In: arXiv
inferior to that achieved using the reward function proposed preprint arXiv:1611.01142 (2016).
in this paper. [9] E. Van Der Pol. “Deep reinforcement learning for
coordinationintrafficlightcontrol”.In:Master’sthesis,
VI. CONCLUSION University of Amsterdam (2016).
[10] K. Behrendt, L. Novak, and R. Botros. “A deep learn-
Inthispaper,wehaveintroducedanovelmulti-agentactor-
ing approach to traffic lights: Detection, tracking, and
critic framework with an interpretable influence mechanism
classification”. In: 2017 IEEE International Confer-
based on the EHHNN. Specifically, we used the BReLU
ence on Robotics and Automation (ICRA). IEEE. 2017,
neural network as a function approximator for both the value
pp. 1370–1377.
and policy functions and thus construct the PWL-actor-critic
framework. Besides, the proposed influence mechanism based
on the EHHNN can capture the spatiotemporal dependencies
edosipE-reP-draweR
egarevA
)elacs-egral(
edosipE-reP-draweR
egarevA
)elacs-llams(
edosipE-reP-draweR
egarevA
)elacs-egral(
edosipE
reP
ytilibatS wolF ciffarT
)elacs-egral(
edosipE
reP ytilibatS
wolF ciffarT
)elacs-llams(
edosipE
reP
ytilibatS wolF ciffarT
)elacs-llams(
edosipE
reP ytilibatS
wolF ciffarT11
[11] J. Xu, Q. Tao, Z. Li, et al. “Efficient hinging hyper- national Conference on Computer and Communication
planes neural network and its application in nonlin- Systems (ICCCS). IEEE. 2021, pp. 645–650.
ear system identification”. In: Automatica 116 (2020), [28] Y. Wang, T. Xu, X. Niu, et al. “STMARL: A spatio-
p. 108906. temporal multi-agent reinforcement learning approach
[12] X. Liang and J. Xu. “Biased ReLU neural networks”. for cooperative traffic light control”. In: IEEE Transac-
In: Neurocomputing 423 (2021), pp. 71–79. tionsonMobileComputing21.6(2020),pp.2228–2242.
[13] E. Van der Pol and F. A. Oliehoek. “Coordinated [29] L. Breiman. “Hinging hyperplanes for regression, clas-
deep reinforcement learners for traffic light control”. sification,andfunctionapproximation”.In:IEEETrans-
In: Proceedings of Learning, Inference and Control of actions on Information Theory 39.3 (1993), pp. 999–
Multi-agent Systems (NIPS) 8 (2016), pp. 21–38. 1013.
[14] J.A.CalvoandI.Dusparic.“Heterogeneousmulti-agent [30] J. H. Friedman. “Multivariate adaptive regression
deep reinforcement learning for traffic lights control”. splines”.In:TheAnnalsofStatistics19.1(1991),pp.1–
In: AICS. 2018, pp. 2–13. 67.
[15] N. Casas. “Deep deterministic policy gradient for [31] B. Jiang and C. Claramunt. “A structural approach to
urban traffic light control”. In: arXiv preprint the model generalization of an urban street network”.
arXiv:1703.09035 (2017). In: GeoInformatica 8 (2004), pp. 157–171.
[16] T. Chu, J. Wang, L. Codeca`, and Z. Li. “Multi-agent [32] S. S. Mousavi, M. Schukat, and E. Howley. “Traffic
deepreinforcementlearningforlarge-scaletrafficsignal light control using deep policy-gradient and value-
control”. In: IEEE Transactions on Intelligent Trans- function-based reinforcement learning”. In: IET Intel-
portation Systems 21.3 (2019), pp. 1086–1095. ligent Transport Systems 11.7 (2017), pp. 417–423.
[17] T. Tan, F. Bao, Y. Deng, et al. “Cooperative deep [33] Q. Tao, Z. Li, J. Xu, et al. “Short-term traffic flow
reinforcementlearningforlarge-scaletrafficgridsignal prediction based on the efficient hinging hyperplanes
control”. In: IEEE Transactions on Cybernetics 50.6 neural network”. In: IEEE Transactions on Intelligent
(2019), pp. 2687–2700. Transportation Systems 23.9 (2022), pp. 15616–15628.
[18] T. N. Kipf and M. Welling. “Semi-supervised classi- [34] A.Bemporad,M.Morari,V.Dua,andE.N.Pistikopou-
fication with graph convolutional networks”. In: arXiv los. “The explicit linear quadratic regulator for con-
preprint arXiv:1609.02907 (2016). strainedsystems”.In:Automatica38.1(2002),pp.3–20.
[19] W. Hamilton, Z. Ying, and J. Leskovec. “Inductive [35] D. Guo, L. Tang, X. Zhang, and Y.-C. Liang. “Joint
representation learning on large graphs”. In: Advances optimization of handover control and power allocation
in Neural Information Processing Systems 30 (2017). based on multi-agent deep reinforcement learning”.
[20] P. Velicˇkovic´, G. Cucurull, A. Casanova, et al. In: IEEE Transactions on Vehicular Technology 69.11
“Graph attention networks”. In: arXiv preprint (2020), pp. 13124–13138.
arXiv:1710.10903 (2017). [36] J. Schulman, P. Moritz, S. Levine, et al. “High-
[21] V. Zambaldi, D. Raposo, A. Santoro, et al. “Rela- dimensional continuous control using generalized
tional deep reinforcement learning”. In: arXiv preprint advantage estimation”. In: arXiv preprint
arXiv:1806.01830 (2018). arXiv:1506.02438 (2015).
[22] P. W. Battaglia, J. B. Hamrick, V. Bapst, et al. “Rela- [37] M.Behrisch,L.Bieker,J.Erdmann,andD.Krajzewicz.
tional inductive biases, deep learning, and graph net- “SUMO–simulation of urban mobility: an overview”.
works”. In: arXiv preprint arXiv:1806.01261 (2018). In: Proceedings of SIMUL 2011, The Third Interna-
[23] J. Jiang, C. Dun, T. Huang, and Z. Lu. “Graph con- tional Conference on Advances in System Simulation.
volutional reinforcement learning”. In: arXiv preprint ThinkMind. 2011.
arXiv:1810.09202 (2018). [38] Y. Li, R. Yu, C. Shahabi, and Y. Liu. “Diffusion con-
[24] H. Chen, Y. Liu, Z. Zhou, et al. “Gama: Graph at- volutional recurrent neural network: Data-driven traf-
tention multi-agent reinforcement learning algorithm fic forecasting”. In: arXiv preprint arXiv:1707.01926
for cooperation”. In: Applied Intelligence 50 (2020), (2017).
pp. 4195–4205. [39] M. F. Niri, K. Liu, G. Apachitei, et al. “Machine learn-
[25] F.-X. Devailly, D. Larocque, and L. Charlin. “IG-RL: ing for optimised and clean Li-ion battery manufactur-
Inductive graph reinforcement learning for massive- ing: Revealing the dependency between electrode and
scale traffic signal control”. In: IEEE Transactions cell characteristics”. In: Journal of Cleaner Production
on Intelligent Transportation Systems 23.7 (2021), 324 (2021), p. 129272.
pp. 7496–7507.
[26] L. Yan, L. Zhu, K. Song, et al. “Graph cooperation
deep reinforcement learning for ecological urban traffic
signal control”. In: Applied Intelligence 53.6 (2023),
pp. 6248–6265.
[27] Z.Zeng.“GraphLight:graph-basedreinforcementlearn-
ing for traffic signal control”. In: 2021 IEEE 6th Inter-