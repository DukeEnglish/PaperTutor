Does differentially private synthetic data lead to
synthetic discoveries?
Ileana Montoya Perez Parisa Movahedi Valtteri Nieminen
Antti Airola Tapio Pahikkala
Department of Computing, University of Turku, Finland
Abstract
Background: Synthetic data has been proposed as a solution for sharing
anonymized versions of sensitive biomedical datasets. Ideally, synthetic data
should preserve the structure and statistical properties of the original data,
while protecting the privacy of the individual subjects. Differential privacy
(DP) is currently considered the gold standard approach for balancing this
trade-off.
Objectives: The aim of this study is to evaluate the Mann-Whitney U test
on DP-synthetic biomedical data in terms of Type I and Type II errors, in
order to establish whether statistical hypothesis testing performed on privacy
preserving synthetic data is likely to lead to loss of test’s validity or decreased
power.
Methods: We evaluate the Mann-Whitney U test on DP-synthetic data
generated from real-world data, including a prostate cancer dataset (n=500)
and a cardiovascular dataset (n=70 000), as well as on data drawn from two
Gaussian distributions. Five different DP-synthetic data generation methods
areevaluated,includingtwobasicDPhistogramreleasemethodsandMWEM,
Private-PGM, and DP GAN algorithms.
Conclusion: Most of the tested DP-synthetic data generation methods
showed inflated Type I error, especially at privacy budget levels of ϵ ≤ 1.
This result calls for caution when releasing and analyzing DP-synthetic data:
low p-values may be obtained in statistical tests simply as a byproduct of the
noise added to protect privacy. A DP smoothed histogram-based synthetic
datagenerationmethodwasshowntoproducevalidTypeIerrorforallprivacy
levels tested but required a large original dataset size and a modest privacy
budget (ϵ ≥ 5) in order to have reasonable Type II error levels.
1 Introduction
As the amount of health and medical data collected from individuals has grown, so
has the interest in using it for secondary purposes such as research and innovation.
Many benefits have been proposed to arise from sharing this data [1], for example,
enhancing research reproducibility, building on existing research, performing meta-
analyses, and reducing clinical trial costs by reusing existing data. However, privacy
concerns about the potential harm to individuals that may come from sharing their
sensitive data, along with legislation aimed at addressing these concerns, restrict
the opportunities for sharing individuals’ data.
1
4202
raM
02
]GL.sc[
1v21631.3042:viXraThe release of synthetic data, generated using a statistical model derived from
an original sensitive dataset, has been proposed as a potential solution for sharing
biomedical data while preserving individuals’ privacy [2, 3, 4]. It has been argued
that since synthetic data consists of synthetic records instead of actual records, and
synthetic records are not associated with any specific identity, privacy is preserved
[2]. However, it has been repeatedly demonstrated that this is not the case as
synthetic data is not inherently privacy-preserving [5, 6, 7, 8]. In the worst case, a
generativemodel could createnear copies ofthe originalsensitive datait was trained
on. Moreover, there are many more subtle ways that models can leak information
about their training data [5, 9]. At the other extreme, perfect anonymity is guaran-
teed only when no useful information from the original data remains. Therefore, in
addition to preserving privacy, the generated data should have high utility, meaning
the degree to which the inferences obtained from the synthetic data correspond to
inferences obtained from the original data [5, 10]. Consequently, when generating
synthetic data, it is essential to find a balance between the privacy and utility of
the data, ensuring that the generated data captures the primary statistical trends in
the original data while also preventing the disclosure of sensitive information about
individuals [11].
Differentialprivacy(DP),amathematicalformulationthatprovidesprobabilistic
guarantees on the privacy risk associated with disclosing the output of a computa-
tional task, has been widely accepted as the gold standard of privacy protection
[12, 13, 14, 15]. As a result, methods that ensure DP guarantees have been intro-
ducedinabroadrangeofsettings, includingdescriptivestatistics[13,16], inferential
statistics [17, 18, 19, 20], and machine learning applications [15, 21]. Furthermore,
DP offers a theoretically well-founded approach that provides probabilistic privacy
guarantees also for the release of synthetic data. Therefore, several methods for
releasing DP-synthetic data have been proposed (see e.g. [22, 23, 24, 25, 26]). Some
state-of-the-art methods for generating DP-synthetic data use multi-dimensional
histograms, which are standard tools for estimating the distribution of data with
minimal a priori assumptions about its statistical properties. Other methods are
basedonmachinelearning-basedgenerativemodels, forexample, BayesianandGen-
erative Adversarial Network (GAN) based methods. The aim of DP-synthetic data
is to be a privacy-preserving version of the original data that could be safely used
in its place, requiring no expertise on DP or changes to the workflow from the end-
user. However, DP-synthetic data is always a distorted version of the original data,
and especially when high levels of privacy are enforced the level of distortion can
be quite considerable. Even though combining DP with synthetic data guarantees
a desired level of privacy, preservation of the utility remains unclear. In particular,
the validity of statistical significance tests, namely the statistical guarantees of the
false finding probabilities being at most the significance level, may be lost.
Hypothesis tests are essential statistical tools in analyzing biomedical data.
Mann-Whitney (MW) U test (a.k.a. Wilcoxon rank-sum test or Mann-Whitney-
Wilcoxon test) is the de facto standard for testing whether two groups are drawn
from the same distribution [27, 28]. It is widely applied in medical research [29], for
example, when analyzing a biomarker between non-healthy and healthy patients in
clinical trials. It is well known (see e.g. [30]) that MW U test is valid for this ques-
tion, that is, the probability of falsely rejecting the null hypothesis of the two groups
being drawn from the same distribution is at most the significance level determined
a priori.
InorderforDP-syntheticdatatobeusefulforbasicusecasesinmedicalresearch,
2such as the MW U test, one would hope to observe roughly similar results when
carrying out tests on sensitive medical datasets. Otherwise, there is a risk that
discoveries are missed because of information lost in synthetization, or worse, that
false discoveries are made due to artifacts introduced in the data generation process.
2 Objectives
DP-syntheticdatahasbeenproposedasasolutionforpubliclyreleasinganonymized
versions of sensitive data such as medical records. Ideally, this would allow for per-
forming reliable statistical analyses on the DP-synthetic data without ever needing
to access the original data (see Figure 1). However, there is a risk that DP-synthetic
data generation methods distort the original data in ways that can lead to loss of
information and even to false discoveries.
In this study, we empirically evaluate the validity and power of the MW U test
applied to DP-synthetic data in terms of Type I and Type II errors, respectively.
On one hand, a test is valid if, for any significance level, the probability that it
falsely rejects a correct null hypothesis is no larger than the significance level [31].
If the test is not valid, its use can lead to false scientific discoveries, and hence its
practical utility can be even negative. On the other hand, the test’s power refers to
the probability of correctly rejecting a false null hypothesis.
In our experiments, we test five different DP-synthetic data generation methods
on two real-world medical datasets, as well as data drawn from Gaussian distribu-
tions. Our study contributes to understanding the reliability of statistical analysis
when DP-synthetic data is utilized as a proxy for private data whose public release
is challenging or even impossible.
CURATOR (Private) ANALYST (Public)
MW-U test
DP-
Synth P-value
Dataset
DP-synthesizer
Figure 1: The overall configuration of the study.
3 Methods
In this section, we first present the formal definition of DP. Next, we describe the
five DP-synthetic data generation methods used in this study. Finally, we explain
the Mann-Whitney U test and how its validity and power can be measured in terms
of Type I and Type II errors.
3.1 Differential Privacy
DP is a mathematical definition that makes it possible to quantify privacy [12, 13].
A randomized algorithm M satisfies (ϵ,δ)-differential privacy if for all outputs S of
M and for all possible neighboring datasets D,D′ that differ by only one row,
Pr[M(D) = S] ≤ eϵPr[M(D′) = S]+δ (1)
3where ϵ is an upper bound on the privacy loss, and δ is a small constant correspond-
ing to a small probability of breaking the DP constraints. For δ = 0 in particular,
solving (1) wrt. ϵ results to:
log(Pr[M(D) = S])−log(Pr[M(D′) = S]) ≤ ϵ (2)
indicating that the log-probability of any output can change no more than ϵ. Ac-
cordingly, an algorithm M which is ϵ-DP guarantees that, for every run of M(D)
the outcome obtained is almost equally likely to be obtained on any neighboring
dataset, bounded by the value of ϵ. Informally, in DP, privacy is understood to
be protected at a given level of ϵ if the algorithm’s output does not overly depend
on the input data of any single contributor; it should yield a similar result if the
individual’s information is present in the input or not.
Typically, DP methods are non-private methods that are transformed to fulfill
the DP definition. This is achieved by adding noise using a noise mechanism cal-
ibrated based on the ϵ and the algorithm to be privatized [12, 13]. Choosing the
appropriate value of epsilon is context-specific and an open question, but, for exam-
ple, ϵ ≤ 1 has been proposed to provide a strong guarantee [32], while 1 < ϵ ≤ 10
is considered to still produce useful privacy guarantees [33], depending on the task
and type of data.
3.2 DP Methods for Synthetic Data Generation
In recent years, several methods for generating DP-synthetic data have been pro-
posed [23, 24, 26, 34, 35]. Some of the proposed methods are based on histograms
or marginals. These methods privatize the cell counts or proportions of a cross-
tabulation of the original sensitive data to generate the DP-synthetic data. Other
methods use a parameterized distribution or a generative model that has been pri-
vately derived from the original data. While DP methods based on histograms or
marginals have been found to produce usable DP-synthetic data with a reasonable
level of privacy guarantee, methods based on parameterized distributions or deep
learning-based generative models have presented greater challenges [36, 37].
In this study, we focus on five well-known DP methods for generating synthetic
data. These methods have established algorithms or available packages, making
them accessible to any practitioner. Following, we provide a brief description of
each of these DP methods.
• DP Perturbed Histogram
This method uses the Laplace mechanism [13] to privatize the original his-
togram bin counts. The noise added to each bin is sampled separately from
a calibrated Laplace distribution. After adding the noise, all negative counts
are set to zero, and individual-level data is generated from the noisy counts.
• DP Smoothed Histogram
This method generates synthetic data by randomly sampling from the prob-
ability distribution determined by the following histogram. The probabilities
of the histogram bins are proportional to c +2m/ϵ, where c is the number of
i i
original data points in the ith histogram bin and m is the size of the synthetic
dataset drawn. The approach is similar to the one discussed by Wasserman
and Zhou [14]. Unlike the other considered DP methods, the utility of this
method is inversely proportional also to the amount of synthetic data drawn.
4Therefore, in our experiments, we use the method only in settings where the
size of the synthetic data generated is considerably smaller than that of the
original sensitive data. A proof of the approach being DP is presented in the
supplementary material A.1.
• Multiplicative Weights Exponential Mechanism (MWEM)
This algorithm proposed by Hardt et al.[22] is based on a combination of
the multiplicative weights update rule with the exponential mechanism. The
MWEM algorithm estimates the original data distribution using a DP iter-
ative process. Here, a uniform distribution over the variables of the original
data is updated using the multiplicative weighting of a query or bin count
selected through the exponential mechanism and privatized with the Laplace
mechanism in each iteration. The privacy budget ϵ is split by the number of
iterations, as in every iteration the original data needs to be accessed.
• Private-PGM
McKenna et al.[26] propose this approach for DP-synthetic data genera-
tion. It consists of three basic steps: 1) Selecting a set of low-dimensional
marginals (i.e., queries) from the original data. 2) Adding calibrated noise
to the marginals. 3) Generating synthetic data that best explains the noisy
marginals. In step 3, based on the noisy marginals, a probabilistic graphical
model (PGM) is used to determine the data distribution that best captures
the variables’ relationship and enables synthetic data generation.
• Differentially private GAN (DP GAN)
Generative adversarial networks (GAN) [38] consist of a generator, denoted
with G, and one or more discriminators D. The goal is, that G would learn
to produce synthetic data similar to the original data. The two networks
are initialized randomly and trained iteratively in a game-like setup: G is
fed noise to create synthetic data, which the D tries to discriminate as being
original or synthetic. The generator uses feedback from the discriminator(s)
to update its parameters via gradient descent (for a detailed explanation see
[39]). GANs, and other deep learning models, can attain privacy guarantees
by using a DP version of an optimization algorithm, most often differentially
private stochastic gradient descent (DPSGD) [33].
3.3 Mann Whitney U test
TheMWUtestisastatisticaltestfirstproposedbyFrankWilcoxon[40]in1945and
later, in 1947, formalized by Henry Mann and Donald Whitney [41]. While there are
many different uses and interpretations of the test (see e.g. [30] for a comprehensive
review), in this paper we focus on the null hypothesis that two samples or groups are
drawn from the same distribution. The test carried out on two groups produces a
valueoftheMWUstatisticandthecorrespondingp-value. TheUstatisticmeasures
the difference between the groups as the number of times an observed member of
the first group is smaller than that of the second group, ties being counted as a half
time. The p-value indicates the strength of evidence the value of the U statistic
provides against the null hypothesis, given that the assumption of the data being
independently drawn holds. Following [31], we say that p-value is valid, if it is at
mosttheprobabilityofobservingasextremedifferencebetweenthegroupsastheone
5Real distribution Synthetic data
TN
F
P
X X
N
F
TP
X X
Figure 2: Possible outcomes of a hypothesis test that tests whether two distributions are
the same. TN: true negative, TP: true positive, FP: false positive (Type I error), FN: false
negative (Type II error).
obtained, for any distribution of the data under the null hypothesis. Consequently,
the significance test is valid if its p-value is valid, indicating that the probability of
falsely rejecting the null hypothesis is at most the significance level.
The utility of the MW U test on DP-synthetic data can be evaluated in terms
of Type I and Type II errors. Let us recall that Type I is the error incurred when
a “True” null hypothesis is rejected, producing false inference. On the other hand,
Type II is the error of failing to reject a “False” null hypothesis (see Figure 2). A
priori selected significance level α defines a threshold that, for any valid hypothesis
test, forms an upper bound on the probability of committing Type I error. A typical
choice for α is 0.05, indicating a maximum 5% chance of incorrectly rejecting a true
null hypothesis. The probability of making a Type II error is often denoted as β
(beta), from which the power of the test can be determined by computing 1 − β.
The power of a test can be interpreted as the probability of correctly rejecting a null
hypothesis when it is in fact ”False”. The power depends on the analysis task, being
affected by factors such as chosen significance level, sample size, and the effect size.
Couch et al. [19] proposed a differentially private version of the Mann-Whitney
U test (DP-MW). The DP-MW U test is presented as (ϵ, δ)-DP, where a portion
of the privacy budget ϵ, and δ are used for privatizing the smallest group size. The
privatized size and the rest of ϵ are then used for privatizing the U statistic using
a calibrated Laplace distribution. In order to calculate the corresponding p-value,
the DP-MW U distribution under the null hypothesis is generated based on the
privatized group sizes. Detailed information and algorithms are provided by Couch
et al. [19]. The DP-MW U test is not based on analyzing synthetic data, but rather
the test is carried out directly on the original sensitive dataset, and DP guarantees
that sensitive information about individuals is not leaked when releasing the test
results.
Inthisstudy, theDP-MWUtestontheoriginalsensitivedataprovidesuswitha
reference point, a valid test with the best-known achievable power when performing
MW U test under DP. In contrast, the ordinary MW U test is evaluated on the DP-
synthetic data. If the validity of the ordinary test is preserved, comparison to the
reference point indicates how much power is lost when general purpose DP-synthetic
data is generated as an intermediate step.
6
ytisneD
ytisneD
ycneuqerF
ycneuqerF4 Experimental Evaluation
To empirically evaluate the utility of the MW U test on synthetic data generated
by various DP generation methods, we performed a set of experiments using DP-
synthetic datasets generated from both real-world datasets and data drawn from
Gaussian distributions. As depicted by real distribution in Figure 2, we considered
two cases for Gaussian data: one where both groups are drawn from the same
distribution (i.e., the null hypothesis is true) and one where they are drawn from
distributions with different means (i.e., the null hypothesis is false). In the overall
study design (see Figure 1), the real-world and Gaussian datasets correspond to
the sensitive data given as input to a DP-synthesizer method that produces a DP-
synthetic dataset as output. In the following subsections, we present the datasets,
the implementation details of the DP-synthetic data generation methods used, and
the experiments conducted.
4.1 Original Datasets
Alltheoriginaldatasetsusedinourexperimentsconsistoftwovariables(i.e.,binary-
and continuous variable). The binary variable represents the label of two groups
(e.g., healthy and non-healthy), while the continuous variable is the one used to
compare the groups with the MW U test.
To establish a controlled environment where the amount of signal (i.e., the effect
size) in the population is known, we drew two groups of data from two Gaussian
distributions with a known mean (µ) and standard deviation (σ). More precisely,
for non-signal data, which corresponds to a setting where the null hypothesis is
true, the two groups were randomly drawn from the same Gaussian distribution
(µ = 50,σ = 2). For the signal data, which corresponds to a setting where the null
hypothesis is false, two Gaussian distributions with effect size µ − µ = σ (i.e.,
1 2
µ = 51,σ = 1,µ = 50,σ = 1) were used to sample each group. Additionally, for
1 1 2 2
those DP methods based on histograms or marginals, the sampled values for each
group were discretized into 100 bins (ranging from 1 to 100).
In order to verify our experiment’s results on the Gaussian data, we also carried
out experiments using real-world medical data. In this case, we use the following
two datasets:
• The Prostate Cancer Dataset
The data is from two registered clinical trials, IMPROD [42] and MULTI-
IMPROD [43], with trial numbers NCT01864135 and NCT02241122, respec-
tively. These trials were approved by the Institutional Review Board, and
each enrolled patient gave written informed consent. The dataset consists of
500 prostate cancer (PCa) patients (242 high-risk and 258 benign/low-risk
PCa) with clinical variables, blood biomarkers, and MRI features. For our
experiments, we selected two variables: a binary label that indicates the con-
dition of the patient and the prostate-specific antigen (PSA) level. The PSA
is a continuous variable that has been associated with the presence of PCa
[44, 45]. Therefore, in this study, we considered the null hypothesis under test
to be “The PSA level of high-risk and benign/low-risk PCa patients originate
from the same distribution.” Figure 3(a) presents the PSA distribution for
both groups in this dataset. In those DP methods based on histograms or
marginals, the PSA values were discretized using a 40 bins histogram (ranging
from 1 to 40, where PSA ≥ 40 are in the last bin).
7• Kaggle Cardiovascular Disease Dataset
This dataset is publicly available [46] and consists of 70 000 subjects and 12
variables, wherethetargetvariableisthecardioconditionofthesubjects, with
34 979 presenting cardiovascular disease and 35 021 without the disease. For
our experiments, we use each subject body mass index (BMI), calculated from
their weight and height, which has been related to cardiovascular conditions
[47]. Here, the null hypothesis under test is “The BMI level for individuals
with the presence of cardiovascular disease and the ones with absence cardio-
vascular disease originate from the same distribution.” Figure 3(b) presents
the distribution of both groups (i.e., cardio disease vs. no cardio disease). The
BMI variable was discretized into 24 bins, where the first bin contains BMI <
18 and the last bin BMI ≥ 40, in those DP methods that require it.
a) Prostate Cancer Dataset b) Cardiovascular Disease Dataset
50 10000
Benign/low PCa No Cardio disease
High PCa Cardio disease
40 8000
Mean Mean
Median Median
30 6000
20 4000
10 2000
0 0
50 10000
40 8000
30 6000
20 4000
10 2000
0 0
0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 80 90 100
PSA level BMI level
Figure 3: a) Prostate cancer (PCa) dataset: prostate-specific antigen (PSA) level distri-
bution for high-risk and benign/low PCa. The difference between the groups is statisti-
cally significant (MW U stat= 22713, p-value= 1.4e-07), b) Kaggle Cardiovascular disease
dataset: body mass index (BMI) distribution for subjects with the absence and presence
of cardio disease. The difference between the groups is statistically significant (MW U
∼
stat= 471500929.50, p-value = 0.000).
4.2 Implementations
In our experiments, for all generated DP-synthetic data, the “two-sided” MW U
test from SciPy (v1.6.3) package was used to compute their U statistic and p-value.
As a point of reference, we also computed the DP-MW U statistic and p-value on
the corresponding original sensitive dataset. The DP-MW U test was implemented
using Python v3.7 and following the algorithms presented in [19], where 65% of ϵ
and δ = 10−6 are used for estimating the size of the smallest group, and the U
statistic is privatized using the estimated size and the remaining ϵ.
8In the case of the DP Perturbed Histogram, Python v3.7 was also used in the
implementation. The noise, added to the original histogram, was sampled from a
discrete Laplacian distribution [48] scaled by 2, then the noisy counts were nor-
ϵ
malized by the original dataset size. After that, the synthetic data was obtained
by transforming the histogram counts to values using the bin center point. For
Private-PGM [49] and MWEM [50], their corresponding open-source packages were
used to generate DP-synthetic data. The Private-PGM synthetic data was gener-
ated by following the demonstration in Python code presented by McKenna et al.
[26] using Laplace distribution scaled by 2 where split(ϵ) is the privacy bud-
split(ϵ)
get (ϵ) divided by the number of marginal queries selected. MWEM was run with
default hyperparameters; only ϵ was changed to show the effect of different privacy
budgets. The resulting DP-synthetic data was sampled using the histogram noisy
weights returned by the MWEM algorithm. The implementation of DP Smoothed
Histogram was also coded in Python v.3.7 following Algorithm 1 provided in the
supplement material.
The GAN model used is based on the GS-WGAN by Chen et al. [25]. The im-
plementation is a modification of the freely available source code [51], with changes
made to suit tabular data generation instead of images. The generator architecture
was changed from a convolutional- to a fully-connected three-layer network, and the
gradient perturbation procedure was modified to accommodate these changes along
with making the source code compatible with an up-to-date version of PyTorch
(v1.10.2) [52]. Hyperparameter settings were chosen based on the recommendations
of Gulrajani et al. on the WGAN-GP [53], which of the GS-WGAN is a DP ex-
tension. This model uses privacy amplification by subsampling [25], a strategy to
achieve stronger privacy guarantees by splitting training data into mutually exclu-
sive subsets according to a subsampling rate γ. Each subset is used to train one
discriminator and the generator randomly queries one discriminator for one update.
4.3 Experimental Setup
In the experiments, we investigated the utility of the MW U test at different levels
of privacy ϵ. For the DP-MW U test and all DP-synthetic data generation methods,
except for DP GAN, we used ϵ values of 0.01, 0.1, 1, 5, and 10. For the DP GAN
experiments, the ϵ values were 1, 2, 3, 4, 5, and 10. The higher minimum of ϵ = 1
was set due to differences between the DP GAN and the other methods. Every
experiment was repeated 1000 times, and the proportion of Type I and Type II
errors were computed and evaluated at a significance level α = 0.05.
Setup for Gaussian Data
In our experiments on Gaussian data using the DP-MW U test, DP Perturbed
Histogram, Private-PGM, and MWEM, each method was applied to original dataset
sizes of 50, 100, 500, 1000, and 20 000 with a group ratio of 50% and at the different
values of ϵ. In these methods, the original dataset size was considered to be of public
knowledge, thus, the size of the generated DP-synthetic dataset was around or equal
to the original size.
Experiments with DP Smoothed Histogram were performed by randomly sam-
pling original Gaussian dataset of large size (i.e., dataset size of 20 000 with a group
ratio of 50%). Then, the method was applied using the different values of ϵ, and for
every ϵ synthetic data of size 50, 100, 500, and 1000 were generated using the noisy
9probabilities returned by the method.
In all experiments with the GAN discriminator networks a subsampling rate γ,
of 1/500 was used, resulting in mutually exclusive subsets of size 40. The sample size
for the GAN training data was 20 000 in all settings and 1000 different generators
were trained with models saved at the chosen values of ϵ (1, 2,3, 4, 5, and 10).
Five synthetic datasets of sizes 50, 100, 500, and 1000 were sampled from each of
the generators and MW-U tests were conducted on each of these synthetic datasets
separately. The DP-hyperparameters were all set to C = 1 for the gradient clipping
bound and 1.07 for the noise multiplier as in [25].
A summary of the settings for the experiments with original Gaussian data is
provided in Table 1.
Original Dataset Size DP-synthetic
DP Method Privacy Budget
Group ratio 50% Dataset Size
DP-MW U test 50, 100, 500, 1000, 20 000 N/A ϵ=0.01,0.1,1,5,10
DP Perturbed Histogram
Similar to the
Private-PGM 50, 100, 500, 1000, 20 000 ϵ=0.01,0.1,1,5,10
original dataset
MWEM
DP Smoothed Histogram 20 000 50, 100, 500, 1000 ϵ=0.01,0.1,1,5,10
DP GAN 20 000 50, 100, 500, 1000 ϵ=1,2,3,4,5,10
Table 1: Setup for experiments using original Gaussian data. For the DP-MW U
test, DP-synthetic dataset size is not applicable (’N/A’), because this method is
computed on the original sensitive data.
Setup for Real-World Data
The size of the prostate cancer dataset constrained some of the experiments. There-
fore, DP Smoothed Histogram and DP GAN experiments with this dataset were
excluded, as these methods require a larger original dataset size (i.e., thousands
of observations) to apply them accurately. On the other hand, the cardiovascular
dataset size allowed us to carry out experiments with all the DP methods.
In the experiments with the prostate cancer dataset, we applied each considered
DP method at each epsilon value 1000 times. While in the cardiovascular dataset
experiments, we used the data to sample 1000 original datasets for each dataset
size 50, 100, 500, 1000, and 20 000; then, for each sampled dataset, we applied the
DP methods at each epsilon. The proportion of Type II error was measured over
the 1000 repetitions for each experiment setting. For DP Smoothed Histogram and
DP GAN, due to their nature, the experiments were performed differently; however,
they had a similar setting to the ones with Gaussian signal data.
5 Results
5.1 Gaussian Data
In Figure 4(a), experiments on Gaussian non-signal data (i.e., both groups originate
from the same Gaussian distribution) show that when the DP-MW U test is applied
to the 1000 datasets, the proportion of Type I stays close to α = 0.05 for all dataset
sizes at all ϵ. Meanwhile, the MW U test on DP-synthetic data from DP Perturbed
Histogram, Private-PGM, and MWEM have a high proportion of Type I error for
10ϵ < 5, falsely indicating a significant difference between the two groups. From these
DP methods, DP Perturbed Histogram and Private-PGM benefit of having a large
original dataset size (i.e., 20 000), as ϵ can be reduced to 1 while still having a Type
I error close to α = 0.05. MWEM is the method with the worst performance as the
proportion of Type I error for all sample sizes stays above 0.05 even for ϵ = 10.
Private Mann-Whitney U test
a) Non-Signal Data
DP-MW U test DP Perturbed Histogram Private-PGM MWEM
1.0 1.0 1.0 1.0
Dataset size
0.85 0.85 0.85 0.85 50
100
0.65 0.65 0.65 0.65 500
1000 0.45 0.45 0.45 0.45 20000
0.25 0.25 0.25 0.25
0.05 0.05 0.05 0.05
0.01 0.1 1 5 10 0.01 0.1 1 5 10 0.01 0.1 1 5 10 0.01 0.1 1 5 10
Epsilon ( ) Epsilon ( ) Epsilon ( ) Epsilon ( )
b) Signal Data
DP-MW U test DP Perturbed Histogram Private-PGM MWEM
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.01 0.1 1 5 10 0.01 0.1 1 5 10 0.01 0.1 1 5 10 0.01 0.1 1 5 10
Epsilon ( ) Epsilon ( ) Epsilon ( ) Epsilon ( )
Figure 4: The proportion of Type I and Type II errors for the Mann-Whitney U test
using four differentially private (DP) methods: DP-MW U test, DP Perturbed Histogram,
Private-PGM, and MWEM at different privacy budget (ϵ). The dataset size indicates the
size of the original data used in the experiments by the DP methods. The proportions
of Type I error and Type II error were measured over 1000 repetitions of the experiment
using Gaussian a) non-signal data and b) signal data, respectively.
Figure 4(b) presents the results for Gaussian signal data where a difference be-
tweenthetwogroupsexists(i.e., normallydistributeddataoftwogroupswithmeans
one standard deviation apart). From these results, we observed that the MW U test
Type II error for all the DP methods, with low ϵ, can be reduced by increasing
the dataset size, corroborating the trade-off that exists between privacy, utility, and
dataset size.
Results for the MW U test on DP-synthetic data from DP Smoothed Histogram
and DP GAN are presented in Figure 5. The DP Smoothed Histogram method
controls the Type I error reliably. However, the price for this is that in most of
our experiment settings, it has high Type II error, meaning that the real difference
between the groups present in the original data is lost in the DP-synthetic data
generation process. DP GAN shows very high Type I error that as an interesting
contrast to the other methods grows as privacy level is reduced.
To summarize, these results show that except for DP Smoothed Histogram, all
the DP-synthetic data generation methods have highly inflated Type I error. This
means that they are prone to generating data from which false discoveries are likely
to be made. For the histogram-based methods, increased Type I error was asso-
ciated with increased level of privacy, the effect being especially clear for ϵ < 5.
Figure 6 presents an example of false discovery on synthetic data generated with
the DP Perturbed histogram at ϵ = 0.1, and also demonstrates how DP Smoothed
Histogram does not exhibit the same behavior.
11
noitroporP
rorrE
I epyT
noitroporP
rorrE
II
epyTPrivate Mann-Whitney U test
Original Dataset Size = 20 000
a) Non-Signal Data
DP Smoothed Histogram DP GAN
1.0 1.0
DP-synthetic data size
0.85 0.85 50
100
0.65 0.65 500
1000
0.45 0.45
0.25 0.25
0.05 0.05
0.01 0.1 1.0 5 10 1 2 3 4 5 10
Epsilon ( ) Epsilon ( )
b) Signal Data
DP Smoothed Histogram DP GAN
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.01 0.1 1.0 5 10 1 2 3 4 5 10
Epsilon ( ) Epsilon ( )
Figure 5: The proportion of Type I and Type II error of MW U test applied to synthetic
data generated from DP Smoothed Histogram and DP GAN. The size of the original
dataset is 20 000 with a group ratio of 50%. DP-synthetic data of sizes 50, 100, 500, and
1000 were generated from both methods. The proportions of Type I error and Type II
error were measured over 1000 DP-synthetic datasets using Gaussian a) non-signal data
and b) signal data, respectively.
5.2 Real-world Data
Figure 7(a) shows the results of experiments conducted with the prostate cancer
dataset. The DP-MW U test performs as expected for an original dataset size of
500 with a group ratio of approx. 50%. The effect of the signal is detected for
ϵ ≥ 1, while for ϵ < 1 the effect is often not detected. Similar behavior is present
in DP-synthetic data from DP Perturbed Histogram and MWEM, yet the chance
of detecting the effect when ϵ < 1 is higher than in the DP-MW U test. In DP-
synthetic data from Private-PGM, the signal effect is detected for ϵ ≥ 5, with a
lower chance of detection for ϵ < 5.
The experiment results for the DP-MW U test, DP Perturbed Histogram,
Private-PGM, and MWEM applied to the Cardiovascular disease dataset are pre-
sented in Figure 7(b). In this dataset, we observe that MWEM and Private-PGM
are the methods that benefit the most from increasing the original sample size, as
stronger privacy guarantees can be provided without the MW U test losing power.
These results agree with the ones obtained when using Gaussian signal data.
Results for DP Smoothed Histogram and DP GAN applied to the cardiovascular
dataset are presented in Figure 8. With DP Smoothed Histogram, Type II error is
on an acceptable level when ϵ ≥ 5 and the sample size is 500 or 1000, whereas for
lower ϵ values the effect is not found. DP GAN results have lower Type II error,
but given how high Type I error the method shows in the non-signal experiments
the approach is less reliable compared to the DP Smoothed Histogram method.
12
noitroporP
rorrE
I
epyT
noitroporP
rorrE
II
epyT
noitroporP
rorrE
I
epyT
noitroporP
rorrE
II
epyTOriginal Histogram DP Perturbed Histogram ( =0.1) DP Smoothed Histogram ( =0.1)
100 Control group
Case group
80 Mean
Median
60
40
20
0
100
80
60
40
20
0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Figure 6: Example of the two groups distributions in a non-signal original dataset of size
500(Ustat=31460.5p-value=0.8953)andcorrespondingdistributionsforsyntheticdata
generated using DP Perturbed Histogram (MW U stat = 38191.5 p-value = 0.00001774)
and DP Smoothed Histogram (MW U stat = 29621.5 p-value = 0.3314) with ε = 0.1 as
the privacy budget. With such high level of privacy enforced neither of the DP-synthetic
datasets preserve well the structure of the original data. However, the DP Perturbed
Histogram has the tendency to create artificial differences between the two groups such
that result in low p-values for MW U test, whereas with DP Smoothed Histogram method
both the generated case and control groups follow similar close to uniform distributions.
a)Prostate Cancer PSA levels data
1.0
Private Methods
0.8 DP-MW U test
0.6 DP Perturbed Histogram
Private-PGM
0.4 MWEM
0.2
0.0
0.01 0.1 1 5 10
Epsilon ( )
b)Cardiovascular disease BMI level data
DP-MW U test DP Perturbed Histogram Private-PGM MWEM 1.0 1.0 1.0 1.0
Dataset size
0.8 0.8 0.8 0.8 50
100
0.6 0.6 0.6 0.6
500
0.4 0.4 0.4 0.4 1000
20000
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.01 0.1 1 5 10 0.01 0.1 1 5 10 0.01 0.1 1 5 10 0.01 0.1 1 5 10
Epsilon ( ) Epsilon ( ) Epsilon ( ) Epsilon ( )
Figure 7: The proportion of Type II error for the Mann-Whitney U test using four DP
methods: DP-MW U test, DP Perturbed Histogram, Private-PGM and MWEM applied
to a) the PSA level data in the prostate cancer dataset (dataset size = 500), b) the body
mass index (BMI) data in the Kaggle cardiovascular disease dataset.
13
noitroporP
rorrE
II
epyT
noitroporP
rorrE
II
epyTPrivate Mann-Whitney U test on BMI data
Original Dataset Size = 20 000 (group ratio = 50%)
DP Smoothed Histogram DP GAN
1.0 1.0
DP-synthetic data size
50
0.8 0.8 100
500
0.6 0.6 1000
0.4 0.4
0.2 0.2
0.0 0.0
0.01 0.1 1.0 5 10 1 2 3 4 5 10
Epsilon ( ) Epsilon ( )
Figure 8: The proportion of Type II error of MW U test applied to synthetic data
generated with DP Smoothed Histogram and DP GAN. The original dataset of size 20
000withagroupratioof50%wasdrawnfromthepubliclyavailableCardiovasculardisease
dataset. DP-synthetic data of sizes 50, 100, 500, and 1000 were generated using both DP
methods 1000 times.
6 Discussion
This study investigated to what extent the validity and power of the MW U test
are preserved in DP-synthetic data. The experimental results on both Gaussian
and real-world data demonstrate, that DP-synthetic data, especially with strong
privacy guarantees (ϵ ≤ 1) can lead to false discoveries, specifically in the case of
the MW U test. We empirically show that many state-of-the-art DP methods for
generating synthetic data have highly inflated Type I error when the privacy level
is high. These results indicate, that false discoveries or inferences are likely to be
drawn from the DP-synthetic data produced by these DP methods. Our findings
are in line with other studies that have presented or stated that DP-synthetic data
can be invalid for statistical inference and indicated the need for methods that are
noise-aware in order to produce accurate statistical inferences [17, 54, 55, 56, 57].
Additionally, it is necessary to be cautious when analyzing Type II error results,
asthisisonlymeaningfulforvalidtestswheretheTypeIerrorisproperlycontrolled.
The Type II error tends to decrease with the increase of Type I error, as these errors
are inversely related. In our study, the only DP method based on synthetic data
generation that had a valid Type I error over all the privacy budgets tested was
the DP Smooth Histogram method. However, the method is applicable only when
the original dataset size is fairly large (e.g. n= 20 000 in our experiments), and
tended to have high Type II error when the amount of privacy enforced was high
(e.g. ϵ ≤ 1). For DP Perturbed Histogram and Private-PGM methods both Type I
and Type II errors remained low for ϵ ≥ 5, whereas MWEM and DP GAN did not
provide valid Type I error levels even with lowest privacy values tested.
There are several limitations in our study that could be addressed in future
research. Onelimitationisthatwegenerateddatasetswithonlytwovariables, which
isaverysimplecase. However, evenwiththissimplecase, weempiricallyshowedthe
risk of false inference when using DP-synthetic data. Increasing the dimensionality
of the dataset will increase the complexity of the generation process, which may
result in a higher risk of false inference. Another limitation is that marginals or
histograms based DP methods require continuous variables to be discretized. This
discretization must be performed in a private matter or based on literature to avoid
leaking private information. Besides, it is well-known that the number of bins used
14
noitroporP
rorrE
II
epyT
noitroporP
rorrE
II
epyTto discretize the data has a significant impact on the quality of the resulting data
[16, 58]. Therefore, choosing the number of bins is problem- and data-dependent
and can affect the results. In our experiments with Gaussian data, the continuous
values were discretized using 100 bins. This number of bins was selected to show a
possible extreme case where having bins empty or with small counts deteriorates the
quality of the generated DP-synthetic data. On the other hand, for our experiments
with real-world data, the number of bins used was based on literature. Finally,
testing different hyperparameter values for the DP method implementations could
yield different results for the methods.
7 Conclusions
Our results suggest caution when releasing DP-synthetic data, as false discoveries
or loss of information are likely to happen especially when a high level of privacy
is enforced. To an extent, these issues may be mitigated by having large enough
originaldatasets, selectingmethodsthatarelesspronetoaddingfalsesignaltodata,
and by carefully comparing the quality of the DP-synthetic data to the original one
based on various quality metrics (see e.g. [4]) before data release. Still, with current
methods DP-synthetic data may be a poor substitute for real data when performing
statistical hypothesis testing, as one cannot be sure if the results obtained are based
on trends that hold true in the real data, or due to artefacts introduced when
synthetizing the data.
8 Acknowledgements
This work has received funding from Business Finland (grant number
37428/31/2020) and European Union’s Horizon Europe research and innovation
programme (grant number 101095384).The authors would like to express their grat-
itude to Peter J. Bostr¨om, Ivan Jambor, and collaborators for their support and
contribution in providing the prostate cancer datasets used in the real-world data
experiments.
References
[1] K. El Emam, S. Rodgers, and B. Malin, “Anonymising and sharing individual
patient data,” bmj, vol. 350, 2015.
[2] D. B. Rubin, “Statistical disclosure limitation,” Journal of official Statistics,
vol. 9, no. 2, pp. 461–468, 1993.
[3] R. J. Chen, M. Y. Lu, T. Y. Chen, D. F. Williamson, and F. Mahmood, “Syn-
theticdatainmachinelearningformedicineandhealthcare,”Nature Biomedical
Engineering, vol. 5, no. 6, pp. 493–497, 2021.
[4] M. Hernadez, G. Epelde, A. Alberdi, R. Cilla, and D. Rankin, “Synthetic tab-
ular data evaluation in the health domain covering resemblance, utility, and
privacy dimensions,” Methods of Information in Medicine, 2023.
15[5] J. Jordon, L. Szpruch, F. Houssiau, M. Bottarelli, G. Cherubin, C. Maple, S. N.
Cohen, and A. Weller, “Synthetic Data–what, why and how?,” arXiv preprint
arXiv:2205.03257, 2022.
[6] D. Chen, N. Yu, Y. Zhang, and M. Fritz, “GAN-leaks: a taxonomy of member-
ship inference attacks against generative models,” in Proceedings of the 2020
ACM SIGSAC conference on computer and communications security, pp. 343–
362, 2020.
[7] J. Hayes, L. Melis, G. Danezis, and E. De Cristofaro, “Logan: Membership
inference attacks against generative models,” arXiv preprint arXiv:1705.07663,
2017.
[8] T. Stadler, B. Oprisanu, and C. Troncoso, “Synthetic data–anonymisation
groundhog day,” in 31st USENIX Security Symposium (USENIX Security 22),
pp. 1451–1468, 2022.
´
[9] N.Carlini, C.Liu, U.Erlingsson, J.Kos, andD.Song, “Thesecretsharer: Eval-
uating and testing unintended memorization in neural networks.,” in USENIX
Security Symposium, vol. 267, 2019.
[10] A. F. Karr, C. N. Kohnen, A. Oganian, J. P. Reiter, and A. P. Sanil, “A
framework for evaluating the utility of data altered to protect confidentiality,”
The American Statistician, vol. 60, no. 3, pp. 224–232, 2006.
[11] M. Boedihardjo, T. Strohmer, and R. Vershynin, “Covariance’s loss is privacy’s
gain: Computationally efficient, private and accurate synthetic data,” Founda-
tions of Computational Mathematics, pp. 1–48, 2022.
[12] C.Dwork,F.McSherry,K.Nissim,andA.Smith,“Calibratingnoisetosensitiv-
ityinprivatedataanalysis,” inTheory of cryptography conference, pp.265–284,
Springer, 2006.
[13] C. Dwork, A. Roth, et al., “The algorithmic foundations of differential privacy,”
Foundations and Trends® in Theoretical Computer Science, vol. 9, no. 3–4,
pp. 211–407, 2014.
[14] L. Wasserman and S. Zhou, “A statistical framework for differential privacy,”
Journal of the American Statistical Association, vol. 105, pp. 375–389, mar
2010.
[15] M. Gong, Y. Xie, K. Pan, K. Feng, and A. K. Qin, “A Survey on Differ-
entially Private Machine Learning [Review Article],” in IEEE Computational
Intelligence Magazine, vol. 15, pp. 49–64, Institute of Electrical and Electronics
Engineers Inc., may 2020.
[16] J. Xu, Z. Zhang, X. Xiao, Y. Yang, G. Yu, and M. Winslett, “Differentially
private histogram publication,” The VLDB journal, vol. 22, no. 6, pp. 797–822,
2013.
[17] M. Gaboardi, H. Lim, R. Rogers, and S. Vadhan, “Differentially private chi-
squared hypothesis testing: Goodness of fit and independence testing,” in In-
ternational conference on machine learning, pp. 2111–2120, PMLR, 2016.
16[18] C. Task and C. Clifton, “Differentially private significance testing on paired-
sample data,” in Proceedings of the 2016 SIAM International Conference on
Data Mining, pp. 153–161, SIAM, 2016.
[19] S. Couch, Z. Kazan, K. Shi, A. Bray, and A. Groce, “Differentially private
nonparametric hypothesis testing,” in Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security, pp. 737–751, 2019.
[20] C. Ferrando, S. Wang, and D. Sheldon, “Parametric bootstrap for differentially
private confidence intervals,” in International Conference on Artificial Intelli-
gence and Statistics, pp. 1598–1618, PMLR, 2022.
[21] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate, “Differentially private empir-
ical risk minimization.,” Journal of Machine Learning Research, vol. 12, no. 3,
2011.
[22] M. Hardt, K. Ligett, and F. McSherry, “A simple and practical algorithm for
differentially private data release,” Advances in neural information processing
systems, vol. 25, 2012.
[23] H. Ping, J. Stoyanovich, and B. Howe, “Datasynthesizer: Privacy-preserving
synthetic datasets,” in Proceedings of the 29th International Conference on
Scientific and Statistical Database Management, pp. 1–5, 2017.
[24] J. Snoke and A. Slavkovi´c, “pMSE mechanism: differentially private synthetic
data with maximal distributional similarity,” in International conference on
privacy in statistical databases, pp. 138–159, Springer, 2018.
[25] D. Chen, T. Orekondy, and M. Fritz, “GS-WGAN: A gradient-sanitized ap-
proach for learning differentially private generators,” Advances in Neural In-
formation Processing Systems, vol. 33, pp. 12673–12684, 2020.
[26] R. McKenna, G. Miklau, and D. Sheldon, “Winning the NIST Contest: A
scalable and general approach to differentially private synthetic data,” Journal
of Privacy and Confidentiality, vol. 11, dec 2021.
[27] N. Nachar et al., “The Mann-Whitney U: A test for assessing whether two in-
dependent samples come from the same distribution,” Tutorials in quantitative
Methods for Psychology, vol. 4, no. 1, pp. 13–20, 2008.
[28] J. Zar, Biostatistical Analysis. Prentice Hall, 2010.
[29] U. Okeh et al., “Statistical analysis of the application of Wilcoxon and Mann-
Whitney U test in medical research studies,” Biotechnology and molecular bi-
ology reviews, vol. 4, no. 6, pp. 128–131, 2009.
[30] M. P. Fay and M. A. Proschan, “Wilcoxon-Mann-Whitney or t-test? On as-
sumptions for hypothesis tests and multiple interpretations of decision rules,”
Statistics surveys, vol. 4, p. 1, 2010.
[31] G. Casella and R. L. Berger, Statistical inference. Pacific Grove, CA, USA:
Duxbury Press, 2 ed., 2002.
[32] C. Arnold and M. Neunhoeffer, “Really useful synthetic data–a framework to
evaluate the quality of differentially private synthetic data,” arXiv preprint
arXiv:2004.07740, 2020.
17[33] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang, “Deep learning with differential privacy,” in Proceedings of the 2016
ACM SIGSAC conference on computer and communications security, pp. 308–
318, 2016.
[34] N. C. Abay, Y. Zhou, M. Kantarcioglu, B. Thuraisingham, and L. Sweeney,
“Privacy preserving synthetic data release using deep learning,” in Machine
Learning and Knowledge Discovery in Databases: European Conference, ECML
PKDD 2018, Dublin, Ireland, September 10–14, 2018, Proceedings, Part I 18,
pp. 510–526, Springer, 2019.
[35] J. Jordon, J. Yoon, and M. Van Der Schaar, “PATE-GaN: Generating synthetic
data with differential privacy guarantees,” in 7th International Conference on
Learning Representations, ICLR 2019, feb 2019.
[36] C. M. Bowen and J. Snoke, “Comparative study of differentially private syn-
thetic data algorithms from the nist pscr differential privacy synthetic data
challenge,” J. Priv. Confidentiality, vol. 11, 2019.
[37] C. M. Bowen and F. Liu, “Comparative Study of Differentially Private Data
Synthesis Methods,” Statistical Science, vol. 35, no. 2, pp. 280 – 307, 2020.
[38] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio, “Generative adversarial networks,” Communica-
tions of the ACM, vol. 63, no. 11, pp. 139–144, 2020.
[39] I. Goodfellow, “NIPS 2016 tutorial: Generative adversarial networks,” arXiv
preprint arXiv:1701.00160, 2016.
[40] F. Wilcoxon, “Individual Comparisons by Ranking Methods,” Biometrics Bul-
letin, vol. 1, no. 6, pp. 80–83, 1945.
[41] H. B. Mann and D. R. Whitney, “On a test of whether one of two random
variables is stochastically larger than the other,” The annals of mathematical
statistics, pp. 50–60, 1947.
[42] I. Jambor, P. J. Bostr¨om, P. Taimen, K. Syva¨nen, E. K¨ahk¨onen, M. Kallajoki,
I. M. Perez, T. Kauko, J. Matoma¨ki, O. Ettala, et al., “Novel biparametric mri
and targeted biopsy improves risk stratification in men with a clinical suspicion
of prostate cancer (improd trial),” Journal of Magnetic Resonance Imaging,
vol. 46, no. 4, pp. 1089–1095, 2017.
[43] I. Jambor, J. Verho, O. Ettala, J. Knaapila, P. Taimen, K. T. Syv¨anen,
A. Kiviniemi, E. K¨ahko¨nen, I. M. Perez, M. Seppa¨nen, et al., “Validation
of improd biparametric mri in men with clinically suspected prostate can-
cer: a prospective multi-institutional trial,” PLoS medicine, vol. 16, no. 6,
p. e1002813, 2019.
[44] T. A. Stamey, N. Yang, A. R. Hay, J. E. McNeal, F. S. Freiha, and E. Red-
wine, “Prostate-specific antigen as a serum marker for adenocarcinoma of the
prostate,” New England Journal of Medicine, vol. 317, no. 15, pp. 909–916,
1987.
18[45] W. J. Catalona, D. S. Smith, T. L. Ratliff, K. M. Dodds, D. E. Coplen, J. J.
Yuan, J. A. Petros, and G. L. Andriole, “Measurement of prostate-specific
antigen in serum as a screening test for prostate cancer,” New England journal
of medicine, vol. 324, no. 17, pp. 1156–1161, 1991.
[46] S. Ulianova, “Cardiovascular Disease dataset — Kaggle,” 2019.
[47] S. C. Larsson, M. Ba¨ck, J. M. Rees, A. M. Mason, and S. Burgess, “Body mass
index and body composition in relation to 14 cardiovascular conditions in UK
Biobank: A Mendelian randomization study,” European Heart Journal, vol. 41,
pp. 221–226, jan 2020.
[48] C. L. Canonne, G. Kamath, and T. Steinke, “The Discrete Gaussian for Differ-
ential Privacy,” Journal of Privacy and Confidentiality, vol. 12, no. 1, 2022.
[49] R. McKenna, G. Miklau, and D. Sheldon, “Private-PGM,” 2021.
[50] M. Hardt, K. Ligett, and F. McSherry, “PrivateMultiplicativeWeights
(MWEM),” 2020.
[51] D. Chen, “GS-WGAN github-repository.” https://github.com/
DingfanChen/GS-WGAN, 2020.
[52] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
Z. Lin, N. Gimelshein, L. Antiga, et al., “Pytorch: An imperative style, high-
performance deep learning library,” Advances in neural information processing
systems, vol. 32, 2019.
[53] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, “Im-
provedtrainingofwassersteingans,” Advances in neural information processing
systems, vol. 30, 2017.
[54] A.-S. Charest, “How can we analyze differentially-private synthetic datasets?,”
Journal of Privacy and Confidentiality, vol. 2, no. 2, 2011.
[55] A.-S. Charest, “Empirical evaluation of statistical inference from differentially-
private contingency tables,” in Privacy in Statistical Databases: UNESCO
Chair in Data Privacy, International Conference, PSD 2012, Palermo, Italy,
September 26-28, 2012. Proceedings, pp. 257–272, Springer, 2012.
[56] O. Giles, K. Hosseini, G. Mingas, O. Strickson, L. Bowler, C. R. Smith,
H. Wilde, J. N. Lim, B. Mateen, K. Amarasinghe, et al., “Faking feature im-
portance: A cautionary tale on the use of differentially-private synthetic data,”
arXiv preprint arXiv:2203.01363, 2022.
[57] O. Ra¨is¨a, J. Ja¨lko¨, S. Kaski, and A. Honkela, “Noise-aware statistical infer-
ence with differentially private synthetic data,” in International Conference on
Artificial Intelligence and Statistics, pp. 3620–3643, PMLR, 2023.
[58] R. McKenna, B. Mullins, D. Sheldon, and G. Miklau, “Aim: An adaptive and
iterative mechanism for differentially private synthetic data,” arXiv preprint
arXiv:2201.12677, 2022.
[59] H. Do¨rie, 100 great problems of elementary mathematics: Their history and
solutions. Dover Publications, 1965. pp. 44-48.
19A Supplementary Material
A.1 Drawing data from additively smoothed histograms is
differentially private
Proof:
Drawing a single datum from a distribution determined by a histogram can be
carried out in a differentially private way using the exponential mechanism, in which
the original probabilities are modified as follows. Instead of the relative size of a
histogram bin, each possible outcome of the draw has its probability proportional
to
eϵsi
, (3)
2∆
where s is the score of the ith option (e.g. histogram bin) and ∆ is the sensitivity of
i
the scoring, indicating the maximum amount the score can change for any outcome
if a single datum is changed among the sensitive data. By substituting (3) into (2),
it is easy to verify that the exponential mechanism fulfills differential privacy. If we
define the score for the ith output as s = αln(c + α), where c is the number of
i i i
sensitive data in the ith bin and α = 2, the maximal change of the score value takes
ϵ
place when c = 1 changes to c = 0 or vice versa. For this maximal change, we have
i i
the following upper bound:
(cid:18) (cid:19)
1
αln(1+α)−αln(α) = αln 1+
α
(cid:18)(cid:18) 1(cid:19)α(cid:19)
= ln 1+
α
< ln(e)
= 1 ,
where the inequality follows from the well-known property of e (see e.g. pages 44-48
of [59]):
(cid:18) 1(cid:19)x
1+ < e,
x
for all x > 0. Thus, the sensitivity of the scoring is 1. Substituting the scoring into
(3) indicates that drawing a single datum according to the probabilities proportional
to c + 2 is ϵ differentially private.
i ϵ
Due to the sequential composability property, drawing m synthetic data instead
of only one datum requires m times larger privacy budget. Consequently, drawing
m according to the probabilities proportional to c + 2m is ϵ differentially private.
i ϵ
20Algorithm 1 Draw synthetic sample
Require: c ▷ Histogram count vector of length h
Require: m ▷ Number of synthetic data to be drawn
Require: ϵ
s ← score vector of length h
p ← probability vector of length h
r ← synthetic data count vector of length h
α ← 2m/ϵ ▷ Calculate amount of additive smoothing
for i ∈ {1,...,h} do
s ← αln(c +α) ▷ Calculate scores for bins
i i
for i ∈ {1,...,h} do
p ←
eϵsi
▷ Calculate probabilities according to the exponential mechanism
i 2m
for i ∈ {1,...,h} do
p ← p /∥p∥ ▷ Normalize the probabilities to sum up to one
i i 1
r ← Draw m synthetic data according to the probabilities p.
return r
21