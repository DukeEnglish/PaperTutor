Uncertainty quantification for data-driven
weather models
Christopher Bu¬®lte‚àó1, Nina Horat1, Julian Quinting1 and
Sebastian
Lerch‚Ä†1,2
1Karlsruhe Institute of Technology
2Heidelberg Institute for Theoretical Studies
March 21, 2024
Abstract
Artificial intelligence (AI)-based data-driven weather forecasting models have experienced
rapid progress over the last years. Recent studies, with models trained on reanalysis data,
achieve impressive results and demonstrate substantial improvements over state-of-the-art
physics-basednumericalweatherpredictionmodelsacrossarangeofvariablesandevaluation
metrics. Beyond improved predictions, the main advantages of data-driven weather models
aretheirsubstantiallylowercomputationalcostsandthefastergenerationofforecasts,oncea
model has been trained. However, most efforts in data-driven weather forecasting have been
limited to deterministic, point-valued predictions, making it impossible to quantify forecast
uncertainties, which is crucial in research and for optimal decision making in applications.
Ouroverarchingaimistosystematicallystudyandcompareuncertaintyquantificationmeth-
ods to generate probabilistic weather forecasts from a state-of-the-art deterministic data-
driven weather model, Pangu-Weather. Specifically, we compare approaches for quantifying
forecast uncertainty based on generating ensemble forecasts via perturbations to the initial
conditions,withtheuseofstatisticalandmachinelearningmethodsforpost-hocuncertainty
quantification. In a case study on medium-range forecasts of selected weather variables over
Europe, the probabilistic forecasts obtained by using the Pangu-Weather model in concert
with uncertainty quantification methods show promising results and provide improvements
over ensemble forecasts from the physics-based ensemble weather model of the European
Centre for Medium-Range Weather Forecasts for lead times of up to 5 days.
1. Introduction
Modernweatherforecastsareusuallybasedonsimulationsfromphysics-basednumericalweather
prediction (NWP) models, which describe atmospheric processes via systems of partial differ-
ential equations. To quantify forecast uncertainty and provide probabilistic predictions, NWP
models are typically run several times with varying initial conditions and perturbed model
physics, resulting in an ensemble of predictions. Numerically solving the differential equations
requires tremendous computational resources, limiting the spatial and temporal resolution, as
wellasthenumberofensembleruns. ThehistoryofNWPsinceitsinceptionaround70yearsago
‚àóCurrent affiliation: Ludwig-Maximilians-Universita¬®t, Munich
‚Ä†corresponding author, sebastian.lerch@kit.edu
1
4202
raM
02
]hp-oa.scisyhp[
1v85431.3042:viXrahas been a success story, albeit a ‚Äúquiet‚Äù one characterized by continued, small improvements
throughthesteadyaccumulationofscientificknowledgeandtechnologicaladvances(Baueretal.,
2015).
Currently, a major leap in the formerly quiet success story of NWP can be observed due to
the unprecedented success and rapid advancement of purely data-driven machine learning (ML)
models for weather prediction. Contrary to NWP, data-driven weather models do not include
any physics-based equations and aim to predict the future weather state (typically iteratively in
stepsofhourstodays)fromtheinitialweatherstateonly, usingstatisticalrelationslearnedfrom
past data. Beyond improved forecasts, the major advantages of data-driven models are their
substantially lower computational costs (and accompanied energy consumption) and the faster
generation of forecasts, once a model has been trained. Over the past two years, fundamental
advances have been achieved, with purely data-driven weather models now convincingly outper-
forming state-of-the-art NWP systems, as recently reviewed in Ben-Bouall`egue et al. (2023a).
The most notable contributions and models include Keisler (2022), FourCastNet (Pathak et al.,
2022), Pangu-Weather (Bi et al., 2023), GraphCast (Lam et al., 2022), ClimaX (Nguyen et al.,
2023a), FengWu (Chen etal., 2023a), FuXi (Chen et al., 2023c), SwinRDM (Chen et al., 2023b),
AtmoRep (Lessig et al., 2023), NeuralGCM (Kochkov et al., 2023), Stormer (Nguyen et al.,
2023b), and GenCast (Price et al., 2023). All models utilize the ERA5 global reanalysis dataset
(Hersbach et al., 2020) for training and evaluation, and are run at grid spacings of up to 0.25‚ó¶.
However, most of these efforts have been focused on deterministic forecasts only, making it
impossibletoquantifyforecastuncertaintieswhichiscrucialforoptimaldecisionmaking,andone
of the reasons underlying a transdisciplinary transition towards probabilistic forecasts (Gneiting
and Katzfuss, 2014). Therefore, the overarching aim of our work is to investigate approaches to
generate probabilistic predictions from deterministic data-driven weather models.1 In order to
generate probabilistic forecasts, we consider two main approaches for uncertainty quantification
(UQ) for data-driven weather models. A schematic overview is provided in Figure 1.
Initial condition (IC)-based approaches generate an ensemble forecast by running a data-
driven model multiple times based on a number of (slightly) different initial conditions. These
initial condition ensembles can be generated in various ways, and we consider three variants:
Adding random noise to the initial weather state obtained from the ERA5 dataset, as first
proposed by Pathak et al. (2022); utilizing the perturbed initial conditions of a physics-based
NWP ensemble model (Buizza et al., 2008); and generating conditions based on perturbations
computed from randomly selected past data, as proposed by Magnusson et al. (2009). IC
approachesgenerallyrequirethecapabilitiestorunthedata-drivenmodelsforasetofinputdata
(i.e., estimated models, code, data, and suitable hardware infrastructure), which has recently
becomepossiblesincecodeanddatahavebeenmadepublicforsomeofthemodels(mostnotably
FourCastNet, Pangu-Weather, and GraphCast), but still poses technical challenges due to the
substantial computing and disk space requirements.
Post-hoc (PH) UQ approaches, by contrast, utilize statistical or ML methods to supplement
deterministic forecasts with uncertainty information and thus turn them into probabilistic fore-
casts. Thesemethodsonlyrequireatrainingdatasetofdeterministicforecastsandcorresponding
observations. A large variety of such approaches has been proposed, e.g., conformal predic-
tion (Angelopoulos and Bates, 2021) or distributional regression (Gneiting and Katzfuss, 2014).
Here, we consider two distributional regression approaches particularly relevant for atmospheric
science applications, where such methods have been mostly used in the context of statistical
post-processing to correct systematic errors of NWP forecasts (Vannitsem et al., 2021). The
EasyUQ (Walz et al., 2024a) approach builds on the recent isotonic distributional regression
1An ideal solution to this challenge might be inherently probabilistic data-driven approaches, for example gen-
erative ML methods. The AtmoRep (Lessig et al., 2023), NeuralGCM (Kochkov et al., 2023) and GenCast
(Price et al., 2023) models represent first steps in this direction.
2Post-hoc approaches
DRN
UQ
EasyUQ
Initial weather Forecasted
Data-driven model
state ùë° 0 weather state ùë° 1
Uncertainty
estimate
Initial-condition based approaches
ùëò ùëò
UQ
Uncertainty
Initial weather k perturbated initial k forecasted weather
Data-driven model estimate
state ùë° 0 weather states ùë° 0 states ùë° 1
Gaussian noise perturbations (GNP) ECMWF ICS (IFSP) Random field perturbations (RFP)
+ ‚Ä¶ + ( - )
Initial weather Gaussian Noise Initial conditions from ECMWF Initial weather Difference of two past
state ùë° 0 ensemble model state ùë° 0 atmospheric states
Figure 1: Schematic illustration of the different uncertainty quantification approaches to gener-
ate probabilistic forecasts from deterministic data-driven weather models. A detailed
description of the UQ methods is provided in Section 4.
technique (Henzi et al., 2021) and yields statistically optimal discrete predictive distributions by
leveraging the pool-adjacent-violators algorithm for nonparametric isotonic regression. EasyUQ
utilizes deterministic forecasts of the target variable as sole input, and has, e.g., recently been
used in Walz et al. (2024b) to generate probabilistic forecasts of precipitation from determin-
istic inputs. Over the past years, modern ensemble post-processing methods based on neural
networks have been proposed which enable the incorporation of additional input variables and
the data-driven learning of complex relationships between the inputs and distribution forecasts.
We will build on the parametric distributional regression network approach first proposed in
Rasp and Lerch (2018), which has been successfully extended for many target variables (e.g.,
in Schulz and Lerch, 2022) and has been used to generate corrected probabilistic forecasts from
deterministic inputs from an NWP model (Chapman et al., 2022; Gneiting et al., 2023).
Our overarching aim is to systematically evaluate and compare the proposed UQ approaches
forselecteduser-relevanttargetvariables.2 WeutilizethePangu-Weathermodel(Bietal.,2023)
to produce deterministic and ensemble forecasts over Europe for a time period of five years, and
conduct a systematic evaluation of the out-of-sample forecast performance of the various UQ
approaches. The operational ensemble forecast of the European Centre for Medium-Range
2Note that recently, Brenowitz et al. (2024) proposed the use of a lagged ensemble of deterministic data-driven
weather predictions as an alternative approach to obtain a probabilistic forecast. However, as argued by
the authors themselves, this approach cannot be used for constructing real (out of sample) forecasts since
it requires observations from a window around the initialization date, including initial conditions from the
future. Additional adaptations of this approach thus seem necessary to enable a fair comparison to the UQ
methods considered here.
3Weather Forecasts (ECMWF) thereby serves as a benchmark model.
Theremainderofthisarticleisstructuredasfollows. Section2describesthedataandsetupof
our case studies. Section 3 introduces the notation and provides the mathematical formulation
oftheproblem,andSection4introducestheUQmethods,thepredictiveperformanceofwhichis
evaluatedinSection5. Section6concludeswithadiscussion. Pythoncodewithimplementations
of all UQ methods is available at https://github.com/cbuelt/dduq.
2. Data and setup
OurstudyfocusesonthePangu-WeathermodeldevelopedbyBietal.(2023). Additionalresults
for the FourCastNet model (Pathak et al., 2022) are available in the supplemental material.
Pangu-Weather is a three-dimensional vision transformer architecture with specific adaptations
and extensions to weather prediction, and was one of the first data-driven models to achieve
improvements over physics-based NWP models. The Pangu-Weather model produces global
forecasts of five atmospheric variables (Z, Q, T, U, V) on 13 pressure levels and four surface
variables (MSL, U10M, V10M, T2M) at a grid spacing of 0.25‚ó¶. It is trained based on 39 years
of ERA5 reanalysis data from 1979‚Äì2017. In Bi et al. (2023), data from the year 2018 was used
as validation data, and data from 2019 serves as a test dataset. For details regarding the model
architecture, training procedure, and forecast quality, we refer to Bi et al. (2023). To implement
the UQ methods described below, we adapted Pangu-Weather code and data provided by the
authors3 for our purposes.
Since some of the UQ methods discussed below require training and validation data on their
own, we further produced both deterministic Pangu-Weather forecasts, as well as forecasts from
thevariousUQapproaches,foradditionalrecentyears. Inordertoevaluateondataindependent
from the training data used in Bi et al. (2023), we utilize data from 2018‚Äì2021 as training and
validation data for the UQ methods (if necessary), and evaluate all methods on data from 2022.
Forecasts from all methods are initialized at 00 UTC every day, for a total of H = 31 steps of
6 hours each (i.e., up to maximum lead time of 186 hours). Due to the substantial computing
and disk space requirements (in particular when generating and storing ensemble forecasts), we
restrictourattentiontoselecteduser-relevantweathervariables(u-componentandv-component
of 10-m wind speed (U10 and V10), temperature at 2m and 850 hPa (T2M and T850), and
geopotential height at 500 hPa (Z500)), and a European domain, covering an area from 35¬∞N ‚Äì
75¬∞Nand12.5¬∞W‚Äì42.5¬∞E.ThegroundtruthforevaluationistheERA5datasetwithatemporal
resolution of 6 hours and a spatial grid spacing of 0.25¬∞.
As a reference forecast but also as initial condition perturbations (see Section 4, we retrieve
operational ensemble forecasts of ECMWF‚Äôs ensemble prediction system, which is based on the
ECMWF Integrated Forecasting System (IFS). The data are retrieved for the same training
and evaluation periods on a regular latitude-longitude grid of 0.25√ó0.25¬∞ covering the identical
spatial domain and forecast lead times. It should be noted that the native spatial resolution of
theensemblepredictionsystemisslightlyhigherthanthatofERA5whichmaycausedifferences
in regions of high topography.
3. Notation
In the following, we will consider probabilistic forecasts for several meteorological variables on
a two-dimensional gridded domain. The grid point locations (i,j),i = 1,...,I;j = 1,...,J will
be summarized via a generic location index l = 1,...,L, where each value of l denotes a specific
combinationofiandj, andL = IJ. Wherehelpful, wewilldistinguishbetweenaglobaldomain
3https://github.com/198808xc/Pangu-Weather
4l ‚àà L and a European domain l ‚àà L , see Section 2. The different target variables are treated
G E
separately and thus are omitted in the notation. Following common practice in NWP, we will
consider forecasts to be initialized at time t, and to provide predictions for forecast horizons
h = 1,2,...,H steps ahead. In our case study, the forecast model runs will be started daily at
00 UTC and forecast steps will be 6 hours each. A deterministic Pangu-Weather (PW) forecast
for location l, initialized at time t and for a horizon of h steps will be denoted by XPW.
l,t,h
Ouroverarchingaimistoquantifyforecastuncertaintyintheformofapredictivedistribution
F . In most cases, this predictive distribution will be given in the form of a sample of size M,
l,t,h
i.e., an ensemble forecast
X = (cid:8) X1 ,...,XM (cid:9) ,
l,t,h l,t,h l,t,h
where, e.g., each ensemble member is started from a different set of initial conditions.
An observation corresponding to an h-step ahead forecast initialized at time t is available at
time t+h, and will be denoted by Y . As detailed below, we use the ERA5 reanalysis dataset
l,t+h
asgroundtruth. Sincethedeterministicdata-drivenmodelrunswilltypicallybeinitializedfrom
the corresponding ERA5 data at the initialization time, the starting conditions can be seen as
0-step ahead forecasts and will be denoted by Y .
l,t
4. Methods
This section provides a description of the different UQ methods we use to generate probabilistic
forecasts from deterministic data-driven weather models. A schematic overview is available in
Figure 1.
4.1. Initial condition ensemble approaches
The general idea behind all considered IC ensemble approaches is that based on (slightly) dif-
ferent initial conditions
X = (cid:8) X1 ,...,XM (cid:9) ,
l,t,0 l,t,0 l,t,0
an ensemble forecast of size M is generated by starting M runs of the deterministic data-driven
weathermodel(Pangu-Weatherinourcase)fromthoseinitialconditionstoproduceanensemble
forecast
X = (cid:8) X1 ,...,XM (cid:9) = (cid:8) g (X1 ),...,g (XM )(cid:9)
l,t,h l,t,h l,t,h h l,t,0 h l,t,0
for h = 1,...,H, where g (Xm ) denotes the h-step ahead Pangu-Weather forecast started from
h l,t,0
the IC ensemble member Xm . Note that all IC approaches are based on generating global
l,t,0
initial conditions for the full model grid and a global Pangu-Weather forecast is computed, even
though we later restrict our attention to the grid over Europe for evaluation. The locations l in
the description of the IC approaches below should thus be understood as grid point locations of
the global 0.25‚ó¶ grid, L .
G
The IC approaches described below mainly differ in the way the IC ensemble X is gen-
l,t,0
erated. Specifically, we consider Gaussian noise perturbations, random field perturbations and
IFS perturbed initial conditions, which are introduced in detail below. Exemplary perturba-
tions for a common initialization date are visualized in Figure 2. A natural shortcoming of all
IC approaches is that they are inherently limited to accounting for initial condition uncertainty
only, and not for model uncertainty, which is, e.g., addressed in physics-based NWP models via
stochastic parametrizations of subgrid processes (Palmer, 2019b).
5(a) GNP
(b) RFP
(c) IFSP
Figure 2: Exemplary perturbations of the different initial condition ensemble approaches across
the European domain. Each row shows the residual to the original ERA5 observation
for forecasts initialized on June 1, 2022, the variable T2M, and four randomly selected
perturbations.
4.1.1. Gaussian noise perturbations (GNP)
A simple and straightforward method to generate an IC ensemble is to add random noise to the
ERA5-based initial weather state Y from which the deterministic Pangu-Weather model would
l,t
be initialized. We here follow Pathak et al. (2022), who first proposed this approach for the
FourCastNet model, and generate inital conditions by adding independently sampled Gaussian
noise to all variables after standardization, i.e.,
XGNP,m = Y +Œµ for m = 1,...,M,
l,t,0 l,t l,t
where Œµ ‚àº N(0,Œ≥œÉ), œÉ denotes the mean standard deviation of the respective variable over
l,t
all grid points, and Œ≥ is a tuning parameter. Samples of the Gaussian noise process are thus
generatedindependentlyovermembersm, locationsl, variablesandinitializationtimest. While
Pathak et al. (2022) use Œ≥ = 0.3 for the FourCastNet model, our initial experiments with the
Pangu-Weather model suggested that Œ≥ = 0.001 leads to better results.
Alternative specifications of the noise process have been considered. For example, Bi et al.
(2023) use Perlin noise, but our initial experiments indicated only negligible differences to the
performance of Gaussian noise-based GNP forecasts for our case study. Price et al. (2023)
recently proposed a noise process where spatial dependencies on the sphere are retained in
the context of a generative data-driven weather model, which might constitute an interesting
alternative.
64.1.2. Random field perturbations (RFP)
Analternative, data-drivenapproachtogenerateICensembles, whichwewillrefertoasrandom
field perturbations, was proposed by Magnusson et al. (2009) in the context of physics-based
NWP ensemble models. They argue that adding noise to the initial conditions ignores the
underlying dynamics of the weather system. Instead, they suggest to use the scaled difference
of two independent, randomly selected atmospheric states from the past as perturbation, which
has the advantage of preserving linear balances in the system. The random field perturbations
are calculated as
Œæm,Œ± = Œ± Y œÑ 1m ‚àíY œÑ 2m for m = 1,...,M,
t ‚à•Y œÑm ‚àíY œÑm‚à• Etot
1 2
where Y œÑm = {Y l,œÑm,l ‚àà L G},i = 1,2 denotes the global observed ERA5 field of the variable of
i i
interest at date œÑm, and ‚à•¬∑‚à• denotes the total energy norm. We choose the dates œÑm,œÑm
i Etot 1 2
randomlyfromthesamemonthastfromthetrainingdataset(2018‚Äì2021)toaccountforseasonal
variability, but sample œÑm and œÑm from different years to ensure (approximate) independence.
1 2
The constant Œ± is a tuning parameter and controls the dispersion of the IC ensemble. Based on
some preliminary tests, we chose Œ± = 5¬∑106 for our case study.
With these choices, global perturbations Œæm,Œ±,m = 1,...,M, are computed and added to the
t
corresponding ERA5 initial conditions, i.e.,
XRFP,m = Y +Œæm,Œ±
l,t,0 l,t l,t
for m = 1,...,M and all l ‚àà L .
G
4.1.3. IFS initial conditions (IFSP)
Finally, we consider an IC approach more akin to the operational practice of running NWP
ensemblemodels(Palmer,2019a)byutilizingthe(ensembleof)initialconditionsoftheECMWF
ensemble model to initialize the deterministic data-driven model. Specifically, we select the
values at initialization time (i.e., the forecasts at step h = 0) of the perturbed members Zm =
t,0
{Zm ,l ‚àà L },m = 1,...,M, of the ECMWF ensemble, i.e.,
l,t,0 G
XIFSP,m = Zm for m = 1,...,M.
l,t,0 l,t,0
For the period 2018‚Äì2022, we remapped the initial conditions from a native grid spacing to a
regular latitude-longitude grid of 0.25¬∞ grid spacing.
The initial condition uncertainty in ECMWF‚Äôs ensemble prediction system is incorporated by
two approaches. The ensemble of 4D-var data assimilations generates 25 independent ensemble
members by introducing perturbations to observations, physical processes in the short-term
forecasts and the sea surface temperature state (Isaksen et al., 2010). Further, singular vector
perturbations are added to the analysis field which lead to a rapid dispersion of the ensemble
members (Leutbecher and Palmer, 2008). Accordingly, one would expect faster dispersion of
ensemble members than with Gaussian perturbations.
4.2. Post-hoc approaches
IncontrasttotheICapproaches, thePHmethodsoperatedirectlyonagivendeterministicfore-
cast from a data-driven weather model, and learn from past pairs of forecasts and observations
howtobestgenerateaprobabilisticforecastfromthedeterministicinput. Fromameteorological
perspective, this can be viewed as a post-processing task (Vannitsem et al., 2021). In the follow-
ing, we assume that a dataset of past deterministic Pangu-Weather forecasts and corresponding
7observations,
(cid:0) XPW,Y (cid:1) , for l ‚àà L ,
l,t,h l,t+h E
is available, where t denotes an initialalization time in the training dataset (2018‚Äì2021).
Based on the training dataset, the PH methods yield forecast distributions F . Given a
l,t,h
deterministic Pangu-Weather forecast XPW in the test dataset (2022), a probabilistic forecast
l,t‚àó,h
for the date t‚àó and lead time h can thus be obtained by using the corresponding deterministic
forecast as input to the trained PH model. In the following, we consider two complementary
PH methods based on statistical and ML approaches.
An advantage of the PH methods compared to the IC approaches is their ability to correct
systematic errors such as biases in the deterministic forecasts, and that they are not limited
to accounting for initial condition uncertainty only. However, these methods require sufficient
training data to generate forecasts, unlike, e.g., the GNP and IFSP approaches. We here utilize
four years of training data to ensure a separation to the data used to train the Pangu-Weather
model by Bi et al. (2023). In principle, larger training datasets could be obtained by generating
Pangu-Weather forecasts for the preceding years, at the potential risk of overfitting.
4.2.1. EasyUQ
EasyUQ, proposed by Walz et al. (2024a), aims at learning a predictive distribution from de-
terministic, single-valued model output. EasyUQ proceeds separately for every location l ‚àà L
E
and lead time h. To simplify notation, we will suppress the lead time index h in the current
subsection, and note that all forecasts and observations should be understood as those for the
corresponding lead time only. Given corresponding data of the form (XPW,Y ),t = 1,...,T,
l,t l,t
and assuming that the predictive cumulative distribution functions (CDFs) F (y) = P(Y ‚â§
x l,t
y|X = x) are increasing in stochastic order in x, i.e., F (y) ‚â• F (y) for all y ‚àà R if x ‚â• x‚Ä≤,
l,t x x‚Ä≤
the IDR-estimated predictive CDF is then given by
‚Ñì
1 (cid:88)
FÀÜIDR(y) = FÀÜIDR (y) = min max 1{Y ‚â§ y}, t = 1,...,T.
l,t X lP ,tW k=1,...,t‚Ñì=t,...,T ‚Ñì‚àík+1 l,t‚Ä≤
t‚Ä≤=k
Building on the recent isotonic distributional regression (IDR) technique (Henzi et al., 2021),
FÀÜIDR isastatisticallyoptimaldiscretepredictivedistributioninthatitminimizesthecontinuous
l,t
ranked probability score (CRPS, see Section 4.3) over all conditional distributions satisfying the
assumptionofstochasticordering. FortheoreticalresultsandmoredetailsonEasyUQandIDR,
we refer to Walz et al. (2024a) and Henzi et al. (2021). EasyUQ does not require any choices
of tuning parameters and thus constitutes an attractive benchmark method that can be applied
in a fully automated manner. Walz et al. (2024a) note that EasyUQ yields similar forecast
performance as conformal prediction, and in case studies on post-processing, EasyUQ showed
predictive performance comparable to other statistical models (e.g., Schulz and Lerch, 2022).
4.2.2. Distributional regression network (DRN)
A key limitation of the EasyUQ approach is that there is no straightforward way to include
additionalpredictorvariablesbesidesthedeterministicforecastsofthetargetvariableofinterest.
However, recent research on ML-based ensemble post-processing methods has highlighted that
incorporating additional predictors is a key aspect in the substantial improvements achieved
by these approaches (Rasp and Lerch, 2018). Therefore, we consider a DRN, a parametric
neural network (NN)-based approach first proposed in Rasp and Lerch (2018) as an alternative
PH method. To introduce DRN, we slightly extend the notation from above and use XPW to
l,t,h
denote the (vector of) deterministic Pangu-Weather forecasts for all considered output variables
at location l ‚àà L , initialization time t and lead time h. Note that we consider the output
E
8variables from the corresponding vertical level only, and leave extensions towards incorporating
additional inputs from other levels for future work.
Basedonthedeterministicmodeloutput,XPW, theDRNapproachproceedsbytrainingaNN
l,t,h
which yields the parameters of a suitable parametric distribution for the target variable as its
output. DRN enables the use of arbitrary predictors as inputs to the NN, including additional
meteorological variables (in our case from Pangu-Weather outputs) and location information.
The NN parameters are determined by minimizing the CRPS over the training dataset. As for
EasyUQ, we estimate separate models for every lead time, but note that considering multiple
lead times jointly can be a viable alternative (Primo et al., 2024). In our case study, we use
a Gaussian predictive distribution for all target variables and closely follow Rasp and Lerch
(2018) in our implementation. We fit a single DRN model for each forecast horizon h jointly
overallgridpointsl ‚àà L inthetargetdomain. InadditiontothedeterministicPangu-Weather
E
forecastsXPW,thelocationsareencodedviaapositionalembeddingthatmapsalocationl ‚àà L
l,t,h E
to a vector of latent features, which are then used as auxiliary input variables of the NN. This
procedure aims at making the model locally adaptive, while avoiding the training of a separate
model at every grid point. The DRN model thus yields a predictive distribution
FDRN = N
l,t,h ¬µ l,t,h,œÉ l,t,h
for each location l ‚àà L and lead time h, where ¬µ and œÉ are the location and scale
E l,t,h l,t,h
parameter of the Gaussian forecast distribution obtained as output of the NN.
We fit separate DRN models for all target variables, and use an identical NN architecture,
the hyperparameters of which were determined based on a limited series of initial tuning exper-
iments. Specifically, we use a NN with a single hidden layer of size 512, a location embedding
of dimension 5, a batch size of 1024, and train the model for 30 epochs. In principle, it might
be possible to improve the predictive performance of the DRN models further, e.g., by a more
extensive hyperparameter search. However, the forecast performance of DRN models has been
demonstrated to be fairly robust in this regard (e.g., Schulz and Lerch, 2022).
4.3. Forecast evaluation methods
TocomparethevariousUQmethodsintroducedabove, wemainlyrelyoncomparingtheiroutof
sampleforecastperformancebasedonproperscoringrules(GneitingandRaftery,2007). Proper
scoring rules enable a simultaneous assessment of calibration and sharpness of a probabilistic
forecast, and have become widely used across disciplines. Generally, a scoring rule S assigns a
numerical score to a predictive distribution F and a corresponding realized observation y, and is
calledproper, ifinexpectation, thetruedistributionoftheobservationreceivesthebestpossible
(i.e., minimal) score, i.e.,
E S(G,Y) ‚â§ E S(F,Y) for all F,G ‚àà F,
Y‚àºG Y‚àºG
where F denotes a suitable class of forecast distributions, see Gneiting and Raftery (2007)
for details. A commonly used scoring rule for evaluating univariate probabilistic forecasts in
meteorological applications is the continuous ranked probability score (CRPS, Matheson and
Winkler, 1976),
(cid:90) ‚àû
CRPS(F,y) = (cid:0) F(z)‚àí1{y ‚â§ z}(cid:1)2 dz,
‚àí‚àû
where 1 denotes the indicator function, and F is assumed to have a finite first moment. Closed-
form expressions of the CRPS are available for both sample-based predictive distributions in the
formofanM-memberensemble,aswellasformanyparametricfamiliesofforecastdistributions,
see, e.g., Jordan et al. (2019).
9Table 1: Mean CRPS of all methods and variables across the spatial domain for three different
groups of lead times, with the best-performing method highlighted in bold. Note that
the CRPS values for Z500 are scaled by a factor of 0.01.
Variable ECMWF IFS GNP IFSP RFP EasyUQ DRN
U10 0.54 0.71 0.78 0.58 0.53 0.51
V10 0.54 0.71 0.79 0.58 0.53 0.51
6h - 48h T2M 0.57 0.60 0.83 0.50 0.43 0.41
T850 0.43 0.57 0.81 0.45 0.43 0.41
Z500 0.33 0.48 1.71 0.36 0.36 0.32
U10 0.96 1.39 1.54 1.03 1.05 1.03
V10 0.96 1.39 1.57 1.02 1.05 1.03
48h - 120h T2M 0.75 0.96 1.22 0.74 0.69 0.67
T850 0.75 1.09 1.38 0.80 0.82 0.79
Z500 1.21 1.75 2.52 1.26 1.35 1.29
U10 1.54 2.31 2.09 1.59 1.70 1.68
V10 1.58 2.36 2.17 1.62 1.74 1.71
‚â• 120h T2M 1.05 1.51 1.57 1.07 1.13 1.10
T850 1.33 2.01 2.04 1.39 1.55 1.48
Z500 2.91 4.29 4.73 3.00 3.36 3.25
Skill scores based on proper scoring rules are a common tool to assess the relative im-
provements over a reference forecasting method. The continuous ranked probability skill score
(CRPSS) is given by
CRPS ‚àíCRPS
ref F
CRPSS = ,
F
CRPS
ref
where CRPS denotes the average CRPS of F over a test set, and CRPS denotes the corre-
F ref
sponding average CRPS of a reference method. The CRPSS is positively oriented, with negative
values indicating worse performance than the reference, 0 indicating no improvement, and a
maximum value of 1.
Further, we employ probability integral transform (PIT, Gneiting et al., 2007) histograms to
assess the calibration of the probabilistic forecasts. The PIT F(y) is the value the predictive
CDF F of the forecast obtains at the realized outcome y. For a calibrated forecast, the PIT
should follow a uniform distribution and corresponding deviations can be attributed to specific
typesofmiscalibration(Gneitingetal.,2007). Inaddition,wefurtherassessthereliabilityofthe
ensemble forecasts by comparing the root mean squared error (RMSE) of the ensemble mean to
the average ensemble spread, which should approximately be equal across time for a calibrated
ensemble (Fortin et al., 2014).
5. Results
WeherecomparethepreviouslyintroducedUQmethodsbasedontheirout-of-samplepredictive
performance. The ensemble forecasts from the operational 50-member ECMWF model are used
as a baseline, and can be considered as a state-of-the-art physics-based NWP ensemble model.
Note that we did not apply any post-processing to the ECMWF ensemble forecasts here, see the
discussion in Section 6 for details. The evaluation and training setup follows the descriptions in
Section2,andwegenerateensemblesofsizeM = 50forallinitialconditionensembleapproaches.
The PH methods are evaluated based on their predictive distributions, i.e., the empirical CDF
10for EasyUQ and the Gaussian forecast distribution for DRN.
T2M T850
2.5
1.75
1.50 2.0
1.25
1.5
1.00
1.0
0.75
0.50 0.5
0.25
0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 175
U10 V10
2.5 2.5
2.0 2.0
1.5 1.5
1.0 1.0
0.5 0.5
0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 175
Z500 Forecast lead time [h]
500
ECMWF IFS
400 GNP
IFSP
300
RFP
200 EasyUQ
100 DRN
0
0 25 50 75 100 125 150 175
Forecast lead time [h]
Figure 3: Mean CRPS as a function of the forecast lead time for the different UQ methods,
aggregated over all locations.
Table 1 provides the mean CRPS for all UQ methods, averaged over all grid points and
stratified into three groups of forecast lead times. Figure 3 shows the mean CRPS as a function
of the forecast lead time for all variables. Both illustrations indicate that the use of data-
driven weather forecasts in concert with the PH methods proposed here can yield improvements
over the ECMWF ensemble forecasts. The extent of these improvements, and the relative
performance of the different UQ methods, strongly depends on the variable of interest as well as
the forecast lead time. Generally, DRN yields the best forecasts at shorter lead times, followed
closely by the EasyUQ model. For longer lead times up to 120h, the CRPS of the ECMWF
ensemble is similar to that of the DRN, EasyUQ and RFP approaches, and for lead times
beyond 120h, the ECMWF ensemble performs better than all compared UQ methods. The
most pronounced differences and most clear improvements over the ECMWF ensemble forecasts
can be observed for T2M. The rankings among the different UQ methods are mostly identical
acrosstheconsideredtargetvariables, withthePHmethods(DRNandEasyUQ)showingbetter
forecasts at shorter lead times, whereas the RFP approach yields the best forecasts at longer
lead times. Interestingly and in contrast to previous studies on ensemble post-processing (e.g.,
Schulz and Lerch, 2022), DRN only yields relatively minor improvements over the considerably
simplerEasyUQmethod. ApotentialexplanationmightbethatDRNhereisrestrictedtomuch
11
SPRC
SPRC
SPRCfeweradditionalpredictorvariablescomparedtootherstudies. Therefore, furtherimprovements
mightbeachievedby, e.g., incorporatingPangu-Weatheroutputsfromotherverticallevels. The
GNP and IFSP approaches lead to substantially worse forecasts compared to the other methods
for all variables, in particular for z500.
Figure 4 shows the CRPSS of the different UQ methods over the spatial domain, using the
ECMWF ensemble as a reference forecast. For most methods, target variables and lead times,
there are some geographical regions where improvements over the ECMWF ensemble are ob-
tained. The most notable improvements and variations can be observed for the variable T2M.
There, the improvements over the ECMWF ensemble forecasts are most pronounced over land,
and even for a lead time of 168h, all methods show a positive skill score over mountainous re-
gions. In particular, the PH methods indicate a notablybetter performance. A generally similar
spatial pattern, albeit with less pronounced improvements over the ECMWF ensemble, can be
observed for U10. The areas with positive CRPSS values for Z500 seem to be more concentrated
around the Mediterranean and south-eastern Europe, and the GNP and IFSP methods show
notable worse performance.
To investigate the calibration of the UQ methods, Figure 5 shows PIT histograms for selected
target variables and 10 randomly chosen grid points. Most notable is the clear underdispersion
of the GNP, IFSP and RFP approaches for the surface variables (U10 and T2M). For Z500, the
GNP and IFSP forecasts are also underdispersed and show an additional bias, whereas the RFP
forecasts are better calibrated. The ECMWF ensemble forecasts are relatively well calibrated
for most combinations of target variable and lead time, but tend to show minor underdispersion
and biases. The best calibration can be observed for the PH methods, apart from minor biases
of DRN for Z500 forecasts at a lead time of 24 hours.
A complementary perspective on calibration is provided by the spread-skill plots in Figure 6,
whichshowstherelationshipbetweentheRMSEofthemeanforecastandthestandarddeviation
oftheensemblepredictionsofthedifferentUQmethodsforthetwotemperaturevariables. Fora
well-calibrated ensemble forecast, the temporal evolution of the RMSE and the ensemble spread
should roughly be equal (Fortin et al., 2014). As Figure 6 indicates, this is not the case for most
oftheUQmethodsconsideredhere4. InparticularfortheICmethods,thestandarddeviationis
notablylowerthantheRMSE,indicatingaclearunderestimationofthetrueforecastuncertainty
by these approaches. It is noteworthy that the standard deviation of the IFSP forecasts actually
decreases during the first 24h. This is somewhat surprising as the singular vectors at ECMWF
representthefastestgrowingperturbationsoveranoptimizationtimewindowof48hours. Thus,
a growth of the standard deviation would be expected as it is also the case in IFSP forecasts.
A similar behavior with initially slow perturbation growth was already documented in Selz
and Craig (2023) when Pangu-Weather was initialized from the members from the ECMWF
ensemble data assimilation. We attribute the different standard deviation growth between RFP
and IFSP to the fact that the RFP method leads to perturbations which are larger in scale than
the IFSP perturbations (Fig. 2) and thus grow faster initially. The two post-hoc methods show
a similar behavior, with a slight underdispersion but in general good calibration, for short lead
times even better than the ECMWF ensemble forecast. It should be kept in mind though that
the ECMWF ensemble forecasts also contain stochastic perturbed parametrization tendencies,
which lead to a better calibrated ensemble.
Figure 7 shows the mean bias for forecasts of selected target variables for all UQ methods.
A strong negative bias in the IFSP forecasts is apparent already at shorter lead times for T2M
and, in particular, Z500. The likely cause of this bias, which, in turn, explains the observed
bad performance of the IFSP approach, are systematic differences between the initial conditions
of the operational ECMWF ensemble and the ERA5 reanalysis fields, which we considered
4Inordertomakethepost-processingmethodscomparable,thepredictedmeanandstandarddeviationateach
gridpoint was extracted from the forecasts to compute the RMSE and spread, respectively.
12(a) U10
(b) T2M
(c) Z500
Figure 4: CRPSS of the different UQ methods over the spatial domain, using the ECMWF
ensemble as a reference method. The rows correspond to specific forecasting lead
times. Note that positive CRPSS values indicate an improvement over the reference
in terms of the CRPS at the respective grid point.
13(a) U10
ECMWF IFS GNP IFSP RFP EasyUQ DRN
2000 2000 2000 2000 2000 2000
1500 1500 1500 1500 1500 1500
1000 1000 1000 1000 1000 1000
500 500 500 500 500 500
00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00
2000 2000 2000 2000 2000 2000
1500 1500 1500 1500 1500 1500
1000 1000 1000 1000 1000 1000
500 500 500 500 500 500
00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00
2000 2000 2000 2000 2000 2000
1500 1500 1500 1500 1500 1500
1000 1000 1000 1000 1000 1000
500 500 500 500 500 500
00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00
(b) T2M
ECMWF IFS GNP IFSP RFP EasyUQ DRN
2000 2000 2000 2000 2000 2000
1500 1500 1500 1500 1500 1500
1000 1000 1000 1000 1000 1000
500 500 500 500 500 500
00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00
2000 2000 2000 2000 2000 2000
1500 1500 1500 1500 1500 1500
1000 1000 1000 1000 1000 1000
500 500 500 500 500 500
00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00
2000 2000 2000 2000 2000 2000
1500 1500 1500 1500 1500 1500
1000 1000 1000 1000 1000 1000
500 500 500 500 500 500
00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00
(c) Z500
ECMWF IFS GNP IFSP RFP EasyUQ DRN
2000 2000 2000 2000 2000 2000
1500 1500 1500 1500 1500 1500
1000 1000 1000 1000 1000 1000
500 500 500 500 500 500
00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00
2000 2000 2000 2000 2000 2000
1500 1500 1500 1500 1500 1500
1000 1000 1000 1000 1000 1000
500 500 500 500 500 500
00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00
2000 2000 2000 2000 2000 2000
1500 1500 1500 1500 1500 1500
1000 1000 1000 1000 1000 1000
500 500 500 500 500 500
00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00 00.00 0.25 0.50 0.75 1.00
Figure 5: PIT histograms for all UQ methods and selected target variables. The results are
aggregated over all test cases at 10 randomly chosen grid points. The rows correspond
to specific forecast lead times.
14
h42
:
emit
daeL
h27
:
emit
daeL
h861
:
emit
daeL
h42
:
emit
daeL
h27
:
emit
daeL
h861
:
emit
daeL
h42
: emit
daeL
h27
: emit
daeL
h861
: emit
daeL(a) T2M (b) T850
3.0 ECMWF IFS ECMWF IFS
3.5
GNP GNP
IFSP IFSP
2.5 3.0
RFP RFP
EasyUQ EasyUQ
2.5
2.0 DRN DRN
2.0
1.5
1.5
1.0
1.0
0.5
0.5
0.0 0.0
0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 175
Time [h] Time [h]
Figure 6: Spread-skill relationship between the RMSE of the ensemble mean and the ensemble
spread of the different UQ methods, averaged over all grid points. The solid lines
represents the RMSE, and the dotted line represents the standard deviations of the
different methods, respectively.
as ground truth. Further, there likely are differences in the representation of topography in
the operational version of the IFS model and ERA5 due to differences in the underlying grid
spacing. One pathway to addressing this, which has been suggested for example in Rasp et al.
(2023),mightbetoevaluatetheNWP-basedforecasts(including,potentially,theIFSPforecasts)
against the operational analysis. Although both post-processing methods operate separately
on every grid point, only the DRN approach shows a strong granular pattern, while the bias
pattern of the EasyUQ method appears notably more smooth and comparable to that of the
RFP approach. The GNP methods shows fairly small biases which are on a comparable level to
those of the better-performing RFP and DRN approaches. While no substantial differences in
the overall level of the bias among these methods can be observed, the DRN forecasts for Z500
at a lead time of 168 hours interestingly shows the most pronounced biases among all compared
methods.
6. Discussion and conclusions
To the best of our knowledge, our study is the first systematic comparison of different UQ
methods to generate probabilistic weather forecasts from the deterministic data-driven weather
model Pangu-Weather (Bi et al., 2023). The UQ approaches can be divided into IC-based
methods, where an ensemble forecast is generated by initializing Pangu-Weather model runs
fromdifferentsets ofinitialconditions, and PHmethods, which operateon deterministicPangu-
Weatherforecastsandgenerateprobabilisticforecastsfromthedeterministicmodelinputs,based
on past forecasts and corresponding observations. Overall, our results suggest that most of the
UQ methods are able to provide probabilistic forecasts competitive with the operational (non-
post-processed) ECMWF ensemble forecast.
Whiletheresultsdiffersubstantiallybyvariableandforecastleadtime,theRFP,EasyUQand
DRNapproachesperformgenerallysimilartotheoperationalECMWFensemble,whiletheGNP
and IFSP approaches fail to achieve comparable forecast skill. The most notable improvements
over the ECMWF ensemble are achieved for 2-m temperature, where the use of the Pangu-
Weather model in concert with UQ methods yields improvements in terms of the CRPS for lead
15(a) U10
(b) T2M
(c) Z500
Figure 7: Bias of the mean forecast of different UQ methods for different lead times, averaged
over the test period.
16times up to around 120 hours. Generally, the PH methods (EasyUQ and, in particular, DRN)
yield the best forecasts at shorter lead times, whereas the RFP approach yields better forecasts
at longer lead times. As discussed in Section 4.3, we use the ERA5 data as ground truth for
evaluation throughout, and the model rankings might change if the operational analysis was
used instead.
TheICapproacheshavetheadvantagethattheygeneraterealisticspatialforecastfields,asthe
input is perturbed over the whole spatial domain, whereas the PH methods generate a separate
predictive distribution at every grid point. In particular, the RFP approach seems promising in
that IC ensembles can be straightforwardly generated from past observation data with minimal
tuning. However, the use of the IC methods comes at the cost of having to run the deterministic
data-driven weather model multiple times, which can be demanding in terms of the comput-
ing and disk space requirements, in particular for generating and storing global high-resolution
ensemble forecasts. By contrast, PH methods require a training dataset of past forecasts from
the deterministic data-driven weather model and corresponding observations. While they have
the advantage of potentially correcting systematic errors such as biases of the underlying deter-
ministic model, additional modeling steps are required to generate spatially coherent forecast
fields. A variety of two-step methods for multivariate post-processing is available, where in a
firststep, forecastsarepost-processedseparatelyateverygridpointorleadtime(usingmethods
like, e.g., EasyUQ or DRN). In a second step, multivariate (e.g., spatial or temporal) dependen-
cies are introduced by re-ordering samples from the univariate forecast distribution according
to a dependence template via the use of copula functions. Popular approaches include the use
of empirical copulas based on the physics-based NWP ensemble models (ensemble copula cou-
pling, ECC; Schefzik et al., 2013), or based on past observations (Schaake shuffle; Clark et al.,
2004). Comprehensive comparisons are for example available in Lerch et al. (2020) and Lakatos
et al. (2023). In the context of post-processing data-driven weather model forecasts, the use
of the ECC method comes with the benefit of obtaining a hybrid combination of a data-driven
modelproducingtheunivariateforecastsateachgridpoint,andaphysics-basedensemblemodel
that provides information on the spatio-temporal dependencies. Recently proposed multivariate
post-processing methods based on generative ML (Chen et al., 2024) further have the potential
to better utilize various sources of input information and improve the multivariate probabilistic
forecasts.
The main objective of our study was to provide a general proof of concept for how to generate
probabilistic forecasts from deterministic data-driven weather models. It should be seen as a
first step into the direction of probabilistic data-driven weather models, and our results provide
several avenues for further generalization and analysis. A natural benchmark method, which
is missing from our comparisons, is the combination of the ECMWF ensemble forecasts with
post-processing. Correcting systematic errors in NWP ensemble predictions was the original
motivation for the development of post-processing methods such as DRN, and notable improve-
ments in terms of the CRPS have been observed in numerous studies (Vannitsem et al., 2021).
We were not able to include post-processed ECMWF ensemble forecasts since it proved difficult
to obtain a suitably large archive of past forecasts due to exceedingly slow download processes.
Nonetheless, this comparison would be of substantial interest. Recently, Bremnes et al. (2023)
comparepost-processedPangu-Weatherandphysics-basedweatherforecastsonastationdataset
over Norway and find that the forecast quality tends to be very similar after post-processing.
Further, our comparisons have been focused on selected target variables, on using the gridded
ERA5 data as ground truth, and on the CRPS as main evaluation metric. Operational weather
services tend to evaluate their forecasts against the model‚Äôs own operational analysis, station
observations (Rasp et al., 2023), and comprehensive comparisons of the UQ methods constitute
an interesting starting point for future research. Over Europe, suitable station observation data
has for example been collected within the EUPPBench benchmark dataset for post-processing
17(Demaeyer et al., 2023). Another important open question regarding the potential and lim-
itations of data-driven weather models is whether they can reliably predict extreme weather
events. Therefore, a targeted evaluation of the UQ methods in this regard, e.g., using proper
weighted scoring rules (Lerch et al., 2017), represents another important direction for future
model comparisons.
The large data volumes and high dimensionality of global gridded predictions further poses
a challenge regarding the scalability of ML-based post-processing methods such as DRN, for
which it is an open question whether they will generalize well to global high-resolution forecasts.
This calls for the development of new spatial post-processing methods which operate on the
spatial forecast fields directly and are able to leverage predictive information present in the
spatial structures. Over the past years, several approaches have been proposed, which utilize
convolutionalneuralnetworkortransformerarchitecturestoenableprobabilisticpost-processing
ofspatialforecastfields(e.g.,Gr¬®onquistetal.,2021;Ashkboosetal.,2022;Chapmanetal.,2022;
Ben-Bouall`egue et al., 2023b; Horat and Lerch, 2024). The recently introduced WeatherBench
2 dataset (Rasp et al., 2023) provides a useful framework for comparisons.
Asdiscussedabove,theICapproachesaregenerallydisadvantagedbytheirinabilitytoaccount
for sources of uncertainty beyond initial condition uncertainty. One approach to address this
might be to add scaled-down IC uncertainty information during the forward integration of the
data-driven weather model, for example based on the use of perturbations determined from past
analysis states. Further, online bias correction or post-processing during the forward integration
mighthelptoalleviatesystematicerrorssuchasthoseobservedfortheIFSPapproachandmight
constitute an interesting approach for combining the advantages of IC and PH methods.
Acknowledgments
The research leading to these results has been done within the project ‚ÄúData-driven weather
models: Towards improved uncertainty quantification, interpretability and efficiency‚Äù funded by
the Young Investigator Network at KIT. Christopher Bu¬®lte, Nina Horat and Sebastian Lerch
gratefully acknowledges support by the Vector Stiftung through the Young Investigator Group
‚ÄúArtificialIntelligenceforProbabilisticWeatherForecasting‚Äù. ThecontributionofJulianQuint-
ing was funded by the European Union (ERC, ASPIRE, 101077260). We thank Delong Chen,
Jieyu Chen, Charlotte Debus, Tilmann Gneiting, Christian Grams, Peter Knippertz, Linus
Magnusson, and Jannik Wilhelm for helpful discussions. ECMWF and Deutscher Wetterdient
are acknowledged for granting access to the operational ensemble forecast data. The authors
acknowledge support by the state of Baden-Wu¬®rttemberg through bwHPC.
References
Angelopoulos, A. N. and Bates, S. (2021). A gentle introduction to conformal prediction and
distribution-free uncertainty quantification. Preprint, available at https://arxiv.org/abs/
2107.07511.
Ashkboos, S., Huang, L., Dryden, N., Ben-Nun, T., Dueben, P. D., Gianinazzi, L., Kummer,
L. N. and Hoefler, T. (2022). ENS-10: A dataset for post-processing ensemble weather fore-
casts. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track.
Bauer, P., Thorpe, A. and Brunet, G. (2015). The quiet revolution of numerical weather pre-
diction. Nature, 525, 47‚Äì55.
18Ben-Bouall`egue, Z., Clare, M. C., Magnusson, L., Gascon, E., Maier-Gerber, M., Janousek, M.,
Rodwell, M., Pinault, F., Dramsch, J. S., Lang, S. T., Raoult, B., Rabier, F., Chevallier, M.,
Sandu, S., Dueben, P., Chantry, M. and Pappenberger, F. (2023a). The rise of data-driven
weather forecasting. Preprint https://arxiv.org/abs/2307.10128.
Ben-Bouall`egue, Z., Weyn, J. A., Clare, M. C., Dramsch, J., Dueben, P. and Chantry, M.
(2023b). Improving medium-range ensemble weather forecasts with hierarchical ensemble
transformers. Preprint, available at https://arxiv.org/abs/2303.17195.
Bi, K., Xie, L., Zhang, H., Chen, X., Gu, X. and Tian, Q. (2023). Accurate medium-range
global weather forecasting with 3D neural networks. Nature, 619, 533‚Äì‚Äì538.
Bremnes,J.B.,Nipen,T.N.andSeierstad,I.A.(2023). Evaluationofforecastsbyaglobaldata-
driven weather model with and without probabilistic post-processing at Norwegian stations.
Preprint, available at https://arxiv.org/abs/2309.01247.
Brenowitz, N. D., Cohen, Y., Pathak, J., Mahesh, A., Bonev, B., Kurth, T., Durran, D. R.,
Harrington, P. and Pritchard, M. S. (2024). A Practical Probabilistic Benchmark for AI
Weather Models. Preprint, available at https://arxiv.org/abs/2401.15305.
Buizza, R., Leutbecher, M. and Isaksen, L. (2008). Potential use of an ensemble of analyses
in the ECMWF ensemble prediction system. Quarterly Journal of the Royal Meteorological
Society, 134, 2051‚Äì2066.
Chapman,W.E.,Monache,L.D.,Alessandrini,S.,Subramanian,A.C.,Ralph,F.M.,Xie,S.-P.,
Lerch, S. and Hayatbini, N. (2022). Probabilistic predictions from deterministic atmospheric
river forecasts with deep learning. Monthly Weather Review, 150, 215‚Äì234.
Chen, J., Janke, T., Steinke, F. and Lerch, S. (2024). Generative machine learning methods for
multivariate ensemble post-processing. Annals of Applied Statistics, 18, 159‚Äì183.
Chen, K., Han, T., Gong, J., Bai, L., Ling, F., Luo, J.-J., Chen, X., Ma, L., Zhang, T.,
Su, R., Ci, Y., Li, B., Yang, X. and Ouyang, W. (2023a). FengWu: Pushing the Skillful
Global Medium-range Weather Forecast beyond 10 Days Lead. Preprint, available at https:
//arxiv.org/abs/2304.02948.
Chen, L., Du, F., Hu, Y., Wang, F. and Wang, Z. (2023b). SwinRDM: Integrate SwinRNN with
Diffusion Model towards High-Resolution and High-Quality Weather Forecasting. Preprint,
available at https://arxiv.org/abs/2306.03110.
Chen, L., Zhong, X., Zhang, F., Cheng, Y., Xu, Y., Qi, Y. and Li, H. (2023c). FuXi: A cascade
machine learning forecasting system for 15-day global weather forecast. npj Climate and
Atmospheric Science, 6,.
Clark, M., Gangopadhyay, S., Hay, L., Rajagopalan, B. and Wilby, R. (2004). The Schaake
shuffle: A method for reconstructing space‚Äìtime variability in forecasted precipitation and
temperature fields. Journal of Hydrometeorology, 5, 243‚Äì262.
Demaeyer, J., Bhend, J., Lerch, S., Primo, C., Schaeybroeck, B. V., Atencia, A., Bouall`egue,
Z. B., Chen, J., Dabernig, M., Evans, G., Pucer, J. F., Hooper, B., Horat, N., Jobst, D.,
MerÀáse, J., Mlakar, P., M¬®oller, A., Mestre, O., Taillardat, M. and Vannitsem, S. (2023). The
EUPPBench postprocessing benchmark dataset v1.0. Earth System Science Data, 15, 2635‚Äì
2653.
Fortin, V., Abaza, M., Anctil, F. and Turcotte, R. (2014). Why should ensemble spread match
the rmse of the ensemble mean? Journal of Hydrometeorology, 15, 1708‚Äì1713.
19Gneiting, T., Balabdaoui, F. and Raftery, A. E. (2007). Probabilistic forecasts, calibration
and sharpness. Journal of the Royal Statistical Society Series B: Statistical Methodology, 69,
243‚Äì268.
Gneiting, T. and Katzfuss, M. (2014). Probabilistic forecasting. Annual Review of Statistics
and Its Application, 1, 125‚Äì151.
Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association, 102, 359‚Äì378.
Gneiting, T., Schulz, B. and Lerch, S. (2023). Probabilistic solar forecasting: Benchmarks,
post-processing, verification. Solar Energy, 252, 72‚Äì80.
Gr¬®onquist, P., Yao, C., Ben-Nun, T., Dryden, N., Dueben, P., Li, S. and Hoefler, T. (2021).
Deep learning for post-processing ensemble weather forecasts. Philosophical Transactions of
the Royal Society A, 379, 20200092.
Henzi, A., Ziegel, J. F. and Gneiting, T. (2021). Isotonic distributional regression. Journal of
the Royal Statistical Society Series B: Statistical Methodology, 83, 963‚Äì993.
Hersbach, H., Bell, B., Berrisford, P., Hirahara, S., Hor¬¥anyi, A., MunÀúoz-Sabater, J., Nicolas, J.,
Peubey, C., Radu, R., Schepers, D., Simmons, A., Soci, C., Abdalla, S., Abellan, X., Balsamo,
G., Bechtold, P., Biavati, G., Bidlot, J., Bonavita, M., De Chiara, G., Dahlgren, P., Dee, D.,
Diamantakis, M., Dragani, R., Flemming, J., Forbes, R., Fuentes, M., Geer, A., Haimberger,
L., Healy, S., Hogan, R. J., H¬¥olm, E., Janiskov¬¥a, M., Keeley, S., Laloyaux, P., Lopez, P.,
Lupu, C., Radnoti, G., de Rosnay, P., Rozum, I., Vamborg, F., Villaume, S. and Th¬¥epaut,
J.-N. (2020). The ERA5 global reanalysis. Quarterly Journal of the Royal Meteorological
Society, 146, 1999‚Äì2049.
Horat, N. and Lerch, S. (2024). Deep learning for post-processing global probabilistic forecasts
on sub-seasonal time scales. Monthly Weather Review, 152, in press.
Isaksen, L., Bonavita, M., Buizza, R., Fisher, M., Haseler, J., Leutbecher, M. and Raynaud, L.
(2010). Ensemble of data assimilations at ECMWF. ECMWF Technical Memorandum 636,
available at https://doi.org/10.21957/obke4k60.
Jordan,A.,Kru¬®ger,F.andLerch,S.(2019). EvaluatingprobabilisticforecastswithscoringRules.
Journal of Statistical Software, 90, 1‚Äì37.
Keisler, R. (2022). Forecasting global weather with graph neural networks. Preprint, available
at https://arxiv.org/abs/2202.07575.
Kochkov, D., Yuval, J., Langmore, I., Norgaard, P., Smith, J., Mooers, G., Lottes, J., Rasp,
S., Du¬®ben, P., Kl¬®ower, M., Hatfield, S., Battaglia, P., Sanchez-Gonzalez, A., Willson, M.,
Brenner, M. P. and Hoyer, S. (2023). Neural general circulation models. Preprint, available
at https://arxiv.org/abs/2311.07222.
Lakatos, M., Lerch, S., Hemri, S. and Baran, S. (2023). Comparison of multivariate post-
processing methods using global ECMWF ensemble forecasts. Quarterly Journal of the Royal
Meteorological Society, 149, 856‚Äì877.
Lam,R.,Sanchez-Gonzalez,A.,Willson,M.,Wirnsberger,P.,Fortunato,M.,Pritzel,A.,Ravuri,
S.,Ewalds,T.,Alet,F.,Eaton-Rosen,Z.,Hu,W.,Merose,A.,Hoyer,S.,Holland,G.,Vinyals,
O., Stott, J., Pritzel, A., Mohamed, S. and Battaglia, P. (2022). GraphCast: Learning skillful
medium-range global weather forecasting. Preprint, available at https://arxiv.org/abs/
2212.12794.
20Lerch, S., Baran, S., M¬®oller, A., Gro√ü, J., Schefzik, R., Hemri, S. and Graeter, M. (2020).
Simulation-based comparison of multivariate ensemble post-processing methods. Nonlinear
Processes in Geophysics, 27, 349‚Äì371.
Lerch, S., Thorarinsdottir, T. L., Ravazzolo, F. and Gneiting, T. (2017). Forecaster‚Äôs dilemma:
Extreme events and forecast evaluation. Statistical Science, 32, 106‚Äì127.
Lessig, C., Luise, I., Gong, B., Langguth, M., Stadler, S. and Schultz, M. (2023). AtmoRep: A
stochastic model of atmosphere dynamics using large scale representation learning. Preprint,
available at https://arxiv.org/abs/2308.13280.
Leutbecher,M.andPalmer,T.(2008). Ensembleforecasting. Journal of Computational Physics,
227, 3515‚Äì3539.
Magnusson,L.,Nycander,J.andK¬®all¬¥en,E.(2009). Flow-dependentversusflow-independentini-
tialperturbationsforensembleprediction. Tellus A: Dynamic Meteorology and Oceanography,
61, 194‚Äì209.
Matheson,J.E.andWinkler,R.L.(1976). Scoringrulesforcontinuousprobabilitydistributions.
Management science, 22, 1087‚Äì1096.
Nguyen, T., Brandstetter, J., Kapoor, A., Gupta, J. K. and Grover, A. (2023a). ClimaX: A
foundation model for weather and climate. Preprint, available at https://arxiv.org/abs/
2301.10343.
Nguyen, T., Shah, R., Bansal, H., Arcomano, T., Madireddy, S., Maulik, R., Kotamarthi, V.,
Foster, I. and Grover, A. (2023b). Scaling transformer neural networks for skillful and reliable
medium-range weather forecasting. Preprint, available at https://arxiv.org/abs/2312.
03876.
Palmer, T. (2019a). The ECMWF ensemble prediction system: Looking back (more than) 25
years and projecting forward 25 years. Quarterly Journal of the Royal Meteorological Society,
145, 12‚Äì24.
Palmer, T. N. (2019b). Stochastic weather and climate models. Nature Reviews Physics, 1,
463‚Äì471.
Pathak, J., Subramanian, S., Harrington, P., Raja, S., Chattopadhyay, A., Mardani, M., Kurth,
T., Hall, D., Li, Z., Azizzadenesheli, K., Hassanzadeh, P., Kashinath, K. and Anandkumar,
A. (2022). FourCastNet: A global data-driven high-resolution weather model using adaptive
Fourier neural operators. Preprint, available at https://arxiv.org/abs/2202.11214.
Price, I., Sanchez-Gonzalez, A., Alet, F., Ewalds, T., El-Kadi, A., Stott, J., Mohamed, S.,
Battaglia, P., Lam, R. and Willson, M. (2023). Gencast: Diffusion-based ensemble forecasting
for medium-range weather. Preprint, available at https://arxiv.org/abs/2312.15796.
Primo, C., Schulz, B., Lerch, S. and Hess, R. (2024). Comparison of Model Output Statistics
andNeuralNetworkstoPostprocessWindGusts. Preprint, availableathttps://arxiv.org/
abs/2401.11896.
Rasp, S., Hoyer, S., Merose, A., Langmore, I., Battaglia, P., Russel, T., Sanchez-Gonzalez, A.,
Yang, V., Carver, R., Agrawal, S., Chantry, M., Bouallegue, Z. B., Dueben, P., Bromberg,
C., Sisk, J., Barrington, L., Bell, A. and Sha, F. (2023). WeatherBench 2: A benchmark
for the next generation of data-driven global weather models. Preprint, available at https:
//arxiv.org/abs/2308.15560.
21Rasp, S. and Lerch, S. (2018). Neural networks for postprocessing ensemble weather forecasts.
Monthly Weather Review, 146, 3885‚Äì3900.
Schefzik, R., Thorarinsdottir, T. L. and Gneiting, T. (2013). Uncertainty quantification in
complex simulation models using ensemble copula coupling. Statistical Science, 28, 616‚Äì640.
Schulz, B.andLerch, S.(2022). Machinelearningmethodsforpostprocessingensembleforecasts
of wind gusts: A systematic comparison. Monthly Weather Review, 150, 235‚Äì257.
Selz, T. and Craig, G. C. (2023). Can artificial intelligence-based weather prediction models
simulate the butterfly effect? Geophysical Research Letters, 50, e2023GL105747.
Vannitsem, S., Bremnes, J. B., Demaeyer, J., Evans, G. R., Flowerdew, J., Hemri, S., Lerch,
S., Roberts, N., Theis, S., Atencia, A., Bouall`egue, Z. B., Bhend, J., Dabernig, M., Cruz,
L. D., Hieta, L., Mestre, O., Moret, L., Plenkovi¬¥c, I. O., Schmeits, M., Taillardat, M., den
Bergh, J.V., Schaeybroeck, B.V., Whan, K.andYlhaisi, J.(2021). Statisticalpostprocessing
for weather forecasts: Review, challenges, and avenues in a big data world. Bulletin of the
American Meteorological Society, 102, E681‚ÄìE699.
Walz, E.-M., Henzi, A., Ziegel, J. and Gneiting, T. (2024a). Easy Uncertainty Quantification
(EasyUQ): Generating predictive distributions from single-valued model output. SIAM Re-
view, 66, 91‚Äì122.
Walz, E.-M., Knippertz, P., Fink, A. H., K¬®ohler, G. and Gneiting, T. (2024b). Physics-based
vs. data-driven 24-hour probabilistic forecasts of precipitation for northern tropical Africa.
Preprint, available at https://arxiv.org/abs/2401.03746.
22A. Results for FourCastNet
Here, we present additional results for the previously introduced UQ methods utilizing the
FourCastNet (Pathak et al., 2022) model as the underlying data-driven weather model. For
this purpose, FourCastNet version v0.0.0 was used, based on code accompanying the original
publication5.
Table 2: Mean CRPS of all methods and variables across the spatial domain for three different
groupsofleadtimes,withthebest-performingmethodhighlightedinbold,respectively.
TheresultsshownhereareanalogoustothoseinTable1,butbasedontheFourCastNet
model. Note that the CRPS values for Z500 are scaled by a factor of 0.01.
Variable ECMWF IFS GNP IFSP RFP EasyUQ DRN
U10 0.54 0.67 0.68 0.64 0.62 0.60
V10 0.54 0.67 0.68 0.63 0.61 0.59
Short time
T2M 0.57 0.66 0.73 0.62 0.56 0.54
0h - 48h
T850 0.43 0.56 0.57 0.51 0.52 0.49
Z500 0.33 0.74 0.71 0.78 0.68 0.72
U10 0.96 1.35 1.41 1.30 1.38 1.35
V10 0.96 1.37 1.43 1.32 1.39 1.37
Mid time
T2M 0.75 1.01 1.07 0.97 0.95 0.93
48h - 120h
T850 0.75 1.13 1.18 1.09 1.16 1.12
Z500 1.21 2.44 2.50 2.35 2.42 2.40
U10 1.54 1.93 2.04 1.87 1.99 1.97
V10 1.58 1.99 2.11 1.93 2.05 2.01
Long time
T2M 1.05 1.44 1.51 1.38 1.45 1.42
120h+
T850 1.33 1.86 1.98 1.80 2.02 1.92
Z500 2.91 4.44 4.76 4.31 4.80 4.66
Table 2 shows the mean CRPS results over all test samples for different groups of lead times,
and Figure 8 shows the mean CRPS as a function of the lead time. Overall, we observe qual-
itatively similar results to the probabilistic forecasts based on the Pangu-Weather model, but
the forecast quality is notably worse. This is likely due to the worse forecast performance of
the underlying FourCastNet model compared to Pangu-Weather. While analogous rankings be-
tween IC and PH approaches can be observed, forecasts of T2M and lead times below around
50 hours are the only case among all considered methods and target variables, where any of the
UQ methods can achieve any improvements over the operational ECMWF ensemble.
Figure9showstheCRPSSofthedifferentmethodsoverthespatialdomainforselectedtarget
variables and lead times. Compared to the corresponding results for the Pangu-Weather model,
the results are notably worse everywhere, but some improvements over the ECMWF ensemble
can be observed over the land grid points, in particular at higher altitudes and for the PH
methods.
5https://github.com/NVlabs/FourCastNet
23T2M T850
1.8
1.6
2.0
1.4
1.2 1.5
1.0
1.0
0.8
0.6
0.5
0.4
0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 175
U10 V10
2.0 2.0
1.5 1.5
1.0 1.0
0.5 0.5
0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 175
Z500 Forecast lead time [h]
500 ECMWF IFS
400 GNP
IFSP
300
RFP
200 EasyUQ
DRN
100
0
0 25 50 75 100 125 150 175
Forecast lead time [h]
Figure 8: Mean CRPS as a function of the forecast lead time for the different UQ methods,
aggregated over all locations. The results shown here are analogous to those in Figure
3, but based on the FourCastNet model.
24
SPRC
SPRC
SPRC(a) U10
(b) T2M
(c) Z500
Figure 9: CRPSS of the different UQ methods over the spatial domain, using the ECMWF
ensemble as a reference method. The rows correspond to specific forecasting lead
times. Note thatpositive CRPSS values indicate animprovement over the referencein
terms of the CRPS at the respective grid point. The results shown here are analogous
to those in Figure 4, but based on the FourCastNet model.
25