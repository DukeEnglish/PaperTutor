LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliency
Prediction
FLORIANSTROHM,InstituteforVisualizationandInteractiveSystems,Germany
MIHAIBÂCE,InstituteforVisualizationandInteractiveSystems,Germany
ANDREASBULLING,InstituteforVisualizationandInteractiveSystems,Germany
Fig.1. Ourmethod(left)takesasinputasmallsetofimagesandtheircorrespondinguser-specificsaliencymapsobtainedfrom
humangazerecordedusingastationaryeyetrackerandproducesoneuserembeddingforeachuser,whichcapturestheuser-specific
differencesinviewingbehaviour.Wethendemonstrate(right)howthelearnedembeddingscanbeusedtorefineauniversalsaliency
map(e.g.obtainedfromasaliencypredictor)toanindividual,personalsaliencymapforanynewimages.
Reusableembeddingsofuserbehaviourhaveshownsignificantperformanceimprovementsforthepersonalisedsaliencyprediction
task.However,priorworksrequireexplicitusercharacteristicsandpreferencesasinput,whichareoftendifficulttoobtain.Wepresent
anovelmethodtoextractuserembeddingsfrompairsofnaturalimagesandcorrespondingsaliencymapsgeneratedfromasmall
amountofuser-specificeyetrackingdata.AtthecoreofourmethodisaSiameseconvolutionalneuralencoderthatlearnstheuser
embeddingsbycontrastingtheimageandpersonalsaliencymappairsofdifferentusers.Evaluationsontwopublicsaliencydatasets
showthatthegeneratedembeddingshavehighdiscriminativepower,areeffectiveatrefininguniversalsaliencymapstotheindividual
users,andgeneralisewellacrossusersandimages.Finally,basedonourmodel’sabilitytoencodeindividualusercharacteristics,our
workpointstowardsotherapplicationsthatcanbenefitfromreusableembeddingsofgazebehaviour.
CCSConcepts:•Human-centeredcomputing→Usermodels;•Computingmethodologies→Interestpointandsalientregion
detections.
AdditionalKeyWordsandPhrases:gaze,eye-tracking,saliency,personalsaliency,userembeddings,usermodel,deeplearning
1 INTRODUCTION
Saliencypredictionisthetaskofidentifyingsalientregionswithinanimagewhicharelikelytoattractgaze.Various
modelshavebeendevelopedwhichtakeintoaccountbothlow-levelfeatures[26,27,57]andhigh-levelimagecharac-
teristics[16,37,43],incorporatingbottom-upattentionmechanisms,aswellastaskdemands[11,41,61],whichinvolve
top-downattentionprocesses.Giventhepotentialforanticipatinguserattention,saliencypredictionmodelshavehad
1
4202
raM
02
]VC.sc[
1v35631.3042:viXraStrohmetal.
significantimpactincomputervisionandbeyond,andhaveprovenhighlybeneficialforawiderangeoftasks,from
servingasaninductivebiasforneuralattentionmechanisms[19,50,51]toestimatingusers’cognitivestates[1,25,56],
orenablingpersonalisedpredictionsforvarioushuman-computer-interactiontasks[3,20,39,55,64].
Alargebodyofworkonsaliencymodellinghasfocusedonuniversalsaliency,i.e.thetaskofpredictingsaliencymaps
thataggregategazedatafrommultipleobserversand,assuch,disregardindividualdifferencesinviewingbehaviour.
Thereare,however,significantindividualvariationsinhowvisualattentionisdeployedonimagestimulithatare
duetoarangeoffactors,suchasscenecomplexityandsemantics[13,62],levelofexpertise[6,9,14,49],age[63],or
personalitytraits[4,45].
Despitethesedifferencesamongindividualsandthemanyapplications(e.g.assistivesystems[47,53,54])that
couldbenefitfromabetterunderstandingoftheindividual,onlyfewpreviousworkshaveproposedmethodsto
predictpersonalisedsaliency.Oneapproachinvolvedtraininganindividual(sub-)modelforeachuser,whichlacks
generalisability[42,59,60].Anotheroneleveragesperson-specificinformationsuchasage,gender,orpreference
towardsspecificobjectcategoriesorcolours[59].However,inadditiontoraisingprivacyconcerns,collectingthese
characteristicsistediousandrequiresexplicituserinput.
Incontrasttoexplicitlycollectinguserinformation,weintroduceanovelmethodtoextractembeddingsfromusers’
gazebehaviourwhileviewingnaturalimages.OurmethodusesaSiameseconvolutionalneuralencoderthattakes
multipleimagesandtheircorrespondingsaliencymapsofaparticularuserasinputandproducesauserembeddingas
output.Theembeddingislearnedbycontrastinginputpairsfromoneusertootherusersexhibitingdifferentgaze
behavioursonthesameimagestimuli.Byintegratingtheuserembeddingintoasaliencypredictionnetwork(Figure1),
thisadditionalinputplaysacrucialroleinpredictingfiltersthatareconvolvedoverextractedimagefeatures,thus
integratinguser-specificinformationessentialforthepersonalisedsaliencypredictiontask.Ourfindingsdemonstrate
thehighlydiscriminativenatureoftheseembeddings,enablingustoeffectivelycompareindividualsbasedontheir
distinctivegazebehaviour(Figure6).Resultsonthedownstreamtaskofpersonalsaliencypredictiontaskshowhow
thegeneratedembeddingscanbeusedtoeffectivelyrefinetheuniversalsaliencymappredictionsandtailorthemto
individualusers.Moreover,weobservethattheseembeddingsexhibitgoodgeneralisationcapabilitieswhenappliedto
bothunseenusersandimages.1
2 RELATEDWORK
2.1 IndividualDifferencesinVisualSaliency
Traditionalsaliencypredictionmethodsignoreindividualdifferencesinvisualsaliencebetweenhumansandinstead
predictanaverage,universalsaliencymap[5].However,therearemultiplepriorworksthatshowthathumanshave
differentvisualpreferenceswhichdrawstheirattention,whicharestableandpredictable.DeHaasandLinkaet
al.[13,38]haveidentifiedmultiplesemanticdimensionsalongwhichhumansaliencesignificantlydiffers,likefaces.
Priorworkshaveincorporatedfacedetectorsinsaliencypredictionpipelines,astheygenerallytendtoattractsignificant
attention[5,10].However,theresultsfromDeHaasetal.showthatforspecifichumans,suchpredictionsareimprecise
astheirattentionisnotattractedbyfaces.LaterBrodaetal.[7,8]identifiedthathumanscanberoughlyclusteredin
twocategorieswhenobservingpersons.Eithertheytendtofocustheheadandinnerfacialfeaturesortheyfixateon
bodypartslikearmsandlegs.Priorworkhasalsostudiedwhichhumantraitsinfluencetheindividualattentionand
foundthatforexampleage[33]isanimportantfactoraswellaspersonality[23]andgender[46].Xuetal.[59]were
1Projectcodewillbemadepubliclyavailableuponacceptance.
2LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
thefirstthatproposedamethodtopredictpersonalisedinsteadofuniversalsaliencybyutilisingsuchusertraitsasan
additionalinputtotheirnetwork.However,itisstillunclearwhichusertraitsareusefulforsaliencyprediction,and
explicitlycollectingpersonalinformationmightnotbeappropriate.Morotoetal.[42]laterproposedamethodinvolving
amulti-taskCNNtopredictpersonalisedsaliencyforeachuserinthetrainingdataset.Duringinference,unseenusers
werematchedtotheseenusersbasedontheirsimilarityinattentionallocation.However,thisrequiresmanydifferent
usersinthetrainingdatatogeneralisetounseenusers.Moreover,findinganappropriatesimilarityfunctiontomatch
unseentoseenusersbasedontheirgazebehaviourischallenging.Incontrast,weproposetotrainasingle-taskCNN
thatincorporatesanadditionaluserembeddingasinput,enablingthenetworktoleverageuser-specificinformation.
2.2 UserEmbeddings
Userembeddingshavefoundapplicationsacrossdiversedomains,servingeitherasameanstoinferuser-related
informationortopersonalisetheuserexperience.InaseminalworkbyPazzanietal.[44],anapproachwasintroduced
whereanagentlearnsauserembeddingbyleveragingexplicitfeedbackprovidedbyusersintheformofpageratings.
Thisuserembeddingwasthenutilisedtoprovidepersonalisedwebsiterecommendationstailoredtotheuser’sinterests
andpreferences.Later,variousmethodshavebeenproposedtoconstructuserembeddingsbyincorporatingdiverse
formsofapplication-specificexplicitfeedback[34,48].Methodsthatcollectuserinformationexplicitlyrequiretheuser
tobeactiveandmightbecomeinaccurateovertimeastheuser’sinterestchanges.
Toaddressthislimitation,implicitmethodstocreateuserembeddingshavegainedpopularity.Thesemethods
allowuserstosimplyinteractwithasystemwhilethesystemcreatesauserembedding,withoutrelyingonexplicit
feedback[15,17].InarecentworkbyWuetal.[58],anovelapproachcalledAuthor2Vecwasintroducedtoderiveuser
embeddingsbyanalysingthetextualcontentauthoredbyusersonsocialmediaplatforms.Theauthorsdemonstrated
thesuperiorperformanceoftheseembeddingsinpredictinguserpersonalitytraitsormentalstatescomparedto
alternativemethods.Similarly,Anetal.[2]usedwebbrowsingeventstoextractrichuserembeddingsfordifferent
downstreamtasks.Inotherrelatedworks,researchersinvestigatedtheincorporationofuserembeddingsasunique
wordtokensalongsidetheuser’stext.Thisapproachenablesthemodeltocomprehendthesentenceinthecontextof
theuserembedding[40,65].Intheeye-gazedomainHeetal.[18]haveutilisedappearancebaseduserembeddings
extractedfromfacestopersonaliseanappearance-basedgazeestimator.Theseworksshowthepotentialtoleverage
implicituserembeddingsonarangeofapplications.Inourwork,tothebestofourknowledge,wearethefirstto
learnuserembeddingsfromvisualattentivebehaviourandleveragethemtoenhanceperformanceonthepersonalised
saliencypredictiontask.
3 METHODOLOGY
Traditionalsaliencypredictionmodelslearnafunction𝑓(𝐼)=USM𝐼,where𝐼isanimageandUSM𝐼 isthecorresponding
UniversalSaliencyMap(USM).ThegroundtruthUSMiscalculatedbyaveraging𝑛PersonalSaliencyMaps(PSM)obtained
fromdifferenthumansobservingthesameimage:USM𝐼 = 𝑛1 (cid:205)𝑛 𝑘=1PSM𝐼,𝑘.InsteadofpredictingUSM𝐼,ourgoalis
topredictPSM𝐼,𝑈 foragivenimage𝐼 anduser𝑈.TopredictPSM𝐼,𝑈 itisessentialtoincorporateadditionalinput
informationthatisspecifictotheuser𝑈 forwhomthepredictionsareintended.
3.1 LearningUserEmbeddings
Weproposeanovelmethodforextractinguserembeddings𝑒
𝑈
foreachindividualuser𝑈.Theseembeddingsare
derivedfromthedistinctvariationsobservedinthevisualattentionpatternsofindividualusers.Ourhypothesisisthat
3Strohmetal.
Fig.2. Thearchitectureofourproposeduserembeddingextractorinvolvesprocessingmultipleimagesalongsideanadditional
channelthatincludesthesaliencyinformationofaspecificuser,fromwhichweaimtoextractanembedding.Toaccomplishthis,we
employaSiameseconvolutionalneuralnetwork,whichisresponsibleforextractingfeaturesfromeachpairofimage-saliencymaps.
Subsequently,theextractedfeaturesareaveragedandnormalised,resultingintheuserembedding.
theseembeddingscanbeeffectivelyusedbyapersonalisedsaliencypredictionmodeltogenerateuser-specificsaliency
outputs.
ThearchitectureofourproposedembeddingneuralnetworkisshowninFigure2.ASiameseuserembedding
extractor𝐸 takes𝑚differentimages{𝐼 1,...,𝐼 𝑚}alongwiththecorrespondingPSMs{PSM𝐼 1,𝑈,...,PSM𝐼𝑚,𝑈}forthe
sameuser𝑈 asinput.Thegoalistoextractjointimage-saliencyfeatureswhichallowthenetworktounderstand
thevisualpreferencesoftheuserandextractameaningfuluserembedding.ThePSMforeachimageistreatedas
anadditionalimagechannelbesidestheexistingthreeRGBchannelsandresizedtoaresolutionof160×120.Each
image-PSMtensorispassedthroughfourconvolutionblocksconsistingofa2Dconvolutionlayerwithstridetwo,a
4LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
batchnormalisationlayer[24]andaRectifiedLinearUnit(ReLU)activationfunction.Subsequently,aglobalaverage
poolinglayer[36]isemployedtoreducetheoutputtoaone-dimensionalvector,whichhelpspreventoverfittingin
conjunctionwithadropoutlayer[52].Finally,alinearlayerpredictstheoutputfeaturesforeachimage-PSMinput.
ThisSiamesenetworkisusedtoextractjointimage-saliencyfeaturesforeachofthe𝑚image-PSMpairs,which
aresubsequentlyaveragedandnormalisedtounitlengthresultingintheextracteduserembedding.Preliminary
experimentsrevealedthatcombiningtheextractedfeaturesfromeachimage-PSMpairwitharecurrentortransformer
networkresultsinsevereoverfitting.Thus,averagingthefeatureshelpspreventoverfittingandyieldsbetterresults
overall.
Thenetworkisoptimisedtominimisethetripletmarginlosswithonline(semi-)hardtripletmining[21]definedas:
𝑒
𝑎
=(cid:13) (cid:13)𝐸({𝐼 1,...,𝐼 𝑚},{PSM𝐼
1,𝑈
1,...,PSM𝐼𝑚,𝑈 1})(cid:13)
(cid:13)
𝑒
𝑝
=(cid:13) (cid:13)𝐸({𝐼 𝑚+1,...,𝐼 2𝑚},{PSM𝐼𝑚+1,𝑈 1,...,PSM𝐼
2𝑚,𝑈
1})(cid:13)
(cid:13)
(1)
𝑒
𝑛
=(cid:13) (cid:13)𝐸({𝐼 2𝑚+1,...,𝐼 3𝑚},{PSM𝐼
2𝑚+1,𝑈
2,...,PSM𝐼
3𝑚,𝑈
2})(cid:13)
(cid:13)
L𝐸 =max(𝑒 𝑎·𝑒 𝑝 −𝑒 𝑎·𝑒 𝑛+𝑚,0).
Tocalculatetheanchoruserembedding𝑒 𝑎inEquation(1),arandomuser𝑈 1and𝑚randomimages{𝐼 1,...,𝐼 𝑚}with
correspondingPSMs{PSM𝐼 1,𝑈 1,...,PSM𝐼𝑚,𝑈 1}areselectedandpassedthroughourembeddingnetwork.Similarly,the
positiveembeddingexample𝑒 𝑝 iscalculatedusingthesameuser𝑈 1butwithdifferentimages{𝐼 𝑚+1,...,𝐼 2𝑚}andPSMs
{PSM𝐼𝑚+1,𝑈 1,...,PSM𝐼 2𝑚,𝑈 1},whilethenegativeembeddingexample𝑒 𝑛 isobtainedbyselectingadifferentrandom
user𝑈 2,alsowithdifferentimages{𝐼 2𝑚+1,...,𝐼 3𝑚}andcorrespondingPSMs{PSM𝐼 2𝑚+1,𝑈 2,...,PSM𝐼 3𝑚,𝑈 2}.Basedonthe
threeembeddings𝑒 𝑎,𝑒 𝑝 and𝑒 𝑛thestandardtripletlosscanbecalculatedasdefinedEquation(1).Itiscrucialtonote
thateachimage𝐼 inthetrainingdatasethasacorrespondingpersonalisedsaliencymapPSM𝐼,𝑈 foreachuser𝑈.This
ensuresthattheuserembeddingsaresolelyderivedfromtheindividualdifferencesinusers’visualattentionbehaviour,
ratherthanbeinginfluencedbytheimagesthemselves.Byminimisingthedistancebetweentheanchorandthepositive
embedding,thenetworklearnstorecognisesimilarhigh-levelgazebehaviourbetweendifferentinputsforthesame
user.Similarly,maximisingthedistancebetweentheanchorandthenegativeembeddingencouragesthenetworkto
distinguishthedifferentattentivebehaviourbetweentwousers.
Whilethetrainingobjectiveistodifferentiatebetweendifferentusers,priorresearchhasshownthatembeddings
learnedthroughoptimisingthetripletlosshavethecapabilitytoencodesubstantialclass-specificinformation[21].
Additionally,similarclassestendtobeclosetoeachotherintheembeddingspace,whiledissimilarclassestendto
havegreaterseparation.Thisindicatesthattheembeddingspaceeffectivelycapturestherelevantcharacteristicsof
theclassesandpresentsastructuredrepresentationthatreflectstheunderlyingrelationshipsbetweenthem.Byusing
theuserembedding,adownstreamtaskmodelcanleveragethecapturedinformationandtailoritspredictionstothe
specificcharacteristicsandvisualattentionbehaviourofeachuser.
3.2 PersonalisedSaliencyPrediction
Todemonstratetheeffectivenessoftheuserembeddings,weadaptedanexistingmethodforpersonalisedsaliency
prediction[59]andreplacethemanually,explicitlydefinedusercharacteristicswithourlearnedembeddingsfrom
implicitgazebehaviour.
SimilartoXuetal[59]wedefinethePSMpredictiontaskasarefinementoftheUSM:
PSM(𝐼,𝑈)=USM(𝐼)+Δ(𝐼,𝑈), (2)
5Strohmetal.
Fig.3. Thepersonalisedsaliencymap(PSM)networkoperatesbytakinganimagestimulusanditscorrespondinguniversalsaliency
map(USM)asinput.Inaddition,itincorporatesuserembedding,whichisutilisedtopredictkernelweights,whicharethenconvolved
overtheimage-USMfeatures.ThenetworkoutputsadiscrepancymapwhichcanbeaddedtotheUSMinordertogeneratethePSM.
wherePSM(𝐼,𝑈) isthePSMofauser𝑈 foragivenimage𝐼,USM(𝐼) istheUSMforthatimageandΔ(𝐼,𝑈) isthe
discrepancymapofthatuserandthatimage.Thediscrepancymapessentiallydefineswhichpartsoftheimageattract
thespecificusersattentionmoreorlesscomparedtotheaverageuser.Usingthisdefinitionallowsustodisentanglethe
predictionofthePSM,focusingonpredictingthediscrepancymapΔ(𝐼,𝑈)whileusingexistingstate-of-the-artmodels
topredicttheUSM.Figure3visualisesourneuralnetworkarchitectureforpersonalisedsaliencyprediction.Inputtothe
network𝑁 aretheimage,theUSMasaforthimagechannelandauserembedding.First,aseriesofconvolutionlayers
extractimage-saliencyfeatures.Inparallel,aseriesoflinearlayerspredictfiltersbasedontheuserembedding,which
aresubsequentlyconvolvedovertheextractedimage-saliencyfeatures.Thiswaythenetworkcanlearnfeaturesthat
arespecificallyimportanttotheuserbasedonthatusersembedding[59].Afterafinalsequenceofconvolutionlayers
themodeloutputsadiscrepancymapofsize15×20withavaluerangebetween-1and1.Weoptimisethenetwork
withthemeansquarederror(MSE)lossbetweenthepredictedandthetargetdiscrepancymap.Previousworkhas
shownthatbyprojectingextractedfeaturesfromintermediatelayerstotheoutputspaceforadditionalsupervisioncan
improveperformance[12,35].Thereforeweaddanadditionalprojectionlayerbeforethelasttwoconvolutionlayersto
predictintermediatediscrepancymaps.OuroveralllossobjectiveL𝑃𝑆𝑀 isthengivenas:
(cid:13) (cid:13)∑︁3 (cid:13) (cid:13)
L𝑃𝑆𝑀 =(cid:13)
(cid:13)
𝑁(USM(𝐼),𝐼,𝐸 𝑈)𝑘 −Δ(𝐼,𝑈)(cid:13) (cid:13), (3)
(cid:13)𝑘=1 (cid:13)
where𝑁(...)𝑘 isoneofthethreepredicteddiscrepancymapsofthepersonalisedsaliencynetwork.Tocalculatethe
groundtruthdiscrepancymapsΔ(𝐼,𝑈)weuseEquation(2)andsubtracttheUSM𝐼 fromthePSM𝐼,𝑈.
6LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
Model CC SIM AUC NSS KLD
DeepGazeIIE(DG) 0.622 0.556 0.904 2.121 1.123
Fine-tunedDG 0.715 0.619 0.905 2.140 0.526
MultiCNNw/DG 0.735 0.643 0.897 2.142 0.708
Oursw/DG 0.736 0.651∗ 0.907 2.170∗ 0.509∗
GroundTruthUSM 0.801 0.685 0.921 2.373 0.372
MultiCNNw/GT 0.804 0.683 0.919 2.378 0.511
Oursw/GT 0.813∗ 0.706∗ 0.922 2.405∗ 0.357∗
Table1. Closed-setresultsfortheIDdataset[13].*indicatessignificantimprovementoverthestrongestbaseline.
4 EXPERIMENTS
4.1 ImplementationDetails
UserEmbeddingNetwork. ThearchitectureoftheuserembeddingnetworkisshowninFigure2.Thenumberofinput
image-PSMsamples𝑚toourproposedSiameseCNNdependsontheexperimentandvariesbetween4and32.Each
modelwasoptimisedwiththetripletmarginlossasdefinedinEquation(1)withamarginof0.05andonlinesemi-hard
andhardtripletminingwithinabatchof256samples.TheweightswereupdatedusingtheAdamoptimiser[32]witha
learningrateof0.001anddefaultparametersotherwise.TheDropoutlayer[52]maskedneuronactivations50%ofthe
timeduringtraining.
PersonalisedSaliencyNetwork. Figure3illustratesthearchitectureofourpersonalisedsaliencynetwork.Toeffectively
trainthisnetwork,wefirsttrainedtheembeddingnetworktoextractuserembeddings,whichserveasinput.Sincethe
embeddingsrelyon𝑚image-PSMpairsfortheircalculation,theycanexhibitvariability,particularlywhen𝑚issmall.
Hence,aftertrainingtheembeddingnetworks,wegenerated100embeddingsforeachparticipantwithineachdataset
byrandomlysamplingdatafromthecorrespondingparticipant.Duringthetrainingprocessofthepersonalisedsaliency
network,werandomlyselectedembeddingsfromthispooltoaccountforthesevariationsandimprovegeneralisation.
ThemodelsweretrainedwithanSGDoptimiserwithinitiallearningrateof0.02,momentumof0.9,weightdecayof
0.0005andbatchsize32.Thelearningratewasreducedbyafactorof2every25epochs.Thetrainedmodelrunsat27
framespersecondonanRTX4070GPU,achievingreal-timepersonalisedsaliencypredictions.
EvaluationMetrics. Wereportmultiplemetricscommonlyusedtoevaluatethesimilaritybetweensaliencymaps[30,
43,59].ThesemetricsarethePearson’scorrelationcoefficient(CC),similarity/histogramintersection(SIM),Area
underROCCurve(AUC-Judd)[31],normalisedscanpathsaliency(NSS)andKullback-Leiblerdivergence(KLD).In
addition,toassesstheaccuracyoftheuserembeddings,weemployalabellingapproachwhereanextractedembedding
isconsideredcorrectifitsnearestneighbourintheembeddingspacebelongstothesameuser.Thisevaluationmetricis
commonlyreferredtoasprecisionatone[28].
4.2 Datasets
Saliencypredictionmodelsaretypicallypre-trainedusingthelarge-scaleSALICONdataset[29].However,wecannot
utilisetheSALICONdatasetforlearningtheembeddingsaseachsubjectfromthedatasetobservedadifferentsubsetof
imagestimuli.AsdiscussedinSubsection3.1,itiscriticalthateachparticipantlooksatallimages,oratleastthatthere
7Strohmetal.
Model CC SIM AUC NSS KLD
DeepGazeIIE(DG) 0.584 0.530 0.881 2.288 1.294
Fine-tunedDG 0.734 0.622 0.887 2.293 0.562
MultiCNNw/DG 0.746 0.637 0.892 2.299 0.604
Oursw/DG 0.760∗ 0.649∗ 0.896 2.308 0.481∗
GroundTruthUSM 0.821 0.698 0.912 2.500 0.366
MultiCNNw/GT 0.845 0.711 0.914 2.609 0.415
Oursw/GT 0.846 0.725∗ 0.915 2.595 0.336∗
Table2. Closed-setresultsforthePSdataset[59].*indicatessignificantimprovementoverstrongestbaseline.
isasignificantoverlapbetweenparticipants,asotherwisethemodelcansimplyidentifytheuserbasedontheimages
theyobservedignoringtheirspecificvisualattentionbehaviour.
Basedontheaboverequirement,weselectedtwopubliclyavailabledatasetswhereeachparticipantobservedeach
imagewhiletheirgazewasrecordedwithaneye-tracker.ThefirstdatasetwascollectedbyXuetal.[59],which
wecallthePersonalisedSaliency(PS)dataset.ThePSdatasetcontains1,600images,whichtheyselectedtocontain
manydifferentsemanticcategoriesineachimage,astheyarguethatthismaximisesthevariationofvisualattention
betweenparticipants.Eachstimuliinthedatasetwasobservedby30participantsforthreesecondsatotaloffourtimes,
allowingthemtoaverageoutstochasticvariationsineachparticipantsattentivebehaviour.Toevaluatehowwellour
systemgeneralisestounseenimageswesplittheimagesinto80%fortraining,10%forvalidationand10%fortesting.
Furthermore,toevaluatehowwelloursystemgeneralisestounseenparticipants,wesplitthe30participantsinto20
fortrainingand5eachforvalidationandtesting.
TheseconddatasetwascollectedbyHaasetal.[13]whichwecalltheIndividualDifferences(ID)dataset.Thestimuli
wereselectedtobecomprisedofcomplexscenescontainingmultipledifferentsemanticcategoriesandobjects.They
recordedgazedatafromatotalof102differentparticipantseachlookingat700differentimagestimuli.Similartothe
PSdatasetwesplittheimagesinto80%fortraining,10%forvalidationand10%fortesting.Furthermore,wesplitthe
102participantsinto80fortraining,10forvalidationand12testing.
FollowingXuetal.[59]weconductbothclosed-setandopen-setexperimentsforeachdataset.Intheclosed-set
experiments,thesameparticipantswereusedinboththetraining,validation,andtestsetswiththevalidationandtest
setscontainingunseenimages.Thisenablesustoassessthecapabilitytopredictpersonalisedsaliencyforfamiliarusers.
Asfortheopen-setexperiments,weassessthepersonalisedsaliencypredictionperformanceonunseenparticipants
fromthetestset.Thisanalysishelpsusunderstandhoweffectivelymodelscangeneralisetonewusers.
4.3 Baselines
Weevaluateourmethodagainstthreebaselines.Firstly,weutiliseDeepGazeIIE[37],astate-of-the-artuniversalsaliency
predictionmodel.WecomparethepredictedUSMsbyDeepGazeIIEwithourrefinedPSMsbasedontheDeepGazeIIE
prediction.SinceDeepGazeIIEwasnotoriginallytrainedonourdataset,wefine-tunethemodel’spredictiononeach
datasetandpresentresultsbothwithandwithoutfine-tuning.Secondly,wecomparethegroundtruthUSMswithour
predictedPSMsgeneratedthroughrefiningtheUSMs.
Intheliterature,therearetworelevantworksbyXuetal.[59]andMorotoetal.[42]thatproposemethodsforPSM
prediction,asdiscussedinSection2.Unfortunately,wewereunabletodirectlycompareourresultswithXuetal.due
8LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
Model CC SIM AUC NSS KLD
DeepGazeIIE(DG) 0.615 0.553 0.916 2.239 1.083
Fine-tunedDG 0.721 0.624 0.914 2.244 0.542
MultiCNNw/DG 0.723 0.627 0.910 2.244 0.620
Oursw/DG 0.726 0.638∗ 0.916 2.253 0.527∗
GroundTruthUSM 0.804 0.682 0.922 2.506 0.387
MultiCNNw/GT 0.804 0.690 0.929 2.550 0.440
Oursw/GT 0.813∗ 0.702∗ 0.933 2.552 0.366∗
Table3. Open-setresultsfortheIDdataset[13].*indicatessignificantimprovementoverstrongestbaseline.
Model CC SIM AUC NSS KLD
DeepGazeIIE(DG) 0.564 0.522 0.860 1.944 1.431
Fine-tunedDG 0.692 0.618 0.862 2.032 0.685
MultiCNNw/DG 0.693 0.615 0.862 2.006 0.682
Oursw/DG 0.712 0.634 0.870 2.033 0.539
OursCDw/DG 0.713∗ 0.634∗ 0.871∗ 2.033 0.533∗
GroundTruthUSM 0.788 0.691 0.892 2.218 0.399
MultiCNNw/GT 0.800 0.694 0.892 2.290 0.516
Oursw/GT 0.804 0.701 0.893 2.273 0.371
OursCDw/GT 0.807 0.704∗ 0.894 2.288 0.361∗
Table4. Open-setresultsforthePSdataset[59].*indicatessignificantimprovementoverstrongestbaseline.
totheunavailabilityoftheuser-specificinformationrequiredfortheirmethod.However,fortheclosedsetexperiments
theyalsoproposedMultiCNN,wheretheytrainaseparateclassifierforeachparticipant,whichwewilluseasourthird
baseline.TheMultiCNNarchitectureisidenticaltoournetworkshowninFigure3withouttheembeddingpathway.
Morotoetal.[42]proposeamethodtomapunseenuserstothetraininguserwiththemostsimilarvisualattention
behaviour,whichallowsthemtomakepersonalisedsaliencypredictionswiththecorrespondingtrainedclassifier.
Inspiredbythis,weproposeanoraclemappingbyevaluatingunseenparticipantsusingeverytrainedMultiCNNmodel
andthenchoosethemodelachievingthelowestloss.Thisallowsustoprovidetheupper-boundMultiCNNperformance
fortheopensetexperiments.
4.4 PersonalisedSaliencyPrediction
Closed-SetResults. Table1showstheclosed-setresultsfortheIDdatasetandTable2showstheresultsforthePS
dataset.Theresultswithourmethodwereobtainedusingembeddingsextractedfrom𝑚 =32image-PSMpairsfor
eachuser.Thebestperformingmodelsarehighlightedinbold,whiletheperformanceofthesecondbestmodelis
underlined.SignificancetestswereconductedusingtheMann-Whitney-Utestforeachmetricwithap-valuethreshold
of<0.05.Anasterisknexttometricsforourmethodindicatesthattheimprovementcomparedtothestrongestbaseline
wasstatisticallysignificant.ThegroundtruthUSMistheupperboundtraditionalsaliencypredictionmethodscould
potentiallyachieve.TheresultsforMultiCNNandOursshowthatitispossibletofurtherrefinetheseUSMsasoverall
bothmethodsoutperformthegroundtruthUSMbaseline.Furthermore,weobservethatforbothdatasetsourmethod
9Strohmetal.
Fig.4. ExamplePSMpredictionsfortwousersfromthePS[59]testsetwithourproposedmethodcomparedtothegroundtruths.
outperformsallbaselinesincludingMultiCNNsinallmetricsexceptforNSSonthePSdataset.However,forareal-world
scenariothegroundtruthUSMmightnotbeavailableandhastobepredictedfirst.TheresultsshowthatMultiCNNs
andOursusingthenonfine-tunedUSMpredictionsfromDeepGazeIIEstilloutperformthefine-tunedDeepGazeIIE
baseline,withourmethodachievingthebestperformance.Thisindicatesthatourmethodcanbeappliedtodifferent
potentialsuboptimalUSMpredictionsandstillproduceamorerefinedPSMprediction.
Open-SetResults. Table3showstheopen-setresultsfortheIDdatasetandTable4showstheresultsforthePSdataset.
Wecanobserveaverysimilartrendaswiththeclosed-setexperimentswithourmethodoveralloutperformingall
baselines,indicatingthatourembeddingshelpthepersonalsaliencynetworktogeneralisewelltounseenparticipants.
SincethePSdatasetconsistsonlyof30differentparticipantsofwhich20areusedfortraining,learninggeneralisable
userembeddingsismorechallenging.WethereforeexperimentwithcombiningthetrainingsplitsofthePSandID
datasetswhentrainingtheuserembeddingnetwork,allowingthenetworktoobservetheattentivebehaviourofatotal
of100differentparticipantsduringtraining.Notethatwestilltrainedandevaluatedourpersonalisedsaliencynetwork
usingonlythePSdataset.Wereporttheresultingperformanceonthecombineddataset(CD)asOursCDinTable4.We
canobservethatusingthesenewembeddingstheperformanceofthepersonalisedsaliencymodelfurtherimprovesfor
allmetrics.
Inadditiontothequantitativeresults,Figure4showsexampleopen-setPSMpredictionsformultipleimagestimuli
onthetestsplitofthePSdataset.Intheprovidedexamples,itisevidentthattheuserrepresentedinthelasttwocolumns
exhibitsamorefocusedattentionbehaviourcomparedtothefirstuser.Ourembeddingssuccessfullycapturethis
distinction,asindicatedbythecorrespondingsaliencypredictions.Notably,intheexampledepictedinthebottomrow,
theseconduserappearstoallocatelessattentiontofacesandinsteadalsofocusesonotherbodyparts.Ourembeddings
seemtocapturethisbehaviour,asevidencedbythesaliencyallocationbelowthefaceregioninthecorrespondingPSM
10LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
1.0
0.9
0.8
0.7
0.6
0.5
ID
0.4 PS
CD (ID + PS train split)
0.3
4 8 16 32 48 64
Number of examples
Fig.5. Performancecomparisonofdifferentuserembeddingextractionmodels.They-axisindicatesthemodel’saccuracyandthe
x-axishowmanyimage-PSMexamples𝑚wereusedasinputtoextracttheembedding(4,8,16,32,48or64).Wereporttheaccuracy
forunseenparticipantsontheIndividualDifferences(ID)dataset,thePersonalSaliency(PS)datasetandforthecombinedCD
dataset.
predictions.Togetherwithourquantitativeresultsthisfurtherdemonstratestheeffectivenessofusinguserembeddings
learnedfromvisualattentionformorepersonalisedsaliencypredictions.
4.5 UserEmbeddingsAnalysis
Togainabetterunderstandingofourextracteduserembeddings,wefurtheranalysetheperformanceoftheuser
embeddingnetwork.
NumberofExamplesforEmbeddingExtraction. Figure5showsthemodel’stestsetaccuracyonthey-axisfordifferent
datasetsandthenumberofexamples𝑚usedforembeddingextractiononthex-axis.Wereportthetestsetaccuracy
forbothdatasetsIDandPS,aswellasthePStestsetaccuracywhentrainingonthecombinedtrainingsetCD.The
userembeddingnetworkachievesanaccuracyof98.1%ontheIDtestsplitwhenusing𝑚 =64examples,showing
thatitisabletodifferentiateverywellbetweenparticipantsthatthemodelneversawduringtraining.Similarlythe
modeltrainedonthePSdatasetachievesanaccuracyof79.1%whenonlyusingthePStrainingsplitandanaccuracyof
92.7%whencombiningthetrainingsplitsfromPSandID.Asalreadyindicatedbytheimprovedpersonalisedsaliency
predictionresultsreportedinTable4,trainingonbothdatasetsincreasestheperformanceonthePSdataset,which
onlycontainsasmallnumberofparticipants.NotethattherandombaselineaccuracyfortheIDdatasetis8.3%(12
participantsinthetestset),whileitis20%forthePSdataset(5participantsinthetestset).Furthermore,wecanobserve
thatthemodel’saccuracyincreaseswiththenumberofexamples𝑚providedasinput.Combiningthetrainingdatasets
especiallyimprovestheaccuracyforlarger𝑚.Thisislikelyduetotheincreaseduserinformationthemodelcanextract
withlarger𝑚,resultinginbettergeneralisationifprovidedwithmorediverseusersduringtraining.
11
ycaruccAStrohmetal.
(a) (b)
Fig.6. Weuset-SNEtoreducethe32-dimensionalembeddingsintotwodimensions.Figure(a)showstheembeddingspacefor
embeddingsextractedusing𝑚=8image-PSMpairswhilein(b)weused𝑚=32pairs.Eachdotrepresentsoneembeddingextracted
using𝑚randomlysampledimage-PSMpairs.EachcolourcorrespondstoauniqueuserfromtheIDtestset.
VisualisationoftheEmbeddingSpace. Toanalysetheembeddingspacewereducedthe32dimensionaluserembeddings
to2dimensionsusingt-SNE.Figure6illustratesthe2-dimensionalembeddingsforthe12participantsfromtheIDtest
split.Inthisvisualisation,eachdotrepresentsanembedding,andthecolourofeachdotcorrespondstoaspecificuser.
Wecalculated20embeddingsforeachparticipantbyrandomlysampling𝑚inputimage-PSMpairs.In(a)wevisualise
theembeddingsfor𝑚=8while(b)showstheembeddingsfor𝑚=32.Weobservethattheembeddingsfor𝑚=8have
amuchlargervariancewithinaparticipantcomparedtotheembeddingsextractedwith𝑚=32,aslessinformation
abouttheusercanbeextracted.Foralow𝑚theboundariesbetweenparticipantsbecomesfuzzywhichisalsoreflected
bytheloweraccuracyreportedinFigure5.Wecanobservethatfor𝑚=32theembeddingsarehighlydiscriminative
asclearuserclustersareformed.
5 BROADERIMPACT
Ourmethoddemonstratestheabilitytoextractuserembeddingsfromasmallamountofgazedata,showcasingthat
theseembeddingseffectivelycapturerelevantinformationformodellingpersonalsaliency.Predictingsaliencyisa
crucialtaskwithimplicationsforvariousdownstreamapplicationsincomputervisionandhuman-computerinteraction.
Forinstance,attentiveuserinterfacesaimtomanagetheuser’sattentioneffectively,relyingonknowledgeabouttheir
visualattention[55].Recommendersystemscanbenefitfromunderstandingusers’visualattentiontoenhancethe
visibilityoftop-rankedentities[64].Otherdownstreamtasksthatbenefitfromaccuratepersonalisedsaliencyinclude
videosummarization[39],automatedimagecropping[3],andimagecaptioning[20].Whilethedownstreamtask
ofthisworkispersonalisedsaliencyprediction,ourproposedmethodforextractinguserembeddingscouldprove
advantageousforotherdownstreamtaskswherepersonalisationiscrucial.
12LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
Althoughpersonalisedsaliencyishelpfulformanytasks,theunderlyingcomputationalusermodelcouldalsobe
misusedtosynthesisedataforagivenuserbyimpersonatingtheuser’sownsaliency.Furthermore,insteadofusingthe
embeddingstopredictpersonalsaliency,theymightbemisusedtodirectlyextractusersensitiveinformation.These
embeddingsmightencodeusercharacteristicsorprivateinformationabouttheuserthatcorrelateswiththeirgaze
behaviour.Forexample,priorworkhasshownthatgenderisastrongmodulatorofsaliencypreference[22],andas
such,thisprivateinformationmightbeextractablefromourembeddings.Thisencodedinformationcouldbeusedto
identifyusersbasedontheirpersonalsaliency,especiallyasattentiontrackingbecomesmorepervasiveandcheap
throughappearancebasedgazeestimationormousetracking.Anotherfactortoconsideristhatourembeddingsmay
bebiasedastheyareextractedfromasmallnumberofstimuliwithcorrespondinggazedata.AswecanseeinFigure6,
theembeddingsvaryforthesamepersonandwecannoticemultipleoutlierembeddingsthatarefarapartformtheir
cluster,evenwhenusingmoredata.Thus,whenusedfortaskslikeuserprofilingthismightleadtoincorrectuser
profilesandsubsequentlyincorrectpredictionsindownstreamtasks.
Continuingthislineofresearch,itisconceivablethatfutureworkwillnotonlysynthesisepersonalisedsaliencybut
evenrawgazeofspecificuserswithsuchembeddings.Accuratecomputationalmodelsofuser-specificgazebehaviour
wouldbeofgreatsignificanceforevenmoredownstreamtasks.However,thisadvancementmightraisepotential
securityrisksforapplicationsthatfundamentallyrelyongazebehaviouranalysis,suchasgaze-basedauthentication.
6 SUMMARY
Inthiswork,weproposedanovelmethodthatextractsuserembeddingsfrompairsofnaturalimagesandcorresponding
user-specificsaliencymaps.Thelearnedembeddingscapturetheusers’uniquecharacteristicsandcanbeusedto
addressthepersonalisedsaliencypredictiontask.Incontrasttopriorworkforthistaskthatrequiredexplicituserinput,
ourmethodonlyrequiresimplicitinputfromgazebehaviourcollectedusinganeyetracker.Ourproposedmethoduses
aSiameseconvolutionalneuralencodertolearntheembeddingmodel,trainedbycontrastingauser’sgazebehaviour
withthatofdifferentusers.Resultsontwosaliencydatasetsdemonstratedtheembeddings’discriminativepower,our
method’sgeneralisabilitytounseenusersandimages,andimprovedperformanceoveruniversalsaliencyprediction
models.Assuch,ourworkpresentsapromisingapproachtolearningandleveraginguserembeddingsfromimplicit
behaviouralsoforothertasksorapplicationsthatrequireindividualusercharacteristics.
REFERENCES
[1] AhmedAbdou,EktaSood,PhilippMüller,andAndreasBulling.2022.Gaze-enhancedCrossmodalEmbeddingsforEmotionRecognition.Proceedings
oftheACMonHuman-ComputerInteraction6,ETRA(2022),1–18.
[2] MingxiaoAnandSundongKim.2021.Neuraluserembeddingfrombrowsingevents.InMachineLearningandKnowledgeDiscoveryinDatabases:
AppliedDataScienceTrack:EuropeanConference,ECMLPKDD2020,Ghent,Belgium,September14–18,2020,Proceedings,PartIV.Springer,175–191.
[3] EdoardoArdizzone,AlessandroBruno,andGiuseppeMazzola.2013.Saliencybasedimagecropping.InImageAnalysisandProcessing–ICIAP2013:
17thInternationalConference,Naples,Italy,September9-13,2013.Proceedings,PartI17.Springer,773–782.
[4] AdrienBaranes,Pierre-YvesOudeyer,andJacquelineGottlieb.2015.Eyemovementsrevealepistemiccuriosityinhumanobservers.Visionresearch
117(2015),81–90.
[5] AliBorji,Ming-MingCheng,QibinHou,HuaizuJiang,andJiaLi.2019.Salientobjectdetection:Asurvey.Computationalvisualmedia5(2019),
117–150.
[6] StephanieBrams,GalZiv,OronLevin,JochimSpitz,JohanWagemans,AMarkWilliams,andWernerFHelsen.2019.Therelationshipbetween
gazebehavior,expertise,andperformance:Asystematicreview.Psychologicalbulletin145,10(2019),980.
[7] MaximilianDavideBrodaandBenjaminDeHaas.2022.Individualdifferencesinlookingatpersonsinscenes.JournalofVision22,12(2022),9–9.
[8] MaximilianDBrodaandBenjamindeHaas.2022.Individualfixationtendenciesinpersonviewinggeneralizefromimagestovideos.i-Perception
13,6(2022),20416695221128844.
[9] GuyThomasBuswell.1935.Howpeoplelookatpictures:astudyofthepsychologyandperceptioninart.(1935).
13Strohmetal.
[10] MoranCerf,JonathanHarel,WolfgangEinhäuser,andChristofKoch.2007.Predictinghumangazeusinglow-levelsaliencycombinedwithface
detection.Advancesinneuralinformationprocessingsystems20(2007).
[11] XianyuChen,MingJiang,andQiZhao.2021.Predictinghumanscanpathsinvisualquestionanswering.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition.10876–10885.
[12] MarcellaCornia,LorenzoBaraldi,GiuseppeSerra,andRitaCucchiara.2016. Adeepmulti-levelnetworkforsaliencyprediction.In201623rd
InternationalConferenceonPatternRecognition(ICPR).IEEE,3488–3493.
[13] BenjaminDeHaas,AlexiosLIakovidis,DSamuelSchwarzkopf,andKarlRGegenfurtner.2019.Individualdifferencesinvisualsaliencevaryalong
semanticdimensions.ProceedingsoftheNationalAcademyofSciences116,24(2019),11687–11692.
[14] ShahramEivazi,RomanBednarik,MarkkuTukiainen,MikaelvonundzuFraunberg,VilleLeinonen,andJuhaEJääskeläinen.2012.Gazebehaviour
ofexpertandnovicemicroneurosurgeonsdiffersduringobservationsoftumorremovalrecordings.InProceedingsoftheSymposiumonEyeTracking
ResearchandApplications.377–380.
[15] ChristopherIfeanyiEke,AzahAnirNorman,LiyanaShuib,andHenryFridayNweke.2019.Asurveyofuserprofiling:State-of-the-art,challenges,
andsolutions.IEEEAccess7(2019),144907–144924.
[16] CamiloFosco,AneliseNewman,PatSukhum,YunBinZhang,NanxuanZhao,AudeOliva,andZoyaBylinskii.2020.Howmuchtimedoyouhave?
modelingmulti-durationsaliency.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.4473–4482.
[17] SusanGauch,MircoSperetta,AravindChandramouli,andAlessandroMicarelli.2007. Userprofilesforpersonalizedinformationaccess. The
adaptiveWeb:methodsandstrategiesofWebpersonalization(2007),54–89.
[18] JunfengHe,KhoiPham,NachiappanValliappan,PingmeiXu,ChaseRoberts,DmitryLagun,andVidhyaNavalpakkam.2019.On-devicefew-shot
personalizationforreal-timegazeestimation.InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervisionworkshops.0–0.
[19] ShengfengHe,ChuHan,GuoqiangHan,andJingQin.2019.Exploringdualityinvisualquestion-driventop-downsaliency.IEEEtransactionson
neuralnetworksandlearningsystems31,7(2019),2672–2679.
[20] SenHe,HamedRTavakoli,AliBorji,andNicolasPugeault.2019.Humanattentioninimagecaptioning:Datasetandanalysis.InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision.8529–8538.
[21] AlexanderHermans,LucasBeyer,andBastianLeibe.2017.Indefenseofthetripletlossforpersonre-identification.arXivpreprintarXiv:1703.07737
(2017).
[22] JohannesHewig,RalfHTrippe,HolgerHecht,ThomasStraube,andWolfgangHRMiltner.2008.Genderdifferencesforspecificbodyregionswhen
lookingatmenandwomen.JournalofNonverbalBehavior32(2008),67–78.
[23] SabrinaHoppe,TobiasLoetscher,StephanieAMorey,andAndreasBulling.2018.Eyemovementsduringeverydaybehaviorpredictpersonality
traits.Frontiersinhumanneuroscience(2018),105.
[24] SergeyIoffeandChristianSzegedy.2015. Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift.In
Internationalconferenceonmachinelearning.pmlr,448–456.
[25] ShamsiTIqbalandBrianPBailey.2004.Usingeyegazepatternstoidentifyusertasks.InTheGraceHopperCelebrationofWomeninComputing,
Vol.4.2004.
[26] LaurentIttiandChristofKoch.2001.Computationalmodellingofvisualattention.Naturereviewsneuroscience2,3(2001),194–203.
[27] LaurentItti,ChristofKoch,andErnstNiebur.1998.Amodelofsaliency-basedvisualattentionforrapidsceneanalysis.IEEETransactionsonpattern
analysisandmachineintelligence20,11(1998),1254–1259.
[28] KalervoJärvelinandJaanaKekäläinen.2017.IRevaluationmethodsforretrievinghighlyrelevantdocuments.InACMSIGIRForum,Vol.51.ACM
NewYork,NY,USA,243–250.
[29] MingJiang,ShengshengHuang,JuanyongDuan,andQiZhao.2015.Salicon:Saliencyincontext.InProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition.1072–1080.
[30] TilkeJudd,FrédoDurand,andAntonioTorralba.2012.Abenchmarkofcomputationalmodelsofsaliencytopredicthumanfixations.(2012).
[31] TilkeJudd,KristaEhinger,FrédoDurand,andAntonioTorralba.2009.Learningtopredictwherehumanslook.In2009IEEE12thinternational
conferenceoncomputervision.IEEE,2106–2113.
[32] DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980(2014).
[33] OnkarKrishna,AndreaHelo,PiaRämä,andKiyoharuAizawa.2018.Gazedistributionanalysisandsaliencypredictionacrossagegroups.PloSone
13,2(2018),e0193149.
[34] BruceKrulwich.1997.Lifestylefinder:Intelligentuserprofilingusinglarge-scaledemographicdata.AImagazine18,2(1997),37–37.
[35] Chen-YuLee,SainingXie,PatrickGallagher,ZhengyouZhang,andZhuowenTu.2015.Deeply-supervisednets.InArtificialintelligenceandstatistics.
PMLR,562–570.
[36] MinLin,QiangChen,andShuichengYan.2013.Networkinnetwork.arXivpreprintarXiv:1312.4400(2013).
[37] AkisLinardos,MatthiasKümmerer,OriPress,andMatthiasBethge.2021.DeepGazeIIE:Calibratedpredictioninandout-of-domainforstate-of-
the-artsaliencymodeling.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.12919–12928.
[38] MarcelLinkaandBenjamindeHaas.2020.OSIEshort:Asmallstimulussetcanreliablyestimateindividualdifferencesinsemanticsalience.Journal
ofvision20,9(2020),13–13.
[39] Yu-FeiMa,LieLu,Hong-JiangZhang,andMingjingLi.2002.Auserattentionmodelforvideosummarization.InProceedingsofthetenthACM
internationalconferenceonMultimedia.533–542.
14LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
[40] FatemehsadatMireshghallah,VaishnaviShrivastava,MiladShokouhi,TaylorBerg-Kirkpatrick,RobertSim,andDimitriosDimitriadis.2021.
Useridentifier:Implicituserrepresentationsforsimpleandeffectivepersonalizedsentimentanalysis.arXivpreprintarXiv:2110.00135(2021).
[41] SounakMondal,ZhiboYang,SeoyoungAhn,DimitrisSamaras,GregoryZelinsky,andMinhHoai.2023.Gazeformer:Scalable,EffectiveandFast
PredictionofGoal-DirectedHumanAttention.arXivpreprintarXiv:2303.15274(2023).
[42] YuyaMoroto,KeisukeMaeda,TakahiroOgawa,andMikiHaseyama.2020.Few-shotpersonalizedsaliencypredictionbasedonadaptiveimage
selectionconsideringobjectandvisualattention.Sensors20,8(2020),2170.
[43] JuntingPan,ElisaSayrol,XavierGiro-iNieto,KevinMcGuinness,andNoelEO’Connor.2016. Shallowanddeepconvolutionalnetworksfor
saliencyprediction.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.598–606.
[44] MichaelJPazzani,JackMuramatsu,DanielBillsus,etal.1996.Syskill&Webert:Identifyinginterestingwebsites.InAAAI/IAAI,Vol.1.54–61.
[45] EvanFRisko,NicolaCAnderson,SophieLanthier,andAlanKingstone.2012. Curiouseyes:Individualdifferencesinpersonalitypredicteye
movementbehaviorinscene-viewing.Cognition122,1(2012),86–90.
[46] NegarSammaknejad,HamidrezaPouretemad,ChangizEslahchi,AlirezaSalahirad,andAshkanAlinejad.2017.Genderclassificationbasedoneye
movements:Aprocessingeffectduringpassivefaceviewing.Advancesincognitivepsychology13,3(2017),232.
[47] HosniehSattar,MarioFritz,andAndreasBulling.2020. Deepgazepooling:Inferringandvisuallydecodingsearchintentsfromhumangaze
fixations.Neurocomputing387(2020),369–382.
[48] JudeShavlik,SusanCalcari,TinaEliassi-Rad,andJackSolock.1998.Aninstructable,adaptiveinterfacefordiscoveringandmonitoringinformation
ontheworld-wideweb.InProceedingsofthe4thinternationalconferenceonIntelligentuserinterfaces.157–160.
[49] AnaFilipaSilva,FranciscoTomásGonzálezFernández,etal.2022.Differencesinvisualsearchbehaviorbetweenexpertandnoviceteamsports
athletes:Asystematicreviewwithmeta-analysis.(2022).
[50] EktaSood,FabianKögel,FlorianStrohm,PrajitDhar,andAndreasBulling.2021.VQA-MHUG:Agazedatasettostudymultimodalneuralattention
invisualquestionanswering.arXivpreprintarXiv:2109.13116(2021).
[51] EktaSood,FabianKögel,PhilippMüller,DominikeThomas,MihaiBâce,andAndreasBulling.2023. MultimodalIntegrationofHuman-
Like Attention in Visual Question Answering. In Proc. Workshop on Gaze Estimation and Prediction in the Wild (GAZE), CVPRW. 2647–
2657. https://openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Sood_Multimodal_Integration_of_Human-Like_Attention_in_Visual_
Question_Answering_CVPRW_2023_paper.pdf
[52] NitishSrivastava,GeoffreyHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdinov.2014.Dropout:asimplewaytopreventneural
networksfromoverfitting.Thejournalofmachinelearningresearch15,1(2014),1929–1958.
[53] FlorianStrohm,EktaSood,SvenMayer,PhilippMüller,MihaiBâce,andAndreasBulling.2021. NeuralPhotofit:gaze-basedmentalimage
reconstruction.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.245–254.
[54] FlorianStrohm,EktaSood,DominikeThomas,MihaiBâce,andAndreasBulling.2023.FacialCompositeGenerationwithIterativeHumanFeedback.
InAnnualConferenceonNeuralInformationProcessingSystems.PMLR,165–183.
[55] RoelVertegaal.2002.Designingattentiveinterfaces.InProceedingsofthe2002symposiumonEyetrackingresearch&applications.23–30.
[56] LennartWachowiak,PeterTisnikar,GerardCanal,AndrewColes,MatteoLeonetti,andOyaCeliktutan.2022.Analysingeyegazepatternsduring
confusionanderrorsinhuman–agentcollaborations.In202231stIEEEInternationalConferenceonRobotandHumanInteractiveCommunication
(RO-MAN).IEEE,224–229.
[57] DirkWaltherandChristofKoch.2006.Modelingattentiontosalientproto-objects.Neuralnetworks19,9(2006),1395–1407.
[58] XiaodongWu,WeizheLin,ZhilinWang,andElenaRastorgueva.2020.Author2vec:Aframeworkforgeneratinguserembedding.arXivpreprint
arXiv:2003.11627(2020).
[59] YanyuXu,ShenghuaGao,JunruWu,NianyiLi,andJingyiYu.2018.Personalizedsaliencyanditsprediction.IEEEtransactionsonpatternanalysis
andmachineintelligence41,12(2018),2975–2989.
[60] YanyuXu,NianyiLi,JunruWu,JingyiYu,ShenghuaGao,etal.2017.BeyondUniversalSaliency:PersonalizedSaliencyPredictionwithMulti-task
CNN..InIJCAI.3887–3893.
[61] ZhiboYang,LihanHuang,YupeiChen,ZijunWei,SeoyoungAhn,GregoryZelinsky,DimitrisSamaras,andMinhHoai.2020.Predictinggoal-directed
humanattentionusinginversereinforcementlearning.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.193–202.
[62] AlfredLYarbusandAlfredLYarbus.1967.Eyemovementsduringperceptionofcomplexobjects.Eyemovementsandvision(1967),171–211.
[63] BingqingYu.2018.Personalizationofsaliencyestimation.McGillUniversity(Canada).
[64] QianZhao,ShuoChang,FMaxwellHarper,andJosephAKonstan.2016.Gazepredictionforrecommendersystems.InProceedingsofthe10thACM
ConferenceonRecommenderSystems.131–138.
[65] WanjunZhong,DuyuTang,JiahaiWang,JianYin,andNanDuan.2021.UserAdapter:Few-shotuserlearninginsentimentanalysis.InFindingsof
theAssociationforComputationalLinguistics:ACL-IJCNLP2021.1484–1488.
15