Editing Massive Concepts in Text-to-Image
Diffusion Models
Tianwei Xiong1,2∗, Yue Wu3∗, Enze Xie4†,
Yue Wu4, Zhenguo Li4, and Xihui Liu1†
1The University of Hong Kong 2Tsinghua University 3Peking University
4Huawei Noah’s Ark Lab
Abstract. Text-to-imagediffusionmodelssufferfromtheriskofgener-
atingoutdated,copyrighted,incorrect,andbiasedcontent.Whileprevi-
ousmethodshavemitigatedtheissuesonasmallscale,itisessentialto
handlethemsimultaneouslyinlarger-scalereal-worldscenarios.Wepro-
pose a two-stage method, Editing Massive Concepts In Diffusion Mod-
els (EMCID). The first stage performs memory optimization for each
individual concept with dual self-distillation from text alignment loss
and diffusion noise prediction loss. The second stage conducts massive
concept editing with multi-layer, closed form model editing. We further
proposeacomprehensivebenchmark,namedImageNetConceptEditing
Benchmark(ICEB),forevaluatingmassiveconcepteditingforT2Imod-
els with two subtasks, free-form prompts, massive concept categories,
and extensive evaluation metrics. Extensive experiments conducted on
our proposed benchmark and previous benchmarks demonstrate the su-
perior scalability of EMCID for editing up to 1,000 concepts, providing
a practical approach for fast adjustment and re-deployment of T2I dif-
fusion models in real-world applications.
Keywords: T2I Generation, Diffusion Model, Concept Editing
1 Introduction
Text-to-imagediffusionmodels[2,4,9,16,28,31–33]haveadvancedremarkablyin
recentyears.However,varioussocietalconcernshavealsobeenraised[22,35–37].
These models may produce inaccurate content due to outdated or flawed inter-
nal knowledge. They may also pose risks related to copyright infringement and
societal biases inherited from training data. While the inappropriate generation
largelystemsfromrelateddataintheunfilteredweb-scaletrainingset,itispro-
hibitively expensive to resolve the issues by reprocessing the training set and
retraining the models. To provide more practical solutions, we would edit mod-
els’ knowledge of concepts related to the issues, by modifying a portion of the
model weights.
*Equal contribution. †Corresponding authors.
4202
raM
02
]VC.sc[
1v70831.3042:viXra2 T. Xiong et al.
The president of the United States Bedroom in Arles by Van Gogh
Update Forget
“The US President” “Van Gogh”Style
An image of a snowbird Two doctors standing together
Rectify Gender Debias
“Snowbird” “Doctor”
Fig.1: Our method EMCID generally edits source concepts, the concepts intended
to be modified, to match destination concepts, the concepts towards which source
conceptsaretobealtered.Ourmethodcanupdate,forget,rectify,anddebiasvarious
concepts simultaneously at a large scale.
Previous methods either fine-tune the T2I model [12,14,17,19] or adopt ex-
isting approaches from editing large language models [6,13,29]. However, most
ofthemmodifymodelweightssequentiallywheneditingmultipleconcepts,lead-
ingtothe catastrophic forgetting [23]phenomenonwherethemodeldegenerates
withtheincreasingnumberofedits.RecentworkUCE[13]unifiesmultiplecon-
cept editing scenarios and edits multiple concepts in parallel. However, it still
faces a huge generation quality drop when editing more than 100 concepts. So
a challenge arises: How to preserve the generation ability of a T2I model when
massive concept editing such as editing over 1,000 concepts?
To tackle this challenge, we propose a two-stage framework, Editing Massive
Concepts In Diffusion Models (EMCID). The first stage performs decentralized
memory optimization of each individual concept with dual self-distillation loss.
The dual self-distillation loss aligns both the text features of the text encoder
andthenoisepredictionsoftheU-Net,encouragingsemantics-awareandvisual-
detail-awareaccuratememoryoptimizationofindividualconcepts.Inthesecond
stage,theoptimizationforindividualconceptsareaggregatedforeditinginpar-
allel.Wederivemulti-layerclosed-formmodeleditingtoenablemassiveconcept
editing.OurdesignsenableEMCIDtoexcelinlarge-scaleconcepteditinginT2I
models, allowing successful editing of up to 1,000 concepts.
Toconductcomprehensiveevaluationsandanalysisofconcepteditingmeth-
ods, we curate a new benchmark, named ImageNet Concept Editing Bench-
mark(ICEB).Inadditiontothesub-taskoflarge-scalearbitraryconceptediting
(including concept updating and concept erasing), we further propose a novel
and applicable sub-task, Concept Rectification, which is to rectify the incorrect
generation results of the less popular aliases of concepts. In contrast to earlier
benchmarks,whichmaysufferfromsmall-scaleevaluations,imprecisemetrics,or
limitedevaluationprompts,ourbenchmarkisequippedwithfree-formprompts,
up to 300 concept edits, and extensive metrics.
In summary, our contributions are three-fold. (1) We propose a two-stage
pipeline, EMCID, to edit T2I diffusion models with high generalization ability,Editing Massive Concepts in Text-to-Image Diffusion Models 3
specificity,andcapacity.Thedualself-distillationsupervisioninstageIforcesthe
modeltobeawareofbothsemanticsandvisualdetailsoftheeditingconcept.The
multi-layer editing and closed-form solution in stage II enable massive concept
editing. (2) We create a comprehensive benchmark to evaluate concept editing
methods for T2I diffusion models, spanning a magnitude of up to 300 edits
with two sub-tasks, free-form prompts, and extensive evaluation metrics. (3)
ExtensiveexperimentsdemonstratethescalabilityofEMCIDineditingmassive
concepts (up to 1,000 concepts), surpassing previous approaches that can only
edit at most 100 concepts.
2 Related Work
Text-to-image diffusion models.Diffusionmodelshavebeensuccessfullyap-
pliedfortext-to-imagegeneration[2,4,26,28,32,33].WiththeadvancedT2Idif-
fusion models gaining more popularity, they give rise to risks caused by various
issues: generation of images reflecting outdated or incorrect knowledge, copy-
right infringement, and reinforcement of societal biases [7,22]. These problems
canbemitigatedthroughextensivepreparationandmodification[3,11]oftrain-
ing data. However, this approach demands a significant investment of time and
computational resources. Therefore, the importance of a method that can han-
dle diverse concept editing tasks and enable extensive-scale editing cannot be
overstated. Our approach meets this need by scaling up the capacity for edited
concepts to a remarkable 1,000.
Fine-tuning T2I models for concept editing. A line of previous meth-
ods [12,14,17,19,38] fine-tune T2I diffusion models, particularly the cross-
attention layers, to selectively edit source concepts. A part of them focuses
on erasing concepts [12,17,38] while the others [14,19] generally edits source
concepts as destination concepts. However, during the continuous fine-tuning to
edit multiple concepts, these methods often encounter issues including catas-
trophic forgetting and significant time costs. Our method does not fine-tune
model weights directly, instead, we edit the weights with closed-form solutions.
Concept editing with closed-form solutions. To edit concepts in T2I dif-
fusion models, model-editing-based methods [6,13,29] are another line of work,
modifyingamodel’sweightswithclosed-formsolutions.Thesemethodstake in-
spirationfromthesuccessofknowledge-editinginNLP,where[24,25]haveintro-
ducedtheperspectiveofviewingMLPsaslinearassociativememories[5,18]and
successfullyeditedknowledgewithinLLMsfromthisperspective.Amongediting
based methods for T2I diffusion models, ReFACT [6] takes inspiration from the
method of ROME [24] and edits the text encoder of Stable Diffusion [32], while
UCE [13] and TIME [29] edit the cross-attention layers. Our method is also a
model-editing-based method, while being able to edit a much larger number of
concepts, compared to previous methods. Different from previous methods, our
method gives attention to the diffusion process itself and meticulously designs
the approaches to edit the text encoder of T2I diffusion models.4 T. Xiong et al.
3 Method
3.1 Overview
Task formulation.Weintegratevarioustypesofconcepteditingtasksfortext-
to-image generation, including updating concepts, erasing art styles, rectifying
imprecise generation, and gender debias, into a unified formulation. We define
concept editing in text-to-image generation as modifying the generated images
conditioned on the source concept to match the destination concept. This prob-
lemformulationunifiesvarioustypesofconcepteditingtasks,asshowninTab.1.
Table 1: Our problem setup for various tasks. We give an example for each task.
Tasks Examples SourcePrompts DestinationPrompts
UpdatingConcepts UpdateUSpresidentas “USpresident” “JoeBiden”
JoeBiden
ErasingArtStyles VanGoghtonormalstyle “ImageinVanGoghstyle” “Imageinnormalart
style”
RectifyImprecise Rectifysnowbird “Snowbird” “Junco” (amorepopular
Generation generation name)
GenderDebias Balance"doctor"gender “Doctor” “Femaledoctor”/“Male
ratio doctor” (1:1)
Where to edit? We consider the text-to-image diffusion models composed of
a transformer-based text encoder E(p) that encodes the input text prompt p
into feature embeddings and the U-Net image generator that predicts the noise
maps ϵ conditioned on the text embeddings. It is intuitive to assume that the
most textual and semantic knowledge are stored in the text encoder E and the
image prior are stored in the U-Net generator. In addition, prior works [24,25]
revealed that the feed-forward multi-layer perceptions (MLPs) store the factual
knowledge in large language models. Therefore, we focus on the MLP layers in
the text encoder in order to edit concepts in the text-to-image diffusion models.
Compared with previous approaches that edit cross-attention layers [13,29] or
finetune U-Net parameters [12,19], our approach ensures large concept editing
capacity without affecting the image generation quality.
How to edit?EachMLPintransformerconsistsoftwoweightmatriceswitha
non-linear activation in between, formulated as W ·σ(W ). We view W
proj fc proj
asalinearassociativememory[5,18],followingpreviouswork[24,25].Fromthis
perspective,alinearprojectionisakey-valuestoreWK ≈V,whichassociatesa
set of input keys K =[k |k |···k ] to a set of corresponding memory values
0 1 2 n
V =[v |v |···v ].Therefore,thegoalofourmodeleditingistoaddnewkey-
0 1 2 n
value pairs, K = [k | k | ··· | k ] and V∗ = [v∗ | v∗ | ··· | v∗ ],
1 n+1 n+2 n+e 1 n+1 n+2 n+e
into the associative memories while preserving existing key-value associations.
Mathematically, we formulate the objective as:
n n+e
(cid:88) (cid:88)
W∗ =argmin((1−α) ||Wk −v ||2+α ||Wk −v∗||2) (1)
i i i i
W
i=1 i=n+1
whereαisahyperparametertocontrolthetrade-offbetweenpreservingexisting
memories and editing new concepts. The existing key-value pairs K and V
0 0Editing Massive Concepts in Text-to-Image Diffusion Models 5
are estimated on the large-scale image-caption pairs (In practice, we use CCS
(filtered)datasetfromBLIP[20]).ThekeyvectorsinK representingthesource
1
concepts are derived from the features of the last subject token in the source
prompt, following previous work [6,24,25]. The remaining problems are how to
derive V∗ = [v∗ | v∗ | ··· | v∗ ] and how to solve the overall optimization
1 n+1 n+2 n+e
objective.
Overview of EMCID. Our EMCID is a two-stage method that edits multiple
layers of the text encoder of T2I diffusion models with closed-form solutions,
as illustrated in Fig. 2. The first stage (Sec. 3.2) performs decentralized mem-
ory optimization for each individual concept, aiming to optimize v∗ for each k
i i
in Eq. 1. With the proposed dual self-distillation optimization, the differences
between the source concept and the destination concept are distilled into an
optimizedfeatureoffsetvector,frombothsemanticconceptsofthetextencoder
and visual concepts of the diffusion model. The second stage (Sec. 3.3) aggre-
gates the optimized v∗ from individual concept optimization in the first stage
i
to optimize the objective in Eq. 1. We perform multi-layer model editing with
closed-form solutions to enable massive concept editing.
Text Encoder  
…the US president Add     =     ( )
EOS EOS EOS
So …urc Joe eP r Bo idm ep nts   Inte Lr am yee rd siate LL aa ys et
r
ℒ   
EOS EOS EOS
Destination Prompts    TS ho eu Urc Se pC ro en sic de ep nt t Destin Ja ot eio Bn id C eo nncept    n  o  is= y la  te( n  t̂ )        
Stage Ⅰ
Example Task: update as U-Net ℒ     
Optimization
Outdated Result Up-to-date Result
    
   
Independent Values Modify MLPs by Minimizing in Closed Form: ∗=argmin((1− )    −    +       −  ∗     )
from Stage Ⅰ       …...    
rectify as optimize
 ∗     
Stage Ⅱ the Vs Un a Sno p w G rb eo si gr id h dent ufo pr dg ae tt e a as sn Jooj eru m n Bc a ido l ea nrt + ++ {   ∗} Inte Lr am yee rd siate LL aa ys et r
Editing
Text Encoder  
 , modification to  ∗, optimized Last-Subject Token of Last-Subject Token of
initial value      modification to      Anchor Concept Source Concept Frozen Weights
Fig.2: The two-stage pipeline of EMCID. We demonstrate stage I with the example
of updating the source concept, “the US president”, as the destination concept “Joe
Biden”. In the first stage, we align both the embeddings of the text prompts and the
noise predictions ϵ ≜ϵ(x ,c ,t) and ϵ ≜ϵ(x ,c ,t). Multiple source concepts
dst t dst src t src
can be independently updated. In stage II, we edit the MLPs of the intermediate
layersofthetextencoderusingaclosed-formsolutionbasedontheindependentvalues
obtained from stage I.
3.2 Stage I: Memory Optimization with Dual Self-Distillation
Thegoalofthisstageistoobtainthevaluevectorsv∗ foreachkeyoftheconcept
to edit, which will be further used for the second term of the objective in Eq. 1.
… … …6 T. Xiong et al.
Specifically, given a group of source prompts p and destination prompts pˆ,
we encode the source text prompts p with the transformer-based text encoder,
and compute the average value of the last subject token after the non-linear
activation in the l-th MLP as the key k. The original value associated with the
source concept is v = Wk, and we aim to optimize an offset vector δ∗ so that
the new value v∗ = v+δ∗ can associate the source concept to the destination
concept.
In order to capture both the semantic-level concept and the visual details
of the destination concept, we design a novel dual self-distillation method to
optimize δ∗ with text alignment loss from the text encoder and noise prediction
loss from the diffusion model.
Self-distillation of semantic concepts from text encoder. In order to
associate the source concept with the destination concept, we optimize δ with
the text alignment loss L . It aligns between the source prompt embedding
txt
with updated values and the destination prompt embedding, as the following
optimization objective:
δ∗ =argmin||E (p)−E(pˆ)||2, (2)
v=+δ
δ
where E(·) denotes the feature vector of the [EOS] token encoded by the text
encoder, representing the embedding of the whole text prompt. The notation
E represents that we modify the computation of the text encoder by sub-
v=+δ
stituting the value vector v of the last subject token with v+δ.
Self-distillationofvisualconceptsfromdiffusionmodel.Theself-distillation
from text encoder only considers the text feature alignment between the source
prompt and the target prompt, but ignores the information from the diffusion-
basedimagegenerationmodel.Onlyaligningthetextembeddingsdoesnotguar-
antee our final goal of generating images with destination concepts. Therefore,
we propose the noise prediction loss L to distill visual knowledge of the
noise
destination concepts from diffusion models. The optimization objective is:
δ∗ =argminE ||ϵ(x ,E (p),t)−ϵ(x ,E(pˆ),t)||2, (3)
xt,t t v=+δ t
δ
where x denotes the images generated from the destination prompts added
t
by noise with timestep t, E(·) denotes the text embeddings which are injected
to the diffusion U-Net with cross-attention layers, ϵ(x ,E(pˆ),t) represents the
t
noise prediction with the destination prompt, and ϵ(x ,E (p),t) represents
t v=+δ
the noise prediction from the source prompt with the optimized offset δ. The
noise prediction loss enables end-to-end optimization of δ with a direct con-
straint, that the images generated from the source prompts should match the
images generated from the destination prompts. In addition, the self-distillation
from diffusion models allows the users to provide destination images instead of
destination text prompts, in scenarios where the destination concepts are diffi-
cult to describe with text prompts. In this case, the optimization objective is:
δ∗ =argminE ||ϵ(x ,E (p),t)−ϵ ||2, (4)
xt,t t v=+δ t
δEditing Massive Concepts in Text-to-Image Diffusion Models 7
where x denotes the user-provided images added by noise with timestep t, ϵ
t t
denotes the ground-truth noise added to the user-provided image, and other
notations remain the same with Eq. 3.
Dual self-distillation. We derive the overall optimizing objective by integrat-
ing the self-distillation objective L from the text encoder (Eq. 2) and the
txt
self-distillation objective L from the diffusion models (Eq. 3 or Eq. 4). The
noise
dual self-distillation enables us to find the optimal δ∗ and derive the updated
value v∗ =v+δ∗ to associate the source concept with the destination concept.
3.3 Stage II: Model Editing for Massive Concepts
Closed-form model editing for massive concepts. The previous value op-
timization stage finds the optimal values V∗ =[v∗ |v∗ |···|v∗ ] for each
1 n+1 n+2 n+e
concepttoedit.Backtoourfinalgoalofeditingmassiveconceptsandtheoverall
objective of modifying the weight matrix W to minimize the objective function
inEq.1.Followingpreviouswork[25],wederivetheclosed-formsolutionforthe
editing objective as,
W∗ =W +α(V∗−W K )KT (cid:2) (1−α)K KT +αK KT(cid:3)−1 , (5)
0 1 0 1 1 0 0 1 1
where W is the original weight matrix. The editing intensity hyperparameter α
0
controls the trade-off between editing concepts and preserving existing knowl-
edge, and is set to 0.5 by default. Detailed math derivation and experiments
exploring the trade-off effect concerning α are included in the Appendix Sec. C.
Multi-layer model editing for massive concepts.Editingmassiveconcepts
requiresalargecapacityofthemodelparametersthatcanbeupdated.Therefore,
insteadofeditingasingleMLPlayer,weproposetoupdatemultipleMLPlayers
in the text encoder. Specifically, we sequentially edit the weight matrices of the
MLP layers from the shallow layers to the deep layers. It is worth mentioning
that previous work MEMIT [25] observes improved robustness when spreading
the weight updates to multiple layers in large language models, which evidences
our design from another perspective. Different from MEMIT which spreads the
weight updates over the critical path, we conduct ablation studies and analysis
onwhichlayerstospreadtheweightupdatesoverandhowtheselectionoflayers
will affect the concept editing performance in Sec. 5.5.
4 Benchmark
Table 2: Comparison of concept editing benchmarks.
Benchmark Prompt Diversity Prompts Concepts Metrics Tasks
TIMED[29] Template 410 82 4 Update
RoAD[6] Template 450 90 4 Update
Artists-Forget[13] Template 7500 1000 4 Forget
Gender-Debias[13] Template 175 35 3 Debias
ICEB ChatGPT+Template 3330+900 300 6 Update,Rectify
Inpreviousliterature,therehasbeenanotablelackofastandardbenchmark
capableofevaluatingconcepteditingmethodsforT2Idiffusionmodelsatalarge8 T. Xiong et al.
scale. To address this problem, we have curated a comprehensive benchmark,
ImageNet Concept Editing Benchmark (ICEB). Our benchmark consists of two
sub-tasks. The first sub-task is designed for general evaluations in terms of the
abilitytoupdateorerasearbitraryconcepts,allowingforupdatingorerasingup
to 300 concepts. The second sub-task aims at a novel and practical application,
specificallyrectifyingtheincorrectgenerationresultsconditionedonlesspopular
aliases of concepts. In contrast to indirect metrics built on CLIP accuracy in
previouswork[6,19,29],weproposecomprehensivemetricstoevaluateaconcept
editing method from diverse aspects, as shown in Fig. 3.
4.1 Data Collection
Utilizing ChatGPT [27], we obtained 3,300 diverse and effective prompts for
666classesfromImageNet[8]afterfilteringpromptsandclassesthatstruggleto
guidethegenerationofcorrectimages.Forfiltering,weutilizedViT-B[10]asthe
evaluator. Only those prompts were retained whose generated images attained
a classification probability exceeding 0.5, as determined by ViT-B, accurately
reflecting the classes they describe. To present the potential editing effect gap
between simple template prompts and more diverse prompts, We also collected
900template-basedpromptsforevaluation.MoredetailsareinAppendixSec.J.
4.2 Task definition
Arbitrary Concept Editing. We define the first sub-task as a general task,
ArbitraryConceptEditing,whereasetofsourceconceptsfromImageNetclasses
are updated as another set of destination concepts from ImageNet. This task
supports updating up to 300 concepts. Concretely, we randomly sample 300
classesfromthe666collectedclassesofICEBassourceconcepts.Foreachsource
concept,adestinationconceptissampledfromthe5-nearestconceptsmeasured
byCLIPtextdistanceamongthe366classesleft.Wemeasuretheperformanceof
concepteditingmethodsonthistaskbythesuccessofediting,thegeneralization
to various aliases and prompts, and the preservation of non-source concepts.
Detailed definitions of these metrics are demonstrated in Sec. 4.3. Moreover,
the ability of erasing concepts can also be evaluated on this task by setting the
destination concepts as null.
Concept Rectification. We test the performance of Stable Diffusion v1.4 on
the less popular aliases on ImageNet, and we observe that many aliases cannot
guidethemodeltogeneratecorrectimages,asshowninFig.4.Thisphenomenon
gives rise to the demand for rectifying the generation results of T2I models.
Basedontheobservation,weproposeanovelsub-taskforICEB,namedConcept
Rectification. The task is to rectify the incorrect generation of “misunderstood
aliases” of ImageNet classes. Concretely, we collected 140 misunderstood aliases
and700evaluationpromptsbasedonICEB.Thistaskisevaluatedbytheextent
of rectifying these concepts and preserving existing knowledge.
4.3 Evaluation Metrics
Inpreviousbenchmarks[6,29],thesuccessofaneditwasdeterminedbyassessing
if the generated images conditioned on the source concept resemble the desti-Editing Massive Concepts in Text-to-Image Diffusion Models 9
Table 3: Summary of evaluation metrics
Metric Names Meanings of the Metrics Definition Equations
Sour (c Se FF )orget Th oe re igff ie nc at liv se on ue rs cs eo cf onfo cr eg pe tt st .ing SF= S1 (cid:80)S (cid:2) pM(si,si)−p Mˆ(si,si)(cid:3)
i=1
Theeffectivenessoftransforming
Sou (r Sc 2e D2D )est sourceconc ce op nts cei pnt to s.destination S2D= S1 i(cid:80) =S 1(cid:2) p Mˆ(si,di)−pM(si,di)(cid:3)
Theeffectivenessoftransforming
A (l Aia Ls2 2D De )st the ia nl tia ose ds eso tf int ah te ios nou cr oc ne cc eo pn tsc .epts AL2D= A1
L
iA (cid:80) =L 1(cid:2) p Mˆ(ali,di)−pM(ali,di)(cid:3)
Thedropingeneration
Hold (o Hu DtD )elta cap ca ob ni cl eit pie tss cfo ar usn eo dn- be ydi tt he ed eh do il td so .ut HD= H1 i(cid:80) =H 1(cid:2) p Mˆ(hi,hi)−pM(hi,hi)(cid:3)
nation concept more than the source concept itself, as evaluated by CLIP [30].
However, this approach overlooks the degree to which the source concepts have
beenalteredtoresemblethedestinationconcepts.Basedonthisobservation,we
propose four novel metrics, as listed in Tab. 3.
ForSourceForgetandSource2Dest,templateprompts(e.g.“animageof{}”)
and diverse prompts (generated by ChatGPT) are used separately to evaluate
the editing efficacy and generalization ability towards more complex prompts.
For other metrics we use ChatGPT generated diverse prompts for calculation.
Specifically, we define p (a,b) as the average confidence that the images
M
generated by the T2I model M conditioned on class b are classified as class a,
by a ViT-B image classification model pretrained on ImageNet. We denote the
original T2I model by M and the edited model by Mˆ. Thus the metrics are
defined as in the col. 3, Tab. 3. Here S, AL, and H represent the numbers of
editedsourceconcepts,aliases,andnon-editedholdoutconcepts,respectively.s ,
i
al , d , and h denote the i-th edited source concept, alias, destination concept,
i i i
and non-edited concept, respectively.
ForConceptRectification,thesourceconceptsarethemisunderstoodaliases,
andthedestinationconceptsaredefinedastheirclasses.So,Source2Destisused
tomeasurethesuccessofrectifyingthegenerationresultsofthealiases.Wedon’t
calculate Source Forget or Alias2Dest because they are ill-defined for this task.
WefurtheruseCLIPscoreandFID[15]onCOCO-30kprompts[21]toevaluate
the preservation of the T2I model’s generation capabilities.
5 Experiments
Inthissection,wefirstdemonstrateexperimentssetupinSec.5.1,thenwecom-
pareourmethodwithbothfine-tuning-basedandmodel-editing-basedbaselines
on the two sub-tasks of ICEB, in Sec. 5.2 and Sec. 5.3. We also compare our
method on the task of erasing 1,000 artist styles with the recent SOTA method
UCE [13] in Sec. 5.4. We further present ablation studies about our method in
Sec. 5.5. Our method achieves comparable results to UCE in gender-debiasing
multiple professions, as explained in the Appendix Sec.G.10 T. Xiong et al.
Holdout Delta Efficacy: Source Forget Efficacy: Source2Dest
0.0 1.0 1.0
0.8 0.8
0.2
0.6 0.6
0.4 0.4 0.4
0.6 0.2 0.2
0.0 0.0
0.8
10 50 100 150 200 250 300 10 50 100 150 200 250 300 10 50 100 150 200 250 300
Edit Number Edit Number Edit Number
Generalization: Source Forget Generalization: Source2Dest Alias2Dest
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
10 50 100 150 200 250 300 10 50 100 150 200 250 300 10 50 100 150 200 250 300
Edit Number Edit Number Edit Number
emcid(ours) refact time uce esd
Fig.3: WepresentcomparisonsonthetaskofArbitraryConceptEditing.Weusethe
dot marker for methods editing source concepts as designated concepts, and the cross
marker for concept erasing methods. Source2Dest and Alias2Dest are not suitable for
comparisons for concept erasing methods, thus not presented for them. Our EMCID
can successfully edit up to 300 concepts with minor influence to holdout concepts.
In comparison, the success of our baselines in editing source concepts and preserving
holdout concepts exhibits a rapid decline as the number of edits increases.
5.1 Experiments Setup
We test our method and baselines for concept editing tasks on the SD v1.4 [2]
model. By default, we utilize dual self-distillation in stage I, integrating objec-
tives from Eq. 2 and Eq. 3, as a hybrid loss L , with a balance factor λ set
hybrid s
empirically as 0.01:
δ∗ =argmin(L +λ L ) (6)
noise s txt
δ
This optimization stage for each concept takes only 200 gradient update steps
with a constant 0.2 learning rate. For stage II, by default we spread the weight
updates over all the layers of the text encoder except the last one, which can
largely enhance the editing effect, according to our ablation studies in Sec. 5.5.
On an NVIDIA RTX 4090 GPU, stage I for each concept can be accomplished
within 92 seconds, and stage II for aggregating eidts of 300 concepts can be
completed within 10 seconds. The parallelization of the first stage and the fast
speed of the second stage allow our method to edit massive concepts in a signif-
icantly shorter period, in contrast to methods sequentially editing the concepts.
Forpromptspandpˆusedfortraining,wechosesimpletemplate-basedprompts,
as a typical approach of previous methods [13,29], for fair comparisons.
5.2 Large-Scale Arbitrary Concept Editing
Setup.Inthisexperiment,weemployconcepteditingmethodsforT2Idiffusion
models on the Arbitrary Concepts Editing task, as defined in Sec. 4.2. We com-
pare with methods capable of editing more than 10 concepts on ICEB. Among
them,ESD-x[12]fine-tunesthecross-attentionlayersofStableDiffusiontoerase
erocS
slC
erocS
slCEditing Massive Concepts in Text-to-Image Diffusion Models 11
concepts. Besides, UCE [13], TIME [29] and ReFACT [6] modify either cross-
attention layers or the text encoder with closed-form solutions, to alter source
concepts towards destination concepts. We set α=0.6 for EMCID on this task.
Metrics. We use metrics defined in Sec. 4.3 for evaluation. For concept erasing
methods, we only present 2 metrics, Source Forget and Holdout Delta, as the
remaining metrics are unsuitable for comparisons in their case.
Analysis. As shown in Fig. 3, EMCID can edit the source concepts as desti-
nation concepts with high Source2Dest across different edit scales, while for all
baselines the success of edits drops quickly with the increase of edit number.
Moreover, EMCID presents superior specificity for preserving non-source con-
cepts, proved by the relatively minor Holdout Delta. The edit effect of EMCID
can also generalize to diverse prompts and aliases of the source concepts.
Class African King African Mountain Snow
Junco Kola bear Ostrich
Names grey penguin hunting dog lion leopard
Correct
Images
Misunderstood Phascolarctos Psittacus Aptenodytes Cape Struthio Felis Panthera
Snowbird
Aliases cinereus erithacus patagonica Hunting dog camelus concolor uncia
Wrong
Images
UCE
Mended
Images
EMCID
Mended
Images
Fig.4: The qualitative comparison between EMCID and UCE on the task of rectify-
ing misunderstood aliases. The correct generation results are wrapped in green, while
the incorrect ones are wrapped in red. EMCID presents remarkable efficacy while the
baseline method, UCE, often fails to rectify the aliases effectively.
Table 4: ThecomparisonofUCEandEMCIDforrectifying140misunderstoodIma-
geNetaliases.Ourmethodcanaccomplishthetaskwithminordamagetothemodel’s
generation capabilities, while UCE leads to the corruption of the model on this task.
Method Efficacy:S2D↑Generalization:S2D↑HoldoutDelta↑CLIP↑FID↓
UCE -0.0312 -0.0760 -0.7460 12.71 138.42
EMCID(ours) 0.5692 0.3453 -0.1447 26.24 15.00
OriginalSD - - - 26.62 13.93
5.3 Concept Rectification
Setup.WecompareEMCIDwiththeSOTAmethodUCE[13]onthetaskofrec-
tifying 140 misunderstood aliases of concepts, defined as Concept Rectification
in Sec. 4.2. We also conduct qualitative experiments to rectify 8 misunderstood
aliases, as shown in Fig. 4. The rectification is accomplished by editing the mis-
understood aliases (e.g., “Snowbird”) as its popular class name (e.g., “Junco” )
which can correctly guide the generation of the T2I diffusion model. Because
UCE needs to specify the concepts to preserve, we choose 200 ImageNet classes12 T. Xiong et al.
# of
emcid uce
Erased 0 5 100 500 1000 COCO-30k: CLIP Score COCO-30k: FID Score
Artists 25.0 80
22.5 60
UCE 12 70 .. 50 24 00
15.0 1 5 10 50 100 5001000 1 5 10 50 100 5001000
Edit Number Edit Number
Holdout Artists: CLIP Score Holdout Artists: LPIPS
0.4
26
EMCID 24 0.3
22 0.2
20
18 0.1
1 5 10 50 100 5001000 1 5 10 50 100 5001000
Edit Number Edit Number
Fig.5: We present comparisons between EMCID and the baseline method, UCE, fo-
cusing on the preservation of holdout artist styles and overall generation capabilities
after erasing a large number of artist styles. (a) For the qualitative results in the left
part, we showcase the preservation of the style of The Great Wave off Kanagawa by
Hokusai.(b)Thequantitativeresultsattherightpartdemonstratethepreservationof
both 500 holdout artist styles and the overall generation capabilities. (c) Our method
excels at preserving the unique styles of holdout artists, particularly when removing
more than 500 styles. Moreover, the drop in the overall generation capabilities caused
by EMCID is negligible even after erasing 1,000 styles.
Class Appenzeller Rock
Shoji Poke bonnet Mortar Bolete
Names sennenhund beauty
Incorrect
Generated
Images
After
EMCID
Rectification
Fig.6:Weprovideconceptrectificationresultsgivenonlyreferenceimagesforconcept
editing.Previousmodeleditingmethods[6,13,29]cannotperformthistaskbydesign.
as concepts for it to preserve. We further apply EMCID to rectify 6 classes that
cannot be generated properly by Stable Diffusion v1.4, using only reference im-
ages from ImageNet validation set. Note that for this scenario, previous model
editing methods [6,13,29], including UCE, are not applicable.
Metrics. As explained in Sec. 4.3, for large-scale quantitative evaluation, we
use Source2Dest to measure the success of the edits. And for evaluating the
preservationofgenerationcapabilities,besidesHoldoutDeltawefurthermeasure
CLIP score and FID on COCO-30k prompts.
Analysis. As shown in Fig. 4, UCE struggles to rectify aliases even at a small
scale, while our method can effectively correct the results. We demonstrate that
this is because UCE’s objective neglects the diffusion process and only focuses
on the encoding of the texts, while our method maintains high efficacy through
dualself-distillation.What’smore,whenonlyreferenceimagesareavailable,our
method can still successfully rectify the source concepts, as shown in Fig. 6.
5.4 Erasing Artist Styles
Setup. For a fair comparison, we follow the experiment setting of UCE. We
conductexperimentsforerasingfrom1to1,000artiststyles,anderaseanartist’sEditing Massive Concepts in Text-to-Image Diffusion Models 13
# of Erased Artists 1 5 10 100 500 1000
Erased Artist Andrew Antonio John Vincent Van Claude Pablo
Example Ferez J. Manzanedo Constable Gogh Monet Picasso
Original
SD
EMCID
Fig.7: We present the style erasing effects after erasing 1 ∼ 1000 styles. For each
column, we present an example from the erased artist styles. The results prove that
EMCID can successfully erase up to 1,000 artist styles.
Generalization:S2D Holdout Delta F1
1 90 0.5 1 90 0.02 1 90 0.175
678 0.4 678 00 .. 00 64 678 00 .. 11 25 50
5 0.3 5 0.08 5 0.100 34 0.2 34 0.10 34 0.075
2 2 2 0.050
1 0.1 1 0.12 1 0.025
0 0 0.14 0
012345678910 012345678910 012345678910
start layer start layer start layer
Fig.8:Ablationforeditlayersofthetextencoder.Thefirsttwographspresentatrade-
offbetweeneditsuccessandthepreservationofnon-editconcepts.ForSDv1.4[2],the
text encoder has 12 layers. According to the results measured by F1, the best setting
is to edit 0-10 layers.
style (e.g. “Van Gogh”) by editing it as “art”. Note that it is also feasible to edit
the styles as any designated concepts with our method. We present qualitative
resultsofourEMCIDinFig.7,showcasingtheeffectoferasingupto1,000artist
styles.WefurthercomparewithUCE,forthepreservationofboth500non-edited
artist styles and the model’s overall generation capabilities, as shown in Fig. 5.
We observe that erasing artist styles is a less challenging task and requires less
parameter modifications, so we edit the 7 to 10 instead of 0 to 10 (our default
setting) layers of the text encoder, based on observations in Sec. 5.5.
Metrics. To evaluate the preservation of non-source artist styles, we calculate
the CLIP score of the modified T2I model on 2,500 prompts trying to mimic
the 500 holdout artist styles. LPIPS [39] between images generated by pre-edit
and post-edit T2I models is also used to measure the influence on holdout artist
styles.AhigherCLIPscoreorlowerLPIPSmeansbetterpreservationofholdout
styles.Forthepreservationofoverallgenerationcapabilities,weadoptthetypical
approach of measuring CLIP score and FID on COCO-30k prompts.
Analysis. As shown in Fig. 7, our method can successfully erase the styles of
artistsinthediffusionmodel.Forthepreservationofholdoutartists,asdepicted
inFig.5,whileourEMCIDismarginallybetterthanUCEforlessthan100edits,
itsurpassesUCEbyalargemarginformorethan500edits.Moreover,theoverall
generationcapabilitiesmeasuredbyFIDandCLIPscoreonCOCO-30kprompts
arehardlycontaminatedwithEMCID.Incontrast,UCEleadstothecorruption
of the model after erasing at most 500 artist styles.
reyal
tsal
reyal
tsal
reyal
tsal14 T. Xiong et al.
5.5 Ablation Studies
Wepresentablationstudiesabouttherangeoflayerstoeditinthetextencoder
andouroptimizationobjectives.WechosethetaskofArbitraryConceptEditing
for evaluation, at a scale of 100 edits. Generalization:S2D and Holdout Delta
serveasevaluationmetrics,andtheiraverage(denotedF1)isthedecisionmetric.
Weconductahyper-searchofallpossiblerangesofeditlayers.Thelastlayer
of the text encoder cannot be edited because this will disable the optimization
forEq.2,asdepictedinFig.2.TheresultsarepresentedinFig.8,whichdemon-
strate a trade-off between edit effectiveness and specificity for different numbers
of edit layers. With fewer edit layers, the specificity improves, but the effective-
ness of edits degrades. We analyze that increasing the modified parameters can
enhance the editing effect but also risks affecting non-edited concepts. The best
setting decided by F1 is editing all the layers before the last layer, which is the
default setting in this paper.
Table 5: The results of 100 arbitrary concept editing. We employ different objectives
in the first optimization stage. L outperforms single L or L .
hybrid noise txt
Objective Generalization:S2D↑ HoldoutDelta↑ F1↑
Lnoise 0.5176 -0.2037 0.1569
Ltxt 0.5134 -0.1431 0.1852
Lhybrid 0.5326 -0.1403 0.1962
In the ablation study of optimization objectives, we test L , L , and
noise txt
L separately. As shown in Tab. 5, L presents the best editing effect
hybrid hybrid
andpreservationofnon-editconcepts.ThusL issetasthedefaultobjective
hybrid
in the first optimization stage of our work.
6 Ethical Impact & Limitations
OurmethodisdesignedtoalleviatetheinappropriategenerationofT2Idiffusion
models. However, we recognize the possibility of our method being used mali-
ciously to inject disinformation into the models. Thus we strongly urge models
edited by EMCID to provide exact information about the edited concepts.
Despite the exceptional generalization ability and scalability, our method
cannot prevent the problem of NSFW generation conditioned on prompts with
lowtoxicity,asobservedin[12,17,34].Wepresentdetaileddiscussionsaboutthe
limitation in the Appendix Sec. I.
7 Conclusion
Wehavedesignedatwo-stagealgorithm,EMCID,foreditingmassiveconceptsin
T2Idiffusionmodels,whichexcelsinawidespectrumofmulti-concepttasks.Be-
sides, we have proposed a standard benchmark, ICEB, to enable comprehensive
evaluation of concept editing methods for T2I diffusion models at a large scale.
Extensive experiments have demonstrated the superior scalability and effective-
nessofourmethod,comparedtoexistingfine-tuningandmodeleditingmethods.
Wehopeourworkwillinspirefutureresearchoncomprehensivelydetectingand
resolving the inappropriate generation problems of generative models.Editing Massive Concepts in Text-to-Image Diffusion Models 1
A Overview
In this supplementary material, we provide the following content:
– Sec. B, we give the math derivations and details of our method for editing
the T2I diffusion models.
– Sec. C is the ablation study on editing intensity, showcasing the results in
terms of editing concepts and erasing artistic styles.
– Sec.D-Hdelveintothericherexperimentalcontent,includingextensivebase-
line testing on the ImageNet Concept Editing Benchmark (ICEB) (Sec. D)
and experimental performance of EMCID on updating concepts (Sec. E),
erasing artistic styles (Sec. F), eliminating gender biases (Sec. G) in profes-
sions and single concept editing for 2 previous benchmarks (Sec. H).
– Sec. I discusses the limitations of our method, particularly its inability to
eliminate NSFW content.
– In Sec. J, we provide details about the data collection process for ICEB
and exploration of the performance of Stable Diffusion in generating images
about Imagenet [8] concepts.
B Math Derivation about the Editing Stage
To derive the closed-form solution for the model editing objective:
n n+e
(cid:88) (cid:88)
argmin((1−α) ||Wk −v ||2+α ||Wk −v∗||2) (7)
i i i i
W
i=1 i=n+1
We can define the loss function:
n n+e
(cid:88) (cid:88)
L(W)=(1−α) ||Wk −v ||2+α ||Wk −v∗||2 (8)
i i i i
i=1 i=n+1
And the optimal W∗ can thus be derived from:
∂L(W∗)
=0 (9)
∂W
which is:
n n+e
(cid:88) (cid:88)
(1−α) (W∗k −v )kT +α (W∗k −v∗)kT =0 (10)
i i i i i i
i=1 i=n+1
We further define W∗ =W +∆, and rearrange the equation above as:
0
(W +∆)(cid:2) (1−α)K KT +αK KT(cid:3)
0 0 0 1 1 (11)
=(1−α)V KT +αV∗K
0 0 1 1
where V =[v |···|v ], K =[k |···|k ], K =[k |···|k ] and V∗ =
0 1 n 0 1 n 1 n+1 n+e 1
[v |···|v ],asdefinedinthemainpaper.Wecanassumethattheoriginal
n+1 n+e2 T. Xiong et al.
weightmatrixhasbeenoptimizedtoachieveminimalsquarederrorforkey-value
associations:
n
(cid:88)
W =argmin ||Wk −v ||2 (12)
0 i i
W
i=1
Thus we can easily derive that W satisfies the equation:
0
W K KT =V KT (13)
0 0 0 0 0
According to Eq. 11 and Eq. 13, we can derive the final result:
∆=α(V∗−W K )KT (cid:2) (1−α)C +αK KT(cid:3)−1 (14)
1 0 1 1 0 1 1
where C = K KT and is estimated by C ≈ λE[kkT], following MEMIT [25].
0 0 0 0
We use the CCS (filtered) image-text-pair dataset of BLIP [20] for the estima-
tion. While it is possible to adjust λ to trade off between editing success and
the preservation of non-source concepts, we argue that this is just an empirical
approach. We instead use the well-defined editing intensity α for the trade-off.
The bigger the α, the stronger the edit, and the less the preservation for other
concepts. Setting α=0.5 will give the same solution as the original objective of
MEMIT [25].
C Experiments about the Editing Intensity α
Editing Intensity Ablation
0.5
0.4
0.3
0.2
0.1
0.0
0.1
0.2
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Editing Intensity
Source2Dest Holdout Delta F1
Fig.9: We demonstrate a trade-off between editing success measured by Source2Dest
andpreservationforgenerationcapabilitiesmeasuredbyHoldoutDelta,undervarious
editing intensities α. All results are obtained on the task of 100 Arbitrary Concept
Editing. We note the importance of adjusting the alpha parameter to achieve a har-
moniousbalancebetweentheeditingsuccessandthepreservationofotherconcepts.α
is set as 0.5 by default.
We conduct experiments to evaluate the effect of adjusting editing intensity
α.AsshowninFig.9,increasingαwillboosttheperformanceofconceptediting,
whileimpairingthepreservationofothernon-editedconcepts.Thus,thereexists
erocS
noitacifissalCEditing Massive Concepts in Text-to-Image Diffusion Models 3
Fig.10: Wedemonstratetheinfluenceoneditingsuccessandthepreservationofhold-
outconceptsofdifferenteditingintensitiesαontwotasks:ArbitraryConceptEditing
and erasing artist style. (a) We present qualitative results for editing “timber wolf” as
“tiger”,whichisoneofthe100editsappliedtotheT2Imodelinthistask.(b)Wepresent
the generated images after erasing “Vincent Van Gogh” and “The Starry Night” with
various editing intensities. In (a) and (b), we observe successful concept editing and
artiststyleerasingoccurringwhenαisgreaterthan0.5.Furtherincreasingtheediting
intensityhasrelativelyminoreffectsonbothconcepteditingandconceptpreservation.
Meanwhile,Ourmethoddemonstratesexcellentpreservationofbothholdoutconcepts
and styles.
a trade-off in the value of α. We further present qualitative results of increasing
editing intensity in Fig. 10 (a).
TheeffectofadjustingαforerasingartiststylesisalsopresentedinFig.10(b).
Inthispart,weeraseboth“VincentvanGogh” and“theStarryNight”,andgen-
erate images conditioned on “The Starry Night by Vincent van Gogh”, with
various α. Unlike other works of Vincent van Gogh, the extraordinarily famous
“TheStarryNight” ismemorizedbyStableDiffusionandcannotbeforgottenby
simplyerasing“VincentvanGogh” withourmethod.Wefurthergenerateimages
conditionedon“GirlwithaPearlEarringbyJohannesVermeer” toshowcasethe
preservation of other styles. The results reveal that increasing editing intensity
can increase the erasing effect, while slightly influencing other styles.
D Arbitrary Concept Editing: All Baselines
We conduct experiments for all existing baselines on the task of Arbitrary Con-
cept Editing. All baselines are implemented with official code, and we have fur-
ther adjusted the learning rates of some methods for better performance on this4 T. Xiong et al.
Holdout Delta Efficacy: Source Forget Efficacy: Source2Dest
0.0 1.0 1.0
0.8 0.8
0.2
0.6 0.6
0.4 0.4 0.4
0.6 0.2 0.2
0.0 0.0
0.8
1510 20 30 40 50 100 1510 20 30 40 50 100 1510 20 30 40 50 100
Edit Number Edit Number Edit Number
Generalization: Source Forget Generalization: Source2Dest Alias2Dest
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
1510 20 30 40 50 100 1510 20 30 40 50 100 1510 20 30 40 50 100
Edit Number Edit Number Edit Number
emcid(ours) time ablate esd sdd
refact uce sa
Fig.11: We test all existing concept editing methods for T2I diffusion models on the
taskofArbitraryConceptEditing,foreditingupto100concepts.Wedonottestfine-
tuning-based methods losing all specificity after 30 edits for larger-scale editing. Our
method presents both exceptional generalization abilities and specificity.
task. The baselines are divided into two categories, fine-tuning-based methods
and editing-based methods. Among fine-tuning-based methods, Concept Ab-
lation (Ablate) [19] and Selective Amnesia (SA) [14] fine-tune the weights of
T2I diffusion models to alter the source concepts towards the destination con-
cepts,whileESD-x[12],Forget-Me-Not(FGMN)[38]andSDD[34]aredesigned
to erase source concepts in T2I diffusion models. For editing-based methods,
TIME [29] and UCE [13] utilize closed-form solutions to modify the K-V pro-
jection matrices in the cross-attention modules of Stable Diffusion [32], while
ReFACT [6] modifies the transformer MLP of a single layer in the text encoder.
We use 3 simple template-based prompts to train our method and all base-
lines, except for Concept Ablation and Selective Amnesia which depend on di-
verse training prompts to achieve good performance. So we follow the official
settings for the training of the two methods. We still measure their efficacy us-
ingtemplate-basedprompts,insteadofusingtheirdiversetrainingprompts.The
same metrics for the Arbitrary Concept Editing experiment in the main paper
are utilized in this part.
As shown in Fig. 11 (row 1, col. 1), only ESD-x, ReFACT, and our method
can retain the generation capabilities of the T2I model, measured by Holdout
Delta, after editing 30 concepts, while ESD-x and ReFACT fail to keep the
editingsuccessmeasuredbyTargetForgetorSource2Dest.Incontrast,ourEM-
CIDpresentsthebesteditingsuccessfordiverseChatGPTgeneratedevaluation
prompts across all edit numbers (row 2, col. 2).
erocS
slC
erocS
slCEditing Massive Concepts in Text-to-Image Diffusion Models 5
The president of the United States under the sea The president of the United States under the sea
The president of the United States playing guitar The president of the United States playing guitar
The president of the United States in the forest The president of the United States in the forest
Fig.12:Wedemonstratethestrongeditabilityofourmethodfortheupdatedconcept.
Wepresentthegenerationresultsbeforeandafterupdatingtheconcept“theUSpres-
ident” as “Joe Biden”. The left three columns represent the results from the original
SD model, while the right three columns are the results after applying our method
to update “the US president”. Instead of overfitting certain poses or backgrounds, the
update of our method can generalize to diverse prompts. Moreover, while the original
model can often fail to generate images aligned well to the given concept, after it is
modified by EMCID, the generated images can constantly correspond to “Joe Biden”.
E More Experiments on Updating Concepts
In this section, we demonstrate details about the performance of our EMCID
for updating concepts, with the example of updating“the US president” as “Joe
Biden”.InFig.12,wheretheleft3columnsareimagesgeneratedbythepre-edit
T2I model and the right 3 columns are generated by the post-edit T2I model,
EMCIDdemonstrategoodeditabilityfortheupdatedconcept.Weobserveano-
ticeable improvement in the model’s response to "the US Presidents" following
concept updates. For the pre-edit SD model, while some Trump-related images
can be generated for the conceptof “theUS president”, thegenerated images of-
ten fail to reveal content related to “the US president” if conditioned on diverse
prompts (1st row, 3rd column). However, after “the US president” is updated
with our EMCID, the generated images consistently feature “Joe Biden”. More-
over, as shown in Fig. 13, the update of “the US president” can easily generalize
to“theAmericanpresident”,whilehavinglimitedinfluenceonneighborconcepts
such as “Prime Minister of Canada” or “the president of Mexico”.
F More Experiments on Erasing Artist Styles
In this section, we explore our method’s capability to generalize the erasing of
artist styles to the famous works of the artists, and demonstrate our method’s6 T. Xiong et al.
The American President The American President
Prime Minister of Canada Prime Minister of Canada
Thepresident of Mexico The president of Mexico
Fig.13: To demonstrate both the specificity and generalization capability of our
method,wepresenttheeffectofupdating“TheUSPresident” to“TheAmericanPresi-
dent”,“PrimeMinisterofCanada”,and“ThepresidentofMexico”.Ourmethodsuccess-
fully generalizes the update to “The American president” (top row) while preserving
neighbor concepts such as “Prime Minister of Canada” (2nd row) and “The president
of Mexico” (3rd row).
generalization ability in the scenario where the names of the artists are mis-
spelled. We take “Vincent van Gogh” for example. Because erasing artwork re-
quiresmoregeneralizationability,weadjusttheeditingintensityas0.6forbetter
visual results, while keeping the same edit layers, 7 to 10, as in the artist-style
erasing experiment in the main paper. The results are shown in Fig. 15 and
Fig. 14.
AftererasingthestyleofVincentvanGoghbyediting“VincentvanGogh” as
“a realist artist”, we utilize prompts related to his artworks to generate images,
as shown in Fig. 15. The style of “Vincent van Gogh” is successfully erased from
the generation results about his works. This demonstrates the generalization
capability of our method to erase artist styles. The erasing results for more
diverse artist styles are depicted in Fig. 19.
Someextraordinarilyfamousartworks(e.g.“TheStarryNight”)maybemem-
orizedbyStableDiffusionandthuscannotbeerasedbysimplyerasingtheartist
style, (e.g. “Vincent van Gogh”) with EMCID. And our solution is to erase both
the works and the artist style, as shown in Fig. 10 (b).
G Gender Debiasing with EMCID
Stable Diffusion has been reported to exhibit gender bias when generating im-
ages for certain professions [13,29]. From the perspective of EMCID, we can
mitigate gender bias about a profession by associating gender-debiased new val-
ues with input keys of these professions. In our experiments, we found that aEditing Massive Concepts in Text-to-Image Diffusion Models 7
Fig.14: To prove that the editing effect of our method can generalize to misspelled
names,fortheprompt“BedroominArlesbyVincentvanGogh”,wemisspelled“Vincent
vanGogh” togeneratesimilarimages,asshownintheleftthreecolumns.Intheright
threecolumns,ourmethodsuccessfullygeneralizestheeditingtothemisspellednames.
Fig.15: We chose three famous works of Vincent van Gogh to test the generalization
of erasing artist styles to their works. The left three columns are the results for the
prompt located below the image, and the right three columns are the results after
editing“VincentvanGogh” as“Arealistartist”.OurEMCIDcansuccessfullyerasethe
style in the works after erasing “Vincent van Gogh”.
weighted mean of the values of “male [profession]” and “female [profession]” can
be the gender-debiased value. So we propose a simple algorithm 1 to adjust the
weightsinmultiplerounds,accordingtotheratioof“female[profession]” images
in the generated images conditioned on “[profession]”, by the edited model. The
qualitative results of our approach are illustrated in Fig. 16.
We have compared our method with UCE on the task of gender-debiasing
professions. EMCID is on par with UCE for debiasing multiple concepts si-
multaneously while showcasing superior performance when debiasing a single
profession.
Setup. To evaluate our method, we test our EMCID and the SOTA method
UCEonthetasksofmultipleandsingleprofessiondebiasing.37professionsfrom
WinoBias[40]areusedforevaluation.Weevaluatethesuccessforbothseparate
debiasing a single profession and simultaneously debiasing multiple professions,
as shown in Tab. 6.
Metrics. Wegenerate250imagesforeachprofessionanduseCLIP[30]toclassify
the images as “female [profession]” or “male [profession]”. Following TIME [29],
we calculate the normalized absolute difference between the desired percentage
offemaleimages,50%,andtherealpercentageF afterdebiasing,givenby∆ =
p p
|F −50|/50. The ideal value for ∆ is 0. Besides, we use FID and CLIP score
p p
on COCO-30k prompts to measure the preservation of the model’s generation
capabilities, when multiple professions are debiased.8 T. Xiong et al.
Algorithm 1 Get Debiased Value for Model Editing
1: Input: Diffusion Model M
2: Input: Concept c to debias
3: Input: Attributes A to balance , the size p of A
4: Input: Initial learning step η , desired ratios R
0 des
5: Input: max iterations m, min absolute difference d
6: V∗ ← OPTIMIZE(M,c,A) ▷ EMCID stage I, V∗ =(v∗ ,...,v∗ )
a1 ap
7: F ← (1/p,··· ,1/p)T ▷ factors to balance V∗
8: v∗ ← V∗F ▷ debiased value
d
9: for i in range(0, m) do
10: M ← EDIT(M,c,v∗) ▷ EMCID stage II
d
11: R ← get_ratios(M,c,A) ▷ ratios of A
curr
12: df ← R −R
curr des
13: M ← restore(M)
14: if max(df)≤d then
15: return v∗
d
16: end if
17: η← η (1−i/m) ▷ linear scheduler
0
18: F ←F −η·df
19: v∗ ←V∗F
d
20: end for
21: return v∗ ▷ debiased value
d
Analysis. AsshowninTab.6,EMCIDisonparwithUCEfordebiasingmultiple
concepts. In contrast, when debiasing a single profession, EMCID showcases
superior performance.
(a) Debiassingleprofession
Metric Original SD UCE EMCID
∆ ↓ 0.62 ± 0.02 0.37 ± 0.02 0.23 ± 0.01
p
(b) Debiasmultipleprofessions
Metric Original SD UCE EMCID
∆ ↓ 0.62 ± 0.02 0.32 ± 0.02 0.33 ± 0.02
p
CLIP↑ 26.62 26.59 26.60
FID↓ 13.93 13.69 13.56
Table 6: Our method demonstrates its competence in addressing the prevalent issue
ofgenderbiaswithinStableDiffusion.Wepresentresultsforbothdebiasingsingleand
multipleprofessions.In(a)and(b),∆ isaveragedforthe37professions.OurEMCID
p
demonstratesbetterresultsfordebiasingasingleprofessionandiscomparabletoUCE
when debiasing multiple professions.
H Experiments on RoAD and TIMED
While our EMCID uniformly shows strong performances across different multi-
concept editing tasks, it also presents good performance on existing single-Editing Massive Concepts in Text-to-Image Diffusion Models 9
Gender Biased Gender Debiased
Fig.16: We sampled 4 seriously biased professions for the demonstration after simul-
taneously gender-debiasing 37 professions. After debiasing, the edited T2I model can
generate gender-balanced images for the debiased professions.
concepteditingbenchmarks,namelyRoAD[6]andTIMED[29].RoADcontains
concept editing requests about roles (e.g. “Canada’s Prime Minister” → “Bey-
once”) as well as appearances of people and objects. TIMED includes editing
requests concerning changing the implicit assumptions about the attributes of
certain concepts (e.g. “rose” → “blue rose”).
Dataset Method Efficacy (↑) Generalization (↑) Specificity (↑) F1 (↑)
Oracle 92.11% ±2.66 92.69% ±0.93 95.58% ±1.05 94.14
ReFACT 92.08% ±1.81 81.82% ±1.45 78.81% ±1.46 80.32
TIMED
UCE 91.54% ±3.20 75.58% ±2.21 71.69% ±1.40 73.64
EMCID(ours) 81.58% ±3.21 80.99% ±0.83 73.32% ±1.82 77.12
Oracle 98.27% ±1.14 98.30% ±0.61 99.35% ±0.28 98.80
ReFACT 92.89% ±2.20 86.44% ±0.60 96.41% ±0.50 91.43
RoAD
UCE 78.22% ±2.18 69.29% ±1.44 92.09% ±0.98 80.69
EMCID(ours) 94.13% ±2.75 89.70% ±0.71 90.55% ±0.54 90.13
Table 7: Results on 2 single concept editing benchmarks: TIMED and RoAD. The
best results of the non-oracle method are in bold. Oracle method generates images
conditioned on the anchor concepts directly instead of the target concepts. EMCID
is comparable to ReFACT in the single concept editing scenario for editing roles and
appearance on RoAD.
RoAD and TIMED both evaluate concept editing methods with 3 metrics:
efficacy, generalization, and specificity. They are all a sort of success rate.
Efficacy is defined as the success rate that the generated images conditioned
on prompts about the edited source concept is classified as the destination con-
cept.
noitcurtsnoC
esruN
rekrow
fehC
OEC10 T. Xiong et al.
Generalization’s definition is similar to efficacy. The only difference is that
the testing prompts are not used in the editing stage for training.
Specificity measures the influences of the edit on the related neighbor con-
cepts (e.g. “Hagrid” is a neighbor concept of “Harry Potter”). So specificity is
defined as the success rate that the generated images conditioned on prompts
about the neighbor concepts are classified as the neighbor concepts themselves
instead of the destination concept. The higher the specificity, the less probable
thataneighborconceptisalsoeditedasthedestinationconceptafterthesource
concept is edited.
Following [6,29], the F1 score is defined as the mean of generalization and
specificity, serving as the key decision metric.
AsshowninTable7,ourmethod’sperformancesarecomparabletotheSOTA
method ReFACT on RoAD, while being slightly worse on TIMED. This perfor-
mance difference in the 2 benchmarks comes from the bias in datasets. Mean-
while,ourEMCIDisbetterthanUCEonthe2benchmarks,especiallyonRoAD.
WhileReFACTcanachievethebestperformanceoncertaintypesoftasksinthe
single concept editing scenario, it struggles to retain its performance for editing
morethan30concepts.However,ourmethodcontinuestoperformeffectivelyin
editing scenarios involving over 300 concepts.
I Limitations on Erasing NSFW Contents
Metric UCE EMCID Mixed
Source Forget ↑ - 0.53 0.53
Source2Dest ↑ - 0.55 0.52
Holdout Delta↑ - -0.11 -0.15
Nudity Erased Rate↑ 0.65 - 0.64
Table 8: OurEMCIDiscomplementarywithUCE.EMCIDandUCEcanbeapplied
to erasing nudity and editing ImageNet concepts simultaneously.
OurmethodinherentlyfaceslimitationswhenitcomestoeliminatingNSFW(Not
Safe for Work) content. For example, visual concepts associated with "nudity"
maybeintertwinedwithawiderangeofphrasesandexpressions.Therefore,the
impact of concept editing by simply modifying the single word "nudity" cannot
be easily generalized to all prompts potentially incurring the generation of “nu-
dity” content,especiallywhenonlyeditingthetextencoder.Wefurtherconduct
anexperimentprovingthatourEMCIDiscomplementarywithUCE[13],which
can erase “nudity” by modifying the cross-attention layers of Stable Diffusion.
A mix of the two methods can simultaneously achieve good performance on 2
tasks, which can not be accomplished with only one of the methods.
The first task is to 50 Arbitrary Concept Editing from ICEB, which will
lead to the corruption of the diffusion model if UCE is applied for the task. But
ourEMCIDcansuccessfullyedittheconceptswithlittledamagetothemodel’s
generation capabilities.
The second task is to erase “nudity” from T2I diffusion models. To measure
thesuccessoferasing“nudity”,wefirstusetheoriginaldiffusionmodeltogener-Editing Massive Concepts in Text-to-Image Diffusion Models 11
ate images conditioned on prompts in I2P [34]. Then we edit the model to erase
“nudity” and again generate images using the same prompts. Then NudeNet [1]
is used to classify images containing undesired nudity contents in pre-edit and
post-editevaluationimages.Andweusethemetricnudityerasurerategivenby
(nˆ −n)/nˆ, where nˆ, n are the numbers of images containing undesired nudity
contents in pre-edit images and post-edit images accordingly. UCE can partly
erase “nudity” while EMCID can not.
We test EMCID on the first task, and UCE on the second task. To mix the
two methods for the two tasks, we first edit the text encoder of Stable Diffusion
with EMCID for the first task. Then we continue to edit the cross-attention
layers with UCE to erase “nudity”. As shown in Tab. 8, the simple composition
of the two methods can achieve good performances on both tasks. The results
prove that our EMCID is complementary with UCE.
J Details for Proposed Benchmark
Class Score Distribution
200
150
100
50
0
0.0 0.2 0.4 0.6 0.8 1.0
Score
Name Score Distribution
300
200
100
0
0.0 0.2 0.4 0.6 0.8 1.0
Score
Fig.17: WedemonstratedevidencethattheSDmodelgenerateserroneousimagesfor
someconcepts.ThescoreofanameistheViT-Bclassificationprobabilityofitsclass.
A class’s score is represented by the highest score of its name.
We aim to collect a large set of diverse and effective text prompts for evalu-
ation. To ensure the diversity of the prompts, we employ ChatGPT [27] to gen-
erate text prompts, describing a scene about a concept from the 1,000 classes
of ImageNet [8], rather than using the template prompts as in previous meth-
ods [6,13,29]. For effectiveness, we use these prompts to generate images with
SableDiffusionv1.4[2]andcalculatetheViT-B[10]classificationprobabilityfor
theconceptitdescribes,asitseffectivenessscore.Wefilteredineffectiveprompts
with low scores. Furthermore, classes whose ChatGPT-generated prompts are
mostlyfilteredareregardsunfamiliartotheT2ImodelandexcludedfromICEB.
At last, 3,300 effective prompts for 666 ImageNet classes have been collected.
We further conducted an experiment to study the generation capabilities of
StableDiffusionforImageNetconcepts.Intheexperiment,foreachclassname,8
sessalC
fo
rebmuN
semaN
fo
rebmuN12 T. Xiong et al.
Current Monarch of the United Kingdom Current Monarch of the United Kingdom
Current Prince of Wales Current Prince of Wales
Fig.18: More results for updating “Current Monarch of the United Kingdom” and
“Current Prince of Wales”. Images in the left 3 columns are generated by the original
Stable Diffusion, and the right 3 columns are generated by the edited model after
updating the concepts.
BeforeErasing Artist Styles AfterErasing Artist Styles
Fig.19: Our approach constantly succeeds in erasing a diverse set of artist styles.
imagesaregeneratedconditionedontheprompt“animageof[name]” andscored
by ViT-B using the classification probability of its class. The score of name is
defined as the average classification score of the generated images. The score of
a class is defined as the highest score of its names. As shown in Fig. 17, Stable
Diffusion cannot generate correct images for some classes and a large number
of class names. This suggests the importance of the task, Concept Rectification,
which can evaluate a concept editing method in terms of the ability to prevent
incorrect and misleading generation.
rodavlaS
odranoeL
ydnA
ílaD
olegnalehciM
icniV
ad
lohraWEditing Massive Concepts in Text-to-Image Diffusion Models 13
References
1. Nudenet:lightweightnuditydetection.https://github.com/notAI-tech/NudeNet
2. Stable Diffusion v1.4. https://huggingface.co/CompVis/stable-diffusion-v-
1-4-original (2022)
3. Stable Diffusion v2.0. https://huggingface.co/stabilityai/stable-
diffusion-2 (2022)
4. Midjourny. https://www.midjourney.com/ (2023)
5. Anderson,J.A.:Asimpleneuralnetworkgeneratinganinteractivememory.Math-
ematical biosciences (1972)
6. Arad,D.,Orgad,H.,Belinkov,Y.:Refact:Updatingtext-to-imagemodelsbyedit-
ing the text encoder. arXiv preprint arXiv:2306.00738 (2023)
7. Cho, J., Zala, A., Bansal, M.: Dall-eval: Probing the reasoning skills and social
biases of text-to-image generation models. In: ICCV (2023)
8. Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:Imagenet:Alarge-scale
hierarchical image database. In: CVPR (2009)
9. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. In:
NeurIPS (2021)
10. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T.,Dehghani,M.,Minderer,M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,Houlsby,N.:
An image is worth 16x16 words: Transformers for image recognition at scale. In:
ICLR (2021)
11. Gandhi, S., Kokkula, S., Chaudhuri, A., Magnani, A., Stanley, T., Ahmadi, B.,
Kandaswamy, V., Ovenc, O., Mannor, S.: Scalable detection of offensive and non-
compliant content / logo in product images. In: WACV (2020)
12. Gandikota, R., Materzyńska, J., Fiotto-Kaufman, J., Bau, D.: Erasing concepts
from diffusion models. In: ICCV (2023)
13. Gandikota,R.,Orgad,H.,Belinkov,Y.,Materzyńska,J.,Bau,D.:Unifiedconcept
editing in diffusion models. In: WACV (2024)
14. Heng, A., Soh, H.: Selective amnesia: A continual learning approach to forgetting
in deep generative models. In: NeurIPS (2023)
15. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
byatwotime-scaleupdateruleconvergetoalocalnashequilibrium.In:NeurIPS
(2017)
16. Ho, J., Saharia, C., Chan, W., Fleet, D.J., Norouzi, M., Salimans, T.: Cascaded
diffusion models for high fidelity image generation. JMLR (2022)
17. Kim,S.,Jung,S.,Kim,B.,Choi,M.,Shin,J.,Lee,J.:Towardssafeself-distillation
of internet-scale text-to-image diffusion models. In: ICML (2023)
18. Kohonen,T.:Correlationmatrixmemories.IEEEtransactionsoncomputers(1972)
19. Kumari,N.,Zhang,B.,Wang,S.Y.,Shechtman,E.,Zhang,R.,Zhu,J.Y.:Ablating
concepts in text-to-image diffusion models. In: CVPR (2023)
20. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training
for unified vision-language understanding and generation. In: ICML (2022)
21. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)
22. Luccioni, S., Akiki, C., Mitchell, M., Jernite, Y.: Stable bias: Evaluating societal
representations in diffusion models. In: NeurIPS (2023)
23. McCloskey, M., Cohen, N.J.: Catastrophic interference in connectionist networks:
Thesequentiallearningproblem.In:Psychologyoflearningandmotivation(1989)14 T. Xiong et al.
24. Meng,K.,Bau,D.,Andonian,A.,Belinkov,Y.:Locatingandeditingfactualasso-
ciations in gpt. In: NeurIPS (2022)
25. Meng,K.,Sharma,A.S.,Andonian,A.J.,Belinkov,Y.,Bau,D.:Mass-editingmem-
ory in a transformer. In: ICLR (2023)
26. Mou, C., Wang, X., Xie, L., Wu, Y., Zhang, J., Qi, Z., Shan, Y., Qie, X.: T2i-
adapter: Learning adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv preprint arXiv:2302.08453 (2023)
27. OpenAI: ChatGPT. https://openai.com/blog/chatgpt (2023)
28. OpenAI: Dall-e-3. https://openai.com/dall-e-3 (2023)
29. Orgad,H.,Kawar,B.,Belinkov,Y.:Editingimplicitassumptionsintext-to-image
diffusion models. In: ICCV (2023)
30. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021)
31. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
(2022)
32. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022)
33. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour,
S.K.S., Gontijo-Lopes, R., Ayan, B.K., Salimans, T., Ho, J., Fleet, D.J., Norouzi,
M.: Photorealistic text-to-image diffusion models with deep language understand-
ing. In: NeurIPS (2022)
34. Schramowski, P., Brack, M., Deiseroth, B., Kersting, K.: Safe latent diffusion:
Mitigating inappropriate degeneration in diffusion models. In: CVPR (2023)
35. Shan, S., Cryan, J., Wenger, E., Zheng, H., Hanocka, R., Zhao, B.Y.: Glaze: Pro-
tectingartistsfromstylemimicrybyText-to-Imagemodels.In:USENIXSecurity
(2023)
36. Somepalli, G., Singla, V., Goldblum, M., Geiping, J., Goldstein, T.: Diffusion art
or digital forgery? investigating data replication in diffusion models. In: CVPR
(2023)
37. Struppek, L., Hintersdorf, D., Kersting, K.: The biased artist: Exploiting cultural
biases via homoglyphs in text-guided image generation models. arXiv preprint
arXiv:2209.08891 (2022)
38. Zhang,E.,Wang,K.,Xu,X.,Wang,Z.,Shi,H.:Forget-me-not:Learningtoforget
in text-to-image diffusion models. arXiv preprint arXiv:2211.08332 (2023)
39. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: CVPR (2018)
40. Zhao,J.,Wang,T.,Yatskar,M.,Ordonez,V.,Chang,K.W.:Genderbiasincoref-
erence resolution: Evaluation and debiasing methods. In: NAACL (2018)