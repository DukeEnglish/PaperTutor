Natural Language as Polices:
Reasoning for Coordinate-Level Embodied Control with LLMs
Yusuke Mikami1, Andrew Melnik2, Jun Miura3, Ville Hautama¨ki4
Abstract—We demonstrate experimental results with LLMs it is crucial to exploit their in-context learning capability to
that address robotics action planning problems. Recently, LLMs address a complicated novel situation and task by utilizing
havebeenappliedinroboticsactionplanning,particularlyusing
knowledge of previously encountered similar tasks.
a code generation approach that converts complex high-level
In the in-context learning process of the recent LLM-based
instructionsintomid-levelpolicycodes.Incontrast,ourapproach
acquires text descriptions of the task and scene objects, then code generation approaches, we suppose there are two primal
formulates action planning through natural language reasoning, limitations. First, code implementation itself lacks high-level
and outputs coordinate level control commands, thus reducing contextual meaning to efficiently describe embodied skills
the necessity for intermediate representation code as policies.
since usually it is symbolized, indirectly connected to the
Our approach is evaluated on a multi-modal prompt simulation
scene, and abstracted. Second, these approaches are limited
benchmark, demonstrating that our prompt engineering exper-
iments with natural language reasoning significantly enhance bypre-definedAPIs.Wehypothesizethatthenaturallanguage
success rates compared to its absence. Furthermore, our ap- descriptionofthewholeplanningprocess,insteadofcode,can
proachillustratesthepotentialfornaturallanguagedescriptions contribute to removing the limitations.
totransferroboticsskillsfromknowntaskstopreviouslyunseen
To overcome the limitations and advance the current state
tasks.
of robotics toward more semantic capability, we introduce a
I. INTRODUCTION Chain-of-Thought (CoT) [35]-based reasoning framework. In
this framework, we initially possess all necessary information
Roboticsactionplanningguidedbyhuman-levelinstruction
astextandgeneratethenaturallanguagereasoningandaction
presents a challenging topic for general robotics applications,
plans without relying on pre-defined APIs. Our goal is to
as it entails the decomposition of high-level instructions into
make everything explicit with natural language to efficiently
low-level executable robotics commands. Conventional ap-
describe embodied skills semantically for LLMs. For prompt-
proaches tend to address these problems in a highly task-
engineering experiments, we used tabletop manipulation tasks
specificandstaticmannerandconsequentlystruggletoachieve
with multimodal prompts (VIMABench [10]).
open-vocabulary object detection, novel task generalization,
and a reduction of the training process in general.
Our Contributions:
Large language models (LLMs) [1], [30] have had a sig-
nificant impact on various applications, not only text gen- • Reasoning with direct interaction of environment: Our
eration tasks but also robotics action planning where LLMs approach enables agents to interact directly with infor-
attempt to interpret human-level instruction or demonstra- mation from the current environment.
tions. Specifically, LLM-based robotics action planning has • Coordinate-Level action prediction: The output of our
focused primarily on code generation approaches (CaP [14], approach consists of coordinates that can be directly
Progprompt [33], Chain of Code [12], SocraticModels [37], executed by the target robot.
and Instruct2Act [6]) which leverage the in-context learning • Teaching robots with natural language: Our approach
capability of LLM to produce code implementations by inte- suggests a way for humans to teach robots to perform
gratingpredefinedAPIsthatinterfacewiththephysicalworld. tasksinamannersimilartohowtheyteachotherhumans.
These studies solve the embodied control problem from an • Newwaytotacklenovelsituation:Ourapproachsuggests
algorithmic perspective since they try to make intermediate a way to improve the transferability of robotics skills
code implementation from high-level instructions. fromaknowntasktoanoveltaskatthenaturallanguage
Recently, LLM-based robotic planning has emphasized context level.
zero-shot scenarios in robotic planning (Kwon et al. [11],
Socratic Models [37]) which have a huge advantage since II. RELATEDWORKS
robotics action planning often encounters novel objects, situa-
One of the key objectives in the field of robotics is to
tions,andtasks.However,weproposethatalthoughLLMscan
develop a system capable of learning new tasks described
address general situations without any in-context examples,
in natural language, using only a handful of demonstration
1,3DepartmentofComputerScienceandEngineeringatToyohashiUniver- examples, and capable of working with an open vocabulary
sityofTechnology,Japan. range of objects, similar to human abilities. Multiple attempts
2BielefeldUniversity,Germany.
were made to advance existing architectures towards this
1,4SchoolofComputingatUniversityofEasternFinland,Finland.
long-term goal. Recent developments in Large Language and
1 Correspondenceto:YusukeMikami<mikami.yusuke.iv@tut.jp>
Preprint.Underreview. Foundation Models [25], [30] allowed robotic architectures to
4202
raM
02
]OR.sc[
1v10831.3042:viXraPrompt
m
e You are an action planner for robot manipulation with multimodal instruction.
yst T Mh ae
k
r eo 2b Do t
a
m cta ion nip
s
u fola llt oe ws io nb gj e ec xt as
m
on
pl
t eh se
..
.table, in 2D space.
S
Large Language Model
n
structio
Put
a
"
"
"
n
I
S
T
D
dh
e
"
xa
r
:
et
p
1
u
de
,
r
"
de
:
o
""
:
tF
""
lo
Y
w
e l
e
l
or w" , into
"
""
I
TS
D
e
h
" xa:
t
p2 ue,
r" e: "" :s "q Bu la ur ee "", Put into
n
I
Generated reason and action
ple Scene " {" " " "pI S T xD oh "e :s" xa 2i: t t 0p 3 u i 0e o, r ," n "e: y """ "::s 8 "q 0bu 0la u }r ee "" ,, a {" " " " "nI S T p xD dh "e o : " xa 3sr: et i 0p t4 u d 0e i, or ," d "e n: yo "" " ": tF :: " 4 "l ,o Y 0w e 0l }e lor w", {" " " " "I S T p xD h "e o :" xa 4s: t i 0p t5 u 0e i, or ," "e n: y "" " ":h :: 4 "e b 0x 0la u }g eo sn p" r, ay",
Y
oning "" p LT u oh r oe p
k
t l ea
fo
s ok rb ri js ee dcp t i o"c , bk
j
ere cd
t
io nb tj he ect
s
a cn end
e
p
.
ut it in
am X eas Y "Lo ou
o
fi kn fd
o
ro pb uje rc pt
l
eID
o
b4" je,
ct, you find object ID 3",
ex R "The task is put ID 5 into 3"
xt
e
In-
cont Reasoning
"
"
"
"T
L
L
To
oh
ho
oe
ek
k
t
t
a
af fo
os sk
kr
r
y
bi is
se
l
ulp
pl
eoi uc
w
o
tk
b
I
Dy
o
jee
b
4cl jl
e
to
i,
nc
w
yt
t
o
o
io
n
u
3b
t
"fij he nc
e
dt
s
a ocn
e
bd
n
je
ep
c.
tu
Y
It
o
D
i ut
3
i
fi
"n
,n
b dl u oe
b
jo eb cj te Ic Dt " 4,
", Natu ar sa Pl oL la icn iegu sage
on
" a
{
c t i "o an cs ti" o:[
n_type": "pick_and_place",
acti " "t ra or tg ae tit o_ nob ":j e 0c ,t_id": 5,
el "from": {
ction
label
" a
{
c t i
"
"
"o
a
t
rn
a
oc
rs
tt
g
ai" o:
e
t[
n
it
o__ noty
b
"p
:j e
0e
c
," t: _" ip di "c :k 4_a ,nd_place",
Low-
lev
}
"
,
t
o"
"
"
"x
y
x
y":"
""
"
{:
::
:
6
25
80
00
00
00 0,
,
el
a " f r "o xm
":
" 3: 0{
0,
}
v "y": 400
w-
le } ",
to": {
Lo " "x y"" :: 82 00 00,
Pick and Place API
n
o
e
solved Instructi Put " " " I S T D h e " xa : tp 1 u e, r" e: " " : h "e r x e a d g " o n " , into y e " " " l lI S T oD h e w " xa : dtp 2 u oe, r t" e ": "" :s "q pu ua rr pe l" e, and Put into
b
o
k t Target Robot
s
Target ta Scene
y
"
{" "
"
"peI S
T
xD
l
oh
"e
lo
:s" xa
8w
i:
t
t
0p 3
u
i
0e
d
o,
r
,"
o
n
"e:
yt
""
""
"::
,s
2
"q 0pu 0ua }rr pe l" e,
and
a
{" "
"
"
"nI S
T
p
xD dh
"e
o :
" xa 3sg:
t
i
0p
r
t4
u
e
0e i,
oer
,"
"e
n
n:
y
"
"
"
"s:V
3:
t"" 0rp,
i
0pu }er "p ,le
{" "
"
"
"I S
T
p
xD h
"e
o
:" xa 5s:
t
i
0p t5
u
0e i,
or
,"
"e
n:
y
""
"
":h
6:
"e 0rx 0ea }dg "o ,n",
Y
X
Fig.1. Overviewofourapproach.Weprovideonedemonstrationasanin-contextexample,andaplanningstepemployingnaturallanguagereasoninginstead
ofconventionalcodeimplementation.WeremovetheCoT(Chain-of-Thouht)reasoningcomponentinthein-contextexampleforourablationstudytocheck
the importance of natural language reasoning. We use low-level API(pick-and-place or sweep) to control the robot arm. We presents specific example of
naturallanguagereasonginTable.V.
make substantial progress toward this objective. We present including long training duration [21], overfitting for specific
recent LLM-based approaches in Table.I. tasks [28], and limited input flexibility. Dataset-search policy
approaches [3], [17], [18], [22] propose zero-shot adaptation
A. Imitation learning and RL for robotics action planning
to provided tasks examples, thus improving in flexibility over
Imitation learning and reinforcement learning (RL) [2] are imitation learning approach, however still having limitations
common frameworks for robotics action planning. A neural fornovel-taskexecution.Incontrast,weleveragethereasoning
network takes information from the environment, and the out- capability of pre-trained LLMs with an explicit planning
put comprises executable actions. These learning frameworks process to tackle the limitations.
acquire robotics skills in their neural network parameters im-
plicitly. These conventional approaches encounter limitationsB. Natural language commands to code scripts with LLMs TABLEI
KEYCOMPONENTSOFRELATEDCODEGENERATIONANDREASONING
Some approaches explored the translation of natural lan- APPROACHES:EMBODIEDGPT[24],SOCRATICMODELS[37],INNER
guage commands into executable code scripts [4], [6], [8], MONOLOGUE[8],STATLER[36],DEMO2CODE[32],CHAINOFCODE[12],
[14], [15], [23], [29], [37] where a set of examples of trans-
PROGPROMPT[29],CHATGPTFORROBOTICS[11],CODEAS
POLICIES[14],INSTRUCT2ACT[6],ANDZERO-SHOTTRAJECTORY[11].
lation between natural language commands and executable OURAPPROACHHASDIFFERENCES,ESPECIALLYINNATURALLANGUAGE
scripts or a description of pre-defined APIs is provided, such REASONINGANDCOORDINATE-LEVELOUTPUT.
that the model can do correct translation for new natural
language commands. These code-based approaches attempt to
In- context example Output Manipulaiton type
s rao tl hv ee rro tb ho at nics aac st ei mon anp tl ia cnn oin ng e.fro Cm haa inn -a ol fg -to hr oit uh gm hi tcp (Cer os Tp )ect [i 3v 4e ], Fin pe ro- t cu en si sng
laN na gt uu ara gl e Code
RlaN en aa sgt ouu nara ig nl e
g Coordinates Code Tabletop Navigation
enhances the reasoning abilities of large language models
EmbodiedGPT ● ● ● ●
(LLMs)bydecomposingcomplextasksintosmallerstepsand
providing examples of intermediate reasoning steps through Socratic Models ● ● ● ●
multiple prompts. Some approaches explored the integration
Inner Monologue ● ● ● ● ● ●
of CoT or reasoning processes into robotics tasks. Statler
[36] offers a state management framework for long-horizon Statler ● ● ● ● ●
planning tasks. Demo2Code [32] has an efficient intermediate
representationbysummarizingdemonstrationstoproducefinal Demo2Code ● ● ● ● ●
actions.Progprompt[29]generatessituatedtaskplansascode
Chain of Code ● ● ● ●
implementation.InnerMonologue[8]focusesonfeedbackand
interaction processes in the reasoning process. Chain of Code Progprompt ● ● ●
[12] has advantages in both algorithmic and semantic capa-
ChatGPT for ● ● ●
bility by using LLM as a code interpreter. These reasoning- Robotics
focused studies leverage LLMs as reasoners for grounding, Code as policies ● ● ●
however, still stick to a basic concept where LLM is used
as a code generator by filling arguments of pre-defined API. Instruct2Act ● ● ●
On the other hand, EmbodiedGPT [24] attempts to efficiently
Zero- Shot ● ● ●
integratetheimitationlearningprocessandtheCoTreasoning Trajectory
processinitstrainingprocess,however,itstillrequiresafine- Ours ● ● ● ●
tuning process with a customized dataset. In contrast to these
conventional approaches, our approach solve robotics action
planning by especially focusing on a semantic perspective
planning in zero-shot situations considering executable pre-
rather than conventional algorithmic without additional fine-
defined actions. Socratic Models [37] leverage the zero-shot
tuning.
capability of LLMs to translate simple actions into code with
in-context examples. Teyun et al.[11] propose an LLM-based
C. LLM-based code generation for multi-modal prompts
code generation approach without any in-context examples,
Jiang et al.[10] introduced VIMA, which can act upon relying on reasoning to address zero-shot tasks. Obviously,
multimodal prompts within the end-to-end imitation learning Zero-shot is a promising approach, however, LLM can handle
approach. Jiachen et al.[13] proposed a pre-train and fine- only general and simple situations and prompts in zero-shot
tune approach for the VIMA model. Huang et al.[6] intro- situations[26].Thereforeweemphasizethatitiscrucialtouse
duced Instruct2Act which utilizes ”Code as policy” [14] to in-contextexamplesofpreviouslyencounteredtasksifthetask
generateexecutableactionsfrommultimodalprompts,drawing is complicated, for instance, VIMABench[10].
inspirationfromVISPROG[5].However,thisapproachsolves
the VIMABench [10] task in a task-specific way since it
E. Open vocabulary object detection
needs code implementation as an in-context example for each
task. Also, this approach has limitations from pre-defined
Usage of open vocabulary object detection models [16],
APIs and may not solve complicated text-image matching
[19] is one way of embracing open vocabulary set of objects
problems with only CLIP [25] based perception. In contrast,
into reasoning about a task [20]. Differences in objects of
ourapproachtriestoachieveaflexibleobjectdetectionprocess
the same type in an environment may be complicated to
by describing objects and reasoning in natural language.
express in natural language, thus integration of images of
intended objects and text into multi-modal task specification
D. Novel Tasks and Generalisation
[31] can provide performance benefits. We try to achieve
Novel tasks have been a crucial problem for robotics. open vocabulary by leveraging a pre-trained huge Vision
BC-Z [9] tackles the zero-shot problem by having a huge Language model (for instance, GPT4-Vision) rather than an
datasetandtaskembeddings.Huangetal.[7]introduceaction object detection model.III. METHODS 8(novel-adj-and-noun) has a similar concept to tasks 6(novel-
adj) and 7(novel-noun).
While some studies focus on the code generation approach
with in-context examples, we explore the decomposition of B. Our approach
high-level tasks and the generation of coordinate-level actions
1) Overview: We introduce a robotics action planning
with only natural language reasoning.
framework to solve multimodal prompts by thinking of ev-
erything in natural language with CoT shown in Fig 1. We
A. Problem formulation
first convert all images from the prompt and scene into a
Our approach solves robotic planning for tabletop manipu- text description and make an action prediction with one in-
lation.Thegoalistomodifythestateofobjectsintheenviron- contextexample.Ourapproachstoresroboticsskillsasnatural
ment to match the configuration described in the instructions. language explicitly.
Jiang et al.[10] introduced the VIMABench framework as 2) Pipeline: We translate the segmented images into text,
an open-source project for evaluating the performance of adhering to the object description format. Subsequently, we
multimodal prompts. It comprises 17 tasks across four levels make action predictions incorporating in-context examples
of generation. with a prompt in Fig.2. The language model generates ac-
1) Interface of planning: This section provides inforam- tion predictions, including reasoning steps and actual actions,
tion about the interface of VIMABench[10] for this work. conformingtotheactionoutputformat.Followingthis,weper-
VIMABench provides two information for planner, which form coordinate mapping, translating output coordinates from
consists of multimodal prompts, which include both text and front-view to top-view. Finally, we execute the actions, which
images depicting a single object and a scene with multiple caneitherinvolvepick-and-placeoperationsorsweepingtasks.
objects. The scene image provided offers both top and front
views, showcasing several objects within the scene.
The output of the planning result must include specific SYSTEM:
You are an action planner for robot manipulation with multimodal instruction.
parameters for the start and end points of the action. These
The robot manipulates objects on the table, in 2D space.
parameters encompass the coordinates in the x and y dimen- Make 2D actions following examples.
Consider height, width, and position of x and y of each objects.
sions, representing a top view for the action execution. They Consider color, shape, pattern.
EXAMPLES may be totally different task, you have to extract robotics skills from given examples to
are utilized for both sweeping and picking actions within a solve novel task.
two-dimensional space framework. Additionally, the rotation SPACIAL informaiton:
Each object description has coordinates which includes x and y axis in 2D space.
of the end-effector must be defined for both the start and X axis: Horizontal axis: left to right: 0 to 2048
Y axis: Vertical axis: bottom to top : 0 to 1024
end points. This parameter facilitates the rotation action of Consider how each object occupies the table in 2D space, how they are positioned each other carefully
to avoid object collision.
objects, although it is not relevant for sweep actions. During
DESCRIPTION of availale actions:
sweepactions,therotationoftheend-effectorisautomatically The robot can make two- type actions as follows.
1: "pick_and_place": pick one object, lift it at position A and place it at position B.
managed.
2: "sweep": sweep one object from place position A to B in linear manner without lifting object.
2) Actions: The target robot has two available actions:
DESCRIPTION of input parameters:
”Pick and place,” which involves picking up an object in PROMPT: prompt for robotic manipulation
{...} is a description of one object.
one location and placing it in another, and ”Sweep,” which frame:[ {}{}] is multiple object descriptions in one frame.
Object in the environment: object list the robot can interact with.
entails moving objects by dragging without lifting them. "x", "y": how each object is positioned in the 2D space
These actions are selectable automatically by the benchmark Description of output parameter:
"inference_steps": inference process to get final action prediction. Make reasons that makes sence to
dependingonthetask,eliminatingtheneedforexplicitaction get final aciton.
"action_plan": action plan, may contain multiple steps. This actions have to make sence as the result of
selection. the inference_steps.
3) Generalization level: VIMABench provides four levels EXAMPLE:
of generalization including placement, novel combination, ################ Prompt is here ################
novel objects, and novel task. Each level evaluates differ- ########## Object in the environment is here ###########
ent zero-shot capabilities. In this work, we focus on only ########## Output action plan is here ###########
placement and novel task generalization. Given that our ap-
INPUT:
proach does not necessitate a massive training dataset, novel
################ Prompt is here ################
combinations, and novel object generalization hold limited
########## Object in the environment is here ###########
significanceforourmethodology.Fornovelgeneralization,we
manually select an example from a similar task for in-context
Fig.2. Thefullpromptwithellipsesindicatingomittedsectionsduetospace
learning. In VIMABench, task 10 (follow-motion) and task
limitations.
13(sweep-without-touching), hold significance for novel task
generalizationsincethesetaskshavenovelconceptsandwords 3) Object description format: Our approach converts any
which is not available in other generalizations. Conversely, images into a unified format for each object. This format in-
tasks 8 and 14 are deemed less crucial from the perspective cludesdescriptionsoftheshape[27]andtextureoftheobject.
of novel tasks, as they share similarities with tasks already Additionally, it includes a section named ”position,” which
present in the other three generalizations. For instance, task signifiesthatitcontainsspecialinformationregardingthefrontview of the object. Within this section, the coordinates for the IV. DISCUSSION
center of the object are provided.
We hypothesized the natural language reasoning process
4) Action output format: The output format comprises a
of whole robotics action planning is critical to removing the
continuous combination of x and y coordinates, divided into
limitation of existing LLM-based code generation approach
two main components. The first is the inference process,
and efficiently describing robotics skills for LLMs. Overall,
whichdelineatesthereasoningstepsleadingtothefinalaction
our quantitative results demonstrate that the natural language
prediction. The second component is the action plan, which
reasoning process has a critical role in performing a better
outlines the steps necessary to accomplish tasks. Within the
success rate, especially for novel-task generalization. On the
action plan, multiple plans may exist, each defined by the
other hand, our approach does not perform better than other
followingparameters:action type(either”pick and place”or
existing approaches in most tasks.
”sweep”), target object (the ID of the object being targeted),
Importance of Chain of Thought reasoning: According to
rotation (specifying the degree of rotation required for the
Table. II and III, Chain-of-Thought reasoning is crucial for
target object), from (numerical values indicating the starting
almost all tasks as our hypothesis mentions. The success
position of interaction), and to (indicating the ending position
rate improves from 32% to 59% in placement generalization
of interaction).
(Table.II) with CoT reasoning. This result demonstrates that
Thecoordinatesintheoutputarefromthefrontview,thenit
a natural language reasoning step is a critical component in
hastobeconvertedtothetopview.Weuseageneralmapping
producinglow-levelactionprediction,especiallyfornoveltask
approach without any training process.
generalization (Table.III).
5) How to make reasons for each task manually: We
Skill extraction for novel task: Our results suggest that
generate step-by-step solutions expressed in natural language
the explicitness of whole robotics action planning can have
explanations for each target task independently as human-to-
the potential to transfer robotics skills from known to novel
humanteachinghappens.TableVillustratesspecificexamples.
tasks with LLMs. To tackle the novel-task generalization of
Our focus primarily lies on achieving a high success rate
VIMABench, it is required to extract essential skills from in-
through natural-sounding reasoning; hence, we do not overly
context examples. Our ablation study suggests that the natural
emphasizethequalityofthereasoningprocess.Typically,this
language reasoning process efficiently contributes to novel
processinvolvesthefollowingcomponents:definingthetarget
tasks. Especially for the follow-motion task, GPT4 performs
task to facilitate its decomposition, teaching object matching
90% in extracting skills from the rearrange-and-restore task
between the prompt and the scene, incorporating additional
to solve the follow-motion task as described in Table.IV.
reasoning steps for complex tasks, and providing a specific
On the other hand, our approach still struggles to solve task
action result as the final conclusion.
13 (sweep-without-touching) which requires understanding a
6) LLMs: WeemployGPT-3.5asourprimaryexperimental
novel concept of collision avoidance.
framework,supplementedbyGPT-4foradditionalexperimen-
NumericalvaluesforLLMs:WedemonstratethatLLMcan
tation. Utilizing their respective assistant APIs facilitates the
effectively handle numerical values for most tasks. Although
efficient provision of system prompts and inputs.
both the input and output of our approach include numerical
7) Limitations: We remove task 9 ”twist” and task 8
values, LLM successfully generates action plans containing
”novel adj and noun”fromourexperimentduetothelimita-
coordinates. However, our approach still struggles to solve
tionofourapproach.Forinstance,ourapproachcannotdetect
task 11, a follow-order task that requires how objects are
an exact rotation of objects for task 9(twist) task.
stackedbasedontheircoordinates.Additionally,Weshowthat
LLM has spatial understanding when the coordinates of each
C. Ablation study
object are injected as an input. Our results of task 12(sweep-
Our approach focuses on the importance of reasoning with without-exceeding), suggest that LLM can handle numerical
natural language. Therefore, the ablation study is critical. As valuesofactionsthatcannotbedescribedwithobjectID.CaP
described in Fig.1, we remove the reasoning part from our [14]mentionedthatcode-basedreasoningoutperformsnatural
framework. In Table.II, the ”Without Chain of Thought” (w/o language CoT for spatial-geometric reasoning, however, our
CoT)conditionindicatesthattheinputoftheLanguageModel study provide novel grounding approach for LLMs not only
(LLM) lacks a manual Chain of Thought (CoT) example, yet for geometric reasoning but also planning itself.
the output of the LLM includes CoT reasoning steps. This
Multi-steps action prediction: Task 5 (Table.II) suggests
condition operates as a zero-shot reasoning scenario.
that the CoT reasoning process can support long-step action
prediction effectively. Tasks 4 and 5 (Table.II) require multi-
D. Result
stepactionswherethetargetobject’slocationchangesineach
Table.IIandIIIshowthesuccessrateasatablefordifferent step.Sinceourapproachdoesnotpredictactionsinanautore-
generalizations. Each of our models and tasks undergoes at gressivemanner,allactionsmustbepredictedsimultaneously.
least 30 attempts. Table.IV shows an additional experiment Open problems: We have identified several challenges in
with GPT4 only for task 10, the follow-motion task. This our current approach. Firstly, there is the limited capability
additional experiment has 10 attempts only for our approach. of text description capability of objects. Extracting nuancedTABLEII
PLACEMENTGENERALIZATION:SUCCESSRATECOMPARISONOFOURAPPROACHWITHEXISTINGAPPROACHESTHROUGHABLATIONSTUDIES.THE
BOLDNUMBERSINDICATETHEAVERAGEVALUE.WECOMPAREOURAPPROACHWITHVIMA(200M)[10],ANDINSTRUCT2ACT[6].OURAPPROACH
UTILIZESTHEMODELGPT-3.5-TURBO-1106FROMOPENAIANDISDESCRIBEDWITHABLATIONSTUDIES(WITHCOTANDWITHOUTCOTINPUT).
”W/OCOT”INDICATESANABLATIONSTUDYASDESCRIBEDINFIG.1.THE”ONE-SHOT-EXAMPLE-TASK”COLUMNINDICATESWHICHTASKISUSEDAS
ANIN-CONTEXTEXAMPLEFOROURAPPROACH.EACHOFOURMODELSANDTASKSUNDERGOESATLEAST30ATTEMPTS.WEREMOVETASK9
(”TWIST”)DUETOTHELIMITATIONOFOURAPPROACH.
TaskNum Task One-shotexampleforOurs VIMA200M Instruct2Act Ours(w/oCoT) Ours
1 visual manipulation visual manipulation 100 91 93 100
2 scene understanding scene understanding 100 81 60 67
3 rotate rotate 100 98 93 93
4 rearrange rearrange 100 79 52 73
5 rearrange then restore rearrange then restore 57 72 25 73
6 novel adj novel adj 100 82 13 43
7 novel noun novel noun 100 88 8 80
11 follow order follow order 77 72 0 0
12 sweep without exceeding sweep without exceeding 93 68 17 47
15 same shape same shape 97 78 10 80
16 manipulate old neighbor manipulate old neighbor 77 64 8 20
17 pick in order then restore pick in order then restore 43 85 10 30
87 80 32 59
TABLEIII
NOVELTASKGENERALIZATION:SUCCESSRATECOMPARISONOFOURAPPROACHWITHEXISTINGAPPROACHESTHROUGHABLATIONSTUDIES.THE
BOLDNUMBERSINDICATETHEAVERAGEVALUE.WECOMPAREOURAPPROACHWITHVIMA(200M)[10],ANDINSTRUCT2ACT[6].OURAPPROACH
UTILIZESTHEMODELGPT-3.5-TURBO-1106FROMOPENAIANDISDESCRIBEDWITHABLATIONSTUDIES(WITHCOTANDWITHOUTCOTINPUT).
”W/OCOT”INDICATESANABLATIONSTUDYASDESCRIBEDINFIG.1.THE”ONE-SHOT-EXAMPLE-TASK”COLUMNINDICATESWHICHTASKISUSEDAS
ANIN-CONTEXTEXAMPLEFOROURAPPROACH.EACHOFOURMODELSANDTASKSUNDERGOESATLEAST30ATTEMPTS.WEREMOVETASK8
(”NOVEL ADJ AND NOUN”)DUETOTHELIMITATIONOFOURAPPROACH.
Task Num Task One-shot example for Ours VIMA 200M Instruct2Act Ours (w/o CoT) Ours
10 follow motion rearrange then restore 0 35 0 12
13 sweep without touching sweep without exceeding 0 0 0 3
14 same texture same shape 95 80 3 71
32 38 1 29
TABLEIV
NOVELTASKGENERALIZATION:SUCCESSRATECOMPARISONFORADDITIONALEXPERIMENTWITHGPT4ONLYFORTASK10,FOLLOW-MOTIONTASK.
Task Num Task One-shot example for Ours VIMA 200M Instruct2Act Ours (GPT3.5) Ours (GPT4)
10 follow motion rearrange then restore 0 35 12 90
information such as object height, rotation, or darkness solely existing approaches across various tasks, however, it shows
using a Vision-Language model, rather than a task-specific considerable potential. Importantly, our approach underscores
approach, is particularly difficult. Secondly, there’s the mat- the capability to tackle novel tasks with known skill sets. In
ter of feedback. The current Language-Logic Model (LLM) futureendeavors,weenvisionapplyingourapproachtodiverse
operates a robot within a closed loop, lacking the capacity to tasks and situaitons leveraging a flexible reasoning capability
integratefeedbackfromitsperformance.Thirdly,ourapproach of our approach, and investigating novel task generalization.
is limited in its ability to generate complex actions, currently
APPENDIX
only capable of producing tabletop actions. Fourthly, our
approach focuses on semantic capability for planning prob- Table.V shows a natural language reasoning we manually
lems, whereas robotic action planning typically necessitates made for in-context examples.
algorithmic capability as well.
REFERENCES
V. CONCLUSION
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
Robotics action planning presents a challenging topic due Akkaya,FlorenciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,
to issues such as limited pre-defined skills, and novel task Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv
preprintarXiv:2303.08774,2023.
generalizations. We introduce a novel concept wherein plan-
[2] Nicolas Bach, Andrew Melnik, Malte Schilling, Timo Korthals, and
ning occurs solely within a natural language framework, HelgeRitter. Learntomovethroughacombinationofpolicygradient
conferring advantages over conventional LLM-based code- algorithms: Ddpg, d4pg, and td3. In Machine Learning, Optimization,
andDataScience:6thInternationalConference,LOD2020,Siena,Italy,
generation methodologies. Our quantitative results demon-
July 19–23, 2020, Revised Selected Papers, Part II 6, pages 631–644.
stratethatourapproachdoesnotconsistentlyoutperformother Springer,2020.[3] Shivansh Beohar and Andrew Melnik. Planning with rl and episodic- [24] YaoMu,QinglongZhang,MengkangHu,WenhaiWang,MingyuDing,
memorybehavioralpriors. arXivpreprintarXiv:2207.01845,2022. JunJin,BinWang,JifengDai,YuQiao,andPingLuo. Embodiedgpt:
[4] Yaran Chen, Wenbo Cui, Yuanwen Chen, Mining Tan, Xinyao Zhang, Vision-languagepre-trainingviaembodiedchainofthought. Advances
DongbinZhao,andHeWang. Robogpt:anintelligentagentofmaking inNeuralInformationProcessingSystems,36,2024.
embodiedlong-termdecisionsfordailyinstructiontasks.arXivpreprint [25] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,Gabriel
arXiv:2311.15649,2023. Goh,SandhiniAgarwal,etal. Learningtransferablevisualmodelsfrom
[5] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Com- natural language supervision. In International conference on machine
positional visual reasoning without training. In Proceedings of the learning,pages8748–8763.PMLR,2021.
IEEE/CVF Conference on Computer Vision and Pattern Recognition, [26] Krishan Rana, Andrew Melnik, and Niko Su¨nderhauf. Contrastive
pages14953–14962,2023. language,action,andstatepre-trainingforrobotlearning,2023.
[6] Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, [27] Markus Rothgaenger, Andrew Melnik, and Helge Ritter. Shape com-
and Hongsheng Li. Instruct2act: Mapping multi-modality instruc- plexity estimation using vae. In Intelligent Systems Conference, pages
tions to robotic actions with large language model. arXiv preprint 35–45.Springer,2023.
arXiv:2305.11176,2023. [28] MalteSchillingandAndrewMelnik. Anapproachtohierarchicaldeep
[7] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. reinforcementlearningforadecentralizedwalkingcontrolarchitecture.
Language models as zero-shot planners: Extracting actionable knowl- In Biologically Inspired Cognitive Architectures 2018: Proceedings of
edge for embodied agents. In International Conference on Machine theNinthAnnualMeetingoftheBICASociety,pages272–282.Springer,
Learning,pages9118–9147.PMLR,2022. 2019.
[8] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete [29] IshikaSingh,ValtsBlukis,ArsalanMousavian,AnkitGoyal,DanfeiXu,
Florence,AndyZeng,JonathanTompson,IgorMordatch,YevgenCheb- Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg.
otar,etal.Innermonologue:Embodiedreasoningthroughplanningwith Progprompt:Generatingsituatedrobottaskplansusinglargelanguage
languagemodels. arXivpreprintarXiv:2207.05608,2022. models,2022.
[9] EricJang,AlexIrpan,MohiKhansari,DanielKappler,FrederikEbert, [30] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-
Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task AnneLachaux,Timothe´eLacroix,BaptisteRozie`re,NamanGoyal,Eric
generalizationwithroboticimitationlearning. InConferenceonRobot Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
Learning,pages991–1002.PMLR,2022. languagemodels. arXivpreprintarXiv:2302.13971,2023.
[10] YunfanJiang,AgrimGupta,ZichenZhang,GuanzhiWang,Yongqiang [31] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami,
Dou,YanjunChen,LiFei-Fei,AnimaAnandkumar,YukeZhu,andLinxi OriolVinyals,andFelixHill.Multimodalfew-shotlearningwithfrozen
Fan.Vima:Generalrobotmanipulationwithmultimodalprompts,2023. languagemodels,2021.
[11] Teyun Kwon, Norman Di Palo, and Edward Johns. Language models [32] Huaxiaoyue Wang, Gonzalo Gonzalez-Pumariega, Yash Sharma, and
aszero-shottrajectorygenerators,2023. Sanjiban Choudhury. Demo2code: From summarizing demonstrations
[12] ChengshuLi,JackyLiang,AndyZeng,XinyunChen,KarolHausman, tosynthesizingcodeviaextendedchain-of-thought,2023.
Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. [33] Yen-JenWang,BikeZhang,JianyuChen,andKoushilSreenath.Prompt
Chain of code: Reasoning with a language model-augmented code arobottowalkwithlargelanguagemodels,2023.
emulator,2023. [34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
[13] Jiachen Li, Qiaozi Gao, Michael Johnston, Xiaofeng Gao, Xuehai He, Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought
Suhaila Shakiah, et al. Mastering robot manipulation with multimodal promptingelicitsreasoninginlargelanguagemodels,2023.
promptsthroughpretrainingandmulti-taskfine-tuning,2023. [35] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,
[14] JackyLiang,WenlongHuang,FeiXia,PengXu,KarolHausman,Brian Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting
Ichter,PeteFlorence,andAndyZeng.Codeaspolicies:Languagemodel elicits reasoning in large language models. Advances in neural infor-
programsforembodiedcontrol.In2023IEEEInternationalConference mationprocessingsystems,35:24824–24837,2022.
onRoboticsandAutomation(ICRA),pages9493–9500.IEEE,2023. [36] Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong
[15] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, and
Jeannette Bohg. Text2motion: from natural language instructions to Matthew R Walter. Statler: State-maintaining language models for
feasibleplans. AutonomousRobots,47(8):1345–1365,November2023. embodiedreasoning. arXivpreprintarXiv:2306.17840,2023.
[16] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie [37] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski,
Yang,ChunyuanLi,JianweiYang,HangSu,JunZhu,etal. Grounding AdrianWong,StefanWelker,FedericoTombari,AveekPurohit,Michael
dino: Marrying dino with grounded pre-training for open-set object Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot
detection. arXivpreprintarXiv:2303.05499,2023. multimodalreasoningwithlanguage. arXivpreprintarXiv:2204.00598,
[17] FedercoMalato,FlorianLeopold,AndrewMelnik,andVilleHautamaki. 2022.
Zero-shot imitation policy via search in demonstration dataset. arXiv
preprintarXiv:2401.16398,2024.
[18] FedericoMalato,FlorianLeopold,AmoghRaut,VilleHautama¨ki,and
Andrew Melnik. Behavioral cloning via search in video pretraining
latentspace. arXivpreprintarXiv:2212.13326,2022.
[19] Neil Houlsby Matthias Minderer, Alexey Gritsenko. Scaling open-
vocabularyobjectdetection. NeurIPS,2023.
[20] AndrewMelnik,MichaelBu¨ttner,LeonHarz,LyonBrown,GoraChand
Nandi, Arjun PS, Gaurav Kumar Yadav, Rahul Kala, and Robert
Haschke. Uniteam: Open vocabulary mobile manipulation challenge.
arXivpreprintarXiv:2312.08611,2023.
[21] Andrew Melnik, Luca Lach, Matthias Plappert, Timo Korthals, Robert
Haschke,andHelgeRitter. Usingtactilesensingtoimprovethesample
efficiency and performance of deep deterministic policy gradients for
simulated in-hand manipulation tasks. Frontiers in Robotics and AI,
8:538773,2021.
[22] Stephanie Milani, Anssi Kanervisto, Karolis Ramanauskas, Sander
Schulhoff, Brandon Houghton, Sharada Mohanty, Byron Galbraith,
Ke Chen, Yan Song, Tianze Zhou, et al. Towards solving fuzzy
tasks with human feedback: A retrospective of the minerl basalt 2022
competition. arXivpreprintarXiv:2303.13512,2023.
[23] Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu,
Chongjian Ge, and et al. Robocodex: Multimodal code generation for
roboticbehaviorsynthesis,2024.TABLEV
SPECIFICEXAMPLEOFNATURALLANGUAGEREASONINGWEMANUALLYMADEFORIN-CONTEXTLEARNING.WEDONOTHAVEANYSPECIFICFORMAT
TOPRODUCETHESEREASONINGSANDWETRYTOMAKENATURALREASONINGINAHUMANWAY.
Task Num Task Natural Language Reasoning for in- context example
1 visual_manipu "Find an object that has a similar property as Object 1 in the environment.", "Object 4 has Long Rectangle and this doesn't match the requirement,
lation hoewever, it has similar color and texture. There are no other objects that is more matched to object1. Then found 4 to pick up.", "Find an object that has
a similar property as Object 2 in the environment.", "Object 3 has similar color and similar shape, not exactly same. but its ok because image description
cantains inaccurate information. Then found 3, "As an output, the robot picks up 4 and puts it on 3."
2 scene_underst "The task is red swirl object in frame0 into purple object.", "First, have to find red swirl object in frame0 and look for the founded object and purple
anding object in the environment.", "Find a red swirl object object in frame0. ", "Object id 1 has red color. but it doesn't have swirl texture. but this is ok. Found
object 1. ", "Look for objects that has these property from object 1 in environment which has stripe.", "Object 4 is similar to object 1 because it has red
and white color and stripe.", "Object 4 should be picked up.", "Next, look for purpole object from prompt in the environment.", "Find object id 3 in the
environment, a purple solid object. Object 5 is also possible, but it has dotted texture.object 3 is more suitable for purple object in the prompt. Then found
object 3", "As an output, the robot picks up 4 and puts it on 3."
3 rotate "The task is to rotate the green object (object1) 30 degrees.", "First, let's find a green object (object1) in the environment.", "Found object 2 in
the environment, this is an object which should be rotated.", "The robot should take pick_and_place action and interaction point is center of the
object.", "Chose center_position and same position for pick and place action."
4 rearrange "The task is to arrange objects in same position as descrived in frame0", "frame0 contains several images of objects in one scence and information of
object position", "The task is rearrange objects like frame0 using information of object position", "Look for object id 1 in the current environment.",
"Object 3 is similar to 1. Object 4 is similar to 2.", "The task is arrange objects in the environment like frame0 then first action is to pick up from position 3
and put it at position 1.", "Second action is to pick up from positon 4 and put it at position 2"
5 rearrange_the "The task is to rearrange objects to match the setup described in frame0, and then restore them.", "frame0 contains descriptions of objects in
n_restore specific positions.", "The goal is to first rearrange the objects in the environment to match frame0, then restore them to their original positions.",
"In the current environment, object at object 3 is similar to the one described at oboject 1 in frame0, and object at position 4 matches the one at object 2 in
frame0.", "The first action is to pick up object from position 3 and place it at position of object 1.", "The second action is to pick up object from
position 4 and place it at position of object 2.", "After rearranging, the objects need to be restored to their original positions, which means picking from
current position of object 1 and placing back at position 3, and picking from current position of object 2 and placing back at position 4."
6 novel_adj "Focusing on description of images, the definition of daxer is thinness", "Find the thinness vertion of object 7 in the environment", "You
find object 10", "Search for object 8 in the environment", "You find object 9.", "The task is put object 10 into onject 9."
7 novel_noun "The task is to put a blicket into a dax.", "Have to find corresponding object to blicket and dax", "blicket is defined in a description of object 2, heart,
grey and granite.", "Look for an object which is similar to object 2, however, there is no such oject.", "In this case, you hava to find which is not totally
qualified as blicket and exclude them and chose remains", "object 2 and 5 is not defenetly qualified because pan and letter M cannot be same as heart
shape of object 2", "object4, shape round can be similar to heart object. Then chose 4", "Then, you found object 4 as blicket", "Next, you have to find
similar object to object 1 in the environment as dax", "You found object 3 in the environment which has pan shape.", "As a result, the robot pick 4 and
put it on 3."
11 follow_order "The task is to move objects following multiple frames. ", "Each frames are captured from front view, then each coordinates shows how they
are stacked", "green object is object 10 in the environment", "red and white object is object 12 in the environment", "rainbow object is
object 11 in the environment", "Check carefly coordinates to find out how each object is stacked in each frames, same x coordinate means they are
stacked", "In the frame1, green object is on red and white object", "First, put the green object on the red and white object, considering thier
coodinates in frame0", "In the frame2, green object is on rainbow object", "Second put the green object on the reinbow object, considering
thier coodinates in frame0", "Then, the final action output is put 10 on 12 and put 10 on 11"
12 sweep_withou "The task is sweep blue and yellow polka dot object into red and blue object without exceeding yellow and blue object.", "When moving an
t_exceeding object with a sweep motion, start from a point with a little margin in the opposite direction of the movement.", "First, find blue and yellow polka dot
object in the environment.", "Found 6, 7, and 8. They matched the description of object 1. but we need sweep only two, so we ignore 8.",
"Second, find object 2. Found object 4.", "Third, find object 3. Found object 5.", "Then you are done matching process.", "Next step is
make action plan.", "The task is sweep object 6,7,8 into object 2 without exceeding object 3.", "Acoording to thier coordinates, the object 5 is
already in the object 4. So this means if we put object 6, 7 right under object 5, then 6,7,and 8 are in onject 4. The task is solved.", "This means object
6,7 should be right under object 3, however, object 6,7 cannot be touched with object 5.", "Also, the task requires sweep action, so the action
parameter in the output should be "sweep".", "Focus on object 6. The robot should sweep from bottom to top but it has to stop below the object 5.",
"Focus on object 7. The robot should sweep from bottom right to top left but it has to stop below the object 5. ", "This task can be done with
only one- step sweep motion because there is no obstacles."
15 same_shape "The task is to find objects in the environment with a profile similar to object 1", "Found object 2 3 4 which has similar profile, this doesn't has to be
same shape, have to be similar word", "Shape A and shape B may have similar shape, you have to consider many posibilities", "Object 2 is a object
where other similar objects should be placed at, becasue it is not block which also means this should not be moved and "it" in the prompt means object 2",
"It doesn't have to be coantainer or frame to be fixed", "Look for objects should be picked, object 3 and 4 which is block,block means square shape as
object 1, They have different color but it doesn't matter in this task because it have to have one common property at least.", "Found 3 and 4 as object that
should be picked.", "Pentagon should not be picked because it has apparrently different shape. But frame and square has common concept.", "As a
result, the robot should pick up 3,4 and put them in 2."
16 manipulate_ol "The task has two steps. First, pick object similar to object 1 and put it in an object similar to object 2. Second, pick object in east side of object which
d_neighbor is similar to object 1 and put it in object similar to object 2.", "Find object which is similar to object 1, from the environment. Object 4 is possibility
because it has green and stripe.", "Find object which is similar to object 2, from the environment. Object 9 is possibility because it has square shape
and .", "The first step is pick object 4 and put it in object 2.", "Next, find an object in east side of object which is similar to object 1", "East
side means right side and plus x axis direction in this simulated environment", "There is a object 4 similar to object 1. In the east side of the object 4,
there is a object 5 based on thier "center_position"", "So the second step is pick 5 and put it in object 9.", "As an output, pick 4 and put it in 9,
pick 5 and put it in 9"
17 pick_in_order_ "The task is put object A into B then C and finally put A into its original position.", "The task is put object 1 into 2, then put object 1 into 3, then put
then_restore object 1 into its original container.", "Then find object 1 in the environment. Found object 4.", "Then find object 2 in the environment. Found object
6.", "Then find object 3 in the environment. Found object 7.", "Then find its original container in the environment. Considering the current
coordinates of object 4, object 4 is placed on 5. Then the original container is object 5.", "All information which is nesassary to achive the task is
corrected.", "The task is put object 4 into 6, then put object 4 into 7, then put object 4 into 5"