On Pretraining Data Diversity
for Self-Supervised Learning
Hasan Abed Al Kader Hammoud1⋆, Tuhin Das2⋆, Fabio Pizzati2⋆,
Philip Torr2, Adel Bibi2, and Bernard Ghanem1
1 KAUST, Saudi Arabia
2 University of Oxford, UK
Abstract. Weexploretheimpactoftrainingwithmorediversedatasets,
characterized by the number of unique samples, on the performance of
self-supervised learning (SSL) under a fixed computational budget. Our
findingsconsistentlydemonstratethatincreasingpretrainingdatadiver-
sity enhances SSL performance, albeit only when the distribution dis-
tance to the downstream data is minimal. Notably, even with an excep-
tionally large pretraining data diversity achieved through methods like
web crawling or diffusion-generated data, among other ways, the dis-
tribution shift remains a challenge. Our experiments are comprehensive
withsevenSSLmethodsusinglarge-scaledatasetssuchasImageNetand
YFCC100Mamountingtoover200GPUdays.Codeandtrainedmodels
willbeavailableathttps://github.com/hammoudhasan/DiversitySSL.
Keywords: self-supervised learning · distribution shift · data diversity
1 Introduction
Self-supervised learning (SSL) has recently emerged as a new paradigm to pre-
train large vision models at scale [25,26,46]. Leveraging the ability to learn
from unlabelled data, pretraining on millions—or even billions [26,52]—of im-
ages turned from an unachievable goal to a common practice. This exposure to
extremely diverse datasets, i.e., composed of a remarkable number of unique
samples, granted impressive performance and unprecedented generalization ca-
pabilities to a growing number of vision models [46]. Large-scale datasets, in
conjunction with substantial computational resources, have been the key driv-
ing forces for the success of SSL-based approaches. For instance, SEER [25],
pretrained for approximately 11 GPU years on a billion images, exemplifies
the massive computation and data resources employed for these models. It has
become the implicit norm that increasing computation and data is beneficial,
without any detailed analysis of how they separately impact SSL effectiveness.
In particular, it is not clear to what extent large datasets are responsible for
⋆ Equal contribution. Correspondence: hasanabedalkader.hammoud@kaust.edu.sa or
fabio@robots.ox.ac.uk
4202
raM
02
]VC.sc[
1v80831.3042:viXra2 Hammoud et al.
(cid:54)(cid:72)(cid:79)(cid:73)(cid:16)(cid:54)(cid:88)(cid:83)(cid:72)(cid:85)(cid:89)(cid:76)(cid:86)(cid:72)(cid:71)(cid:3)(cid:47)(cid:72)(cid:68)(cid:85)(cid:81)(cid:76)(cid:81)(cid:74)
(cid:20)(cid:48)(cid:3)(cid:54)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86) (cid:39)(cid:82)(cid:90)(cid:81)(cid:86)(cid:87)(cid:85)(cid:72)(cid:68)(cid:80)(cid:3)(cid:55)(cid:68)(cid:86)(cid:78)
(cid:51)(cid:85)(cid:72)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81) (cid:55)(cid:85)(cid:68)(cid:81)(cid:86)(cid:73)(cid:72)(cid:85)
(cid:47)(cid:82)(cid:90)(cid:3)(cid:39)(cid:68)(cid:87)(cid:68)(cid:3)(cid:39)(cid:76)(cid:89)(cid:72)(cid:85)(cid:86)(cid:76)(cid:87)(cid:92)
(cid:41)(cid:76)(cid:91)(cid:72)(cid:71)(cid:3)(cid:37)(cid:88)(cid:71)(cid:74)(cid:72)(cid:87)
(cid:39)(cid:82)(cid:80)(cid:68)(cid:76)(cid:81)(cid:29) (cid:54)(cid:80)(cid:68)(cid:79)(cid:79) (cid:39)(cid:82)(cid:80)(cid:68)(cid:76)(cid:81)(cid:29)
(cid:58)(cid:76)(cid:79)(cid:71)(cid:3)(cid:36)(cid:81)(cid:76)(cid:80)(cid:68)(cid:79)(cid:86) (cid:39)(cid:76)(cid:86)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:54)(cid:75)(cid:76)(cid:73)(cid:87) (cid:51)(cid:72)(cid:87)(cid:86)
(cid:20)(cid:19)(cid:19)(cid:48)(cid:3)(cid:54)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)
(cid:54)(cid:72)(cid:79)(cid:73)(cid:16)(cid:54)(cid:88)(cid:83)(cid:72)(cid:85)(cid:89)(cid:76)(cid:86)(cid:72)(cid:71)(cid:3)(cid:47)(cid:72)(cid:68)(cid:85)(cid:81)(cid:76)(cid:81)(cid:74)
(cid:39)(cid:82)(cid:90)(cid:81)(cid:86)(cid:87)(cid:85)(cid:72)(cid:68)(cid:80)(cid:3)(cid:55)(cid:68)(cid:86)(cid:78)
(cid:51)(cid:85)(cid:72)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81) (cid:55)(cid:85)(cid:68)(cid:81)(cid:86)(cid:73)(cid:72)(cid:85)
(cid:41)(cid:76)(cid:91)(cid:72)(cid:71)(cid:3)(cid:37)(cid:88)(cid:71)(cid:74)(cid:72)(cid:87)
(cid:43)(cid:76)(cid:74)(cid:75)(cid:3)(cid:39)(cid:68)(cid:87)(cid:68)(cid:3)(cid:39)(cid:76)(cid:89)(cid:72)(cid:85)(cid:86)(cid:76)(cid:87)(cid:92)
(cid:39)(cid:82)(cid:80)(cid:68)(cid:76)(cid:81)(cid:29) (cid:37)(cid:76)(cid:74) (cid:39)(cid:82)(cid:80)(cid:68)(cid:76)(cid:81)(cid:29)
(cid:50)(cid:88)(cid:87)(cid:71)(cid:82)(cid:82)(cid:85)(cid:3)(cid:54)(cid:70)(cid:72)(cid:81)(cid:72)(cid:86) (cid:39)(cid:76)(cid:86)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:54)(cid:75)(cid:76)(cid:73)(cid:87) (cid:51)(cid:72)(cid:87)(cid:86)
Fig.1: Impact of Diversity on Pretraining: Self-supervised learning (SSL) can
be used to pretrain vision models on smaller datasets closely aligned to downstream
task data, e.g., pets classification, hence with a small distribution shift (top, wild
animals pretraining). Conversely, we could pretrain on an extensively varied dataset,
with wide distribution differences (outdoor scenes, bottom). We demistify the role of
pretraining diversity in SSL under a fixed computational budget, and highlight its
effects in relationship to the distribution shift.
the impressive generalization capabilities of SSL models. Indeed, consider the
example in Figure 1. Assuming a fixed monetary budget, in the form of com-
putational expenses, for pretraining a vision model under the SSL paradigm,
does iterating over a large set of images work best given a downstream task,
or it is better to train repeatedly on a smaller set of samples visually close to
theonesofthedownstream?Inthiscontext,wefacetheever-lastingproblemof
distribution shift on vision models [58]. Without a proper understanding of the
impactofdata,therecouldbeawrongallocationofeffortsinincreasingdataor
computation, leading to inefficiencies in the deployment process of SSL models.
In this paper, we study the role of the diversity of the SSL pretraining data,
under a fixed computational budget, when the pretraining data matches the
distributionofthedownstreamdata,aswellaswhentheydiffer.Ourexperiments
span various SSL methods [4,8–10,14,28,65] and multiple datasets [5,18,37–39,
43,47,54,56,69],andaretestedunderseveralcomputationalbudgetsamounting
to a total of 200 GPU days. We summarize our contributions below:
1. We show that SSL pretraining strategies are currently data-inefficient in
compensatingfordistributionshifts.Undernormalizedcomputationalcosts,
we verify that pretraining on large datasets with high diversity cannot out-
perform the pretraining on datasets with limited diversity, but with a lower
distribution shift with respect to the downstream tasks.Pretaining Data Diversity SSL 3
2. Weconcludethatthereisawidemarginforimprovementintheperformance
of current SSL methods on out-of-distribution classes, and we propose in-
sights for a fair evaluation. This shows the need for future research on new
SSL techniques leveraging data diversity better, to improve generalization
capabilities beyond the training data distribution.
3. We propose a novel strategy for a computationally-normalized evaluation of
SSLandacarefullydesignedsetofexperimentsfocusingonpretrainingdata
diversity, enabling us to draw our novel conclusions.
Ultimately,ourworkprovidesacomprehensiveanalysis,fromexperimentsworth
200 GPU days, on the interaction between data diversity and computational
resources, and their impact on the performance of SSL models, with an aim to
improve pretraining practices. Now, we will analyze the relevant literature.
2 Related Work
Self-SupervisedLearning Earlyworkinself-supervisedlearningusedsimple
pretext tasks, such as relative patch prediction [19,20], image colorization [67],
image rotation prediction [24], or solving jigsaw puzzles [44] to train feature ex-
tractors in absence of annotations. Representations learned with those methods
are limitedly effective, hence more recent literature has moved towards more
sophisticated approaches, such as using image augmentations to generate corre-
latedviewsofatrainingsample,andlearningtoextractaugmentation-invariant
representations for these correlated pairs. Among these multi-view methods,
manyexploitcontrastivelosses[1,8,10–12,31,34,35,41,42,45],enforcingsimilar-
itybetweenviewsofthesameimage(positives)anddissimilarityfromotherim-
ages(negatives).Duetotheneedofmanynegativesamples,contrastivemethods
oftenrequirelargebatchsizestoworkeffectively[10,14].Cluster-basedmethods
suchasSwAV[8],DINO[9]andDeepClusterv2[6]learngeneralizablerepresen-
tations by grouping samples into cluster prototypes. Others exploit prediction
of features with siamese networks [13], learn features in a teacher-student fash-
ion [28], or use redundancy reduction techniques [4,65]. Finally, masked image
modeling[3,30,63]emergedasascalablealternativeforVisionTransformers[21],
learning representations by predicting masked image patches as SSL task.
Pretraining at Scale SSL is most effective when pretraining is conducted at
scale, benefiting from large datasets and great computational resources. Initial
attempts at large-scale pretraining were made via combining contrastive learn-
ingandclustering[7,55].TheSEERmodel[25]wastrainedon1billioninternet
images using SwAV [8] applied on a RegNet [48] backbone. The importance of
model scaling to leverage large-scale datasets is also shown in [26,27,64], as
well as the need for increasing training duration [64]. Additional strategies are
necessary to achieve best performance at scale [17,23,66]. All considered works
focus on reaching the best representations, without many considerations about
trainingcosts,thusencouragingunfaircomparisonsabouttheefficacyofdata.A
preliminarywork[15]foundforSimCLR[10]thatincreasingdatafrom500thou-4 Hammoud et al.
sand to 1 million images leads to a modest boost in downstream performance,
but without limiting the training budget. While large uncurated datasets have
been further explored [55], their efficacy in relationship with the distribution
shift under normalized computation is still limitedly investigated.
Distribution Shift in SSL Some studies have investigated how the pre-
training data domain affects downstream performance on other domains. For
various pretraining datasets, preliminary works [15,27,36,40] observed that the
best performing representations were learned on datasets that were similar to
the downstream test task. Additionally, combining datasets before pretraining
or combining self-supervised features learned from various datasets did not lead
to significant improvements in [15]. In [51,68], they showcase higher generaliza-
tion capabilities of SSL models compared to their supervised counterparts, for
several downstream tasks in the presence of a distribution shift. In [59], they
pretrained on several datasets, and observed different generalization depending
ontheobject-centricorscene-centricappearanceofthepretrainingdataset.Fur-
thermore, initial considerations on the impact of external data on downstream
tasks under distribution shift have been proposed in [60]. Some compensate the
distributionshiftwiththescaleoftrainingdatasets[29].Althoughpartialinfor-
mationcanbeinferredbytheseworks,thereisstillalackofafair,computation-
normalizedevaluationthatallowstostudytheeffectsofthedistributionshiftin
a controlled environment.
3 Preliminaries
Pretraining We first outline the general pretraining procedure common
to state-of-the-art self-supervised learning methods. Specific differences among
these methods are detailed in the supplementary material. The overall pretrain-
ing pipeline, common across many SSL approaches [4,8–10,28,65], goes as fol-
lows: (1) Data Sampling: from a large-scale, unlabeled upstream pretraining
dataset, denoted as D , an image x is randomly sampled; (2) View Gener-
SSL
ation: two correlated views, x˜ and x˜ , are generated from x using two im-
A B
age augmentation functions sampled at random. These random augmentations
include random resized cropping, horizontal flipping, blurring, and color adjust-
ments [2], among others; (3) Feature Extraction and Projection: the corre-
latedviewsundergofeatureextractionthroughanetwork,f ,parameterizedby
θf
θ ,suchasaResNet[32]oraViT[21],leadingtorepresentationsh =f (x˜ )
f A θf A
and h = f (x˜ ). A linear projection head, g , parameterized by θ , then
B θf B θg g
maps these representations to a latent space, resulting in z = g (h ) and
A θg A
z = g (h ); (4) Joint Optimization: The feature extractor f and the
B θg B θf
projection head g are jointly optimized according to the following objective:
θg
θ f∗,θ g∗ =argmin E x∼D
SSL
L SSL(z A,z B), (1)
θf,θg
where L is a loss function specific to the chosen SSL pretraining method.
SSL
After pretraining, the feature extractor f can be deployed for various down-
θ∗
fPretaining Data Diversity SSL 5
stream taskssuchasimageclassification,objectdetection,orsegmentation.This
is typically achieved by training a task-specific head. Alternatively, the feature
extractor f can either be fine-tuned or used together with a k-nearest neigh-
θ∗
f
bors (kNN) classifier.
Linear Probing Thereareseveralwaystoevaluatetheperformanceofaself-
supervised learning method such as linear probing [10,14,28], kNN [8,9,62,71],
andfew-shotevaluation[22,25].Consistentwiththegeneralprotocolestablished
in the literature [10,15,31,67], we use linear probing to measure the quality of
the features extracted for classification tasks. The procedure is as follows: (1) a
labeleddownstream dataset,D ,consistingofimage-classpairs(x,y)∼D ,
task task
is selected for evaluation. (2) For each image x, its representation is extracted
using the pretrained feature extractor f , after which the linear classification
θ∗
f
head t , parameterized by θ , is then applied to obtain t (f (x)). (3) The
θt t θt θ f∗
linear head t is optimized as follows:
θt
(cid:104) (cid:105)
θ t∗ =arg θm
t
in E (x,y)∼D task L task(t θt(f θ f∗(x)),y) , (2)
Note that only the parameters of the linear head θ are optimized, while the
t
parameters θ∗ of the feature extractor are kept frozen. The quality of features
f
extracted by f is directly inferred from the classification accuracy achieved on
θ∗
the test set of
Df
, which serves as the primary indicator of the quality of the
task
extracted features.
4 Normalized Evaluation
We stress how for a correct understanding of the impact of data diversity we
needtoanalyzeitseffectsisolatingthemfromtheimpactofincreasedcomputa-
tion.Toenablethis,weintroduce(1)acomputationalbudgetusedtonormalize
computation across experiments, and (2) a quantification of the data diversity
seen by models during pretraining.
Computational Budget Current progress in SSL pretraining simultane-
ously scales computational budget and dataset size to achieve the best perfor-
mance [25]. This makes it difficult to assess the reasons behind the success of
many SSL algorithms. Do these methods benefit mainly from the large amounts
of computation, i.e., running SSL pretraining for large numbers of epochs, or
do they benefit from data diversity in larger datasets containing a vast variety
of visual features? To perform a disentangled evaluation of these two factors,
we first introduce C as a measure of computational budget, which quantifies the
total number of images an SSL method is allowed to process during pretrain-
ing. This is calculated as C = N ·E, where N is the number of unique samples
in the pretraining dataset D , hence the data diversity of the dataset, and E
SSL
is the number of pretraining epochs. Constraining multiple models pretrained
with SSL to a fixed computational budget C allows for meaningful comparison,6 Hammoud et al.
Table 1: Impact of Data Diversity on CIFAR-100 and Tiny ImageNet SSL
Pretraining Performance: We study the effects of diversity on CIFAR-100 a and
TinyImageNetbacrosssevendifferentmethodsandthreedatadiversitysettingsfora
ResNet-18pretraining,whereforall,D =D .Thiscomparisonincludesanalyzing
task SSL
classificationaccuracythroughlinearprobingonthetrainingsetandevaluationonthe
test set of the respective datasets. Although performance fluctuates among different
methods, a consistent trend is observed: higher data diversity typically leads to the
generation of higher quality representations.
N D Accuracy↑ N D Accuracy↑
(×103)(×10−3) SimCLRB.T. BYOL SwAV VICRegMoCoV3DINO (×103)(×10−3) SimCLRB.T. BYOL SwAV VICRegMoCoV3DINO
5 0.1 37.95 44.30 41.14 14.06 44.06 12.51 15.74 10 0.2 36.91 40.98 35.62 34.23 37.58 36.56 34.54
25 0.5 50.13 54.29 51.01 45.44 50.37 51.58 35.89 50 1.0 48.77 52.01 48.05 43.63 48.75 46.45 44.39
50 1.0 58.59 58.42 58.28 56.37 55.83 55.61 40.93 100 2.0 49.83 55.60 50.29 47.48 54.10 48.58 47.32
C=50×106 C=50×106
(a) CIFAR-100 (b) TinyImageNet
as it is guaranteed that all SSL methods will have processed the same number
of pretraining images.
Quantifying Pretraining Diversity EvenundernormalizedcomputationC,
various SSL approaches may be exposed to different data diversity as they are
trainedwithdifferentdatasets.Forinstance,amodeltrainedforE =1000epochs
on a dataset of size N =1000 will see less diversity than a model pretrained for
E =1 epoch on a dataset of size N =106, despite that they are both pretrained
undernormalizedcomputation,processingthesamenumberofimages.Hence,to
capture this notion of exposure to diversity while pretraining under normalized
computation C, we define a pretraining diversity D, which captures the number
ofuniquesamplesencounteredduringtraininggivenafixedC asD =N/C =1/E.
A model pretrained with larger D indicates that the model is presented with
a larger number of unique images during training with fixed C, and hence is
exposed to more pretraining data diversity. In the next section, we explore the
effects of variations in D on SSL performance under a distribution shift, while
keeping C constant.
5 Fixed Budget SSL: In & Out-of-Distribution
Training Configuration WeevaluatesevenSSLmethods:SimCLR[10],Mo-
CoV3[14],VICReg[4],DINO[9],BYOL[28],BarlowTwins[65],andSwAV[8].
We use different datasets for both pretraining D and linear probing D for
SSL task
different sections. We use solo-learn [16] as a codebase. For each method, we
use the default parameters provided when available, otherwise, we conduct a
hyperparameter search to ensure proper convergence.
5.1 Performance on the Same Distribution
We aim to answer a first question: does collecting more samples from the same
distribution help SSL pretraining with a fixed budget? We conduct experimentsPretaining Data Diversity SSL 7
to capture how the pretraining diversity D influences SSL pretraining within a
normalized C, focusing on the simplest setting where the upstream and down-
stream datasets belong to the same distribution such that D = D . This
SSL task
serves as a fundamental ground for our further experiments.
Setup Forpretraining,weuseCIFAR-100[38]andTinyImageNet[39]asD ,
SSL
which contain 50×103 and 100×103 images, respectively. We set C =50×106,
chosen such that it allows all methods to converge during pretraining. In Sec-
tion 6, we study the effect of varying the budget C. We pretrain on subsets of
D withdifferentsizesN (10%,50%,and100%ofD ),enablingustoobserve
SSL SSL
the effects of pretraining diversity D on training outcomes where E is adjusted
accordingly to match the budget C. For example, using 100% of CIFAR-100 in-
volves 1000 epochs of training, while 10% and 50% of the dataset lead to 10000
and2000epochs,respectively.ThesesubsetsofD arecreatedbyshufflingthe
SSL
dataset and then selecting the first 10%,50%, or 100% split. All models in this
section use a ResNet-18 [32] backbone pretrained from scratch. For evaluation,
we use in-distribution linear probing, i.e., D = D , where performance is
task SSL
measured by classification accuracy on the test set.
Results The results of linear probing are presented in Tables 1a and 1b for
CIFAR-100andTinyImageNet,respectively.WhiledifferentSSLmethodsshow
varying levels of performance, we observe that, for all methods, an increase in
the diversity D consistently leads to higher linear probing accuracy, suggesting
that including more in-distribution samples in D helps under a normalized
SSL
C. For example, SimCLR achieves an accuracy of 37.95 when 10% of CIFAR-
100 is provided, whereas this performance improve by around 12% when only
50% of the dataset is provided and by another 8% after pretraining on 100% of
uniquesamples.ThissuggeststhatSSLmethodssubstantiallybenefitfromhav-
ing a more diverse pretraining set in computationally budgeted in-distribution
evaluation, a fundamental verification that allows us to proceed in our analysis.
Insight (1)
When the distributions of the upstream and downstream tasks are the
same, i.e., D = D , in a normalized computation setup, increasing
SSL task
pretraining diversity D proves to be an effective strategy for enhancing
SSL pretraining performance.
5.2 Increasing Data Diversity
AsobservedinSection5.1,ifD =D havingamorediversepretrainingset
SSL task
benefits SSL methods, under a normalized computation assumption. However,
to increase diversity, sampling from the same distribution D to extend D
task SSL
isnotalwaysattainable.Infact,pretrainingdataisoftensampledfromadistri-
butiondifferentthanD .Theaddedsampleswillthenintroduceadistribution
task
shift between D and D .
SSL task8 Hammoud et al.
Fig.2: Data Collection
goldfinch, hornbill, sea anemone Strategies: We analyze
strategies for collecting addi-
tional data (A), i.e., collecting
more source data, crawling the
goldfinch web or using synthetic images.
Using a class prior (top row)
Existing data Internet crawl Diffusion model simulates In-distribution
hornbill trainings. We also collect im-
ages without class prior (bot-
tom row) to analize the inter-
anemone
actions between diversity and
Out-of-distribution classes.
japanese spaniel, ibex, squirrel monkey
Then, we raise the following question: is increasing pretraining diversity still
beneficial when there is a distribution shift between D and D , i.e., D ̸=
SSL task SSL
D ? We explore strategies for acquiring new data for increasing D, namely,
task
including existing samples, crawled internet data, and data synthetically gen-
erated by diffusion models. To evaluate the effects of the distribution shift in
a controlled scenario, we analyze distributions closer to D by using a class
task
prior (in-distribution) and without a class prior (out-of-distribution).
Setup We use ImageNet-100 [56] as D , and we construct multiple D
task SSL
to evaluate the effects of different data collection strategies and the distribution
shift. We first introduce a set B composed of 65×103 images from ImageNet-
100 (50% of the dataset), which we use as a baseline for D with minimum
SSL
diversity. We denote the 100 classes in ImageNet-100 as T , and sample B
100
uniformly including images from all classes. Next, we compare with pretraining
on more diverse datasets as D , imposing D = B∪A where A includes
SSL SSL
65×103 images sampled with one of three strategies. To highlight the effects
of the distribution shift, we include in A either images from In-distribution
classes, i.e., selecting images from classes overlapping with T , or images from
100
Out-of-distributionclasses,whichdonotoverlapwithT .Thisistostudythe
100
effects of the distribution shift, since we do not assume access to downstream
classesinrealscenarios.FortheOut-of-distributionsamples,wedefineAas(1)
randomimagessampledfromthefullImageNet[18]dataset;(2)imagescrawled
from Flickr, Bing, and DuckDuckGo; or (3) images generated with Stable Diffu-
sion V2.1 [50]. We respectively call these sets AOut , AOut, and AOut . We
Source Web Synthetic
also define their In-distribution counterparts as AIn , AIn , and AIn ,
Source Web Synthetic
respectively. Note how B ∪ AIn is the full ImageNet-100, coherently with
Source
Section 5.1. Figure 2 shows each collection strategy, for which we provide im-
plementation details in supplementary material. Although many factors (such
as the appearance of images) impact the distribution shift, using a class prior
imposes that any strategy using it would still result in closer distribution with
respect to the same strategy without class priors [15]. We pretrain a ResNet-18
roirp
ssalc
htiw
roirp
ssalc
tuohtiwPretaining Data Diversity SSL 9
Fig.3: Effect of Various Data Sources on SSL Pretraining: We use a baseline
set B (black dashed line), comprising 65×103 images from ImageNet-100, for pre-
training a ResNet-18 with C = 50×106. Augmenting B with In-distribution images
enhancesperformance(aboveblackline),whileOut-of-distributionaugmentationsre-
duce it (below black line).
from scratch, with the same settings of Section 5.1, and C =50×106. Note that
this results in D = 1.25×10−3 for pretraining on B, and D = 2.5×10−3 for
pretraining on B∪A, introducing a D difference of a factor of 2.
Results WereportthelinearprobingaccuraciesforpretrainingoneachB∪A
as bars in Figure 3, showing the C-normalized training on B as a dashed line.
Surprisingly, without class priors (Out), including AOut , AOut, and AOut
Source Web Synthetic
underperformscomparedtopretrainingonBonly.Forinstance,forSimCLRand
whileBscores72.3%accuracy,increasingdiversityreducestheaccuracyby1%in
allcases.ThismightappeartoconflictwithourfindingsinSection5.1,however,
theinclusionof Out samplesleadstoD ̸=D ,sincewesampleonlyclasses
SSL task
notincludedinD .Weinferthat,withnormalizedC,increasingDwithoutdis-
task
tributionpriorsmaynegativelyimpactperformance.Conversely,whenclasspri-
ors are available (In), increasing pretraining diversity D improves performance
compared to B pretraining. For instance, pretraining on AIn performs compa-
Web
rably to augmenting with additional ImageNet samples (AIn ), as in the case
Source
ofSimCLRwheretheinclusionofAIn scoresonly0.2%lowerthanAIn .In-
Web Source
cludingAIn dataalsohelps,SyntheticAIn datahelps,althoughmore
Synthetic Synthetic
limitedlyduetothevisualdiscrepanciesbetweengeneratedandrealimages.Ulti-
mately,theeffectivenessofdiversityislinkedtothedistributionshift.Thesefind-
ingshighlighttheimpactofthedistributionshiftoncomputationally-normalized
SSLpretrainingandhelpdefineevaluationpracticesforthistask(seeSection7).
Insight (2)
When the distributions of the upstream and downstream tasks differ,
D ̸= D , and in a normalized computation setup, increasing pre-
SSL task
training diversity D may harm SSL pretraining performance, reflecting
the adverse effects of distribution shift.10 Hammoud et al.
Table 2: Performance of ImageNet and YFCC100M SSL Pretraining on
Various Downstream Tasks: We train ResNet-50 and ViT-B/16 under normalized
computation(C =98×106)usingSimCLR(left)andMoCoV3(right)onImageNetand
YFCC100M(YFCC)withmultipleD.DespitethesignificantlylargerD whentrained
onYFCC100M,thesemodelscannotoffsettheeffectsofthedistributionshiftandare
outperformed by models pretrained on ImageNet in the majority of downstreams.
SimCLR MoCoV3
DSSL (×N 106) (×1D 0−3) ImageNet CarsA Fc lc ou wr .ac Py e↑ ts Places Food DSSL (×N 106) (×1D 0−3) ImageNet CarsA Fc lc ou wr .ac Py e↑ ts Places Food
ImageNet 0.128 1.31 56.9 43.0 82.3 73.9 45.4 59.9 ImageNet 0.128 1.31 58.1 40.6 81.8 76.04 45.1 63.9
ImageNet 0.256 2.61 61.1 45.5 84.0 76.0 47.0 64.4 ImageNet 0.256 2.61 62.9 45.3 85.0 79.8 47.6 68.7
ImageNet 0.640 6.54 63.7 46.8 84.2 78.8 48.3 67.2 ImageNet 0.640 6.54 65.4 48.4 86.1 81.9 49.2 71.0
ImageNet 1.281 13.0 64.5 46.4 84.6 79.5 48.8 68.0 ImageNet 1.281 13.0 65.9 48.8 86.6 82.6 49.5 71.9
YFCC 98.17 1000 57.3 37.2 76.6 58.9 50.1 62.1 YFCC 98.17 1000 60.4 42.2 82.6 66.3 50.7 67.3
ImageNet 0.128 1.31 54.2 37.2 81.8 70.3 44.6 64.3 ImageNet 0.128 1.31 57.9 33.4 82.8 78.0 46.7 67.8
ImageNet 0.256 2.61 61.3 39.5 83.8 77.7 47.2 69.4 ImageNet 0.256 2.61 63.7 35.0 85.3 82.9 48.4 71.0
ImageNet 0.640 6.54 65.5 39.9 84.6 80.4 49.1 73.1 ImageNet 0.640 6.54 67.2 39.5 85.0 85.8 49.7 72.8
ImageNet 1.281 13.0 66.7 39.5 83.7 81.7 50.0 73.6 ImageNet 1.281 13.0 68.8 41.9 86.5 86.5 50.3 73.8
YFCC 98.17 1000 54.5 25.0 72.5 59.7 49.5 65.4 YFCC 98.17 1000 57.2 25.2 70.3 43.8 50.3 64.0
C=98×106 C=98×106
5.3 Scaling Pretraining Diversity
We showed that diversity improves pretraining performance when the training
set and downstream task share the same data distribution (D = D ), as
SSL task
discussedinSection5.1.Thismaychangewhenadistributionshiftisintroduced,
as explored in Section 5.2. However, it is still unclear from Section 5.2, whether
including a larger number of samples, and thus increasing considerably the pre-
training diversity, can compensate for the negative effects of the distribution
shift. To address this, the following section presents larger-scale experiments,
employingsignificantlyvariedD values,aimingtoexploretheinterplaybetween
pretraining diversity and different distributions using millions of samples.
Setup For our large-scale pretraining experiments, we set D to be two
SSL
datasetsofsignificantlydifferentsizes:ImageNet[18]andYFCC100M[54],com-
prising 1.28 million and 98 million images, respectively. Following Section 5.1,
we explore multiple D values for pretraining. We set C =98×106, which corre-
spondstooneepochonYFCC100M,iteratingoncethrougheachofits98million
images to maximize diversity (D =1). Normalizing C (see Section 5.1), we pre-
trainonImageNetforapproximately77epochs,cumulativelyutilizing98million
samples. Due to the extensive cost of these experiments, we focus on SimCLR
and MoCoV3 only. We also employ larger architectures, namely ResNet-50 [32]
and ViT-B/16 [21], to leverage the extensive scale of the pretraining datasets.
WealsousemultipleD includingImageNet[18],StanfordCars[37],Flowers-
task
102 [43], Oxford-IIIT Pets [47], Places365 [69], and Food-101 [5].
Results The results of our large-scale experiments are detailed in Table 2.
Consistently with findings in Section 5.1, increasing D leads to better pretrain-
ing efficacy when D =D . This is evident when ImageNet is used for both
SSL task
pretraininganddownstreamclassification,reinforcingthatourobservationshold
even at a larger scale. Instead, models pretrained on YFCC100M show substan-
tially lower performance compared to those pretrained on ImageNet, although
having much higher D. This highlights the inefficiency of collecting data indis-
05-teNseR
61/B-TiV
05-teNseR
61/B-TiVPretaining Data Diversity SSL 11
criminately without considering distribution shifts. To stress this, note how the
model pretrained on YFCC100M (D =1) often performs similarly to those pre-
trained with drastically lower D on ImageNet (D = 1.31×10−3). This aligns
with our observations in Section 5.2, emphasizing that distribution differences
remain a significant factor even when training with large datasets. However,
the YFCC100M-pretrained model outperforms the ImageNet-pretrained model
in Places365, suggesting a closer distribution relationship between YFCC100M
andPlaces365.WeexplorethishypothesisfurtherinSection6,whereweanalyze
distribution distances with existing metrics. Ultimately, our analysis highlights
that scaling the data is not an effective solution for compensating the distribu-
tion shift, when computational resources are normalized.
Insight (3)
Even an extremely large data diversity cannot mitigate the distribution
shift under normalized computation. This emphasizes the importance of
furtherresearchinhoweffectivelySSLpretrainingalgorithmsgeneralize.
6 Additional Analysis
Previously, we proposed a computationally-normalized evaluation to assess the
role of D with and without the distribution shift. We highlighted that, although
pretrainingdiversityhelpswhenD =D ,visionmodelsareunabletocom-
SSL task
pensate for the distributional differences under normalized computation. Now,
we analyze additional elements that support our previous observations.
Distribution Distances Using FID & VisualDNA In Section 5.3, we
showed that pretraining on ImageNet typically outperforms YFCC100M on a
variety of downstream tasks, with Places365 being the exception. We speculate
that the distribution of ImageNet is more aligned with those of the downstream
taskscomparedtoYFCC100M.Toverifythis,weevaluatethesimilaritybetween
the datasets using FID [33] and VisualDNA [49] and report results in Table 3.
With both FID or VisualDNA, the distribution of ImageNet is always closer to
the downstream tasks, except for Places365 where YFCC100M is closer. This
aligns with the lower performance of ImageNet on this dataset only (Table 2),
furthersuggestingthattheperformancedropiscausedbythedistributionshift.
Importance of Normalized Computation We now study the impact of
normalizing computation on the performance of SSL methods. We aim to un-
derstand if C is not normalized across methods, this will lead to misleading
conclusions, and thus motivating our normalized computation. In Table 4, we
compare the performance of ResNet-50 and ViT-B/16 models pretrained using
MoCoV3 with C =98×106 (as used in Section 5.3) against a tripled budget of
C =294×106.Weshowthatinconsistenciesarisewhenpretrainingdatadiversity
D and computation C are not fairly compared. For instance, in scenarios high-
lightedin red,wenoticethatalowerpretrainingdiversitycoupledwithahigher12 Hammoud et al.
Table3:ComparingVisualDNAand Table 4: Importance of Normaliza-
FID Scores Across Datasets: We tion. We report the accuracy of Mo-
assess the relationship between Visu- CoV3trainedonImageNetwithdifferent
alDNA[49]andFIDScore[33]forvarious data diversity and variable C, for the in-
large-scale D and several downstream distribution assumption (D = D ).
SSL SSL task
tasks.ForVisualDNA,activationsareex- For a given network, cells in red high-
tractedassuggested[49]withMUGS[70] light inconsistencies where although the
withViT-B/16(MUGS)orDINOv2[46] model trained with C = 294 × 106 has
withViT-B/16(DINOv2),whileFIDac- seenlesssamples,itoutperformsthebest
tivations are obtained via an Inception model trained with one third of the com-
network[53].Consistently,acrossallmet- putational budget (C = 90×106), show-
rics, the ranking of dataset distances be- ing the importance of normalization for
tween D and various D aligns with understanding how models exploit data.
SSL task
the accuracy ranking in Table 2. Models
exhibiting lower VisualDNA/FID scores MoCoV3
b tre an inefi int gm do ar tae ff or rom SSLth .e diversity in pre- Network D SSL (×1C 06)(×N 106)(×1D 0−3)A Imcc au gr ea Nc ey t↑
ImageNet 98 0.128 1.31 58.1
ImageNet 98 0.640 6.54 65.4
D SSL BackboneImageNet CaV rsisu Fa lol wD .N PA et↓ sPlacesFood ResNet-50I Im ma ag ge eN Ne et t 29 98 4 1 0. .2 18 21 8 1 03 .4.0 3 56 75 .. 49
ImageNet MUGS 0.00 11.4912.466.09 7.19 9.08 ImageNet 294 0.640 2.17 69.8
YFCC MUGS 3.72 11.57 12.71 7.93 6.20 9.72 ImageNet 294 1.281 4.35 71.4
Im Ya Fg CeN Cet D DI IN NO Ovv 22 0 2. .0 370 7 7. .0 125 6 7. .9 465 4 5. .4 753 23 .. 752
4
7 7. .2 906 I Im ma ag ge eN Ne et
t
9 98
8
0 0. .1 62 48
0
1 6. .3 51
4
5 67 7. .9
2
DSSL BackboneImageNet Cars FlF owID .↓ Pets Places Food ViT-B/16I Im ma ag ge eN Ne et t 29 98 4 1 0. .2 18 21 8 1 03 .4.0 3 56 68 .. 98
ImageNet Inception 0.00 143.40192.8988.85 64.91114.92 ImageNet 294 0.640 2.17 71.9
YFCC Inception 48.14 174.27 214.78145.7938.86 154.51 ImageNet 294 1.281 4.35 74.9
computational budget can yield better results. We observe that under pretrain-
ing diversity of only D =13×10−3 and a computational budget of C =98×106
ResNet-50 only enjoys a 65.9% accuracy compared against with 69.8% with a
smallerpretrainingdiversityof2.17×10−3 butwithalargercomputationalbud-
get of 294×106. This shows that without normalized computation, it could be
incorrectly concluded that pretraining diversity does not play a significant role.
Model Saturation In Section 5.3, we evaluated the effects of extreme differ-
ences in D. We aim to understand the trend in pretraining on YFCC100M, and
if adding more samples could compensate the distribution shift with ImageNet.
Hence,weextendtheYFCC100MexperimentsfromSection5.3,examiningvar-
ious subsets—specifically 0.1%, 1%, 10%, and 100% of the dataset. Again, we
normalize the computational budget to C = 98×106, equivalent to one epoch
on YFCC100M. The results of linear probing models pretrained using SimCLR
and MoCoV3 with ResNet-50 and ViT-B/16 on ImageNet are shown in Fig-
ure4.Interestingly,weobtainaperformanceplateau.Thisobservationpointsto
a saturation point in performance, showing that simply increasing the dataset
furtherwouldnotbridgethegapbetweentheD andD .Hence,arbitrarily
SSL task
increasingpretrainingdiversityDisnotsufficienttobridgethedistributionshift
problem.HereD ̸=D ,hencestillaligningwithourfindingsinSection 5.1.
SSL taskPretaining Data Diversity SSL 13
SimCLR MoCoV3
60 60 60 60
50 50 50 50
0.001 0.01 0.1 1.0 0.001 0.01 0.1 1.0 0.001 0.01 0.1 1.0 0.001 0.01 0.1 1.0
ResNet-50 ViT-Base
Fig.4:DataDiversityImpactonYFCC100MPretrainingPerformance:Pre-
training (C = 98×106) of networks with D = YFCC100M and D = ImageNet
SSL task
for several dataset subsets. In the presence of a distribution shift, performance tends
to saturate and does not benefit from additional data diversity.
Table5:EvaluatingNetworkAccuracyWithVariedLabelQuantity:Weeval-
uate the accuracy of networks trained on ImageNet and YFCC with different labeling
percentagesofD =ImageNet.Theincreaseddiversitystilldoesnotcompensatefor
task
the distribution shift. However, for in-distribution data, one can get away with fewer
labels with more diverse pretraining data.
SimCLR MoCoV3
Network DSSL (×N 106) (×1D 0−3) 5% 1A 0c %cura 5c 0y %↑ 100% Network DSSL (×N 106) (×1D 0−3) 5% 1A 0c %cura 5c 0y %↑ 100%
ResNet-50 ImageNet 1.28 1.31 50.3 54.2 61.7 64.5 ResNet-50 ImageNet 1.28 1.31 52.1 56.2 63.8 65.9
YFCC 98.2 1000 39.4 44.3 54.0 57.3 YFCC 98.2 1000 42.8 48.2 57.7 60.4
ViT-B/16 Im Ya Fg CeN Cet 1 9. 82 .8 2 11 0.3 01 0 5 45 0.. 72 5 49 5.. 74 6 55 3.. 36 6 56 4.. 57 ViT-B/16 Im Ya Fg CeN Cet 1 9. 82 .8 2 11 0.3 01 0 5 49 3.. 93 6 43 9.. 00 6 58 6.. 03 6 58 7.. 28
C=98×106 C=98×106
Label Quantity A question that arises in our setting is whether having a
higher pretraining diversity leads to requiring less labeled samples during linear
probing. In this section, we focus on understanding the impact of increased D
on the number of labeled samples required for effective linear probing in down-
stream tasks. We use the same trained models from Section 5.3, i.e., SimCLR
and MoCoV3, with ResNet-50 and ViT-B/16 architectures. In this setup, we
set D = ImageNet, with two upstream dataset scenarios: ImageNet (where
task
D =D )andYFCC100M(whereD ̸=D ).Ourexperimentsaresum-
SSL task SSL task
marizedinTable5.WenotethatwithViT-B/16,ifImageNetisusedforpretrain-
ing, linear probing with just 5% labeled samples can surpass the performance
achieved using 100% labeled data when YFCC100M serves as D . This also
SSL
implies that when D ̸= D , a higher quantity of labeled data is necessary
SSL task
toattaincompetitiveperformance,andincreasingDdoesnotbringconsiderable
benefitsonlabelquantityunderdistributionshift.Thisimpliesthatourfindings
in Section 5.3 hold regardless of the linear probing labeled set. We note that in
scenarios where D = D , using only 50% of the labeled data can achieve
SSL task
similar performance as utilizing the full 100% of labeled samples, implying that
increasing D leads to reduced label requirement efforts for downstream tasks.
7 Discussion
Here, we discuss the implications of our findings. We aim to highlight inefficien-
cies in current strategies, and provide takeaways for good SSL practices.
.ccA14 Hammoud et al.
Main Conclusions OursetofexperimentsleadstoInsight(3)inSection5.3,
revealingthatwithnormalizedcomputationalcosts,SSLpretrainingswithlarge
diversity cannot compensate for the distribution shift. This is surprising, since
the variety of information that SSL algorithms could benefit from during train-
ing is much higher in large generic datasets than in small ones. Hence, since our
evaluation is cost-normalized, (3) also implies that SSL strategies are not effi-
ciently exploiting the pretraining diversity on large datasets for representation
extraction. This inefficiency reflects in a wide margin for improvement of gen-
eralization performance of SSL models, making better use of the computational
powerinvolvedfortraining.Theroleofexistingmodelsmustalsobediscussedin
this context. Following Insights (1) and (2), respectively in Sections 5.1 and 5.2,
we have studied how in-distribution and out-of-distribution data impact perfor-
manceinacontrolledscenario.Wearguethatthisbehaviorshouldbetakeninto
account in the evaluation of the performance of SSL models. Indeed, training at
scale may enlarge the in-distribution space, including classes of the downstream
tasks in the training set. In recent literature, this is a design choice to maximize
performanceonpopularbenchmarks[46].Whilethisallowsforachievingimpres-
sive results in many tasks, we stress that this does not permit a fair evaluation.
Now, we summarize practical takeaways.
TrainingTakeaways CoherentlywithourfindingsinSection5.1andsimilarly
topriorart[15,27,36],wefindthataligneddistributionsbenefitperformance,in
particular increasing D diversity helps in learning better feature extractors
SSL
as long as the distribution of the new samples match those of the downstream
task data. Differently from the state-of-the-art, we demonstrate that this holds
alsoinacomputationally-normalizedsetting,implyingthatcollectinglargescale
in-distribution data matching the downstream task could be an effective and ef-
ficientapproachtoimprovingSSL.Hence,forpracticalapplicationsdistribution
priorsshouldbeused,ifavailable.Onthecontrary,forafairevaluationofmod-
els, this should not be the case, as specified below.
Evaluation Takeaways Our analysis reveals that to permit a fair evalua-
tion of SSL methods, computationally normalized tests are necessary to avoid
inconsistencies, as shown in Section 6. Moreover, it is crucial to identify out-of-
distribution downstream tasks for a correct evaluation of generalization perfor-
mance.ByevaluatingonlyonD withalowdistributionshift,thereisariskof
task
reportinginflatedmetrics,notrepresentativeofarealgainingeneralization.This
is important, since new SSL approaches may be reporting higher downstream
performance when pretrained on a different dataset. We relate this to Sections
5 and 6, where we show that increasing the computation and the in-distribution
data, respectively, can improve performance. Ultimately, wrong practices may
result in incorrectly concluding that an SSL algorithm is performing better.
Differences With Language Models In Section 5.3 we showed that even
very diverse datasets, such as YFCC100M, fall short in satisfactory generaliza-
tionperformance.Beyondpavingthewayforfurtherexplorationintogeneraliza-
tion for SSL pretraining, this open doors to investigating why language modelsPretaining Data Diversity SSL 15
enjoyenhancedgeneralizationwhenexposedtoawideSSLpretrainingdiversity
compared to vision models [61].
References
1. Bachman,P.,Hjelm,R.D.,Buchwalter,W.:Learningrepresentationsbymaximiz-
ing mutual information across views. In: NeurIPS (2019) 3
2. Balestriero, R., Ibrahim, M., Sobal, V., Morcos, A., Shekhar, S., Goldstein, T.,
Bordes,F.,Bardes,A.,Mialon,G.,Tian,Y.,etal.:Acookbookofself-supervised
learning. arXiv preprint arXiv:2304.12210 (2023) 4
3. Bao,H.,Dong,L.,Piao,S.,Wei,F.:BEit:BERTpre-trainingofimagetransform-
ers. In: ICML (2022) 3
4. Bardes, A., Ponce, J., LeCun, Y.: VICReg: Variance-invariance-covariance regu-
larization for self-supervised learning. In: ICLR (2022) 2, 3, 4, 6
5. Bossard, L., Guillaumin, M., Van Gool, L.: Food-101–Mining discriminative com-
ponents with random forests. In: ECCV (2014) 2, 10, 3
6. Caron,M.,Bojanowski,P.,Joulin,A.,Douze,M.:Deepclusteringforunsupervised
learning of visual features. In: ECCV (2018) 3
7. Caron, M., Bojanowski, P., Mairal, J., Joulin, A.: Unsupervised pre-training of
image features on non-curated data. In: ICCV (2019) 3
8. Caron,M.,Misra,I.,Mairal,J.,Goyal,P.,Bojanowski,P.,Joulin,A.:Unsupervised
learning of visual features by contrasting cluster assignments. In: NeurIPS (2020)
2, 3, 4, 5, 6, 7
9. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., Joulin,
A.:Emergingpropertiesinself-supervisedvisiontransformers.In:ICCV(2021) 2,
3, 4, 5, 6
10. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-
trastive learning of visual representations. In: ICML (2020) 2, 3, 4, 5, 6, 1
11. Chen, T., Kornblith, S., Swersky, K., Norouzi, M., Hinton, G.: Big self-supervised
models are strong semi-supervised learners. In: NeurIPS (2020) 3
12. Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum con-
trastive learning. arXiv preprint arXiv:2003.04297 (2020) 3
13. Chen, X., He, K.: Exploring simple siamese representation learning. In: CVPR
(2021) 3
14. Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision
transformers. In: ICCV (2021) 2, 3, 5, 6, 1
15. Cole,E.,Yang,X.,Wilber,K.,Aodha,O.M.,Belongie,S.:Whendoescontrastive
visual representation learning work? In: CVPR (2022) 3, 4, 5, 8, 14
16. Costa, V.G.T.d., Fini, E., Nabi, M., Sebe, N., Ricci, E.: solo-learn: A library of
self-supervised methods for visual representation learning. In: JMLR (2022) 6, 8
17. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J.,
Steiner, A.P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al.: Scaling vision
transformers to 22 billion parameters. In: ICML (2023) 3
18. Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:Imagenet:Alarge-scale
hierarchical image database. In: CVPR (2009) 2, 8, 10, 3
19. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning
by context prediction. In: ICCV (2015) 3
20. Doersch, C., Zisserman, A.: Multi-task self-supervised visual learning. In: ICCV
(2017) 316 Hammoud et al.
21. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T.,Dehghani,M.,Minderer,M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,Houlsby,N.:
An image is worth 16x16 words: Transformers for image recognition at scale. In:
ICLR (2021) 3, 4, 10
22. Ericsson,L.,Gouk,H.,Hospedales,T.M.:Howwelldoself-supervisedmodelstrans-
fer? In: CVPR (2021) 5
23. Fini, E., Astolfi, P., Romero-Soriano, A., Verbeek, J., Drozdzal, M.: Improved
baselines for vision-language pre-training. TMLR (2023) 3
24. Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by
predicting image rotations. In: ICLR (2018) 3
25. Goyal, P., Caron, M., Lefaudeux, B., Xu, M., Wang, P., Pai, V., Singh, M.,
Liptchinsky, V., Misra, I., Joulin, A., et al.: Self-supervised pretraining of visual
features in the wild. arXiv preprint arXiv:2103.01988 (2021) 1, 3, 5
26. Goyal, P., Duval, Q., Seessel, I., Caron, M., Misra, I., Sagun, L., Joulin, A., Bo-
janowski,P.:Visionmodelsaremorerobustandfairwhenpretrainedonuncurated
images without supervision. arXiv preprint arXiv:2202.08360 (2022) 1, 3
27. Goyal, P., Mahajan, D., Gupta, A., Misra, I.: Scaling and benchmarking self-
supervised visual representation learning. In: ICCV (2019) 3, 4, 14
28. Grill, J.B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Do-
ersch,C.,AvilaPires,B.,Guo,Z.,GheshlaghiAzar,M.,etal.:Bootstrapyourown
latent: A new approach to self-supervised learning. In: NeurIPS. vol. 33 (2020) 2,
3, 4, 5, 6
29. Hammoud, H.A.A.K., Itani, H., Pizzati, F., Torr, P., Bibi, A., Ghanem, B.:
Synthclip: Are we ready for a fully synthetic clip training? arXiv preprint
arXiv:2402.01832 (2024) 4
30. He,K.,Chen,X.,Xie,S.,Li,Y.,Dollár,P.,Girshick,R.:Maskedautoencodersare
scalable vision learners. In: CVPR (2022) 3, 4
31. He,K.,Fan,H.,Wu,Y.,Xie,S.,Girshick,R.:Momentumcontrastforunsupervised
visual representation learning. In: CVPR (2020) 3, 5
32. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR (2016) 4, 7, 10
33. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
by a two time-scale update rule converge to a local nash equilibrium. NeurIPS 30
(2017) 11, 12, 8
34. Hjelm, R.D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P.,
Trischler, A., Bengio, Y.: Learning deep representations by mutual information
estimation and maximization. In: ICLR (2018) 3
35. Hénaff, O.J., Srinivas, A., De Fauw, J., Razavi, A., Doersch, C., Eslami, S.M.A.,
Oord, A.v.d.: Data-efficient image recognition with contrastive predictive coding.
In: ICML (2019) 3
36. Kotar, K., Ilharco, G., Schmidt, L., Ehsani, K., Mottaghi, R.: Contrasting con-
trastive self-supervised representation learning pipelines. In: ICCV (2021) 4, 14
37. Krause, J., Deng, J., Stark, M., Fei-Fei, L.: Collecting a large-scale dataset of
fine-grained cars. In: FGVC (2013) 2, 10, 3
38. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny
images (2009) 2, 7
39. Le,Y.,Yang,X.:Tinyimagenetvisualrecognitionchallenge.CS231N(2015) 2,7
40. Li, A.C., Brown, E.L., Efros, A.A., Pathak, D.: Internet explorer: Targeted repre-
sentation learning on the open web. In: ICML (2023) 4
41. Li,J.,Zhou,P.,Xiong,C.,Hoi,S.C.H.:Prototypicalcontrastivelearningofunsu-
pervised representations. In: ICLR (2021) 3Pretaining Data Diversity SSL 17
42. Misra, I., Maaten, L.v.d.: Self-supervised learning of pretext-invariant representa-
tions. In: CVPR (2020) 3
43. Nilsback,M.E.,Zisserman,A.:Automatedflowerclassificationoveralargenumber
of classes. In: ICVGIP (2008) 2, 10, 3
44. Noroozi,M.,Favaro,P.:Unsupervisedlearningofvisualrepresentationsbysolving
jigsaw puzzles. In: ECCV (2016) 3
45. Oord,A.v.d.,Li,Y.,Vinyals,O.:Representationlearningwithcontrastivepredic-
tive coding. arXiv preprint arXiv:1807.03748 (2018) 3, 6
46. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,
Fernandez,P.,Haziza,D.,Massa,F.,El-Nouby,A.,etal.:DINOv2:Learningrobust
visualfeatureswithoutsupervision.arXivpreprintarXiv:2304.07193(2023) 1,12,
14
47. Parkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.: Cats and dogs. In: CVPR
(2012) 2, 10, 3
48. Radosavovic,I.,Kosaraju,R.P.,Girshick,R.,He,K.,Dollár,P.:Designingnetwork
design spaces. In: CVPR (2020) 3
49. Ramtoula,B.,Gadd,M.,Newman,P.,DeMartini,D.:VisualDNA:Representing
andcomparingimagesusingdistributionsofneuronactivations.In:CVPR(2023)
11, 12, 8
50. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022) 8
51. Shi,Y.,Daunhawer,I.,Vogt,J.E.,Torr,P.,Sanyal,A.:Howrobustisunsupervised
representation learning to distribution shift? In: ICLR (2022) 4
52. Sun,C.,Shrivastava,A.,Singh,S.,Gupta,A.:Revisitingunreasonableeffectiveness
of data in deep learning era. In: ICCV (2017) 1
53. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the incep-
tion architecture for computer vision. In: CVPR (2016) 12
54. Thomee,B.,Shamma,D.A.,Friedland,G.,Elizalde,B.,Ni,K.,Poland,D.,Borth,
D.,Li,L.J.:YFCC100M:Thenewdatainmultimediaresearch.In:Commun.ACM
(2016) 2, 10
55. Tian, Y., Henaff, O.J., Van Den Oord, A.: Divide and contrast: Self-supervised
learning from uncurated data. In: ICCV (2021) 3, 4
56. Tian, Y., Krishnan, D., Isola, P.: Contrastive multiview coding. In: ECCV (2020)
2, 8
57. Tong, S., Chen, Y., Ma, Y., Lecun, Y.: Emp-ssl: Towards self-supervised learning
in one training epoch. arXiv preprint arXiv:2304.03977 (2023) 5
58. Torralba, A., Efros, A.A.: Unbiased look at dataset bias. In: CVPR (2011) 2
59. Van Gansbeke, W., Vandenhende, S., Georgoulis, S., Gool, L.V.: Revisiting con-
trastive methods for unsupervised learning of visual representations. In: NeurIPS
(2021) 4
60. VanHorn,G.,Cole,E.,Beery,S.,Wilber,K.,Belongie,S.,MacAodha,O.:Bench-
marking representation learning for natural world image collections. In: CVPR
(2021) 4
61. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama,
D., Bosma, M., Zhou, D., Metzler, D., et al.: Emergent abilities of large language
models. TMLR (2022) 15
62. Wu, Z., Xiong, Y., Yu, S.X., Lin, D.: Unsupervised feature learning via non-
parametric instance discrimination. In: CVPR (2018) 5
63. Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., Hu, H.: SimMIM:
A simple framework for masked image modeling. In: CVPR (2022) 318 Hammoud et al.
64. Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Wei, Y., Dai, Q., Hu, H.: On data scaling in
masked image modeling. In: CVPR (2023) 3
65. Zbontar,J.,Jing,L.,Misra,I.,LeCun,Y.,Deny,S.:BarlowTwins:Self-supervised
learning via redundancy reduction. In: ICML (2021) 2, 3, 4, 6
66. Zhai, X., Kolesnikov, A., Houlsby, N., Beyer, L.: Scaling vision transformers. In:
CVPR (2022) 3
67. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: ECCV (2016) 3,
5
68. Zhao,N.,Wu,Z.,Lau,R.W.H.,Lin,S.:Whatmakesinstancediscriminationgood
for transfer learning? In: ICML (2021) 4
69. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million
image database for scene recognition. T-PAMI (2017) 2, 10, 3
70. Zhou, P., Zhou, Y., Si, C., Yu, W., Ng, T.K., Yan, S.: Mugs: A multi-granular
self-supervised learning framework. arXiv preprint arXiv:2203.14415 (2022) 12
71. Zhuang, C., Zhai, A.L., Yamins, D.: Local aggregation for unsupervised learning
of visual embeddings. In: ICCV (2019) 5On Pretraining Data Diversity
for Self-Supervised Learning
Supplementary Material
In this supplementary material, we present additional experiments and insights
onourfindingspresentedinthemainpaper.First,wefurtherdeveloptheincon-
sistencies found within an incorrectly-normalized framework (Section A). Then,
weproposedifferentsettingsforouranalysisatscale(SectionB).Weextendour
analysis on label quantity in Section C. Finally, we introduce additional details
about our settings and implementations (Section D). For reproducibility of our
results, we will share on GitHub our codebase, along the fine-tuned parameters
and the data ordering files.
For improved readability, all references to the main paper Sections and Tables
in blue (e.g. “Section 1”).
A Importance of Normalization of Computation
In this section, we aim to complement our experiment in Section 6, providing
furtherproofhighlightingtheimportanceofhavinganormalizedcomputational
budget when evaluating the performance of SSL methods. In the following ex-
periments, we show that if computation is not normalized properly, one might
fall into unfair comparisons and misleading conclusions.
A.1 Increasing Total Computation
Inourfirstsetup,wepretrainSimCLR[10]andMoCoV3[14]onTinyImageNet
with various D, over a range of increasing amounts of budget C. We assume
D = D . We take the same subsets of Tiny ImageNet as in Section 5.1,
SSL task
whichconsistof10%,50%and100%ofthetrainingdata.WevaryC from5×106
to100×106,andwemeasuretheaccuracyofthepretrainedmodelsontheD
task
testset,resultsofwhichareshowninTable1.Notethatwerefertomodelsinthis
sectionusingthedatadiversityN insteadofpretrainingdiversityD,aseachcell
in Table 1 has a different D. Following our previous experiments, we argue that
comparison between different diversities only hold as long as the computation
is normalized. This implies that comparisons only hold within any given C. In
agreementwithourpriorfindings,thethirdrowwiththehighestdiversityalways
outperforms the lower pretraining diversity models on in-domain evaluation, for
both SimCLR and MoCoV3. However, only when we compare between different
columns, i.e. different amounts of computation, we may observe that models
pretrained with lower diversity may outperform higher diversity models. For
example, for both SimCLR and MoCoV3, the models pretrained with N =
50×103 and C = 10×106 outperform the models with higher data diversities2
Table 1: Pretraining Diversity With Increasing Computational Budget: We
showforbothSimCLR(left)andMoCoV3(right)thatincreasingpretrainingdiversity
always leads to better in-domain downstream accuracies, given that computation is
normalized,i.e.,comparisonsholdwithinthecolumnsofthetables.Comparingmodels
between different columns may lead to inconsistencies, where lower diversity models
with more computation obtain higher results than higher diversity models with less
computation.
SimCLR MoCoV3
N C (×106) N C (×106)
(×103) 5 10 25 50 100 (×103) 5 10 25 50 100
10 36.92 36.63 36.30 36.91 35.03 10 39.78 41.82 40.06 36.56 28.92
50 40.76 44.30 47.69 48.77 48.91 50 39.88 43.42 46.68 46.45 48.14
100 41.43 44.76 49.32 49.83 51.62 100 40.35 44.03 47.63 48.58 50.71
N = 100 × 103 but less computation C = 5 × 106. As a result, we see that
models with lower pretraining diversity can still outperform models with higher
diversity, given that more computation is used. This highlights the importance
of normalizing computation costs when evaluating the effects of diversity.
A.2 Epoch-based normalization
In Section 5.2, we adhered to a fixed computational budget of C = 50×106,
pretraining models on D = B for 800 epochs, and on B∪A for 400 epochs,
SSL
consideringthatthelatterdatasetwastwicethesize.Wefurtherdemonstratethe
importance of a computationally-normalized evaluation by exposing the incon-
sistencies of an alternative epoch-based normalization, hence in which networks
are trained for 400 epochs, regardless of the dataset size.
WeproposethisalternativescenarioinFigure1,wherethecompute-normalized
baseline(theblackdashedlineinFigure3)isreplacedwithanepoch-normalized
baseline (indicated by the red dashed line), obtained by pretraining for 400
epochs on B. Here, we observe that augmenting with additional samples consis-
tently enhances performance, irrespective of the data augmentation technique
used and whether the sample labels are in or out of distribution. This finding
does not align with the insights from Section 5.2, and we highlight that it does
not take into account the difference in costs for training models for the same
number of epochs, but on a dataset twice the size. Hence, this constitutes an
unfair comparison that may lead to incorrect conclusions, advocating for the
effectiveness of our computational-based normalization.
B Alternative Settings
B.1 Non-Contrastive Methods
For large-scale experiments in section 5.3 we only considered SimCLR [10] and
MoCoV3 [14], both of which are contrastive SSL methods. Here we show that
the results are consistent for the non-contrastive methods Barlow Twins [65]Title Suppressed Due to Excessive Length 3
Fig.1: Impact of Epoch Normalization on SSL Pretraining Performance:
Thisfigurecontrastsanepoch-normalizedbaseline(redline)withthetrainedmethods
inthemainpaper,Figure3.Underepochnormalization,wenoticecontrastingfindings,
i.e. more diverse trainings, irrespective of their origin (source, web, or synthetic) and
label distribution (in or out-of-distribution), consistently enhances performance. This
is an unfair comparison due to the greater costs of each augmented pretraining if
epochsarenormalized.Thisillustrateshowalternativenormalizationcanleadtowrong
conclusions compared to compute normalization. DINO B epoch-normalized baseline
is shown in text only (Acc. 41.14) for ease of visualization.
Table 2: Non-contrastive pretraining:Weexploretwomorepretrainingmethods,
namely, Barlow Twins and BYOL, for our large-scale pretraining experiments. Again,
the budget is set to C = 98 × 106, we find that our earlier conclusions still hold
here:(1)ImageNetpretrainingoutperformsYFCC100M(YFCC)pretrainingexceptfor
Places365 due to the distribution shift (2) Increased pretraining diversity D generally
correlates with improved downstream performance with the exception. Those findings
are consistent for both Barlow Twins and BYOL.
BarlowTwins BYOL
DSSL (×N 106)(×1D 0−3)ImageNet CarsA Fc lc ou wr .ac Py e↑ ts Places Food DSSL (×N 106)(×1D 0−3)ImageNet CarsA Fc lc ou wr .ac Py e↑ ts Places Food
ImageNet 0.128 1.31 57.17 51.51 85.84 75.81 45.87 63.72 ImageNet 0.128 1.31 61.82 46.62 85.84 80.28 46.91 67.01
ImageNet 1.281 13.0 65.40 59.5689.1683.89 49.19 70.42 ImageNet 1.281 13.0 68.39 52.5188.7784.75 49.96 73.52
YFCC 98.17 1000 57.85 44.76 83.46 67.21 50.01 65.20 YFCC 98.17 1000 60.73 42.78 84.38 68.63 50.92 68.63
C=98×106 C=98×106
and BYOL [28]. We highlight that these experiments are computationally in-
tensive, hence we explore a reduced setting with a single backbone and lower
D variability. We pretrained a ResNet-50 backbone using Barlow Twins and
BYOLonImageNetandthesamesubsetsfromSection5.3,aswellasonthefull
YFCC100M dataset, ensuring that the total compute is equal to a single epoch
on YFCC100M, i.e. C = 98×106. Again, we show linear evaluation on multi-
ple downstream datasets including ImageNet [18], Stanford Cars [37], Flowers-
102 [43], Oxford-IIIT Pets [47], Places365 [69], and Food-101 [5] in Table 2. In
accordance with Section 5.3, we observe that pretraining on higher diversities
leads to improved downstream accuracies when D = D , i.e. pretraining
SSL task
andevaluatingonImageNet.Also,thehighestpretrainingdiversityinImageNet
leads to the best results for all downstream datasets, except for Places365, for
which pretraining YFCC100M performs best, for which we refer again to distri-
bution distances analysis in Section 6. Again for these methods, the maximum
diversity of YFCC100M is not enough to diminish the effects of the domain gap
between the pretraining data and the datasets other than Places365.4
B.2 Different Architectures
We investigate how pretraining diversity interacts with varying backbone archi-
tecture sizes, as well as the total computational budget. With this, we aim to
highlight how different models react to pretraining diversity. To benchmark the
interaction of these factors, we focus on MoCoV3 and pretrain and evaluate on
ImageNet using C = 98×106 and the tripled amount C = 294×106. We use
two different architecture sizes for ViT backbones as well as ResNet backbones:
ViT-Small/16pairedwithViT-Base/16andResNet-50pairedwithResNet-101.
Results are shown in Table 3, and the first observation we make, is that
for any combination of architecture size and total amount of computation, the
model pretrained with largest amount of pretraining diversity D =13.0×10−3
alwayshasthehighestin-domaindownstreamperformance.Increasingpretrain-
ing diversity thus remains a reliable method to improve the quality of learned
representations, regardless of the architecture size. Secondly, we see that for
every diversity value, regardless of the backbone type or the amount of compu-
tation, an increase in backbone size, i.e. ResNet-50 to ResNet-101 or ViT-S/16
to ViT-B/16, leads to an increased performance. It is thus again of importance
toonlycomparemodelswithdifferentpretrainingdiversitiesforfixedmodelsize,
as we did with fixed computational budget.
Finally,keepinmindthatthelargerarchitecturesrequiremorecomputation,
which is not incorporated in C as this term only describes the number of images
that are seen during pretraining.
A note on MAE Masked Autoencoders (MAE) [30] is a Transformer-specific
pretraining method based on an autoregressive loss. This differs considerably
from what has been presented in Section 3, and it has significant impact on the
components needed for our normalized analysis. Indeed, for a C =98×106 bud-
get, MAE is far from providing optimal performance [30], making comparisons
unfair without incurring in unsubstainable costs. Also, the reconstruction task
used for supervision extracts features requiring a deep decoder for best perfor-
mance in linear probing [30], and it results in considerably better performance
with full finetuning exclusively. We will analyze pretraining diversity effects for
MAE in a future work.
B.3 Convergence insights
TheconvergenceofmodelstrainedonYFCCandImagenetleadingtoourInsight
3 must be further discussed (see main paper, Section 5.3). One may argue that
althoughC =98×106 maximizespretrainingdiversityonYFCC100M,thismay
not enough for making trained models fully converge. First, we highlight how
relevant literature sets similar training budget C = 100×106 requirements for
drawingreliableconclusions[10].Secondly,westresshowbringingtoconvergence
both models pretrained on Imagenet and YFCC100M would inevitably result in
a different sizing of the computational budget, preventing a fair evaluation. Al-
ternatively, increasing the computational budget for a complete convergence ofTitle Suppressed Due to Excessive Length 5
Table 3: Pretraining Diversity With Different Architectures Sizes:Weinves-
tigatehowpretrainingdiversity,totalcomputationbudgetandmodelarchitecturesize
interact for MoCoV3 when pretraining and evaluating on ImageNet. Regardless of C
andthearchitecturechoice,increasingpretrainingdiversityremainsareliablemethod
to improve downstream results. Further, increasing model size also seems to consis-
tently lead to better learned representations. Again, comparing pretraining diversity
values only holds when the model architecture and C are fixed.
MoCoV3
D C N D Network
SSL (×106) (×106) (×10−3) ResNet-50 ResNet-101 Vit-S/16 ViT-B/16
ImageNet 98 0.128 1.31 58.1 58.9 56.3 57.9
ImageNet 98 0.640 6.54 65.4 67.2 64.7 67.2
ImageNet 98 1.281 13.0 65.9 67.7 65.4 68.8
ImageNet 294 0.128 0.43 57.5 59.0 52.9 56.9
ImageNet 294 0.640 2.17 69.8 71.4 68.9 71.9
ImageNet 294 1.281 4.35 71.4 73.3 71.4 74.9
both settings would inevitably lead to the overfitting of the model trained on
Imagenet. This may lead to misleading results, since the overfitting-related loss
of performance could lead to wrong conclusions related to the distribution shift
impact. Instead, our setup guarantees a reliable evaluation, by preventing over-
fitting while training enough for a reasonable representation extraction. More-
over, we relate to relevant literature highlighting the importance of single-epoch
training for representation extractors [57].
C Additional Insights on Label Quantity
In Section 6 we considered how pretraining diversity affects the number of la-
belsnecessaryforthebestdownstreamImageNetaccuracieswhenpretrainedon
ImageNet (D = D ) or on YFCC100M (D ̸= D ). Here explore the
SSL task SSL task
setting where upstream and downstream data are the same, i.e., D =D ,
SSL task
and we repeat the experiment with models pretrained on various diversities on
ImageNet. Table 4 shows the in-domain results on ImageNet for SimCLR and
MoCoV3pretrainedwithResNet-50andViT-B/16backbones.Itisclearthatfor
in-domain evaluation, the models pretrained with largest pretraining diversity
always perform the best, regardless of the label quantity used for linear evalu-
ation. More interestingly, it is possible to achieve better performance with less
labelsifamodelispretrainedwithhigherD.Forexample,foreverycombination
of backbone and SSL method, the models pretrained with maximum diversity
D = 13.0 × 10−3 using 50% of the labels outperform the models pretrained
with D = 2.61×10−3 with 100% of the labels. Thus, if models are evaluated
or deployed in few-shot downstream tasks, it may be desirable to use models
pretrained with the highest pretraining diversity available.6
Table 4: Evaluating Network Accuracy With Varied Label Quantity: We
evaluate the accuracy of networks pretrained on ImageNet with various pretraining
diversities and evaluate with different labeling percentages of D = ImageNet. For
task
in-distributiondata,onecangetawaywithfewerlabelsusingmorediversepretraining
data.
SimCLR MoCoV3
Network D SSL (×N 106)(×1D 0−3) 5% A 10c %cur 5a 0c %y↑ 100% Network D SSL (×N 106)(×1D 0−3) 5% A 10c %cur 5a 0c %y↑ 100%
ImageNet 0.128 1.31 42.1 45.4 53.2 56.9 ImageNet 0.128 1.31 43.9 47.4 55.2 58.1
ImageNet 0.256 2.61 46.8 50.3 57.8 61.1 ImageNet 0.256 2.61 49.0 52.7 60.4 62.9
ResNet-50 ResNet-50
ImageNet 0.640 6.54 49.1 53.0 60.6 63.7 ImageNet 0.640 6.54 51.3 55.4 63.1 65.4
ImageNet 1.281 13.0 50.354.261.7 64.5 ImageNet 1.281 13.0 52.156.263.8 65.9
ImageNet 0.128 1.31 40.5 44.8 53.0 54.2 ImageNet 0.128 1.31 47.2 51.3 58.0 57.9
ImageNet 0.256 2.61 48.0 52.2 59.6 61.3 ImageNet 0.256 2.61 53.5 57.6 63.1 63.7
ViT-B/16 ViT-B/16
ImageNet 0.640 6.54 53.4 57.6 64.3 65.5 ImageNet 0.640 6.54 57.9 61.7 66.7 67.2
ImageNet 1.281 13.0 55.259.465.6 66.7 ImageNet 1.281 13.0 59.363.068.3 68.8
D Additional details
D.1 SSL Methods
In Section 3 we described a general framework for self-supervised pretraining
thatiscommontomanystate-of-the-artSSLmethods.Althoughallthemethods
we use in our experiments mostly follow this procedure, they do differ in loss
functions as well as in certain architectural choices. For each of the methods we
use for our experiments, we describe in depth the key aspects that specifically
define the SSL method and make them different from the introduced framework
in Section 3. Further details on the methods can be found in their respective
papers and repositories.
SimCLR [10], Barlow Twins [65] and VICReg [4] closely follow the general
framework,andmainlydifferinthelossfunctionL usedduringoptimization.
SSL
SimCLR makes use of the InfoNCE loss [45], which is applied to the representa-
tionsofeachpositivepairofsamplesinthebatch,andalsoincorporatesnegatives
from the current batch. Barlow Twins uses a loss function that makes the cross-
correlation matrix between the representations of the distorted samples as close
totheidentitymatrixaspossible.Asaresult,representationsofcorrelatedviews
of the same sample are forced to become similar, while redundancy between the
components of these vectors is minimized. The loss function used for VICReg is
a combination of the mean-squared euclidean distance between representations
with an additional variance and covariance term for feature decorrelation and
avoiding representation collapse.
BYOL[28],DINO[9]andMoCoV3[14]haveslightlymoreevidentdifferences
from the proposed framework, as they do not use shared parameters θ and θ
f g
forthefeatureextractorandprojectionheadbetweenthetwoaugmentedviews.
Insteadthetwoaugmentedviewspassthroughtwodifferentnetworks:astudent
network with feature extractor f and prediction head g , parameterised with
θf θg
θ and θ , and a teacher network with its own respective components f′ and
f g θ f′
g′ with unique parameters θ and θ . The weights θ and θ in the teacher
θ g′ f′ g′ f′ g′Title Suppressed Due to Excessive Length 7
network are an exponential moving average of the weights θ and θ in the
f g
student network. The three methods differ in how they compute L from
SSL
the representations z from the student and z from the teacher. For BYOL,
A B
after the correlated views are passed through the two networks, an additional
predictor network q , parameterised with θ , tries to predict the representation
θq q
of the teacher network z from the output of the student network as q (z ),
B θq A
and the mean squared error between the teacher representation and the student
prediction is minimised. DINO performs knowledge distillation in the student
by minimising the cross-entropy loss between the direct outputs z and z .
A B
MoCoV3usesthestudentandteachernetworktogeneraterepresentationsfrom
the augmented views called queries z and keys z , and stores the keys in a
B B
queue. The contrastive InfoNCE loss is again used as SSL objective, and uses
matching queries and keys as positive samples and the recent keys from the
dictionary as negative samples. For all three methods, a stop-gradient operator
is applied after the teacher network, to avoid any weight updates in the teacher
network during backpropagation.
SwAV [8] does share weights for f and g between correlated views, but
θf θg
instead relies on additional components. First, the representations of different
views z and z are assigned to prototype vectors, resulting in codes q and
A B A
q . The prototype vectors are trainable vectors and represent the notion of
B
clusters. A swapped prediction problem is solved afterwards, where the code of
one augmented view is predicted using the other view. The swapped prediction
problemisusedtodefinethelossasL (z ,z )=ℓ(z ,q )+ℓ(z ,q ),where
SSL A B A B B A
ℓ measures the fit between features and codes.
D.2 Data Collection Strategies
This section outlines the data collection strategies for our three approaches de-
tailedinSection5.2:Source,Web,andSynthetic.Webasethesestrategiesonthe
Base dataset B (introduced in 5.2), consisting of half of ImageNet100, totaling
65,000 samples.
SourceDataset WeexpandthedatasetBbyintegratingtheremaininghalfof
ImageNet100, forming AIn . For AOut , we begin by selecting 100 random,
Source Source
non-overlapping classes from ImageNet. We then gather 65,000 corresponding
samples from these classes and add them to B.
Web Dataset We utilize three search engines—Flickr, Bing, and Duck-
DuckGo—togatherwebsampleswhileemployingSafe-Searchforcontentappro-
priateness.Ourqueries,basedonclassnames,arecarefullycraftedtoavoidam-
biguity. For AIn , we collect approximately 100,000 samples from ImageNet100
Web
classes,selectingthetop65,000forinclusioninB.Similarly,forAOut,wefollow
Web
the same process for the 100 randomly selected classes from the Source dataset.
Synthetic Dataset Forsyntheticsamplegeneration,weemployStableDiffu-
sion V2.1 (SDV2.1). Using the prompt ’A photo of a class_name’, we generate
images for each class in ImageNet100 for AIn and the 100 distinct classes
Synthetic
fromImageNetforAOut .Eachclasscontributes650images,totaling65,000
Synthetic8
samples. We utilize the DPMSolver++ scheduler with start and end β values of
0.00085 and 0.012, respectively. The guidance scale is set at w = 7.5, and each
image is generated in 50 steps.
D.3 Details on Distribution Distances
In Section 6 of our study, we explored the relationship between pretraining
datasets,specificallyImageNetandYFCC100M,anddownstreamdatasets,which
include ImageNet, Stanford Cars, Flowers102, OxfordIIITPets, Places365, and
Food101. To quantitatively measure the distance between these datasets, we
employed two distinct metrics: VisualDNA (VDNA) and the Fréchet Inception
Distance (FID).
Our methodology for calculating distribution distances involved selecting a
substantial number of samples from each dataset. We used 50,000 samples from
eachdatasetforcomputingthedistributiondistances,ifthedatasetiscomposed
from less than 50,000 samples, the whole dataset is used. This approach en-
sured a robust and comprehensive analysis of the dataset distributions. For the
implementation of VisualDNA, we utilized two different architectures: MUGS
ViT-B/16, as recommended by the original paper [49], and DinoV2 ViT-B/16.
The FID scores were computed using a standard approach with an Inception
network, as detailed in [33].
Theresults,asdiscussedinSection6,revealedconsistentfindingsacrossboth
VDNA and FID metrics. Our analysis showed that a greater distance between
theupstreamanddownstreamdatasetscorrelatedwithadecreaseindownstream
classification accuracy.
E Implementation
Inallourexperiments,wehaveutilizedthesolo-learnlibrary[16]asourmain
codebase. For the ImageNet100 and CIFAR100 experiments, we used to the pa-
rametersprovidedbysolo-learn.InthecaseofImageNet,whilewebeganwith
the parameters provided in the original papers, we made slight modifications to
optimize performance. These modifications included changes to the number of
warmup epochs and an adjustment of the learning rate. For the YFCC100M
dataset, we found the parameters optimized for ImageNet to be the most effec-
tive,whereasforTinyImageNet,weusedtheCIFAR100parametersprovidedby
solo-learn.
To create different fractions of each dataset, our first step involved the cre-
ationofanH5filecontainingallimagepaths.Thisfileisthenshuffledandsaved.
When a specific percentage of the data is required for our SSL pretraining, we
simply select the first k% of the image paths from this H5 file. Since we use a
fixed computational budget, we scaled the number of epochs accordingly. This
scalingisachievedbyafactorof1/k×100.Forexample,ifweutilized10%ofthe
data for pretraining, we would increase the base number of epochs by a factor
of 10.