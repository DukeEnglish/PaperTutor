Probabilistic Forecasting with Stochastic Interpolants and Fo¨llmer Processes
YifanChen*1 MarkGoldstein*1 MengjianHua*1
MichaelS.Albergo1 NicholasM.Boffi1
EricVanden-Eijnden1
Abstract deterministically,wherebythegoalistocomputeasingle
predictionforthefuturestate(Giannakisetal.,2023), or
We propose a framework for probabilistic fore-
probabilistically,wherebythegoalistopredictadistribu-
castingofdynamicalsystemsbasedongenerative
tionoverfuturestatesconsistentwiththecurrentinforma-
modeling. Givenobservationsofthesystemstate
tion (Gneiting & Katzfuss, 2014). Probabilistic forecast-
overtime,weformulatetheforecastingproblem
ingisthenaturalformulationwhentheunderlyingsystem
assamplingfromtheconditionaldistributionof
dynamics are stochastic, when the full system state can-
thefuturesystemstategivenitscurrentstate. To
notbemeasured,orwhenmeasurementsarecorruptedby
thisend,weleveragetheframeworkofstochas-
noise,asisthecaseformostreal-worldsystemsofinterest.
ticinterpolants,whichfacilitatestheconstruction
Moreover,whiledeterministicforecastingappearstobea
ofagenerativemodelbetweenanarbitrarybase
simplerproblem,recentworkshaveuncovereddifficulties
distributionandthetarget. Wedesignafictitious,
withdeterministicforecastingmethodswhentheunderlying
non-physical stochastic dynamics that takes as
dynamicsarechaotic(Jiangetal.,2023).
initialconditionthecurrentsystemstateandpro-
ducesasoutputasamplefromthetargetcondi- Motivatedbytherecentsuccessofgenerativemodelsbuilt
tionaldistributioninfinitetimeandwithoutbias. upondynamicaltransportofmeasure,suchasscore-based
Thisprocessthereforemapsapointmasscentered diffusion(Hoetal.,2020;Songetal.,2020),flowmatching
atthecurrentstateontoaprobabilisticensemble (Lipmanetal.,2022),andstochasticinterpolants(Albergo
offorecasts. Weprovethatthedriftcoefficienten- & Vanden-Eijnden, 2022; Albergo et al., 2023), here we
teringthestochasticdifferentialequation(SDE) introduceagenerativemodelingapproachforprobabilistic
achieving this task is non-singular, and that it forecasting. This approach maps the current state of the
canbelearnedefficientlybysquarelossregres- systemontotheensembleofpossiblefutureoutcomesafter
sionoverthetime-seriesdata. Weshowthatthe afixedtimelag. Fromatransportperspective,thisrequires
drift and the diffusion coefficients of this SDE pushingapointmassmeasureontoaprobabilitydistribu-
canbeadjustedaftertraining,andthataspecific tionwithlargersupport. Inwhatfollows,weshowthatthe
choicethatminimizestheimpactoftheestima- interpolantframeworkenablesustodesignanartificialdy-
tionerrorgivesaFo¨llmerprocess. Wehighlight namicsthatperformsthistaskusingastochasticdifferential
the utility of our approach on several complex, equation. Inpractice,thedriftfieldsenteringtheSDEswe
high-dimensional forecasting problems, includ- introduce can be learned via square loss regression. We
ingstochasticallyforcedNavier-Stokesandvideo alsoshowthatthediffusioncoefficientintheseSDEscan
predictionontheKTHandCLEVRERdatasets. betuneda-posteriori(i.e. withouthavingtoretrainadrift).
Weshowthataspecificchoicethatminimizestheimpact
oftheestimationerrorrecoversaFo¨llmerprocess(Fo¨llmer,
1.Introduction 1986), a specific instantiation of the Schro¨dinger bridge
problem(Schro¨dinger,1932;Le´onard,2014)inwhichthe
Forecasting the future state of a dynamical system given basedistributionisapointmassmeasure.
completeorpartialinformationaboutthecurrentstateisa
To demonstrate the utility and scalability of our ap-
ubiquitousproblemacrossscienceandengineering,withdi-
proach,weconsiderseveralexamples: anillustrativelow-
verseapplicationsinclimatemodeling(Smagorinsky,1963;
dimensionalscenariobasedonamulti-modaljumpdiffu-
Palmer et al., 1992; Gneiting et al., 2005; Pathak et al.,
sionprocess,ahigh-dimensionalproblemgeneratedbya
2022),fluiddynamics(Buaria&Sreenivasan,2023),video
stochasticNavier-Stokesequationonthetorus,andvideo
prediction(Opreaetal.,2022;Finnetal.,2016;Leeetal.,
generationontheKTH(Schuldtetal.,2004)andCLEVRER
2018),andextrapolationoftimeseriesdata(Lim&Zohren,
datasets(Yietal.,2019). InthecaseofNavier-Stokes,we
2021;Masinietal.,2023). Forecastingcanbeperformed
1
4202
raM
02
]GL.sc[
1v42731.3042:viXraProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
showthatourprobabilisticforecastingmethodiscapable reducestofindingatractablefinite-dimensionalapproxima-
ofreproducingquantitativemetrics,suchastheenstrophy tion. Inallformulations,thekeyistoidentifyandlearnac-
spectrumofthedataset,usingeitherhigh-orlow-resolution curaterepresentationsofthedynamics,forexamplethrough
measurement of the current system state. In each case, nonparametric approaches such as diffusion maps (Berry
we highlight that we are able to obtain diverse samples etal.,2015)andkernelregression(Alexander&Giannakis,
consistentwiththeconditionaldistributionofinterest,and 2020),orparametricapproachessuchaslinearregression,
demonstratetheneedforprobabilistic,asopposedtodeter- dynamicalmodedecomposition(Kutzetal.,2016),neural
ministic, forecasting. For the video generation tasks, we networks(Lietal.,2021;Guetal.,2021)andoperators(Lu
showthatourlearnedmodelsaremoreeffectivethanstan- etal.,2021;Jiangetal.,2023;Lietal.,2020). Ultimately,
dardconditionalgenerativemodeling. Wealsoshowthat theseapproachesproduceafunctionthatmapsthecurrent
thisforecastingprocedurecanbeiteratedautoregressively statetoasingleoutputwhichisthedeterministicforecast.
withoutretrainingtocomputeapredictedtrajectory. Our Many methods under this category train models with the
maincontributionscanbesummarizedasfollows: MSE(meansquareerror)orRMSE(rootmeansquareerror)
asthe objective, butthese lossesmay bepoor signalsfor
• Wedesignnewgenerativemodelsforprobabilisticfore- trainingforecastersforchaoticsystems(Jiangetal.,2023).
castingbasedonstochasticdifferentialequations(SDEs)
Fordynamicsthatareinherentlystochastic,orwithincom-
thatmapapointmassmeasuretoadistributionwithfull
pleteinformation,astochasticforecastisimportanttoincor-
supportbyincorporatingstochasticityinaprincipledway,
porateuncertainties. Thisstochasticitycanbeintroduced
enablingustoinitializetheSDEdirectlyatthemeasured
byfittingaprobabilisticmodel,suchasastochasticprocess
systemstate.
oragraphicalmodel,todata. Theycanalsobeapproached
fromtheperspectiveofstochasticKoopmanoperators(Wan-
• We prove that the drifts entering these SDEs can be
ner&Mezic,2022;Zhao&Jiang,2023). Althoughmany
learnedviasquarelossregressionoverthedata,andthat
approacheshavebeenproposed,mostofthemtargetlow-
theresultinglosshasboundedvariance.
dimensionalproblemsorsimpleconditionalstatisticssuch
• WeshowthatthedriftandnoisetermsintheseSDEscan asGaussians.
beadjustedpost-training,andthatthespecificchoiceof
noise that minimizes the Kullback-Leibler (KL) diver-
gencebetweenthepathmeasuresoftheexactforecasting ProbabilisticForecastingwithGenerativeModels. Re-
processandtheestimatedoneisrealizable. Weshowthat cently, generative modeling techniques in machine learn-
thisdriftanddiffusionpairgivesaFo¨llmerprocess. ing have received increasing attention for handling high-
dimensional,complexdistributions. Probabilisticforecast-
• Wevalidateourtheoreticalresultsempiricallyonseveral ing can be seen as a form of the conditional generation
challenginghigh-dimensionalforecastingtasks,includ- problem. Variousconditionalgenerativemodels,including
ingtheNavier-Stokesequationandvideoprediction. conditionalGAN(Mirza&Osindero,2014), VAE(Sohn
etal.,2015),andnormalizingflows(deBe´zenacetal.,2020;
2.RelatedWork Kidgeretal.,2021)havebeendeveloped.Morerecently,dif-
fusiongenerativemodels(Hoetal.,2020;Songetal.,2020)
Thereisavastbodyofliteratureonforecasting. Methodolo- havegainedpopularityduetotheirstate-of-the-artperfor-
giescanbebroadlyclassifiedintotwomaincategories:fore- mance. Thestochasticinterpolantmethodology,whichis
castingasingleoutput,typicallyachievedthroughregres- relatedtocontemporaryworklikeflowmatching(Lipman
sionandoperatortheoreticapproaches(Kutzetal.,2016; etal.,2022),isageneralframeworkthatencompassesdif-
Alexander&Giannakis,2020;Lietal.,2021),andproba- fusionmodels. Intheliterature,therehasbeensomework
bilisticforecasting(Gneiting&Katzfuss,2014),basedon todevelopconditionalmodelsfortimeseries(Rasuletal.,
stochasticandgenerativemodeling. 2021; Lienen et al., 2023), including diffusion (Ho et al.,
2022;Blattmannetal.,2023)andflowmatchingmodelsfor
DeterministicvsProbabilisticForecasting. Fortheclass videoprediction(Davtyanetal.,2023). Theseapproaches
of methodologies focused on generating a single output, learnODEsorSDEsthatmapaGaussianbasetothecon-
awidelyusedapproachisregressionorsupervisedlearn- ditionaldistributionofinterest. Therearealsoworksthat
ing,whichentailsdirectlylearningamapinthestatespace. forecaststochasticdynamicsbyaddingnoisetotheneural
One can also work in the space of probability densities networklayers(Cachayetal.,2023)usingtechniquessuch
or functionals on the state space, under the setting of the asdropout. Incontrast,weincorporatestochasticityrigor-
Frobenius-PerronorKoopmanoperatorapproach(Dellnitz ouslythroughinterpolantsanddirectSDEmodeling. Our
&Junge,1999;Kaiseretal.,2021);thisleadstolineardy- constructionofthestochasticgenerativemodel,whichmaps
namics in an infinite-dimensional space, and forecasting thecurrentstatetothedistributionoftheforecastedstate,is
2ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
newandcanbeseenasadirectstochasticextensionofthe Assumethatthistime-seriesisastationaryprocessandthat
deterministicmapapproachthatispredominantinsingle the law of successive observations (x ,x ) is cap-
kτ (k+1)τ
outputforecasting. tured by the joint PDF ρ(x ,x ). Then sampling
kτ (k+1)τ
fromρ (·|x )producestheensembleofforecastsx
c kτ (k+1)τ
Fo¨llmerprocesses. TheconstructionofSDEsthatmap giventheobservationx kτ.
apointmasstoatargetdistributiondatesbackatleastto
theFo¨llmerprocess(Fo¨llmer,1986),whichisaparticular Signal recovery from corrupted data. Suppose that,
solutionoftheSchro¨dingerbridgeproblem(Schro¨dinger, givencleandatax 1 ∈Rd,weobservethecorruptedsignal
1932; Le´onard, 2014; Chen et al., 2021) that minimizes x 0 ∈ Rd (e.g. a low-resolution or noisy image obtained
the relative entropy with respect to the Wiener process. fromahigh-resolutionimage). Ifweassumethatthejoint
This approach has the desirable feature that it offers an PDFof(x 0,x 1)isρ(x 0,x 1),thensamplingfromρ c(·|x 0)
entropy-regularizedsolutiontotheoptimaltransportprob- producestheensembleofcleandatax 1consistentwiththe
lem. Forthisreason,theconceptsoftheFo¨llmerprocess corruptedsignalx 0.
andtheSchro¨dingerbridgehavefoundmanyapplicationsin
samplingdensitieswithunknownnormalizationconstants Probabilisticforecastingfromnoisyobservations. We
(Zhang&Chen,2021;Huangetal.,2021;Jiaoetal.,2021; can combine the previous two setups if we assume that
Vargasetal.,2023),generativemodeling(Tzen&Raginsky, wearegivenadiscrete-time-series{x kτ,x˜ kτ} k∈Zwithx kτ
2019;Wangetal.,2021;DeBortolietal.,2021;Liuetal., the clean data and x˜ kτ the corrupted observation at time
2023;Peluchetti,2023;Shietal.,2024)aswellasstochastic kτ. If we assume that the joint PDF of (x˜ kτ,x (k+1)τ) is
analysisandfunctionalinequalities(Lehec,2013;Eldan& ρ(x˜ kτ,x (k+1)τ), then sampling from ρ c(·|x˜ kτ) produces
Lee,2018;Eldanetal.,2020). Inthisarticle,weshowthat the ensemble of clean forecasts x (k+1)τ given the noisy
the stochastic interpolant framework offers a simple way observationx˜ kτ.
toconstructnewtypesofFo¨llmerprocesses. Inaddition,
wegiveanewinterpretationofFo¨llmerprocessesasmin- 3.2.GenerationwithStochasticInterpolants
imizers of a KL divergence between the path measure of
Thegenerativemodelsthatwedevelopherearebasedon
anSDEthatforecastsexactlyandtheapproximate,learned
stochasticdifferentialequationsthatmapafixedinitialcon-
SDE.TheseresultssupporttheusesofFo¨llmerprocessesin
ditionX =x tosamplesfromtheconditionaldistribu-
probabilisticforecasting. s=0 0
tionX ∼ ρ (·|x ). TowardsthedesignofsuchSDEs,
s=1 c 0
wefirstintroducethestochasticinterpolant
3.SetupandMainResults
I =α x +β x +σ W (2)
s s 0 s 1 s s
3.1.ConditionalPDF
where (x ,x ) ∼ ρ(x ,x ) and W = (W ) is a
0 1 0 1 s s∈[0,1]
Assumethatwearegivenajointprobabilitydensityfunction WienerprocesswithW ⊥(x 0,x 1). Inaddition,weimpose
(PDF)ρ(x ,x )supportedonRd×Rdandstrictlypositive that α,β,σ ∈ C1([0,1]) satisfy the boundary conditions
0 1
everywhere. Our aim is to design a generative model to α 0 = β 1 = 1andα 1 = β 0 = σ 1 = 0. Tofacilitatesome
sampletheconditionalPDFofx 1givenx 0: calculations,weassumethatβ˙ s > 0foralls ∈ (0,1]and
σ˙ <0foralls∈[0,1].Herewewilluseα =σ =1−s,
s s s
ρ (x |x )=
ρ(x 0,x 1)
>0, (1)
andβ
s
= sorβ
s
= s2 (seeAppendixA.1). Thissecond
c 1 0 ρ 0(x 0) choiceforβ shassomeadvantagesthatwediscussbelow.
whereρ (x )=(cid:82) ρ(x ,x )dx >0.1 Phrasingtheques- The boundary conditions on α,β, and σ guarantee that
0 0 Rd 0 1 1
I = x and I = x , so that the probability distri-
tionthisgeneralwayallowsustoconsiderseveralinstantia- s=0 0 s=1 1
bution of I |x bridges the point mass measure at x to
tionsofinterestbyappropriatelydefiningthejointPDFρ. s 0 0
ρ (·|x )assvariesfrom0to1. Thefollowingresultshows
Inparticular,westudythefollowingthreeproblemsettings: c 0
that this probability distribution is also the law of the so-
lution to a specific SDE that can be used as a generative
Probabilisticforecasting. Supposethatwearegivenadis-
model.
cretetime-series{...,x −τ,x 0,x τ,...}={x kτ} k∈Z with
eachx
kτ
∈Rdcontaining,forexample,dailyweathermea- Theorem3.1. Letb s(x,x 0)betheuniqueminimizerover
surementsorvideoframes,acquiredeverylag-timeτ >0. allˆb s(x,x 0)oftheobjective
(cid:90) 1
1Ourapproachactuallyrequiresthatx 1|x 0hasapositiveden- L [ˆb ]= E(cid:2) |ˆb (I ,x )−R |2]ds, (3)
sity,but(x ,x )doesnothaveto.Inparticular,wecantargetany b s s s 0 s
0 1 0
densityρ (x ) > 0bydrawing(x ,x )fromthejointdistribu-
tionµ(dx1 0,d1 x 1)=µ 0(dx 0)ρ 1(x 1)0 dx 11 withanyµ 0(e.g.apoint where E denotes an expectation over (x 0,x 1) ∼ ρ and
mass),sincewethenhaveρ c(x 1|x 0)=ρ 1(x 1). W with (x 0,x 1) ⊥ W, I
s
is given in (2), and where we
3ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
defined2 aresuchthatLaw(Xg) = Law(X ) = Law(I |x )forall
s s s 0
R =α˙ x +β˙ x +σ˙ W , (4) (s,x )∈[0,1]×Rd. InparticularXg ∼ρ (·|x ).
s s 0 s 1 s s 0 s=1 c 0
ThenthesolutionstotheSDE
ThistheoremisproveninAppendixA.4,whereweexplain
whytheconditionsong guaranteethattheSDE(9)iswell-
dX =b (X ,x )ds+σ dW , X =x , (5) s
s s s 0 s s s=0 0
posed. WorkingwiththisSDErequiresthescore∇logρ .
s
are such that Law(X ) = Law(I |x ) for all (s,x ) ∈ Interestingly, this score can be expressed in terms of the
s s 0 0
[0,1]×Rd. Inparticular,X
s=1
∼ρ c(·|x 0). drift b s. A direct calculation reported in Appendix A.4
showsthat
ThistheoremisproveninAppendixA.2. Theresultisfor-
∇logρ (x|x )=A [β b (x,x )−c (x,x )], (10)
mulatedinawaythatistailoredtopracticalapproximation s 0 s s s 0 s 0
ofthedriftb ,sincetheobjective(3)canbeestimatedem-
s where
piricallybygeneratingsamplesofI andR usingsample
s s
d √ A =[sσ (β˙ σ −β σ˙ )]−1,
pairs(x ,x )fromρandrealizationsofW = sz with s s s s s s
0 1 s (11)
z ∼ N(0,Id). Thatis, b s maybelearnedoverneuralnet- c s(x,x 0)=β˙ sx+(β sα˙ s−β˙ sα s)x 0.
worksbyminimizingthesimulation-freeloss(3)overthe
parameters. Itiseasytoseethattheminimizerisgivenby Using(10)in(8)showsthat,toworkwiththeSDE(9),we
canestimatebfirstandthenadjustboththenoiseamplitude
b (x,x )=Ex0[R |I =x], (6) g andthedriftbg a-posterioriwithouthavingtoretrainb.3
s 0 s s s
This offers flexibility at sampling time that can be lever-
where Ex0[·|I
s
= x] denotes an expectation over x
1
∼
agedtomaximizeperformance,asshowninournumerical
ρ (·|x )andW withx ⊥W conditionedontheeventI =
c 0 1 s experimentsbelow.
x. Thedrift(6)iswell-definedforall(s,x,x )∈[0,1]×
0
Rd×Rd,andweshowinAppendixA.3thatenforcingβ˙ =
0 3.4.KLOptimizationandFo¨llmerProcesses
0offersadditionalcontrolontheboundednessandLipschitz
constantofbattheinitialtime. Weobserveempiricallythat In light of Theorem 3.2, it is natural to ask if a specific
thishascomputationaladvantagesatbothoptimizationand choiceofg isoptimalinasuitablesense. Toprovideone
s
samplingtime;seetheexperimentsinAppendixB. answer to this question, we consider the KL divergence
betweenthepathmeasureoftheprocessXg =(Xg)
s s∈[0,1]
3.3.GeneralizationswithTunableDiffusion (whichsolvestheidealSDE(9))andthepathmeasureofthe
processXˆg = (Xˆg) (whichsolvesanapproximate,
s s∈[0,1]
We now show that learning the drift coefficient (6) gives learnedversionof(9)obtainedthroughanestimateˆbofb).
accesstoabroadersetofSDEstouseasagenerativemodel
BecauseLaw(Xg)=Law(I |x )forall(s,x )∈[0,1]×
beyondjust(5),andthatselectingfromthemhasappealing s s 0 0
Rd,thisKLdivergenceisgivenby(seeAppendixA.5for
theoreticalmotivation.
details)
d
Let ρ (x|x ) be the PDF of X = I |x . From (5), ρ
s 0 s s 0 s (cid:90) 1 |1+ 1β A (g2−σ2)|2L
solvestheFokker-Planckequation D (Xg||Xˆg)= 2 s s s s s ds (12)
KL 2|g |2
0 s
∂ ρ +∇·(b (x,x )ρ )= 1σ2∆ρ . (7)
s s s 0 s 2 s s whereL
s
=Ex0(cid:2) |ˆb s(I s,x 0)−b s(I s,x 0)|2(cid:3) . Eq.(12)mea-
Givenacandidatediffusioncoefficientg ,wecanusethe sureshowtheestimationerroronbimpactsthegenerative
s
identity 1σ2∆ρ = 1g2∆ρ −1(g2−σ2)∇·(ρ ∇logρ ) process,andassuchitisnaturaltominimizeitoverg.Since
2 s s 2 s s 2 s s s s
to trade diffusion for transport in (7). This construction L isindependentofg ,thisminimizationcanbeperformed
s s
leadstoafamilyofSDEswithtunablediffusion analytically. Theresultisthat(12)isminimizedifweset
Theorem 3.2. Given any g ∈ C0([0,1]) such that g s =g sF with
lim s−1[g2−σ2]andlim g2σ−1exist,define
s→0+ s s s→1− s s (cid:12) (cid:12)1/2
gF =(cid:12)2sσ (β−1β˙ σ −σ˙ )−σ2(cid:12) . (13)
s (cid:12) s s s s s s(cid:12)
bg(x,x )=b (x,x )+ 1(g2−σ2)∇logρ (x|x ) (8)
s 0 s 0 2 s s s 0
This expression is well-defined for all s ∈ [0,1], since
where b s(x,x 0) is the minimizer of (3) given in (6) and lim 2sβ−1β˙ <∞becauseβ isdifferentiableats=
d s→0+ s s s
ρ s(x|x 0)isthePDFofX s =I s|x 0. Thenthesolutionsto 0byassumption. Theresultin(13)isalsoamenabletoan
theSDE interestinginterpretation:
dXg =bg(Xg,x )ds+g dW , Xg =x , (9) 3Using(10)in(8)requiressomecareats=0ands=1dueto
s s s 0 s s s=0 0
thefactor[sσ ]−1inA ,butthisleadstonoissueseeAlgorithm2
s s
2Hereandbelowthedotdenotesderivativewithrespecttos. andalsoAppendicesA.1andA.4.
4ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
√
Theorem 3.3. If β /[ sσ ] is non-decreasing, then the anensembleofforecastswithstatisticsconsistentwiththose
s s
processXF ≡XgF thatsolves(9)withg =gFisaFo¨llmer ofthetime-seriesseenduringtraining.Thisprocesscanalso
s s
process. beiteratedautoregressivelybysettingXˆk+1 =Xˆk and
s=0 s=1
bysolvingtheSDE(16)withkreplacedbyk+1togetan
This theorem is proven in Appendix A.6. To understand approximatesampleXˆk+1ofρ (·|x ). Thisiteration
s=1 c (k+1)τ
its significance, recall that the Fo¨llmer process is the so- does not require any additional training, since it uses the
lutiontotheSchro¨dingerbridgeproblemwhenoneofthe sameˆb . TheseproceduresaresummarizedinAlgorithms1
s
endpointmeasuresisapointmass(inthiscase,atx 0). As and2. ThefirststeptogetX
s=s1
=Xˆ
1
inAlgorithm2is
such, it offers an entropy-regularized solution to the op- consistentanddesignedsothatitavoidscomputingˆbg ,
s=0
timal transport problem. The Fo¨llmer process is usually
sinceusing(8)and(10)canexhibitnumericalsingularities
defined by minimizing its KL divergence with respect to even though bg is well defined when the conditions of
s=0
theWienerprocesssubjecttoconstraintsontheendpoints.
Theorem3.2aremet.
Theorem3.3offersageneralizationandnewinterpretation
of this process as the minimizer of the KL divergence of
Algorithm1Training
theexactforecastingprocessfromtheestimatedone,which
is more tailored to statistical inference. For more details 1: Input:DatasetS K ={x kτ}K k=+ 01;minibatchsizeK′ ≤
aboutFo¨llmerprocessesandtheSchro¨dingerbridgeprob- K;coefficientsα s,β s,σ s.
lemwereferthereadertoAppendixA.5. Wealsotestthe 2: repeat
performanceof(13)inAppendixB.2. 3: ComputetheempiricallossLK b [ˆb]in(14).
4: TakegradientsteponLK
b
[ˆb]toupdateˆb s.
3.5.Implementation 5: untilconverged
6: Return: driftˆb s(x,x 0).
Forconcreteness,weconsidertheproblemofprobabilistic
forecasting,butthealternativeproblemsettingscoveredby
ourframeworkcanbehandledsimilarly.Giventhetruncated
timeseriesS ={x }K+1withK ∈NandK′ ≤K,we Algorithm2Sampling
K kτ k=0
canapproximatetheobjectivein(3)bytheempiricalloss 1: Input: Observationx kτ;modelˆb s(x,x 0);noisecoeffi-
LK b [ˆb]= K1 ′ (cid:88) (cid:90) 1 |ˆb s(I sk,x kτ)−R sk|2ds, (14) c iii den ηt ng ∼s,g Nr (id 0,s I0 d)= fo0 r< n=s 1 0·· :· N< −s N 1.=1withN ∈N;
k∈B K′ 0 2: Set∆s n =s n+1−s n,n=0:N −1. √
where B
K′
⊂ {0 : K} is a subset of indices of cardinal- 3: SetXˆ 1 =x kτ +ˆb s0(x kτ,x kτ)∆s 0+σ s0 ∆s 0η 0.
ityK′and 4: forn=1:N −1do
RI sk
k
= =α α˙sx xkτ + +β β˙sx x(k+1)τ + +√ √s sσσ ˙sz zk
(15)
75 6 :: : enSC deo ft om X rˆp nu +te 1ˆb =g sn X( ˆXˆ nn +, ˆbx g sk nτ () Xˆfr no ,m x k( τ8 )) ∆an sd n+(1 g0 s) n. √ ∆s nη n.
s s kτ s (k+1)τ s k
8: Return: Xˆ N+1 ∼ρˆ c(·|x kτ)≈ρ c(·|x kτ).
withz ∼N(0,Id),z ⊥(x ,x ). Toarriveat(14)
k k kτ (k+1)τ
√
d
we used that W = sz with z ∼ N(0,Id) at all s ∈
s
[0,1]. In (14) and (16) below, the physical lag τ > 0 is
fixed,whilesvariesover[0,1],andtheintegraloverscan 4.Numericalillustrations
beapproximatedviaanempiricalexpectationoverdraws
of s ∼ U([0,1]). By approximating ˆb in an expressive In what follows, we test our proposed method in several
applicationdomains. Foralltests,aninterpolantwithco-
parametricclasssuchasaclassofneuralnetworks,wecan
efficientsα = 1−s,σ = ε(1−s)forsomeε > 0,and
optimize(14)overtheparameterswithstandardgradient- s s
β =s2isused. Theconditionthatβ˙ =0empiricallyen-
based methods. This can be performed by batching over s 0
suresthatthenormoftheparametergradientsusedtotrain
subsequences in the available time series, or via online
our neural networks are well behaved. We report results
learningifastreamofdataiscontinuouslyobserved.
withthediffusioncoefficientin(16)chosentobeg =σ ,
s s
Having learned an approximationˆb, we can construct an aswefoundthattheimpactoflearningˆb wellbychoice
s
approximationofˆbg using(8),(10),and(11). Wemaythen oftherightinterpolantoutweighedtheeffectofvaryingthe
formourmodelgiventhenewobservationx bysolving SDE for the systems we study. For additional numerical
kτ
dXˆk =ˆbg(Xˆk,x )ds+g dW , Xˆk =x , (16)
experimentswithα
s
= 1−s,σ
s
= ε(1−s)andβ
s
= s,
s s s kτ s s s=0 kτ andwithg =gF,wereferthereadertoAppendixB.2. In-
s s
withvariousrealizationsofthenoiseW togenerateasetof vestigationoftheFo¨llmerSDEdescribedinTheorem3.3,in
s
Xˆk thatapproximatelysamplesρ (·|x ). Thisgenerates boththeoryandexperiment,willbesavedforfuturework.
s=1 c kτ
5ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
4.1.Multi-modaljumpdiffusionprocess AngularKDE GroundTruthKDE ForecastedKDE
truth
Our first example is syn- forecast
thetic, and consists of fore-
casting a two-dimensional
jump-diffusionprocesswith
invariant PDF given by a
Gaussian mixture (Fig. 1).
We study a particle gov-
erned by Langevin dynam-
ics that is randomly kicked
Figure1.Invariant PDF for
in the counterclockwise di-
thejump-diffusionprocess.
rection, where the times be-
tween kicks are specified by a Poisson process (see Ap-
pendixB.1fordetails).
Inthisexample,iftheprocessstartsatapointinonemode,
itsPDFspreadsinthemodeandleaksintotheothernearby
modesinthecounterclockwisedirection(seeFig.2). Corre-
spondingly, the conditional PDF of x τ given x 0 is itself -1π -0.5π 0 0.5π 1π
a Gaussian mixture PDF that is sharply peaked around
x when τ is small, and which slowly evolves into the
0
5-modeinvariantmeasureassociatedwiththepotentialas Figure2.Forecastingcomparisonforthejump-diffusionpro-
cess.(Left)Comparisonofthetruthtotheforecastedpredictionin
τ increases. Wegeneratealongtimeseriesofthisprocess
theangularcoordinates.(Middle)GroundtruthKDEsatvarious
and use it at different lags τ in the empirical loss (14) to
learn the drift velocityˆb, which we model as a fully con- lagtimesτ.(Right)ForecastedKDEsatthesamelagtimesτ.
nected neural network. We then use the estimated ˆbg in
the SDE (16) to generate probabilistic forecasts. The re- &Mattingly,2006). Ourobjectiveistoforecastthesolution
sults(Fig.2)indicatethatthelawoftheseforecastsisin to (17) at time t+τ given its solution at time t after the
excellentagreementwiththetrueρ c(x τ|x 0). Wecanalso processhasreachedastatisticallysteadystate. Wedoso
iterateusingtheproceduredescribedinSec.3.5toestimate usingbothfull-andlow-resolutiondataattimet,butour
ρ c(x kτ|x 0) for k > 1 without additional retraining. We goalisalwaystoforecastatfullresolution.
findexcellentagreementindoingso,includingbeyondthe
decorrelationtime,whentheconditionalPDFrelaxesinto VorticityData. Weemployapseudo-spectralmethodto
the equilibrium distribution independent of x 0. We note simulate(17)andhencetoobtainadatasetofsnapshotsof
thatthisisanexampleinwhichprobabilisticforecastingis thevorticityfield. Wesetthetimestep∆t=10−4andgrid
key,asregressingx τ givenx 0wouldgiveresultswithlittle sizeto256×256. Westoresnapshotsatregularintervals
information. Probabilisticforecastingisneededtocapture of∆t=0.5. Weconductsimulationsfor2000trajectories
thenon-Gaussianandmultimodalnatureoftheforecasts, withinthetimerangeoft ∈ [0,100];wethenexcludethe
whichwouldbehardtocapturewithdeterministicmethods. initialphaset∈[0,50]fromourdata.Ultimately,wecollect
atotalof2×105 snapshots,whicharetreatedassamples
4.2.Forecastingthe2dNavier-StokesEquations from the invariant measure of (17). To reduce memory
requirements, we downsize the dataset to a resolution of
Our second numerical example considers forecasting the
128×128.Inallourexperiments,weusedaUNet(Hoetal.,
dynamicsofthesolutiontothe2dNavier-Stokesequations
withrandomforcingonthetorusT2 = [0,2π]2. Withthe 2020)asournetworkforapproximatingthevelocityfield.
Detailedparametersforthetraininganddatasetgeneration
vorticityformulation,theNavier-Stokesequationsread
canbefoundintheAppendixB.2.
dω+v·∇ωdt=ν∆ωdt−αωdt+εdη, (17)
ForecastingatFullResolution. First,weconsiderpre-
Herev = ∇⊥ψ = (−∂ ψ,∂ ψ)isthevelocityexpressed dictingthedistributionofthevorticityfieldthatmayevolve
y x
in terms of the stream function ψ, which is a solution to from a given realization. To this end, we learn the SDE
−∆ψ = ω,dη iswhite-in-timerandomforcingactingon thatsamplestheconditionaldistributionofvorticityfields
afewFouriermodes,andν,α,ε > 0areparameters(see afterlagτ =0.5,andweiteratethisSDEtogetforecasted
AppendixB.2fordetails). Weworkinasettingwhere(17) predictionsafterlag2τ,3τ,etc. InthetoprowofFig.3,the
isprovablyergodicwithauniqueinvariantmeasure(Hairer firstpanelshowsasnapshotofavorticityfield,whilethe
6
5.0=τ
0.1=τ
0.2=τProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
nextthreepanelsshowdifferentsamplesofvorticityfields generativemodelinginthislatentspaceforefficiency. As
generatedafterlag2byiteratingourforecastingprocedure. such, our aim is to sample from the conditional density
Notethatthesegeneratedvorticityfieldsaredifferentfrom ρ (yt|yt−1,...,yt−C)describingtheprobabilityofaframe
c
one another, emphasizing the need for probabilistic fore- given a sequence of previous frames, which can then be
casting. This is corroborated by the true and forecasted decoded xt = Decode(yt). We set the interpolant base
conditionalmeansofthefieldshowninthefirsttwopanels distributiontoapointmassony =yt−1andwemodel
s=0
onthebottomrowofFig.3.Whilecorrectlycapturedbyour y =yt. Notethatweswitchtoy fromthenotationx
s=1 s s
approach,thisconditionalmeanisclearlynotinformative usedearliertoemphasizethattheinterpolationhappensina
onitsown,asitaveragesoverthespatialfeaturespresentin latentspace.
theactualforecast. Thespreadofthisensembleofforecasts
is also apparent from the standard deviation of the field, Generation. BecauseconditioningonthewholesetofC
showninthethirdandfourthpanelonthebottomrowof previoustimeslicesiscostly,wefollowRIVER(Davtyan
Fig.3. Alsoshownintherightpanelistheenstrophyspec- etal.,2023)anduseaMonte-Carloestimatorthatgenerates
trumofthetruevorticityfieldandtheensembleofforecasts ourestimateofthetthlatentframeyˆtconditionalonthees-
(seeAppendixB.2),showingthatourmethodcapturesthis timatedlatentframeyt−1andanadditionalestimatedframe
importantphysicalquantitycorrectly. yt−j randomlychosenforj >1. Togivethenetworkcon-
textfortheconditionedframe,wealsoconditiononthetime
ForecastingfromLow-ResolutionData. Wenowcon- indext−j.Thisrandomconditioningset(yt−1,yt−j,t−j)
siderforecastingthevorticityfieldataresolutionof128× avoids the need to compute functions of the entire condi-
128 from a downsized version at resolution 32 × 32 by tioningcontext,whichconsistsofmanyframes. Samples
learningtheSDEasbeforewithadriftvelocitythatiscondi- are then decoded by the auto-encoder to produce images
tionedonthelow-resolutionfieldusedasx 0. Theoutcome xt = Decode(yt). To sample a full video, we apply this
ofthistaskisshowninFig.4,wherethefourpanelsonthe forecasting strategy autoregressively. For further details,
rightshowthelow-resolutionfieldusedasinputsurrounded seeAlgorithm3inAppendixB.3.
by three full-resolution forecasts generated after lag = 1.
Wealsoplottheenstrophyspectrumofthelow-resolution
FVD metric. The Fre´chet Video Distance (FVD) (Un-
field(whichdoesnotgopastwavenumber16)andthespec-
terthineretal.,2018)extendstheFre´chetInceptionDistance
traofthetrueandtheforecastedfieldatfullresolutionin
(FID)(Heuseletal.,2017).Weselect256testsetvideosand
theleftpanel. Ascanbeseen, ourapproachrecoversthe
generate100completionsforeachone,therebycomparing
truespectrumveryaccurately.
256realvideosto25,600generatedvideos. Wealsoreport
Wepresentadditionalexperimentssuchassuperresolution, several qualitative features of the generated videos. One
forecastingefficiencycomparedtosimulationtime(100× consequenceofusingtheVQGANisthattheperformance
faster), comparisons between using σ and gF (Fo¨llmer isboundedbytheFVDofthedecoded-encodeddatasince
s s
process)intermsofKLaccuracy,andcomparisonstoflow thegenerativemodeltargetstheencodeddistribution.
matching (Lipman et al., 2022; Liu et al., 2022; Albergo
&Vanden-Eijnden,2022)anddeterministicforecastingin Baseline. Wecomparewiththesetupfrom(Davtyanetal.,
AppendixB.2. 2023),whichlearnsadeterministicflowusingflowmatch-
ing(Lipmanetal.,2022;Liuetal.,2022;Albergo&Vanden-
4.3.VideoForecasting Eijnden,2022)tomapaGaussiansampletothenextvideo
frame,conditionedonthesameinformationasinoursetup.
We model the KTH and CLEVRER datasets. We follow
Bycontrast,wegeneratewithanSDEsamplerinitialized
RIVER (Davtyan etal., 2023) andmodel these videosin
at the previous video frame, which is more proximal to
thelatentspaceofaVQGAN(Esseretal.,2021)trained
thenexttargetframethanpurenoise. TheRIVERmethod
toauto-encodethedatasets. Modelinginalatentspaceisa
requires training a new embedding model for the videos
computationallyefficientstrategyforgenerativemodeling
usingaVQGAN.WeusetheVQGANcheckpointsfrom
of high resolution images (Vahdat et al., 2021; Rombach
their work so that we can study our proposed generative
etal.,2022;Peebles&Xie,2023;Maetal.,2024)andvideo
modelingmethodinacontrolledcontext.
(Blattmann et al., 2023; Davtyan et al., 2023), because it
reducesthedimensionalityofthedataset.
Training. Wetrainmodelsfor250kgradientstepsusing
AdamWstartingatalearningrateof2e-4. WeusetheUNet
TaskDescription. ForvideoframesxtwithC channels
architecturepopularized4 in(Hoetal.,2020). Wemodify
andresolutionH×W,theVQGANmapseachvideoframe
thearchitecturetoconditiononpastframesbyconcatenating
toalatentimageyt =Encode(xt)withC latentchannels
ℓ
andlatentresolutionH ℓ×W ℓ. Weperformtheinterpolant 4Weusethelucidrainsrepository.
7ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
101
100
ω t ←threeforecastsofω t+2 →
10 1
10 2
true conditional
forecast conditional
100 101
truev.s.forecastconditionalmean truev.s.forecastconditionalstd enstrophyspectrum(v.s.k)
Figure3.Temporalforecastingonstochastically-forcedNavierStokes.(Topleft)Differentforecastsfromourmethodatlagτ =2,for
afixedω .(Bottomleft)Comparisonsbetweentheforecastsamplemeanandstandarddeviationforthisω againstthetruths.(Right)
t t
Enstrophyspectrumofthetrueandforecastedconditionaldistribution.Note:AlltheNSfiguresinthispapersharethesamecolorbarsfor
thevorticityfield(onascalefrom−5to5)andstd(onascalefrom0to3)respectively.
a generative model needs to deduce physical phenomena
101
tosucceedatthegeneration. Forexample,objectsshould
notgothrougheachother,andinsteadshouldbounceoff
100
oneanother. The3×128×128-dimensionaldataspaceis
mappedtoa4×16×16latentspace. Duringgeneration,
32×32ωt forecastωt+1
10 1 weconditiononjust2realframesandgenerate14.
10 2 truth k=16 KTH CLEVRER
super resolution
low resolution
10 3 Method 100k 250k 100k 250k
100 101
forecastωt+1 forecastωt+1 enstrophyspectrum(v.s.k) RIVER 46.69 41.88 60.40 48.96
PFI(ours) 44.38 39.13 54.7 39.31
Figure4. Spatiotemporalforecastingonstochastically-forced
NavierStokes. (Left)Lowresolutionω t andthreeofourfore- Auto-enc. 33.45 33.45 2.79 2.79
castedsamplesatt+1.(Right)Enstrophyspectrumsofthelow
ShiftedFVD
resolutionω ,thesuperresolutionforecastofω ,andthetrue
t t+1
ω t+1. RIVER 13.24 8.43 57.61 46.17
PFI(ours) 10.93 5.68 51.91 36.52
themalongthechanneldimensionoftheinput. Eachmodel
Table1.VideoResults. FVDcomputedon256testsetvideos,
istrainedonfourA100GPUsforapproximately1-2days.
withthemodelgenerating100completionsforeachvideo.Results
are reported for 100k and for 250k gradient steps. “Auto-enc.”
Datasets. TheKTHdataset(Schuldtetal.,2004)consists
representstheFVDofthepretrainedencoder-decodercompared
ofblack-and-whitevideosof25peoplecompletingoneof6 totherealdata.Itservesasaboundonthepossiblemodelperfor-
actionssuchasjoggingandhand-waving. Weusethelast mance,becausethemodelingisdoneinthelatentspacecomputed
5peopleasthetestset. The1×64×64-dimensionaldata bytheencoder-decoderpair.WealsosupplyashiftedFVD,where
space ismapped to a4×8×8-dimensional latentspace encoded-decodedFVDissubtractedfromFVDvaluestoshow
bytheVQGAN.Duringgeneration,westartwith10given theyapproachthisapproximatebound.
video frames, and we generate the next 30 frames. The
CLEVRERdataset5(Yietal.,2019)containsvideoscreated
Results. Table 1 shows the FVD performance of our
forstudyingreasoningandphysicstasks.Thevideosfeature
model,probabilisticforecastingwithinterpolants(PFI),as
cubes,spheres,andothershapestravelingacrossthescreen
compared to the RIVER baseline. We train both models
and interacting through collisions while being subject to
underthesameconditionsforacontrolledcomparison. On
forcesandrotations. Oneinterpretationofthisdataisthat
boththeKTHandCLEVRERdatasets,PFIsurpassesthe
5http://clevrer.csail.mit.edu/ standardflowmatchingapproach. Inadditiontothenumer-
8ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
Figure5. VideogenerationontheCLEVRERdataset.(Toprow)Realtrajectory.(Secondrow)Generatedtrajectory.Anew,redcube
entersthescene.(Thirdrow)Realtrajectory.(Fourthrow)Generatedtrajectory.Anewgreencubeentersthescene,andcollisionphysics
isrespected(greenballhitsredcube).
Figure6. VideogenerationontheKTHdataset.(Toprow)Realtrajectory.(Middleandbottomrows)Generatedtrajectories.Astime
proceeds,thegeneratedvideosdriftfromthetruevideo,butstilldisplaytemporally-consistenthand-wavingmotions.
icalcomparison,wedemonstratethatourmodelsproduce tionoverfuturestatesofthatsystem,ourproposedmethod
diverseforecaststhatcapturephysicalrulesinherenttothe usesdynamicalmeasuretransportinawaythatnaturally
videos. InFigure5,weillustratetwotrajectoriesoftheani- alignswiththeframeworkofprobabilisticforecasting. It
mation.Foreach,wesupplytheinitialcondition,thedataset alsoallowsustominimizetheimpactoftheestimationerror
trajectory,andageneratedtrajectorybasedoffofthesame bytuningofthediffusioncoefficient, whichcanbedone
initialconditiontoshowthatthecontinuationofframesis aftertrainingandoffersanewperspectiveontheFo¨llmer
probabilistic. Forexample,thereisvariationinthedataset process. We have shown various uses of this approach,
trajectoryofagreencubecollidingwithagreencylinderas ranging from predicting the evolution of stochastic fluid
comparedtotheforecastedtrajectory,whilealsopreserving dynamicstovideocompletiontasks. Futureworkwillcon-
theanimatedphysics. InFigure6,weshowthat,fromthe sider the use of these models for empirical weather data
sameinitialframe,theforecastsgivevariedrealizationsof andincorporationofphysicalstructureintothegenerative
thehand-wavingvideocategory. model.
5.ConclusionandFutureWork ImpactStatement
In this work, we introduced a principled approach to the Thispaperpresentsworkwhosegoalistoadvancethefield
useofgenerativemodelingforprobabilisticforecasting. By of Machine Learning. There are many potential societal
introducingstochasticprocessesthattransportapointmass consequencesofourwork,noneofwhichwefeelmustbe
centeredatacurrentobservationofthesystemtoadistribu- specificallyhighlightedhere. Wenote,however,thatvideo
9ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
generation,whichisoneofthepossibleapplicationsofthe de Be´zenac, E., Rangapuram, S. S., Benidis, K., Bohlke-
forecastingframeworkpresented,isalessexploreddomain Schneider,M.,Kurle,R.,Stella,L.,Hasson,H.,Gallinari,
thatcouldpromoteharmthroughbiasesinthemodel. We P., and Januschowski, T. Normalizing kalman filters
surmisethatitcarriesthesamerisksasimagegeneration formultivariatetimeseriesanalysis. InLarochelle,H.,
technologies. Ranzato,M.,Hadsell,R.,Balcan,M.,andLin,H.(eds.),
Advances in Neural Information Processing Systems,
volume 33, pp. 2995–3007. Curran Associates, Inc.,
Acknowledgments
2020. URL https://proceedings.neurips.
We thank Joan Bruna, Jon Niles-Weed, Loucas Pillaud- cc/paper_files/paper/2020/file/
Vivien,andValentinDeBortoliforusefuldiscussions. 1f47cef5e38c952f94c5d61726027439-Paper.
pdf.
References
DeBortoli,V.,Thornton,J.,Heng,J.,andDoucet,A. Diffu-
Albergo,M.S.andVanden-Eijnden,E. Buildingnormaliz- sionschro¨dingerbridgewithapplicationstoscore-based
ingflowswithstochasticinterpolants. InTheEleventh generative modeling. Advances in Neural Information
InternationalConferenceonLearningRepresentations, ProcessingSystems,34:17695–17709,2021.
2022.
Dellnitz,M.andJunge,O. Ontheapproximationofcompli-
Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. cateddynamicalbehavior. SIAMJournalonNumerical
Stochasticinterpolants: Aunifyingframeworkforflows Analysis,36(2):491–515,1999.
anddiffusions. arXivpreprintarXiv:2303.08797,2023.
Dresdner,G.,Kochkov,D.,Norgaard,P.,Zepeda-Nu´n˜ez,L.,
Alexander,R.andGiannakis,D. Operator-theoreticframe-
Smith,J.A.,Brenner,M.P.,andHoyer,S. Learningto
work for forecasting nonlinear time series with kernel
correctspectralmethodsforsimulatingturbulentflows.
analog techniques. Physica D: Nonlinear Phenomena,
arXivpreprintarXiv:2207.00556,2022.
409:132520,2020.
Eldan, R. and Lee, J. R. Regularization under diffusion
Berry,T.,Giannakis,D.,andHarlim,J. Nonparametricfore-
andanticoncentrationoftheinformationcontent. Duke
castingoflow-dimensionaldynamicalsystems. Physical
MathematicalJournal,167(5):969–993,2018.
ReviewE,91(3):032915,2015.
Eldan,R.,Lehec,J.,andShenfeld,Y. Stabilityofthelog-
Blattmann,A.,Dockhorn,T.,Kulal,S.,Mendelevitch,D.,
arithmicsobolevinequalityviathefollmerprocess. In
Kilian,M.,Lorenz,D.,Levi,Y.,English,Z.,Voleti,V.,
Annales De L Institut Henri Poincare-Probabilites Et
Letts, A., et al. Stable video diffusion: Scaling latent
Statistiques,volume56,pp.2253–2269.InstituteofMath-
videodiffusionmodelstolargedatasets. arXivpreprint
ematicalStatistics,2020.
arXiv:2311.15127,2023.
Esser,P.,Rombach,R.,andOmmer,B.Tamingtransformers
Buaria,D.andSreenivasan,K.R. Forecastingsmall-scale
forhigh-resolutionimagesynthesis. InProceedingsof
dynamicsoffluidturbulenceusingdeepneuralnetworks.
theIEEE/CVFconferenceoncomputervisionandpattern
ProceedingsoftheNationalAcademyofSciences,120
recognition,pp.12873–12883,2021.
(30), July 2023. URL https://www.pnas.org/
doi/10.1073/pnas.2305765120.Publisher:Pro-
Finn,C.,Goodfellow,I.,andLevine,S.UnsupervisedLearn-
ceedingsoftheNationalAcademyofSciences.
ingforPhysicalInteractionthroughVideoPrediction. In
Cachay,S.R.,Zhao,B.,James,H.,andYu,R. Dyffusion:
AdvancesinNeuralInformationProcessingSystems,vol-
Adynamics-informeddiffusionmodelforspatiotemporal ume29.CurranAssociates,Inc.,2016.
forecasting. NeurIPS,2023.
Fo¨llmer,H. Timereversalonwienerspace. StochasticPro-
Chen,Y.,Georgiou,T.T.,andPavon,M. Stochasticcontrol cesses—MathematicsandPhysics,pp.119–129,1986.
liaisons: Richard sinkhorn meets gaspard monge on a
schrodingerbridge. SiamReview,63(2):249–313,2021. Giannakis, D., Henriksen, A., Tropp, J. A., and Ward, R.
LearningtoForecastDynamicalSystemsfromStreaming
Davtyan, A., Sameni, S., and Favaro, P. Efficient video Data. SIAMJournalonAppliedDynamicalSystems,22
prediction via sparsely conditioned flow matching. In (2):527–558,June2023. ISSN1536-0040. doi: 10.1137/
ProceedingsoftheIEEE/CVFInternationalConference 21M144983X. URLhttps://epubs.siam.org/
onComputerVision,pp.23263–23274,2023. doi/10.1137/21M144983X.
10ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
Gneiting, T. and Katzfuss, M. Probabilistic fore- Lee, A. X., Zhang, R., Ebert, F., Abbeel, P., Finn, C.,
casting. Annual Review of Statistics and Its Ap- andLevine,S. StochasticAdversarialVideoPrediction,
plication, 1(1):125–151, 2014. doi: 10.1146/ April2018. URLhttp://arxiv.org/abs/1804.
annurev-statistics-062713-085831. 01523. arXiv:1804.01523[cs].
Gneiting,T.,Raftery,A.E.,Westveld,A.H.,andGoldman, Lehec,J. Representationformulafortheentropyandfunc-
T. Calibrated probabilistic forecasting using ensemble tionalinequalities. InAnnalesdel’IHPProbabilite´set
model output statistics and minimum crps estimation. statistiques,volume49,pp.885–899,2013.
MonthlyWeatherReview,133(5):1098–1118,2005. doi:
Le´onard, C. A survey of the schro¨dinger problem and
10.1175/MWR2904.1.
someofitsconnectionswithoptimaltransport. Discrete
Gu, A., Goel, K., and Re´, C. Efficiently modeling long andContinuousDynamicalSystems-SeriesA,34(4):1533–
sequences with structured state spaces. arXiv preprint 1574,2014.
arXiv:2111.00396,2021.
Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhat-
Hairer,M.andMattingly,J.C. Ergodicityofthe2dnavier- tacharya, K., Stuart, A., and Anandkumar, A. Fourier
stokesequationswithdegeneratestochasticforcing. An- neuraloperatorforparametricpartialdifferentialequa-
nalsofMathematics,pp.993–1032,2006. tions. arXivpreprintarXiv:2010.08895,2020.
Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,and Li,Z.,Liu-Schiaffini,M.,Kovachki,N.,Liu,B.,Azizzade-
Hochreiter,S. Ganstrainedbyatwotime-scaleupdate nesheli,K.,Bhattacharya,K.,Stuart,A.,andAnandku-
ruleconvergetoalocalnashequilibrium. Advancesin mar,A.Learningdissipativedynamicsinchaoticsystems.
neuralinformationprocessingsystems,30,2017. arXivpreprintarXiv:2106.06898,2021.
Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba- Lienen,M.,Lu¨dke,D.,Hansen-Palmus,J.,andGu¨nnemann,
bilisticmodels. Advancesinneuralinformationprocess- S. Fromzerototurbulence: Generativemodelingfor3d
ingsystems,33:6840–6851,2020. flowsimulation. InTheTwelfthInternationalConference
onLearningRepresentations,2023.
Ho,J.,Chan,W.,Saharia,C.,Whang,J.,Gao,R.,Gritsenko,
A.,Kingma,D.P.,Poole,B.,Norouzi,M.,Fleet,D.J., Lim,B.andZohren,S. Time-seriesforecastingwithdeep
et al. Imagen video: High definition video generation learning: a survey. Philosophical Transactions of the
withdiffusionmodels. arXivpreprintarXiv:2210.02303, RoyalSocietyA:Mathematical,PhysicalandEngineer-
2022. ingSciences,379(2194):20200209,February2021. doi:
10.1098/rsta.2020.0209.
Huang,J.,Jiao,Y.,Kang,L.,Liao,X.,Liu,J.,andLiu,Y.
Schro¨dinger-Fo¨llmersampler: samplingwithoutergodic- Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M., and
ity. arXivpreprintarXiv:2106.10880,2021. Le,M. Flowmatchingforgenerativemodeling. InThe
Eleventh International Conference on Learning Repre-
Jiang, R., Lu, P. Y., Orlova, E., and Willett, R. Training sentations,2022.
neuraloperatorstopreserveinvariantmeasuresofchaotic
attractors. arXivpreprintarXiv:2306.01187,2023. Liu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E. A.,
Nie, W., andAnandkumar, A. I2SB: Image-to-image
Jiao,Y.,Kang,L.,Liu,Y.,andZhou,Y. Convergenceanal- Schro¨dinger bridge. arXiv preprint arXiv:2302.05872,
ysisofSchro¨dinger-Fo¨llmersamplerwithoutconvexity. 2023.
arXivpreprintarXiv:2107.04766,2021.
Liu, X., Gong, C., and Liu, Q. Flow straight and fast:
Kaiser,E.,Kutz,J.N.,andBrunton,S.L. Data-drivendis- Learningtogenerateandtransferdatawithrectifiedflow.
coveryofkoopmaneigenfunctionsforcontrol. Machine InTheEleventhInternationalConferenceonLearning
Learning: ScienceandTechnology,2(3):035023,2021. Representations,2022.
Kidger,P.,Foster,J.,Li,X.,andLyons,T.J. Neuralsdesas Loshchilov,I.andHutter,F. Decoupledweightdecayregu-
infinite-dimensionalgans. InInternationalconferenceon larization. arXivpreprintarXiv:1711.05101,2017.
machinelearning,pp.5453–5463.PMLR,2021.
Lu,L.,Jin,P.,Pang,G.,Zhang,Z.,andKarniadakis,G.E.
Kutz,J.N.,Brunton,S.L.,Brunton,B.W.,andProctor,J.L. Learningnonlinearoperatorsviadeeponetbasedonthe
Dynamicmodedecomposition: data-drivenmodelingof universal approximation theorem of operators. Nature
complexsystems. SIAM,2016. machineintelligence,3(3):218–229,2021.
11ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
Ma,N.,Goldstein,M.,Albergo,M.S.,Boffi,N.M.,Vanden- InternationalConferenceonPatternRecognition,2004.
Eijnden,E.,andXie,S.Sit:Exploringflowanddiffusion- ICPR2004.,volume3,pp.32–36.IEEE,2004.
basedgenerativemodelswithscalableinterpolanttrans-
formers. arXivpreprintarXiv:2401.08740,2024. Shi,Y.,DeBortoli,V.,Campbell,A.,andDoucet,A. Diffu-
sionschro¨dingerbridgematching. AdvancesinNeural
Masini,R.P.,Medeiros,M.C.,andMendes,E.F. Machine InformationProcessingSystems,36,2024.
learningadvancesfortimeseriesforecasting. Journalof
EconomicSurveys,37(1):76–111,2023.ISSN1467-6419. Smagorinsky, J. General circulation experiments
doi: 10.1111/joes.12429. with the primitive equations. Monthly Weather
Review, 91(3):99 – 164, 1963. doi: 10.1175/
Mirza,M.andOsindero,S. Conditionalgenerativeadver-
1520-0493(1963)091⟨0099:GCEWTP⟩2.3.CO;2. URL
sarialnets. arXivpreprintarXiv:1411.1784,2014.
https://journals.ametsoc.org/view/
journals/mwre/91/3/1520-0493_1963_
Oprea,S.,Martinez-Gonzalez,P.,Garcia-Garcia,A.,Castro-
091_0099_gcewtp_2_3_co_2.xml.
Vargas, J. A., Orts-Escolano, S., Garcia-Rodriguez, J.,
andArgyros,A. AReviewonDeepLearningTechniques
Sohn,K.,Lee,H.,andYan,X. Learningstructuredoutput
forVideoPrediction. IEEETransactionsonPatternAnal-
representationusingdeepconditionalgenerativemodels.
ysis and Machine Intelligence, 44(6):2806–2826, June
Advancesinneuralinformationprocessingsystems,28,
2022. ISSN 1939-3539. doi: 10.1109/TPAMI.2020.
2015.
3045007.
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er-
Palmer,T.,Molteni,F.,Mureau,R.,Buizza,R.,Chapelet,P.,
mon,S.,andPoole,B. Score-basedgenerativemodeling
andTribbia,J. Ensembleprediction,07/19921992. URL
https://www.ecmwf.int/node/11560. throughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020.
Pathak,J.,Subramanian,S.,Harrington,P.,Raja,S.,Chat-
topadhyay,A.,Mardani,M.,Kurth,T.,Hall,D.,Li,Z., Tzen,B.andRaginsky,M. Theoreticalguaranteesforsam-
Azizzadenesheli,K.,Hassanzadeh,P.,Kashinath,K.,and plingandinferenceingenerativemodelswithlatentdiffu-
Anandkumar,A. Fourcastnet: Aglobaldata-drivenhigh- sions.InConferenceonLearningTheory,pp.3084–3114.
resolutionweathermodelusingadaptivefourierneural PMLR,2019.
operators,2022.
Unterthiner,T.,VanSteenkiste,S.,Kurach,K.,Marinier,R.,
Peebles, W. and Xie, S. Scalable diffusion models with Michalski,M.,andGelly,S. Towardsaccurategenerative
transformers. InProceedingsoftheIEEE/CVFInterna- models of video: A new metric & challenges. arXiv
tionalConferenceonComputerVision,pp.4195–4205, preprintarXiv:1812.01717,2018.
2023.
Vahdat,A.,Kreis,K.,andKautz,J. Score-basedgenerative
Peluchetti,S.Non-denoisingforward-timediffusions.arXiv
modelinginlatentspace.arxiv,2021.
preprintarXiv:2312.14589,2023.
Vargas, F., Ovsianas, A., Fernandes, D., Girolami, M.,
Rasul, K., Seward, C., Schuster, I., and Vollgraf, R. Au-
Lawrence, N. D., and Nu¨sken, N. Bayesian learning
toregressivedenoisingdiffusionmodelsformultivariate
vianeuralschro¨dinger–fo¨llmerflows. StatisticsandCom-
probabilistictimeseriesforecasting.InInternationalCon-
puting,33(1):3,2023.
ference on Machine Learning, pp. 8857–8868. PMLR,
2021.
Wang,G.,Jiao,Y.,Xu,Q.,Wang,Y.,andYang,C. Deep
generative learning via schro¨dinger bridge. In Inter-
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
nationalConferenceonMachineLearning,pp.10794–
Ommer,B. High-resolutionimagesynthesiswithlatent
10804.PMLR,2021.
diffusionmodels. InProceedingsoftheIEEE/CVFcon-
ferenceoncomputervisionandpatternrecognition,pp.
Wanner, M. and Mezic, I. Robust approximation of the
10684–10695,2022.
stochastickoopmanoperator. SIAMJournalonApplied
Schro¨dinger, E. Sur la the´orie relativiste de l’e´lectron et DynamicalSystems,21(3):1930–1951,2022.
l’interpre´tationdelame´caniquequantique. InAnnalesde
Yi, K., Gan, C., Li, Y., Kohli, P., Wu, J., Torralba, A.,
l’institutHenriPoincare´,volume3,pp.269–310,1932.
and Tenenbaum, J. B. Clevrer: Collision events for
Schuldt,C.,Laptev,I.,andCaputo,B. Recognizinghuman video representation and reasoning. arXiv preprint
actions: alocalsvmapproach. InProceedingsofthe17th arXiv:1910.01442,2019.
12ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
Zhang,Q.andChen,Y. Pathintegralsampler: Astochastic
controlapproachforsampling. InInternationalConfer-
enceonLearningRepresentations,2021.
Zhao, M. and Jiang, L. Data-driven probability density
forecast for stochastic dynamical systems. Journal of
ComputationalPhysics,492:112422,2023.
13ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
A.Detailsonstochasticinterpolants
A.1.Analyticalformulasforsomespecificα ,β ,σ
s s s
Inthissection,wepresenttheformulasforsomespecificchoicesofα ,β ,andσ ,thecorrespondingoptimal(Fo¨llmer)
s s s
driftgF,andtheexpressionforbg intermsofb .
s s s
Forα =1−s,σ =ε(1−s)whereε>0isatunableparameter,andβ =s,wehave
s s s
√ √
b (x,x )=Ex0[x −x −ε sz|(1−s)x +sx +ε(1−s) sz =x], for s∈(0,1)
s 0 1 0 0 1
b (x ,x )=Ex0[x ]−x , (18)
0 0 0 1 0
b (x,x )=x−x .
1 0 0
Moreover,b istheminimizeroftheloss(withrespecttoˆb)
s
(cid:90) 1 √ √
L [ˆb]= E(cid:2) |ˆb ((1−s)x +sx +(1−s)ε sz,x )−(x −x −ε sz)|2(cid:3) ds. (19)
b s 0 1 0 1 0
0
Wealsohave
1
A = , c (x,x )=x−x (20)
s ε2s(1−s) s 0 0
sothat,foranyg suchthatlim s−1[g2−ε2(1−s)2]andlim g2/(ε(1−s))exists,
s s→0+ s s→1− s
(cid:18) (cid:19)
bg(x,x )=b (x,x )+ 1(cid:0) g2−ε2(1−s)2(cid:1) b s(x,x 0) − x−x 0 for s∈(0,1),
s 0 s 0 2 s ε2(1−s) ε2s(1−s)
(21)
bg(x ,x )=b (x ,x )=Ex0[x ]−x ,
0 0 0 0 0 0 1 0
bg(x,x )=b (x,x )=x−x .
1 0 1 0 0
Inaddition,wehave
(cid:112)
gF =ε (1−s)(1+s), (22)
s
sothat
bF(x,x )=(1+s)b (x,x )−x+x for s∈[0,1]. (23)
s 0 s 0 0
Forα =1−s,σ =ε(1−s),andβ =s2,wehave
s s s
√ √
b (x,x )=Ex0[2sx −x −ε sz|(1−s)x +s2x +ε(1−s) sz =x], for s∈(0,1)
s 0 1 0 0 1
b (x ,x )=−x , (24)
0 0 0 0
b (x,x )=x−x .
1 0 0
Moreover,b istheminimizeroftheloss
s
(cid:90) 1 √ √
L [ˆb]= E(cid:2) |ˆb ((1−s)x +s2x +ε(1−s) sz,x )−(2sx −x −ε sz)|2(cid:3) ds. (25)
b s 0 1 0 1 0
0
Wealsohave
1
A = , c (x,x )=2sx−s(2−s)x , (26)
s ε2s2(1−s)(2−s) s 0 0
sothat,foranyg suchthatlim s−1[g2−ε2(1−s)2]andlim g2/(ε(1−s))exist,
s s→0+ s s→1− s
(cid:18) (cid:19)
bg(x,x )=b (x,x )+ 1(cid:0) g2−ε2(1−s)2(cid:1) b s(x,x 0) − x−x 0 for s∈(0,1)
s 0 s 0 2 s ε2(1−s)(2−s) ε2s2(1−s)(2−s)
(27)
bg(x ,x )=b (x ,x )=Ex0[x ]−x
0 0 0 0 0 0 1 0
bg(x,x )=b (x,x )=x−x .
1 0 1 0 0
Inadditionwehave
(cid:112)
gF =ε (1−s)(3−s) (28)
s
14ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
sothat
(cid:16) 1 (cid:17) 1
bF(x,x )= 1+ b (x,x )− (2x−(2−s)x ) for s∈(0,1],
s 0 2−s s 0 s(2−s) 0 (29)
bF(x ,x )=−2x .
0 0 0 0
Wesummarizetheabovecalculationsinthefollowingtable:
α β σ gF A c (x,x ) bF(x,x )
s s s s s s 0 s 0
1−s s ε(1−s) ε(cid:112) (1−s)(1+s) 1 x−x (1+s)b −x+x
ε2s(1−s) 0 s 0
1−s s2 ε(1−s) ε(cid:112) (3−s)(1−s) 1 2sx−s(2−s)x (1+ 1 )b − 1 (2x−(2−s)x )
ε2s2(1−s)(2−s) 0 2−s s s(2−s) 0
A.2.ProofofTheorem3.1
Recallthat:
DefinitionA.1. ThestochasticinterpolantI isthestochasticprocessdefinedas
s
I =α x +β x +σ W s∈[0,1], (30)
s s 0 s 1 s s
where
• α,β,σ ∈C1([0,1])satisfyα2+β2+σ2 >0foralls∈[0,1],β˙ >0foralls∈(0,1],andσ˙ <0foralls∈[0,1],as
s s s s s
wellastheboundaryconditionsα =β =1,α =β =σ =0.
0 1 1 0 1
• Thepair(x ,x )arejointlydrawnfromaPDFρ(x ,x )suchthatE (cid:2) |x |2+|x |2(cid:3) <∞.
0 1 0 1 (x0,x1)∼ρ 0 1
• W =(W ) isastandardWienerprocesswithW ⊥(x ,x ).
s s∈[0,1] 0 1
Inviewofthisdefinition,letusgiveamorepreciseformulationofTheorem3.1:
TheoremA.2. LetI bethestochasticinterpolantintroducedinDefinitionA.1andlet
s
√
∀s∈[0,1]: x =α x +β x +σ sz
s s 0 s 1 s
√ (31)
∀(s,x,x )∈(0,1]×Rd×Rd : b (x,x )=Ex0[α˙ x +β˙ x +σ˙ sz|x =x]
0 s 0 s 0 s 1 s s
whereEx0[·|x
s
= x]denotesanexpectationoverx
1
∼ ρ c(·|x 0)andz ∼ N(0,Id)conditionalonx
s
= x. Moreover,set
b s=0(x 0,x 0):=α˙ 0x 0+β˙ 0Ex0[x 1]. ThenthesolutionstotheSDE
dX =b (X ,x )ds+σ dW , X =x , (32)
s s s 0 s s s=0 0
aresuchthatLaw(X )=Law(I |x )atall(s,x )∈[0,1]×Rd. InparticularX ∼ρ (·|x ). Inadditionthedriftbis
s s 0 0 s=1 c 0
theuniqueminizeroverallˆboftheobjectivefunction
(cid:90) 1 √
L [ˆb]= E(cid:2) |ˆb (x ,x )−(α˙ x +β˙ x +σ˙ sz)|2]ds, (33)
b s s 0 s 0 s 1 s
0
whereEdenotesanexpectationover(x ,x )∼ρ(x ,x )andz ∼N(0,Id)with(x ,x )⊥z.
0 1 0 1 0 1
√
Notethattheobjective(33)isthesameas(3)becausex =d I andα˙ x +β˙ x +σ˙ sz =d R atalls∈[0,1]. Notealso
√s s s 0 s 1 s s
that,sinceα,β,σ ∈C1([0,1]),thefactorsα˙ ,β˙ ,σ˙ sintheloss(33)areallbounded.
s s s
ProofofTheoremA.2. NoticethattheprocessI definedin(30)hasthesamelawatanys∈[0,1]asx . Ifwedenoteby
s s
µ(s,dx|x )themeasureofI |x orx |x ,andbyϕ:Rd →Ratwice-differentiabletestfunctionwithcompactsupport,
0 s 0 s 0
bydefinitionwehave
(cid:90)
∀(s,x )∈[0,1]×Rd : ϕ(x)µ(s,dx|x )=E[ϕ(x )|x ]=E[ϕ(I )|x ], (34)
0 0 s 0 s 0
Rd
15ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
wherethefirstconditionalexpectationisover(x ,x )∼ρandz ∼N(0,Id)andthesecondover(x ,x )∼ρandW. By
0 1 0 1
theItoˆ formulawehave
dϕ(I )=(α˙ x +β˙ x +σ˙ W )·∇ϕ(I )ds+ 1σ2∆ϕ(I )ds+∇ϕ(I )·dW . (35)
s s 0 s 1 s s s 2 s s s s
Integratingthisequationintimeover[0,s],takingtheexpectationconditionalonx ,andusingbothW ⊥(x ,x )andthe
0 s 0 1
Itoˆ isometry,wededucethat
E[ϕ(I s)|x 0]=ϕ(x 0)+(cid:90) s(cid:16) E(cid:2) (α˙ rx 0+β˙ rx 1+σ˙ rW r)·∇ϕ(I r)(cid:12) (cid:12)x 0(cid:3) + 1 2σ r2E[∆ϕ(I r)|x 0](cid:17) dr. (36)
0
Inserting(36)into(34),wededucethat,∀(s,x )∈[0,1]×Rd,
0
(cid:90)
ϕ(x)µ(s,dx|x 0)=ϕ(x
0)+(cid:90) s(cid:16)
E(cid:2) (α˙ rx 0+β˙ rx 1+σ˙
r√
rz)·∇ϕ(x r)(cid:12) (cid:12)x 0(cid:3)
Rd 0 (37)
+1σ2E[∆ϕ(x )|x ](cid:1) dr,
2 r r 0
√
whereweusedthefactthatx andI sharethesamelawateachs. Also,W and szsharethesamelawateachs.
s s s
Usingthetowerpropertyoftheconditionalexpectation,(37)canalsobewrittenas
(cid:90)
ϕ(x)µ(s,dx|x )
0
Rd
(cid:90) s(cid:90) √
=ϕ(x 0)+ E(cid:2) (α˙ rx 0+β˙ rx 1+σ˙
r
rz)·∇ϕ(x r)(cid:12) (cid:12)x
r
=x,x 0(cid:3) µ(r,dx|x 0)dr
0 Rd
(cid:90) s (cid:90)
+ 1 σ2 ∆ϕ(x)µ(r,dx|x )dr
2 r 0
0 Rd (38)
(cid:90) s(cid:90) √
=ϕ(x 0)+ E(cid:2) (α˙ rx 0+β˙ rx 1+σ˙
r
rz)(cid:12) (cid:12)x
r
=x,x 0(cid:3) ·∇ϕ(x)µ(r,dx|x 0)dr
0 Rd
(cid:90) s (cid:90)
+ 1 σ2 ∆ϕ(x)µ(r,dx|x )dr
2 r 0
0 Rd
(cid:90) s(cid:90)
=ϕ(x )+
(cid:0)
b (x,x )·∇ϕ(x)+
1σ2∆ϕ(x)(cid:1)
µ(r,dx|x )
0 r 0 2 r 0
0 Rd
whereweusedthedefinitionofbin(31)togetthelastequality. Ifwenowrepeatthesamestepstoderiveanevolution
equationforE[ϕ(X )]whereX solvestheSDE(5)wearriveatthesameequation(38)forthemeasureofthisprocess,
s s
indicatingthatthismeasureisalsoµ(s,dx|x ).
0
√
Itremainstoshowthatthedriftb (x,x )=E[α˙ x +β˙ x +σ˙ sz|x =x,x ]istheuniqueminimizeroftheobjective
s 0 s 0 s 1 s s 0
function(3). Tothisend,noticethatbistheuniqueminimizeroverallˆbof
(cid:90) 1
E(cid:2) |ˆb (x ,x )−b (x ,x )|2ds,
s s 0 s s 0
0 (39)
=(cid:90) 1(cid:16) E(cid:2) |ˆb (x ,x )−(α˙ x +β˙ x +σ˙ √ sz)|2+varx0[α˙ x +β˙ x +σ˙ √ sz|x ](cid:3)(cid:17) ds,
s s 0 s 0 s 1 s s 0 s 1 s s
0
whereEdenotesanexpectationover(x ,x )∼ρ(x ,x )andz ∼N(0,Id)with(x ,x )⊥zandwhere
0 1 0 1 0 1
√ √ √
varx0[α˙ sx 0+β˙ sx 1+σ˙
s
sz|x s]=Ex0(cid:2) |α˙ sx 0+β˙ sx 1+σ˙
s
sz|2|x s(cid:3) −(cid:12) (cid:12)Ex0[α˙ sx 0+β˙ sx 1+σ˙
s
sz|x s](cid:12) (cid:12)2 ,
√ (40)
=Ex0(cid:2) |α˙ x +β˙ x +σ˙ sz|2|x (cid:3) −|b (x ,x )|2,
s 0 s 1 s s s s 0
√
andwherewe used thetowerproperty oftheconditionalexpectation E[E[|α˙ x +β˙ x +σ˙ sz|2|x ]] = E[|α˙ x +
√ √ s 0 s 1 s s s 0
β˙ sx 1+σ˙
s
sz|2|x s]. Sincevarx0[α˙ sx 0+β˙ sx 1+σ˙
s
sz|x
s
=x]isindependentofˆb,wecandropthistermtoarriveat
theobjective(33).
16ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
A.3.Regularityofthedriftats=0
Inthissection,wediscusstheregularityofthedriftb (x,x ).
s 0
AssumptionA.3. Theconditionaldistributionρ (·|x )isexponentialtailed. Moreprecisely,thereexistconstantsC ,C >
c 0 1 2
0(whichcandependonx ),suchthat
0
ρ (x|x )≤C exp(−C |x|),
c 0 2 1
foranyx∈Rd.
Theaforementionedassumptionisneededfortechnicalreasons,andisusedtoensurethevalidityofastepthatinvolvesthe
interchangeoflimitsandintegrationsintheproofofTheoremA.4.
Bylinearityofx inx ,x ,andz,wecanalsoestablishafewpropertiesofthevelocitybin(31)whichwestateas:
s 0 1
TheoremA.4. UnderAssumptionA.3,ifβ˙ =0,thenthevelocityfieldb (x,x )canbedecomposedas
s=0 s 0
∀(s,x,x )∈[0,1]×Rd×Rd b (x,x )=α˙ x +β˙ η (s,x,x )+σ˙ η (s,x,x ), (41)
0 s 0 s 0 s 1 0 s z 0
where
(cid:40) η (s,x,x )=Ex0[x |x =x],
∀(s,x,x )∈(0,1]×Rd×Rd : 1 0 √ 1 s
0 η (s,x,x )= sEx0[z|x =x],
z 0 s
 η (0,x,x )= limη (s,x,x )=Ex0[x ], (42)
 1 0
s→0
1 0 1
∀(x,x 0)∈Rd×Rd :
η z(0,x,x 0)= sli →m 0η z(s,x,x 0)=
x−
σ
0x
0.
Inaddition,thesetwofunctionssatisfytheconstraint
∀(s,x,x )∈[0,1]×Rd×Rd : x=α x +β η (s,x,x )+σ η (s,x,x ). (43)
0 s 0 s 1 0 s z 0
Wehavelim s→0b s(x,x 0)=α˙ 0x 0+ σσ˙ 00(x−x 0)andlim s→0∇ xb s(x,x 0)= σ σ˙0 0Idforanyx,x
0
∈Rd.
RemarkA.5. Notethat(43)impliesthatwecangetη fromη atanytimesuchthatβ ̸=0,andη fromη atanys∈(0,1]
1 z s z 1
suchthatσ >0: inparticular
s
x−α x −β η (s,x,x )
∀(s,x,x )∈(0,1]×Rd×Rdwithσ >0 : η (s,x,x )= s 0 s 1 0 . (44)
0 s z 0 σ
s
TheproofofTheoremA.4willrelythefollowingresult:
LemmaA.6. Wehave
∀(s,x,x )∈(0,1)×Rd×Rd : η (s,x,x )=
(cid:82) Rdx 1ρ c(x 1|x 0)e− 21Ms|x1|2+msx1·(x−αsx0)dx
1 (45)
0 1 0 (cid:82) Rdρ c(x 1|x 0)e− 21Ms|x1|2+msx1·(x−αsx0)dx
1
whereη 1(s,x,x 0)=Ex0[x 1|x
s
=x,x 0]andwedefined
β2 β
M = s , m = s . (46)
s sσ2 s sσ2
s s
Proof: Bydefinition,
√
η 1(s,x,x 0)=
(cid:82)
R (cid:82)d
Rdx ρ1ρ
c(c
x(x
1|1
x|x
0)0
e) −e− 21|21 z| |z 2| δ2 (δ x(x −− αα sxsx
00
−− ββ sxsx
11
−− σσ
s√s
szs )z d)d zdzd xx
11 , (47)
whereδ(x)denotestheDiracdeltadistribution. Foranys∈(0,1]suchthatσ >0,wecanperformtheintegrationoverz
s
explicitlytoget
η (s,x,x )=
(cid:82) Rdx 1ρ c(x 1|x 0)e− 21Ms|x1|2+msx1·(x−αsx0)e−1 2s−1σ s−2|x−αsx0|2dx
1
. (48)
1 0 (cid:82) Rdρ c(x 1|x 0)e−1 2Ms|x1|2+msx1·(x−αsx0)e−1 2s−1σs−2|x−αsx0|2dx
1
Sincethefactorse−1 2s−1σ s−2|x−αsx0|2 atthenumeratorandthedenominatordonotdependonx 1,theycanbetakenoutof
theintegralsandsimplified,andwearriveat(45).
17ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
ProofofTheoremA.4. TheonlystatementthatisnotadirectconsequenceofTheoremA.2is(42). Toestablishtheselimits,
noticethat,sinceβ ∈ C2([0,1])andβ˙ = 0,wemusthaveβ = O(s2)ass → 0. Asaresult,sinceσ ∈ C1([0,1])and
0 s
σ >0,wehave
0
limM = limm =0. (49)
s s
s→0 s→0
Asaresult,
x−x
∀(x,x )∈Rd×Rd : limη (s,x,x )=Ex0[x ], limη (s,x,x )= 0, (50)
0 s→0 1 0 1 s→0 z 0 σ 0
whichestablishesthelimitsin(42),whereweusedtheformulain(44)toderivethelimitsofη fromthatofη . Notethatin
z 1
theabovederivation,weneedtoverifytheinterchangeoflimitsandintegrations;itisguaranteedbyusingAssumption
A.3 and the Lebesgue dominated convergence theorem since, for a fixed x,x , when s is sufficiently small, the factor
0
ρ c(x 1|x 0)e−1 2Ms|x1|2+msx1·(x−αsx0)isdominatedbyρ c(x 1|x 0)e21C1|x|,whichisintegrableasafunctionofx 1duetoour
AssumptionA.3.
Toanalyzethelimitof∇ b (x,x ),usingLemmaA.6,wehave
x s 0
P(s,x,x )Q(s,x,x )−R(s,x,x )RT(s,x,x )
∇ η (s,x,x )=m 0 0 0 0 , (51)
x 1 0 s Q(s,x,x )2
0
where
(cid:90)
P(s,x,x 0)= x 1xT 1ρ c(x 1|x 0)e− 21Ms|x1|2+msx1·(x−αsx0)dx 1,
Rd
(cid:90)
Q(s,x,x 0)= ρ c(x 1|x 0)e−1 2Ms|x1|2+msx1·(x−αsx0)dx 1,
Rd
(cid:90)
R(s,x,x 0)= x 1ρ c(x 1|x 0)e−1 2Ms|x1|2+msx1·(x−αsx0)dx 1.
Rd
Toderivetheaboveformula,wealsoneedverifytheinterchangeoflimitsandintegrations;itisagainguaranteedbyusing
AssumptionA.3andtheLebesguedominatedconvergencetheorem,forsufficientlysmalls. Weknowthat
P(s,x,x )Q(s,x,x )−R(s,x,x )RT(s,x,x )
lim 0 0 0 0 =E[x xT|x ]−E[x |x ]E[xT|x ], (52)
s→0 |Q(s,x,x 0)|2 1 1 0 1 0 1 0
andthuslim ∇ η (s,x,x )=0aslim m =0. Usingtheformulain(44),wegetlim ∇ η (s,x,x )= 1 Id.
s→0 x 1 0 s→0 s s→0 x z 0 σ0
Therefore,
σ˙
lim∇ b (x,x )= 0Id, (53)
s→0 x s 0 σ 0
whichcompletestheproof.
A.4.ChangingtheDiffusionCoefficient: ProofofTheorem3.2
Theorem3.2andformula(10)areconsequencesofthefollowingresult:
TheoremA.7. Letg ∈C0([0,1])besuchthatlim s−1[g2−σ2]andlim g2σ−1exist. Letbbegivenby(31)and
s→0+ s s s→1− s s
define
∀s∈(0,1): A =[sσ (β˙ σ −β σ˙ )]−1
s s s s s s
(54)
∀(s,x,x )∈(0,1)×Rd×Rd : c (x,x )=β˙ x+(β α˙ −β˙ α )x .
0 s 0 s s s s s 0
Thenwehave
∀(s,x,x )∈(0,1)×Rd×Rd : ∇logρ (x|x )=A [β b (x,x )−c (x,x )], (55)
0 s 0 s s s 0 s 0
whereρ (x|x )denotesthePDFofX =d Xg =d I |x . Inadditionthedrift
s 0 s s s 0
bg(x,x )=b (x,x )+ 1(g2−σ2)∇logρ (x|x )
s 0 s 0 2 s s s 0 (56)
=b (x,x )+ 1(g2−σ2)A [β b (x,x )−c (x,x )]
s 0 2 s s s s s 0 s 0
iswell-definedforall(s,x,x )∈(0,1)×Rd×Rd,andhasfinitelimitsats=0,1forall(x,x )∈Rd×Rd. Finally,the
0 0
solutionstotheSDE
dXg =bg(Xg,x )ds+g dW , Xg =x , (57)
s s s 0 s s s=0 0
aresuchthatLaw(Xg)=Law(X )=Law(I |x )forall(s,x )∈[0,1]×Rd. InparticularXg ∼ρ (·|x ).
s s s 0 0 s=1 c 0
18ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
Proof. Letusfirstestablish(55). ByadirectextensionofStein’sformula(Albergoetal.,2023),wehave
1
∀(s,x,x )∈(0,1)×Rd×Rd : ∇logρ (x|x )=−√ Ex0[z|x =x] (58)
0 s 0 sσ s
s
√
d d
wherex =α x +β x + sσ zandwhereweusedx |x =I |x =X foralls∈[0,1]. Since
s s 0 s 1 s s 0 s 0 s
√
b (x,x )=α˙ x +β˙ Ex0[x |x =x]+ sσ˙ Ex0[z|x =x]
s 0 s 0 s 1 s s s
√ (59)
x=α x +β Ex0[x |x =x]+ sσ Ex0[z|x =x]
s 0 s 1 s s s
wededucethat
β b (x,x )−β˙ x−(β α˙ −β˙ α )x
Ex0[z|x s =x]= s s 0 √ s(σ˙s
β
−σs β˙s
)
s s 0 (60)
s s s s
whichcanbeinsertedin(58)toshowthat(55)holds.
Second,notethatthePDFρ (x|x )ofthesolutiontotheSDE(57)isthesameasthePDFofthesolutionto
s 0
dX =b (X ,x )ds+σ dW , X =x , (61)
s s s 0 s s s=0 0
sincewecanusetheidentity 1σ2∆ρ = 1g2∆ρ − 1(g2−σ2)∇·(ρ ∇logρ )toshowthatbothdensitiessatisfythe
2 s s 2 s s 2 s s s s
sameFokker-Planckequation(7).
Itremainstoshowthatthedriftcoefficient(56)iswell-defined. Tothisendletuswriteitexplicitlyas
(cid:18) g2−σ2(cid:19)(cid:32) β b (x,x )−β˙ x−(β α˙ −β˙ α )x (cid:33)
bg(x,x )=b (x,x )+ s s s s 0 s s s s s 0 . (62)
s 0 s 0 2sσ s σ sβ˙ s−σ˙ sβ s
Ourassumptionsthatα,β,σ ∈ C1([0,1])andsatisfyα = β = 1, α = β = σ = 0, andβ˙ > 0foralls ∈ (0,1]
0 1 1 0 1 s
and σ˙ < 0 for all s ∈ [0,1] guarantee that β > 0 for all s ∈ (0,1] and σ > 0 for all s ∈ [0,1) and, as a result,
s s s
[sσ (β˙ σ −β σ˙ )]−1ispositiveandfiniteforalls∈(0,1). Theseassumptionsalsoguaranteethatβ [σ β˙ −σ˙ β ]−1and
s s s s s s s s s s
β˙ [σ β˙ −σ˙ β ]−1havefinitelimitsats=0,1. Thereforetheonlyfactorin(62)thatcanbesingularis(g2−σ2)/(2sσ )
s s s s s s s s
at s = 0, because of the factor s−1, and at s = 1 because of the factor σ−1. These singularities disappear under our
1
assumptions that lim s−1[g2 −σ2] and lim g2σ−1 exist and are finite. Therefore the drift bg has the same
s→0+ s s s→1− s s s
regularitypropertieshasb .
s
A.5.Maximizingthelikelihoodwithrespecttothenoiseschedule
LetXg =(Xg) betheprocessdefinedbytheSDE(57),andletXˆg =(Xˆg) denoteanapproximate,learned
s s∈[0,1] s s∈[0,1]
processgovernedby
dXˆg =(cid:0) 1+ 1β A (g2−σ2)(cid:1)ˆb (Xˆg,x )ds−A c (Xˆg,x )ds+g dW , Xˆg =x , (63)
s 2 s s s s s s 0 s s s 0 s s s=0 0
Withaslightabuseofnotation,letusdenotebyD (Xg||Xˆg)theKLdivergenceofthepathmeasureofXg fromthepath
KL
measureofXˆg. ByGirsanov’stheorem,itisgivenby
1(cid:90) 1
D (Xg||Xˆg)= g−2|1+ 1β A (g2−σ2)|2E|bg(Xg,x )−ˆbg(Xg,x )|2ds, (64)
KL 2 s 2 s s s s s s 0 s s 0
0
whereEdenotesanexpectationoverthelawofXg. UsingXg =d I |x ,(64)canalsobewrittenas
s s 0
1(cid:90) 1
D (Xg||Xˆg)= g−2|1+ 1β A (g2−σ2)|2L ds,
KL 2 s 2 s s s s s
0 (65)
1(cid:90) 1
= g−2|1− 1β A σ2+ 1β A g2|2L ds,
2 s 2 s s s 2 s s s s
0
19ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
whereA sisgivenin(11)andL
s
=Ex0(cid:2) |ˆb s(I s,x 0)−b s(I s,x 0)|2(cid:3) . WenowviewtheKLdivergence(65)asanobjective
functionforg,andsolvetheoptimizationproblem
minD (Xg||Xˆg). (66)
KL
g
SinceL isindependentofg ,minimizing(65)amountstominimizing
s s
g−2|1− 1β A σ2+ 1β A g2|2 (67)
s 2 s s s 2 s s s
foralls∈[0,1]. SinceA >0,(67)isminimizedat
s
1β A σ2−1
g2 = 2 s s s when 1− 1β A σ2 ≤0 with (67)=0, (68)
s 1β A 2 s s s
2 s s
andat
1− 1β A σ2
g2 = 2 s s s when 1− 1β A σ2 >0 with (67)=2β A (1− 1β A σ2)>0. (69)
s 1β A 2 s s s s s 2 s s s
2 s s
Together,theserelationsshowthattheminimizerof(67)isg =gF with
s s
g sF =(cid:12) (cid:12) (cid:12) (cid:12)1− 11 2 ββ s AA sσ s2(cid:12) (cid:12) (cid:12) (cid:12)1/2 =(cid:12) (cid:12) (cid:12)2sσ s(β s−1β˙ sσ s−σ˙ s)−σ s2(cid:12) (cid:12) (cid:12)1/2 , (70)
2 s s
whereweusedtheexpressionforA in(11)togetthesecondequality. Thisresultis(13). Noticethatwecanalsowrite
s
d β
2sσ (β−1β˙ σ −σ˙ )−σ2 =2sσ2 log√ s . (71)
s s s s s s sds sσ
s
Sincethesignofthisexpressionisthesameasthesignof1− 1β A σ2, itshowsthat(68)and(69)ariseatvaluesof
√ 2 s s s √
s∈[0,1]whereβ /[ sσ ]isrespectivelydecreasingorincreasingins. Sinceβ˙ >0andσ˙ <0,β /[ sσ ]cannotbe
s s s s s s
decreasingforalls∈[0,1],i.e. theminimumof(67)cannotbezeroforalls∈[0,1]. Asaresult,theminimumoftheKL
divergence(65)mustbepositiveifL >0.
s
Finally, let us investigate the conditions on g in Theorem A.7 if we use the SDE (63) with g = gF. It is easy to see
s s s
from(70)thatthesecondconditionisalwayssatisfiedsince
lim |gF|2σ−1 = lim [2s(β−1β˙ σ −σ˙ )−σ ]=−2σ˙ >0 (72)
s s s s s s s 1
s→1− s→1−
sinceσ =0. Regardingthefirstcondition,wehave
1
lim s−1[|gF|2−σ2]= lim [2σ (β−1β˙ σ −σ˙ )−2s−1σ2]=2 lim [β−1β˙ −s−1]−2σ˙ σ (73)
s s s s s s s s s s 0 0
s→0+ s→0+ s→0+
sinceσ >0,σ˙ <0. Sinceβ =0,ifβ˙ >0andβ¨ exists,wehave
0 0 0 0 0
lim [β−1β˙ −s−1]= 1β¨ β˙−1, (74)
s s 2 0 0
s→0+
sothat
lim s−1[|gF|2−σ2]=σ2β¨ β˙−1−2σ˙ σ ifβ˙ >0andβ¨ exists. (75)
s s 0 0 0 0 0 0 0
s→0+
If, howeverβ˙ = 0, thengF ̸= σ , andthefirstconditionisnotsatisfiedsincelim s−1[|gF|2 −σ2]doesnotexist.
0 0 0 s→0+ s s
Inthiscase, weneedtoconsidermorecarefullyhowtodefinethesolutiontotheSDE(63). Inthespecificcasewhen
(cid:112)
α = 1−s, σ = ε(1−s)andβ = s2, ifwesetg = gF = ε (1−s)(3−s)in(63), thisSDEreducesto(seethe
s s s s s
explicitformulasgiveninAppendixA.1anddenotingXF =XgF):
s s
(cid:16) 1 (cid:17) 1 (cid:112)
dXF = 1+ b (XF,x )ds− (2XF−(2−s)x )ds+ε (1−s)(3−s)dW , XF =x . (76)
s 2−s s s 0 s(2−s) s 0 s s=0 0
20ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
Thedriftinthisequationissingularats=0becauseoftheterm2(XF−x )/[s(2−s)]. Nevertheless,thesolutiontothis
s 0
SDEiswell-definedfortheinitialconditionXF =x ,andsatisfiestheintegralequation
s=0 0
2−s(cid:90) s u (cid:18)(cid:16) 1 (cid:17) 1 (cid:19) 2−s(cid:90) s u(cid:112) (1−u)(3−u)
XF =x + 1+ b (XF,x )− x du+ε dW . (77)
s 0 s 2−u 2−u u u 0 (2−u) 0 s 2−u u
0 0
Sinceb (x ,x )=−x whenβ˙ =0,thisequationimpliesthat
0 0 0 0 0
XF =d x (1−s)+εW +o(s). (78)
s 0 s
whichisalsothelawofI |x asitshould.
s 0
A.6.ConnectionwithFo¨llmerProcessesandProofofTheorem3.3
To begin, we give some background on the Fo¨llmer process (Fo¨llmer, 1986; Tzen & Raginsky, 2019). Originally, the
Fo¨llmerprocesswasdefinedastheprocessX =(X ) whosepathmeasurehasminimalKLdivergencefromthepath
s s∈[0,1]
measureoftheWienerprocessW = (W ) undertheconstraintthatX bedistributedaccordingtosometarget
s s∈[0,1] s=1
distribution. ThisFo¨llmerprocesscanbegeneralizedto“referenceprocesses”thatdifferfromthestandardWienerprocess:
aswewillshownext,inourcontextthenaturalreferenceprocessisthesolutiontothelinearSDE
dY =a (Y −α x )ds+α˙ x ds+gFdW , Y =x , (79)
s s s s 0 s 0 s s s=0 0
wheregF isgivenby(70)andwherewehavedefined
s
2sσ (β−1β˙ σ −σ˙ )−σ2
a :=β−1β˙ − s s s s s s,
s s s β2+sσ2
s s
2β−1β˙ (β2+sσ2)−2β β˙ −2sσ σ˙ −σ2
=β−1β˙ − s s s s s s s s s, (80)
s s β2+sσ2
s s
2β β˙ +2sσ σ˙ +σ2 d β2+sσ2
=−β−1β˙ + s s s s s = log s s.
s s β2+sσ2 ds β
s s s
WecanthendefinetheFo¨llmerprocessXF =(XF) byadjustingthedriftˇb (x,x )in
s s∈[0,1] s 0
dXˇ =ˇb (Xˇ ,x )ds+gFdW , XˇF =x , (81)
s s s 0 s s s=0 0
in such a way that the KL divergence of the path measure of Xˇ from the path measure of Y (the solution to (79)) is
minimizedsubjecttotheconstraintthatXˇ ∼ρ (·|x ). Thatis,theFo¨llmerprocessisdefinedvia
s=1 c 0
minD (Xˇ||Y) subjectto Xˇ ∼ρ (·|x ). (82)
KL s=1 c 0
ˇb
ThistraditionalminimizationproblemfortheFo¨llmerprocessisdistinctfromtheminimizationproblem(66)consideredin
AppendixA.5. Nevertheless,ournextresultshowsthattheminimizersof(66)and(82)coincide:
√
TheoremA.8. Assumethatβ /[ sσ ]isnon-decreasingons ∈ [0,1]. Then,theFo¨llmerprocessassociatedwiththe
s s
referenceprocess(Y ) thatsolves(79)istheprocessXF ≡(XgF) thatsolves(57)withg =gF givenby(70).
s s∈[0,1] s s∈[0,1] s s
Theorem3.3isimpliedbythisresult.
Proof. WecloselyfollowthestepsofFo¨llmer’soriginalconstructioninvolvingtime-reversal(Fo¨llmer,1986). Tobegin,
noticethatwecansolve(55)toexpressb intermsofthescore∇logρ . Wecanthenusetheresultingexpressionin(56)to
s s
writetheSDE(57)as
dXg =(cid:0) β−1A−1+ 1(g2−σ2)(cid:1) ∇logρ (Xg|x )ds+β−1c (Xg,x )ds+g dW , Xg =x , (83)
s s s 2 s s s s 0 s s s 0 s s s=0 0
whereA andc (x,x )aredefinedin(11). Usingthescore,wecantime-reverse(83)andderivethefollowingSDEfor
s s 0
XR =d Xg
s 1−s
dXR =−(cid:0) β−1 A−1 − 1(g2 +σ2 )(cid:1) ∇logρ (XR|x )ds−β−1 c (XR,x )ds+g dW . (84)
s 1−s 1−s 2 1−s 1−s 1−s s 0 1−s 1−s s 0 1−s s
21ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
IfwetakeXR ∼ρ (·|x )in(84),thenbyconstructionwehavethatXR =x . Remarkably,ifweuseg =gF withgF
s=0 c 0 √ s=1 0 s s s
givenin(70),andifwechoose1− 1β A σ2 ≥0(i.e. chooseβ /[ sσ ]nondecreasing),theSDE(84)reducesto
2 s s s s s
dXR =−β−1 c (XR,x )ds+gF dW , XR ∼ρ (·|x ). (85)
s 1−s 1−s s 0 1−s s s=0 c 0
Thisreverse-timeSDEhastheremarkablepropertythatitsdriftisindependentofthescore∇logρ , meaningthatthe
s
informationaboutthetargetPDFρ (·|x )onlyentersthroughitsinitialconditionXR ∼ρ (·|x ). Thismeansthatwe
c 0 s=0 c 0
canchangetheinitialconditionin(85)toconstruct(afterreversingtimeback)areferenceprocess. Wecanuseanydensity
forthispurpose,butforsimplicityitisconvenienttochooseaGaussian,andthereforetoconsider
dYR =−β−1 c (YR,x )ds+gF dW , YR ∼N(0,Id). (86)
s 1−s 1−s s 0 1−s s s=0
Usingtheexplicitformofc in(54),wecanwritethisSDEexplicitlyas
s
dYR =−β−1 β˙ (YR−α x )ds−α˙ x +gF dW , YR ∼N(0,Id). (87)
s 1−s 1−s s 1−s 0 1−s 0 1−s s s=0
Usingthatdα =−α˙ dsanddβ =−β˙ ds,wemayrewritethisas
1−s 1−s 1−s 1−s
d(cid:0) β−1 (YR−α x )(cid:1) =β−1 gF dW , YR ∼N(0,Id). (88)
1−s s 1−s 0 1−s 1−s s s=0
Hence,
(cid:90) s
YR =α x +β YR +β β−1 gF dW . (89)
s 1−s 0 1−s s=0 1−s 1−u 1−u u
0
UsingtheexplicitformofgFgivenin(70)togetherwith(71),wededucethat
s
(cid:34)(cid:12)
(cid:90) s
(cid:12)2(cid:35)
(cid:90) s
E (cid:12) (cid:12) (cid:12)β 1−s β 1− −1 ug 1F −udW u(cid:12) (cid:12) (cid:12) =β 12 −s β 1− −2 u|g 1F −u|2du,
0 0
(cid:90) 1
=β2 β−2|gF|2du,
1−s u u
1−s
(cid:90) 1 2uσ2 d β (90)
=β2 u log√ u du,
1−s β2 du uσ
1−s u u
(cid:90) 1 (cid:18) uσ2(cid:19)
=−β2 d u ,
1−s β2
1−s u
=(1−s)σ2 ,
1−s
whereweusedσ =0. Thismeansthat
1
YR =d α x +β z+σ W ∼N(α x ,β2+sσ2), (91)
1−s s 0 s s s s 0 s s
wherez ∼ N(0,Id)withz ⊥ W. Notethat, unsurprisingly, theprocessontheright-handsideissimplythestochastic
interpolant(2)conditionedonx fixed,withx replacedbyaGaussianz. DenotingbyρY(y|x )thePDFofYR ,(91)
0 1 s 0 1−s
impliesthat
y−α x
∇logρY(y|x )=− s 0. (92)
s 0 β2+sσ2
s s
Wecanusethisresulttotimereverse(86)andobtainthefollowingSDEforY =d YR
s 1−s
dY =β−1β˙ (Y −α x )ds+α˙ x +|gF|2∇logρY(Y |x )ds+gFdW Y =x . (93)
s s s s s 0 s 0 s s s 0 s s s=0 0
Ifweinserttheexplicitformof|gF|2 =2sσ (β−1β˙ σ −σ˙ )−σ2givenin(70)into(93)weget
s s s s s s s
dY =a (Y −α x )ds+α˙ x ds+gFdW , Y =x , (94)
s s s s 0 s 0 s s s=0 0
with
2sσ (β−1β˙ σ −σ˙ )−σ2
a =β−1β˙ − s s s s s s, (95)
s s s β2+sσ2
s s
22ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
i.e. werecovertheSDE(79). Sincea
s
= dd slogβ s2+ βssσ s2 ,wecansolvetheabovetheSDEtoobtainthereferenceprocessas
β2+sσ2 (cid:90) s β gF
Y =α x + s s u u dW .
s s 0 β β2+uσ2 u
s 0 u u
Wecanthenverifythat
E(cid:34)(cid:12) (cid:12)
(cid:12)
(cid:12)β s2+
β
sσ s2 (cid:90) s β2β +ug uuF
σ2dW
u(cid:12) (cid:12)
(cid:12)
(cid:12)2(cid:35) =(β s2+
β
sσ s2 )2(cid:90) s (β2β +u2σ σu2 2u u)2dd
ulog
σβ 2u2
udu
s 0 u u s 0 u u u
β2+sσ2 (cid:90) s 1
=( s s)2 d(β2/σ2u) (96)
β ((β2/σ2u)+1)2 u u
s 0 u u
β2+sσ2 β2
=( s s)2 s =β2+sσ2,
β β2+sσ2 s s
s s s
whichimpliesthatY =d α x +β z+σ W ∼N(α x ,β2+sσ2);thismatchesourpreviouscalculation(91).
s s 0 s s s s 0 s s
ItremainstoshowthattheprocessXF ≡XgF definedbysolutiontotheSDE(57)withg =gFgivenin(70)istheFo¨llmer
s s
processassociatedwiththeprocessY definedbythesolutionto(93). Tothisend,recallthattheKLdivergencebetween
twopathmeasuresisinvariantundertime-reversal,sothat
D (Xˇ||Y)=D (XˇR||YR) (97)
KL KL
whereXˇ istheprocessdefinedbythesolutiontotheSDE(81),andwhereXˇRisitstime-reversal.Wecannowusefollowing
decomposition,knownas“disintegration”(Le´onard,2014),oftheKullback-Leiblerdivergence
(cid:90)
D (XˇR||YR)= D (XˇR,x||YR,x)ρ (x|x )dx+D [ρ (·|x )∥N(x ,Id)] (98)
KL KL c 0 KL c 0 0
Rd
whereXˇR,xandYR,xdenote,respectively,theprocessesXˇRandYRconditionedtostartfromx(i.e.onXˇR,x =YR,x =x).
s=0 s=0
Inaddition,weusedthefactthatYR ∼N(0,Id)whereasXˇR ∼ρ (·|x )bytheconstraintimposedintheminimization
s=0 s=0 c 0
problem (82). The second term at the right hand side of (98) is fixed due to this constraint; the first term is always
non-negative,butwecanmakeitzeroifwetakeXˇR =XR withXR definedasthesolutionto(84)sincethisprocessisthe
sameastheonedefinedbythesolutionto(86)ifweconditionbothonXR =YR =x. ThereforeXˇR =XR minimizes
s=0 s=0
(98),whichmeansthatitstime-reversalXgF minimizes(82).
A.7.Analyticformulaofb forGaussianmixturedistributions
s
Whenρ (x|x )isaGaussianmixturemodel(GMM),thedriftb isavailableanalytically:
c 0 s
PropositionA.9. LetthetargetdensitybeaGMMwithJ ∈Nmodes
J
(cid:88)
ρ (x|x )= p N(x;m ,C ) (99)
c 0 j j j
j=1
wherep ≥ 0with(cid:80)J p = 1,m ∈ Rd,andC = CT ∈ Rd×Rd positive-definite(withbothm andC possibly
j j=1 j j j j j j
dependentonx ). Then
0
(cid:80)J
p m N(x;m (s),C (s))
b (x,x )=α˙ x +β˙ j=1 j j j j
s 0 s 0 s (cid:80)J
p N(x;m (s),C (s))
j=1 j j j
(100)
(cid:80)J p (β β˙ C +sσ σ˙ Id)C−1 (s)(x−m (s))N(x;m (s),C (s))
+ j=1 j s s j s s j j j j
(cid:80)J
p N(x;m (s),C (s))
j=1 j j j
where
m (s)=α x +β m , C (s)=β2C +sσ2Id. (101)
j s 0 s j j s j s
23ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
Proof. Bydefinition
b (x,x )=α˙ x +E[β˙ x +σ˙ W |I =x,x ]
s 0 s 0 s 1 s s s 0
=α˙ x +E[β˙ β−1(x−α x −σ W )+σ˙ W |I =x,x ] (102)
s 0 s s s 0 s s s s s 0
=α˙ x +β˙ β−1(x−α x )+sσ (σ β˙ β−1−σ˙ )∇logρ (x|x ).
s 0 s s s 0 s s s s s s 0
whereweusedthefact∇logρ s(x|x 0)=−[sσ s]−1Ex0[W s|I
s
=x]. FortheGMM,
J
(cid:88)
ρ (x|x )= p N(x;m (s),C (s)), (103)
s 0 j j j
j=1
sothat
(cid:80)J
p
C−1
(s)(x−m (s))N(x;m (s),C (s))
∇logρ (x|x )=− j=1 j j j j j . (104)
s 0 (cid:80)J
p N(x;m (s),C (s))
j=1 j j j
Insertingthisexpressionin(102)weobtain
β˙ β˙
s(x−α x )+sσ2 s∇logρ (x|x )
β s 0 sβ s 0
s s
=β˙
s
(cid:32)
x−α x −
(cid:80)J j=1p j(Id−β s2C jC− j1 (s))(x−m j(s))N(x;m j(s),C j(s))(cid:33)
β s s 0 (cid:80)J j=1p jN(x;m j(s),C j(s))
=β˙
s
(cid:32)(cid:80)J j=1p j(cid:0) β sm
j
+β s2C jC− j1 (x−m j)(cid:1) N(x;m j(s),C j(s))(cid:33) (105)
β s (cid:80)J j=1p jN(x;m j(s),C j(s))
(cid:80)J p m N(x;m (s),C (s)) (cid:80)J p β β˙ C C−1 (x−m¯ )N(x;m (s),C (s))
=β˙ j=1 j j j j + j=1 j s s j j j j j ,
s (cid:80)J
p N(x;m (s),C (s))
(cid:80)J
p N(x;m (s),C (s))
j=1 j j j j=1 j j j
whereinthefirstandsecondidentities,weusedthefactthatsσ2Id=Id−β2C C−1 (s)andx−α x =x−m (s)+β m .
s s j j s 0 j s j
Now,usingb (x,x )=α˙ x +β˙ β−1(x−α x )+sσ2(β˙ β−1−σ˙ )∇logρ (x|x ),wegetthefinalformula.
s 0 s 0 s s s 0 s s s s s 0
B.DetailsofNumericalExperiments
B.1.Multi-modaljump-diffusionprocess
WecreatethesyntheticexamplebystartingwithaGaussiancomponentN(m ,C )withm =[5,0]and
0 0 0
(cid:20) (cid:21)
1.5 0
C = .
0 0 0.1
Werotatethisdistributioncounterclockwiseby2π/5fourtimestoobtaintheremainingfourGaussianmodes. Weassign
eachofthefivemodesequalweightstoobtainour2DGaussianmixturemodelwithdensityρ (x)
GMM
The2Dparticlejump-diffusiondynamicsisconstructedasfollows. Betweenthejumps,theparticlemovesaccordingtothe
Langevindynamics
√
dx =∇logρ (x )dt+ 2dW .
t GMM t t
AtjumptimesspecifiedbyaPoissonprocesswithrateλ=2,theparticleisrotatedcounterclockwisebyanangle2π/5.
Wesimulatethisdynamicsusingthefollowingschemewithδt=0.01:
(cid:40) √
x +δt∇logp(x )+ 2δtξ withprobability1−λδt
x = nδt nδt √
(n+1)δt
R (x +δt∇logp(x )+ 2δtξ) withprobabilityλδt
2π/5 nδt nδt
24ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
where ξ ∼ N(0,I ) and R is the counterclockwise rotation operator in 2D with angle 2π/5. We integrate this
2×2 2π/5
dynamicslongenoughtoreachequilibriumandgetenoughdata.
We keep the data at a regular time interval of ∆t = 0.5 and use paired (x ,x ) as training data for learning the
t t+0.5
conditionaldistributionatlagτ =0.5. Intotalwestore105trainingdatapairs. Weuseafullyconnectedneuralnetwork
with5hiddenlayeswithhiddendimension500toapproximatethevelocityfieldb (x,x )intheSDEs. Theinputtothe
s 0
netisofdimension5andtheoutputisofdimension2. Wetrainthenetworkusingabatchsizeof104,defaultAdamW
optimizerwithbaselearningratel=10−3andcosineschedulerthatdecreasesineachepochthelearningrateeventuallyto
0after300epochs. WetesttheSDEsfornewsimulatedtrajectorydata.
B.2.2DStochasticNavier-StokesExample
Wesetν =10−3,α=0.1,ε=1. Weconsiderthefollowingrandomforcing
(cid:0) (cid:1)
η(t,x,y)= W (t)sin(6x)+W (t)cos(7x)+W (t)sin(5(x+y))+W (t)cos(8(x+y))
1 2 3 4
(106)
(cid:0) (cid:1)
+ W (t)cos(6x)+W (t)sin(7x)+W (t)cos(5(x+y))+W (t)sin(8(x+y)) ,
5 6 7 8
whereW (t),1≤i≤8areindependentWienerprocesses. Forsuchrandomforcing,theNSE(17)hasauniqueinvariant
i
measure,asprovedin(Hairer&Mattingly,2006). Notethat(106)isaGaussianrandomfieldwithcovariancefunction
C(t,t′,x,y,x′,y′) = min(t,t′)[cos(6(x−x′))+cos(7(x−x′))+cos(5(x−x′+y−y′)+cos(8(x−x′+y−y′))]
whichistranslationinvariantinspace. Thedampingterm−αωisusedtoacceleratethemixingofthedynamicsasitdamps
thevorticityatlargelength-scales/smallFouriermodestoavoidallenergyaccumulatinginlargevortices.
Weuseapseudo-spectralsolverwithEuler-Maruyamatime-steppingschemetosolvethestochasticPDEintimeandweuse
thejax-cfdpackage(Dresdneretal.,2022)forthemeshgenerationanddomaindiscretization. Weperformsimulations
withagridsizes256×256whichisfineenoughforresolvingthenumericalsolutionsofthe2DstochasticNavier-Stokes
equationswithourspecifications.
Wenoticethatthereareexistingworksonforecastingwithgenerativemodelsthatexperimentwiththe2DNSE(Lienen
etal.,2023)(Cachayetal.,2023);however,theyareinterestedindeterministicforecasting. Incontrast,weareinterested
inprobabilisticforecastingwhichiswhyweaddastochasticforcingtotheequation. Theprecisenatureofthisforcingis
probablynotimportantinpractice. Inallofourexperiments,wenormalizethetrainingandtestingdatasothatonaverage
theL2normofeachsnapshotfieldinthetrainingdatais1;wefoundsuchnormalizationisusefultoensureastablegradient
thatdoesnotblowupduringtraining. WeuseastandardUNetarchitecturepopularizedby(Hoetal.,2020)tolearnthedrift
velocityb . ThetotalparametersfortheUNetisaround2millions. WeemploythedefaultAdamWoptimizer(Loshchilov
θ
&Hutter,2017). Weuseacosineannealingscheduletodecreasethelearningrateineachepoch. Thebaselearningrate
isl =10−3. Wesplitthedatainto90%trainingdataand10%testdata. Weuseabatchsizeof100. Intotalwetrain50
epochs. ThemodelistrainedonasingleNvidiaA100GPUandittakeslessthan1day. Oncetrained,wetestourSDEs
model(e.g. seeFig.3)byexaminingtheconditionaldistributions. Wefix50initialconditionsdrawnfromtheinvariant
measureandrunsimulationsorourSDEmodelstothedesiredlags,andcollectanensembleofsolutions(wechoosethe
numberofensemblestobe300). Wecomputemeanandstdoftheseensemblesandcomparethemwiththoseobtainedby
usingthesimulationdata. Wealsoconsidertheenstrophyspectrumoftheconditionaldistribution,whichmeasuresthesize
ofthevorticityateachscaleandrevealsimportantphysicalinformation. Moreprecisely,wecomputetheenstrophyofthe
vorticityfieldωatwavenumberk ∈R isdefinedas
+
(cid:88)
Enstrophy(k)= |ωˆ(m)|2,
k≤|m|≤k+1
whereωˆ withm ∈ Z2 denotestheFouriercoefficientsofω. Theenstrophyspectrumcurveisthegraphofthefunction
k → Enstrophy(k). Wecanaveragetheenstrophyoverdifferentsimulationtrajectoriesofω togetthespectrumofthe
distributionofthefield. Weoftensmooththecurvetogetridofthesawtooth.
Weprovidesomeadditionalnumericalresultsonthe2DstochasticNavier-Stokesexamplebelow.
Superresolution Weconsidersuperresolution,i.e.,predictingthefieldwitharesolutionof128×128,fromthedownsized
versionwitharesolutionof32×32. InFig.B.2,weshowa32×32resolutionfield,aswellasthetrue128×128resolution
fieldandthemeanofsamplesdrawnfromourSDEs. Ourmethodachievesanoutstandingrecovery. Additionally, we
25ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
calculate the standard deviation of the samples. The spatial distribution of this standard deviation serves as a tool for
uncertaintyquantification;notably,thereexistsapronouncedcorrelationbetweenthepatternofstandarddeviationandthe
vorticityfield.
0.6
0.5
0.4
0.3
32×32 128×128
0.2
0.1
0.0
0 2 4 6 8 10 12 14
Time Lag
Samplemean Samplestd
Figure7.Leftpanel:Superresolution.Low-resolutionandhigh-resolutionfield,meanandstdofsamplesdrawnfromourlearnedSDE
model. Rightpanel: Wecalculatetheauto-correlationofourdataasafunctionoftime. Theauto-correlationfunctioniscomputed
viaaveragingovertheauto-correlationonallthegridpoints.Notethatthisaveragingcanbedonebecauseourrandomforcingtermis
spatiallyhomogeneousandeachgridpointisidenticalinastatisticalsense.Wenoticethattheauto-correlationdropstoabout0.25at
atimelagτ =1andabout0.1atatimelagτ =2,whichindicatesthepotentialdiversityoftheconditionaldistributionandthusthe
difficultyofourforecastingtask.
Forecastingefficiency WeconductedacomparativeanalysisofpredictionspeedbetweenourmodelsandtheSPDEsolver
utilizedforthe2DstochasticNavier-Stokesequation.Forascenarioinvolvingashorttimelagofτ =0.5,whereinourmodel
accuratelycapturestheresultingconditionaldistribution,samplingfromourSDEforecasterwith200Euler-Maruyama
stepstakes0.05seconds,whileexecutingtheSPDEsolveronthesameNvidiaRTX8000GPUrequires8seconds. This
observationhighlightsthatourmethodacceleratesforecastingbyover100timeswithoutsacrificingphysicalinformation
inherentintheconditionaldistribution. Furthermore,ourapproachcouldstandtogainevenmoreaccelerationinscenarios
withlowerviscosity,whichtypicallynecessitatesasubstantiallyreducedtimestepsizefortheSPDEsolver.
Comparisonwithadeterministicapproach Wealsocontrastourprobabilisticforecastingmethodwithadeterministic
approach. Inthedeterministicapproach,weutilizethesameUNetarchitecturetoparameterizeapointestimator,denoted
asω = f(ω ;θ), andtrainitusingtheMeanSquaredError(MSE)lossfunctionE∥ω −ω ∥2, whereτ = 0.5
(cid:98)t+τ t (cid:98)t+τ t+τ
representsthetimelagweaimtopredict. ThisMinimumMeanSquareError(MMSE)estimatorisdesignedtocapture
the mean of the conditional probability distribution. While the deterministic point estimator performs adequately in
recoveringthemean,itfallsshortinlong-termtotalenstrophypredictionduetoitsinabilitytoaccountforfluctuationsin
variance;seeTable2. Moreprecisely,wereporttheshorttime(lag=0.5)erroroftheconditionalmeanprediction: forthe
probabilisticapproachwecomputethemeanofgeneratedsamplesandcompareitwiththetrueconditionalmean,andfor
thedeterministicapproachweusethepointestimatortocompare. Fortheprobabilisticapproach,wecanalsocomputethe
predictedconditionalstdusingsamplesandcomparewiththetruestd. Theseshorttimemetricscharacterizetheaccuracy
oftheforecastinginonelagtime. WecanalsoiteratethelearnedSDEordeterministicmapsformanystepsandusethe
generateddynamicstomakepredictionsontheinvariantdistributionofthetruedynamics. Wedosobyiterating100steps
andcomputetheaveragedtotalenstrophy(whichistheL2 normofthevorticityfield). Wecomparethepredictedtotal
vorticityoftheinvariantdistributionwiththetruth. Theresultdemonstratesthenecessityofusingprobabilisticforecasting
forstochasticdynamicalsystems.
26
noitcnuF
noitalerrocotuAProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
Relativeerror OurSDEs Deterministic
Shorttimeerrorofmean 1.1e-1 2.3e-1
Shorttimeerrorofstd 6.8e-2 –
Longtimeerroroftotalenstrophy 5.6e-3 3.0e-1
Table2.Short-time and long-time relative accuracy comparisons between the SDE approach and the deterministic approach. The
short-timeaccuracyismeasuredbytherelativeL2normbetweenthetrueconditionalmeanandtheconditionalmeanofthesamplesin
ourSDEapproachorthesingleoutputgivenbythedeterministicnetwork,atalagτ =0.5.ForSDEs,wealsohavethepredictionofthe
conditionalstd.Forthelong-timecomparisons,weiteratetheSDEandthedeterministicmapbothfor100stepsandusethetrajectoriesto
estimatetheaveragedtotalenstrophyoftheinvariantdistributionandcomparethemagainstthetruthcomputedusingsamplesfromthe
truesimulationdata.
ComparisonsbetweendifferentSDEgenerativemodels AlltheaboveexperimentsonNSEisdonewiththeinterpolant
α = 1−s,β = s2,σ = 1−s,asitperformsthebest. BelowinFigures8and9,wealsopostthelossandgradient
s s s
norm curves, as well as the enstrophy spectra of the generated samples, for other choices of interpolants and Fo¨llmer
processes (trained with the same network, data, and number of epochs). We observe that β = s2, namely β˙ = 0,
s 0
is important to ensure a stable gradient norm curve. This experiment demonstrates the superiority of the interpolant
α = 1−s,β = s2,σ = 1−s. Moreover,bychangingthediffusioncoefficientsfromσ dW totheoptimalgFdW
s s s s s s s
doesnotinfluencethespectrumssignificantlyinthisexample.
Loss for training the interpolants Grad norm for training the interpolants
=1 s, =s, =1 s 100000 =1 s, =s, =1 s
=1 s, =s2, =1 s =1 s, =s2, =1 s
104
80000
60000
40000
20000
103
0
0 10 20 30 40 50 0 10 20 30 40 50
Epochs Epochs
Figure8.ThetraininglossonthestochasticNavier-Stokesexperimentsandthenormofgradientsofparametersduringthetrainingfor
stochasticinterpolantswithβ =sorβ =s2.Weobservethatthechoiceβ =s2makesthegradientnormmorestable.
s s s
KLdivergencecomparisonsbetweenσ dW ,gFdW ,andGaussianbaseODE Wefixoneinitialvorticityfieldand
s s s s
comparetheKLdivergencebetweenthetrueconditionaldistributionandthegenerateddistribution. HeretheKLdivergence
iscalculatedforthe1Dconditionaldistributionsoftotalenstrophyandenergy,forafixedω andτ =1. Thegoalhereisto
t
testwhetherchangingthediffusioncoefficientfromσ togFcouldimprovetheKLdivergenceaccuracyofthegenerated
s s
distributionats=1. NotethatourtheoryinTheorem3.3showsthatchangingfromσ togFcouldimprovethepathKL
s s
divergence,whichisanupperboundontheKLdivergenceofthegeneratedmarginaldistributionats=1.
WeconsiderthegenerativeSDEscorrespondingtothestochasticinterpolantwithα =1−s,β =s2,σ =1−s. Wecan
s s s
(cid:112)
varythediffusioncoefficientofthegenerativeSDEs;wechooseσ =1−s,orgF = (3−s)(1−s)thatcorrespondsto
s s
aFo¨llmerprocess. WealsocomparetheresultswiththeGaussianbaseODEgenerativemodel(equivalenttoflowmatching
(Lipmanetal.,2022;Liuetal.,2022;Albergo&Vanden-Eijnden,2022),correspondingtoα = 1−s,β = s,andx
s s 0
intheinterpolantobeysanN(0,Id)distribution). ThequantitativeKLresultsarereportedinTableB.2andthedensities
areshowninFigure10. WeobservethatSDEapproachesleadtobetterKLaccuracy. Moreover,changingthediffusion
27ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
101 101 101 101
100 100 100 100
101 101 101 101
102 102 102 102
truth truth truth truth
forecast with sdWs forecast with gsdWs forecast with sdWs forecast with gsdWs
103 103 103 103
100 101 100 101 100 101 100 101
β =s,useσ dW β =s,usegFdW β =s2,useσ dW β =s2,usegFdW
s s s s s s s s s s s s
Figure9.Theenstrophyspectraofobtainedsamplesfromdifferentgenerativemodels,comparedtothetruth,forinterpolantsα =
s
1−s,β =s,σ =1−sandinterpolantsα =1−s,β =s2,σ =1−s.ThegenerativeSDEischosensuchthatthediffusionterm
s s s s s
areσ dW orgFdW .Thespectrumiscomputedbyaveragingover50differentω (independentlydrawnfromtheinvariantdistribution)
s s s s t
and300independentensemblesofω (simulationdata)orωˆ (forecastresults).Allmodelsaretrainedforlagτ =1,withthesame
t+τ t+τ
2M-parameterUnet,200Kdata,andthesametrainingproceduresasoutlinedinthebeginningofthissection. Weobservethatusing
β =s2leadstobetterperformanceintermsofthespectrums;thisindicatesthatthestablegradientnormforβ =s2(seeFigure8)is
s s
importantinthegenerationquality.Changingg fromσ totheoptimalgFdoesnotinfluencethespectrumsignificantly.
s s s
coefficientscouldpotentiallyimprovetheKLaccuracy,justifyingtheflexibilityandusefulnessoftheinterpolantapproach.
true true
0.20 with sdWs 1.50 with sdWs
with g sFdWs with g sFdWs
with GB-ODE 1.25 with GB-ODE
0.15
1.00
0.10 0.75
0.50
0.05
0.25
0.00 0.00
5 10 15 20 25 1.0 1.5 2.0 2.5 3.0 3.5 4.0
densityoftotalenstrophyofω givenω densityoftotalenergyofω givenω
t+τ t t+τ t
Figure10.The 1D conditional distributions of total enstrophy and total energy of ω , given a fixed initial vorticity field ω and
t+τ t
τ = 1. Herewecomparebetweenthetruth,generatedsamplesviaSDEswithσ dW ,viaSDEwithgFdW whichcorrespondsto
s s s s
aFo¨llmerprocess,andviaODEswithGaussianbase(Lipmanetal.,2022;Liuetal.,2022;Albergo&Vanden-Eijnden,2022). We
usekerneldensityestimationbasedon6000ensemblestocomputethedensity. HeretoconstructtheSDEs,weusetheinterpolant
α = 1−s,β = s2,σ = 1−s. QuantitativeKLdivergencebetweenthesedistributionswithtrutharereportedinTableB.2. We
s s s
observethatSDEsleadtobetterdensityqualitycomparedtoODEs. Fortheenstrophy,gFdW behavesbetterwhilefortheenergy,
s s
σ dW worksbetter,forthisfixedω .WeleaveasafutureworktostudytheeffectoftheFo¨llmergFinmoredetails.
s s t s
28ProbabilisticforecastingwithstochasticinterpolantsandFo¨llmerProcesses
KL:truthversusgeneration densityoftotalenstrophy densityoftotalenergy
SDEwithσ dW 8.49e-3±1.57e-3 4.01e-3±8.95e-4
s s
SDEwithgFdW 2.79e-3±9.19e-4 7.21e-3±1.58e-3
s s
GaussianbaseODE 3.63e-3±9.63e-4 2.17e-2±2.50e-3
Table3.ComputedKLdivergencebetweenseveral1Ddistributionsofgeneratedconditionalsamplesandthetruesimulatedsamples.
The1Ddistributionsareregardingtotalenstrophyandtotalenergyofω ,givenafixedω andτ = 1. Wefirstusekerneldensity
t+τ t
estimationbasedon6000ensemblestocomputethedensity,andthencomputetheKLdivergencewithrespecttothetruth. Herewe
comparebetweenthegeneratedsamplesviaSDEswithσ dW ,viaSDEwithgFdW whichcorrespondstoaFo¨llmerprocess,andvia
s s s s
ODEswithGaussianbase(Lipmanetal.,2022;Liuetal.,2022;Albergo&Vanden-Eijnden,2022).HeretoconstructtheSDEs,weuse
theinterpolantα =1−s,β =s2,σ =1−s.Ineverycolumn,thesmallestvalueishighlightedinbold.Thenumbersarepresented
s s s
intheformatmean±stdwherewedobootstraptoresample6000ensembleswithreplacementandgetdifferentKLresultsandreportthe
meanandstd;thisstdprovidesusefulinformationofthesensitivityofKLwithrespecttoresampling.WeobserveSDEperformsbetter
intermsofKL.
B.3.Videogeneration
Algorithm3VideoEM:(Euler-MarayumainLatentSpace).
Input: Modelˆb (y,y ;yt,t);previousframesx1,...,t−1;grids =0<s ···<s =1withN ∈N;iidη ∼N(0,Id)
s 0 0 1 N n
forn=0:N −1.
Set∆s =s −s forn=0:N −1.
n n+1 n
SetY =Encode(xt−1)
0
forn=0:N −1do
Drawj ∈Unif(2,...,t−1)
yt−j =Encode(xt−j)
√
Y =Y +ˆb (Y ,Y ;yt−j,t−j)∆s +σ ∆s η
n+1 n sn n 0 n sn n n
endfor
xˆt =Decode(Y )
N
Return: Generatedframexˆt
29