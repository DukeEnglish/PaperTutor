# cs.CL 

| Item |Content|
| --- |---|
|idx| 2408.01423v1 |
|title| Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting |
|authors| Xiangyu ZhaoChengqian Ma
|links| http://arxiv.org/abs/2408.01423v1 |
|updated| 2024-08-02 17:59:42 UTC |
|summary| Large Language Models LLMs exhibit remarkable proficiency in addressing adiverse array of tasks within the Natural Language Processing NLP domainwith various prompt design strategies significantly augmenting theircapabilities. However these prompts while beneficial each possess inherentlimitations. The primary prompt design methodologies are twofold: The firstexemplified by the Chain of Thought CoT involves manually crafting promptsspecific to individual datasets hence termed Expert-Designed Prompts EDPs.Once these prompts are established they are unalterable and theireffectiveness is capped by the expertise of the human designers. When appliedto LLMs the static nature of EDPs results in a uniform approach to both simpleand complex problems within the same dataset leading to the inefficient use oftokens for straightforward issues. The second method involves promptsautonomously generated by the LLM known as LLM-Derived Prompts LDPs whichprovide tailored solutions to specific problems mitigating the limitations ofEDPs. However LDPs may encounter a decline in performance when tacklingcomplex problems due to the potential for error accumulation during thesolution planning process. To address these challenges we have conceived anovel Prompt Recursive Search PRS framework that leverages the LLM togenerate solutions specific to the problem thereby conserving tokens. Theframework incorporates an assessment of problem complexity and an adjustablestructure ensuring a reduction in the likelihood of errors. We havesubstantiated the efficacy of PRS framework through extensive experiments usingLLMs with different numbers of parameters across a spectrum of datasets invarious domains. Compared to the CoT method the PRS method has increased theaccuracy on the BBH dataset by 8 using Llama3-7B model achieving a 22improvement. |


| Item |Content|
| --- |---|
|idx| 2408.01420v1 |
|title| Mission Impossible: A Statistical Perspective on Jailbreaking LLMs |
|authors| Jingtong SuJulia KempeKaren Ullrich
|links| http://arxiv.org/abs/2408.01420v1 |
|updated| 2024-08-02 17:55:50 UTC |
|summary| Large language models LLMs are trained on a deluge of text data withlimited quality control. As a result LLMs can exhibit unintended or evenharmful behaviours such as leaking information fake news or hate speech.Countermeasures commonly referred to as preference alignment includefine-tuning the pretrained LLMs with carefully crafted text examples of desiredbehaviour. Even then empirical evidence shows preference aligned LLMs can beenticed to harmful behaviour. This so called jailbreaking of LLMs is typicallyachieved by adversarially modifying the input prompt to the LLM. Our paperprovides theoretical insights into the phenomenon of preference alignment andjailbreaking from a statistical perspective. Under our framework we first showthat pretrained LLMs will mimic harmful behaviour if present in the trainingcorpus. Under that same framework we then introduce a statistical notion ofalignment and lower-bound the jailbreaking probability showing that it isunpreventable under reasonable assumptions. Based on our insights we proposean alteration to the currently prevalent alignment strategy RLHF. Specificallywe introduce a simple modification to the RLHF objective we call E-RLHF thataims to increase the likelihood of safe responses. E-RLHF brings no additionaltraining cost and is compatible with other methods. Empirically wedemonstrate that E-RLHF outperforms RLHF on all alignment problems put forwardby the AdvBench and HarmBench project without sacrificing model performance asmeasured by the MT-Bench project. |


| Item |Content|
| --- |---|
|idx| 2408.01419v1 |
|title| DebateQA: Evaluating Question Answering on Debatable Knowledge |
|authors| Rongwu XuXuan QiZehan QiWei XuZhijiang Guo
|links| http://arxiv.org/abs/2408.01419v1 |
|updated| 2024-08-02 17:54:34 UTC |
|summary| The rise of large language models LLMs has enabled us to seek answers toinherently debatable questions on LLM chatbots necessitating a reliable way toevaluate their ability. However traditional QA benchmarks assume fixed answersare inadequate for this purpose. To address this we introduce DebateQA adataset of 2941 debatable questions each accompanied by multiplehuman-annotated partial answers that capture a variety of perspectives. Wedevelop two metrics: Perspective Diversity which evaluates thecomprehensiveness of perspectives and Dispute Awareness which assesses if theLLM acknowledges the questions debatable nature. Experiments demonstrate thatboth metrics align with human preferences and are stable across differentunderlying models. Using DebateQA with two metrics we assess 12 popular LLMsand retrieval-augmented generation methods. Our findings reveal that while LLMsgenerally excel at recognizing debatable issues their ability to providecomprehensive answers encompassing diverse perspectives varies considerably. |


| Item |Content|
| --- |---|
|idx| 2408.01417v1 |
|title| Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs |
|authors| Yilun HuaYoav Artzi
|links| http://arxiv.org/abs/2408.01417v1 |
|updated| 2024-08-02 17:51:57 UTC |
|summary| Humans spontaneously use increasingly efficient language as interactionsprogress by adapting and forming ad-hoc conventions. This phenomenon has beenstudied extensively using reference games showing properties of human languagethat go beyond relaying intents. It remains unexplored whether multimodal largelanguage models MLLMs similarly increase communication efficiency duringinteractions and what mechanisms they may adopt for this purpose. We introduceICCA an automated framework to evaluate such conversational adaptation as anin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs andobserve that while they may understand the increasingly efficient language oftheir interlocutor they do not spontaneously make their own language moreefficient over time. This latter ability can only be elicited in some modelse.g. GPT-4 with heavy-handed prompting. This shows that this property oflinguistic interaction does not arise from current training regimes eventhough it is a common hallmark of human language. ICCA is available athttps://github.com/lil-lab/ICCA. |


| Item |Content|
| --- |---|
|idx| 2408.01402v1 |
|title| Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer |
|authors| Yu YangPan Xu
|links| http://arxiv.org/abs/2408.01402v1 |
|updated| 2024-08-02 17:25:34 UTC |
|summary| Decision Transformer DT has emerged as a promising class of algorithms inoffline reinforcement learning RL tasks leveraging pre-collected datasetsand Transformers capability to model long sequences. Recent works havedemonstrated that using parts of trajectories from training tasks as prompts inDT enhances its performance on unseen tasks giving rise to Prompt-DT methods.However collecting data from specific environments can be both costly andunsafe in many scenarios leading to suboptimal performance and limitedfew-shot prompt abilities due to the data-hungry nature of Transformer-basedmodels. Additionally the limited datasets used in pre-training make itchallenging for Prompt-DT type of methods to distinguish between various RLtasks through prompts alone. To address these challenges we introduce theLanguage model-initialized Prompt Decision Transformer LPDT which leveragespre-trained language models for meta-RL tasks and fine-tunes the model usingLow-rank Adaptation LoRA. We further incorporate prompt regularization toeffectively differentiate between tasks based on prompt featurerepresentations. Our approach integrates pre-trained language model and RLtasks seamlessly. Extensive empirical studies demonstrate that initializingwith a pre-trained language model significantly enhances the performance ofPrompt-DT on unseen tasks compared to baseline methods. |


# cs.AI 

| Item |Content|
| --- |---|
|idx| 2408.01423v1 |
|title| Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting |
|authors| Xiangyu ZhaoChengqian Ma
|links| http://arxiv.org/abs/2408.01423v1 |
|updated| 2024-08-02 17:59:42 UTC |
|summary| Large Language Models LLMs exhibit remarkable proficiency in addressing adiverse array of tasks within the Natural Language Processing NLP domainwith various prompt design strategies significantly augmenting theircapabilities. However these prompts while beneficial each possess inherentlimitations. The primary prompt design methodologies are twofold: The firstexemplified by the Chain of Thought CoT involves manually crafting promptsspecific to individual datasets hence termed Expert-Designed Prompts EDPs.Once these prompts are established they are unalterable and theireffectiveness is capped by the expertise of the human designers. When appliedto LLMs the static nature of EDPs results in a uniform approach to both simpleand complex problems within the same dataset leading to the inefficient use oftokens for straightforward issues. The second method involves promptsautonomously generated by the LLM known as LLM-Derived Prompts LDPs whichprovide tailored solutions to specific problems mitigating the limitations ofEDPs. However LDPs may encounter a decline in performance when tacklingcomplex problems due to the potential for error accumulation during thesolution planning process. To address these challenges we have conceived anovel Prompt Recursive Search PRS framework that leverages the LLM togenerate solutions specific to the problem thereby conserving tokens. Theframework incorporates an assessment of problem complexity and an adjustablestructure ensuring a reduction in the likelihood of errors. We havesubstantiated the efficacy of PRS framework through extensive experiments usingLLMs with different numbers of parameters across a spectrum of datasets invarious domains. Compared to the CoT method the PRS method has increased theaccuracy on the BBH dataset by 8 using Llama3-7B model achieving a 22improvement. |


| Item |Content|
| --- |---|
|idx| 2408.01420v1 |
|title| Mission Impossible: A Statistical Perspective on Jailbreaking LLMs |
|authors| Jingtong SuJulia KempeKaren Ullrich
|links| http://arxiv.org/abs/2408.01420v1 |
|updated| 2024-08-02 17:55:50 UTC |
|summary| Large language models LLMs are trained on a deluge of text data withlimited quality control. As a result LLMs can exhibit unintended or evenharmful behaviours such as leaking information fake news or hate speech.Countermeasures commonly referred to as preference alignment includefine-tuning the pretrained LLMs with carefully crafted text examples of desiredbehaviour. Even then empirical evidence shows preference aligned LLMs can beenticed to harmful behaviour. This so called jailbreaking of LLMs is typicallyachieved by adversarially modifying the input prompt to the LLM. Our paperprovides theoretical insights into the phenomenon of preference alignment andjailbreaking from a statistical perspective. Under our framework we first showthat pretrained LLMs will mimic harmful behaviour if present in the trainingcorpus. Under that same framework we then introduce a statistical notion ofalignment and lower-bound the jailbreaking probability showing that it isunpreventable under reasonable assumptions. Based on our insights we proposean alteration to the currently prevalent alignment strategy RLHF. Specificallywe introduce a simple modification to the RLHF objective we call E-RLHF thataims to increase the likelihood of safe responses. E-RLHF brings no additionaltraining cost and is compatible with other methods. Empirically wedemonstrate that E-RLHF outperforms RLHF on all alignment problems put forwardby the AdvBench and HarmBench project without sacrificing model performance asmeasured by the MT-Bench project. |


| Item |Content|
| --- |---|
|idx| 2408.01417v1 |
|title| Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs |
|authors| Yilun HuaYoav Artzi
|links| http://arxiv.org/abs/2408.01417v1 |
|updated| 2024-08-02 17:51:57 UTC |
|summary| Humans spontaneously use increasingly efficient language as interactionsprogress by adapting and forming ad-hoc conventions. This phenomenon has beenstudied extensively using reference games showing properties of human languagethat go beyond relaying intents. It remains unexplored whether multimodal largelanguage models MLLMs similarly increase communication efficiency duringinteractions and what mechanisms they may adopt for this purpose. We introduceICCA an automated framework to evaluate such conversational adaptation as anin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs andobserve that while they may understand the increasingly efficient language oftheir interlocutor they do not spontaneously make their own language moreefficient over time. This latter ability can only be elicited in some modelse.g. GPT-4 with heavy-handed prompting. This shows that this property oflinguistic interaction does not arise from current training regimes eventhough it is a common hallmark of human language. ICCA is available athttps://github.com/lil-lab/ICCA. |


| Item |Content|
| --- |---|
|idx| 2408.01416v1 |
|title| The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability |
|authors| Aaron MuellerJannik BrinkmannMillicent LiSamuel MarksKoyena PalNikhil PrakashCan RagerAruna SankaranarayananArnab Sen SharmaJiuding SunEric ToddDavid BauYonatan Belinkov
|links| http://arxiv.org/abs/2408.01416v1 |
|updated| 2024-08-02 17:51:42 UTC |
|summary| Interpretability provides a toolset for understanding how and why neuralnetworks behave in certain ways. However there is little unity in the field:most studies employ ad-hoc evaluations and do not share theoreticalfoundations making it difficult to measure progress and compare the pros andcons of different techniques. Furthermore while mechanistic understanding isfrequently discussed the basic causal units underlying these mechanisms areoften not explicitly defined. In this paper we propose a perspective oninterpretability research grounded in causal mediation analysis. Specificallywe describe the history and current state of interpretability taxonomizedaccording to the types of causal units mediators employed as well as methodsused to search over mediators. We discuss the pros and cons of each mediatorproviding insights as to when particular kinds of mediators and search methodsare most appropriate depending on the goals of a given study. We argue thatthis framing yields a more cohesive narrative of the field as well asactionable insights for future work. Specifically we recommend a focus ondiscovering new mediators with better trade-offs between human-interpretabilityand compute-efficiency and which can uncover more sophisticated abstractionsfrom neural networks than the primarily linear mediators employed in currentwork. We also argue for more standardized evaluations that enable principledcomparisons across mediator types such that we can better understand whenparticular causal units are better suited to particular use cases. |


| Item |Content|
| --- |---|
|idx| 2408.01415v1 |
|title| Conditional LoRA Parameter Generation |
|authors| Xiaolong JinKai WangDongwen TangWangbo ZhaoYukun ZhouJunshu TangYang You
|links| http://arxiv.org/abs/2408.01415v1 |
|updated| 2024-08-02 17:43:34 UTC |
|summary| Generative models have achieved remarkable success in image video and textdomains. Inspired by this researchers have explored utilizing generativemodels to generate neural network parameters. However these efforts have beenlimited by the parameter size and the practicality of generatinghigh-performance parameters. In this paper we propose COND P-DIFF a novelapproach that demonstrates the feasibility of controllable high-performanceparameter generation particularly for LoRA Low-Rank Adaptation weightsduring the fine-tuning process. Specifically we employ an autoencoder toextract efficient latent representations for parameters. We then train aconditional latent diffusion model to synthesize high-performing modelparameters from random noise based on specific task conditions. Experimentalresults in both computer vision and natural language processing domainsconsistently demonstrate that COND P-DIFF can generate high-performanceparameters conditioned on the given task. Moreover we observe that theparameter distribution generated by COND P-DIFF exhibits differences comparedto the distribution obtained through normal optimization methods indicating acertain level of generalization capability. Our work paves the way for furtherexploration of condition-driven parameter generation offering a promisingdirection for task-specific adaptation of neural networks. |


# cs.LG 

| Item |Content|
| --- |---|
|idx| 2408.01420v1 |
|title| Mission Impossible: A Statistical Perspective on Jailbreaking LLMs |
|authors| Jingtong SuJulia KempeKaren Ullrich
|links| http://arxiv.org/abs/2408.01420v1 |
|updated| 2024-08-02 17:55:50 UTC |
|summary| Large language models LLMs are trained on a deluge of text data withlimited quality control. As a result LLMs can exhibit unintended or evenharmful behaviours such as leaking information fake news or hate speech.Countermeasures commonly referred to as preference alignment includefine-tuning the pretrained LLMs with carefully crafted text examples of desiredbehaviour. Even then empirical evidence shows preference aligned LLMs can beenticed to harmful behaviour. This so called jailbreaking of LLMs is typicallyachieved by adversarially modifying the input prompt to the LLM. Our paperprovides theoretical insights into the phenomenon of preference alignment andjailbreaking from a statistical perspective. Under our framework we first showthat pretrained LLMs will mimic harmful behaviour if present in the trainingcorpus. Under that same framework we then introduce a statistical notion ofalignment and lower-bound the jailbreaking probability showing that it isunpreventable under reasonable assumptions. Based on our insights we proposean alteration to the currently prevalent alignment strategy RLHF. Specificallywe introduce a simple modification to the RLHF objective we call E-RLHF thataims to increase the likelihood of safe responses. E-RLHF brings no additionaltraining cost and is compatible with other methods. Empirically wedemonstrate that E-RLHF outperforms RLHF on all alignment problems put forwardby the AdvBench and HarmBench project without sacrificing model performance asmeasured by the MT-Bench project. |


| Item |Content|
| --- |---|
|idx| 2408.01417v1 |
|title| Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs |
|authors| Yilun HuaYoav Artzi
|links| http://arxiv.org/abs/2408.01417v1 |
|updated| 2024-08-02 17:51:57 UTC |
|summary| Humans spontaneously use increasingly efficient language as interactionsprogress by adapting and forming ad-hoc conventions. This phenomenon has beenstudied extensively using reference games showing properties of human languagethat go beyond relaying intents. It remains unexplored whether multimodal largelanguage models MLLMs similarly increase communication efficiency duringinteractions and what mechanisms they may adopt for this purpose. We introduceICCA an automated framework to evaluate such conversational adaptation as anin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs andobserve that while they may understand the increasingly efficient language oftheir interlocutor they do not spontaneously make their own language moreefficient over time. This latter ability can only be elicited in some modelse.g. GPT-4 with heavy-handed prompting. This shows that this property oflinguistic interaction does not arise from current training regimes eventhough it is a common hallmark of human language. ICCA is available athttps://github.com/lil-lab/ICCA. |


| Item |Content|
| --- |---|
|idx| 2408.01416v1 |
|title| The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability |
|authors| Aaron MuellerJannik BrinkmannMillicent LiSamuel MarksKoyena PalNikhil PrakashCan RagerAruna SankaranarayananArnab Sen SharmaJiuding SunEric ToddDavid BauYonatan Belinkov
|links| http://arxiv.org/abs/2408.01416v1 |
|updated| 2024-08-02 17:51:42 UTC |
|summary| Interpretability provides a toolset for understanding how and why neuralnetworks behave in certain ways. However there is little unity in the field:most studies employ ad-hoc evaluations and do not share theoreticalfoundations making it difficult to measure progress and compare the pros andcons of different techniques. Furthermore while mechanistic understanding isfrequently discussed the basic causal units underlying these mechanisms areoften not explicitly defined. In this paper we propose a perspective oninterpretability research grounded in causal mediation analysis. Specificallywe describe the history and current state of interpretability taxonomizedaccording to the types of causal units mediators employed as well as methodsused to search over mediators. We discuss the pros and cons of each mediatorproviding insights as to when particular kinds of mediators and search methodsare most appropriate depending on the goals of a given study. We argue thatthis framing yields a more cohesive narrative of the field as well asactionable insights for future work. Specifically we recommend a focus ondiscovering new mediators with better trade-offs between human-interpretabilityand compute-efficiency and which can uncover more sophisticated abstractionsfrom neural networks than the primarily linear mediators employed in currentwork. We also argue for more standardized evaluations that enable principledcomparisons across mediator types such that we can better understand whenparticular causal units are better suited to particular use cases. |


| Item |Content|
| --- |---|
|idx| 2408.01415v1 |
|title| Conditional LoRA Parameter Generation |
|authors| Xiaolong JinKai WangDongwen TangWangbo ZhaoYukun ZhouJunshu TangYang You
|links| http://arxiv.org/abs/2408.01415v1 |
|updated| 2024-08-02 17:43:34 UTC |
|summary| Generative models have achieved remarkable success in image video and textdomains. Inspired by this researchers have explored utilizing generativemodels to generate neural network parameters. However these efforts have beenlimited by the parameter size and the practicality of generatinghigh-performance parameters. In this paper we propose COND P-DIFF a novelapproach that demonstrates the feasibility of controllable high-performanceparameter generation particularly for LoRA Low-Rank Adaptation weightsduring the fine-tuning process. Specifically we employ an autoencoder toextract efficient latent representations for parameters. We then train aconditional latent diffusion model to synthesize high-performing modelparameters from random noise based on specific task conditions. Experimentalresults in both computer vision and natural language processing domainsconsistently demonstrate that COND P-DIFF can generate high-performanceparameters conditioned on the given task. Moreover we observe that theparameter distribution generated by COND P-DIFF exhibits differences comparedto the distribution obtained through normal optimization methods indicating acertain level of generalization capability. Our work paves the way for furtherexploration of condition-driven parameter generation offering a promisingdirection for task-specific adaptation of neural networks. |


| Item |Content|
| --- |---|
|idx| 2408.01408v1 |
|title| Derivation of Back-propagation for Graph Convolutional Networks using Matrix Calculus and its Application to Explainable Artificial Intelligence |
|authors| Yen-Che HsiaoRongting YueAbhishek Dutta
|links| http://arxiv.org/abs/2408.01408v1 |
|updated| 2024-08-02 17:33:52 UTC |
|summary| This paper provides a comprehensive and detailed derivation of thebackpropagation algorithm for graph convolutional neural networks using matrixcalculus. The derivation is extended to include arbitrary element-wiseactivation functions and an arbitrary number of layers. The study addresses twofundamental problems namely node classification and link prediction. Tovalidate our method we compare it with reverse-mode automatic differentiation.The experimental results demonstrate that the median sum of squared errors ofthe updated weight matrices when comparing our method to the approach usingreverse-mode automatic differentiation falls within the range of 10-18 to10-14. These outcomes are obtained from conducting experiments on afive-layer graph convolutional network applied to a node classificationproblem on Zacharys karate club social network and a link prediction problemon a drug-drug interaction network. Finally we show how the derivedclosed-form solution can facilitate the development of explainable AI andsensitivity analysis. |


# cs.CV 

| Item |Content|
| --- |---|
|idx| 2408.01417v1 |
|title| Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs |
|authors| Yilun HuaYoav Artzi
|links| http://arxiv.org/abs/2408.01417v1 |
|updated| 2024-08-02 17:51:57 UTC |
|summary| Humans spontaneously use increasingly efficient language as interactionsprogress by adapting and forming ad-hoc conventions. This phenomenon has beenstudied extensively using reference games showing properties of human languagethat go beyond relaying intents. It remains unexplored whether multimodal largelanguage models MLLMs similarly increase communication efficiency duringinteractions and what mechanisms they may adopt for this purpose. We introduceICCA an automated framework to evaluate such conversational adaptation as anin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs andobserve that while they may understand the increasingly efficient language oftheir interlocutor they do not spontaneously make their own language moreefficient over time. This latter ability can only be elicited in some modelse.g. GPT-4 with heavy-handed prompting. This shows that this property oflinguistic interaction does not arise from current training regimes eventhough it is a common hallmark of human language. ICCA is available athttps://github.com/lil-lab/ICCA. |


| Item |Content|
| --- |---|
|idx| 2408.01384v1 |
|title| NOLO: Navigate Only Look Once |
|authors| Bohan ZhouJiangxing WangZongqing Lu
|links| http://arxiv.org/abs/2408.01384v1 |
|updated| 2024-08-02 16:41:34 UTC |
|summary| The in-context learning ability of Transformer models has brought newpossibilities to visual navigation. In this paper we focus on the videonavigation setting where an in-context navigation policy needs to be learnedpurely from videos in an offline manner without access to the actualenvironment. For this setting we propose Navigate Only Look Once NOLO amethod for learning a navigation policy that possesses the in-context abilityand adapts to new scenes by taking corresponding context videos as inputwithout finetuning or re-training. To enable learning from videos we firstpropose a pseudo action labeling procedure using optical flow to recover theaction label from egocentric videos. Then offline reinforcement learning isapplied to learn the navigation policy. Through extensive experiments ondifferent scenes we show that our algorithm outperforms baselines by a largemargin which demonstrates the in-context learning ability of the learnedpolicy. |


| Item |Content|
| --- |---|
|idx| 2408.01372v1 |
|title| Spatial-Spectral Morphological Mamba for Hyperspectral Image Classification |
|authors| Muhammad AhmadMuhammad Hassaan Farooq ButtMuhammad UsamaAdil Mehmood KhanManual MazzaraSalvatore Distenano
|links| http://arxiv.org/abs/2408.01372v1 |
|updated| 2024-08-02 16:28:51 UTC |
|summary| In recent years Transformers have garnered significant attention forHyperspectral Image Classification HSIC due to their self-attentionmechanism which provides strong classification performance. However thesemodels face major challenges in computational efficiency as their complexityincreases quadratically with the sequence length. The Mamba architectureleveraging a State Space Model offers a more efficient alternative toTransformers. This paper introduces the Spatial-Spectral Morphological MambaMorpMamba model. In the MorpMamba model a token generation module firstconverts the Hyperspectral Image HSI patch into spatial-spectral tokens.These tokens are then processed by a morphology block which computesstructural and shape information using depthwise separable convolutionaloperations. The extracted information is enhanced in a feature enhancementmodule that adjusts the spatial and spectral tokens based on the center regionof the HSI sample allowing for effective information fusion within each block.Subsequently the tokens are refined in a multi-head self-attention block tofurther improve the feature space. Finally the combined information is fedinto the state space block for classification and the creation of the groundtruth map. Experiments on widely used Hyperspectral HS datasets demonstratethat the MorpMamba model outperforms parametric efficiency both CNN andTransformer models. |


| Item |Content|
| --- |---|
|idx| 2408.01370v1 |
|title| EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using Windowed Nonlinear Optimization |
|authors| Runze YuanTao LiuZijia DaiYi-Fan ZuoLaurent Kneip
|links| http://arxiv.org/abs/2408.01370v1 |
|updated| 2024-08-02 16:24:55 UTC |
|summary| Event cameras are an interesting visual exteroceptive sensor that reacts tobrightness changes rather than integrating absolute image intensities. Owing tothis design the sensor exhibits strong performance in situations ofchallenging dynamics and illumination conditions. While event-basedsimultaneous tracking and mapping remains a challenging problem a number ofrecent works have pointed out the sensors suitability for prior map-basedtracking. By making use of cross-modal registration paradigms the camerasego-motion can be tracked across a large spectrum of illumination and dynamicsconditions on top of accurate maps that have been created a priori by moretraditional sensors. The present paper follows up on a recently introducedevent-based geometric semi-dense tracking paradigm and proposes the additionof inertial signals in order to robustify the estimation. More specificallythe added signals provide strong cues for pose initialization as well asregularization during windowed multi-frame tracking. As a result the proposedframework achieves increased performance under challenging illuminationconditions as well as a reduction of the rate at which intermediate eventrepresentations need to be registered in order to maintain stable trackingacross highly dynamic sequences. Our evaluation focuses on a diverse set ofreal world sequences and comprises a comparison of our proposed method againsta purely event-based alternative running at different rates. |


| Item |Content|
| --- |---|
|idx| 2408.01366v1 |
|title| Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation |
|authors| Ruoxuan FengDi HuWenke MaXuelong Li
|links| http://arxiv.org/abs/2408.01366v1 |
|updated| 2024-08-02 16:20:56 UTC |
|summary| Humans possess a remarkable talent for flexibly alternating to differentsenses when interacting with the environment. Picture a chef skillfully gaugingthe timing of ingredient additions and controlling the heat according to thecolors sounds and aromas seamlessly navigating through every stage of thecomplex cooking process. This ability is founded upon a thorough comprehensionof task stages as achieving the sub-goal within each stage can necessitate theutilization of different senses. In order to endow robots with similar abilitywe incorporate the task stages divided by sub-goals into the imitation learningprocess to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot astage-guided dynamic multi-sensory fusion method with coarse-to-fine stageunderstanding which dynamically adjusts the priority of modalities based onthe fine-grained state within the predicted current stage. We train a robotsystem equipped with visual auditory and tactile sensors to accomplishchallenging robotic manipulation tasks: pouring and peg insertion with keyway.Experimental results indicate that our approach enables more effective andexplainable dynamic fusion aligning more closely with the human fusion processthan existing methods. |


# stat.ML 

| Item |Content|
| --- |---|
|idx| 2408.01379v1 |
|title| Resampling and averaging coordinates on data |
|authors| Andrew J. BlumbergMathieu CarriereJun Hou FungMichael A. Mandell
|links| http://arxiv.org/abs/2408.01379v1 |
|updated| 2024-08-02 16:37:33 UTC |
|summary| We introduce algorithms for robustly computing intrinsic coordinates on pointclouds. Our approach relies on generating many candidate coordinates bysubsampling the data and varying hyperparameters of the embedding algorithme.g. manifold learning. We then identify a subset of representativeembeddings by clustering the collection of candidate coordinates and usingshape descriptors from topological data analysis. The final output is theembedding obtained as an average of the representative embeddings usinggeneralized Procrustes analysis. We validate our algorithm on both syntheticdata and experimental measurements from genomics demonstrating robustness tonoise and outliers. |


| Item |Content|
| --- |---|
|idx| 2408.01367v1 |
|title| Transformers are Universal In-context Learners |
|authors| Takashi FuruyaMaarten V. de HoopGabriel Peyré
|links| http://arxiv.org/abs/2408.01367v1 |
|updated| 2024-08-02 16:21:48 UTC |
|summary| Transformers are deep architectures that define in-context mappings whichenable predicting new tokens based on a given set of tokens such as a promptin NLP applications or a set of patches for vision transformers. This workstudies in particular the ability of these architectures to handle anarbitrarily large number of context tokens. To mathematically and uniformlyaddress the expressivity of these architectures we consider the case that themappings are conditioned on a context represented by a probability distributionof tokens discrete for a finite number of tokens. The related notion ofsmoothness corresponds to continuity in terms of the Wasserstein distancebetween these contexts. We demonstrate that deep transformers are universal andcan approximate continuous in-context mappings to arbitrary precisionuniformly over compact token domains. A key aspect of our results compared toexisting findings is that for a fixed precision a single transformer canoperate on an arbitrary even infinite number of tokens. Additionally itoperates with a fixed embedding dimension of tokens this dimension does notincrease with precision and a fixed number of heads proportional to thedimension. The use of MLP layers between multi-head attention layers is alsoexplicitly controlled. |


| Item |Content|
| --- |---|
|idx| 2408.01362v1 |
|title| Autoencoders in Function Space |
|authors| Justin BunkerMark GirolamiHefin LambleyAndrew M. StuartT. J. Sullivan
|links| http://arxiv.org/abs/2408.01362v1 |
|updated| 2024-08-02 16:13:51 UTC |
|summary| Autoencoders have found widespread application in both their originaldeterministic form and in their variational formulation VAEs. In scientificapplications it is often of interest to consider data that are comprised offunctions the same perspective is useful in image processing. In practicediscretisation of differential equations arising in the sciences orpixellation of images renders problems finite dimensional but conceivingfirst of algorithms that operate on functions and only then discretising orpixellating leads to better algorithms that smoothly operate between differentlevels of discretisation or pixellation. In this paper function-space versionsof the autoencoder FAE and variational autoencoder FVAE are introducedanalysed and deployed. Well-definedness of the objective function governingVAEs is a subtle issue even in finite dimension and more so on functionspace. The FVAE objective is well defined whenever the data distribution iscompatible with the chosen generative model this happens for example whenthe data arise from a stochastic differential equation. The FAE objective isvalid much more broadly and can be straightforwardly applied to data governedby differential equations. Pairing these objectives with neural operatorarchitectures which can thus be evaluated on any mesh enables newapplications of autoencoders to inpainting superresolution and generativemodelling of scientific data. |


| Item |Content|
| --- |---|
|idx| 2408.01336v1 |
|title| Sparse Linear Regression when Noises and Covariates are Heavy-Tailed and Contaminated by Outliers |
|authors| Takeyuki SasaiHironori Fujisawa
|links| http://arxiv.org/abs/2408.01336v1 |
|updated| 2024-08-02 15:33:04 UTC |
|summary| We investigate a problem estimating coefficients of linear regression undersparsity assumption when covariates and noises are sampled from heavy taileddistributions. Additionally we consider the situation where not onlycovariates and noises are sampled from heavy tailed distributions but alsocontaminated by outliers. Our estimators can be computed efficiently andexhibit sharp error bounds. |


| Item |Content|
| --- |---|
|idx| 2408.01318v1 |
|title| Point Prediction for Streaming Data |
|authors| Aleena ChandaN. V. VinodchandranBertrand Clarke
|links| http://arxiv.org/abs/2408.01318v1 |
|updated| 2024-08-02 15:12:52 UTC |
|summary| We present two new approaches for point prediction with streaming data. Oneis based on the Count-Min sketch CMS and the other is based on Gaussianprocess priors with a random bias. These methods are intended for the mostgeneral predictive problems where no true model can be usefully formulated forthe data stream. In statistical contexts this is often called themathcalM-open problem class. Under the assumption that the data consistsof i.i.d samples from a fixed distribution function F we show that theCMS-based estimates of the distribution function are consistent.  We compare our new methods with two established predictors in terms ofcumulative L1 error. One is based on the Shtarkov solution often called thenormalized maximum likelihood in the normal experts setting and the other isbased on Dirichlet process priors. These comparisons are for two cases. Thefirst is one-pass meaning that the updating of the predictors is done using thefact that the CMS is a sketch. For predictors that are not one-pass we usestreaming K-means to give a representative subset of fixed size that can beupdated as data accumulate.  Preliminary computational work suggests that the one-pass median version ofthe CMS method is rarely outperformed by the other methods for sufficientlycomplex data. We also find that predictors based on Gaussian process priorswith random biases perform well. The Shtarkov predictors we use here did notperform as well probably because we were only using the simplest example. Theother predictors seemed to perform well mainly when the data did not look likethey came from an M-open data generator. |


# cs.HC 

| Item |Content|
| --- |---|
|idx| 2408.01334v1 |
|title| A Backbone for Long-Horizon Robot Task Understanding |
|authors| Xiaoshuai ChenWei ChenDongmyoung LeeYukun GeNicolas RojasPetar Kormushev
|links| http://arxiv.org/abs/2408.01334v1 |
|updated| 2024-08-02 15:32:42 UTC |
|summary| End-to-end robot learning particularly for long-horizon tasks often resultsin unpredictable outcomes and poor generalization. To address these challengeswe propose a novel Therblig-based Backbone Framework TBBF to enhance robottask understanding and transferability. This framework uses therbligs basicaction elements as the backbone to decompose high-level robot tasks intoelemental robot configurations which are then integrated with currentfoundation models to improve task understanding. The approach consists of twostages: offline training and online testing. During the offline training stagewe developed the Meta-RGate SynerFusion MGSF network for accurate therbligsegmentation across various tasks. In the online testing stage after aone-shot demonstration of a new task is collected our MGSF network extractshigh-level knowledge which is then encoded into the image using ActionRegistration ActionREG. Additionally the Large Language ModelLLM-Alignment Policy for Visual Correction LAP-VC is employed to ensureprecise action execution facilitating trajectory transfer in novel robotscenarios. Experimental results validate these methods achieving 94.37 recallin therblig segmentation and success rates of 94.4 and 80 in real-worldonline robot testing for simple and complex scenarios respectively.Supplementary material is available at:https://sites.google.com/view/therbligsbasedbackbone/home |


| Item |Content|
| --- |---|
|idx| 2408.01272v1 |
|title| Does This Have a Particular Meaning? Interactive Pattern Explanation for Network Visualizations |
|authors| Xinhuan ShuAlexis PisterJunxiu TangFanny ChevalierBenjamin Bach
|links| http://arxiv.org/abs/2408.01272v1 |
|updated| 2024-08-02 13:50:15 UTC |
|summary| This paper presents an interactive technique to explain visual patterns innetwork visualizations to analysts who do not understand these visualizationsand who are learning to read them. Learning a visualization requires masteringits visual grammar and decoding information presented through visual marksgraphical encodings and spatial configurations. To help people learn networkvisualization designs and extract meaningful information we introduce theconcept of interactive pattern explanation that allows viewers to select anarbitrary area in a visualization then automatically mines the underlying datapatterns and explains both visual and data patterns present in the viewersselection. In a qualitative and a quantitative user study with a total of 32participants we compare interactive pattern explanations to textual-only andvisual-only cheatsheets explanations. Our results show that interactiveexplanations increase learning of i unfamiliar visualizations ii patterns innetwork science and iii the respective network terminology. |


| Item |Content|
| --- |---|
|idx| 2408.01263v1 |
|title| The virtual CAT: A tool for algorithmic thinking assessment in Swiss compulsory education |
|authors| Giorgia AdorniAlberto Piatti
|links| http://arxiv.org/abs/2408.01263v1 |
|updated| 2024-08-02 13:36:17 UTC |
|summary| In todays digital era holding algorithmic thinking AT skills is crucialnot only in computer science-related fields. These abilities enable individualsto break down complex problems into more manageable steps and create a sequenceof actions to solve them. To address the increasing demand for AT assessmentsin educational settings and the limitations of current methods this paperintroduces the virtual Cross Array Task CAT a digital adaptation of anunplugged assessment activity designed to evaluate algorithmic skills in Swisscompulsory education. This tool offers scalable and automated assessmentreducing human involvement and mitigating potential data collection errors. Theplatform features gesture-based and visual block-based programming interfacesensuring its usability for diverse learners further supported by multilingualcapabilities. To evaluate the virtual CAT platform we conducted a pilotevaluation in Switzerland involving a heterogeneous group of students. Thefindings show the platforms usability proficiency and suitability forassessing AT skills among students of diverse ages development stages andeducational backgrounds as well as the feasibility of large-scale datacollection. |


| Item |Content|
| --- |---|
|idx| 2408.01257v1 |
|title| Detection and Characterization of Coordinated Online Behavior: A Survey |
|authors| Lorenzo MannocciMichele MazzaAnna MonrealeMaurizio TesconiStefano Cresci
|links| http://arxiv.org/abs/2408.01257v1 |
|updated| 2024-08-02 13:27:56 UTC |
|summary| Coordination is a fundamental aspect of life. The advent of social media hasmade it integral also to online human interactions such as those thatcharacterize thriving online communities and social movements. At the sametime coordination is also core to effective disinformation manipulation andhate campaigns. This survey collects categorizes and critically discusses thebody of work produced as a result of the growing interest on coordinated onlinebehavior. We reconcile industry and academic definitions propose acomprehensive framework to study coordinated online behavior and review andcritically discuss the existing detection and characterization methods. Ouranalysis identifies open challenges and promising directions of researchserving as a guide for scholars practitioners and policymakers inunderstanding and addressing the complexities inherent to online coordination. |


| Item |Content|
| --- |---|
|idx| 2408.01203v1 |
|title| Zoomable Level-of-Detail ChartTables for Interpreting Probabilistic Model Outputs for Reactionary Train Delays |
|authors| Aidan SlingsbyJonathan Hyde
|links| http://arxiv.org/abs/2408.01203v1 |
|updated| 2024-08-02 11:34:49 UTC |
|summary| Reactionary delay is a result of the accumulated cascading effects ofknock-on train delays which is increasing on UK railways due to increasingutilisation of the railway infrastructure. The chaotic nature of its effects ontrain lateness is notoriously hard to predict. We use a stochasticMonte-Carto-style simulation of reactionary delay that produces wholedistributions of likely reactionary delay and delays this causes. Wedemonstrate how Zoomable Level-of-Detail ChartTables - case-by-variable tableswhere cases are rows variables are columns variables are complex compositemetrics that incorporate distributions and cells contain mini-charts thatdepict these as different levels of detail through zoom interaction - helpinterpret whole distributions of model outputs to help understand the causesand effects of reactionary delay how they inform timetable robustness testingand how they could be used in other contexts. |


# cs.MA 

| Item |Content|
| --- |---|
|idx| 2408.01112v1 |
|title| Agentic LLM Workflows for Generating Patient-Friendly Medical Reports |
|authors| Malavikha SudarshanSophie ShihEstella YeeAlina YangJohn ZouCathy ChenQuan ZhouLeon ChenChinmay SinghalGeorge Shih
|links| http://arxiv.org/abs/2408.01112v1 |
|updated| 2024-08-02 08:40:33 UTC |
|summary| The application of Large Language Models LLMs in healthcare is expandingrapidly with one potential use case being the translation of formal medicalreports into patient-legible equivalents. Currently LLM outputs often need tobe edited and evaluated by a human to ensure both factual accuracy andcomprehensibility and this is true for the above use case. We aim to minimizethis step by proposing an agentic workflow with the Reflexion framework whichuses iterative self-reflection to correct outputs from an LLM. This pipelinewas tested and compared to zero-shot prompting on 16 randomized radiologyreports. In our multi-agent approach reports had an accuracy rate of 94.94when looking at verification of ICD-10 codes compared to zero-shot promptedreports which had an accuracy rate of 68.23. Additionally 81.25 of thefinal reflected reports required no corrections for accuracy or readabilitywhile only 25 of zero-shot prompted reports met these criteria without needingmodifications. These results indicate that our approach presents a feasiblemethod for communicating clinical findings to patients in a quick efficientand coherent manner whilst also retaining medical accuracy. All code isavailable for viewing athttp://github.com/malavikhasudarshan/Multi-Agent-Patient-Letter-Generation. |


| Item |Content|
| --- |---|
|idx| 2408.01093v1 |
|title| CommonUppRoad: A Framework of Formal Modelling, Verifying, Learning, and Visualisation of Autonomous Vehicles |
|authors| Rong GuKaige TanAndreas Holck Høeg-PetersenLei FengKim Guldstrand Larsen
|links| http://arxiv.org/abs/2408.01093v1 |
|updated| 2024-08-02 08:12:04 UTC |
|summary| Combining machine learning and formal methods FMs provides a possiblesolution to overcome the safety issue of autonomous driving AD vehicles.However there are gaps to be bridged before this combination becomespractically applicable and useful. In an attempt to facilitate researchers inboth FMs and AD areas this paper proposes a framework that combines twowell-known tools namely CommonRoad and UPPAAL. On the one hand CommonRoad canbe enhanced by the rigorous semantics of models in UPPAAL which enables asystematic and comprehensive understanding of the AD systems behaviour andthus strengthens the safety of the system. On the other hand controllerssynthesised by UPPAAL can be visualised by CommonRoad in real-world roadnetworks which facilitates AD vehicle designers greatly adopting formal modelsin system design. In this framework we provide automatic model conversionsbetween CommonRoad and UPPAAL. Therefore users only need to program in Pythonand the framework takes care of the formal models learning and verificationin the backend. We perform experiments to demonstrate the applicability of ourframework in various AD scenarios discuss the advantages of solving motionplanning in our framework and show the scalability limit and possiblesolutions. |


| Item |Content|
| --- |---|
|idx| 2408.00682v1 |
|title| Learning in Multi-Objective Public Goods Games with Non-Linear Utilities |
|authors| Nicole OrzanErman AcarDavide GrossiPatrick MannionRoxana Rădulescu
|links| http://arxiv.org/abs/2408.00682v1 |
|updated| 2024-08-01 16:24:37 UTC |
|summary| Addressing the question of how to achieve optimal decision-making under riskand uncertainty is crucial for enhancing the capabilities of artificial agentsthat collaborate with or support humans. In this work we address this questionin the context of Public Goods Games. We study learning in a novelmulti-objective version of the Public Goods Game where agents have differentrisk preferences by means of multi-objective reinforcement learning. Weintroduce a parametric non-linear utility function to model risk preferences atthe level of individual agents over the collective and individual rewardcomponents of the game. We study the interplay between such preferencemodelling and environmental uncertainty on the incentive alignment level in thegame. We demonstrate how different combinations of individual preferences andenvironmental uncertainties sustain the emergence of cooperative patterns innon-cooperative environments i.e. where competitive strategies are dominantwhile others sustain competitive patterns in cooperative environments i.e.where cooperative strategies are dominant. |


| Item |Content|
| --- |---|
|idx| 2408.00317v1 |
|title| Condorcet's Jury Theorem with Abstention |
|authors| Ganesh GhalmeReshef Meir
|links| http://arxiv.org/abs/2408.00317v1 |
|updated| 2024-08-01 06:28:32 UTC |
|summary| The well-known Condorcets Jury theorem posits that the majority rule selectsthe best alternative among two available options with probability one as thepopulation size increases to infinity. We study this result under an asymmetrictwo-candidate setup where supporters of both candidates may have differentparticipation costs.  When the decision to abstain is fully rational i.e. when the vote pivotalityis the probability of a tie the only equilibrium outcome is a trivialequilibrium where all voters except those with zero voting cost abstain. Wepropose and analyze a more practical boundedly rational model where votersoverestimate their pivotality and show that under this model non-trivialequilibria emerge where the winning probability of both candidates is boundedaway from one.  We show that when the pivotality estimate strongly depends on the margin ofvictory victory is not assured to any candidate in any non-trivialequilibrium regardless of population size and in contrast to Condorcetsassertion. Whereas under a weak dependence on margin Condorcets Jury theoremis restored. |


| Item |Content|
| --- |---|
|idx| 2407.21307v1 |
|title| Modeling Urban Transport Choices: Incorporating Sociocultural Aspects |
|authors| Kathleen Salazar-SernaLorena CadavidCarlos J. Franco
|links| http://arxiv.org/abs/2407.21307v1 |
|updated| 2024-07-31 03:19:56 UTC |
|summary| This paper introduces an agent-based simulation model aimed at understandingurban commuters mode choices and evaluating the impacts of transport policiesto promote sustainable mobility. Crafted for developing countries whereutilitarian travel heavily relies on motorcycles the model integratessociocultural factors that influence transport behavior. Multinomial models andinferential statistics applied to survey data from Cali Colombia inform themodel revealing significant influences of sociodemographic factors and travelattributes on mode choice. Findings highlight the importance of cost timesafety comfort and personal security with disparities across socioeconomicgroups. Policy simulations demonstrate positive responses to interventions likefree public transportation increased bus frequency and enhanced security yetwith modest shifts in mode choice. Multifaceted policy approaches are deemedmore effective addressing diverse user preferences. Outputs can be extended tocities with similar sociocultural characteristics and transport dynamics. Themethodology applied in this work can be replicated for other territories. |


