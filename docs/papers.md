# cs.CL 

| Item |Content|
| --- |---|
|idx| 2411.15129v1 |
|title| Measuring Bullshit in the Language Games played by ChatGPT |
|authors| Alessandro TrevisanHarry GiddensSarah DillonAlan F. Blackwell
|links| http://arxiv.org/abs/2411.15129v1 |
|updated| 2024-11-22 18:55:21 UTC |
|summary| Generative large language models LLMs which create text without directcorrespondence to truth value are widely understood to resemble the uses oflanguage described in Frankfurts popular monograph On Bullshit. In this paperwe offer a rigorous investigation of this topic identifying how the phenomenonhas arisen and how it might be analysed. In this paper we elaborate on thisargument to propose that LLM-based chatbots play the language game ofbullshit. We use statistical text analysis to investigate the features of thisWittgensteinian language game based on a dataset constructed to contrast thelanguage of 1000 scientific publications with typical pseudo-scientific textgenerated by ChatGPT. We then explore whether the same language features can bedetected in two well-known contexts of social dysfunction: George Orwellscritique of politics and language and David Graebers characterisation ofbullshit jobs. Using simple hypothesis-testing methods we demonstrate that astatistical model of the language of bullshit can reliably relate theFrankfurtian artificial bullshit of ChatGPT to the political and workplacefunctions of bullshit as observed in natural human language. |


| Item |Content|
| --- |---|
|idx| 2411.15124v1 |
|title| TÜLU 3: Pushing Frontiers in Open Language Model Post-Training |
|authors| Nathan LambertJacob MorrisonValentina PyatkinShengyi HuangHamish IvisonFaeze BrahmanLester James V. MirandaAlisa LiuNouha DziriShane LyuYuling GuSaumya MalikVictoria GrafJena D. HwangJiangjiang YangRonan Le BrasOyvind TafjordChris WilhelmLuca SoldainiNoah A. SmithYizhong WangPradeep DasigiHannaneh Hajishirzi
|links| http://arxiv.org/abs/2411.15124v1 |
|updated| 2024-11-22 18:44:04 UTC |
|summary| Language model post-training is applied to refine behaviors and unlock newskills across a wide range of recent language models but open recipes forapplying these techniques lag behind proprietary ones. The underlying trainingdata and recipes for post-training are simultaneously the most important piecesof the puzzle and the portion with the least transparency. To bridge this gapwe introduce TULU 3 a family of fully-open state-of-the-art post-trainedmodels alongside its data code and training recipes serving as acomprehensive guide for modern post-training techniques. TULU 3 which buildson Llama 3.1 base models achieves results surpassing the instruct versions ofLlama 3.1 Qwen 2.5 Mistral and even closed models such as GPT-4o-mini andClaude 3.5-Haiku. The training algorithms for our models include supervisedfinetuning SFT Direct Preference Optimization DPO and a novel method wecall Reinforcement Learning with Verifiable Rewards RLVR. With TULU 3 weintroduce a multi-task evaluation scheme for post-training recipes withdevelopment and unseen evaluations standard benchmark implementations andsubstantial decontamination of existing open datasets on said benchmarks. Weconclude with analysis and discussion of training methods that did not reliablyimprove performance.  In addition to the TULU 3 model weights and demo we release the completerecipe -- including datasets for diverse core skills a robust toolkit for datacuration and evaluation the training code and infrastructure and mostimportantly a detailed report for reproducing and further adapting the TULU3 approach to more domains. |


| Item |Content|
| --- |---|
|idx| 2411.15122v1 |
|title| ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation |
|authors| Xiaoman ZhangHong-Yu ZhouXiaoli YangOishi BanerjeeJulián N. AcostaJosh MillerOuwen HuangPranav Rajpurkar
|links| http://arxiv.org/abs/2411.15122v1 |
|updated| 2024-11-22 18:40:02 UTC |
|summary| AI-driven models have demonstrated significant potential in automatingradiology report generation for chest X-rays. However there is no standardizedbenchmark for objectively evaluating their performance. To address this wepresent ReXrank https://rexrank.ai a public leaderboard and challenge forassessing AI-powered radiology report generation. Our framework incorporatesReXGradient the largest test dataset consisting of 10000 studies and threepublic datasets MIMIC-CXR IU-Xray CheXpert Plus for report generationassessment. ReXrank employs 8 evaluation metrics and separately assesses modelscapable of generating only findings sections and those providing both findingsand impressions sections. By providing this standardized evaluation frameworkReXrank enables meaningful comparisons of model performance and offers crucialinsights into their robustness across diverse clinical settings. Beyond itscurrent focus on chest X-rays ReXranks framework sets the stage forcomprehensive evaluation of automated reporting across the full spectrum ofmedical imaging. |


| Item |Content|
| --- |---|
|idx| 2411.15115v1 |
|title| VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement |
|authors| Daeun LeeJaehong YoonJaemin ChoMohit Bansal
|links| http://arxiv.org/abs/2411.15115v1 |
|updated| 2024-11-22 18:31:47 UTC |
|summary| Recent text-to-video T2V diffusion models have demonstrated impressivegeneration capabilities across various domains. However these models oftengenerate videos that have misalignments with text prompts especially when theprompts describe complex scenes with multiple objects and attributes. Toaddress this we introduce VideoRepair a novel model-agnostic training-freevideo refinement framework that automatically identifies fine-grainedtext-video misalignments and generates explicit spatial and textual feedbackenabling a T2V diffusion model to perform targeted localized refinements.VideoRepair consists of four stages: In 1 video evaluation we detectmisalignments by generating fine-grained evaluation questions and answeringthose questions with MLLM. In 2 refinement planning we identify accuratelygenerated objects and then create localized prompts to refine other areas inthe video. Next in 3 region decomposition we segment the correctlygenerated area using a combined grounding module. We regenerate the video byadjusting the misaligned regions while preserving the correct regions in 4localized refinement. On two popular video generation benchmarks EvalCrafterand T2V-CompBench VideoRepair substantially outperforms recent baselinesacross various text-video alignment metrics. We provide a comprehensiveanalysis of VideoRepair components and qualitative examples. |


| Item |Content|
| --- |---|
|idx| 2411.15113v1 |
|title| Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion |
|authors| Samarth N RameshZhixue Zhao
|links| http://arxiv.org/abs/2411.15113v1 |
|updated| 2024-11-22 18:29:37 UTC |
|summary| As text-to-image models grow increasingly powerful and complex theirburgeoning size presents a significant obstacle to widespread adoptionespecially on resource-constrained devices. This paper presents a pioneeringstudy on post-training pruning of Stable Diffusion 2 addressing the criticalneed for model compression in text-to-image domain. Our study tackles thepruning techniques for the previously unexplored multi-modal generation modelsand particularly examines the pruning impact on the textual component and theimage generation component separately. We conduct a comprehensive comparison onpruning the model or the single component of the model in various sparsities.Our results yield previously undocumented findings. For example contrary toestablished trends in language model pruning we discover that simple magnitudepruning outperforms more advanced techniques in text-to-image context.Furthermore our results show that Stable Diffusion 2 can be pruned to 38.5sparsity with minimal quality loss achieving a significant reduction in modelsize. We propose an optimal pruning configuration that prunes the text encoderto 47.5 and the diffusion generator to 35. This configuration maintains imagegeneration quality while substantially reducing computational requirements. Inaddition our work uncovers intriguing questions about information encoding intext-to-image models: we observe that pruning beyond certain thresholds leadsto sudden performance drops unreadable images suggesting that specificweights encode critical semantics information. This finding opens new avenuesfor future research in model compression interoperability and biasidentification in text-to-image models. By providing crucial insights into thepruning behavior of text-to-image models our study lays the groundwork fordeveloping more efficient and accessible AI-driven image generation systems |


# cs.AI 

| Item |Content|
| --- |---|
|idx| 2411.15129v1 |
|title| Measuring Bullshit in the Language Games played by ChatGPT |
|authors| Alessandro TrevisanHarry GiddensSarah DillonAlan F. Blackwell
|links| http://arxiv.org/abs/2411.15129v1 |
|updated| 2024-11-22 18:55:21 UTC |
|summary| Generative large language models LLMs which create text without directcorrespondence to truth value are widely understood to resemble the uses oflanguage described in Frankfurts popular monograph On Bullshit. In this paperwe offer a rigorous investigation of this topic identifying how the phenomenonhas arisen and how it might be analysed. In this paper we elaborate on thisargument to propose that LLM-based chatbots play the language game ofbullshit. We use statistical text analysis to investigate the features of thisWittgensteinian language game based on a dataset constructed to contrast thelanguage of 1000 scientific publications with typical pseudo-scientific textgenerated by ChatGPT. We then explore whether the same language features can bedetected in two well-known contexts of social dysfunction: George Orwellscritique of politics and language and David Graebers characterisation ofbullshit jobs. Using simple hypothesis-testing methods we demonstrate that astatistical model of the language of bullshit can reliably relate theFrankfurtian artificial bullshit of ChatGPT to the political and workplacefunctions of bullshit as observed in natural human language. |


| Item |Content|
| --- |---|
|idx| 2411.15128v1 |
|title| Health AI Developer Foundations |
|authors| Atilla P. KiralySebastien BaurKenneth PhilbrickFereshteh MahvarLiron YatzivTiffany ChenBram SterlingNick GeorgeFayaz JamilJing TangKai BaileyFaruk AhmedAkshay GoelAbbi WardLin YangAndrew SellergrenYossi MatiasAvinatan HassidimShravya ShettyDaniel GoldenShekoofeh AziziDavid F. SteinerYun LiuTim ThelinRory PilgrimCan Kirmizibayrak
|links| http://arxiv.org/abs/2411.15128v1 |
|updated| 2024-11-22 18:51:51 UTC |
|summary| Robust medical Machine Learning ML models have the potential torevolutionize healthcare by accelerating clinical research improving workflowsand outcomes and producing novel insights or capabilities. Developing such MLmodels from scratch is cost prohibitive and requires substantial compute dataand time e.g. expert labeling. To address these challenges we introduceHealth AI Developer Foundations HAI-DEF a suite of pre-traineddomain-specific foundation models tools and recipes to accelerate building MLfor health applications. The models cover various modalities and domainsincluding radiology X-rays and computed tomography histopathologydermatological imaging and audio. These models provide domain specificembeddings that facilitate AI development with less labeled data shortertraining times and reduced computational costs compared to traditionalapproaches. In addition we utilize a common interface and style across thesemodels and prioritize usability to enable developers to integrate HAI-DEFefficiently. We present model evaluations across various tasks and concludewith a discussion of their application and evaluation covering the importanceof ensuring efficacy fairness and equity. Finally while HAI-DEF andspecifically the foundation models lower the barrier to entry for ML inhealthcare we emphasize the importance of validation with problem- andpopulation-specific data for each desired usage setting. This technical reportwill be updated over time as more modalities and features are added. |


| Item |Content|
| --- |---|
|idx| 2411.15122v1 |
|title| ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation |
|authors| Xiaoman ZhangHong-Yu ZhouXiaoli YangOishi BanerjeeJulián N. AcostaJosh MillerOuwen HuangPranav Rajpurkar
|links| http://arxiv.org/abs/2411.15122v1 |
|updated| 2024-11-22 18:40:02 UTC |
|summary| AI-driven models have demonstrated significant potential in automatingradiology report generation for chest X-rays. However there is no standardizedbenchmark for objectively evaluating their performance. To address this wepresent ReXrank https://rexrank.ai a public leaderboard and challenge forassessing AI-powered radiology report generation. Our framework incorporatesReXGradient the largest test dataset consisting of 10000 studies and threepublic datasets MIMIC-CXR IU-Xray CheXpert Plus for report generationassessment. ReXrank employs 8 evaluation metrics and separately assesses modelscapable of generating only findings sections and those providing both findingsand impressions sections. By providing this standardized evaluation frameworkReXrank enables meaningful comparisons of model performance and offers crucialinsights into their robustness across diverse clinical settings. Beyond itscurrent focus on chest X-rays ReXranks framework sets the stage forcomprehensive evaluation of automated reporting across the full spectrum ofmedical imaging. |


| Item |Content|
| --- |---|
|idx| 2411.15115v1 |
|title| VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement |
|authors| Daeun LeeJaehong YoonJaemin ChoMohit Bansal
|links| http://arxiv.org/abs/2411.15115v1 |
|updated| 2024-11-22 18:31:47 UTC |
|summary| Recent text-to-video T2V diffusion models have demonstrated impressivegeneration capabilities across various domains. However these models oftengenerate videos that have misalignments with text prompts especially when theprompts describe complex scenes with multiple objects and attributes. Toaddress this we introduce VideoRepair a novel model-agnostic training-freevideo refinement framework that automatically identifies fine-grainedtext-video misalignments and generates explicit spatial and textual feedbackenabling a T2V diffusion model to perform targeted localized refinements.VideoRepair consists of four stages: In 1 video evaluation we detectmisalignments by generating fine-grained evaluation questions and answeringthose questions with MLLM. In 2 refinement planning we identify accuratelygenerated objects and then create localized prompts to refine other areas inthe video. Next in 3 region decomposition we segment the correctlygenerated area using a combined grounding module. We regenerate the video byadjusting the misaligned regions while preserving the correct regions in 4localized refinement. On two popular video generation benchmarks EvalCrafterand T2V-CompBench VideoRepair substantially outperforms recent baselinesacross various text-video alignment metrics. We provide a comprehensiveanalysis of VideoRepair components and qualitative examples. |


| Item |Content|
| --- |---|
|idx| 2411.15114v1 |
|title| RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts |
|authors| Hjalmar WijkTao LinJoel BeckerSami JawharNeev ParikhThomas BroadleyLawrence ChanMichael ChenJosh ClymerJai DhyaniElena ErichevaKatharyn GarciaBrian GoodrichNikola JurkovicMegan KinnimentAron LajkoSeraphina NixLucas SatoWilliam SaundersMaksym TaranBen WestElizabeth Barnes
|links| http://arxiv.org/abs/2411.15114v1 |
|updated| 2024-11-22 18:30:46 UTC |
|summary| Frontier AI safety policies highlight automation of AI research anddevelopment RD by AI agents as an important capability to anticipate.However there exist few evaluations for AI RD capabilities and none that arehighly realistic and have a direct comparison to human performance. Weintroduce RE-Bench Research Engineering Benchmark v1 which consists of 7challenging open-ended ML research engineering environments and data from 718-hour attempts by 61 distinct human experts. We confirm that our experts makeprogress in the environments given 8 hours with 82 of expert attemptsachieving a non-zero score and 24 matching or exceeding our strong referencesolutions. We compare humans to several public frontier models throughbest-of-k with varying time budgets and agent designs and find that the bestAI agents achieve a score 4x higher than human experts when both are given atotal time budget of 2 hours per environment. However humans currently displaybetter returns to increasing time budgets narrowly exceeding the top AI agentscores given an 8-hour budget and achieving 2x the score of the top AI agentwhen both are given 32 total hours across different attempts. Qualitativelywe find that modern AI agents possess significant expertise in many ML topics-- e.g. an agent wrote a faster custom Triton kernel than any of our humanexperts -- and can generate and test solutions over ten times faster thanhumans at much lower cost. We open-source the evaluation environments humanexpert data analysis code and agent trajectories to facilitate futureresearch. |


# cs.LG 

| Item |Content|
| --- |---|
|idx| 2411.15131v1 |
|title| WildLMa: Long Horizon Loco-Manipulation in the Wild |
|authors| Ri-Zhao QiuYuchen SongXuanbin PengSai Aneesh SuryadevaraGe YangMinghuan LiuMazeyu JiChengzhe JiaRuihan YangXueyan ZouXiaolong Wang
|links| http://arxiv.org/abs/2411.15131v1 |
|updated| 2024-11-22 18:56:56 UTC |
|summary| In-the-wild mobile manipulation aims to deploy robots in diverse real-worldenvironments which requires the robot to 1 have skills that generalizeacross object configurations 2 be capable of long-horizon task execution indiverse environments and 3 perform complex manipulation beyondpick-and-place. Quadruped robots with manipulators hold promise for extendingthe workspace and enabling robust locomotion but existing results do notinvestigate such a capability. This paper proposes WildLMa with threecomponents to address these issues: 1 adaptation of learned low-levelcontroller for VR-enabled whole-body teleoperation and traversability 2WildLMa-Skill -- a library of generalizable visuomotor skills acquired viaimitation learning or heuristics and 3 WildLMa-Planner -- an interface oflearned skills that allow LLM planners to coordinate skills for long-horizontasks. We demonstrate the importance of high-quality training data by achievinghigher grasping success rate over existing RL baselines using only tens ofdemonstrations. WildLMa exploits CLIP for language-conditioned imitationlearning that empirically generalizes to objects unseen in trainingdemonstrations. Besides extensive quantitative evaluation we qualitativelydemonstrate practical robot applications such as cleaning up trash inuniversity hallways or outdoor terrains operating articulated objects andrearranging items on a bookshelf. |


| Item |Content|
| --- |---|
|idx| 2411.15128v1 |
|title| Health AI Developer Foundations |
|authors| Atilla P. KiralySebastien BaurKenneth PhilbrickFereshteh MahvarLiron YatzivTiffany ChenBram SterlingNick GeorgeFayaz JamilJing TangKai BaileyFaruk AhmedAkshay GoelAbbi WardLin YangAndrew SellergrenYossi MatiasAvinatan HassidimShravya ShettyDaniel GoldenShekoofeh AziziDavid F. SteinerYun LiuTim ThelinRory PilgrimCan Kirmizibayrak
|links| http://arxiv.org/abs/2411.15128v1 |
|updated| 2024-11-22 18:51:51 UTC |
|summary| Robust medical Machine Learning ML models have the potential torevolutionize healthcare by accelerating clinical research improving workflowsand outcomes and producing novel insights or capabilities. Developing such MLmodels from scratch is cost prohibitive and requires substantial compute dataand time e.g. expert labeling. To address these challenges we introduceHealth AI Developer Foundations HAI-DEF a suite of pre-traineddomain-specific foundation models tools and recipes to accelerate building MLfor health applications. The models cover various modalities and domainsincluding radiology X-rays and computed tomography histopathologydermatological imaging and audio. These models provide domain specificembeddings that facilitate AI development with less labeled data shortertraining times and reduced computational costs compared to traditionalapproaches. In addition we utilize a common interface and style across thesemodels and prioritize usability to enable developers to integrate HAI-DEFefficiently. We present model evaluations across various tasks and concludewith a discussion of their application and evaluation covering the importanceof ensuring efficacy fairness and equity. Finally while HAI-DEF andspecifically the foundation models lower the barrier to entry for ML inhealthcare we emphasize the importance of validation with problem- andpopulation-specific data for each desired usage setting. This technical reportwill be updated over time as more modalities and features are added. |


| Item |Content|
| --- |---|
|idx| 2411.15127v1 |
|title| PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision |
|authors| Arnav M. DasChi Ian TangFahim KawsarMohammad Malekzadeh
|links| http://arxiv.org/abs/2411.15127v1 |
|updated| 2024-11-22 18:46:30 UTC |
|summary| Sensing human motions through Inertial Measurement Units IMUs embedded inpersonal devices has enabled significant applications in health and wellness.While labeled IMU data is scarce we can collect unlabeled or weakly labeledIMU data to model human motions. For video or text modalities the pretrainand adapt approach utilizes large volumes of unlabeled or weakly labeled datafor pretraining building a strong feature extractor followed by adaptation tospecific tasks using limited labeled data. This approach has not been widelyadopted in the IMU domain for two reasons: 1 pretraining methods are poorlyunderstood in the context of IMU and 2 open-source pretrained models thatgeneralize across datasets are rarely publicly available. In this paper we aimto address the first issue by proposing PRIMUS a method for PRetraining IMUencoderS. We conduct a systematic and unified evaluation of variousself-supervised and multimodal learning pretraining objectives. Our findingsindicate that using PRIMUS which combines self-supervision multimodalsupervision and nearest-neighbor supervision can significantly enhancedownstream performance. With fewer than 500 labeled samples per class PRIMUSeffectively enhances downstream performance by up to 15 in held-out test datacompared to the state-of-the-art multimodal training method. To benefit thebroader community our code and pre-trained IMU encoders will be made publiclyavailable at github.com/nokia-bell-labs upon publication. |


| Item |Content|
| --- |---|
|idx| 2411.15114v1 |
|title| RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts |
|authors| Hjalmar WijkTao LinJoel BeckerSami JawharNeev ParikhThomas BroadleyLawrence ChanMichael ChenJosh ClymerJai DhyaniElena ErichevaKatharyn GarciaBrian GoodrichNikola JurkovicMegan KinnimentAron LajkoSeraphina NixLucas SatoWilliam SaundersMaksym TaranBen WestElizabeth Barnes
|links| http://arxiv.org/abs/2411.15114v1 |
|updated| 2024-11-22 18:30:46 UTC |
|summary| Frontier AI safety policies highlight automation of AI research anddevelopment RD by AI agents as an important capability to anticipate.However there exist few evaluations for AI RD capabilities and none that arehighly realistic and have a direct comparison to human performance. Weintroduce RE-Bench Research Engineering Benchmark v1 which consists of 7challenging open-ended ML research engineering environments and data from 718-hour attempts by 61 distinct human experts. We confirm that our experts makeprogress in the environments given 8 hours with 82 of expert attemptsachieving a non-zero score and 24 matching or exceeding our strong referencesolutions. We compare humans to several public frontier models throughbest-of-k with varying time budgets and agent designs and find that the bestAI agents achieve a score 4x higher than human experts when both are given atotal time budget of 2 hours per environment. However humans currently displaybetter returns to increasing time budgets narrowly exceeding the top AI agentscores given an 8-hour budget and achieving 2x the score of the top AI agentwhen both are given 32 total hours across different attempts. Qualitativelywe find that modern AI agents possess significant expertise in many ML topics-- e.g. an agent wrote a faster custom Triton kernel than any of our humanexperts -- and can generate and test solutions over ten times faster thanhumans at much lower cost. We open-source the evaluation environments humanexpert data analysis code and agent trajectories to facilitate futureresearch. |


| Item |Content|
| --- |---|
|idx| 2411.15113v1 |
|title| Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion |
|authors| Samarth N RameshZhixue Zhao
|links| http://arxiv.org/abs/2411.15113v1 |
|updated| 2024-11-22 18:29:37 UTC |
|summary| As text-to-image models grow increasingly powerful and complex theirburgeoning size presents a significant obstacle to widespread adoptionespecially on resource-constrained devices. This paper presents a pioneeringstudy on post-training pruning of Stable Diffusion 2 addressing the criticalneed for model compression in text-to-image domain. Our study tackles thepruning techniques for the previously unexplored multi-modal generation modelsand particularly examines the pruning impact on the textual component and theimage generation component separately. We conduct a comprehensive comparison onpruning the model or the single component of the model in various sparsities.Our results yield previously undocumented findings. For example contrary toestablished trends in language model pruning we discover that simple magnitudepruning outperforms more advanced techniques in text-to-image context.Furthermore our results show that Stable Diffusion 2 can be pruned to 38.5sparsity with minimal quality loss achieving a significant reduction in modelsize. We propose an optimal pruning configuration that prunes the text encoderto 47.5 and the diffusion generator to 35. This configuration maintains imagegeneration quality while substantially reducing computational requirements. Inaddition our work uncovers intriguing questions about information encoding intext-to-image models: we observe that pruning beyond certain thresholds leadsto sudden performance drops unreadable images suggesting that specificweights encode critical semantics information. This finding opens new avenuesfor future research in model compression interoperability and biasidentification in text-to-image models. By providing crucial insights into thepruning behavior of text-to-image models our study lays the groundwork fordeveloping more efficient and accessible AI-driven image generation systems |


# cs.CV 

| Item |Content|
| --- |---|
|idx| 2411.15139v1 |
|title| DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving |
|authors| Bencheng LiaoShaoyu ChenHaoran YinBo JiangCheng WangSixu YanXinbang ZhangXiangyu LiYing ZhangQian ZhangXinggang Wang
|links| http://arxiv.org/abs/2411.15139v1 |
|updated| 2024-11-22 18:59:47 UTC |
|summary| Recently the diffusion model has emerged as a powerful generative techniquefor robotic policy learning capable of modeling multi-mode actiondistributions. Leveraging its capability for end-to-end autonomous driving is apromising direction. However the numerous denoising steps in the roboticdiffusion policy and the more dynamic open-world nature of traffic scenes posesubstantial challenges for generating diverse driving actions at a real-timespeed. To address these challenges we propose a novel truncated diffusionpolicy that incorporates prior multi-mode anchors and truncates the diffusionschedule enabling the model to learn denoising from anchored Gaussiandistribution to the multi-mode driving action distribution. Additionally wedesign an efficient cascade diffusion decoder for enhanced interaction withconditional scene context. The proposed model DiffusionDrive demonstrates10times reduction in denoising steps compared to vanilla diffusion policydelivering superior diversity and quality in just 2 steps. On theplanning-oriented NAVSIM dataset with the aligned ResNet-34 backboneDiffusionDrive achieves 88.1 PDMS without bells and whistles setting a newrecord while running at a real-time speed of 45 FPS on an NVIDIA 4090.Qualitative results on challenging scenarios further confirm thatDiffusionDrive can robustly generate diverse plausible driving actions. Codeand model will be available at https://github.com/hustvl/DiffusionDrive. |


| Item |Content|
| --- |---|
|idx| 2411.15138v1 |
|title| Material Anything: Generating Materials for Any 3D Object via Diffusion |
|authors| Xin HuangTengfei WangZiwei LiuQing Wang
|links| http://arxiv.org/abs/2411.15138v1 |
|updated| 2024-11-22 18:59:39 UTC |
|summary| We present Material Anything a fully-automated unified diffusion frameworkdesigned to generate physically-based materials for 3D objects. Unlike existingmethods that rely on complex pipelines or case-specific optimizations MaterialAnything offers a robust end-to-end solution adaptable to objects underdiverse lighting conditions. Our approach leverages a pre-trained imagediffusion model enhanced with a triple-head architecture and rendering loss toimprove stability and material quality. Additionally we introduce confidencemasks as a dynamic switcher within the diffusion model enabling it toeffectively handle both textured and texture-less objects across varyinglighting conditions. By employing a progressive material generation strategyguided by these confidence masks along with a UV-space material refiner ourmethod ensures consistent UV-ready material outputs. Extensive experimentsdemonstrate our approach outperforms existing methods across a wide range ofobject categories and lighting conditions. |


| Item |Content|
| --- |---|
|idx| 2411.15131v1 |
|title| WildLMa: Long Horizon Loco-Manipulation in the Wild |
|authors| Ri-Zhao QiuYuchen SongXuanbin PengSai Aneesh SuryadevaraGe YangMinghuan LiuMazeyu JiChengzhe JiaRuihan YangXueyan ZouXiaolong Wang
|links| http://arxiv.org/abs/2411.15131v1 |
|updated| 2024-11-22 18:56:56 UTC |
|summary| In-the-wild mobile manipulation aims to deploy robots in diverse real-worldenvironments which requires the robot to 1 have skills that generalizeacross object configurations 2 be capable of long-horizon task execution indiverse environments and 3 perform complex manipulation beyondpick-and-place. Quadruped robots with manipulators hold promise for extendingthe workspace and enabling robust locomotion but existing results do notinvestigate such a capability. This paper proposes WildLMa with threecomponents to address these issues: 1 adaptation of learned low-levelcontroller for VR-enabled whole-body teleoperation and traversability 2WildLMa-Skill -- a library of generalizable visuomotor skills acquired viaimitation learning or heuristics and 3 WildLMa-Planner -- an interface oflearned skills that allow LLM planners to coordinate skills for long-horizontasks. We demonstrate the importance of high-quality training data by achievinghigher grasping success rate over existing RL baselines using only tens ofdemonstrations. WildLMa exploits CLIP for language-conditioned imitationlearning that empirically generalizes to objects unseen in trainingdemonstrations. Besides extensive quantitative evaluation we qualitativelydemonstrate practical robot applications such as cleaning up trash inuniversity hallways or outdoor terrains operating articulated objects andrearranging items on a bookshelf. |


| Item |Content|
| --- |---|
|idx| 2411.15128v1 |
|title| Health AI Developer Foundations |
|authors| Atilla P. KiralySebastien BaurKenneth PhilbrickFereshteh MahvarLiron YatzivTiffany ChenBram SterlingNick GeorgeFayaz JamilJing TangKai BaileyFaruk AhmedAkshay GoelAbbi WardLin YangAndrew SellergrenYossi MatiasAvinatan HassidimShravya ShettyDaniel GoldenShekoofeh AziziDavid F. SteinerYun LiuTim ThelinRory PilgrimCan Kirmizibayrak
|links| http://arxiv.org/abs/2411.15128v1 |
|updated| 2024-11-22 18:51:51 UTC |
|summary| Robust medical Machine Learning ML models have the potential torevolutionize healthcare by accelerating clinical research improving workflowsand outcomes and producing novel insights or capabilities. Developing such MLmodels from scratch is cost prohibitive and requires substantial compute dataand time e.g. expert labeling. To address these challenges we introduceHealth AI Developer Foundations HAI-DEF a suite of pre-traineddomain-specific foundation models tools and recipes to accelerate building MLfor health applications. The models cover various modalities and domainsincluding radiology X-rays and computed tomography histopathologydermatological imaging and audio. These models provide domain specificembeddings that facilitate AI development with less labeled data shortertraining times and reduced computational costs compared to traditionalapproaches. In addition we utilize a common interface and style across thesemodels and prioritize usability to enable developers to integrate HAI-DEFefficiently. We present model evaluations across various tasks and concludewith a discussion of their application and evaluation covering the importanceof ensuring efficacy fairness and equity. Finally while HAI-DEF andspecifically the foundation models lower the barrier to entry for ML inhealthcare we emphasize the importance of validation with problem- andpopulation-specific data for each desired usage setting. This technical reportwill be updated over time as more modalities and features are added. |


| Item |Content|
| --- |---|
|idx| 2411.15122v1 |
|title| ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation |
|authors| Xiaoman ZhangHong-Yu ZhouXiaoli YangOishi BanerjeeJulián N. AcostaJosh MillerOuwen HuangPranav Rajpurkar
|links| http://arxiv.org/abs/2411.15122v1 |
|updated| 2024-11-22 18:40:02 UTC |
|summary| AI-driven models have demonstrated significant potential in automatingradiology report generation for chest X-rays. However there is no standardizedbenchmark for objectively evaluating their performance. To address this wepresent ReXrank https://rexrank.ai a public leaderboard and challenge forassessing AI-powered radiology report generation. Our framework incorporatesReXGradient the largest test dataset consisting of 10000 studies and threepublic datasets MIMIC-CXR IU-Xray CheXpert Plus for report generationassessment. ReXrank employs 8 evaluation metrics and separately assesses modelscapable of generating only findings sections and those providing both findingsand impressions sections. By providing this standardized evaluation frameworkReXrank enables meaningful comparisons of model performance and offers crucialinsights into their robustness across diverse clinical settings. Beyond itscurrent focus on chest X-rays ReXranks framework sets the stage forcomprehensive evaluation of automated reporting across the full spectrum ofmedical imaging. |


# stat.ML 

| Item |Content|
| --- |---|
|idx| 2411.15095v1 |
|title| Dimension-independent rates for structured neural density estimation |
|authors| Robert A. VandermeulenWai Ming TaiBryon Aragam
|links| http://arxiv.org/abs/2411.15095v1 |
|updated| 2024-11-22 17:50:27 UTC |
|summary| We show that deep neural networks achieve dimension-independent rates ofconvergence for learning structured densities such as those arising in imageaudio video and text applications. More precisely we demonstrate that neuralnetworks with a simple L2-minimizing loss achieve a rate of n-1/4rin nonparametric density estimation when the underlying density is Markov to agraph whose maximum clique size is at most r and we provide evidence that inthe aforementioned applications this size is typically constant i.e.rO1. We then establish that the optimal rate in L1 is n-1/2rwhich compared to the standard nonparametric rate of n-1/2d revealsthat the effective dimension of such problems is the size of the largest cliquein the Markov random field. These rates are independent of the datas ambientdimension making them applicable to realistic models of image sound videoand text data. Our results provide a novel justification for deep learningsability to circumvent the curse of dimensionality demonstratingdimension-independent convergence rates in these contexts. |


| Item |Content|
| --- |---|
|idx| 2411.15014v1 |
|title| On the Linear Speedup of Personalized Federated Reinforcement Learning with Shared Representations |
|authors| Guojun XiongShufan WangDaniel JiangJian Li
|links| http://arxiv.org/abs/2411.15014v1 |
|updated| 2024-11-22 15:42:43 UTC |
|summary| Federated reinforcement learning FedRL enables multiple agents tocollaboratively learn a policy without sharing their local trajectoriescollected during agent-environment interactions. However in practice theenvironments faced by different agents are often heterogeneous leading to poorperformance by the single policy learned by existing FedRL algorithms onindividual agents. In this paper we take a further step and introduce aemphpersonalized FedRL framework PFedRL by taking advantage of possiblyshared common structure among agents in heterogeneous environments.Specifically we develop a class of PFedRL algorithms named PFedRL-Rep thatlearns 1 a shared feature representation collaboratively among all agentsand 2 an agent-specific weight vector personalized to its local environment.We analyze the convergence of PFedTD-Rep a particular instance of theframework with temporal difference TD learning and linear representations. Tothe best of our knowledge we are the first to prove a linear convergencespeedup with respect to the number of agents in the PFedRL setting. To achievethis we show that PFedTD-Rep is an example of the federated two-timescalestochastic approximation with Markovian noise. Experimental results demonstratethat PFedTD-Rep along with an extension to the control setting based on deepQ-networks DQN not only improve learning in heterogeneous settings but alsoprovide better generalization to new environments. |


| Item |Content|
| --- |---|
|idx| 2411.14991v1 |
|title| Free Energy Projective Simulation (FEPS): Active inference with interpretability |
|authors| Joséphine PazemMarius KrummAlexander Q. ViningLukas J. FidererHans J. Briegel
|links| http://arxiv.org/abs/2411.14991v1 |
|updated| 2024-11-22 15:01:44 UTC |
|summary| In the last decade the free energy principle FEP and active inferenceAIF have achieved many successes connecting conceptual models of learning andcognition to mathematical models of perception and action. This effort isdriven by a multidisciplinary interest in understanding aspects ofself-organizing complex adaptive systems including elements of agency. Variousreinforcement learning RL models performing active inference have beenproposed and trained on standard RL tasks using deep neural networks. Recentwork has focused on improving such agents performance in complex environmentsby incorporating the latest machine learning techniques. In this paper we takean alternative approach. Within the constraints imposed by the FEP and AIF weattempt to model agents in an interpretable way without deep neural networks byintroducing Free Energy Projective Simulation FEPS. Using internal rewardsonly FEPS agents build a representation of their partially observableenvironments with which they interact. Following AIF the policy to achieve agiven task is derived from this world model by minimizing the expected freeenergy. Leveraging the interpretability of the model techniques are introducedto deal with long-term goals and reduce prediction errors caused by erroneoushidden state estimation. We test the FEPS model on two RL environments inspiredfrom behavioral biology: a timed response task and a navigation task in apartially observable grid. Our results show that FEPS agents fully resolve theambiguity of both environments by appropriately contextualizing theirobservations based on prediction accuracy only. In addition they infer optimalpolicies flexibly for any target observation in the environment. |


| Item |Content|
| --- |---|
|idx| 2411.14875v1 |
|title| Iterative Reweighted Framework Based Algorithms for Sparse Linear Regression with Generalized Elastic Net Penalty |
|authors| Yanyun DingZhenghua YaoPeili LiYunhai Xiao
|links| http://arxiv.org/abs/2411.14875v1 |
|updated| 2024-11-22 11:55:37 UTC |
|summary| The elastic net penalty is frequently employed in high-dimensional statisticsfor parameter regression and variable selection. It is particularly beneficialcompared to lasso when the number of predictors greatly surpasses the number ofobservations. However empirical evidence has shown that the ell_q-normpenalty where 0  q  1 often provides better regression compared to theell_1-norm penalty demonstrating enhanced robustness in various scenarios.In this paper we explore a generalized elastic net model that employs aell_r-norm where r geq 1 in loss function to accommodate various typesof noise and employs a ell_q-norm where 0  q  1 to replace theell_1-norm in elastic net penalty. Theoretically we establish thecomputable lower bounds for the nonzero entries of the generalized first-orderstationary points of the proposed generalized elastic net model. Forimplementation we develop two efficient algorithms based on the locallyLipschitz continuous epsilon-approximation to ell_q-norm. The firstalgorithm employs an alternating direction method of multipliers ADMM whilethe second utilizes a proximal majorization-minimization method PMM wherethe subproblems are addressed using the semismooth Newton method SNN. We alsoperform extensive numerical experiments with both simulated and real datashowing that both algorithms demonstrate superior performance. Notably thePMM-SSN is efficient than ADMM even though the latter provides a simplerimplementation. |


| Item |Content|
| --- |---|
|idx| 2411.14679v1 |
|title| Recursive Gaussian Process State Space Model |
|authors| Tengjie ZhengLin ChengShengping GongXu Huang
|links| http://arxiv.org/abs/2411.14679v1 |
|updated| 2024-11-22 02:22:59 UTC |
|summary| Learning dynamical models from data is not only fundamental but also holdsgreat promise for advancing principle discovery time-series prediction andcontroller design. Among various approaches Gaussian Process State-SpaceModels GPSSMs have recently gained significant attention due to theircombination of flexibility and interpretability. However for online learningthe field lacks an efficient method suitable for scenarios where priorinformation regarding data distribution and model function is limited. Toaddress this issue this paper proposes a recursive GPSSM method with adaptivecapabilities for both operating domains and Gaussian process GPhyperparameters. Specifically we first utilize first-order linearization toderive a Bayesian update equation for the joint distribution between the systemstate and the GP model enabling closed-form and domain-independent learning.Second an online selection algorithm for inducing points is developed based oninformative criteria to achieve lightweight learning. Third to support onlinehyperparameter optimization we recover historical measurement information fromthe current filtering distribution. Comprehensive evaluations on both syntheticand real-world datasets demonstrate the superior accuracy computationalefficiency and adaptability of our method compared to state-of-the-art onlineGPSSM techniques. |


# cs.HC 

| Item |Content|
| --- |---|
|idx| 2411.15129v1 |
|title| Measuring Bullshit in the Language Games played by ChatGPT |
|authors| Alessandro TrevisanHarry GiddensSarah DillonAlan F. Blackwell
|links| http://arxiv.org/abs/2411.15129v1 |
|updated| 2024-11-22 18:55:21 UTC |
|summary| Generative large language models LLMs which create text without directcorrespondence to truth value are widely understood to resemble the uses oflanguage described in Frankfurts popular monograph On Bullshit. In this paperwe offer a rigorous investigation of this topic identifying how the phenomenonhas arisen and how it might be analysed. In this paper we elaborate on thisargument to propose that LLM-based chatbots play the language game ofbullshit. We use statistical text analysis to investigate the features of thisWittgensteinian language game based on a dataset constructed to contrast thelanguage of 1000 scientific publications with typical pseudo-scientific textgenerated by ChatGPT. We then explore whether the same language features can bedetected in two well-known contexts of social dysfunction: George Orwellscritique of politics and language and David Graebers characterisation ofbullshit jobs. Using simple hypothesis-testing methods we demonstrate that astatistical model of the language of bullshit can reliably relate theFrankfurtian artificial bullshit of ChatGPT to the political and workplacefunctions of bullshit as observed in natural human language. |


| Item |Content|
| --- |---|
|idx| 2411.15091v1 |
|title| Somesite I Used To Crawl: Awareness, Agency and Efficacy in Protecting Content Creators From AI Crawlers |
|authors| Enze LiuElisa LuoShawn ShanGeoffrey M. VoelkerBen Y. ZhaoStefan Savage
|links| http://arxiv.org/abs/2411.15091v1 |
|updated| 2024-11-22 17:40:16 UTC |
|summary| The success of generative AI relies heavily on training on data scrapedthrough extensive crawling of the Internet a practice that has raisedsignificant copyright privacy and ethical concerns. While few measures aredesigned to resist a resource-rich adversary determined to scrape a sitecrawlers can be impacted by a range of existing tools such as robots.txt NoAImeta tags and active crawler blocking by reverse proxies.  In this work we seek to understand the ability and efficacy of todaysnetworking tools to protect content creators against AI-related crawling. Fortargeted populations like human artists do they have the technical knowledgeand agency to utilize crawler-blocking tools such as robots.txt and can suchtools be effective Using large scale measurements and a targeted user study of182 professional artists we find strong demand for tools like robots.txt butsignificantly constrained by significant hurdles in technical awareness agencyin deploying them and limited efficacy against unresponsive crawlers. Wefurther test and evaluate network level crawler blockers by reverse-proxiesand find that despite very limited deployment today their reliable andcomprehensive blocking of AI-crawlers make them the strongest protection forartists moving forward. |


| Item |Content|
| --- |---|
|idx| 2411.15033v1 |
|title| One to rule them all: natural language to bind communication, perception and action |
|authors| Simone ColombaniDimitri OgnibeneGiuseppe Boccignone
|links| http://arxiv.org/abs/2411.15033v1 |
|updated| 2024-11-22 16:05:54 UTC |
|summary| In recent years research in the area of human-robot interaction has focusedon developing robots capable of understanding complex human instructions andperforming tasks in dynamic and diverse environments. These systems have a widerange of applications from personal assistance to industrial roboticsemphasizing the importance of robots interacting flexibly naturally and safelywith humans. This paper presents an advanced architecture for robotic actionplanning that integrates communication perception and planning with LargeLanguage Models LLMs. Our system is designed to translate commands expressedin natural language into executable robot actions incorporating environmentalinformation and dynamically updating plans based on real-time feedback. ThePlanner Module is the core of the system where LLMs embedded in a modifiedReAct framework are employed to interpret and carry out user commands. Byleveraging their extensive pre-trained knowledge LLMs can effectively processuser requests without the need to introduce new knowledge on the changingenvironment. The modified ReAct framework further enhances the execution spaceby providing real-time environmental perception and the outcomes of physicalactions. By combining robust and dynamic semantic map representations as graphswith control components and failure explanations this architecture enhances arobot adaptability task execution and seamless collaboration with human usersin shared and dynamic environments. Through the integration of continuousfeedback loops with the environment the system can dynamically adjusts the planto accommodate unexpected changes optimizing the robot ability to performtasks. Using a dataset of previous experience is possible to provide detailedfeedback about the failure. Updating the LLMs context of the next iterationwith suggestion on how to overcame the issue. |


| Item |Content|
| --- |---|
|idx| 2411.15027v1 |
|title| Time is on my sight: scene graph filtering for dynamic environment perception in an LLM-driven robot |
|authors| Simone ColombaniLuca BriniDimitri OgnibeneGiuseppe Boccignone
|links| http://arxiv.org/abs/2411.15027v1 |
|updated| 2024-11-22 15:58:26 UTC |
|summary| Robots are increasingly being used in dynamic environments like workplaceshospitals and homes. As a result interactions with robots must be simple andintuitive with robots perception adapting efficiently to human-inducedchanges. This paper presents a robot control architecture that addresses keychallenges in human-robot interaction with a particular focus on the dynamiccreation and continuous update of the robot state representation. Thearchitecture uses Large Language Models to integrate diverse informationsources including natural language commands robotic skills representationreal-time dynamic semantic mapping of the perceived scene. This enablesflexible and adaptive robotic behavior in complex dynamic environments.Traditional robotic systems often rely on static pre-programmed instructionsand settings limiting their adaptability to dynamic environments and real-timecollaboration. In contrast this architecture uses LLMs to interpret complexhigh-level instructions and generate actionable plans that enhance human-robotcollaboration. At its core the system Perception Module generates andcontinuously updates a semantic scene graph using RGB-D sensor data providinga detailed and structured representation of the environment. A particle filteris employed to ensure accurate object localization in dynamic real-worldsettings. The Planner Module leverages this up-to-date semantic map to breakdown high-level tasks into sub-tasks and link them to robotic skills such asnavigation object manipulation e.g. PICK and PLACE and movement e.g.GOTO. By combining real-time perception state tracking and LLM-drivencommunication and task planning the architecture enhances adaptability taskefficiency and human-robot collaboration in dynamic environments. |


| Item |Content|
| --- |---|
|idx| 2411.14967v1 |
|title| SwissADT: An Audio Description Translation System for Swiss Languages |
|authors| Lukas FischerYingqiang GaoAlexa LintnerSarah Ebling
|links| http://arxiv.org/abs/2411.14967v1 |
|updated| 2024-11-22 14:23:07 UTC |
|summary| Audio description AD is a crucial accessibility service provided to blindpersons and persons with visual impairment designed to convey visualinformation in acoustic form. Despite recent advancements in multilingualmachine translation research the lack of well-crafted and time-synchronized ADdata impedes the development of audio description translation ADT systemsthat address the needs of multilingual countries such as Switzerland.Furthermore since the majority of ADT systems rely solely on text uncertaintyexists as to whether incorporating visual information from the correspondingvideo clips can enhance the quality of ADT outputs. In this work we presentSwissADT the first ADT system implemented for three main Swiss languages andEnglish. By collecting well-crafted AD data augmented with video clips inGerman French Italian and English and leveraging the power of LargeLanguage Models LLMs we aim to enhance information accessibility for diverselanguage populations in Switzerland by automatically translating AD scripts tothe desired Swiss language. Our extensive experimental ADT results composed ofboth automatic and human evaluations of ADT quality demonstrate the promisingcapability of SwissADT for the ADT task. We believe that combining humanexpertise with the generation power of LLMs can further enhance the performanceof ADT systems ultimately benefiting a larger multilingual target population. |


# cs.MA 

| Item |Content|
| --- |---|
|idx| 2411.14637v1 |
|title| Enhancing Clinical Trial Patient Matching through Knowledge Augmentation with Multi-Agents |
|authors| Hanwen ShiJin ZhangKunpeng Zhang
|links| http://arxiv.org/abs/2411.14637v1 |
|updated| 2024-11-22 00:07:36 UTC |
|summary| Matching patients effectively and efficiently for clinical trials is asignificant challenge due to the complexity and variability of patient profilesand trial criteria. This paper presents a novel framework Multi-Agents forKnowledge Augmentation MAKA designed to enhance patient-trial matching bydynamically supplementing matching prompts with external domain-specificknowledge. The MAKA architecture consists of five key components: a knowledgeprobing agent that detects gaps in domain knowledge a navigation agent thatmanages interactions among multiple specialized knowledge augmentation agentsa knowledge augmentation agent that incorporates relevant information intopatient-trial matching prompts a supervision agent aligning the outputs fromother agents with the instructions and a matching agent making the finalselection decision. This approach enhances the accuracy and contextual richnessof patient matching addresses inherent knowledge gaps in both trail criteriaand large language models LLMs and improves the alignment between patientcharacteristics and the criteria. |


| Item |Content|
| --- |---|
|idx| 2411.14593v1 |
|title| A Systematic Study of Multi-Agent Deep Reinforcement Learning for Safe and Robust Autonomous Highway Ramp Entry |
|authors| Larry SchesterLuis E. Ortiz
|links| http://arxiv.org/abs/2411.14593v1 |
|updated| 2024-11-21 21:23:46 UTC |
|summary| Vehicles today can drive themselves on highways and driverless robotaxisoperate in major cities with more sophisticated levels of autonomous drivingexpected to be available and become more common in the future. Yet technicallyspeaking so-called Level 5 L5 operation corresponding to full autonomyhas not been achieved. For that to happen functions such as fully autonomoushighway ramp entry must be available and provide provably safe and reliablyrobust behavior to enable full autonomy. We present a systematic study of ahighway ramp function that controls the vehicles forward-moving actions tominimize collisions with the stream of highway traffic into which a mergingego vehicle enters. We take a game-theoretic multi-agent MA approach tothis problem and study the use of controllers based on deep reinforcementlearning DRL. The virtual environment of the MA DRL uses self-play withsimulated data where merging vehicles safely learn to control longitudinalposition during a taper-type merge. The work presented in this paper extendsexisting work by studying the interaction of more than two vehicles agentsand does so by systematically expanding the road scene with additional trafficand ego vehicles. While previous work on the two-vehicle setting establishedthat collision-free controllers are theoretically impossible in fullydecentralized non-coordinated environments we empirically show thatcontrollers learned using our approach are nearly ideal when measured againstidealized optimal controllers. |


| Item |Content|
| --- |---|
|idx| 2411.14411v1 |
|title| Multi-Agent Environments for Vehicle Routing Problems |
|authors| Ricardo GamaDaniel FuertesCarlos R. del-BlancoHugo L. Fernandes
|links| http://arxiv.org/abs/2411.14411v1 |
|updated| 2024-11-21 18:46:23 UTC |
|summary| Research on Reinforcement Learning RL approaches for discrete optimizationproblems has increased considerably extending RL to an area classicallydominated by Operations Research OR. Vehicle routing problems are a goodexample of discrete optimization problems with high practical relevance whereRL techniques have had considerable success. Despite these advancesopen-source development frameworks remain scarce hampering both the testing ofalgorithms and the ability to objectively compare results. This ultimatelyslows down progress in the field and limits the exchange of ideas between theRL and OR communities.  Here we propose a library composed of multi-agent environments that simulatesclassic vehicle routing problems. The library built on PyTorch provides aflexible modular architecture design that allows easy customization andincorporation of new routing problems. It follows the Agent Environment CycleAEC games model and has an intuitive API enabling rapid adoption and easyintegration into existing reinforcement learning frameworks.  The library allows for a straightforward use of classical OR benchmarkinstances in order to narrow the gap between the test beds for algorithmbenchmarking used by the RL and OR communities. Additionally we providebenchmark instance sets for each environment as well as baseline RL models andtraining code. |


| Item |Content|
| --- |---|
|idx| 2411.14371v1 |
|title| Synthesising Robust Controllers for Robot Collectives with Recurrent Tasks: A Case Study |
|authors| Till SchnittkaMario Gleirscher
|links| http://dx.doi.org/10.4204/EPTCS.411.7 |
|updated| 2024-11-21 18:08:18 UTC |
|summary| When designing correct-by-construction controllers for autonomouscollectives three key challenges are the task specification the modellingand its use at practical scale. In this paper we focus on a simple yet usefulabstraction for high-level controller synthesis for robot collectives withoptimisation goals e.g. maximum cleanliness minimum energy consumption andrecurrence e.g. re-establish contamination and charge thresholds and safetye.g. avoid full discharge mutually exclusive room occupation constraints.Due to technical limitations related to scalability and using constraints inthe synthesis we simplify our graph-based setting from a stochastictwo-player game into a single-player game on a partially observable Markovdecision process POMDP. Robustness against environmental uncertainty isencoded via partial observability. Linear-time correctness properties areverified separately after synthesising the POMDP strategy. We contributeat-scale guidance on POMDP modelling and controller synthesis for tasked robotcollectives exemplified by the scenario of battery-driven robots responsiblefor cleaning public buildings with utilisation constraints. |


| Item |Content|
| --- |---|
|idx| 2411.14369v1 |
|title| Model Checking and Verification of Synchronisation Properties of Cobot Welding |
|authors| Yvonne MurrayHenrik NordlieDavid A. AnisiPedro RibeiroAna Cavalcanti
|links| http://dx.doi.org/10.4204/EPTCS.411.6 |
|updated| 2024-11-21 18:08:02 UTC |
|summary| This paper describes use of model checking to verify synchronisationproperties of an industrial welding system consisting of a cobot arm and anexternal turntable. The robots must move synchronously but sometimes get outof synchronisation giving rise to unsatisfactory weld qualities in problemareas such as around corners. These mistakes are costly since time is lostboth in the robotic welding and in manual repairs needed to improve the weld.Verification of the synchronisation properties has shown that they arefulfilled as long as assumptions of correctness made about parts outside thescope of the model hold indicating limitations in the hardware. These resultshave indicated the source of the problem and motivated a re-calibration of thereal-life system. This has drastically improved the welding results and is ademonstration of how formal methods can be useful in an industrial setting. |


