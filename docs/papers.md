# cs.CL 

| Item |Content|
| --- |---|
|idx| 2406.15352v1 |
|title| A SMART Mnemonic Sounds like "Glue Tonic": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick |
|authors| Nishant BalepurMatthew ShuAlexander HoyleAlison RobeyShi FengSeraphina Goldfarb-TarrantJordan Boyd-Graber
|links| http://arxiv.org/abs/2406.15352v1 |
|updated| 2024-06-21 17:59:51 UTC |
|summary| Keyword mnemonics are memorable explanations that link new terms to simplerkeywords. Prior works generate mnemonics for students but they do not guidemodels toward mnemonics students prefer and aid learning. We build SMART amnemonic generator trained on feedback from real students learning new terms.To train SMART we first fine-tune LLaMA-2 on a curated set of user-writtenmnemonics. We then use LLM alignment to enhance SMART: we deploy mnemonicsgenerated by SMART in a flashcard app to find preferences on mnemonics studentsfavor. We gather 2684 preferences from 45 students across two types: expressedinferred from ratings and observed inferred from student learning yieldingthree key findings. First expressed and observed preferences disagree whatstudents think is helpful does not fully capture what is truly helpful. SecondBayesian models can synthesize complementary data from multiple preferencetypes into a single effectiveness signal. SMART is tuned via Direct PreferenceOptimization on this signal which we show resolves ties and missing labels inthe typical method of pairwise comparisons augmenting data for LLM outputquality gains. Third mnemonic experts assess SMART as matching GPT-4 at muchlower deployment costs showing the utility of capturing diverse studentfeedback to align LLMs in education. |


| Item |Content|
| --- |---|
|idx| 2406.15334v1 |
|title| Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning |
|authors| Brandon HuangChancharik MitraAssaf ArbelleLeonid KarlinskyTrevor DarrellRoei Herzig
|links| http://arxiv.org/abs/2406.15334v1 |
|updated| 2024-06-21 17:50:02 UTC |
|summary| The recent success of interleaved Large Multimodal Models LMMs in few-shotlearning suggests that in-context learning ICL with many examples can bepromising for learning new tasks. However this many-shot multimodal ICLsetting has one crucial problem: it is fundamentally limited by the modelscontext length set at pretraining. The problem is especially prominent in themultimodal domain which processes both text and images requiring additionaltokens. This motivates the need for a multimodal method to compress many shotsinto fewer tokens without finetuning. In this work we enable LMMs to performmultimodal many-shot in-context learning by leveraging Multimodal Task VectorsMTV--compact implicit representations of in-context examples compressed inthe models attention heads. Specifically we first demonstrate the existenceof such MTV in LMMs and then leverage these extracted MTV to enable many-shotin-context learning for various vision-and-language tasks. Our experimentssuggest that MTV can scale in performance with the number of compressed shotsand generalize to similar out-of-domain tasks without additional context lengthfor inference. |


| Item |Content|
| --- |---|
|idx| 2406.15330v1 |
|title| Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance |
|authors| Haoling LiXin ZhangXiao LiuYeyun GongYifan WangYujiu YangQi ChenPeng Cheng
|links| http://arxiv.org/abs/2406.15330v1 |
|updated| 2024-06-21 17:42:52 UTC |
|summary| Large language models LLMs have revolutionized lots of fields of research.Although it is well-known that fine-tuning is essential for enhancing thecapabilities of LLMs existing research suggests that there is potentialredundancy in the fine-tuning process and therefore proposes to update only asubset of parameters. However these methods fail to leverage the task-specificinformation to identify important parameters during training. Based on theinsight that gradients inherently contain information on task-specific data wepropose Gradient-Mask Tuning GMT a method that selectively updatesparameters during training based on their gradient information. Specificallywe compute the absolute values of the gradients and apply masking to those withrelatively smaller magnitudes. Our empirical results across various tasksdemonstrate that GMT not only outperforms traditional fine-tuning methods butalso elevates the upper limits of LLM performance. Further analysis indicatesthat GMT exhibits insensitivity to mask ratio and possesses computationalefficiency comparable to vanilla SFT. |


| Item |Content|
| --- |---|
|idx| 2406.15319v1 |
|title| LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs |
|authors| Ziyan JiangXueguang MaWenhu Chen
|links| http://arxiv.org/abs/2406.15319v1 |
|updated| 2024-06-21 17:23:21 UTC |
|summary| In traditional RAG framework the basic retrieval units are normally short.The common retrievers like DPR normally work with 100-word Wikipediaparagraphs. Such a design forces the retriever to search over a large corpus tofind the needle unit. In contrast the readers only need to extract answersfrom the short retrieved units. Such an imbalanced heavy retriever andlight reader design can lead to sub-optimal performance. In order toalleviate the imbalance we propose a new framework LongRAG consisting of along retriever and a long reader. LongRAG processes the entire Wikipediainto 4K-token units which is 30x longer than before. By increasing the unitsize we significantly reduce the total units from 22M to 700K. Thissignificantly lowers the burden of retriever which leads to a remarkableretrieval score: answer recall171 on NQ previously 52 and answerrecall272 previously 47 on HotpotQA full-wiki. Then we feed the top-kretrieved units approx 30K tokens to an existing long-context LLM toperform zero-shot answer extraction. Without requiring any training LongRAGachieves an EM of 62.7 on NQ which is the best known result. LongRAG alsoachieves 64.3 on HotpotQA full-wiki which is on par of the SoTA model. Ourstudy offers insights into the future roadmap for combining RAG withlong-context LLMs. |


| Item |Content|
| --- |---|
|idx| 2406.15313v1 |
|title| STARD: A Chinese Statute Retrieval Dataset with Real Queries Issued by Non-professionals |
|authors| Weihang SuYiran HuAnzhe XieQingyao AiZibing QueNing ZhengYun LiuWeixing ShenYiqun Liu
|links| http://arxiv.org/abs/2406.15313v1 |
|updated| 2024-06-21 17:10:09 UTC |
|summary| Statute retrieval aims to find relevant statutory articles for specificqueries. This process is the basis of a wide range of legal applications suchas legal advice automated judicial decisions legal document drafting etc.Existing statute retrieval benchmarks focus on formal and professional queriesfrom sources like bar exams and legal case documents thereby neglectingnon-professional queries from the general public which often lack preciselegal terminology and references. To address this gap we introduce the STAtuteRetrieval Dataset STARD a Chinese dataset comprising 1543 query casescollected from real-world legal consultations and 55348 candidate statutoryarticles. Unlike existing statute retrieval datasets which primarily focus onprofessional legal queries STARD captures the complexity and diversity of realqueries from the general public. Through a comprehensive evaluation of variousretrieval baselines we reveal that existing retrieval approaches all fallshort of these real queries issued by non-professional users. The best methodonly achieves a Recall100 of 0.907 suggesting the necessity for furtherexploration and additional research in this area.  All the codes and datasets are available at:https://github.com/oneal2000/STARD/tree/main |


# cs.AI 

| Item |Content|
| --- |---|
|idx| 2406.15349v1 |
|title| NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking |
|authors| Daniel DaunerMarcel HallgartenTianyu LiXinshuo WengZhiyu HuangZetong YangHongyang LiIgor GilitschenskiBoris IvanovicMarco PavoneAndreas GeigerKashyap Chitta
|links| http://arxiv.org/abs/2406.15349v1 |
|updated| 2024-06-21 17:59:02 UTC |
|summary| Benchmarking vision-based driving policies is challenging. On one handopen-loop evaluation with real data is easy but these results do not reflectclosed-loop performance. On the other closed-loop evaluation is possible insimulation but is hard to scale due to its significant computational demands.Further the simulators available today exhibit a large domain gap to realdata. This has resulted in an inability to draw clear conclusions from therapidly growing body of research on end-to-end autonomous driving. In thispaper we present NAVSIM a middle ground between these evaluation paradigmswhere we use large datasets in combination with a non-reactive simulator toenable large-scale real-world benchmarking. Specifically we gathersimulation-based metrics such as progress and time to collision by unrollingbirds eye view abstractions of the test scenes for a short simulation horizon.Our simulation is non-reactive i.e. the evaluated policy and environment donot influence each other. As we demonstrate empirically this decoupling allowsopen-loop metric computation while being better aligned with closed-loopevaluations than traditional displacement errors. NAVSIM enabled a newcompetition held at CVPR 2024 where 143 teams submitted 463 entries resultingin several new insights. On a large set of challenging scenarios we observethat simple methods with moderate compute requirements such as TransFuser canmatch recent large-scale end-to-end driving architectures such as UniAD. Ourmodular framework can potentially be extended with new datasets data curationstrategies and metrics and will be continually maintained to host futurechallenges. Our code is available athttps://github.com/autonomousvision/navsim. |


| Item |Content|
| --- |---|
|idx| 2406.15346v1 |
|title| Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach |
|authors| Chengzhe PiaoTaiyu ZhuYu WangStephanie E BaldewegPaul TaylorPantelis GeorgiouJiahao SunJun WangKezhi Li
|links| http://arxiv.org/abs/2406.15346v1 |
|updated| 2024-06-21 17:57:39 UTC |
|summary| Newly diagnosed Type 1 Diabetes T1D patients often struggle to obtaineffective Blood Glucose BG prediction models due to the lack of sufficient BGdata from Continuous Glucose Monitoring CGM presenting a significant coldstart problem in patient care. Utilizing population models to address thischallenge is a potential solution but collecting patient data for trainingpopulation models in a privacy-conscious manner is challenging especiallygiven that such data is often stored on personal devices. Considering theprivacy protection and addressing the cold start problem in diabetes care wepropose GluADFL blood Glucose prediction by Asynchronous DecentralizedFederated Learning. We compared GluADFL with eight baseline methods using fourdistinct T1D datasets comprising 298 participants which demonstrated itssuperior performance in accurately predicting BG levels for cross-patientanalysis. Furthermore patients data might be stored and shared across variouscommunication networks in GluADFL ranging from highly interconnected e.g.random performs the best among others to more structured topologies e.g.cluster and ring suitable for various social networks. The asynchronoustraining framework supports flexible participation. By adjusting the ratios ofinactive participants we found it remains stable if less than 70 areinactive. Our results confirm that GluADFL offers a practicalprivacy-preserving solution for BG prediction in T1D significantly enhancingthe quality of diabetes management. |


| Item |Content|
| --- |---|
|idx| 2406.15341v1 |
|title| GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians |
|authors| Haoyang LiuHaohan Wang
|links| http://arxiv.org/abs/2406.15341v1 |
|updated| 2024-06-21 17:55:24 UTC |
|summary| Recent advancements in machine learning have significantly improved theidentification of disease-associated genes from gene expression datasets.However these processes often require extensive expertise and manual effortlimiting their scalability. Large Language Model LLM-based agents have shownpromise in automating these tasks due to their increasing problem-solvingabilities. To support the evaluation and development of such methods weintroduce GenoTEX a benchmark dataset for the automatic exploration of geneexpression data involving the tasks of dataset selection preprocessing andstatistical analysis. GenoTEX provides annotated code and results for solving awide range of gene identification problems in a full analysis pipeline thatfollows the standard of computational genomics. These annotations are curatedby human bioinformaticians who carefully analyze the datasets to ensureaccuracy and reliability. To provide baselines for these tasks we presentGenoAgents a team of LLM-based agents designed with context-aware planningiterative correction and domain expert consultation to collaboratively exploregene datasets. Our experiments with GenoAgents demonstrate the potential ofLLM-based approaches in genomics data analysis while error analysis highlightsthe challenges and areas for future improvement. We propose GenoTEX as apromising resource for benchmarking and enhancing AI-driven methods forgenomics data analysis. We make our benchmark publicly available aturlhttps://github.com/Liu-Hy/GenoTex. |


| Item |Content|
| --- |---|
|idx| 2406.15339v1 |
|title| Image Conductor: Precision Control for Interactive Video Synthesis |
|authors| Yaowei LiXintao WangZhaoyang ZhangZhouxia WangZiyang YuanLiangbin XieYuexian ZouYing Shan
|links| http://arxiv.org/abs/2406.15339v1 |
|updated| 2024-06-21 17:55:05 UTC |
|summary| Filmmaking and animation production often require sophisticated techniquesfor coordinating camera transitions and object movements typically involvinglabor-intensive real-world capturing. Despite advancements in generative AI forvideo creation achieving precise control over motion for interactive videoasset generation remains challenging. To this end we propose Image Conductora method for precise control of camera transitions and object movements togenerate video assets from a single image. An well-cultivated training strategyis proposed to separate distinct camera and object motion by camera LoRAweights and object LoRA weights. To further address cinematographic variationsfrom ill-posed trajectories we introduce a camera-free guidance techniqueduring inference enhancing object movements while eliminating cameratransitions. Additionally we develop a trajectory-oriented video motion datacuration pipeline for training. Quantitative and qualitative experimentsdemonstrate our methods precision and fine-grained control in generatingmotion-controllable videos from images advancing the practical application ofinteractive video synthesis. Project webpage available athttps://liyaowei-stu.github.io/project/ImageConductor/ |


| Item |Content|
| --- |---|
|idx| 2406.15334v1 |
|title| Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning |
|authors| Brandon HuangChancharik MitraAssaf ArbelleLeonid KarlinskyTrevor DarrellRoei Herzig
|links| http://arxiv.org/abs/2406.15334v1 |
|updated| 2024-06-21 17:50:02 UTC |
|summary| The recent success of interleaved Large Multimodal Models LMMs in few-shotlearning suggests that in-context learning ICL with many examples can bepromising for learning new tasks. However this many-shot multimodal ICLsetting has one crucial problem: it is fundamentally limited by the modelscontext length set at pretraining. The problem is especially prominent in themultimodal domain which processes both text and images requiring additionaltokens. This motivates the need for a multimodal method to compress many shotsinto fewer tokens without finetuning. In this work we enable LMMs to performmultimodal many-shot in-context learning by leveraging Multimodal Task VectorsMTV--compact implicit representations of in-context examples compressed inthe models attention heads. Specifically we first demonstrate the existenceof such MTV in LMMs and then leverage these extracted MTV to enable many-shotin-context learning for various vision-and-language tasks. Our experimentssuggest that MTV can scale in performance with the number of compressed shotsand generalize to similar out-of-domain tasks without additional context lengthfor inference. |


# cs.LG 

| Item |Content|
| --- |---|
|idx| 2406.15349v1 |
|title| NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking |
|authors| Daniel DaunerMarcel HallgartenTianyu LiXinshuo WengZhiyu HuangZetong YangHongyang LiIgor GilitschenskiBoris IvanovicMarco PavoneAndreas GeigerKashyap Chitta
|links| http://arxiv.org/abs/2406.15349v1 |
|updated| 2024-06-21 17:59:02 UTC |
|summary| Benchmarking vision-based driving policies is challenging. On one handopen-loop evaluation with real data is easy but these results do not reflectclosed-loop performance. On the other closed-loop evaluation is possible insimulation but is hard to scale due to its significant computational demands.Further the simulators available today exhibit a large domain gap to realdata. This has resulted in an inability to draw clear conclusions from therapidly growing body of research on end-to-end autonomous driving. In thispaper we present NAVSIM a middle ground between these evaluation paradigmswhere we use large datasets in combination with a non-reactive simulator toenable large-scale real-world benchmarking. Specifically we gathersimulation-based metrics such as progress and time to collision by unrollingbirds eye view abstractions of the test scenes for a short simulation horizon.Our simulation is non-reactive i.e. the evaluated policy and environment donot influence each other. As we demonstrate empirically this decoupling allowsopen-loop metric computation while being better aligned with closed-loopevaluations than traditional displacement errors. NAVSIM enabled a newcompetition held at CVPR 2024 where 143 teams submitted 463 entries resultingin several new insights. On a large set of challenging scenarios we observethat simple methods with moderate compute requirements such as TransFuser canmatch recent large-scale end-to-end driving architectures such as UniAD. Ourmodular framework can potentially be extended with new datasets data curationstrategies and metrics and will be continually maintained to host futurechallenges. Our code is available athttps://github.com/autonomousvision/navsim. |


| Item |Content|
| --- |---|
|idx| 2406.15346v1 |
|title| Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach |
|authors| Chengzhe PiaoTaiyu ZhuYu WangStephanie E BaldewegPaul TaylorPantelis GeorgiouJiahao SunJun WangKezhi Li
|links| http://arxiv.org/abs/2406.15346v1 |
|updated| 2024-06-21 17:57:39 UTC |
|summary| Newly diagnosed Type 1 Diabetes T1D patients often struggle to obtaineffective Blood Glucose BG prediction models due to the lack of sufficient BGdata from Continuous Glucose Monitoring CGM presenting a significant coldstart problem in patient care. Utilizing population models to address thischallenge is a potential solution but collecting patient data for trainingpopulation models in a privacy-conscious manner is challenging especiallygiven that such data is often stored on personal devices. Considering theprivacy protection and addressing the cold start problem in diabetes care wepropose GluADFL blood Glucose prediction by Asynchronous DecentralizedFederated Learning. We compared GluADFL with eight baseline methods using fourdistinct T1D datasets comprising 298 participants which demonstrated itssuperior performance in accurately predicting BG levels for cross-patientanalysis. Furthermore patients data might be stored and shared across variouscommunication networks in GluADFL ranging from highly interconnected e.g.random performs the best among others to more structured topologies e.g.cluster and ring suitable for various social networks. The asynchronoustraining framework supports flexible participation. By adjusting the ratios ofinactive participants we found it remains stable if less than 70 areinactive. Our results confirm that GluADFL offers a practicalprivacy-preserving solution for BG prediction in T1D significantly enhancingthe quality of diabetes management. |


| Item |Content|
| --- |---|
|idx| 2406.15341v1 |
|title| GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians |
|authors| Haoyang LiuHaohan Wang
|links| http://arxiv.org/abs/2406.15341v1 |
|updated| 2024-06-21 17:55:24 UTC |
|summary| Recent advancements in machine learning have significantly improved theidentification of disease-associated genes from gene expression datasets.However these processes often require extensive expertise and manual effortlimiting their scalability. Large Language Model LLM-based agents have shownpromise in automating these tasks due to their increasing problem-solvingabilities. To support the evaluation and development of such methods weintroduce GenoTEX a benchmark dataset for the automatic exploration of geneexpression data involving the tasks of dataset selection preprocessing andstatistical analysis. GenoTEX provides annotated code and results for solving awide range of gene identification problems in a full analysis pipeline thatfollows the standard of computational genomics. These annotations are curatedby human bioinformaticians who carefully analyze the datasets to ensureaccuracy and reliability. To provide baselines for these tasks we presentGenoAgents a team of LLM-based agents designed with context-aware planningiterative correction and domain expert consultation to collaboratively exploregene datasets. Our experiments with GenoAgents demonstrate the potential ofLLM-based approaches in genomics data analysis while error analysis highlightsthe challenges and areas for future improvement. We propose GenoTEX as apromising resource for benchmarking and enhancing AI-driven methods forgenomics data analysis. We make our benchmark publicly available aturlhttps://github.com/Liu-Hy/GenoTex. |


| Item |Content|
| --- |---|
|idx| 2406.15334v1 |
|title| Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning |
|authors| Brandon HuangChancharik MitraAssaf ArbelleLeonid KarlinskyTrevor DarrellRoei Herzig
|links| http://arxiv.org/abs/2406.15334v1 |
|updated| 2024-06-21 17:50:02 UTC |
|summary| The recent success of interleaved Large Multimodal Models LMMs in few-shotlearning suggests that in-context learning ICL with many examples can bepromising for learning new tasks. However this many-shot multimodal ICLsetting has one crucial problem: it is fundamentally limited by the modelscontext length set at pretraining. The problem is especially prominent in themultimodal domain which processes both text and images requiring additionaltokens. This motivates the need for a multimodal method to compress many shotsinto fewer tokens without finetuning. In this work we enable LMMs to performmultimodal many-shot in-context learning by leveraging Multimodal Task VectorsMTV--compact implicit representations of in-context examples compressed inthe models attention heads. Specifically we first demonstrate the existenceof such MTV in LMMs and then leverage these extracted MTV to enable many-shotin-context learning for various vision-and-language tasks. Our experimentssuggest that MTV can scale in performance with the number of compressed shotsand generalize to similar out-of-domain tasks without additional context lengthfor inference. |


| Item |Content|
| --- |---|
|idx| 2406.15331v1 |
|title| Masked Extended Attention for Zero-Shot Virtual Try-On In The Wild |
|authors| Nadav OrzechYotam NitzanUlysse MizrahiDov DanonAmit H. Bermano
|links| http://arxiv.org/abs/2406.15331v1 |
|updated| 2024-06-21 17:45:37 UTC |
|summary| Virtual Try-On VTON is a highly active line of research with increasingdemand. It aims to replace a piece of garment in an image with one fromanother while preserving person and garment characteristics as well as imagefidelity. Current literature takes a supervised approach for the taskimpairing generalization and imposing heavy computation. In this paper wepresent a novel zero-shot training-free method for inpainting a clothinggarment by reference. Our approach employs the prior of a diffusion model withno additional training fully leveraging its native generalizationcapabilities. The method employs extended attention to transfer imageinformation from reference to target images overcoming two significantchallenges. We first initially warp the reference garment over the target humanusing deep features alleviating texture sticking. We then leverage theextended attention mechanism with careful masking eliminating leakage ofreference background and unwanted influence. Through a user study qualitativeand quantitative comparison to state-of-the-art approaches we demonstratesuperior image quality and garment preservation compared unseen clothing piecesor human figures. |


# cs.CV 

| Item |Content|
| --- |---|
|idx| 2406.15349v1 |
|title| NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking |
|authors| Daniel DaunerMarcel HallgartenTianyu LiXinshuo WengZhiyu HuangZetong YangHongyang LiIgor GilitschenskiBoris IvanovicMarco PavoneAndreas GeigerKashyap Chitta
|links| http://arxiv.org/abs/2406.15349v1 |
|updated| 2024-06-21 17:59:02 UTC |
|summary| Benchmarking vision-based driving policies is challenging. On one handopen-loop evaluation with real data is easy but these results do not reflectclosed-loop performance. On the other closed-loop evaluation is possible insimulation but is hard to scale due to its significant computational demands.Further the simulators available today exhibit a large domain gap to realdata. This has resulted in an inability to draw clear conclusions from therapidly growing body of research on end-to-end autonomous driving. In thispaper we present NAVSIM a middle ground between these evaluation paradigmswhere we use large datasets in combination with a non-reactive simulator toenable large-scale real-world benchmarking. Specifically we gathersimulation-based metrics such as progress and time to collision by unrollingbirds eye view abstractions of the test scenes for a short simulation horizon.Our simulation is non-reactive i.e. the evaluated policy and environment donot influence each other. As we demonstrate empirically this decoupling allowsopen-loop metric computation while being better aligned with closed-loopevaluations than traditional displacement errors. NAVSIM enabled a newcompetition held at CVPR 2024 where 143 teams submitted 463 entries resultingin several new insights. On a large set of challenging scenarios we observethat simple methods with moderate compute requirements such as TransFuser canmatch recent large-scale end-to-end driving architectures such as UniAD. Ourmodular framework can potentially be extended with new datasets data curationstrategies and metrics and will be continually maintained to host futurechallenges. Our code is available athttps://github.com/autonomousvision/navsim. |


| Item |Content|
| --- |---|
|idx| 2406.15340v1 |
|title| Full-Scale Indexing and Semantic Annotation of CT Imaging: Boosting FAIRness |
|authors| Hannes UlrichRobin HendelSantiago PazminoBjörn BerghBjörn Schreiweis
|links| http://arxiv.org/abs/2406.15340v1 |
|updated| 2024-06-21 17:55:22 UTC |
|summary| Background: The integration of artificial intelligence into medicine has ledto significant advances particularly in diagnostics and treatment planning.However the reliability of AI models is highly dependent on the quality of thetraining data especially in medical imaging where varying patient data andevolving medical knowledge pose a challenge to the accuracy andgeneralizability of given datasets. Results: The proposed approach focuses onthe integration and enhancement of clinical computed tomography CT imageseries for better findability accessibility interoperability andreusability. Through an automated indexing process CT image series aresemantically enhanced using the TotalSegmentator framework for segmentation andresulting SNOMED CT annotations. The metadata is standardized with HL7 FHIRresources to enable efficient data recognition and data exchange betweenresearch projects. Conclusions: The study successfully integrates a robustprocess within the UKSH MeDIC leading to the semantic enrichment of over230000 CT image series and over 8 million SNOMED CT annotations. Thestandardized representation using HL7 FHIR resources improves discoverabilityand facilitates interoperability providing a foundation for the FAIRness ofmedical imaging data. However developing automated annotation methods that cankeep pace with growing clinical datasets remains a challenge to ensurecontinued progress in large-scale integration and indexing of medical imagingfor advanced healthcare AI applications. |


| Item |Content|
| --- |---|
|idx| 2406.15339v1 |
|title| Image Conductor: Precision Control for Interactive Video Synthesis |
|authors| Yaowei LiXintao WangZhaoyang ZhangZhouxia WangZiyang YuanLiangbin XieYuexian ZouYing Shan
|links| http://arxiv.org/abs/2406.15339v1 |
|updated| 2024-06-21 17:55:05 UTC |
|summary| Filmmaking and animation production often require sophisticated techniquesfor coordinating camera transitions and object movements typically involvinglabor-intensive real-world capturing. Despite advancements in generative AI forvideo creation achieving precise control over motion for interactive videoasset generation remains challenging. To this end we propose Image Conductora method for precise control of camera transitions and object movements togenerate video assets from a single image. An well-cultivated training strategyis proposed to separate distinct camera and object motion by camera LoRAweights and object LoRA weights. To further address cinematographic variationsfrom ill-posed trajectories we introduce a camera-free guidance techniqueduring inference enhancing object movements while eliminating cameratransitions. Additionally we develop a trajectory-oriented video motion datacuration pipeline for training. Quantitative and qualitative experimentsdemonstrate our methods precision and fine-grained control in generatingmotion-controllable videos from images advancing the practical application ofinteractive video synthesis. Project webpage available athttps://liyaowei-stu.github.io/project/ImageConductor/ |


| Item |Content|
| --- |---|
|idx| 2406.15335v1 |
|title| Keystroke Dynamics Against Academic Dishonesty in the Age of LLMs |
|authors| Debnath KunduAtharva MehtaRajesh KumarNaman LalAvinash AnandApoorv SinghRajiv Ratn Shah
|links| http://arxiv.org/abs/2406.15335v1 |
|updated| 2024-06-21 17:51:26 UTC |
|summary| The transition to online examinations and assignments raises significantconcerns about academic integrity. Traditional plagiarism detection systemsoften struggle to identify instances of intelligent cheating particularly whenstudents utilize advanced generative AI tools to craft their responses. Thisstudy proposes a keystroke dynamics-based method to differentiate between bonafide and assisted writing within academic contexts. To facilitate this adataset was developed to capture the keystroke patterns of individuals engagedin writing tasks both with and without the assistance of generative AI. Thedetector trained using a modified TypeNet architecture achieved accuraciesranging from 74.98 to 85.72 in condition-specific scenarios and from 52.24to 80.54 in condition-agnostic scenarios. The findings highlight significantdifferences in keystroke dynamics between genuine and assisted writing. Theoutcomes of this study enhance our understanding of how users interact withgenerative AI and have implications for improving the reliability of digitaleducational platforms. |


| Item |Content|
| --- |---|
|idx| 2406.15334v1 |
|title| Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning |
|authors| Brandon HuangChancharik MitraAssaf ArbelleLeonid KarlinskyTrevor DarrellRoei Herzig
|links| http://arxiv.org/abs/2406.15334v1 |
|updated| 2024-06-21 17:50:02 UTC |
|summary| The recent success of interleaved Large Multimodal Models LMMs in few-shotlearning suggests that in-context learning ICL with many examples can bepromising for learning new tasks. However this many-shot multimodal ICLsetting has one crucial problem: it is fundamentally limited by the modelscontext length set at pretraining. The problem is especially prominent in themultimodal domain which processes both text and images requiring additionaltokens. This motivates the need for a multimodal method to compress many shotsinto fewer tokens without finetuning. In this work we enable LMMs to performmultimodal many-shot in-context learning by leveraging Multimodal Task VectorsMTV--compact implicit representations of in-context examples compressed inthe models attention heads. Specifically we first demonstrate the existenceof such MTV in LMMs and then leverage these extracted MTV to enable many-shotin-context learning for various vision-and-language tasks. Our experimentssuggest that MTV can scale in performance with the number of compressed shotsand generalize to similar out-of-domain tasks without additional context lengthfor inference. |


# stat.ML 

| Item |Content|
| --- |---|
|idx| 2406.15229v1 |
|title| ExDAG: Exact learning of DAGs |
|authors| Pavel RytířAleš WodeckiJakub Mareček
|links| http://arxiv.org/abs/2406.15229v1 |
|updated| 2024-06-21 15:15:38 UTC |
|summary| There has been a growing interest in causal learning in recent years.Commonly used representations of causal structures including Bayesian networksand structural equation models SEM take the form of directed acyclic graphsDAGs. We provide a novel mixed-integer quadratic programming formulation andassociated algorithm that identifies DAGs on up to 50 vertices where these areidentifiable. We call this method ExDAG which stands for Exact learning ofDAGs. Although there is a superexponential number of constraints that preventthe formation of cycles the algorithm adds constraints violated by solutionsfound rather than imposing all constraints in each continuous-valuedrelaxation. Our empirical results show that ExDAG outperforms localstate-of-the-art solvers in terms of precision and outperforms state-of-the-artglobal solvers with respect to scaling when considering Gaussian noise. Wealso provide validation with respect to other noise distributions. |


| Item |Content|
| --- |---|
|idx| 2406.15152v1 |
|title| Generative Topological Networks |
|authors| Alona Levy-JurgensonZohar Yakhini
|links| http://arxiv.org/abs/2406.15152v1 |
|updated| 2024-06-21 13:55:34 UTC |
|summary| Generative models have seen significant advancements in recent years yetoften remain challenging and costly to train and use. We introduce GenerativeTopological Networks GTNs -- a new class of generative models that addressesthese shortcomings. GTNs are trained deterministically using a simplesupervised learning approach grounded in topology theory. GTNs are fast totrain and require only a single forward pass in a standard feedforward neuralnetwork to generate samples. We demonstrate the strengths of GTNs in severaldatasets including MNIST celebA and the Hands and Palm Images dataset.Finally the theory behind GTNs offers insights into how to train generativemodels for improved performance. |


| Item |Content|
| --- |---|
|idx| 2406.15131v1 |
|title| KalMamba: Towards Efficient Probabilistic State Space Models for RL under Uncertainty |
|authors| Philipp BeckerNiklas FreymuthGerhard Neumann
|links| http://arxiv.org/abs/2406.15131v1 |
|updated| 2024-06-21 13:27:36 UTC |
|summary| Probabilistic State Space Models SSMs are essential for ReinforcementLearning RL from high-dimensional partial information as they provideconcise representations for control. Yet they lack the computationalefficiency of their recent deterministic counterparts such as S4 or Mamba. Wepropose KalMamba an efficient architecture to learn representations for RLthat combines the strengths of probabilistic SSMs with the scalability ofdeterministic SSMs. KalMamba leverages Mamba to learn the dynamics parametersof a linear Gaussian SSM in a latent space. Inference in this latent spaceamounts to standard Kalman filtering and smoothing. We realize these operationsusing parallel associative scanning similar to Mamba to obtain a principledhighly efficient and scalable probabilistic SSM. Our experiments show thatKalMamba competes with state-of-the-art SSM approaches in RL whilesignificantly improving computational efficiency especially on longerinteraction sequences. |


| Item |Content|
| --- |---|
|idx| 2406.15012v1 |
|title| Exact discovery is polynomial for sparse causal Bayesian networks |
|authors| Felix L. RiosGiusi MoffaJack Kuipers
|links| http://arxiv.org/abs/2406.15012v1 |
|updated| 2024-06-21 09:41:30 UTC |
|summary| Causal Bayesian networks are widely used tools for summarising thedependencies between variables and elucidating their putative causalrelationships. Learning networks from data is computationally hard in general.The current state-of-the-art approaches for exact causal discovery are integerlinear programming over the underlying space of directed acyclic graphsdynamic programming and shortest-path searches over the space of topologicalorders and constraint programming combining both. For dynamic programming overorders the computational complexity is known to be exponential base 2 in thenumber of variables in the network. We demonstrate how to use properties ofBayesian networks to prune the search space and lower the computational costwhile still guaranteeing exact discovery. When including new path-search anddivide-and-conquer criteria we prove optimality in quadratic time formatchings and polynomial time for any network class with logarithmically-boundlargest connected components. In simulation studies we observe the polynomialdependence for sparse networks and that beyond some critical value thelogarithm of the base grows with the network density. Our approach thenout-competes the state-of-the-art at lower densities. These results thereforepave the way for faster exact causal discovery in larger and sparser networks. |


| Item |Content|
| --- |---|
|idx| 2406.14995v1 |
|title| Probabilistic and Differentiable Wireless Simulation with Geometric Transformers |
|authors| Thomas HehnMarkus PeschlTribhuvanesh OrekondyArash BehboodiJohann Brehmer
|links| http://arxiv.org/abs/2406.14995v1 |
|updated| 2024-06-21 09:14:11 UTC |
|summary| Modelling the propagation of electromagnetic signals is critical fordesigning modern communication systems. While there are precise simulatorsbased on ray tracing they do not lend themselves to solving inverse problemsor the integration in an automated design loop. We propose to address thesechallenges through differentiable neural surrogates that exploit the geometricaspects of the problem. We first introduce the Wireless Geometric AlgebraTransformer Wi-GATr a generic backbone architecture for simulating wirelesspropagation in a 3D environment. It uses versatile representations based ongeometric algebra and is equivariant with respect to E3 the symmetry groupof the underlying physics. Second we study two algorithmic approaches tosignal prediction and inverse problems based on differentiable predictivemodelling and diffusion models. We show how these let us predict receivedpower localize receivers and reconstruct the 3D environment from the receivedsignal. Finally we introduce two large geometry-focused datasets of wirelesssignal propagation in indoor scenes. In experiments we show that ourgeometry-forward approach achieves higher-fidelity predictions with less datathan various baselines. |


# cs.HC 

| Item |Content|
| --- |---|
|idx| 2406.15259v1 |
|title| V-RECS, a Low-Cost LLM4VIS Recommender with Explanations, Captioning and Suggestions |
|authors| Luca PodoMarco AngeliniPaola Velardi
|links| http://arxiv.org/abs/2406.15259v1 |
|updated| 2024-06-21 15:50:10 UTC |
|summary| NL2VIS natural language to visualization is a promising and recent researcharea that involves interpreting natural language queries and translating theminto visualizations that accurately represent the underlying data. As wenavigate the era of big data NL2VIS holds considerable application potentialsince it greatly facilitates data exploration by non-expert users. Followingthe increasingly widespread usage of generative AI in NL2VIS applications inthis paper we present V-RECS the first LLM-based Visual Recommender augmentedwith explanationsE captioningC and suggestionsS for further dataexploration. V-RECS visualization narratives facilitate both responseverification and data exploration by non-expert users. Furthermore ourproposed solution mitigates computational controllability and cost issuesassociated with using powerful LLMs by leveraging a methodology to effectivelyfine-tune small models. To generate insightful visualization narratives we useChain-of-Thoughts CoT a prompt engineering technique to help LLM identifyand generate the logical steps to produce a correct answer. Since CoT isreported to perform poorly with small LLMs we adopted a strategy in which alarge LLM GPT-4 acting as a Teacher generates CoT-based instructions tofine-tune a small model Llama-2-7B which plays the role of a Student.Extensive experiments-based on a framework for the quantitative evaluation ofAI-based visualizations and on manual assessment by a group ofparticipants-show that V-RECS achieves performance scores comparable to GPT-4at a much lower cost. The efficacy of the V-RECS teacher-student paradigm isalso demonstrated by the fact that the un-tuned Llama fails to perform the taskin the vast majority of test cases. We release V-RECS for the visualizationcommunity to assist visualization designers throughout the entire visualizationgeneration process. |


| Item |Content|
| --- |---|
|idx| 2406.15198v1 |
|title| Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms |
|authors| Santiago Berrezueta-GuzmanMohanad KandilMaría-Luisa Martín-RuizIván Pau-de-la-CruzStephan Krusche
|links| http://arxiv.org/abs/2406.15198v1 |
|updated| 2024-06-21 14:38:25 UTC |
|summary| Attention Deficit Hyperactivity Disorder ADHD is a neurodevelopmentalcondition characterized by inattention hyperactivity and impulsivity whichcan significantly impact an individuals daily functioning and quality of life.Occupational therapy plays a crucial role in managing ADHD by fostering thedevelopment of skills needed for daily living and enhancing an individualsability to participate fully in school home and social situations. Recentstudies highlight the potential of integrating Large Language Models LLMslike ChatGPT and Socially Assistive Robots SAR to improve psychologicaltreatments. This integration aims to overcome existing limitations in mentalhealth therapy by providing tailored support and adapting to the unique needsof this sensitive group. However there remains a significant gap in researchexploring the combined use of these advanced technologies in ADHD therapysuggesting an opportunity for novel therapeutic approaches.  Thus we integrated two advanced language models ChatGPT-4 Turbo andClaude-3 Opus into a robotic assistant to explore how well each model performsin robot-assisted interactions. Additionally we have compared theirperformance in a simulated therapy scenario to gauge their effectivenessagainst a clinically validated customized model. The results of this study showthat ChatGPT-4 Turbo excelled in performance and responsiveness making itsuitable for time-sensitive applications. Claude-3 Opus on the other handshowed strengths in understanding coherence and ethical considerationsprioritizing safe and engaging interactions. Both models demonstratedinnovation and adaptability but ChatGPT-4 Turbo offered greater ease ofintegration and broader language support. The selection between them hinges onthe specific demands of ADHD therapy. |


| Item |Content|
| --- |---|
|idx| 2406.15074v1 |
|title| Balancing The Perception of Cheating Detection, Privacy and Fairness: A Mixed-Methods Study of Visual Data Obfuscation in Remote Proctoring |
|authors| Suvadeep MukherjeeVerena DistlerGabriele LenziniPedro Cardoso-Leite
|links| http://arxiv.org/abs/2406.15074v1 |
|updated| 2024-06-21 11:40:56 UTC |
|summary| Remote proctoring technology a cheating-preventive measure often raisesprivacy and fairness concerns that may affect test-takers experiences and thevalidity of test results. Our study explores how selectively obfuscatinginformation in video recordings can protect test-takers privacy while ensuringeffective and fair cheating detection. Interviews with experts N9 identifiedfour key video regions indicative of potential cheating behaviors: thetest-takers face body background and the presence of individuals in thebackground. Experts recommended specific obfuscation methods for each regionbased on privacy significance and cheating behavior frequency ranging fromconventional blurring to advanced methods like replacement with deepfake 3Davatars and silhouetting. We then conducted a vignette experiment withpotential test-takers N259 non-experts to evaluate their perceptions ofcheating detection visual privacy and fairness using descriptions andexamples of still images for each expert-recommended combination of videoregions and obfuscation methods. Our results indicate that the effectiveness ofobfuscation methods varies by region. Tailoring remote proctoring withregion-specific advanced obfuscation methods can improve the perceptions ofprivacy and fairness compared to the conventional methods though it maydecrease perceived information sufficiency for detecting cheating. Howevernon-experts preferred conventional blurring for videos they were more willingto share highlighting a gap between the perceived effectiveness of theadvanced obfuscation methods and their practical acceptance. This studycontributes to the field of user-centered privacy by suggesting promisingdirections to address current remote proctoring challenges and guiding futureresearch. |


| Item |Content|
| --- |---|
|idx| 2406.15003v1 |
|title| Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN |
|authors| Oluwaleke YusufMaki HabibMohamed Moustafa
|links| http://arxiv.org/abs/2406.15003v1 |
|updated| 2024-06-21 09:30:59 UTC |
|summary| This study focuses on Hand Gesture Recognition HGR which is vital forperceptual computing across various real-world contexts. The primary challengein the HGR domain lies in dealing with the individual variations inherent inhuman hand morphology. To tackle this challenge we introduce an innovative HGRframework that combines data-level fusion and an Ensemble Tuner Multi-streamCNN architecture. This approach effectively encodes spatiotemporal gestureinformation from the skeleton modality into RGB images thereby minimizingnoise while improving semantic gesture comprehension. Our framework operates inreal-time significantly reducing hardware requirements and computationalcomplexity while maintaining competitive performance on benchmark datasets suchas SHREC2017 DHG1428 FPHA LMDHG and CNR. This improvement in HGRdemonstrates robustness and paves the way for practical real-time applicationsthat leverage resource-limited devices for human-machine interaction andambient intelligence. |


| Item |Content|
| --- |---|
|idx| 2406.14981v1 |
|title| Human-AI collectives produce the most accurate differential diagnoses |
|authors| N. ZöllerJ. BergerI. LinN. FuJ. KomarneniG. BarabucciK. LaskowskiV. ShiaB. HarackE. A. ChuV. TrianniR. H. J. M. KurversS. M. Herzog
|links| http://arxiv.org/abs/2406.14981v1 |
|updated| 2024-06-21 08:46:30 UTC |
|summary| Artificial intelligence systems particularly large language models LLMsare increasingly being employed in high-stakes decisions that impact bothindividuals and society at large often without adequate safeguards to ensuresafety quality and equity. Yet LLMs hallucinate lack common sense and arebiased - shortcomings that may reflect LLMs inherent limitations and thus maynot be remedied by more sophisticated architectures more data or more humanfeedback. Relying solely on LLMs for complex high-stakes decisions istherefore problematic. Here we present a hybrid collective intelligence systemthat mitigates these risks by leveraging the complementary strengths of humanexperience and the vast information processed by LLMs. We apply our method toopen-ended medical diagnostics combining 40762 differential diagnoses made byphysicians with the diagnoses of five state-of-the art LLMs across 2133medical cases. We show that hybrid collectives of physicians and LLMsoutperform both single physicians and physician collectives as well as singleLLMs and LLM ensembles. This result holds across a range of medical specialtiesand professional experience and can be attributed to humans and LLMscomplementary contributions that lead to different kinds of errors. Ourapproach highlights the potential for collective human and machine intelligenceto improve accuracy in complex open-ended domains like medical diagnostics. |


# cs.MA 

| Item |Content|
| --- |---|
|idx| 2406.15202v1 |
|title| Phase-Bounded Broadcast Networks over Topologies of Communication |
|authors| Lucie GuillouArnaud SangnierNathalie Sznajder
|links| http://arxiv.org/abs/2406.15202v1 |
|updated| 2024-06-21 14:43:23 UTC |
|summary| We study networks of processes that all execute the same finite stateprotocol and that communicate through broadcasts. The processes are organizedin a graph a topology and only the neighbors of a process in this graph canreceive its broadcasts. The coverability problem asks given a protocol and astate of the protocol whether there is a topology for the processes such thatone of them at least reaches the given state. This problem is undecidable. Westudy here an under-approximation of the problem where processes alternate abounded number of times k between phases of broadcasting and phases ofreceiving messages. We show that if the problem remains undecidable when kis greater than 6 it becomes decidable for k2 and EXPSPACE-complete fork1. Furthermore we show that if we restrict ourselves to line topologiesthe problem is in P for k1 and k2. |


| Item |Content|
| --- |---|
|idx| 2406.15096v1 |
|title| Towards General Negotiation Strategies with End-to-End Reinforcement Learning |
|authors| Bram M. RentingThomas M. MoerlandHolger H. HoosCatholijn M. Jonker
|links| http://arxiv.org/abs/2406.15096v1 |
|updated| 2024-06-21 12:24:36 UTC |
|summary| The research field of automated negotiation has a long history of designingagents that can negotiate with other agents. Such negotiation strategies aretraditionally based on manual design and heuristics. More recentlyreinforcement learning approaches have also been used to train agents tonegotiate. However negotiation problems are diverse causing observation andaction dimensions to change which cannot be handled by default linear policynetworks. Previous work on this topic has circumvented this issue either byfixing the negotiation problem causing policies to be non-transferable betweennegotiation problems or by abstracting the observations and actions intofixed-size representations causing loss of information and expressiveness dueto feature design. We developed an end-to-end reinforcement learning method fordiverse negotiation problems by representing observations and actions as agraph and applying graph neural networks in the policy. With empiricalevaluations we show that our method is effective and that we can learn tonegotiate with other agents on never-before-seen negotiation problems. Ourresult opens up new opportunities for reinforcement learning in negotiationagents. |


| Item |Content|
| --- |---|
|idx| 2406.15036v1 |
|title| Effects of non-uniform number of actions by Hawkes process on spatial cooperation |
|authors| Daiki MiyagawaGenki Ichinose
|links| http://arxiv.org/abs/2406.15036v1 |
|updated| 2024-06-21 10:33:12 UTC |
|summary| The emergence of cooperative behavior despite natural selection favoringrational self-interest presents a significant evolutionary puzzle.Evolutionary game theory elucidates why cooperative behavior can beadvantageous for survival. However the impact of non-uniformity in thefrequency of actions particularly when actions are altered in the short termhas received little scholarly attention. To demonstrate the relationshipbetween the non-uniformity in the frequency of actions and the evolution ofcooperation we conducted multi-agent simulations of evolutionary games. In ourmodel each agent performs actions in a chain-reaction resulting in anon-uniform distribution of the number of actions. To achieve a variety ofnon-uniform action frequency we introduced two types of chain-reaction rules:one where an agents actions trigger subsequent actions and another where anagents actions depend on the actions of others. Our results revealed thatcooperation evolves more effectively in scenarios with even slightnon-uniformity in action frequency compared to completely uniform cases. Inaddition scenarios where agents actions are primarily triggered by their ownprevious actions more effectively support cooperation whereas those triggeredby others actions are less effective. This implies that a few highly activeindividuals contribute positively to cooperation while the tendency to followothers actions can hinder it. |


| Item |Content|
| --- |---|
|idx| 2406.14928v1 |
|title| Autonomous Agents for Collaborative Task under Information Asymmetry |
|authors| Wei LiuChenxi WangYifei WangZihao XieRennai QiuYufan DangZhuoyun DuWeize ChenCheng YangChen Qian
|links| http://arxiv.org/abs/2406.14928v1 |
|updated| 2024-06-21 07:37:19 UTC |
|summary| Large Language Model Multi-Agent Systems LLM-MAS have achieved greatprogress in solving complex tasks. It performs communication among agentswithin the system to collaboratively solve tasks under the premise of sharedinformation. However when agents communication is leveraged to enhance humancooperation a new challenge arises due to information asymmetry since eachagent can only access the information of its human user. Previous MAS struggleto complete tasks under this condition. To address this we propose a new MASparadigm termed iAgents which denotes Informative Multi-Agent Systems. IniAgents the human social network is mirrored in the agent network whereagents proactively exchange human information necessary for task resolutionthereby overcoming information asymmetry. iAgents employs a novel agentreasoning mechanism InfoNav to navigate agents communication towardseffective information exchange. Together with InfoNav iAgents organizes humaninformation in a mixed memory to provide agents with accurate and comprehensiveinformation for exchange. Additionally we introduce InformativeBench thefirst benchmark tailored for evaluating LLM agents task-solving ability underinformation asymmetry. Experimental results show that iAgents can collaboratewithin a social network of 140 individuals and 588 relationships autonomouslycommunicate over 30 turns and retrieve information from nearly 70000 messagesto complete tasks within 3 minutes. |


| Item |Content|
| --- |---|
|idx| 2406.14922v1 |
|title| Social learning with complex contagion |
|authors| Hiroaki Chiba-OkabeJoshua B. Plotkin
|links| http://arxiv.org/abs/2406.14922v1 |
|updated| 2024-06-21 07:32:58 UTC |
|summary| We introduce a mathematical model that combines the concepts of complexcontagion with payoff-biased imitation to describe how social behaviors spreadthrough a population. Traditional models of social learning by imitation arebased on simple contagion -- where an individual may imitate a more successfulneighbor following a single interaction. Our framework generalizes this processto incorporate complex contagion which requires multiple exposures before anindividual considers adopting a different behavior. We formulate this as adiscrete time and state stochastic process in a finite population and wederive its continuum limit as an ordinary differential equation thatgeneralizes the replicator equation the most widely used dynamical model inevolutionary game theory. When applied to linear frequency-dependent games oursocial learning with complex contagion produces qualitatively differentoutcomes than traditional imitation dynamics: it can shift the PrisonersDilemma from a unique all-defector equilibrium to either a stable mixture ofcooperators and defectors in the population or a bistable system it changesthe Snowdrift game from a single to a bistable equilibrium and it can alterthe Coordination game from bistability at the boundaries to two internalequilibria. The long-term outcome depends on the balance between the complexityof the contagion process and the strength of selection that biases imitationtowards more successful types. Our analysis intercalates the fields ofevolutionary game theory with complex contagions and it provides a syntheticframework that describes more realistic forms of behavioral change in socialsystems. |


