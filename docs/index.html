
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Distributed Finite-time Differentiator for Multi-agent Systems Under Directed Graph</h3>
                <p>Authors: Weile ChenHaibo DuShihua Li</p>
                <p><a href="http://arxiv.org/abs/2402.16260v1">Link to paper</a></p>
                <p>This paper proposes a new distributed finite-time differentiator DFD formulti-agent systems MAS under directed graph which extends thedifferentiator algorithm from the centralized case to the distributed case byonly using relative/absolute position information. By skillfully constructing aLyapunov function the finite-time stability of the closed-loop system underDFD is proved. Inspired by the duality principle of control theory adistributed continuous finite-time output consensus algorithm extended from DFDfor a class of leader-follower MAS is provided which not only completelysuppresses disturbance but also avoids chattering. Finally several simulationexamples are given to verify the effectiveness of the DFD.</p>
                <p>Last Updated: 2024-02-26 02:45:19 UTC</p>
                <button class="interpret-button" data-id="2402.16260v1">Interpret</button>
                <div id="interpretation-2402.16260v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition</h3>
                <p>Authors: Dylan CopePeter McBurney</p>
                <p><a href="http://arxiv.org/abs/2402.16247v1">Link to paper</a></p>
                <p>In Emergent Communication EC agents learn to communicate with one anotherbut the protocols that they develop are specialised to their trainingcommunity. This observation led to research into Zero-Shot Coordination ZSCfor learning communication strategies that are robust to agents not encounteredduring training. However ZSC typically assumes that no prior data is availableabout the agents that will be encountered in the zero-shot setting. In manycases this presents an unnecessarily hard problem and rules out communicationvia preestablished conventions. We propose a novel AI challenge called aCooperative Language Acquisition Problem CLAP in which the ZSC assumptionsare relaxed by allowing a joiner agent to learn from a dataset ofinteractions between agents in a target community. We propose and compare twomethods for solving CLAPs: Imitation Learning IL and Emergent Communicationpretraining and Translation Learning ECTL in which an agent is trained inself-play with EC and then learns from the data to translate between theemergent protocol and the target communitys protocol.</p>
                <p>Last Updated: 2024-02-26 02:13:36 UTC</p>
                <button class="interpret-button" data-id="2402.16247v1">Interpret</button>
                <div id="interpretation-2402.16247v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Honeybee: Decentralized Peer Sampling with Verifiable Random Walks for Blockchain Data Sharding</h3>
                <p>Authors: Yunqi ZhangShaileshh Bojja Venkatakrishnan</p>
                <p><a href="http://arxiv.org/abs/2402.16201v1">Link to paper</a></p>
                <p>Data sharding - in which block data is sharded without sharding compute - isat the present the favored approach for scaling Ethereum. A key challengetoward implementing data sharding is verifying whether the entirety of ablocks data is available in the network across its shards. A centraltechnique proposed to conduct this verification uses erasure coded blocks andis called data availability sampling DAS. While the high-level protocoldetails of DAS has been well discussed in the community discussions around howsuch a protocol will be implemented at the peer-to-peer layer are lacking. Weidentify random sampling of nodes as a fundamental primitive necessary to carryout DAS and present Honeybee a decentralized algorithm for sampling node thatuses verifiable random walks. Honeybee is secure against attacks even in thepresence of a large number of Byzantine nodes e.g. 50 of the network. Weevaluate Honeybee through experiments and show that the quality of samplingachieved by Honeybee is significantly better compared to the state-of-the-art.Our proposed algorithm has implications for DAS functions in both full nodesand light nodes.</p>
                <p>Last Updated: 2024-02-25 21:29:44 UTC</p>
                <button class="interpret-button" data-id="2402.16201v1">Interpret</button>
                <div id="interpretation-2402.16201v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Egalitarian Price of Fairness for Indivisible Goods</h3>
                <p>Authors: Karen Frilya CelineMuhammad Ayaz DzulfikarIvan Adrian Koswara</p>
                <p><a href="http://dx.doi.org/10.1007/978-981-99-7019-3_3">Link to paper</a></p>
                <p>In the context of fair division the concept of price of fairness has beenintroduced to quantify the loss of welfare when we have to satisfy somefairness condition. In other words it is the price we have to pay to guaranteefairness. Various settings of fair division have been considered previously weextend to the setting of indivisible goods by using egalitarian welfare as thewelfare measure instead of the commonly used utilitarian welfare. We providelower and upper bounds for various fairness and efficiency conditions such asenvy-freeness up to one good EF1 and maximum Nash welfare MNW.</p>
                <p>Last Updated: 2024-02-25 16:54:51 UTC</p>
                <button class="interpret-button" data-id="2402.16145v1">Interpret</button>
                <div id="interpretation-2402.16145v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Cooperation and Control in Delegation Games</h3>
                <p>Authors: Oliver SourbutLewis HammondHarriet Wood</p>
                <p><a href="http://arxiv.org/abs/2402.15821v1">Link to paper</a></p>
                <p>Many settings of interest involving humans and machines -- from virtualpersonal assistants to autonomous vehicles -- can naturally be modelled asprincipals humans delegating to agents machines which then interact witheach other on their principals behalf. We refer to these multi-principalmulti-agent scenarios as delegation games. In such games there are twoimportant failure modes: problems of control where an agent fails to act inline their principals preferences and problems of cooperation where theagents fail to work well together. In this paper we formalise and analysethese problems further breaking them down into issues of alignment do theplayers have similar preferences and capabilities how competent are theplayers at satisfying those preferences. We show -- theoretically andempirically -- how these measures determine the principals welfare how theycan be estimated using limited observations and thus how they might be used tohelp us design more aligned and cooperative AI systems.</p>
                <p>Last Updated: 2024-02-24 14:17:41 UTC</p>
                <button class="interpret-button" data-id="2402.15821v1">Interpret</button>
                <div id="interpretation-2402.15821v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Q-FOX Learning: Breaking Tradition in Reinforcement Learning</h3>
                <p>Authors: Mahmood AlqaseerYossra H. AliTarik A. Rashid</p>
                <p><a href="http://arxiv.org/abs/2402.16562v1">Link to paper</a></p>
                <p>Reinforcement learning RL is a subset of artificial intelligence AI whereagents learn the best action by interacting with the environment making itsuitable for tasks that do not require labeled data or direct supervision.Hyperparameters HP tuning refers to choosing the best parameter that leads tooptimal solutions in RL algorithms. Manual or random tuning of the HP may be acrucial process because variations in this parameter lead to changes in theoverall learning aspects and different rewards. In this paper a novel andautomatic HP-tuning method called Q-FOX is proposed. This uses both the FOXoptimizer a new optimization method inspired by nature that mimics red foxeshunting behavior and the commonly used easy-to-implement RL Q-learningalgorithm to solve the problem of HP tuning. Moreover a new objective functionis proposed which prioritizes the reward over the mean squared error MSE andlearning time steps. Q-FOX has been evaluated on two OpenAI Gym environmentcontrol tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewardsthan HP tuning with other optimizers such as PSO GA Bee or randomlyselected HP. The cumulative reward for the Cart Pole task was 32.08 and forthe Frozen Lake task was 0.95. Despite the robustness of Q-FOX it haslimitations. It cannot be used directly in real-word problems before choosingthe HP in a simulation environment because its processes work iterativelymaking it time-consuming. The results indicate that Q-FOX has played anessential role in HP tuning for RL algorithms to effectively solve differentcontrol tasks.</p>
                <p>Last Updated: 2024-02-26 13:39:04 UTC</p>
                <button class="interpret-button" data-id="2402.16562v1">Interpret</button>
                <div id="interpretation-2402.16562v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep Learning Projects</h3>
                <p>Authors: Han WangSijia YuChunyang ChenBurak TurhanXiaodong Zhu</p>
                <p><a href="http://dx.doi.org/10.1145/3638245">Link to paper</a></p>
                <p>Deep Learning DL models have rapidly advanced focusing on achieving highperformance through testing model accuracy and robustness. However it isunclear whether DL projects as software systems are tested thoroughly orfunctionally correct when there is a need to treat and test them like othersoftware systems. Therefore we empirically study the unit tests in open-sourceDL projects analyzing 9129 projects from GitHub. We find that: 1 unit testedDL projects have positive correlation with the open-source project metrics andhave a higher acceptance rate of pull requests 2 68 of the sampled DLprojects are not unit tested at all 3 the layer and utilities utils of DLmodels have the most unit tests. Based on these findings and previous researchoutcomes we built a mapping taxonomy between unit tests and faults in DLprojects. We discuss the implications of our findings for developers andresearchers and highlight the need for unit testing in open-source DL projectsto ensure their reliability and stability. The study contributes to thiscommunity by raising awareness of the importance of unit testing in DL projectsand encouraging further research in this area.</p>
                <p>Last Updated: 2024-02-26 13:08:44 UTC</p>
                <button class="interpret-button" data-id="2402.16546v1">Interpret</button>
                <div id="interpretation-2402.16546v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RoboGrind: Intuitive and Interactive Surface Treatment with Industrial Robots</h3>
                <p>Authors: Benjamin AltFlorian StöcklSilvan MüllerChristopher BraunJulian RaibleSaad AlhasanOliver RettigLukas RingleDarko KaticRainer JäkelMichael BeetzMarcus StrandMarco F. Huber</p>
                <p><a href="http://arxiv.org/abs/2402.16542v2">Link to paper</a></p>
                <p>Surface treatment tasks such as grinding sanding or polishing are a vitalstep of the value chain in many industries but are notoriously challenging toautomate. We present RoboGrind an integrated system for the intuitiveinteractive automation of surface treatment tasks with industrial robots. Itcombines a sophisticated 3D perception pipeline for surface scanning andautomatic defect identification an interactive voice-controlled wizard systemfor the AI-assisted bootstrapping and parameterization of robot programs andan automatic planning and execution pipeline for force-controlled roboticsurface treatment. RoboGrind is evaluated both under laboratory and real-worldconditions in the context of refabricating fiberglass wind turbine blades.</p>
                <p>Last Updated: 2024-02-27 08:57:43 UTC</p>
                <button class="interpret-button" data-id="2402.16542v2">Interpret</button>
                <div id="interpretation-2402.16542v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Memory GAPS: Would LLM pass the Tulving Test?</h3>
                <p>Authors: Jean-Marie Chauvet</p>
                <p><a href="http://arxiv.org/abs/2402.16505v1">Link to paper</a></p>
                <p>The Tulving Test was designed to investigate memory performance inrecognition and recall tasks. Its results help assess the relevance of theSynergistic Ecphory Model of memory and similar RK paradigms in humanperformance. This paper starts investigating whether the more thanforty-year-old framework sheds some light on LLMs acts of remembering.</p>
                <p>Last Updated: 2024-02-26 11:40:51 UTC</p>
                <button class="interpret-button" data-id="2402.16505v1">Interpret</button>
                <div id="interpretation-2402.16505v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification</h3>
                <p>Authors: Ahmad SaeedHaasha Bin AtifUsman HabibMohsin Bilal</p>
                <p><a href="http://arxiv.org/abs/2402.16486v1">Link to paper</a></p>
                <p>Precise aircraft recognition in low-resolution remote sensing imagery is achallenging yet crucial task in aviation especially combat identification.This research addresses this problem with a novel scalable and AI-drivensolution. The primary hurdle in combat identification in remote sensing imageryis the accurate recognition of Novel/Unknown types of aircraft in addition toKnown types. Traditional methods human expert-driven combat identification andimage classification fall short in identifying Novel classes. Our methodologyemploys similarity learning to discern features of a broad spectrum of militaryand civilian aircraft. It discerns both Known and Novel aircraft typesleveraging metric learning for the identification and supervised few-shotlearning for aircraft type classification. To counter the challenge of limitedlow-resolution remote sensing data we propose an end-to-end framework thatadapts to the diverse and versatile process of military aircraft recognition bytraining a generalized embedder in fully supervised manner. Comparativeanalysis with earlier aircraft image classification methods shows that ourapproach is effective for aircraft image classification F1-score Aircraft Typeof 0.861 and pioneering for quantifying the identification of Novel typesF1-score Bipartitioning of 0.936. The proposed methodology effectivelyaddresses inherent challenges in remote sensing data thereby setting newstandards in dataset quality. The research opens new avenues for domain expertsand demonstrates unique capabilities in distinguishing various aircraft typescontributing to a more robust domain-adapted potential for real-time aircraftrecognition.</p>
                <p>Last Updated: 2024-02-26 11:08:26 UTC</p>
                <button class="interpret-button" data-id="2402.16486v1">Interpret</button>
                <div id="interpretation-2402.16486v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>A kernel-based analysis of Laplacian Eigenmaps</h3>
                <p>Authors: Martin Wahl</p>
                <p><a href="http://arxiv.org/abs/2402.16481v1">Link to paper</a></p>
                <p>Given i.i.d. observations uniformly distributed on a closed manifoldmathcalMsubseteq mathbbRp we study the spectral properties of theassociated empirical graph Laplacian based on a Gaussian kernel. Our mainresults are non-asymptotic error bounds showing that the eigenvalues andeigenspaces of the empirical graph Laplacian are close to the eigenvalues andeigenspaces of the Laplace-Beltrami operator of mathcalM. In our analysiswe connect the empirical graph Laplacian to kernel principal componentanalysis and consider the heat kernel of mathcalM as reproducing kernelfeature map. This leads to novel points of view and allows to leverage resultsfor empirical covariance operators in infinite dimensions.</p>
                <p>Last Updated: 2024-02-26 11:00:09 UTC</p>
                <button class="interpret-button" data-id="2402.16481v1">Interpret</button>
                <div id="interpretation-2402.16481v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Training Implicit Generative Models via an Invariant Statistical Loss</h3>
                <p>Authors: José Manuel de FrutosPablo M. OlmosManuel A. VázquezJoaquín Míguez</p>
                <p><a href="http://arxiv.org/abs/2402.16435v1">Link to paper</a></p>
                <p>Implicit generative models have the capability to learn arbitrary complexdata distributions. On the downside training requires telling apart real datafrom artificially-generated ones using adversarial discriminators leading tounstable training and mode-dropping issues. As reported by Zahee et al. 2017even in the one-dimensional 1D case training a generative adversarialnetwork GAN is challenging and often suboptimal. In this work we develop adiscriminator-free method for training one-dimensional 1D generative implicitmodels and subsequently expand this method to accommodate multivariate cases.Our loss function is a discrepancy measure between a suitably chosentransformation of the model samples and a uniform distribution hence it isinvariant with respect to the true distribution of the data. We first formulateour method for 1D random variables providing an effective solution forapproximate reparameterization of arbitrary complex distributions. Then weconsider the temporal setting both univariate and multivariate in which wemodel the conditional distribution of each sample given the history of theprocess. We demonstrate through numerical simulations that this new methodyields promising results successfully learning true distributions in a varietyof scenarios and mitigating some of the well-known problems thatstate-of-the-art implicit methods present.</p>
                <p>Last Updated: 2024-02-26 09:32:28 UTC</p>
                <button class="interpret-button" data-id="2402.16435v1">Interpret</button>
                <div id="interpretation-2402.16435v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Stable Training of Normalizing Flows for High-dimensional Variational Inference</h3>
                <p>Authors: Daniel Andrade</p>
                <p><a href="http://arxiv.org/abs/2402.16408v1">Link to paper</a></p>
                <p>Variational inference with normalizing flows NFs is an increasingly popularalternative to MCMC methods. In particular NFs based on coupling layers RealNVPs are frequently used due to their good empirical performance. In theoryincreasing the depth of normalizing flows should lead to more accurateposterior approximations. However in practice training deep normalizing flowsfor approximating high-dimensional posterior distributions is often infeasibledue to the high variance of the stochastic gradients. In this work we showthat previous methods for stabilizing the variance of stochastic gradientdescent can be insufficient to achieve stable training of Real NVPs. As thesource of the problem we identify that during training samples often exhibitunusual high values. As a remedy we propose a combination of two methods: 1soft-thresholding of the scale in Real NVPs and 2 a bijective soft logtransformation of the samples. We evaluate these and other previously proposedmodification on several challenging target distributions including ahigh-dimensional horseshoe logistic regression model. Our experiments show thatwith our modifications stable training of Real NVPs for posteriors withseveral thousand dimensions is possible allowing for more accurate marginallikelihood estimation via importance sampling. Moreover we evaluate severalcommon training techniques and architecture choices and provide practicaladvise for training NFs for high-dimensional variational inference.</p>
                <p>Last Updated: 2024-02-26 09:04:07 UTC</p>
                <button class="interpret-button" data-id="2402.16408v1">Interpret</button>
                <div id="interpretation-2402.16408v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values</h3>
                <p>Authors: Oliver HennhöferChristine Preisach</p>
                <p><a href="http://arxiv.org/abs/2402.16388v1">Link to paper</a></p>
                <p>Given the growing significance of reliable trustworthy and explainablemachine learning the requirement of uncertainty quantification for anomalydetection systems has become increasingly important. In this contexteffectively controlling Type I error rates alpha without compromising thestatistical power 1-beta of these systems can build trust and reduce costsrelated to false discoveries particularly when follow-up procedures areexpensive. Leveraging the principles of conformal prediction emerges as apromising approach for providing respective statistical guarantees bycalibrating a models uncertainty. This work introduces a novel framework foranomaly detection termed cross-conformal anomaly detection building uponwell-known cross-conformal methods designed for prediction tasks. With that itaddresses a natural research gap by extending previous works in the context ofinductive conformal anomaly detection relying on the split-conformal approachfor model calibration. Drawing on insights from conformal prediction wedemonstrate that the derived methods for calculating cross-conformal p-valuesstrike a practical compromise between statistical efficiency full-conformaland computational efficiency split-conformal for uncertainty-quantifiedanomaly detection on benchmark datasets.</p>
                <p>Last Updated: 2024-02-26 08:22:40 UTC</p>
                <button class="interpret-button" data-id="2402.16388v1">Interpret</button>
                <div id="interpretation-2402.16388v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Self Supervised Correlation-based Permutations for Multi-View Clustering</h3>
                <p>Authors: Ran EisenbergJonathan SvirskyOfir Lindenbaum</p>
                <p><a href="http://arxiv.org/abs/2402.16383v1">Link to paper</a></p>
                <p>Fusing information from different modalities can enhance data analysis tasksincluding clustering. However existing multi-view clustering MVC solutionsare limited to specific domains or rely on a suboptimal and computationallydemanding two-stage procedure of representation and clustering. We propose anend-to-end deep learning-based MVC framework for general data image tabularetc.. Our approach involves learning meaningful fused data representationswith a novel permutation-based canonical correlation objective. Concurrentlywe learn cluster assignments by identifying consistent pseudo-labels acrossmultiple views. We demonstrate the effectiveness of our model using ten MVCbenchmark datasets. Theoretically we show that our model approximates thesupervised linear discrimination analysis LDA representation. Additionallywe provide an error bound induced by false-pseudo label annotations.</p>
                <p>Last Updated: 2024-02-26 08:08:30 UTC</p>
                <button class="interpret-button" data-id="2402.16383v1">Interpret</button>
                <div id="interpretation-2402.16383v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Integrating Large Language Models with Graphical Session-Based Recommendation</h3>
                <p>Authors: Naicheng GuoHongwei ChengQianqiao LiangLinxun ChenBing Han</p>
                <p><a href="http://arxiv.org/abs/2402.16539v1">Link to paper</a></p>
                <p>With the rapid development of Large Language Models LLMs variousexplorations have arisen to utilize LLMs capability of context understanding onrecommender systems. While pioneering strategies have primarily transformedtraditional recommendation tasks into challenges of natural languagegeneration there has been a relative scarcity of exploration in the domain ofsession-based recommendation SBR due to its specificity. SBR has beenprimarily dominated by Graph Neural Networks which have achieved manysuccessful outcomes due to their ability to capture both the implicit andexplicit relationships between adjacent behaviors. The structural nature ofgraphs contrasts with the essence of natural language posing a significantadaptation gap for LLMs. In this paper we introduce large language models withgraphical Session-Based recommendation named LLMGR an effective frameworkthat bridges the aforementioned gap by harmoniously integrating LLMs with GraphNeural Networks GNNs for SBR tasks. This integration seeks to leverage thecomplementary strengths of LLMs in natural language understanding and GNNs inrelational data processing leading to a more powerful session-basedrecommender system that can understand and recommend items within a session.Moreover to endow the LLM with the capability to empower SBR tasks we designa series of prompts for both auxiliary and major instruction tuning tasks.These prompts are crafted to assist the LLM in understanding graph-structureddata and align textual information with nodes effectively translating nuanceduser interactions into a format that can be understood and utilized by LLMarchitectures. Extensive experiments on three real-world datasets demonstratethat LLMGR outperforms several competitive baselines indicating itseffectiveness in enhancing SBR tasks and its potential as a research directionfor future exploration.</p>
                <p>Last Updated: 2024-02-26 12:55:51 UTC</p>
                <button class="interpret-button" data-id="2402.16539v1">Interpret</button>
                <div id="interpretation-2402.16539v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification</h3>
                <p>Authors: Yiping SongJuhua ZhangZhiliang TianYuxin YangMinlie HuangDongsheng Li</p>
                <p><a href="http://arxiv.org/abs/2402.16515v1">Link to paper</a></p>
                <p>As sufficient data are not always publically accessible for model trainingresearchers exploit limited data with advanced learning algorithms or expandthe dataset via data augmentation DA. Conducting DA in private domainrequires private protection approaches i.e. anonymization and perturbationbut those methods cannot provide protection guarantees. Differential privacyDP learning methods theoretically bound the protection but are not skilled atgenerating pseudo text samples with large models. In this paper we transferDP-based pseudo sample generation task to DP-based generated samplesdiscrimination task where we propose a DP-based DA method with a LLM and aDP-based discriminator for text classification on private domains. We constructa knowledge distillation model as the DP-based discriminator: teacher modelsaccessing private data teaches students how to select private samples withcalibrated noise to achieve DP. To constrain the distribution of DAsgeneration we propose a DP-based tutor that models the noised privatedistribution and controls samples generation with a low privacy cost. Wetheoretically analyze our models privacy protection and empirically verify ourmodel.</p>
                <p>Last Updated: 2024-02-26 11:52:55 UTC</p>
                <button class="interpret-button" data-id="2402.16515v1">Interpret</button>
                <div id="interpretation-2402.16515v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision</h3>
                <p>Authors: Fan JiangTom DrummondTrevor Cohn</p>
                <p><a href="http://arxiv.org/abs/2402.16508v1">Link to paper</a></p>
                <p>Cross-lingual question answering CLQA is a complex problem comprisingcross-lingual retrieval from a multilingual knowledge base followed by answergeneration either in English or the query language. Both steps are usuallytackled by separate models requiring substantial annotated datasets andtypically auxiliary resources like machine translation systems to bridgebetween languages. In this paper we show that CLQA can be addressed using asingle encoder-decoder model. To effectively train this model we propose aself-supervised method based on exploiting the cross-lingual link structurewithin Wikipedia. We demonstrate how linked Wikipedia pages can be used tosynthesise supervisory signals for cross-lingual retrieval through a form ofcloze query and generate more natural queries to supervise answer generation.Together we show our approach textttCLASS outperforms comparable methodson both supervised and zero-shot language adaptation settings including thoseusing machine translation.</p>
                <p>Last Updated: 2024-02-26 11:42:29 UTC</p>
                <button class="interpret-button" data-id="2402.16508v1">Interpret</button>
                <div id="interpretation-2402.16508v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments</h3>
                <p>Authors: Junzhe ChenXuming HuShuodi LiuShiyu HuangWei-Wei TuZhaofeng HeLijie Wen</p>
                <p><a href="http://arxiv.org/abs/2402.16499v1">Link to paper</a></p>
                <p>Recent advancements in large language models LLMs have revealed theirpotential for achieving autonomous agents possessing human-level intelligence.However existing benchmarks for evaluating LLM Agents either use staticdatasets potentially leading to data leakage or focus only on single-agentscenarios overlooking the complexities of multi-agent interactions. There is alack of a benchmark that evaluates the diverse capabilities of LLM agents inmulti-agent dynamic environments. To this end we introduce LLMArena a noveland easily extensible framework for evaluating the diverse capabilities of LLMin multi-agent dynamic environments. LLMArena encompasses seven distinct gamingenvironments employing Trueskill scoring to assess crucial abilities in LLMagents including spatial reasoning strategic planning numerical reasoningrisk assessment communication opponent modeling and team collaboration. Weconduct an extensive experiment and human evaluation among different sizes andtypes of LLMs showing that LLMs still have a significant journey ahead intheir development towards becoming fully autonomous agents especially inopponent modeling and team collaboration. We hope LLMArena could guide futureresearch towards enhancing these capabilities in LLMs ultimately leading tomore sophisticated and practical applications in dynamic multi-agent settings.The code and data will be available.</p>
                <p>Last Updated: 2024-02-26 11:31:48 UTC</p>
                <button class="interpret-button" data-id="2402.16499v1">Interpret</button>
                <div id="interpretation-2402.16499v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On Languaging a Simulation Engine</h3>
                <p>Authors: Han LiuLiantang Li</p>
                <p><a href="http://arxiv.org/abs/2402.16482v1">Link to paper</a></p>
                <p>Language model intelligence is revolutionizing the way we program materialssimulations. However the diversity of simulation scenarios renders itchallenging to precisely transform human language into a tailored simulator.Here using three functionalized types of language model we propose alanguage-to-simulation Lang2Sim framework that enables interactive navigationon languaging a simulation engine by taking a scenario instance of watersorption in porous matrices. Unlike line-by-line coding of a target simulatorthe language models interpret each simulator as an assembly of invariant toolfunction and its variant input-output pair. Lang2Sim enables the precisetransform of textual description by functionalizing and sequentializing thelanguage models of respectively rationalizing the tool categorizationcustomizing its input-output combinations and distilling the simulator inputinto executable format. Importantly depending on its functionalized type eachlanguage model features a distinct processing of chat history to best balanceits memory limit and information completeness thus leveraging the modelintelligence to unstructured nature of human request. Overall this workestablishes language model as an intelligent platform to unlock the era oflanguaging a simulation engine.</p>
                <p>Last Updated: 2024-02-26 11:01:54 UTC</p>
                <button class="interpret-button" data-id="2402.16482v1">Interpret</button>
                <div id="interpretation-2402.16482v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Open Your Ears to Take a Look: A State-of-the-Art Report on the Integration of Sonification and Visualization</h3>
                <p>Authors: Kajetan EngeElias ElmquistValentina CaiolaNiklas RönnbergAlexander RindMichael IberSara LenziFangfei LanRobert HöldrichWolfgang Aigner</p>
                <p><a href="http://arxiv.org/abs/2402.16558v1">Link to paper</a></p>
                <p>The research communities studying visualization and sonification for datadisplay and analysis share exceptionally similar goals essentially making dataof any kind interpretable to humans. One community does so by using visualrepresentations of data the other community does so by employing auditorynon-speech representations of data. While the two communities have a lot incommon they developed mostly in parallel over the course of the last fewdecades. With this STAR we discuss a collection of work that bridges theborders of the two communities hence a collection of work that aims tointegrate the two techniques to one form of audiovisual display which we argueto be more than the sum of the two. We introduce and motivate aclassification system applicable to such audiovisual displays and categorize acorpus of 57 academic publications that appeared between 2011 and 2023 incategories such as reading level dataset type or evaluation system tomention a few. The corpus also enables a meta-analysis of the field includingregularly occurring design patterns such as type of visualization andsonification techniques or the use of visual and auditory channels and theanalysis of a co-author network of the field which shows individual teamswithout much interconnection. The body of work covered in this STAR alsorelates to three adjacent topics: audiovisual monitoring accessibility andaudiovisual data art. These three topics are discussed individually in additionto the systematically conducted part of this research. The findings of thisreport may be used by researchers from both fields to understand the potentialsand challenges of such integrated designs while inspiring them for futurecollaboration with experts from the respective other field.</p>
                <p>Last Updated: 2024-02-26 13:34:57 UTC</p>
                <button class="interpret-button" data-id="2402.16558v1">Interpret</button>
                <div id="interpretation-2402.16558v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Improving behavior based authentication against adversarial attack using XAI</h3>
                <p>Authors: Dong QinGeorge AmariucaiDaji QiaoYong Guan</p>
                <p><a href="http://arxiv.org/abs/2402.16430v1">Link to paper</a></p>
                <p>In recent years machine learning models especially deep neural networkshave been widely used for classification tasks in the security domain. Howeverthese models have been shown to be vulnerable to adversarial manipulation:small changes learned by an adversarial attack model when applied to theinput can cause significant changes in the output. Most research onadversarial attacks and corresponding defense methods focuses only on scenarioswhere adversarial samples are directly generated by the attack model. In thisstudy we explore a more practical scenario in behavior-based authenticationwhere adversarial samples are collected from the attacker. The generatedadversarial samples from the model are replicated by attackers with a certainlevel of discrepancy. We propose an eXplainable AI XAI based defense strategyagainst adversarial attacks in such scenarios. A feature selector trained withour method can be used as a filter in front of the original authenticator. Itfilters out features that are more vulnerable to adversarial attacks orirrelevant to authentication while retaining features that are more robust.Through comprehensive experiments we demonstrate that our XAI based defensestrategy is effective against adversarial attacks and outperforms other defensestrategies such as adversarial training and defensive distillation.</p>
                <p>Last Updated: 2024-02-26 09:29:05 UTC</p>
                <button class="interpret-button" data-id="2402.16430v1">Interpret</button>
                <div id="interpretation-2402.16430v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Human-AI Co-Creation of Worked Examples for Programming Classes</h3>
                <p>Authors: Mohammad HassanyPeter BrusilovskyJiaze KeKamil AkhuseyinogluArun Balajiee Lekshmi Narayanan</p>
                <p><a href="http://arxiv.org/abs/2402.16235v1">Link to paper</a></p>
                <p>Worked examples solutions to typical programming problems presented as asource code in a certain language and are used to explain the topics from aprogramming class are among the most popular types of learning content inprogramming classes. Most approaches and tools for presenting these examples tostudents are based on line-by-line explanations of the example code. Howeverinstructors rarely have time to provide line-by-line explanations for a largenumber of examples typically used in a programming class. In this paper weexplore and assess a human-AI collaboration approach to authoring workedexamples for Java programming. We introduce an authoring system for creatingJava worked examples that generates a starting version of code explanations andpresents it to the instructor to edit if necessary.We also present a study thatassesses the quality of explanations created with this approach</p>
                <p>Last Updated: 2024-02-26 01:44:24 UTC</p>
                <button class="interpret-button" data-id="2402.16235v1">Interpret</button>
                <div id="interpretation-2402.16235v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MoodCapture: Depression Detection Using In-the-Wild Smartphone Images</h3>
                <p>Authors: Subigya NepalArvind PillaiWeichen WangTess GriffinAmanda C. CollinsMichael HeinzDamien LekkasShayan MirjafariMatthew NemesureGeorge PriceNicholas C. JacobsonAndrew T. Campbell</p>
                <p><a href="http://dx.doi.org/10.1145/3613904.3642680">Link to paper</a></p>
                <p>MoodCapture presents a novel approach that assesses depression based onimages automatically captured from the front-facing camera of smartphones aspeople go about their daily lives. We collect over 125000 photos in the wildfrom N177 participants diagnosed with major depressive disorder for 90 days.Images are captured naturalistically while participants respond to the PHQ-8depression survey question: textitI have felt down depressed orhopeless. Our analysis explores important image attributes such as angledominant colors location objects and lighting. We show that a random foresttrained with face landmarks can classify samples as depressed or non-depressedand predict raw PHQ-8 scores effectively. Our post-hoc analysis providesseveral insights through an ablation study feature importance analysis andbias assessment. Importantly we evaluate user concerns about using MoodCaptureto detect depression based on sharing photos providing critical insights intoprivacy concerns that inform the future design of in-the-wild image-basedmental health assessment tools.</p>
                <p>Last Updated: 2024-02-25 20:08:58 UTC</p>
                <button class="interpret-button" data-id="2402.16182v1">Interpret</button>
                <div id="interpretation-2402.16182v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>InstructEdit: Instruction-based Knowledge Editing for Large Language Models</h3>
                <p>Authors: Bozhong TianSiyuan ChengXiaozhuan LiangNingyu ZhangYi HuKouying XueYanjie GouXi ChenHuajun Chen</p>
                <p><a href="http://arxiv.org/abs/2402.16123v1">Link to paper</a></p>
                <p>Knowledge editing for large language models can offer an efficient solutionto alter a models behavior without negatively impacting the overallperformance. However the current approach encounters issues with limitedgeneralizability across tasks necessitating one distinct editor for each taskwhich significantly hinders the broader applications. To address this we takethe first step to analyze the multi-task generalization issue in knowledgeediting. Specifically we develop an instruction-based editing techniquetermed InstructEdit which facilitates the editors adaptation to various taskperformances simultaneously using simple instructions. With only one unifiededitor for each LLM we empirically demonstrate that InstructEdit can improvethe editors control leading to an average 14.86 increase in Reliability inmulti-task editing setting. Furthermore experiments involving holdout unseentask illustrate that InstructEdit consistently surpass previous strongbaselines. To further investigate the underlying mechanisms ofinstruction-based knowledge editing we analyze the principal components of theediting gradient directions which unveils that instructions can help controloptimization direction with stronger OOD generalization. Code and datasets willbe available in https://github.com/zjunlp/EasyEdit.</p>
                <p>Last Updated: 2024-02-25 15:46:33 UTC</p>
                <button class="interpret-button" data-id="2402.16123v1">Interpret</button>
                <div id="interpretation-2402.16123v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Q-FOX Learning: Breaking Tradition in Reinforcement Learning</h3>
                <p>Authors: Mahmood AlqaseerYossra H. AliTarik A. Rashid</p>
                <p><a href="http://arxiv.org/abs/2402.16562v1">Link to paper</a></p>
                <p>Reinforcement learning RL is a subset of artificial intelligence AI whereagents learn the best action by interacting with the environment making itsuitable for tasks that do not require labeled data or direct supervision.Hyperparameters HP tuning refers to choosing the best parameter that leads tooptimal solutions in RL algorithms. Manual or random tuning of the HP may be acrucial process because variations in this parameter lead to changes in theoverall learning aspects and different rewards. In this paper a novel andautomatic HP-tuning method called Q-FOX is proposed. This uses both the FOXoptimizer a new optimization method inspired by nature that mimics red foxeshunting behavior and the commonly used easy-to-implement RL Q-learningalgorithm to solve the problem of HP tuning. Moreover a new objective functionis proposed which prioritizes the reward over the mean squared error MSE andlearning time steps. Q-FOX has been evaluated on two OpenAI Gym environmentcontrol tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewardsthan HP tuning with other optimizers such as PSO GA Bee or randomlyselected HP. The cumulative reward for the Cart Pole task was 32.08 and forthe Frozen Lake task was 0.95. Despite the robustness of Q-FOX it haslimitations. It cannot be used directly in real-word problems before choosingthe HP in a simulation environment because its processes work iterativelymaking it time-consuming. The results indicate that Q-FOX has played anessential role in HP tuning for RL algorithms to effectively solve differentcontrol tasks.</p>
                <p>Last Updated: 2024-02-26 13:39:04 UTC</p>
                <button class="interpret-button" data-id="2402.16562v1">Interpret</button>
                <div id="interpretation-2402.16562v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Label Learning Method Based on Tensor Projection</h3>
                <p>Authors: Jing LiQuanxue GaoQianqian WangCheng DengDeyan Xie</p>
                <p><a href="http://arxiv.org/abs/2402.16544v1">Link to paper</a></p>
                <p>Multi-view clustering method based on anchor graph has been widely concerneddue to its high efficiency and effectiveness. In order to avoidpost-processing most of the existing anchor graph-based methods learnbipartite graphs with connected components. However such methods have highrequirements on parameters and in some cases it may not be possible to obtainbipartite graphs with clear connected components. To end this we propose alabel learning method based on tensor projection LLMTP. Specifically weproject anchor graph into the label space through an orthogonal projectionmatrix to obtain cluster labels directly. Considering that the spatialstructure information of multi-view data may be ignored to a certain extentwhen projected in different views separately we extend the matrix projectiontransformation to tensor projection so that the spatial structure informationbetween views can be fully utilized. In addition we introduce the tensorSchatten p-norm regularization to make the clustering label matrices ofdifferent views as consistent as possible. Extensive experiments have provedthe effectiveness of the proposed method.</p>
                <p>Last Updated: 2024-02-26 13:03:26 UTC</p>
                <button class="interpret-button" data-id="2402.16544v1">Interpret</button>
                <div id="interpretation-2402.16544v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Model-based deep reinforcement learning for accelerated learning from flow simulations</h3>
                <p>Authors: Andre WeinerJanis Geise</p>
                <p><a href="http://arxiv.org/abs/2402.16543v1">Link to paper</a></p>
                <p>In recent years deep reinforcement learning has emerged as a technique tosolve closed-loop flow control problems. Employing simulation-basedenvironments in reinforcement learning enables a priori end-to-end optimizationof the control system provides a virtual testbed for safety-critical controlapplications and allows to gain a deep understanding of the controlmechanisms. While reinforcement learning has been applied successfully in anumber of rather simple flow control benchmarks a major bottleneck towardreal-world applications is the high computational cost and turnaround time offlow simulations. In this contribution we demonstrate the benefits ofmodel-based reinforcement learning for flow control applications. Specificallywe optimize the policy by alternating between trajectories sampled from flowsimulations and trajectories sampled from an ensemble of environment models.The model-based learning reduces the overall training time by up to 85 forthe fluidic pinball test case. Even larger savings are expected for moredemanding flow simulations.</p>
                <p>Last Updated: 2024-02-26 13:01:45 UTC</p>
                <button class="interpret-button" data-id="2402.16543v1">Interpret</button>
                <div id="interpretation-2402.16543v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Integrating Large Language Models with Graphical Session-Based Recommendation</h3>
                <p>Authors: Naicheng GuoHongwei ChengQianqiao LiangLinxun ChenBing Han</p>
                <p><a href="http://arxiv.org/abs/2402.16539v1">Link to paper</a></p>
                <p>With the rapid development of Large Language Models LLMs variousexplorations have arisen to utilize LLMs capability of context understanding onrecommender systems. While pioneering strategies have primarily transformedtraditional recommendation tasks into challenges of natural languagegeneration there has been a relative scarcity of exploration in the domain ofsession-based recommendation SBR due to its specificity. SBR has beenprimarily dominated by Graph Neural Networks which have achieved manysuccessful outcomes due to their ability to capture both the implicit andexplicit relationships between adjacent behaviors. The structural nature ofgraphs contrasts with the essence of natural language posing a significantadaptation gap for LLMs. In this paper we introduce large language models withgraphical Session-Based recommendation named LLMGR an effective frameworkthat bridges the aforementioned gap by harmoniously integrating LLMs with GraphNeural Networks GNNs for SBR tasks. This integration seeks to leverage thecomplementary strengths of LLMs in natural language understanding and GNNs inrelational data processing leading to a more powerful session-basedrecommender system that can understand and recommend items within a session.Moreover to endow the LLM with the capability to empower SBR tasks we designa series of prompts for both auxiliary and major instruction tuning tasks.These prompts are crafted to assist the LLM in understanding graph-structureddata and align textual information with nodes effectively translating nuanceduser interactions into a format that can be understood and utilized by LLMarchitectures. Extensive experiments on three real-world datasets demonstratethat LLMGR outperforms several competitive baselines indicating itseffectiveness in enhancing SBR tasks and its potential as a research directionfor future exploration.</p>
                <p>Last Updated: 2024-02-26 12:55:51 UTC</p>
                <button class="interpret-button" data-id="2402.16539v1">Interpret</button>
                <div id="interpretation-2402.16539v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning</h3>
                <p>Authors: Matteo CaldanaPaola F. AntoniettiLuca Dede'</p>
                <p><a href="http://arxiv.org/abs/2402.16517v1">Link to paper</a></p>
                <p>Finite element-based high-order solvers of conservation laws offer largeaccuracy but face challenges near discontinuities due to the Gibbs phenomenon.Artificial viscosity is a popular and effective solution to this problem basedon physical insight. In this work we present a physics-informed machinelearning algorithm to automate the discovery of artificial viscosity models ina non-supervised paradigm. The algorithm is inspired by reinforcement learningand trains a neural network acting cell-by-cell the viscosity model byminimizing a loss defined as the difference with respect to a referencesolution thanks to automatic differentiation. This enables a dataset-freetraining procedure. We prove that the algorithm is effective by integrating itinto a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcaseseveral numerical tests on scalar and vectorial problems such as Burgers andEulers equations in one and two dimensions. Results demonstrate that theproposed approach trains a model that is able to outperform classical viscositymodels. Moreover we show that the learnt artificial viscosity model is able togeneralize across different problems and parameters.</p>
                <p>Last Updated: 2024-02-26 11:58:02 UTC</p>
                <button class="interpret-button" data-id="2402.16517v1">Interpret</button>
                <div id="interpretation-2402.16517v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Enhancement of 3D Camera Synthetic Training Data with Noise Models</h3>
                <p>Authors: Katarína OsvaldováLukáš GajdošechViktor KocurMartin Madaras</p>
                <p><a href="http://dx.doi.org/10.5281/zenodo.10694437">Link to paper</a></p>
                <p>The goal of this paper is to assess the impact of noise in 3D camera-captureddata by modeling the noise of the imaging process and applying it on synthetictraining data. We compiled a dataset of specifically constructed scenes toobtain a noise model. We specifically model lateral noise affecting theposition of captured points in the image plane and axial noise affecting theposition along the axis perpendicular to the image plane. The estimated modelscan be used to emulate noise in synthetic training data. The added benefit ofadding artificial noise is evaluated in an experiment with rendered data forobject segmentation. We train a series of neural networks with varying levelsof noise in the data and measure their ability to generalize on real data. Theresults show that using too little or too much noise can hurt the networksperformance indicating that obtaining a model of noise from real scanners isbeneficial for synthetic data generation.</p>
                <p>Last Updated: 2024-02-26 11:50:42 UTC</p>
                <button class="interpret-button" data-id="2402.16514v1">Interpret</button>
                <div id="interpretation-2402.16514v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Stochastic Conditional Diffusion Models for Semantic Image Synthesis</h3>
                <p>Authors: Juyeon KoInho KongDogyun ParkHyunwoo J. Kim</p>
                <p><a href="http://arxiv.org/abs/2402.16506v2">Link to paper</a></p>
                <p>Semantic image synthesis SIS is a task to generate realistic imagescorresponding to semantic maps labels. It can be applied to diversereal-world practices such as photo editing or content creation. However inreal-world applications SIS often encounters noisy user inputs. To addressthis we propose Stochastic Conditional Diffusion Model SCDM which is arobust conditional diffusion model that features novel forward and generationprocesses tailored for SIS with noisy labels. It enhances robustness bystochastically perturbing the semantic label maps through Label Diffusionwhich diffuses the labels with discrete diffusion. Through the diffusion oflabels the noisy and clean semantic maps become similar as the timestepincreases eventually becoming identical at tT. This facilitates thegeneration of an image close to a clean image enabling robust generation.Furthermore we propose a class-wise noise schedule to differentially diffusethe labels depending on the class. We demonstrate that the proposed methodgenerates high-quality samples through extensive experiments and analyses onbenchmark datasets including a novel experimental setup simulating humanerrors during real-world applications.</p>
                <p>Last Updated: 2024-02-27 04:46:35 UTC</p>
                <button class="interpret-button" data-id="2402.16506v2">Interpret</button>
                <div id="interpretation-2402.16506v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification</h3>
                <p>Authors: Ahmad SaeedHaasha Bin AtifUsman HabibMohsin Bilal</p>
                <p><a href="http://arxiv.org/abs/2402.16486v1">Link to paper</a></p>
                <p>Precise aircraft recognition in low-resolution remote sensing imagery is achallenging yet crucial task in aviation especially combat identification.This research addresses this problem with a novel scalable and AI-drivensolution. The primary hurdle in combat identification in remote sensing imageryis the accurate recognition of Novel/Unknown types of aircraft in addition toKnown types. Traditional methods human expert-driven combat identification andimage classification fall short in identifying Novel classes. Our methodologyemploys similarity learning to discern features of a broad spectrum of militaryand civilian aircraft. It discerns both Known and Novel aircraft typesleveraging metric learning for the identification and supervised few-shotlearning for aircraft type classification. To counter the challenge of limitedlow-resolution remote sensing data we propose an end-to-end framework thatadapts to the diverse and versatile process of military aircraft recognition bytraining a generalized embedder in fully supervised manner. Comparativeanalysis with earlier aircraft image classification methods shows that ourapproach is effective for aircraft image classification F1-score Aircraft Typeof 0.861 and pioneering for quantifying the identification of Novel typesF1-score Bipartitioning of 0.936. The proposed methodology effectivelyaddresses inherent challenges in remote sensing data thereby setting newstandards in dataset quality. The research opens new avenues for domain expertsand demonstrates unique capabilities in distinguishing various aircraft typescontributing to a more robust domain-adapted potential for real-time aircraftrecognition.</p>
                <p>Last Updated: 2024-02-26 11:08:26 UTC</p>
                <button class="interpret-button" data-id="2402.16486v1">Interpret</button>
                <div id="interpretation-2402.16486v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Edge Detectors Can Make Deep Convolutional Neural Networks More Robust</h3>
                <p>Authors: Jin DingJie-Chao ZhaoYong-Zhi SunPing TanJia-Wei WangJi-En MaYou-Tong Fang</p>
                <p><a href="http://arxiv.org/abs/2402.16479v1">Link to paper</a></p>
                <p>Deep convolutional neural networks DCNN for short are vulnerable toexamples with small perturbations. Improving DCNNs robustness is of greatsignificance to the safety-critical applications such as autonomous drivingand industry automation. Inspired by the principal way that human eyesrecognize objects i.e. largely relying on the shape features this paperfirst employs the edge detectors as layer kernels and designs a binary edgefeature branch BEFB for short to learn the binary edge features which can beeasily integrated into any popular backbone. The four edge detectors can learnthe horizontal vertical positive diagonal and negative diagonal edgefeatures respectively and the branch is stacked by multiple Sobel layersusing edge detectors as kernels and one threshold layer. The binary edgefeatures learned by the branch concatenated with the texture features learnedby the backbone are fed into the fully connected layers for classification. Weintegrate the proposed branch into VGG16 and ResNet34 respectively andconduct experiments on multiple datasets. Experimental results demonstrate theBEFB is lightweight and has no side effects on training. And the accuracy ofthe BEFB integrated models is better than the original ones on all datasetswhen facing FGSM PGD and CW attacks. Besides BEFB integrated modelsequipped with the robustness enhancing techniques can achieve betterclassification accuracy compared to the original models. The work in this paperfor the first time shows it is feasible to enhance the robustness of DCNNsthrough combining both shape-like features and texture features.</p>
                <p>Last Updated: 2024-02-26 10:54:26 UTC</p>
                <button class="interpret-button" data-id="2402.16479v1">Interpret</button>
                <div id="interpretation-2402.16479v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DCVSMNet: Double Cost Volume Stereo Matching Network</h3>
                <p>Authors: Mahmoud TahmasebiSaif HuqKevin MeehanMarion McAfee</p>
                <p><a href="http://arxiv.org/abs/2402.16473v1">Link to paper</a></p>
                <p>We introduce Double Cost Volume Stereo Matching NetworkDCVSMNet which is anovel architecture characterised by by two small upper group-wise and lowernorm correlation cost volumes. Each cost volume is processed separately anda coupling module is proposed to fuse the geometry information extracted fromthe upper and lower cost volumes. DCVSMNet is a fast stereo matching networkwith a 67 ms inference time and strong generalization ability which can producecompetitive results compared to state-of-the-art methods. The results onseveral bench mark datasets show that DCVSMNet achieves better accuracy thanmethods such as CGI-Stereo and BGNet at the cost of greater inference time.</p>
                <p>Last Updated: 2024-02-26 10:42:25 UTC</p>
                <button class="interpret-button" data-id="2402.16473v1">Interpret</button>
                <div id="interpretation-2402.16473v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-02-28</p>
        </div>
    
        </div>
    </body>
    </html>
    