
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Thing2Reality: Transforming 2D Content into Conditioned Multiviews and 3D Gaussian Objects for XR Communication</h3>
                <p>Authors: Erzhen HuMingyi LiJungtaek HongXun QianAlex OlwalDavid KimSeongkook HeoRuofei Du</p>
                <p><a href="http://arxiv.org/abs/2410.07119v1">Link to paper</a></p>
                <p>During remote communication participants often share both digital andphysical content such as product designs digital assets and environments toenhance mutual understanding. Recent advances in augmented communication havefacilitated users to swiftly create and share digital 2D copies of physicalobjects from video feeds into a shared space. However conventional 2Drepresentations of digital objects restricts users ability to spatiallyreference items in a shared immersive environment. To address this we proposeThing2Reality an Extended Reality XR communication platform that enhancesspontaneous discussions of both digital and physical items during remotesessions. With Thing2Reality users can quickly materialize ideas or physicalobjects in immersive environments and share them as conditioned multiviewrenderings or 3D Gaussians. Thing2Reality enables users to interact with remoteobjects or discuss concepts in a collaborative manner. Our user study revealedthat the ability to interact with and manipulate 3D representations of objectssignificantly enhances the efficiency of discussions with the potential toaugment discussion of 2D artifacts.</p>
                <p>Last Updated: 2024-10-09 17:49:06 UTC</p>
                <button class="interpret-button" data-id="2410.07119v1">Interpret</button>
                <div id="interpretation-2410.07119v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Robots in the Middle: Evaluating LLMs in Dispute Resolution</h3>
                <p>Authors: Jinzhe TanHannes WestermannNikhil Reddy PottanigariJaromír ŠavelkaSébastien MeeùsMia GodetKarim Benyekhlef</p>
                <p><a href="http://arxiv.org/abs/2410.07053v1">Link to paper</a></p>
                <p>Mediation is a dispute resolution method featuring a neutral third-partymediator who intervenes to help the individuals resolve their dispute. Inthis paper we investigate to which extent large language models LLMs areable to act as mediators. We investigate whether LLMs are able to analyzedispute conversations select suitable intervention types and generateappropriate intervention messages. Using a novel manually created dataset of50 dispute scenarios we conduct a blind evaluation comparing LLMs with humanannotators across several key metrics. Overall the LLMs showed strongperformance even outperforming our human annotators across dimensions.Specifically in 62 of the cases the LLMs chose intervention types that wererated as better than or equivalent to those chosen by humans. Moreover in 84of the cases the intervention messages generated by the LLMs were rated asbetter than or equal to the intervention messages written by humans. LLMslikewise performed favourably on metrics such as impartiality understandingand contextualization. Our results demonstrate the potential of integrating AIin online dispute resolution ODR platforms.</p>
                <p>Last Updated: 2024-10-09 16:51:10 UTC</p>
                <button class="interpret-button" data-id="2410.07053v1">Interpret</button>
                <div id="interpretation-2410.07053v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Diamond of Thought: A Design Thinking-Based Framework for LLMs in Wearable Design</h3>
                <p>Authors: Qiyang MiaoJiang XuZhihao SongChengrui WangYu Cui</p>
                <p><a href="http://arxiv.org/abs/2410.06972v1">Link to paper</a></p>
                <p>Wearable design is an interdisciplinary field that balances technologicalinnovation human factors and human-computer interactions. Despitecontributions from various disciplines many projects lack stableinterdisciplinary teams which often leads to design failures. Large languagemodels LLMs integrate diverse information and generate innovative solutionsmaking them a valuable tool for enhancing design processes. Thus we haveexplored the use of LLMs in wearable design by combining design-thinkingprinciples with LLM capabilities. We have developed the Diamond of Thoughtframework and analysed 1603 prototypes and 1129 products from a body-centricperspective to create a comprehensive database. We employed retrieval-augmentedgeneration to input database details into the LLMs ensuring applicability towearable design challenges and integration of embodied cognition into theprocess. Our LLM-based methodology for wearables has been experimentallyvalidated demonstrating the potential of LLMs for the advancement of designpractices. This study offers new tools and methods for future wearable designs.</p>
                <p>Last Updated: 2024-10-09 15:10:00 UTC</p>
                <button class="interpret-button" data-id="2410.06972v1">Interpret</button>
                <div id="interpretation-2410.06972v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Digital Dotted Lines: Design and Evaluation of a Prototype for Digitally Signing Documents Using Identity Wallets</h3>
                <p>Authors: Yorick LastJorrit GeelsHanna Schraffenberger</p>
                <p><a href="http://dx.doi.org/10.1145/3613905.3650977">Link to paper</a></p>
                <p>Documents are largely stored and shared digitally. Yet digital documents arestill commonly signed using copies of handwritten signatures which aresensitive to fraud. Though secure cryptography-based signature solutionsexist they are hardly used due to usability issues. This paper proposes to usedigital identity wallets for securely and intuitively signing digital documentswith verified personal data. Using expert feedback we implemented this visionin an interactive prototype. The prototype was assessed in a moderatedusability test N  15 and a subsequent unmoderated remote usability test N 99. While participants generally expressed satisfaction with the system theyalso misunderstood how to interpret the signature information displayed by theprototype. Specifically signed documents were also trusted when the documentwas signed with irrelevant personal data of the signer. We conclude that suchunwarranted trust forms a threat to usable digital signatures and requiresattention by the usable security community.</p>
                <p>Last Updated: 2024-10-09 13:20:13 UTC</p>
                <button class="interpret-button" data-id="2410.06857v1">Interpret</button>
                <div id="interpretation-2410.06857v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions</h3>
                <p>Authors: Chuanjun ZhengYicheng ZhanLiang ShiOzan CakmakciKaan Akşit</p>
                <p><a href="http://arxiv.org/abs/2410.06854v1">Link to paper</a></p>
                <p>Computer-Generated Holography CGH is a set of algorithmic methods foridentifying holograms that reconstruct Three-Dimensi-onal 3D scenes inholographic displays. CGH algorithms decompose 3D scenes into multiplanes atdifferent depth levels and rely on simulations of light that propagated from asource plane to a targeted plane. Thus for n planes CGH typically optimizesholograms using n plane-to-plane light transport simulations leading to majortime and computational demands. Our work replaces multiple planes with a focalsurface and introduces a learned light transport model that could propagate alight field from a source plane to the focal surface in a single inference. Ourlearned light transport model leverages spatially adaptive convolution toachieve depth-varying propagation demanded by targeted focal surfaces. Theproposed model reduces the hologram optimization process up to 1.5x whichcontributes to hologram dataset generation and the training of future learnedCGH models.</p>
                <p>Last Updated: 2024-10-09 13:17:22 UTC</p>
                <button class="interpret-button" data-id="2410.06854v1">Interpret</button>
                <div id="interpretation-2410.06854v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy</h3>
                <p>Authors: Gian Maria CampedelliNicolò PenzoMassimo StefanRoberto DessìMarco GueriniBruno LepriJacopo Staiano</p>
                <p><a href="http://arxiv.org/abs/2410.07109v1">Link to paper</a></p>
                <p>As Large Language Model LLM-based agents become increasingly autonomous andwill more freely interact with each other studying interactions between thembecomes crucial to anticipate emergent phenomena and potential risks. Drawinginspiration from the widely popular Stanford Prison Experiment we contributeto this line of research by studying interaction patterns of LLM agents in acontext characterized by strict social hierarchy. We do so by specificallystudying two types of phenomena: persuasion and anti-social behavior insimulated scenarios involving a guard and a prisoner agent who seeks to achievea specific goal i.e. obtaining additional yard time or escape from prison.Leveraging 200 experimental scenarios for a total of 2000 machine-machineconversations across five different popular LLMs we provide a set ofnoteworthy findings. We first document how some models consistently fail incarrying out a conversation in our multi-agent setup where power dynamics areat play. Then for the models that were able to engage in successfulinteractions we empirically show how the goal that an agent is set to achieveimpacts primarily its persuasiveness while having a negligible effect withrespect to the agents anti-social behavior. Third we highlight how agentspersonas and particularly the guards personality drive both the likelihoodof successful persuasion from the prisoner and the emergence of anti-socialbehaviors. Fourth we show that even without explicitly prompting for specificpersonalities anti-social behavior emerges by simply assigning agents roles.These results bear implications for the development of interactive LLM agentsas well as the debate on their societal impact.</p>
                <p>Last Updated: 2024-10-09 17:45:47 UTC</p>
                <button class="interpret-button" data-id="2410.07109v1">Interpret</button>
                <div id="interpretation-2410.07109v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders</h3>
                <p>Authors: Cheng LiMay FungQingyun WangChi HanManling LiJindong WangHeng Ji</p>
                <p><a href="http://arxiv.org/abs/2410.06845v1">Link to paper</a></p>
                <p>Mental health disorders are one of the most serious diseases in the world.Most people with such a disease lack access to adequate care which highlightsthe importance of training models for the diagnosis and treatment of mentalhealth disorders. However in the mental health domain privacy concerns limitthe accessibility of personalized treatment data making it challenging tobuild powerful models. In this paper we introduce MentalArena a self-playframework to train language models by generating domain-specific personalizeddata where we obtain a better model capable of making a personalized diagnosisand treatment as a therapist and providing information as a patient. Toaccurately model human-like mental health patients we devise Symptom Encoderwhich simulates a real patient from both cognition and behavior perspectives.To address intent bias during patient-therapist interactions we proposeSymptom Decoder to compare diagnosed symptoms with encoded symptoms anddynamically manage the dialogue between patient and therapist according to theidentified deviations. We evaluated MentalArena against 6 benchmarks includingbiomedicalQA and mental health tasks compared to 6 advanced models. Ourmodels fine-tuned on both GPT-3.5 and Llama-3-8b significantly outperformtheir counterparts including GPT-4o. We hope that our work can inspire futureresearch on personalized care. Code is available inhttps://github.com/Scarelette/MentalArena/tree/main</p>
                <p>Last Updated: 2024-10-09 13:06:40 UTC</p>
                <button class="interpret-button" data-id="2410.06845v1">Interpret</button>
                <div id="interpretation-2410.06845v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning</h3>
                <p>Authors: Hao MaTianyi HuZhiqiang PuBoyin LiuXiaolin AiYanyan LiangMin Chen</p>
                <p><a href="http://arxiv.org/abs/2410.06101v1">Link to paper</a></p>
                <p>Reinforcement learning RL has emerged as a pivotal technique forfine-tuning large language models LLMs on specific tasks. However prevailingRL fine-tuning methods predominantly rely on PPO and its variants. Though thesealgorithms are effective in general RL settings they often exhibit suboptimalperformance and vulnerability to distribution collapse when applied to thefine-tuning of LLMs. In this paper we propose CORY extending the RLfine-tuning of LLMs to a sequential cooperative multi-agent reinforcementlearning framework to leverage the inherent coevolution and emergentcapabilities of multi-agent systems. In CORY the LLM to be fine-tuned isinitially duplicated into two autonomous agents: a pioneer and an observer. Thepioneer generates responses based on queries while the observer generatesresponses using both the queries and the pioneers responses. The two agentsare trained together. During training the agents exchange roles periodicallyfostering cooperation and coevolution between them. Experiments evaluate CORYsperformance by fine-tuning GPT-2 and Llama-2 under subjective and objectivereward functions on the IMDB Review and GSM8K datasets respectively. Resultsshow that CORY outperforms PPO in terms of policy optimality resistance todistribution collapse and training robustness thereby underscoring itspotential as a superior methodology for refining LLMs in real-worldapplications.</p>
                <p>Last Updated: 2024-10-08 14:55:26 UTC</p>
                <button class="interpret-button" data-id="2410.06101v1">Interpret</button>
                <div id="interpretation-2410.06101v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Concurrent-Learning Based Relative Localization in Shape Formation of Robot Swarms</h3>
                <p>Authors: Jinhu LüKunrui ZeShuoyu YueKexin LiuWei WangGuibin Sun</p>
                <p><a href="http://arxiv.org/abs/2410.06052v1">Link to paper</a></p>
                <p>In this paper we address the shape formation problem for massive robotswarms in environments where external localization systems are unavailable.Achieving this task effectively with solely onboard measurements is stillscarcely explored and faces some practical challenges. To solve thischallenging problem we propose the following novel results. Firstly toestimate the relative positions among neighboring robots a concurrent-learningbased estimator is proposed. It relaxes the persistent excitation conditionrequired in the classical ones such as least-square estimator. Secondly weintroduce a finite-time agreement protocol to determine the shape location.This is achieved by estimating the relative position between each robot and arandomly assigned seed robot. The initial position of the seed one marks theshape location. Thirdly based on the theoretical results of the relativelocalization a novel behavior-based control strategy is devised. This strategynot only enables adaptive shape formation of large group of robots but alsoenhances the observability of inter-robot relative localization. Numericalsimulation results are provided to verify the performance of our proposedstrategy compared to the state-of-the-art ones. Additionally outdoorexperiments on real robots further demonstrate the practical effectiveness androbustness of our methods.</p>
                <p>Last Updated: 2024-10-08 13:54:04 UTC</p>
                <button class="interpret-button" data-id="2410.06052v1">Interpret</button>
                <div id="interpretation-2410.06052v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Online Dynamic Pricing for Electric Vehicle Charging Stations with Reservations</h3>
                <p>Authors: Jan MrkosAntonín KomendaDavid FiedlerJiří Vokřínek</p>
                <p><a href="http://arxiv.org/abs/2410.05538v1">Link to paper</a></p>
                <p>The transition to electric vehicles EVs coupled with the rise of renewableenergy sources will significantly impact the electric grid. Unlikeconventional fuel sources electricity for EVs is constrained by grid capacityprice fluctuations and long EV charging times requiring new pricing solutionsto manage demand and supply. This paper proposes a model for online dynamicpricing of reserved EV charging services including reservation parking andcharging as a bundled service priced as a whole. Our approach focuses on theindividual charging station operator employing a stochastic demand model andonline dynamic pricing based on expected demand. The proposed model uses aMarkov Decision Process MDP formulation to optimize sequential pricingdecisions for charging session requests. A key contribution is the noveldefinition and quantification of discretization error introduced by thediscretization of the Poisson process for use in the MDP. The models viabilityis demonstrated with a heuristic solution method based on Monte-Carlo treesearch offering a viable path for real-world application.</p>
                <p>Last Updated: 2024-10-07 22:36:40 UTC</p>
                <button class="interpret-button" data-id="2410.05538v1">Interpret</button>
                <div id="interpretation-2410.05538v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation</h3>
                <p>Authors: Fabian PaischerLukas HauzenbergerThomas SchmiedBenedikt AlkinMarc Peter DeisenrothSepp Hochreiter</p>
                <p><a href="http://arxiv.org/abs/2410.07170v1">Link to paper</a></p>
                <p>Foundation models FMs are pre-trained on large-scale datasets and thenfine-tuned on a downstream task for a specific application. The most successfuland most commonly used fine-tuning method is to update the pre-trained weightsvia a low-rank adaptation LoRA. LoRA introduces new weight matrices that areusually initialized at random with a uniform rank distribution across modelweights. Recent works focus on weight-driven initialization or learning ofadaptive ranks during training. Both approaches have only been investigated inisolation resulting in slow convergence or a uniform rank distribution inturn leading to sub-optimal performance. We propose to enhance LoRA byinitializing the new weights in a data-driven manner by computing singularvalue decomposition on minibatches of activation vectors. Then we initializethe LoRA matrices with the obtained right-singular vectors and re-distributeranks among all weight matrices to explain the maximal amount of variance andcontinue the standard LoRA fine-tuning procedure. This results in our newmethod Explained Variance Adaptation EVA. We apply EVA to a variety offine-tuning tasks ranging from language generation and understanding to imageclassification and reinforcement learning. EVA exhibits faster convergence thancompetitors and attains the highest average score across a multitude of tasksper domain.</p>
                <p>Last Updated: 2024-10-09 17:59:06 UTC</p>
                <button class="interpret-button" data-id="2410.07170v1">Interpret</button>
                <div id="interpretation-2410.07170v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Collusion Detection with Graph Neural Networks</h3>
                <p>Authors: Lucas GomesJannis KueckMara MattesMartin SpindlerAlexey Zaytsev</p>
                <p><a href="http://arxiv.org/abs/2410.07091v1">Link to paper</a></p>
                <p>Collusion is a complex phenomenon in which companies secretly collaborate toengage in fraudulent practices. This paper presents an innovative methodologyfor detecting and predicting collusion patterns in different national marketsusing neural networks NNs and graph neural networks GNNs. GNNs areparticularly well suited to this task because they can exploit the inherentnetwork structures present in collusion and many other economic problems. Ourapproach consists of two phases: In Phase I we develop and train models onindividual market datasets from Japan the United States two regions inSwitzerland Italy and Brazil focusing on predicting collusion in singlemarkets. In Phase II we extend the models applicability through zero-shotlearning employing a transfer learning approach that can detect collusion inmarkets in which training data is unavailable. This phase also incorporatesout-of-distribution OOD generalization to evaluate the models performance onunseen datasets from other countries and regions. In our empirical study weshow that GNNs outperform NNs in detecting complex collusive patterns. Thisresearch contributes to the ongoing discourse on preventing collusion andoptimizing detection methodologies providing valuable guidance on the use ofNNs and GNNs in economic applications to enhance market fairness and economicwelfare.</p>
                <p>Last Updated: 2024-10-09 17:31:41 UTC</p>
                <button class="interpret-button" data-id="2410.07091v1">Interpret</button>
                <div id="interpretation-2410.07091v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Do Contemporary CATE Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark</h3>
                <p>Authors: Haining YuYizhou Sun</p>
                <p><a href="http://arxiv.org/abs/2410.07021v1">Link to paper</a></p>
                <p>We present unexpected findings from a large-scale benchmark study evaluatingConditional Average Treatment Effect CATE estimation algorithms. By running16 modern CATE models across 43200 datasets we find that: a 62 of CATEestimates have a higher Mean Squared Error MSE than a trivial zero-effectpredictor rendering them ineffective b in datasets with at least one usefulCATE estimate 80 still have higher MSE than a constant-effect model and cOrthogonality-based models outperform other models only 30 of the timedespite widespread optimism about their performance. These findings exposesignificant limitations in current CATE models and suggest ample opportunitiesfor further research.  Our findings stem from a novel application of textitobservationalsampling originally developed to evaluate Average Treatment Effect ATEestimates from observational methods with experiment data. To adaptobservational sampling for CATE evaluation we introduce a statisticalparameter Q equal to MSE minus a constant and preserves the ranking ofmodels by their MSE. We then derive a family of sample statistics collectivelycalled hatQ that can be computed from real-world data. We prove thathatQ is a consistent estimator of Q under mild technical conditions.When used in observational sampling hatQ is unbiased and asymptoticallyselects the model with the smallest MSE. To ensure the benchmark reflectsreal-world heterogeneity we handpick datasets where outcomes come from fieldrather than simulation. By combining the new observational sampling method newstatistics and real-world datasets the benchmark provides a uniqueperspective on CATE estimator performance and uncover gaps in capturingreal-world heterogeneity.</p>
                <p>Last Updated: 2024-10-09 16:04:40 UTC</p>
                <button class="interpret-button" data-id="2410.07021v1">Interpret</button>
                <div id="interpretation-2410.07021v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Optimizing Estimators of Squared Calibration Errors in Classification</h3>
                <p>Authors: Sebastian G. GruberFrancis Bach</p>
                <p><a href="http://arxiv.org/abs/2410.07014v1">Link to paper</a></p>
                <p>In this work we propose a mean-squared error-based risk that enables thecomparison and optimization of estimators of squared calibration errors inpractical settings. Improving the calibration of classifiers is crucial forenhancing the trustworthiness and interpretability of machine learning modelsespecially in sensitive decision-making scenarios. Although various calibrationerror estimators exist in the current literature there is a lack of guidanceon selecting the appropriate estimator and tuning its hyperparameters. Byleveraging the bilinear structure of squared calibration errors we reformulatecalibration estimation as a regression problem with independent and identicallydistributed i.i.d. input pairs. This reformulation allows us to quantify theperformance of different estimators even for the most challenging calibrationcriterion known as canonical calibration. Our approach advocates for atraining-validation-testing pipeline when estimating a calibration error on anevaluation dataset. We demonstrate the effectiveness of our pipeline byoptimizing existing calibration estimators and comparing them with novel kernelridge regression-based estimators on standard image classification tasks.</p>
                <p>Last Updated: 2024-10-09 15:58:06 UTC</p>
                <button class="interpret-button" data-id="2410.07014v1">Interpret</button>
                <div id="interpretation-2410.07014v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax</h3>
                <p>Authors: Ivan ButakovAlexander SememenkoAlexander TolmachevAndrey GladkovMarina MunkhoevaAlexey Frolov</p>
                <p><a href="http://arxiv.org/abs/2410.06993v1">Link to paper</a></p>
                <p>Deep InfoMax DIM is a well-established method for self-supervisedrepresentation learning SSRL based on maximization of the mutual informationbetween the input and the output of a deep neural network encoder. Despite theDIM and contrastive SSRL in general being well-explored the task of learningrepresentations conforming to a specific distribution i.e. distributionmatching DM is still under-addressed. Motivated by the importance of DM toseveral downstream tasks including generative modeling disentanglementoutliers detection and other we enhance DIM to enable automatic matching oflearned representations to a selected prior distribution. To achieve this wepropose injecting an independent noise into the normalized outputs of theencoder while keeping the same InfoMax training objective. We show that suchmodification allows for learning uniformly and normally distributedrepresentations as well as representations of other absolutely continuousdistributions. Our approach is tested on various downstream tasks. The resultsindicate a moderate trade-off between the performance on the downstream tasksand quality of DM.</p>
                <p>Last Updated: 2024-10-09 15:40:04 UTC</p>
                <button class="interpret-button" data-id="2410.06993v1">Interpret</button>
                <div id="interpretation-2410.06993v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>MM-Ego: Towards Building Egocentric Multimodal LLMs</h3>
                <p>Authors: Hanrong YeHaotian ZhangErik DaxbergerLin ChenZongyu LinYanghao LiBowen ZhangHaoxuan YouDan XuZhe GanJiasen LuYinfei Yang</p>
                <p><a href="http://arxiv.org/abs/2410.07177v1">Link to paper</a></p>
                <p>This research aims to comprehensively explore building a multimodalfoundation model for egocentric video understanding. To achieve this goal wework on three fronts. First as there is a lack of QA data for egocentric videounderstanding we develop a data engine that efficiently generates 7Mhigh-quality QA samples for egocentric videos ranging from 30 seconds to onehour long based on human-annotated data. This is currently the largestegocentric QA dataset. Second we contribute a challenging egocentric QAbenchmark with 629 videos and 7026 questions to evaluate the models abilityin recognizing and memorizing visual details across videos of varying lengths.We introduce a new de-biasing evaluation method to help mitigate theunavoidable language bias present in the models being evaluated. Third wepropose a specialized multimodal architecture featuring a novel Memory PointerPrompting mechanism. This design includes a global glimpse step to gain anoverarching understanding of the entire video and identify key visualinformation followed by a fallback step that utilizes the key visualinformation to generate responses. This enables the model to more effectivelycomprehend extended video content. With the data benchmark and model wesuccessfully build MM-Ego an egocentric multimodal LLM that shows powerfulperformance on egocentric video understanding.</p>
                <p>Last Updated: 2024-10-09 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2410.07177v1">Interpret</button>
                <div id="interpretation-2410.07177v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Do better language models have crisper vision?</h3>
                <p>Authors: Jona RuthardtGertjan J. BurghoutsSerge BelongieYuki M. Asano</p>
                <p><a href="http://arxiv.org/abs/2410.07173v1">Link to paper</a></p>
                <p>How well do text-only Large Language Models LLMs grasp the visual world AsLLMs are increasingly used in computer vision addressing this question becomesboth fundamental and pertinent. However existing studies have primarilyfocused on limited scenarios such as their ability to generate visual contentor cluster multimodal data. To this end we propose the Visual TextRepresentation Benchmark ViTeRB to isolate key properties that make languagemodels well-aligned with the visual world. With this we identify large-scaledecoder-based LLMs as ideal candidates for representing text in vision-centriccontexts counter to the current practice of utilizing text encoders. Buildingon these findings we propose ShareLock an ultra-lightweight CLIP-like model.By leveraging precomputable frozen features from strong vision and languagemodels ShareLock achieves an impressive 51 accuracy on ImageNet despiteutilizing just 563k image-caption pairs. Moreover training requires only 1 GPUhour or 10 hours including the precomputation of features - orders ofmagnitude less than prior methods. Code will be released.</p>
                <p>Last Updated: 2024-10-09 17:59:33 UTC</p>
                <button class="interpret-button" data-id="2410.07173v1">Interpret</button>
                <div id="interpretation-2410.07173v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation</h3>
                <p>Authors: Xinchen ZhangLing YangGuohao LiYaqi CaiJiake XieYong TangYujiu YangMengdi WangBin Cui</p>
                <p><a href="http://arxiv.org/abs/2410.07171v1">Link to paper</a></p>
                <p>Advanced diffusion models like RPG Stable Diffusion 3 and FLUX have madenotable strides in compositional text-to-image generation. However thesemethods typically exhibit distinct strengths for compositional generation withsome excelling in handling attribute binding and others in spatialrelationships. This disparity highlights the need for an approach that canleverage the complementary strengths of various models to comprehensivelyimprove the composition capability. To this end we introduce IterComp a novelframework that aggregates composition-aware model preferences from multiplemodels and employs an iterative feedback learning approach to enhancecompositional generation. Specifically we curate a gallery of six powerfulopen-source diffusion models and evaluate their three key compositionalmetrics: attribute binding spatial relationships and non-spatialrelationships. Based on these metrics we develop a composition-aware modelpreference dataset comprising numerous image-rank pairs to traincomposition-aware reward models. Then we propose an iterative feedbacklearning method to enhance compositionality in a closed-loop manner enablingthe progressive self-refinement of both the base diffusion model and rewardmodels over multiple iterations. Theoretical proof demonstrates theeffectiveness and extensive experiments show our significant superiority overprevious SOTA methods e.g. Omost and FLUX particularly in multi-categoryobject composition and complex semantic alignment. IterComp opens new researchavenues in reward feedback learning for diffusion models and compositionalgeneration. Code: https://github.com/YangLing0818/IterComp</p>
                <p>Last Updated: 2024-10-09 17:59:13 UTC</p>
                <button class="interpret-button" data-id="2410.07171v1">Interpret</button>
                <div id="interpretation-2410.07171v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate</h3>
                <p>Authors: Qidong HuangXiaoyi DongPan ZhangYuhang ZangYuhang CaoJiaqi WangDahua LinWeiming ZhangNenghai Yu</p>
                <p><a href="http://arxiv.org/abs/2410.07167v1">Link to paper</a></p>
                <p>We present the Modality Integration Rate MIR an effective robust andgeneralized metric to indicate the multi-modal pre-training quality of LargeVision Language Models LVLMs. Large-scale pre-training plays a critical rolein building capable LVLMs while evaluating its training quality without thecostly supervised fine-tuning stage is under-explored. Loss perplexity andin-context evaluation results are commonly used pre-training metrics for LargeLanguage Models LLMs while we observed that these metrics are lessindicative when aligning a well-trained LLM with a new modality. Due to thelack of proper metrics the research of LVLMs in the critical pre-trainingstage is hindered greatly including the training data choice efficient moduledesign etc. In this paper we propose evaluating the pre-training quality fromthe inter-modal distribution distance perspective and present MIR the ModalityIntegration Rate which is 1 textbfEffective to represent the pre-trainingquality and show a positive relation with the benchmark performance aftersupervised fine-tuning. 2 textbfRobust toward different training/evaluationdata. 3 textbfGeneralize across training configurations and architecturechoices. We conduct a series of pre-training experiments to explore theeffectiveness of MIR and observe satisfactory results that MIR is indicativeabout training data selection training strategy schedule and modelarchitecture design to get better pre-training results. We hope MIR could be ahelpful metric for building capable LVLMs and inspire the following researchabout modality alignment in different areas. Our code is at:https://github.com/shikiw/Modality-Integration-Rate.</p>
                <p>Last Updated: 2024-10-09 17:59:04 UTC</p>
                <button class="interpret-button" data-id="2410.07167v1">Interpret</button>
                <div id="interpretation-2410.07167v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation</h3>
                <p>Authors: Yukang CaoLiang PanKai HanKwan-Yee K. WongZiwei Liu</p>
                <p><a href="http://arxiv.org/abs/2410.07164v1">Link to paper</a></p>
                <p>Recent advancements in diffusion models have led to significant improvementsin the generation and animation of 4D full-body human-object interactionsHOI. Nevertheless existing methods primarily focus on SMPL-based motiongeneration which is limited by the scarcity of realistic large-scaleinteraction data. This constraint affects their ability to create everyday HOIscenes. This paper addresses this challenge using a zero-shot approach with apre-trained diffusion model. Despite this potential achieving our goals isdifficult due to the diffusion models lack of understanding of where andhow objects interact with the human body. To tackle these issues weintroduce AvatarGO a novel framework designed to generate animatable 4D HOIscenes directly from textual inputs. Specifically 1 for the wherechallenge we propose LLM-guided contact retargeting which employs Lang-SAM toidentify the contact body part from text prompts ensuring preciserepresentation of human-object spatial relations. 2 For the how challengewe introduce correspondence-aware motion optimization that constructs motionfields for both human and object models using the linear blend skinningfunction from SMPL-X. Our framework not only generates coherent compositionalmotions but also exhibits greater robustness in handling penetration issues.Extensive experiments with existing methods validate AvatarGOs superiorgeneration and animation capabilities on a variety of human-object pairs anddiverse poses. As the first attempt to synthesize 4D avatars with objectinteractions we hope AvatarGO could open new doors for human-centric 4Dcontent creation.</p>
                <p>Last Updated: 2024-10-09 17:58:56 UTC</p>
                <button class="interpret-button" data-id="2410.07164v1">Interpret</button>
                <div id="interpretation-2410.07164v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>MM-Ego: Towards Building Egocentric Multimodal LLMs</h3>
                <p>Authors: Hanrong YeHaotian ZhangErik DaxbergerLin ChenZongyu LinYanghao LiBowen ZhangHaoxuan YouDan XuZhe GanJiasen LuYinfei Yang</p>
                <p><a href="http://arxiv.org/abs/2410.07177v1">Link to paper</a></p>
                <p>This research aims to comprehensively explore building a multimodalfoundation model for egocentric video understanding. To achieve this goal wework on three fronts. First as there is a lack of QA data for egocentric videounderstanding we develop a data engine that efficiently generates 7Mhigh-quality QA samples for egocentric videos ranging from 30 seconds to onehour long based on human-annotated data. This is currently the largestegocentric QA dataset. Second we contribute a challenging egocentric QAbenchmark with 629 videos and 7026 questions to evaluate the models abilityin recognizing and memorizing visual details across videos of varying lengths.We introduce a new de-biasing evaluation method to help mitigate theunavoidable language bias present in the models being evaluated. Third wepropose a specialized multimodal architecture featuring a novel Memory PointerPrompting mechanism. This design includes a global glimpse step to gain anoverarching understanding of the entire video and identify key visualinformation followed by a fallback step that utilizes the key visualinformation to generate responses. This enables the model to more effectivelycomprehend extended video content. With the data benchmark and model wesuccessfully build MM-Ego an egocentric multimodal LLM that shows powerfulperformance on egocentric video understanding.</p>
                <p>Last Updated: 2024-10-09 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2410.07177v1">Interpret</button>
                <div id="interpretation-2410.07177v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</h3>
                <p>Authors: Fei WangXingchen WanRuoxi SunJiefeng ChenSercan Ö. Arık</p>
                <p><a href="http://arxiv.org/abs/2410.07176v1">Link to paper</a></p>
                <p>Retrieval-Augmented Generation RAG while effective in integrating externalknowledge to address the limitations of large language models LLMs can beundermined by imperfect retrieval which may introduce irrelevant misleadingor even malicious information. Despite its importance previous studies haverarely explored the behavior of RAG through joint analysis on how errors fromimperfect retrieval attribute and propagate and how potential conflicts arisebetween the LLMs internal knowledge and external sources. We find thatimperfect retrieval augmentation might be inevitable and quite harmful throughcontrolled analysis under realistic conditions. We identify the knowledgeconflicts between LLM-internal and external knowledge from retrieval as abottleneck to overcome in the post-retrieval stage of RAG. To render LLMsresilient to imperfect retrieval we propose Astute RAG a novel RAG approachthat adaptively elicits essential information from LLMs internal knowledgeiteratively consolidates internal and external knowledge with source-awarenessand finalizes the answer according to information reliability. Our experimentsusing Gemini and Claude demonstrate that Astute RAG significantly outperformsprevious robustness-enhanced RAG methods. Notably Astute RAG is the onlyapproach that matches or exceeds the performance of LLMs without RAG underworst-case scenarios. Further analysis reveals that Astute RAG effectivelyresolves knowledge conflicts improving the reliability and trustworthiness ofRAG systems.</p>
                <p>Last Updated: 2024-10-09 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2410.07176v1">Interpret</button>
                <div id="interpretation-2410.07176v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Neural Circuit Architectural Priors for Quadruped Locomotion</h3>
                <p>Authors: Nikhil X. BhattasaliVenkatesh PattabiramanLerrel PintoGrace W. Lindsay</p>
                <p><a href="http://arxiv.org/abs/2410.07174v1">Link to paper</a></p>
                <p>Learning-based approaches to quadruped locomotion commonly adopt genericpolicy architectures like fully connected MLPs. As such architectures containfew inductive biases it is common in practice to incorporate priors in theform of rewards training curricula imitation data or trajectory generators.In nature animals are born with priors in the form of their nervous systemsarchitecture which has been shaped by evolution to confer innate ability andefficient learning. For instance a horse can walk within hours of birth andcan quickly improve with practice. Such architectural priors can also be usefulin ANN architectures for AI. In this work we explore the advantages of abiologically inspired ANN architecture for quadruped locomotion based on neuralcircuits in the limbs and spinal cord of mammals. Our architecture achievesgood initial performance and comparable final performance to MLPs while usingless data and orders of magnitude fewer parameters. Our architecture alsoexhibits better generalization to task variations even admitting deployment ona physical robot without standard sim-to-real methods. This work shows thatneural circuits can provide valuable architectural priors for locomotion andencourages future work in other sensorimotor skills.</p>
                <p>Last Updated: 2024-10-09 17:59:45 UTC</p>
                <button class="interpret-button" data-id="2410.07174v1">Interpret</button>
                <div id="interpretation-2410.07174v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Do better language models have crisper vision?</h3>
                <p>Authors: Jona RuthardtGertjan J. BurghoutsSerge BelongieYuki M. Asano</p>
                <p><a href="http://arxiv.org/abs/2410.07173v1">Link to paper</a></p>
                <p>How well do text-only Large Language Models LLMs grasp the visual world AsLLMs are increasingly used in computer vision addressing this question becomesboth fundamental and pertinent. However existing studies have primarilyfocused on limited scenarios such as their ability to generate visual contentor cluster multimodal data. To this end we propose the Visual TextRepresentation Benchmark ViTeRB to isolate key properties that make languagemodels well-aligned with the visual world. With this we identify large-scaledecoder-based LLMs as ideal candidates for representing text in vision-centriccontexts counter to the current practice of utilizing text encoders. Buildingon these findings we propose ShareLock an ultra-lightweight CLIP-like model.By leveraging precomputable frozen features from strong vision and languagemodels ShareLock achieves an impressive 51 accuracy on ImageNet despiteutilizing just 563k image-caption pairs. Moreover training requires only 1 GPUhour or 10 hours including the precomputation of features - orders ofmagnitude less than prior methods. Code will be released.</p>
                <p>Last Updated: 2024-10-09 17:59:33 UTC</p>
                <button class="interpret-button" data-id="2410.07173v1">Interpret</button>
                <div id="interpretation-2410.07173v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation</h3>
                <p>Authors: Fabian PaischerLukas HauzenbergerThomas SchmiedBenedikt AlkinMarc Peter DeisenrothSepp Hochreiter</p>
                <p><a href="http://arxiv.org/abs/2410.07170v1">Link to paper</a></p>
                <p>Foundation models FMs are pre-trained on large-scale datasets and thenfine-tuned on a downstream task for a specific application. The most successfuland most commonly used fine-tuning method is to update the pre-trained weightsvia a low-rank adaptation LoRA. LoRA introduces new weight matrices that areusually initialized at random with a uniform rank distribution across modelweights. Recent works focus on weight-driven initialization or learning ofadaptive ranks during training. Both approaches have only been investigated inisolation resulting in slow convergence or a uniform rank distribution inturn leading to sub-optimal performance. We propose to enhance LoRA byinitializing the new weights in a data-driven manner by computing singularvalue decomposition on minibatches of activation vectors. Then we initializethe LoRA matrices with the obtained right-singular vectors and re-distributeranks among all weight matrices to explain the maximal amount of variance andcontinue the standard LoRA fine-tuning procedure. This results in our newmethod Explained Variance Adaptation EVA. We apply EVA to a variety offine-tuning tasks ranging from language generation and understanding to imageclassification and reinforcement learning. EVA exhibits faster convergence thancompetitors and attains the highest average score across a multitude of tasksper domain.</p>
                <p>Last Updated: 2024-10-09 17:59:06 UTC</p>
                <button class="interpret-button" data-id="2410.07170v1">Interpret</button>
                <div id="interpretation-2410.07170v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</h3>
                <p>Authors: Fei WangXingchen WanRuoxi SunJiefeng ChenSercan Ö. Arık</p>
                <p><a href="http://arxiv.org/abs/2410.07176v1">Link to paper</a></p>
                <p>Retrieval-Augmented Generation RAG while effective in integrating externalknowledge to address the limitations of large language models LLMs can beundermined by imperfect retrieval which may introduce irrelevant misleadingor even malicious information. Despite its importance previous studies haverarely explored the behavior of RAG through joint analysis on how errors fromimperfect retrieval attribute and propagate and how potential conflicts arisebetween the LLMs internal knowledge and external sources. We find thatimperfect retrieval augmentation might be inevitable and quite harmful throughcontrolled analysis under realistic conditions. We identify the knowledgeconflicts between LLM-internal and external knowledge from retrieval as abottleneck to overcome in the post-retrieval stage of RAG. To render LLMsresilient to imperfect retrieval we propose Astute RAG a novel RAG approachthat adaptively elicits essential information from LLMs internal knowledgeiteratively consolidates internal and external knowledge with source-awarenessand finalizes the answer according to information reliability. Our experimentsusing Gemini and Claude demonstrate that Astute RAG significantly outperformsprevious robustness-enhanced RAG methods. Notably Astute RAG is the onlyapproach that matches or exceeds the performance of LLMs without RAG underworst-case scenarios. Further analysis reveals that Astute RAG effectivelyresolves knowledge conflicts improving the reliability and trustworthiness ofRAG systems.</p>
                <p>Last Updated: 2024-10-09 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2410.07176v1">Interpret</button>
                <div id="interpretation-2410.07176v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Do better language models have crisper vision?</h3>
                <p>Authors: Jona RuthardtGertjan J. BurghoutsSerge BelongieYuki M. Asano</p>
                <p><a href="http://arxiv.org/abs/2410.07173v1">Link to paper</a></p>
                <p>How well do text-only Large Language Models LLMs grasp the visual world AsLLMs are increasingly used in computer vision addressing this question becomesboth fundamental and pertinent. However existing studies have primarilyfocused on limited scenarios such as their ability to generate visual contentor cluster multimodal data. To this end we propose the Visual TextRepresentation Benchmark ViTeRB to isolate key properties that make languagemodels well-aligned with the visual world. With this we identify large-scaledecoder-based LLMs as ideal candidates for representing text in vision-centriccontexts counter to the current practice of utilizing text encoders. Buildingon these findings we propose ShareLock an ultra-lightweight CLIP-like model.By leveraging precomputable frozen features from strong vision and languagemodels ShareLock achieves an impressive 51 accuracy on ImageNet despiteutilizing just 563k image-caption pairs. Moreover training requires only 1 GPUhour or 10 hours including the precomputation of features - orders ofmagnitude less than prior methods. Code will be released.</p>
                <p>Last Updated: 2024-10-09 17:59:33 UTC</p>
                <button class="interpret-button" data-id="2410.07173v1">Interpret</button>
                <div id="interpretation-2410.07173v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation</h3>
                <p>Authors: Fabian PaischerLukas HauzenbergerThomas SchmiedBenedikt AlkinMarc Peter DeisenrothSepp Hochreiter</p>
                <p><a href="http://arxiv.org/abs/2410.07170v1">Link to paper</a></p>
                <p>Foundation models FMs are pre-trained on large-scale datasets and thenfine-tuned on a downstream task for a specific application. The most successfuland most commonly used fine-tuning method is to update the pre-trained weightsvia a low-rank adaptation LoRA. LoRA introduces new weight matrices that areusually initialized at random with a uniform rank distribution across modelweights. Recent works focus on weight-driven initialization or learning ofadaptive ranks during training. Both approaches have only been investigated inisolation resulting in slow convergence or a uniform rank distribution inturn leading to sub-optimal performance. We propose to enhance LoRA byinitializing the new weights in a data-driven manner by computing singularvalue decomposition on minibatches of activation vectors. Then we initializethe LoRA matrices with the obtained right-singular vectors and re-distributeranks among all weight matrices to explain the maximal amount of variance andcontinue the standard LoRA fine-tuning procedure. This results in our newmethod Explained Variance Adaptation EVA. We apply EVA to a variety offine-tuning tasks ranging from language generation and understanding to imageclassification and reinforcement learning. EVA exhibits faster convergence thancompetitors and attains the highest average score across a multitude of tasksper domain.</p>
                <p>Last Updated: 2024-10-09 17:59:06 UTC</p>
                <button class="interpret-button" data-id="2410.07170v1">Interpret</button>
                <div id="interpretation-2410.07170v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate</h3>
                <p>Authors: Qidong HuangXiaoyi DongPan ZhangYuhang ZangYuhang CaoJiaqi WangDahua LinWeiming ZhangNenghai Yu</p>
                <p><a href="http://arxiv.org/abs/2410.07167v1">Link to paper</a></p>
                <p>We present the Modality Integration Rate MIR an effective robust andgeneralized metric to indicate the multi-modal pre-training quality of LargeVision Language Models LVLMs. Large-scale pre-training plays a critical rolein building capable LVLMs while evaluating its training quality without thecostly supervised fine-tuning stage is under-explored. Loss perplexity andin-context evaluation results are commonly used pre-training metrics for LargeLanguage Models LLMs while we observed that these metrics are lessindicative when aligning a well-trained LLM with a new modality. Due to thelack of proper metrics the research of LVLMs in the critical pre-trainingstage is hindered greatly including the training data choice efficient moduledesign etc. In this paper we propose evaluating the pre-training quality fromthe inter-modal distribution distance perspective and present MIR the ModalityIntegration Rate which is 1 textbfEffective to represent the pre-trainingquality and show a positive relation with the benchmark performance aftersupervised fine-tuning. 2 textbfRobust toward different training/evaluationdata. 3 textbfGeneralize across training configurations and architecturechoices. We conduct a series of pre-training experiments to explore theeffectiveness of MIR and observe satisfactory results that MIR is indicativeabout training data selection training strategy schedule and modelarchitecture design to get better pre-training results. We hope MIR could be ahelpful metric for building capable LVLMs and inspire the following researchabout modality alignment in different areas. Our code is at:https://github.com/shikiw/Modality-Integration-Rate.</p>
                <p>Last Updated: 2024-10-09 17:59:04 UTC</p>
                <button class="interpret-button" data-id="2410.07167v1">Interpret</button>
                <div id="interpretation-2410.07167v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Sylber: Syllabic Embedding Representation of Speech from Raw Audio</h3>
                <p>Authors: Cheol Jun ChoNicholas LeeAkshat GuptaDhruv AgarwalEthan ChenAlan W BlackGopala K. Anumanchipalli</p>
                <p><a href="http://arxiv.org/abs/2410.07168v1">Link to paper</a></p>
                <p>Syllables are compositional units of spoken language that play a crucial rolein human speech perception and production. However current neural speechrepresentations lack structure resulting in dense token sequences that arecostly to process. To bridge this gap we propose a new model Sylber thatproduces speech representations with clean and robust syllabic structure.Specifically we propose a self-supervised model that regresses features onsyllabic segments distilled from a teacher model which is an exponential movingaverage of the model in training. This results in a highly structuredrepresentation of speech features offering three key benefits: 1 a fastlinear-time syllable segmentation algorithm 2 efficient syllabic tokenizationwith an average of 4.27 tokens per second and 3 syllabic units better suitedfor lexical and syntactic understanding. We also train token-to-speechgenerative models with our syllabic units and show that fully intelligiblespeech can be reconstructed from these tokens. Lastly we observe thatcategorical perception a linguistic phenomenon of speech perception emergesnaturally in our model making the embedding space more categorical and sparsethan previous self-supervised learning approaches. Together we present a novelself-supervised approach for representing speech as syllables with significantpotential for efficient speech tokenization and spoken language modeling.</p>
                <p>Last Updated: 2024-10-09 17:59:04 UTC</p>
                <button class="interpret-button" data-id="2410.07168v1">Interpret</button>
                <div id="interpretation-2410.07168v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>MM-Ego: Towards Building Egocentric Multimodal LLMs</h3>
                <p>Authors: Hanrong YeHaotian ZhangErik DaxbergerLin ChenZongyu LinYanghao LiBowen ZhangHaoxuan YouDan XuZhe GanJiasen LuYinfei Yang</p>
                <p><a href="http://arxiv.org/abs/2410.07177v1">Link to paper</a></p>
                <p>This research aims to comprehensively explore building a multimodalfoundation model for egocentric video understanding. To achieve this goal wework on three fronts. First as there is a lack of QA data for egocentric videounderstanding we develop a data engine that efficiently generates 7Mhigh-quality QA samples for egocentric videos ranging from 30 seconds to onehour long based on human-annotated data. This is currently the largestegocentric QA dataset. Second we contribute a challenging egocentric QAbenchmark with 629 videos and 7026 questions to evaluate the models abilityin recognizing and memorizing visual details across videos of varying lengths.We introduce a new de-biasing evaluation method to help mitigate theunavoidable language bias present in the models being evaluated. Third wepropose a specialized multimodal architecture featuring a novel Memory PointerPrompting mechanism. This design includes a global glimpse step to gain anoverarching understanding of the entire video and identify key visualinformation followed by a fallback step that utilizes the key visualinformation to generate responses. This enables the model to more effectivelycomprehend extended video content. With the data benchmark and model wesuccessfully build MM-Ego an egocentric multimodal LLM that shows powerfulperformance on egocentric video understanding.</p>
                <p>Last Updated: 2024-10-09 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2410.07177v1">Interpret</button>
                <div id="interpretation-2410.07177v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</h3>
                <p>Authors: Fei WangXingchen WanRuoxi SunJiefeng ChenSercan Ö. Arık</p>
                <p><a href="http://arxiv.org/abs/2410.07176v1">Link to paper</a></p>
                <p>Retrieval-Augmented Generation RAG while effective in integrating externalknowledge to address the limitations of large language models LLMs can beundermined by imperfect retrieval which may introduce irrelevant misleadingor even malicious information. Despite its importance previous studies haverarely explored the behavior of RAG through joint analysis on how errors fromimperfect retrieval attribute and propagate and how potential conflicts arisebetween the LLMs internal knowledge and external sources. We find thatimperfect retrieval augmentation might be inevitable and quite harmful throughcontrolled analysis under realistic conditions. We identify the knowledgeconflicts between LLM-internal and external knowledge from retrieval as abottleneck to overcome in the post-retrieval stage of RAG. To render LLMsresilient to imperfect retrieval we propose Astute RAG a novel RAG approachthat adaptively elicits essential information from LLMs internal knowledgeiteratively consolidates internal and external knowledge with source-awarenessand finalizes the answer according to information reliability. Our experimentsusing Gemini and Claude demonstrate that Astute RAG significantly outperformsprevious robustness-enhanced RAG methods. Notably Astute RAG is the onlyapproach that matches or exceeds the performance of LLMs without RAG underworst-case scenarios. Further analysis reveals that Astute RAG effectivelyresolves knowledge conflicts improving the reliability and trustworthiness ofRAG systems.</p>
                <p>Last Updated: 2024-10-09 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2410.07176v1">Interpret</button>
                <div id="interpretation-2410.07176v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Neural Circuit Architectural Priors for Quadruped Locomotion</h3>
                <p>Authors: Nikhil X. BhattasaliVenkatesh PattabiramanLerrel PintoGrace W. Lindsay</p>
                <p><a href="http://arxiv.org/abs/2410.07174v1">Link to paper</a></p>
                <p>Learning-based approaches to quadruped locomotion commonly adopt genericpolicy architectures like fully connected MLPs. As such architectures containfew inductive biases it is common in practice to incorporate priors in theform of rewards training curricula imitation data or trajectory generators.In nature animals are born with priors in the form of their nervous systemsarchitecture which has been shaped by evolution to confer innate ability andefficient learning. For instance a horse can walk within hours of birth andcan quickly improve with practice. Such architectural priors can also be usefulin ANN architectures for AI. In this work we explore the advantages of abiologically inspired ANN architecture for quadruped locomotion based on neuralcircuits in the limbs and spinal cord of mammals. Our architecture achievesgood initial performance and comparable final performance to MLPs while usingless data and orders of magnitude fewer parameters. Our architecture alsoexhibits better generalization to task variations even admitting deployment ona physical robot without standard sim-to-real methods. This work shows thatneural circuits can provide valuable architectural priors for locomotion andencourages future work in other sensorimotor skills.</p>
                <p>Last Updated: 2024-10-09 17:59:45 UTC</p>
                <button class="interpret-button" data-id="2410.07174v1">Interpret</button>
                <div id="interpretation-2410.07174v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Glider: Global and Local Instruction-Driven Expert Router</h3>
                <p>Authors: Pingzhi LiPrateek YadavJaehong YoonJie PengYi-Lin SungMohit BansalTianlong Chen</p>
                <p><a href="http://arxiv.org/abs/2410.07172v1">Link to paper</a></p>
                <p>The availability of performant pre-trained models has led to a proliferationof fine-tuned expert models that are specialized to particular domains. Thishas enabled the creation of powerful and adaptive routing-based ModelMoErging methods with the goal of using expert modules to create an aggregatesystem with improved performance or generalization. However existing MoErgingmethods often prioritize generalization to unseen tasks at the expense ofperformance on held-in tasks which limits its practical applicability inreal-world deployment scenarios. We observe that current token-level routingmechanisms neglect the global semantic context of the input task. Thistoken-wise independence hinders effective expert selection for held-in tasksas routing decisions fail to incorporate the semantic properties of the task.To address this we propose Global and Local Instruction Driven Expert RouterGLIDER that integrates a multi-scale routing mechanism encompassing asemantic global router and a learned local router. The global router leveragesLLMs advanced reasoning capabilities for semantic-related contexts to enhanceexpert selection. Given the input query and LLM the router generates semantictask instructions that guide the retrieval of the most relevant experts acrossall layers. This global guidance is complemented by a local router thatfacilitates token-level routing decisions within each module enabling finercontrol and enhanced performance on unseen tasks. Our experiments usingT5-based models for T0 and FLAN tasks demonstrate that GLIDER achievessubstantially improved held-in performance while maintaining stronggeneralization on held-out tasks. We also perform ablations experiments to divedeeper into the components of GLIDER. Our experiments highlight the importanceof our multi-scale routing that leverages LLM-driven semantic reasoning forMoErging methods.</p>
                <p>Last Updated: 2024-10-09 17:59:14 UTC</p>
                <button class="interpret-button" data-id="2410.07172v1">Interpret</button>
                <div id="interpretation-2410.07172v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation</h3>
                <p>Authors: Fabian PaischerLukas HauzenbergerThomas SchmiedBenedikt AlkinMarc Peter DeisenrothSepp Hochreiter</p>
                <p><a href="http://arxiv.org/abs/2410.07170v1">Link to paper</a></p>
                <p>Foundation models FMs are pre-trained on large-scale datasets and thenfine-tuned on a downstream task for a specific application. The most successfuland most commonly used fine-tuning method is to update the pre-trained weightsvia a low-rank adaptation LoRA. LoRA introduces new weight matrices that areusually initialized at random with a uniform rank distribution across modelweights. Recent works focus on weight-driven initialization or learning ofadaptive ranks during training. Both approaches have only been investigated inisolation resulting in slow convergence or a uniform rank distribution inturn leading to sub-optimal performance. We propose to enhance LoRA byinitializing the new weights in a data-driven manner by computing singularvalue decomposition on minibatches of activation vectors. Then we initializethe LoRA matrices with the obtained right-singular vectors and re-distributeranks among all weight matrices to explain the maximal amount of variance andcontinue the standard LoRA fine-tuning procedure. This results in our newmethod Explained Variance Adaptation EVA. We apply EVA to a variety offine-tuning tasks ranging from language generation and understanding to imageclassification and reinforcement learning. EVA exhibits faster convergence thancompetitors and attains the highest average score across a multitude of tasksper domain.</p>
                <p>Last Updated: 2024-10-09 17:59:06 UTC</p>
                <button class="interpret-button" data-id="2410.07170v1">Interpret</button>
                <div id="interpretation-2410.07170v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-10-11</p>
        </div>
    
        </div>
    </body>
    </html>
    