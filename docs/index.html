
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Verbalized Machine Learning: Revisiting Machine Learning with Language Models</h3>
                <p>Authors: Tim Z. XiaoRobert BamlerBernhard SchölkopfWeiyang Liu</p>
                <p><a href="http://arxiv.org/abs/2406.04344v1">Link to paper</a></p>
                <p>Motivated by the large progress made by large language models LLMs weintroduce the framework of verbalized machine learning VML. In contrast toconventional machine learning models that are typically optimized over acontinuous parameter space VML constrains the parameter space to behuman-interpretable natural language. Such a constraint leads to a newperspective of function approximation where an LLM with a text prompt can beviewed as a function parameterized by the text prompt. Guided by thisperspective we revisit classical machine learning problems such as regressionand classification and find that these problems can be solved by anLLM-parameterized learner and optimizer. The major advantages of VML include1 easy encoding of inductive bias: prior knowledge about the problem andhypothesis class can be encoded in natural language and fed into theLLM-parameterized learner 2 automatic model class selection: the optimizercan automatically select a concrete model class based on data and verbalizedprior knowledge and it can update the model class during training and 3interpretable learner updates: the LLM-parameterized optimizer can provideexplanations for why each learner update is performed. We conduct severalstudies to empirically evaluate the effectiveness of VML and hope that VML canserve as a stepping stone to stronger interpretability and trustworthiness inML.</p>
                <p>Last Updated: 2024-06-06 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2406.04344v1">Interpret</button>
                <div id="interpretation-2406.04344v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PaCE: Parsimonious Concept Engineering for Large Language Models</h3>
                <p>Authors: Jinqi LuoTianjiao DingKwan Ho Ryan ChanDarshan ThakerAditya ChattopadhyayChris Callison-BurchRené Vidal</p>
                <p><a href="http://arxiv.org/abs/2406.04331v1">Link to paper</a></p>
                <p>Large Language Models LLMs are being used for a wide variety of tasks.While they are capable of generating human-like responses they can alsoproduce undesirable output including potentially harmful information racist orsexist language and hallucinations. Alignment methods are designed to reducesuch undesirable output via techniques such as fine-tuning promptengineering and representation engineering. However existing methods faceseveral challenges: some require costly fine-tuning for every alignment tasksome do not adequately remove undesirable concepts failing alignment someremove benign concepts lowering the linguistic capabilities of LLMs. Toaddress these issues we propose Parsimonious Concept Engineering PaCE anovel activation engineering framework for alignment. First to sufficientlymodel the concepts we construct a large-scale concept dictionary in theactivation space in which each atom corresponds to a semantic concept. Thengiven any alignment task we instruct a concept partitioner to efficientlyannotate the concepts as benign or undesirable. Finally at inference time wedecompose the LLM activations along the concept dictionary via sparse codingto accurately represent the activation as a linear combination of the benignand undesirable components. By removing the latter ones from the activation wereorient the behavior of LLMs towards alignment goals. We conduct experimentson tasks such as response detoxification faithfulness enhancement andsentiment revising and show that PaCE achieves state-of-the-art alignmentperformance while maintaining linguistic capabilities.</p>
                <p>Last Updated: 2024-06-06 17:59:10 UTC</p>
                <button class="interpret-button" data-id="2406.04331v1">Interpret</button>
                <div id="interpretation-2406.04331v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Improving Alignment and Robustness with Short Circuiting</h3>
                <p>Authors: Andy ZouLong PhanJustin WangDerek DuenasMaxwell LinMaksym AndriushchenkoRowan WangZico KolterMatt FredriksonDan Hendrycks</p>
                <p><a href="http://arxiv.org/abs/2406.04313v1">Link to paper</a></p>
                <p>AI systems can take harmful actions and are highly vulnerable to adversarialattacks. We present an approach inspired by recent advances in representationengineering that short-circuits models as they respond with harmful outputs.Existing techniques aimed at improving alignment such as refusal training areoften bypassed. Techniques such as adversarial training try to plug these holesby countering specific attacks. As an alternative to refusal training andadversarial training short-circuiting directly controls the representationsthat are responsible for harmful outputs in the first place. Our technique canbe applied to both text-only and multimodal language models to prevent thegeneration of harmful outputs without sacrificing utility -- even in thepresence of powerful unseen attacks. Notably while adversarial robustness instandalone image recognition remains an open challenge short-circuiting allowsthe larger multimodal system to reliably withstand image hijacks that aim toproduce harmful content. Finally we extend our approach to AI agentsdemonstrating considerable reductions in the rate of harmful actions when theyare under attack. Our approach represents a significant step forward in thedevelopment of reliable safeguards to harmful behavior and adversarial attacks.</p>
                <p>Last Updated: 2024-06-06 17:57:04 UTC</p>
                <button class="interpret-button" data-id="2406.04313v1">Interpret</button>
                <div id="interpretation-2406.04313v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Measuring and Addressing Indexical Bias in Information Retrieval</h3>
                <p>Authors: Caleb ZiemsWilliam HeldJane Dwivedi-YuDiyi Yang</p>
                <p><a href="http://arxiv.org/abs/2406.04298v1">Link to paper</a></p>
                <p>Information Retrieval IR systems are designed to deliver relevant contentbut traditional systems may not optimize rankings for fairness neutrality orthe balance of ideas. Consequently IR can often introduce indexical biases orbiases in the positional order of documents. Although indexical bias candemonstrably affect peoples opinion voting patterns and other behaviorsthese issues remain understudied as the field lacks reliable metrics andprocedures for automatically measuring indexical bias. Towards this end weintroduce the PAIR framework which supports automatic bias audits for rankeddocuments or entire IR systems. After introducing DUO the firstgeneral-purpose automatic bias metric we run an extensive evaluation of 8 IRsystems on a new corpus of 32k synthetic and 4.7k natural documents with 4kqueries spanning 1.4k controversial issue topics. A human behavioral studyvalidates our approach showing that our bias metric can help predict when andhow indexical bias will shift a readers opinion.</p>
                <p>Last Updated: 2024-06-06 17:42:37 UTC</p>
                <button class="interpret-button" data-id="2406.04298v1">Interpret</button>
                <div id="interpretation-2406.04298v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval</h3>
                <p>Authors: Junjie ZhouZheng LiuShitao XiaoBo ZhaoYongping Xiong</p>
                <p><a href="http://arxiv.org/abs/2406.04292v1">Link to paper</a></p>
                <p>Multi-modal retrieval becomes increasingly popular in practice. However theexisting retrievers are mostly text-oriented which lack the capability toprocess visual information. Despite the presence of vision-language models likeCLIP the current methods are severely limited in representing the text-onlyand image-only data. In this work we present a new embedding model VISTA foruniversal multi-modal retrieval. Our work brings forth threefold technicalcontributions. Firstly we introduce a flexible architecture which extends apowerful text encoder with the image understanding capability by introducingvisual token embeddings. Secondly we develop two data generation strategieswhich bring high-quality composed image-text to facilitate the training of theembedding model. Thirdly we introduce a multi-stage training algorithm whichfirst aligns the visual token embedding with the text encoder using massiveweakly labeled data and then develops multi-modal representation capabilityusing the generated composed image-text data. In our experiments VISTAachieves superior performances across a variety of multi-modal retrieval tasksin both zero-shot and supervised settings. Our model data and source code areavailable at https://github.com/FlagOpen/FlagEmbedding.</p>
                <p>Last Updated: 2024-06-06 17:37:47 UTC</p>
                <button class="interpret-button" data-id="2406.04292v1">Interpret</button>
                <div id="interpretation-2406.04292v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Simplified and Generalized Masked Diffusion for Discrete Data</h3>
                <p>Authors: Jiaxin ShiKehang HanZhe WangArnaud DoucetMichalis K. Titsias</p>
                <p><a href="http://arxiv.org/abs/2406.04329v1">Link to paper</a></p>
                <p>Masked or absorbing diffusion is actively explored as an alternative toautoregressive models for generative modeling of discrete data. Howeverexisting work in this area has been hindered by unnecessarily complex modelformulations and unclear relationships between different perspectives leadingto suboptimal parameterization training objectives and ad hoc adjustments tocounteract these issues. In this work we aim to provide a simple and generalframework that unlocks the full potential of masked diffusion models. We showthat the continuous-time variational objective of masked diffusion models is asimple weighted integral of cross-entropy losses. Our framework also enablestraining generalized masked diffusion models with state-dependent maskingschedules. When evaluated by perplexity our models trained on OpenWebTextsurpass prior diffusion language models at GPT-2 scale and demonstrate superiorperformance on 4 out of 5 zero-shot language modeling tasks. Furthermore ourmodels vastly outperform previous discrete diffusion models on pixel-levelimage modeling achieving 2.78CIFAR-10 and 3.42 ImageNet 64times64 bitsper dimension that are comparable or better than autoregressive models ofsimilar sizes.</p>
                <p>Last Updated: 2024-06-06 17:59:10 UTC</p>
                <button class="interpret-button" data-id="2406.04329v1">Interpret</button>
                <div id="interpretation-2406.04329v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Regularized KL-Divergence for Well-Defined Function-Space Variational Inference in Bayesian neural networks</h3>
                <p>Authors: Tristan CinquinRobert Bamler</p>
                <p><a href="http://arxiv.org/abs/2406.04317v1">Link to paper</a></p>
                <p>Bayesian neural networks BNN promise to combine the predictive performanceof neural networks with principled uncertainty modeling important forsafety-critical systems and decision making. However posterior uncertaintyestimates depend on the choice of prior and finding informative priors inweight-space has proven difficult. This has motivated variational inferenceVI methods that pose priors directly on the function generated by the BNNrather than on weights. In this paper we address a fundamental issue with suchfunction-space VI approaches pointed out by Burt et al. 2020 who showed thatthe objective function ELBO is negative infinite for most priors of interest.Our solution builds on generalized VI Knoblauch et al. 2019 with theregularized KL divergence Quang 2019 and is to the best of our knowledgethe first well-defined variational objective for function-space inference inBNNs with Gaussian process GP priors. Experiments show that our methodincorporates the properties specified by the GP prior on synthetic and smallreal-world data sets and provides competitive uncertainty estimates forregression classification and out-of-distribution detection compared to BNNbaselines with both function and weight-space priors.</p>
                <p>Last Updated: 2024-06-06 17:57:49 UTC</p>
                <button class="interpret-button" data-id="2406.04317v1">Interpret</button>
                <div id="interpretation-2406.04317v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Approximation-Aware Bayesian Optimization</h3>
                <p>Authors: Natalie MausKyurae KimGeoff PleissDavid ErikssonJohn P. CunninghamJacob R. Gardner</p>
                <p><a href="http://arxiv.org/abs/2406.04308v1">Link to paper</a></p>
                <p>High-dimensional Bayesian optimization BO tasks such as molecular designoften require 10000 function evaluations before obtaining meaningful results.While methods like sparse variational Gaussian processes SVGPs reducecomputational requirements in these settings the underlying approximationsresult in suboptimal data acquisitions that slow the progress of optimization.In this paper we modify SVGPs to better align with the goals of BO: targetinginformed data acquisition rather than global posterior fidelity. Using theframework of utility-calibrated variational inference we unify GPapproximation and data acquisition into a joint optimization problem therebyensuring optimal decisions under a limited computational budget. Our approachcan be used with any decision-theoretic acquisition function and is compatiblewith trust region methods like TuRBO. We derive efficient joint objectives forthe expected improvement and knowledge gradient acquisition functions in boththe standard and batch BO settings. Our approach outperforms standard SVGPs onhigh-dimensional benchmark tasks in control and molecular design.</p>
                <p>Last Updated: 2024-06-06 17:55:02 UTC</p>
                <button class="interpret-button" data-id="2406.04308v1">Interpret</button>
                <div id="interpretation-2406.04308v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Stratified Prediction-Powered Inference for Hybrid Language Model Evaluation</h3>
                <p>Authors: Adam FischJoshua MaynezR. Alex HoferBhuwan DhingraAmir GlobersonWilliam W. Cohen</p>
                <p><a href="http://arxiv.org/abs/2406.04291v1">Link to paper</a></p>
                <p>Prediction-powered inference PPI is a method that improves statisticalestimates based on limited human-labeled data. PPI achieves this by combiningsmall amounts of human-labeled data with larger amounts of data labeled by areasonably accurate -- but potentially biased -- automatic system in a waythat results in tighter confidence intervals for certain parameters of intereste.g. the mean performance of a language model. In this paper we propose amethod called Stratified Prediction-Powered Inference StratPPI in which weshow that the basic PPI estimates can be considerably improved by employingsimple data stratification strategies. Without making any assumptions on theunderlying automatic labeling system or data distribution we derive analgorithm for computing provably valid confidence intervals for populationparameters such as averages that is based on stratified sampling. Inparticular we show both theoretically and empirically that with appropriatechoices of stratification and sample allocation our approach can providesubstantially tighter confidence intervals than unstratified approaches.Specifically StratPPI is expected to improve in cases where the performance ofthe autorater varies across different conditional distributions of the targetdata.</p>
                <p>Last Updated: 2024-06-06 17:37:39 UTC</p>
                <button class="interpret-button" data-id="2406.04291v1">Interpret</button>
                <div id="interpretation-2406.04291v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Online learning of quantum processes</h3>
                <p>Authors: Asad RazaMatthias C. CaroJens EisertSumeet Khatri</p>
                <p><a href="http://arxiv.org/abs/2406.04250v1">Link to paper</a></p>
                <p>Among recent insights into learning quantum states online learning andshadow tomography procedures are notable for their ability to accuratelypredict expectation values even of adaptively chosen observables. In contrastto the state case quantum process learning tasks with a similarly adaptivenature have received little attention. In this work we investigate onlinelearning tasks for quantum processes. Whereas online learning is infeasible forgeneral quantum channels we show that channels of bounded gate complexity aswell as Pauli channels can be online learned in the regret and mistake-boundedmodels of online learning. In fact we can online learn probabilistic mixturesof any exponentially large set of known channels. We also provide a provablysample-efficient shadow tomography procedure for Pauli channels. Our resultsextend beyond quantum channels to non-Markovian multi-time processes withfavorable regret and mistake bounds as well as a shadow tomography procedure.We complement our online learning upper bounds with mistake as well ascomputational lower bounds. On the technical side we make use of themultiplicative weights update algorithm classical adaptive data analysis andBell sampling as well as tools from the theory of quantum combs for multi-timequantum processes. Our work initiates a study of online learning for classes ofquantum channels and more generally non-Markovian quantum processes. Giventhe importance of online learning for state shadow tomography this may serveas a step towards quantum channel variants of adaptive shadow tomography.</p>
                <p>Last Updated: 2024-06-06 16:54:20 UTC</p>
                <button class="interpret-button" data-id="2406.04250v1">Interpret</button>
                <div id="interpretation-2406.04250v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Stereo-Depth Fusion through Virtual Pattern Projection</h3>
                <p>Authors: Luca BartolomeiMatteo PoggiFabio TosiAndrea ContiStefano Mattoccia</p>
                <p><a href="http://arxiv.org/abs/2406.04345v1">Link to paper</a></p>
                <p>This paper presents a novel general-purpose stereo and depth data fusionparadigm that mimics the active stereo principle by replacing the unreliablephysical pattern projector with a depth sensor. It works by projecting virtualpatterns consistent with the scene geometry onto the left and right imagesacquired by a conventional stereo camera using the sparse hints obtained froma depth sensor to facilitate the visual correspondence. Purposely any depthsensing device can be seamlessly plugged into our framework enabling thedeployment of a virtual active stereo setup in any possible environment andovercoming the severe limitations of physical pattern projection such as thelimited working range and environmental conditions. Exhaustive experiments onindoor and outdoor datasets featuring both long and close range includingthose providing raw unfiltered depth hints from off-the-shelf depth sensorshighlight the effectiveness of our approach in notably boosting the robustnessand accuracy of algorithms and deep stereo without any code modification andeven without re-training. Additionally we assess the performance of ourstrategy on active stereo evaluation datasets with conventional patternprojection. Indeed in all these scenarios our virtual pattern projectionparadigm achieves state-of-the-art performance. The source code is availableat: https://github.com/bartn8/vppstereo.</p>
                <p>Last Updated: 2024-06-06 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2406.04345v1">Interpret</button>
                <div id="interpretation-2406.04345v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning 1D Causal Visual Representation with De-focus Attention Networks</h3>
                <p>Authors: Chenxin TaoXizhou ZhuShiqian SuLewei LuChangyao TianXuan LuoGao HuangHongsheng LiYu QiaoJie ZhouJifeng Dai</p>
                <p><a href="http://arxiv.org/abs/2406.04342v1">Link to paper</a></p>
                <p>Modality differences have led to the development of heterogeneousarchitectures for vision and language models. While images typically require 2Dnon-causal modeling texts utilize 1D causal modeling. This distinction posessignificant challenges in constructing unified multi-modal models. This paperexplores the feasibility of representing images using 1D causal modeling. Weidentify an over-focus issue in existing 1D causal vision models whereattention overly concentrates on a small proportion of visual tokens. The issueof over-focus hinders the models ability to extract diverse visual featuresand to receive effective gradients for optimization. To address this wepropose De-focus Attention Networks which employ learnable bandpass filters tocreate varied attention patterns. During training large and scheduled droppath rates and an auxiliary loss on globally pooled features for globalunderstanding tasks are introduced. These two strategies encourage the model toattend to a broader range of tokens and enhance network optimization. Extensiveexperiments validate the efficacy of our approach demonstrating that 1D causalvisual representation can perform comparably to 2D non-causal representation intasks such as global perception dense prediction and multi-modalunderstanding. Code is released athttps://github.com/OpenGVLab/De-focus-Attention-Networks.</p>
                <p>Last Updated: 2024-06-06 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2406.04342v1">Interpret</button>
                <div id="interpretation-2406.04342v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image</h3>
                <p>Authors: Stanislaw SzymanowiczEldar InsafutdinovChuanxia ZhengDylan CampbellJoão F. HenriquesChristian RupprechtAndrea Vedaldi</p>
                <p><a href="http://arxiv.org/abs/2406.04343v1">Link to paper</a></p>
                <p>In this paper we propose Flash3D a method for scene reconstruction andnovel view synthesis from a single image which is both very generalisable andefficient. For generalisability we start from a foundation model formonocular depth estimation and extend it to a full 3D shape and appearancereconstructor. For efficiency we base this extension on feed-forward GaussianSplatting. Specifically we predict a first layer of 3D Gaussians at thepredicted depth and then add additional layers of Gaussians that are offset inspace allowing the model to complete the reconstruction behind occlusions andtruncations. Flash3D is very efficient trainable on a single GPU in a day andthus accessible to most researchers. It achieves state-of-the-art results whentrained and tested on RealEstate10k. When transferred to unseen datasets likeNYU it outperforms competitors by a large margin. More impressively whentransferred to KITTI Flash3D achieves better PSNR than methods trainedspecifically on that dataset. In some instances it even outperforms recentmethods that use multiple views as input. Code models demo and more resultsare available at https://www.robots.ox.ac.uk/vgg/research/flash3d/.</p>
                <p>Last Updated: 2024-06-06 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2406.04343v1">Interpret</button>
                <div id="interpretation-2406.04343v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Verbalized Machine Learning: Revisiting Machine Learning with Language Models</h3>
                <p>Authors: Tim Z. XiaoRobert BamlerBernhard SchölkopfWeiyang Liu</p>
                <p><a href="http://arxiv.org/abs/2406.04344v1">Link to paper</a></p>
                <p>Motivated by the large progress made by large language models LLMs weintroduce the framework of verbalized machine learning VML. In contrast toconventional machine learning models that are typically optimized over acontinuous parameter space VML constrains the parameter space to behuman-interpretable natural language. Such a constraint leads to a newperspective of function approximation where an LLM with a text prompt can beviewed as a function parameterized by the text prompt. Guided by thisperspective we revisit classical machine learning problems such as regressionand classification and find that these problems can be solved by anLLM-parameterized learner and optimizer. The major advantages of VML include1 easy encoding of inductive bias: prior knowledge about the problem andhypothesis class can be encoded in natural language and fed into theLLM-parameterized learner 2 automatic model class selection: the optimizercan automatically select a concrete model class based on data and verbalizedprior knowledge and it can update the model class during training and 3interpretable learner updates: the LLM-parameterized optimizer can provideexplanations for why each learner update is performed. We conduct severalstudies to empirically evaluate the effectiveness of VML and hope that VML canserve as a stepping stone to stronger interpretability and trustworthiness inML.</p>
                <p>Last Updated: 2024-06-06 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2406.04344v1">Interpret</button>
                <div id="interpretation-2406.04344v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Interpreting the Second-Order Effects of Neurons in CLIP</h3>
                <p>Authors: Yossi GandelsmanAlexei A. EfrosJacob Steinhardt</p>
                <p><a href="http://arxiv.org/abs/2406.04341v1">Link to paper</a></p>
                <p>We interpret the function of individual neurons in CLIP by automaticallydescribing them using text. Analyzing the direct effects i.e. the flow from aneuron through the residual stream to the output or the indirect effectsoverall contribution fails to capture the neurons function in CLIP.Therefore we present the second-order lens analyzing the effect flowingfrom a neuron through the later attention heads directly to the output. Wefind that these effects are highly selective: for each neuron the effect issignificant for 2 of the images. Moreover each effect can be approximated bya single direction in the text-image space of CLIP. We describe neurons bydecomposing these directions into sparse sets of text representations. The setsreveal polysemantic behavior - each neuron corresponds to multiple oftenunrelated concepts e.g. ships and cars. Exploiting this neuron polysemy wemass-produce semantic adversarial examples by generating images with conceptsspuriously correlated to the incorrect class. Additionally we use thesecond-order effects for zero-shot segmentation and attribute discovery inimages. Our results indicate that a scalable understanding of neurons can beused for model deception and for introducing new model capabilities.</p>
                <p>Last Updated: 2024-06-06 17:59:52 UTC</p>
                <button class="interpret-button" data-id="2406.04341v1">Interpret</button>
                <div id="interpretation-2406.04341v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People</h3>
                <p>Authors: Dun-Ming HuangPol Van RijnIlia SucholutskyRaja MarjiehNori Jacoby</p>
                <p><a href="http://arxiv.org/abs/2406.04278v1">Link to paper</a></p>
                <p>Conversational tones -- the manners and attitudes in which speakerscommunicate -- are essential to effective communication. Amidst the increasingpopularization of Large Language Models LLMs over recent years it becomesnecessary to characterize the divergences in their conversational tonesrelative to humans. However existing investigations of conversationalmodalities rely on pre-existing taxonomies or text corpora which suffer fromexperimenter bias and may not be representative of real-world distributions forthe studies psycholinguistic domains. Inspired by methods from cognitivescience we propose an iterative method for simultaneously elicitingconversational tones and sentences where participants alternate between twotasks: 1 one participant identifies the tone of a given sentence and 2 adifferent participant generates a sentence based on that tone. We run 100iterations of this process with human participants and GPT-4 then obtain adataset of sentences and frequent conversational tones. In an additionalexperiment humans and GPT-4 annotated all sentences with all tones. With datafrom 1339 human participants 33370 human judgments and 29900 GPT-4queries we show how our approach can be used to create an interpretablegeometric representation of relations between conversational tones in humansand GPT-4. This work demonstrates how combining ideas from machine learning andcognitive science can address challenges in human-computer interactions.</p>
                <p>Last Updated: 2024-06-06 17:26:00 UTC</p>
                <button class="interpret-button" data-id="2406.04278v1">Interpret</button>
                <div id="interpretation-2406.04278v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The 3D-PC: a benchmark for visual perspective taking in humans and machines</h3>
                <p>Authors: Drew LinsleyPeisen ZhouAlekh Karkada AshokAkash NagarajGaurav GaonkarFrancis E LewisZygmunt PizloThomas Serre</p>
                <p><a href="http://arxiv.org/abs/2406.04138v1">Link to paper</a></p>
                <p>Visual perspective taking VPT is the ability to perceive and reason aboutthe perspectives of others. It is an essential feature of human intelligencewhich develops over the first decade of life and requires an ability to processthe 3D structure of visual scenes. A growing number of reports have indicatedthat deep neural networks DNNs become capable of analyzing 3D scenes aftertraining on large image datasets. We investigated if this emergent ability for3D analysis in DNNs is sufficient for VPT with the 3D perception challenge3D-PC: a novel benchmark for 3D perception in humans and DNNs. The 3D-PC iscomprised of three 3D-analysis tasks posed within natural scene images: 1. asimple test of object depth order 2. a basic VPT task VPT-basic and 3.another version of VPT VPT-Strategy designed to limit the effectiveness ofshortcut visual strategies. We tested human participants N33 and linearlyprobed or text-prompted over 300 DNNs on the challenge and found that nearlyall of the DNNs approached or exceeded human accuracy in analyzing object depthorder. Surprisingly DNN accuracy on this task correlated with their objectrecognition performance. In contrast there was an extraordinary gap betweenDNNs and humans on VPT-basic. Humans were nearly perfect whereas most DNNswere near chance. Fine-tuning DNNs on VPT-basic brought them close to humanperformance but they unlike humans dropped back to chance when tested onVPT-perturb. Our challenge demonstrates that the training routines andarchitectures of todays DNNs are well-suited for learning basic 3D propertiesof scenes and objects but are ill-suited for reasoning about these propertieslike humans do. We release our 3D-PC datasets and code to help bridge this gapin 3D perception between humans and machines.</p>
                <p>Last Updated: 2024-06-06 14:59:39 UTC</p>
                <button class="interpret-button" data-id="2406.04138v1">Interpret</button>
                <div id="interpretation-2406.04138v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Watching Popular Musicians Learn by Ear: A Hypothesis-Generating Study of Human-Recording Interactions in YouTube Videos</h3>
                <p>Authors: Christopher LiscioDaniel G. Brown</p>
                <p><a href="http://arxiv.org/abs/2406.04058v1">Link to paper</a></p>
                <p>Popular musicians often learn music by ear. It is unclear what roletechnology plays for those with experience at this task. In search ofopportunities for the development of novel human-recording interactions weanalyze 18 YouTube videos depicting real-world examples of by-ear learning anddiscuss why during this preliminary phase of research online videos areappropriate data. From our observations we generate hypotheses that can informfuture work. For example a musicians scope of learning may influence whattechnological interactions would help them they could benefit from tools thataccommodate their working memory and transcription does not appear to play akey role in ear learning. Based on these findings we pose a number of researchquestions and discuss their methodological considerations to guide futurestudy.</p>
                <p>Last Updated: 2024-06-06 13:25:42 UTC</p>
                <button class="interpret-button" data-id="2406.04058v1">Interpret</button>
                <div id="interpretation-2406.04058v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Exploring Topic Modelling of User Reviews as a Monitoring Mechanism for Emergent Issues Within Social VR Communities</h3>
                <p>Authors: Angelo SinghJoseph O'Hagan</p>
                <p><a href="http://arxiv.org/abs/2406.03994v1">Link to paper</a></p>
                <p>Users of social virtual reality VR platforms often use user reviews todocument incidents of witnessed and/or experienced user harassment. However atpresent research has yet to be explore utilising this data as a monitoringmechanism to identify emergent issues within social VR communities. Such asystem would be of much benefit to developers and researchers as it wouldenable the automatic identification of emergent issues as they occur provide ameans of longitudinally analysing harassment and reduce the reliance onalternative high cost monitoring methodologies e.g. observation or interviewstudies. To contribute towards the development of such a system we collectedapproximately 40000 Rec Room user reviews from the Steam storefront. We thenanalysed our datasets sentiment word/term frequencies and conducted a topicmodelling analysis of the negative reviews detected in our dataset. We reportour approach was capable of longitudinally monitoring changes in reviewsentiment and identifying high level themes related to types of harassmentknown to occur in social VR platforms.</p>
                <p>Last Updated: 2024-06-06 12:15:39 UTC</p>
                <button class="interpret-button" data-id="2406.03994v1">Interpret</button>
                <div id="interpretation-2406.03994v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</h3>
                <p>Authors: Jianben HeXingbo WangShiyi LiuGuande WuClaudio SilvaHuamin Qu</p>
                <p><a href="http://arxiv.org/abs/2406.03843v1">Link to paper</a></p>
                <p>Large language models LLMs have exhibited impressive abilities formultimodal content comprehension and reasoning with proper prompting in zero-or few-shot settings. Despite the proliferation of interactive systemsdeveloped to support prompt engineering for LLMs across various tasks mosthave primarily focused on textual or visual inputs thus neglecting the complexinterplay between modalities within multimodal inputs. This oversight hindersthe development of effective prompts that guide model multimodal reasoningprocesses by fully exploiting the rich context provided by multiple modalities.In this paper we present POEM a visual analytics system to facilitateefficient prompt engineering for enhancing the multimodal reasoning performanceof LLMs. The system enables users to explore the interaction patterns acrossmodalities at varying levels of detail for a comprehensive understanding of themultimodal knowledge elicited by various prompts. Through diverserecommendations of demonstration examples and instructional principles POEMsupports users in iteratively crafting and refining prompts to better align andenhance model knowledge with human insights. The effectiveness and efficiencyof our system are validated through two case studies and interviews withexperts.</p>
                <p>Last Updated: 2024-06-06 08:21:30 UTC</p>
                <button class="interpret-button" data-id="2406.03843v1">Interpret</button>
                <div id="interpretation-2406.03843v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion</h3>
                <p>Authors: Fangfu LiuHanyang WangShunyu YaoShengjun ZhangJie ZhouYueqi Duan</p>
                <p><a href="http://arxiv.org/abs/2406.04338v1">Link to paper</a></p>
                <p>In recent years there has been rapid development in 3D generation modelsopening up new possibilities for applications such as simulating the dynamicmovements of 3D objects and customizing their behaviors. However current 3Dgenerative models tend to focus only on surface features such as color andshape neglecting the inherent physical properties that govern the behavior ofobjects in the real world. To accurately simulate physics-aligned dynamics itis essential to predict the physical properties of materials and incorporatethem into the behavior prediction process. Nonetheless predicting the diversematerials of real-world objects is still challenging due to the complex natureof their physical attributes. In this paper we propose textbfPhysics3D anovel method for learning various physical properties of 3D objects through avideo diffusion model. Our approach involves designing a highly generalizablephysical simulation system based on a viscoelastic material model whichenables us to simulate a wide range of materials with high-fidelitycapabilities. Moreover we distill the physical priors from a video diffusionmodel that contains more understanding of realistic object materials. Extensiveexperiments demonstrate the effectiveness of our method with both elastic andplastic materials. Physics3D shows great potential for bridging the gap betweenthe physical world and virtual neural space providing a better integration andapplication of realistic physical principles in virtual environments. Projectpage: https://liuff19.github.io/Physics3D.</p>
                <p>Last Updated: 2024-06-06 17:59:47 UTC</p>
                <button class="interpret-button" data-id="2406.04338v1">Interpret</button>
                <div id="interpretation-2406.04338v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Coherent Zero-Shot Visual Instruction Generation</h3>
                <p>Authors: Quynh PhungSongwei GeJia-Bin Huang</p>
                <p><a href="http://arxiv.org/abs/2406.04337v1">Link to paper</a></p>
                <p>Despite the advances in text-to-image synthesis particularly with diffusionmodels generating visual instructions that require consistent representationand smooth state transitions of objects across sequential steps remains aformidable challenge. This paper introduces a simple training-free frameworkto tackle the issues capitalizing on the advancements in diffusion models andlarge language models LLMs. Our approach systematically integrates textcomprehension and image generation to ensure visual instructions are visuallyappealing and maintain consistency and accuracy throughout the instructionsequence. We validate the effectiveness by testing multi-step instructions andcomparing the text alignment and consistency with several baselines. Ourexperiments show that our approach can visualize coherent and visually pleasinginstructions</p>
                <p>Last Updated: 2024-06-06 17:59:44 UTC</p>
                <button class="interpret-button" data-id="2406.04337v1">Interpret</button>
                <div id="interpretation-2406.04337v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PaCE: Parsimonious Concept Engineering for Large Language Models</h3>
                <p>Authors: Jinqi LuoTianjiao DingKwan Ho Ryan ChanDarshan ThakerAditya ChattopadhyayChris Callison-BurchRené Vidal</p>
                <p><a href="http://arxiv.org/abs/2406.04331v1">Link to paper</a></p>
                <p>Large Language Models LLMs are being used for a wide variety of tasks.While they are capable of generating human-like responses they can alsoproduce undesirable output including potentially harmful information racist orsexist language and hallucinations. Alignment methods are designed to reducesuch undesirable output via techniques such as fine-tuning promptengineering and representation engineering. However existing methods faceseveral challenges: some require costly fine-tuning for every alignment tasksome do not adequately remove undesirable concepts failing alignment someremove benign concepts lowering the linguistic capabilities of LLMs. Toaddress these issues we propose Parsimonious Concept Engineering PaCE anovel activation engineering framework for alignment. First to sufficientlymodel the concepts we construct a large-scale concept dictionary in theactivation space in which each atom corresponds to a semantic concept. Thengiven any alignment task we instruct a concept partitioner to efficientlyannotate the concepts as benign or undesirable. Finally at inference time wedecompose the LLM activations along the concept dictionary via sparse codingto accurately represent the activation as a linear combination of the benignand undesirable components. By removing the latter ones from the activation wereorient the behavior of LLMs towards alignment goals. We conduct experimentson tasks such as response detoxification faithfulness enhancement andsentiment revising and show that PaCE achieves state-of-the-art alignmentperformance while maintaining linguistic capabilities.</p>
                <p>Last Updated: 2024-06-06 17:59:10 UTC</p>
                <button class="interpret-button" data-id="2406.04331v1">Interpret</button>
                <div id="interpretation-2406.04331v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories</h3>
                <p>Authors: Qianlan YangYu-Xiong Wang</p>
                <p><a href="http://arxiv.org/abs/2406.04323v1">Link to paper</a></p>
                <p>Training autonomous agents with sparse rewards is a long-standing problem inonline reinforcement learning RL due to low data efficiency. Prior workovercomes this challenge by extracting useful knowledge from offline dataoften accomplished through the learning of action distribution from offlinedata and utilizing the learned distribution to facilitate online RL. Howeversince the offline data are given and fixed the extracted knowledge isinherently limited making it difficult to generalize to new tasks. We proposea novel approach that leverages offline data to learn a generative diffusionmodel coined as Adaptive Trajectory Diffuser ATraDiff. This model generatessynthetic trajectories serving as a form of data augmentation and consequentlyenhancing the performance of online RL methods. The key strength of ourdiffuser lies in its adaptability allowing it to effectively handle varyingtrajectory lengths and mitigate distribution shifts between online and offlinedata. Because of its simplicity ATraDiff seamlessly integrates with a widespectrum of RL methods. Empirical evaluation shows that ATraDiff consistentlyachieves state-of-the-art performance across a variety of environments withparticularly pronounced improvements in complicated settings. Our code and demovideo are available at https://atradiff.github.io .</p>
                <p>Last Updated: 2024-06-06 17:58:15 UTC</p>
                <button class="interpret-button" data-id="2406.04323v1">Interpret</button>
                <div id="interpretation-2406.04323v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models</h3>
                <p>Authors: Ali BehrouzMichele SantacatterinaRamin Zabih</p>
                <p><a href="http://arxiv.org/abs/2406.04320v1">Link to paper</a></p>
                <p>Modeling multivariate time series is a well-established problem with a widerange of applications from healthcare to financial markets. Traditional StateSpace Models SSMs are classical approaches for univariate time seriesmodeling due to their simplicity and expressive power to represent lineardependencies. They however have fundamentally limited expressive power tocapture non-linear dependencies are slow in practice and fail to model theinter-variate information flow. Despite recent attempts to improve theexpressive power of SSMs by using deep structured SSMs the existing methodsare either limited to univariate time series fail to model complex patternse.g. seasonal patterns fail to dynamically model the dependencies ofvariate and time dimensions and/or are input-independent. We present Chimerathat uses two input-dependent 2-D SSM heads with different discretizationprocesses to learn long-term progression and seasonal patterns. To improve theefficiency of complex 2D recurrence we present a fast training using a new2-dimensional parallel selective scan. We further present and discuss2-dimensional Mamba and Mamba-2 as the spacial cases of our 2D SSM. Ourexperimental evaluation shows the superior performance of Chimera on extensiveand diverse benchmarks including ECG and speech time series classificationlong-term and short-term time series forecasting and time series anomalydetection.</p>
                <p>Last Updated: 2024-06-06 17:58:09 UTC</p>
                <button class="interpret-button" data-id="2406.04320v1">Interpret</button>
                <div id="interpretation-2406.04320v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Quantifying Misalignment Between Agents</h3>
                <p>Authors: Aidan KieransAvijit GhoshHananel HazanShiri Dori-Hacohen</p>
                <p><a href="http://arxiv.org/abs/2406.04231v1">Link to paper</a></p>
                <p>Growing concerns about the AI alignment problem have emerged in recent yearswith previous work focusing mainly on 1 qualitative descriptions of thealignment problem 2 attempting to align AI actions with human interests byfocusing on value specification and learning and/or 3 focusing on a singleagent or on humanity as a singular unit. Recent work in sociotechnical AIalignment has made some progress in defining alignment inclusively but thefield as a whole still lacks a systematic understanding of how to specifydescribe and analyze misalignment among entities which may include individualhumans AI agents and complex compositional entities such as corporationsnation-states and so forth. Previous work on controversy in computationalsocial science offers a mathematical model of contention among populations ofhumans. In this paper we adapt this contention model to the alignmentproblem and show how misalignment can vary depending on the population ofagents human or otherwise being observed the domain in question and theagents probability-weighted preferences between possible outcomes. Our modeldeparts from value specification approaches and focuses instead on the morassof complex interlocking sometimes contradictory goals that agents may have inpractice. We apply our model by analyzing several case studies ranging fromsocial media moderation to autonomous vehicle behavior. By applying our modelwith appropriately representative value data AI engineers can ensure thattheir systems learn values maximally aligned with diverse human interests.</p>
                <p>Last Updated: 2024-06-06 16:31:22 UTC</p>
                <button class="interpret-button" data-id="2406.04231v1">Interpret</button>
                <div id="interpretation-2406.04231v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Towards Principled Superhuman AI for Multiplayer Symmetric Games</h3>
                <p>Authors: Jiawei GeYuanhao WangWenzhe LiChi Jin</p>
                <p><a href="http://arxiv.org/abs/2406.04201v1">Link to paper</a></p>
                <p>Multiplayer games when the number of players exceeds two present uniquechallenges that fundamentally distinguish them from the extensively studiedtwo-player zero-sum games. These challenges arise from the non-uniqueness ofequilibria and the risk of agents performing highly suboptimally when adoptingequilibrium strategies. While a line of recent works developed learning systemssuccessfully achieving human-level or even superhuman performance in popularmultiplayer games such as Mahjong Poker and Diplomacy two critical questionsremain unaddressed: 1 What is the correct solution concept that AI agentsshould find and 2 What is the general algorithmic framework that provablysolves all games within this class This paper takes the first step towardssolving these unique challenges of multiplayer games by provably addressingboth questions in multiplayer symmetric normal-form games. We also demonstratethat many meta-algorithms developed in prior practical systems for multiplayergames can fail to achieve even the basic goal of obtaining agents equal shareof the total reward.</p>
                <p>Last Updated: 2024-06-06 15:59:17 UTC</p>
                <button class="interpret-button" data-id="2406.04201v1">Interpret</button>
                <div id="interpretation-2406.04201v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MARLander: A Local Path Planning for Drone Swarms using Multiagent Deep Reinforcement Learning</h3>
                <p>Authors: Demetros AschuRobinroy PeterSausar KarafAleksey FedoseevDzmitry Tsetserukou</p>
                <p><a href="http://arxiv.org/abs/2406.04159v1">Link to paper</a></p>
                <p>Achieving safe and precise landings for a swarm of drones poses a significantchallenge primarily attributed to conventional control and planning methods.This paper presents the implementation of multi-agent deep reinforcementlearning MADRL techniques for the precise landing of a drone swarm atrelocated target locations. The system is trained in a realistic simulatedenvironment with a maximum velocity of 3 m/s in training spaces of 4 x 4 x 4 mand deployed utilizing Crazyflie drones with a Vicon indoor localizationsystem. The experimental results revealed that the proposed approach achieved alanding accuracy of 2.26 cm on stationary and 3.93 cm on moving platformssurpassing a baseline method used with a Proportional-integral-derivative PIDcontroller with an Artificial Potential Field APF. This research highlightsdrone landing technologies that eliminate the need for analytical centralizedsystems potentially offering scalability and revolutionizing applications inlogistics safety and rescue missions.</p>
                <p>Last Updated: 2024-06-06 15:19:15 UTC</p>
                <button class="interpret-button" data-id="2406.04159v1">Interpret</button>
                <div id="interpretation-2406.04159v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Mini Honor of Kings: A Lightweight Environment for Multi-Agent Reinforcement Learning</h3>
                <p>Authors: Lin LiuJian ZhaoCheng HuZhengtao CaoYoupeng ZhaoZhenbin YeMeng MengWenjun WangZhaofeng HeHouqiang LiXia LinLanxiao Huang</p>
                <p><a href="http://arxiv.org/abs/2406.03978v1">Link to paper</a></p>
                <p>Games are widely used as research environments for multi-agent reinforcementlearning MARL but they pose three significant challenges: limitedcustomization high computational demands and oversimplification. To addressthese issues we introduce the first publicly available map editor for thepopular mobile game Honor of Kings and design a lightweight environment MiniHonor of Kings Mini HoK for researchers to conduct experiments. Mini HoK ishighly efficient allowing experiments to be run on personal PCs or laptopswhile still presenting sufficient challenges for existing MARL algorithms. Wehave tested our environment on common MARL algorithms and demonstrated thatthese algorithms have yet to find optimal solutions within this environment.This facilitates the dissemination and advancement of MARL methods within theresearch community. Additionally we hope that more researchers will leveragethe Honor of Kings map editor to develop innovative and scientifically valuablenew maps. Our code and user manual are available at:https://github.com/tencent-ailab/mini-hok.</p>
                <p>Last Updated: 2024-06-06 11:42:33 UTC</p>
                <button class="interpret-button" data-id="2406.03978v1">Interpret</button>
                <div id="interpretation-2406.03978v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Amortized Equation Discovery in Hybrid Dynamical Systems</h3>
                <p>Authors: Yongtuo LiuSara MagliacaneMiltiadis KofinasEfstratios Gavves</p>
                <p><a href="http://arxiv.org/abs/2406.03818v1">Link to paper</a></p>
                <p>Hybrid dynamical systems are prevalent in science and engineering to expresscomplex systems with continuous and discrete states. To learn the laws ofsystems all previous methods for equation discovery in hybrid systems follow atwo-stage paradigm i.e. they first group time series into small clusterfragments and then discover equations in each fragment separately throughmethods in non-hybrid systems. Although effective these methods do not fullytake advantage of the commonalities in the shared dynamics of multiplefragments that are driven by the same equations. Besides the two-stageparadigm breaks the interdependence between categorizing and representingdynamics that jointly form hybrid systems. In this paper we reformulate theproblem and propose an end-to-end learning framework i.e. Amortized EquationDiscovery AMORE to jointly categorize modes and discover equationscharacterizing the dynamics of each mode by all segments of the mode.Experiments on four hybrid and six non-hybrid systems show that our methodoutperforms previous methods on equation discovery segmentation andforecasting.</p>
                <p>Last Updated: 2024-06-06 07:49:02 UTC</p>
                <button class="interpret-button" data-id="2406.03818v1">Interpret</button>
                <div id="interpretation-2406.03818v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Verbalized Machine Learning: Revisiting Machine Learning with Language Models</h3>
                <p>Authors: Tim Z. XiaoRobert BamlerBernhard SchölkopfWeiyang Liu</p>
                <p><a href="http://arxiv.org/abs/2406.04344v1">Link to paper</a></p>
                <p>Motivated by the large progress made by large language models LLMs weintroduce the framework of verbalized machine learning VML. In contrast toconventional machine learning models that are typically optimized over acontinuous parameter space VML constrains the parameter space to behuman-interpretable natural language. Such a constraint leads to a newperspective of function approximation where an LLM with a text prompt can beviewed as a function parameterized by the text prompt. Guided by thisperspective we revisit classical machine learning problems such as regressionand classification and find that these problems can be solved by anLLM-parameterized learner and optimizer. The major advantages of VML include1 easy encoding of inductive bias: prior knowledge about the problem andhypothesis class can be encoded in natural language and fed into theLLM-parameterized learner 2 automatic model class selection: the optimizercan automatically select a concrete model class based on data and verbalizedprior knowledge and it can update the model class during training and 3interpretable learner updates: the LLM-parameterized optimizer can provideexplanations for why each learner update is performed. We conduct severalstudies to empirically evaluate the effectiveness of VML and hope that VML canserve as a stepping stone to stronger interpretability and trustworthiness inML.</p>
                <p>Last Updated: 2024-06-06 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2406.04344v1">Interpret</button>
                <div id="interpretation-2406.04344v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On the Expressive Power of Spectral Invariant Graph Neural Networks</h3>
                <p>Authors: Bohang ZhangLingxiao ZhaoHaggai Maron</p>
                <p><a href="http://arxiv.org/abs/2406.04336v1">Link to paper</a></p>
                <p>Incorporating spectral information to enhance Graph Neural Networks GNNshas shown promising results but raises a fundamental challenge due to theinherent ambiguity of eigenvectors. Various architectures have been proposed toaddress this ambiguity referred to as spectral invariant architectures.Notable examples include GNNs and Graph Transformers that use spectraldistances spectral projection matrices or other invariant spectral features.However the potential expressive power of these spectral invariantarchitectures remains largely unclear. The goal of this work is to gain a deeptheoretical understanding of the expressive power obtainable when usingspectral features. We first introduce a unified message-passing framework fordesigning spectral invariant GNNs called Eigenspace Projection GNN EPNN. Acomprehensive analysis shows that EPNN essentially unifies all prior spectralinvariant architectures in that they are either strictly less expressive orequivalent to EPNN. A fine-grained expressiveness hierarchy among differentarchitectures is also established. On the other hand we prove that EPNN itselfis bounded by a recently proposed class of Subgraph GNNs implying that allthese spectral invariant architectures are strictly less expressive than 3-WL.Finally we discuss whether using spectral features can gain additionalexpressiveness when combined with more expressive GNNs.</p>
                <p>Last Updated: 2024-06-06 17:59:41 UTC</p>
                <button class="interpret-button" data-id="2406.04336v1">Interpret</button>
                <div id="interpretation-2406.04336v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Coarse-To-Fine Tensor Trains for Compact Visual Representations</h3>
                <p>Authors: Sebastian LoeschckeDan WangChristian Leth-EspensenSerge BelongieMichael J. KastoryanoSagie Benaim</p>
                <p><a href="http://arxiv.org/abs/2406.04332v1">Link to paper</a></p>
                <p>The ability to learn compact high-quality and easy-to-optimizerepresentations for visual data is paramount to many applications such as novelview synthesis and 3D reconstruction. Recent work has shown substantial successin using tensor networks to design such compact and high-qualityrepresentations. However the ability to optimize tensor-based representationsand in particular the highly compact tensor train representation is stilllacking. This has prevented practitioners from deploying the full potential oftensor networks for visual data. To this end we propose ProlongationUpsampling Tensor Train PuTT a novel method for learning tensor trainrepresentations in a coarse-to-fine manner. Our method involves the prolongingor upsampling of a learned tensor train representation creating a sequenceof coarse-to-fine tensor trains that are incrementally refined. We evaluateour representation along three axes: 1. compression 2. denoisingcapability and 3. image completion capability. To assess these axes weconsider the tasks of image fitting 3D fitting and novel view synthesiswhere our method shows an improved performance compared to state-of-the-arttensor-based methods. For full results see our project webpage:https://sebulo.github.io/PuTT_website/</p>
                <p>Last Updated: 2024-06-06 17:59:23 UTC</p>
                <button class="interpret-button" data-id="2406.04332v1">Interpret</button>
                <div id="interpretation-2406.04332v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Simplified and Generalized Masked Diffusion for Discrete Data</h3>
                <p>Authors: Jiaxin ShiKehang HanZhe WangArnaud DoucetMichalis K. Titsias</p>
                <p><a href="http://arxiv.org/abs/2406.04329v1">Link to paper</a></p>
                <p>Masked or absorbing diffusion is actively explored as an alternative toautoregressive models for generative modeling of discrete data. Howeverexisting work in this area has been hindered by unnecessarily complex modelformulations and unclear relationships between different perspectives leadingto suboptimal parameterization training objectives and ad hoc adjustments tocounteract these issues. In this work we aim to provide a simple and generalframework that unlocks the full potential of masked diffusion models. We showthat the continuous-time variational objective of masked diffusion models is asimple weighted integral of cross-entropy losses. Our framework also enablestraining generalized masked diffusion models with state-dependent maskingschedules. When evaluated by perplexity our models trained on OpenWebTextsurpass prior diffusion language models at GPT-2 scale and demonstrate superiorperformance on 4 out of 5 zero-shot language modeling tasks. Furthermore ourmodels vastly outperform previous discrete diffusion models on pixel-levelimage modeling achieving 2.78CIFAR-10 and 3.42 ImageNet 64times64 bitsper dimension that are comparable or better than autoregressive models ofsimilar sizes.</p>
                <p>Last Updated: 2024-06-06 17:59:10 UTC</p>
                <button class="interpret-button" data-id="2406.04329v1">Interpret</button>
                <div id="interpretation-2406.04329v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PaCE: Parsimonious Concept Engineering for Large Language Models</h3>
                <p>Authors: Jinqi LuoTianjiao DingKwan Ho Ryan ChanDarshan ThakerAditya ChattopadhyayChris Callison-BurchRené Vidal</p>
                <p><a href="http://arxiv.org/abs/2406.04331v1">Link to paper</a></p>
                <p>Large Language Models LLMs are being used for a wide variety of tasks.While they are capable of generating human-like responses they can alsoproduce undesirable output including potentially harmful information racist orsexist language and hallucinations. Alignment methods are designed to reducesuch undesirable output via techniques such as fine-tuning promptengineering and representation engineering. However existing methods faceseveral challenges: some require costly fine-tuning for every alignment tasksome do not adequately remove undesirable concepts failing alignment someremove benign concepts lowering the linguistic capabilities of LLMs. Toaddress these issues we propose Parsimonious Concept Engineering PaCE anovel activation engineering framework for alignment. First to sufficientlymodel the concepts we construct a large-scale concept dictionary in theactivation space in which each atom corresponds to a semantic concept. Thengiven any alignment task we instruct a concept partitioner to efficientlyannotate the concepts as benign or undesirable. Finally at inference time wedecompose the LLM activations along the concept dictionary via sparse codingto accurately represent the activation as a linear combination of the benignand undesirable components. By removing the latter ones from the activation wereorient the behavior of LLMs towards alignment goals. We conduct experimentson tasks such as response detoxification faithfulness enhancement andsentiment revising and show that PaCE achieves state-of-the-art alignmentperformance while maintaining linguistic capabilities.</p>
                <p>Last Updated: 2024-06-06 17:59:10 UTC</p>
                <button class="interpret-button" data-id="2406.04331v1">Interpret</button>
                <div id="interpretation-2406.04331v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-06-09</p>
        </div>
    
        </div>
    </body>
    </html>
    