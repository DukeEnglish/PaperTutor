
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Data Debiasing with Datamodels (D3M): Improving Subgroup Robustness via Data Selection</h3>
                <p>Authors: Saachi JainKimia HamidiehKristian GeorgievAndrew IlyasMarzyeh GhassemiAleksander Madry</p>
                <p><a href="http://arxiv.org/abs/2406.16846v1">Link to paper</a></p>
                <p>Machine learning models can fail on subgroups that are underrepresentedduring training. While techniques such as dataset balancing can improveperformance on underperforming groups they require access to training groupannotations and can end up removing large portions of the dataset. In thispaper we introduce Data Debiasing with Datamodels D3M a debiasing approachwhich isolates and removes specific training examples that drive the modelsfailures on minority groups. Our approach enables us to efficiently traindebiased classifiers while removing only a small number of examples and doesnot require training group annotations or additional hyperparameter tuning.</p>
                <p>Last Updated: 2024-06-24 17:51:01 UTC</p>
                <button class="interpret-button" data-id="2406.16846v1">Interpret</button>
                <div id="interpretation-2406.16846v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Concentration Inequalities for $(f,Γ)$-GANs</h3>
                <p>Authors: Jeremiah Birrell</p>
                <p><a href="http://arxiv.org/abs/2406.16834v1">Link to paper</a></p>
                <p>Generative adversarial networks GANs are unsupervised learning methods fortraining a generator distribution to produce samples that approximate thosedrawn from a target distribution. Many such methods can be formulated asminimization of a metric or divergence. Recent works have proven thestatistical consistency of GANs that are based on integral probability metricsIPMs e.g. WGAN which is based on the 1-Wasserstein metric. IPMs are definedby optimizing a linear functional difference of expectations over a space ofdiscriminators. A much larger class of GANs which allow for the use ofnonlinear objective functionals can be constructed usingfGamma-divergences these generalize and interpolate between IPMs andf-divergences e.g. KL or alpha-divergences. Instances offGamma-GANs have been shown to exhibit improved performance in a numberof applications. In this work we study the statistical consistency offGamma-GANs for general f and Gamma. Specifically we derivefinite-sample concentration inequalities. These derivations require novelarguments due to nonlinearity of the objective functional. We demonstrate thatour new results reduce to the known results for IPM-GANs in the appropriatelimit while also significantly extending the domain of applicability of thistheory.</p>
                <p>Last Updated: 2024-06-24 17:42:03 UTC</p>
                <button class="interpret-button" data-id="2406.16834v1">Interpret</button>
                <div id="interpretation-2406.16834v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Improved Regret Bounds for Bandits with Expert Advice</h3>
                <p>Authors: Nicolò Cesa-BianchiKhaled EldowaEmmanuel EspositoJulia Olkhovskaya</p>
                <p><a href="http://arxiv.org/abs/2406.16802v1">Link to paper</a></p>
                <p>In this research note we revisit the bandits with expert advice problem.Under a restricted feedback model we prove a lower bound of order sqrtK TlnN/K for the worst-case regret where K is the number of actions NKthe number of experts and T the time horizon. This matches a previouslyknown upper bound of the same order and improves upon the best available lowerbound of sqrtK T ln N / ln K. For the standard feedback model weprove a new instance-based upper bound that depends on the agreement betweenthe experts and provides a logarithmic improvement compared to prior results.</p>
                <p>Last Updated: 2024-06-24 17:14:31 UTC</p>
                <button class="interpret-button" data-id="2406.16802v1">Interpret</button>
                <div id="interpretation-2406.16802v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Conformal time series decomposition with component-wise exchangeability</h3>
                <p>Authors: Derck W. E. PrinzhornThijmen NijdamPutri A. van der LindenAlexander Timans</p>
                <p><a href="http://arxiv.org/abs/2406.16766v1">Link to paper</a></p>
                <p>Conformal prediction offers a practical framework for distribution-freeuncertainty quantification providing finite-sample coverage guarantees underrelatively mild assumptions on data exchangeability. However these assumptionscease to hold for time series due to their temporally correlated nature. Inthis work we present a novel use of conformal prediction for time seriesforecasting that incorporates time series decomposition. This approach allowsus to model different temporal components individually. By applying specificconformal algorithms to each component and then merging the obtained predictionintervals we customize our methods to account for the differentexchangeability regimes underlying each component. Our decomposition-basedapproach is thoroughly discussed and empirically evaluated on synthetic andreal-world data. We find that the method provides promising results onwell-structured time series but can be limited by factors such as thedecomposition step for more complex data.</p>
                <p>Last Updated: 2024-06-24 16:23:30 UTC</p>
                <button class="interpret-button" data-id="2406.16766v1">Interpret</button>
                <div id="interpretation-2406.16766v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Inferring stochastic low-rank recurrent neural networks from neural data</h3>
                <p>Authors: Matthijs PalsA Erdem SağtekinFelix PeiManuel GloecklerJakob H Macke</p>
                <p><a href="http://arxiv.org/abs/2406.16749v1">Link to paper</a></p>
                <p>A central aim in computational neuroscience is to relate the activity oflarge populations of neurons to an underlying dynamical system. Models of theseneural dynamics should ideally be both interpretable and fit the observed datawell. Low-rank recurrent neural networks RNNs exhibit such interpretabilityby having tractable dynamics. However it is unclear how to best fit low-rankRNNs to data consisting of noisy observations of an underlying stochasticsystem. Here we propose to fit stochastic low-rank RNNs with variationalsequential Monte Carlo methods. We validate our method on several datasetsconsisting of both continuous and spiking neural data where we obtain lowerdimensional latent dynamics than current state of the art methods.Additionally for low-rank models with piecewise linear nonlinearities we showhow to efficiently identify all fixed points in polynomial rather thanexponential cost in the number of units making analysis of the inferreddynamics tractable for large RNNs. Our method both elucidates the dynamicalsystems underlying experimental recordings and provides a generative modelwhose trajectories match observed trial-to-trial variability.</p>
                <p>Last Updated: 2024-06-24 15:57:49 UTC</p>
                <button class="interpret-button" data-id="2406.16749v1">Interpret</button>
                <div id="interpretation-2406.16749v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Preserving Real-World Finger Dexterity Using a Lightweight Fingertip Haptic Device for Virtual Dexterous Manipulation</h3>
                <p>Authors: Yunxiu XUSiyu WangShoichi Hasegawa</p>
                <p><a href="http://arxiv.org/abs/2406.16835v1">Link to paper</a></p>
                <p>This study presents a lightweight wearable fingertip haptic device thatprovides physics-based haptic feedback for dexterous manipulation in virtualenvironments without hindering real-world interactions. The devices designutilizes thin strings and actuators attached to the fingernails minimizing theweight 1.76g each finger while preserving finger flexibility. Multiple typesof haptic feedback are simulated by integrating the software with a physicsengine. Experiments evaluate the devices performance in pressure perceptionslip feedback and typical dexterous manipulation tasks. and daily operationswhile subjective assessments gather user experiences. Results demonstrate thatparticipants can perceive and respond to pressure and vibration feedback. Theselimited haptic cues are crucial as they significantly enhance efficiency invirtual dexterous manipulation tasks. The devices ability to preserve tactilesensations and minimize hindrance to real-world operations is a key advantageover glove-type haptic devices. This research offers a potential solution fordesigning haptic interfaces that balance lightweight haptic feedback fordexterous manipulation and daily wearability.</p>
                <p>Last Updated: 2024-06-24 17:44:17 UTC</p>
                <button class="interpret-button" data-id="2406.16835v1">Interpret</button>
                <div id="interpretation-2406.16835v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Digital Human Model for Symptom Progression of Vestibular Motion Sickness based on Subjective Vertical Conflict Theory</h3>
                <p>Authors: Shota InoueHailong LiuTakahiro Wada</p>
                <p><a href="http://arxiv.org/abs/2406.16737v1">Link to paper</a></p>
                <p>Digital human models of motion sickness have been actively developed amongwhich models based on subjective vertical conflict SVC theory are the mostactively studied. These models facilitate the prediction of motion sickness invarious scenarios such as riding in a car. Most SVC theory models predict themotion sickness incidence MSI which is defined as the percentage of peoplewho would vomit with the given specific motion stimulus. However no model hasbeen developed to describe milder forms of discomfort or specific symptoms ofmotion sickness even though predicting milder symptoms is important forapplications in automobiles and daily use vehicles. Therefore the purpose ofthis study was to build a computational model of symptom progression ofvestibular motion sickness based on SVC theory. We focused on a model ofvestibular motion sickness with six degrees-of-freedom 6DoF head motions. Themodel was developed by updating the output part of the state-of-the-art SVCmodel termed the 6DoF-SVC IN1 model from MSI to the MIsery SCale MISCwhich is a subjective rating scale for symptom progression. We conducted anexperiment to measure the progression of motion sickness during a straightfore-aft motion. It was demonstrated that our proposed method with theparameters of the output parts optimized by the experimental results fits wellwith the observed MISC.</p>
                <p>Last Updated: 2024-06-24 15:44:55 UTC</p>
                <button class="interpret-button" data-id="2406.16737v1">Interpret</button>
                <div id="interpretation-2406.16737v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ChatGPT's financial discrimination between rich and poor -- misaligned with human behavior and expectations</h3>
                <p>Authors: Dmitri BershadskyyFlorian E. SachsJoachim Weimann</p>
                <p><a href="http://arxiv.org/abs/2406.16572v1">Link to paper</a></p>
                <p>ChatGPT disrupted the application of machine-learning methods and drasticallyreduced the usage barrier. Chatbots are now widely used in a lot of differentsituations. They provide advice assist in writing source code or assess andsummarize information from various sources. However their scope is not onlylimited to aiding humans they can also be used to take on tasks likenegotiating or bargaining. To understand the implications of Chatbot usage onbargaining situations we conduct a laboratory experiment with the ultimatumgame. In the ultimatum game two human players interact: The receiver decideson accepting or rejecting a monetary offer from the proposer. To shed light onthe new bargaining situation we let ChatGPT provide an offer to a humanplayer. In the novel design we vary the wealth of the receivers. Our resultsindicate that humans have the same beliefs about other humans and chatbots.However our results contradict these beliefs in an important point: Humansfavor poor receivers as correctly anticipated by the humans but ChatGPT favorsrich receivers which the humans did not expect to happen. These results implythat ChatGPTs answers are not aligned with those of humans and that humans donot anticipate this difference.</p>
                <p>Last Updated: 2024-06-24 12:09:34 UTC</p>
                <button class="interpret-button" data-id="2406.16572v1">Interpret</button>
                <div id="interpretation-2406.16572v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PenSLR: Persian end-to-end Sign Language Recognition Using Ensembling</h3>
                <p>Authors: Amirparsa SalmankhahAmirreza RajabiNegin KheirmandAli FadaeimaneshAmirreza TarabkhahAmirreza KazemzadehHamed Farbeh</p>
                <p><a href="http://arxiv.org/abs/2406.16388v1">Link to paper</a></p>
                <p>Sign Language Recognition SLR is a fast-growing field that aims to fill thecommunication gaps between the hearing-impaired and people without hearingloss. Existing solutions for Persian Sign Language PSL are limited toword-level interpretations underscoring the need for more advanced andcomprehensive solutions. Moreover previous work on other languages mainlyfocuses on manipulating the neural network architectures or hardwareconfigurations instead of benefiting from the aggregated results of multiplemodels. In this paper we introduce PenSLR a glove-based sign language systemconsisting of an Inertial Measurement Unit IMU and five flexible sensorspowered by a deep learning framework capable of predicting variable-lengthsequences. We achieve this in an end-to-end manner by leveraging theConnectionist Temporal Classification CTC loss function eliminating the needfor segmentation of input signals. To further enhance its capabilities wepropose a novel ensembling technique by leveraging a multiple sequencealignment algorithm known as Star Alignment. Furthermore we introduce a newPSL dataset including 16 PSL signs with more than 3000 time-series samples intotal. We utilize this dataset to evaluate the performance of our system basedon four word-level and sentence-level metrics. Our evaluations show that PenSLRachieves a remarkable word accuracy of 94.58 and 96.70 in subject-independentand subject-dependent setups respectively. These achievements are attributableto our ensembling algorithm which not only boosts the word-level performanceby 0.51 and 1.32 in the respective scenarios but also yields significantenhancements of 1.46 and 4.00 respectively in sentence-level accuracy.</p>
                <p>Last Updated: 2024-06-24 07:59:34 UTC</p>
                <button class="interpret-button" data-id="2406.16388v1">Interpret</button>
                <div id="interpretation-2406.16388v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Flowy: Supporting UX Design Decisions Through AI-Driven Pattern Annotation in Multi-Screen User Flows</h3>
                <p>Authors: Yuwen LuZiang TongQinyi ZhaoYewon OhBryan WangToby Jia-Jun Li</p>
                <p><a href="http://arxiv.org/abs/2406.16177v1">Link to paper</a></p>
                <p>Many recent AI-powered UX design tools focus on generating individual staticUI screens from natural language. However they overlook the crucial aspect ofinteractions and user experiences across multiple screens. Through formativestudies with UX professionals we identified limitations of these tools insupporting realistic UX design workflows. In response we designed anddeveloped Flowy an app that augments designers information foraging processin ideation by supplementing specific user flow examples with distilled designpattern knowledge. Flowy utilizes large multimodal AI models and a high-qualityuser flow dataset to help designers identify and understand relevant abstractdesign patterns in the design space for multi-screen user flows. Our user studywith professional UX designers demonstrates how Flowy supports realistic UXtasks. Our design considerations in Flowy such as representations withappropriate levels of abstraction and assisted navigation through the solutionspace are generalizable to other creative tasks and embody a human-centeredintelligence augmentation approach to using AI in UX design.</p>
                <p>Last Updated: 2024-06-23 18:09:49 UTC</p>
                <button class="interpret-button" data-id="2406.16177v1">Interpret</button>
                <div id="interpretation-2406.16177v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>StableNormal: Reducing Diffusion Variance for Stable and Sharp Normal</h3>
                <p>Authors: Chongjie YeLingteng QiuXiaodong GuQi ZuoYushuang WuZilong DongLiefeng BoYuliang XiuXiaoguang Han</p>
                <p><a href="http://arxiv.org/abs/2406.16864v1">Link to paper</a></p>
                <p>This work addresses the challenge of high-quality surface normal estimationfrom monocular colored inputs i.e. images and videos a field which hasrecently been revolutionized by repurposing diffusion priors. However previousattempts still struggle with stochastic inference conflicting with thedeterministic nature of the Image2Normal task and costly ensembling stepwhich slows down the estimation process. Our method StableNormal mitigatesthe stochasticity of the diffusion process by reducing inference variance thusproducing Stable-and-Sharp normal estimates without any additional ensemblingprocess. StableNormal works robustly under challenging imaging conditions suchas extreme lighting blurring and low quality. It is also robust againsttransparent and reflective surfaces as well as cluttered scenes with numerousobjects. Specifically StableNormal employs a coarse-to-fine strategy whichstarts with a one-step normal estimator YOSO to derive an initial normalguess that is relatively coarse but reliable then followed by asemantic-guided refinement process SG-DRN that refines the normals to recovergeometric details. The effectiveness of StableNormal is demonstrated throughcompetitive performance in standard datasets such as DIODE-indoor iBimsScannetV2 and NYUv2 and also in various downstream tasks such as surfacereconstruction and normal enhancement. These results evidence that StableNormalretains both the stability and sharpness for accurate normal estimation.StableNormal represents a baby attempt to repurpose diffusion priors fordeterministic estimation. To democratize this code and models have beenpublicly available in hf.co/Stable-X</p>
                <p>Last Updated: 2024-06-24 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2406.16864v1">Interpret</button>
                <div id="interpretation-2406.16864v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models</h3>
                <p>Authors: Jierun ChenFangyun WeiJinjing ZhaoSizhe SongBohuai WuZhuoxuan PengS. -H. Gary ChanHongyang Zhang</p>
                <p><a href="http://arxiv.org/abs/2406.16866v1">Link to paper</a></p>
                <p>Referring expression comprehension REC involves localizing a targetinstance based on a textual description. Recent advancements in REC have beendriven by large multimodal models LMMs like CogVLM which achieved 92.44accuracy on RefCOCO. However this study questions whether existing benchmarkssuch as RefCOCO RefCOCO and RefCOCOg capture LMMs comprehensivecapabilities. We begin with a manual examination of these benchmarks revealinghigh labeling error rates: 14 in RefCOCO 24 in RefCOCO and 5 in RefCOCOgwhich undermines the authenticity of evaluations. We address this by excludingproblematic instances and reevaluating several LMMs capable of handling the RECtask showing significant accuracy improvements thus highlighting the impactof benchmark noise. In response we introduce Ref-L4 a comprehensive RECbenchmark specifically designed to evaluate modern REC models. Ref-L4 isdistinguished by four key features: 1 a substantial sample size with 45341annotations 2 a diverse range of object categories with 365 distinct typesand varying instance scales from 30 to 3767 3 lengthy referring expressionsaveraging 24.2 words and 4 an extensive vocabulary comprising 22813 uniquewords. We evaluate a total of 24 large models on Ref-L4 and provide valuableinsights. The cleaned versions of RefCOCO RefCOCO and RefCOCOg as well asour Ref-L4 benchmark and evaluation code are available athttps://github.com/JierunChen/Ref-L4.</p>
                <p>Last Updated: 2024-06-24 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2406.16866v1">Interpret</button>
                <div id="interpretation-2406.16866v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models</h3>
                <p>Authors: Haonan QiuZhaoxi ChenZhouxia WangYingqing HeMenghan XiaZiwei Liu</p>
                <p><a href="http://arxiv.org/abs/2406.16863v1">Link to paper</a></p>
                <p>Diffusion model has demonstrated remarkable capability in video generationwhich further sparks interest in introducing trajectory control into thegeneration process. While existing works mainly focus on training-based methodse.g. conditional adapter we argue that diffusion model itself allows decentcontrol over the generated content without requiring any training. In thisstudy we introduce a tuning-free framework to achieve trajectory-controllablevideo generation by imposing guidance on both noise construction and attentioncomputation. Specifically 1 we first show several instructive phenomenons andanalyze how initial noises influence the motion trajectory of generatedcontent. 2 Subsequently we propose FreeTraj a tuning-free approach thatenables trajectory control by modifying noise sampling and attentionmechanisms. 3 Furthermore we extend FreeTraj to facilitate longer and largervideo generation with controllable trajectories. Equipped with these designsusers have the flexibility to provide trajectories manually or opt fortrajectories automatically generated by the LLM trajectory planner. Extensiveexperiments validate the efficacy of our approach in enhancing the trajectorycontrollability of video diffusion models.</p>
                <p>Last Updated: 2024-06-24 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2406.16863v1">Interpret</button>
                <div id="interpretation-2406.16863v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Dreamitate: Real-World Visuomotor Policy Learning via Video Generation</h3>
                <p>Authors: Junbang LiangRuoshi LiuEge OzgurogluSruthi SudhakarAchal DavePavel TokmakovShuran SongCarl Vondrick</p>
                <p><a href="http://arxiv.org/abs/2406.16862v1">Link to paper</a></p>
                <p>A key challenge in manipulation is learning a policy that can robustlygeneralize to diverse visual environments. A promising mechanism for learningrobust policies is to leverage video generative models which are pretrained onlarge-scale datasets of internet videos. In this paper we propose a visuomotorpolicy learning framework that fine-tunes a video diffusion model on humandemonstrations of a given task. At test time we generate an example of anexecution of the task conditioned on images of a novel scene and use thissynthesized execution directly to control the robot. Our key insight is thatusing common tools allows us to effortlessly bridge the embodiment gap betweenthe human hand and the robot manipulator. We evaluate our approach on fourtasks of increasing complexity and demonstrate that harnessing internet-scalegenerative models allows the learned policy to achieve a significantly higherdegree of generalization than existing behavior cloning approaches.</p>
                <p>Last Updated: 2024-06-24 17:59:45 UTC</p>
                <button class="interpret-button" data-id="2406.16862v1">Interpret</button>
                <div id="interpretation-2406.16862v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</h3>
                <p>Authors: Shengbang TongEllis BrownPenghao WuSanghyun WooManoj MiddepoguSai Charitha AkulaJihan YangShusheng YangAdithya IyerXichen PanAustin WangRob FergusYann LeCunSaining Xie</p>
                <p><a href="http://arxiv.org/abs/2406.16860v1">Link to paper</a></p>
                <p>We introduce Cambrian-1 a family of multimodal LLMs MLLMs designed with avision-centric approach. While stronger language models can enhance multimodalcapabilities the design choices for vision components are often insufficientlyexplored and disconnected from visual representation learning research. Thisgap hinders accurate sensory grounding in real-world scenarios. Our study usesLLMs and visual instruction tuning as an interface to evaluate various visualrepresentations offering new insights into different models and architectures-- self-supervised strongly supervised or combinations thereof -- based onexperiments with over 20 vision encoders. We critically examine existing MLLMbenchmarks addressing the difficulties involved in consolidating andinterpreting results from various tasks and introduce a new vision-centricbenchmark CV-Bench. To further improve visual grounding we propose theSpatial Vision Aggregator SVA a dynamic and spatially-aware connector thatintegrates high-resolution vision features with LLMs while reducing the numberof tokens. Additionally we discuss the curation of high-quality visualinstruction-tuning data from publicly available sources emphasizing theimportance of data source balancing and distribution ratio. CollectivelyCambrian-1 not only achieves state-of-the-art performance but also serves as acomprehensive open cookbook for instruction-tuned MLLMs. We provide modelweights code supporting tools datasets and detailed instruction-tuning andevaluation recipes. We hope our release will inspire and accelerateadvancements in multimodal systems and visual representation learning.</p>
                <p>Last Updated: 2024-06-24 17:59:42 UTC</p>
                <button class="interpret-button" data-id="2406.16860v1">Interpret</button>
                <div id="interpretation-2406.16860v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>StableNormal: Reducing Diffusion Variance for Stable and Sharp Normal</h3>
                <p>Authors: Chongjie YeLingteng QiuXiaodong GuQi ZuoYushuang WuZilong DongLiefeng BoYuliang XiuXiaoguang Han</p>
                <p><a href="http://arxiv.org/abs/2406.16864v1">Link to paper</a></p>
                <p>This work addresses the challenge of high-quality surface normal estimationfrom monocular colored inputs i.e. images and videos a field which hasrecently been revolutionized by repurposing diffusion priors. However previousattempts still struggle with stochastic inference conflicting with thedeterministic nature of the Image2Normal task and costly ensembling stepwhich slows down the estimation process. Our method StableNormal mitigatesthe stochasticity of the diffusion process by reducing inference variance thusproducing Stable-and-Sharp normal estimates without any additional ensemblingprocess. StableNormal works robustly under challenging imaging conditions suchas extreme lighting blurring and low quality. It is also robust againsttransparent and reflective surfaces as well as cluttered scenes with numerousobjects. Specifically StableNormal employs a coarse-to-fine strategy whichstarts with a one-step normal estimator YOSO to derive an initial normalguess that is relatively coarse but reliable then followed by asemantic-guided refinement process SG-DRN that refines the normals to recovergeometric details. The effectiveness of StableNormal is demonstrated throughcompetitive performance in standard datasets such as DIODE-indoor iBimsScannetV2 and NYUv2 and also in various downstream tasks such as surfacereconstruction and normal enhancement. These results evidence that StableNormalretains both the stability and sharpness for accurate normal estimation.StableNormal represents a baby attempt to repurpose diffusion priors fordeterministic estimation. To democratize this code and models have beenpublicly available in hf.co/Stable-X</p>
                <p>Last Updated: 2024-06-24 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2406.16864v1">Interpret</button>
                <div id="interpretation-2406.16864v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GeoMFormer: A General Architecture for Geometric Molecular Representation Learning</h3>
                <p>Authors: Tianlang ChenShengjie LuoDi HeShuxin ZhengTie-Yan LiuLiwei Wang</p>
                <p><a href="http://arxiv.org/abs/2406.16853v1">Link to paper</a></p>
                <p>Molecular modeling a central topic in quantum mechanics aims to accuratelycalculate the properties and simulate the behaviors of molecular systems. Themolecular model is governed by physical laws which impose geometricconstraints such as invariance and equivariance to coordinate rotation andtranslation. While numerous deep learning approaches have been developed tolearn molecular representations under these constraints most of them are builtupon heuristic and costly modules. We argue that there is a strong need for ageneral and flexible framework for learning both invariant and equivariantfeatures. In this work we introduce a novel Transformer-based molecular modelcalled GeoMFormer to achieve this goal. Using the standard Transformer modulestwo separate streams are developed to maintain and learn invariant andequivariant representations. Carefully designed cross-attention modules bridgethe two streams allowing information fusion and enhancing geometric modelingin each stream. As a general and flexible architecture we show that manyprevious architectures can be viewed as special instantiations of GeoMFormer.Extensive experiments are conducted to demonstrate the power of GeoMFormer. Allempirical results show that GeoMFormer achieves strong performance on bothinvariant and equivariant tasks of different types and scales. Code and modelswill be made publicly available at https://github.com/c-tl/GeoMFormer.</p>
                <p>Last Updated: 2024-06-24 17:58:13 UTC</p>
                <button class="interpret-button" data-id="2406.16853v1">Interpret</button>
                <div id="interpretation-2406.16853v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts</h3>
                <p>Authors: Aditya SharmaMichael SaxonWilliam Yang Wang</p>
                <p><a href="http://arxiv.org/abs/2406.16851v1">Link to paper</a></p>
                <p>We present LoCoVQA a dynamic benchmark generator for evaluating long-contextextractive reasoning in vision language models VLMs. LoCoVQA augments testexamples for mathematical reasoning VQA and character recognition tasks withincreasingly long visual contexts composed of both in-distribution andout-of-distribution distractor images.  Across these tasks a diverse set of VLMs rapidly lose performance as thevisual context length grows often exhibiting a striking exponential decaytrend. This test assesses how well VLMs can ignore irrelevant information whenanswering queries -- a task that is quite easy for language models LMs in thetext domain -- demonstrating that current state-of-the-art VLMs lack thisessential capability for many long-context applications.</p>
                <p>Last Updated: 2024-06-24 17:58:03 UTC</p>
                <button class="interpret-button" data-id="2406.16851v1">Interpret</button>
                <div id="interpretation-2406.16851v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$onversations</h3>
                <p>Authors: Mounika MarreddySubba Reddy OotaVenkata Charan ChinniManish GuptaLucie Flek</p>
                <p><a href="http://arxiv.org/abs/2406.16833v1">Link to paper</a></p>
                <p>Identifying users opinions and stances in long conversation threads onvarious topics can be extremely critical for enhanced personalization marketresearch political campaigns customer service conflict resolution targetedadvertising and content moderation. Hence training language models toautomate this task is critical. However to train such models gathering manualannotations has multiple challenges: 1 It is time-consuming and costly 2Conversation threads could be very long increasing chances of noisyannotations and 3 Interpreting instances where a user changes their opinionwithin a conversation is difficult because often such transitions are subtleand not expressed explicitly. Inspired by the recent success of large languagemodels LLMs for complex natural language processing NLP tasks we leverageMistral Large and GPT-4 to automate the human annotation process on thefollowing two tasks while also providing reasoning: i User Stanceclassification which involves labeling a users stance of a post in aconversation on a five-point scale ii User Dogmatism classification whichdeals with labeling a users overall opinion in the conversation on afour-point scale. The majority voting on zero-shot one-shot and few-shotannotations from these two LLMs on 764 multi-user Reddit conversations helps uscurate the USDC dataset. USDC is then used to finetune and instruction-tunemultiple deployable small language models for the 5-class stance and 4-classdogmatism classification tasks. We make the code and dataset publicly availablehttps://anonymous.4open.science/r/USDC-0F7F.</p>
                <p>Last Updated: 2024-06-24 17:41:53 UTC</p>
                <button class="interpret-button" data-id="2406.16833v1">Interpret</button>
                <div id="interpretation-2406.16833v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Understanding and Mitigating Tokenization Bias in Language Models</h3>
                <p>Authors: Buu PhanMarton HavasiMatthew MuckleyKaren Ullrich</p>
                <p><a href="http://arxiv.org/abs/2406.16829v1">Link to paper</a></p>
                <p>State-of-the-art language models are autoregressive and operate on subwordunits known as tokens. Specifically one must encode the conditioning stringinto a list of tokens before passing to the language models for next-tokenprediction. We show that for encoding schemes such as maximum prefix matchingtokenization induces a sampling bias that cannot be mitigated with moretraining or data. To counter this universal problem we propose a novelalgorithm to obtain unbiased estimates from a model that was trained ontokenized data. Our method does not require finetuning the model and itscomplexity defined as the number of model runs scales linearly with thesequence length. As a consequence we show that one can simulate token-freebehavior from a tokenized language model. We empirically verify the correctnessof our method through a Markov-chain setup where it accurately recovers thetransition probabilities as opposed to the conventional method of directlyprompting tokens into the language model.</p>
                <p>Last Updated: 2024-06-24 17:38:02 UTC</p>
                <button class="interpret-button" data-id="2406.16829v1">Interpret</button>
                <div id="interpretation-2406.16829v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees</h3>
                <p>Authors: Yuhui LiFangyun WeiChao ZhangHongyang Zhang</p>
                <p><a href="http://arxiv.org/abs/2406.16858v1">Link to paper</a></p>
                <p>Inference with modern Large Language Models LLMs is expensive andtime-consuming and speculative sampling has proven to be an effectivesolution. Most speculative sampling methods such as EAGLE use a static drafttree implicitly assuming that the acceptance rate of draft tokens depends onlyon their position. Interestingly we found that the acceptance rate of drafttokens is also context-dependent. In this paper building upon EAGLE wepropose EAGLE-2 which introduces a new technique of context-aware dynamicdraft tree into drafting modeling. This improvement leverages the fact that thedraft model of EAGLE is well-calibrated: the confidence scores from the draftmodel approximate acceptance rates with small errors. We conducted extensiveevaluations on three series of LLMs and six tasks with EAGLE-2 achievingspeedup ratios 3.05x-4.26x which is 20-40 faster than EAGLE-1. EAGLE-2 alsoensures that the distribution of the generated text remains unchanged makingit a lossless acceleration algorithm.</p>
                <p>Last Updated: 2024-06-24 17:59:11 UTC</p>
                <button class="interpret-button" data-id="2406.16858v1">Interpret</button>
                <div id="interpretation-2406.16858v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts</h3>
                <p>Authors: Aditya SharmaMichael SaxonWilliam Yang Wang</p>
                <p><a href="http://arxiv.org/abs/2406.16851v1">Link to paper</a></p>
                <p>We present LoCoVQA a dynamic benchmark generator for evaluating long-contextextractive reasoning in vision language models VLMs. LoCoVQA augments testexamples for mathematical reasoning VQA and character recognition tasks withincreasingly long visual contexts composed of both in-distribution andout-of-distribution distractor images.  Across these tasks a diverse set of VLMs rapidly lose performance as thevisual context length grows often exhibiting a striking exponential decaytrend. This test assesses how well VLMs can ignore irrelevant information whenanswering queries -- a task that is quite easy for language models LMs in thetext domain -- demonstrating that current state-of-the-art VLMs lack thisessential capability for many long-context applications.</p>
                <p>Last Updated: 2024-06-24 17:58:03 UTC</p>
                <button class="interpret-button" data-id="2406.16851v1">Interpret</button>
                <div id="interpretation-2406.16851v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RaTEScore: A Metric for Radiology Report Generation</h3>
                <p>Authors: Weike ZhaoChaoyi WuXiaoman ZhangYa ZhangYanfeng WangWeidi Xie</p>
                <p><a href="http://arxiv.org/abs/2406.16845v1">Link to paper</a></p>
                <p>This paper introduces a novel entity-aware metric termed as RadiologicalReport Text Evaluation RaTEScore to assess the quality of medical reportsgenerated by AI models. RaTEScore emphasizes crucial medical entities such asdiagnostic outcomes and anatomical details and is robust against complexmedical synonyms and sensitive to negation expressions. Technically wedeveloped a comprehensive medical NER dataset RaTE-NER and trained an NERmodel specifically for this purpose. This model enables the decomposition ofcomplex radiological reports into constituent medical entities. The metricitself is derived by comparing the similarity of entity embeddings obtainedfrom a language model based on their types and relevance to clinicalsignificance. Our evaluations demonstrate that RaTEScore aligns more closelywith human preference than existing metrics validated both on establishedpublic benchmarks and our newly proposed RaTE-Eval benchmark.</p>
                <p>Last Updated: 2024-06-24 17:49:28 UTC</p>
                <button class="interpret-button" data-id="2406.16845v1">Interpret</button>
                <div id="interpretation-2406.16845v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Exploring Factual Entailment with NLI: A News Media Study</h3>
                <p>Authors: Guy Mor-LanEffi Levi</p>
                <p><a href="http://arxiv.org/abs/2406.16842v1">Link to paper</a></p>
                <p>We explore the relationship between factuality and Natural Language InferenceNLI by introducing FactRel -- a novel annotation scheme that modelstextitfactual rather than textittextual entailment and use it toannotate a dataset of naturally occurring sentences from news articles. Ouranalysis shows that 84 of factually supporting pairs and 63 of factuallyundermining pairs do not amount to NLI entailment or contradictionrespectively suggesting that factual relationships are more apt for analyzingmedia discourse. We experiment with models for pairwise classification on thenew dataset and find that in some cases generating synthetic data with GPT-4on the basis of the annotated dataset can improve performance. Surprisinglyfew-shot learning with GPT-4 yields strong results on par with medium LMsDeBERTa trained on the labelled dataset. We hypothesize that these resultsindicate the fundamental dependence of this task on both world knowledge andadvanced reasoning abilities.</p>
                <p>Last Updated: 2024-06-24 17:47:55 UTC</p>
                <button class="interpret-button" data-id="2406.16842v1">Interpret</button>
                <div id="interpretation-2406.16842v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models</h3>
                <p>Authors: Sean WelleckAmanda BertschMatthew FinlaysonHailey SchoelkopfAlex XieGraham NeubigIlia KulikovZaid Harchaoui</p>
                <p><a href="http://arxiv.org/abs/2406.16838v1">Link to paper</a></p>
                <p>One of the most striking findings in modern research on large language modelsLLMs is that scaling up compute during training leads to better results.However less attention has been given to the benefits of scaling computeduring inference. This survey focuses on these inference-time approaches. Weexplore three areas under a unified mathematical formalism: token-levelgeneration algorithms meta-generation algorithms and efficient generation.Token-level generation algorithms often called decoding algorithms operate bysampling a single token at a time or constructing a token-level search spaceand then selecting an output. These methods typically assume access to alanguage models logits next-token distributions or probability scores.Meta-generation algorithms work on partial or full sequences incorporatingdomain knowledge enabling backtracking and integrating external information.Efficient generation methods aim to reduce token costs and improve the speed ofgeneration. Our survey unifies perspectives from three research communities:traditional natural language processing modern LLMs and machine learningsystems.</p>
                <p>Last Updated: 2024-06-24 17:45:59 UTC</p>
                <button class="interpret-button" data-id="2406.16838v1">Interpret</button>
                <div id="interpretation-2406.16838v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees</h3>
                <p>Authors: Yuhui LiFangyun WeiChao ZhangHongyang Zhang</p>
                <p><a href="http://arxiv.org/abs/2406.16858v1">Link to paper</a></p>
                <p>Inference with modern Large Language Models LLMs is expensive andtime-consuming and speculative sampling has proven to be an effectivesolution. Most speculative sampling methods such as EAGLE use a static drafttree implicitly assuming that the acceptance rate of draft tokens depends onlyon their position. Interestingly we found that the acceptance rate of drafttokens is also context-dependent. In this paper building upon EAGLE wepropose EAGLE-2 which introduces a new technique of context-aware dynamicdraft tree into drafting modeling. This improvement leverages the fact that thedraft model of EAGLE is well-calibrated: the confidence scores from the draftmodel approximate acceptance rates with small errors. We conducted extensiveevaluations on three series of LLMs and six tasks with EAGLE-2 achievingspeedup ratios 3.05x-4.26x which is 20-40 faster than EAGLE-1. EAGLE-2 alsoensures that the distribution of the generated text remains unchanged makingit a lossless acceleration algorithm.</p>
                <p>Last Updated: 2024-06-24 17:59:11 UTC</p>
                <button class="interpret-button" data-id="2406.16858v1">Interpret</button>
                <div id="interpretation-2406.16858v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GeoMFormer: A General Architecture for Geometric Molecular Representation Learning</h3>
                <p>Authors: Tianlang ChenShengjie LuoDi HeShuxin ZhengTie-Yan LiuLiwei Wang</p>
                <p><a href="http://arxiv.org/abs/2406.16853v1">Link to paper</a></p>
                <p>Molecular modeling a central topic in quantum mechanics aims to accuratelycalculate the properties and simulate the behaviors of molecular systems. Themolecular model is governed by physical laws which impose geometricconstraints such as invariance and equivariance to coordinate rotation andtranslation. While numerous deep learning approaches have been developed tolearn molecular representations under these constraints most of them are builtupon heuristic and costly modules. We argue that there is a strong need for ageneral and flexible framework for learning both invariant and equivariantfeatures. In this work we introduce a novel Transformer-based molecular modelcalled GeoMFormer to achieve this goal. Using the standard Transformer modulestwo separate streams are developed to maintain and learn invariant andequivariant representations. Carefully designed cross-attention modules bridgethe two streams allowing information fusion and enhancing geometric modelingin each stream. As a general and flexible architecture we show that manyprevious architectures can be viewed as special instantiations of GeoMFormer.Extensive experiments are conducted to demonstrate the power of GeoMFormer. Allempirical results show that GeoMFormer achieves strong performance on bothinvariant and equivariant tasks of different types and scales. Code and modelswill be made publicly available at https://github.com/c-tl/GeoMFormer.</p>
                <p>Last Updated: 2024-06-24 17:58:13 UTC</p>
                <button class="interpret-button" data-id="2406.16853v1">Interpret</button>
                <div id="interpretation-2406.16853v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Data Debiasing with Datamodels (D3M): Improving Subgroup Robustness via Data Selection</h3>
                <p>Authors: Saachi JainKimia HamidiehKristian GeorgievAndrew IlyasMarzyeh GhassemiAleksander Madry</p>
                <p><a href="http://arxiv.org/abs/2406.16846v1">Link to paper</a></p>
                <p>Machine learning models can fail on subgroups that are underrepresentedduring training. While techniques such as dataset balancing can improveperformance on underperforming groups they require access to training groupannotations and can end up removing large portions of the dataset. In thispaper we introduce Data Debiasing with Datamodels D3M a debiasing approachwhich isolates and removes specific training examples that drive the modelsfailures on minority groups. Our approach enables us to efficiently traindebiased classifiers while removing only a small number of examples and doesnot require training group annotations or additional hyperparameter tuning.</p>
                <p>Last Updated: 2024-06-24 17:51:01 UTC</p>
                <button class="interpret-button" data-id="2406.16846v1">Interpret</button>
                <div id="interpretation-2406.16846v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models</h3>
                <p>Authors: Sean WelleckAmanda BertschMatthew FinlaysonHailey SchoelkopfAlex XieGraham NeubigIlia KulikovZaid Harchaoui</p>
                <p><a href="http://arxiv.org/abs/2406.16838v1">Link to paper</a></p>
                <p>One of the most striking findings in modern research on large language modelsLLMs is that scaling up compute during training leads to better results.However less attention has been given to the benefits of scaling computeduring inference. This survey focuses on these inference-time approaches. Weexplore three areas under a unified mathematical formalism: token-levelgeneration algorithms meta-generation algorithms and efficient generation.Token-level generation algorithms often called decoding algorithms operate bysampling a single token at a time or constructing a token-level search spaceand then selecting an output. These methods typically assume access to alanguage models logits next-token distributions or probability scores.Meta-generation algorithms work on partial or full sequences incorporatingdomain knowledge enabling backtracking and integrating external information.Efficient generation methods aim to reduce token costs and improve the speed ofgeneration. Our survey unifies perspectives from three research communities:traditional natural language processing modern LLMs and machine learningsystems.</p>
                <p>Last Updated: 2024-06-24 17:45:59 UTC</p>
                <button class="interpret-button" data-id="2406.16838v1">Interpret</button>
                <div id="interpretation-2406.16838v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Concentration Inequalities for $(f,Γ)$-GANs</h3>
                <p>Authors: Jeremiah Birrell</p>
                <p><a href="http://arxiv.org/abs/2406.16834v1">Link to paper</a></p>
                <p>Generative adversarial networks GANs are unsupervised learning methods fortraining a generator distribution to produce samples that approximate thosedrawn from a target distribution. Many such methods can be formulated asminimization of a metric or divergence. Recent works have proven thestatistical consistency of GANs that are based on integral probability metricsIPMs e.g. WGAN which is based on the 1-Wasserstein metric. IPMs are definedby optimizing a linear functional difference of expectations over a space ofdiscriminators. A much larger class of GANs which allow for the use ofnonlinear objective functionals can be constructed usingfGamma-divergences these generalize and interpolate between IPMs andf-divergences e.g. KL or alpha-divergences. Instances offGamma-GANs have been shown to exhibit improved performance in a numberof applications. In this work we study the statistical consistency offGamma-GANs for general f and Gamma. Specifically we derivefinite-sample concentration inequalities. These derivations require novelarguments due to nonlinearity of the objective functional. We demonstrate thatour new results reduce to the known results for IPM-GANs in the appropriatelimit while also significantly extending the domain of applicability of thistheory.</p>
                <p>Last Updated: 2024-06-24 17:42:03 UTC</p>
                <button class="interpret-button" data-id="2406.16834v1">Interpret</button>
                <div id="interpretation-2406.16834v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Towards Physically Talented Aerial Robots with Tactically Smart Swarm Behavior thereof: An Efficient Co-design Approach</h3>
                <p>Authors: Prajit KrisshnaKumarSteve PaulHemanth ManjunathaMary CorraEhsan EsfahaniSouma Chowdhury</p>
                <p><a href="http://arxiv.org/abs/2406.16612v1">Link to paper</a></p>
                <p>The collective performance or capacity of collaborative autonomous systemssuch as a swarm of robots is jointly influenced by the morphology and thebehavior of individual systems in that collective. In that context this paperexplores how morphology impacts the learned tactical behavior of unmannedaerial/ground robots performing reconnaissance and search  rescue. This isachieved by presenting a computationally efficient framework to solve thisotherwise challenging problem of jointly optimizing the morphology and tacticalbehavior of swarm robots. Key novel developments to this end include the use ofphysical talent metrics and modification of graph reinforcement learningarchitectures to allow joint learning of the swarm tactical policy and thetalent metrics search speed flight range and cruising speed that constrainmobility and object/victim search capabilities of the aerial robots executingthese tactics. Implementation of this co-design approach is supported byadvancements to an open-source Pybullet-based swarm simulator that allows theuse of variable aerial asset capabilities. The results of the co-design areobserved to outperform those of tactics learning with a fixed Pareto designwhen compared in terms of mission performance metrics. Significant differencesin morphology and learned behavior are also observed by comparing the baselinedesign and the co-design outcomes.</p>
                <p>Last Updated: 2024-06-24 12:52:17 UTC</p>
                <button class="interpret-button" data-id="2406.16612v1">Interpret</button>
                <div id="interpretation-2406.16612v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TornadoDrone: Bio-inspired DRL-based Drone Landing on 6D Platform with Wind Force Disturbances</h3>
                <p>Authors: Robinroy PeterLavanya RatnabalaDemetros AschuAleksey FedoseevDzmitry Tsetserukou</p>
                <p><a href="http://arxiv.org/abs/2406.16164v1">Link to paper</a></p>
                <p>Autonomous drone navigation faces a critical challenge in achieving accuratelandings on dynamic platforms especially under unpredictable conditions suchas wind turbulence. Our research introduces TornadoDrone a novel DeepReinforcement Learning DRL model that adopts bio-inspired mechanisms to adaptto wind forces mirroring the natural adaptability seen in birds. This modelunlike traditional approaches derives its adaptability from indirect cues suchas changes in position and velocity rather than direct wind forcemeasurements. TornadoDrone was rigorously trained in the gym-pybullet-dronesimulator which closely replicates the complexities of wind dynamics in thereal world. Through extensive testing with Crazyflie 2.1 drones in bothsimulated and real windy conditions TornadoDrone demonstrated a highperformance in maintaining high-precision landing accuracy on moving platformssurpassing conventional control methods such as PID controllers with ExtendedKalman Filters. The study not only highlights the potential of DRL to tacklecomplex aerodynamic challenges but also paves the way for advanced autonomoussystems that can adapt to environmental changes in real-time. The success ofTornadoDrone signifies a leap forward in drone technology particularly forcritical applications such as surveillance and emergency response wherereliability and precision are paramount.</p>
                <p>Last Updated: 2024-06-23 16:54:07 UTC</p>
                <button class="interpret-button" data-id="2406.16164v1">Interpret</button>
                <div id="interpretation-2406.16164v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models</h3>
                <p>Authors: Yang ZhangChenjia BaiBin ZhaoJunchi YanXiu LiXuelong Li</p>
                <p><a href="http://arxiv.org/abs/2406.15836v1">Link to paper</a></p>
                <p>Learning a world model for model-free Reinforcement Learning RL agents cansignificantly improve the sample efficiency by learning policies inimagination. However building a world model for Multi-Agent RL MARL can beparticularly challenging due to the scalability issue in a centralizedarchitecture arising from a large number of agents and also thenon-stationarity issue in a decentralized architecture stemming from theinter-dependency among agents. To address both challenges we propose a novelworld model for MARL that learns decentralized local dynamics for scalabilitycombined with a centralized representation aggregation from all agents. We castthe dynamics learning as an auto-regressive sequence modeling problem overdiscrete tokens by leveraging the expressive Transformer architecture in orderto model complex local dynamics across different agents and provide accurateand consistent long-term imaginations. As the first pioneeringTransformer-based world model for multi-agent systems we introduce a PerceiverTransformer as an effective solution to enable centralized representationaggregation within this context. Results on Starcraft Multi-Agent ChallengeSMAC show that it outperforms strong model-free approaches and existingmodel-based methods in both sample efficiency and overall performance.</p>
                <p>Last Updated: 2024-06-22 12:40:03 UTC</p>
                <button class="interpret-button" data-id="2406.15836v1">Interpret</button>
                <div id="interpretation-2406.15836v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Contextual Sprint Classification in Soccer Based on Deep Learning</h3>
                <p>Authors: Hyunsung KimGun-Hee JoeJinsung YoonSang-Ki Ko</p>
                <p><a href="http://arxiv.org/abs/2406.15659v1">Link to paper</a></p>
                <p>The analysis of high-intensity runs or sprints in soccer has long been atopic of interest for sports science researchers and practitioners. Inparticular recent studies suggested contextualizing sprints based on theirtactical purposes to better understand the physical-tactical requirements ofmodern match-play. However they have a limitation in scalability as humanexperts have to manually classify hundreds of sprints for every match. Toaddress this challenge this paper proposes a deep learning framework forautomatically classifying sprints in soccer into contextual categories. Theproposed model covers the permutation-invariant and sequential nature ofmulti-agent trajectories in soccer by deploying Set Transformers and abidirectional GRU. We train the model with category labels made through thecollaboration of human annotators and a rule-based classifier. Experimentalresults show that our model classifies sprints in the test dataset into 15categories with the accuracy of 77.65 implying the potential of the proposedframework for facilitating the integrated analysis of soccer sprints at scale.</p>
                <p>Last Updated: 2024-06-21 21:33:51 UTC</p>
                <button class="interpret-button" data-id="2406.15659v1">Interpret</button>
                <div id="interpretation-2406.15659v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Phase-Bounded Broadcast Networks over Topologies of Communication</h3>
                <p>Authors: Lucie GuillouArnaud SangnierNathalie Sznajder</p>
                <p><a href="http://arxiv.org/abs/2406.15202v1">Link to paper</a></p>
                <p>We study networks of processes that all execute the same finite stateprotocol and that communicate through broadcasts. The processes are organizedin a graph a topology and only the neighbors of a process in this graph canreceive its broadcasts. The coverability problem asks given a protocol and astate of the protocol whether there is a topology for the processes such thatone of them at least reaches the given state. This problem is undecidable. Westudy here an under-approximation of the problem where processes alternate abounded number of times k between phases of broadcasting and phases ofreceiving messages. We show that if the problem remains undecidable when kis greater than 6 it becomes decidable for k2 and EXPSPACE-complete fork1. Furthermore we show that if we restrict ourselves to line topologiesthe problem is in P for k1 and k2.</p>
                <p>Last Updated: 2024-06-21 14:43:23 UTC</p>
                <button class="interpret-button" data-id="2406.15202v1">Interpret</button>
                <div id="interpretation-2406.15202v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-06-25</p>
        </div>
    
        </div>
    </body>
    </html>
    