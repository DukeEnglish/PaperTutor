
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation</h3>
                <p>Authors: Raphael TangXinyu ZhangLixinyu XuYao LuWenyan LiPontus StenetorpJimmy LinFerhan Ture</p>
                <p><a href="http://arxiv.org/abs/2406.08482v1">Link to paper</a></p>
                <p>Diffusion models are the state of the art in text-to-image generation buttheir perceptual variability remains understudied. In this paper we examinehow prompts affect image variability in black-box diffusion-based models. Wepropose W1KP a human-calibrated measure of variability in a set of imagesbootstrapped from existing image-pair perceptual distances. Current datasets donot cover recent diffusion models thus we curate three test sets forevaluation. Our best perceptual distance outperforms nine baselines by up to 18points in accuracy and our calibration matches graded human judgements 78 ofthe time. Using W1KP we study prompt reusability and show that Imagen promptscan be reused for 10-50 random seeds before new images become too similar toalready generated images while Stable Diffusion XL and DALL-E 3 can be reused50-200 times. Lastly we analyze 56 linguistic features of real promptsfinding that the prompts length CLIP embedding norm concreteness and wordsenses influence variability most. As far as we are aware we are the first toanalyze diffusion variability from a visuolinguistic perspective. Our projectpage is at http://w1kp.com</p>
                <p>Last Updated: 2024-06-12 17:59:27 UTC</p>
                <button class="interpret-button" data-id="2406.08482v1">Interpret</button>
                <div id="interpretation-2406.08482v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>What If We Recaption Billions of Web Images with LLaMA-3?</h3>
                <p>Authors: Xianhang LiHaoqin TuMude HuiZeyu WangBingchen ZhaoJunfei XiaoSucheng RenJieru MeiQing LiuHuangjie ZhengYuyin ZhouCihang Xie</p>
                <p><a href="http://arxiv.org/abs/2406.08478v1">Link to paper</a></p>
                <p>Web-crawled image-text pairs are inherently noisy. Prior studies demonstratethat semantically aligning and enriching textual descriptions of these pairscan significantly enhance model training across various vision-language tasksparticularly text-to-image generation. However large-scale investigations inthis area remain predominantly closed-source. Our paper aims to bridge thiscommunity effort leveraging the powerful and textitopen-sourced LLaMA-3 aGPT-4 level LLM. Our recaptioning pipeline is simple: first we fine-tune aLLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion imagesfrom the DataComp-1B dataset. Our empirical results confirm that this enhanceddataset Recap-DataComp-1B offers substantial benefits in training advancedvision-language models. For discriminative models like CLIP we observeenhanced zero-shot performance in cross-modal retrieval tasks. For generativemodels like text-to-image Diffusion Transformers the generated images exhibita significant improvement in alignment with users text instructionsespecially in following complex queries. Our project page ishttps://www.haqtu.me/Recap-Datacomp-1B/</p>
                <p>Last Updated: 2024-06-12 17:59:07 UTC</p>
                <button class="interpret-button" data-id="2406.08478v1">Interpret</button>
                <div id="interpretation-2406.08478v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing</h3>
                <p>Authors: Zhangchen XuFengqing JiangLuyao NiuYuntian DengRadha PoovendranYejin ChoiBill Yuchen Lin</p>
                <p><a href="http://arxiv.org/abs/2406.08464v1">Link to paper</a></p>
                <p>High-quality instruction data is critical for aligning large language modelsLLMs. Although some models such as Llama-3-Instruct have open weightstheir alignment data remain private which hinders the democratization of AI.High human labor costs and a limited predefined scope for prompting preventexisting open-source data creation methods from scaling effectivelypotentially limiting the diversity and quality of public alignment datasets. Isit possible to synthesize high-quality instruction data at scale by extractingit directly from an aligned LLM We present a self-synthesis method forgenerating large-scale alignment data named Magpie. Our key observation is thataligned LLMs like Llama-3-Instruct can generate a user query when we input onlythe left-side templates up to the position reserved for user messages thanksto their auto-regressive nature. We use this method to prompt Llama-3-Instructand generate 4 million instructions along with their corresponding responses.We perform a comprehensive analysis of the extracted data and select 300Khigh-quality instances. To compare Magpie data with other public instructiondatasets we fine-tune Llama-3-8B-Base with each dataset and evaluate theperformance of the fine-tuned models. Our results indicate that in some tasksmodels fine-tuned with Magpie perform comparably to the officialLlama-3-8B-Instruct despite the latter being enhanced with 10 million datapoints through supervised fine-tuning SFT and subsequent feedback learning.We also show that using Magpie solely for SFT can surpass the performance ofprevious public datasets utilized for both SFT and preference optimizationsuch as direct preference optimization with UltraFeedback. This advantage isevident on alignment benchmarks such as AlpacaEval ArenaHard and WildBench.</p>
                <p>Last Updated: 2024-06-12 17:52:30 UTC</p>
                <button class="interpret-button" data-id="2406.08464v1">Interpret</button>
                <div id="interpretation-2406.08464v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Impact of Initialization on LoRA Finetuning Dynamics</h3>
                <p>Authors: Soufiane HayouNikhil GhoshBin Yu</p>
                <p><a href="http://arxiv.org/abs/2406.08447v1">Link to paper</a></p>
                <p>In this paper we study the role of initialization in Low Rank AdaptationLoRA as originally introduced in Hu et al. 2021. Essentially to start fromthe pretrained model as initialization for finetuning one can eitherinitialize B to zero and A to random default initialization in PEFT packageor vice-versa. In both cases the product BA is equal to zero atinitialization which makes finetuning starts from the pretrained model. Thesetwo initialization schemes are seemingly similar. They should in-principleyield the same performance and share the same optimal learning rate. Wedemonstrate that this is an incorrect intuition and that the first schemeinitializing B to zero and A to random on average yields better performancecompared to the other scheme. Our theoretical analysis shows that the reasonbehind this might be that the first initialization allows the use of largerlearning rates without causing output instability compared to the secondinitialization resulting in more efficient learning of the first scheme. Wevalidate our results with extensive experiments on LLMs.</p>
                <p>Last Updated: 2024-06-12 17:38:20 UTC</p>
                <button class="interpret-button" data-id="2406.08447v1">Interpret</button>
                <div id="interpretation-2406.08447v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>OLMES: A Standard for Language Model Evaluations</h3>
                <p>Authors: Yuling GuOyvind TafjordBailey KuehlDany HaddadJesse DodgeHannaneh Hajishirzi</p>
                <p><a href="http://arxiv.org/abs/2406.08446v1">Link to paper</a></p>
                <p>Progress in AI is often demonstrated by new models claiming improvedperformance on tasks measuring model capabilities. Evaluating language modelsin particular is challenging as small changes to how a model is evaluated on atask can lead to large changes in measured performance. There is no commonstandard setup so different models are evaluated on the same tasks indifferent ways leading to claims about which models perform best not beingreproducible. We propose OLMES a completely documented practical openstandard for reproducible LLM evaluations. In developing this standard weidentify and review the varying factors in evaluation practices adopted by thecommunity - such as details of prompt formatting choice of in-contextexamples probability normalizations and task formulation. In particularOLMES supports meaningful comparisons between smaller base models that requirethe unnatural cloze formulation of multiple-choice questions against largermodels that can utilize the original formulation. OLMES includeswell-considered recommendations guided by results from existing literature aswell as new experiments investigating open questions.</p>
                <p>Last Updated: 2024-06-12 17:37:09 UTC</p>
                <button class="interpret-button" data-id="2406.08446v1">Interpret</button>
                <div id="interpretation-2406.08446v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Scaling Laws in Linear Regression: Compute, Parameters, and Data</h3>
                <p>Authors: Licong LinJingfeng WuSham M. KakadePeter L. BartlettJason D. Lee</p>
                <p><a href="http://arxiv.org/abs/2406.08466v1">Link to paper</a></p>
                <p>Empirically large-scale deep learning models often satisfy a neural scalinglaw: the test error of the trained model improves polynomially as the modelsize and data size grow. However conventional wisdom suggests the test errorconsists of approximation bias and variance errors where the variance errorincreases with model size. This disagrees with the general form of neuralscaling laws which predict that increasing model size monotonically improvesperformance.  We study the theory of scaling laws in an infinite dimensional linearregression setup. Specifically we consider a model with M parameters as alinear function of sketched covariates. The model is trained by one-passstochastic gradient descent SGD using N data. Assuming the optimalparameter satisfies a Gaussian prior and the data covariance matrix has apower-law spectrum of degree a1 we show that the reducible part of the testerror is ThetaM-a-1  N-a-1/a. The variance error whichincreases with M is dominated by the other errors due to the implicitregularization of SGD thus disappearing from the bound. Our theory isconsistent with the empirical neural scaling laws and verified by numericalsimulation.</p>
                <p>Last Updated: 2024-06-12 17:53:29 UTC</p>
                <button class="interpret-button" data-id="2406.08466v1">Interpret</button>
                <div id="interpretation-2406.08466v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Impact of Initialization on LoRA Finetuning Dynamics</h3>
                <p>Authors: Soufiane HayouNikhil GhoshBin Yu</p>
                <p><a href="http://arxiv.org/abs/2406.08447v1">Link to paper</a></p>
                <p>In this paper we study the role of initialization in Low Rank AdaptationLoRA as originally introduced in Hu et al. 2021. Essentially to start fromthe pretrained model as initialization for finetuning one can eitherinitialize B to zero and A to random default initialization in PEFT packageor vice-versa. In both cases the product BA is equal to zero atinitialization which makes finetuning starts from the pretrained model. Thesetwo initialization schemes are seemingly similar. They should in-principleyield the same performance and share the same optimal learning rate. Wedemonstrate that this is an incorrect intuition and that the first schemeinitializing B to zero and A to random on average yields better performancecompared to the other scheme. Our theoretical analysis shows that the reasonbehind this might be that the first initialization allows the use of largerlearning rates without causing output instability compared to the secondinitialization resulting in more efficient learning of the first scheme. Wevalidate our results with extensive experiments on LLMs.</p>
                <p>Last Updated: 2024-06-12 17:38:20 UTC</p>
                <button class="interpret-button" data-id="2406.08447v1">Interpret</button>
                <div id="interpretation-2406.08447v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Nyström Kernel Stein Discrepancy</h3>
                <p>Authors: Florian KalinkeZoltan SzaboBharath K. Sriperumbudur</p>
                <p><a href="http://arxiv.org/abs/2406.08401v1">Link to paper</a></p>
                <p>Kernel methods underpin many of the most successful approaches in datascience and statistics and they allow representing probability measures aselements of a reproducing kernel Hilbert space without loss of information.Recently the kernel Stein discrepancy KSD which combines Steins methodwith kernel techniques gained considerable attention. Through the Steinoperator KSD allows the construction of powerful goodness-of-fit tests whereit is sufficient to know the target distribution up to a multiplicativeconstant. However the typical U- and V-statistic-based KSD estimators sufferfrom a quadratic runtime complexity which hinders their application inlarge-scale settings. In this work we propose a Nystrom-based KSDacceleration -- with runtime mathcal Oleftmnm3right for n samplesand mll n Nystrom points --  show its sqrtn-consistency under thenull with a classical sub-Gaussian assumption and demonstrate itsapplicability for goodness-of-fit testing on a suite of benchmarks.</p>
                <p>Last Updated: 2024-06-12 16:50:12 UTC</p>
                <button class="interpret-button" data-id="2406.08401v1">Interpret</button>
                <div id="interpretation-2406.08401v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Differentiable Cost-Parameterized Monge Map Estimators</h3>
                <p>Authors: Samuel HowardGeorge DeligiannidisPatrick RebeschiniJames Thornton</p>
                <p><a href="http://arxiv.org/abs/2406.08399v1">Link to paper</a></p>
                <p>Within the field of optimal transport OT the choice of ground cost iscrucial to ensuring that the optimality of a transport map corresponds tousefulness in real-world applications. It is therefore desirable to use knowninformation to tailor cost functions and hence learn OT maps which are adaptedto the problem at hand. By considering a class of neural ground costs whoseMonge maps have a known form we construct a differentiable Monge map estimatorwhich can be optimized to be consistent with known information about an OT map.In doing so we simultaneously learn both an OT map estimator and acorresponding adapted cost function. Through suitable choices of loss functionour method provides a general approach for incorporating prior informationabout the Monge map itself when learning adapted OT maps and cost functions.</p>
                <p>Last Updated: 2024-06-12 16:47:54 UTC</p>
                <button class="interpret-button" data-id="2406.08399v1">Interpret</button>
                <div id="interpretation-2406.08399v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Large Language Models Must Be Taught to Know What They Don't Know</h3>
                <p>Authors: Sanyam KapoorNate GruverManley RobertsKatherine CollinsArka PalUmang BhattAdrian WellerSamuel DooleyMicah GoldblumAndrew Gordon Wilson</p>
                <p><a href="http://arxiv.org/abs/2406.08391v1">Link to paper</a></p>
                <p>When using large language models LLMs in high-stakes applications we needto know when we can trust their predictions. Some works argue that promptinghigh-performance LLMs is sufficient to produce calibrated uncertainties whileothers introduce sampling methods that can be prohibitively expensive. In thiswork we first argue that prompting on its own is insufficient to achieve goodcalibration and then show that fine-tuning on a small dataset of correct andincorrect answers can create an uncertainty estimate with good generalizationand small computational overhead. We show that a thousand graded examples aresufficient to outperform baseline methods and that training through thefeatures of a model is necessary for good performance and tractable for largeopen-source models when using LoRA. We also investigate the mechanisms thatenable reliable LLM uncertainty estimation finding that many models can beused as general-purpose uncertainty estimators applicable not just to theirown uncertainties but also the uncertainty of other models. Lastly we showthat uncertainty estimates inform human use of LLMs in human-AI collaborativesettings through a user study.</p>
                <p>Last Updated: 2024-06-12 16:41:31 UTC</p>
                <button class="interpret-button" data-id="2406.08391v1">Interpret</button>
                <div id="interpretation-2406.08391v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>ICE-G: Image Conditional Editing of 3D Gaussian Splats</h3>
                <p>Authors: Vishnu JaganathanHannah Hanyun HuangMuhammad Zubair IrshadVarun JampaniAmit RajZsolt Kira</p>
                <p><a href="http://arxiv.org/abs/2406.08488v1">Link to paper</a></p>
                <p>Recently many techniques have emerged to create high quality 3D assets andscenes. When it comes to editing of these objects however existing approachesare either slow compromise on quality or do not provide enough customization.We introduce a novel approach to quickly edit a 3D model from a singlereference view. Our technique first segments the edit image and then matchessemantically corresponding regions across chosen segmented dataset views usingDINO features. A color or texture change from a particular region of the editimage can then be applied to other views automatically in a semanticallysensible manner. These edited views act as an updated dataset to further trainand re-style the 3D scene. The end-result is therefore an edited 3D model. Ourframework enables a wide variety of editing tasks such as manual local editscorrespondence based style transfer from any example image and a combinationof different styles from multiple example images. We use Gaussian Splats as ourprimary 3D representation due to their speed and ease of local editing but ourtechnique works for other methods such as NeRFs as well. We show throughmultiple examples that our method produces higher quality results whileoffering fine-grained control of editing. Project page: ice-gaussian.github.io</p>
                <p>Last Updated: 2024-06-12 17:59:52 UTC</p>
                <button class="interpret-button" data-id="2406.08488v1">Interpret</button>
                <div id="interpretation-2406.08488v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models</h3>
                <p>Authors: Yi-Fan ZhangQingsong WenChaoyou FuXue WangZhang ZhangLiang WangRong Jin</p>
                <p><a href="http://arxiv.org/abs/2406.08487v1">Link to paper</a></p>
                <p>Seeing clearly with high resolution is a foundation of Large MultimodalModels LMMs which has been proven to be vital for visual perception andreasoning. Existing works usually employ a straightforward resolution upscalingmethod where the image consists of global and local branches with the latterbeing the sliced image patches but resized to the same resolution as theformer. This means that higher resolution requires more local patchesresulting in exorbitant computational expenses and meanwhile the dominance oflocal image tokens may diminish the global context. In this paper we dive intothe problems and propose a new framework as well as an elaborate optimizationstrategy. Specifically we extract contextual information from the global viewusing a mixture of adapters based on the observation that different adaptersexcel at different tasks. With regard to local patches learnable queryembeddings are introduced to reduce image tokens the most important tokensaccounting for the user question will be further selected by a similarity-basedselector. Our empirical results demonstrate a less is more pattern wheretextitutilizing fewer but more informative local image tokens leads toimproved performance. Besides a significant challenge lies in the trainingstrategy as simultaneous end-to-end training of the global mining block andlocal compression block does not yield optimal results. We thus advocate for analternating training way ensuring balanced learning between global and localaspects. Finally we also introduce a challenging dataset with highrequirements for image detail enhancing the training of the local compressionlayer. The proposed method termed LMM with Sophisticated Tasks Local imagecompression and Mixture of global Experts SliME achieves leadingperformance across various benchmarks with only 2 million training data.</p>
                <p>Last Updated: 2024-06-12 17:59:49 UTC</p>
                <button class="interpret-button" data-id="2406.08487v1">Interpret</button>
                <div id="interpretation-2406.08487v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On Evaluating Adversarial Robustness of Volumetric Medical Segmentation Models</h3>
                <p>Authors: Hashmat Shadab MalikNuman SaeedAsif HanifMuzammal NaseerMohammad YaqubSalman KhanFahad Shahbaz Khan</p>
                <p><a href="http://arxiv.org/abs/2406.08486v1">Link to paper</a></p>
                <p>Volumetric medical segmentation models have achieved significant success onorgan and tumor-based segmentation tasks in recent years. However theirvulnerability to adversarial attacks remains largely unexplored raisingserious concerns regarding the real-world deployment of tools employing suchmodels in the healthcare sector. This underscores the importance ofinvestigating the robustness of existing models. In this context our work aimsto empirically examine the adversarial robustness across current volumetricsegmentation architectures encompassing Convolutional Transformer andMamba-based models. We extend this investigation across four volumetricsegmentation datasets evaluating robustness under both white box and black boxadversarial attacks. Overall we observe that while both pixel andfrequency-based attacks perform reasonably well under white box setting thelatter performs significantly better under transfer-based black box attacks.Across our experiments we observe transformer-based models show higherrobustness than convolution-based models with Mamba-based models being the mostvulnerable. Additionally we show that large-scale training of volumetricsegmentation models improves the models robustness against adversarialattacks. The code and pretrained models will be made available athttps://github.com/HashmatShadab/Robustness-of-Volumetric-Medical-Segmentation-Models.</p>
                <p>Last Updated: 2024-06-12 17:59:42 UTC</p>
                <button class="interpret-button" data-id="2406.08486v1">Interpret</button>
                <div id="interpretation-2406.08486v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation</h3>
                <p>Authors: Raphael TangXinyu ZhangLixinyu XuYao LuWenyan LiPontus StenetorpJimmy LinFerhan Ture</p>
                <p><a href="http://arxiv.org/abs/2406.08482v1">Link to paper</a></p>
                <p>Diffusion models are the state of the art in text-to-image generation buttheir perceptual variability remains understudied. In this paper we examinehow prompts affect image variability in black-box diffusion-based models. Wepropose W1KP a human-calibrated measure of variability in a set of imagesbootstrapped from existing image-pair perceptual distances. Current datasets donot cover recent diffusion models thus we curate three test sets forevaluation. Our best perceptual distance outperforms nine baselines by up to 18points in accuracy and our calibration matches graded human judgements 78 ofthe time. Using W1KP we study prompt reusability and show that Imagen promptscan be reused for 10-50 random seeds before new images become too similar toalready generated images while Stable Diffusion XL and DALL-E 3 can be reused50-200 times. Lastly we analyze 56 linguistic features of real promptsfinding that the prompts length CLIP embedding norm concreteness and wordsenses influence variability most. As far as we are aware we are the first toanalyze diffusion variability from a visuolinguistic perspective. Our projectpage is at http://w1kp.com</p>
                <p>Last Updated: 2024-06-12 17:59:27 UTC</p>
                <button class="interpret-button" data-id="2406.08482v1">Interpret</button>
                <div id="interpretation-2406.08482v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Enhancing End-to-End Autonomous Driving with Latent World Model</h3>
                <p>Authors: Yingyan LiLue FanJiawei HeYuqi WangYuntao ChenZhaoxiang ZhangTieniu Tan</p>
                <p><a href="http://arxiv.org/abs/2406.08481v1">Link to paper</a></p>
                <p>End-to-end autonomous driving has garnered widespread attention. Currentend-to-end approaches largely rely on the supervision from perception taskssuch as detection tracking and map segmentation to aid in learning scenerepresentations. However these methods require extensive annotationshindering the data scalability. To address this challenge we propose a novelself-supervised method to enhance end-to-end driving without the need forcostly labels. Specifically our framework textbfLAW uses a LAtent Worldmodel to predict future latent features based on the predicted ego actions andthe latent feature of the current frame. The predicted latent features aresupervised by the actually observed features in the future. This supervisionjointly optimizes the latent feature learning and action prediction whichgreatly enhances the driving performance. As a result our approach achievesstate-of-the-art performance in both open-loop and closed-loop benchmarkswithout costly annotations.</p>
                <p>Last Updated: 2024-06-12 17:59:21 UTC</p>
                <button class="interpret-button" data-id="2406.08481v1">Interpret</button>
                <div id="interpretation-2406.08481v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Bridging the Gap: Unravelling Local Government Data Sharing Barriers in Estonia and Beyond</h3>
                <p>Authors: Katrin Rajamäe SoosaarAnastasija Nikiforova</p>
                <p><a href="http://arxiv.org/abs/2406.08461v1">Link to paper</a></p>
                <p>Estonias digital government success has received global acclaim yet itsOpen Government Data OGD initiatives especially at the local levelencounter persistent challenges. Despite significant progress of national OGDinitiative in OGD rankings local governments lag in OGD provision. This studyaims to examine barriers hindering municipalities from openly sharing OGD.Employing a qualitative approach through interviews with Estonianmunicipalities and drawing on the OGD-adapted Innovation Resistance Theorymodel the study sheds light on barriers impeding OGD sharing. Practicalrecommendations are proposed to bridge the gap between national policies andlocal implementation including enhancing awareness improving data governanceframeworks and fostering collaboration be-tween local and nationalauthorities. By addressing overlooked weaknesses in the Estonian open dataecosystem and providing actionable recommendations this research contributesto a more resilient and sustainable open data ecosystem. Additionally byvalidating the OGD-adapted Innovation Resistance Theory model and proposing arevised version tailored for local government contexts the study advancestheoretical frameworks for understanding data sharing resistance. Ultimatelythis study serves as a call to action for policymakers and practitioners toprioritize local OGD initiatives.</p>
                <p>Last Updated: 2024-06-12 17:51:47 UTC</p>
                <button class="interpret-button" data-id="2406.08461v1">Interpret</button>
                <div id="interpretation-2406.08461v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ORES-Inspect: A technology probe for machine learning audits on enwiki</h3>
                <p>Authors: Zachary LevonianLauren HagenLu LiJada LilleboeSolvejg WastvedtAaron HalfakerLoren Terveen</p>
                <p><a href="http://arxiv.org/abs/2406.08453v1">Link to paper</a></p>
                <p>Auditing the machine learning ML models used on Wikipedia is important forensuring that vandalism-detection processes remain fair and effective. Howeverconducting audits is challenging because stakeholders have diverse prioritiesand assembling evidence for a models inefficacy is technically complex. Wedesigned an interface to enable editors to learn about and audit theperformance of the ORES edit quality model. ORES-Inspect is an open-source webtool and a provocative technology probe for researching how editors think aboutauditing the many ML models used on Wikipedia. We describe the design ofORES-Inspect and our plans for further research with this system.</p>
                <p>Last Updated: 2024-06-12 17:45:04 UTC</p>
                <button class="interpret-button" data-id="2406.08453v1">Interpret</button>
                <div id="interpretation-2406.08453v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Designing Child-Centered Content Exposure and Moderation</h3>
                <p>Authors: Belén Saldías</p>
                <p><a href="http://arxiv.org/abs/2406.08420v1">Link to paper</a></p>
                <p>Research on childrens online experience and computer interaction oftenoverlooks the relationship children have with hidden algorithms that controlthe content they encounter. Furthermore it is not only about how childreninteract with targeted content but also how their development and agency arelargely affected by these. By engaging with the body of literature at theintersection of i human-centered design approaches ii exclusion anddiscrimination in A.I. iii privacy transparency and accountability and ivchildrens online citizenship this article dives into the question of How canwe approach the design of a child-centered moderation process to 1 includeaspects that families value for their children and 2 provide explanations forcontent appropriateness and removal so that we can scale according to systemsand human needs the moderation process assisted by A.I..  This article contributes a sociotechnical highlight of core challenges andopportunities of designing child-centered content control tools. The articleconcludes by grounding and characterizing design considerations for achild-centered family-guided moderation system. We hope this work serves as astepping stone for designers and researchers pursuing childrens safety onlinewith an eye on hidden agents controlling childrens online experiences and byextension the values and opportunities children are exposed to.</p>
                <p>Last Updated: 2024-06-12 17:04:36 UTC</p>
                <button class="interpret-button" data-id="2406.08420v1">Interpret</button>
                <div id="interpretation-2406.08420v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Tailoring Generative AI Chatbots for Multiethnic Communities in Disaster Preparedness Communication: Extending the CASA Paradigm</h3>
                <p>Authors: Xinyan ZhaoYuan SunWenlin LiuChau-Wai Wong</p>
                <p><a href="http://arxiv.org/abs/2406.08411v1">Link to paper</a></p>
                <p>This study is among the first to develop different prototypes of generativeAI GenAI chatbots powered by GPT 4 to communicate hurricane preparednessinformation to diverse residents. Drawing from the Computers Are Social ActorsCASA paradigm and the literature on disaster vulnerability and culturaltailoring this study conducted a between-subjects experiment with 441 BlackHispanic and Caucasian residents of Florida. A computational analysis of chatlogs N  7848 shows that anthropomorphism and personalization are keycommunication topics in GenAI chatbot-user interactions. SEM results N  441suggest that GenAI chatbots varying in tone formality and cultural tailoringsignificantly predict bot perceptions and subsequently hurricane preparednessoutcomes. These results highlight the potential of using GenAI chatbots toimprove diverse communities disaster preparedness.</p>
                <p>Last Updated: 2024-06-12 16:57:28 UTC</p>
                <button class="interpret-button" data-id="2406.08411v1">Interpret</button>
                <div id="interpretation-2406.08411v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SonicID: User Identification on Smart Glasses with Acoustic Sensing</h3>
                <p>Authors: Ke LiDevansh AgarwalRuidong ZhangVipin GundaTianjun MoSaif MahmudBoao ChenFrançois GuimbretièreCheng Zhang</p>
                <p><a href="http://arxiv.org/abs/2406.08273v1">Link to paper</a></p>
                <p>Smart glasses have become more prevalent as they provide an increasing numberof applications for users. They store various types of private information orcan access it via connections established with other devices. Therefore thereis a growing need for user identification on smart glasses. In this paper weintroduce a low-power and minimally-obtrusive system called SonicID designedto authenticate users on glasses. SonicID extracts unique biometric informationfrom users by scanning their faces with ultrasonic waves and utilizes thisinformation to distinguish between different users powered by a customizedbinary classifier with the ResNet-18 architecture. SonicID can authenticateusers within 0.12 seconds with an energy consumption of 19.8 mAs per trial. Auser study involving 24 participants confirms that SonicID achieves a truepositive rate of 96.5 a false positive rate of 4.1 and a balanced accuracyof 96.2 using just 4 minutes of training data collected for each new user.This performance is relatively consistent across different remounting sessionsand days. Given this promising performance we further discuss the potentialapplications of SonicID and methods to improve its performance in the future.</p>
                <p>Last Updated: 2024-06-12 14:38:34 UTC</p>
                <button class="interpret-button" data-id="2406.08273v1">Interpret</button>
                <div id="interpretation-2406.08273v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>ICE-G: Image Conditional Editing of 3D Gaussian Splats</h3>
                <p>Authors: Vishnu JaganathanHannah Hanyun HuangMuhammad Zubair IrshadVarun JampaniAmit RajZsolt Kira</p>
                <p><a href="http://arxiv.org/abs/2406.08488v1">Link to paper</a></p>
                <p>Recently many techniques have emerged to create high quality 3D assets andscenes. When it comes to editing of these objects however existing approachesare either slow compromise on quality or do not provide enough customization.We introduce a novel approach to quickly edit a 3D model from a singlereference view. Our technique first segments the edit image and then matchessemantically corresponding regions across chosen segmented dataset views usingDINO features. A color or texture change from a particular region of the editimage can then be applied to other views automatically in a semanticallysensible manner. These edited views act as an updated dataset to further trainand re-style the 3D scene. The end-result is therefore an edited 3D model. Ourframework enables a wide variety of editing tasks such as manual local editscorrespondence based style transfer from any example image and a combinationof different styles from multiple example images. We use Gaussian Splats as ourprimary 3D representation due to their speed and ease of local editing but ourtechnique works for other methods such as NeRFs as well. We show throughmultiple examples that our method produces higher quality results whileoffering fine-grained control of editing. Project page: ice-gaussian.github.io</p>
                <p>Last Updated: 2024-06-12 17:59:52 UTC</p>
                <button class="interpret-button" data-id="2406.08488v1">Interpret</button>
                <div id="interpretation-2406.08488v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RMem: Restricted Memory Banks Improve Video Object Segmentation</h3>
                <p>Authors: Junbao ZhouZiqi PangYu-Xiong Wang</p>
                <p><a href="http://arxiv.org/abs/2406.08476v1">Link to paper</a></p>
                <p>With recent video object segmentation VOS benchmarks evolving tochallenging scenarios we revisit a simple but overlooked strategy: restrictingthe size of memory banks. This diverges from the prevalent practice ofexpanding memory banks to accommodate extensive historical information. Ourspecially designed memory deciphering study offers a pivotal insightunderpinning such a strategy: expanding memory banks while seeminglybeneficial actually increases the difficulty for VOS modules to decoderelevant features due to the confusion from redundant information. Byrestricting memory banks to a limited number of essential frames we achieve anotable improvement in VOS accuracy. This process balances the importance andfreshness of frames to maintain an informative memory bank within a boundedcapacity. Additionally restricted memory banks reduce the training-inferencediscrepancy in memory lengths compared with continuous expansion. This fostersnew opportunities in temporal reasoning and enables us to introduce thepreviously overlooked temporal positional embedding. Finally our insightsare embodied in RMem R for restricted a simple yet effective VOSmodification that excels at challenging VOS scenarios and establishes new stateof the art for object state changes on the VOST dataset and long videos onthe Long Videos dataset. Our code and demo are available athttps://restricted-memory.github.io/.</p>
                <p>Last Updated: 2024-06-12 17:59:04 UTC</p>
                <button class="interpret-button" data-id="2406.08476v1">Interpret</button>
                <div id="interpretation-2406.08476v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Real2Code: Reconstruct Articulated Objects via Code Generation</h3>
                <p>Authors: Zhao MandiYijia WengDominik BauerShuran Song</p>
                <p><a href="http://arxiv.org/abs/2406.08474v1">Link to paper</a></p>
                <p>We present Real2Code a novel approach to reconstructing articulated objectsvia code generation. Given visual observations of an object we firstreconstruct its part geometry using an image segmentation model and a shapecompletion model. We then represent the object parts with oriented boundingboxes which are input to a fine-tuned large language model LLM to predictjoint articulation as code. By leveraging pre-trained vision and languagemodels our approach scales elegantly with the number of articulated parts andgeneralizes from synthetic training data to real world objects in unstructuredenvironments. Experimental results demonstrate that Real2Code significantlyoutperforms previous state-of-the-art in reconstruction accuracy and is thefirst approach to extrapolate beyond objects structural complexity in thetraining set and reconstructs objects with up to 10 articulated parts. Whenincorporated with a stereo reconstruction model Real2Code also generalizes toreal world objects from a handful of multi-view RGB images without the needfor depth or camera information.</p>
                <p>Last Updated: 2024-06-12 17:57:06 UTC</p>
                <button class="interpret-button" data-id="2406.08474v1">Interpret</button>
                <div id="interpretation-2406.08474v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RILe: Reinforced Imitation Learning</h3>
                <p>Authors: Mert AlbabaSammy ChristenChristoph GebhardtThomas LangarekMichael J. BlackOtmar Hilliges</p>
                <p><a href="http://arxiv.org/abs/2406.08472v1">Link to paper</a></p>
                <p>Reinforcement Learning has achieved significant success in generating complexbehavior but often requires extensive reward function engineering. Adversarialvariants of Imitation Learning and Inverse Reinforcement Learning offer analternative by learning policies from expert demonstrations via adiscriminator. Employing discriminators increases their data- and computationalefficiency over the standard approaches however results in sensitivity toimperfections in expert data. We propose RILe a teacher-student system thatachieves both robustness to imperfect data and efficiency. In RILe the studentlearns an action policy while the teacher dynamically adjusts a reward functionbased on the students performance and its alignment with expertdemonstrations. By tailoring the reward function to both performance of thestudent and expert similarity our system reduces dependence on thediscriminator and hence increases robustness against data imperfections.Experiments show that RILe outperforms existing methods by 2x in settings withlimited or noisy expert data.</p>
                <p>Last Updated: 2024-06-12 17:56:31 UTC</p>
                <button class="interpret-button" data-id="2406.08472v1">Interpret</button>
                <div id="interpretation-2406.08472v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Surprise! Using Physiological Stress for Allostatic Regulation Under the Active Inference Framework [Pre-Print]</h3>
                <p>Authors: Imran KhanRobert Lowe</p>
                <p><a href="http://arxiv.org/abs/2406.08471v1">Link to paper</a></p>
                <p>Allostasis proposes that long-term viability of a living system is achievedthrough anticipatory adjustments of its physiology and behaviour: emphasisingphysiological and affective stress as an adaptive state of adaptation thatminimizes long-term prediction errors. More recently the active inferenceframework AIF has also sought to explain action and long-term adaptationthrough the minimization of future errors free energy through the learningof statistical contingencies of the world offering a formalism for allostaticregulation. We suggest that framing prediction errors through the lens ofbiological hormonal dynamics proposed by allostasis offers a way to integratethese two models together in a biologically-plausible manner. In this paper wedescribe our initial work in developing a model that grounds prediction errorssurprisal into the secretion of a physiological stress hormone cortisolacting as an adaptive allostatic mediator on a homeostatically-controlledphysiology. We evaluate this using a computational model in simulations usingan active inference agent endowed with an artificial physiology regulatedthrough homeostatic and allostatic control in a stochastic environment. Ourresults find that allostatic functions of cortisol stress secreted as afunction of prediction errors provide adaptive advantages to the agentslong-term physiological regulation. We argue that the coupling ofinformation-theoretic prediction errors to low-level biological hormonaldynamics of stress can provide a computationally efficient model to long-termregulation for embodied intelligent systems.</p>
                <p>Last Updated: 2024-06-12 17:56:15 UTC</p>
                <button class="interpret-button" data-id="2406.08471v1">Interpret</button>
                <div id="interpretation-2406.08471v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Adaptive Swarm Mesh Refinement using Deep Reinforcement Learning with Local Rewards</h3>
                <p>Authors: Niklas FreymuthPhilipp DahlingerTobias WürthSimon ReischLuise KärgerGerhard Neumann</p>
                <p><a href="http://arxiv.org/abs/2406.08440v1">Link to paper</a></p>
                <p>Simulating physical systems is essential in engineering but analyticalsolutions are limited to straightforward problems. Consequently numericalmethods like the Finite Element Method FEM are widely used. However the FEMbecomes computationally expensive as problem complexity and accuracy demandsincrease. Adaptive Mesh Refinement AMR improves the FEM by dynamicallyallocating mesh elements on the domain balancing computational speed andaccuracy. Classical AMR depends on heuristics or expensive error estimatorslimiting its use in complex simulations. While learning-based AMR methods arepromising they currently only scale to simple problems. In this work weformulate AMR as a system of collaborating homogeneous agents that iterativelysplit into multiple new agents. This agent-wise perspective enables a spatialreward formulation focused on reducing the maximum mesh element error. Ourapproach Adaptive Swarm Mesh Refinement ASMR offers efficient stableoptimization and generates highly adaptive meshes at user-defined resolutionduring inference. Extensive experiments including volumetric meshes andNeumann boundary conditions demonstrate that ASMR exceeds heuristic approachesand learned baselines matching the performance of expensive error-based oracleAMR strategies. ASMR additionally generalizes to different domains duringinference and produces meshes that simulate up to 2 orders of magnitude fasterthan uniform refinements in more demanding settings.</p>
                <p>Last Updated: 2024-06-12 17:26:54 UTC</p>
                <button class="interpret-button" data-id="2406.08440v1">Interpret</button>
                <div id="interpretation-2406.08440v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning</h3>
                <p>Authors: Yizhe HuangAnji LiuFanqi KongYaodong YangSong-Chun ZhuXue Feng</p>
                <p><a href="http://arxiv.org/abs/2406.08002v1">Link to paper</a></p>
                <p>Despite the recent successes of multi-agent reinforcement learning MARLalgorithms efficiently adapting to co-players in mixed-motive environmentsremains a significant challenge. One feasible approach is to hierarchicallymodel co-players behavior based on inferring their characteristics. Howeverthese methods often encounter difficulties in efficient reasoning andutilization of inferred information. To address these issues we proposeHierarchical Opponent modeling and Planning HOP a novel multi-agentdecision-making algorithm that enables few-shot adaptation to unseen policiesin mixed-motive environments. HOP is hierarchically composed of two modules: anopponent modeling module that infers others goals and learns correspondinggoal-conditioned policies and a planning module that employs Monte Carlo TreeSearch MCTS to identify the best response. Our approach improves efficiencyby updating beliefs about others goals both across and within episodes and byusing information from the opponent modeling module to guide planning.Experimental results demonstrate that in mixed-motive environments HOPexhibits superior few-shot adaptation capabilities when interacting withvarious unseen agents and excels in self-play scenarios. Furthermore theemergence of social intelligence during our experiments underscores thepotential of our approach in complex multi-agent environments.</p>
                <p>Last Updated: 2024-06-12 08:48:06 UTC</p>
                <button class="interpret-button" data-id="2406.08002v1">Interpret</button>
                <div id="interpretation-2406.08002v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Carbon Market Simulation with Adaptive Mechanism Design</h3>
                <p>Authors: Han WangWenhao LiHongyuan ZhaBaoxiang Wang</p>
                <p><a href="http://arxiv.org/abs/2406.07875v1">Link to paper</a></p>
                <p>A carbon market is a market-based tool that incentivizes economic agents toalign individual profits with the global utility i.e. reducing carbonemissions to tackle climate change.  textitCap and trade stands as a critical principle based on allocating andtrading carbon allowances carbon emission credit enabling economic agents tofollow planned emissions and penalizing excess emissions.  A central authority is responsible for introducing and allocating thoseallowances in cap and trade.  However the complexity of carbon market dynamics makes accurate simulationintractable which in turn hinders the design of effective allocationstrategies.  To address this we propose an adaptive mechanism design frameworksimulating the market using hierarchical model-free multi-agent reinforcementlearning MARL.  Government agents allocate carbon credits while enterprises engage ineconomic activities and carbon trading.  This framework illustrates agents behavior comprehensively.  Numerical results show MARL enables government agents to balanceproductivity equality and carbon emissions.  Our project is available aturlhttps://github.com/xwanghan/Carbon-Simulator.</p>
                <p>Last Updated: 2024-06-12 05:08:51 UTC</p>
                <button class="interpret-button" data-id="2406.07875v1">Interpret</button>
                <div id="interpretation-2406.07875v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-agent Reinforcement Learning with Deep Networks for Diverse Q-Vectors</h3>
                <p>Authors: Zhenglong LuoZhiyong ChenJames Welsh</p>
                <p><a href="http://arxiv.org/abs/2406.07848v1">Link to paper</a></p>
                <p>Multi-agent reinforcement learning MARL has become a significant researchtopic due to its ability to facilitate learning in complex environments. Inmulti-agent tasks the state-action value commonly referred to as the Q-valuecan vary among agents because of their individual rewards resulting in aQ-vector. Determining an optimal policy is challenging as it involves morethan just maximizing a single Q-value. Various optimal policies such as a Nashequilibrium have been studied in this context. Algorithms like Nash Q-learningand Nash Actor-Critic have shown effectiveness in these scenarios. This paperextends this research by proposing a deep Q-networks DQN algorithm capable oflearning various Q-vectors using Max Nash and Maximin strategies. Theeffectiveness of this approach is demonstrated in an environment where dualrobotic arms collaborate to lift a pot.</p>
                <p>Last Updated: 2024-06-12 03:30:10 UTC</p>
                <button class="interpret-button" data-id="2406.07848v1">Interpret</button>
                <div id="interpretation-2406.07848v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Choreographing the Rhythms of Observation: Dynamics for Ranged Observer Bipartite-Unipartite SpatioTemporal (ROBUST) Networks</h3>
                <p>Authors: Ted Edward Holmberg</p>
                <p><a href="http://arxiv.org/abs/2406.07473v1">Link to paper</a></p>
                <p>Existing network analysis methods struggle to optimize observer placements indynamic environments with limited visibility. This dissertation introduces thenovel ROBUST Ranged Observer Bipartite-Unipartite SpatioTemporal frameworkoffering a significant advancement in modeling analyzing and optimizingobserver networks within complex spatiotemporal domains. ROBUST leverages aunique bipartite-unipartite approach distinguishing between observer andobservable entities while incorporating spatial constraints and temporaldynamics.  This research extends spatiotemporal network theory by introducing novelgraph-based measures including myopic degree spatial closeness centralityand edge length proportion. These measures coupled with advanced clusteringtechniques like Proximal Recurrence provide insights into network structureresilience and the effectiveness of observer placements. The ROBUST frameworkdemonstrates superior resource allocation and strategic responsiveness comparedto conventional models. Case studies in oceanographic monitoring urban safetynetworks and multi-agent path planning showcases its practical applicabilityand adaptability. Results demonstrate significant improvements in coverageresponse times and overall network efficiency.  This work paves the way for future research in incorporating imperfectknowledge refining temporal pathing methodologies and expanding the scope ofapplications. By bridging theoretical advancements with practical solutionsROBUST stands as a significant contribution to the field promising to informand inspire ongoing and future endeavors in network optimization andmulti-agent system planning.</p>
                <p>Last Updated: 2024-06-11 17:20:01 UTC</p>
                <button class="interpret-button" data-id="2406.07473v1">Interpret</button>
                <div id="interpretation-2406.07473v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>ICE-G: Image Conditional Editing of 3D Gaussian Splats</h3>
                <p>Authors: Vishnu JaganathanHannah Hanyun HuangMuhammad Zubair IrshadVarun JampaniAmit RajZsolt Kira</p>
                <p><a href="http://arxiv.org/abs/2406.08488v1">Link to paper</a></p>
                <p>Recently many techniques have emerged to create high quality 3D assets andscenes. When it comes to editing of these objects however existing approachesare either slow compromise on quality or do not provide enough customization.We introduce a novel approach to quickly edit a 3D model from a singlereference view. Our technique first segments the edit image and then matchessemantically corresponding regions across chosen segmented dataset views usingDINO features. A color or texture change from a particular region of the editimage can then be applied to other views automatically in a semanticallysensible manner. These edited views act as an updated dataset to further trainand re-style the 3D scene. The end-result is therefore an edited 3D model. Ourframework enables a wide variety of editing tasks such as manual local editscorrespondence based style transfer from any example image and a combinationof different styles from multiple example images. We use Gaussian Splats as ourprimary 3D representation due to their speed and ease of local editing but ourtechnique works for other methods such as NeRFs as well. We show throughmultiple examples that our method produces higher quality results whileoffering fine-grained control of editing. Project page: ice-gaussian.github.io</p>
                <p>Last Updated: 2024-06-12 17:59:52 UTC</p>
                <button class="interpret-button" data-id="2406.08488v1">Interpret</button>
                <div id="interpretation-2406.08488v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Real2Code: Reconstruct Articulated Objects via Code Generation</h3>
                <p>Authors: Zhao MandiYijia WengDominik BauerShuran Song</p>
                <p><a href="http://arxiv.org/abs/2406.08474v1">Link to paper</a></p>
                <p>We present Real2Code a novel approach to reconstructing articulated objectsvia code generation. Given visual observations of an object we firstreconstruct its part geometry using an image segmentation model and a shapecompletion model. We then represent the object parts with oriented boundingboxes which are input to a fine-tuned large language model LLM to predictjoint articulation as code. By leveraging pre-trained vision and languagemodels our approach scales elegantly with the number of articulated parts andgeneralizes from synthetic training data to real world objects in unstructuredenvironments. Experimental results demonstrate that Real2Code significantlyoutperforms previous state-of-the-art in reconstruction accuracy and is thefirst approach to extrapolate beyond objects structural complexity in thetraining set and reconstructs objects with up to 10 articulated parts. Whenincorporated with a stereo reconstruction model Real2Code also generalizes toreal world objects from a handful of multi-view RGB images without the needfor depth or camera information.</p>
                <p>Last Updated: 2024-06-12 17:57:06 UTC</p>
                <button class="interpret-button" data-id="2406.08474v1">Interpret</button>
                <div id="interpretation-2406.08474v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Strategies for Pretraining Neural Operators</h3>
                <p>Authors: Anthony ZhouCooper LorsungAmirPouya HemmasianAmir Barati Farimani</p>
                <p><a href="http://arxiv.org/abs/2406.08473v1">Link to paper</a></p>
                <p>Pretraining for partial differential equation PDE modeling has recentlyshown promise in scaling neural operators across datasets to improvegeneralizability and performance. Despite these advances our understanding ofhow pretraining affects neural operators is still limited studies generallypropose tailored architectures and datasets that make it challenging to compareor examine different pretraining frameworks. To address this we comparevarious pretraining methods without optimizing architecture choices tocharacterize pretraining dynamics on different models and datasets as well asto understand its scaling and generalization behavior. We find that pretrainingis highly dependent on model and dataset choices but in general transferlearning or physics-based pretraining strategies work best. In additionpretraining performance can be further improved by using data augmentations.Lastly pretraining is additionally beneficial when fine-tuning in scarce dataregimes or when generalizing to downstream data similar to the pretrainingdistribution. Through providing insights into pretraining neural operators forphysics prediction we hope to motivate future work in developing andevaluating pretraining methods for PDEs.</p>
                <p>Last Updated: 2024-06-12 17:56:46 UTC</p>
                <button class="interpret-button" data-id="2406.08473v1">Interpret</button>
                <div id="interpretation-2406.08473v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RILe: Reinforced Imitation Learning</h3>
                <p>Authors: Mert AlbabaSammy ChristenChristoph GebhardtThomas LangarekMichael J. BlackOtmar Hilliges</p>
                <p><a href="http://arxiv.org/abs/2406.08472v1">Link to paper</a></p>
                <p>Reinforcement Learning has achieved significant success in generating complexbehavior but often requires extensive reward function engineering. Adversarialvariants of Imitation Learning and Inverse Reinforcement Learning offer analternative by learning policies from expert demonstrations via adiscriminator. Employing discriminators increases their data- and computationalefficiency over the standard approaches however results in sensitivity toimperfections in expert data. We propose RILe a teacher-student system thatachieves both robustness to imperfect data and efficiency. In RILe the studentlearns an action policy while the teacher dynamically adjusts a reward functionbased on the students performance and its alignment with expertdemonstrations. By tailoring the reward function to both performance of thestudent and expert similarity our system reduces dependence on thediscriminator and hence increases robustness against data imperfections.Experiments show that RILe outperforms existing methods by 2x in settings withlimited or noisy expert data.</p>
                <p>Last Updated: 2024-06-12 17:56:31 UTC</p>
                <button class="interpret-button" data-id="2406.08472v1">Interpret</button>
                <div id="interpretation-2406.08472v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences</h3>
                <p>Authors: Daiwei ChenYi ChenAniket RegeRamya Korlakai Vinayak</p>
                <p><a href="http://arxiv.org/abs/2406.08469v1">Link to paper</a></p>
                <p>Large foundation models pretrained on raw web-scale data are not readilydeployable without additional step of extensive alignment to human preferences.Such alignment is typically done by collecting large amounts of pairwisecomparisons from humans Do you prefer output A or B and learning a rewardmodel or a policy with the Bradley-Terry-Luce BTL model as a proxy for ahumans underlying implicit preferences. These methods generally suffer fromassuming a universal preference shared by all humans which lacks theflexibility of adapting to plurality of opinions and preferences. In this workwe propose PAL a framework to model human preference complementary to existingpretraining strategies which incorporates plurality from the ground up. Wepropose using the ideal point model as a lens to view alignment usingpreference comparisons. Together with our novel reformulation and using mixturemodeling our framework captures the plurality of population preferences whilesimultaneously learning a common preference latent space across differentpreferences which can few-shot generalize to new unseen users. Our approachenables us to use the penultimate-layer representation of large foundationmodels and simple MLP layers to learn reward functions that are on-par with theexisting large state-of-the-art reward models thereby enhancing efficiency ofreward modeling significantly. We show that PAL achieves competitive rewardmodel accuracy compared to strong baselines on 1 Language models with Summarydataset  2 Image Generative models with Pick-a-Pic dataset  3 A newsemisynthetic heterogeneous dataset generated using Anthropic Personas.Finally our experiments also highlight the shortcoming of current preferencedatasets that are created using rigid rubrics which wash away heterogeneityand call for more nuanced data collection approaches.</p>
                <p>Last Updated: 2024-06-12 17:54:54 UTC</p>
                <button class="interpret-button" data-id="2406.08469v1">Interpret</button>
                <div id="interpretation-2406.08469v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-06-13</p>
        </div>
    
        </div>
    </body>
    </html>
    