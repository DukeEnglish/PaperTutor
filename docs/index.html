
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Establishing Rigorous and Cost-effective Clinical Trials for Artificial Intelligence Models</h3>
                <p>Authors: Wanling GaoYunyou HuangDandan CuiZhuoming YuWenjing LiuXiaoshuang LiangJiahui ZhaoJiyue XieHao LiLi MaNing YeYumiao KangDingfeng LuoPeng PanWei HuangZhongmou LiuJizhong HuGangyuan ZhaoChongrong JiangFan HuangTianyi WeiSuqin TangBingjie XiaZhifei ZhangJianfeng Zhan</p>
                <p><a href="http://arxiv.org/abs/2407.08554v1">Link to paper</a></p>
                <p>A profound gap persists between artificial intelligence AI and clinicalpractice in medicine primarily due to the lack of rigorous and cost-effectiveevaluation methodologies. State-of-the-art and state-of-the-practice AI modelevaluations are limited to laboratory studies on medical datasets or directclinical trials with no or solely patient-centered controls. Moreover thecrucial role of clinicians in collaborating with AI pivotal for determiningits impact on clinical practice is often overlooked. For the first time weemphasize the critical necessity for rigorous and cost-effective evaluationmethodologies for AI models in clinical practice featuringpatient/clinician-centered dual-centered AI randomized controlled trialsDC-AI RCTs and virtual clinician-based in-silico trials VC-MedAI as aneffective proxy for DC-AI RCTs. Leveraging 7500 diagnosis records fromtwo-phase inaugural DC-AI RCTs across 14 medical centers with 125 cliniciansour results demonstrate the necessity of DC-AI RCTs and the effectiveness ofVC-MedAI. Notably VC-MedAI performs comparably to human cliniciansreplicating insights and conclusions from prospective DC-AI RCTs. We envisionDC-AI RCTs and VC-MedAI as pivotal advancements presenting innovative andtransformative evaluation methodologies for AI models in clinical practiceoffering a preclinical-like setting mirroring conventional medicine andreshaping development paradigms in a cost-effective and fast-iterative manner.Chinese Clinical Trial Registration: ChiCTR2400086816.</p>
                <p>Last Updated: 2024-07-11 14:37:08 UTC</p>
                <button class="interpret-button" data-id="2407.08554v1">Interpret</button>
                <div id="interpretation-2407.08554v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DIDUP: Dynamic Iterative Development for UI Prototyping</h3>
                <p>Authors: Jenny MaKarthik SreedharVivian LiuSitong WangPedro Alejandro PerezLydia B. Chilton</p>
                <p><a href="http://arxiv.org/abs/2407.08474v1">Link to paper</a></p>
                <p>Large language models LLMs are remarkably good at writing code. Aparticularly valuable case of human-LLM collaboration is code-based UIprototyping a method for creating interactive prototypes that allows users toview and fully engage with a user interface. We conduct a formative study ofGPT Pilot a leading LLM-generated code-prototyping system and find that itsinflexibility towards change once development has started leads to weaknessesin failure prevention and dynamic planning it closely resembles the linearworkflow of the waterfall model. We introduce DIDUP a system for code-based UIprototyping that follows an iterative spiral model which takes changes anditerations that come up during the development process into account. We proposethree novel mechanisms for LLM-generated code-prototyping systems: 1 adaptiveplanning where plans should be dynamic and reflect changes duringimplementation 2 code injection where the system should write a minimalamount of code and inject it instead of rewriting code so users have a bettermental model of the code evolution and 3 lightweight state management asimplified version of source control so users can quickly revert to differentworking states. Together this enables users to rapidly develop and iterate onprototypes.</p>
                <p>Last Updated: 2024-07-11 13:10:11 UTC</p>
                <button class="interpret-button" data-id="2407.08474v1">Interpret</button>
                <div id="interpretation-2407.08474v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Towards a Quality Approach to Hierarchical Color Maps</h3>
                <p>Authors: Tobias MertzJÃ¶rn Kohlhammer</p>
                <p><a href="http://arxiv.org/abs/2407.08287v1">Link to paper</a></p>
                <p>To improve the perception of hierarchical structures in data sets severalcolor map generation algorithms have been proposed to take this structure intoaccount. But the design of hierarchical color maps elicits differentrequirements to those of color maps for tabular data. Within this paper wemake an initial effort to put design rules from the color map literature intothe context of hierarchical color maps. We investigate the impact of severaldesign decisions and provide recommendations for various analysis scenarios.Thus we lay the foundation for objective quality criteria to evaluatehierarchical color maps.</p>
                <p>Last Updated: 2024-07-11 08:32:49 UTC</p>
                <button class="interpret-button" data-id="2407.08287v1">Interpret</button>
                <div id="interpretation-2407.08287v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Leveraging LLMs to Predict Affective States via Smartphone Sensor Features</h3>
                <p>Authors: Tianyi ZhangSongyan TengHong JiaSimon D'Alfonso</p>
                <p><a href="http://arxiv.org/abs/2407.08240v1">Link to paper</a></p>
                <p>As mental health issues for young adults present a pressing public healthconcern daily digital mood monitoring for early detection has become animportant prospect. An active research area digital phenotyping involvescollecting and analysing data from personal digital devices such as smartphonesusage and sensors and wearables to infer behaviours and mental health. Whilstthis data is standardly analysed using statistical and machine learningapproaches the emergence of large language models LLMs offers a new approachto make sense of smartphone sensing data. Despite their effectiveness acrossvarious domains LLMs remain relatively unexplored in digital mental healthparticularly in integrating mobile sensor data. Our study aims to bridge thisgap by employing LLMs to predict affect outcomes based on smartphone sensingdata from university students. We demonstrate the efficacy of zero-shot andfew-shot embedding LLMs in inferring general wellbeing. Our findings revealthat LLMs can make promising predictions of affect measures using solelysmartphone sensing data. This research sheds light on the potential of LLMs foraffective state prediction emphasizing the intricate link between smartphonebehavioral patterns and affective states. To our knowledge this is the firstwork to leverage LLMs for affective state prediction and digital phenotypingtasks.</p>
                <p>Last Updated: 2024-07-11 07:37:52 UTC</p>
                <button class="interpret-button" data-id="2407.08240v1">Interpret</button>
                <div id="interpretation-2407.08240v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Generating Contextually-Relevant Navigation Instructions for Blind and Low Vision People</h3>
                <p>Authors: Zain MerchantAbrar AnwarEmily WangSouti ChattopadhyayJesse Thomason</p>
                <p><a href="http://arxiv.org/abs/2407.08219v1">Link to paper</a></p>
                <p>Navigating unfamiliar environments presents significant challenges for blindand low-vision BLV individuals. In this work we construct a dataset ofimages and goals across different scenarios such as searching through kitchensor navigating outdoors. We then investigate how grounded instruction generationmethods can provide contextually-relevant navigational guidance to users inthese instances. Through a sighted user study we demonstrate that largepretrained language models can produce correct and useful instructionsperceived as beneficial for BLV users. We also conduct a survey and interviewwith 4 BLV users and observe useful insights on preferences for differentinstructions based on the scenario.</p>
                <p>Last Updated: 2024-07-11 06:40:36 UTC</p>
                <button class="interpret-button" data-id="2407.08219v1">Interpret</button>
                <div id="interpretation-2407.08219v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Transformer Circuit Faithfulness Metrics are not Robust</h3>
                <p>Authors: Joseph MillerBilal ChughtaiWilliam Saunders</p>
                <p><a href="http://arxiv.org/abs/2407.08734v1">Link to paper</a></p>
                <p>Mechanistic interpretability work attempts to reverse engineer the learnedalgorithms present inside neural networks. One focus of this work has been todiscover circuits -- subgraphs of the full model that explain behaviour onspecific tasks. But how do we measure the performance of such circuits Priorwork has attempted to measure circuit faithfulness -- the degree to which thecircuit replicates the performance of the full model. In this work we surveymany considerations for designing experiments that measure circuit faithfulnessby ablating portions of the models computation. Concerningly we find existingmethods are highly sensitive to seemingly insignificant changes in the ablationmethodology. We conclude that existing circuit faithfulness scores reflect boththe methodological choices of researchers as well as the actual components ofthe circuit - the task a circuit is required to perform depends on the ablationused to test it. The ultimate goal of mechanistic interpretability work is tounderstand neural networks so we emphasize the need for more clarity in theprecise claims being made about circuits. We open source a library athttps://github.com/UFO-101/auto-circuit that includes highly efficientimplementations of a wide range of ablation methodologies and circuit discoveryalgorithms.</p>
                <p>Last Updated: 2024-07-11 17:59:00 UTC</p>
                <button class="interpret-button" data-id="2407.08734v1">Interpret</button>
                <div id="interpretation-2407.08734v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist</h3>
                <p>Authors: Zihao ZhouShudong LiuMaizhen NingWei LiuJindong WangDerek F. WongXiaowei HuangQiufeng WangKaizhu Huang</p>
                <p><a href="http://arxiv.org/abs/2407.08733v1">Link to paper</a></p>
                <p>Exceptional mathematical reasoning ability is one of the key features thatdemonstrate the power of large language models LLMs. How to comprehensivelydefine and evaluate the mathematical abilities of LLMs and even reflect theuser experience in real-world scenarios has emerged as a critical issue.Current benchmarks predominantly concentrate on problem-solving capabilitieswhich presents a substantial risk of model overfitting and fails to accuratelyrepresent genuine mathematical reasoning abilities. In this paper we arguethat if a model really understands a problem it should be robustly and readilyapplied across a diverse array of tasks. Motivated by this we introduceMATHCHECK a well-designed checklist for testing task generalization andreasoning robustness as well as an automatic tool to generate checklistsefficiently. MATHCHECK includes multiple mathematical reasoning tasks androbustness test types to facilitate a comprehensive evaluation of bothmathematical reasoning ability and behavior testing. Utilizing MATHCHECK wedevelop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textualreasoning and multi-modal reasoning capabilities respectively serving asupgraded versions of benchmarks including GSM8k GeoQA UniGeo and Geometry3K.We adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 20 LLMs and 11 MLLMsassessing their comprehensive mathematical reasoning abilities. Our resultsdemonstrate that while frontier LLMs like GPT-4o continue to excel in variousabilities on the checklist many other model families exhibit a significantdecline. Further experiments indicate that compared to traditional mathbenchmarks MATHCHECK better reflects true mathematical abilities andrepresents mathematical intelligence more linearly thereby supporting ourdesign. On our MATHCHECK we can easily conduct detailed behavior analysis todeeply investigate models.</p>
                <p>Last Updated: 2024-07-11 17:58:58 UTC</p>
                <button class="interpret-button" data-id="2407.08733v1">Interpret</button>
                <div id="interpretation-2407.08733v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Taxonomy for Data Contamination in Large Language Models</h3>
                <p>Authors: Medha PalavalliAmanda BertschMatthew R. Gormley</p>
                <p><a href="http://arxiv.org/abs/2407.08716v1">Link to paper</a></p>
                <p>Large language models pretrained on extensive web corpora demonstrateremarkable performance across a wide range of downstream tasks. However agrowing concern is data contamination where evaluation datasets may becontained in the pretraining corpus inflating model performance.Decontamination the process of detecting and removing such data is apotential solution yet these contaminants may originate from altered versionsof the test set evading detection during decontamination. How different typesof contamination impact the performance of language models on downstream tasksis not fully understood. We present a taxonomy that categorizes the varioustypes of contamination encountered by LLMs during the pretraining phase andidentify which types pose the highest risk. We analyze the impact ofcontamination on two key NLP tasks -- summarization and question answering --revealing how different types of contamination influence task performanceduring evaluation.</p>
                <p>Last Updated: 2024-07-11 17:50:34 UTC</p>
                <button class="interpret-button" data-id="2407.08716v1">Interpret</button>
                <div id="interpretation-2407.08716v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GTA: A Benchmark for General Tool Agents</h3>
                <p>Authors: Jize WangZerun MaYining LiSongyang ZhangCailian ChenKai ChenXinyi Le</p>
                <p><a href="http://arxiv.org/abs/2407.08713v1">Link to paper</a></p>
                <p>Significant focus has been placed on integrating large language models LLMswith various tools in developing general-purpose agents. This poses a challengeto LLMs tool-use capabilities. However there are evident gaps betweenexisting tool-use evaluations and real-world scenarios. Current evaluationsoften use AI-generated queries single-step tasks dummy tools and text-onlyinteractions failing to reveal the agents real-world problem-solvingabilities effectively. To address this we propose GTA a benchmark for GeneralTool Agents featuring three main aspects: i Real user queries: human-writtenqueries with simple real-world objectives but implicit tool-use requiring theLLM to reason the suitable tools and plan the solution steps. ii Realdeployed tools: an evaluation platform equipped with tools across perceptionoperation logic and creativity categories to evaluate the agents actual taskexecution performance. iii Real multimodal inputs: authentic image filessuch as spatial scenes web page screenshots tables code snippets andprinted/handwritten materials used as the query contexts to align withreal-world scenarios closely. We design 229 real-world tasks and executabletool chains to evaluate mainstream LLMs. Our findings show that real-world userqueries are challenging for existing LLMs with GPT-4 completing less than 50of the tasks and most LLMs achieving below 25. This evaluation reveals thebottlenecks in the tool-use capabilities of current LLMs in real-worldscenarios which provides future direction for advancing general-purpose toolagents. The code and dataset are available athttps://github.com/open-compass/GTA.</p>
                <p>Last Updated: 2024-07-11 17:50:09 UTC</p>
                <button class="interpret-button" data-id="2407.08713v1">Interpret</button>
                <div id="interpretation-2407.08713v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Uncertainty Estimation of Large Language Models in Medical Question Answering</h3>
                <p>Authors: Jiaxin WuYizhou YuHong-Yu Zhou</p>
                <p><a href="http://arxiv.org/abs/2407.08662v1">Link to paper</a></p>
                <p>Large Language Models LLMs show promise for natural language generation inhealthcare but risk hallucinating factually incorrect information. DeployingLLMs for medical question answering necessitates reliable uncertaintyestimation UE methods to detect hallucinations. In this work we benchmarkpopular UE methods with different model sizes on medical question-answeringdatasets. Our results show that current approaches generally perform poorly inthis domain highlighting the challenge of UE for medical applications. We alsoobserve that larger models tend to yield better results suggesting acorrelation between model size and the reliability of UE. To address thesechallenges we propose Two-phase Verification a probability-free UncertaintyEstimation approach. First an LLM generates a step-by-step explanationalongside its initial answer followed by formulating verification questions tocheck the factual claims in the explanation. The model then answers thesequestions twice: first independently and then referencing the explanation.Inconsistencies between the two sets of answers measure the uncertainty in theoriginal response. We evaluate our approach on three biomedicalquestion-answering datasets using Llama 2 Chat models and compare it againstthe benchmarked baseline methods. The results show that our Two-phaseVerification method achieves the best overall accuracy and stability acrossvarious datasets and model sizes and its performance scales as the model sizeincreases.</p>
                <p>Last Updated: 2024-07-11 16:51:33 UTC</p>
                <button class="interpret-button" data-id="2407.08662v1">Interpret</button>
                <div id="interpretation-2407.08662v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>A Review of Nine Physics Engines for Reinforcement Learning Research</h3>
                <p>Authors: Michael KaupCornelius WolffHyerim HwangJulius MayerElia Bruni</p>
                <p><a href="http://arxiv.org/abs/2407.08590v1">Link to paper</a></p>
                <p>We present a review of popular simulation engines and frameworks used inreinforcement learning RL research aiming to guide researchers in selectingtools for creating simulated physical environments for RL and training setups.It evaluates nine frameworks Brax Chrono Gazebo MuJoCo ODE PhysXPyBullet Webots and Unity based on their popularity feature range qualityusability and RL capabilities. We highlight the challenges in selecting andutilizing physics engines for RL research including the need for detailedcomparisons and an understanding of each frameworks capabilities. Key findingsindicate MuJoCo as the leading framework due to its performance andflexibility despite usability challenges. Unity is noted for its ease of usebut lacks scalability and simulation fidelity. The study calls for furtherdevelopment to improve simulation engines usability and performance andstresses the importance of transparency and reproducibility in RL research.This review contributes to the RL community by offering insights into theselection process for simulation engines facilitating informeddecision-making.</p>
                <p>Last Updated: 2024-07-11 15:13:28 UTC</p>
                <button class="interpret-button" data-id="2407.08590v1">Interpret</button>
                <div id="interpretation-2407.08590v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility</h3>
                <p>Authors: Yuchen XiaJize ZhangNasser JazdiMichael Weyrich</p>
                <p><a href="http://dx.doi.org/10.51202/9783181024379">Link to paper</a></p>
                <p>This paper introduces a novel approach to integrating large language modelLLM agents into automated production systems aimed at enhancing taskautomation and flexibility. We organize production operations within ahierarchical framework based on the automation pyramid. Atomic operationfunctionalities are modeled as microservices which are executed throughinterface invocation within a dedicated digital twin system. This allows for ascalable and flexible foundation for orchestrating production processes. Inthis digital twin system low-level hardware-specific data is semanticallyenriched and made interpretable for LLMs for production planning and controltasks. Large language model agents are systematically prompted to interpretthese production-specific data and knowledge. Upon receiving a user request oridentifying a triggering event the LLM agents generate a process plan. Thisplan is then decomposed into a series of atomic operations executed asmicroservices within the real-world automation system. We implement thisoverall approach on an automated modular production facility at our laboratorydemonstrating how the LLMs can handle production planning and control tasksthrough a concrete case study. This results in an intuitive production facilitywith higher levels of task automation and flexibility. Finally we reveal theseveral limitations in realizing the full potential of the large languagemodels in autonomous systems and point out promising benefits. Demos of thisseries of ongoing research series can be accessed at:https://github.com/YuchenXia/GPT4IndustrialAutomation</p>
                <p>Last Updated: 2024-07-11 14:34:43 UTC</p>
                <button class="interpret-button" data-id="2407.08550v1">Interpret</button>
                <div id="interpretation-2407.08550v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>United We Stand: Decentralized Multi-Agent Planning With Attrition</h3>
                <p>Authors: Nhat NguyenDuong NguyenGianluca RizzoHung Nguyen</p>
                <p><a href="http://arxiv.org/abs/2407.08254v1">Link to paper</a></p>
                <p>Decentralized planning is a key element of cooperative multi-agent systemsfor information gathering tasks. However despite the high frequency of agentfailures in realistic large deployment scenarios current approaches performpoorly in the presence of failures by not converging at all and/or by makingvery inefficient use of resources e.g. energy. In this work we proposeAttritable MCTS A-MCTS a decentralized MCTS algorithm capable of timely andefficient adaptation to changes in the set of active agents. It is based on theuse of a global reward function for the estimation of each agents localcontribution and regret matching for coordination. We evaluate itseffectiveness in realistic data-harvesting problems under different scenarios.We show both theoretically and experimentally that A-MCTS enables efficientadaptation even under high failure rates. Results suggest that in the presenceof frequent failures our solution improves substantially over the bestexisting approaches in terms of global utility and scalability.</p>
                <p>Last Updated: 2024-07-11 07:55:50 UTC</p>
                <button class="interpret-button" data-id="2407.08254v1">Interpret</button>
                <div id="interpretation-2407.08254v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Text-to-Game Engine for UGC-Based Role-Playing Games</h3>
                <p>Authors: Lei ZhangXuezheng PengShuyi YangFeiyang Wang</p>
                <p><a href="http://arxiv.org/abs/2407.08195v1">Link to paper</a></p>
                <p>The shift from professionally generated content PGC to user-generatedcontent UGC has revolutionized various media formats from text to video.With the rapid advancements in generative AI a similar shift is set totransform the game industry particularly in the realm of role-playing gamesRPGs. This paper introduces a new framework for a text-to-game engine thatutilizes foundation models to convert simple textual inputs into complexinteractive RPG experiences. The engine dynamically renders the game story in amulti-modal format and adjusts the game character environment and mechanicsin real-time in response to player actions. Using this framework we developedthe Zagii game engine which has successfully supported hundreds of RPG gamesacross a diverse range of genres and facilitated tens of thousands of onlineuser gameplay instances. This validates the effectiveness of our frame-work.Our work showcases the potential for a more open and democratized gamingparadigm highlighting the transformative impact of generative AI on the gamelife cycle.</p>
                <p>Last Updated: 2024-07-11 05:33:19 UTC</p>
                <button class="interpret-button" data-id="2407.08195v1">Interpret</button>
                <div id="interpretation-2407.08195v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Hierarchical Consensus-Based Multi-Agent Reinforcement Learning for Multi-Robot Cooperation Tasks</h3>
                <p>Authors: Pu FengJunkang LiangSize WangXin YuRongye ShiWenjun Wu</p>
                <p><a href="http://arxiv.org/abs/2407.08164v1">Link to paper</a></p>
                <p>In multi-agent reinforcement learning MARL the Centralized Training withDecentralized Execution CTDE framework is pivotal but struggles due to a gap:global state guidance in training versus reliance on local observations inexecution lacking global signals. Inspired by human societal consensusmechanisms we introduce the Hierarchical Consensus-based Multi-AgentReinforcement Learning HC-MARL framework to address this limitation. HC-MARLemploys contrastive learning to foster a global consensus among agentsenabling cooperative behavior without direct communication. This approachenables agents to form a global consensus from local observations using it asan additional piece of information to guide collaborative actions duringexecution. To cater to the dynamic requirements of various tasks consensus isdivided into multiple layers encompassing both short-term and long-termconsiderations. Short-term observations prompt the creation of an immediatelow-layer consensus while long-term observations contribute to the formationof a strategic high-layer consensus. This process is further refined throughan adaptive attention mechanism that dynamically adjusts the influence of eachconsensus layer. This mechanism optimizes the balance between immediatereactions and strategic planning tailoring it to the specific demands of thetask at hand. Extensive experiments and real-world applications in multi-robotsystems showcase our frameworks superior performance marking significantadvancements over baselines.</p>
                <p>Last Updated: 2024-07-11 03:55:55 UTC</p>
                <button class="interpret-button" data-id="2407.08164v1">Interpret</button>
                <div id="interpretation-2407.08164v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Video Diffusion Alignment via Reward Gradients</h3>
                <p>Authors: Mihir PrabhudesaiRussell MendoncaZheyang QinKaterina FragkiadakiDeepak Pathak</p>
                <p><a href="http://arxiv.org/abs/2407.08737v1">Link to paper</a></p>
                <p>We have made significant progress towards building foundational videodiffusion models. As these models are trained using large-scale unsuperviseddata it has become crucial to adapt these models to specific downstream tasks.Adapting these models via supervised fine-tuning requires collecting targetdatasets of videos which is challenging and tedious. In this work we utilizepre-trained reward models that are learned via preferences on top of powerfulvision discriminative models to adapt video diffusion models. These modelscontain dense gradient information with respect to generated RGB pixels whichis critical to efficient learning in complex search spaces such as videos. Weshow that backpropagating gradients from these reward models to a videodiffusion model can allow for compute and sample efficient alignment of thevideo diffusion model. We show results across a variety of reward models andvideo diffusion models demonstrating that our approach can learn much moreefficiently in terms of reward queries and computation than prior gradient-freeapproaches. Our code model weightsand more visualization are available athttps://vader-vid.github.io.</p>
                <p>Last Updated: 2024-07-11 17:59:45 UTC</p>
                <button class="interpret-button" data-id="2407.08737v1">Interpret</button>
                <div id="interpretation-2407.08737v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Transformer Circuit Faithfulness Metrics are not Robust</h3>
                <p>Authors: Joseph MillerBilal ChughtaiWilliam Saunders</p>
                <p><a href="http://arxiv.org/abs/2407.08734v1">Link to paper</a></p>
                <p>Mechanistic interpretability work attempts to reverse engineer the learnedalgorithms present inside neural networks. One focus of this work has been todiscover circuits -- subgraphs of the full model that explain behaviour onspecific tasks. But how do we measure the performance of such circuits Priorwork has attempted to measure circuit faithfulness -- the degree to which thecircuit replicates the performance of the full model. In this work we surveymany considerations for designing experiments that measure circuit faithfulnessby ablating portions of the models computation. Concerningly we find existingmethods are highly sensitive to seemingly insignificant changes in the ablationmethodology. We conclude that existing circuit faithfulness scores reflect boththe methodological choices of researchers as well as the actual components ofthe circuit - the task a circuit is required to perform depends on the ablationused to test it. The ultimate goal of mechanistic interpretability work is tounderstand neural networks so we emphasize the need for more clarity in theprecise claims being made about circuits. We open source a library athttps://github.com/UFO-101/auto-circuit that includes highly efficientimplementations of a wide range of ablation methodologies and circuit discoveryalgorithms.</p>
                <p>Last Updated: 2024-07-11 17:59:00 UTC</p>
                <button class="interpret-button" data-id="2407.08734v1">Interpret</button>
                <div id="interpretation-2407.08734v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>BiEquiFormer: Bi-Equivariant Representations for Global Point Cloud Registration</h3>
                <p>Authors: Stefanos PertigkiozoglouEvangelos ChatzipantazisKostas Daniilidis</p>
                <p><a href="http://arxiv.org/abs/2407.08729v1">Link to paper</a></p>
                <p>The goal of this paper is to address the problem of textitglobal pointcloud registration PCR i.e. finding the optimal alignment between pointclouds irrespective of the initial poses of the scans. This problem isnotoriously challenging for classical optimization methods due to computationalconstraints. First we show that state-of-the-art deep learning methods sufferfrom huge performance degradation when the point clouds are arbitrarily placedin space. We propose that textitequivariant deep learning should be utilizedfor solving this task and we characterize the specific type of bi-equivarianceof PCR. Then we design BiEquiformer a novel and scalabletextitbi-equivariant pipeline i.e. equivariant to the independenttransformations of the input point clouds. While a naive approach would processthe point clouds independently we design expressive bi-equivariant layers thatfuse the information from both point clouds. This allows us to extracthigh-quality superpoint correspondences and in turn robust point-cloudregistration. Extensive comparisons against state-of-the-art methods show thatour method achieves comparable performance in the canonical setting andsuperior performance in the robust setting in both the 3DMatch and thechallenging low-overlap 3DLoMatch dataset.</p>
                <p>Last Updated: 2024-07-11 17:58:10 UTC</p>
                <button class="interpret-button" data-id="2407.08729v1">Interpret</button>
                <div id="interpretation-2407.08729v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Topological Generalization Bounds for Discrete-Time Stochastic Optimization Algorithms</h3>
                <p>Authors: Rayna AndreevaBenjamin DupuisRik SarkarTolga BirdalUmut ÅimÅekli</p>
                <p><a href="http://arxiv.org/abs/2407.08723v1">Link to paper</a></p>
                <p>We present a novel set of rigorous and computationally efficienttopology-based complexity notions that exhibit a strong correlation with thegeneralization gap in modern deep neural networks DNNs. DNNs show remarkablegeneralization properties yet the source of these capabilities remainselusive defying the established statistical learning theory. Recent studieshave revealed that properties of training trajectories can be indicative ofgeneralization. Building on this insight state-of-the-art methods haveleveraged the topology of these trajectories particularly their fractaldimension to quantify generalization. Most existing works compute thisquantity by assuming continuous- or infinite-time training dynamicscomplicating the development of practical estimators capable of accuratelypredicting generalization without access to test data. In this paper werespect the discrete-time nature of training trajectories and investigate theunderlying topological quantities that can be amenable to topological dataanalysis tools. This leads to a new family of reliable topological complexitymeasures that provably bound the generalization error eliminating the need forrestrictive geometric assumptions. These measures are computationally friendlyenabling us to propose simple yet effective algorithms for computinggeneralization indices. Moreover our flexible framework can be extended todifferent domains tasks and architectures. Our experimental resultsdemonstrate that our new complexity measures correlate highly withgeneralization error in industry-standards architectures such as transformersand deep graph networks. Our approach consistently outperforms existingtopological bounds across a wide range of datasets models and optimizershighlighting the practical relevance and effectiveness of our complexitymeasures.</p>
                <p>Last Updated: 2024-07-11 17:56:03 UTC</p>
                <button class="interpret-button" data-id="2407.08723v1">Interpret</button>
                <div id="interpretation-2407.08723v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Unifying 3D Representation and Control of Diverse Robots with a Single Camera</h3>
                <p>Authors: Sizhe Lester LiAnnan ZhangBoyuan ChenHanna MatusikChao LiuDaniela RusVincent Sitzmann</p>
                <p><a href="http://arxiv.org/abs/2407.08722v1">Link to paper</a></p>
                <p>Mirroring the complex structures and diverse functions of natural organismsis a long-standing challenge in robotics. Modern fabrication techniques havedramatically expanded feasible hardware yet deploying these systems requirescontrol software to translate desired motions into actuator commands. Whileconventional robots can easily be modeled as rigid links connected via jointsit remains an open challenge to model and control bio-inspired robots that areoften multi-material or soft lack sensing capabilities and may change theirmaterial properties with use. Here we introduce Neural Jacobian Fields anarchitecture that autonomously learns to model and control robots from visionalone. Our approach makes no assumptions about the robots materialsactuation or sensing requires only a single camera for control and learns tocontrol the robot without expert intervention by observing the execution ofrandom commands. We demonstrate our method on a diverse set of robotmanipulators varying in actuation materials fabrication and cost. Ourapproach achieves accurate closed-loop control and recovers the causal dynamicstructure of each robot. By enabling robot control with a generic camera as theonly sensor we anticipate our work will dramatically broaden the design spaceof robotic systems and serve as a starting point for lowering the barrier torobotic automation.</p>
                <p>Last Updated: 2024-07-11 17:55:49 UTC</p>
                <button class="interpret-button" data-id="2407.08722v1">Interpret</button>
                <div id="interpretation-2407.08722v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>MAVIS: Mathematical Visual Instruction Tuning</h3>
                <p>Authors: Renrui ZhangXinyu WeiDongzhi JiangYichi ZhangZiyu GuoChengzhuo TongJiaming LiuAojun ZhouBin WeiShanghang ZhangPeng GaoHongsheng Li</p>
                <p><a href="http://arxiv.org/abs/2407.08739v1">Link to paper</a></p>
                <p>Multi-modal Large Language Models MLLMs have recently emerged as asignificant focus in academia and industry. Despite their proficiency ingeneral multi-modal scenarios the mathematical problem-solving capabilities invisual contexts remain insufficiently explored. We identify three key areaswithin MLLMs that need to be improved: visual encoding of math diagramsdiagram-language alignment and mathematical reasoning skills. This draws forthan urgent demand for large-scale high-quality data and training pipelines invisual mathematics. In this paper we propose MAVIS the first MAthematicalVISual instruction tuning paradigm for MLLMs involving a series ofmathematical visual datasets and specialized MLLMs. Targeting the three issuesMAVIS contains three progressive training stages from scratch. First we curateMAVIS-Caption consisting of 558K diagram-caption pairs to fine-tune amath-specific vision encoder CLIP-Math through contrastive learning tailoredfor improved diagram visual encoding. Second we utilize MAVIS-Caption to alignthe CLIP-Math with a large language model LLM by a projection layerenhancing vision-language alignment in mathematical domains. Third weintroduce MAVIS-Instruct including 900K meticulously collected and annotatedvisual math problems which is adopted to finally instruct-tune the MLLM forrobust mathematical reasoning skills. In MAVIS-Instruct we incorporatecomplete chain-of-thought CoT rationales for each problem and minimizetextual redundancy thereby concentrating the model towards the visualelements. Data and Models are released at https://github.com/ZrrSkywalker/MAVIS</p>
                <p>Last Updated: 2024-07-11 17:59:47 UTC</p>
                <button class="interpret-button" data-id="2407.08739v1">Interpret</button>
                <div id="interpretation-2407.08739v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Video Diffusion Alignment via Reward Gradients</h3>
                <p>Authors: Mihir PrabhudesaiRussell MendoncaZheyang QinKaterina FragkiadakiDeepak Pathak</p>
                <p><a href="http://arxiv.org/abs/2407.08737v1">Link to paper</a></p>
                <p>We have made significant progress towards building foundational videodiffusion models. As these models are trained using large-scale unsuperviseddata it has become crucial to adapt these models to specific downstream tasks.Adapting these models via supervised fine-tuning requires collecting targetdatasets of videos which is challenging and tedious. In this work we utilizepre-trained reward models that are learned via preferences on top of powerfulvision discriminative models to adapt video diffusion models. These modelscontain dense gradient information with respect to generated RGB pixels whichis critical to efficient learning in complex search spaces such as videos. Weshow that backpropagating gradients from these reward models to a videodiffusion model can allow for compute and sample efficient alignment of thevideo diffusion model. We show results across a variety of reward models andvideo diffusion models demonstrating that our approach can learn much moreefficiently in terms of reward queries and computation than prior gradient-freeapproaches. Our code model weightsand more visualization are available athttps://vader-vid.github.io.</p>
                <p>Last Updated: 2024-07-11 17:59:45 UTC</p>
                <button class="interpret-button" data-id="2407.08737v1">Interpret</button>
                <div id="interpretation-2407.08737v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>BiEquiFormer: Bi-Equivariant Representations for Global Point Cloud Registration</h3>
                <p>Authors: Stefanos PertigkiozoglouEvangelos ChatzipantazisKostas Daniilidis</p>
                <p><a href="http://arxiv.org/abs/2407.08729v1">Link to paper</a></p>
                <p>The goal of this paper is to address the problem of textitglobal pointcloud registration PCR i.e. finding the optimal alignment between pointclouds irrespective of the initial poses of the scans. This problem isnotoriously challenging for classical optimization methods due to computationalconstraints. First we show that state-of-the-art deep learning methods sufferfrom huge performance degradation when the point clouds are arbitrarily placedin space. We propose that textitequivariant deep learning should be utilizedfor solving this task and we characterize the specific type of bi-equivarianceof PCR. Then we design BiEquiformer a novel and scalabletextitbi-equivariant pipeline i.e. equivariant to the independenttransformations of the input point clouds. While a naive approach would processthe point clouds independently we design expressive bi-equivariant layers thatfuse the information from both point clouds. This allows us to extracthigh-quality superpoint correspondences and in turn robust point-cloudregistration. Extensive comparisons against state-of-the-art methods show thatour method achieves comparable performance in the canonical setting andsuperior performance in the robust setting in both the 3DMatch and thechallenging low-overlap 3DLoMatch dataset.</p>
                <p>Last Updated: 2024-07-11 17:58:10 UTC</p>
                <button class="interpret-button" data-id="2407.08729v1">Interpret</button>
                <div id="interpretation-2407.08729v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Map It Anywhere (MIA): Empowering Bird's Eye View Mapping using Large-scale Public Data</h3>
                <p>Authors: Cherie HoJiaye ZouOmar AlamaSai Mitheran Jagadesh KumarBenjamin ChiangTaneesh GuptaChen WangNikhil KeethaKatia SycaraSebastian Scherer</p>
                <p><a href="http://arxiv.org/abs/2407.08726v1">Link to paper</a></p>
                <p>Top-down Birds Eye View BEV maps are a popular representation for groundrobot navigation due to their richness and flexibility for downstream tasks.While recent methods have shown promise for predicting BEV maps fromFirst-Person View FPV images their generalizability is limited to smallregions captured by current autonomous vehicle-based datasets. In this contextwe show that a more scalable approach towards generalizable map prediction canbe enabled by using two large-scale crowd-sourced mapping platforms Mapillaryfor FPV images and OpenStreetMap for BEV semantic maps. We introduce Map ItAnywhere MIA a data engine that enables seamless curation and modeling oflabeled map prediction data from existing open-source map platforms. Using ourMIA data engine we display the ease of automatically collecting a dataset of1.2 million pairs of FPV images  BEV maps encompassing diverse geographieslandscapes environmental factors camera models  capture scenarios. Wefurther train a simple camera model-agnostic model on this data for BEV mapprediction. Extensive evaluations using established benchmarks and our datasetshow that the data curated by MIA enables effective pretraining forgeneralizable BEV map prediction with zero-shot performance far exceedingbaselines trained on existing datasets by 35. Our analysis highlights thepromise of using large-scale public maps for developing  testing generalizableBEV perception paving the way for more robust autonomous navigation.</p>
                <p>Last Updated: 2024-07-11 17:57:22 UTC</p>
                <button class="interpret-button" data-id="2407.08726v1">Interpret</button>
                <div id="interpretation-2407.08726v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MetaUrban: A Simulation Platform for Embodied AI in Urban Spaces</h3>
                <p>Authors: Wayne WuHonglin HeYiran WangChenda DuanJack HeZhizheng LiuQuanyi LiBolei Zhou</p>
                <p><a href="http://arxiv.org/abs/2407.08725v1">Link to paper</a></p>
                <p>Public urban spaces like streetscapes and plazas serve residents andaccommodate social life in all its vibrant variations. Recent advances inRobotics and Embodied AI make public urban spaces no longer exclusive tohumans. Food delivery bots and electric wheelchairs have started sharingsidewalks with pedestrians while diverse robot dogs and humanoids haverecently emerged in the street. Ensuring the generalizability and safety ofthese forthcoming mobile machines is crucial when navigating through thebustling streets in urban spaces. In this work we present MetaUrban acompositional simulation platform for Embodied AI research in urban spaces.MetaUrban can construct an infinite number of interactive urban scenes fromcompositional elements covering a vast array of ground plans objectplacements pedestrians vulnerable road users and other mobile agentsappearances and dynamics. We design point navigation and social navigationtasks as the pilot study using MetaUrban for embodied AI research and establishvarious baselines of Reinforcement Learning and Imitation Learning. Experimentsdemonstrate that the compositional nature of the simulated environments cansubstantially improve the generalizability and safety of the trained mobileagents. MetaUrban will be made publicly available to provide more researchopportunities and foster safe and trustworthy embodied AI in urban spaces.</p>
                <p>Last Updated: 2024-07-11 17:56:49 UTC</p>
                <button class="interpret-button" data-id="2407.08725v1">Interpret</button>
                <div id="interpretation-2407.08725v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Video Diffusion Alignment via Reward Gradients</h3>
                <p>Authors: Mihir PrabhudesaiRussell MendoncaZheyang QinKaterina FragkiadakiDeepak Pathak</p>
                <p><a href="http://arxiv.org/abs/2407.08737v1">Link to paper</a></p>
                <p>We have made significant progress towards building foundational videodiffusion models. As these models are trained using large-scale unsuperviseddata it has become crucial to adapt these models to specific downstream tasks.Adapting these models via supervised fine-tuning requires collecting targetdatasets of videos which is challenging and tedious. In this work we utilizepre-trained reward models that are learned via preferences on top of powerfulvision discriminative models to adapt video diffusion models. These modelscontain dense gradient information with respect to generated RGB pixels whichis critical to efficient learning in complex search spaces such as videos. Weshow that backpropagating gradients from these reward models to a videodiffusion model can allow for compute and sample efficient alignment of thevideo diffusion model. We show results across a variety of reward models andvideo diffusion models demonstrating that our approach can learn much moreefficiently in terms of reward queries and computation than prior gradient-freeapproaches. Our code model weightsand more visualization are available athttps://vader-vid.github.io.</p>
                <p>Last Updated: 2024-07-11 17:59:45 UTC</p>
                <button class="interpret-button" data-id="2407.08737v1">Interpret</button>
                <div id="interpretation-2407.08737v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Real-Time Anomaly Detection and Reactive Planning with Large Language Models</h3>
                <p>Authors: Rohan SinhaAmine ElhafsiChristopher AgiaMatthew FoutterEdward SchmerlingMarco Pavone</p>
                <p><a href="http://arxiv.org/abs/2407.08735v1">Link to paper</a></p>
                <p>Foundation models e.g. large language models LLMs trained oninternet-scale data possess zero-shot generalization capabilities that makethem a promising technology towards detecting and mitigatingout-of-distribution failure modes of robotic systems. Fully realizing thispromise however poses two challenges: i mitigating the considerablecomputational expense of these models such that they may be applied online andii incorporating their judgement regarding potential anomalies into a safecontrol framework. In this work we present a two-stage reasoning framework:First is a fast binary anomaly classifier that analyzes observations in an LLMembedding space which may then trigger a slower fallback selection stage thatutilizes the reasoning capabilities of generative LLMs. These stages correspondto branch points in a model predictive control strategy that maintains thejoint feasibility of continuing along various fallback plans to account for theslow reasoners latency as soon as an anomaly is detected thus ensuringsafety. We show that our fast anomaly classifier outperforms autoregressivereasoning with state-of-the-art GPT models even when instantiated withrelatively small language models. This enables our runtime monitor to improvethe trustworthiness of dynamic robotic systems such as quadrotors orautonomous vehicles under resource and time constraints. Videos illustratingour approach in both simulation and real-world experiments are available onthis project page: https://sites.google.com/view/aesop-llm.</p>
                <p>Last Updated: 2024-07-11 17:59:22 UTC</p>
                <button class="interpret-button" data-id="2407.08735v1">Interpret</button>
                <div id="interpretation-2407.08735v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Transformer Circuit Faithfulness Metrics are not Robust</h3>
                <p>Authors: Joseph MillerBilal ChughtaiWilliam Saunders</p>
                <p><a href="http://arxiv.org/abs/2407.08734v1">Link to paper</a></p>
                <p>Mechanistic interpretability work attempts to reverse engineer the learnedalgorithms present inside neural networks. One focus of this work has been todiscover circuits -- subgraphs of the full model that explain behaviour onspecific tasks. But how do we measure the performance of such circuits Priorwork has attempted to measure circuit faithfulness -- the degree to which thecircuit replicates the performance of the full model. In this work we surveymany considerations for designing experiments that measure circuit faithfulnessby ablating portions of the models computation. Concerningly we find existingmethods are highly sensitive to seemingly insignificant changes in the ablationmethodology. We conclude that existing circuit faithfulness scores reflect boththe methodological choices of researchers as well as the actual components ofthe circuit - the task a circuit is required to perform depends on the ablationused to test it. The ultimate goal of mechanistic interpretability work is tounderstand neural networks so we emphasize the need for more clarity in theprecise claims being made about circuits. We open source a library athttps://github.com/UFO-101/auto-circuit that includes highly efficientimplementations of a wide range of ablation methodologies and circuit discoveryalgorithms.</p>
                <p>Last Updated: 2024-07-11 17:59:00 UTC</p>
                <button class="interpret-button" data-id="2407.08734v1">Interpret</button>
                <div id="interpretation-2407.08734v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MetaUrban: A Simulation Platform for Embodied AI in Urban Spaces</h3>
                <p>Authors: Wayne WuHonglin HeYiran WangChenda DuanJack HeZhizheng LiuQuanyi LiBolei Zhou</p>
                <p><a href="http://arxiv.org/abs/2407.08725v1">Link to paper</a></p>
                <p>Public urban spaces like streetscapes and plazas serve residents andaccommodate social life in all its vibrant variations. Recent advances inRobotics and Embodied AI make public urban spaces no longer exclusive tohumans. Food delivery bots and electric wheelchairs have started sharingsidewalks with pedestrians while diverse robot dogs and humanoids haverecently emerged in the street. Ensuring the generalizability and safety ofthese forthcoming mobile machines is crucial when navigating through thebustling streets in urban spaces. In this work we present MetaUrban acompositional simulation platform for Embodied AI research in urban spaces.MetaUrban can construct an infinite number of interactive urban scenes fromcompositional elements covering a vast array of ground plans objectplacements pedestrians vulnerable road users and other mobile agentsappearances and dynamics. We design point navigation and social navigationtasks as the pilot study using MetaUrban for embodied AI research and establishvarious baselines of Reinforcement Learning and Imitation Learning. Experimentsdemonstrate that the compositional nature of the simulated environments cansubstantially improve the generalizability and safety of the trained mobileagents. MetaUrban will be made publicly available to provide more researchopportunities and foster safe and trustworthy embodied AI in urban spaces.</p>
                <p>Last Updated: 2024-07-11 17:56:49 UTC</p>
                <button class="interpret-button" data-id="2407.08725v1">Interpret</button>
                <div id="interpretation-2407.08725v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WhisperNetV2: SlowFast Siamese Network For Lip-Based Biometrics</h3>
                <p>Authors: Abdollah ZakeriHamid HassanpourMohammad Hossein KhosraviAmir Masoud Nourollah</p>
                <p><a href="http://arxiv.org/abs/2407.08717v1">Link to paper</a></p>
                <p>Lip-based biometric authentication LBBA has attracted many researchersduring the last decade. The lip is specifically interesting for biometricresearchers because it is a twin biometric with the potential to function bothas a physiological and a behavioral trait. Although much valuable research wasconducted on LBBA none of them considered the different emotions of the clientduring the video acquisition step of LBBA which can potentially affect theclients facial expressions and speech tempo. We proposed a novel networkstructure called WhisperNetV2 which extends our previously proposed networkcalled WhisperNet. Our proposed network leverages a deep Siamese structure withtriplet loss having three identical SlowFast networks as embedding networks.The SlowFast network is an excellent candidate for our task since the fastpathway extracts motion-related features behavioral lip movements with a highframe rate and low channel capacity. The slow pathway extracts visual featuresphysiological lip appearance with a low frame rate and high channel capacity.Using an open-set protocol we trained our network using the CREMA-D datasetand acquired an Equal Error Rate EER of 0.005 on the test set. Consideringthat the acquired EER is less than most similar LBBA methods our method can beconsidered as a state-of-the-art LBBA method.</p>
                <p>Last Updated: 2024-07-11 17:51:49 UTC</p>
                <button class="interpret-button" data-id="2407.08717v1">Interpret</button>
                <div id="interpretation-2407.08717v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>How to beat a Bayesian adversary</h3>
                <p>Authors: Zihan DingKexin JinJonas LatzChenguang Liu</p>
                <p><a href="http://arxiv.org/abs/2407.08678v1">Link to paper</a></p>
                <p>Deep neural networks and other modern machine learning models are oftensusceptible to adversarial attacks. Indeed an adversary may often be able tochange a models prediction through a small directed perturbation of themodels input - an issue in safety-critical applications. Adversarially robustmachine learning is usually based on a minmax optimisation problem thatminimises the machine learning loss under maximisation-based adversarialattacks.  In this work we study adversaries that determine their attack using aBayesian statistical approach rather than maximisation. The resulting Bayesianadversarial robustness problem is a relaxation of the usual minmax problem. Tosolve this problem we propose Abram - a continuous-time particle system thatshall approximate the gradient flow corresponding to the underlying learningproblem. We show that Abram approximates a McKean-Vlasov process and justifythe use of Abram by giving assumptions under which the McKean-Vlasov processfinds the minimiser of the Bayesian adversarial robustness problem. We discusstwo ways to discretise Abram and show its suitability in benchmark adversarialdeep learning experiments.</p>
                <p>Last Updated: 2024-07-11 17:12:42 UTC</p>
                <button class="interpret-button" data-id="2407.08678v1">Interpret</button>
                <div id="interpretation-2407.08678v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Estimation of spatio-temporal extremes via generative neural networks</h3>
                <p>Authors: Christopher BÃ¼lteLisa LeimenstollMelanie Schienle</p>
                <p><a href="http://arxiv.org/abs/2407.08668v1">Link to paper</a></p>
                <p>Recent methods in modeling spatial extreme events have focused on utilizingparametric max-stable processes and their underlying dependence structure. Inthis work we provide a unified approach for analyzing spatial extremes withlittle available data by estimating the distribution of model parameters or thespatial dependence directly. By employing recent developments in generativeneural networks we predict a full sample-based distribution allowing fordirect assessment of uncertainty regarding model parameters or other parameterdependent functionals. We validate our method by fitting several simulatedmax-stable processes showing a high accuracy of the approach regardingparameter estimation as well as uncertainty quantification. Additionalrobustness checks highlight the generalization and extrapolation capabilitiesof the model while an application to precipitation extremes across WesternGermany demonstrates the usability of our approach in real-world scenarios.</p>
                <p>Last Updated: 2024-07-11 16:57:17 UTC</p>
                <button class="interpret-button" data-id="2407.08668v1">Interpret</button>
                <div id="interpretation-2407.08668v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Adaptive Smooth Non-Stationary Bandits</h3>
                <p>Authors: Joe Suk</p>
                <p><a href="http://arxiv.org/abs/2407.08654v1">Link to paper</a></p>
                <p>We study a K-armed non-stationary bandit model where rewards changesmoothly as captured by Holder class assumptions on rewards as functionsof time. Such smooth changes are parametrized by a Holder exponent betaand coefficient lambda. While various sub-cases of this general model havebeen studied in isolation we first establish the minimax dynamic regret rategenerally for all Kbetalambda. Next we show this optimal dynamic regretcan be attained adaptively without knowledge of betalambda. To contrasteven with parameter knowledge upper bounds were only previously known forlimited regimes betaleq 1 and beta2 Slivkins 2014 Krishnamurthy andGopalan 2021 Manegueu et al. 2021 Jia et al.2023. Thus our work resolvesopen questions raised by these disparate threads of the literature.  We also study the problem of attaining faster gap-dependent regret rates innon-stationary bandits. While such rates are long known to be impossible ingeneral Garivier and Moulines 2011 we show that environments admitting asafe arm Suk and Kpotufe 2022 allow for much faster rates than theworst-case scaling with sqrtT. While previous works in this directionfocused on attaining the usual logarithmic regret bounds as summed overstationary periods our new gap-dependent rates reveal new optimistic regimesof non-stationarity where even the logarithmic bounds are pessimistic. We showour new gap-dependent rate is tight and that its achievability i.e. as madepossible by a safe arm has a surprisingly simple and clean characterizationwithin the smooth Holder class model.</p>
                <p>Last Updated: 2024-07-11 16:37:15 UTC</p>
                <button class="interpret-button" data-id="2407.08654v1">Interpret</button>
                <div id="interpretation-2407.08654v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-Group Proportional Representation</h3>
                <p>Authors: Alex OesterlingClaudio Mayrink VerdunCarol Xuan LongAlex GlynnLucas Monteiro PaesSajani VithanaMartina CardoneFlavio P. Calmon</p>
                <p><a href="http://arxiv.org/abs/2407.08571v1">Link to paper</a></p>
                <p>Image search and retrieval tasks can perpetuate harmful stereotypes erasecultural identities and amplify social disparities. Current approaches tomitigate these representational harms balance the number of retrieved itemsacross population groups defined by a small number of often binaryattributes. However most existing methods overlook intersectional groupsdetermined by combinations of group attributes such as gender race andethnicity. We introduce Multi-Group Proportional Representation MPR a novelmetric that measures representation across intersectional groups. We developpractical methods for estimating MPR provide theoretical guarantees andpropose optimization algorithms to ensure MPR in retrieval. We demonstrate thatexisting methods optimizing for equal and proportional representation metricsmay fail to promote MPR. Crucially our work shows that optimizing MPR yieldsmore proportional representation across multiple intersectional groupsspecified by a rich function class often with minimal compromise in retrievalaccuracy.</p>
                <p>Last Updated: 2024-07-11 14:59:17 UTC</p>
                <button class="interpret-button" data-id="2407.08571v1">Interpret</button>
                <div id="interpretation-2407.08571v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Causal inference through multi-stage learning and doubly robust deep neural networks</h3>
                <p>Authors: Yuqian ZhangJelena Bradic</p>
                <p><a href="http://arxiv.org/abs/2407.08560v1">Link to paper</a></p>
                <p>Deep neural networks DNNs have demonstrated remarkable empiricalperformance in large-scale supervised learning problems particularly inscenarios where both the sample size n and the dimension of covariates pare large. This study delves into the application of DNNs across a widespectrum of intricate causal inference tasks where direct estimation fallsshort and necessitates multi-stage learning. Examples include estimating theconditional average treatment effect and dynamic treatment effect. In thisframework DNNs are constructed sequentially with subsequent stages buildingupon preceding ones. To mitigate the impact of estimation errors from earlystages on subsequent ones we integrate DNNs in a doubly robust manner. Incontrast to previous research our study offers theoretical assurancesregarding the effectiveness of DNNs in settings where the dimensionality pexpands with the sample size. These findings are significant independently andextend to degenerate single-stage learning problems.</p>
                <p>Last Updated: 2024-07-11 14:47:44 UTC</p>
                <button class="interpret-button" data-id="2407.08560v1">Interpret</button>
                <div id="interpretation-2407.08560v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-07-13</p>
        </div>
    
        </div>
    </body>
    </html>
    