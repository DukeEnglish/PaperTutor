
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
        </div>
        <div id="content">
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Vlogger: Make Your Dream A Vlog</h3>
                <p>Authors: Shaobin ZhuangKunchang LiXinyuan ChenYaohui WangZiwei LiuYu QiaoYali Wang</p>
                <p><a href="http://arxiv.org/abs/2401.09414v1">Link to paper</a></p>
                <p>In this work we present Vlogger a generic AI system for generating aminute-level video blog i.e. vlog of user descriptions. Different from shortvideos with a few seconds vlog often contains a complex storyline withdiversified scenes which is challenging for most existing video generationapproaches. To break through this bottleneck our Vlogger smartly leveragesLarge Language Model LLM as Director and decomposes a long video generationtask of vlog into four key stages where we invoke various foundation models toplay the critical roles of vlog professionals including 1 Script 2 Actor3 ShowMaker and 4 Voicer. With such a design of mimicking human beingsour Vlogger can generate vlogs through explainable cooperation of top-downplanning and bottom-up shooting. Moreover we introduce a novel video diffusionmodel ShowMaker which serves as a videographer in our Vlogger for generatingthe video snippet of each shooting scene. By incorporating Script and Actorattentively as textual and visual prompts it can effectively enhancespatial-temporal coherence in the snippet. Besides we design a concise mixedtraining paradigm for ShowMaker boosting its capacity for both T2V generationand prediction. Finally the extensive experiments show that our methodachieves state-of-the-art performance on zero-shot T2V generation andprediction tasks. More importantly Vlogger can generate over 5-minute vlogsfrom open-world descriptions without loss of video coherence on script andactor. The code and model is all available athttps://github.com/zhuangshaobin/Vlogger.</p>
                <p>Last Updated: 2024-01-17 18:55:12 UTC</p>
            </li>
        
            <li>
                <h3>Through the Looking-Glass: Transparency Implications and Challenges in Enterprise AI Knowledge Systems</h3>
                <p>Authors: Karina Cortiñas-LorenzoSiân LindleyIda Larsen-LedetBhaskar Mitra</p>
                <p><a href="http://arxiv.org/abs/2401.09410v1">Link to paper</a></p>
                <p>Knowledge cant be disentangled from people. As AI knowledge systems minevast volumes of work-related data the knowledge thats being extracted andsurfaced is intrinsically linked to the people who create and use it. Whenthese systems get embedded in organizational settings the information that isbrought to the foreground and the information thats pushed to the peripherycan influence how individuals see each other and how they see themselves atwork. In this paper we present the looking-glass metaphor and use it toconceptualize AI knowledge systems as systems that reflect and distortexpanding our view on transparency requirements implications and challenges.We formulate transparency as a key mediator in shaping different ways ofseeing including seeing into the system which unveils its capabilitieslimitations and behavior and seeing through the system which shapes workersperceptions of their own contributions and others within the organization.Recognizing the sociotechnical nature of these systems we identify threetransparency dimensions necessary to realize the value of AI knowledge systemsnamely system transparency procedural transparency and transparency ofoutcomes. We discuss key challenges hindering the implementation of these formsof transparency bringing to light the wider sociotechnical gap andhighlighting directions for future Computer-supported Cooperative Work CSCWresearch.</p>
                <p>Last Updated: 2024-01-17 18:47:30 UTC</p>
            </li>
        
            <li>
                <h3>Neural Contractive Dynamical Systems</h3>
                <p>Authors: Hadi Beik-MohammadiSøren HaubergGeorgios ArvanitidisNadia FigueroaGerhard NeumannLeonel Rozo</p>
                <p><a href="http://arxiv.org/abs/2401.09352v1">Link to paper</a></p>
                <p>Stability guarantees are crucial when ensuring a fully autonomous robot doesnot take undesirable or potentially harmful actions. Unfortunately globalstability guarantees are hard to provide in dynamical systems learned fromdata especially when the learned dynamics are governed by neural networks. Wepropose a novel methodology to learn neural contractive dynamical systemswhere our neural architecture ensures contraction and hence global stability.To efficiently scale the method to high-dimensional dynamical systems wedevelop a variant of the variational autoencoder that learns dynamics in alow-dimensional latent representation space while retaining contractivestability after decoding. We further extend our approach to learningcontractive systems on the Lie group of rotations to account for full-poseend-effector dynamic motions. The result is the first highly flexible learningarchitecture that provides contractive stability guarantees with capability toperform obstacle avoidance. Empirically we demonstrate that our approachencodes the desired dynamics more accurately than the current state-of-the-artwhich provides less strong stability guarantees.</p>
                <p>Last Updated: 2024-01-17 17:18:21 UTC</p>
            </li>
        
            <li>
                <h3>SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding</h3>
                <p>Authors: Baoxiong JiaYixin ChenHuangyue YuYan WangXuesong NiuTengyu LiuQing LiSiyuan Huang</p>
                <p><a href="http://arxiv.org/abs/2401.09340v1">Link to paper</a></p>
                <p>3D vision-language grounding which focuses on aligning language with the 3Dphysical environment stands as a cornerstone in the development of embodiedagents. In comparison to recent advancements in the 2D domain groundinglanguage in 3D scenes faces several significant challenges: i the inherentcomplexity of 3D scenes due to the diverse object configurations their richattributes and intricate relationships ii the scarcity of paired 3Dvision-language data to support grounded learning and iii the absence of aunified learning framework to distill knowledge from grounded 3D data. In thiswork we aim to address these three major challenges in 3D vision-language byexamining the potential of systematically upscaling 3D vision-language learningin indoor environments. We introduce the first million-scale 3D vision-languagedataset SceneVerse encompassing about 68K 3D indoor scenes and comprising2.5M vision-language pairs derived from both human annotations and our scalablescene-graph-based generation approach. We demonstrate that this scaling allowsfor a unified pre-training framework Grounded Pre-training for Scenes GPSfor 3D vision-language learning. Through extensive experiments we showcase theeffectiveness of GPS by achieving state-of-the-art performance on all existing3D visual grounding benchmarks. The vast potential of SceneVerse and GPS isunveiled through zero-shot transfer experiments in the challenging 3Dvision-language tasks. Project website: https://scene-verse.github.io .</p>
                <p>Last Updated: 2024-01-17 17:04:35 UTC</p>
            </li>
        
            <li>
                <h3>Large Language Models Are Neurosymbolic Reasoners</h3>
                <p>Authors: Meng FangShilong DengYudi ZhangZijing ShiLing ChenMykola PechenizkiyJun Wang</p>
                <p><a href="http://arxiv.org/abs/2401.09334v1">Link to paper</a></p>
                <p>A wide range of real-world applications is characterized by their symbolicnature necessitating a strong capability for symbolic reasoning. This paperinvestigates the potential application of Large Language Models LLMs assymbolic reasoners. We focus on text-based games significant benchmarks foragents with natural language capabilities particularly in symbolic tasks likemath map reading sorting and applying common sense in text-based worlds. Tofacilitate these agents we propose an LLM agent designed to tackle symbolicchallenges and achieve in-game objectives. We begin by initializing the LLMagent and informing it of its role. The agent then receives observations and aset of valid actions from the text-based games along with a specific symbolicmodule. With these inputs the LLM agent chooses an action and interacts withthe game environments. Our experimental results demonstrate that our methodsignificantly enhances the capability of LLMs as automated agents for symbolicreasoning and our LLM agent is effective in text-based games involvingsymbolic tasks achieving an average performance of 88 across all tasks.</p>
                <p>Last Updated: 2024-01-17 16:57:19 UTC</p>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>GARField: Group Anything with Radiance Fields</h3>
                <p>Authors: Chung Min KimMingxuan WuJustin KerrKen GoldbergMatthew TancikAngjoo Kanazawa</p>
                <p><a href="http://arxiv.org/abs/2401.09419v1">Link to paper</a></p>
                <p>Grouping is inherently ambiguous due to the multiple levels of granularity inwhich one can decompose a scene -- should the wheels of an excavator beconsidered separate or part of the whole We present Group Anything withRadiance Fields GARField an approach for decomposing 3D scenes into ahierarchy of semantically meaningful groups from posed image inputs. To do thiswe embrace group ambiguity through physical scale: by optimizing ascale-conditioned 3D affinity feature field a point in the world can belong todifferent groups of different sizes. We optimize this field from a set of 2Dmasks provided by Segment Anything SAM in a way that respects coarse-to-finehierarchy using scale to consistently fuse conflicting masks from differentviewpoints. From this field we can derive a hierarchy of possible groupings viaautomatic tree construction or user interaction. We evaluate GARField on avariety of in-the-wild scenes and find it effectively extracts groups at manylevels: clusters of objects objects and various subparts. GARField inherentlyrepresents multi-view consistent groupings and produces higher fidelity groupsthan the input SAM masks. GARFields hierarchical grouping could have excitingdownstream applications such as 3D asset extraction or dynamic sceneunderstanding. See the project website at https://www.garfield.studio/</p>
                <p>Last Updated: 2024-01-17 18:57:53 UTC</p>
            </li>
        
            <li>
                <h3>Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model</h3>
                <p>Authors: Lianghui ZhuBencheng LiaoQian ZhangXinlong WangWenyu LiuXinggang Wang</p>
                <p><a href="http://arxiv.org/abs/2401.09417v1">Link to paper</a></p>
                <p>Recently the state space models SSMs with efficient hardware-aware designsi.e. Mamba have shown great potential for long sequence modeling. Buildingefficient and generic vision backbones purely upon SSMs is an appealingdirection. However representing visual data is challenging for SSMs due to theposition-sensitivity of visual data and the requirement of global context forvisual understanding. In this paper we show that the reliance of visualrepresentation learning on self-attention is not necessary and propose a newgeneric vision backbone with bidirectional Mamba blocks Vim which marks theimage sequences with position embeddings and compresses the visualrepresentation with bidirectional state space models. On ImageNetclassification COCO object detection and ADE20k semantic segmentation tasksVim achieves higher performance compared to well-established visiontransformers like DeiT while also demonstrating significantly improvedcomputation  memory efficiency. For example Vim is 2.8times faster thanDeiT and saves 86.8 GPU memory when performing batch inference to extractfeatures on images with a resolution of 1248times1248. The resultsdemonstrate that Vim is capable of overcoming the computation  memoryconstraints on performing Transformer-style understanding for high-resolutionimages and it has great potential to become the next-generation backbone forvision foundation models. Code is available at https://github.com/hustvl/Vim.</p>
                <p>Last Updated: 2024-01-17 18:56:18 UTC</p>
            </li>
        
            <li>
                <h3>TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion</h3>
                <p>Authors: Yu-Ying YehJia-Bin HuangChangil KimLei XiaoThu Nguyen-PhuocNumair KhanCheng ZhangManmohan ChandrakerCarl S MarshallZhao DongZhengqin Li</p>
                <p><a href="http://arxiv.org/abs/2401.09416v1">Link to paper</a></p>
                <p>We present TextureDreamer a novel image-guided texture synthesis method totransfer relightable textures from a small number of input images 3 to 5 totarget 3D shapes across arbitrary categories. Texture creation is a pivotalchallenge in vision and graphics. Industrial companies hire experienced artiststo manually craft textures for 3D assets. Classical methods require denselysampled views and accurately aligned geometry while learning-based methods areconfined to category-specific shapes within the dataset. In contrastTextureDreamer can transfer highly detailed intricate textures from real-worldenvironments to arbitrary objects with only a few casually captured imagespotentially significantly democratizing texture creation. Our core ideapersonalized geometry-aware score distillation PGSD draws inspiration fromrecent advancements in diffuse models including personalized modeling fortexture information extraction variational score distillation for detailedappearance synthesis and explicit geometry guidance with ControlNet. Ourintegration and several essential modifications substantially improve thetexture quality. Experiments on real images spanning different categories showthat TextureDreamer can successfully transfer highly realistic semanticmeaningful texture to arbitrary objects surpassing the visual quality ofprevious state-of-the-art.</p>
                <p>Last Updated: 2024-01-17 18:55:49 UTC</p>
            </li>
        
            <li>
                <h3>Vlogger: Make Your Dream A Vlog</h3>
                <p>Authors: Shaobin ZhuangKunchang LiXinyuan ChenYaohui WangZiwei LiuYu QiaoYali Wang</p>
                <p><a href="http://arxiv.org/abs/2401.09414v1">Link to paper</a></p>
                <p>In this work we present Vlogger a generic AI system for generating aminute-level video blog i.e. vlog of user descriptions. Different from shortvideos with a few seconds vlog often contains a complex storyline withdiversified scenes which is challenging for most existing video generationapproaches. To break through this bottleneck our Vlogger smartly leveragesLarge Language Model LLM as Director and decomposes a long video generationtask of vlog into four key stages where we invoke various foundation models toplay the critical roles of vlog professionals including 1 Script 2 Actor3 ShowMaker and 4 Voicer. With such a design of mimicking human beingsour Vlogger can generate vlogs through explainable cooperation of top-downplanning and bottom-up shooting. Moreover we introduce a novel video diffusionmodel ShowMaker which serves as a videographer in our Vlogger for generatingthe video snippet of each shooting scene. By incorporating Script and Actorattentively as textual and visual prompts it can effectively enhancespatial-temporal coherence in the snippet. Besides we design a concise mixedtraining paradigm for ShowMaker boosting its capacity for both T2V generationand prediction. Finally the extensive experiments show that our methodachieves state-of-the-art performance on zero-shot T2V generation andprediction tasks. More importantly Vlogger can generate over 5-minute vlogsfrom open-world descriptions without loss of video coherence on script andactor. The code and model is all available athttps://github.com/zhuangshaobin/Vlogger.</p>
                <p>Last Updated: 2024-01-17 18:55:12 UTC</p>
            </li>
        
            <li>
                <h3>POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images</h3>
                <p>Authors: Antonin VobeckyOriane SiméoniDavid HurychSpyros GidarisAndrei BursucPatrick PérezJosef Sivic</p>
                <p><a href="http://arxiv.org/abs/2401.09413v1">Link to paper</a></p>
                <p>We describe an approach to predict open-vocabulary 3D semantic voxeloccupancy map from input 2D images with the objective of enabling 3D groundingsegmentation and retrieval of free-form language queries. This is a challengingproblem because of the 2D-3D ambiguity and the open-vocabulary nature of thetarget tasks where obtaining annotated training data in 3D is difficult. Thecontributions of this work are three-fold. First we design a new modelarchitecture for open-vocabulary 3D semantic occupancy prediction. Thearchitecture consists of a 2D-3D encoder together with occupancy prediction and3D-language heads. The output is a dense voxel map of 3D grounded languageembeddings enabling a range of open-vocabulary tasks. Second we develop atri-modal self-supervised learning algorithm that leverages three modalities:i images ii language and iii LiDAR point clouds and enables trainingthe proposed architecture using a strong pre-trained vision-language modelwithout the need for any 3D manual language annotations. Finally wedemonstrate quantitatively the strengths of the proposed model on severalopen-vocabulary tasks: Zero-shot 3D semantic segmentation using existingdatasets 3D grounding and retrieval of free-form language queries using asmall dataset that we propose as an extension of nuScenes. You can find theproject page here https://vobecant.github.io/POP3D.</p>
                <p>Last Updated: 2024-01-17 18:51:53 UTC</p>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text</h3>
                <p>Authors: Mazal BethanyBrandon WherryEmet BethanyNishant VishwamitraPeyman Najafirad</p>
                <p><a href="http://arxiv.org/abs/2401.09407v1">Link to paper</a></p>
                <p>With the recent proliferation of Large Language Models LLMs there has beenan increasing demand for tools to detect machine-generated text. The effectivedetection of machine-generated text face two pertinent problems: First theyare severely limited in generalizing against real-world scenarios wheremachine-generated text is produced by a variety of generators including butnot limited to GPT-4 and Dolly and spans diverse domains ranging fromacademic manuscripts to social media posts. Second existing detectionmethodologies treat texts produced by LLMs through a restrictive binaryclassification lens neglecting the nuanced diversity of artifacts generated bydifferent LLMs. In this work we undertake a systematic study on the detectionof machine-generated text in real-world scenarios. We first study theeffectiveness of state-of-the-art approaches and find that they are severelylimited against text produced by diverse generators and domains in the realworld. Furthermore t-SNE visualizations of the embeddings from a pretrainedLLMs encoder show that they cannot reliably distinguish between human andmachine-generated text. Based on our findings we introduce a novel systemT5LLMCipher for detecting machine-generated text using a pretrained T5 encodercombined with LLM embedding sub-clustering to address the text produced bydiverse generators and domains in the real world. We evaluate our approachacross 9 machine-generated text systems and 9 domains and find that ourapproach provides state-of-the-art generalization ability with an averageincrease in F1 score on machine-generated text of 19.6 on unseen generatorsand domains compared to the top performing existing approaches and correctlyattributes the generator of text with an accuracy of 93.6.</p>
                <p>Last Updated: 2024-01-17 18:45:13 UTC</p>
            </li>
        
            <li>
                <h3>Stuck in the Quicksand of Numeracy, Far from AGI Summit: Evaluating LLMs' Mathematical Competency through Ontology-guided Perturbations</h3>
                <p>Authors: Pengfei HongDeepanway GhosalNavonil MajumderSomak AdityaRada MihalceaSoujanya Poria</p>
                <p><a href="http://arxiv.org/abs/2401.09395v1">Link to paper</a></p>
                <p>Recent advancements in Large Language Models LLMs have showcased strikingresults on existing logical reasoning benchmarks with some models evensurpassing human performance. However the true depth of their competencies androbustness in mathematical reasoning tasks remains an open question. Inresponse we develop i an ontology of perturbations of maths questions iia semi-automatic method of perturbation and iii a dataset of perturbed mathsquestions to probe the limits of LLM capabilities in mathematical reasoningtasks. These controlled perturbations span across multiple fine dimensions ofthe structural and representational aspects of maths questions. Using GPT-4 wegenerated the MORE dataset by perturbing randomly selected five seed questionsfrom GSM8K. This process was guided by our ontology and involved a thoroughautomatic and manual filtering process yielding a set of 216 maths problems.We conducted comprehensive evaluation of both closed-source and open-sourceLLMs on MORE. The results show a significant performance drop across all themodels against the perturbed questions. This strongly suggests that currentLLMs lack robust mathematical skills and deep reasoning abilities. Thisresearch not only identifies multiple gaps in the capabilities of currentmodels but also highlights multiple potential directions for futuredevelopment. Our dataset will be made publicly available athttps://huggingface.co/datasets/declare-lab/GSM8k_MORE.</p>
                <p>Last Updated: 2024-01-17 18:13:07 UTC</p>
            </li>
        
            <li>
                <h3>Efficient slot labelling</h3>
                <p>Authors: Vladimir Vlasov</p>
                <p><a href="http://arxiv.org/abs/2401.09343v1">Link to paper</a></p>
                <p>Slot labelling is an essential component of any dialogue system aiming tofind important arguments in every user turn. Common approaches involve largepre-trained language models PLMs like BERT or RoBERTa but they facechallenges such as high computational requirements and dependence onpre-training data. In this work we propose a lightweight method which performson par or better than the state-of-the-art PLM-based methods while havingalmost 10x less trainable parameters. This makes it especially applicable forreal-life industry scenarios.</p>
                <p>Last Updated: 2024-01-17 17:08:36 UTC</p>
            </li>
        
            <li>
                <h3>SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding</h3>
                <p>Authors: Baoxiong JiaYixin ChenHuangyue YuYan WangXuesong NiuTengyu LiuQing LiSiyuan Huang</p>
                <p><a href="http://arxiv.org/abs/2401.09340v1">Link to paper</a></p>
                <p>3D vision-language grounding which focuses on aligning language with the 3Dphysical environment stands as a cornerstone in the development of embodiedagents. In comparison to recent advancements in the 2D domain groundinglanguage in 3D scenes faces several significant challenges: i the inherentcomplexity of 3D scenes due to the diverse object configurations their richattributes and intricate relationships ii the scarcity of paired 3Dvision-language data to support grounded learning and iii the absence of aunified learning framework to distill knowledge from grounded 3D data. In thiswork we aim to address these three major challenges in 3D vision-language byexamining the potential of systematically upscaling 3D vision-language learningin indoor environments. We introduce the first million-scale 3D vision-languagedataset SceneVerse encompassing about 68K 3D indoor scenes and comprising2.5M vision-language pairs derived from both human annotations and our scalablescene-graph-based generation approach. We demonstrate that this scaling allowsfor a unified pre-training framework Grounded Pre-training for Scenes GPSfor 3D vision-language learning. Through extensive experiments we showcase theeffectiveness of GPS by achieving state-of-the-art performance on all existing3D visual grounding benchmarks. The vast potential of SceneVerse and GPS isunveiled through zero-shot transfer experiments in the challenging 3Dvision-language tasks. Project website: https://scene-verse.github.io .</p>
                <p>Last Updated: 2024-01-17 17:04:35 UTC</p>
            </li>
        
            <li>
                <h3>Large Language Models Are Neurosymbolic Reasoners</h3>
                <p>Authors: Meng FangShilong DengYudi ZhangZijing ShiLing ChenMykola PechenizkiyJun Wang</p>
                <p><a href="http://arxiv.org/abs/2401.09334v1">Link to paper</a></p>
                <p>A wide range of real-world applications is characterized by their symbolicnature necessitating a strong capability for symbolic reasoning. This paperinvestigates the potential application of Large Language Models LLMs assymbolic reasoners. We focus on text-based games significant benchmarks foragents with natural language capabilities particularly in symbolic tasks likemath map reading sorting and applying common sense in text-based worlds. Tofacilitate these agents we propose an LLM agent designed to tackle symbolicchallenges and achieve in-game objectives. We begin by initializing the LLMagent and informing it of its role. The agent then receives observations and aset of valid actions from the text-based games along with a specific symbolicmodule. With these inputs the LLM agent chooses an action and interacts withthe game environments. Our experimental results demonstrate that our methodsignificantly enhances the capability of LLMs as automated agents for symbolicreasoning and our LLM agent is effective in text-based games involvingsymbolic tasks achieving an average performance of 88 across all tasks.</p>
                <p>Last Updated: 2024-01-17 16:57:19 UTC</p>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Randomized Kaczmarz with geometrically smoothed momentum</h3>
                <p>Authors: Seth J. AldermanRoan W. LuikartNicholas F. Marshall</p>
                <p><a href="http://arxiv.org/abs/2401.09415v1">Link to paper</a></p>
                <p>This paper studies the effect of adding geometrically smoothed momentum tothe randomized Kaczmarz algorithm which is an instance of stochastic gradientdescent on a linear least squares loss function. We prove a result about theexpected error in the direction of singular vectors of the matrix defining theleast squares loss. We present several numerical examples illustrating theutility of our result and pose several questions.</p>
                <p>Last Updated: 2024-01-17 18:55:24 UTC</p>
            </li>
        
            <li>
                <h3>Unlocking Unlabeled Data: Ensemble Learning with the Hui- Walter Paradigm for Performance Estimation in Online and Static Settings</h3>
                <p>Authors: Kevin SloteElaine Lee</p>
                <p><a href="http://arxiv.org/abs/2401.09376v1">Link to paper</a></p>
                <p>In the realm of machine learning and statistical modeling practitionersoften work under the assumption of accessible static labeled data forevaluation and training. However this assumption often deviates from realitywhere data may be private encrypted difficult- to-measure or unlabeled. Inthis paper we bridge this gap by adapting the Hui-Walter paradigm a methodtraditionally applied in epidemiology and medicine to the field of machinelearning. This approach enables us to estimate key performance metrics such asfalse positive rate false negative rate and priors in scenarios where noground truth is available. We further extend this paradigm for handling onlinedata opening up new possibilities for dynamic data environments. Ourmethodology involves partitioning data into latent classes to simulate multipledata populations if natural populations are unavailable and independentlytraining models to replicate multiple tests. By cross-tabulating binaryoutcomes across ensemble categorizers and multiple populations we are able toestimate unknown parameters through Gibbs sampling eliminating the need forground-truth or labeled data. This paper showcases the potential of ourmethodology to transform machine learning practices by allowing for accuratemodel assessment under dynamic and uncertain data conditions.</p>
                <p>Last Updated: 2024-01-17 17:46:10 UTC</p>
            </li>
        
            <li>
                <h3>High Confidence Level Inference is Almost Free using Parallel Stochastic Optimization</h3>
                <p>Authors: Wanrong ZhuZhipeng LouZiyang WeiWei Biao Wu</p>
                <p><a href="http://arxiv.org/abs/2401.09346v1">Link to paper</a></p>
                <p>Uncertainty quantification for estimation through stochastic optimizationsolutions in an online setting has gained popularity recently. This paperintroduces a novel inference method focused on constructing confidenceintervals with efficient computation and fast convergence to the nominal level.Specifically we propose to use a small number of independent multi-runs toacquire distribution information and construct a t-based confidence interval.Our method requires minimal additional computation and memory beyond thestandard updating of estimates making the inference process almost cost-free.We provide a rigorous theoretical guarantee for the confidence intervaldemonstrating that the coverage is approximately exact with an explicitconvergence rate and allowing for high confidence level inference. Inparticular a new Gaussian approximation result is developed for the onlineestimators to characterize the coverage properties of our confidence intervalsin terms of relative errors. Additionally our method also allows forleveraging parallel computing to further accelerate calculations using multiplecores. It is easy to implement and can be integrated with existing stochasticalgorithms without the need for complicated modifications.</p>
                <p>Last Updated: 2024-01-17 17:11:45 UTC</p>
            </li>
        
            <li>
                <h3>Central Limit Theorem for Two-Timescale Stochastic Approximation with Markovian Noise: Theory and Applications</h3>
                <p>Authors: Jie HuVishwaraj DoshiDo Young Eun</p>
                <p><a href="http://arxiv.org/abs/2401.09339v1">Link to paper</a></p>
                <p>Two-timescale stochastic approximation TTSA is among the most generalframeworks for iterative stochastic algorithms. This includes well-knownstochastic optimization methods such as SGD variants and those designed forbilevel or minimax problems as well as reinforcement learning like the familyof gradient-based temporal difference GTD algorithms. In this paper weconduct an in-depth asymptotic analysis of TTSA under controlled Markoviannoise via central limit theorem CLT uncovering the coupled dynamics of TTSAinfluenced by the underlying Markov chain which has not been addressed byprevious CLT results of TTSA only with Martingale difference noise. Buildingupon our CLT we expand its application horizon of efficient samplingstrategies from vanilla SGD to a wider TTSA context in distributed learningthus broadening the scope of Hu et al. 2022. In addition we leverage our CLTresult to deduce the statistical properties of GTD algorithms with nonlinearfunction approximation using Markovian samples and show their identicalasymptotic performance a perspective not evident from current finite-timebounds.</p>
                <p>Last Updated: 2024-01-17 17:01:08 UTC</p>
            </li>
        
            <li>
                <h3>Mitigating distribution shift in machine learning-augmented hybrid simulation</h3>
                <p>Authors: Jiaxi ZhaoQianxiao Li</p>
                <p><a href="http://arxiv.org/abs/2401.09259v1">Link to paper</a></p>
                <p>We study the problem of distribution shift generally arising inmachine-learning augmented hybrid simulation where parts of simulationalgorithms are replaced by data-driven surrogates. We first establish amathematical framework to understand the structure of machine-learningaugmented hybrid simulation problems and the cause and effect of theassociated distribution shift. We show correlations between distribution shiftand simulation error both numerically and theoretically. Then we propose asimple methodology based on tangent-space regularized estimator to control thedistribution shift thereby improving the long-term accuracy of the simulationresults. In the linear dynamics case we provide a thorough theoreticalanalysis to quantify the effectiveness of the proposed method. Moreover weconduct several numerical experiments including simulating a partially knownreaction-diffusion equation and solving Navier-Stokes equations using theprojection method with a data-driven pressure solver. In all cases we observemarked improvements in simulation accuracy under the proposed methodespecially for systems with high degrees of distribution shift such as thosewith relatively strong non-linear reaction mechanisms or flows at largeReynolds numbers.</p>
                <p>Last Updated: 2024-01-17 15:05:39 UTC</p>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model</h3>
                <p>Authors: Lianghui ZhuBencheng LiaoQian ZhangXinlong WangWenyu LiuXinggang Wang</p>
                <p><a href="http://arxiv.org/abs/2401.09417v1">Link to paper</a></p>
                <p>Recently the state space models SSMs with efficient hardware-aware designsi.e. Mamba have shown great potential for long sequence modeling. Buildingefficient and generic vision backbones purely upon SSMs is an appealingdirection. However representing visual data is challenging for SSMs due to theposition-sensitivity of visual data and the requirement of global context forvisual understanding. In this paper we show that the reliance of visualrepresentation learning on self-attention is not necessary and propose a newgeneric vision backbone with bidirectional Mamba blocks Vim which marks theimage sequences with position embeddings and compresses the visualrepresentation with bidirectional state space models. On ImageNetclassification COCO object detection and ADE20k semantic segmentation tasksVim achieves higher performance compared to well-established visiontransformers like DeiT while also demonstrating significantly improvedcomputation  memory efficiency. For example Vim is 2.8times faster thanDeiT and saves 86.8 GPU memory when performing batch inference to extractfeatures on images with a resolution of 1248times1248. The resultsdemonstrate that Vim is capable of overcoming the computation  memoryconstraints on performing Transformer-style understanding for high-resolutionimages and it has great potential to become the next-generation backbone forvision foundation models. Code is available at https://github.com/hustvl/Vim.</p>
                <p>Last Updated: 2024-01-17 18:56:18 UTC</p>
            </li>
        
            <li>
                <h3>Vlogger: Make Your Dream A Vlog</h3>
                <p>Authors: Shaobin ZhuangKunchang LiXinyuan ChenYaohui WangZiwei LiuYu QiaoYali Wang</p>
                <p><a href="http://arxiv.org/abs/2401.09414v1">Link to paper</a></p>
                <p>In this work we present Vlogger a generic AI system for generating aminute-level video blog i.e. vlog of user descriptions. Different from shortvideos with a few seconds vlog often contains a complex storyline withdiversified scenes which is challenging for most existing video generationapproaches. To break through this bottleneck our Vlogger smartly leveragesLarge Language Model LLM as Director and decomposes a long video generationtask of vlog into four key stages where we invoke various foundation models toplay the critical roles of vlog professionals including 1 Script 2 Actor3 ShowMaker and 4 Voicer. With such a design of mimicking human beingsour Vlogger can generate vlogs through explainable cooperation of top-downplanning and bottom-up shooting. Moreover we introduce a novel video diffusionmodel ShowMaker which serves as a videographer in our Vlogger for generatingthe video snippet of each shooting scene. By incorporating Script and Actorattentively as textual and visual prompts it can effectively enhancespatial-temporal coherence in the snippet. Besides we design a concise mixedtraining paradigm for ShowMaker boosting its capacity for both T2V generationand prediction. Finally the extensive experiments show that our methodachieves state-of-the-art performance on zero-shot T2V generation andprediction tasks. More importantly Vlogger can generate over 5-minute vlogsfrom open-world descriptions without loss of video coherence on script andactor. The code and model is all available athttps://github.com/zhuangshaobin/Vlogger.</p>
                <p>Last Updated: 2024-01-17 18:55:12 UTC</p>
            </li>
        
            <li>
                <h3>Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text</h3>
                <p>Authors: Mazal BethanyBrandon WherryEmet BethanyNishant VishwamitraPeyman Najafirad</p>
                <p><a href="http://arxiv.org/abs/2401.09407v1">Link to paper</a></p>
                <p>With the recent proliferation of Large Language Models LLMs there has beenan increasing demand for tools to detect machine-generated text. The effectivedetection of machine-generated text face two pertinent problems: First theyare severely limited in generalizing against real-world scenarios wheremachine-generated text is produced by a variety of generators including butnot limited to GPT-4 and Dolly and spans diverse domains ranging fromacademic manuscripts to social media posts. Second existing detectionmethodologies treat texts produced by LLMs through a restrictive binaryclassification lens neglecting the nuanced diversity of artifacts generated bydifferent LLMs. In this work we undertake a systematic study on the detectionof machine-generated text in real-world scenarios. We first study theeffectiveness of state-of-the-art approaches and find that they are severelylimited against text produced by diverse generators and domains in the realworld. Furthermore t-SNE visualizations of the embeddings from a pretrainedLLMs encoder show that they cannot reliably distinguish between human andmachine-generated text. Based on our findings we introduce a novel systemT5LLMCipher for detecting machine-generated text using a pretrained T5 encodercombined with LLM embedding sub-clustering to address the text produced bydiverse generators and domains in the real world. We evaluate our approachacross 9 machine-generated text systems and 9 domains and find that ourapproach provides state-of-the-art generalization ability with an averageincrease in F1 score on machine-generated text of 19.6 on unseen generatorsand domains compared to the top performing existing approaches and correctlyattributes the generator of text with an accuracy of 93.6.</p>
                <p>Last Updated: 2024-01-17 18:45:13 UTC</p>
            </li>
        
            <li>
                <h3>Élivágar: Efficient Quantum Circuit Search for Classification</h3>
                <p>Authors: Sashwat AnagolumNarges AlavisamaniPoulami DasMoinuddin QureshiEric KesslerYunong Shi</p>
                <p><a href="http://arxiv.org/abs/2401.09393v1">Link to paper</a></p>
                <p>Designing performant and noise-robust circuits for Quantum Machine LearningQML is challenging -- the design space scales exponentially with circuitsize and there are few well-supported guiding principles for QML circuitdesign. Although recent Quantum Circuit Search QCS methods attempt to searchfor performant QML circuits that are also robust to hardware noise theydirectly adopt designs from classical Neural Architecture Search NAS that aremisaligned with the unique constraints of quantum hardware resulting in highsearch overheads and severe performance bottlenecks.  We present Elivagar a novel resource-efficient noise-guided QCSframework. Elivagar innovates in all three major aspects of QCS -- searchspace search algorithm and candidate evaluation strategy -- to address thedesign flaws in current classically-inspired QCS methods. Elivagar achieveshardware-efficiency and avoids an expensive circuit-mapping co-search vianoise- and device topology-aware candidate generation. By introducing twocheap-to-compute predictors Clifford noise resilience and Representationalcapacity Elivagar decouples the evaluation of noise robustness andperformance enabling early rejection of low-fidelity circuits and reducingcircuit evaluation costs. Due to its resource-efficiency Elivagar canfurther search for data embeddings significantly improving performance.  Based on a comprehensive evaluation of Elivagar on 12 real quantumdevices and 9 QML applications Elivagar achieves 5.3 higher accuracy anda 271times speedup compared to state-of-the-art QCS methods.</p>
                <p>Last Updated: 2024-01-17 18:09:26 UTC</p>
            </li>
        
            <li>
                <h3>Diverse Part Synthesis for 3D Shape Creation</h3>
                <p>Authors: Yanran GuanOliver van Kaick</p>
                <p><a href="http://arxiv.org/abs/2401.09384v1">Link to paper</a></p>
                <p>Methods that use neural networks for synthesizing 3D shapes in the form of apart-based representation have been introduced over the last few years. Thesemethods represent shapes as a graph or hierarchy of parts and enable a varietyof applications such as shape sampling and reconstruction. However currentmethods do not allow easily regenerating individual shape parts according touser preferences. In this paper we investigate techniques that allow the userto generate multiple diverse suggestions for individual parts. Specificallywe experiment with multimodal deep generative models that allow samplingdiverse suggestions for shape parts and focus on models which have not beenconsidered in previous work on shape synthesis. To provide a comparative studyof these techniques we introduce a method for synthesizing 3D shapes in apart-based representation and evaluate all the part suggestion techniqueswithin this synthesis method. In our method which is inspired by previouswork shapes are represented as a set of parts in the form of implicitfunctions which are then positioned in space to form the final shape. Synthesisin this representation is enabled by a neural network architecture based on animplicit decoder and a spatial transformer. We compare the various multimodalgenerative models by evaluating their performance in generating partsuggestions. Our contribution is to show with qualitative and quantitativeevaluations which of the new techniques for multimodal part generation performthe best and that a synthesis method based on the top-performing techniquesallows the user to more finely control the parts that are generated in the 3Dshapes while maintaining high shape fidelity when reconstructing shapes.</p>
                <p>Last Updated: 2024-01-17 17:55:06 UTC</p>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Through the Looking-Glass: Transparency Implications and Challenges in Enterprise AI Knowledge Systems</h3>
                <p>Authors: Karina Cortiñas-LorenzoSiân LindleyIda Larsen-LedetBhaskar Mitra</p>
                <p><a href="http://arxiv.org/abs/2401.09410v1">Link to paper</a></p>
                <p>Knowledge cant be disentangled from people. As AI knowledge systems minevast volumes of work-related data the knowledge thats being extracted andsurfaced is intrinsically linked to the people who create and use it. Whenthese systems get embedded in organizational settings the information that isbrought to the foreground and the information thats pushed to the peripherycan influence how individuals see each other and how they see themselves atwork. In this paper we present the looking-glass metaphor and use it toconceptualize AI knowledge systems as systems that reflect and distortexpanding our view on transparency requirements implications and challenges.We formulate transparency as a key mediator in shaping different ways ofseeing including seeing into the system which unveils its capabilitieslimitations and behavior and seeing through the system which shapes workersperceptions of their own contributions and others within the organization.Recognizing the sociotechnical nature of these systems we identify threetransparency dimensions necessary to realize the value of AI knowledge systemsnamely system transparency procedural transparency and transparency ofoutcomes. We discuss key challenges hindering the implementation of these formsof transparency bringing to light the wider sociotechnical gap andhighlighting directions for future Computer-supported Cooperative Work CSCWresearch.</p>
                <p>Last Updated: 2024-01-17 18:47:30 UTC</p>
            </li>
        
            <li>
                <h3>Establishing Awareness through Pointing Gestures during Collaborative Decision-Making in a Wall-Display Environment</h3>
                <p>Authors: Valérie MaquilDimitra AnastasiouHoorieh AfkariAdrien CoppensJohannes HermenLou Schwartz</p>
                <p><a href="http://dx.doi.org/10.1145/3544549.3585830">Link to paper</a></p>
                <p>Sharing a physical environment such as that of a wall-display facilitatesgaining awareness of others actions and intentions thereby bringing benefitsfor collaboration. Previous studies have provided first insights on awarenessin the context of tabletops or smaller vertical displays. This paper seeks toadvance the current understanding on how users share awareness information inwall-display environments and focusses on mid-air pointing gestures as afoundational part of communication. We present a scenario dealing with theorganization of medical supply chains in crisis situations and report on theresults of a user study with 24 users split into 6 groups of 4 performingseveral tasks. We investigate pointing gestures and identify three subtypesused as awareness cues during face-to-face collaboration: narrative pointingloose pointing and sharp pointing. Our observations show that reliance ongesture subtypes varies across participants and groups and that sometimesvague pointing is sufficient to support verbal negotiations.</p>
                <p>Last Updated: 2024-01-17 16:47:46 UTC</p>
            </li>
        
            <li>
                <h3>Same Data, Diverging Perspectives: The Power of Visualizations to Elicit Competing Interpretations</h3>
                <p>Authors: Cindy Xiong BearfieldLisanne van WeeldenAdam WaytzSteven Franconeri</p>
                <p><a href="http://arxiv.org/abs/2401.09289v1">Link to paper</a></p>
                <p>People routinely rely on data to make decisions but the process can beriddled with biases. We show that patterns in data might be noticed first ormore strongly depending on how the data is visually represented or what theviewer finds salient. We also demonstrate that viewer interpretation of data issimilar to that of ambiguous figures such that two people looking at the samedata can come to different decisions. In our studies participants readvisualizations depicting competitions between two entities where one has ahistorical lead A but the other has been gaining momentum B and predicted awinner across two chart types and three annotation approaches. They either sawthe historical lead as salient and predicted that A would win or saw theincreasing momentum as salient and predicted B to win. These results suggestthat decisions can be influenced by both how data are presented and whatpatterns people find visually salient.</p>
                <p>Last Updated: 2024-01-17 15:43:12 UTC</p>
            </li>
        
            <li>
                <h3>Learning from Emotions, Demographic Information and Implicit User Feedback in Task-Oriented Document-Grounded Dialogues</h3>
                <p>Authors: Dominic PetrakThy Thy TranIryna Gurevych</p>
                <p><a href="http://arxiv.org/abs/2401.09248v1">Link to paper</a></p>
                <p>The success of task-oriented and document-grounded dialogue systems dependson users accepting and enjoying using them. To achieve this recently publishedwork in the field of Human-Computer Interaction suggests that the combinationof considering demographic information user emotions and learning from theimplicit feedback in their utterances is particularly important. Howeverthese findings have not yet been transferred to the field of Natural LanguageProcessing where these data are primarily studied separately. Accordingly nosufficiently annotated dataset is available. To address this gap we introduceFEDI the first English dialogue dataset for task-oriented document-groundeddialogues annotated with demographic information user emotions and implicitfeedback. Our experiments with FLAN-T5 GPT-2 and LLaMA-2 show that these datahave the potential to improve task completion and the factual consistency ofthe generated responses and user acceptance.</p>
                <p>Last Updated: 2024-01-17 14:52:26 UTC</p>
            </li>
        
            <li>
                <h3>BibSonomy Meets ChatLLMs for Publication Management: From Chat to Publication Management: Organizing your related work using BibSonomy & LLMs</h3>
                <p>Authors: Tom VölkerJan PfisterTobias KoopmannAndreas Hotho</p>
                <p><a href="http://dx.doi.org/10.1145/3627508.3638298">Link to paper</a></p>
                <p>The ever-growing corpus of scientific literature presents significantchallenges for researchers with respect to discovery management andannotation of relevant publications. Traditional platforms like SemanticScholar BibSonomy and Zotero offer tools for literature management butlargely require manual laborious and error-prone input of tags and metadata.Here we introduce a novel retrieval augmented generation system that leverageschat-based large language models LLMs to streamline and enhance the processof publication management. It provides a unified chat-based interface enablingintuitive interactions with various backends including Semantic ScholarBibSonomy and the Zotero Webscraper. It supports two main use-cases: 1Explorative Search  Retrieval - leveraging LLMs to search for and retrieveboth specific and general scientific publications while addressing thechallenges of content hallucination and data obsolescence and 2 Cataloguing Management - aiding in the organization of personal publication libraries inthis case BibSonomy by automating the addition of metadata and tags whilefacilitating manual edits and updates. We compare our system to different LLMmodels in three different settings including a user study and we can show itsadvantages in different metrics.</p>
                <p>Last Updated: 2024-01-17 09:53:50 UTC</p>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Improved Consensus ADMM for Cooperative Motion Planning of Large-Scale Connected Autonomous Vehicles with Limited Communication</h3>
                <p>Authors: Haichao LiuZhenmin HuangZicheng ZhuYulin LiShaojie ShenJun Ma</p>
                <p><a href="http://arxiv.org/abs/2401.09032v1">Link to paper</a></p>
                <p>This paper investigates a cooperative motion planning problem for large-scaleconnected autonomous vehicles CAVs under limited communications whichaddresses the challenges of high communication and computing resourcerequirements. Our proposed methodology incorporates a parallel optimizationalgorithm with improved consensus ADMM considering a more realistic locallyconnected topology network and time complexity of ON is achieved byexploiting the sparsity in the dual update process. To further enhance thecomputational efficiency we employ a lightweight evolution strategy for thedynamic connectivity graph of CAVs and each sub-problem split from theconsensus ADMM only requires managing a small group of CAVs. The proposedmethod implemented with the receding horizon scheme is validated thoroughlyand comparisons with existing numerical solvers and approaches demonstrate theefficiency of our proposed algorithm. Also simulations on large-scalecooperative driving tasks involving 80 vehicles are performed in thehigh-fidelity CARLA simulator which highlights the remarkable computationalefficiency scalability and effectiveness of our proposed development.Demonstration videos are available athttps://henryhcliu.github.io/icadmm_cmp_carla.</p>
                <p>Last Updated: 2024-01-17 07:58:48 UTC</p>
            </li>
        
            <li>
                <h3>Data assimilation approach for addressing imperfections in people flow measurement techniques using particle filter</h3>
                <p>Authors: Ryo MurataKenji Tanaka</p>
                <p><a href="http://arxiv.org/abs/2401.09014v1">Link to paper</a></p>
                <p>Understanding and predicting people flow in urban areas is useful fordecision-making in urban planning and marketing strategies. Traditional methodsfor understanding people flow can be divided into measurement-based approachesand simulation-based approaches. Measurement-based approaches have theadvantage of directly capturing actual people flow but they face the challengeof data imperfection. On the other hand simulations can obtain complete dataon a computer but they only consider some of the factors determining humanbehavior leading to a divergence from actual people flow. Both measurement andsimulation methods have unresolved issues and combining the two cancomplementarily overcome them. This paper proposes a method that applies dataassimilation a fusion technique of measurement and simulation to agent-basedsimulation. Data assimilation combines the advantages of both measurement andsimulation contributing to the creation of an environment that can reflectreal people flow while acquiring richer data. The paper verifies theeffectiveness of the proposed method in a virtual environment and demonstratesthe potential of data assimilation to compensate for the three types ofimperfection in people flow measurement techniques. These findings can serve asguidelines for supplementing sparse measurement data in physical environments.</p>
                <p>Last Updated: 2024-01-17 07:20:15 UTC</p>
            </li>
        
            <li>
                <h3>AgentMixer: Multi-Agent Correlated Policy Factorization</h3>
                <p>Authors: Zhiyuan LiWenshuai ZhaoLijun WuJoni Pajarinen</p>
                <p><a href="http://arxiv.org/abs/2401.08728v1">Link to paper</a></p>
                <p>Centralized training with decentralized execution CTDE is widely employedto stabilize partially observable multi-agent reinforcement learning MARL byutilizing a centralized value function during training. However existingmethods typically assume that agents make decisions based on their localobservations independently which may not lead to a correlated joint policywith sufficient coordination. Inspired by the concept of correlatedequilibrium we propose to introduce a textitstrategy modification toprovide a mechanism for agents to correlate their policies. Specifically wepresent a novel framework AgentMixer which constructs the joint fullyobservable policy as a non-linear combination of individual partiallyobservable policies. To enable decentralized execution one can deriveindividual policies by imitating the joint policy. Unfortunately suchimitation learning can lead to textitasymmetric learning failure caused bythe mismatch between joint policy and individual policy information. Tomitigate this issue we jointly train the joint policy and individual policiesand introduce textitIndividual-Global-Consistency to guarantee modeconsistency between the centralized and decentralized policies. We thentheoretically prove that AgentMixer converges to an epsilon-approximateCorrelated Equilibrium. The strong experimental performance on three MARLbenchmarks demonstrates the effectiveness of our method.</p>
                <p>Last Updated: 2024-01-16 15:32:41 UTC</p>
            </li>
        
            <li>
                <h3>A Day-to-Day Dynamical Approach to the Most Likely User Equilibrium Problem</h3>
                <p>Authors: Jiayang LiQianni WangLiyang FengJun XieYu Marco Nie</p>
                <p><a href="http://arxiv.org/abs/2401.08013v1">Link to paper</a></p>
                <p>The lack of a unique user equilibrium UE route flow in traffic assignmenthas posed a significant challenge to many transportation applications. Themaximum-entropy principle which advocates for the consistent selection of themost likely solution as a representative is often used to address thechallenge. Built on a recently proposed day-to-day DTD discrete-timedynamical model called cumulative logit CULO this study provides a newbehavioral underpinning for the maximum-entropy UE MEUE route flow. It hasbeen proven that CULO can reach a UE state without presuming travelers areperfectly rational. Here we further establish that CULO always converges tothe MEUE route flow if i travelers have zero prior information about routesand thus are forced to give all routes an equal choice probability or ii alltravelers gather information from the same source such that the so-calledgeneral proportionality condition is satisfied. Thus CULO may be used as apractical solution algorithm for the MEUE problem. To put this idea intopractice we propose to eliminate the route enumeration requirement of theoriginal CULO model through an iterative route discovery scheme. We alsoexamine the discrete-time versions of four popular continuous-time dynamicalmodels and compare them to CULO. The analysis shows that the replicator dynamicis the only one that has the potential to reach the MEUE solution with someregularity. The analytical results are confirmed through numerical experiments.</p>
                <p>Last Updated: 2024-01-15 23:43:41 UTC</p>
            </li>
        
            <li>
                <h3>Emergency Localization for Mobile Ground Users: An Adaptive UAV Trajectory Planning Method</h3>
                <p>Authors: Zhihao ZhuJiafan HeLuyang HouLianming XuWendi ZhuLi Wang</p>
                <p><a href="http://arxiv.org/abs/2401.07256v1">Link to paper</a></p>
                <p>In emergency search and rescue scenarios the quick location of trappedpeople is essential. However disasters can render the Global PositioningSystem GPS unusable. Unmanned aerial vehicles UAVs with localizationdevices can serve as mobile anchors due to their agility and high line-of-sightLoS probability. Nonetheless the number of available UAVs during the initialstages of disaster relief is limited and innovative methods are needed toquickly plan UAV trajectories to locate non-uniformly distributed dynamictargets while ensuring localization accuracy. To address this challenge wedesign a single UAV localization method without hovering use the maximumlikelihood estimation MLE method to estimate the location of mobile users anddefine the upper bound of the localization error by considering usersmovement.Combining this localization method and localization error-index weutilize the enhanced particle swarm optimization EPSO algorithm and edgeaccess strategy to develop a low complexity localization-oriented adaptivetrajectory planning algorithm. Simulation results demonstrate that our methodoutperforms other baseline algorithms enabling faster localization withoutcompromising localization accuracy.</p>
                <p>Last Updated: 2024-01-14 11:20:20 UTC</p>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-01-19</p>
        </div>
    
        </div>
    </body>
    </html>
    