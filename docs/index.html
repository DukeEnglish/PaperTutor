
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</h3>
                <p>Authors: Dongzhi JiangGuanglu SongXiaoshi WuRenrui ZhangDazhong ShenZhuofan ZongYu LiuHongsheng Li</p>
                <p><a href="http://arxiv.org/abs/2404.03653v1">Link to paper</a></p>
                <p>Diffusion models have demonstrated great success in the field oftext-to-image generation. However alleviating the misalignment between thetext prompts and images is still challenging. The root reason behind themisalignment has not been extensively investigated. We observe that themisalignment is caused by inadequate token attention activation. We furtherattribute this phenomenon to the diffusion models insufficient conditionutilization which is caused by its training paradigm. To address the issue wepropose CoMat an end-to-end diffusion model fine-tuning strategy with animage-to-text concept matching mechanism. We leverage an image captioning modelto measure image-to-text alignment and guide the diffusion model to revisitignored tokens. A novel attribute concentration module is also proposed toaddress the attribute binding problem. Without any image or human preferencedata we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.Extensive experiments show that CoMat-SDXL significantly outperforms thebaseline model SDXL in two text-to-image alignment benchmarks and achievesstart-of-the-art performance.</p>
                <p>Last Updated: 2024-04-04 17:59:46 UTC</p>
                <button class="interpret-button" data-id="2404.03653v1">Interpret</button>
                <div id="interpretation-2404.03653v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent</h3>
                <p>Authors: Hanyu LaiXiao LiuIat Long IongShuntian YaoYuxuan ChenPengbo ShenHao YuHanchen ZhangXiaohan ZhangYuxiao DongJie Tang</p>
                <p><a href="http://arxiv.org/abs/2404.03648v1">Link to paper</a></p>
                <p>Large language models LLMs have fueled many intelligent agent tasks suchas web navigation -- but most existing agents perform far from satisfying inreal-world webpages due to three factors: 1 the versatility of actions onwebpages 2 HTML text exceeding model processing capacity and 3 thecomplexity of decision-making due to the open-domain nature of web. In light ofthe challenge we develop AutoWebGLM a GPT-4-outperforming automated webnavigation agent built upon ChatGLM3-6B. Inspired by human browsing patternswe design an HTML simplification algorithm to represent webpages preservingvital information succinctly. We employ a hybrid human-AI method to build webbrowsing data for curriculum training. Then we bootstrap the model byreinforcement learning and rejection sampling to further facilitate webpagecomprehension browser operations and efficient task decomposition by itself.For testing we establish a bilingual benchmark -- AutoWebBench -- forreal-world web browsing tasks. We evaluate AutoWebGLM across diverse webnavigation benchmarks revealing its improvements but also underlyingchallenges to tackle real environments. Related code model and data will bereleased at urlhttps://github.com/THUDM/AutoWebGLM.</p>
                <p>Last Updated: 2024-04-04 17:58:40 UTC</p>
                <button class="interpret-button" data-id="2404.03648v1">Interpret</button>
                <div id="interpretation-2404.03648v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Locating and Editing Factual Associations in Mamba</h3>
                <p>Authors: Arnab Sen SharmaDavid AtkinsonDavid Bau</p>
                <p><a href="http://arxiv.org/abs/2404.03646v1">Link to paper</a></p>
                <p>We investigate the mechanisms of factual recall in the Mamba state spacemodel. Our work is inspired by previous findings in autoregressive transformerlanguage models suggesting that their knowledge recall is localized toparticular modules at specific token locations we therefore ask whetherfactual recall in Mamba can be similarly localized. To investigate this weconduct four lines of experiments on Mamba. First we apply causal tracing orinterchange interventions to localize key components inside Mamba that areresponsible for recalling facts revealing that specific components withinmiddle layers show strong causal effects at the last token of the subjectwhile the causal effect of intervening on later layers is most pronounced atthe last token of the prompt matching previous findings on autoregressivetransformers. Second we show that rank-one model editing methods cansuccessfully insert facts at specific locations again resembling findings ontransformer models. Third we examine the linearity of Mambas representationsof factual relations. Finally we adapt attention-knockout techniques to Mambato dissect information flow during factual recall. We compare Mamba directly toa similar-sized transformer and conclude that despite significant differencesin architectural approach when it comes to factual recall the twoarchitectures share many similarities.</p>
                <p>Last Updated: 2024-04-04 17:58:31 UTC</p>
                <button class="interpret-button" data-id="2404.03646v1">Interpret</button>
                <div id="interpretation-2404.03646v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WorDepth: Variational Language Prior for Monocular Depth Estimation</h3>
                <p>Authors: Ziyao ZengDaniel WangFengyu YangHyoungseob ParkYangchao WuStefano SoattoByung-Woo HongDong LaoAlex Wong</p>
                <p><a href="http://arxiv.org/abs/2404.03635v1">Link to paper</a></p>
                <p>Three-dimensional 3D reconstruction from a single image is an ill-posedproblem with inherent ambiguities i.e. scale. Predicting a 3D scene from textdescriptions is similarly ill-posed i.e. spatial arrangements of objectsdescribed. We investigate the question of whether two inherently ambiguousmodalities can be used in conjunction to produce metric-scaled reconstructions.To test this we focus on monocular depth estimation the problem of predictinga dense depth map from a single image but with an additional text captiondescribing the scene. To this end we begin by encoding the text caption as amean and standard deviation using a variational framework we learn thedistribution of the plausible metric reconstructions of 3D scenes correspondingto the text captions as a prior. To select a specific reconstruction or depthmap we encode the given image through a conditional sampler that samples fromthe latent space of the variational text encoder which is then decoded to theoutput depth map. Our approach is trained alternatingly between the text andimage branches: in one optimization step we predict the mean and standarddeviation from the text description and sample from a standard Gaussian and inthe other we sample using a image conditional sampler. Once trained wedirectly predict depth from the encoded text using the conditional sampler. Wedemonstrate our approach on indoor NYUv2 and outdoor KITTI scenarios wherewe show that language can consistently improve performance in both.</p>
                <p>Last Updated: 2024-04-04 17:54:33 UTC</p>
                <button class="interpret-button" data-id="2404.03635v1">Interpret</button>
                <div id="interpretation-2404.03635v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Training LLMs over Neurally Compressed Text</h3>
                <p>Authors: Brian LesterJaehoon LeeAlex AlemiJeffrey PenningtonAdam RobertsJascha Sohl-DicksteinNoah Constant</p>
                <p><a href="http://arxiv.org/abs/2404.03626v1">Link to paper</a></p>
                <p>In this paper we explore the idea of training large language models LLMsover highly compressed text. While standard subword tokenizers compress text bya small factor neural text compressors can achieve much higher rates ofcompression. If it were possible to train LLMs directly over neurallycompressed text this would confer advantages in training and servingefficiency as well as easier handling of long text spans. The main obstacle tothis goal is that strong compression tends to produce opaque outputs that arenot well-suited for learning. In particular we find that text naivelycompressed via Arithmetic Coding is not readily learnable by LLMs. To overcomethis we propose Equal-Info Windows a novel compression technique whereby textis segmented into blocks that each compress to the same bit length. Using thismethod we demonstrate effective learning over neurally compressed text thatimproves with scale and outperforms byte-level baselines by a wide margin onperplexity and inference speed benchmarks. While our method delivers worseperplexity than subword tokenizers for models trained with the same parametercount it has the benefit of shorter sequence lengths. Shorter sequence lengthsrequire fewer autoregressive generation steps and reduce latency. Finally weprovide extensive analysis of the properties that contribute to learnabilityand offer concrete suggestions for how to further improve the performance ofhigh-compression tokenizers.</p>
                <p>Last Updated: 2024-04-04 17:48:28 UTC</p>
                <button class="interpret-button" data-id="2404.03626v1">Interpret</button>
                <div id="interpretation-2404.03626v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Creator Hearts: Investigating the Impact Positive Signals from YouTube Creators in Shaping Comment Section Behavior</h3>
                <p>Authors: Frederick ChoiCharlotte LambertVinay KoshySowmya PratipatiTue DoEshwar Chandrasekharan</p>
                <p><a href="http://arxiv.org/abs/2404.03612v1">Link to paper</a></p>
                <p>Much of the research in online moderation focuses on punitive actions.However emerging research has shown that positive reinforcement is effectiveat encouraging desirable behavior on online platforms. We extend this researchby studying the creator heart feature on YouTube quantifying their primaryeffects on comments that receive hearts and on videos where hearts have beengiven. We find that creator hearts increased the visibility of comments andincreased the amount of positive engagement they received from other users. Wealso find that the presence of a creator hearted comment soon after a video ispublished can incentivize viewers to comment increasing the total engagementwith the video over time. We discuss the potential for creators to use heartsto shape behavior in their communities by highlighting rewarding andincentivizing desirable behaviors from users. We discuss avenues for extendingour study to understanding positive signals from moderators on other platforms.</p>
                <p>Last Updated: 2024-04-04 17:34:35 UTC</p>
                <button class="interpret-button" data-id="2404.03612v1">Interpret</button>
                <div id="interpretation-2404.03612v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work</h3>
                <p>Authors: Somin ParkCarol C. MenassaVineet R. Kamat</p>
                <p><a href="http://arxiv.org/abs/2404.03498v1">Link to paper</a></p>
                <p>In the construction industry where work environments are complexunstructured and often dangerous the implementation of Human-RobotCollaboration HRC is emerging as a promising advancement. This underlines thecritical need for intuitive communication interfaces that enable constructionworkers to collaborate seamlessly with robotic assistants. This studyintroduces a conversational Virtual Reality VR interface integratingmultimodal interaction to enhance intuitive communication between constructionworkers and robots. By integrating voice and controller inputs with the RobotOperating System ROS Building Information Modeling BIM and a game enginefeaturing a chat interface powered by a Large Language Model LLM theproposed system enables intuitive and precise interaction within a VR setting.Evaluated by twelve construction workers through a drywall installation casestudy the proposed system demonstrated its low workload and high usabilitywith succinct command inputs. The proposed multimodal interaction systemsuggests that such technological integration can substantially advance theintegration of robotic assistants in the construction industry.</p>
                <p>Last Updated: 2024-04-04 14:56:41 UTC</p>
                <button class="interpret-button" data-id="2404.03498v1">Interpret</button>
                <div id="interpretation-2404.03498v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Agora Elevator Bodily Sensation Study -- a report</h3>
                <p>Authors: Rebekah Rousi</p>
                <p><a href="http://arxiv.org/abs/2404.03356v1">Link to paper</a></p>
                <p>This study set out to examine the relationship between expressed socialemotions i.e. that what people say they are feeling and physical sensationsthe connection between emotion and bodily experience. It additionally providedthe opportunity to investigate how the neurological findings of genderdifferences can be observed in practice what difference does it make inbehaviour and judgment that we have varying levels of mirror neuron activityThe following report documents the study procedure results and findings.</p>
                <p>Last Updated: 2024-04-04 10:50:41 UTC</p>
                <button class="interpret-button" data-id="2404.03356v1">Interpret</button>
                <div id="interpretation-2404.03356v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Influence of Gameplay Duration, Hand Tracking, and Controller Based Control Methods on UX in VR</h3>
                <p>Authors: Tanja KojićMaurizio VergariSimon KnuthMaximilian WarsinkeSebastian MöllerJan-Niklas Voigt-Antons</p>
                <p><a href="http://dx.doi.org/10.1145/3652212.3652222">Link to paper</a></p>
                <p>Inside-out tracking is growing popular in consumer VR enhancingaccessibility. It uses HMD camera data and neural networks for effective handtracking. However limited user experience studies have compared this method totraditional controllers with no consensus on the optimal control technique.This paper investigates the impact of control methods and gaming duration on VRuser experience hypothesizing hand tracking might be preferred for shortsessions and by users new to VR due to its simplicity. Through a lab study withtwenty participants evaluating presence emotional response UX quality andflow findings revealed control type and session length affect user experiencewithout significant interaction. Controllers were generally superiorattributed to their reliability and longer sessions increased presence andrealism. The study found that individuals with more VR experience were moreinclined to recommend hand tracking to others which contradicted predictions.</p>
                <p>Last Updated: 2024-04-04 10:06:45 UTC</p>
                <button class="interpret-button" data-id="2404.03337v1">Interpret</button>
                <div id="interpretation-2404.03337v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Exploring Emotions in Multi-componential Space using Interactive VR Games</h3>
                <p>Authors: Rukshani SomarathnaGelareh Mohammadi</p>
                <p><a href="http://arxiv.org/abs/2404.03239v1">Link to paper</a></p>
                <p>Emotion understanding is a complex process that involves multiple components.The ability to recognise emotions not only leads to new context awarenessmethods but also enhances system interactions effectiveness by perceiving andexpressing emotions. Despite the attention to discrete and dimensional modelsneuroscientific evidence supports those emotions as being complex andmulti-faceted. One framework that resonated well with such findings is theComponent Process Model CPM a theory that considers the complexity ofemotions with five interconnected components: appraisal expressionmotivation physiology and feeling. However the relationship between CPM anddiscrete emotions has not yet been fully explored. Therefore to betterunderstand emotions underlying processes we operationalised a data-drivenapproach using interactive Virtual Reality VR games and collected multimodalmeasures self-reports physiological and facial signals from 39 participants.We used Machine Learning ML methods to identify the unique contributions ofeach component to emotion differentiation. Our results showed the role ofdifferent components in emotion differentiation with the model including allcomponents demonstrating the most significant contribution. Moreover we foundthat at least five dimensions are needed to represent the variation of emotionsin our dataset. These findings also have implications for using VR environmentsin emotion research and highlight the role of physiological signals in emotionrecognition within such environments.</p>
                <p>Last Updated: 2024-04-04 06:54:44 UTC</p>
                <button class="interpret-button" data-id="2404.03239v1">Interpret</button>
                <div id="interpretation-2404.03239v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Leveraging Interpolation Models and Error Bounds for Verifiable Scientific Machine Learning</h3>
                <p>Authors: Tyler ChangAndrew GilletteRomit Maulik</p>
                <p><a href="http://arxiv.org/abs/2404.03586v1">Link to paper</a></p>
                <p>Effective verification and validation techniques for modern scientificmachine learning workflows are challenging to devise. Statistical methods areabundant and easily deployed but often rely on speculative assumptions aboutthe data and methods involved. Error bounds for classical interpolationtechniques can provide mathematically rigorous estimates of accuracy but oftenare difficult or impractical to determine computationally. In this work wepresent a best-of-both-worlds approach to verifiable scientific machinelearning by demonstrating that 1 multiple standard interpolation techniqueshave informative error bounds that can be computed or estimated efficiently2 comparative performance among distinct interpolants can aid in validationgoals 3 deploying interpolation methods on latent spaces generated by deeplearning techniques enables some interpretability for black-box models. Wepresent a detailed case study of our approach for predicting lift-drag ratiosfrom airfoil images. Code developed for this work is available in a publicGithub repository.</p>
                <p>Last Updated: 2024-04-04 16:52:17 UTC</p>
                <button class="interpret-button" data-id="2404.03586v1">Interpret</button>
                <div id="interpretation-2404.03586v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm</h3>
                <p>Authors: Miao LuHan ZhongTong ZhangJose Blanchet</p>
                <p><a href="http://arxiv.org/abs/2404.03578v1">Link to paper</a></p>
                <p>The sim-to-real gap which represents the disparity between training andtesting environments poses a significant challenge in reinforcement learningRL. A promising approach to addressing this challenge is distributionallyrobust RL often framed as a robust Markov decision process RMDP. In thisframework the objective is to find a robust policy that achieves goodperformance under the worst-case scenario among all environments within apre-specified uncertainty set centered around the training environment. Unlikeprevious work which relies on a generative model or a pre-collected offlinedataset enjoying good coverage of the deployment environment we tackle robustRL via interactive data collection where the learner interacts with thetraining environment only and refines the policy through trial and error. Inthis robust RL paradigm two main challenges emerge: managing distributionalrobustness while striking a balance between exploration and exploitation duringdata collection. Initially we establish that sample-efficient learning withoutadditional assumptions is unattainable owing to the curse of support shifti.e. the potential disjointedness of the distributional supports between thetraining and testing environments. To circumvent such a hardness result weintroduce the vanishing minimal value assumption to RMDPs with atotal-variation TV distance robust set postulating that the minimal value ofthe optimal robust value function is zero. We prove that such an assumptioneffectively eliminates the support shift issue for RMDPs with a TV distancerobust set and present an algorithm with a provable sample complexityguarantee. Our work makes the initial step to uncovering the inherentdifficulty of robust RL via interactive data collection and sufficientconditions for designing a sample-efficient algorithm accompanied by sharpsample complexity analysis.</p>
                <p>Last Updated: 2024-04-04 16:40:22 UTC</p>
                <button class="interpret-button" data-id="2404.03578v1">Interpret</button>
                <div id="interpretation-2404.03578v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Approximate Gradient Coding for Privacy-Flexible Federated Learning with Non-IID Data</h3>
                <p>Authors: Okko MakkonenSampo NiemeläCamilla HollantiSerge Kas Hanna</p>
                <p><a href="http://arxiv.org/abs/2404.03524v1">Link to paper</a></p>
                <p>This work focuses on the challenges of non-IID data and stragglers/dropoutsin federated learning. We introduce and explore a privacy-flexible paradigmthat models parts of the clients local data as non-private offering a moreversatile and business-oriented perspective on privacy. Within this frameworkwe propose a data-driven strategy for mitigating the effects of labelheterogeneity and client straggling on federated learning. Our solutioncombines both offline data sharing and approximate gradient coding techniques.Through numerical simulations using the MNIST dataset we demonstrate that ourapproach enables achieving a deliberate trade-off between privacy and utilityleading to improved model convergence and accuracy while using an adaptableportion of non-private data.</p>
                <p>Last Updated: 2024-04-04 15:29:50 UTC</p>
                <button class="interpret-button" data-id="2404.03524v1">Interpret</button>
                <div id="interpretation-2404.03524v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CountARFactuals -- Generating plausible model-agnostic counterfactual explanations with adversarial random forests</h3>
                <p>Authors: Susanne DandlKristin BleschTimo FreieslebenGunnar KönigJan KaparBernd BischlMarvin Wright</p>
                <p><a href="http://arxiv.org/abs/2404.03506v1">Link to paper</a></p>
                <p>Counterfactual explanations elucidate algorithmic decisions by pointing toscenarios that would have led to an alternative desired outcome. Givinginsight into the models behavior they hint users towards possible actions andgive grounds for contesting decisions. As a crucial factor in achieving thesegoals counterfactuals must be plausible i.e. describing realisticalternative scenarios within the data manifold. This paper leverages a recentlydeveloped generative modeling technique -- adversarial random forests ARFs --to efficiently generate plausible counterfactuals in a model-agnostic way. ARFscan serve as a plausibility measure or directly generate counterfactualexplanations. Our ARF-based approach surpasses the limitations of existingmethods that aim to generate plausible counterfactual explanations: It is easyto train and computationally highly efficient handles continuous andcategorical data naturally and allows integrating additional desiderata suchas sparsity in a straightforward manner.</p>
                <p>Last Updated: 2024-04-04 15:10:13 UTC</p>
                <button class="interpret-button" data-id="2404.03506v1">Interpret</button>
                <div id="interpretation-2404.03506v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LancBiO: dynamic Lanczos-aided bilevel optimization via Krylov subspace</h3>
                <p>Authors: Bin GaoYan YangYa-xiang Yuan</p>
                <p><a href="http://arxiv.org/abs/2404.03331v1">Link to paper</a></p>
                <p>Bilevel optimization with broad applications in machine learning has anintricate hierarchical structure. Gradient-based methods have emerged as acommon approach to large-scale bilevel problems. However the computation ofthe hyper-gradient which involves a Hessian inverse vector product confinesthe efficiency and is regarded as a bottleneck. To circumvent the inverse weconstruct a sequence of low-dimensional approximate Krylov subspaces with theaid of the Lanczos process. As a result the constructed subspace is able todynamically and incrementally approximate the Hessian inverse vector productwith less effort and thus leads to a favorable estimate of the hyper-gradient.Moreover we propose aprovable subspace-based framework for bilevel problemswhere one central step is to solve a small-size tridiagonal linear system. Tothe best of our knowledge this is the first time that subspace techniques areincorporated into bilevel optimization. This successful trial not only enjoysmathcalOepsilon-1 convergence rate but also demonstrates efficiencyin a synthetic problem and two deep learning tasks.</p>
                <p>Last Updated: 2024-04-04 09:57:29 UTC</p>
                <button class="interpret-button" data-id="2404.03331v1">Interpret</button>
                <div id="interpretation-2404.03331v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>OW-VISCap: Open-World Video Instance Segmentation and Captioning</h3>
                <p>Authors: Anwesa ChoudhuriGirish ChowdharyAlexander G. Schwing</p>
                <p><a href="http://arxiv.org/abs/2404.03657v1">Link to paper</a></p>
                <p>Open-world video instance segmentation is an important video understandingtask. Yet most methods either operate in a closed-world setting require anadditional user-input or use classic region-based proposals to identify neverbefore seen objects. Further these methods only assign a one-word label todetected objects and dont generate rich object-centric descriptions. Theyalso often suffer from highly overlapping predictions. To address these issueswe propose Open-World Video Instance Segmentation and Captioning OW-VISCapan approach to jointly segment track and caption previously seen or unseenobjects in a video. For this we introduce open-world object queries todiscover never before seen objects without additional user-input. We generaterich and descriptive object-centric captions for each detected object via amasked attention augmented LLM input. We introduce an inter-query contrastiveloss to ensure that the object queries differ from one another. Our generalizedapproach matches or surpasses state-of-the-art on three tasks: open-world videoinstance segmentation on the BURST dataset dense video object captioning onthe VidSTG dataset and closed-world video instance segmentation on the OVISdataset.</p>
                <p>Last Updated: 2024-04-04 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2404.03657v1">Interpret</button>
                <div id="interpretation-2404.03657v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</h3>
                <p>Authors: Dongzhi JiangGuanglu SongXiaoshi WuRenrui ZhangDazhong ShenZhuofan ZongYu LiuHongsheng Li</p>
                <p><a href="http://arxiv.org/abs/2404.03653v1">Link to paper</a></p>
                <p>Diffusion models have demonstrated great success in the field oftext-to-image generation. However alleviating the misalignment between thetext prompts and images is still challenging. The root reason behind themisalignment has not been extensively investigated. We observe that themisalignment is caused by inadequate token attention activation. We furtherattribute this phenomenon to the diffusion models insufficient conditionutilization which is caused by its training paradigm. To address the issue wepropose CoMat an end-to-end diffusion model fine-tuning strategy with animage-to-text concept matching mechanism. We leverage an image captioning modelto measure image-to-text alignment and guide the diffusion model to revisitignored tokens. A novel attribute concentration module is also proposed toaddress the attribute binding problem. Without any image or human preferencedata we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.Extensive experiments show that CoMat-SDXL significantly outperforms thebaseline model SDXL in two text-to-image alignment benchmarks and achievesstart-of-the-art performance.</p>
                <p>Last Updated: 2024-04-04 17:59:46 UTC</p>
                <button class="interpret-button" data-id="2404.03653v1">Interpret</button>
                <div id="interpretation-2404.03653v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra</h3>
                <p>Authors: Darioush KevianUsman SyedXingang GuoAaron HavensGeir DullerudPeter SeilerLianhui QinBin Hu</p>
                <p><a href="http://arxiv.org/abs/2404.03647v1">Link to paper</a></p>
                <p>In this paper we explore the capabilities of state-of-the-art large languagemodels LLMs such as GPT-4 Claude 3 Opus and Gemini 1.0 Ultra in solvingundergraduate-level control problems. Controls provides an interesting casestudy for LLM reasoning due to its combination of mathematical theory andengineering design. We introduce ControlBench a benchmark dataset tailored toreflect the breadth depth and complexity of classical control design. We usethis dataset to study and evaluate the problem-solving abilities of these LLMsin the context of control engineering. We present evaluations conducted by apanel of human experts providing insights into the accuracy reasoning andexplanatory prowess of LLMs in control engineering. Our analysis reveals thestrengths and limitations of each LLM in the context of classical control andour results imply that Claude 3 Opus has become the state-of-the-art LLM forsolving undergraduate control problems. Our study serves as an initial steptowards the broader goal of employing artificial general intelligence incontrol engineering.</p>
                <p>Last Updated: 2024-04-04 17:58:38 UTC</p>
                <button class="interpret-button" data-id="2404.03647v1">Interpret</button>
                <div id="interpretation-2404.03647v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WorDepth: Variational Language Prior for Monocular Depth Estimation</h3>
                <p>Authors: Ziyao ZengDaniel WangFengyu YangHyoungseob ParkYangchao WuStefano SoattoByung-Woo HongDong LaoAlex Wong</p>
                <p><a href="http://arxiv.org/abs/2404.03635v1">Link to paper</a></p>
                <p>Three-dimensional 3D reconstruction from a single image is an ill-posedproblem with inherent ambiguities i.e. scale. Predicting a 3D scene from textdescriptions is similarly ill-posed i.e. spatial arrangements of objectsdescribed. We investigate the question of whether two inherently ambiguousmodalities can be used in conjunction to produce metric-scaled reconstructions.To test this we focus on monocular depth estimation the problem of predictinga dense depth map from a single image but with an additional text captiondescribing the scene. To this end we begin by encoding the text caption as amean and standard deviation using a variational framework we learn thedistribution of the plausible metric reconstructions of 3D scenes correspondingto the text captions as a prior. To select a specific reconstruction or depthmap we encode the given image through a conditional sampler that samples fromthe latent space of the variational text encoder which is then decoded to theoutput depth map. Our approach is trained alternatingly between the text andimage branches: in one optimization step we predict the mean and standarddeviation from the text description and sample from a standard Gaussian and inthe other we sample using a image conditional sampler. Once trained wedirectly predict depth from the encoded text using the conditional sampler. Wedemonstrate our approach on indoor NYUv2 and outdoor KITTI scenarios wherewe show that language can consistently improve performance in both.</p>
                <p>Last Updated: 2024-04-04 17:54:33 UTC</p>
                <button class="interpret-button" data-id="2404.03635v1">Interpret</button>
                <div id="interpretation-2404.03635v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Standardizing Knowledge Engineering Practices with a Reference Architecture</h3>
                <p>Authors: Bradley P. AllenFilip Ilievski</p>
                <p><a href="http://arxiv.org/abs/2404.03624v1">Link to paper</a></p>
                <p>Knowledge engineering is the process of creating and maintainingknowledge-producing systems. Throughout the history of computer science and AIknowledge engineering workflows have been widely used given the importance ofhigh-quality knowledge for reliable intelligent agents. Meanwhile the scope ofknowledge engineering as apparent from its target tasks and use cases hasbeen shifting together with its paradigms such as expert systems semanticweb and language modeling. The intended use cases and supported userrequirements between these paradigms have not been analyzed globally as newparadigms often satisfy prior pain points while possibly introducing new ones.The recent abstraction of systemic patterns into a boxology provides an openingfor aligning the requirements and use cases of knowledge engineering with thesystems components and software that can satisfy them best. This paperproposes a vision of harmonizing the best practices in the field of knowledgeengineering by leveraging the software engineering methodology of creatingreference architectures. We describe how a reference architecture can beiteratively designed and implemented to associate user needs with recurringsystemic patterns building on top of existing knowledge engineering workflowsand boxologies. We provide a six-step roadmap that can enable the developmentof such an architecture providing an initial design and outcome of thedefinition of architectural scope selection of information sources andanalysis. We expect that following through on this vision will lead towell-grounded reference architectures for knowledge engineering will advancethe ongoing initiatives of organizing the neurosymbolic knowledge engineeringspace and will build new links to the software architectures and data sciencecommunities.</p>
                <p>Last Updated: 2024-04-04 17:46:32 UTC</p>
                <button class="interpret-button" data-id="2404.03624v1">Interpret</button>
                <div id="interpretation-2404.03624v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra</h3>
                <p>Authors: Darioush KevianUsman SyedXingang GuoAaron HavensGeir DullerudPeter SeilerLianhui QinBin Hu</p>
                <p><a href="http://arxiv.org/abs/2404.03647v1">Link to paper</a></p>
                <p>In this paper we explore the capabilities of state-of-the-art large languagemodels LLMs such as GPT-4 Claude 3 Opus and Gemini 1.0 Ultra in solvingundergraduate-level control problems. Controls provides an interesting casestudy for LLM reasoning due to its combination of mathematical theory andengineering design. We introduce ControlBench a benchmark dataset tailored toreflect the breadth depth and complexity of classical control design. We usethis dataset to study and evaluate the problem-solving abilities of these LLMsin the context of control engineering. We present evaluations conducted by apanel of human experts providing insights into the accuracy reasoning andexplanatory prowess of LLMs in control engineering. Our analysis reveals thestrengths and limitations of each LLM in the context of classical control andour results imply that Claude 3 Opus has become the state-of-the-art LLM forsolving undergraduate control problems. Our study serves as an initial steptowards the broader goal of employing artificial general intelligence incontrol engineering.</p>
                <p>Last Updated: 2024-04-04 17:58:38 UTC</p>
                <button class="interpret-button" data-id="2404.03647v1">Interpret</button>
                <div id="interpretation-2404.03647v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WorDepth: Variational Language Prior for Monocular Depth Estimation</h3>
                <p>Authors: Ziyao ZengDaniel WangFengyu YangHyoungseob ParkYangchao WuStefano SoattoByung-Woo HongDong LaoAlex Wong</p>
                <p><a href="http://arxiv.org/abs/2404.03635v1">Link to paper</a></p>
                <p>Three-dimensional 3D reconstruction from a single image is an ill-posedproblem with inherent ambiguities i.e. scale. Predicting a 3D scene from textdescriptions is similarly ill-posed i.e. spatial arrangements of objectsdescribed. We investigate the question of whether two inherently ambiguousmodalities can be used in conjunction to produce metric-scaled reconstructions.To test this we focus on monocular depth estimation the problem of predictinga dense depth map from a single image but with an additional text captiondescribing the scene. To this end we begin by encoding the text caption as amean and standard deviation using a variational framework we learn thedistribution of the plausible metric reconstructions of 3D scenes correspondingto the text captions as a prior. To select a specific reconstruction or depthmap we encode the given image through a conditional sampler that samples fromthe latent space of the variational text encoder which is then decoded to theoutput depth map. Our approach is trained alternatingly between the text andimage branches: in one optimization step we predict the mean and standarddeviation from the text description and sample from a standard Gaussian and inthe other we sample using a image conditional sampler. Once trained wedirectly predict depth from the encoded text using the conditional sampler. Wedemonstrate our approach on indoor NYUv2 and outdoor KITTI scenarios wherewe show that language can consistently improve performance in both.</p>
                <p>Last Updated: 2024-04-04 17:54:33 UTC</p>
                <button class="interpret-button" data-id="2404.03635v1">Interpret</button>
                <div id="interpretation-2404.03635v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Training LLMs over Neurally Compressed Text</h3>
                <p>Authors: Brian LesterJaehoon LeeAlex AlemiJeffrey PenningtonAdam RobertsJascha Sohl-DicksteinNoah Constant</p>
                <p><a href="http://arxiv.org/abs/2404.03626v1">Link to paper</a></p>
                <p>In this paper we explore the idea of training large language models LLMsover highly compressed text. While standard subword tokenizers compress text bya small factor neural text compressors can achieve much higher rates ofcompression. If it were possible to train LLMs directly over neurallycompressed text this would confer advantages in training and servingefficiency as well as easier handling of long text spans. The main obstacle tothis goal is that strong compression tends to produce opaque outputs that arenot well-suited for learning. In particular we find that text naivelycompressed via Arithmetic Coding is not readily learnable by LLMs. To overcomethis we propose Equal-Info Windows a novel compression technique whereby textis segmented into blocks that each compress to the same bit length. Using thismethod we demonstrate effective learning over neurally compressed text thatimproves with scale and outperforms byte-level baselines by a wide margin onperplexity and inference speed benchmarks. While our method delivers worseperplexity than subword tokenizers for models trained with the same parametercount it has the benefit of shorter sequence lengths. Shorter sequence lengthsrequire fewer autoregressive generation steps and reduce latency. Finally weprovide extensive analysis of the properties that contribute to learnabilityand offer concrete suggestions for how to further improve the performance ofhigh-compression tokenizers.</p>
                <p>Last Updated: 2024-04-04 17:48:28 UTC</p>
                <button class="interpret-button" data-id="2404.03626v1">Interpret</button>
                <div id="interpretation-2404.03626v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On the Efficiency of Convolutional Neural Networks</h3>
                <p>Authors: Andrew Lavin</p>
                <p><a href="http://arxiv.org/abs/2404.03617v1">Link to paper</a></p>
                <p>Since the breakthrough performance of AlexNet in 2012 convolutional neuralnetworks convnets have grown into extremely powerful vision models. Deeplearning researchers have used convnets to produce accurate results that wereunachievable a decade ago. Yet computer scientists make computationalefficiency their primary objective. Accuracy with exorbitant cost is notacceptable an algorithm must also minimize its computational requirements.Confronted with the daunting computation that convnets use deep learningresearchers also became interested in efficiency. Researchers appliedtremendous effort to find the convnet architectures that have the greatestefficiency. However skepticism grew among researchers and engineers alikeabout the relevance of arithmetic complexity. Contrary to the prevailing viewthat latency and arithmetic complexity are irreconcilable a simple formularelates both through computational efficiency. This insight enabled us toco-optimize the separate factors that determine latency. We observed that thedegenerate conv2d layers that produce the best accuracy-complexity trade-offalso have low operational intensity. Therefore kernels that implement theselayers use significant memory resources. We solved this optimization problemwith block-fusion kernels that implement all layers of a residual blockthereby creating temporal locality avoiding communication and reducingworkspace size. Our ConvFirst model with block-fusion kernels ran approximatelyfour times as fast as the ConvNeXt baseline with PyTorch Inductor at equalaccuracy on the ImageNet-1K classification task. Our unified approach toconvnet efficiency envisions a new era of models and kernels that achievegreater accuracy at lower cost.</p>
                <p>Last Updated: 2024-04-04 17:39:41 UTC</p>
                <button class="interpret-button" data-id="2404.03617v1">Interpret</button>
                <div id="interpretation-2404.03617v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization</h3>
                <p>Authors: Aniruddha NrusimhaMayank MishraNaigang WangDan AlistarhRameswar PandaYoon Kim</p>
                <p><a href="http://arxiv.org/abs/2404.03605v1">Link to paper</a></p>
                <p>We consider the problem of accurate quantization for language models whereboth the weights and activations are uniformly quantized to 4 bits perparameter the lowest bitwidth format natively supported by GPU hardware. Inthis context the key challenge is activation quantization: it is known thatlanguage models contain outlier channels whose values on average are orders ofmagnitude higher than than other channels which prevents accurate low-bitwidthquantization with known techniques. We systematically study this phenomena andfind that these outlier channels emerge early in training and that they occurmore frequently in layers with residual streams. We then propose a simplestrategy which regularizes a layers inputs via quantization-aware trainingQAT and its outputs via activation kurtosis regularization. We show thatregularizing both the inputs and outputs is crucial for preventing a modelsmigrating the difficulty in input quantization to the weights which makespost-training quantization PTQ of weights more difficult. When combined withweight PTQ we show that our approach can obtain a W4A4 model that performscompetitively to the standard-precision W16A16 baseline.</p>
                <p>Last Updated: 2024-04-04 17:25:30 UTC</p>
                <button class="interpret-button" data-id="2404.03605v1">Interpret</button>
                <div id="interpretation-2404.03605v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning</h3>
                <p>Authors: Rui LiTobias FischerMattia SeguMarc PollefeysLuc Van GoolFederico Tombari</p>
                <p><a href="http://arxiv.org/abs/2404.03658v1">Link to paper</a></p>
                <p>Recovering the 3D scene geometry from a single view is a fundamental yetill-posed problem in computer vision. While classical depth estimation methodsinfer only a 2.5D scene representation limited to the image plane recentapproaches based on radiance fields reconstruct a full 3D representation.However these methods still struggle with occluded regions since inferringgeometry without visual observation requires i semantic knowledge of thesurroundings and ii reasoning about spatial context. We propose KYN a novelmethod for single-view scene reconstruction that reasons about semantic andspatial context to predict each points density. We introduce a vision-languagemodulation module to enrich point features with fine-grained semanticinformation. We aggregate point representations across the scene through alanguage-guided spatial attention mechanism to yield per-point densitypredictions aware of the 3D semantic context. We show that KYN improves 3Dshape recovery compared to predicting density for each 3D point in isolation.We achieve state-of-the-art results in scene and object reconstruction onKITTI-360 and show improved zero-shot generalization compared to prior work.Project page: https://ruili3.github.io/kyn.</p>
                <p>Last Updated: 2024-04-04 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2404.03658v1">Interpret</button>
                <div id="interpretation-2404.03658v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>OW-VISCap: Open-World Video Instance Segmentation and Captioning</h3>
                <p>Authors: Anwesa ChoudhuriGirish ChowdharyAlexander G. Schwing</p>
                <p><a href="http://arxiv.org/abs/2404.03657v1">Link to paper</a></p>
                <p>Open-world video instance segmentation is an important video understandingtask. Yet most methods either operate in a closed-world setting require anadditional user-input or use classic region-based proposals to identify neverbefore seen objects. Further these methods only assign a one-word label todetected objects and dont generate rich object-centric descriptions. Theyalso often suffer from highly overlapping predictions. To address these issueswe propose Open-World Video Instance Segmentation and Captioning OW-VISCapan approach to jointly segment track and caption previously seen or unseenobjects in a video. For this we introduce open-world object queries todiscover never before seen objects without additional user-input. We generaterich and descriptive object-centric captions for each detected object via amasked attention augmented LLM input. We introduce an inter-query contrastiveloss to ensure that the object queries differ from one another. Our generalizedapproach matches or surpasses state-of-the-art on three tasks: open-world videoinstance segmentation on the BURST dataset dense video object captioning onthe VidSTG dataset and closed-world video instance segmentation on the OVISdataset.</p>
                <p>Last Updated: 2024-04-04 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2404.03657v1">Interpret</button>
                <div id="interpretation-2404.03657v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</h3>
                <p>Authors: Hanzhe HuZhizhuo ZhouVarun JampaniShubham Tulsiani</p>
                <p><a href="http://arxiv.org/abs/2404.03656v1">Link to paper</a></p>
                <p>We present MVD-Fusion: a method for single-view 3D inference via generativemodeling of multi-view-consistent RGB-D images. While recent methods pursuing3D inference advocate learning novel-view generative models these generationsare not 3D-consistent and require a distillation process to generate a 3Doutput. We instead cast the task of 3D inference as directly generatingmutually-consistent multiple views and build on the insight that additionallyinferring depth can provide a mechanism for enforcing this consistency.Specifically we train a denoising diffusion model to generate multi-view RGB-Dimages given a single RGB input image and leverage the intermediate noisydepth estimates to obtain reprojection-based conditioning to maintainmulti-view consistency. We train our model using large-scale synthetic datasetObajverse as well as the real-world CO3D dataset comprising of generic cameraviewpoints. We demonstrate that our approach can yield more accurate synthesiscompared to recent state-of-the-art including distillation-based 3D inferenceand prior multi-view generation methods. We also evaluate the geometry inducedby our multi-view depth prediction and find that it yields a more accuraterepresentation than other direct 3D inference approaches.</p>
                <p>Last Updated: 2024-04-04 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2404.03656v1">Interpret</button>
                <div id="interpretation-2404.03656v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RaFE: Generative Radiance Fields Restoration</h3>
                <p>Authors: Zhongkai WuZiyu WanJing ZhangJing LiaoDong Xu</p>
                <p><a href="http://arxiv.org/abs/2404.03654v1">Link to paper</a></p>
                <p>NeRF Neural Radiance Fields has demonstrated tremendous potential in novelview synthesis and 3D reconstruction but its performance is sensitive to inputimage quality which struggles to achieve high-fidelity rendering when providedwith low-quality sparse input viewpoints. Previous methods for NeRF restorationare tailored for specific degradation type ignoring the generality ofrestoration. To overcome this limitation we propose a generic radiance fieldsrestoration pipeline named RaFE which applies to various types ofdegradations such as low resolution blurriness noise compression artifactsor their combinations. Our approach leverages the success of off-the-shelf 2Drestoration methods to recover the multi-view images individually. Instead ofreconstructing a blurred NeRF by averaging inconsistencies we introduce anovel approach using Generative Adversarial Networks GANs for NeRF generationto better accommodate the geometric and appearance inconsistencies present inthe multi-view images. Specifically we adopt a two-level tri-planearchitecture where the coarse level remains fixed to represent the low-qualityNeRF and a fine-level residual tri-plane to be added to the coarse level ismodeled as a distribution with GAN to capture potential variations inrestoration. We validate RaFE on both synthetic and real cases for variousrestoration tasks demonstrating superior performance in both quantitative andqualitative evaluations surpassing other 3D restoration methods specific tosingle task. Please see our project websitehttps://zkaiwu.github.io/RaFE-Project/.</p>
                <p>Last Updated: 2024-04-04 17:59:50 UTC</p>
                <button class="interpret-button" data-id="2404.03654v1">Interpret</button>
                <div id="interpretation-2404.03654v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</h3>
                <p>Authors: Dongzhi JiangGuanglu SongXiaoshi WuRenrui ZhangDazhong ShenZhuofan ZongYu LiuHongsheng Li</p>
                <p><a href="http://arxiv.org/abs/2404.03653v1">Link to paper</a></p>
                <p>Diffusion models have demonstrated great success in the field oftext-to-image generation. However alleviating the misalignment between thetext prompts and images is still challenging. The root reason behind themisalignment has not been extensively investigated. We observe that themisalignment is caused by inadequate token attention activation. We furtherattribute this phenomenon to the diffusion models insufficient conditionutilization which is caused by its training paradigm. To address the issue wepropose CoMat an end-to-end diffusion model fine-tuning strategy with animage-to-text concept matching mechanism. We leverage an image captioning modelto measure image-to-text alignment and guide the diffusion model to revisitignored tokens. A novel attribute concentration module is also proposed toaddress the attribute binding problem. Without any image or human preferencedata we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.Extensive experiments show that CoMat-SDXL significantly outperforms thebaseline model SDXL in two text-to-image alignment benchmarks and achievesstart-of-the-art performance.</p>
                <p>Last Updated: 2024-04-04 17:59:46 UTC</p>
                <button class="interpret-button" data-id="2404.03653v1">Interpret</button>
                <div id="interpretation-2404.03653v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Laser Learning Environment: A new environment for coordination-critical multi-agent tasks</h3>
                <p>Authors: Yannick MolinghenRaphaël AvalosMark Van AchterAnn NowéTom Lenaerts</p>
                <p><a href="http://arxiv.org/abs/2404.03596v1">Link to paper</a></p>
                <p>We introduce the Laser Learning Environment LLE a collaborativemulti-agent reinforcement learning environment in which coordination iscentral. In LLE agents depend on each other to make progressinterdependence must jointly take specific sequences of actions to succeedperfect coordination and accomplishing those joint actions does not yieldany intermediate reward zero-incentive dynamics. The challenge of suchproblems lies in the difficulty of escaping state space bottlenecks caused byinterdependence steps since escaping those bottlenecks is not rewarded. We testmultiple state-of-the-art value-based MARL algorithms against LLE and show thatthey consistently fail at the collaborative task because of their inability toescape state space bottlenecks even though they successfully achieve perfectcoordination. We show that Q-learning extensions such as prioritized experiencereplay and n-steps return hinder exploration in environments withzero-incentive dynamics and find that intrinsic curiosity with random networkdistillation is not sufficient to escape those bottlenecks. We demonstrate theneed for novel methods to solve this problem and the relevance of LLE ascooperative MARL benchmark.</p>
                <p>Last Updated: 2024-04-04 17:05:42 UTC</p>
                <button class="interpret-button" data-id="2404.03596v1">Interpret</button>
                <div id="interpretation-2404.03596v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>No Panacea in Planning: Algorithm Selection for Suboptimal Multi-Agent Path Finding</h3>
                <p>Authors: Weizhe ChenZhihan WangJiaoyang LiSven KoenigBistra Dilkina</p>
                <p><a href="http://arxiv.org/abs/2404.03554v1">Link to paper</a></p>
                <p>Since more and more algorithms are proposed for multi-agent path findingMAPF and each of them has its strengths choosing the correct one for aspecific scenario that fulfills some specified requirements is an importanttask. Previous research in algorithm selection for MAPF built a standardworkflow and showed that machine learning can help. In this paper we studygeneral solvers for MAPF which further include suboptimal algorithms. Wepropose different groups of optimization objectives and learning tasks tohandle the new tradeoff between runtime and solution quality. We conductextensive experiments to show that the same loss can not be used for differentgroups of optimization objectives and that standard computer vision models areno worse than customized architecture. We also provide insightful discussionson how feature-sensitive pre-processing is needed for learning for MAPF andhow different learning metrics are correlated to different learning tasks.</p>
                <p>Last Updated: 2024-04-04 16:06:39 UTC</p>
                <button class="interpret-button" data-id="2404.03554v1">Interpret</button>
                <div id="interpretation-2404.03554v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MEDIATE: Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange</h3>
                <p>Authors: Philipp AltmannKatharina WinterMichael KölleMaximilian ZornThomy PhanClaudia Linnhoff-Popien</p>
                <p><a href="http://arxiv.org/abs/2404.03431v1">Link to paper</a></p>
                <p>Recent advances in multi-agent systems MAS have shown that incorporatingpeer incentivization PI mechanisms vastly improves cooperation. Especially insocial dilemmas communication between the agents helps to overcome sub-optimalNash equilibria. However incentivization tokens need to be carefully selected.Furthermore real-world applications might yield increased privacy requirementsand limited exchange. Therefore we extend the PI protocol for mutualacknowledgment token exchange MATE and provide additional analysis on theimpact of the chosen tokens. Building upon those insights we propose mutuallyendorsed distributed incentive acknowledgment token exchange MEDIATE anextended PI architecture employing automatic token derivation via decentralizedconsensus. Empirical results show the stable agreement on appropriate tokensyielding superior performance compared to static tokens and state-of-the-artapproaches in different social dilemma environments with various rewarddistributions.</p>
                <p>Last Updated: 2024-04-04 13:24:33 UTC</p>
                <button class="interpret-button" data-id="2404.03431v1">Interpret</button>
                <div id="interpretation-2404.03431v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large Neighborhoods Search</h3>
                <p>Authors: Weizhe ChenSven KoenigBistra Dilkina</p>
                <p><a href="http://arxiv.org/abs/2404.03101v1">Link to paper</a></p>
                <p>Cooperative multi-agent reinforcement learning MARL has been anincreasingly important research topic in the last half-decade because of itsgreat potential for real-world applications. Because of the curse ofdimensionality the popular centralized training decentralized executionframework requires a long time in training yet still cannot convergeefficiently. In this paper we propose a general training framework MARL-LNSto algorithmically address these issues by training on alternating subsets ofagents using existing deep MARL algorithms as low-level trainers while notinvolving any additional parameters to be trained. Based on this framework weprovide three algorithm variants based on the framework: random largeneighborhood search RLNS batch large neighborhood search BLNS andadaptive large neighborhood search ALNS which alternate the subsets ofagents differently. We test our algorithms on both the StarCraft Multi-AgentChallenge and Google Research Football showing that our algorithms canautomatically reduce at least 10 of training time while reaching the samefinal skill level as the original algorithm.</p>
                <p>Last Updated: 2024-04-03 22:51:54 UTC</p>
                <button class="interpret-button" data-id="2404.03101v1">Interpret</button>
                <div id="interpretation-2404.03101v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Traffic Divergence Theory: An Analysis Formalism for Dynamic Networks</h3>
                <p>Authors: Matin MacktoobianZhan ShuQing Zhao</p>
                <p><a href="http://dx.doi.org/10.1109/ACCESS.2024.3383436">Link to paper</a></p>
                <p>Traffic dynamics is universally crucial in analyzing and designing almost anynetwork. This article introduces a novel theoretical approach to analyzingnetwork traffic dynamics. This theorys machinery is based on the notion oftraffic divergence which captures the flow imbalance of network nodes andlinks. It features various analytical probes to investigate both spatial andtemporal traffic dynamics. In particular the maximal traffic distribution in anetwork can be characterized by spatial traffic divergence rate which revealsthe relative difference among node traffic divergence. To illustrate theusefulness we apply the theory to two network-driven problems: throughputestimation of data center networks and power-optimized communication planningfor robot networks and show the merits of the proposed theory throughsimulations.</p>
                <p>Last Updated: 2024-04-03 21:13:15 UTC</p>
                <button class="interpret-button" data-id="2404.03066v1">Interpret</button>
                <div id="interpretation-2404.03066v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-04-07</p>
        </div>
    
        </div>
    </body>
    </html>
    