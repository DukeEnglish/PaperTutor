
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding</h3>
                <p>Authors: Yunze ManShuhong ZhengZhipeng BaoMartial HebertLiang-Yan GuiYu-Xiong Wang</p>
                <p><a href="http://arxiv.org/abs/2409.03757v1">Link to paper</a></p>
                <p>Complex 3D scene understanding has gained increasing attention with sceneencoding strategies playing a crucial role in this success. However theoptimal scene encoding strategies for various scenarios remain unclearparticularly compared to their image-based counterparts. To address this issuewe present a comprehensive study that probes various visual encoding models for3D scene understanding identifying the strengths and limitations of each modelacross different scenarios. Our evaluation spans seven vision foundationencoders including image-based video-based and 3D foundation models. Weevaluate these models in four tasks: Vision-Language Scene Reasoning VisualGrounding Segmentation and Registration each focusing on different aspectsof scene understanding. Our evaluations yield key findings: DINOv2 demonstratessuperior performance video models excel in object-level tasks diffusionmodels benefit geometric tasks and language-pretrained models show unexpectedlimitations in language-related tasks. These insights challenge someconventional understandings provide novel perspectives on leveraging visualfoundation models and highlight the need for more flexible encoder selectionin future vision-language and scene-understanding tasks.</p>
                <p>Last Updated: 2024-09-05 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2409.03757v1">Interpret</button>
                <div id="interpretation-2409.03757v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild</h3>
                <p>Authors: Yuntian DengWenting ZhaoJack HesselXiang RenClaire CardieYejin Choi</p>
                <p><a href="http://arxiv.org/abs/2409.03753v1">Link to paper</a></p>
                <p>The increasing availability of real-world conversation data offers excitingopportunities for researchers to study user-chatbot interactions. However thesheer volume of this data makes manually examining individual conversationsimpractical. To overcome this challenge we introduce WildVis an interactivetool that enables fast versatile and large-scale conversation analysis.WildVis provides search and visualization capabilities in the text andembedding spaces based on a list of criteria. To manage million-scale datasetswe implemented optimizations including search index construction embeddingprecomputation and compression and caching to ensure responsive userinteractions within seconds. We demonstrate WildViss utility through threecase studies: facilitating chatbot misuse research visualizing and comparingtopic distributions across datasets and characterizing user-specificconversation patterns. WildVis is open-source and designed to be extendablesupporting additional datasets and customized search and visualizationfunctionalities.</p>
                <p>Last Updated: 2024-09-05 17:59:15 UTC</p>
                <button class="interpret-button" data-id="2409.03753v1">Interpret</button>
                <div id="interpretation-2409.03753v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Attention Heads of Large Language Models: A Survey</h3>
                <p>Authors: Zifan ZhengYezhaohui WangYuxin HuangShichao SongBo TangFeiyu XiongZhiyu Li</p>
                <p><a href="http://arxiv.org/abs/2409.03752v1">Link to paper</a></p>
                <p>Since the advent of ChatGPT Large Language Models LLMs have excelled invarious tasks but remain largely as black-box systems. Consequently theirdevelopment relies heavily on data-driven approaches limiting performanceenhancement through changes in internal architecture and reasoning pathways. Asa result many researchers have begun exploring the potential internalmechanisms of LLMs aiming to identify the essence of their reasoningbottlenecks with most studies focusing on attention heads. Our survey aims toshed light on the internal reasoning processes of LLMs by concentrating on theinterpretability and underlying mechanisms of attention heads. We first distillthe human thought process into a four-stage framework: Knowledge RecallingIn-Context Identification Latent Reasoning and Expression Preparation. Usingthis framework we systematically review existing research to identify andcategorize the functions of specific attention heads. Furthermore we summarizethe experimental methodologies used to discover these special heads dividingthem into two categories: Modeling-Free methods and Modeling-Required methods.Also we outline relevant evaluation methods and benchmarks. Finally wediscuss the limitations of current research and propose several potentialfuture directions. Our reference list is open-sourced aturlhttps://github.com/IAAR-Shanghai/Awesome-Attention-Heads.</p>
                <p>Last Updated: 2024-09-05 17:59:12 UTC</p>
                <button class="interpret-button" data-id="2409.03752v1">Interpret</button>
                <div id="interpretation-2409.03752v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Planning In Natural Language Improves LLM Search For Code Generation</h3>
                <p>Authors: Evan WangFederico CassanoCatherine WuYunfeng BaiWill SongVaskar NathZiwen HanSean HendryxSummer YueHugh Zhang</p>
                <p><a href="http://arxiv.org/abs/2409.03733v1">Link to paper</a></p>
                <p>While scaling training compute has led to remarkable improvements in largelanguage models LLMs scaling inference compute has not yet yielded analogousgains. We hypothesize that a core missing component is a lack of diverse LLMoutputs leading to inefficient search due to models repeatedly sampling highlysimilar yet incorrect generations. We empirically demonstrate that this lackof diversity can be mitigated by searching over candidate plans for solving aproblem in natural language. Based on this insight we propose PLANSEARCH anovel search algorithm which shows strong results across HumanEval MBPP andLiveCodeBench a contamination-free benchmark for competitive coding.PLANSEARCH generates a diverse set of observations about the problem and thenuses these observations to construct plans for solving the problem. Bysearching over plans in natural language rather than directly over codesolutions PLANSEARCH explores a significantly more diverse range of potentialsolutions compared to baseline search methods. Using PLANSEARCH on top ofClaude 3.5 Sonnet achieves a state-of-the-art pass200 of 77.0 onLiveCodeBench outperforming both the best score achieved without searchpass1  41.4 and using standard repeated sampling pass200  60.6.Finally we show that across all models search algorithms and benchmarksanalyzed we can accurately predict performance gains due to search as a directfunction of the diversity over generated ideas.</p>
                <p>Last Updated: 2024-09-05 17:44:49 UTC</p>
                <button class="interpret-button" data-id="2409.03733v1">Interpret</button>
                <div id="interpretation-2409.03733v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RAG based Question-Answering for Contextual Response Prediction System</h3>
                <p>Authors: Sriram VeturiSaurabh VaichalNafis Irtiza TriptoReshma Lal JagadheeshNian Yan</p>
                <p><a href="http://arxiv.org/abs/2409.03708v1">Link to paper</a></p>
                <p>Large Language Models LLMs have shown versatility in various NaturalLanguage Processing NLP tasks including their potential as effectivequestion-answering systems. However to provide precise and relevantinformation in response to specific customer queries in industry settings LLMsrequire access to a comprehensive knowledge base to avoid hallucinations.Retrieval Augmented Generation RAG emerges as a promising technique toaddress this challenge. Yet developing an accurate question-answeringframework for real-world applications using RAG entails several challenges: 1data availability issues 2 evaluating the quality of generated content and3 the costly nature of human evaluation. In this paper we introduce anend-to-end framework that employs LLMs with RAG capabilities for industry usecases. Given a customer query the proposed system retrieves relevant knowledgedocuments and leverages them along with previous chat history to generateresponse suggestions for customer service agents in the contact centers of amajor retail company. Through comprehensive automated and human evaluations weshow that this solution outperforms the current BERT-based algorithms inaccuracy and relevance. Our findings suggest that RAG-based LLMs can be anexcellent support to human customer service representatives by lightening theirworkload.</p>
                <p>Last Updated: 2024-09-05 17:14:23 UTC</p>
                <button class="interpret-button" data-id="2409.03708v1">Interpret</button>
                <div id="interpretation-2409.03708v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron</h3>
                <p>Authors: Christian SchmidJames M. Murray</p>
                <p><a href="http://arxiv.org/abs/2409.03749v1">Link to paper</a></p>
                <p>The ability of a brain or a neural network to efficiently learn dependscrucially on both the task structure and the learning rule. Previous works haveanalyzed the dynamical equations describing learning in the relativelysimplified context of the perceptron under assumptions of a student-teacherframework or a linearized output. While these assumptions have facilitatedtheoretical understanding they have precluded a detailed understanding of theroles of the nonlinearity and input-data distribution in determining thelearning dynamics limiting the applicability of the theories to realbiological or artificial neural networks. Here we use a stochastic-processapproach to derive flow equations describing learning applying this frameworkto the case of a nonlinear perceptron performing binary classification. Wecharacterize the effects of the learning rule supervised or reinforcementlearning SL/RL and input-data distribution on the perceptrons learning curveand the forgetting curve as subsequent tasks are learned. In particular wefind that the input-data noise differently affects the learning speed under SLvs. RL as well as determines how quickly learning of a task is overwritten bysubsequent learning. Additionally we verify our approach with real data usingthe MNIST dataset. This approach points a way toward analyzing learningdynamics for more-complex circuit architectures.</p>
                <p>Last Updated: 2024-09-05 17:58:28 UTC</p>
                <button class="interpret-button" data-id="2409.03749v1">Interpret</button>
                <div id="interpretation-2409.03749v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Safety vs. Performance: How Multi-Objective Learning Reduces Barriers to Market Entry</h3>
                <p>Authors: Meena JagadeesanMichael I. JordanJacob Steinhardt</p>
                <p><a href="http://arxiv.org/abs/2409.03734v1">Link to paper</a></p>
                <p>Emerging marketplaces for large language models and other large-scale machinelearning ML models appear to exhibit market concentration which has raisedconcerns about whether there are insurmountable barriers to entry in suchmarkets. In this work we study this issue from both an economic and analgorithmic point of view focusing on a phenomenon that reduces barriers toentry. Specifically an incumbent company risks reputational damage unless itsmodel is sufficiently aligned with safety objectives whereas a new company canmore easily avoid reputational damage. To study this issue formally we definea multi-objective high-dimensional regression framework that capturesreputational damage and we characterize the number of data points that a newcompany needs to enter the market. Our results demonstrate how multi-objectiveconsiderations can fundamentally reduce barriers to entry -- the requirednumber of data points can be significantly smaller than the incumbent companysdataset size. En route to proving these results we develop scaling laws forhigh-dimensional linear regression in multi-objective environments showingthat the scaling rate becomes slower when the dataset size is large whichcould be of independent interest.</p>
                <p>Last Updated: 2024-09-05 17:45:01 UTC</p>
                <button class="interpret-button" data-id="2409.03734v1">Interpret</button>
                <div id="interpretation-2409.03734v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Iterative thresholding for non-linear learning in the strong $\varepsilon$-contamination model</h3>
                <p>Authors: Arvind RathnashyamAlex Gittens</p>
                <p><a href="http://arxiv.org/abs/2409.03703v1">Link to paper</a></p>
                <p>We derive approximation bounds for learning single neuron models usingthresholded gradient descent when both the labels and the covariates arepossibly corrupted adversarially. We assume the data follows the model y sigmamathbfw cdot mathbfx  xi where sigma is a nonlinearactivation function the noise xi is Gaussian and the covariate vectormathbfx is sampled from a sub-Gaussian distribution. We study sigmoidalleaky-ReLU and ReLU activation functions and derive aOnusqrtepsilonlog1/epsilon approximation bound in ell_2-normwith sample complexity Od/epsilon and failure probabilitye-Omegad.  We also study the linear regression problem where sigmamathbfx mathbfx. We derive a Onuepsilonlog1/epsilon approximation boundimproving upon the previous Onu approximation bounds for thegradient-descent based iterative thresholding algorithms of Bhatia et al.NeurIPS 2015 and Shen and Sanghavi ICML 2019. Our algorithm has aOtextrmpolylogNdlogR/epsilon runtime complexity whenmathbfw_2 leq R improving upon theOtextpolylogNd/epsilon2 runtime complexity of Awasthi et al.NeurIPS 2022.</p>
                <p>Last Updated: 2024-09-05 16:59:56 UTC</p>
                <button class="interpret-button" data-id="2409.03703v1">Interpret</button>
                <div id="interpretation-2409.03703v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A method to benchmark high-dimensional process drift detection</h3>
                <p>Authors: Edgar WolfTobias Windisch</p>
                <p><a href="http://arxiv.org/abs/2409.03669v1">Link to paper</a></p>
                <p>Process curves are multi-variate finite time series data coming frommanufacturing processes. This paper studies machine learning methods for driftsof process curves. A theoretic framework to synthetically generate processcurves in a controlled way is introduced in order to benchmark machine learningalgorithms for process drift detection. A evaluation score called the temporalarea under the curve is introduced which allows to quantify how well machinelearning models unveil curves belonging to drift segments. Finally a benchmarkstudy comparing popular machine learning approaches on synthetic data generatedwith the introduced framework shown.</p>
                <p>Last Updated: 2024-09-05 16:23:07 UTC</p>
                <button class="interpret-button" data-id="2409.03669v1">Interpret</button>
                <div id="interpretation-2409.03669v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DART2: a robust multiple testing method to smartly leverage helpful or misleading ancillary information</h3>
                <p>Authors: Xuechan LiJichun Xie</p>
                <p><a href="http://arxiv.org/abs/2409.03618v1">Link to paper</a></p>
                <p>In many applications of multiple testing ancillary information is availablereflecting the hypothesis null or alternative status. Several methods have beendeveloped to leverage this ancillary information to enhance testing powertypically requiring the ancillary information is helpful enough to ensurefavorable performance. In this paper we develop a robust and effectivedistance-assisted multiple testing procedure named DART2 designed to bepowerful and robust regardless of the quality of ancillary information. Whenthe ancillary information is helpful DART2 can asymptotically control FDRwhile improving power otherwise DART2 can still control FDR and maintainpower at least as high as ignoring the ancillary information. We demonstratedDART2s superior performance compared to existing methods through numericalstudies under various settings. In addition DART2 has been applied to a geneassociation study where we have shown its superior accuracy and robustnessunder two different types of ancillary information.</p>
                <p>Last Updated: 2024-09-05 15:22:39 UTC</p>
                <button class="interpret-button" data-id="2409.03618v1">Interpret</button>
                <div id="interpretation-2409.03618v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding</h3>
                <p>Authors: Yunze ManShuhong ZhengZhipeng BaoMartial HebertLiang-Yan GuiYu-Xiong Wang</p>
                <p><a href="http://arxiv.org/abs/2409.03757v1">Link to paper</a></p>
                <p>Complex 3D scene understanding has gained increasing attention with sceneencoding strategies playing a crucial role in this success. However theoptimal scene encoding strategies for various scenarios remain unclearparticularly compared to their image-based counterparts. To address this issuewe present a comprehensive study that probes various visual encoding models for3D scene understanding identifying the strengths and limitations of each modelacross different scenarios. Our evaluation spans seven vision foundationencoders including image-based video-based and 3D foundation models. Weevaluate these models in four tasks: Vision-Language Scene Reasoning VisualGrounding Segmentation and Registration each focusing on different aspectsof scene understanding. Our evaluations yield key findings: DINOv2 demonstratessuperior performance video models excel in object-level tasks diffusionmodels benefit geometric tasks and language-pretrained models show unexpectedlimitations in language-related tasks. These insights challenge someconventional understandings provide novel perspectives on leveraging visualfoundation models and highlight the need for more flexible encoder selectionin future vision-language and scene-understanding tasks.</p>
                <p>Last Updated: 2024-09-05 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2409.03757v1">Interpret</button>
                <div id="interpretation-2409.03757v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild</h3>
                <p>Authors: Yuntian DengWenting ZhaoJack HesselXiang RenClaire CardieYejin Choi</p>
                <p><a href="http://arxiv.org/abs/2409.03753v1">Link to paper</a></p>
                <p>The increasing availability of real-world conversation data offers excitingopportunities for researchers to study user-chatbot interactions. However thesheer volume of this data makes manually examining individual conversationsimpractical. To overcome this challenge we introduce WildVis an interactivetool that enables fast versatile and large-scale conversation analysis.WildVis provides search and visualization capabilities in the text andembedding spaces based on a list of criteria. To manage million-scale datasetswe implemented optimizations including search index construction embeddingprecomputation and compression and caching to ensure responsive userinteractions within seconds. We demonstrate WildViss utility through threecase studies: facilitating chatbot misuse research visualizing and comparingtopic distributions across datasets and characterizing user-specificconversation patterns. WildVis is open-source and designed to be extendablesupporting additional datasets and customized search and visualizationfunctionalities.</p>
                <p>Last Updated: 2024-09-05 17:59:15 UTC</p>
                <button class="interpret-button" data-id="2409.03753v1">Interpret</button>
                <div id="interpretation-2409.03753v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLM-CI: Assessing Contextual Integrity Norms in Language Models</h3>
                <p>Authors: Yan ShvartzshnaiderVasisht DudduJohn Lacalamita</p>
                <p><a href="http://arxiv.org/abs/2409.03735v1">Link to paper</a></p>
                <p>Large language models LLMs while memorizing parts of their training datascraped from the Internet may also inadvertently encode societal preferencesand norms. As these models are integrated into sociotechnical systems it iscrucial that the norms they encode align with societal expectations. Thesenorms could vary across models hyperparameters optimization techniques anddatasets. This is especially challenging due to prompt sensitivity-smallvariations in prompts yield different responses rendering existing assessmentmethodologies unreliable. There is a need for a comprehensive frameworkcovering various models optimization and datasets along with a reliablemethodology to assess encoded norms.  We present LLM-CI the first open-sourced framework to assess privacy normsencoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignettemethodology to assess the encoded norms across different contexts and LLMs. Wepropose the multi-prompt assessment methodology to address prompt sensitivityby assessing the norms from only the prompts that yield consistent responsesacross multiple variants. Using LLM-CI and our proposed methodology wecomprehensively evaluate LLMs using IoT and COPPA vignettes datasets from priorwork examining the impact of model properties e.g. hyperparameterscapacity and optimization strategies e.g. alignment quantization.</p>
                <p>Last Updated: 2024-09-05 17:50:31 UTC</p>
                <button class="interpret-button" data-id="2409.03735v1">Interpret</button>
                <div id="interpretation-2409.03735v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Planning In Natural Language Improves LLM Search For Code Generation</h3>
                <p>Authors: Evan WangFederico CassanoCatherine WuYunfeng BaiWill SongVaskar NathZiwen HanSean HendryxSummer YueHugh Zhang</p>
                <p><a href="http://arxiv.org/abs/2409.03733v1">Link to paper</a></p>
                <p>While scaling training compute has led to remarkable improvements in largelanguage models LLMs scaling inference compute has not yet yielded analogousgains. We hypothesize that a core missing component is a lack of diverse LLMoutputs leading to inefficient search due to models repeatedly sampling highlysimilar yet incorrect generations. We empirically demonstrate that this lackof diversity can be mitigated by searching over candidate plans for solving aproblem in natural language. Based on this insight we propose PLANSEARCH anovel search algorithm which shows strong results across HumanEval MBPP andLiveCodeBench a contamination-free benchmark for competitive coding.PLANSEARCH generates a diverse set of observations about the problem and thenuses these observations to construct plans for solving the problem. Bysearching over plans in natural language rather than directly over codesolutions PLANSEARCH explores a significantly more diverse range of potentialsolutions compared to baseline search methods. Using PLANSEARCH on top ofClaude 3.5 Sonnet achieves a state-of-the-art pass200 of 77.0 onLiveCodeBench outperforming both the best score achieved without searchpass1  41.4 and using standard repeated sampling pass200  60.6.Finally we show that across all models search algorithms and benchmarksanalyzed we can accurately predict performance gains due to search as a directfunction of the diversity over generated ideas.</p>
                <p>Last Updated: 2024-09-05 17:44:49 UTC</p>
                <button class="interpret-button" data-id="2409.03733v1">Interpret</button>
                <div id="interpretation-2409.03733v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Different Level Text Protection Mechanism With Differential Privacy</h3>
                <p>Authors: Qingwen Fu</p>
                <p><a href="http://arxiv.org/abs/2409.03707v1">Link to paper</a></p>
                <p>The article introduces a method for extracting words of different degrees ofimportance based on the BERT pre-training model and proves the effectiveness ofthis method. The article also discusses the impact of maintaining the sameperturbation results for words of different importance on the overall textutility. This method can be applied to long text protection.</p>
                <p>Last Updated: 2024-09-05 17:13:38 UTC</p>
                <button class="interpret-button" data-id="2409.03707v1">Interpret</button>
                <div id="interpretation-2409.03707v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding</h3>
                <p>Authors: Yunze ManShuhong ZhengZhipeng BaoMartial HebertLiang-Yan GuiYu-Xiong Wang</p>
                <p><a href="http://arxiv.org/abs/2409.03757v1">Link to paper</a></p>
                <p>Complex 3D scene understanding has gained increasing attention with sceneencoding strategies playing a crucial role in this success. However theoptimal scene encoding strategies for various scenarios remain unclearparticularly compared to their image-based counterparts. To address this issuewe present a comprehensive study that probes various visual encoding models for3D scene understanding identifying the strengths and limitations of each modelacross different scenarios. Our evaluation spans seven vision foundationencoders including image-based video-based and 3D foundation models. Weevaluate these models in four tasks: Vision-Language Scene Reasoning VisualGrounding Segmentation and Registration each focusing on different aspectsof scene understanding. Our evaluations yield key findings: DINOv2 demonstratessuperior performance video models excel in object-level tasks diffusionmodels benefit geometric tasks and language-pretrained models show unexpectedlimitations in language-related tasks. These insights challenge someconventional understandings provide novel perspectives on leveraging visualfoundation models and highlight the need for more flexible encoder selectionin future vision-language and scene-understanding tasks.</p>
                <p>Last Updated: 2024-09-05 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2409.03757v1">Interpret</button>
                <div id="interpretation-2409.03757v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild</h3>
                <p>Authors: Yuntian DengWenting ZhaoJack HesselXiang RenClaire CardieYejin Choi</p>
                <p><a href="http://arxiv.org/abs/2409.03753v1">Link to paper</a></p>
                <p>The increasing availability of real-world conversation data offers excitingopportunities for researchers to study user-chatbot interactions. However thesheer volume of this data makes manually examining individual conversationsimpractical. To overcome this challenge we introduce WildVis an interactivetool that enables fast versatile and large-scale conversation analysis.WildVis provides search and visualization capabilities in the text andembedding spaces based on a list of criteria. To manage million-scale datasetswe implemented optimizations including search index construction embeddingprecomputation and compression and caching to ensure responsive userinteractions within seconds. We demonstrate WildViss utility through threecase studies: facilitating chatbot misuse research visualizing and comparingtopic distributions across datasets and characterizing user-specificconversation patterns. WildVis is open-source and designed to be extendablesupporting additional datasets and customized search and visualizationfunctionalities.</p>
                <p>Last Updated: 2024-09-05 17:59:15 UTC</p>
                <button class="interpret-button" data-id="2409.03753v1">Interpret</button>
                <div id="interpretation-2409.03753v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron</h3>
                <p>Authors: Christian SchmidJames M. Murray</p>
                <p><a href="http://arxiv.org/abs/2409.03749v1">Link to paper</a></p>
                <p>The ability of a brain or a neural network to efficiently learn dependscrucially on both the task structure and the learning rule. Previous works haveanalyzed the dynamical equations describing learning in the relativelysimplified context of the perceptron under assumptions of a student-teacherframework or a linearized output. While these assumptions have facilitatedtheoretical understanding they have precluded a detailed understanding of theroles of the nonlinearity and input-data distribution in determining thelearning dynamics limiting the applicability of the theories to realbiological or artificial neural networks. Here we use a stochastic-processapproach to derive flow equations describing learning applying this frameworkto the case of a nonlinear perceptron performing binary classification. Wecharacterize the effects of the learning rule supervised or reinforcementlearning SL/RL and input-data distribution on the perceptrons learning curveand the forgetting curve as subsequent tasks are learned. In particular wefind that the input-data noise differently affects the learning speed under SLvs. RL as well as determines how quickly learning of a task is overwritten bysubsequent learning. Additionally we verify our approach with real data usingthe MNIST dataset. This approach points a way toward analyzing learningdynamics for more-complex circuit architectures.</p>
                <p>Last Updated: 2024-09-05 17:58:28 UTC</p>
                <button class="interpret-button" data-id="2409.03749v1">Interpret</button>
                <div id="interpretation-2409.03749v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?</h3>
                <p>Authors: Rui WenMichael BackesYang Zhang</p>
                <p><a href="http://arxiv.org/abs/2409.03741v1">Link to paper</a></p>
                <p>Machine learning has revolutionized numerous domains playing a crucial rolein driving advancements and enabling data-centric processes. The significanceof data in training models and shaping their performance cannot be overstated.Recent research has highlighted the heterogeneous impact of individual datasamples particularly the presence of valuable data that significantlycontributes to the utility and effectiveness of machine learning models.However a critical question remains unanswered: are these valuable datasamples more vulnerable to machine learning attacks In this work weinvestigate the relationship between data importance and machine learningattacks by analyzing five distinct attack types. Our findings reveal notableinsights. For example we observe that high importance data samples exhibitincreased vulnerability in certain attacks such as membership inference andmodel stealing. By analyzing the linkage between membership inferencevulnerability and data importance we demonstrate that sample characteristicscan be integrated into membership metrics by introducing sample-specificcriteria therefore enhancing the membership inference performance. Thesefindings emphasize the urgent need for innovative defense mechanisms thatstrike a balance between maximizing utility and safeguarding valuable dataagainst potential exploitation.</p>
                <p>Last Updated: 2024-09-05 17:54:26 UTC</p>
                <button class="interpret-button" data-id="2409.03741v1">Interpret</button>
                <div id="interpretation-2409.03741v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Differentiable Discrete Event Simulation for Queuing Network Control</h3>
                <p>Authors: Ethan CheJing DongHongseok Namkoong</p>
                <p><a href="http://arxiv.org/abs/2409.03740v1">Link to paper</a></p>
                <p>Queuing network control is essential for managing congestion injob-processing systems such as service systems communication networks andmanufacturing processes. Despite growing interest in applying reinforcementlearning RL techniques queueing network control poses distinct challengesincluding high stochasticity large state and action spaces and lack ofstability. To tackle these challenges we propose a scalable framework forpolicy optimization based on differentiable discrete event simulation. Our maininsight is that by implementing a well-designed smoothing technique fordiscrete event dynamics we can compute pathwise policy gradients forlarge-scale queueing networks using auto-differentiation software e.g.Tensorflow PyTorch and GPU parallelization. Through extensive empiricalexperiments we observe that our policy gradient estimators are several ordersof magnitude more accurate than typical REINFORCE-based estimators. Inaddition We propose a new policy architecture which drastically improvesstability while maintaining the flexibility of neural-network policies. In awide variety of scheduling and admission control tasks we demonstrate thattraining control policies with pathwise gradients leads to a 50-1000ximprovement in sample efficiency over state-of-the-art RL methods. Unlike priortailored approaches to queueing our methods can flexibly handle realisticscenarios including systems operating in non-stationary environments and thosewith non-exponential interarrival/service times.</p>
                <p>Last Updated: 2024-09-05 17:53:54 UTC</p>
                <button class="interpret-button" data-id="2409.03740v1">Interpret</button>
                <div id="interpretation-2409.03740v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding</h3>
                <p>Authors: Yunze ManShuhong ZhengZhipeng BaoMartial HebertLiang-Yan GuiYu-Xiong Wang</p>
                <p><a href="http://arxiv.org/abs/2409.03757v1">Link to paper</a></p>
                <p>Complex 3D scene understanding has gained increasing attention with sceneencoding strategies playing a crucial role in this success. However theoptimal scene encoding strategies for various scenarios remain unclearparticularly compared to their image-based counterparts. To address this issuewe present a comprehensive study that probes various visual encoding models for3D scene understanding identifying the strengths and limitations of each modelacross different scenarios. Our evaluation spans seven vision foundationencoders including image-based video-based and 3D foundation models. Weevaluate these models in four tasks: Vision-Language Scene Reasoning VisualGrounding Segmentation and Registration each focusing on different aspectsof scene understanding. Our evaluations yield key findings: DINOv2 demonstratessuperior performance video models excel in object-level tasks diffusionmodels benefit geometric tasks and language-pretrained models show unexpectedlimitations in language-related tasks. These insights challenge someconventional understandings provide novel perspectives on leveraging visualfoundation models and highlight the need for more flexible encoder selectionin future vision-language and scene-understanding tasks.</p>
                <p>Last Updated: 2024-09-05 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2409.03757v1">Interpret</button>
                <div id="interpretation-2409.03757v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DC-Solver: Improving Predictor-Corrector Diffusion Sampler via Dynamic Compensation</h3>
                <p>Authors: Wenliang ZhaoHaolin WangJie ZhouJiwen Lu</p>
                <p><a href="http://arxiv.org/abs/2409.03755v1">Link to paper</a></p>
                <p>Diffusion probabilistic models DPMs have shown remarkable performance invisual synthesis but are computationally expensive due to the need for multipleevaluations during the sampling. Recent predictor-corrector diffusion samplershave significantly reduced the required number of function evaluations NFEbut inherently suffer from a misalignment issue caused by the extra correctorstep especially with a large classifier-free guidance scale CFG. In thispaper we introduce a new fast DPM sampler called DC-Solver which leveragesdynamic compensation DC to mitigate the misalignment of thepredictor-corrector samplers. The dynamic compensation is controlled bycompensation ratios that are adaptive to the sampling steps and can beoptimized on only 10 datapoints by pushing the sampling trajectory toward aground truth trajectory. We further propose a cascade polynomial regressionCPR which can instantly predict the compensation ratios on unseen samplingconfigurations. Additionally we find that the proposed dynamic compensationcan also serve as a plug-and-play module to boost the performance ofpredictor-only samplers. Extensive experiments on both unconditional samplingand conditional sampling demonstrate that our DC-Solver can consistentlyimprove the sampling quality over previous methods on different DPMs with awide range of resolutions up to 1024times1024. Notably we achieve 10.38 FIDNFE5 on unconditional FFHQ and 0.394 MSE NFE5 CFG7.5 onStable-Diffusion-2.1. Code is available at https://github.com/wl-zhao/DC-Solver</p>
                <p>Last Updated: 2024-09-05 17:59:46 UTC</p>
                <button class="interpret-button" data-id="2409.03755v1">Interpret</button>
                <div id="interpretation-2409.03755v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Foundation Model or Finetune? Evaluation of few-shot semantic segmentation for river pollution</h3>
                <p>Authors: Marga DonStijn PinsonBlanca Guillen CebrianYuki M. Asano</p>
                <p><a href="http://arxiv.org/abs/2409.03754v1">Link to paper</a></p>
                <p>Foundation models FMs are a popular topic of research in AI. Their abilityto generalize to new tasks and datasets without retraining or needing anabundance of data makes them an appealing candidate for applications onspecialist datasets. In this work we compare the performance of FMs tofinetuned pre-trained supervised models in the task of semantic segmentation onan entirely new dataset. We see that finetuned models consistently outperformthe FMs tested even in cases were data is scarce. We release the code anddataset for this work on GitHub.</p>
                <p>Last Updated: 2024-09-05 17:59:32 UTC</p>
                <button class="interpret-button" data-id="2409.03754v1">Interpret</button>
                <div id="interpretation-2409.03754v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ArtiFade: Learning to Generate High-quality Subject from Blemished Images</h3>
                <p>Authors: Shuya YangShaozhe HaoYukang CaoKwan-Yee K. Wong</p>
                <p><a href="http://arxiv.org/abs/2409.03745v1">Link to paper</a></p>
                <p>Subject-driven text-to-image generation has witnessed remarkable advancementsin its ability to learn and capture characteristics of a subject using only alimited number of images. However existing methods commonly rely onhigh-quality images for training and may struggle to generate reasonable imageswhen the input images are blemished by artifacts. This is primarily attributedto the inadequate capability of current techniques in distinguishingsubject-related features from disruptive artifacts. In this paper we introduceArtiFade to tackle this issue and successfully generate high-qualityartifact-free images from blemished datasets. Specifically ArtiFade exploitsfine-tuning of a pre-trained text-to-image model aiming to remove artifacts.The elimination of artifacts is achieved by utilizing a specialized datasetthat encompasses both unblemished images and their corresponding blemishedcounterparts during fine-tuning. ArtiFade also ensures the preservation of theoriginal generative capabilities inherent within the diffusion model therebyenhancing the overall performance of subject-driven methods in generatinghigh-quality and artifact-free images. We further devise evaluation benchmarkstailored for this task. Through extensive qualitative and quantitativeexperiments we demonstrate the generalizability of ArtiFade in effectiveartifact removal under both in-distribution and out-of-distribution scenarios.</p>
                <p>Last Updated: 2024-09-05 17:57:59 UTC</p>
                <button class="interpret-button" data-id="2409.03745v1">Interpret</button>
                <div id="interpretation-2409.03745v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with Image-Based Surface Representation</h3>
                <p>Authors: Slava ElizarovCiara RowlesSimon Donné</p>
                <p><a href="http://arxiv.org/abs/2409.03718v1">Link to paper</a></p>
                <p>Generating high-quality 3D objects from textual descriptions remains achallenging problem due to computational cost the scarcity of 3D data andcomplex 3D representations. We introduce Geometry Image DiffusionGIMDiffusion a novel Text-to-3D model that utilizes geometry images toefficiently represent 3D shapes using 2D images thereby avoiding the need forcomplex 3D-aware architectures. By integrating a Collaborative Controlmechanism we exploit the rich 2D priors of existing Text-to-Image models suchas Stable Diffusion. This enables strong generalization even with limited 3Dtraining data allowing us to use only high-quality training data as well asretaining compatibility with guidance techniques such as IPAdapter. In shortGIMDiffusion enables the generation of 3D assets at speeds comparable tocurrent Text-to-Image models. The generated objects consist of semanticallymeaningful separate parts and include internal structures enhancing bothusability and versatility.</p>
                <p>Last Updated: 2024-09-05 17:21:54 UTC</p>
                <button class="interpret-button" data-id="2409.03718v1">Interpret</button>
                <div id="interpretation-2409.03718v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild</h3>
                <p>Authors: Yuntian DengWenting ZhaoJack HesselXiang RenClaire CardieYejin Choi</p>
                <p><a href="http://arxiv.org/abs/2409.03753v1">Link to paper</a></p>
                <p>The increasing availability of real-world conversation data offers excitingopportunities for researchers to study user-chatbot interactions. However thesheer volume of this data makes manually examining individual conversationsimpractical. To overcome this challenge we introduce WildVis an interactivetool that enables fast versatile and large-scale conversation analysis.WildVis provides search and visualization capabilities in the text andembedding spaces based on a list of criteria. To manage million-scale datasetswe implemented optimizations including search index construction embeddingprecomputation and compression and caching to ensure responsive userinteractions within seconds. We demonstrate WildViss utility through threecase studies: facilitating chatbot misuse research visualizing and comparingtopic distributions across datasets and characterizing user-specificconversation patterns. WildVis is open-source and designed to be extendablesupporting additional datasets and customized search and visualizationfunctionalities.</p>
                <p>Last Updated: 2024-09-05 17:59:15 UTC</p>
                <button class="interpret-button" data-id="2409.03753v1">Interpret</button>
                <div id="interpretation-2409.03753v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Limited but consistent gains in adversarial robustness by co-training object recognition models with human EEG</h3>
                <p>Authors: Manshan GuoBhavin ChoksiSari SadiyaAlessandro T. GiffordMartina G. VilasRadoslaw M. CichyGemma Roig</p>
                <p><a href="http://arxiv.org/abs/2409.03646v1">Link to paper</a></p>
                <p>In contrast to human vision artificial neural networks ANNs remainrelatively susceptible to adversarial attacks. To address this vulnerabilityefforts have been made to transfer inductive bias from human brains to ANNsoften by training the ANN representations to match their biologicalcounterparts. Previous works relied on brain data acquired in rodents orprimates using invasive techniques from specific regions of the brain undernon-natural conditions anesthetized animals and with stimulus datasetslacking diversity and naturalness. In this work we explored whether aligningmodel representations to human EEG responses to a rich set of real-world imagesincreases robustness to ANNs. Specifically we trained ResNet50-backbone modelson a dual task of classification and EEG prediction and evaluated their EEGprediction accuracy and robustness to adversarial attacks. We observedsignificant correlation between the networks EEG prediction accuracy oftenhighest around 100 ms post stimulus onset and their gains in adversarialrobustness. Although effect size was limited effects were consistent acrossdifferent random initializations and robust for architectural variants. Wefurther teased apart the data from individual EEG channels and observedstrongest contribution from electrodes in the parieto-occipital regions. Thedemonstrated utility of human EEG for such tasks opens up avenues for futureefforts that scale to larger datasets under diverse stimuli conditions with thepromise of stronger effects.</p>
                <p>Last Updated: 2024-09-05 16:04:57 UTC</p>
                <button class="interpret-button" data-id="2409.03646v1">Interpret</button>
                <div id="interpretation-2409.03646v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Reimagining Data Visualization to Address Sustainability Goals</h3>
                <p>Authors: Narges Mahyar</p>
                <p><a href="http://arxiv.org/abs/2409.03611v1">Link to paper</a></p>
                <p>Information visualization holds significant potential to supportsustainability goals such as environmental stewardship and climate resilienceby transforming complex data into accessible visual formats that enhance publicunderstanding of complex climate change data and drive actionable insights.While the field has predominantly focused on analytical orientation ofvisualization challenging traditional visualization techniques and goalsthrough critical visualization research expands existing assumptions andconventions in the field. In this paper I explore how reimagining overlookedaspects of data visualization such as engagement emotional resonancecommunication and community empowerment can contribute to achievingsustainability objectives. I argue that by focusing on inclusive datavisualization that promotes clarity understandability and publicparticipation we can make complex data more relatable and actionablefostering broader connections and mobilizing collective action on criticalissues like climate change. Moreover I discuss the role of emotionalreceptivity in environmental data communication stressing the need forvisualizations that respect diverse cultural perspectives and emotionalresponses to achieve impactful outcomes. Drawing on insights from a decade ofresearch in public participation and community engagement I aim to highlighthow data visualization can democratize data access and increase publicinvolvement in order to contribute to a more sustainable and resilient future.</p>
                <p>Last Updated: 2024-09-05 15:16:37 UTC</p>
                <button class="interpret-button" data-id="2409.03611v1">Interpret</button>
                <div id="interpretation-2409.03611v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation</h3>
                <p>Authors: Prerak ModyNicolas F. Chaves-de-PlazaChinmay RaoEleftheria AstrenidouMischa de RidderNienke HoekstraKlaus HildebrandtMarius Staring</p>
                <p><a href="http://dx.doi.org/10.59275/j.melba.2024-5gc8">Link to paper</a></p>
                <p>Increased usage of automated tools like deep learning in medical imagesegmentation has alleviated the bottleneck of manual contouring. This hasshifted manual labour to quality assessment QA of automated contours whichinvolves detecting errors and correcting them. A potential solution tosemi-automated QA is to use deep Bayesian uncertainty to recommend potentiallyerroneous regions thus reducing time spent on error detection. Previous workhas investigated the correspondence between uncertainty and error however nowork has been done on improving the utility of Bayesian uncertainty maps suchthat it is only present in inaccurate regions and not in the accurate ones. Ourwork trains the FlipOut model with the Accuracy-vs-Uncertainty AvU loss whichpromotes uncertainty to be present only in inaccurate regions. We apply thismethod on datasets of two radiotherapy body sites c.f. head-and-neck CT andprostate MR scans. Uncertainty heatmaps i.e. predictive entropy are evaluatedagainst voxel inaccuracies using Receiver Operating Characteristic ROC andPrecision-Recall PR curves. Numerical results show that when compared to theBayesian baseline the proposed method successfully suppresses uncertainty foraccurate voxels with similar presence of uncertainty for inaccurate voxels.Code to reproduce experiments is available athttps://github.com/prerakmody/bayesuncertainty-error-correspondence</p>
                <p>Last Updated: 2024-09-05 12:31:51 UTC</p>
                <button class="interpret-button" data-id="2409.03470v1">Interpret</button>
                <div id="interpretation-2409.03470v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation</h3>
                <p>Authors: Yu WangShiwan ZhaoZhihu WangHeyuan HuangMing FanYubo ZhangZhixing WangHaijun WangTing Liu</p>
                <p><a href="http://arxiv.org/abs/2409.03271v1">Link to paper</a></p>
                <p>The Chain-of-Thought CoT paradigm has emerged as a critical approach forenhancing the reasoning capabilities of large language models LLMs. Howeverdespite their widespread adoption and success CoT methods often exhibitinstability due to their inability to consistently ensure the quality ofgenerated reasoning paths leading to sub-optimal reasoning performance. Toaddress this challenge we propose the textbfStrategic Chain-of-ThoughtSCoT a novel methodology designed to refine LLM performance by integratingstrategic knowledge prior to generating intermediate reasoning steps. SCoTemploys a two-stage approach within a single prompt: first eliciting aneffective problem-solving strategy which is then used to guide the generationof high-quality CoT paths and final answers. Our experiments across eightchallenging reasoning datasets demonstrate significant improvements includinga 21.05 increase on the GSM8K dataset and 24.13 on the Tracking_Objectsdataset respectively using the Llama3-8b model. Additionally we extend theSCoT framework to develop a few-shot method with automatically matcheddemonstrations yielding even stronger results. These findings underscore theefficacy of SCoT highlighting its potential to substantially enhance LLMperformance in complex reasoning tasks.</p>
                <p>Last Updated: 2024-09-05 06:28:05 UTC</p>
                <button class="interpret-button" data-id="2409.03271v1">Interpret</button>
                <div id="interpretation-2409.03271v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Non-stationary and Sparsely-correlated Multi-output Gaussian Process with Spike-and-Slab Prior</h3>
                <p>Authors: Wang XinmingLi YongxiangYue XiaoweiWu Jianguo</p>
                <p><a href="http://arxiv.org/abs/2409.03149v1">Link to paper</a></p>
                <p>Multi-output Gaussian process MGP is commonly used as a transfer learningmethod to leverage information among multiple outputs. A key advantage of MGPis providing uncertainty quantification for prediction which is highlyimportant for subsequent decision-making tasks. However traditional MGP maynot be sufficiently flexible to handle multivariate data with dynamiccharacteristics particularly when dealing with complex temporal correlations.Additionally since some outputs may lack correlation transferring informationamong them may lead to negative transfer. To address these issues this studyproposes a non-stationary MGP model that can capture both the dynamic andsparse correlation among outputs. Specifically the covariance functions of MGPare constructed using convolutions of time-varying kernel functions. Then adynamic spike-and-slab prior is placed on correlation parameters toautomatically decide which sources are informative to the target output in thetraining process. An expectation-maximization EM algorithm is proposed forefficient model fitting. Both numerical studies and a real case demonstrate itsefficacy in capturing dynamic and sparse correlation structure and mitigatingnegative transfer for high-dimensional time-series data. Finally amountain-car reinforcement learning case highlights its potential applicationin decision making problems.</p>
                <p>Last Updated: 2024-09-05 00:56:25 UTC</p>
                <button class="interpret-button" data-id="2409.03149v1">Interpret</button>
                <div id="interpretation-2409.03149v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>An Introduction to Centralized Training for Decentralized Execution in Cooperative Multi-Agent Reinforcement Learning</h3>
                <p>Authors: Christopher Amato</p>
                <p><a href="http://arxiv.org/abs/2409.03052v1">Link to paper</a></p>
                <p>Multi-agent reinforcement learning MARL has exploded in popularity inrecent years. Many approaches have been developed but they can be divided intothree main types: centralized training and execution CTE centralizedtraining for decentralized execution CTDE and Decentralized training andexecution DTE.  CTDE methods are the most common as they can use centralized informationduring training but execute in a decentralized manner -- using only informationavailable to that agent during execution. CTDE is the only paradigm thatrequires a separate training phase where any available information e.g. otheragent policies underlying states can be used. As a result they can be morescalable than CTE methods do not require communication during execution andcan often perform well. CTDE fits most naturally with the cooperative case butcan be potentially applied in competitive or mixed settings depending on whatinformation is assumed to be observed.  This text is an introduction to CTDE in cooperative MARL. It is meant toexplain the setting basic concepts and common methods. It does not cover allwork in CTDE MARL as the subarea is quite extensive. I have included work thatI believe is important for understanding the main concepts in the subarea andapologize to those that I have omitted.</p>
                <p>Last Updated: 2024-09-04 19:54:40 UTC</p>
                <button class="interpret-button" data-id="2409.03052v1">Interpret</button>
                <div id="interpretation-2409.03052v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CONClave -- Secure and Robust Cooperative Perception for CAVs Using Authenticated Consensus and Trust Scoring</h3>
                <p>Authors: Edward AndertFrancis MendozaHans Walter BehrensAviral Shrivastava</p>
                <p><a href="http://dx.doi.org/10.1145/3649329.3658491">Link to paper</a></p>
                <p>Connected Autonomous Vehicles have great potential to improve automobilesafety and traffic flow especially in cooperative applications whereperception data is shared between vehicles. However this cooperation must besecured from malicious intent and unintentional errors that could causeaccidents. Previous works typically address singular security or reliabilityissues for cooperative driving in specific scenarios rather than the set oferrors together. In this paper we propose CONClave a tightly coupledauthentication consensus and trust scoring mechanism that providescomprehensive security and reliability for cooperative perception in autonomousvehicles. CONClave benefits from the pipelined nature of the steps such thatfaults can be detected significantly faster and with less compute. OverallCONClave shows huge promise in preventing security flaws detecting evenrelatively minor sensing faults and increasing the robustness and accuracy ofcooperative perception in CAVs while adding minimal overhead.</p>
                <p>Last Updated: 2024-09-04 16:42:40 UTC</p>
                <button class="interpret-button" data-id="2409.02863v1">Interpret</button>
                <div id="interpretation-2409.02863v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Survey on Emergent Language</h3>
                <p>Authors: Jannik PetersConstantin Waubert de PuiseauHasan TercanArya GopikrishnanGustavo Adolpho Lucas De CarvalhoChristian BitterTobias Meisen</p>
                <p><a href="http://arxiv.org/abs/2409.02645v1">Link to paper</a></p>
                <p>The field of emergent language represents a novel area of research within thedomain of artificial intelligence particularly within the context ofmulti-agent reinforcement learning. Although the concept of studying languageemergence is not new early approaches were primarily concerned with explaininghuman language formation with little consideration given to its potentialutility for artificial agents. In contrast studies based on reinforcementlearning aim to develop communicative capabilities in agents that arecomparable to or even superior to human language. Thus they extend beyond thelearned statistical representations that are common in natural languageprocessing research. This gives rise to a number of fundamental questions fromthe prerequisites for language emergence to the criteria for measuring itssuccess. This paper addresses these questions by providing a comprehensivereview of 181 scientific publications on emergent language in artificialintelligence. Its objective is to serve as a reference for researchersinterested in or proficient in the field. Consequently the main contributionsare the definition and overview of the prevailing terminology the analysis ofexisting evaluation methods and metrics and the description of the identifiedresearch gaps.</p>
                <p>Last Updated: 2024-09-04 12:22:05 UTC</p>
                <button class="interpret-button" data-id="2409.02645v1">Interpret</button>
                <div id="interpretation-2409.02645v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Context-Aware Agent-based Model for Smart Long Distance Transport System</h3>
                <p>Authors: Muhammad RaeesAfzal Ahmed</p>
                <p><a href="http://arxiv.org/abs/2409.02434v1">Link to paper</a></p>
                <p>Long-distance transport plays a vital role in the economic growth ofcountries. However there is a lack of systems being developed for monitoringand support of long-route vehicles LRV. Sustainable and context-awaretransport systems with modern technologies are needed. We model forlong-distance vehicle transportation monitoring and support systems in amulti-agent environment. Our model incorporates the distance vehicle transportmechanism through agent-based modeling ABM. This model constitutes the designprotocol of ABM called Overview Design and Details ODD. This modelconstitutes that every category of agents is offering information as a service.Hence a federation of services through protocol for the communication betweensensors and software components is desired. Such integration of servicessupports monitoring and tracking of vehicles on the route. The modelsimulations provide useful results for the integration of services based onsmart objects.</p>
                <p>Last Updated: 2024-09-04 04:25:10 UTC</p>
                <button class="interpret-button" data-id="2409.02434v1">Interpret</button>
                <div id="interpretation-2409.02434v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-09-07</p>
        </div>
    
        </div>
    </body>
    </html>
    