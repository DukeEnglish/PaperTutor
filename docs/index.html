
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>The Double-Edged Sword of Behavioral Responses in Strategic Classification: Theory and User Studies</h3>
                <p>Authors: Raman EbrahimiKristen VaccaroParinaz Naghizadeh</p>
                <p><a href="http://arxiv.org/abs/2410.18066v1">Link to paper</a></p>
                <p>When humans are subject to an algorithmic decision system they canstrategically adjust their behavior accordingly game the system. While agrowing line of literature on strategic classification has used game-theoreticmodeling to understand and mitigate such gaming these existing works considerstandard models of fully rational agents. In this paper we propose a strategicclassification model that considers behavioral biases in human responses toalgorithms. We show how misperceptions of a classifier specifically of itsfeature weights can lead to different types of discrepancies between biasedand rational agents responses and identify when behavioral agents over- orunder-invest in different features. We also show that strategic agents withbehavioral biases can benefit or perhaps unexpectedly harm the firm comparedto fully rational strategic agents. We complement our analytical results withuser studies which support our hypothesis of behavioral biases in humanresponses to the algorithm. Together our findings highlight the need toaccount for human cognitive biases when designing AI systems and providingexplanations of them to strategic human in the loop.</p>
                <p>Last Updated: 2024-10-23 17:42:54 UTC</p>
                <button class="interpret-button" data-id="2410.18066v1">Interpret</button>
                <div id="interpretation-2410.18066v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Comparative Assessment of Technology Acceptance and Learning Outcomes in Computer-based versus VR-based Pedagogical Agents</h3>
                <p>Authors: Aimilios HadjiliasiLouis NisiotisIrene Polycarpou</p>
                <p><a href="http://arxiv.org/abs/2410.18048v1">Link to paper</a></p>
                <p>As educational technology evolves the potential of Pedagogical Agents PAsin supporting education is extensively explored. Typically research on PAs hasprimarily focused on computer-based learning environments but their use inVR-based environments and integration into education is still in its infancy.To address this gap this paper presents a mixed method comparative study thathas been conducted to evaluate and examine how these computer-based PAs andVR-based PAs compare towards their learning efficacy and technologyacceptance. 92 Computing and Engineering undergraduate students were recruitedand participated in an educational experience focusing on computing machineryeducation. The findings of this study revealed that both approaches caneffectively facilitate learning acquisition and both technologies have beenpositively perceived by participants toward acceptance without any significantdifferences. The findings of this study shed light on the potential ofutilizing intelligent PAs to support education contributing towards theadvancement of our understanding of how to integrate such technologies todevelop learning interventions and establishing the foundation for futureinvestigations that aim to successfully integrate and use PAs in education.</p>
                <p>Last Updated: 2024-10-23 17:20:35 UTC</p>
                <button class="interpret-button" data-id="2410.18048v1">Interpret</button>
                <div id="interpretation-2410.18048v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AI as a Bridge Across Ages: Exploring The Opportunities of Artificial Intelligence in Supporting Inter-Generational Communication in Virtual Reality</h3>
                <p>Authors: Qiuxin DuXiaoying WeiJiawei LiEmily KuangJie HaoDongdong WengMingming Fan</p>
                <p><a href="http://arxiv.org/abs/2410.17909v1">Link to paper</a></p>
                <p>Inter-generational communication is essential for bridging generational gapsand fostering mutual understanding. However maintaining it is complex due tocultural communicative and geographical differences. Recent researchindicated that while Virtual Reality VR creates a relaxed atmosphere andpromotes companionship it inadequately addresses the complexities ofinter-generational dialogue including variations in values and relationaldynamics. To address this gap we explored the opportunities of ArtificialIntelligence AI in supporting inter-generational communication in VR. Wedeveloped three technology probes e.g. Content Generator CommunicationFacilitator and Info Assistant in VR and employed them in a probe-basedparticipatory design study with twelve inter-generational pairs. Our resultsshow that AI-powered VR facilitates inter-generational communication byenhancing mutual understanding fostering conversation fluency and promotingactive participation. We also introduce several challenges when usingAI-powered VR in supporting inter-generational communication and derive designimplications for future VR platforms aiming to improve inter-generationalcommunication.</p>
                <p>Last Updated: 2024-10-23 14:28:16 UTC</p>
                <button class="interpret-button" data-id="2410.17909v1">Interpret</button>
                <div id="interpretation-2410.17909v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination</h3>
                <p>Authors: Salman RakinMd. A. R. ShiblyZahin M. HossainZeeshan KhanMd. Mostofa Akbar</p>
                <p><a href="http://arxiv.org/abs/2410.17783v1">Link to paper</a></p>
                <p>While ongoing advancements in Large Language Models have demonstratedremarkable success across various NLP tasks Retrieval Augmented GenerationModel stands out to be highly effective on downstream applications likeQuestion Answering. Recently RAG-end2end model further optimized thearchitecture and achieved notable performance improvements on domainadaptation. However the effectiveness of these RAG-based architectures remainsrelatively unexplored when fine-tuned on specialized domains such as customerservice for building a reliable conversational AI system. Furthermore acritical challenge persists in reducing the occurrence of hallucinations whilemaintaining high domain-specific accuracy. In this paper we investigated theperformance of diverse RAG and RAG-like architectures through domain adaptationand evaluated their ability to generate accurate and relevant response groundedin the contextual knowledge base. To facilitate the evaluation of the modelswe constructed a novel dataset HotelConvQA sourced from wide range ofhotel-related conversations and fine-tuned all the models on our domainspecific dataset. We also addressed a critical research gap on determining theimpact of domain adaptation on reducing hallucinations across different RAGarchitectures an aspect that was not properly measured in prior work. Ourevaluation shows positive results in all metrics by employing domainadaptation demonstrating strong performance on QA tasks and providing insightsinto their efficacy in reducing hallucinations. Our findings clearly indicatethat domain adaptation not only enhances the models performance on QA tasksbut also significantly reduces hallucination across all evaluated RAGarchitectures.</p>
                <p>Last Updated: 2024-10-23 11:32:46 UTC</p>
                <button class="interpret-button" data-id="2410.17783v1">Interpret</button>
                <div id="interpretation-2410.17783v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Efficient and Aesthetic UI Design with a Deep Learning-Based Interface Generation Tree Algorithm</h3>
                <p>Authors: Shiyu DuanRunsheng ZhangMengmeng ChenZiyi WangShixiao Wang</p>
                <p><a href="http://arxiv.org/abs/2410.17586v1">Link to paper</a></p>
                <p>This paper presents a novel method for user interface UI generation basedon the Transformer architecture addressing the increasing demand for efficientand aesthetically pleasing UI designs in software development. Traditional UIdesign relies heavily on designers expertise which can be time-consuming andcostly. Leveraging the capabilities of Transformers particularly their abilityto capture complex design patterns and long-range dependencies we propose aTransformer-based interface generation tree algorithm. This method constructs ahierarchical representation of UI components as nodes in a tree structureutilizing pre-trained Transformer models for encoding and decoding. We define amarkup language to describe UI components and their properties and use a richdataset of real-world web and mobile application interfaces for training. Theexperimental results demonstrate that our approach not only significantlyenhances design quality and efficiency but also outperforms traditional modelsin user satisfaction and aesthetic appeal. We also provide a comparativeanalysis with existing models illustrating the advantages of our method interms of accuracy user ratings and design similarity. Overall our studyunderscores the potential of the Transformer based approach to revolutionizethe UI design process making it accessible for non-professionals whilemaintaining high standards of quality.</p>
                <p>Last Updated: 2024-10-23 06:20:37 UTC</p>
                <button class="interpret-button" data-id="2410.17586v1">Interpret</button>
                <div id="interpretation-2410.17586v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration</h3>
                <p>Authors: Xin LiQizhi ChuYubin ChenYang LiuYaoqi LiuZekai YuWeize ChenChen QianChuan ShiCheng Yang</p>
                <p><a href="http://arxiv.org/abs/2410.18032v1">Link to paper</a></p>
                <p>Graphs are widely used for modeling relational data in real-world scenariossuch as social networks and urban computing. Existing LLM-based graph analysisapproaches either integrate graph neural networks GNNs for specific machinelearning tasks limiting their transferability or rely solely on LLMsinternal reasoning ability resulting in suboptimal performance. To addressthese limitations we take advantage of recent advances in LLM-based agentswhich have shown capabilities of utilizing external knowledge or tools forproblem solving. By simulating human problem-solving strategies such as analogyand collaboration we propose a multi-agent system based on LLMs namedGraphTeam for graph analysis. GraphTeam consists of five LLM-based agents fromthree modules and the agents with different specialities can collaborate witheach other to address complex problems. Specifically 1 input-outputnormalization module: the question agent extracts and refines four keyarguments from the original question facilitating the problem understandingand the answer agent organizes the results to meet the output requirement 2external knowledge retrieval module: we first build a knowledge base consistingof relevant documentation and experience information and then the search agentretrieves the most relevant entries for each question. 3 problem-solvingmodule: given the retrieved information from search agent the coding agentuses established algorithms via programming to generate solutions and in casethe coding agent does not work the reasoning agent will directly compute theresults without programming. Extensive experiments on six graph analysisbenchmarks demonstrate that GraphTeam achieves state-of-the-art performancewith an average 25.85 improvement over the best baseline in terms of accuracy.The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.</p>
                <p>Last Updated: 2024-10-23 17:02:59 UTC</p>
                <button class="interpret-button" data-id="2410.18032v1">Interpret</button>
                <div id="interpretation-2410.18032v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Scalable Offline Reinforcement Learning for Mean Field Games</h3>
                <p>Authors: Axel BrunnbauerJulian LemmelZahra BabaieeSophie NeubauerRadu Grosu</p>
                <p><a href="http://arxiv.org/abs/2410.17898v1">Link to paper</a></p>
                <p>Reinforcement learning algorithms for mean-field games offer a scalableframework for optimizing policies in large populations of interacting agents.Existing methods often depend on online interactions or access to systemdynamics limiting their practicality in real-world scenarios where suchinteractions are infeasible or difficult to model. In this paper we presentOffline Munchausen Mirror Descent Off-MMD a novel mean-field RL algorithmthat approximates equilibrium policies in mean-field games using purely offlinedata. By leveraging iterative mirror descent and importance samplingtechniques Off-MMD estimates the mean-field distribution from static datasetswithout relying on simulation or environment dynamics. Additionally weincorporate techniques from offline reinforcement learning to address commonissues like Q-value overestimation ensuring robust policy learning even withlimited data coverage. Our algorithm scales to complex environments anddemonstrates strong performance on benchmark tasks like crowd exploration ornavigation highlighting its applicability to real-world multi-agent systemswhere online experimentation is infeasible. We empirically demonstrate therobustness of Off-MMD to low-quality datasets and conduct experiments toinvestigate its sensitivity to hyperparameter choices.</p>
                <p>Last Updated: 2024-10-23 14:16:34 UTC</p>
                <button class="interpret-button" data-id="2410.17898v1">Interpret</button>
                <div id="interpretation-2410.17898v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TranSPORTmer: A Holistic Approach to Trajectory Understanding in Multi-Agent Sports</h3>
                <p>Authors: Guillem CapelleraLuis FerrazAntonio RubioAntonio AgudoFrancesc Moreno-Noguer</p>
                <p><a href="http://arxiv.org/abs/2410.17785v1">Link to paper</a></p>
                <p>Understanding trajectories in multi-agent scenarios requires addressingvarious tasks including predicting future movements imputing missingobservations inferring the status of unseen agents and classifying differentglobal states. Traditional data-driven approaches often handle these tasksseparately with specialized models. We introduce TranSPORTmer a unifiedtransformer-based framework capable of addressing all these tasks showcasingits application to the intricate dynamics of multi-agent sports scenarios likesoccer and basketball. Using Set Attention Blocks TranSPORTmer effectivelycaptures temporal dynamics and social interactions in an equivariant manner.The models tasks are guided by an input mask that conceals missing oryet-to-be-predicted observations. Additionally we introduce a CLS extra agentto classify states along soccer trajectories including passes possessionsuncontrolled states and out-of-play intervals contributing to an enhancementin modeling trajectories. Evaluations on soccer and basketball datasets showthat TranSPORTmer outperforms state-of-the-art task-specific models in playerforecasting player forecasting-imputation ball inference and ballimputation. https://youtu.be/8VtSRm8oGoE</p>
                <p>Last Updated: 2024-10-23 11:35:44 UTC</p>
                <button class="interpret-button" data-id="2410.17785v1">Interpret</button>
                <div id="interpretation-2410.17785v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Markov Potential Game with Final-time Reach-Avoid Objectives</h3>
                <p>Authors: Sarah H. Q. LiAbraham P. Vinod</p>
                <p><a href="http://arxiv.org/abs/2410.17690v1">Link to paper</a></p>
                <p>We formulate a Markov potential game with final-time reach-avoid objectivesby integrating potential game theory with stochastic reach-avoid control. Ourfocus is on multi-player trajectory planning where players maximize the samemulti-player reach-avoid objective: the probability of all participantsreaching their designated target states by a specified time while avoidingcollisions with one another. Existing approaches require centralizedcomputation of actions via a global policy which may have prohibitivelyexpensive communication costs. Instead we focus on approximations of theglobal policy via local state feedback policies. First we adapt the recursivesingle player reach-avoid value iteration to the multi-player framework withlocal policies and show that the same recursion holds on the joint statespace. To find each players optimal local policy the multi-player reach-avoidvalue function is projected from the joint state to the local state using theother players occupancy measures. Then we propose an iterative best responsescheme for the multi-player value iteration to converge to a pure Nashequilibrium. We demonstrate the utility of our approach in findingcollision-free policies for multi-player motion planning in simulation.</p>
                <p>Last Updated: 2024-10-23 09:13:02 UTC</p>
                <button class="interpret-button" data-id="2410.17690v1">Interpret</button>
                <div id="interpretation-2410.17690v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Bridging Swarm Intelligence and Reinforcement Learning</h3>
                <p>Authors: Karthik SomaYann BouteillerHeiko HamannGiovanni Beltrame</p>
                <p><a href="http://arxiv.org/abs/2410.17517v1">Link to paper</a></p>
                <p>Swarm intelligence SI explores how large groups of simple individualse.g. insects fish birds collaborate to produce complex behaviorsexemplifying that the whole is greater than the sum of its parts. A fundamentaltask in SI is Collective Decision-Making CDM where a group selects the bestoption among several alternatives such as choosing an optimal foraging site.In this work we demonstrate a theoretical and empirical equivalence betweenCDM and single-agent reinforcement learning RL in multi-armed banditproblems utilizing concepts from opinion dynamics evolutionary game theoryand RL. This equivalence bridges the gap between SI and RL and leads us tointroduce a novel abstract RL update rule called Maynard-Cross Learning.Additionally it provides a new population-based perspective on common RLpractices like learning rate adjustment and batching. Our findings enablecross-disciplinary fertilization between RL and SI allowing techniques fromone field to enhance the understanding and methodologies of the other.</p>
                <p>Last Updated: 2024-10-23 02:49:37 UTC</p>
                <button class="interpret-button" data-id="2410.17517v1">Interpret</button>
                <div id="interpretation-2410.17517v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration</h3>
                <p>Authors: Max WilcoxsonQiyang LiKevin FransSergey Levine</p>
                <p><a href="http://arxiv.org/abs/2410.18076v1">Link to paper</a></p>
                <p>Unsupervised pretraining has been transformative in many supervised domains.However applying such ideas to reinforcement learning RL presents a uniquechallenge in that fine-tuning does not involve mimicking task-specific databut rather exploring and locating the solution through iterativeself-improvement. In this work we study how unlabeled prior trajectory datacan be leveraged to learn efficient exploration strategies. While prior datacan be used to pretrain a set of low-level skills or as additional off-policydata for online RL it has been unclear how to combine these ideas effectivelyfor online exploration. Our method SUPE Skills from Unlabeled Prior data forExploration demonstrates that a careful combination of these ideas compoundstheir benefits. Our method first extracts low-level skills using a variationalautoencoder VAE and then pseudo-relabels unlabeled trajectories using anoptimistic reward model transforming prior data into high-level task-relevantexamples. Finally SUPE uses these transformed examples as additionaloff-policy data for online RL to learn a high-level policy that composespretrained low-level skills to explore efficiently. We empirically show thatSUPE reliably outperforms prior strategies successfully solving a suite oflong-horizon sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.</p>
                <p>Last Updated: 2024-10-23 17:58:45 UTC</p>
                <button class="interpret-button" data-id="2410.18076v1">Interpret</button>
                <div id="interpretation-2410.18076v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices</h3>
                <p>Authors: Chanwoo ChunSueYeon ChungDaniel D. Lee</p>
                <p><a href="http://arxiv.org/abs/2410.17998v2">Link to paper</a></p>
                <p>Analyzing the structure of sampled features from an input data distributionis challenging when constrained by limited measurements in both the number ofinputs and features. Traditional approaches often rely on the eigenvaluespectrum of the sample covariance matrix derived from finite measurementmatrices however these spectra are sensitive to the size of the measurementmatrix leading to biased insights. In this paper we introduce a novelalgorithm that provides unbiased estimates of the spectral moments of thekernel integral operator in the limit of infinite inputs and features fromfinitely sampled measurement matrices. Our method based on dynamicprogramming is efficient and capable of estimating the moments of the operatorspectrum. We demonstrate the accuracy of our estimator on radial basis functionRBF kernels highlighting its consistency with the theoretical spectra.Furthermore we showcase the practical utility and robustness of our method inunderstanding the geometry of learned representations in neural networks.</p>
                <p>Last Updated: 2024-10-24 17:47:20 UTC</p>
                <button class="interpret-button" data-id="2410.17998v2">Interpret</button>
                <div id="interpretation-2410.17998v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Semi-Implicit Functional Gradient Flow</h3>
                <p>Authors: Shiyue ZhangZiheng ChengCheng Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.17935v1">Link to paper</a></p>
                <p>Particle-based variational inference methods ParVIs use non-parametricvariational families represented by particles to approximate the targetdistribution according to the kernelized Wasserstein gradient flow for theKullback-Leibler KL divergence. Recent works introduce functional gradientflows to substitute the kernel for better flexibility. However thedeterministic updating mechanism may suffer from limited exploration andrequire expensive repetitive runs for new samples. In this paper we proposeSemi-Implicit Functional Gradient flow SIFG a functional gradient ParVImethod that uses perturbed particles as the approximation family. Thecorresponding functional gradient flow which can be estimated via denoisingscore matching exhibits strong theoretical convergence guarantee. We alsopresent an adaptive version of our method to automatically choose the suitablenoise magnitude. Extensive experiments demonstrate the effectiveness andefficiency of the proposed framework on both simulated and real data problems.</p>
                <p>Last Updated: 2024-10-23 15:00:30 UTC</p>
                <button class="interpret-button" data-id="2410.17935v1">Interpret</button>
                <div id="interpretation-2410.17935v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Deep learning for model correction of dynamical systems with data scarcity</h3>
                <p>Authors: Caroline TatsuokaDongbin Xiu</p>
                <p><a href="http://arxiv.org/abs/2410.17913v1">Link to paper</a></p>
                <p>We present a deep learning framework for correcting existing dynamical systemmodels utilizing only a scarce high-fidelity data set. In many practicalsituations one has a low-fidelity model that can capture the dynamicsreasonably well but lacks high resolution due to the inherent limitation ofthe model and the complexity of the underlying physics. When high resolutiondata become available it is natural to seek model correction to improve theresolution of the model predictions. We focus on the case when the amount ofhigh-fidelity data is so small that most of the existing data driven modelingmethods cannot be applied. In this paper we address these challenges with amodel-correction method which only requires a scarce high-fidelity data set.Our method first seeks a deep neural network DNN model to approximate theexisting low-fidelity model. By using the scarce high-fidelity data the methodthen corrects the DNN model via transfer learning TL. After TL an improvedDNN model with high prediction accuracy to the underlying dynamics is obtained.One distinct feature of the propose method is that it does not assume aspecific form of the model correction terms. Instead it offers an inherentcorrection to the low-fidelity model via TL. A set of numerical examples arepresented to demonstrate the effectiveness of the proposed method.</p>
                <p>Last Updated: 2024-10-23 14:33:11 UTC</p>
                <button class="interpret-button" data-id="2410.17913v1">Interpret</button>
                <div id="interpretation-2410.17913v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Reinforcement Learning under Latent Dynamics: Toward Statistical and Algorithmic Modularity</h3>
                <p>Authors: Philip AmortilaDylan J. FosterNan JiangAkshay KrishnamurthyZakaria Mhammedi</p>
                <p><a href="http://arxiv.org/abs/2410.17904v1">Link to paper</a></p>
                <p>Real-world applications of reinforcement learning often involve environmentswhere agents operate on complex high-dimensional observations but theunderlying latent dynamics are comparatively simple. However outside ofrestrictive settings such as small latent spaces the fundamental statisticalrequirements and algorithmic principles for reinforcement learning under latentdynamics are poorly understood.  This paper addresses the question of reinforcement learning undertextitgeneral latent dynamics from a statistical and algorithmicperspective. On the statistical side our main negative result shows that mostwell-studied settings for reinforcement learning with function approximationbecome intractable when composed with rich observations we complement thiswith a positive result identifying latent pushforward coverability as ageneral condition that enables statistical tractability. Algorithmically wedevelop provably efficient observable-to-latent reductions -- that isreductions that transform an arbitrary algorithm for the latent MDP into analgorithm that can operate on rich observations -- in two settings: one wherethe agent has access to hindsight observations of the latent dynamics LADZ23and one where the agent can estimate self-predictive latent models SAGHCB20.Together our results serve as a first step toward a unified statistical andalgorithmic theory for reinforcement learning under latent dynamics.</p>
                <p>Last Updated: 2024-10-23 14:22:49 UTC</p>
                <button class="interpret-button" data-id="2410.17904v1">Interpret</button>
                <div id="interpretation-2410.17904v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes</h3>
                <p>Authors: Hengwei BianLingdong KongHaozhe XieLiang PanYu QiaoZiwei Liu</p>
                <p><a href="http://arxiv.org/abs/2410.18084v1">Link to paper</a></p>
                <p>LiDAR scene generation has been developing rapidly recently. Howeverexisting methods primarily focus on generating static and single-frame scenesoverlooking the inherently dynamic nature of real-world driving environments.In this work we introduce DynamicCity a novel 4D LiDAR generation frameworkcapable of generating large-scale high-quality LiDAR scenes that capture thetemporal evolution of dynamic environments. DynamicCity mainly consists of twokey models. 1 A VAE model for learning HexPlane as the compact 4Drepresentation. Instead of using naive averaging operations DynamicCityemploys a novel Projection Module to effectively compress 4D LiDAR featuresinto six 2D feature maps for HexPlane construction which significantlyenhances HexPlane fitting quality up to 12.56 mIoU gain. Furthermore weutilize an Expansion  Squeeze Strategy to reconstruct 3D feature volumes inparallel which improves both network training efficiency and reconstructionaccuracy than naively querying each 3D point up to 7.05 mIoU gain 2.06xtraining speedup and 70.84 memory reduction. 2 A DiT-based diffusion modelfor HexPlane generation. To make HexPlane feasible for DiT generation a PaddedRollout Operation is proposed to reorganize all six feature planes of theHexPlane as a squared 2D feature map. In particular various conditions couldbe introduced in the diffusion or sampling process supporting versatile 4Dgeneration applications such as trajectory- and command-driven generationinpainting and layout-conditioned generation. Extensive experiments on theCarlaSC and Waymo datasets demonstrate that DynamicCity significantlyoutperforms existing state-of-the-art 4D LiDAR generation methods acrossmultiple metrics. The code will be released to facilitate future research.</p>
                <p>Last Updated: 2024-10-23 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2410.18084v1">Interpret</button>
                <div id="interpretation-2410.18084v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>FIPER: Generalizable Factorized Fields for Joint Image Compression and Super-Resolution</h3>
                <p>Authors: Yang-Che SunCheng Yu YeoErnie ChuJun-Cheng ChenYu-Lun Liu</p>
                <p><a href="http://arxiv.org/abs/2410.18083v1">Link to paper</a></p>
                <p>In this work we propose a unified representation for Super-Resolution SRand Image Compression termed Factorized Fields motivated by the sharedprinciples between these two tasks. Both SISR and Image Compression requirerecovering and preserving fine image details--whether by enhancing resolutionor reconstructing compressed data. Unlike previous methods that mainly focus onnetwork architecture our proposed approach utilizes a basis-coefficientdecomposition to explicitly capture multi-scale visual features and structuralcomponents in images addressing the core challenges of both tasks. We firstderive our SR model which includes a Coefficient Backbone and Basis SwinTransformer for generalizable Factorized Fields. Then to further unify thesetwo tasks we leverage the strong information-recovery capabilities of thetrained SR modules as priors in the compression pipeline improving bothcompression efficiency and detail reconstruction. Additionally we introduce amerged-basis compression branch that consolidates shared structures furtheroptimizing the compression process. Extensive experiments show that our unifiedrepresentation delivers state-of-the-art performance achieving an averagerelative improvement of 204.4 in PSNR over the baseline in Super-ResolutionSR and 9.35 BD-rate reduction in Image Compression compared to the previousSOTA.</p>
                <p>Last Updated: 2024-10-23 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2410.18083v1">Interpret</button>
                <div id="interpretation-2410.18083v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>FreeVS: Generative View Synthesis on Free Driving Trajectory</h3>
                <p>Authors: Qitai WangLue FanYuqi WangYuntao ChenZhaoxiang Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.18079v1">Link to paper</a></p>
                <p>Existing reconstruction-based novel view synthesis methods for driving scenesfocus on synthesizing camera views along the recorded trajectory of the egovehicle. Their image rendering performance will severely degrade on viewpointsfalling out of the recorded trajectory where camera rays are untrained. Wepropose FreeVS a novel fully generative approach that can synthesize cameraviews on free new trajectories in real driving scenes. To control thegeneration results to be 3D consistent with the real scenes and accurate inviewpoint pose we propose the pseudo-image representation of view priors tocontrol the generation process. Viewpoint transformation simulation is appliedon pseudo-images to simulate camera movement in each direction. Once trainedFreeVS can be applied to any validation sequences without reconstructionprocess and synthesis views on novel trajectories. Moreover we propose two newchallenging benchmarks tailored to driving scenes which are novel camerasynthesis and novel trajectory synthesis emphasizing the freedom ofviewpoints. Given that no ground truth images are available on noveltrajectories we also propose to evaluate the consistency of images synthesizedon novel trajectories with 3D perception models. Experiments on the Waymo OpenDataset show that FreeVS has a strong image synthesis performance on both therecorded trajectories and novel trajectories. Project Page:https://freevs24.github.io/</p>
                <p>Last Updated: 2024-10-23 17:59:11 UTC</p>
                <button class="interpret-button" data-id="2410.18079v1">Interpret</button>
                <div id="interpretation-2410.18079v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>UnCLe: Unsupervised Continual Learning of Depth Completion</h3>
                <p>Authors: Suchisrit GangopadhyayXien ChenMichael ChuPatrick RimHyoungseob ParkAlex Wong</p>
                <p><a href="http://arxiv.org/abs/2410.18074v1">Link to paper</a></p>
                <p>We propose UnCLe a standardized benchmark for Unsupervised ContinualLearning of a multimodal depth estimation task: Depth completion aims to infera dense depth map from a pair of synchronized RGB image and sparse depth map.We benchmark depth completion models under the practical scenario ofunsupervised learning over continuous streams of data. Existing methods aretypically trained on a static or stationary dataset. However when adaptingto novel non-stationary distributions they catastrophically forgetpreviously learned information. UnCLe simulates these non-stationarydistributions by adapting depth completion models to sequences of datasetscontaining diverse scenes captured from distinct domains using different visualand range sensors. We adopt representative methods from continual learningparadigms and translate them to enable unsupervised continual learning of depthcompletion. We benchmark these models for indoor and outdoor and investigatethe degree of catastrophic forgetting through standard quantitative metrics.Furthermore we introduce model inversion quality as an additional measure offorgetting. We find that unsupervised continual learning of depth completion isan open problem and we invite researchers to leverage UnCLe as a developmentplatform.</p>
                <p>Last Updated: 2024-10-23 17:56:33 UTC</p>
                <button class="interpret-button" data-id="2410.18074v1">Interpret</button>
                <div id="interpretation-2410.18074v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WorldSimBench: Towards Video Generation Models as World Simulators</h3>
                <p>Authors: Yiran QinZhelun ShiJiwen YuXijun WangEnshen ZhouLijun LiZhenfei YinXihui LiuLu ShengJing ShaoLei BaiWanli OuyangRuimao Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.18072v1">Link to paper</a></p>
                <p>Recent advancements in predictive models have demonstrated exceptionalcapabilities in predicting the future state of objects and scenes. However thelack of categorization based on inherent characteristics continues to hinderthe progress of predictive model development. Additionally existing benchmarksare unable to effectively evaluate higher-capability highly embodiedpredictive models from an embodied perspective. In this work we classify thefunctionalities of predictive models into a hierarchy and take the first stepin evaluating World Simulators by proposing a dual evaluation framework calledWorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation andImplicit Manipulative Evaluation encompassing human preference assessmentsfrom the visual perspective and action-level evaluations in embodied taskscovering three representative embodied scenarios: Open-Ended EmbodiedEnvironment Autonomous Driving and Robot Manipulation. In the ExplicitPerceptual Evaluation we introduce the HF-Embodied Dataset a video assessmentdataset based on fine-grained human feedback which we use to train a HumanPreference Evaluator that aligns with human perception and explicitly assessesthe visual fidelity of World Simulators. In the Implicit ManipulativeEvaluation we assess the video-action consistency of World Simulators byevaluating whether the generated situation-aware video can be accuratelytranslated into the correct control signals in dynamic environments. Ourcomprehensive evaluation offers key insights that can drive further innovationin video generation models positioning World Simulators as a pivotaladvancement toward embodied artificial intelligence.</p>
                <p>Last Updated: 2024-10-23 17:56:11 UTC</p>
                <button class="interpret-button" data-id="2410.18072v1">Interpret</button>
                <div id="interpretation-2410.18072v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>ALTA: Compiler-Based Analysis of Transformers</h3>
                <p>Authors: Peter ShawJames CohanJacob EisensteinKenton LeeJonathan BerantKristina Toutanova</p>
                <p><a href="http://arxiv.org/abs/2410.18077v1">Link to paper</a></p>
                <p>We propose a new programming language called ALTA and a compiler that can mapALTA programs to Transformer weights. ALTA is inspired by RASP a languageproposed by Weiss et al. 2021 and Tracr Lindner et al. 2023 a compilerfrom RASP programs to Transformer weights. ALTA complements and extends thisprior work offering the ability to express loops and to compile programs toUniversal Transformers among other advantages. ALTA allows us toconstructively show how Transformers can represent length-invariant algorithmsfor computing parity and addition as well as a solution to the SCAN benchmarkof compositional generalization tasks without requiring intermediatescratchpad decoding steps. We also propose tools to analyze cases where theexpressibility of an algorithm is established but end-to-end training on agiven training set fails to induce behavior consistent with the desiredalgorithm. To this end we explore training from ALTA execution traces as amore fine-grained supervision signal. This enables additional experiments andtheoretical analyses relating the learnability of various algorithms to dataavailability and modeling decisions such as positional encodings. We make theALTA framework -- language specification symbolic interpreter and weightcompiler -- available to the community to enable further applications andinsights.</p>
                <p>Last Updated: 2024-10-23 17:58:49 UTC</p>
                <button class="interpret-button" data-id="2410.18077v1">Interpret</button>
                <div id="interpretation-2410.18077v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration</h3>
                <p>Authors: Max WilcoxsonQiyang LiKevin FransSergey Levine</p>
                <p><a href="http://arxiv.org/abs/2410.18076v1">Link to paper</a></p>
                <p>Unsupervised pretraining has been transformative in many supervised domains.However applying such ideas to reinforcement learning RL presents a uniquechallenge in that fine-tuning does not involve mimicking task-specific databut rather exploring and locating the solution through iterativeself-improvement. In this work we study how unlabeled prior trajectory datacan be leveraged to learn efficient exploration strategies. While prior datacan be used to pretrain a set of low-level skills or as additional off-policydata for online RL it has been unclear how to combine these ideas effectivelyfor online exploration. Our method SUPE Skills from Unlabeled Prior data forExploration demonstrates that a careful combination of these ideas compoundstheir benefits. Our method first extracts low-level skills using a variationalautoencoder VAE and then pseudo-relabels unlabeled trajectories using anoptimistic reward model transforming prior data into high-level task-relevantexamples. Finally SUPE uses these transformed examples as additionaloff-policy data for online RL to learn a high-level policy that composespretrained low-level skills to explore efficiently. We empirically show thatSUPE reliably outperforms prior strategies successfully solving a suite oflong-horizon sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.</p>
                <p>Last Updated: 2024-10-23 17:58:45 UTC</p>
                <button class="interpret-button" data-id="2410.18076v1">Interpret</button>
                <div id="interpretation-2410.18076v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts</h3>
                <p>Authors: Yuxuan XieTianhua LiWenqi ShaoKaipeng Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.18071v1">Link to paper</a></p>
                <p>Recently multimodal large language models MLLMs have received muchattention for their impressive capabilities. The evaluation of MLLMs isbecoming critical to analyzing attributes of MLLMs and providing valuableinsights. However current benchmarks overlook the problem of promptsensitivity - minor prompt variations may lead to significant performancefluctuations. Thus inappropriate prompts may obscure the models capabilitiesunderestimating the models performance. Moreover different models havedifferent preferences for different prompts and thus using the same promptfor all models will cause evaluation bias. This paper analyzes this deficiencyin existing benchmarks and further introduces a new evaluation framework namedTP-Eval which introduces a prompt customization method to reduce evaluationbiases and tap models potential. TP-Eval will rewrite the original prompts todifferent customized prompts for different models. In particular we proposesome well-designed modules for prompt customization tailored to the scenario ofMLLM evaluation. Extensive experiments demonstrate the effectiveness of ourapproach to uncovering models capabilities and TP-Eval should benefit thecommunity in developing more comprehensive and convincing MLLM evaluationbenchmarks.</p>
                <p>Last Updated: 2024-10-23 17:54:43 UTC</p>
                <button class="interpret-button" data-id="2410.18071v1">Interpret</button>
                <div id="interpretation-2410.18071v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Training Free Guided Flow Matching with Optimal Control</h3>
                <p>Authors: Luran WangChaoran ChengYizhen LiaoYanru QuGe Liu</p>
                <p><a href="http://arxiv.org/abs/2410.18070v1">Link to paper</a></p>
                <p>Controlled generation with pre-trained Diffusion and Flow Matching models hasvast applications. One strategy for guiding ODE-based generative models isthrough optimizing a target loss Rx_1 while staying close to the priordistribution. Along this line some recent work showed the effectiveness ofguiding flow model by differentiating through its ODE sampling process. Despitethe superior performance the theoretical understanding of this line of methodsis still preliminary leaving space for algorithm improvement. Moreoverexisting methods predominately focus on Euclidean data manifold and there is acompelling need for guided flow methods on complex geometries such as SO3which prevails in high-stake scientific applications like protein design. Wepresent OC-Flow a general and theoretically grounded training-free frameworkfor guided flow matching using optimal control. Building upon advances inoptimal control theory we develop effective and practical algorithms forsolving optimal control in guided ODE-based generation and provide a systematictheoretical analysis of the convergence guarantee in both Euclidean and SO3.We show that existing backprop-through-ODE methods can be interpreted asspecial cases of Euclidean OC-Flow. OC-Flow achieved superior performance inextensive experiments on text-guided image manipulation conditional moleculegeneration and all-atom peptide design.</p>
                <p>Last Updated: 2024-10-23 17:53:11 UTC</p>
                <button class="interpret-button" data-id="2410.18070v1">Interpret</button>
                <div id="interpretation-2410.18070v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Beyond position: how rotary embeddings shape representations and memory in autoregressive transfomers</h3>
                <p>Authors: Valeria RuscioFabrizio Silvestri</p>
                <p><a href="http://arxiv.org/abs/2410.18067v1">Link to paper</a></p>
                <p>Rotary Positional Embeddings RoPE enhance positional encoding inTransformer models yet their full impact on model dynamics remainsunderexplored. This paper studies how RoPE introduces position-dependentrotations causing phase shifts in token embeddings that influencehigher-frequency components within the models internal representations.Through spectral analysis we demonstrate that RoPEs rotation matrices induceoscillatory behaviors in embeddings affecting information retention acrosslayers and shaping temporal modeling capabilities. We show that activationfunctions in feed-forward networks interact with RoPE-modulated embeddings togenerate harmonics leading to constructive or destructive interference basedon phase alignment. Our findings reveal that phase alignment amplifiesactivations and sharpens attention while misalignment weakens activations anddisrupts focus on positional patterns. This study underscores the importance offrequency components as intrinsic elements of model behavior offering newinsights beyond traditional analyses.</p>
                <p>Last Updated: 2024-10-23 17:48:28 UTC</p>
                <button class="interpret-button" data-id="2410.18067v1">Interpret</button>
                <div id="interpretation-2410.18067v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>ALTA: Compiler-Based Analysis of Transformers</h3>
                <p>Authors: Peter ShawJames CohanJacob EisensteinKenton LeeJonathan BerantKristina Toutanova</p>
                <p><a href="http://arxiv.org/abs/2410.18077v1">Link to paper</a></p>
                <p>We propose a new programming language called ALTA and a compiler that can mapALTA programs to Transformer weights. ALTA is inspired by RASP a languageproposed by Weiss et al. 2021 and Tracr Lindner et al. 2023 a compilerfrom RASP programs to Transformer weights. ALTA complements and extends thisprior work offering the ability to express loops and to compile programs toUniversal Transformers among other advantages. ALTA allows us toconstructively show how Transformers can represent length-invariant algorithmsfor computing parity and addition as well as a solution to the SCAN benchmarkof compositional generalization tasks without requiring intermediatescratchpad decoding steps. We also propose tools to analyze cases where theexpressibility of an algorithm is established but end-to-end training on agiven training set fails to induce behavior consistent with the desiredalgorithm. To this end we explore training from ALTA execution traces as amore fine-grained supervision signal. This enables additional experiments andtheoretical analyses relating the learnability of various algorithms to dataavailability and modeling decisions such as positional encodings. We make theALTA framework -- language specification symbolic interpreter and weightcompiler -- available to the community to enable further applications andinsights.</p>
                <p>Last Updated: 2024-10-23 17:58:49 UTC</p>
                <button class="interpret-button" data-id="2410.18077v1">Interpret</button>
                <div id="interpretation-2410.18077v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts</h3>
                <p>Authors: Yuxuan XieTianhua LiWenqi ShaoKaipeng Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.18071v1">Link to paper</a></p>
                <p>Recently multimodal large language models MLLMs have received muchattention for their impressive capabilities. The evaluation of MLLMs isbecoming critical to analyzing attributes of MLLMs and providing valuableinsights. However current benchmarks overlook the problem of promptsensitivity - minor prompt variations may lead to significant performancefluctuations. Thus inappropriate prompts may obscure the models capabilitiesunderestimating the models performance. Moreover different models havedifferent preferences for different prompts and thus using the same promptfor all models will cause evaluation bias. This paper analyzes this deficiencyin existing benchmarks and further introduces a new evaluation framework namedTP-Eval which introduces a prompt customization method to reduce evaluationbiases and tap models potential. TP-Eval will rewrite the original prompts todifferent customized prompts for different models. In particular we proposesome well-designed modules for prompt customization tailored to the scenario ofMLLM evaluation. Extensive experiments demonstrate the effectiveness of ourapproach to uncovering models capabilities and TP-Eval should benefit thecommunity in developing more comprehensive and convincing MLLM evaluationbenchmarks.</p>
                <p>Last Updated: 2024-10-23 17:54:43 UTC</p>
                <button class="interpret-button" data-id="2410.18071v1">Interpret</button>
                <div id="interpretation-2410.18071v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CLEAR: Character Unlearning in Textual and Visual Modalities</h3>
                <p>Authors: Alexey DontsovDmitrii KorzhAlexey ZhavoronkinBoris MikheevDenis BobkovAibek AlanovOleg Y. RogovIvan OseledetsElena Tutubalina</p>
                <p><a href="http://arxiv.org/abs/2410.18057v1">Link to paper</a></p>
                <p>Machine Unlearning MU is critical for enhancing privacy and security indeep learning models particularly in large multimodal language models MLLMsby removing specific private or hazardous information. While MU has madesignificant progress in textual and visual modalities multimodal unlearningMMU remains significantly underexplored partially due to the absence of asuitable open-source benchmark. To address this we introduce CLEAR a newbenchmark designed to evaluate MMU methods. CLEAR contains 200 fictitiousindividuals and 3700 images linked with corresponding question-answer pairsenabling a thorough evaluation across modalities. We assess 10 MU methodsadapting them for MMU and highlight new challenges specific to multimodalforgetting. We also demonstrate that simple ell_1 regularization on LoRAweights significantly mitigates catastrophic forgetting preserving modelperformance on retained data. The dataset is available athttps://huggingface.co/datasets/therem/CLEAR</p>
                <p>Last Updated: 2024-10-23 17:30:50 UTC</p>
                <button class="interpret-button" data-id="2410.18057v1">Interpret</button>
                <div id="interpretation-2410.18057v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering</h3>
                <p>Authors: Qingfei ZhaoRuobing WangYukuo CenDaren ZhaShicheng TanYuxiao DongJie Tang</p>
                <p><a href="http://arxiv.org/abs/2410.18050v1">Link to paper</a></p>
                <p>Long-Context Question Answering LCQA a challenging task aims to reasonover long-context documents to yield accurate answers to questions. Existinglong-context Large Language Models LLMs for LCQA often struggle with thelost in the middle issue. Retrieval-Augmented Generation RAG mitigates thisissue by providing external factual evidence. However its chunking strategydisrupts the global long-context information and its low-quality retrieval inlong contexts hinders LLMs from identifying effective factual details due tosubstantial noise. To this end we propose LongRAG a generaldual-perspective and robust LLM-based RAG system paradigm for LCQA to enhanceRAGs understanding of complex long-context knowledge i.e. global informationand factual details. We design LongRAG as a plug-and-play paradigmfacilitating adaptation to various domains and LLMs. Extensive experiments onthree multi-hop datasets demonstrate that LongRAG significantly outperformslong-context LLMs up by 6.94 advanced RAG up by 6.16 and Vanilla RAGup by 17.25. Furthermore we conduct quantitative ablation studies andmulti-dimensional analyses highlighting the effectiveness of the systemscomponents and fine-tuning strategies. Data and code are available athttps://github.com/QingFei1/LongRAG.</p>
                <p>Last Updated: 2024-10-23 17:24:58 UTC</p>
                <button class="interpret-button" data-id="2410.18050v1">Interpret</button>
                <div id="interpretation-2410.18050v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for Russian Scientific Keyphrases</h3>
                <p>Authors: Anna GlazkovaDmitry MorozovTimur Garipov</p>
                <p><a href="http://arxiv.org/abs/2410.18040v1">Link to paper</a></p>
                <p>Keyphrase selection is a challenging task in natural language processing thathas a wide range of applications. Adapting existing supervised and unsupervisedsolutions for the Russian language faces several limitations due to the richmorphology of Russian and the limited number of training datasets available.Recent studies conducted on English texts show that large language modelsLLMs successfully address the task of generating keyphrases. LLMs allowachieving impressive results without task-specific fine-tuning using textprompts instead. In this work we access the performance of prompt-basedmethods for generating keyphrases for Russian scientific abstracts. First wecompare the performance of zero-shot and few-shot prompt-based methodsfine-tuned models and unsupervised methods. Then we assess strategies forselecting keyphrase examples in a few-shot setting. We present the outcomes ofhuman evaluation of the generated keyphrases and analyze the strengths andweaknesses of the models through expert assessment. Our results suggest thatprompt-based methods can outperform common baselines even using simple textprompts.</p>
                <p>Last Updated: 2024-10-23 17:07:32 UTC</p>
                <button class="interpret-button" data-id="2410.18040v1">Interpret</button>
                <div id="interpretation-2410.18040v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Prioritized Generative Replay</h3>
                <p>Authors: Renhao WangKevin FransPieter AbbeelSergey LevineAlexei A. Efros</p>
                <p><a href="http://arxiv.org/abs/2410.18082v1">Link to paper</a></p>
                <p>Sample-efficient online reinforcement learning often uses replay buffers tostore experience for reuse when updating the value function. However uniformreplay is inefficient since certain classes of transitions can be morerelevant to learning. While prioritization of more useful samples is helpfulthis strategy can also lead to overfitting as useful samples are likely to bemore rare. In this work we instead propose a prioritized parametric versionof an agents memory using generative models to capture online experience.This paradigm enables 1 densification of past experience with newgenerations that benefit from the generative models generalization capacityand 2 guidance via a family of relevance functions that push thesegenerations towards more useful parts of an agents acquired history. We showthis recipe can be instantiated using conditional diffusion models and simplerelevance functions such as curiosity- or value-based metrics. Our approachconsistently improves performance and sample efficiency in both state- andpixel-based domains. We expose the mechanisms underlying these gains showinghow guidance promotes diversity in our generated transitions and reducesoverfitting. We also showcase how our approach can train policies with evenhigher update-to-data ratios than before opening up avenues to better scaleonline RL agents.</p>
                <p>Last Updated: 2024-10-23 17:59:52 UTC</p>
                <button class="interpret-button" data-id="2410.18082v1">Interpret</button>
                <div id="interpretation-2410.18082v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ALTA: Compiler-Based Analysis of Transformers</h3>
                <p>Authors: Peter ShawJames CohanJacob EisensteinKenton LeeJonathan BerantKristina Toutanova</p>
                <p><a href="http://arxiv.org/abs/2410.18077v1">Link to paper</a></p>
                <p>We propose a new programming language called ALTA and a compiler that can mapALTA programs to Transformer weights. ALTA is inspired by RASP a languageproposed by Weiss et al. 2021 and Tracr Lindner et al. 2023 a compilerfrom RASP programs to Transformer weights. ALTA complements and extends thisprior work offering the ability to express loops and to compile programs toUniversal Transformers among other advantages. ALTA allows us toconstructively show how Transformers can represent length-invariant algorithmsfor computing parity and addition as well as a solution to the SCAN benchmarkof compositional generalization tasks without requiring intermediatescratchpad decoding steps. We also propose tools to analyze cases where theexpressibility of an algorithm is established but end-to-end training on agiven training set fails to induce behavior consistent with the desiredalgorithm. To this end we explore training from ALTA execution traces as amore fine-grained supervision signal. This enables additional experiments andtheoretical analyses relating the learnability of various algorithms to dataavailability and modeling decisions such as positional encodings. We make theALTA framework -- language specification symbolic interpreter and weightcompiler -- available to the community to enable further applications andinsights.</p>
                <p>Last Updated: 2024-10-23 17:58:49 UTC</p>
                <button class="interpret-button" data-id="2410.18077v1">Interpret</button>
                <div id="interpretation-2410.18077v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration</h3>
                <p>Authors: Max WilcoxsonQiyang LiKevin FransSergey Levine</p>
                <p><a href="http://arxiv.org/abs/2410.18076v1">Link to paper</a></p>
                <p>Unsupervised pretraining has been transformative in many supervised domains.However applying such ideas to reinforcement learning RL presents a uniquechallenge in that fine-tuning does not involve mimicking task-specific databut rather exploring and locating the solution through iterativeself-improvement. In this work we study how unlabeled prior trajectory datacan be leveraged to learn efficient exploration strategies. While prior datacan be used to pretrain a set of low-level skills or as additional off-policydata for online RL it has been unclear how to combine these ideas effectivelyfor online exploration. Our method SUPE Skills from Unlabeled Prior data forExploration demonstrates that a careful combination of these ideas compoundstheir benefits. Our method first extracts low-level skills using a variationalautoencoder VAE and then pseudo-relabels unlabeled trajectories using anoptimistic reward model transforming prior data into high-level task-relevantexamples. Finally SUPE uses these transformed examples as additionaloff-policy data for online RL to learn a high-level policy that composespretrained low-level skills to explore efficiently. We empirically show thatSUPE reliably outperforms prior strategies successfully solving a suite oflong-horizon sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.</p>
                <p>Last Updated: 2024-10-23 17:58:45 UTC</p>
                <button class="interpret-button" data-id="2410.18076v1">Interpret</button>
                <div id="interpretation-2410.18076v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ProFL: Performative Robust Optimal Federated Learning</h3>
                <p>Authors: Xue ZhengTian XieXuwei TanAylin YenerXueru ZhangAli PayaniMyungjin Lee</p>
                <p><a href="http://arxiv.org/abs/2410.18075v1">Link to paper</a></p>
                <p>Performative prediction PP is a framework that captures distribution shiftsthat occur during the training of machine learning models due to theirdeployment. As the trained model is used its generated data could cause themodel to evolve leading to deviations from the original data distribution. Theimpact of such model-induced distribution shifts in the federated learning FLsetup remains unexplored despite being increasingly likely to transpire inreal-life use cases. Although Jin et al. 2024 recently extended PP to FL in astraightforward manner the resulting model only converges to a performativestable point which may be far from optimal. The methods in Izzo et al. 2021Miller et al. 2021 can find a performative optimal point in centralizedsettings but they require the performative risk to be convex and the trainingdata to be noiseless assumptions often violated in realistic FL systems. Thispaper overcomes all of these shortcomings and proposes Performative robustoptimal Federated Learning ProFL an algorithm that finds performativeoptimal points in FL from noisy and contaminated data. We present theconvergence analysis under the Polyak-Lojasiewicz condition which applies tonon-convex objectives. Extensive experiments on multiple datasets validate ourproposed algorithms efficiency.</p>
                <p>Last Updated: 2024-10-23 17:57:14 UTC</p>
                <button class="interpret-button" data-id="2410.18075v1">Interpret</button>
                <div id="interpretation-2410.18075v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>UnCLe: Unsupervised Continual Learning of Depth Completion</h3>
                <p>Authors: Suchisrit GangopadhyayXien ChenMichael ChuPatrick RimHyoungseob ParkAlex Wong</p>
                <p><a href="http://arxiv.org/abs/2410.18074v1">Link to paper</a></p>
                <p>We propose UnCLe a standardized benchmark for Unsupervised ContinualLearning of a multimodal depth estimation task: Depth completion aims to infera dense depth map from a pair of synchronized RGB image and sparse depth map.We benchmark depth completion models under the practical scenario ofunsupervised learning over continuous streams of data. Existing methods aretypically trained on a static or stationary dataset. However when adaptingto novel non-stationary distributions they catastrophically forgetpreviously learned information. UnCLe simulates these non-stationarydistributions by adapting depth completion models to sequences of datasetscontaining diverse scenes captured from distinct domains using different visualand range sensors. We adopt representative methods from continual learningparadigms and translate them to enable unsupervised continual learning of depthcompletion. We benchmark these models for indoor and outdoor and investigatethe degree of catastrophic forgetting through standard quantitative metrics.Furthermore we introduce model inversion quality as an additional measure offorgetting. We find that unsupervised continual learning of depth completion isan open problem and we invite researchers to leverage UnCLe as a developmentplatform.</p>
                <p>Last Updated: 2024-10-23 17:56:33 UTC</p>
                <button class="interpret-button" data-id="2410.18074v1">Interpret</button>
                <div id="interpretation-2410.18074v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-10-25</p>
        </div>
    
        </div>
    </body>
    </html>
    