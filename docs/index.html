
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Learning from Models and Data for Visual Grounding</h3>
                <p>Authors: Ruozhen HePaola Cascante-BonillaZiyan YangAlexander C. BergVicente Ordonez</p>
                <p><a href="http://arxiv.org/abs/2403.13804v1">Link to paper</a></p>
                <p>We introduce SynGround a novel framework that combines data-driven learningand knowledge transfer from various large-scale pretrained models to enhancethe visual grounding capabilities of a pretrained vision-and-language model.The knowledge transfer from the models initiates the generation of imagedescriptions through an image description generator. These descriptions servedual purposes: they act as prompts for synthesizing images through atext-to-image generator and as queries for synthesizing text from whichphrases are extracted using a large language model. Finally we leverage anopen-vocabulary object detector to generate synthetic bounding boxes for thesynthetic images and texts. We finetune a pretrained vision-and-language modelon this dataset by optimizing a mask-attention consistency objective thataligns region annotations with gradient-based model explanations. The resultingmodel improves the grounding capabilities of an off-the-shelfvision-and-language model. Particularly SynGround improves the pointing gameaccuracy of ALBEF on the Flickr30k dataset from 79.38 to 87.26 and onRefCOCO Test A from 69.35 to 79.06 and on RefCOCO Test B from 53.77 to63.67.</p>
                <p>Last Updated: 2024-03-20 17:59:43 UTC</p>
                <button class="interpret-button" data-id="2403.13804v1">Interpret</button>
                <div id="interpretation-2403.13804v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ZigMa: Zigzag Mamba Diffusion Model</h3>
                <p>Authors: Vincent Tao HuStefan Andreas BaumannMing GuiOlga GrebenkovaPingchuan MaJohannes FischerBjorn Ommer</p>
                <p><a href="http://arxiv.org/abs/2403.13802v1">Link to paper</a></p>
                <p>The diffusion model has long been plagued by scalability and quadraticcomplexity issues especially within transformer-based structures. In thisstudy we aim to leverage the long sequence modeling capability of aState-Space Model called Mamba to extend its applicability to visual datageneration. Firstly we identify a critical oversight in most currentMamba-based vision methods namely the lack of consideration for spatialcontinuity in the scan scheme of Mamba. Secondly building upon this insightwe introduce a simple plug-and-play zero-parameter method named Zigzag Mambawhich outperforms Mamba-based baselines and demonstrates improved speed andmemory utilization compared to transformer-based baselines. Lastly weintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigatethe scalability of the model on large-resolution visual datasets such asFacesHQ 1024times 1024 and UCF101 MultiModal-CelebA-HQ and MS COCO256times 256. Code will be released at https://taohu.me/zigma/</p>
                <p>Last Updated: 2024-03-20 17:59:14 UTC</p>
                <button class="interpret-button" data-id="2403.13802v1">Interpret</button>
                <div id="interpretation-2403.13802v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs</h3>
                <p>Authors: Yusuke MikamiAndrew MelnikJun MiuraVille Hautam√§ki</p>
                <p><a href="http://arxiv.org/abs/2403.13801v1">Link to paper</a></p>
                <p>We demonstrate experimental results with LLMs that address robotics actionplanning problems. Recently LLMs have been applied in robotics actionplanning particularly using a code generation approach that converts complexhigh-level instructions into mid-level policy codes. In contrast our approachacquires text descriptions of the task and scene objects then formulatesaction planning through natural language reasoning and outputs coordinatelevel control commands thus reducing the necessity for intermediaterepresentation code as policies. Our approach is evaluated on a multi-modalprompt simulation benchmark demonstrating that our prompt engineeringexperiments with natural language reasoning significantly enhance success ratescompared to its absence. Furthermore our approach illustrates the potentialfor natural language descriptions to transfer robotics skills from known tasksto previously unseen tasks.</p>
                <p>Last Updated: 2024-03-20 17:58:12 UTC</p>
                <button class="interpret-button" data-id="2403.13801v1">Interpret</button>
                <div id="interpretation-2403.13801v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Reverse Training to Nurse the Reversal Curse</h3>
                <p>Authors: Olga GolovnevaZeyuan Allen-ZhuJason WestonSainbayar Sukhbaatar</p>
                <p><a href="http://arxiv.org/abs/2403.13799v1">Link to paper</a></p>
                <p>Large language models LLMs have a surprising failure: when trained on Ahas a feature B they do not generalize to B is a feature of A which istermed the Reversal Curse. Even when training with trillions of tokens thisissue still appears due to Zipfs law - hence even if we train on the entireinternet. This work proposes an alternative training scheme called reversetraining whereby all words are used twice doubling the amount of availabletokens. The LLM is trained in both forward and reverse directions by reversingthe training strings while preserving i.e. not reversing chosen substringssuch as entities. We show that data-matched reverse-trained models providesuperior performance to standard models on standard tasks and compute-matchedreverse-trained models provide far superior performance on reversal taskshelping resolve the reversal curse issue.</p>
                <p>Last Updated: 2024-03-20 17:55:35 UTC</p>
                <button class="interpret-button" data-id="2403.13799v1">Interpret</button>
                <div id="interpretation-2403.13799v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts</h3>
                <p>Authors: Guangzeng HanWeisi LiuXiaolei HuangBrian Borsari</p>
                <p><a href="http://arxiv.org/abs/2403.13786v1">Link to paper</a></p>
                <p>Automatic coding patient behaviors is essential to support decision makingfor psychotherapists during the motivational interviewing MI a collaborativecommunication intervention approach to address psychiatric issues such asalcohol and drug addiction. While the behavior coding task has rapidly adaptedmachine learning to predict patient states during the MI sessions lacking ofdomain-specific knowledge and overlooking patient-therapist interactions aremajor challenges in developing and deploying those models in real practice. Toencounter those challenges we introduce the Chain-of-Interaction CoIprompting method aiming to contextualize large language models LLMs forpsychiatric decision support by the dyadic interactions. The CoI promptingapproach systematically breaks down the coding task into three key reasoningsteps extract patient engagement learn therapist question strategies andintegrates dyadic interactions between patients and therapists. This approachenables large language models to leverage the coding scheme patient state anddomain knowledge for patient behavioral coding. Experiments on real-worlddatasets can prove the effectiveness and flexibility of our prompting methodwith multiple state-of-the-art LLMs over existing prompting baselines. We haveconducted extensive ablation analysis and demonstrate the critical role ofdyadic interactions in applying LLMs for psychotherapy behavior understanding.</p>
                <p>Last Updated: 2024-03-20 17:47:49 UTC</p>
                <button class="interpret-button" data-id="2403.13786v1">Interpret</button>
                <div id="interpretation-2403.13786v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>MotorEase: Automated Detection of Motor Impairment Accessibility Issues in Mobile App UIs</h3>
                <p>Authors: Arun KrishnavajjalaSM Hasan MansurJustin JoseKevin Moran</p>
                <p><a href="http://dx.doi.org/10.1145/3597503.3639167">Link to paper</a></p>
                <p>Recent research has begun to examine the potential of automatically findingand fixing accessibility issues that manifest in software. However whilerecent work makes important progress it has generally been skewed towardidentifying issues that affect users with certain disabilities such as thosewith visual or hearing impairments. However there are other groups of userswith different types of disabilities that also need software tooling support toimprove their experience. As such this paper aims to automatically identifyaccessibility issues that affect users with motor-impairments.  To move toward this goal this paper introduces a novel approach calledMotorEase capable of identifying accessibility issues in mobile app UIs thatimpact motor-impaired users. Motor-impaired users often have limited ability tointeract with touch-based devices and instead may make use of a switch orother assistive mechanism -- hence UIs must be designed to support both limitedtouch gestures and the use of assistive devices. MotorEase adapts computervision and text processing techniques to enable a semantic understanding of appUI screens enabling the detection of violations related to four popularpreviously unexplored UI design guidelines that support motor-impaired usersincluding: i visual touch target size ii expanding sections iiipersisting elements and iv adjacent icon visual distance. We evaluateMotorEase on a newly derived benchmark called MotorCheck that contains 555manually annotated examples of violations to the above accessibilityguidelines across 1599 screens collected from 70 applications via a mobile apptesting tool. Our experiments illustrate that MotorEase is able to identifyviolations with an average accuracy of 90 and a false positive rate of lessthan 9 outperforming baseline techniques.</p>
                <p>Last Updated: 2024-03-20 15:53:07 UTC</p>
                <button class="interpret-button" data-id="2403.13690v1">Interpret</button>
                <div id="interpretation-2403.13690v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning User Embeddings from Human Gaze for Personalised Saliency Prediction</h3>
                <p>Authors: Florian StrohmMihai B√¢ceAndreas Bulling</p>
                <p><a href="http://arxiv.org/abs/2403.13653v1">Link to paper</a></p>
                <p>Reusable embeddings of user behaviour have shown significant performanceimprovements for the personalised saliency prediction task. However priorworks require explicit user characteristics and preferences as input which areoften difficult to obtain. We present a novel method to extract user embeddingsfrom pairs of natural images and corresponding saliency maps generated from asmall amount of user-specific eye tracking data. At the core of our method is aSiamese convolutional neural encoder that learns the user embeddings bycontrasting the image and personal saliency map pairs of different users.Evaluations on two public saliency datasets show that the generated embeddingshave high discriminative power are effective at refining universal saliencymaps to the individual users and generalise well across users and images.Finally based on our models ability to encode individual usercharacteristics our work points towards other applications that can benefitfrom reusable embeddings of gaze behaviour.</p>
                <p>Last Updated: 2024-03-20 14:58:40 UTC</p>
                <button class="interpret-button" data-id="2403.13653v1">Interpret</button>
                <div id="interpretation-2403.13653v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VCounselor: A Psychological Intervention Chat Agent Based on a Knowledge-Enhanced Large Language Model</h3>
                <p>Authors: H. ZhangZ. QiaoH. WangB. DuanJ. Yin</p>
                <p><a href="http://arxiv.org/abs/2403.13553v1">Link to paper</a></p>
                <p>Conversational artificial intelligence can already independently engage inbrief conversations with clients with psychological problems and provideevidence-based psychological interventions. The main objective of this study isto improve the effectiveness and credibility of the large language model inpsychological intervention by creating a specialized agent the VCounselor toaddress the limitations observed in popular large language models such asChatGPT in domain applications. We achieved this goal by proposing a newaffective interaction structure and knowledge-enhancement structure. In orderto evaluate VCounselor this study compared the general large language modelthe fine-tuned large language model and VCounselors knowledge-enhanced largelanguage model. At the same time the general large language model and thefine-tuned large language model will also be provided with an avatar to comparethem as an agent with VCounselor. The comparison results indicated that theaffective interaction structure and knowledge-enhancement structure ofVCounselor significantly improved the effectiveness and credibility of thepsychological intervention and VCounselor significantly provided positivetendencies for clients emotions. The conclusion of this study stronglysupports that VConselor has a significant advantage in providing psychologicalsupport to clients by being able to analyze the patients problems withrelative accuracy and provide professional-level advice that enhances supportfor clients.</p>
                <p>Last Updated: 2024-03-20 12:46:02 UTC</p>
                <button class="interpret-button" data-id="2403.13553v1">Interpret</button>
                <div id="interpretation-2403.13553v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Tribal Theater Model: Social Regulation for Dynamic User Adaptation in Virtual Interactive Environments</h3>
                <p>Authors: H. ZhangB. DuanH. WangZ. QiaoJ. Yin</p>
                <p><a href="http://arxiv.org/abs/2403.13550v1">Link to paper</a></p>
                <p>This paper proposes a social regulation model for dynamic adaptationaccording to user characteristics in virtual interactive environments namelythe tribal theater model. The model focuses on organizational regulation andbuilds an interaction scheme with more resilient user performance by improvingthe subjectivity of the user. This paper discusses the sociological theoreticalbasis of this model and how it was migrated to an engineering implementation ofa virtual interactive environment. The model defines user interactions within afield that are regulated by a matrix through the allocation of resources. Toverify the effectiveness of the tribal theater model we designed anexperimental scene using a chatroom as an example. We trained the matrix as anAI model using a temporal transformer and compared it with an interaction fieldwith different levels of control. The experimental results showed that thetribal theater model can improve users interactive experience enhanceresilient user performance and effectively complete environmental interactiontasks under rule-based interaction.</p>
                <p>Last Updated: 2024-03-20 12:40:06 UTC</p>
                <button class="interpret-button" data-id="2403.13550v1">Interpret</button>
                <div id="interpretation-2403.13550v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>From One to Many: How Active Robot Swarm Sizes Influence Human Cognitive Processes</h3>
                <p>Authors: Julian KadukM√ºge CavdanKnut DrewingHeiko Hamann</p>
                <p><a href="http://arxiv.org/abs/2403.13541v1">Link to paper</a></p>
                <p>In robotics understanding human interaction with autonomous systems iscrucial for enhancing collaborative technologies. We focus on human-swarminteraction HSI exploring how differently sized groups of active robotsaffect operators cognitive and perceptual reactions over different durations.We analyze the impact of different numbers of active robots within a 15-robotswarm on operators time perception emotional state flow experience and taskdifficulty perception. Our findings indicate that managing multiple activerobots when compared to one active robot significantly alters time perceptionand flow experience leading to a faster passage of time and increased flow.More active robots and extended durations cause increased emotional arousal andperceived task difficulty highlighting the interaction between robot thenumber of active robots and human cognitive processes. These insights informthe creation of intuitive human-swarm interfaces and aid in developing swarmrobotic systems aligned with human cognitive structures enhancing human-robotcollaboration.</p>
                <p>Last Updated: 2024-03-20 12:26:02 UTC</p>
                <button class="interpret-button" data-id="2403.13541v1">Interpret</button>
                <div id="interpretation-2403.13541v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>An Ordering of Divergences for Variational Inference with Factorized Gaussian Approximations</h3>
                <p>Authors: Charles C. MargossianLoucas Pillaud-VivienLawrence K. Saul</p>
                <p><a href="http://arxiv.org/abs/2403.13748v1">Link to paper</a></p>
                <p>Given an intractable distribution p the problem of variational inferenceVI is to compute the best approximation q from some more tractable familymathcalQ. Most commonly the approximation is found by minimizing aKullback-Leibler KL divergence. However there exist other valid choices ofdivergences and when mathcalQ does not containp each divergencechampions a different solution. We analyze how the choice of divergence affectsthe outcome of VI when a Gaussian with a dense covariance matrix isapproximated by a Gaussian with a diagonal covariance matrix. In this settingwe show that different divergences can be textitordered by the amount thattheir variational approximations misestimate various measures of uncertaintysuch as the variance precision and entropy. We also derive an impossibilitytheorem showing that no two of these measures can be simultaneously matched bya factorized approximation hence the choice of divergence informs whichmeasure if any is correctly estimated. Our analysis covers the KL divergencethe Renyi divergences and a score-based divergence that compares nablalogp and nablalog q. We empirically evaluate whether these orderings holdwhen VI is used to approximate non-Gaussian distributions.</p>
                <p>Last Updated: 2024-03-20 16:56:08 UTC</p>
                <button class="interpret-button" data-id="2403.13748v1">Interpret</button>
                <div id="interpretation-2403.13748v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Probabilistic Forecasting with Stochastic Interpolants and F√∂llmer Processes</h3>
                <p>Authors: Yifan ChenMark GoldsteinMengjian HuaMichael S. AlbergoNicholas M. BoffiEric Vanden-Eijnden</p>
                <p><a href="http://arxiv.org/abs/2403.13724v1">Link to paper</a></p>
                <p>We propose a framework for probabilistic forecasting of dynamical systemsbased on generative modeling. Given observations of the system state over timewe formulate the forecasting problem as sampling from the conditionaldistribution of the future system state given its current state. To this endwe leverage the framework of stochastic interpolants which facilitates theconstruction of a generative model between an arbitrary base distribution andthe target. We design a fictitious non-physical stochastic dynamics that takesas initial condition the current system state and produces as output a samplefrom the target conditional distribution in finite time and without bias. Thisprocess therefore maps a point mass centered at the current state onto aprobabilistic ensemble of forecasts. We prove that the drift coefficiententering the stochastic differential equation SDE achieving this task isnon-singular and that it can be learned efficiently by square loss regressionover the time-series data. We show that the drift and the diffusioncoefficients of this SDE can be adjusted after training and that a specificchoice that minimizes the impact of the estimation error gives a Follmerprocess. We highlight the utility of our approach on several complexhigh-dimensional forecasting problems including stochastically forcedNavier-Stokes and video prediction on the KTH and CLEVRER datasets.</p>
                <p>Last Updated: 2024-03-20 16:33:06 UTC</p>
                <button class="interpret-button" data-id="2403.13724v1">Interpret</button>
                <div id="interpretation-2403.13724v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Does Differentially Private Synthetic Data Lead to Synthetic Discoveries?</h3>
                <p>Authors: Ileana Montoya PerezParisa MovahediValtteri NieminenAntti AirolaTapio Pahikkala</p>
                <p><a href="http://arxiv.org/abs/2403.13612v1">Link to paper</a></p>
                <p>Background: Synthetic data has been proposed as a solution for sharinganonymized versions of sensitive biomedical datasets. Ideally synthetic datashould preserve the structure and statistical properties of the original datawhile protecting the privacy of the individual subjects. Differential privacyDP is currently considered the gold standard approach for balancing thistrade-off.  Objectives: The aim of this study is to evaluate the Mann-Whitney U test onDP-synthetic biomedical data in terms of Type I and Type II errors in order toestablish whether statistical hypothesis testing performed on privacypreserving synthetic data is likely to lead to loss of tests validity ordecreased power.  Methods: We evaluate the Mann-Whitney U test on DP-synthetic data generatedfrom real-world data including a prostate cancer dataset n500 and acardiovascular dataset n70 000 as well as on data drawn from two Gaussiandistributions. Five different DP-synthetic data generation methods areevaluated including two basic DP histogram release methods and MWEMPrivate-PGM and DP GAN algorithms.  Conclusion: Most of the tested DP-synthetic data generation methods showedinflated Type I error especially at privacy budget levels of epsilonleq 1.This result calls for caution when releasing and analyzing DP-synthetic data:low p-values may be obtained in statistical tests simply as a byproduct of thenoise added to protect privacy. A DP smoothed histogram-based synthetic datageneration method was shown to produce valid Type I error for all privacylevels tested but required a large original dataset size and a modest privacybudget epsilongeq 5 in order to have reasonable Type II error levels.</p>
                <p>Last Updated: 2024-03-20 14:03:57 UTC</p>
                <button class="interpret-button" data-id="2403.13612v1">Interpret</button>
                <div id="interpretation-2403.13612v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression</h3>
                <p>Authors: Zelin HeYing SunJingyuan LiuRunze Li</p>
                <p><a href="http://arxiv.org/abs/2403.13565v1">Link to paper</a></p>
                <p>We consider the transfer learning problem in the high dimensional settingwhere the feature dimension is larger than the sample size. To learntransferable information which may vary across features or the source sampleswe propose an adaptive transfer learning method that can detect and aggregatethe feature-wise F-AdaTrans or sample-wise S-AdaTrans transferablestructures. We achieve this by employing a novel fused-penalty coupled withweights that can adapt according to the transferable structure. To choose theweight we propose a theoretically informed data-driven procedure enablingF-AdaTrans to selectively fuse the transferable signals with the target whilefiltering out non-transferable signals and S-AdaTrans to obtain the optimalcombination of information transferred from each source sample. Thenon-asymptotic rates are established which recover existing near-minimaxoptimal rates in special cases. The effectiveness of the proposed method isvalidated using both synthetic and real data.</p>
                <p>Last Updated: 2024-03-20 12:58:46 UTC</p>
                <button class="interpret-button" data-id="2403.13565v1">Interpret</button>
                <div id="interpretation-2403.13565v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Uncertainty quantification for data-driven weather models</h3>
                <p>Authors: Christopher B√ºlteNina HoratJulian QuintingSebastian Lerch</p>
                <p><a href="http://arxiv.org/abs/2403.13458v1">Link to paper</a></p>
                <p>Artificial intelligence AI-based data-driven weather forecasting modelshave experienced rapid progress over the last years. Recent studies withmodels trained on reanalysis data achieve impressive results and demonstratesubstantial improvements over state-of-the-art physics-based numerical weatherprediction models across a range of variables and evaluation metrics. Beyondimproved predictions the main advantages of data-driven weather models aretheir substantially lower computational costs and the faster generation offorecasts once a model has been trained. However most efforts in data-drivenweather forecasting have been limited to deterministic point-valuedpredictions making it impossible to quantify forecast uncertainties which iscrucial in research and for optimal decision making in applications. Ouroverarching aim is to systematically study and compare uncertaintyquantification methods to generate probabilistic weather forecasts from astate-of-the-art deterministic data-driven weather model Pangu-Weather.Specifically we compare approaches for quantifying forecast uncertainty basedon generating ensemble forecasts via perturbations to the initial conditionswith the use of statistical and machine learning methods for post-hocuncertainty quantification. In a case study on medium-range forecasts ofselected weather variables over Europe the probabilistic forecasts obtained byusing the Pangu-Weather model in concert with uncertainty quantificationmethods show promising results and provide improvements over ensemble forecastsfrom the physics-based ensemble weather model of the European Centre forMedium-Range Weather Forecasts for lead times of up to 5 days.</p>
                <p>Last Updated: 2024-03-20 10:07:51 UTC</p>
                <button class="interpret-button" data-id="2403.13458v1">Interpret</button>
                <div id="interpretation-2403.13458v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>On Pretraining Data Diversity for Self-Supervised Learning</h3>
                <p>Authors: Hasan Abed Al Kader HammoudTuhin DasFabio PizzatiPhilip TorrAdel BibiBernard Ghanem</p>
                <p><a href="http://arxiv.org/abs/2403.13808v1">Link to paper</a></p>
                <p>We explore the impact of training with more diverse datasets characterizedby the number of unique samples on the performance of self-supervised learningSSL under a fixed computational budget. Our findings consistently demonstratethat increasing pretraining data diversity enhances SSL performance albeitonly when the distribution distance to the downstream data is minimal. Notablyeven with an exceptionally large pretraining data diversity achieved throughmethods like web crawling or diffusion-generated data among other ways thedistribution shift remains a challenge. Our experiments are comprehensive withseven SSL methods using large-scale datasets such as ImageNet and YFCC100Mamounting to over 200 GPU days. Code and trained models will be available athttps://github.com/hammoudhasan/DiversitySSL .</p>
                <p>Last Updated: 2024-03-20 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2403.13808v1">Interpret</button>
                <div id="interpretation-2403.13808v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition</h3>
                <p>Authors: Ziyu LiuZeyi SunYuhang ZangWei LiPan ZhangXiaoyi DongYuanjun XiongDahua LinJiaqi Wang</p>
                <p><a href="http://arxiv.org/abs/2403.13805v1">Link to paper</a></p>
                <p>CLIP Contrastive Language-Image Pre-training uses contrastive learning fromnoise image-text pairs to excel at recognizing a wide array of candidates yetits focus on broad associations hinders the precision in distinguishing subtledifferences among fine-grained items. Conversely Multimodal Large LanguageModels MLLMs excel at classifying fine-grained categories thanks to theirsubstantial knowledge from pre-training on web-level corpora. However theperformance of MLLMs declines with an increase in category numbers primarilydue to growing complexity and constraints of limited context window size. Tosynergize the strengths of both approaches and enhance the few-shot/zero-shotrecognition abilities for datasets characterized by extensive and fine-grainedvocabularies this paper introduces RAR a Retrieving And Ranking augmentedmethod for MLLMs. We initially establish a multi-modal retriever based on CLIPto create and store explicit memory for different categories beyond theimmediate context window. During inference RAR retrieves the top-k similarresults from the memory and uses MLLMs to rank and make the final predictions.Our proposed approach not only addresses the inherent limitations infine-grained recognition but also preserves the models comprehensive knowledgebase significantly boosting accuracy across a range of vision-languagerecognition tasks. Notably our approach demonstrates a significant improvementin performance on 5 fine-grained visual recognition benchmarks 11 few-shotimage recognition datasets and the 2 object detection datasets under thezero-shot recognition setting.</p>
                <p>Last Updated: 2024-03-20 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2403.13805v1">Interpret</button>
                <div id="interpretation-2403.13805v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ZigMa: Zigzag Mamba Diffusion Model</h3>
                <p>Authors: Vincent Tao HuStefan Andreas BaumannMing GuiOlga GrebenkovaPingchuan MaJohannes FischerBjorn Ommer</p>
                <p><a href="http://arxiv.org/abs/2403.13802v1">Link to paper</a></p>
                <p>The diffusion model has long been plagued by scalability and quadraticcomplexity issues especially within transformer-based structures. In thisstudy we aim to leverage the long sequence modeling capability of aState-Space Model called Mamba to extend its applicability to visual datageneration. Firstly we identify a critical oversight in most currentMamba-based vision methods namely the lack of consideration for spatialcontinuity in the scan scheme of Mamba. Secondly building upon this insightwe introduce a simple plug-and-play zero-parameter method named Zigzag Mambawhich outperforms Mamba-based baselines and demonstrates improved speed andmemory utilization compared to transformer-based baselines. Lastly weintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigatethe scalability of the model on large-resolution visual datasets such asFacesHQ 1024times 1024 and UCF101 MultiModal-CelebA-HQ and MS COCO256times 256. Code will be released at https://taohu.me/zigma/</p>
                <p>Last Updated: 2024-03-20 17:59:14 UTC</p>
                <button class="interpret-button" data-id="2403.13802v1">Interpret</button>
                <div id="interpretation-2403.13802v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs</h3>
                <p>Authors: Yusuke MikamiAndrew MelnikJun MiuraVille Hautam√§ki</p>
                <p><a href="http://arxiv.org/abs/2403.13801v1">Link to paper</a></p>
                <p>We demonstrate experimental results with LLMs that address robotics actionplanning problems. Recently LLMs have been applied in robotics actionplanning particularly using a code generation approach that converts complexhigh-level instructions into mid-level policy codes. In contrast our approachacquires text descriptions of the task and scene objects then formulatesaction planning through natural language reasoning and outputs coordinatelevel control commands thus reducing the necessity for intermediaterepresentation code as policies. Our approach is evaluated on a multi-modalprompt simulation benchmark demonstrating that our prompt engineeringexperiments with natural language reasoning significantly enhance success ratescompared to its absence. Furthermore our approach illustrates the potentialfor natural language descriptions to transfer robotics skills from known tasksto previously unseen tasks.</p>
                <p>Last Updated: 2024-03-20 17:58:12 UTC</p>
                <button class="interpret-button" data-id="2403.13801v1">Interpret</button>
                <div id="interpretation-2403.13801v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Reverse Training to Nurse the Reversal Curse</h3>
                <p>Authors: Olga GolovnevaZeyuan Allen-ZhuJason WestonSainbayar Sukhbaatar</p>
                <p><a href="http://arxiv.org/abs/2403.13799v1">Link to paper</a></p>
                <p>Large language models LLMs have a surprising failure: when trained on Ahas a feature B they do not generalize to B is a feature of A which istermed the Reversal Curse. Even when training with trillions of tokens thisissue still appears due to Zipfs law - hence even if we train on the entireinternet. This work proposes an alternative training scheme called reversetraining whereby all words are used twice doubling the amount of availabletokens. The LLM is trained in both forward and reverse directions by reversingthe training strings while preserving i.e. not reversing chosen substringssuch as entities. We show that data-matched reverse-trained models providesuperior performance to standard models on standard tasks and compute-matchedreverse-trained models provide far superior performance on reversal taskshelping resolve the reversal curse issue.</p>
                <p>Last Updated: 2024-03-20 17:55:35 UTC</p>
                <button class="interpret-button" data-id="2403.13799v1">Interpret</button>
                <div id="interpretation-2403.13799v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>On Pretraining Data Diversity for Self-Supervised Learning</h3>
                <p>Authors: Hasan Abed Al Kader HammoudTuhin DasFabio PizzatiPhilip TorrAdel BibiBernard Ghanem</p>
                <p><a href="http://arxiv.org/abs/2403.13808v1">Link to paper</a></p>
                <p>We explore the impact of training with more diverse datasets characterizedby the number of unique samples on the performance of self-supervised learningSSL under a fixed computational budget. Our findings consistently demonstratethat increasing pretraining data diversity enhances SSL performance albeitonly when the distribution distance to the downstream data is minimal. Notablyeven with an exceptionally large pretraining data diversity achieved throughmethods like web crawling or diffusion-generated data among other ways thedistribution shift remains a challenge. Our experiments are comprehensive withseven SSL methods using large-scale datasets such as ImageNet and YFCC100Mamounting to over 200 GPU days. Code and trained models will be available athttps://github.com/hammoudhasan/DiversitySSL .</p>
                <p>Last Updated: 2024-03-20 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2403.13808v1">Interpret</button>
                <div id="interpretation-2403.13808v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Editing Massive Concepts in Text-to-Image Diffusion Models</h3>
                <p>Authors: Tianwei XiongYue WuEnze XieYue WuZhenguo LiXihui Liu</p>
                <p><a href="http://arxiv.org/abs/2403.13807v1">Link to paper</a></p>
                <p>Text-to-image diffusion models suffer from the risk of generating outdatedcopyrighted incorrect and biased content. While previous methods havemitigated the issues on a small scale it is essential to handle themsimultaneously in larger-scale real-world scenarios. We propose a two-stagemethod Editing Massive Concepts In Diffusion Models EMCID. The first stageperforms memory optimization for each individual concept with dualself-distillation from text alignment loss and diffusion noise prediction loss.The second stage conducts massive concept editing with multi-layer closed formmodel editing. We further propose a comprehensive benchmark named ImageNetConcept Editing Benchmark ICEB for evaluating massive concept editing forT2I models with two subtasks free-form prompts massive concept categoriesand extensive evaluation metrics. Extensive experiments conducted on ourproposed benchmark and previous benchmarks demonstrate the superior scalabilityof EMCID for editing up to 1000 concepts providing a practical approach forfast adjustment and re-deployment of T2I diffusion models in real-worldapplications.</p>
                <p>Last Updated: 2024-03-20 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2403.13807v1">Interpret</button>
                <div id="interpretation-2403.13807v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition</h3>
                <p>Authors: Ziyu LiuZeyi SunYuhang ZangWei LiPan ZhangXiaoyi DongYuanjun XiongDahua LinJiaqi Wang</p>
                <p><a href="http://arxiv.org/abs/2403.13805v1">Link to paper</a></p>
                <p>CLIP Contrastive Language-Image Pre-training uses contrastive learning fromnoise image-text pairs to excel at recognizing a wide array of candidates yetits focus on broad associations hinders the precision in distinguishing subtledifferences among fine-grained items. Conversely Multimodal Large LanguageModels MLLMs excel at classifying fine-grained categories thanks to theirsubstantial knowledge from pre-training on web-level corpora. However theperformance of MLLMs declines with an increase in category numbers primarilydue to growing complexity and constraints of limited context window size. Tosynergize the strengths of both approaches and enhance the few-shot/zero-shotrecognition abilities for datasets characterized by extensive and fine-grainedvocabularies this paper introduces RAR a Retrieving And Ranking augmentedmethod for MLLMs. We initially establish a multi-modal retriever based on CLIPto create and store explicit memory for different categories beyond theimmediate context window. During inference RAR retrieves the top-k similarresults from the memory and uses MLLMs to rank and make the final predictions.Our proposed approach not only addresses the inherent limitations infine-grained recognition but also preserves the models comprehensive knowledgebase significantly boosting accuracy across a range of vision-languagerecognition tasks. Notably our approach demonstrates a significant improvementin performance on 5 fine-grained visual recognition benchmarks 11 few-shotimage recognition datasets and the 2 object detection datasets under thezero-shot recognition setting.</p>
                <p>Last Updated: 2024-03-20 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2403.13805v1">Interpret</button>
                <div id="interpretation-2403.13805v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning from Models and Data for Visual Grounding</h3>
                <p>Authors: Ruozhen HePaola Cascante-BonillaZiyan YangAlexander C. BergVicente Ordonez</p>
                <p><a href="http://arxiv.org/abs/2403.13804v1">Link to paper</a></p>
                <p>We introduce SynGround a novel framework that combines data-driven learningand knowledge transfer from various large-scale pretrained models to enhancethe visual grounding capabilities of a pretrained vision-and-language model.The knowledge transfer from the models initiates the generation of imagedescriptions through an image description generator. These descriptions servedual purposes: they act as prompts for synthesizing images through atext-to-image generator and as queries for synthesizing text from whichphrases are extracted using a large language model. Finally we leverage anopen-vocabulary object detector to generate synthetic bounding boxes for thesynthetic images and texts. We finetune a pretrained vision-and-language modelon this dataset by optimizing a mask-attention consistency objective thataligns region annotations with gradient-based model explanations. The resultingmodel improves the grounding capabilities of an off-the-shelfvision-and-language model. Particularly SynGround improves the pointing gameaccuracy of ALBEF on the Flickr30k dataset from 79.38 to 87.26 and onRefCOCO Test A from 69.35 to 79.06 and on RefCOCO Test B from 53.77 to63.67.</p>
                <p>Last Updated: 2024-03-20 17:59:43 UTC</p>
                <button class="interpret-button" data-id="2403.13804v1">Interpret</button>
                <div id="interpretation-2403.13804v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ZigMa: Zigzag Mamba Diffusion Model</h3>
                <p>Authors: Vincent Tao HuStefan Andreas BaumannMing GuiOlga GrebenkovaPingchuan MaJohannes FischerBjorn Ommer</p>
                <p><a href="http://arxiv.org/abs/2403.13802v1">Link to paper</a></p>
                <p>The diffusion model has long been plagued by scalability and quadraticcomplexity issues especially within transformer-based structures. In thisstudy we aim to leverage the long sequence modeling capability of aState-Space Model called Mamba to extend its applicability to visual datageneration. Firstly we identify a critical oversight in most currentMamba-based vision methods namely the lack of consideration for spatialcontinuity in the scan scheme of Mamba. Secondly building upon this insightwe introduce a simple plug-and-play zero-parameter method named Zigzag Mambawhich outperforms Mamba-based baselines and demonstrates improved speed andmemory utilization compared to transformer-based baselines. Lastly weintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigatethe scalability of the model on large-resolution visual datasets such asFacesHQ 1024times 1024 and UCF101 MultiModal-CelebA-HQ and MS COCO256times 256. Code will be released at https://taohu.me/zigma/</p>
                <p>Last Updated: 2024-03-20 17:59:14 UTC</p>
                <button class="interpret-button" data-id="2403.13802v1">Interpret</button>
                <div id="interpretation-2403.13802v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>On Pretraining Data Diversity for Self-Supervised Learning</h3>
                <p>Authors: Hasan Abed Al Kader HammoudTuhin DasFabio PizzatiPhilip TorrAdel BibiBernard Ghanem</p>
                <p><a href="http://arxiv.org/abs/2403.13808v1">Link to paper</a></p>
                <p>We explore the impact of training with more diverse datasets characterizedby the number of unique samples on the performance of self-supervised learningSSL under a fixed computational budget. Our findings consistently demonstratethat increasing pretraining data diversity enhances SSL performance albeitonly when the distribution distance to the downstream data is minimal. Notablyeven with an exceptionally large pretraining data diversity achieved throughmethods like web crawling or diffusion-generated data among other ways thedistribution shift remains a challenge. Our experiments are comprehensive withseven SSL methods using large-scale datasets such as ImageNet and YFCC100Mamounting to over 200 GPU days. Code and trained models will be available athttps://github.com/hammoudhasan/DiversitySSL .</p>
                <p>Last Updated: 2024-03-20 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2403.13808v1">Interpret</button>
                <div id="interpretation-2403.13808v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Editing Massive Concepts in Text-to-Image Diffusion Models</h3>
                <p>Authors: Tianwei XiongYue WuEnze XieYue WuZhenguo LiXihui Liu</p>
                <p><a href="http://arxiv.org/abs/2403.13807v1">Link to paper</a></p>
                <p>Text-to-image diffusion models suffer from the risk of generating outdatedcopyrighted incorrect and biased content. While previous methods havemitigated the issues on a small scale it is essential to handle themsimultaneously in larger-scale real-world scenarios. We propose a two-stagemethod Editing Massive Concepts In Diffusion Models EMCID. The first stageperforms memory optimization for each individual concept with dualself-distillation from text alignment loss and diffusion noise prediction loss.The second stage conducts massive concept editing with multi-layer closed formmodel editing. We further propose a comprehensive benchmark named ImageNetConcept Editing Benchmark ICEB for evaluating massive concept editing forT2I models with two subtasks free-form prompts massive concept categoriesand extensive evaluation metrics. Extensive experiments conducted on ourproposed benchmark and previous benchmarks demonstrate the superior scalabilityof EMCID for editing up to 1000 concepts providing a practical approach forfast adjustment and re-deployment of T2I diffusion models in real-worldapplications.</p>
                <p>Last Updated: 2024-03-20 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2403.13807v1">Interpret</button>
                <div id="interpretation-2403.13807v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition</h3>
                <p>Authors: Ziyu LiuZeyi SunYuhang ZangWei LiPan ZhangXiaoyi DongYuanjun XiongDahua LinJiaqi Wang</p>
                <p><a href="http://arxiv.org/abs/2403.13805v1">Link to paper</a></p>
                <p>CLIP Contrastive Language-Image Pre-training uses contrastive learning fromnoise image-text pairs to excel at recognizing a wide array of candidates yetits focus on broad associations hinders the precision in distinguishing subtledifferences among fine-grained items. Conversely Multimodal Large LanguageModels MLLMs excel at classifying fine-grained categories thanks to theirsubstantial knowledge from pre-training on web-level corpora. However theperformance of MLLMs declines with an increase in category numbers primarilydue to growing complexity and constraints of limited context window size. Tosynergize the strengths of both approaches and enhance the few-shot/zero-shotrecognition abilities for datasets characterized by extensive and fine-grainedvocabularies this paper introduces RAR a Retrieving And Ranking augmentedmethod for MLLMs. We initially establish a multi-modal retriever based on CLIPto create and store explicit memory for different categories beyond theimmediate context window. During inference RAR retrieves the top-k similarresults from the memory and uses MLLMs to rank and make the final predictions.Our proposed approach not only addresses the inherent limitations infine-grained recognition but also preserves the models comprehensive knowledgebase significantly boosting accuracy across a range of vision-languagerecognition tasks. Notably our approach demonstrates a significant improvementin performance on 5 fine-grained visual recognition benchmarks 11 few-shotimage recognition datasets and the 2 object detection datasets under thezero-shot recognition setting.</p>
                <p>Last Updated: 2024-03-20 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2403.13805v1">Interpret</button>
                <div id="interpretation-2403.13805v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS</h3>
                <p>Authors: Michael NiemeyerFabian ManhardtMarie-Julie RakotosaonaMichael OechsleDaniel DuckworthRama GosulaKeisuke TatenoJohn BatesDominik KaeserFederico Tombari</p>
                <p><a href="http://arxiv.org/abs/2403.13806v1">Link to paper</a></p>
                <p>Recent advances in view synthesis and real-time rendering have achievedphotorealistic quality at impressive rendering speeds. While RadianceField-based methods achieve state-of-the-art quality in challenging scenariossuch as in-the-wild captures and large-scale scenes they often suffer fromexcessively high compute requirements linked to volumetric rendering. GaussianSplatting-based methods on the other hand rely on rasterization and naturallyachieve real-time rendering but suffer from brittle optimization heuristicsthat underperform on more challenging scenes. In this work we presentRadSplat a lightweight method for robust real-time rendering of complexscenes. Our main contributions are threefold. First we use radiance fields asa prior and supervision signal for optimizing point-based scenerepresentations leading to improved quality and more robust optimization.Next we develop a novel pruning technique reducing the overall point countwhile maintaining high quality leading to smaller and more compact scenerepresentations with faster inference speeds. Finally we propose a noveltest-time filtering approach that further accelerates rendering and allows toscale to larger house-sized scenes. We find that our method enablesstate-of-the-art synthesis of complex captures at 900 FPS.</p>
                <p>Last Updated: 2024-03-20 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2403.13806v1">Interpret</button>
                <div id="interpretation-2403.13806v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning from Models and Data for Visual Grounding</h3>
                <p>Authors: Ruozhen HePaola Cascante-BonillaZiyan YangAlexander C. BergVicente Ordonez</p>
                <p><a href="http://arxiv.org/abs/2403.13804v1">Link to paper</a></p>
                <p>We introduce SynGround a novel framework that combines data-driven learningand knowledge transfer from various large-scale pretrained models to enhancethe visual grounding capabilities of a pretrained vision-and-language model.The knowledge transfer from the models initiates the generation of imagedescriptions through an image description generator. These descriptions servedual purposes: they act as prompts for synthesizing images through atext-to-image generator and as queries for synthesizing text from whichphrases are extracted using a large language model. Finally we leverage anopen-vocabulary object detector to generate synthetic bounding boxes for thesynthetic images and texts. We finetune a pretrained vision-and-language modelon this dataset by optimizing a mask-attention consistency objective thataligns region annotations with gradient-based model explanations. The resultingmodel improves the grounding capabilities of an off-the-shelfvision-and-language model. Particularly SynGround improves the pointing gameaccuracy of ALBEF on the Flickr30k dataset from 79.38 to 87.26 and onRefCOCO Test A from 69.35 to 79.06 and on RefCOCO Test B from 53.77 to63.67.</p>
                <p>Last Updated: 2024-03-20 17:59:43 UTC</p>
                <button class="interpret-button" data-id="2403.13804v1">Interpret</button>
                <div id="interpretation-2403.13804v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Hyper Strategy Logic</h3>
                <p>Authors: Raven BeutnerBernd Finkbeiner</p>
                <p><a href="http://arxiv.org/abs/2403.13741v1">Link to paper</a></p>
                <p>Strategy logic SL is a powerful temporal logic that enables strategicreasoning in multi-agent systems. SL supports explicit first-orderquantification over strategies and provides a logical framework to express manyimportant properties such as Nash equilibria dominant strategies etc. Whilein SL the same strategy can be used in multiple strategy profiles each suchprofile is evaluated w.r.t. a path-property i.e. a property that considersthe single path resulting from a particular strategic interaction. In thispaper we present Hyper Strategy Logic HyperSL a strategy logic where theoutcome of multiple strategy profiles can be compared w.r.t. a hyperpropertyi.e. a property that relates multiple paths. We show that HyperSL can captureimportant properties that cannot be expressed in SL includingnon-interference quantitative Nash equilibria optimal adversarial planningand reasoning under imperfect information. On the algorithmic side we identifyan expressive fragment of HyperSL with decidable model checking and present amodel-checking algorithm. We contribute a prototype implementation of ouralgorithm and report on encouraging experimental results.</p>
                <p>Last Updated: 2024-03-20 16:47:53 UTC</p>
                <button class="interpret-button" data-id="2403.13741v1">Interpret</button>
                <div id="interpretation-2403.13741v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Agent-based MST Construction</h3>
                <p>Authors: Ajay D. KshemkalyaniManish KumarAnisur Rahaman MollaGokarna Sharma</p>
                <p><a href="http://arxiv.org/abs/2403.13716v1">Link to paper</a></p>
                <p>em Minimum-weight spanning tree MST is one of the fundamental andwell-studied problems in distributed computing. In this paper we initiate thestudy of constructing MST using mobile agents aka robots. Suppose n agentsare positioned initially arbitrarily on the nodes of a connected undirectedarbitrary anonymous port-labeled weighted n-node m-edge graph G ofdiameter D and maximum degree Delta. The agents relocate themselvesautonomously and compute an MST of G such that exactly one agent positions ona node and tracks in its memory which of its adjacent edges belong to the MST.The objective is to minimize time and memory requirements. Following theliterature we consider the synchronous setting in which each agent performsits operations synchronously with others and hence time can be measured inrounds. We first establish a generic result: if n and Delta are known apriori and memory per agent is as much as node memory in the message-passingmodel of distributed computing agents can simulate any OT-rounddeterministic algorithm for any problem in the message-passing model to theagent model in ODelta T log nnlog2n rounds. As a corollary MST can beconstructed in the agent model in OmaxDelta sqrtn log n lognDelta D log nnlog2n rounds simulating the celebrated Osqrtnlogn D-round GKP algorithm for MST in the message-passing model. We thenestablish that without knowing any graph parameter a priori there exists adeterministic algorithm to construct MST in the agent model in Omnlog nrounds with On log n bits memory at each agent. The presented algorithmneeds to overcome highly non-trivial challenges on how to synchronize agents incomputing MST as they may initially be positioned arbitrarily on the graphnodes.</p>
                <p>Last Updated: 2024-03-20 16:22:53 UTC</p>
                <button class="interpret-button" data-id="2403.13716v1">Interpret</button>
                <div id="interpretation-2403.13716v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-agent Reinforcement Traffic Signal Control based on Interpretable Influence Mechanism and Biased ReLU Approximation</h3>
                <p>Authors: Zhiyue LuoJun XuFanglin Chen</p>
                <p><a href="http://arxiv.org/abs/2403.13639v1">Link to paper</a></p>
                <p>Traffic signal control is important in intelligent transportation system ofwhich cooperative control is difficult to realize but yet vital. Many methodsmodel multi-intersection traffic networks as grids and address the problemusing multi-agent reinforcement learning RL. Despite these existing studiesthere is an opportunity to further enhance our understanding of theconnectivity and globality of the traffic networks by capturing thespatiotemporal traffic information with efficient neural networks in deep RL.In this paper we propose a novel multi-agent actor-critic framework based onan interpretable influence mechanism with a centralized learning anddecentralized execution method. Specifically we first construct anactor-critic framework for which the piecewise linear neural network PWLNNnamed biased ReLU BReLU is used as the function approximator to obtain amore accurate and theoretically grounded approximation. Finally our proposedframework is validated on two synthetic traffic networks to coordinate signalcontrol between intersections achieving lower traffic delays across the entiretraffic network compared to state-of-the-art SOTA performance.</p>
                <p>Last Updated: 2024-03-20 14:43:23 UTC</p>
                <button class="interpret-button" data-id="2403.13639v1">Interpret</button>
                <div id="interpretation-2403.13639v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Caching-Augmented Lifelong Multi-Agent Path Finding</h3>
                <p>Authors: Yimin TangZhenghong YuYi ZhengT. K. Satish KumarJiaoyang LiSven Koenig</p>
                <p><a href="http://arxiv.org/abs/2403.13421v1">Link to paper</a></p>
                <p>Multi-Agent Path Finding MAPF which involves finding collision-free pathsfor multiple robots is crucial in various applications. Lifelong MAPF wheretargets are reassigned to agents as soon as they complete their initialobjectives offers a more accurate approximation of real-world warehouseplanning. In this paper we present a novel mechanism named Caching-AugmentedLifelong MAPF CAL-MAPF designed to improve the performance of Lifelong MAPF.We have developed a new map grid type called cache for temporary item storageand replacement and designed a lock mechanism for it to improve the stabilityof the planning solution. This cache mechanism was evaluated using variouscache replacement policies and a spectrum of input task distributions. Weidentified three main factors significantly impacting CAL-MAPF performancethrough experimentation: suitable input task distribution high cache hit rateand smooth traffic. Overall CAL-MAPF has demonstrated potential forperformance improvements in certain task distributions maps and agentconfigurations.</p>
                <p>Last Updated: 2024-03-20 09:07:23 UTC</p>
                <button class="interpret-button" data-id="2403.13421v1">Interpret</button>
                <div id="interpretation-2403.13421v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Causal Graph Dynamics and Kan Extensions</h3>
                <p>Authors: Luidnel MaignanAntoine Spicher</p>
                <p><a href="http://arxiv.org/abs/2403.13393v1">Link to paper</a></p>
                <p>On the one side the formalism of Global Transformations comes with the claimof capturing any transformation of space that is local synchronous anddeterministic.The claim has been proven for different classes of models such asmesh refinements from computer graphics Lindenmayer systems from morphogenesismodeling and cellular automata from biological physical and parallelcomputation modeling.The Global Transformation formalism achieves this by usingcategory theory for its genericity and more precisely the notion of Kanextension to determine the global behaviors based on the local ones.On theother side Causal Graph Dynamics describe the transformation of port graphs ina synchronous and deterministic way and has not yet being tackled.In thispaper we show the precise sense in which the claim of Global Transformationsholds for them as well.This is done by showing different ways in which they canbe expressed as Kan extensions each of them highlighting different features ofCausal Graph Dynamics.Along the way this work uncovers the interesting classof Monotonic Causal Graph Dynamics and their universality among General CausalGraph Dynamics.</p>
                <p>Last Updated: 2024-03-20 08:35:14 UTC</p>
                <button class="interpret-button" data-id="2403.13393v1">Interpret</button>
                <div id="interpretation-2403.13393v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-03-21</p>
        </div>
    
        </div>
    </body>
    </html>
    