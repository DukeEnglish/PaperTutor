
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting 3D Objects with Realistic Materials</h3>
                <p>Authors: Ye FangZeyi SunTong WuJiaqi WangZiwei LiuGordon WetzsteinDahua Lin</p>
                <p><a href="http://arxiv.org/abs/2404.16829v1">Link to paper</a></p>
                <p>Physically realistic materials are pivotal in augmenting the realism of 3Dassets across various applications and lighting conditions. However existing3D assets and generative models often lack authentic material properties.Manual assignment of materials using graphic software is a tedious andtime-consuming task. In this paper we exploit advancements in Multimodal LargeLanguage Models MLLMs particularly GPT-4V to present a novel approachMake-it-Real: 1 We demonstrate that GPT-4V can effectively recognize anddescribe materials allowing the construction of a detailed material library.2 Utilizing a combination of visual cues and hierarchical text prompts GPT-4Vprecisely identifies and aligns materials with the corresponding components of3D objects. 3 The correctly matched materials are then meticulously applied asreference for the new SVBRDF material generation according to the originaldiffuse map significantly enhancing their visual authenticity. Make-it-Realoffers a streamlined integration into the 3D content creation workflowshowcasing its utility as an essential tool for developers of 3D assets.</p>
                <p>Last Updated: 2024-04-25 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2404.16829v1">Interpret</button>
                <div id="interpretation-2404.16829v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning Visuotactile Skills with Two Multifingered Hands</h3>
                <p>Authors: Toru LinYu ZhangQiyang LiHaozhi QiBrent YiSergey LevineJitendra Malik</p>
                <p><a href="http://arxiv.org/abs/2404.16823v1">Link to paper</a></p>
                <p>Aiming to replicate human-like dexterity perceptual experiences and motionpatterns we explore learning from human demonstrations using a bimanual systemwith multifingered hands and visuotactile data. Two significant challengesexist: the lack of an affordable and accessible teleoperation system suitablefor a dual-arm setup with multifingered hands and the scarcity ofmultifingered hand hardware equipped with touch sensing. To tackle the firstchallenge we develop HATO a low-cost hands-arms teleoperation system thatleverages off-the-shelf electronics complemented with a software suite thatenables efficient data collection the comprehensive software suite alsosupports multimodal data processing scalable policy learning and smoothpolicy deployment. To tackle the latter challenge we introduce a novelhardware adaptation by repurposing two prosthetic hands equipped with touchsensors for research. Using visuotactile data collected from our system welearn skills to complete long-horizon high-precision tasks which are difficultto achieve without multifingered dexterity and touch feedback. Furthermore weempirically investigate the effects of dataset size sensing modality andvisual input preprocessing on policy learning. Our results mark a promisingstep forward in bimanual multifingered manipulation from visuotactile data.Videos code and datasets can be found at https://toruowo.github.io/hato/ .</p>
                <p>Last Updated: 2024-04-25 17:59:41 UTC</p>
                <button class="interpret-button" data-id="2404.16823v1">Interpret</button>
                <div id="interpretation-2404.16823v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Make Your LLM Fully Utilize the Context</h3>
                <p>Authors: Shengnan AnZexiong MaZeqi LinNanning ZhengJian-Guang Lou</p>
                <p><a href="http://arxiv.org/abs/2404.16811v1">Link to paper</a></p>
                <p>While many contemporary large language models LLMs can process lengthyinput they still struggle to fully utilize information within the longcontext known as the lost-in-the-middle challenge. We hypothesize that itstems from insufficient explicit supervision during the long-context trainingwhich fails to emphasize that any position in a long context can hold crucialinformation. Based on this intuition our study presents information-intensiveIN2 training a purely data-driven solution to overcome lost-in-the-middle.Specifically IN2 training leverages a synthesized long-context question-answerdataset where the answer requires 1 fine-grained information awareness on ashort segment 128 tokens within a synthesized long context 4K-32K tokensand 2 the integration and reasoning of information from two or more shortsegments. Through applying this information-intensive training on Mistral-7Bwe present FILM-7B FILl-in-the-Middle. To thoroughly assess the ability ofFILM-7B for utilizing long contexts we design three probing tasks thatencompass various context styles document code and structured-data contextand information retrieval patterns forward backward and bi-directionalretrieval. The probing results demonstrate that FILM-7B can robustly retrieveinformation from different positions in its 32K context window. Beyond theseprobing tasks FILM-7B significantly improves the performance on real-worldlong-context tasks e.g. 23.5-26.9 F1 score on NarrativeQA whilemaintaining a comparable performance on short-context tasks e.g. 59.3-59.2accuracy on MMLU. Github Link: https://github.com/microsoft/FILM.</p>
                <p>Last Updated: 2024-04-25 17:55:14 UTC</p>
                <button class="interpret-button" data-id="2404.16811v1">Interpret</button>
                <div id="interpretation-2404.16811v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AAPL: Adding Attributes to Prompt Learning for Vision-Language Models</h3>
                <p>Authors: Gahyeon KimSohee KimSeokju Lee</p>
                <p><a href="http://arxiv.org/abs/2404.16804v1">Link to paper</a></p>
                <p>Recent advances in large pre-trained vision-language models have demonstratedremarkable performance on zero-shot downstream tasks. Building upon thisrecent studies such as CoOp and CoCoOp have proposed the use of promptlearning where context within a prompt is replaced with learnable vectorsleading to significant improvements over manually crafted prompts. However theperformance improvement for unseen classes is still marginal and to tacklethis problem data augmentation has been frequently used in traditionalzero-shot learning techniques. Through our experiments we have identifiedimportant issues in CoOp and CoCoOp: the context learned through traditionalimage augmentation is biased toward seen classes negatively impactinggeneralization to unseen classes. To address this problem we proposeadversarial token embedding to disentangle low-level visual augmentationfeatures from high-level class information when inducing bias in learnableprompts. Through our novel mechanism called Adding Attributes to PromptLearning AAPL we guide the learnable context to effectively extract textfeatures by focusing on high-level features for unseen classes. We haveconducted experiments across 11 datasets and overall AAPL shows favorableperformances compared to the existing methods in few-shot learning zero-shotlearning cross-dataset and domain generalization tasks.</p>
                <p>Last Updated: 2024-04-25 17:51:10 UTC</p>
                <button class="interpret-button" data-id="2404.16804v1">Interpret</button>
                <div id="interpretation-2404.16804v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Weak-to-Strong Extrapolation Expedites Alignment</h3>
                <p>Authors: Chujie ZhengZiqi WangHeng JiMinlie HuangNanyun Peng</p>
                <p><a href="http://arxiv.org/abs/2404.16792v1">Link to paper</a></p>
                <p>Although the capabilities of large language models LLMs ideally scale upwith increasing data and compute they are inevitably constrained by limitedresources in reality. Suppose we have a moderately trained LLM e.g. trainedto align with human preference in hand can we further exploit its potentialand cheaply acquire a stronger model In this paper we propose a simple methodcalled ExPO to boost LLMs alignment with human preference. ExPO assumes that amedium-aligned model can be interpolated between a less-aligned weaker modele.g. the initial SFT model and a better-aligned stronger one therebydirectly obtaining this stronger model by extrapolating from the weights of theformer two relatively weaker models. On the AlpacaEval 2.0 benchmark we showthat ExPO pushes models trained with less preference data e.g. 10 or 20 toreach and even surpass the fully-trained one without any additional training.Furthermore ExPO also significantly improves off-the-shelf DPO/RLHF models andexhibits decent scalability across model sizes from 7B to 70B. Our workdemonstrates the efficacy of model extrapolation in exploiting LLMscapabilities suggesting a promising direction that deserves futureexploration.</p>
                <p>Last Updated: 2024-04-25 17:39:50 UTC</p>
                <button class="interpret-button" data-id="2404.16792v1">Interpret</button>
                <div id="interpretation-2404.16792v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Estimating the Number of Components in Finite Mixture Models via Variational Approximation</h3>
                <p>Authors: Chenyang WangYun Yang</p>
                <p><a href="http://arxiv.org/abs/2404.16746v1">Link to paper</a></p>
                <p>This work introduces a new method for selecting the number of components infinite mixture models FMMs using variational Bayes inspired by thelarge-sample properties of the Evidence Lower Bound ELBO derived frommean-field MF variational approximation. Specifically we establish matchingupper and lower bounds for the ELBO without assuming conjugate priorssuggesting the consistency of model selection for FMMs based on maximizing theELBO. As a by-product of our proof we demonstrate that the MF approximationinherits the stable behavior benefited from model singularity of theposterior distribution which tends to eliminate the extra components undermodel misspecification where the number of mixture components isover-specified. This stable behavior also leads to the n-1/2 convergencerate for parameter estimation up to a logarithmic factor under this modeloverspecification. Empirical experiments are conducted to validate ourtheoretical findings and compare with other state-of-the-art methods forselecting the number of components in FMMs.</p>
                <p>Last Updated: 2024-04-25 17:00:24 UTC</p>
                <button class="interpret-button" data-id="2404.16746v1">Interpret</button>
                <div id="interpretation-2404.16746v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Conformalized Ordinal Classification with Marginal and Conditional Coverage</h3>
                <p>Authors: Subhrasish ChakrabortyChhavi TyagiHaiyan QiaoWenge Guo</p>
                <p><a href="http://arxiv.org/abs/2404.16610v1">Link to paper</a></p>
                <p>Conformal prediction is a general distribution-free approach for constructingprediction sets combined with any machine learning algorithm that achieve validmarginal or conditional coverage in finite samples. Ordinal classification iscommon in real applications where the target variable has natural orderingamong the class labels. In this paper we discuss constructingdistribution-free prediction sets for such ordinal classification problems byleveraging the ideas of conformal prediction and multiple testing with FWERcontrol. Newer conformal prediction methods are developed for constructingcontiguous and non-contiguous prediction sets based on marginal and conditionalclass-specific conformal p-values respectively. Theoretically we provethat the proposed methods respectively achieve satisfactory levels of marginaland class-specific conditional coverages. Through simulation study and realdata analysis these proposed methods show promising performance compared tothe existing conformal method.</p>
                <p>Last Updated: 2024-04-25 13:49:59 UTC</p>
                <button class="interpret-button" data-id="2404.16610v1">Interpret</button>
                <div id="interpretation-2404.16610v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Automated Model Selection for Generalized Linear Models</h3>
                <p>Authors: Benjamin SchwendingerFlorian SchwendingerLaura Vana-Gür</p>
                <p><a href="http://arxiv.org/abs/2404.16560v1">Link to paper</a></p>
                <p>In this paper we show how mixed-integer conic optimization can be used tocombine feature subset selection with holistic generalized linear models tofully automate the model selection process. Concretely we directly optimizefor the Akaike and Bayesian information criteria while imposing constraintsdesigned to deal with multicollinearity in the feature selection task.Specifically we propose a novel pairwise correlation constraint that combinesthe sign coherence constraint with ideas from classical statistical models likeRidge regression and the OSCAR model.</p>
                <p>Last Updated: 2024-04-25 12:16:58 UTC</p>
                <button class="interpret-button" data-id="2404.16560v1">Interpret</button>
                <div id="interpretation-2404.16560v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Automating the Discovery of Partial Differential Equations in Dynamical Systems</h3>
                <p>Authors: Weizhen LiRui Carvalho</p>
                <p><a href="http://arxiv.org/abs/2404.16444v1">Link to paper</a></p>
                <p>Identifying partial differential equations PDEs from data is crucial forunderstanding the governing mechanisms of natural phenomena yet it remains achallenging task. We present an extension to the ARGOS framework ARGOS-RALwhich leverages sparse regression with the recurrent adaptive lasso to identifyPDEs from limited prior knowledge automatically. Our method automatescalculating partial derivatives constructing a candidate library andestimating a sparse model. We rigorously evaluate the performance of ARGOS-RALin identifying canonical PDEs under various noise levels and sample sizesdemonstrating its robustness in handling noisy and non-uniformly distributeddata. We also test the algorithms performance on datasets consisting solely ofrandom noise to simulate scenarios with severely compromised data quality. Ourresults show that ARGOS-RAL effectively and reliably identifies the underlyingPDEs from data outperforming the sequential threshold ridge regression methodin most cases. We highlight the potential of combining statistical methodsmachine learning and dynamical systems theory to automatically discovergoverning equations from collected data streamlining the scientific modelingprocess.</p>
                <p>Last Updated: 2024-04-25 09:23:03 UTC</p>
                <button class="interpret-button" data-id="2404.16444v1">Interpret</button>
                <div id="interpretation-2404.16444v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Distributionally Robust Safe Screening</h3>
                <p>Authors: Hiroyuki HanadaSatoshi AkahaneTatsuya AoyamaTomonari TanakaYoshito OkuraYu InatsuNoriaki HashimotoTaro MurayamaLee HanjuShinya KojimaIchiro Takeuchi</p>
                <p><a href="http://arxiv.org/abs/2404.16328v1">Link to paper</a></p>
                <p>In this study we propose a method Distributionally Robust Safe ScreeningDRSS for identifying unnecessary samples and features within a DR covariateshift setting. This method effectively combines DR learning a paradigm aimedat enhancing model robustness against variations in data distribution withsafe screening SS a sparse optimization technique designed to identifyirrelevant samples and features prior to model training. The core concept ofthe DRSS method involves reformulating the DR covariate-shift problem as aweighted empirical risk minimization problem where the weights are subject touncertainty within a predetermined range. By extending the SS technique toaccommodate this weight uncertainty the DRSS method is capable of reliablyidentifying unnecessary samples and features under any future distributionwithin a specified range. We provide a theoretical guarantee of the DRSS methodand validate its performance through numerical experiments on both syntheticand real-world datasets.</p>
                <p>Last Updated: 2024-04-25 04:29:25 UTC</p>
                <button class="interpret-button" data-id="2404.16328v1">Interpret</button>
                <div id="interpretation-2404.16328v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting 3D Objects with Realistic Materials</h3>
                <p>Authors: Ye FangZeyi SunTong WuJiaqi WangZiwei LiuGordon WetzsteinDahua Lin</p>
                <p><a href="http://arxiv.org/abs/2404.16829v1">Link to paper</a></p>
                <p>Physically realistic materials are pivotal in augmenting the realism of 3Dassets across various applications and lighting conditions. However existing3D assets and generative models often lack authentic material properties.Manual assignment of materials using graphic software is a tedious andtime-consuming task. In this paper we exploit advancements in Multimodal LargeLanguage Models MLLMs particularly GPT-4V to present a novel approachMake-it-Real: 1 We demonstrate that GPT-4V can effectively recognize anddescribe materials allowing the construction of a detailed material library.2 Utilizing a combination of visual cues and hierarchical text prompts GPT-4Vprecisely identifies and aligns materials with the corresponding components of3D objects. 3 The correctly matched materials are then meticulously applied asreference for the new SVBRDF material generation according to the originaldiffuse map significantly enhancing their visual authenticity. Make-it-Realoffers a streamlined integration into the 3D content creation workflowshowcasing its utility as an essential tool for developers of 3D assets.</p>
                <p>Last Updated: 2024-04-25 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2404.16829v1">Interpret</button>
                <div id="interpretation-2404.16829v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages</h3>
                <p>Authors: Harman SinghNitish GuptaShikhar BharadwajDinesh TewariPartha Talukdar</p>
                <p><a href="http://arxiv.org/abs/2404.16816v1">Link to paper</a></p>
                <p>As large language models LLMs see increasing adoption across the globe itis imperative for LLMs to be representative of the linguistic diversity of theworld. India is a linguistically diverse country of 1.4 Billion people. Tofacilitate research on multilingual LLM evaluation we release IndicGenBench -the largest benchmark for evaluating LLMs on user-facing generation tasksacross a diverse set 29 of Indic languages covering 13 scripts and 4 languagefamilies. IndicGenBench is composed of diverse generation tasks likecross-lingual summarization machine translation and cross-lingual questionanswering. IndicGenBench extends existing benchmarks to many Indic languagesthrough human curation providing multi-way parallel evaluation data for manyunder-represented Indic languages for the first time. We evaluate a wide rangeof proprietary and open-source LLMs including GPT-3.5 GPT-4 PaLM-2 mT5Gemma BLOOM and LLaMA on IndicGenBench in a variety of settings. The largestPaLM-2 models performs the best on most tasks however there is a significantperformance gap in all languages compared to English showing that furtherresearch is needed for the development of more inclusive multilingual languagemodels. IndicGenBench is released atwww.github.com/google-research-datasets/indic-gen-bench</p>
                <p>Last Updated: 2024-04-25 17:57:36 UTC</p>
                <button class="interpret-button" data-id="2404.16816v1">Interpret</button>
                <div id="interpretation-2404.16816v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Make Your LLM Fully Utilize the Context</h3>
                <p>Authors: Shengnan AnZexiong MaZeqi LinNanning ZhengJian-Guang Lou</p>
                <p><a href="http://arxiv.org/abs/2404.16811v1">Link to paper</a></p>
                <p>While many contemporary large language models LLMs can process lengthyinput they still struggle to fully utilize information within the longcontext known as the lost-in-the-middle challenge. We hypothesize that itstems from insufficient explicit supervision during the long-context trainingwhich fails to emphasize that any position in a long context can hold crucialinformation. Based on this intuition our study presents information-intensiveIN2 training a purely data-driven solution to overcome lost-in-the-middle.Specifically IN2 training leverages a synthesized long-context question-answerdataset where the answer requires 1 fine-grained information awareness on ashort segment 128 tokens within a synthesized long context 4K-32K tokensand 2 the integration and reasoning of information from two or more shortsegments. Through applying this information-intensive training on Mistral-7Bwe present FILM-7B FILl-in-the-Middle. To thoroughly assess the ability ofFILM-7B for utilizing long contexts we design three probing tasks thatencompass various context styles document code and structured-data contextand information retrieval patterns forward backward and bi-directionalretrieval. The probing results demonstrate that FILM-7B can robustly retrieveinformation from different positions in its 32K context window. Beyond theseprobing tasks FILM-7B significantly improves the performance on real-worldlong-context tasks e.g. 23.5-26.9 F1 score on NarrativeQA whilemaintaining a comparable performance on short-context tasks e.g. 59.3-59.2accuracy on MMLU. Github Link: https://github.com/microsoft/FILM.</p>
                <p>Last Updated: 2024-04-25 17:55:14 UTC</p>
                <button class="interpret-button" data-id="2404.16811v1">Interpret</button>
                <div id="interpretation-2404.16811v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning</h3>
                <p>Authors: Tianhui ZhangBei PengDanushka Bollegala</p>
                <p><a href="http://arxiv.org/abs/2404.16807v1">Link to paper</a></p>
                <p>Generative Commonsense Reasoning GCR requires a model to reason about asituation using commonsense knowledge while generating coherent sentences.Although the quality of the generated sentences is crucial the diversity ofthe generation is equally important because it reflects the models ability touse a range of commonsense knowledge facts. Large Language Models LLMs haveshown proficiency in enhancing the generation quality across various tasksthrough in-context learning ICL using given examples without the need for anyfine-tuning. However the diversity aspect in LLM outputs has not beensystematically studied before. To address this we propose a simple method thatdiversifies the LLM generations while preserving their quality. Experimentalresults on three benchmark GCR datasets show that our method achieves an idealbalance between the quality and diversity. Moreover the sentences generated byour proposed method can be used as training data to improve diversity inexisting commonsense generators.</p>
                <p>Last Updated: 2024-04-25 17:52:39 UTC</p>
                <button class="interpret-button" data-id="2404.16807v1">Interpret</button>
                <div id="interpretation-2404.16807v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Weak-to-Strong Extrapolation Expedites Alignment</h3>
                <p>Authors: Chujie ZhengZiqi WangHeng JiMinlie HuangNanyun Peng</p>
                <p><a href="http://arxiv.org/abs/2404.16792v1">Link to paper</a></p>
                <p>Although the capabilities of large language models LLMs ideally scale upwith increasing data and compute they are inevitably constrained by limitedresources in reality. Suppose we have a moderately trained LLM e.g. trainedto align with human preference in hand can we further exploit its potentialand cheaply acquire a stronger model In this paper we propose a simple methodcalled ExPO to boost LLMs alignment with human preference. ExPO assumes that amedium-aligned model can be interpolated between a less-aligned weaker modele.g. the initial SFT model and a better-aligned stronger one therebydirectly obtaining this stronger model by extrapolating from the weights of theformer two relatively weaker models. On the AlpacaEval 2.0 benchmark we showthat ExPO pushes models trained with less preference data e.g. 10 or 20 toreach and even surpass the fully-trained one without any additional training.Furthermore ExPO also significantly improves off-the-shelf DPO/RLHF models andexhibits decent scalability across model sizes from 7B to 70B. Our workdemonstrates the efficacy of model extrapolation in exploiting LLMscapabilities suggesting a promising direction that deserves futureexploration.</p>
                <p>Last Updated: 2024-04-25 17:39:50 UTC</p>
                <button class="interpret-button" data-id="2404.16792v1">Interpret</button>
                <div id="interpretation-2404.16792v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>The Third Monocular Depth Estimation Challenge</h3>
                <p>Authors: Jaime SpencerFabio TosiMatteo PoggiRipudaman Singh AroraChris RussellSimon HadfieldRichard BowdenGuangYuan ZhouZhengXin LiQiang RaoYiPing BaoXiao LiuDohyeong KimJinseong KimMyunghyun KimMykola LavreniukRui LiQing MaoJiang WuYu ZhuJinqiu SunYanning ZhangSuraj PatniAradhye AgarwalChetan AroraPihai SunKui JiangGang WuJian LiuXianming LiuJunjun JiangXidan ZhangJianing WeiFangjun WangZhiming TanJiabao WangAlbert LuginovMuhammad ShahzadSeyed HosseiniAleksander TrajcevskiJames H. Elder</p>
                <p><a href="http://arxiv.org/abs/2404.16831v1">Link to paper</a></p>
                <p>This paper discusses the results of the third edition of the Monocular DepthEstimation Challenge MDEC. The challenge focuses on zero-shot generalizationto the challenging SYNS-Patches dataset featuring complex scenes in naturaland indoor settings. As with the previous edition methods can use any form ofsupervision i.e. supervised or self-supervised. The challenge received a totalof 19 submissions outperforming the baseline on the test set: 10 among themsubmitted a report describing their approach highlighting a diffused use offoundational models such as Depth Anything at the core of their method. Thechallenge winners drastically improved 3D F-Score performance from 17.51 to23.72.</p>
                <p>Last Updated: 2024-04-25 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2404.16831v1">Interpret</button>
                <div id="interpretation-2404.16831v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting 3D Objects with Realistic Materials</h3>
                <p>Authors: Ye FangZeyi SunTong WuJiaqi WangZiwei LiuGordon WetzsteinDahua Lin</p>
                <p><a href="http://arxiv.org/abs/2404.16829v1">Link to paper</a></p>
                <p>Physically realistic materials are pivotal in augmenting the realism of 3Dassets across various applications and lighting conditions. However existing3D assets and generative models often lack authentic material properties.Manual assignment of materials using graphic software is a tedious andtime-consuming task. In this paper we exploit advancements in Multimodal LargeLanguage Models MLLMs particularly GPT-4V to present a novel approachMake-it-Real: 1 We demonstrate that GPT-4V can effectively recognize anddescribe materials allowing the construction of a detailed material library.2 Utilizing a combination of visual cues and hierarchical text prompts GPT-4Vprecisely identifies and aligns materials with the corresponding components of3D objects. 3 The correctly matched materials are then meticulously applied asreference for the new SVBRDF material generation according to the originaldiffuse map significantly enhancing their visual authenticity. Make-it-Realoffers a streamlined integration into the 3D content creation workflowshowcasing its utility as an essential tool for developers of 3D assets.</p>
                <p>Last Updated: 2024-04-25 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2404.16829v1">Interpret</button>
                <div id="interpretation-2404.16829v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Made to Order: Discovering monotonic temporal changes via self-supervised video ordering</h3>
                <p>Authors: Charig YangWeidi XieAndrew Zisserman</p>
                <p><a href="http://arxiv.org/abs/2404.16828v1">Link to paper</a></p>
                <p>Our objective is to discover and localize monotonic temporal changes in asequence of images. To achieve this we exploit a simple proxy task of orderinga shuffled image sequence with time serving as a supervisory signal sinceonly changes that are monotonic with time can give rise to the correctordering. We also introduce a flexible transformer-based model forgeneral-purpose ordering of image sequences of arbitrary length with built-inattribution maps. After training the model successfully discovers andlocalizes monotonic changes while ignoring cyclic and stochastic ones. Wedemonstrate applications of the model in multiple video settings coveringdifferent scene and object types discovering both object-level andenvironmental changes in unseen sequences. We also demonstrate that theattention-based attribution maps function as effective prompts for segmentingthe changing regions and that the learned representations can be used fordownstream applications. Finally we show that the model achieves the state ofthe art on standard benchmarks for ordering a set of images.</p>
                <p>Last Updated: 2024-04-25 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2404.16828v1">Interpret</button>
                <div id="interpretation-2404.16828v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ResVR: Joint Rescaling and Viewport Rendering of Omnidirectional Images</h3>
                <p>Authors: Weiqi LiShijie ZhaoBin ChenXinhua ChengJunlin LiLi ZhangJian Zhang</p>
                <p><a href="http://arxiv.org/abs/2404.16825v1">Link to paper</a></p>
                <p>With the advent of virtual reality technology omnidirectional image ODIrescaling techniques are increasingly embraced for reducing transmitted andstored file sizes while preserving high image quality. Despite this progresscurrent ODI rescaling methods predominantly focus on enhancing the quality ofimages in equirectangular projection ERP format which overlooks the factthat the content viewed on head mounted displays HMDs is actually a renderedviewport instead of an ERP image. In this work we emphasize that focusingsolely on ERP quality results in inferior viewport visual experiences forusers. Thus we propose ResVR which is the first comprehensive framework forthe joint Rescaling and Viewport Rendering of ODIs. ResVR allows obtaining LRERP images for transmission while rendering high-quality viewports for users towatch on HMDs. In our ResVR a novel discrete pixel sampling strategy isdeveloped to tackle the complex mapping between the viewport and ERP enablingend-to-end training of ResVR pipeline. Furthermore a spherical pixel shaperepresentation technique is innovatively derived from spherical differentiationto significantly improve the visual quality of rendered viewports. Extensiveexperiments demonstrate that our ResVR outperforms existing methods in viewportrendering tasks across different fields of view resolutions and viewdirections while keeping a low transmission overhead.</p>
                <p>Last Updated: 2024-04-25 17:59:46 UTC</p>
                <button class="interpret-button" data-id="2404.16825v1">Interpret</button>
                <div id="interpretation-2404.16825v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation Localization and Copyright Protection</h3>
                <p>Authors: Xuanyu ZhangYoumin XuRunyi LiJiwen YuWeiqi LiZhipei XuJian Zhang</p>
                <p><a href="http://arxiv.org/abs/2404.16824v1">Link to paper</a></p>
                <p>AI-generated video has revolutionized short video production filmmaking andpersonalized media making video local editing an essential tool. However thisprogress also blurs the line between reality and fiction posing challenges inmultimedia forensics. To solve this urgent issue V2A-Mark is proposed toaddress the limitations of current video tampering forensics such as poorgeneralizability singular function and single modality focus. Combining thefragility of video-into-video steganography with deep robust watermarking ourmethod can embed invisible visual-audio localization watermarks and copyrightwatermarks into the original video frames and audio enabling precisemanipulation localization and copyright protection. We also design a temporalalignment and fusion module and degradation prompt learning to enhance thelocalization accuracy and decoding robustness. Meanwhile we introduce asample-level audio localization method and a cross-modal copyright extractionmechanism to couple the information of audio and video frames. Theeffectiveness of V2A-Mark has been verified on a visual-audio tamperingdataset emphasizing its superiority in localization precision and copyrightaccuracy crucial for the sustainable development of video editing in the AIGCvideo era.</p>
                <p>Last Updated: 2024-04-25 17:59:45 UTC</p>
                <button class="interpret-button" data-id="2404.16824v1">Interpret</button>
                <div id="interpretation-2404.16824v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Made to Order: Discovering monotonic temporal changes via self-supervised video ordering</h3>
                <p>Authors: Charig YangWeidi XieAndrew Zisserman</p>
                <p><a href="http://arxiv.org/abs/2404.16828v1">Link to paper</a></p>
                <p>Our objective is to discover and localize monotonic temporal changes in asequence of images. To achieve this we exploit a simple proxy task of orderinga shuffled image sequence with time serving as a supervisory signal sinceonly changes that are monotonic with time can give rise to the correctordering. We also introduce a flexible transformer-based model forgeneral-purpose ordering of image sequences of arbitrary length with built-inattribution maps. After training the model successfully discovers andlocalizes monotonic changes while ignoring cyclic and stochastic ones. Wedemonstrate applications of the model in multiple video settings coveringdifferent scene and object types discovering both object-level andenvironmental changes in unseen sequences. We also demonstrate that theattention-based attribution maps function as effective prompts for segmentingthe changing regions and that the learned representations can be used fordownstream applications. Finally we show that the model achieves the state ofthe art on standard benchmarks for ordering a set of images.</p>
                <p>Last Updated: 2024-04-25 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2404.16828v1">Interpret</button>
                <div id="interpretation-2404.16828v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning Visuotactile Skills with Two Multifingered Hands</h3>
                <p>Authors: Toru LinYu ZhangQiyang LiHaozhi QiBrent YiSergey LevineJitendra Malik</p>
                <p><a href="http://arxiv.org/abs/2404.16823v1">Link to paper</a></p>
                <p>Aiming to replicate human-like dexterity perceptual experiences and motionpatterns we explore learning from human demonstrations using a bimanual systemwith multifingered hands and visuotactile data. Two significant challengesexist: the lack of an affordable and accessible teleoperation system suitablefor a dual-arm setup with multifingered hands and the scarcity ofmultifingered hand hardware equipped with touch sensing. To tackle the firstchallenge we develop HATO a low-cost hands-arms teleoperation system thatleverages off-the-shelf electronics complemented with a software suite thatenables efficient data collection the comprehensive software suite alsosupports multimodal data processing scalable policy learning and smoothpolicy deployment. To tackle the latter challenge we introduce a novelhardware adaptation by repurposing two prosthetic hands equipped with touchsensors for research. Using visuotactile data collected from our system welearn skills to complete long-horizon high-precision tasks which are difficultto achieve without multifingered dexterity and touch feedback. Furthermore weempirically investigate the effects of dataset size sensing modality andvisual input preprocessing on policy learning. Our results mark a promisingstep forward in bimanual multifingered manipulation from visuotactile data.Videos code and datasets can be found at https://toruowo.github.io/hato/ .</p>
                <p>Last Updated: 2024-04-25 17:59:41 UTC</p>
                <button class="interpret-button" data-id="2404.16823v1">Interpret</button>
                <div id="interpretation-2404.16823v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Meta-Transfer Derm-Diagnosis: Exploring Few-Shot Learning and Transfer Learning for Skin Disease Classification in Long-Tail Distribution</h3>
                <p>Authors: Zeynep ÖzdemirHacer Yalim KelesÖmer Özgür Tanrıöver</p>
                <p><a href="http://arxiv.org/abs/2404.16814v1">Link to paper</a></p>
                <p>Addressing the challenges of rare diseases is difficult especially with thelimited number of reference images and a small patient population. This is moreevident in rare skin diseases where we encounter long-tailed datadistributions that make it difficult to develop unbiased and broadly effectivemodels. The diverse ways in which image datasets are gathered and theirdistinct purposes also add to these challenges. Our study conducts a detailedexamination of the benefits and drawbacks of episodic and conventional trainingmethodologies adopting a few-shot learning approach alongside transferlearning. We evaluated our models using the ISIC2018 Derm7pt and SD-198datasets. With minimal labeled examples our models showed substantialinformation gains and better performance compared to previously trained models.Our research emphasizes the improved ability to represent features inDenseNet121 and MobileNetV2 models achieved by using pre-trained models onImageNet to increase similarities within classes. Moreover our experimentsranging from 2-way to 5-way classifications with up to 10 examples showed agrowing success rate for traditional transfer learning methods as the number ofexamples increased. The addition of data augmentation techniques significantlyimproved our transfer learning based model performance leading to higherperformances than existing methods especially in the SD-198 and ISIC2018datasets. All source code related to this work will be made publicly availablesoon at the provided URL.</p>
                <p>Last Updated: 2024-04-25 17:56:45 UTC</p>
                <button class="interpret-button" data-id="2404.16814v1">Interpret</button>
                <div id="interpretation-2404.16814v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AAPL: Adding Attributes to Prompt Learning for Vision-Language Models</h3>
                <p>Authors: Gahyeon KimSohee KimSeokju Lee</p>
                <p><a href="http://arxiv.org/abs/2404.16804v1">Link to paper</a></p>
                <p>Recent advances in large pre-trained vision-language models have demonstratedremarkable performance on zero-shot downstream tasks. Building upon thisrecent studies such as CoOp and CoCoOp have proposed the use of promptlearning where context within a prompt is replaced with learnable vectorsleading to significant improvements over manually crafted prompts. However theperformance improvement for unseen classes is still marginal and to tacklethis problem data augmentation has been frequently used in traditionalzero-shot learning techniques. Through our experiments we have identifiedimportant issues in CoOp and CoCoOp: the context learned through traditionalimage augmentation is biased toward seen classes negatively impactinggeneralization to unseen classes. To address this problem we proposeadversarial token embedding to disentangle low-level visual augmentationfeatures from high-level class information when inducing bias in learnableprompts. Through our novel mechanism called Adding Attributes to PromptLearning AAPL we guide the learnable context to effectively extract textfeatures by focusing on high-level features for unseen classes. We haveconducted experiments across 11 datasets and overall AAPL shows favorableperformances compared to the existing methods in few-shot learning zero-shotlearning cross-dataset and domain generalization tasks.</p>
                <p>Last Updated: 2024-04-25 17:51:10 UTC</p>
                <button class="interpret-button" data-id="2404.16804v1">Interpret</button>
                <div id="interpretation-2404.16804v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization</h3>
                <p>Authors: Herilalaina RakotoarisonSteven AdriaensenNeeratyoy MallikSamir GaribovEdward BergmanFrank Hutter</p>
                <p><a href="http://arxiv.org/abs/2404.16795v1">Link to paper</a></p>
                <p>With the increasing computational costs associated with deep learningautomated hyperparameter optimization methods strongly relying on black-boxBayesian optimization BO face limitations. Freeze-thaw BO offers a promisinggrey-box alternative strategically allocating scarce resources incrementallyto different configurations. However the frequent surrogate model updatesinherent to this approach pose challenges for existing methods requiringretraining or fine-tuning their neural network surrogates online introducingoverhead instability and hyper-hyperparameters. In this work we proposeFT-PFN a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-datafitted network PFN that leverages the transformers in-context learningability to efficiently and reliably do Bayesian learning curve extrapolation ina single forward pass. Our empirical analysis across three benchmark suitesshows that the predictions made by FT-PFN are more accurate and 10-100 timesfaster than those of the deep Gaussian process and deep ensemble surrogatesused in previous work. Furthermore we show that when combined with our novelacquisition mechanism MFPI-random the resulting in-context freeze-thaw BOmethod ifBO yields new state-of-the-art performance in the same threefamilies of deep learning HPO benchmarks considered in prior work.</p>
                <p>Last Updated: 2024-04-25 17:40:52 UTC</p>
                <button class="interpret-button" data-id="2404.16795v1">Interpret</button>
                <div id="interpretation-2404.16795v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>SwarmRL: Building the Future of Smart Active Systems</h3>
                <p>Authors: Samuel ToveyChristoph LohrmannTobias MerktDavid ZimmerKonstantin NikolaouSimon KoppenhöferAnna BushmakinaJonas ScheunemannChristian Holm</p>
                <p><a href="http://arxiv.org/abs/2404.16388v1">Link to paper</a></p>
                <p>This work introduces SwarmRL a Python package designed to study intelligentactive particles. SwarmRL provides an easy-to-use interface for developingmodels to control microscopic colloids using classical control and deepreinforcement learning approaches. These models may be deployed in simulationsor real-world environments under a common framework. We explain the structureof the software and its key features and demonstrate how it can be used toaccelerate research. With SwarmRL we aim to streamline research intomicro-robotic control while bridging the gap between experimental andsimulation-driven sciences. SwarmRL is available open-source on GitHub athttps://github.com/SwarmRL/SwarmRL.</p>
                <p>Last Updated: 2024-04-25 07:57:11 UTC</p>
                <button class="interpret-button" data-id="2404.16388v1">Interpret</button>
                <div id="interpretation-2404.16388v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Optimal and Bounded Suboptimal Any-Angle Multi-agent Pathfinding</h3>
                <p>Authors: Konstantin YakovlevAnton AndreychukRoni Stern</p>
                <p><a href="http://arxiv.org/abs/2404.16379v1">Link to paper</a></p>
                <p>Multi-agent pathfinding MAPF is the problem of finding a set ofconflict-free paths for a set of agents. Typically the agents moves arelimited to a pre-defined graph of possible locations and allowed transitionsbetween them e.g. a 4-neighborhood grid. We explore how to solve MAPF problemswhen each agent can move between any pair of possible locations as long astraversing the line segment connecting them does not lead to the collision withthe obstacles. This is known as any-angle pathfinding. We present the firstoptimal any-angle multi-agent pathfinding algorithm. Our planner is based onthe Continuous Conflict-based Search CCBS algorithm and an optimal any-anglevariant of the Safe Interval Path Planning TO-AA-SIPP. The straightforwardcombination of those however scales poorly since any-angle path findinginduces search trees with a very large branching factor. To mitigate this weadapt two techniques from classical MAPF to the any-angle setting namelyDisjoint Splitting and Multi-Constraints. Experimental results on differentcombinations of these techniques show they enable solving over 30 moreproblems than the vanilla combination of CCBS and TO-AA-SIPP. In addition wepresent a bounded-suboptimal variant of our algorithm that enables tradingruntime for solution cost in a controlled manner.</p>
                <p>Last Updated: 2024-04-25 07:41:47 UTC</p>
                <button class="interpret-button" data-id="2404.16379v1">Interpret</button>
                <div id="interpretation-2404.16379v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Hypergraph Approach to Distributed Broadcast</h3>
                <p>Authors: Qi CaoYulin ShaoFan Yang</p>
                <p><a href="http://arxiv.org/abs/2404.16376v1">Link to paper</a></p>
                <p>This paper explores the distributed broadcast problem within the context ofnetwork communications a critical challenge in decentralized informationdissemination. We put forth a novel hypergraph-based approach to address thisissue focusing on minimizing the number of broadcasts to ensure comprehensivedata sharing among all network users. A key contribution of our work is theestablishment of a general lower bound for the problem using the min-cutcapacity of hypergraphs. Additionally we present the distributed broadcast forquasi-trees DBQT algorithm tailored for the unique structure of quasi-treeswhich is proven to be optimal. This paper advances both network communicationstrategies and hypergraph theory with implications for a wide range ofreal-world applications from vehicular and sensor networks to distributedstorage systems.</p>
                <p>Last Updated: 2024-04-25 07:31:42 UTC</p>
                <button class="interpret-button" data-id="2404.16376v1">Interpret</button>
                <div id="interpretation-2404.16376v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>3D Guidance Law for Maximal Coverage and Target Enclosing with Inherent Safety</h3>
                <p>Authors: Praveen Kumar RanjanAbhinav SinhaYongcan Cao</p>
                <p><a href="http://arxiv.org/abs/2404.16312v1">Link to paper</a></p>
                <p>In this paper we address the problem of enclosing an arbitrarily movingtarget in three dimensions by a single pursuer which is an unmanned aerialvehicle UAV for maximum coverage while also ensuring the pursuers safety bypreventing collisions with the target. The proposed guidance strategy steersthe pursuer to a safe region of space surrounding the target allowing it tomaintain a certain distance from the latter while offering greater flexibilityin positioning and converging to any orbit within this safe zone. Our approachis distinguished by the use of nonholonomic constraints to model vehicles withaccelerations serving as control inputs and coupled engagement kinematics tocraft the pursuers guidance law meticulously. Furthermore we leverage theconcept of the Lyapunov Barrier Function as a powerful tool to constrain thedistance between the pursuer and the target within asymmetric bounds therebyensuring the pursuers safety within the predefined region. To validate theefficacy and robustness of our algorithm we conduct experimental tests byimplementing a high-fidelity quadrotor model within Software-in-the-loop SITLsimulations encompassing various challenging target maneuver scenarios. Theresults obtained showcase the resilience of the proposed guidance laweffectively handling arbitrarily maneuvering targets vehicle/autopilotdynamics and external disturbances. Our method consistently delivers stableglobal enclosing behaviors even in response to aggressive target maneuversand requires only relative information for successful execution.</p>
                <p>Last Updated: 2024-04-25 03:38:07 UTC</p>
                <button class="interpret-button" data-id="2404.16312v1">Interpret</button>
                <div id="interpretation-2404.16312v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A communication protocol based on NK boolean networks for coordinating collective action</h3>
                <p>Authors: Yori Ong</p>
                <p><a href="http://arxiv.org/abs/2404.16240v1">Link to paper</a></p>
                <p>In this paper I describe a digital social communication protocol Gridtbased on Kauffmans NK boolean networks. The main assertion is that acommunication network with this topology supports infinitely scalableself-organization of collective action without requiring hierarchy or centralcontrol. The paper presents the functionality of this protocol andsubstantiates the following propositions about its function and implications:1 Communication via NK boolean networks facilitates coordination oncollective action games for any variable number of users and justifies theassumption that the games payoff structure is common knowledge 2 Use ofthis protocol increases its users transfer empowerment a form of intrinsicmotivation that motivates coordinated action independent of the task oroutcome 3 Communication via this network can be considered cheap talk andbenefits the strategy of players with aligned interests but not of playerswith conflicting interests 4 Absence of significant barriers for itsrealization warrants a timely and continuing discussion on the ethics andimplications of this technology 5 Full realization of the technologyspotential calls for a free-to-use service with maximal transparency of designand associated economic incentives.</p>
                <p>Last Updated: 2024-04-24 22:56:26 UTC</p>
                <button class="interpret-button" data-id="2404.16240v1">Interpret</button>
                <div id="interpretation-2404.16240v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class</h3>
                <p>Authors: Mazda MoayeriMichael RabbatMark IbrahimDiane Bouchacourt</p>
                <p><a href="http://dx.doi.org/10.1145/3630106.3659039">Link to paper</a></p>
                <p>Vision-language models enable open-world classification of objects withoutthe need for any retraining. While this zero-shot paradigm marks a significantadvance even todays best models exhibit skewed performance when objects aredissimilar from their typical depiction. Real world objects such as pearsappear in a variety of forms -- from diced to whole on a table or in a bowl --yet standard VLM classifiers map all instances of a class to a itsinglevector based on the class label. We argue that to represent this richdiversity within a class zero-shot classification should move beyond a singlevector. We propose a method to encode and account for diversity within a classusing inferred attributes still in the zero-shot setting without retraining.We find our method consistently outperforms standard zero-shot classificationover a large suite of datasets encompassing hierarchies diverse object statesand real-world geographic diversity as well finer-grained datasets whereintra-class diversity may be less prevalent. Importantly our method isinherently interpretable offering faithful explanations for each inference tofacilitate model debugging and enhance transparency. We also find our methodscales efficiently to a large number of attributes to account for diversity --leading to more accurate predictions for atypical instances. Finally wecharacterize a principled trade-off between overall and worst class accuracywhich can be tuned via a hyperparameter of our method. We hope this work spursfurther research into the promise of zero-shot classification beyond a singleclass vector for capturing diversity in the world and building transparent AIsystems without compromising performance.</p>
                <p>Last Updated: 2024-04-25 16:29:06 UTC</p>
                <button class="interpret-button" data-id="2404.16717v1">Interpret</button>
                <div id="interpretation-2404.16717v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Benchmarking Mobile Device Control Agents across Diverse Configurations</h3>
                <p>Authors: Juyong LeeTaywon MinMinyong AnChangyeon KimKimin Lee</p>
                <p><a href="http://arxiv.org/abs/2404.16660v1">Link to paper</a></p>
                <p>Developing autonomous agents for mobile devices can significantly enhanceuser interactions by offering increased efficiency and accessibility. Howeverdespite the growing interest in mobile device control agents the absence of acommonly adopted benchmark makes it challenging to quantify scientific progressin this area. In this work we introduce B-MoCA: a novel benchmark designedspecifically for evaluating mobile device control agents. To create a realisticbenchmark we develop B-MoCA based on the Android operating system and define60 common daily tasks. Importantly we incorporate a randomization feature thatchanges various aspects of mobile devices including user interface layouts andlanguage settings to assess generalization performance. We benchmark diverseagents including agents employing large language models LLMs or multi-modalLLMs as well as agents trained from scratch using human expert demonstrations.While these agents demonstrate proficiency in executing straightforward taskstheir poor performance on complex tasks highlights significant opportunitiesfor future research to enhance their effectiveness. Our source code is publiclyavailable at https://b-moca.github.io.</p>
                <p>Last Updated: 2024-04-25 14:56:32 UTC</p>
                <button class="interpret-button" data-id="2404.16660v1">Interpret</button>
                <div id="interpretation-2404.16660v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Comparing Continuous and Retrospective Emotion Ratings in Remote VR Study</h3>
                <p>Authors: Maximilian WarsinkeTanja KojićMaurizio VergariRobert SpangJan-Niklas Voigt-AntonsSebastian Möller</p>
                <p><a href="http://arxiv.org/abs/2404.16487v1">Link to paper</a></p>
                <p>This study investigates the feasibility of remote virtual reality VRstudies conducted at home using VR headsets and video conferencing by deployingan experiment on emotion ratings. 20 participants used head-mounted displays toimmerse themselves in 360deg videos selected to evoke emotional responses.The research compares continuous ratings using a graphical interface toretrospective questionnaires on a digitized Likert Scale for measuring arousaland valence both based on the self-assessment manikin SAM. It washypothesized that the two different rating methods would lead to significantlydifferent values for both valence and arousal. The goal was to investigatewhether continuous ratings during the experience would better reflect usersemotions compared to the post-questionnaire by mitigating biases such as thepeak-end rule. The results show significant differences with moderate to strongeffect sizes for valence and no significant differences for arousal with low tomoderate effect sizes. This indicates the need for further investigation of themethods used to assess emotion ratings in VR studies. Overall this study is anexample of a remotely conducted VR experiment offering insights into methodsfor emotion elicitation in VR by varying the timing and interface of therating.</p>
                <p>Last Updated: 2024-04-25 10:19:44 UTC</p>
                <button class="interpret-button" data-id="2404.16487v1">Interpret</button>
                <div id="interpretation-2404.16487v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CoCoG: Controllable Visual Stimuli Generation based on Human Concept Representations</h3>
                <p>Authors: Chen WeiJiachen ZouDietmar HeinkeQuanying Liu</p>
                <p><a href="http://arxiv.org/abs/2404.16482v1">Link to paper</a></p>
                <p>A central question for cognitive science is to understand how humans processvisual objects i.e to uncover human low-dimensional concept representationspace from high-dimensional visual stimuli. Generating visual stimuli withcontrolling concepts is the key. However there are currently no generativemodels in AI to solve this problem. Here we present the Concept basedControllable Generation CoCoG framework. CoCoG consists of two components asimple yet efficient AI agent for extracting interpretable concept andpredicting human decision-making in visual similarity judgment tasks and aconditional generation model for generating visual stimuli given the concepts.We quantify the performance of CoCoG from two aspects the human behaviorprediction accuracy and the controllable generation ability. The experimentswith CoCoG indicate that 1 the reliable concept embeddings in CoCoG allows topredict human behavior with 64.07 accuracy in the THINGS-similarity dataset2 CoCoG can generate diverse objects through the control of concepts 3 CoCoGcan manipulate human similarity judgment behavior by intervening key concepts.CoCoG offers visual objects with controlling concepts to advance ourunderstanding of causality in human cognition. The code of CoCoG is availableat urlhttps://github.com/ncclab-sustech/CoCoG.</p>
                <p>Last Updated: 2024-04-25 10:10:48 UTC</p>
                <button class="interpret-button" data-id="2404.16482v1">Interpret</button>
                <div id="interpretation-2404.16482v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Impact of Social Environment and Interaction Focus on User Experience and Social Acceptability of an Augmented Reality Game</h3>
                <p>Authors: Lorenzo CocchiaMaurizio VergariTanja KojicFrancesco VonaSebastian MollerFranca GarzottoJan-Niklas Voigt-Antons</p>
                <p><a href="http://arxiv.org/abs/2404.16479v1">Link to paper</a></p>
                <p>One of the most promising technologies inside the Extended Reality XRspectrum is Augmented Reality. This technology is already in peoples pocketsregarding Mobile Augmented Reality with their smartphones. The scientificcommunity still needs answers about how humans could and should interact inenvironments where perceived stimuli are different from fully physical ordigital circumstances. Moreover it is still being determined if people acceptthese new technologies in different social environments and interactionsettings or if some obstacles could exist. This paper explores the impact ofthe Social Environment and the Focus of social interaction on users whileplaying a location-based augmented reality game measuring it with userexperience and social acceptance indicators. An empirical study in awithin-subject fashion was performed in different social environments and underdifferent settings of social interaction focus with N  28 participantscompiling self-reported questionnaires after playing a Scavenger Hunt inAugmented Reality. The measures from two different Social Environments Crowdedvs. Uncrowded resulted in statistically relevant mean differences withindicators from the Social Acceptability dimension. Moreover the analyses showstatistically relevant differences between the variances from different degreesof Social Interaction Focus with Overall Social Presence PerceivedPsychological Engagement Perceived Attentional Engagement and PerceivedEmotional Contagion. The results suggest that a location-based AR game playedin different social environments and settings can influence the userexperiences social dimension. Therefore they should be carefully consideredwhile designing immersive technological experiences in public spaces involvingsocial interactions between players.</p>
                <p>Last Updated: 2024-04-25 10:04:36 UTC</p>
                <button class="interpret-button" data-id="2404.16479v1">Interpret</button>
                <div id="interpretation-2404.16479v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-04-26</p>
        </div>
    
        </div>
    </body>
    </html>
    