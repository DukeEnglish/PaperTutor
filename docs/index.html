
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Tool-Assisted Learning of Computational Reductions</h3>
                <p>Authors: Tristan KneiselElias RadtkeMarko SchmellenkampFabian VehlkenThomas Zeume</p>
                <p><a href="http://arxiv.org/abs/2407.18215v1">Link to paper</a></p>
                <p>Computational reductions are an important and powerful concept in computerscience. However they are difficult for many students to grasp. In this paperwe outline a concept for how the learning of reductions can be supported byeducational support systems. We present an implementation of the concept withinsuch a system concrete web-based and interactive learning material forreductions and report on our experiences using the material in a largeintroductory course on theoretical computer science.</p>
                <p>Last Updated: 2024-07-25 17:28:30 UTC</p>
                <button class="interpret-button" data-id="2407.18215v1">Interpret</button>
                <div id="interpretation-2407.18215v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>IRIS: Wireless Ring for Vision-based Smart Home Interaction</h3>
                <p>Authors: Maruchi KimAntonio GlennBandhav VeluriYunseo LeeEyoel GebreAditya BagariaShwetak PatelShyamnath Gollakota</p>
                <p><a href="http://dx.doi.org/10.1145/3654777.3676327">Link to paper</a></p>
                <p>Integrating cameras into wireless smart rings has been challenging due tosize and power constraints. We introduce IRIS the first wirelessvision-enabled smart ring system for smart home interactions. Equipped with acamera Bluetooth radio inertial measurement unit IMU and an onboardbattery IRIS meets the small size weight and power SWaP requirements forring devices. IRIS is context-aware adapting its gesture set to the detecteddevice and can last for 16-24 hours on a single charge. IRIS leverages thescene semantics to achieve instance-level device recognition. In a studyinvolving 23 participants IRIS consistently outpaced voice commands with ahigher proportion of participants expressing a preference for IRIS over voicecommands regarding toggling a devices state granular control and socialacceptability. Our work pushes the boundary of what is possible with ringform-factor devices addressing system challenges and opening up novelinteraction capabilities.</p>
                <p>Last Updated: 2024-07-25 15:45:17 UTC</p>
                <button class="interpret-button" data-id="2407.18141v1">Interpret</button>
                <div id="interpretation-2407.18141v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ComPeer: A Generative Conversational Agent for Proactive Peer Support</h3>
                <p>Authors: Tianjian LiuHongzheng ZhaoYuheng LiuXingbo WangZhenhui Peng</p>
                <p><a href="http://arxiv.org/abs/2407.18064v1">Link to paper</a></p>
                <p>Conversational Agents CAs acting as peer supporters have been widelystudied and demonstrated beneficial for peoples mental health. Howeverprevious peer support CAs either are user-initiated or follow predefined rulesto initiate the conversations which may discourage users to engage and buildrelationships with the CAs for long-term benefits. In this paper we developComPeer a generative CA that can proactively offer adaptive peer support tousers. ComPeer leverages large language models to detect and reflectsignificant events in the dialogue enabling it to strategically plan thetiming and content of proactive care. In addition ComPeer incorporates peersupport strategies conversation history and its persona into the generativemessages. Our one-week between-subjects study N24 demonstrates ComPeersstrength in providing peer support over time and boosting users engagementcompared to a baseline user-initiated CA.</p>
                <p>Last Updated: 2024-07-25 14:19:35 UTC</p>
                <button class="interpret-button" data-id="2407.18064v1">Interpret</button>
                <div id="interpretation-2407.18064v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>iNNspector: Visual, Interactive Deep Model Debugging</h3>
                <p>Authors: Thilo SpinnerDaniel FürstMennatallah El-Assady</p>
                <p><a href="http://arxiv.org/abs/2407.17998v1">Link to paper</a></p>
                <p>Deep learning model design development and debugging is a process driven bybest practices guidelines trial-and-error and the personal experiences ofmodel developers. At multiple stages of this process performance and internalmodel data can be logged and made available. However due to the sheercomplexity and scale of this data and process model developers often resort toevaluating their model performance based on abstract metrics like accuracy andloss. We argue that a structured analysis of data along the modelsarchitecture and at multiple abstraction levels can considerably streamline thedebugging process. Such a systematic analysis can further connect thedevelopers design choices to their impacts on the model behavior facilitatingthe understanding diagnosis and refinement of deep learning models. Hence inthis paper we 1 contribute a conceptual framework structuring the data spaceof deep learning experiments. Our framework grounded in literature analysisand requirements interviews captures design dimensions and proposes mechanismsto make this data explorable and tractable. To operationalize our framework ina ready-to-use application we 2 present the iNNspector system. iNNspectorenables tracking of deep learning experiments and provides interactivevisualizations of the data on all levels of abstraction from multiple models toindividual neurons. Finally we 3 evaluate our approach with three real-worlduse-cases and a user study with deep learning developers and data analystsproving its effectiveness and usability.</p>
                <p>Last Updated: 2024-07-25 12:48:41 UTC</p>
                <button class="interpret-button" data-id="2407.17998v1">Interpret</button>
                <div id="interpretation-2407.17998v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Discursive Patinas: Anchoring Discussions in Data Visualizations</h3>
                <p>Authors: Tobias KauerDerya AkbabaMarian DörkBenjamin Bach</p>
                <p><a href="http://arxiv.org/abs/2407.17994v1">Link to paper</a></p>
                <p>This paper presents discursive patinas a technique to visualize discussionsonto data visualizations inspired by how people leave traces in the physicalworld. While data visualizations are widely discussed in online communities andsocial media comments tend to be displayed separately from the visualizationand we lack ways to relate these discussions back to the content of thevisualization e.g. to situate comments explain visual patterns or questionassumptions. In our visualization annotation interface users can designateareas within the visualization. Discursive patinas are made of overlaid visualmarks anchors attached to textual comments with category labels likes andreplies. By coloring and styling the anchors a meta visualization emergesshowing what and where people comment and annotate the visualization. Thesepatinas show regions of heavy discussions recent commenting activity and thedistribution of questions suggestions or personal stories. We ran workshopswith 90 students domain experts and visualization researchers to study howpeople use anchors to discuss visualizations and how patinas influence peoplesunderstanding of the discussion. Our results show that discursive patinasimprove the ability to navigate discussions and guide people to comments thathelp understand contextualize or scrutinize the visualization. We discuss thepotential of anchors and patinas to support discursive engagements includingcritical readings of visualizations design feedback and feminist approachesto data visualization.</p>
                <p>Last Updated: 2024-07-25 12:40:20 UTC</p>
                <button class="interpret-button" data-id="2407.17994v1">Interpret</button>
                <div id="interpretation-2407.17994v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal Models: An Empirical Analysis</h3>
                <p>Authors: Cristian-Alexandru BotocanRaphael MeierLjiljana Dolamic</p>
                <p><a href="http://arxiv.org/abs/2407.18251v1">Link to paper</a></p>
                <p>Assessing the robustness of multimodal models against adversarial examples isan important aspect for the safety of its users. We craft L0-norm perturbationattacks on the preprocessed input images. We launch them in a black-box setupagainst four multimodal models and two unimodal DNNs considering both targetedand untargeted misclassification. Our attacks target less than 0.04 ofperturbed image area and integrate different spatial positioning of perturbedpixels: sparse positioning and pixels arranged in different contiguous shapesrow column diagonal and patch. To the best of our knowledge we are thefirst to assess the robustness of three state-of-the-art multimodal modelsALIGN AltCLIP GroupViT against different sparse and contiguous pixeldistribution perturbations. The obtained results indicate that unimodal DNNsare more robust than multimodal models. Furthermore models using CNN-basedImage Encoder are more vulnerable than models with ViT - for untargetedattacks we obtain a 99 success rate by perturbing less than 0.02 of theimage area.</p>
                <p>Last Updated: 2024-07-25 17:59:48 UTC</p>
                <button class="interpret-button" data-id="2407.18251v1">Interpret</button>
                <div id="interpretation-2407.18251v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VGGHeads: A Large-Scale Synthetic Dataset for 3D Human Heads</h3>
                <p>Authors: Orest KupynEugene KhvedcheniaChristian Rupprecht</p>
                <p><a href="http://arxiv.org/abs/2407.18245v1">Link to paper</a></p>
                <p>Human head detection keypoint estimation and 3D head model fitting areimportant tasks with many applications. However traditional real-worlddatasets often suffer from bias privacy and ethical concerns and they havebeen recorded in laboratory environments which makes it difficult for trainedmodels to generalize. Here we introduce VGGHeads -- a large scale syntheticdataset generated with diffusion models for human head detection and 3D meshestimation. Our dataset comprises over 1 million high-resolution images eachannotated with detailed 3D head meshes facial landmarks and bounding boxes.Using this dataset we introduce a new model architecture capable ofsimultaneous heads detection and head meshes reconstruction from a single imagein a single step. Through extensive experimental evaluations we demonstratethat models trained on our synthetic data achieve strong performance on realimages. Furthermore the versatility of our dataset makes it applicable acrossa broad spectrum of tasks offering a general and comprehensive representationof human heads. Additionally we provide detailed information about thesynthetic data generation pipeline enabling it to be re-used for other tasksand domains.</p>
                <p>Last Updated: 2024-07-25 17:58:17 UTC</p>
                <button class="interpret-button" data-id="2407.18245v1">Interpret</button>
                <div id="interpretation-2407.18245v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LoRA-Pro: Are Low-Rank Adapters Properly Optimized?</h3>
                <p>Authors: Zhengbo WangJian Liang</p>
                <p><a href="http://arxiv.org/abs/2407.18242v1">Link to paper</a></p>
                <p>Low-Rank Adaptation also known as LoRA has emerged as a prominent methodfor parameter-efficient fine-tuning foundation models by re-parameterizing theoriginal matrix into the product of two low-rank matrices. Despite itsefficiency LoRA often yields inferior performance compared to fullfine-tuning. In this paper we propose LoRA-Pro to bridge this performance gap.Firstly we delve into the optimization processes in LoRA and full fine-tuning.We reveal that while LoRA employs low-rank approximation it neglects toapproximate the optimization process of full fine-tuning. To address this weintroduce a novel concept called the equivalent gradient. This virtualgradient makes the optimization process on the re-parameterized matrixequivalent to LoRA which can be used to quantify the differences between LoRAand full fine-tuning. The equivalent gradient is derived from the gradients ofmatrices A and B. To narrow the performance gap our approach minimizes thedifferences between the equivalent gradient and the gradient obtained from fullfine-tuning during the optimization process. By solving this objective wederive optimal closed-form solutions for updating matrices A and B. Ourmethod constrains the optimization process shrinking the performance gapbetween LoRA and full fine-tuning. Extensive experiments on natural languageprocessing tasks validate the effectiveness of our method.</p>
                <p>Last Updated: 2024-07-25 17:57:12 UTC</p>
                <button class="interpret-button" data-id="2407.18242v1">Interpret</button>
                <div id="interpretation-2407.18242v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Numerical Literals in Link Prediction: A Critical Examination of Models and Datasets</h3>
                <p>Authors: Moritz BlumBasil EllHannes IllPhilipp Cimiano</p>
                <p><a href="http://arxiv.org/abs/2407.18241v1">Link to paper</a></p>
                <p>Link PredictionLP is an essential task over Knowledge GraphsKGstraditionally focussed on using and predicting the relations between entities.Textual entity descriptions have already been shown to be valuable but modelsthat incorporate numerical literals have shown minor improvements on existingbenchmark datasets. It is unclear whether a model is actually better in usingnumerical literals or better capable of utilizing the graph structure. Thisraises doubts about the effectiveness of these methods and about thesuitability of the existing benchmark datasets.  We propose a methodology to evaluate LP models that incorporate numericalliterals. We propose i a new synthetic dataset to better understand how wellthese models use numerical literals and ii dataset ablations strategies toinvestigate potential difficulties with the existing datasets. We identify aprevalent trend: many models underutilize literal information and potentiallyrely on additional parameters for performance gains. Our investigationhighlights the need for more extensive evaluations when releasing new modelsand datasets.</p>
                <p>Last Updated: 2024-07-25 17:55:33 UTC</p>
                <button class="interpret-button" data-id="2407.18241v1">Interpret</button>
                <div id="interpretation-2407.18241v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Automated Ensemble Multimodal Machine Learning for Healthcare</h3>
                <p>Authors: Fergus ImrieStefan DennerLucas S. BrunschwigKlaus Maier-HeinMihaela van der Schaar</p>
                <p><a href="http://arxiv.org/abs/2407.18227v1">Link to paper</a></p>
                <p>The application of machine learning in medicine and healthcare has led to thecreation of numerous diagnostic and prognostic models. However despite theirsuccess current approaches generally issue predictions using data from asingle modality. This stands in stark contrast with clinician decision-makingwhich employs diverse information from multiple sources. While severalmultimodal machine learning approaches exist significant challenges indeveloping multimodal systems remain that are hindering clinical adoption. Inthis paper we introduce a multimodal framework AutoPrognosis-M that enablesthe integration of structured clinical tabular data and medical imaging usingautomated machine learning. AutoPrognosis-M incorporates 17 imaging modelsincluding convolutional neural networks and vision transformers and threedistinct multimodal fusion strategies. In an illustrative application using amultimodal skin lesion dataset we highlight the importance of multimodalmachine learning and the power of combining multiple fusion strategies usingensemble learning. We have open-sourced our framework as a tool for thecommunity and hope it will accelerate the uptake of multimodal machine learningin healthcare and spur further innovation.</p>
                <p>Last Updated: 2024-07-25 17:46:38 UTC</p>
                <button class="interpret-button" data-id="2407.18227v1">Interpret</button>
                <div id="interpretation-2407.18227v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning</h3>
                <p>Authors: Tianduo WangShichen LiWei Lu</p>
                <p><a href="http://arxiv.org/abs/2407.18248v1">Link to paper</a></p>
                <p>Effective training of language models LMs for mathematical reasoning tasksdemands high-quality supervised fine-tuning data. Besides obtaining annotationsfrom human experts a common alternative is sampling from larger and morepowerful LMs. However this knowledge distillation approach can be costly andunstable particularly when relying on closed-source proprietary LMs likeGPT-4 whose behaviors are often unpredictable. In this work we demonstratethat the reasoning abilities of small-scale LMs can be enhanced throughself-training a process where models learn from their own outputs. We alsoshow that the conventional self-training can be further augmented by apreference learning algorithm called Direct Preference Optimization DPO. Byintegrating DPO into self-training we leverage preference data to guide LMstowards more accurate and diverse chain-of-thought reasoning. We evaluate ourmethod across various mathematical reasoning tasks using different base models.Our experiments show that this approach not only improves LMs reasoningperformance but also offers a more cost-effective and scalable solutioncompared to relying on large proprietary LMs.</p>
                <p>Last Updated: 2024-07-25 17:59:16 UTC</p>
                <button class="interpret-button" data-id="2407.18248v1">Interpret</button>
                <div id="interpretation-2407.18248v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LoRA-Pro: Are Low-Rank Adapters Properly Optimized?</h3>
                <p>Authors: Zhengbo WangJian Liang</p>
                <p><a href="http://arxiv.org/abs/2407.18242v1">Link to paper</a></p>
                <p>Low-Rank Adaptation also known as LoRA has emerged as a prominent methodfor parameter-efficient fine-tuning foundation models by re-parameterizing theoriginal matrix into the product of two low-rank matrices. Despite itsefficiency LoRA often yields inferior performance compared to fullfine-tuning. In this paper we propose LoRA-Pro to bridge this performance gap.Firstly we delve into the optimization processes in LoRA and full fine-tuning.We reveal that while LoRA employs low-rank approximation it neglects toapproximate the optimization process of full fine-tuning. To address this weintroduce a novel concept called the equivalent gradient. This virtualgradient makes the optimization process on the re-parameterized matrixequivalent to LoRA which can be used to quantify the differences between LoRAand full fine-tuning. The equivalent gradient is derived from the gradients ofmatrices A and B. To narrow the performance gap our approach minimizes thedifferences between the equivalent gradient and the gradient obtained from fullfine-tuning during the optimization process. By solving this objective wederive optimal closed-form solutions for updating matrices A and B. Ourmethod constrains the optimization process shrinking the performance gapbetween LoRA and full fine-tuning. Extensive experiments on natural languageprocessing tasks validate the effectiveness of our method.</p>
                <p>Last Updated: 2024-07-25 17:57:12 UTC</p>
                <button class="interpret-button" data-id="2407.18242v1">Interpret</button>
                <div id="interpretation-2407.18242v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Recursive Introspection: Teaching Language Model Agents How to Self-Improve</h3>
                <p>Authors: Yuxiao QuTianjun ZhangNaman GargAviral Kumar</p>
                <p><a href="http://arxiv.org/abs/2407.18219v1">Link to paper</a></p>
                <p>A central piece in enabling intelligent agentic behavior in foundation modelsis to make them capable of introspecting upon their behavior reasoning andcorrecting their mistakes as more computation or interaction is available. Eventhe strongest proprietary large language models LLMs do not quite exhibit theability of continually improving their responses sequentially even inscenarios where they are explicitly told that they are making a mistake. Inthis paper we develop RISE: Recursive IntroSpEction an approach forfine-tuning LLMs to introduce this capability despite prior work hypothesizingthat this capability may not be possible to attain. Our approach prescribes aniterative fine-tuning procedure which attempts to teach the model how to alterits response after having executed previously unsuccessful attempts to solve ahard test-time problem with optionally additional environment feedback. RISEposes fine-tuning for a single-turn prompt as solving a multi-turn Markovdecision process MDP where the initial state is the prompt. Inspired byprinciples in online imitation learning and reinforcement learning we proposestrategies for multi-turn data collection and training so as to imbue an LLMwith the capability to recursively detect and correct its previous mistakes insubsequent iterations. Our experiments show that RISE enables Llama2 Llama3and Mistral models to improve themselves with more turns on math reasoningtasks outperforming several single-turn strategies given an equal amount ofinference-time computation. We also find that RISE scales well often attaininglarger benefits with more capable models. Our analysis shows that RISE makesmeaningful improvements to responses to arrive at the correct solution forchallenging prompts without disrupting one-turn abilities as a result ofexpressing more complex distributions.</p>
                <p>Last Updated: 2024-07-25 17:35:59 UTC</p>
                <button class="interpret-button" data-id="2407.18219v1">Interpret</button>
                <div id="interpretation-2407.18219v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Exploring Scaling Trends in LLM Robustness</h3>
                <p>Authors: Nikolhaus HoweMichał ZajacIan McKenzieOskar HollinsworthTom TsengPierre-Luc BaconAdam Gleave</p>
                <p><a href="http://arxiv.org/abs/2407.18213v1">Link to paper</a></p>
                <p>Language model capabilities predictably improve from scaling a models sizeand training data. Motivated by this increasingly large language models havebeen trained yielding an array of impressive capabilities. Yet these modelsare vulnerable to adversarial prompts such as jailbreaks that hijack modelsto perform undesired behaviors posing a significant risk of misuse. Prior workindicates that computer vision models become more robust with model and datascaling raising the question: does language model robustness also improve withscale We study this question empirically finding that larger models respondsubstantially better to adversarial training but there is little to no benefitfrom model scale in the absence of explicit defenses.</p>
                <p>Last Updated: 2024-07-25 17:26:41 UTC</p>
                <button class="interpret-button" data-id="2407.18213v1">Interpret</button>
                <div id="interpretation-2407.18213v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The FIGNEWS Shared Task on News Media Narratives</h3>
                <p>Authors: Wajdi ZaghouaniMustafa JarrarNizar HabashHouda BouamorImed ZitouniMona DiabSamhaa R. El-BeltagyMuhammed AbuOdeh</p>
                <p><a href="http://arxiv.org/abs/2407.18147v1">Link to paper</a></p>
                <p>We present an overview of the FIGNEWS shared task organized as part of theArabicNLP 2024 conference co-located with ACL 2024. The shared task addressesbias and propaganda annotation in multilingual news posts. We focus on theearly days of the Israel War on Gaza as a case study. The task aims to fostercollaboration in developing annotation guidelines for subjective tasks bycreating frameworks for analyzing diverse narratives highlighting potentialbias and propaganda. In a spirit of fostering and encouraging diversity weaddress the problem from a multilingual perspective namely within fivelanguages: English French Arabic Hebrew and Hindi. A total of 17 teamsparticipated in two annotation subtasks: bias 16 teams and propaganda 6teams. The teams competed in four evaluation tracks: guidelines developmentannotation quality annotation quantity and consistency. Collectively theteams produced 129800 data points. Key findings and implications for the fieldare discussed.</p>
                <p>Last Updated: 2024-07-25 15:58:19 UTC</p>
                <button class="interpret-button" data-id="2407.18147v1">Interpret</button>
                <div id="interpretation-2407.18147v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Statistical optimal transport</h3>
                <p>Authors: Sinho ChewiJonathan Niles-WeedPhilippe Rigollet</p>
                <p><a href="http://arxiv.org/abs/2407.18163v1">Link to paper</a></p>
                <p>We present an introduction to the field of statistical optimal transportbased on lectures given at Ecole dEte de Probabilites de Saint-FlourXLIX.</p>
                <p>Last Updated: 2024-07-25 16:25:10 UTC</p>
                <button class="interpret-button" data-id="2407.18163v1">Interpret</button>
                <div id="interpretation-2407.18163v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models</h3>
                <p>Authors: Sanae LotfiYilun KuangBrandon AmosMicah GoldblumMarc FinziAndrew Gordon Wilson</p>
                <p><a href="http://arxiv.org/abs/2407.18158v1">Link to paper</a></p>
                <p>Large language models LLMs with billions of parameters excel at predictingthe next token in a sequence. Recent work computes non-vacuouscompression-based generalization bounds for LLMs but these bounds are vacuousfor large models at the billion-parameter scale. Moreover these bounds areobtained through restrictive compression techniques bounding compressed modelsthat generate low-quality text. Additionally the tightness of these existingbounds depends on the number of IID documents in a training set rather than themuch larger number of non-IID constituent tokens leaving untapped potentialfor tighter bounds. In this work we instead use properties of martingales toderive generalization bounds that benefit from the vast number of tokens in LLMtraining sets. Since a dataset contains far more tokens than documents ourgeneralization bounds not only tolerate but actually benefit from far lessrestrictive compression schemes. With Monarch matrices Kroneckerfactorizations and post-training quantization we achieve non-vacuousgeneralization bounds for LLMs as large as LLaMA2-70B. Unlike previousapproaches our work achieves the first non-vacuous bounds for models that aredeployed in practice and generate high-quality text.</p>
                <p>Last Updated: 2024-07-25 16:13:58 UTC</p>
                <button class="interpret-button" data-id="2407.18158v1">Interpret</button>
                <div id="interpretation-2407.18158v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fast convergence of the Expectation Maximization algorithm under a logarithmic Sobolev inequality</h3>
                <p>Authors: Rocco CaprioAdam M Johansen</p>
                <p><a href="http://arxiv.org/abs/2407.17949v1">Link to paper</a></p>
                <p>By utilizing recently developed tools for constructing gradient flows onWasserstein spaces we extend an analysis technique commonly employed tounderstand alternating minimization algorithms on Euclidean space to theExpectation Maximization EM algorithm via its representation ascoordinate-wise minimization on the product of a Euclidean space and a space ofprobability distributions due to Neal and Hinton 1998. In so doing we obtainfinite sample error bounds and exponential convergence of the EM algorithmunder a natural generalisation of a log-Sobolev inequality. We furtherdemonstrate that the analysis technique is sufficiently flexible to allow alsothe analysis of several variants of the EM algorithm.</p>
                <p>Last Updated: 2024-07-25 11:08:53 UTC</p>
                <button class="interpret-button" data-id="2407.17949v1">Interpret</button>
                <div id="interpretation-2407.17949v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Causal Deepsets for Off-policy Evaluation under Spatial or Spatio-temporal Interferences</h3>
                <p>Authors: Runpeng DaiJianing WangFan ZhouShikai LuoZhiwei QinChengchun ShiHongtu Zhu</p>
                <p><a href="http://arxiv.org/abs/2407.17910v1">Link to paper</a></p>
                <p>Off-policy evaluation OPE is widely applied in sectors such aspharmaceuticals and e-commerce to evaluate the efficacy of novel products orpolicies from offline datasets. This paper introduces a causal deepsetframework that relaxes several key structural assumptions primarily themean-field assumption prevalent in existing OPE methodologies that handlespatio-temporal interference. These traditional assumptions frequently proveinadequate in real-world settings thereby restricting the capability ofcurrent OPE methods to effectively address complex interference effects. Inresponse we advocate for the implementation of the permutation invariance PIassumption. This innovative approach enables the data-driven adaptive learningof the mean-field function offering a more flexible estimation method beyondconventional averaging. Furthermore we present novel algorithms thatincorporate the PI assumption into OPE and thoroughly examine their theoreticalfoundations. Our numerical analyses demonstrate that this novel approach yieldssignificantly more precise estimations than existing baseline algorithmsthereby substantially improving the practical applicability and effectivenessof OPE methodologies. A Python implementation of our proposed method isavailable at https://github.com/BIG-S2/Causal-Deepsets.</p>
                <p>Last Updated: 2024-07-25 10:02:11 UTC</p>
                <button class="interpret-button" data-id="2407.17910v1">Interpret</button>
                <div id="interpretation-2407.17910v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Superior Scoring Rules for Probabilistic Evaluation of Single-Label Multi-Class Classification Tasks</h3>
                <p>Authors: Rouhollah AhmadianMehdi GhateeJohan Wahlström</p>
                <p><a href="http://arxiv.org/abs/2407.17697v1">Link to paper</a></p>
                <p>This study introduces novel superior scoring rules called Penalized BrierScore PBS and Penalized Logarithmic Loss PLL to improve model evaluationfor probabilistic classification. Traditional scoring rules like Brier Scoreand Logarithmic Loss sometimes assign better scores to misclassifications incomparison with correct classifications. This discrepancy from the actualpreference for rewarding correct classifications can lead to suboptimal modelselection. By integrating penalties for misclassifications PBS and PLL modifytraditional proper scoring rules to consistently assign better scores tocorrect predictions. Formal proofs demonstrate that PBS and PLL satisfystrictly proper scoring rule properties while also preferentially rewardingaccurate classifications. Experiments showcase the benefits of using PBS andPLL for model selection model checkpointing and early stopping. PBS exhibitsa higher negative correlation with the F1 score compared to the Brier Scoreduring training. Thus PBS more effectively identifies optimal checkpoints andearly stopping points leading to improved F1 scores. Comparative analysisverifies models selected by PBS and PLL achieve superior F1 scores. ThereforePBS and PLL address the gap between uncertainty quantification and accuracymaximization by encapsulating both proper scoring principles and explicitpreference for true classifications. The proposed metrics can enhance modelevaluation and selection for reliable probabilistic classification.</p>
                <p>Last Updated: 2024-07-25 01:46:05 UTC</p>
                <button class="interpret-button" data-id="2407.17697v1">Interpret</button>
                <div id="interpretation-2407.17697v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>LoRA-Pro: Are Low-Rank Adapters Properly Optimized?</h3>
                <p>Authors: Zhengbo WangJian Liang</p>
                <p><a href="http://arxiv.org/abs/2407.18242v1">Link to paper</a></p>
                <p>Low-Rank Adaptation also known as LoRA has emerged as a prominent methodfor parameter-efficient fine-tuning foundation models by re-parameterizing theoriginal matrix into the product of two low-rank matrices. Despite itsefficiency LoRA often yields inferior performance compared to fullfine-tuning. In this paper we propose LoRA-Pro to bridge this performance gap.Firstly we delve into the optimization processes in LoRA and full fine-tuning.We reveal that while LoRA employs low-rank approximation it neglects toapproximate the optimization process of full fine-tuning. To address this weintroduce a novel concept called the equivalent gradient. This virtualgradient makes the optimization process on the re-parameterized matrixequivalent to LoRA which can be used to quantify the differences between LoRAand full fine-tuning. The equivalent gradient is derived from the gradients ofmatrices A and B. To narrow the performance gap our approach minimizes thedifferences between the equivalent gradient and the gradient obtained from fullfine-tuning during the optimization process. By solving this objective wederive optimal closed-form solutions for updating matrices A and B. Ourmethod constrains the optimization process shrinking the performance gapbetween LoRA and full fine-tuning. Extensive experiments on natural languageprocessing tasks validate the effectiveness of our method.</p>
                <p>Last Updated: 2024-07-25 17:57:12 UTC</p>
                <button class="interpret-button" data-id="2407.18242v1">Interpret</button>
                <div id="interpretation-2407.18242v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Recursive Introspection: Teaching Language Model Agents How to Self-Improve</h3>
                <p>Authors: Yuxiao QuTianjun ZhangNaman GargAviral Kumar</p>
                <p><a href="http://arxiv.org/abs/2407.18219v1">Link to paper</a></p>
                <p>A central piece in enabling intelligent agentic behavior in foundation modelsis to make them capable of introspecting upon their behavior reasoning andcorrecting their mistakes as more computation or interaction is available. Eventhe strongest proprietary large language models LLMs do not quite exhibit theability of continually improving their responses sequentially even inscenarios where they are explicitly told that they are making a mistake. Inthis paper we develop RISE: Recursive IntroSpEction an approach forfine-tuning LLMs to introduce this capability despite prior work hypothesizingthat this capability may not be possible to attain. Our approach prescribes aniterative fine-tuning procedure which attempts to teach the model how to alterits response after having executed previously unsuccessful attempts to solve ahard test-time problem with optionally additional environment feedback. RISEposes fine-tuning for a single-turn prompt as solving a multi-turn Markovdecision process MDP where the initial state is the prompt. Inspired byprinciples in online imitation learning and reinforcement learning we proposestrategies for multi-turn data collection and training so as to imbue an LLMwith the capability to recursively detect and correct its previous mistakes insubsequent iterations. Our experiments show that RISE enables Llama2 Llama3and Mistral models to improve themselves with more turns on math reasoningtasks outperforming several single-turn strategies given an equal amount ofinference-time computation. We also find that RISE scales well often attaininglarger benefits with more capable models. Our analysis shows that RISE makesmeaningful improvements to responses to arrive at the correct solution forchallenging prompts without disrupting one-turn abilities as a result ofexpressing more complex distributions.</p>
                <p>Last Updated: 2024-07-25 17:35:59 UTC</p>
                <button class="interpret-button" data-id="2407.18219v1">Interpret</button>
                <div id="interpretation-2407.18219v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Exploring Scaling Trends in LLM Robustness</h3>
                <p>Authors: Nikolhaus HoweMichał ZajacIan McKenzieOskar HollinsworthTom TsengPierre-Luc BaconAdam Gleave</p>
                <p><a href="http://arxiv.org/abs/2407.18213v1">Link to paper</a></p>
                <p>Language model capabilities predictably improve from scaling a models sizeand training data. Motivated by this increasingly large language models havebeen trained yielding an array of impressive capabilities. Yet these modelsare vulnerable to adversarial prompts such as jailbreaks that hijack modelsto perform undesired behaviors posing a significant risk of misuse. Prior workindicates that computer vision models become more robust with model and datascaling raising the question: does language model robustness also improve withscale We study this question empirically finding that larger models respondsubstantially better to adversarial training but there is little to no benefitfrom model scale in the absence of explicit defenses.</p>
                <p>Last Updated: 2024-07-25 17:26:41 UTC</p>
                <button class="interpret-button" data-id="2407.18213v1">Interpret</button>
                <div id="interpretation-2407.18213v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Differentiable Quantum Architecture Search in Asynchronous Quantum Reinforcement Learning</h3>
                <p>Authors: Samuel Yen-Chi Chen</p>
                <p><a href="http://arxiv.org/abs/2407.18202v1">Link to paper</a></p>
                <p>The emergence of quantum reinforcement learning QRL is propelled byadvancements in quantum computing QC and machine learning ML particularlythrough quantum neural networks QNN built on variational quantum circuitsVQC. These advancements have proven successful in addressing sequentialdecision-making tasks. However constructing effective QRL models demandssignificant expertise due to challenges in designing quantum circuitarchitectures including data encoding and parameterized circuits whichprofoundly influence model performance. In this paper we propose addressingthis challenge with differentiable quantum architecture search DiffQASenabling trainable circuit parameters and structure weights usinggradient-based optimization. Furthermore we enhance training efficiencythrough asynchronous reinforcement learning RL methods facilitating paralleltraining. Through numerical simulations we demonstrate that our proposedDiffQAS-QRL approach achieves performance comparable to manually-craftedcircuit architectures across considered environments showcasing stabilityacross diverse scenarios. This methodology offers a pathway for designing QRLmodels without extensive quantum knowledge ensuring robust performance andfostering broader application of QRL.</p>
                <p>Last Updated: 2024-07-25 17:11:00 UTC</p>
                <button class="interpret-button" data-id="2407.18202v1">Interpret</button>
                <div id="interpretation-2407.18202v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning</h3>
                <p>Authors: Sindhura KommuYizhi WangYue WangXuan Wang</p>
                <p><a href="http://arxiv.org/abs/2407.18181v1">Link to paper</a></p>
                <p>Inferring gene regulatory networks GRNs from single-cell RNA sequencingscRNA-seq data is a complex challenge that requires capturing the intricaterelationships between genes and their regulatory interactions. In this studywe tackle this challenge by leveraging the single-cell BERT-based pre-trainedtransformer model scBERT trained on extensive unlabeled scRNA-seq data toaugment structured biological knowledge from existing GRNs. We introduce anovel joint graph learning approach that combines the rich contextualrepresentations learned by pre-trained single-cell language models with thestructured knowledge encoded in GRNs using graph neural networks GNNs. Byintegrating these two modalities our approach effectively reasons over boththegene expression level constraints provided by the scRNA-seq data and thestructured biological knowledge inherent in GRNs. We evaluate our method onhuman cell benchmark datasets from the BEELINE study with cell type-specificground truth networks. The results demonstrate superior performance overcurrent state-of-the-art baselines offering a deeper understanding of cellularregulatory mechanisms.</p>
                <p>Last Updated: 2024-07-25 16:42:08 UTC</p>
                <button class="interpret-button" data-id="2407.18181v1">Interpret</button>
                <div id="interpretation-2407.18181v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Strategic Cost Selection in Participatory Budgeting</h3>
                <p>Authors: Piotr FaliszewskiŁukasz JaneczkoAndrzej KaczmarczykGrzegorz LisowskiPiotr SkowronStanisław Szufa</p>
                <p><a href="http://arxiv.org/abs/2407.18092v1">Link to paper</a></p>
                <p>We study strategic behavior of project proposers in the context ofapproval-based participatory budgeting PB. In our model we assume that thevotes are fixed and known and the proposers want to set as high project pricesas possible provided that their projects get selected and the prices are notbelow the minimum costs of their delivery. We study the existence of pure Nashequilibria NE in such games focusing on the AV/Cost Phragmen and Methodof Equal Shares rules. Furthermore we report an experimental study ofstrategic cost selection on real-life PB election data.</p>
                <p>Last Updated: 2024-07-25 15:00:12 UTC</p>
                <button class="interpret-button" data-id="2407.18092v1">Interpret</button>
                <div id="interpretation-2407.18092v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Principal-Agent Reinforcement Learning</h3>
                <p>Authors: Dima IvanovPaul DüttingInbal Talgam-CohenTonghan WangDavid C. Parkes</p>
                <p><a href="http://arxiv.org/abs/2407.18074v1">Link to paper</a></p>
                <p>Contracts are the economic framework which allows a principal to delegate atask to an agent -- despite misaligned interests and even without directlyobserving the agents actions. In many modern reinforcement learning settingsself-interested agents learn to perform a multi-stage task delegated to them bya principal. We explore the significant potential of utilizing contracts toincentivize the agents. We model the delegated task as an MDP and study astochastic game between the principal and agent where the principal learns whatcontracts to use and the agent learns an MDP policy in response. We present alearning-based algorithm for optimizing the principals contracts whichprovably converges to the subgame-perfect equilibrium of the principal-agentgame. A deep RL implementation allows us to apply our method to very large MDPswith unknown transition dynamics. We extend our approach to multiple agentsand demonstrate its relevance to resolving a canonical sequential socialdilemma with minimal intervention to agent rewards.</p>
                <p>Last Updated: 2024-07-25 14:28:58 UTC</p>
                <button class="interpret-button" data-id="2407.18074v1">Interpret</button>
                <div id="interpretation-2407.18074v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Stochastic Games with Minimally Bounded Action Costs</h3>
                <p>Authors: David Mguni</p>
                <p><a href="http://arxiv.org/abs/2407.18010v1">Link to paper</a></p>
                <p>In many multi-player interactions players incur strictly positive costs eachtime they execute actions e.g. menu costs or transaction costs in financialsystems. Since acting at each available opportunity would accumulateprohibitively large costs the resulting decision problem is one in whichplayers must make strategic decisions about when to execute actions in additionto their choice of action. This paper analyses a discrete-time stochastic gameSG in which players face minimally bounded positive costs for each action andinfluence the system using impulse controls. We prove SGs of two-sided impulsecontrol have a unique value and characterise the saddle point equilibrium inwhich the players execute actions at strategically chosen times in accordancewith Markovian strategies. We prove the game respects a dynamic programmingprinciple and that the Markov perfect equilibrium can be computed as a limitpoint of a sequence of Bellman operations. We then introduce a new Q-learningvariant which we show converges almost surely to the value of the game enablingsolutions to be extracted in unknown settings. Lastly we extend our results tosettings with budgetory constraints.</p>
                <p>Last Updated: 2024-07-25 13:04:49 UTC</p>
                <button class="interpret-button" data-id="2407.18010v1">Interpret</button>
                <div id="interpretation-2407.18010v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Limited Voting for Better Representation?</h3>
                <p>Authors: Maaike Venema-LosZoé ChristoffDavide Grossi</p>
                <p><a href="http://arxiv.org/abs/2407.17973v1">Link to paper</a></p>
                <p>Limited Voting LV is an approval-based method for multi-winner electionswhere all ballots are required to have a same fixed size. While it appears tobe used as voting method in corporate governance and has some politicalapplications to the best of our knowledge no formal analysis of the ruleexists to date. We provide such an analysis here prompted by a request foradvice about this voting rule by a health insurance company in the Netherlandswhich uses it to elect its work council. We study conditions under which LVwould improve representation over standard approval voting and when it wouldnot. We establish the extent of such an improvement or lack thereof both interms of diversity and proportionality notions. These results help usunderstand if and how LV may be used as a low-effort fix of approval votingin order to enhance representation.</p>
                <p>Last Updated: 2024-07-25 12:08:01 UTC</p>
                <button class="interpret-button" data-id="2407.17973v1">Interpret</button>
                <div id="interpretation-2407.17973v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Very Large-Scale Multi-Agent Simulation in AgentScope</h3>
                <p>Authors: Xuchen PanDawei GaoYuexiang XieZhewei WeiYaliang LiBolin DingJi-Rong WenJingren Zhou</p>
                <p><a href="http://arxiv.org/abs/2407.17789v1">Link to paper</a></p>
                <p>Recent advances in large language models LLMs have opened new avenues forapplying multi-agent systems in very large-scale simulations. However thereremain several challenges when conducting multi-agent simulations with existingplatforms such as limited scalability and low efficiency unsatisfied agentdiversity and effort-intensive management processes. To address thesechallenges we develop several new features and components for AgentScope auser-friendly multi-agent platform enhancing its convenience and flexibilityfor supporting very large-scale multi-agent simulations. Specifically wepropose an actor-based distributed mechanism as the underlying technologicalinfrastructure towards great scalability and high efficiency and provideflexible environment support for simulating various real-world scenarios whichenables parallel execution of multiple agents centralized workfloworchestration and both inter-agent and agent-environment interactions amongagents. Moreover we integrate an easy-to-use configurable tool and anautomatic background generation pipeline in AgentScope simplifying the processof creating agents with diverse yet detailed background settings. Last but notleast we provide a web-based interface for conveniently monitoring andmanaging a large number of agents that might deploy across multiple devices. Weconduct a comprehensive simulation to demonstrate the effectiveness of theproposed enhancements in AgentScope and provide detailed observations anddiscussions to highlight the great potential of applying multi-agent systems inlarge-scale simulations. The source code is released on GitHub athttps://github.com/modelscope/agentscope to inspire further research anddevelopment in large-scale multi-agent simulations.</p>
                <p>Last Updated: 2024-07-25 05:50:46 UTC</p>
                <button class="interpret-button" data-id="2407.17789v1">Interpret</button>
                <div id="interpretation-2407.17789v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal Models: An Empirical Analysis</h3>
                <p>Authors: Cristian-Alexandru BotocanRaphael MeierLjiljana Dolamic</p>
                <p><a href="http://arxiv.org/abs/2407.18251v1">Link to paper</a></p>
                <p>Assessing the robustness of multimodal models against adversarial examples isan important aspect for the safety of its users. We craft L0-norm perturbationattacks on the preprocessed input images. We launch them in a black-box setupagainst four multimodal models and two unimodal DNNs considering both targetedand untargeted misclassification. Our attacks target less than 0.04 ofperturbed image area and integrate different spatial positioning of perturbedpixels: sparse positioning and pixels arranged in different contiguous shapesrow column diagonal and patch. To the best of our knowledge we are thefirst to assess the robustness of three state-of-the-art multimodal modelsALIGN AltCLIP GroupViT against different sparse and contiguous pixeldistribution perturbations. The obtained results indicate that unimodal DNNsare more robust than multimodal models. Furthermore models using CNN-basedImage Encoder are more vulnerable than models with ViT - for untargetedattacks we obtain a 99 success rate by perturbing less than 0.02 of theimage area.</p>
                <p>Last Updated: 2024-07-25 17:59:48 UTC</p>
                <button class="interpret-button" data-id="2407.18251v1">Interpret</button>
                <div id="interpretation-2407.18251v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Trajectory-aligned Space-time Tokens for Few-shot Action Recognition</h3>
                <p>Authors: Pulkit KumarNamitha PadmanabhanLuke LuoSai Saketh RambhatlaAbhinav Shrivastava</p>
                <p><a href="http://arxiv.org/abs/2407.18249v1">Link to paper</a></p>
                <p>We propose a simple yet effective approach for few-shot action recognitionemphasizing the disentanglement of motion and appearance representations. Byharnessing recent progress in tracking specifically point trajectories andself-supervised representation learning we build trajectory-aligned tokensTATs that capture motion and appearance information. This approachsignificantly reduces the data requirements while retaining essentialinformation. To process these representations we use a Masked Space-timeTransformer that effectively learns to aggregate information to facilitatefew-shot action recognition. We demonstrate state-of-the-art results onfew-shot action recognition across multiple datasets. Our project page isavailable at https://www.cs.umd.edu/pulkit/tats</p>
                <p>Last Updated: 2024-07-25 17:59:31 UTC</p>
                <button class="interpret-button" data-id="2407.18249v1">Interpret</button>
                <div id="interpretation-2407.18249v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RegionDrag: Fast Region-Based Image Editing with Diffusion Models</h3>
                <p>Authors: Jingyi LuXinghui LiKai Han</p>
                <p><a href="http://arxiv.org/abs/2407.18247v1">Link to paper</a></p>
                <p>Point-drag-based image editing methods like DragDiffusion have attractedsignificant attention. However point-drag-based approaches suffer fromcomputational overhead and misinterpretation of user intentions due to thesparsity of point-based editing instructions. In this paper we propose aregion-based copy-and-paste dragging method RegionDrag to overcome theselimitations. RegionDrag allows users to express their editing instructions inthe form of handle and target regions enabling more precise control andalleviating ambiguity. In addition region-based operations complete editing inone iteration and are much faster than point-drag-based methods. We alsoincorporate the attention-swapping technique for enhanced stability duringediting. To validate our approach we extend existing point-drag-based datasetswith region-based dragging instructions. Experimental results demonstrate thatRegionDrag outperforms existing point-drag-based approaches in terms of speedaccuracy and alignment with user intentions. Remarkably RegionDrag completesthe edit on an image with a resolution of 512x512 in less than 2 seconds whichis more than 100x faster than DragDiffusion while achieving betterperformance. Project page: https://visual-ai.github.io/regiondrag.</p>
                <p>Last Updated: 2024-07-25 17:59:13 UTC</p>
                <button class="interpret-button" data-id="2407.18247v1">Interpret</button>
                <div id="interpretation-2407.18247v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VGGHeads: A Large-Scale Synthetic Dataset for 3D Human Heads</h3>
                <p>Authors: Orest KupynEugene KhvedcheniaChristian Rupprecht</p>
                <p><a href="http://arxiv.org/abs/2407.18245v1">Link to paper</a></p>
                <p>Human head detection keypoint estimation and 3D head model fitting areimportant tasks with many applications. However traditional real-worlddatasets often suffer from bias privacy and ethical concerns and they havebeen recorded in laboratory environments which makes it difficult for trainedmodels to generalize. Here we introduce VGGHeads -- a large scale syntheticdataset generated with diffusion models for human head detection and 3D meshestimation. Our dataset comprises over 1 million high-resolution images eachannotated with detailed 3D head meshes facial landmarks and bounding boxes.Using this dataset we introduce a new model architecture capable ofsimultaneous heads detection and head meshes reconstruction from a single imagein a single step. Through extensive experimental evaluations we demonstratethat models trained on our synthetic data achieve strong performance on realimages. Furthermore the versatility of our dataset makes it applicable acrossa broad spectrum of tasks offering a general and comprehensive representationof human heads. Additionally we provide detailed information about thesynthetic data generation pipeline enabling it to be re-used for other tasksand domains.</p>
                <p>Last Updated: 2024-07-25 17:58:17 UTC</p>
                <button class="interpret-button" data-id="2407.18245v1">Interpret</button>
                <div id="interpretation-2407.18245v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RefMask3D: Language-Guided Transformer for 3D Referring Segmentation</h3>
                <p>Authors: Shuting HeHenghui Ding</p>
                <p><a href="http://arxiv.org/abs/2407.18244v1">Link to paper</a></p>
                <p>3D referring segmentation is an emerging and challenging vision-language taskthat aims to segment the object described by a natural language expression in apoint cloud scene. The key challenge behind this task is vision-languagefeature fusion and alignment. In this work we propose RefMask3D to explore thecomprehensive multi-modal feature interaction and understanding. First wepropose a Geometry-Enhanced Group-Word Attention to integrate language withgeometrically coherent sub-clouds through cross-modal group-word attentionwhich effectively addresses the challenges posed by the sparse and irregularnature of point clouds. Then we introduce a Linguistic Primitives Constructionto produce semantic primitives representing distinct semantic attributes whichgreatly enhance the vision-language understanding at the decoding stage.Furthermore we introduce an Object Cluster Module that analyzes theinterrelationships among linguistic primitives to consolidate their insightsand pinpoint common characteristics helping to capture holistic informationand enhance the precision of target identification. The proposed RefMask3Dachieves new state-of-the-art performance on 3D referring segmentation 3Dvisual grounding and also 2D referring image segmentation. EspeciallyRefMask3D outperforms previous state-of-the-art method by a large margin of3.16 mIoU on the challenging ScanRefer dataset. Code is available athttps://github.com/heshuting555/RefMask3D.</p>
                <p>Last Updated: 2024-07-25 17:58:03 UTC</p>
                <button class="interpret-button" data-id="2407.18244v1">Interpret</button>
                <div id="interpretation-2407.18244v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-07-27</p>
        </div>
    
        </div>
    </body>
    </html>
    