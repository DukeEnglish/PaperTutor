
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Effect of Weight Quantization on Learning Models by Typical Case Analysis</h3>
                <p>Authors: Shuhei KashiwamuraAyaka SakataMasaaki Imaizumi</p>
                <p><a href="http://arxiv.org/abs/2401.17269v1">Link to paper</a></p>
                <p>This paper examines the quantization methods used in large-scale dataanalysis models and their hyperparameter choices. The recent surge in dataanalysis scale has significantly increased computational resource requirements.To address this quantizing model weights has become a prevalent practice indata analysis applications such as deep learning. Quantization is particularlyvital for deploying large models on devices with limited computationalresources. However the selection of quantization hyperparameters like thenumber of bits and value range for weight quantization remains anunderexplored area. In this study we employ the typical case analysis fromstatistical physics specifically the replica method to explore the impact ofhyperparameters on the quantization of simple learning models. Our analysisyields three key findings: i an unstable hyperparameter phase known asreplica symmetry breaking occurs with a small number of bits and a largequantization width ii there is an optimal quantization width that minimizeserror and iii quantization delays the onset of overparameterization helpingto mitigate overfitting as indicated by the double descent phenomenon. We alsodiscover that non-uniform quantization can enhance stability. Additionally wedevelop an approximate message-passing algorithm to validate our theoreticalresults.</p>
                <p>Last Updated: 2024-01-30 18:58:46 UTC</p>
                <button class="interpret-button" data-id="2401.17269v1">Interpret</button>
                <div id="interpretation-2401.17269v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Weaver: Foundation Models for Creative Writing</h3>
                <p>Authors: Tiannan WangJiamin ChenQingrui JiaShuai WangRuoyu FangHuilin WangZhaowei GaoChunzhao XieChuou XuJihong DaiYibin LiuJialong WuShengwei DingLong LiZhiwei HuangXinle DengTeng YuGangan MaHan XiaoZixin ChenDanjun XiangYunxia WangYuanyuan ZhuYi XiaoJing WangYiru WangSiran DingJiayang HuangJiayi XuYilihamu TayierZhenyu HuYuan GaoChengfeng ZhengYueshu YeYihang LiLei WanXinyue JiangYujie WangSiyu ChengZhule SongXiangru TangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor JiangWangchunshu Zhou</p>
                <p><a href="http://arxiv.org/abs/2401.17268v1">Link to paper</a></p>
                <p>This work introduces Weaver our first family of large language models LLMsdedicated to content creation. Weaver is pre-trained on a carefully selectedcorpus that focuses on improving the writing capabilities of large languagemodels. We then fine-tune Weaver for creative and professional writing purposesand align it to the preference of professional writers using a suit of novelmethods for instruction data synthesis and LLM alignment making it able toproduce more human-like texts and follow more diverse instructions for contentcreation. The Weaver family consists of models of Weaver Mini 1.8B WeaverBase 6B Weaver Pro 14B and Weaver Ultra 34B sizes suitable fordifferent applications and can be dynamically dispatched by a routing agentaccording to query complexity to balance response quality and computation cost.Evaluation on a carefully curated benchmark for assessing the writingcapabilities of LLMs shows Weaver models of all sizes outperform generalistLLMs several times larger than them. Notably our most-capable Weaver Ultramodel surpasses GPT-4 a state-of-the-art generalist LLM on various writingscenarios demonstrating the advantage of training specialized LLMs for writingpurposes. Moreover Weaver natively supports retrieval-augmented generationRAG and function calling tool usage. We present various use cases of theseabilities for improving AI-assisted writing systems including integration ofexternal knowledge bases tools or APIs and providing personalized writingassistance. Furthermore we discuss and summarize a guideline and bestpractices for pre-training and fine-tuning domain-specific LLMs.</p>
                <p>Last Updated: 2024-01-30 18:58:43 UTC</p>
                <button class="interpret-button" data-id="2401.17268v1">Interpret</button>
                <div id="interpretation-2401.17268v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ReacLLaMA: Merging chemical and textual information in chemical reactivity AI models</h3>
                <p>Authors: Aline HartgersRamil NugmanovKostiantyn ChernichenkoJoerg Kurt Wegner</p>
                <p><a href="http://arxiv.org/abs/2401.17267v1">Link to paper</a></p>
                <p>Chemical reactivity models are developed to predict chemical reactionoutcomes in the form of classification success/failure or regression productyield tasks. The vast majority of the reported models are trained solely onchemical information such as reactants products reagents and solvents butnot on the details of a synthetic protocol. Herein incorporation of proceduraltext with the aim to augment the Graphormer reactivity model and improve itsaccuracy is presented. Two major approaches are used: training an adapterGraphormer model that is provided with a GPT-2-derived latent representation ofthe text procedure ReacLLaMA-Adapter and labeling an unlabeled part of adataset with the LLaMA 2 model followed by training the Graphormer on anextended dataset Zero-Shot Labeling ReacLLaMA. Both methodologies enhance thediscernment of unpromising reactions thereby providing more accurate modelswith improved specificity.</p>
                <p>Last Updated: 2024-01-30 18:57:08 UTC</p>
                <button class="interpret-button" data-id="2401.17267v1">Interpret</button>
                <div id="interpretation-2401.17267v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks</h3>
                <p>Authors: Andy ZhouBo LiHaohan Wang</p>
                <p><a href="http://arxiv.org/abs/2401.17263v1">Link to paper</a></p>
                <p>Despite advances in AI alignment language models LM remain vulnerable toadversarial attacks or jailbreaking in which adversaries modify input promptsto induce harmful behavior. While some defenses have been proposed they focuson narrow threat models and fall short of a strong defense which we positshould be effective universal and practical. To achieve this we propose thefirst adversarial objective for defending LMs against jailbreaking attacks andan algorithm robust prompt optimization RPO that uses gradient-based tokenoptimization to enforce harmless outputs. This results in an easily accessiblesuffix that significantly improves robustness to both jailbreaks seen duringoptimization and unknown held-out jailbreaks reducing the attack success rateon Starling-7B from 84 to 8.66 across 20 jailbreaks. In addition we findthat RPO has a minor effect on normal LM use is successful under adaptiveattacks and can transfer to black-box models reducing the success rate of thestrongest attack on GPT-4 from 92 to 6.</p>
                <p>Last Updated: 2024-01-30 18:56:08 UTC</p>
                <button class="interpret-button" data-id="2401.17263v1">Interpret</button>
                <div id="interpretation-2401.17263v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ReAlnet: Achieving More Human Brain-Like Vision via Human Neural Representational Alignment</h3>
                <p>Authors: Zitong LuYile WangJulie D. Golomb</p>
                <p><a href="http://arxiv.org/abs/2401.17231v1">Link to paper</a></p>
                <p>Despite the remarkable strides made in artificial intelligence currentobject recognition models still lag behind in emulating the mechanism of visualinformation processing in human brains. Recent studies have highlighted thepotential of using neural data to mimic brain processing however these oftenreply on invasive neural recordings from non-human subjects leaving a criticalgap in our understanding of human visual perception and the development of morehuman brain-like vision models. Addressing this gap we present for the firsttime RepresentationalAlignmentnet a vision model aligned with humanbrain activity based on non-invasive EEG recordings demonstrating asignificantly higher similarity to human brain representations. Our innovativeimage-to-brain multi-layer encoding alignment framework not only optimizesmultiple layers of the model marking a substantial leap in neural alignmentbut also enables the model to efficiently learn and mimic human brains visualrepresentational patterns across object categories and different neural datamodalities. Furthermore we discover that alignment with human brainrepresentations improves the models adversarial robustness. Our findingssuggest that ReAlnet sets a new precedent in the field bridging the gapbetween artificial and human vision and paving the way for more brain-likeartificial intelligence systems.</p>
                <p>Last Updated: 2024-01-30 18:18:41 UTC</p>
                <button class="interpret-button" data-id="2401.17231v1">Interpret</button>
                <div id="interpretation-2401.17231v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>A simple, strong baseline for building damage detection on the xBD dataset</h3>
                <p>Authors: Sebastian GerardPaul Borne-PonsJosephine Sullivan</p>
                <p><a href="http://arxiv.org/abs/2401.17271v1">Link to paper</a></p>
                <p>We construct a strong baseline method for building damage detection bystarting with the highly-engineered winning solution of the xView2 competitionand gradually stripping away components. This way we obtain a much simplermethod while retaining adequate performance. We expect the simplified solutionto be more widely and easily applicable. This expectation is based on thereduced complexity as well as the fact that we choose hyperparameters based onsimple heuristics that transfer to other datasets. We then re-arrange thexView2 dataset splits such that the test locations are not seen duringtraining contrary to the competition setup. In this setting we find that boththe complex and the simplified model fail to generalize to unseen locations.Analyzing the dataset indicates that this failure to generalize is not only amodel-based problem but that the difficulty might also be influenced by theunequal class distributions between events.  Code including the baseline model is available underhttps://github.com/PaulBorneP/Xview2_Strong_Baseline</p>
                <p>Last Updated: 2024-01-30 18:59:56 UTC</p>
                <button class="interpret-button" data-id="2401.17271v1">Interpret</button>
                <div id="interpretation-2401.17271v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>YOLO-World: Real-Time Open-Vocabulary Object Detection</h3>
                <p>Authors: Tianheng ChengLin SongYixiao GeWenyu LiuXinggang WangYing Shan</p>
                <p><a href="http://arxiv.org/abs/2401.17270v1">Link to paper</a></p>
                <p>The You Only Look Once YOLO series of detectors have established themselvesas efficient and practical tools. However their reliance on predefined andtrained object categories limits their applicability in open scenarios.Addressing this limitation we introduce YOLO-World an innovative approachthat enhances YOLO with open-vocabulary detection capabilities throughvision-language modeling and pre-training on large-scale datasets.Specifically we propose a new Re-parameterizable Vision-Language PathAggregation Network RepVL-PAN and region-text contrastive loss to facilitatethe interaction between visual and linguistic information. Our method excels indetecting a wide range of objects in a zero-shot manner with high efficiency.On the challenging LVIS dataset YOLO-World achieves 35.4 AP with 52.0 FPS onV100 which outperforms many state-of-the-art methods in terms of both accuracyand speed. Furthermore the fine-tuned YOLO-World achieves remarkableperformance on several downstream tasks including object detection andopen-vocabulary instance segmentation.</p>
                <p>Last Updated: 2024-01-30 18:59:38 UTC</p>
                <button class="interpret-button" data-id="2401.17270v1">Interpret</button>
                <div id="interpretation-2401.17270v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks</h3>
                <p>Authors: Andy ZhouBo LiHaohan Wang</p>
                <p><a href="http://arxiv.org/abs/2401.17263v1">Link to paper</a></p>
                <p>Despite advances in AI alignment language models LM remain vulnerable toadversarial attacks or jailbreaking in which adversaries modify input promptsto induce harmful behavior. While some defenses have been proposed they focuson narrow threat models and fall short of a strong defense which we positshould be effective universal and practical. To achieve this we propose thefirst adversarial objective for defending LMs against jailbreaking attacks andan algorithm robust prompt optimization RPO that uses gradient-based tokenoptimization to enforce harmless outputs. This results in an easily accessiblesuffix that significantly improves robustness to both jailbreaks seen duringoptimization and unknown held-out jailbreaks reducing the attack success rateon Starling-7B from 84 to 8.66 across 20 jailbreaks. In addition we findthat RPO has a minor effect on normal LM use is successful under adaptiveattacks and can transfer to black-box models reducing the success rate of thestrongest attack on GPT-4 from 92 to 6.</p>
                <p>Last Updated: 2024-01-30 18:56:08 UTC</p>
                <button class="interpret-button" data-id="2401.17263v1">Interpret</button>
                <div id="interpretation-2401.17263v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation</h3>
                <p>Authors: Mehdi NorooziIsma HadjiBrais MartinezAdrian BulatGeorgios Tzimiropoulos</p>
                <p><a href="http://arxiv.org/abs/2401.17258v1">Link to paper</a></p>
                <p>In this paper we introduce YONOS-SR a novel stable diffusion-based approachfor image super-resolution that yields state-of-the-art results using only asingle DDIM step. We propose a novel scale distillation approach to train ourSR model. Instead of directly training our SR model on the scale factor ofinterest we start by training a teacher model on a smaller magnificationscale thereby making the SR problem simpler for the teacher. We then train astudent model for a higher magnification scale using the predictions of theteacher as a target during the training. This process is repeated iterativelyuntil we reach the target scale factor of the final model. The rationale behindour scale distillation is that the teacher aids the student diffusion modeltraining by i providing a target adapted to the current noise level ratherthan using the same target coming from ground truth data for all noise levelsand ii providing an accurate target as the teacher has a simpler task tosolve. We empirically show that the distilled model significantly outperformsthe model trained for high scales directly specifically with few steps duringinference. Having a strong diffusion model that requires only one step allowsus to freeze the U-Net and fine-tune the decoder on top of it. We show that thecombination of spatially distilled U-Net and fine-tuned decoder outperformsstate-of-the-art methods requiring 200 steps with only one single step.</p>
                <p>Last Updated: 2024-01-30 18:49:44 UTC</p>
                <button class="interpret-button" data-id="2401.17258v1">Interpret</button>
                <div id="interpretation-2401.17258v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SLIC: A Learned Image Codec Using Structure and Color</h3>
                <p>Authors: Srivatsa PrativadibhayankaramMahadev Prasad PandaThomas RichterHeiko SparenbergSiegfried FößelAndré Kaup</p>
                <p><a href="http://arxiv.org/abs/2401.17246v1">Link to paper</a></p>
                <p>We propose the structure and color based learned image codec SLIC in whichthe task of compression is split into that of luminance and chrominance. Thedeep learning model is built with a novel multi-scale architecture for Y and UVchannels in the encoder where the features from various stages are combined toobtain the latent representation. An autoregressive context model is employedfor backward adaptation and a hyperprior block for forward adaptation. Variousexperiments are carried out to study and analyze the performance of theproposed model and to compare it with other image codecs. We also illustratethe advantages of our method through the visualization of channel impulseresponses latent channels and various ablation studies. The model achievesBjontegaard delta bitrate gains of 7.5 and 4.66 in terms of MS-SSIM andCIEDE2000 metrics with respect to other state-of-the-art reference codecs.</p>
                <p>Last Updated: 2024-01-30 18:39:54 UTC</p>
                <button class="interpret-button" data-id="2401.17246v1">Interpret</button>
                <div id="interpretation-2401.17246v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>GazeGPT: Augmenting Human Capabilities using Gaze-contingent Contextual AI for Smart Eyewear</h3>
                <p>Authors: Robert KonradNitish PadmanabanJ. Gabriel BuckmasterKevin C. BoyleGordon Wetzstein</p>
                <p><a href="http://arxiv.org/abs/2401.17217v2">Link to paper</a></p>
                <p>Multimodal large language models LMMs excel in world knowledge andproblem-solving abilities. Through the use of a world-facing camera andcontextual AI emerging smart accessories aim to provide a seamless interfacebetween humans and LMMs. Yet these wearable computing systems lack anunderstanding of the users attention. We introduce GazeGPT as a new userinteraction paradigm for contextual AI. GazeGPT uses eye tracking to help theLMM understand which object in the world-facing camera view a user is payingattention to. Using extensive user evaluations we show that thisgaze-contingent mechanism is a faster and more accurate pointing mechanism thanalternatives that it augments human capabilities by significantly improvingtheir accuracy in a dog-breed classification task and that it is consistentlyranked as more natural than head- or body-driven selection mechanisms forcontextual AI. Moreover we prototype a variety of application scenarios thatsuggest GazeGPT could be of significant value to users as part of futureAI-driven personal assistants.</p>
                <p>Last Updated: 2024-01-31 05:21:13 UTC</p>
                <button class="interpret-button" data-id="2401.17217v2">Interpret</button>
                <div id="interpretation-2401.17217v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat</h3>
                <p>Authors: John ChenXi LuMichael RejtigDavid DuRuth BagleyMichael S. HornUri J. Wilensky</p>
                <p><a href="http://dx.doi.org/10.1145/3613904.3642377">Link to paper</a></p>
                <p>Large Language Models LLMs have the potential to fundamentally change theway people engage in computer programming. Agent-based modeling ABM hasbecome ubiquitous in natural and social sciences and education yet no priorstudies have explored the potential of LLMs to assist it. We designed NetLogoChat to support the learning and practice of NetLogo a programming languagefor ABM. To understand how users perceive use and need LLM-based interfaceswe interviewed 30 participants from global academia industry and graduateschools. Experts reported more perceived benefits than novices and were moreinclined to adopt LLMs in their workflow. We found significant differencesbetween experts and novices in their perceptions behaviors and needs forhuman-AI collaboration. We surfaced a knowledge gap between experts and novicesas a possible reason for the benefit gap. We identified guidancepersonalization and integration as major needs for LLM-based interfaces tosupport the programming of ABM.</p>
                <p>Last Updated: 2024-01-30 16:49:50 UTC</p>
                <button class="interpret-button" data-id="2401.17163v1">Interpret</button>
                <div id="interpretation-2401.17163v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Wrist movement classification for adaptive mobile phone based rehabilitation of children with motor skill impairments</h3>
                <p>Authors: Kayleigh SchoorlTamara Pinos CisnerosAlbert Ali SalahBen Schouten</p>
                <p><a href="http://arxiv.org/abs/2401.17134v1">Link to paper</a></p>
                <p>Rehabilitation exercises performed by children with cerebral palsy aretedious and repetitive. To make them more engaging we propose to use anexergame approach where an adaptive application can help the child remainstimulated and interested during exercises. In this paper we describe how themobile phone sensors can be used to classify wrist movements of the user duringthe rehabilitation exercises to detect if the user is performing the correctexercise and illustrate the use of our approach in an actual mobile phoneapplication. We also show how an adaptive difficulty system was added to theapplication to allow the system to adjust to the user. We present experimentalresults from a pilot with healthy subjects that were constrained to simulaterestricted wrist movements as well as from tests with a target group ofchildren with cerebral palsy. Our results show that wrist movementclassification is successfully achieved and results in improved interactions.</p>
                <p>Last Updated: 2024-01-30 16:11:31 UTC</p>
                <button class="interpret-button" data-id="2401.17134v1">Interpret</button>
                <div id="interpretation-2401.17134v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PlantoGraphy: Incorporating Iterative Design Process into Generative Artificial Intelligence for Landscape Rendering</h3>
                <p>Authors: Rong HuangHai-Chuan LinChuanzhang ChenKang ZhangWei Zeng</p>
                <p><a href="http://dx.doi.org/10.1145/3613904.3642824">Link to paper</a></p>
                <p>Landscape renderings are realistic images of landscape sites allowingstakeholders to perceive better and evaluate design ideas. While recentadvances in Generative Artificial Intelligence GAI enable automatedgeneration of landscape renderings the end-to-end methods are not compatiblewith common design processes leading to insufficient alignment with designidealizations and limited cohesion of iterative landscape design. Informed by aformative study for comprehending design requirements we present PlantoGraphyan iterative design system that allows for interactive configuration of GAImodels to accommodate human-centered design practice. A two-stage pipeline isincorporated: first concretization module transforms conceptual ideas intoconcrete scene layouts with a domain-oriented large language model and secondillustration module converts scene layouts into realistic landscape renderingsusing a fine-tuned low-rank adaptation diffusion model. PlantoGraphy hasundergone a series of performance evaluations and user studies demonstratingits effectiveness in landscape rendering generation and the high recognition ofits interactive functionality.</p>
                <p>Last Updated: 2024-01-30 15:53:42 UTC</p>
                <button class="interpret-button" data-id="2401.17120v1">Interpret</button>
                <div id="interpretation-2401.17120v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Influence of Presentation and Performance on User Satisfaction</h3>
                <p>Authors: Kanaad PathakLeif AzzopardiMartin Halvey</p>
                <p><a href="http://arxiv.org/abs/2401.17100v1">Link to paper</a></p>
                <p>The effectiveness of an IR system is gauged not just by its ability toretrieve relevant results but also by how it presents these results to usersan engaging presentation often correlates with increased user satisfaction.While existing research has delved into the link between user satisfaction IRperformance metrics and presentation these aspects have typically beeninvestigated in isolation. Our research aims to bridge this gap by examiningthe relationship between query performance presentation and user satisfaction.For our analysis we conducted a between-subjects experiment comparing theeffectiveness of various result card layouts for an ad-hoc news searchinterface. Drawing data from the TREC WaPo 2018 collection we centered ourstudy on four specific topics. Within each of these topics we assessed sixdistinct queries with varying nDCG values. Our study involved 164 participantswho were exposed to one of five distinct layouts containing result cards suchas title titleimage or titleimagesummary. Our findings indicatethat while nDCG is a strong predictor of user satisfaction at the query levelthere exists no linear relationship between the performance of the querypresentation of results and user satisfaction. However when considering thetotal gain on the initial result page we observed that presentation does playa significant role in user satisfaction at the query level for certainlayouts with result cards such as titleimage or titleimagesummary. Ourresults also suggest that the layout differences have complex and multifacetedimpacts on satisfaction. We demonstrate the capacity to equalize usersatisfaction levels between queries of varying performance by changing howresults are presented. This emphasizes the necessity to harmonize bothperformance and presentation in IR systems considering users diversepreferences.</p>
                <p>Last Updated: 2024-01-30 15:32:56 UTC</p>
                <button class="interpret-button" data-id="2401.17100v1">Interpret</button>
                <div id="interpretation-2401.17100v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Weaver: Foundation Models for Creative Writing</h3>
                <p>Authors: Tiannan WangJiamin ChenQingrui JiaShuai WangRuoyu FangHuilin WangZhaowei GaoChunzhao XieChuou XuJihong DaiYibin LiuJialong WuShengwei DingLong LiZhiwei HuangXinle DengTeng YuGangan MaHan XiaoZixin ChenDanjun XiangYunxia WangYuanyuan ZhuYi XiaoJing WangYiru WangSiran DingJiayang HuangJiayi XuYilihamu TayierZhenyu HuYuan GaoChengfeng ZhengYueshu YeYihang LiLei WanXinyue JiangYujie WangSiyu ChengZhule SongXiangru TangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor JiangWangchunshu Zhou</p>
                <p><a href="http://arxiv.org/abs/2401.17268v1">Link to paper</a></p>
                <p>This work introduces Weaver our first family of large language models LLMsdedicated to content creation. Weaver is pre-trained on a carefully selectedcorpus that focuses on improving the writing capabilities of large languagemodels. We then fine-tune Weaver for creative and professional writing purposesand align it to the preference of professional writers using a suit of novelmethods for instruction data synthesis and LLM alignment making it able toproduce more human-like texts and follow more diverse instructions for contentcreation. The Weaver family consists of models of Weaver Mini 1.8B WeaverBase 6B Weaver Pro 14B and Weaver Ultra 34B sizes suitable fordifferent applications and can be dynamically dispatched by a routing agentaccording to query complexity to balance response quality and computation cost.Evaluation on a carefully curated benchmark for assessing the writingcapabilities of LLMs shows Weaver models of all sizes outperform generalistLLMs several times larger than them. Notably our most-capable Weaver Ultramodel surpasses GPT-4 a state-of-the-art generalist LLM on various writingscenarios demonstrating the advantage of training specialized LLMs for writingpurposes. Moreover Weaver natively supports retrieval-augmented generationRAG and function calling tool usage. We present various use cases of theseabilities for improving AI-assisted writing systems including integration ofexternal knowledge bases tools or APIs and providing personalized writingassistance. Furthermore we discuss and summarize a guideline and bestpractices for pre-training and fine-tuning domain-specific LLMs.</p>
                <p>Last Updated: 2024-01-30 18:58:43 UTC</p>
                <button class="interpret-button" data-id="2401.17268v1">Interpret</button>
                <div id="interpretation-2401.17268v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Proactive Detection of Voice Cloning with Localized Watermarking</h3>
                <p>Authors: Robin San RomanPierre FernandezAlexandre DéfossezTeddy FuronTuan TranHady Elsahar</p>
                <p><a href="http://arxiv.org/abs/2401.17264v1">Link to paper</a></p>
                <p>In the rapidly evolving field of speech generative models there is apressing need to ensure audio authenticity against the risks of voice cloning.We present AudioSeal the first audio watermarking technique designedspecifically for localized detection of AI-generated speech. AudioSeal employsa generator/detector architecture trained jointly with a localization loss toenable localized watermark detection up to the sample level and a novelperceptual loss inspired by auditory masking that enables AudioSeal to achievebetter imperceptibility. AudioSeal achieves state-of-the-art performance interms of robustness to real life audio manipulations and imperceptibility basedon automatic and human evaluation metrics. Additionally AudioSeal is designedwith a fast single-pass detector that significantly surpasses existing modelsin speed - achieving detection up to two orders of magnitude faster making itideal for large-scale and real-time applications.</p>
                <p>Last Updated: 2024-01-30 18:56:22 UTC</p>
                <button class="interpret-button" data-id="2401.17264v1">Interpret</button>
                <div id="interpretation-2401.17264v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks</h3>
                <p>Authors: Andy ZhouBo LiHaohan Wang</p>
                <p><a href="http://arxiv.org/abs/2401.17263v1">Link to paper</a></p>
                <p>Despite advances in AI alignment language models LM remain vulnerable toadversarial attacks or jailbreaking in which adversaries modify input promptsto induce harmful behavior. While some defenses have been proposed they focuson narrow threat models and fall short of a strong defense which we positshould be effective universal and practical. To achieve this we propose thefirst adversarial objective for defending LMs against jailbreaking attacks andan algorithm robust prompt optimization RPO that uses gradient-based tokenoptimization to enforce harmless outputs. This results in an easily accessiblesuffix that significantly improves robustness to both jailbreaks seen duringoptimization and unknown held-out jailbreaks reducing the attack success rateon Starling-7B from 84 to 8.66 across 20 jailbreaks. In addition we findthat RPO has a minor effect on normal LM use is successful under adaptiveattacks and can transfer to black-box models reducing the success rate of thestrongest attack on GPT-4 from 92 to 6.</p>
                <p>Last Updated: 2024-01-30 18:56:08 UTC</p>
                <button class="interpret-button" data-id="2401.17263v1">Interpret</button>
                <div id="interpretation-2401.17263v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation</h3>
                <p>Authors: Yuan ChiangChia-Hong ChouJanosh Riebesell</p>
                <p><a href="http://arxiv.org/abs/2401.17244v1">Link to paper</a></p>
                <p>Reducing hallucination of Large Language Models LLMs is imperative for usein the sciences where reproducibility is crucial. However LLMs inherently lacklong-term memory making it a nontrivial ad hoc and inevitably biased task tofine-tune them on domain-specific literature and data. Here we introduce LLaMPa multimodal retrieval-augmented generation RAG framework of multipledata-aware reasoning-and-acting ReAct agents that dynamically interact withcomputational and experimental data on Materials Project MP. Withoutfine-tuning LLaMP demonstrates an ability to comprehend and integrate variousmodalities of materials science concepts fetch relevant data stores on thefly process higher-order data such as crystal structures and elastictensors and summarize multi-step procedures for solid-state synthesis. Weshow that LLaMP effectively corrects errors in GPT-3.5s intrinsic knowledgereducing a 5.21 MAPE on frequently-documented bandgaps and a significant1103.54 MAPE on formation energies -- errors that GPT-3.5 seems to derive frommixed data sources. Additionally LLaMP substantially reduces the hallucinatedvolumetric strain in a diamond cubic silicon structure from 66.3 to 0. Theproposed framework offers an intuitive and nearly hallucination-free approachto exploring materials informatics and establishes a pathway for knowledgedistillation and fine-tuning other language models. We envision the frameworkas a valuable component for scientific hypotheses and a foundation for futureautonomous laboratories where multiple LLM agents communicate and cooperatewith robotics to drive material synthesis and chemical reactions withouthard-coded human logic and intervention.</p>
                <p>Last Updated: 2024-01-30 18:37:45 UTC</p>
                <button class="interpret-button" data-id="2401.17244v1">Interpret</button>
                <div id="interpretation-2401.17244v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models</h3>
                <p>Authors: Jee-weon JungWangyou ZhangJiatong ShiZakaria AldenehTakuya HiguchiBarry-John TheobaldAhmed Hussen AbdelazizShinji Watanabe</p>
                <p><a href="http://arxiv.org/abs/2401.17230v1">Link to paper</a></p>
                <p>This paper introduces ESPnet-SPK a toolkit designed with several objectivesfor training speaker embedding extractors. First we provide an open-sourceplatform for researchers in the speaker recognition community to effortlesslybuild models. We provide several models ranging from x-vector to recentSKA-TDNN. Through the modularized architecture design variants can bedeveloped easily. We also aspire to bridge developed models with other domainsfacilitating the broad research community to effortlessly incorporatestate-of-the-art embedding extractors. Pre-trained embedding extractors can beaccessed in an off-the-shelf manner and we demonstrate the toolkitsversatility by showcasing its integration with two tasks. Another goal is tointegrate with diverse self-supervised learning features. We release areproducible recipe that achieves an equal error rate of 0.39 on the Vox1-Oevaluation protocol using WavLM-Large with ECAPA-TDNN.</p>
                <p>Last Updated: 2024-01-30 18:18:27 UTC</p>
                <button class="interpret-button" data-id="2401.17230v1">Interpret</button>
                <div id="interpretation-2401.17230v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Scalable Mechanism Design for Multi-Agent Path Finding</h3>
                <p>Authors: Paul FriedrichYulun ZhangMichael CurryLudwig DierksStephen McAleerJiaoyang LiTuomas SandholmSven Seuken</p>
                <p><a href="http://arxiv.org/abs/2401.17044v1">Link to paper</a></p>
                <p>Multi-Agent Path Finding MAPF involves determining paths for multipleagents to travel simultaneously through a shared area toward particular goallocations. This problem is computationally complex especially when dealingwith large numbers of agents as is common in realistic applications likeautonomous vehicle coordination. Finding an optimal solution is oftencomputationally infeasible making the use of approximate algorithms essential.Adding to the complexity agents might act in a self-interested and strategicway possibly misrepresenting their goals to the MAPF algorithm if it benefitsthem. Although the field of mechanism design offers tools to align incentivesusing these tools without careful consideration can fail when only havingaccess to approximately optimal outcomes. Since approximations are crucial forscalable MAPF algorithms this poses a significant challenge. In this work weintroduce the problem of scalable mechanism design for MAPF and propose threestrategyproof mechanisms two of which even use approximate MAPF algorithms. Wetest our mechanisms on realistic MAPF domains with problem sizes ranging fromdozens to hundreds of agents. Our findings indicate that they improve welfarebeyond a simple baseline.</p>
                <p>Last Updated: 2024-01-30 14:26:04 UTC</p>
                <button class="interpret-button" data-id="2401.17044v1">Interpret</button>
                <div id="interpretation-2401.17044v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Congestion Pricing for Efficiency and Equity: Theory and Applications to the San Francisco Bay Area</h3>
                <p>Authors: Chinmay MaheshwariKshitij KulkarniDruv PaiJiarui YangManxi WuShankar Sastry</p>
                <p><a href="http://arxiv.org/abs/2401.16844v1">Link to paper</a></p>
                <p>Congestion pricing while adopted by many cities to alleviate trafficcongestion raises concerns about widening socioeconomic disparities due to itsdisproportionate impact on low-income travelers. In this study we address thisconcern by proposing a new class of congestion pricing schemes that not onlyminimize congestion levels but also incorporate an equity objective to reducecost disparities among travelers with different willingness-to-pay. Ouranalysis builds on a congestion game model with heterogeneous travelerpopulations. We present four pricing schemes that account for practicalconsiderations such as the ability to charge differentiated tolls to varioustraveler populations and the option to toll all or only a subset of edges inthe network. We evaluate our pricing schemes in the calibrated freeway networkof the San Francisco Bay Area. We demonstrate that the proposed congestionpricing schemes improve both efficiency in terms of reduced average traveltime and equity the disparities of travel costs experienced by differentpopulations compared to the current pricing scheme. Moreover our pricingschemes also generate a total revenue comparable to the current pricing scheme.Our results further show that pricing schemes charging differentiated prices totraveler populations with varying willingness-to-pay lead to a more equitabledistribution of travel costs compared to those that charge a homogeneous priceto all.</p>
                <p>Last Updated: 2024-01-30 09:35:02 UTC</p>
                <button class="interpret-button" data-id="2401.16844v1">Interpret</button>
                <div id="interpretation-2401.16844v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning to Manipulate under Limited Information</h3>
                <p>Authors: Wesley H. HollidayAlexander KristoffersenEric Pacuit</p>
                <p><a href="http://arxiv.org/abs/2401.16412v1">Link to paper</a></p>
                <p>By classic results in social choice theory any reasonable preferentialvoting method sometimes gives individuals an incentive to report an insincerepreference. The extent to which different voting methods are more or lessresistant to such strategic manipulation has become a key consideration forcomparing voting methods. Here we measure resistance to manipulation by whetherneural networks of varying sizes can learn to profitably manipulate a givenvoting method in expectation given different types of limited informationabout how other voters will vote. We trained nearly 40000 neural networks of26 sizes to manipulate against 8 different voting methods under 6 types oflimited information in committee-sized elections with 5-21 voters and 3-6candidates. We find that some voting methods such as Borda are highlymanipulable by networks with limited information while others such as InstantRunoff are not despite being quite profitably manipulated by an idealmanipulator with full information.</p>
                <p>Last Updated: 2024-01-29 18:49:50 UTC</p>
                <button class="interpret-button" data-id="2401.16412v1">Interpret</button>
                <div id="interpretation-2401.16412v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Towards Regret Free Slot Allocation in Billboard Advertisement</h3>
                <p>Authors: Dildar AliSuman BanerjeeYamuna Prasad</p>
                <p><a href="http://arxiv.org/abs/2401.16464v1">Link to paper</a></p>
                <p>Creating and maximizing influence among the customers is one of the centralgoals of an advertiser and hence remains an active area of research in recenttimes. In this advertisement technique the advertisers approach an influenceprovider for a specific number of views of their content on a payment basis.Now if the influence provider can provide the required number of views ormore he will receive the full else a partial payment. In the context of aninfluence provider it is a loss for him if he offers more or less views. Thisis formalized as Regret and naturally in the context of the influenceprovider the goal will be to minimize this quantity. In this paper we solvethis problem in the context of billboard advertisement and pose it as adiscrete optimization problem. We propose four efficient solution approachesfor this problem and analyze them to understand their time and spacecomplexity. We implement all the solution methodologies with real-life datasetsand compare the obtained results with the existing solution approaches from theliterature. We observe that the proposed solutions lead to less regret whiletaking less computational time.</p>
                <p>Last Updated: 2024-01-29 16:10:05 UTC</p>
                <button class="interpret-button" data-id="2401.16464v1">Interpret</button>
                <div id="interpretation-2401.16464v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Effective Communication with Dynamic Feature Compression</h3>
                <p>Authors: Pietro TalliFrancesco PaseFederico ChiariottiAndrea ZanellaMichele Zorzi</p>
                <p><a href="http://arxiv.org/abs/2401.16236v1">Link to paper</a></p>
                <p>The remote wireless control of industrial systems is one of the major usecases for 5G and beyond systems: in these cases the massive amounts of sensoryinformation that need to be shared over the wireless medium may overload evenhigh-capacity connections. Consequently solving the effective communicationproblem by optimizing the transmission strategy to discard irrelevantinformation can provide a significant advantage but is often a very complextask. In this work we consider a prototypal system in which an observer mustcommunicate its sensory data to a robot controlling a task e.g. a mobilerobot in a factory. We then model it as a remote Partially Observable MarkovDecision Process POMDP considering the effect of adopting semantic andeffective communication-oriented solutions on the overall system performance.We split the communication problem by considering an ensemble Vector QuantizedVariational Autoencoder VQ-VAE encoding and train a Deep ReinforcementLearning DRL agent to dynamically adapt the quantization level consideringboth the current state of the environment and the memory of past messages. Wetested the proposed approach on the well-known CartPole reference controlproblem obtaining a significant performance increase over traditionalapproaches.</p>
                <p>Last Updated: 2024-01-29 15:35:05 UTC</p>
                <button class="interpret-button" data-id="2401.16236v1">Interpret</button>
                <div id="interpretation-2401.16236v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Weaver: Foundation Models for Creative Writing</h3>
                <p>Authors: Tiannan WangJiamin ChenQingrui JiaShuai WangRuoyu FangHuilin WangZhaowei GaoChunzhao XieChuou XuJihong DaiYibin LiuJialong WuShengwei DingLong LiZhiwei HuangXinle DengTeng YuGangan MaHan XiaoZixin ChenDanjun XiangYunxia WangYuanyuan ZhuYi XiaoJing WangYiru WangSiran DingJiayang HuangJiayi XuYilihamu TayierZhenyu HuYuan GaoChengfeng ZhengYueshu YeYihang LiLei WanXinyue JiangYujie WangSiyu ChengZhule SongXiangru TangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor JiangWangchunshu Zhou</p>
                <p><a href="http://arxiv.org/abs/2401.17268v1">Link to paper</a></p>
                <p>This work introduces Weaver our first family of large language models LLMsdedicated to content creation. Weaver is pre-trained on a carefully selectedcorpus that focuses on improving the writing capabilities of large languagemodels. We then fine-tune Weaver for creative and professional writing purposesand align it to the preference of professional writers using a suit of novelmethods for instruction data synthesis and LLM alignment making it able toproduce more human-like texts and follow more diverse instructions for contentcreation. The Weaver family consists of models of Weaver Mini 1.8B WeaverBase 6B Weaver Pro 14B and Weaver Ultra 34B sizes suitable fordifferent applications and can be dynamically dispatched by a routing agentaccording to query complexity to balance response quality and computation cost.Evaluation on a carefully curated benchmark for assessing the writingcapabilities of LLMs shows Weaver models of all sizes outperform generalistLLMs several times larger than them. Notably our most-capable Weaver Ultramodel surpasses GPT-4 a state-of-the-art generalist LLM on various writingscenarios demonstrating the advantage of training specialized LLMs for writingpurposes. Moreover Weaver natively supports retrieval-augmented generationRAG and function calling tool usage. We present various use cases of theseabilities for improving AI-assisted writing systems including integration ofexternal knowledge bases tools or APIs and providing personalized writingassistance. Furthermore we discuss and summarize a guideline and bestpractices for pre-training and fine-tuning domain-specific LLMs.</p>
                <p>Last Updated: 2024-01-30 18:58:43 UTC</p>
                <button class="interpret-button" data-id="2401.17268v1">Interpret</button>
                <div id="interpretation-2401.17268v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks</h3>
                <p>Authors: Andy ZhouBo LiHaohan Wang</p>
                <p><a href="http://arxiv.org/abs/2401.17263v1">Link to paper</a></p>
                <p>Despite advances in AI alignment language models LM remain vulnerable toadversarial attacks or jailbreaking in which adversaries modify input promptsto induce harmful behavior. While some defenses have been proposed they focuson narrow threat models and fall short of a strong defense which we positshould be effective universal and practical. To achieve this we propose thefirst adversarial objective for defending LMs against jailbreaking attacks andan algorithm robust prompt optimization RPO that uses gradient-based tokenoptimization to enforce harmless outputs. This results in an easily accessiblesuffix that significantly improves robustness to both jailbreaks seen duringoptimization and unknown held-out jailbreaks reducing the attack success rateon Starling-7B from 84 to 8.66 across 20 jailbreaks. In addition we findthat RPO has a minor effect on normal LM use is successful under adaptiveattacks and can transfer to black-box models reducing the success rate of thestrongest attack on GPT-4 from 92 to 6.</p>
                <p>Last Updated: 2024-01-30 18:56:08 UTC</p>
                <button class="interpret-button" data-id="2401.17263v1">Interpret</button>
                <div id="interpretation-2401.17263v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Weak-to-Strong Jailbreaking on Large Language Models</h3>
                <p>Authors: Xuandong ZhaoXianjun YangTianyu PangChao DuLei LiYu-Xiang WangWilliam Yang Wang</p>
                <p><a href="http://arxiv.org/abs/2401.17256v1">Link to paper</a></p>
                <p>Although significant efforts have been dedicated to aligning large languagemodels LLMs red-teaming reports suggest that these carefully aligned LLMscould still be jailbroken through adversarial prompts tuning or decoding.Upon examining the jailbreaking vulnerability of aligned LLMs we observe thatthe decoding distributions of jailbroken and aligned models differ only in theinitial generations. This observation motivates us to propose theweak-to-strong jailbreaking attack where adversaries can utilize smallerunsafe/aligned LLMs e.g. 7B to guide jailbreaking against significantlylarger aligned LLMs e.g. 70B. To jailbreak one only needs to additionallydecode two smaller LLMs once which involves minimal computation and latencycompared to decoding the larger LLMs. The efficacy of this attack isdemonstrated through experiments conducted on five models from three differentorganizations. Our study reveals a previously unnoticed yet efficient way ofjailbreaking exposing an urgent safety issue that needs to be considered whenaligning LLMs. As an initial attempt we propose a defense strategy to protectagainst such attacks but creating more advanced defenses remains challenging.The code for replicating the method is available athttps://github.com/XuandongZhao/weak-to-strong</p>
                <p>Last Updated: 2024-01-30 18:48:37 UTC</p>
                <button class="interpret-button" data-id="2401.17256v1">Interpret</button>
                <div id="interpretation-2401.17256v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation</h3>
                <p>Authors: Yuan ChiangChia-Hong ChouJanosh Riebesell</p>
                <p><a href="http://arxiv.org/abs/2401.17244v1">Link to paper</a></p>
                <p>Reducing hallucination of Large Language Models LLMs is imperative for usein the sciences where reproducibility is crucial. However LLMs inherently lacklong-term memory making it a nontrivial ad hoc and inevitably biased task tofine-tune them on domain-specific literature and data. Here we introduce LLaMPa multimodal retrieval-augmented generation RAG framework of multipledata-aware reasoning-and-acting ReAct agents that dynamically interact withcomputational and experimental data on Materials Project MP. Withoutfine-tuning LLaMP demonstrates an ability to comprehend and integrate variousmodalities of materials science concepts fetch relevant data stores on thefly process higher-order data such as crystal structures and elastictensors and summarize multi-step procedures for solid-state synthesis. Weshow that LLaMP effectively corrects errors in GPT-3.5s intrinsic knowledgereducing a 5.21 MAPE on frequently-documented bandgaps and a significant1103.54 MAPE on formation energies -- errors that GPT-3.5 seems to derive frommixed data sources. Additionally LLaMP substantially reduces the hallucinatedvolumetric strain in a diamond cubic silicon structure from 66.3 to 0. Theproposed framework offers an intuitive and nearly hallucination-free approachto exploring materials informatics and establishes a pathway for knowledgedistillation and fine-tuning other language models. We envision the frameworkas a valuable component for scientific hypotheses and a foundation for futureautonomous laboratories where multiple LLM agents communicate and cooperatewith robotics to drive material synthesis and chemical reactions withouthard-coded human logic and intervention.</p>
                <p>Last Updated: 2024-01-30 18:37:45 UTC</p>
                <button class="interpret-button" data-id="2401.17244v1">Interpret</button>
                <div id="interpretation-2401.17244v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding Space using Contrastive Learning</h3>
                <p>Authors: Jeongwoo ParkEnrico LiscioPradeep K. Murukannaiah</p>
                <p><a href="http://arxiv.org/abs/2401.17228v1">Link to paper</a></p>
                <p>Recent advances in NLP show that language models retain a discernible levelof knowledge in deontological ethics and moral norms. However existing worksoften treat morality as binary ranging from right to wrong. This simplisticview does not capture the nuances of moral judgment. Pluralist moralphilosophers argue that human morality can be deconstructed into a finitenumber of elements respecting individual differences in moral judgment. Inline with this view we build a pluralist moral sentence embedding space via astate-of-the-art contrastive learning approach. We systematically investigatethe embedding space by studying the emergence of relationships among moralelements both quantitatively and qualitatively. Our results show that apluralist approach to morality can be captured in an embedding space. Howevermoral pluralism is challenging to deduce via self-supervision alone andrequires a supervised approach with human labels.</p>
                <p>Last Updated: 2024-01-30 18:15:25 UTC</p>
                <button class="interpret-button" data-id="2401.17228v1">Interpret</button>
                <div id="interpretation-2401.17228v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Effect of Weight Quantization on Learning Models by Typical Case Analysis</h3>
                <p>Authors: Shuhei KashiwamuraAyaka SakataMasaaki Imaizumi</p>
                <p><a href="http://arxiv.org/abs/2401.17269v1">Link to paper</a></p>
                <p>This paper examines the quantization methods used in large-scale dataanalysis models and their hyperparameter choices. The recent surge in dataanalysis scale has significantly increased computational resource requirements.To address this quantizing model weights has become a prevalent practice indata analysis applications such as deep learning. Quantization is particularlyvital for deploying large models on devices with limited computationalresources. However the selection of quantization hyperparameters like thenumber of bits and value range for weight quantization remains anunderexplored area. In this study we employ the typical case analysis fromstatistical physics specifically the replica method to explore the impact ofhyperparameters on the quantization of simple learning models. Our analysisyields three key findings: i an unstable hyperparameter phase known asreplica symmetry breaking occurs with a small number of bits and a largequantization width ii there is an optimal quantization width that minimizeserror and iii quantization delays the onset of overparameterization helpingto mitigate overfitting as indicated by the double descent phenomenon. We alsodiscover that non-uniform quantization can enhance stability. Additionally wedevelop an approximate message-passing algorithm to validate our theoreticalresults.</p>
                <p>Last Updated: 2024-01-30 18:58:46 UTC</p>
                <button class="interpret-button" data-id="2401.17269v1">Interpret</button>
                <div id="interpretation-2401.17269v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Adaptive Experiment Design with Synthetic Controls</h3>
                <p>Authors: Alihan HüyükZhaozhi QianMihaela van der Schaar</p>
                <p><a href="http://arxiv.org/abs/2401.17205v1">Link to paper</a></p>
                <p>Clinical trials are typically run in order to understand the effects of a newtreatment on a given population of patients. However patients in largepopulations rarely respond the same way to the same treatment. Thisheterogeneity in patient responses necessitates trials that investigate effectson multiple subpopulations - especially when a treatment has marginal or nobenefit for the overall population but might have significant benefit for aparticular subpopulation. Motivated by this need we propose Syntax anexploratory trial design that identifies subpopulations with positive treatmenteffect among many subpopulations. Syntax is sample efficient as it i recruitsand allocates patients adaptively and ii estimates treatment effects byforming synthetic controls for each subpopulation that combines control samplesfrom other subpopulations. We validate the performance of Syntax and provideinsights into when it might have an advantage over conventional trial designsthrough experiments.</p>
                <p>Last Updated: 2024-01-30 17:45:47 UTC</p>
                <button class="interpret-button" data-id="2401.17205v1">Interpret</button>
                <div id="interpretation-2401.17205v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Dynamical Survival Analysis with Controlled Latent States</h3>
                <p>Authors: Linus BleisteinVan-Tuan NguyenAdeline FermanianAgathe Guilloux</p>
                <p><a href="http://arxiv.org/abs/2401.17077v1">Link to paper</a></p>
                <p>We consider the task of learning individual-specific intensities of countingprocesses from a set of static variables and irregularly sampled time series.We introduce a novel modelization approach in which the intensity is thesolution to a controlled differential equation. We first design a neuralestimator by building on neural controlled differential equations. In a secondtime we show that our model can be linearized in the signature space undersufficient regularity conditions yielding a signature-based estimator which wecall CoxSig. We provide theoretical learning guarantees for both estimatorsbefore showcasing the performance of our models on a vast array of simulatedand real-world datasets from finance predictive maintenance and food supplychain management.</p>
                <p>Last Updated: 2024-01-30 14:57:32 UTC</p>
                <button class="interpret-button" data-id="2401.17077v1">Interpret</button>
                <div id="interpretation-2401.17077v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Gower's similarity coefficients with automatic weight selection</h3>
                <p>Authors: Marcello D'Orazio</p>
                <p><a href="http://arxiv.org/abs/2401.17041v1">Link to paper</a></p>
                <p>Nearest-neighbor methods have become popular in statistics and play a keyrole in statistical learning. Important decisions in nearest-neighbor methodsconcern the variables to use when many potential candidates exist and how tomeasure the dissimilarity between units. The first decision depends on thescope of the application while second depends mainly on the type of variables.Unfortunately relatively few options permit to handle mixed-type variables asituation frequently encountered in practical applications. The most populardissimilarity for mixed-type variables is derived as the complement to one ofthe Gowers similarity coefficient. It is appealing because ranges between 0and 1 being an average of the scaled dissimilarities calculated variable byvariable handles missing values and allows for a user-defined weighting schemewhen averaging dissimilarities. The discussion on the weighting schemes issometimes misleading since it often ignores that the unweighted standardsetting hides an unbalanced contribution of the single variables to the overalldissimilarity. We address this drawback following the recent idea ofintroducing a weighting scheme that minimizes the differences in thecorrelation between each contributing dissimilarity and the resulting weightedGowers dissimilarity. In particular this note proposes different approachesfor measuring the correlation depending on the type of variables. Theperformances of the proposed approaches are evaluated in simulation studiesrelated to classification and imputation of missing values.</p>
                <p>Last Updated: 2024-01-30 14:21:56 UTC</p>
                <button class="interpret-button" data-id="2401.17041v1">Interpret</button>
                <div id="interpretation-2401.17041v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Bayesian Optimization with Noise-Free Observations: Improved Regret Bounds via Random Exploration</h3>
                <p>Authors: Hwanwoo KimDaniel Sanz-Alonso</p>
                <p><a href="http://arxiv.org/abs/2401.17037v1">Link to paper</a></p>
                <p>This paper studies Bayesian optimization with noise-free observations. Weintroduce new algorithms rooted in scattered data approximation that rely on arandom exploration step to ensure that the fill-distance of query points decaysat a near-optimal rate. Our algorithms retain the ease of implementation of theclassical GP-UCB algorithm and satisfy cumulative regret bounds that nearlymatch those conjectured in arXiv:2002.05096 hence solving a COLT open problem.Furthermore the new algorithms outperform GP-UCB and other popular Bayesianoptimization strategies in several examples.</p>
                <p>Last Updated: 2024-01-30 14:16:06 UTC</p>
                <button class="interpret-button" data-id="2401.17037v1">Interpret</button>
                <div id="interpretation-2401.17037v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-02-01</p>
        </div>
    
        </div>
    </body>
    </html>
    