
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>AudioInsight: Detecting Social Contexts Relevant to Social Anxiety from Speech</h3>
                <p>Authors: Varun ReddyZhiyuan WangEmma TonerMax LarrazabalMehdi BoukhechbaBethany A. TeachmanLaura E. Barnes</p>
                <p><a href="http://arxiv.org/abs/2407.14458v1">Link to paper</a></p>
                <p>During social interactions understanding the intricacies of the context canbe vital particularly for socially anxious individuals. While previousresearch has found that the presence of a social interaction can be detectedfrom ambient audio the nuances within social contexts which influence howanxiety provoking interactions are remain largely unexplored. As analternative to traditional burdensome methods like self-report this studypresents a novel approach that harnesses ambient audio segments to detectsocial threat contexts. We focus on two key dimensions: number of interactionpartners dyadic vs. group and degree of evaluative threat explicitlyevaluative vs. not explicitly evaluative. Building on data from a Zoom-basedsocial interaction study N52 college students of whom the majority N45 aresocially anxious we employ deep learning methods to achieve strong detectionperformance. Under sample-wide 5-fold Cross Validation CV our modeldistinguished dyadic from group interactions with 90 accuracy and detectedevaluative threat at 83. Using a leave-one-group-out CV accuracies were 82and 77 respectively. While our data are based on virtual interactions due topandemic constraints our method has the potential to extend to diversereal-world settings. This research underscores the potential of passive sensingand AI to differentiate intricate social contexts and may ultimately advancethe ability of context-aware digital interventions to offer personalized mentalhealth support.</p>
                <p>Last Updated: 2024-07-19 17:01:12 UTC</p>
                <button class="interpret-button" data-id="2407.14458v1">Interpret</button>
                <div id="interpretation-2407.14458v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>From Instruction to Insight: Exploring the Functional and Semantic Roles of Text in Interactive Dashboards</h3>
                <p>Authors: Nicole SultanumVidya Setlur</p>
                <p><a href="http://arxiv.org/abs/2407.14451v1">Link to paper</a></p>
                <p>There is increased interest in the interplay between text and visuals in thefield of data visualization. However this attention has predominantly been onthe use of text in standalone visualizations or augmenting text storiessupported by a series of independent views. In this paper we shift from thetraditional focus on single-chart annotations to characterize the nuanced butcrucial communication role of text in the complex environment of interactivedashboards. Through a survey and analysis of 190 dashboards in the wild plus13 expert interview sessions with experienced dashboard authors we highlightthe distinctive nature of text as an integral component of the dashboardexperience while delving into the categories semantic levels and functionalroles of text and exploring how these text elements are coalesced by dashboardauthors to guide and inform dashboard users.  Our contributions are: 1 we distill qualitative and quantitative findingsfrom our studies to characterize current practices of text use in dashboardsincluding a categorization of text-based components and design patterns 2 weleverage current practices and existing literature to propose discuss andvalidate recommended practices for text in dashboards embodied as 12heuristics that underscore the semantic and functional role of text in offeringnavigational cues contextualizing data insights supporting reading orderetc 3 we reflect on our findings to identify gaps and propose opportunitiesfor data visualization researchers to push the boundaries on text usage fordashboards from authoring support and interactivity to text generation andcontent personalization.  Our research underscores the significance of elevating text as a first-classcitizen in data visualization and the need to support the inclusion of textualcomponents and their interactive affordances in dashboard design.</p>
                <p>Last Updated: 2024-07-19 16:48:00 UTC</p>
                <button class="interpret-button" data-id="2407.14451v1">Interpret</button>
                <div id="interpretation-2407.14451v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Exploring Indoor Air Quality Dynamics in Developing Nations: A Perspective from India</h3>
                <p>Authors: Prasenjit KarmakarSwadhin PradhanSandip Chakraborty</p>
                <p><a href="http://arxiv.org/abs/2407.14393v1">Link to paper</a></p>
                <p>Indoor air pollution is a major issue in developing countries such as Indiaand Bangladesh exacerbated by factors like traditional cooking methodsinsufficient ventilation and cramped living conditions all of which elevatethe risk of health issues like lung infections and cardiovascular diseases.With the World Health Organization associating around 3.2 million annual deathsglobally to household air pollution the gravity of the problem is clear. Yetextensive empirical studies exploring these unique patterns and indoorpollutions extent are missing. To fill this gap we carried out a six monthslong field study involving over 30 households uncovering the complexity ofindoor air pollution in developing countries such as the longer lingering timeof VOCs in the air or the significant influence of air circulation on thespatiotemporal distribution of pollutants. We introduced an innovative IoT airquality sensing platform the Distributed Air QuaLiTy MONitor DALTON explicitly designed to meet the needs of these nations considering factorslike cost sensor type accuracy network connectivity power and usability.As a result of a multi-device deployment the platform identifies pollutionhot-spots in low and middle-income households in developing nations. Itidentifies best practices to minimize daily indoor pollution exposure. Ourextensive qualitative survey estimates an overall system usability score of2.04 indicating an efficient system for air quality monitoring.</p>
                <p>Last Updated: 2024-07-19 15:15:31 UTC</p>
                <button class="interpret-button" data-id="2407.14393v1">Interpret</button>
                <div id="interpretation-2407.14393v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>As Generative Models Improve, People Adapt Their Prompts</h3>
                <p>Authors: Eaman JahaniBenjamin S. ManningJoe ZhangHong-Yi TuYeMohammed AlsobayChristos NicolaidesSiddharth SuriDavid Holtz</p>
                <p><a href="http://arxiv.org/abs/2407.14333v1">Link to paper</a></p>
                <p>In an online experiment with N  1891 participants we collected and analyzedover 18000 prompts to explore how the importance of prompting will change asthe capabilities of generative AI models continue to improve. Each participantin our experiment was randomly and blindly assigned to use one of threetext-to-image diffusion models: DALL-E 2 its more advanced successor DALL-E 3or a version of DALL-E 3 with automatic prompt revision. Participants were thenasked to write prompts to reproduce a target image as closely as possible in 10consecutive tries. We find that task performance was higher for participantsusing DALL-E 3 than for those using DALL-E 2. This performance gap correspondsto a noticeable difference in the similarity of participants images to theirtarget images and was caused in equal measure by: 1 the increased technicalcapabilities of DALL-E 3 and 2 endogenous changes in participants promptingin response to these increased capabilities. More specifically despite beingblind to the model they were assigned participants assigned to DALL-E 3 wrotelonger prompts that were more semantically similar to each other and containeda greater number of descriptive words. Furthermore while participants assignedto DALL-E 3 with prompt revision still outperformed those assigned to DALL-E 2automatic prompt revision reduced the benefits of using DALL-E 3 by 58. Takentogether our results suggest that as models continue to progress people willcontinue to adapt their prompts to take advantage of new models capabilities.</p>
                <p>Last Updated: 2024-07-19 14:13:02 UTC</p>
                <button class="interpret-button" data-id="2407.14333v1">Interpret</button>
                <div id="interpretation-2407.14333v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Experiences of Censorship on TikTok Across Marginalised Identities</h3>
                <p>Authors: Eddie L. UnglessNina MarklBjörn Ross</p>
                <p><a href="http://arxiv.org/abs/2407.14164v1">Link to paper</a></p>
                <p>TikTok has seen exponential growth as a platform fuelled by the success ofits proprietary recommender algorithm which serves tailored content to everyuser - though not without controversy. Users complain of their content beingunfairly suppressed by the algorithm particularly users with marginalisedidentities such as LGBTQ users. Together with content removal thissuppression acts to censor what is shared on the platform. Journalists haverevealed biases in automatic censorship as well as human moderation. Weinvestigate experiences of censorship on TikTok across users marginalised bytheir gender LGBTQ identity disability or ethnicity. We survey 627 UK-basedTikTok users and find that marginalised users often feel they are subject tocensorship for content that does not violate community guidelines. We highlightmany avenues for future research into censorship on TikTok with a focus onusers folk theories which greatly shape their experiences of the platform.</p>
                <p>Last Updated: 2024-07-19 09:50:13 UTC</p>
                <button class="interpret-button" data-id="2407.14164v1">Interpret</button>
                <div id="interpretation-2407.14164v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Internal Consistency and Self-Feedback in Large Language Models: A Survey</h3>
                <p>Authors: Xun LiangShichao SongZifan ZhengHanyu WangQingchen YuXunkai LiRong-Hua LiFeiyu XiongZhiyu Li</p>
                <p><a href="http://arxiv.org/abs/2407.14507v1">Link to paper</a></p>
                <p>Large language models LLMs are expected to respond accurately but oftenexhibit deficient reasoning or generate hallucinatory content. To addressthese studies prefixed with Self- such as Self-Consistency Self-Improveand Self-Refine have been initiated. They share a commonality: involving LLMsevaluating and updating itself to mitigate the issues. Nonetheless theseefforts lack a unified perspective on summarization as existing surveyspredominantly focus on categorization without examining the motivations behindthese works.  In this paper we summarize a theoretical framework termed InternalConsistency which offers unified explanations for phenomena such as the lackof reasoning and the presence of hallucinations. Internal Consistency assessesthe coherence among LLMs latent layer decoding layer and response layerbased on sampling methodologies. Expanding upon the Internal Consistencyframework we introduce a streamlined yet effective theoretical frameworkcapable of mining Internal Consistency named Self-Feedback. The Self-Feedbackframework consists of two modules: Self-Evaluation and Self-Update. Thisframework has been employed in numerous studies.  We systematically classify these studies by tasks and lines of worksummarize relevant evaluation methods and benchmarks and delve into theconcern Does Self-Feedback Really Work We propose several criticalviewpoints including the Hourglass Evolution of Internal ConsistencyConsistency Is Almost Correctness hypothesis and The Paradox of Latentand Explicit Reasoning. Furthermore we outline promising directions forfuture research. We have open-sourced the experimental code reference listand statistical data available aturlhttps://github.com/IAAR-Shanghai/ICSFSurvey.</p>
                <p>Last Updated: 2024-07-19 17:59:03 UTC</p>
                <button class="interpret-button" data-id="2407.14507v1">Interpret</button>
                <div id="interpretation-2407.14507v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On Pre-training of Multimodal Language Models Customized for Chart Understanding</h3>
                <p>Authors: Wan-Cyuan FanYen-Chun ChenMengchen LiuLu YuanLeonid Sigal</p>
                <p><a href="http://arxiv.org/abs/2407.14506v1">Link to paper</a></p>
                <p>Recent studies customizing Multimodal Large Language Models MLLMs fordomain-specific tasks have yielded promising results especially in the fieldof scientific chart comprehension. These studies generally utilize visualinstruction tuning with specialized datasets to enhance question and answerQA accuracy within the chart domain. However they often neglect thefundamental discrepancy between natural image-caption pre-training data anddigital chart image-QA data particularly in the models capacity to extractunderlying numeric values from charts. This paper tackles this oversight byexploring the training processes necessary to improve MLLMs comprehension ofcharts. We present three key findings: 1 Incorporating raw data values inalignment pre-training markedly improves comprehension of chart data. 2Replacing images with their textual representation randomly during end-to-endfine-tuning transfer the language reasoning capability to chart interpretationskills. 3 Requiring the model to first extract the underlying chart data andthen answer the question in the fine-tuning can further improve the accuracy.Consequently we introduce CHOPINLLM an MLLM tailored for in-depth chartcomprehension. CHOPINLLM effectively interprets various types of chartsincluding unannotated ones while maintaining robust reasoning abilities.Furthermore we establish a new benchmark to evaluate MLLMs understanding ofdifferent chart types across various comprehension levels. Experimental resultsshow that CHOPINLLM exhibits strong performance in understanding both annotatedand unannotated charts across a wide range of types.</p>
                <p>Last Updated: 2024-07-19 17:58:36 UTC</p>
                <button class="interpret-button" data-id="2407.14506v1">Interpret</button>
                <div id="interpretation-2407.14506v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Evaluating the Reliability of Self-Explanations in Large Language Models</h3>
                <p>Authors: Korbinian RandlJohn PavlopoulosAron HenrikssonTony Lindgren</p>
                <p><a href="http://arxiv.org/abs/2407.14487v1">Link to paper</a></p>
                <p>This paper investigates the reliability of explanations generated by largelanguage models LLMs when prompted to explain their previous output. Weevaluate two kinds of such self-explanations - extractive and counterfactual -using three state-of-the-art LLMs 2B to 8B parameters on two differentclassification tasks objective and subjective. Our findings reveal thatwhile these self-explanations can correlate with human judgement they do notfully and accurately follow the models decision process indicating a gapbetween perceived and actual model reasoning. We show that this gap can bebridged because prompting LLMs for counterfactual explanations can producefaithful informative and easy-to-verify results. These counterfactuals offera promising alternative to traditional explainability methods e.g. SHAPLIME provided that prompts are tailored to specific tasks and checked forvalidity.</p>
                <p>Last Updated: 2024-07-19 17:41:08 UTC</p>
                <button class="interpret-button" data-id="2407.14487v1">Interpret</button>
                <div id="interpretation-2407.14487v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities</h3>
                <p>Authors: Peng XuWei PingXianchao WuZihan LiuMohammad ShoeybiBryan Catanzaro</p>
                <p><a href="http://arxiv.org/abs/2407.14482v1">Link to paper</a></p>
                <p>In this work we introduce ChatQA 2 a Llama3-based model designed to bridgethe gap between open-access LLMs and leading proprietary models e.g.GPT-4-Turbo in long-context understanding and retrieval-augmented generationRAG capabilities. These two capabilities are essential for LLMs to processlarge volumes of information that cannot fit into a single prompt and arecomplementary to each other depending on the downstream tasks andcomputational budgets. We present a detailed continued training recipe toextend the context window of Llama3-70B-base from 8K to 128K tokens along witha three-stage instruction tuning process to enhance the modelsinstruction-following RAG performance and long-context understandingcapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B modelachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-contextunderstanding tasks and surpasses it on the RAG benchmark. Interestingly wefind that the state-of-the-art long-context retriever can alleviate the top-kcontext fragmentation issue in RAG further improving RAG-based results forlong-context understanding tasks. We also provide extensive comparisons betweenRAG and long-context solutions using state-of-the-art long-context LLMs.</p>
                <p>Last Updated: 2024-07-19 17:35:47 UTC</p>
                <button class="interpret-button" data-id="2407.14482v1">Interpret</button>
                <div id="interpretation-2407.14482v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Check-Eval: A Checklist-based Approach for Evaluating Text Quality</h3>
                <p>Authors: Jayr PereiraRoberto Lotufo</p>
                <p><a href="http://arxiv.org/abs/2407.14467v1">Link to paper</a></p>
                <p>Evaluating the quality of text generated by large language models LLMsremains a significant challenge. Traditional metrics often fail to align wellwith human judgments particularly in tasks requiring creativity and nuance. Inthis paper we propose Check-Eval a novel evaluation framework leveraging LLMsto assess the quality of generated text through a checklist-based approach.Check-Eval can be employed as both a reference-free and reference-dependentevaluation method providing a structured and interpretable assessment of textquality. The framework consists of two main stages: checklist generation andchecklist evaluation. We validate Check-Eval on two benchmark datasets:Portuguese Legal Semantic Textual Similarity and SummEval. Our resultsdemonstrate that Check-Eval achieves higher correlations with human judgmentscompared to existing metrics such as G-Eval and GPTScore underscoring itspotential as a more reliable and effective evaluation framework for naturallanguage generation tasks. The code for our experiments is available athttps://anonymous.4open.science/r/check-eval-0DB4.</p>
                <p>Last Updated: 2024-07-19 17:14:16 UTC</p>
                <button class="interpret-button" data-id="2407.14467v1">Interpret</button>
                <div id="interpretation-2407.14467v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>The Vision of Autonomic Computing: Can LLMs Make It a Reality?</h3>
                <p>Authors: Zhiyang ZhangFangkai YangXiaoting QinJue ZhangQingwei LinGong ChengDongmei ZhangSaravan RajmohanQi Zhang</p>
                <p><a href="http://arxiv.org/abs/2407.14402v1">Link to paper</a></p>
                <p>The Vision of Autonomic Computing ACV proposed over two decades agoenvisions computing systems that self-manage akin to biological organismsadapting seamlessly to changing environments. Despite decades of researchachieving ACV remains challenging due to the dynamic and complex nature ofmodern computing systems. Recent advancements in Large Language Models LLMsoffer promising solutions to these challenges by leveraging their extensiveknowledge language understanding and task automation capabilities. This paperexplores the feasibility of realizing ACV through an LLM-based multi-agentframework for microservice management. We introduce a five-level taxonomy forautonomous service maintenance and present an online evaluation benchmark basedon the Sock Shop microservice demo project to assess our frameworksperformance. Our findings demonstrate significant progress towards achievingLevel 3 autonomy highlighting the effectiveness of LLMs in detecting andresolving issues within microservice architectures. This study contributes toadvancing autonomic computing by pioneering the integration of LLMs intomicroservice management frameworks paving the way for more adaptive andself-managing computing systems. The code will be made available athttps://aka.ms/ACV-LLM.</p>
                <p>Last Updated: 2024-07-19 15:30:32 UTC</p>
                <button class="interpret-button" data-id="2407.14402v1">Interpret</button>
                <div id="interpretation-2407.14402v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-robot maze exploration using an efficient cost-utility method</h3>
                <p>Authors: Manousos LinardakisIraklis VarlamisGeorgios Th. Papadopoulos</p>
                <p><a href="http://arxiv.org/abs/2407.14218v1">Link to paper</a></p>
                <p>In the field of modern robotics robots are proving to be useful in tacklinghigh-risk situations such as navigating hazardous environments like burningbuildings earthquake-stricken areas or patrolling crime-ridden streets aswell as exploring uncharted caves. These scenarios share similarities with mazeexploration problems in terms of complexity. While several methods have beenproposed for single-agent systems ranging from potential fields to flood-fillmethods recent research endeavors have focused on creating methods tailoredfor multiple agents to enhance the quality and efficiency of maze coverage. Thecontribution of this paper is the implementation of established mazeexploration methods and their comparison with a new cost-utility algorithmdesigned for multiple agents which combines the existing methodologies tooptimize exploration outcomes. Through a comprehensive and comparativeanalysis this paper evaluates the performance of the new approach against theimplemented baseline methods from the literature highlighting its efficacy andpotential advantages in various scenarios. The code and experimental resultssupporting this study are available in the following repositoryhttps://github.com/manouslinard/multiagent-exploration/.</p>
                <p>Last Updated: 2024-07-19 11:35:51 UTC</p>
                <button class="interpret-button" data-id="2407.14218v1">Interpret</button>
                <div id="interpretation-2407.14218v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Integrated Push-and-Pull Update Model for Goal-Oriented Effective Communication</h3>
                <p>Authors: Pouya AgheliNikolaos PappasPetar PopovskiMarios Kountouris</p>
                <p><a href="http://arxiv.org/abs/2407.14092v1">Link to paper</a></p>
                <p>This paper studies decision-making for goal-oriented effective communication.We consider an end-to-end status update system where a sensing agent SAobserves a source generates and transmits updates to an actuation agent AAwhile the AA takes actions to accomplish a goal at the endpoint. We integratethe push- and pull-based update communication models to obtain a push-and-pullmodel which allows the transmission controller at the SA to decide to push anupdate to the AA and the query controller at the AA to pull updates by raisingqueries at specific time instances. To gauge effectiveness we utilize a gradeof effectiveness GoE metric incorporating updates freshness usefulness andtimeliness of actions as qualitative attributes. We then derive effect-awarepolicies to maximize the expected discounted sum of updates effectivenesssubject to induced costs. The effect-aware policy at the SA considers thepotential effectiveness of communicated updates at the endpoint while at theAA it accounts for the probabilistic evolution of the source and importance ofgenerated updates. Our results show the proposed push-and-pull modeloutperforms models solely based on push- or pull-based updates both in terms ofefficiency and effectiveness. Additionally using effect-aware policies at bothagents enhances effectiveness compared to periodic and/or probabilisticeffect-agnostic policies at either or both agents.</p>
                <p>Last Updated: 2024-07-19 07:57:31 UTC</p>
                <button class="interpret-button" data-id="2407.14092v1">Interpret</button>
                <div id="interpretation-2407.14092v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Matching-Driven Deep Reinforcement Learning for Energy-Efficient Transmission Parameter Allocation in Multi-Gateway LoRa Networks</h3>
                <p>Authors: Ziqi LinXu ZhangShimin GongLanhua LiZhou SuBo Gu</p>
                <p><a href="http://arxiv.org/abs/2407.13076v1">Link to paper</a></p>
                <p>Long-range LoRa communication technology distinguished by its low powerconsumption and long communication range is widely used in the Internet ofThings. Nevertheless the LoRa MAC layer adopts pure ALOHA for medium accesscontrol which may suffer from severe packet collisions as the network scaleexpands consequently reducing the system energy efficiency EE. To addressthis issue it is critical to carefully allocate transmission parameters suchas the channel CH transmission power TP and spreading factor SF to eachend device ED. Owing to the low duty cycle and sporadic traffic of LoRanetworks evaluating the system EE under various parameter settings proves tobe time-consuming. Consequently we propose an analytical model aimed atcalculating the system EE while fully considering the impact of multiplegateways duty cycling quasi-orthogonal SFs and capture effects. On thisbasis we investigate a joint CH SF and TP allocation problem with theobjective of optimizing the system EE for uplink transmissions. Due to theNP-hard complexity of the problem the optimization problem is decomposed intotwo subproblems: CH assignment and SF/TP assignment. First a matching-basedalgorithm is introduced to address the CH assignment subproblem. Then anattention-based multiagent reinforcement learning technique is employed toaddress the SF/TP assignment subproblem for EDs allocated to the same CH whichreduces the number of learning agents to achieve fast convergence. Thesimulation outcomes indicate that the proposed approach converges quickly undervarious parameter settings and obtains significantly better system EE thanbaseline algorithms.</p>
                <p>Last Updated: 2024-07-18 00:54:26 UTC</p>
                <button class="interpret-button" data-id="2407.13076v1">Interpret</button>
                <div id="interpretation-2407.13076v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Information Compression in Dynamic Games</h3>
                <p>Authors: Dengwang TangVijay SubramanianDemosthenis Teneketzis</p>
                <p><a href="http://arxiv.org/abs/2407.12318v1">Link to paper</a></p>
                <p>One of the reasons why stochastic dynamic games with an underlying dynamicsystem are challenging is since strategic players have access to enormousamount of information which leads to the use of extremely complex strategies atequilibrium. One approach to resolve this challenge is to simplify playersstrategies by identifying appropriate compression of information maps so thatthe players can make decisions solely based on the compressed version ofinformation called the information state. For finite dynamic games withasymmetric information inspired by the notion of information state forsingle-agent control problems we propose two notions of information statesnamely mutually sufficient information MSI and unilaterally sufficientinformation USI. Both these information states are obtained with informationcompression maps independent of the strategy profile. We show that Bayes-NashEquilibria BNE and Sequential Equilibria SE exist when all players useMSI-based strategies. We prove that when all players employ USI-basedstrategies the resulting sets of BNE and SE payoff profiles are the same as thesets of BNE and SE payoff profiles resulting when all players use fullinformation-based strategies. We prove that when all players use USI-basedstrategies the resulting set of weak Perfect Bayesian Equilibrium wPBE payoffprofiles can be a proper subset of all wPBE payoff profiles. We identify MSIand USI in specific models of dynamic games in the literature. We end bypresenting an open problem: Do there exist strategy-dependent informationcompression maps that guarantee the existence of at least one equilibrium ormaintain all equilibria that exist under perfect recall We show by acounterexample that a well-known strategy-dependent information compressionmap used in the literature does not possess any of the properties of MSI orUSI.</p>
                <p>Last Updated: 2024-07-17 05:08:47 UTC</p>
                <button class="interpret-button" data-id="2407.12318v1">Interpret</button>
                <div id="interpretation-2407.12318v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Nonlinear Schrödinger Network</h3>
                <p>Authors: Yiming ZhouCallen MacPheeTingyi ZhouBahram Jalali</p>
                <p><a href="http://arxiv.org/abs/2407.14504v1">Link to paper</a></p>
                <p>Deep neural networks DNNs have achieved exceptional performance acrossvarious fields by learning complex nonlinear mappings from large-scaledatasets. However they encounter challenges such as high computational costsand limited interpretability. To address these issues hybrid approaches thatintegrate physics with AI are gaining interest. This paper introduces a novelphysics-based AI model called the Nonlinear Schrodinger Network whichtreats the Nonlinear Schrodinger Equation NLSE as a general-purposetrainable model for learning complex patterns including nonlinear mappings andmemory effects from data. Existing physics-informed machine learning methodsuse neural networks to approximate the solutions of partial differentialequations PDEs. In contrast our approach directly treats the PDE as atrainable model to obtain general nonlinear mappings that would otherwiserequire neural networks. As a physics-inspired approach it offers a moreinterpretable and parameter-efficient alternative to traditional black-boxneural networks achieving comparable or better accuracy in time seriesclassification tasks while significantly reducing the number of requiredparameters. Notably the trained Nonlinear Schrodinger Network isinterpretable with all parameters having physical meanings as properties of avirtual physical system that transforms the data to a more separable space.This interpretability allows for insight into the underlying dynamics of thedata transformation process. Applications to time series forecasting have alsobeen explored. While our current implementation utilizes the NLSE the proposedmethod of using physics equations as trainable models to learn nonlinearmappings from data is not limited to the NLSE and may be extended to othermaster equations of physics.</p>
                <p>Last Updated: 2024-07-19 17:58:00 UTC</p>
                <button class="interpret-button" data-id="2407.14504v1">Interpret</button>
                <div id="interpretation-2407.14504v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification</h3>
                <p>Authors: Thomas KwaDrake ThomasAdrià Garriga-Alonso</p>
                <p><a href="http://arxiv.org/abs/2407.14503v1">Link to paper</a></p>
                <p>When applying reinforcement learning from human feedback RLHF the rewardis learned from data and therefore always has some error. It is common tomitigate this by regularizing the policy with KL divergence from a base modelwith the hope that balancing reward with regularization will achieve desirableoutcomes despite this reward misspecification. We show that when the rewardfunction has light-tailed error optimal policies under less restrictive KLpenalties achieve arbitrarily high utility. However if error is heavy-tailedsome policies obtain arbitrarily high reward despite achieving no more utilitythan the base model--a phenomenon we call catastrophic Goodhart. We adapt adiscrete optimization method to measure the tails of reward models findingthat they are consistent with light-tailed error. However the pervasiveness ofheavy-tailed distributions in many real-world applications indicates thatfuture sources of RL reward could have heavy-tailed error increasing thelikelihood of reward hacking even with KL regularization.</p>
                <p>Last Updated: 2024-07-19 17:57:59 UTC</p>
                <button class="interpret-button" data-id="2407.14503v1">Interpret</button>
                <div id="interpretation-2407.14503v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Indoor Air Quality Dataset with Activities of Daily Living in Low to Middle-income Communities</h3>
                <p>Authors: Prasenjit KarmakarSwadhin PradhanSandip Chakraborty</p>
                <p><a href="http://arxiv.org/abs/2407.14501v1">Link to paper</a></p>
                <p>In recent years indoor air pollution has posed a significant threat to oursociety claiming over 3.2 million lives annually. Developing nations such asIndia are most affected since lack of knowledge inadequate regulation andoutdoor air pollution lead to severe daily exposure to pollutants. Howeveronly a limited number of studies have attempted to understand how indoor airpollution affects developing countries like India. To address this gap wepresent spatiotemporal measurements of air quality from 30 indoor sites oversix months during summer and winter seasons. The sites are geographicallylocated across four regions of type: rural suburban and urban covering thetypical low to middle-income population in India. The dataset contains varioustypes of indoor environments e.g. studio apartments classrooms researchlaboratories food canteens and residential households and can provide thebasis for data-driven learning model research aimed at coping with uniquepollution patterns in developing countries. This unique dataset demandsadvanced data cleaning and imputation techniques for handling missing data dueto power failure or network outages during data collection. Furthermorethrough a simple speech-to-text application we provide real-time indooractivity labels annotated by occupants. Therefore environmentalists and MLenthusiasts can utilize this dataset to understand the complex patterns of thepollutants under different indoor activities identify recurring sources ofpollution forecast exposure improve floor plans and room structures of modernindoor designs develop pollution-aware recommender systems etc.</p>
                <p>Last Updated: 2024-07-19 17:53:21 UTC</p>
                <button class="interpret-button" data-id="2407.14501v1">Interpret</button>
                <div id="interpretation-2407.14501v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery</h3>
                <p>Authors: Sukrut RaoSweta MahajanMoritz BöhleBernt Schiele</p>
                <p><a href="http://arxiv.org/abs/2407.14499v1">Link to paper</a></p>
                <p>Concept Bottleneck Models CBMs have recently been proposed to address theblack-box problem of deep neural networks by first mapping images to ahuman-understandable concept space and then linearly combining concepts forclassification. Such models typically require first coming up with a set ofconcepts relevant to the task and then aligning the representations of afeature extractor to map to these concepts. However even with powerfulfoundational feature extractors like CLIP there are no guarantees that thespecified concepts are detectable. In this work we leverage recent advances inmechanistic interpretability and propose a novel CBM approach -- calledDiscover-then-Name-CBM DN-CBM -- that inverts the typical paradigm: insteadof pre-selecting concepts based on the downstream classification task we usesparse autoencoders to first discover concepts learnt by the model and thenname them and train linear probes for classification. Our concept extractionstrategy is efficient since it is agnostic to the downstream task and usesconcepts already known to the model. We perform a comprehensive evaluationacross multiple datasets and CLIP architectures and show that our method yieldssemantically meaningful concepts assigns appropriate names to them that makethem easy to interpret and yields performant and interpretable CBMs. Codeavailable at https://github.com/neuroexplicit-saar/discover-then-name.</p>
                <p>Last Updated: 2024-07-19 17:50:11 UTC</p>
                <button class="interpret-button" data-id="2407.14499v1">Interpret</button>
                <div id="interpretation-2407.14499v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Conformal Thresholded Intervals for Efficient Regression</h3>
                <p>Authors: Rui LuoZhixin Zhou</p>
                <p><a href="http://arxiv.org/abs/2407.14495v1">Link to paper</a></p>
                <p>This paper introduces Conformal Thresholded Intervals CTI a novelconformal regression method that aims to produce the smallest possibleprediction set with guaranteed coverage. Unlike existing methods that rely onnested conformal framework and full conditional distribution estimation CTIestimates the conditional probability density for a new response to fall intoeach interquantile interval using off-the-shelf multi-output quantileregression. CTI constructs prediction sets by thresholding the estimatedconditional interquantile intervals based on their length which is inverselyproportional to the estimated probability density. The threshold is determinedusing a calibration set to ensure marginal coverage. Experimental resultsdemonstrate that CTI achieves optimal performance across various datasets.</p>
                <p>Last Updated: 2024-07-19 17:47:08 UTC</p>
                <button class="interpret-button" data-id="2407.14495v1">Interpret</button>
                <div id="interpretation-2407.14495v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks</h3>
                <p>Authors: Sarah JabbourGregory KondasElla KazerooniMichael SjodingDavid FouheyJenna Wiens</p>
                <p><a href="http://arxiv.org/abs/2407.14509v1">Link to paper</a></p>
                <p>We propose a permutation-based explanation method for image classifiers.Current image-model explanations like activation maps are limited toinstance-based explanations in the pixel space making it difficult tounderstand global model behavior. In contrast permutation based explanationsfor tabular data classifiers measure feature importance by comparing modelperformance on data before and after permuting a feature. We propose anexplanation method for image-based models that permutes interpretable conceptsacross dataset images. Given a dataset of images labeled with specific conceptslike captions we permute a concept across examples in the text space and thengenerate images via a text-conditioned diffusion model. Feature importance isthen reflected by the change in model performance relative to unpermuted data.When applied to a set of concepts the method generates a ranking of featureimportance. We show this approach recovers underlying model feature importanceon synthetic and real-world image classification tasks.</p>
                <p>Last Updated: 2024-07-19 17:59:38 UTC</p>
                <button class="interpret-button" data-id="2407.14509v1">Interpret</button>
                <div id="interpretation-2407.14509v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation</h3>
                <p>Authors: Kaiyue SunKaiyi HuangXian LiuYue WuZihan XuZhenguo LiXihui Liu</p>
                <p><a href="http://arxiv.org/abs/2407.14505v1">Link to paper</a></p>
                <p>Text-to-video T2V generation models have advanced significantly yet theirability to compose different objects attributes actions and motions into avideo remains unexplored. Previous text-to-video benchmarks also neglect thisimportant ability for evaluation. In this work we conduct the first systematicstudy on compositional text-to-video generation. We propose T2V-CompBench thefirst benchmark tailored for compositional text-to-video generation.T2V-CompBench encompasses diverse aspects of compositionality includingconsistent attribute binding dynamic attribute binding spatial relationshipsmotion binding action binding object interactions and generative numeracy.We further carefully design evaluation metrics of MLLM-based metricsdetection-based metrics and tracking-based metrics which can better reflectthe compositional text-to-video generation quality of seven proposed categorieswith 700 text prompts. The effectiveness of the proposed metrics is verified bycorrelation with human evaluations. We also benchmark various text-to-videogenerative models and conduct in-depth analysis across different models anddifferent compositional categories. We find that compositional text-to-videogeneration is highly challenging for current models and we hope that ourattempt will shed light on future research in this direction.</p>
                <p>Last Updated: 2024-07-19 17:58:36 UTC</p>
                <button class="interpret-button" data-id="2407.14505v1">Interpret</button>
                <div id="interpretation-2407.14505v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On Pre-training of Multimodal Language Models Customized for Chart Understanding</h3>
                <p>Authors: Wan-Cyuan FanYen-Chun ChenMengchen LiuLu YuanLeonid Sigal</p>
                <p><a href="http://arxiv.org/abs/2407.14506v1">Link to paper</a></p>
                <p>Recent studies customizing Multimodal Large Language Models MLLMs fordomain-specific tasks have yielded promising results especially in the fieldof scientific chart comprehension. These studies generally utilize visualinstruction tuning with specialized datasets to enhance question and answerQA accuracy within the chart domain. However they often neglect thefundamental discrepancy between natural image-caption pre-training data anddigital chart image-QA data particularly in the models capacity to extractunderlying numeric values from charts. This paper tackles this oversight byexploring the training processes necessary to improve MLLMs comprehension ofcharts. We present three key findings: 1 Incorporating raw data values inalignment pre-training markedly improves comprehension of chart data. 2Replacing images with their textual representation randomly during end-to-endfine-tuning transfer the language reasoning capability to chart interpretationskills. 3 Requiring the model to first extract the underlying chart data andthen answer the question in the fine-tuning can further improve the accuracy.Consequently we introduce CHOPINLLM an MLLM tailored for in-depth chartcomprehension. CHOPINLLM effectively interprets various types of chartsincluding unannotated ones while maintaining robust reasoning abilities.Furthermore we establish a new benchmark to evaluate MLLMs understanding ofdifferent chart types across various comprehension levels. Experimental resultsshow that CHOPINLLM exhibits strong performance in understanding both annotatedand unannotated charts across a wide range of types.</p>
                <p>Last Updated: 2024-07-19 17:58:36 UTC</p>
                <button class="interpret-button" data-id="2407.14506v1">Interpret</button>
                <div id="interpretation-2407.14506v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models</h3>
                <p>Authors: Seunggeun ChiHyung-gun ChiHengbo MaNakul AgarwalFaizan SiddiquiKarthik RamaniKwonjoon Lee</p>
                <p><a href="http://arxiv.org/abs/2407.14502v1">Link to paper</a></p>
                <p>We introduce the Multi-Motion Discrete Diffusion Models M2D2M a novelapproach for human motion generation from textual descriptions of multipleactions utilizing the strengths of discrete diffusion models. This approachadeptly addresses the challenge of generating multi-motion sequences ensuringseamless transitions of motions and coherence across a series of actions. Thestrength of M2D2M lies in its dynamic transition probability within thediscrete diffusion model which adapts transition probabilities based on theproximity between motion tokens encouraging mixing between different modes.Complemented by a two-phase sampling strategy that includes independent andjoint denoising steps M2D2M effectively generates long-term smooth andcontextually coherent human motion sequences utilizing a model trained forsingle-motion generation. Extensive experiments demonstrate that M2D2Msurpasses current state-of-the-art benchmarks for motion generation from textdescriptions showcasing its efficacy in interpreting language semantics andgenerating dynamic realistic motions.</p>
                <p>Last Updated: 2024-07-19 17:57:33 UTC</p>
                <button class="interpret-button" data-id="2407.14502v1">Interpret</button>
                <div id="interpretation-2407.14502v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery</h3>
                <p>Authors: Sukrut RaoSweta MahajanMoritz BöhleBernt Schiele</p>
                <p><a href="http://arxiv.org/abs/2407.14499v1">Link to paper</a></p>
                <p>Concept Bottleneck Models CBMs have recently been proposed to address theblack-box problem of deep neural networks by first mapping images to ahuman-understandable concept space and then linearly combining concepts forclassification. Such models typically require first coming up with a set ofconcepts relevant to the task and then aligning the representations of afeature extractor to map to these concepts. However even with powerfulfoundational feature extractors like CLIP there are no guarantees that thespecified concepts are detectable. In this work we leverage recent advances inmechanistic interpretability and propose a novel CBM approach -- calledDiscover-then-Name-CBM DN-CBM -- that inverts the typical paradigm: insteadof pre-selecting concepts based on the downstream classification task we usesparse autoencoders to first discover concepts learnt by the model and thenname them and train linear probes for classification. Our concept extractionstrategy is efficient since it is agnostic to the downstream task and usesconcepts already known to the model. We perform a comprehensive evaluationacross multiple datasets and CLIP architectures and show that our method yieldssemantically meaningful concepts assigns appropriate names to them that makethem easy to interpret and yields performant and interpretable CBMs. Codeavailable at https://github.com/neuroexplicit-saar/discover-then-name.</p>
                <p>Last Updated: 2024-07-19 17:50:11 UTC</p>
                <button class="interpret-button" data-id="2407.14499v1">Interpret</button>
                <div id="interpretation-2407.14499v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks</h3>
                <p>Authors: Sarah JabbourGregory KondasElla KazerooniMichael SjodingDavid FouheyJenna Wiens</p>
                <p><a href="http://arxiv.org/abs/2407.14509v1">Link to paper</a></p>
                <p>We propose a permutation-based explanation method for image classifiers.Current image-model explanations like activation maps are limited toinstance-based explanations in the pixel space making it difficult tounderstand global model behavior. In contrast permutation based explanationsfor tabular data classifiers measure feature importance by comparing modelperformance on data before and after permuting a feature. We propose anexplanation method for image-based models that permutes interpretable conceptsacross dataset images. Given a dataset of images labeled with specific conceptslike captions we permute a concept across examples in the text space and thengenerate images via a text-conditioned diffusion model. Feature importance isthen reflected by the change in model performance relative to unpermuted data.When applied to a set of concepts the method generates a ranking of featureimportance. We show this approach recovers underlying model feature importanceon synthetic and real-world image classification tasks.</p>
                <p>Last Updated: 2024-07-19 17:59:38 UTC</p>
                <button class="interpret-button" data-id="2407.14509v1">Interpret</button>
                <div id="interpretation-2407.14509v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On Pre-training of Multimodal Language Models Customized for Chart Understanding</h3>
                <p>Authors: Wan-Cyuan FanYen-Chun ChenMengchen LiuLu YuanLeonid Sigal</p>
                <p><a href="http://arxiv.org/abs/2407.14506v1">Link to paper</a></p>
                <p>Recent studies customizing Multimodal Large Language Models MLLMs fordomain-specific tasks have yielded promising results especially in the fieldof scientific chart comprehension. These studies generally utilize visualinstruction tuning with specialized datasets to enhance question and answerQA accuracy within the chart domain. However they often neglect thefundamental discrepancy between natural image-caption pre-training data anddigital chart image-QA data particularly in the models capacity to extractunderlying numeric values from charts. This paper tackles this oversight byexploring the training processes necessary to improve MLLMs comprehension ofcharts. We present three key findings: 1 Incorporating raw data values inalignment pre-training markedly improves comprehension of chart data. 2Replacing images with their textual representation randomly during end-to-endfine-tuning transfer the language reasoning capability to chart interpretationskills. 3 Requiring the model to first extract the underlying chart data andthen answer the question in the fine-tuning can further improve the accuracy.Consequently we introduce CHOPINLLM an MLLM tailored for in-depth chartcomprehension. CHOPINLLM effectively interprets various types of chartsincluding unannotated ones while maintaining robust reasoning abilities.Furthermore we establish a new benchmark to evaluate MLLMs understanding ofdifferent chart types across various comprehension levels. Experimental resultsshow that CHOPINLLM exhibits strong performance in understanding both annotatedand unannotated charts across a wide range of types.</p>
                <p>Last Updated: 2024-07-19 17:58:36 UTC</p>
                <button class="interpret-button" data-id="2407.14506v1">Interpret</button>
                <div id="interpretation-2407.14506v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery</h3>
                <p>Authors: Sukrut RaoSweta MahajanMoritz BöhleBernt Schiele</p>
                <p><a href="http://arxiv.org/abs/2407.14499v1">Link to paper</a></p>
                <p>Concept Bottleneck Models CBMs have recently been proposed to address theblack-box problem of deep neural networks by first mapping images to ahuman-understandable concept space and then linearly combining concepts forclassification. Such models typically require first coming up with a set ofconcepts relevant to the task and then aligning the representations of afeature extractor to map to these concepts. However even with powerfulfoundational feature extractors like CLIP there are no guarantees that thespecified concepts are detectable. In this work we leverage recent advances inmechanistic interpretability and propose a novel CBM approach -- calledDiscover-then-Name-CBM DN-CBM -- that inverts the typical paradigm: insteadof pre-selecting concepts based on the downstream classification task we usesparse autoencoders to first discover concepts learnt by the model and thenname them and train linear probes for classification. Our concept extractionstrategy is efficient since it is agnostic to the downstream task and usesconcepts already known to the model. We perform a comprehensive evaluationacross multiple datasets and CLIP architectures and show that our method yieldssemantically meaningful concepts assigns appropriate names to them that makethem easy to interpret and yields performant and interpretable CBMs. Codeavailable at https://github.com/neuroexplicit-saar/discover-then-name.</p>
                <p>Last Updated: 2024-07-19 17:50:11 UTC</p>
                <button class="interpret-button" data-id="2407.14499v1">Interpret</button>
                <div id="interpretation-2407.14499v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Explainable Post hoc Portfolio Management Financial Policy of a Deep Reinforcement Learning agent</h3>
                <p>Authors: Alejandra de la Rica EscuderoEduardo C. Garrido-MerchanMaria Coronado-Vaca</p>
                <p><a href="http://arxiv.org/abs/2407.14486v1">Link to paper</a></p>
                <p>Financial portfolio management investment policies computed quantitatively bymodern portfolio theory techniques like the Markowitz model rely on a set onassumptions that are not supported by data in high volatility markets. Hencequantitative researchers are looking for alternative models to tackle thisproblem. Concretely portfolio management is a problem that has beensuccessfully addressed recently by Deep Reinforcement Learning DRLapproaches. In particular DRL algorithms train an agent by estimating thedistribution of the expected reward of every action performed by an agent givenany financial state in a simulator. However these methods rely on Deep NeuralNetworks model to represent such a distribution that although they areuniversal approximator models they cannot explain its behaviour given by aset of parameters that are not interpretable. Critically financial investorspolicies require predictions to be interpretable so DRL agents are not suitedto follow a particular policy or explain their actions. In this work wedeveloped a novel Explainable Deep Reinforcement Learning XDRL approach forportfolio management integrating the Proximal Policy Optimization PPO withthe model agnostic explainable techniques of feature importance SHAP and LIMEto enhance transparency in prediction time. By executing our methodology wecan interpret in prediction time the actions of the agent to assess whetherthey follow the requisites of an investment policy or to assess the risk offollowing the agent suggestions. To the best of our knowledge our proposedapproach is the first explainable post hoc portfolio management financialpolicy of a DRL agent. We empirically illustrate our methodology bysuccessfully identifying key features influencing investment decisions whichdemonstrate the ability to explain the agent actions in prediction time.</p>
                <p>Last Updated: 2024-07-19 17:40:39 UTC</p>
                <button class="interpret-button" data-id="2407.14486v1">Interpret</button>
                <div id="interpretation-2407.14486v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities</h3>
                <p>Authors: Peng XuWei PingXianchao WuZihan LiuMohammad ShoeybiBryan Catanzaro</p>
                <p><a href="http://arxiv.org/abs/2407.14482v1">Link to paper</a></p>
                <p>In this work we introduce ChatQA 2 a Llama3-based model designed to bridgethe gap between open-access LLMs and leading proprietary models e.g.GPT-4-Turbo in long-context understanding and retrieval-augmented generationRAG capabilities. These two capabilities are essential for LLMs to processlarge volumes of information that cannot fit into a single prompt and arecomplementary to each other depending on the downstream tasks andcomputational budgets. We present a detailed continued training recipe toextend the context window of Llama3-70B-base from 8K to 128K tokens along witha three-stage instruction tuning process to enhance the modelsinstruction-following RAG performance and long-context understandingcapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B modelachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-contextunderstanding tasks and surpasses it on the RAG benchmark. Interestingly wefind that the state-of-the-art long-context retriever can alleviate the top-kcontext fragmentation issue in RAG further improving RAG-based results forlong-context understanding tasks. We also provide extensive comparisons betweenRAG and long-context solutions using state-of-the-art long-context LLMs.</p>
                <p>Last Updated: 2024-07-19 17:35:47 UTC</p>
                <button class="interpret-button" data-id="2407.14482v1">Interpret</button>
                <div id="interpretation-2407.14482v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Conformal Thresholded Intervals for Efficient Regression</h3>
                <p>Authors: Rui LuoZhixin Zhou</p>
                <p><a href="http://arxiv.org/abs/2407.14495v1">Link to paper</a></p>
                <p>This paper introduces Conformal Thresholded Intervals CTI a novelconformal regression method that aims to produce the smallest possibleprediction set with guaranteed coverage. Unlike existing methods that rely onnested conformal framework and full conditional distribution estimation CTIestimates the conditional probability density for a new response to fall intoeach interquantile interval using off-the-shelf multi-output quantileregression. CTI constructs prediction sets by thresholding the estimatedconditional interquantile intervals based on their length which is inverselyproportional to the estimated probability density. The threshold is determinedusing a calibration set to ensure marginal coverage. Experimental resultsdemonstrate that CTI achieves optimal performance across various datasets.</p>
                <p>Last Updated: 2024-07-19 17:47:08 UTC</p>
                <button class="interpret-button" data-id="2407.14495v1">Interpret</button>
                <div id="interpretation-2407.14495v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Enhancing Variable Importance in Random Forests: A Novel Application of Global Sensitivity Analysis</h3>
                <p>Authors: Giulia VannucciRoberta SicilianoAndrea Saltelli</p>
                <p><a href="http://arxiv.org/abs/2407.14194v1">Link to paper</a></p>
                <p>The present work provides an application of Global Sensitivity Analysis tosupervised machine learning methods such as Random Forests. These methods actas black boxes selecting features in high--dimensional data sets as to provideaccurate classifiers in terms of prediction when new data are fed into thesystem. In supervised machine learning predictors are generally ranked byimportance based on their contribution to the final prediction. GlobalSensitivity Analysis is primarily used in mathematical modelling to investigatethe effect of the uncertainties of the input variables on the output. We applyit here as a novel way to rank the input features by their importance to theexplainability of the data generating process shedding light on how theresponse is determined by the dependence structure of its predictors. Asimulation study shows that our proposal can be used to explore what advancescan be achieved either in terms of efficiency explanatory ability or simplyby way of confirming existing results.</p>
                <p>Last Updated: 2024-07-19 10:45:36 UTC</p>
                <button class="interpret-button" data-id="2407.14194v1">Interpret</button>
                <div id="interpretation-2407.14194v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Achieving Well-Informed Decision-Making in Drug Discovery: A Comprehensive Calibration Study using Neural Network-Based Structure-Activity Models</h3>
                <p>Authors: Hannah Rosa FriesacherOla EngkvistLewis MervinYves MoreauAdam Arany</p>
                <p><a href="http://arxiv.org/abs/2407.14185v1">Link to paper</a></p>
                <p>In the drug discovery process where experiments can be costly andtime-consuming computational models that predict drug-target interactions arevaluable tools to accelerate the development of new therapeutic agents.Estimating the uncertainty inherent in these neural network predictionsprovides valuable information that facilitates optimal decision-making whenrisk assessment is crucial. However such models can be poorly calibratedwhich results in unreliable uncertainty estimates that do not reflect the truepredictive uncertainty. In this study we compare different metrics includingaccuracy and calibration scores used for model hyperparameter tuning toinvestigate which model selection strategy achieves well-calibrated models.Furthermore we propose to use a computationally efficient Bayesian uncertaintyestimation method named Bayesian Linear Probing BLP which generatesHamiltonian Monte Carlo HMC trajectories to obtain samples for the parametersof a Bayesian Logistic Regression fitted to the hidden layer of the baselineneural network. We report that BLP improves model calibration and achieves theperformance of common uncertainty quantification methods by combining thebenefits of uncertainty estimation and probability calibration methods.Finally we show that combining post hoc calibration method withwell-performing uncertainty quantification approaches can boost model accuracyand calibration.</p>
                <p>Last Updated: 2024-07-19 10:29:00 UTC</p>
                <button class="interpret-button" data-id="2407.14185v1">Interpret</button>
                <div id="interpretation-2407.14185v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On Policy Evaluation Algorithms in Distributional Reinforcement Learning</h3>
                <p>Authors: Julian GerstenbergRalph NeiningerDenis Spiegel</p>
                <p><a href="http://arxiv.org/abs/2407.14175v1">Link to paper</a></p>
                <p>We introduce a novel class of algorithms to efficiently approximate theunknown return distributions in policy evaluation problems from distributionalreinforcement learning DRL. The proposed distributional dynamic programmingalgorithms are suitable for underlying Markov decision processes MDPs havingan arbitrary probabilistic reward mechanism including continuous rewarddistributions with unbounded support being potentially heavy-tailed.  For a plain instance of our proposed class of algorithms we prove errorbounds both within Wasserstein and Kolmogorov--Smirnov distances. Furthermorefor return distributions having probability density functions the algorithmsyield approximations for these densities error bounds are given withinsupremum norm. We introduce the concept of quantile-spline discretizations tocome up with algorithms showing promising results in simulation experiments.  While the performance of our algorithms can rigorously be analysed they canbe seen as universal black box algorithms applicable to a large class of MDPs.We also derive new properties of probability metrics commonly used in DRL onwhich our quantitative analysis is based.</p>
                <p>Last Updated: 2024-07-19 10:06:01 UTC</p>
                <button class="interpret-button" data-id="2407.14175v1">Interpret</button>
                <div id="interpretation-2407.14175v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MSCT: Addressing Time-Varying Confounding with Marginal Structural Causal Transformer for Counterfactual Post-Crash Traffic Prediction</h3>
                <p>Authors: Shuang LiZiyuan PuNan ZhangDuxin ChenLu DongDaniel J. GrahamYinhai Wang</p>
                <p><a href="http://arxiv.org/abs/2407.14065v1">Link to paper</a></p>
                <p>Traffic crashes profoundly impede traffic efficiency and pose economicchallenges. Accurate prediction of post-crash traffic status provides essentialinformation for evaluating traffic perturbations and developing effectivesolutions. Previous studies have established a series of deep learning modelsto predict post-crash traffic conditions however these correlation-basedmethods cannot accommodate the biases caused by time-varying confounders andthe heterogeneous effects of crashes. The post-crash traffic prediction modelneeds to estimate the counterfactual traffic speed response to hypotheticalcrashes under various conditions which demonstrates the necessity ofunderstanding the causal relationship between traffic factors. Therefore thispaper presents the Marginal Structural Causal Transformer MSCT a novel deeplearning model designed for counterfactual post-crash traffic prediction. Toaddress the issue of time-varying confounding bias MSCT incorporates astructure inspired by Marginal Structural Models and introduces a balanced lossfunction to facilitate learning of invariant causal features. The proposedmodel is treatment-aware with a specific focus on comprehending and predictingtraffic speed under hypothetical crash intervention strategies. In the absenceof ground-truth data a synthetic data generation procedure is proposed toemulate the causal mechanism between traffic speed crashes and covariates.The model is validated using both synthetic and real-world data demonstratingthat MSCT outperforms state-of-the-art models in multi-step-ahead predictionperformance. This study also systematically analyzes the impact of time-varyingconfounding bias and dataset distribution on model performance contributingvaluable insights into counterfactual prediction for intelligent transportationsystems.</p>
                <p>Last Updated: 2024-07-19 06:42:41 UTC</p>
                <button class="interpret-button" data-id="2407.14065v1">Interpret</button>
                <div id="interpretation-2407.14065v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-07-22</p>
        </div>
    
        </div>
    </body>
    </html>
    