
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Performative Prediction on Games and Mechanism Design</h3>
                <p>Authors: António GóisMehrnaz MofakhamiFernando P. SantosSimon Lacoste-JulienGauthier Gidel</p>
                <p><a href="http://arxiv.org/abs/2408.05146v1">Link to paper</a></p>
                <p>Predictions often influence the reality which they aim to predict an effectknown as performativity. Existing work focuses on accuracy maximization underthis effect but model deployment may have important unintended impactsespecially in multiagent scenarios. In this work we investigate performativeprediction in a concrete game-theoretic setting where social welfare is analternative objective to accuracy maximization. We explore a collective riskdilemma scenario where maximising accuracy can negatively impact socialwelfare when predicting collective behaviours. By assuming knowledge of aBayesian agent behavior model we then show how to achieve better trade-offsand use them for mechanism design.</p>
                <p>Last Updated: 2024-08-09 16:03:44 UTC</p>
                <button class="interpret-button" data-id="2408.05146v1">Interpret</button>
                <div id="interpretation-2408.05146v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Performance Prediction of Hub-Based Swarms</h3>
                <p>Authors: Puneet JainChaitanya DwivediVigynesh BhattNick SmithMichael A Goodrich</p>
                <p><a href="http://arxiv.org/abs/2408.04822v1">Link to paper</a></p>
                <p>A hub-based colony consists of multiple agents who share a common nest sitecalled the hub. Agents perform tasks away from the hub like foraging for foodor gathering information about future nest sites. Modeling hub-based coloniesis challenging because the size of the collective state space grows rapidly asthe number of agents grows. This paper presents a graph-based representation ofthe colony that can be combined with graph-based encoders to createlow-dimensional representations of collective state that can scale to manyagents for a best-of-N colony problem. We demonstrate how the information inthe low-dimensional embedding can be used with two experiments. First we showhow the information in the tensor can be used to cluster collective states bythe probability of choosing the best site for a very small problem. Second weshow how structured collective trajectories emerge when a graph encoder is usedto learn the low-dimensional embedding and these trajectories have informationthat can be used to predict swarm performance.</p>
                <p>Last Updated: 2024-08-09 02:31:03 UTC</p>
                <button class="interpret-button" data-id="2408.04822v1">Interpret</button>
                <div id="interpretation-2408.04822v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Multi-Scale Cognitive Interaction Model of Instrument Operations at the Linac Coherent Light Source</h3>
                <p>Authors: Jonathan SegalWan-Lin HuPaul FuossFrank E. RitterJeff Shrager</p>
                <p><a href="http://arxiv.org/abs/2408.04734v1">Link to paper</a></p>
                <p>We describe a novel multi-agent multi-scale computational cognitiveinteraction model of instrument operations at the Linac Coherent Light SourceLCLS. A leading scientific user facility LCLS is the worlds first hardx-ray free electron laser operated by the SLAC National Accelerator Laboratoryfor the U.S. Department of Energy. As the worlds first x-ray free electronlaser LCLS is in high demand and heavily oversubscribed. Our overall projectemploys cognitive engineering methodologies to improve experimental efficiencyand scientific productivity by refining experimental interfaces and workflowssimplifying tasks reducing errors and improving operator safety and stresslevels. Our model simulates aspects of human cognition at multiple cognitiveand temporal scales ranging from seconds to hours and among agents playingmultiple roles including instrument operator real time data analyst andexperiment manager. The model can predict impacts stemming from proposedchanges to operational interfaces and workflows. Because the model code is opensource and supplemental videos go into detail on all aspects of the model andresults this approach could be applied to other experimental apparatus andprocesses. Example results demonstrate the models potential in guidingmodifications to improve operational efficiency and scientific output. Wediscuss the implications of our findings for cognitive engineering in complexexperimental settings and outline future directions for research.</p>
                <p>Last Updated: 2024-08-08 19:23:44 UTC</p>
                <button class="interpret-button" data-id="2408.04734v1">Interpret</button>
                <div id="interpretation-2408.04734v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning Fair Cooperation in Mixed-Motive Games with Indirect Reciprocity</h3>
                <p>Authors: Martin SmitFernando P. Santos</p>
                <p><a href="http://dx.doi.org/10.24963/ijcai.2024/25">Link to paper</a></p>
                <p>Altruistic cooperation is costly yet socially desirable. As a result agentsstruggle to learn cooperative policies through independent reinforcementlearning RL. Indirect reciprocity where agents consider their interactionpartners reputation has been shown to stabilise cooperation in homogeneousidealised populations. However more realistic settings are comprised ofheterogeneous agents with different characteristics and group-based socialidentities. We study cooperation when agents are stratified into two suchgroups and allow reputation updates and actions to depend on groupinformation. We consider two modelling approaches: evolutionary game theorywhere we comprehensively search for social norms i.e. rules to assignreputations leading to cooperation and fairness and RL where we consider howthe stochastic dynamics of policy learning affects the analytically identifiedequilibria. We observe that a defecting majority leads the minority group todefect but not the inverse. Moreover changing the norms that judge in andout-group interactions can steer a system towards either fair or unfaircooperation. This is made clearer when moving beyond equilibrium analysis toindependent RL agents where convergence to fair cooperation occurs with anarrower set of norms. Our results highlight that in heterogeneous populationswith reputations carefully defining interaction norms is fundamental to tackleboth dilemmas of cooperation and of fairness.</p>
                <p>Last Updated: 2024-08-08 15:57:15 UTC</p>
                <button class="interpret-button" data-id="2408.04549v1">Interpret</button>
                <div id="interpretation-2408.04549v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Emergence in Multi-Agent Systems: A Safety Perspective</h3>
                <p>Authors: Philipp AltmannJulian SchönbergerSteffen IlliumMaximilian ZornFabian RitzTom HaiderSimon BurtonThomas Gabor</p>
                <p><a href="http://arxiv.org/abs/2408.04514v1">Link to paper</a></p>
                <p>Emergent effects can arise in multi-agent systems MAS where execution isdecentralized and reliant on local information. These effects may range fromminor deviations in behavior to catastrophic system failures. To formallydefine these effects we identify misalignments between the global inherentspecification the true specification and its local approximation such as theconfiguration of different reward components or observations. Usingestablished safety terminology we develop a framework to understand theseemergent effects. To showcase the resulting implications we use two broadlyconfigurable exemplary gridworld scenarios where insufficient specificationleads to unintended behavior deviations when derived independently. Recognizingthat a global adaptation might not always be feasible we propose adjusting theunderlying parameterizations to mitigate these issues thereby improving thesystems alignment and reducing the risk of emergent failures.</p>
                <p>Last Updated: 2024-08-08 15:15:28 UTC</p>
                <button class="interpret-button" data-id="2408.04514v1">Interpret</button>
                <div id="interpretation-2408.04514v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>VITA: Towards Open-Source Interactive Omni Multimodal LLM</h3>
                <p>Authors: Chaoyou FuHaojia LinZuwei LongYunhang ShenMeng ZhaoYifan ZhangXiong WangDi YinLong MaXiawu ZhengRan HeRongrong JiYunsheng WuCaifeng ShanXing Sun</p>
                <p><a href="http://arxiv.org/abs/2408.05211v1">Link to paper</a></p>
                <p>The remarkable multimodal capabilities and interactive experience of GPT-4ounderscore their necessity in practical applications yet open-source modelsrarely excel in both areas. In this paper we introduce VITA the first-everopen-source Multimodal Large Language Model MLLM adept at simultaneousprocessing and analysis of Video Image Text and Audio modalities andmeanwhile has an advanced multimodal interactive experience. Starting fromMixtral 8x7B as a language foundation we expand its Chinese vocabularyfollowed by bilingual instruction tuning. We further endow the language modelwith visual and audio capabilities through two-stage multi-task learning ofmultimodal alignment and instruction tuning. VITA demonstrates robustfoundational capabilities of multilingual vision and audio understanding asevidenced by its strong performance across a range of both unimodal andmultimodal benchmarks. Beyond foundational capabilities we have madeconsiderable progress in enhancing the natural multimodal human-computerinteraction experience. To the best of our knowledge we are the first toexploit non-awakening interaction and audio interrupt in MLLM. VITA is thefirst step for the open-source community to explore the seamless integration ofmultimodal understanding and interaction. While there is still lots of work tobe done on VITA to get close to close-source counterparts we hope that itsrole as a pioneer can serve as a cornerstone for subsequent research. ProjectPage: https://vita-home.github.io.</p>
                <p>Last Updated: 2024-08-09 17:59:49 UTC</p>
                <button class="interpret-button" data-id="2408.05211v1">Interpret</button>
                <div id="interpretation-2408.05211v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-Garment Customized Model Generation</h3>
                <p>Authors: Yichen LiuPenghui DuYi Liu Quanwei Zhang</p>
                <p><a href="http://arxiv.org/abs/2408.05206v1">Link to paper</a></p>
                <p>This paper introduces Multi-Garment Customized Model Generation a unifiedframework based on Latent Diffusion Models LDMs aimed at addressing theunexplored task of synthesizing images with free combinations of multiplepieces of clothing. The method focuses on generating customized models wearingvarious targeted outfits according to different text prompts. The primarychallenge lies in maintaining the natural appearance of the dressed model whilepreserving the complex textures of each piece of clothing ensuring that theinformation from different garments does not interfere with each other. Totackle these challenges we first developed a garment encoder which is atrainable UNet copy with shared weights capable of extracting detailedfeatures of garments in parallel. Secondly our framework supports theconditional generation of multiple garments through decoupled multi-garmentfeature fusion allowing multiple clothing features to be injected into thebackbone network significantly alleviating conflicts between garmentinformation. Additionally the proposed garment encoder is a plug-and-playmodule that can be combined with other extension modules such as IP-Adapter andControlNet enhancing the diversity and controllability of the generatedmodels. Extensive experiments demonstrate the superiority of our approach overexisting alternatives opening up new avenues for the task of generating imageswith multiple-piece clothing combinations</p>
                <p>Last Updated: 2024-08-09 17:57:33 UTC</p>
                <button class="interpret-button" data-id="2408.05206v1">Interpret</button>
                <div id="interpretation-2408.05206v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Kalman-Inspired Feature Propagation for Video Face Super-Resolution</h3>
                <p>Authors: Ruicheng FengChongyi LiChen Change Loy</p>
                <p><a href="http://arxiv.org/abs/2408.05205v1">Link to paper</a></p>
                <p>Despite the promising progress of face image super-resolution video facesuper-resolution remains relatively under-explored. Existing approaches eitheradapt general video super-resolution networks to face datasets or applyestablished face image super-resolution models independently on individualvideo frames. These paradigms encounter challenges either in reconstructingfacial details or maintaining temporal consistency. To address these issues weintroduce a novel framework called Kalman-inspired Feature Propagation KEEPdesigned to maintain a stable face prior over time. The Kalman filteringprinciples offer our method a recurrent ability to use the information frompreviously restored frames to guide and regulate the restoration process of thecurrent frame. Extensive experiments demonstrate the effectiveness of ourmethod in capturing facial details consistently across video frames. Code andvideo demo are available at https://jnjaby.github.io/projects/KEEP.</p>
                <p>Last Updated: 2024-08-09 17:57:12 UTC</p>
                <button class="interpret-button" data-id="2408.05205v1">Interpret</button>
                <div id="interpretation-2408.05205v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Cross-Domain Learning for Video Anomaly Detection with Limited Supervision</h3>
                <p>Authors: Yashika JainAli DaboueiMin Xu</p>
                <p><a href="http://arxiv.org/abs/2408.05191v1">Link to paper</a></p>
                <p>Video Anomaly Detection VAD automates the identification of unusual eventssuch as security threats in surveillance videos. In real-world applicationsVAD models must effectively operate in cross-domain settings identifying rareanomalies and scenarios not well-represented in the training data. Howeverexisting cross-domain VAD methods focus on unsupervised learning resulting inperformance that falls short of real-world expectations. Since acquiring weaksupervision i.e. video-level labels for the source domain is cost-effectivewe conjecture that combining it with external unlabeled data has notablepotential to enhance cross-domain performance. To this end we introduce anovel weakly-supervised framework for Cross-Domain Learning CDL in VAD thatincorporates external data during training by estimating its prediction biasand adaptively minimizing that using the predicted uncertainty. We demonstratethe effectiveness of the proposed CDL framework through comprehensiveexperiments conducted in various configurations on two large-scale VADdatasets: UCF-Crime and XD-Violence. Our method significantly surpasses thestate-of-the-art works in cross-domain evaluations achieving an averageabsolute improvement of 19.6 on UCF-Crime and 12.87 on XD-Violence.</p>
                <p>Last Updated: 2024-08-09 17:28:29 UTC</p>
                <button class="interpret-button" data-id="2408.05191v1">Interpret</button>
                <div id="interpretation-2408.05191v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Weak-Annotation of HAR Datasets using Vision Foundation Models</h3>
                <p>Authors: Marius BockKristof Van LaerhovenMichael Moeller</p>
                <p><a href="http://arxiv.org/abs/2408.05169v1">Link to paper</a></p>
                <p>As wearable-based data annotation remains to date a tedious time-consumingtask requiring researchers to dedicate substantial time benchmark datasetswithin the field of Human Activity Recognition in lack richness and sizecompared to datasets available within related fields. Recently visionfoundation models such as CLIP have gained significant attention helping thevision community advance in finding robust generalizable featurerepresentations. With the majority of researchers within the wearable communityrelying on vision modalities to overcome the limited expressiveness of wearabledata and accurately label their to-be-released benchmark datasets offline wepropose a novel clustering-based annotation pipeline to significantly reducethe amount of data that needs to be annotated by a human annotator. We showthat using our approach the annotation of centroid clips suffices to achieveaverage labelling accuracies close to 90 across three publicly available HARbenchmark datasets. Using the weakly annotated datasets we further demonstratethat we can match the accuracy scores of fully-supervised deep learningclassifiers across all three benchmark datasets. Code as well as supplementaryfigures and results are publicly downloadable viagithub.com/mariusbock/weak_har.</p>
                <p>Last Updated: 2024-08-09 16:46:53 UTC</p>
                <button class="interpret-button" data-id="2408.05169v1">Interpret</button>
                <div id="interpretation-2408.05169v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions</h3>
                <p>Authors: Michele MirandaElena Sofia RuzzettiAndrea SantilliFabio Massimo ZanzottoSébastien BratièresEmanuele Rodolà</p>
                <p><a href="http://arxiv.org/abs/2408.05212v1">Link to paper</a></p>
                <p>Large Language Models LLMs represent a significant advancement inartificial intelligence finding applications across various domains. Howevertheir reliance on massive internet-sourced datasets for training brings notableprivacy issues which are exacerbated in critical domains e.g. healthcare.Moreover certain application-specific scenarios may require fine-tuning thesemodels on private data. This survey critically examines the privacy threatsassociated with LLMs emphasizing the potential for these models to memorizeand inadvertently reveal sensitive information. We explore current threats byreviewing privacy attacks on LLMs and propose comprehensive solutions forintegrating privacy mechanisms throughout the entire learning pipeline. Thesesolutions range from anonymizing training datasets to implementing differentialprivacy during training or inference and machine unlearning after training. Ourcomprehensive review of existing literature highlights ongoing challengesavailable tools and future directions for preserving privacy in LLMs. Thiswork aims to guide the development of more secure and trustworthy AI systems byproviding a thorough understanding of privacy preservation methods and theireffectiveness in mitigating risks.</p>
                <p>Last Updated: 2024-08-10 05:41:19 UTC</p>
                <button class="interpret-button" data-id="2408.05212v1">Interpret</button>
                <div id="interpretation-2408.05212v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Cell Morphology-Guided Small Molecule Generation with GFlowNets</h3>
                <p>Authors: Stephen Zhewen LuZiqing LuEhsan HajiramezanaliTommaso BiancalaniYoshua BengioGabriele ScaliaMichał Koziarski</p>
                <p><a href="http://arxiv.org/abs/2408.05196v1">Link to paper</a></p>
                <p>High-content phenotypic screening including high-content imaging HCI hasgained popularity in the last few years for its ability to characterize noveltherapeutics without prior knowledge of the protein target. When combined withdeep learning techniques to predict and represent molecular-phenotypeinteractions these advancements hold the potential to significantly accelerateand enhance drug discovery applications. This work focuses on the novel task ofHCI-guided molecular design. Generative models for molecule design could beguided by HCI data for example with a supervised model that links molecules tophenotypes of interest as a reward function. However limited labeled datacombined with the high-dimensional readouts can make training these methodschallenging and impractical. We consider an alternative approach in which weleverage an unsupervised multimodal joint embedding to define a latentsimilarity as a reward for GFlowNets. The proposed model learns to generate newmolecules that could produce phenotypic effects similar to those of the givenimage target without relying on pre-annotated phenotypic labels. Wedemonstrate that the proposed method generates molecules with highmorphological and structural similarity to the target increasing thelikelihood of similar biological activity as confirmed by an independentoracle model.</p>
                <p>Last Updated: 2024-08-09 17:40:35 UTC</p>
                <button class="interpret-button" data-id="2408.05196v1">Interpret</button>
                <div id="interpretation-2408.05196v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>HistoKernel: Whole Slide Image Level Maximum Mean Discrepancy Kernels for Pan-Cancer Predictive Modelling</h3>
                <p>Authors: Piotr KellerMuhammad DawoodBrinder Singh ChohanFayyaz ul Amir Afsar Minhas</p>
                <p><a href="http://arxiv.org/abs/2408.05195v1">Link to paper</a></p>
                <p>Machine learning in computational pathology CPath often aggregatespatch-level predictions from multi-gigapixel Whole Slide Images WSIs togenerate WSI-level prediction scores for crucial tasks such as survivalprediction and drug effect prediction. However current methods do notexplicitly characterize distributional differences between patch sets withinWSIs. We introduce HistoKernel a novel Maximum Mean Discrepancy MMD kernelthat measures distributional similarity between WSIs for enhanced predictionperformance on downstream prediction tasks.  Our comprehensive analysis demonstrates HistoKernels effectiveness acrossvarious machine learning tasks including retrieval n  9362 drugsensitivity regression n  551 point mutation classification n  3419and survival analysis n  2291 outperforming existing deep learningmethods. Additionally HistoKernel seamlessly integrates multi-modal data andoffers a novel perturbation-based method for patch-level explainability. Thiswork pioneers the use of kernel-based methods for WSI-level predictivemodeling opening new avenues for research. Code is available athttps://github.com/pkeller00/HistoKernel.</p>
                <p>Last Updated: 2024-08-09 17:40:08 UTC</p>
                <button class="interpret-button" data-id="2408.05195v1">Interpret</button>
                <div id="interpretation-2408.05195v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ECG-FM: An Open Electrocardiogram Foundation Model</h3>
                <p>Authors: Kaden McKeenLaura OlivaSameer MasoodAugustin TomaBarry RubinBo Wang</p>
                <p><a href="http://arxiv.org/abs/2408.05178v1">Link to paper</a></p>
                <p>The electrocardiogram ECG is a ubiquitous diagnostic test. Conventionaltask-specific ECG analysis models require large numbers of expensive ECGannotations or associated labels to train. Transfer learning techniques havebeen shown to improve generalization and reduce reliance on labeled data. Wepresent ECG-FM an open foundation model for ECG analysis and conduct acomprehensive study performed on a dataset of 1.66 million ECGs sourced fromboth publicly available and private institutional sources. ECG-FM adopts atransformer-based architecture and is pretrained on 2.5 million samples usingECG-specific augmentations and contrastive learning as well as a continuoussignal masking objective. Our transparent evaluation includes a diverse rangeof downstream tasks where we predict ECG interpretation labels reduced leftventricular ejection fraction and abnormal cardiac troponin. AffirmingECG-FMs effectiveness as a foundation model we demonstrate how its command ofcontextual information results in strong performance rich pretrainedembeddings and reliable interpretability. Due to a lack of open-weightpractices we highlight how ECG analysis is lagging behind other medicalmachine learning subfields in terms of foundation model adoption. Our code isavailable at https://github.com/bowang-lab/ECG-FM/.</p>
                <p>Last Updated: 2024-08-09 17:06:49 UTC</p>
                <button class="interpret-button" data-id="2408.05178v1">Interpret</button>
                <div id="interpretation-2408.05178v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Beyond Closure Models: Learning Chaotic-Systems via Physics-Informed Neural Operators</h3>
                <p>Authors: Chuwei WangJulius BernerZongyi LiDi ZhouJiayun WangJane BaeAnima Anandkumar</p>
                <p><a href="http://arxiv.org/abs/2408.05177v1">Link to paper</a></p>
                <p>Accurately predicting the long-term behavior of chaotic systems is crucialfor various applications such as climate modeling. However achieving suchpredictions typically requires iterative computations over a densespatiotemporal grid to account for the unstable nature of chaotic systemswhich is expensive and impractical in many real-world situations. Analternative approach to such a full-resolved simulation is using a coarse gridand then correcting its errors through a textitclosure model whichapproximates the overall information from fine scales not captured in thecoarse-grid simulation. Recently ML approaches have been used for closuremodeling but they typically require a large number of training samples fromexpensive fully-resolved simulations FRS. In this work we prove an even morefundamental limitation i.e. the standard approach to learning closure modelssuffers from a large approximation error for generic problems no matter howlarge the model is and it stems from the non-uniqueness of the mapping. Wepropose an alternative end-to-end learning approach using a physics-informedneural operator PINO that overcomes this limitation by not using a closuremodel or a coarse-grid solver. We first train the PINO model on data from acoarse-grid solver and then fine-tune it with a small amount of FRS andphysics-based losses on a fine grid. The discretization-free nature of neuraloperators means that they do not suffer from the restriction of a coarse gridthat closure models face and they can provably approximate the long-termstatistics of chaotic systems. In our experiments our PINO model achieves a120x speedup compared to FRS with a relative error sim 5. In contrast theclosure model coupled with a coarse-grid solver is 58x slower than PINO whilehaving a much higher error sim205 when the closure model is trained on thesame FRS dataset.</p>
                <p>Last Updated: 2024-08-09 17:05:45 UTC</p>
                <button class="interpret-button" data-id="2408.05177v1">Interpret</button>
                <div id="interpretation-2408.05177v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Evaluating the capability of large language models to personalize science texts for diverse middle-school-age learners</h3>
                <p>Authors: Michael Vaccaro JrMikayla FridayArash Zaghi</p>
                <p><a href="http://arxiv.org/abs/2408.05204v1">Link to paper</a></p>
                <p>Large language models LLMs including OpenAIs GPT-series have madesignificant advancements in recent years. Known for their expertise acrossdiverse subject areas and quick adaptability to user-provided prompts LLMshold unique potential as Personalized Learning PL tools. Despite thispotential their application in K-12 education remains largely unexplored. Thispaper presents one of the first randomized controlled trials n  23 toevaluate the effectiveness of GPT-4 in personalizing educational science textsfor middle school students. In this study GPT-4 was used to profile studentlearning preferences based on choices made during a training session. For theexperimental group GPT-4 was used to rewrite science texts to align with thestudents predicted profile while for students in the control group textswere rewritten to contradict their learning preferences. The results of aMann-Whitney U test showed that students significantly preferred at the .10level the rewritten texts when they were aligned with their profile p .059. These findings suggest that GPT-4 can effectively interpret and tailoreducational content to diverse learner preferences marking a significantadvancement in PL technology. The limitations of this study and ethicalconsiderations for using artificial intelligence in education are alsodiscussed.</p>
                <p>Last Updated: 2024-08-09 17:53:35 UTC</p>
                <button class="interpret-button" data-id="2408.05204v1">Interpret</button>
                <div id="interpretation-2408.05204v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Weak-Annotation of HAR Datasets using Vision Foundation Models</h3>
                <p>Authors: Marius BockKristof Van LaerhovenMichael Moeller</p>
                <p><a href="http://arxiv.org/abs/2408.05169v1">Link to paper</a></p>
                <p>As wearable-based data annotation remains to date a tedious time-consumingtask requiring researchers to dedicate substantial time benchmark datasetswithin the field of Human Activity Recognition in lack richness and sizecompared to datasets available within related fields. Recently visionfoundation models such as CLIP have gained significant attention helping thevision community advance in finding robust generalizable featurerepresentations. With the majority of researchers within the wearable communityrelying on vision modalities to overcome the limited expressiveness of wearabledata and accurately label their to-be-released benchmark datasets offline wepropose a novel clustering-based annotation pipeline to significantly reducethe amount of data that needs to be annotated by a human annotator. We showthat using our approach the annotation of centroid clips suffices to achieveaverage labelling accuracies close to 90 across three publicly available HARbenchmark datasets. Using the weakly annotated datasets we further demonstratethat we can match the accuracy scores of fully-supervised deep learningclassifiers across all three benchmark datasets. Code as well as supplementaryfigures and results are publicly downloadable viagithub.com/mariusbock/weak_har.</p>
                <p>Last Updated: 2024-08-09 16:46:53 UTC</p>
                <button class="interpret-button" data-id="2408.05169v1">Interpret</button>
                <div id="interpretation-2408.05169v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Large Language Models and Thematic Analysis: Human-AI Synergy in Researching Hate Speech on Social Media</h3>
                <p>Authors: Petre BreazuMiriam SchirmerSongbo HuNapoleon Kastos</p>
                <p><a href="http://arxiv.org/abs/2408.05126v1">Link to paper</a></p>
                <p>In the dynamic field of artificial intelligence AI the development andapplication of Large Language Models LLMs for text analysis are ofsignificant academic interest. Despite the promising capabilities of variousLLMs in conducting qualitative analysis their use in the humanities and socialsciences has not been thoroughly examined. This article contributes to theemerging literature on LLMs in qualitative analysis by documenting anexperimental study involving GPT-4. The study focuses on performing thematicanalysis TA using a YouTube dataset derived from an EU-funded project whichwas previously analyzed by other researchers. This dataset is about therepresentation of Roma migrants in Sweden during 2016 a period marked by theaftermath of the 2015 refugee crisis and preceding the Swedish nationalelections in 2017. Our study seeks to understand the potential of combininghuman intelligence with AIs scalability and efficiency examining theadvantages and limitations of employing LLMs in qualitative research within thehumanities and social sciences. Additionally we discuss future directions forapplying LLMs in these fields.</p>
                <p>Last Updated: 2024-08-09 15:34:41 UTC</p>
                <button class="interpret-button" data-id="2408.05126v1">Interpret</button>
                <div id="interpretation-2408.05126v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Sportify: Question Answering with Embedded Visualizations and Personified Narratives for Sports Video</h3>
                <p>Authors: Chunggi LeeTica LinHanspeter PfisterChen Zhu-Tian</p>
                <p><a href="http://arxiv.org/abs/2408.05123v1">Link to paper</a></p>
                <p>As basketballs popularity surges fans often find themselves confused andoverwhelmed by the rapid game pace and complexity. Basketball tacticsinvolving a complex series of actions require substantial knowledge to befully understood. This complexity leads to a need for additional informationand explanation which can distract fans from the game. To tackle thesechallenges we present Sportify a Visual Question Answering system thatintegrates narratives and embedded visualization for demystifying basketballtactical questions aiding fans in understanding various game aspects. Wepropose three novel action visualizations i.e. Pass Cut and Screen todemonstrate critical action sequences. To explain the reasoning and logicbehind players actions we leverage a large-language model LLM to generatenarratives. We adopt a storytelling approach for complex scenarios from bothfirst and third-person perspectives integrating action visualizations. Weevaluated Sportify with basketball fans to investigate its impact onunderstanding of tactics and how different personal perspectives of narrativesimpact the understanding of complex tactic with action visualizations. Ourevaluation with basketball fans demonstrates Sportifys capability to deepentactical insights and amplify the viewing experience. Furthermore third-personnarration assists people in getting in-depth game explanations whilefirst-person narration enhances fans game engagement</p>
                <p>Last Updated: 2024-08-09 15:30:10 UTC</p>
                <button class="interpret-button" data-id="2408.05123v1">Interpret</button>
                <div id="interpretation-2408.05123v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Evaluating Layout Dimensionalities in PC+VR Asymmetric Collaborative Decision Making</h3>
                <p>Authors: Daniel EnriquezWai TongChris NorthHuamin QuYalong Yang</p>
                <p><a href="http://arxiv.org/abs/2408.05105v1">Link to paper</a></p>
                <p>With the commercialization of virtual/augmented reality VR/AR devicesthere is an increasing interest in combining immersive and non-immersivedevices e.g. desktop computers for asymmetric collaborations. While suchasymmetric settings have been examined in social platforms significantquestions around layout dimensionality in data-driven decision-making remainunderexplored. A crucial inquiry arises: although presenting a consistent 3Dvirtual world on both immersive and non-immersive platforms has been a commonpractice in social applications does the same guideline apply to lay out dataOr should data placement be optimized locally according to each devicesdisplay capacity This study aims to provide empirical insights into the userexperience of asymmetric collaboration in data-driven decision-making. Wetested practical dimensionality combinations between PC and VR resulting inthree conditions: PC2DVR2D PC2DVR3D and PC3DVR3D. The results revealed apreference for PC2DVR3D and PC2DVR2D led to the quickest task completion.Our investigation facilitates an in-depth discussion of the trade-offsassociated with different layout dimensionalities in asymmetric collaborations.</p>
                <p>Last Updated: 2024-08-09 14:55:59 UTC</p>
                <button class="interpret-button" data-id="2408.05105v1">Interpret</button>
                <div id="interpretation-2408.05105v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions</h3>
                <p>Authors: Michele MirandaElena Sofia RuzzettiAndrea SantilliFabio Massimo ZanzottoSébastien BratièresEmanuele Rodolà</p>
                <p><a href="http://arxiv.org/abs/2408.05212v1">Link to paper</a></p>
                <p>Large Language Models LLMs represent a significant advancement inartificial intelligence finding applications across various domains. Howevertheir reliance on massive internet-sourced datasets for training brings notableprivacy issues which are exacerbated in critical domains e.g. healthcare.Moreover certain application-specific scenarios may require fine-tuning thesemodels on private data. This survey critically examines the privacy threatsassociated with LLMs emphasizing the potential for these models to memorizeand inadvertently reveal sensitive information. We explore current threats byreviewing privacy attacks on LLMs and propose comprehensive solutions forintegrating privacy mechanisms throughout the entire learning pipeline. Thesesolutions range from anonymizing training datasets to implementing differentialprivacy during training or inference and machine unlearning after training. Ourcomprehensive review of existing literature highlights ongoing challengesavailable tools and future directions for preserving privacy in LLMs. Thiswork aims to guide the development of more secure and trustworthy AI systems byproviding a thorough understanding of privacy preservation methods and theireffectiveness in mitigating risks.</p>
                <p>Last Updated: 2024-08-10 05:41:19 UTC</p>
                <button class="interpret-button" data-id="2408.05212v1">Interpret</button>
                <div id="interpretation-2408.05212v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VITA: Towards Open-Source Interactive Omni Multimodal LLM</h3>
                <p>Authors: Chaoyou FuHaojia LinZuwei LongYunhang ShenMeng ZhaoYifan ZhangXiong WangDi YinLong MaXiawu ZhengRan HeRongrong JiYunsheng WuCaifeng ShanXing Sun</p>
                <p><a href="http://arxiv.org/abs/2408.05211v1">Link to paper</a></p>
                <p>The remarkable multimodal capabilities and interactive experience of GPT-4ounderscore their necessity in practical applications yet open-source modelsrarely excel in both areas. In this paper we introduce VITA the first-everopen-source Multimodal Large Language Model MLLM adept at simultaneousprocessing and analysis of Video Image Text and Audio modalities andmeanwhile has an advanced multimodal interactive experience. Starting fromMixtral 8x7B as a language foundation we expand its Chinese vocabularyfollowed by bilingual instruction tuning. We further endow the language modelwith visual and audio capabilities through two-stage multi-task learning ofmultimodal alignment and instruction tuning. VITA demonstrates robustfoundational capabilities of multilingual vision and audio understanding asevidenced by its strong performance across a range of both unimodal andmultimodal benchmarks. Beyond foundational capabilities we have madeconsiderable progress in enhancing the natural multimodal human-computerinteraction experience. To the best of our knowledge we are the first toexploit non-awakening interaction and audio interrupt in MLLM. VITA is thefirst step for the open-source community to explore the seamless integration ofmultimodal understanding and interaction. While there is still lots of work tobe done on VITA to get close to close-source counterparts we hope that itsrole as a pioneer can serve as a cornerstone for subsequent research. ProjectPage: https://vita-home.github.io.</p>
                <p>Last Updated: 2024-08-09 17:59:49 UTC</p>
                <button class="interpret-button" data-id="2408.05211v1">Interpret</button>
                <div id="interpretation-2408.05211v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Evaluating the capability of large language models to personalize science texts for diverse middle-school-age learners</h3>
                <p>Authors: Michael Vaccaro JrMikayla FridayArash Zaghi</p>
                <p><a href="http://arxiv.org/abs/2408.05204v1">Link to paper</a></p>
                <p>Large language models LLMs including OpenAIs GPT-series have madesignificant advancements in recent years. Known for their expertise acrossdiverse subject areas and quick adaptability to user-provided prompts LLMshold unique potential as Personalized Learning PL tools. Despite thispotential their application in K-12 education remains largely unexplored. Thispaper presents one of the first randomized controlled trials n  23 toevaluate the effectiveness of GPT-4 in personalizing educational science textsfor middle school students. In this study GPT-4 was used to profile studentlearning preferences based on choices made during a training session. For theexperimental group GPT-4 was used to rewrite science texts to align with thestudents predicted profile while for students in the control group textswere rewritten to contradict their learning preferences. The results of aMann-Whitney U test showed that students significantly preferred at the .10level the rewritten texts when they were aligned with their profile p .059. These findings suggest that GPT-4 can effectively interpret and tailoreducational content to diverse learner preferences marking a significantadvancement in PL technology. The limitations of this study and ethicalconsiderations for using artificial intelligence in education are alsodiscussed.</p>
                <p>Last Updated: 2024-08-09 17:53:35 UTC</p>
                <button class="interpret-button" data-id="2408.05204v1">Interpret</button>
                <div id="interpretation-2408.05204v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TaSL: Task Skill Localization and Consolidation for Language Model Continual Learning</h3>
                <p>Authors: Yujie FengXu ChuYongxin XuZexin LuBo LiuPhilip S. YuXiao-Ming Wu</p>
                <p><a href="http://arxiv.org/abs/2408.05200v1">Link to paper</a></p>
                <p>Language model continual learning CL has recently garnered significantinterest due to its potential to adapt large language models LLMs to dynamicreal-world environments without re-training. A key challenge in this field iscatastrophic forgetting where models lose previously acquired knowledge whenlearning new tasks. Existing methods commonly employ multipleparameter-efficient fine-tuning PEFT blocks to acquire task-specificknowledge for each task but these approaches lack efficiency and overlook thepotential for knowledge transfer through task interaction. In this paper wepresent a novel CL framework for language models called Task Skill Localizationand Consolidation TaSL which enhances knowledge transfer without relying onmemory replay. TaSL first divides the model into skill units based onparameter dependencies enabling more granular control. It then employs a novelgroup-wise skill localization technique to identify the importance distributionof skill units for a new task. By comparing this importance distribution withthose from previous tasks we implement a fine-grained skill consolidationstrategy that retains task-specific knowledge thereby preventing forgettingand updates task-shared knowledge which facilitates bi-directional knowledgetransfer. As a result TaSL achieves a superior balance between retainingprevious knowledge and excelling in new tasks. TaSL also shows stronggeneralizability suitable for general models and customizable for PEFT methodslike LoRA. Additionally it demonstrates notable extensibility allowingintegration with memory replay to further enhance performance. Extensiveexperiments on two CL benchmarks with varying model sizes from 220M to 7Bdemonstrate the effectiveness of TaSL and its variants across differentsettings.</p>
                <p>Last Updated: 2024-08-09 17:44:45 UTC</p>
                <button class="interpret-button" data-id="2408.05200v1">Interpret</button>
                <div id="interpretation-2408.05200v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Separating Style from Substance: Enhancing Cross-Genre Authorship Attribution through Data Selection and Presentation</h3>
                <p>Authors: Steven FinckeElizabeth Boschee</p>
                <p><a href="http://arxiv.org/abs/2408.05192v1">Link to paper</a></p>
                <p>The task of deciding whether two documents are written by the same author ischallenging for both machines and humans. This task is even more challengingwhen the two documents are written about different topics e.g. baseball vs.politics or in different genres e.g. a blog post vs. an academic article.For machines the problem is complicated by the relative lack of real-worldtraining examples that cross the topic boundary and the vanishing scarcity ofcross-genre data. We propose targeted methods for training data selection and anovel learning curriculum that are designed to discourage a models reliance ontopic information for authorship attribution and correspondingly force it toincorporate information more robustly indicative of style no matter the topic.These refinements yield a 62.7 relative improvement in average cross-genreauthorship attribution as well as 16.6 in the per-genre condition.</p>
                <p>Last Updated: 2024-08-09 17:31:37 UTC</p>
                <button class="interpret-button" data-id="2408.05192v1">Interpret</button>
                <div id="interpretation-2408.05192v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Concept learning of parameterized quantum models from limited measurements</h3>
                <p>Authors: Beng Yee GanPo-Wei HuangElies Gil-FusterPatrick Rebentrost</p>
                <p><a href="http://arxiv.org/abs/2408.05116v1">Link to paper</a></p>
                <p>Classical learning of the expectation values of observables for quantumstates is a natural variant of learning quantum states or channels. Whilelearning-theoretic frameworks establish the sample complexity and the number ofmeasurement shots per sample required for learning such statistical quantitiesthe interplay between these two variables has not been adequately quantifiedbefore. In this work we take the probabilistic nature of quantum measurementsinto account in classical modelling and discuss these quantities under a singleunified learning framework. We provide provable guarantees for learningparameterized quantum models that also quantify the asymmetrical effects andinterplay of the two variables on the performance of learning algorithms. Theseresults show that while increasing the sample size enhances the learningperformance of classical machines even with single-shot estimates theimprovements from increasing measurements become asymptotically trivial beyonda constant factor. We further apply our framework and theoretical guarantees tostudy the impact of measurement noise on the classical surrogation ofparameterized quantum circuit models. Our work provides new tools to analysethe operational influence of finite measurement noise in the classical learningof quantum systems.</p>
                <p>Last Updated: 2024-08-09 15:07:42 UTC</p>
                <button class="interpret-button" data-id="2408.05116v1">Interpret</button>
                <div id="interpretation-2408.05116v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On expected signatures and signature cumulants in semimartingale models</h3>
                <p>Authors: Peter K. FrizPaul P. HagerNikolas Tapia</p>
                <p><a href="http://arxiv.org/abs/2408.05085v1">Link to paper</a></p>
                <p>The concept of signatures and expected signatures is vital in data scienceespecially for sequential data analysis. The signature transform a Cartan typedevelopment translates paths into high-dimensional feature vectors capturingtheir intrinsic characteristics. Under natural conditions the expectation ofthe signature determines the law of the signature providing a statisticalsummary of the data distribution. This property facilitates robust modeling andinference in machine learning and stochastic processes. Building on previouswork by the present authors Unified signature cumulants and generalized Magnusexpansions FoM Sigma 22 we here revisit the actual computation of expectedsignatures in a general semimartingale setting. Several new formulae aregiven. A log-transform of expected signatures leads to log-signaturessignature cumulants offering a significant reduction in complexity.</p>
                <p>Last Updated: 2024-08-09 14:16:21 UTC</p>
                <button class="interpret-button" data-id="2408.05085v1">Interpret</button>
                <div id="interpretation-2408.05085v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Variational Bayesian Phylogenetic Inference with Semi-implicit Branch Length Distributions</h3>
                <p>Authors: Tianyu XieFrederick A. Matsen IVMarc A. SuchardCheng Zhang</p>
                <p><a href="http://arxiv.org/abs/2408.05058v1">Link to paper</a></p>
                <p>Reconstructing the evolutionary history relating a collection of molecularsequences is the main subject of modern Bayesian phylogenetic inference.However the commonly used Markov chain Monte Carlo methods can be inefficientdue to the complicated space of phylogenetic trees especially when the numberof sequences is large. An alternative approach is variational Bayesianphylogenetic inference VBPI which transforms the inference problem into anoptimization problem. While effective the default diagonal lognormalapproximation for the branch lengths of the tree used in VBPI is ofteninsufficient to capture the complexity of the exact posterior. In this work wepropose a more flexible family of branch length variational posteriors based onsemi-implicit hierarchical distributions using graph neural networks. We showthat this semi-implicit construction emits straightforward permutationequivariant distributions and therefore can handle the non-Euclidean branchlength space across different tree topologies with ease. To deal with theintractable marginal probability of semi-implicit variational distributions wedevelop several alternative lower bounds for stochastic optimization. Wedemonstrate the effectiveness of our proposed method over baseline methods onbenchmark data examples in terms of both marginal likelihood estimation andbranch length posterior approximation.</p>
                <p>Last Updated: 2024-08-09 13:29:08 UTC</p>
                <button class="interpret-button" data-id="2408.05058v1">Interpret</button>
                <div id="interpretation-2408.05058v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>BoFire: Bayesian Optimization Framework Intended for Real Experiments</h3>
                <p>Authors: Johannes P. DürholtThomas S. AscheJohanna KleinekorteGabriel Mancino-BallBenjamin SchillerSimon SungJulian KeuppAaron OsburgToby BoyneRuth MisenerRosona EldredWagner Steuer CostaChrysoula KappatouRobert M. LeeDominik LinznerDavid WalzNiklas WulkowBehrang Shafei</p>
                <p><a href="http://arxiv.org/abs/2408.05040v1">Link to paper</a></p>
                <p>Our open-source Python package BoFire combines Bayesian Optimization BOwith other design of experiments DoE strategies focusing on developing andoptimizing new chemistry. Previous BO implementations for example as theyexist in the literature or software require substantial adaptation foreffective real-world deployment in chemical industry. BoFire provides a richfeature-set with extensive configurability and realizes our vision offast-tracking research contributions into industrial use via maintainableopen-source software. Owing to quality-of-life features likeJSON-serializability of problem formulations BoFire enables seamlessintegration of BO into RESTful APIs a common architecture component for bothself-driving laboratories and human-in-the-loop setups. This paper discussesthe differences between BoFire and other BO implementations and outlines waysthat BO research needs to be adapted for real-world use in a chemistry setting.</p>
                <p>Last Updated: 2024-08-09 12:50:48 UTC</p>
                <button class="interpret-button" data-id="2408.05040v1">Interpret</button>
                <div id="interpretation-2408.05040v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction</h3>
                <p>Authors: Bhaskarjit SarmahBenika HallRohan RaoSunil PatelStefano PasqualiDhagash Mehta</p>
                <p><a href="http://arxiv.org/abs/2408.04948v1">Link to paper</a></p>
                <p>Extraction and interpretation of intricate information from unstructured textdata arising in financial applications such as earnings call transcriptspresent substantial challenges to large language models LLMs even using thecurrent best practices to use Retrieval Augmented Generation RAG referred toas VectorRAG techniques which utilize vector databases for informationretrieval due to challenges such as domain specific terminology and complexformats of the documents. We introduce a novel approach based on a combinationcalled HybridRAG of the Knowledge Graphs KGs based RAG techniques calledGraphRAG and VectorRAG techniques to enhance question-answer QA systems forinformation extraction from financial documents that is shown to be capable ofgenerating accurate and contextually relevant answers. Using experiments on aset of financial earning call transcripts documents which come in the form ofQA format and hence provide a natural set of pairs of ground-truth QAs weshow that HybridRAG which retrieves context from both vector database and KGoutperforms both traditional VectorRAG and GraphRAG individually when evaluatedat both the retrieval and generation stages in terms of retrieval accuracy andanswer generation. The proposed technique has applications beyond the financialdomain</p>
                <p>Last Updated: 2024-08-09 09:07:48 UTC</p>
                <button class="interpret-button" data-id="2408.04948v1">Interpret</button>
                <div id="interpretation-2408.04948v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions</h3>
                <p>Authors: Michele MirandaElena Sofia RuzzettiAndrea SantilliFabio Massimo ZanzottoSébastien BratièresEmanuele Rodolà</p>
                <p><a href="http://arxiv.org/abs/2408.05212v1">Link to paper</a></p>
                <p>Large Language Models LLMs represent a significant advancement inartificial intelligence finding applications across various domains. Howevertheir reliance on massive internet-sourced datasets for training brings notableprivacy issues which are exacerbated in critical domains e.g. healthcare.Moreover certain application-specific scenarios may require fine-tuning thesemodels on private data. This survey critically examines the privacy threatsassociated with LLMs emphasizing the potential for these models to memorizeand inadvertently reveal sensitive information. We explore current threats byreviewing privacy attacks on LLMs and propose comprehensive solutions forintegrating privacy mechanisms throughout the entire learning pipeline. Thesesolutions range from anonymizing training datasets to implementing differentialprivacy during training or inference and machine unlearning after training. Ourcomprehensive review of existing literature highlights ongoing challengesavailable tools and future directions for preserving privacy in LLMs. Thiswork aims to guide the development of more secure and trustworthy AI systems byproviding a thorough understanding of privacy preservation methods and theireffectiveness in mitigating risks.</p>
                <p>Last Updated: 2024-08-10 05:41:19 UTC</p>
                <button class="interpret-button" data-id="2408.05212v1">Interpret</button>
                <div id="interpretation-2408.05212v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VITA: Towards Open-Source Interactive Omni Multimodal LLM</h3>
                <p>Authors: Chaoyou FuHaojia LinZuwei LongYunhang ShenMeng ZhaoYifan ZhangXiong WangDi YinLong MaXiawu ZhengRan HeRongrong JiYunsheng WuCaifeng ShanXing Sun</p>
                <p><a href="http://arxiv.org/abs/2408.05211v1">Link to paper</a></p>
                <p>The remarkable multimodal capabilities and interactive experience of GPT-4ounderscore their necessity in practical applications yet open-source modelsrarely excel in both areas. In this paper we introduce VITA the first-everopen-source Multimodal Large Language Model MLLM adept at simultaneousprocessing and analysis of Video Image Text and Audio modalities andmeanwhile has an advanced multimodal interactive experience. Starting fromMixtral 8x7B as a language foundation we expand its Chinese vocabularyfollowed by bilingual instruction tuning. We further endow the language modelwith visual and audio capabilities through two-stage multi-task learning ofmultimodal alignment and instruction tuning. VITA demonstrates robustfoundational capabilities of multilingual vision and audio understanding asevidenced by its strong performance across a range of both unimodal andmultimodal benchmarks. Beyond foundational capabilities we have madeconsiderable progress in enhancing the natural multimodal human-computerinteraction experience. To the best of our knowledge we are the first toexploit non-awakening interaction and audio interrupt in MLLM. VITA is thefirst step for the open-source community to explore the seamless integration ofmultimodal understanding and interaction. While there is still lots of work tobe done on VITA to get close to close-source counterparts we hope that itsrole as a pioneer can serve as a cornerstone for subsequent research. ProjectPage: https://vita-home.github.io.</p>
                <p>Last Updated: 2024-08-09 17:59:49 UTC</p>
                <button class="interpret-button" data-id="2408.05211v1">Interpret</button>
                <div id="interpretation-2408.05211v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TaSL: Task Skill Localization and Consolidation for Language Model Continual Learning</h3>
                <p>Authors: Yujie FengXu ChuYongxin XuZexin LuBo LiuPhilip S. YuXiao-Ming Wu</p>
                <p><a href="http://arxiv.org/abs/2408.05200v1">Link to paper</a></p>
                <p>Language model continual learning CL has recently garnered significantinterest due to its potential to adapt large language models LLMs to dynamicreal-world environments without re-training. A key challenge in this field iscatastrophic forgetting where models lose previously acquired knowledge whenlearning new tasks. Existing methods commonly employ multipleparameter-efficient fine-tuning PEFT blocks to acquire task-specificknowledge for each task but these approaches lack efficiency and overlook thepotential for knowledge transfer through task interaction. In this paper wepresent a novel CL framework for language models called Task Skill Localizationand Consolidation TaSL which enhances knowledge transfer without relying onmemory replay. TaSL first divides the model into skill units based onparameter dependencies enabling more granular control. It then employs a novelgroup-wise skill localization technique to identify the importance distributionof skill units for a new task. By comparing this importance distribution withthose from previous tasks we implement a fine-grained skill consolidationstrategy that retains task-specific knowledge thereby preventing forgettingand updates task-shared knowledge which facilitates bi-directional knowledgetransfer. As a result TaSL achieves a superior balance between retainingprevious knowledge and excelling in new tasks. TaSL also shows stronggeneralizability suitable for general models and customizable for PEFT methodslike LoRA. Additionally it demonstrates notable extensibility allowingintegration with memory replay to further enhance performance. Extensiveexperiments on two CL benchmarks with varying model sizes from 220M to 7Bdemonstrate the effectiveness of TaSL and its variants across differentsettings.</p>
                <p>Last Updated: 2024-08-09 17:44:45 UTC</p>
                <button class="interpret-button" data-id="2408.05200v1">Interpret</button>
                <div id="interpretation-2408.05200v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>HistoKernel: Whole Slide Image Level Maximum Mean Discrepancy Kernels for Pan-Cancer Predictive Modelling</h3>
                <p>Authors: Piotr KellerMuhammad DawoodBrinder Singh ChohanFayyaz ul Amir Afsar Minhas</p>
                <p><a href="http://arxiv.org/abs/2408.05195v1">Link to paper</a></p>
                <p>Machine learning in computational pathology CPath often aggregatespatch-level predictions from multi-gigapixel Whole Slide Images WSIs togenerate WSI-level prediction scores for crucial tasks such as survivalprediction and drug effect prediction. However current methods do notexplicitly characterize distributional differences between patch sets withinWSIs. We introduce HistoKernel a novel Maximum Mean Discrepancy MMD kernelthat measures distributional similarity between WSIs for enhanced predictionperformance on downstream prediction tasks.  Our comprehensive analysis demonstrates HistoKernels effectiveness acrossvarious machine learning tasks including retrieval n  9362 drugsensitivity regression n  551 point mutation classification n  3419and survival analysis n  2291 outperforming existing deep learningmethods. Additionally HistoKernel seamlessly integrates multi-modal data andoffers a novel perturbation-based method for patch-level explainability. Thiswork pioneers the use of kernel-based methods for WSI-level predictivemodeling opening new avenues for research. Code is available athttps://github.com/pkeller00/HistoKernel.</p>
                <p>Last Updated: 2024-08-09 17:40:08 UTC</p>
                <button class="interpret-button" data-id="2408.05195v1">Interpret</button>
                <div id="interpretation-2408.05195v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Meta-Learning Guided Label Noise Distillation for Robust Signal Modulation Classification</h3>
                <p>Authors: Xiaoyang HaoZhixi FengTongqing PengShuyuan Yang</p>
                <p><a href="http://arxiv.org/abs/2408.05151v1">Link to paper</a></p>
                <p>Automatic modulation classification AMC is an effective way to deal withphysical layer threats of the internet of things IoT. However there is oftenlabel mislabeling in practice which significantly impacts the performance androbustness of deep neural networks DNNs. In this paper we propose ameta-learning guided label noise distillation method for robust AMC.Specifically a teacher-student heterogeneous network TSHN framework isproposed to distill and reuse label noise. Based on the idea that labels arerepresentations the teacher network with trusted meta-learning divides andconquers untrusted label samples and then guides the student network to learnbetter by reassessing and correcting labels. Furthermore we propose amulti-view signal MVS method to further improve the performance ofhard-to-classify categories with few-shot trusted label samples. Extensiveexperimental results show that our methods can significantly improve theperformance and robustness of signal AMC in various and complex label noisescenarios which is crucial for securing IoT applications.</p>
                <p>Last Updated: 2024-08-09 16:14:40 UTC</p>
                <button class="interpret-button" data-id="2408.05151v1">Interpret</button>
                <div id="interpretation-2408.05151v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-08-12</p>
        </div>
    
        </div>
    </body>
    </html>
    