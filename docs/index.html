
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model</h3>
                <p>Authors: Haoyun XuRunzhe ZhanDerek F. WongLidia S. Chao</p>
                <p><a href="http://arxiv.org/abs/2403.11621v1">Link to paper</a></p>
                <p>Large Language Models LLMs are composed of neurons that exhibit variousbehaviors and roles which become increasingly diversified as models scale.Recent studies have revealed that not all neurons are active across differentdatasets and this sparsity correlates positively with the task-specificability leading to advancements in model pruning and training efficiency.Traditional fine-tuning methods engage all parameters of LLMs which iscomputationally expensive and may not be necessary. In contrastParameter-Efficient Fine-Tuning PEFT approaches aim to minimize the number oftrainable parameters yet they still operate at a relatively macro scale e.g.layer-level. We introduce Neuron-Level Fine-Tuning NeFT a novel approachthat refines the granularity of parameter training down to the individualneuron enabling more precise and computationally efficient model updates. Theexperimental results show that NeFT not only exceeded the performance offull-parameter fine-tuning and PEFT but also provided insights into theanalysis of neurons.</p>
                <p>Last Updated: 2024-03-18 09:55:01 UTC</p>
                <button class="interpret-button" data-id="2403.11621v1">Interpret</button>
                <div id="interpretation-2403.11621v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines</h3>
                <p>Authors: Ekaterina TrofimovaEmil SataevAndrey E. Ustyuzhanin</p>
                <p><a href="http://arxiv.org/abs/2403.11585v1">Link to paper</a></p>
                <p>In the ever-evolving landscape of machine learning seamless translation ofnatural language descriptions into executable code remains a formidablechallenge. This paper introduces Linguacodus an innovative framework designedto tackle this challenge by deploying a dynamic pipeline that iterativelytransforms natural language task descriptions into code through high-leveldata-shaping instructions. The core of Linguacodus is a fine-tuned largelanguage model LLM empowered to evaluate diverse solutions for variousproblems and select the most fitting one for a given task. This paper detailsthe fine-tuning process and sheds light on how natural language descriptionscan be translated into functional code. Linguacodus represents a substantialleap towards automated code generation effectively bridging the gap betweentask descriptions and executable code. It holds great promise for advancingmachine learning applications across diverse domains. Additionally we proposean algorithm capable of transforming a natural description of an ML task intocode with minimal human interaction. In extensive experiments on a vast machinelearning code dataset originating from Kaggle we showcase the effectiveness ofLinguacodus. The investigations highlight its potential applications acrossdiverse domains emphasizing its impact on applied machine learning in variousscientific fields.</p>
                <p>Last Updated: 2024-03-18 08:58:47 UTC</p>
                <button class="interpret-button" data-id="2403.11585v1">Interpret</button>
                <div id="interpretation-2403.11585v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Reinforcement Learning with Token-level Feedback for Controllable Text Generation</h3>
                <p>Authors: Wendi LiWei WeiKaihe XuWenfeng XieDangyang ChenYu Cheng</p>
                <p><a href="http://arxiv.org/abs/2403.11558v1">Link to paper</a></p>
                <p>To meet the requirements of real-world applications it is essential tocontrol generations of large language models LLMs. Prior research has triedto introduce reinforcement learning RL into controllable text generationwhile most existing methods suffer from overfitting issues finetuning-basedmethods or semantic collapse post-processing methods. However current RLmethods are generally guided by coarse-grained sentence/paragraph-levelfeedback which may lead to suboptimal performance owing to semantic twists orprogressions within sentences. To tackle that we propose a novel reinforcementlearning algorithm named TOLE which formulates TOken-LEvel rewards forcontrollable text generation and employs a first-quantize-then-noiseparadigm to enhance the robustness of the RL algorithm.Furthermore TOLE can beflexibly extended to multiple constraints with little computational expense.Experimental results show that our algorithm can achieve superior performanceon both single-attribute and multi-attribute control tasks. We have releasedour codes at https://github.com/WindyLee0822/CTG</p>
                <p>Last Updated: 2024-03-18 08:18:37 UTC</p>
                <button class="interpret-button" data-id="2403.11558v1">Interpret</button>
                <div id="interpretation-2403.11558v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DEE: Dual-stage Explainable Evaluation Method for Text Generation</h3>
                <p>Authors: Shenyu ZhangYu LiRui WuXiutian HuangYongrui ChenWenhao XuGuilin Qi</p>
                <p><a href="http://arxiv.org/abs/2403.11509v1">Link to paper</a></p>
                <p>Automatic methods for evaluating machine-generated texts hold significantimportance due to the expanding applications of generative systems.Conventional methods tend to grapple with a lack of explainability issuing asolitary numerical score to signify the assessment outcome. Recent advancementshave sought to mitigate this limitation by incorporating large language modelsLLMs to offer more detailed error analyses yet their applicability remainsconstrained particularly in industrial contexts where comprehensive errorcoverage and swift detection are paramount. To alleviate these challenges weintroduce DEE a Dual-stage Explainable Evaluation method for estimating thequality of text generation. Built upon Llama 2 DEE follows a dual-stageprinciple guided by stage-specific instructions to perform efficientidentification of errors in generated texts in the initial stage andsubsequently delves into providing comprehensive diagnostic reports in thesecond stage. DEE is fine-tuned on our elaborately assembled dataset AntEvalwhich encompasses 15K examples from 4 real-world applications of Alipay thatemploy generative systems. The dataset concerns newly emerged issues likehallucination and toxicity thereby broadening the scope of DEEs evaluationcriteria. Experimental results affirm that DEEs superiority over existingevaluation methods achieving significant improvements in both humancorrelation as well as efficiency.</p>
                <p>Last Updated: 2024-03-18 06:30:41 UTC</p>
                <button class="interpret-button" data-id="2403.11509v1">Interpret</button>
                <div id="interpretation-2403.11509v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Word Order's Impacts: Insights from Reordering and Generation Analysis</h3>
                <p>Authors: Qinghua ZhaoJiaang LiLei LiZenghui ZhouJunfeng Liu</p>
                <p><a href="http://arxiv.org/abs/2403.11473v1">Link to paper</a></p>
                <p>Existing works have studied the impacts of the order of words within naturaltext. They usually analyze it by destroying the original order of words tocreate a scrambled sequence and then comparing the models performance betweenthe original and scrambled sequences. The experimental results demonstratemarginal drops. Considering this findings different hypothesis about wordorder is proposed including the order of words is redundant with lexicalsemantics and models do not rely on word order. In this paper werevisit the aforementioned hypotheses by adding a order reconstructionperspective and selecting datasets of different spectrum. Specifically wefirst select four different datasets and then design order reconstruction andcontinuing generation tasks. Empirical findings support that ChatGPT relies onword order to infer but cannot support or negate the redundancy relationsbetween word order lexical semantics.</p>
                <p>Last Updated: 2024-03-18 04:45:44 UTC</p>
                <button class="interpret-button" data-id="2403.11473v1">Interpret</button>
                <div id="interpretation-2403.11473v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Inter-individual and inter-site neural code conversion and image reconstruction without shared stimuli</h3>
                <p>Authors: Haibao WangJun Kai HoFan L. ChengShuntaro C. AokiYusuke MurakiMisato TanakaYukiyasu Kamitani</p>
                <p><a href="http://arxiv.org/abs/2403.11517v1">Link to paper</a></p>
                <p>The human brain demonstrates substantial inter-individual variability infine-grained functional topography posing challenges in identifying commonneural representations across individuals. Functional alignment has thepotential to harmonize these individual differences. However it typicallyrequires an identical set of stimuli presented to different individuals whichis often unavailable. To address this we propose a content loss-based neuralcode converter designed to convert brain activity from one subject to anotherrepresenting the same content. The converter is optimized so that the sourcesubjects converted brain activity is decoded into a latent imagerepresentation that closely resembles that of the stimulus given to the sourcesubject. We show that converters optimized using hierarchical imagerepresentations achieve conversion accuracy comparable to those optimized bypaired brain activity as in conventional methods. The brain activity convertedfrom a different individual and even from a different site sharing no stimuliproduced reconstructions that approached the quality of within-individualreconstructions. The converted brain activity had a generalizablerepresentation that can be read out by different decoding schemes. Theconverter required much fewer training samples than that typically required fordecoder training to produce recognizable reconstructions. These resultsdemonstrate that our method can effectively combine image representations toconvert brain activity across individuals without the need for shared stimuliproviding a promising tool for flexibly aligning data from complex cognitivetasks and a basis for brain-to-brain communication.</p>
                <p>Last Updated: 2024-03-18 07:10:52 UTC</p>
                <button class="interpret-button" data-id="2403.11517v1">Interpret</button>
                <div id="interpretation-2403.11517v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Browser Extension for in-place Signaling and Assessment of Misinformation</h3>
                <p>Authors: Farnaz JahanbakhshDavid R. Karger</p>
                <p><a href="http://dx.doi.org/10.1145/3613904.3642473">Link to paper</a></p>
                <p>The status-quo of misinformation moderation is a central authority usuallysocial platforms deciding what content constitutes misinformation and how itshould be handled. However to preserve users autonomy researchers haveexplored democratized misinformation moderation. One proposition is to enableusers to assess content accuracy and specify whose assessments they trust. Weexplore how these affordances can be provided on the web without cooperationfrom the platforms where users consume content. We present a browser extensionthat empowers users to assess the accuracy of any content on the web and showsthe user assessments from their trusted sources in-situ. Through a two-weekuser study we report on how users perceive such a tool the kind of contentusers want to assess and the rationales they use in their assessments. Weidentify implications for designing tools that enable users to moderate contentfor themselves with the help of those they trust.</p>
                <p>Last Updated: 2024-03-18 05:29:31 UTC</p>
                <button class="interpret-button" data-id="2403.11485v1">Interpret</button>
                <div id="interpretation-2403.11485v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Holistic HMI Design for Automated Vehicles: Bridging In-Vehicle and External Communication</h3>
                <p>Authors: Haoyu DongTram Thi Minh TranPavlo BazilinskyyMarius HoggenmüllerDebargha DeySilvia CazacuMervyn FranssenRuolin Gao</p>
                <p><a href="http://dx.doi.org/10.1145/3581961.3609837">Link to paper</a></p>
                <p>As the field of automated vehicles AVs advances it has become increasinglycritical to develop human-machine interfaces HMI for both internal andexternal communication. Critical dialogue is emerging around the potentialnecessity for a holistic approach to HMI designs which promotes theintegration of both in-vehicle user and external road user perspectives. Thisapproach aims to create a unified and coherent experience for differentstakeholders interacting with AVs. This workshop seeks to bring togetherdesigners engineers researchers and other stakeholders to delve intorelevant use cases exploring the potential advantages and challenges of thisapproach. The insights generated from this workshop aim to inform furtherdesign and research in the development of coherent HMIs for AVs ultimately formore seamless integration of AVs into existing traffic.</p>
                <p>Last Updated: 2024-03-18 00:23:24 UTC</p>
                <button class="interpret-button" data-id="2403.11386v1">Interpret</button>
                <div id="interpretation-2403.11386v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Systematic Review of XR-based Remote Human-Robot Interaction Systems</h3>
                <p>Authors: Xian WangLuyao ShenLik-Hang Lee</p>
                <p><a href="http://arxiv.org/abs/2403.11384v1">Link to paper</a></p>
                <p>This survey provides an exhaustive review of the applications of extendedreality XR technologies in the field of remote human-computer interactionHRI. We developed a systematic search strategy based on the PRISMAmethodology. From the initial 2561 articles selected 100 research papers thatmet our inclusion criteria were included. We categorized and summarized thedomain in detail delving into XR technologies including augmented realityAR virtual reality VR and mixed reality MR and their applications infacilitating intuitive and effective remote control and interaction withrobotic systems.The survey highlights existing articles on the application ofXR technologies user experience enhancement and various interaction designsfor XR in remote HRI providing insights into current trends and futuredirections. We also identified potential gaps and opportunities for futureresearch to improve remote HRI systems through XR technology to guide andinform future XR and robotics research.</p>
                <p>Last Updated: 2024-03-18 00:22:30 UTC</p>
                <button class="interpret-button" data-id="2403.11384v1">Interpret</button>
                <div id="interpretation-2403.11384v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Review of Virtual Reality Studies on Autonomous Vehicle--Pedestrian Interaction</h3>
                <p>Authors: Tram Thi Minh TranCallum ParkerMartin Tomitsch</p>
                <p><a href="http://dx.doi.org/10.1109/THMS.2021.3107517">Link to paper</a></p>
                <p>An increasing number of studies employ virtual reality VR to evaluateinteractions between autonomous vehicles AVs and pedestrians. VR simulatorsare valued for their cost-effectiveness flexibility in developing varioustraffic scenarios safe conduct of user studies and acceptable ecologicalvalidity. Reviewing the literature between 2010 and 2020 we found 31 empiricalstudies using VR as a testing apparatus for both implicit and explicitcommunication. By performing a systematic analysis we identified currentcoverage of critical use cases obtained a comprehensive account of factorsinfluencing pedestrian behavior in simulated traffic scenarios and assessedevaluation measures. Based on the findings we present a set of recommendationsfor implementing VR pedestrian simulators and propose directions for futureresearch.</p>
                <p>Last Updated: 2024-03-18 00:08:04 UTC</p>
                <button class="interpret-button" data-id="2403.11378v1">Interpret</button>
                <div id="interpretation-2403.11378v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Generalization error of spectral algorithms</h3>
                <p>Authors: Maksim VelikanovMaxim PanovDmitry Yarotsky</p>
                <p><a href="http://arxiv.org/abs/2403.11696v1">Link to paper</a></p>
                <p>The asymptotically precise estimation of the generalization of kernel methodshas recently received attention due to the parallels between neural networksand their associated kernels. However prior works derive such estimates fortraining by kernel ridge regression KRR whereas neural networks aretypically trained with gradient descent GD. In the present work we considerthe training of kernels with a family of textitspectral algorithmsspecified by profile hlambda and including KRR and GD as special cases.Then we derive the generalization error as a functional of learning profilehlambda for two data models: high-dimensional Gaussian and low-dimensionaltranslation-invariant model. Under power-law assumptions on the spectrum of thekernel and target we use our framework to i give full loss asymptotics forboth noisy and noiseless observations ii show that the loss localizes oncertain spectral scales giving a new perspective on the KRR saturationphenomenon iii conjecture and demonstrate for the considered data modelsthe universality of the loss w.r.t. non-spectral details of the problem butonly in case of noisy observation.</p>
                <p>Last Updated: 2024-03-18 11:52:33 UTC</p>
                <button class="interpret-button" data-id="2403.11696v1">Interpret</button>
                <div id="interpretation-2403.11696v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates</h3>
                <p>Authors: Riccardo GrazziMassimiliano PontilSaverio Salzo</p>
                <p><a href="http://arxiv.org/abs/2403.11687v1">Link to paper</a></p>
                <p>We study the problem of efficiently computing the derivative of thefixed-point of a parametric non-differentiable contraction map. This problemhas wide applications in machine learning including hyperparameteroptimization meta-learning and data poisoning attacks. We analyze two popularapproaches: iterative differentiation ITD and approximate implicitdifferentiation AID. A key challenge behind the nonsmooth setting is that thechain rule does not hold anymore. Building upon the recent work by Bolte et al.2022 who proved the linear convergence of non-differentiable ITD we providerefined linear convergence rates for both ITD and AID in the deterministiccase. We further introduce NSID a new method to compute the implicitderivative when the fixed point is defined as the composition of an outer mapand an inner map which is accessible only through a stochastic unbiasedestimator. We establish rates for the convergence of NSID to the truederivative encompassing the best available rates in the smooth setting. Wepresent illustrative experiments confirming our analysis.</p>
                <p>Last Updated: 2024-03-18 11:37:53 UTC</p>
                <button class="interpret-button" data-id="2403.11687v1">Interpret</button>
                <div id="interpretation-2403.11687v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Value of Reward Lookahead in Reinforcement Learning</h3>
                <p>Authors: Nadav MerlisDorian BaudryVianney Perchet</p>
                <p><a href="http://arxiv.org/abs/2403.11637v1">Link to paper</a></p>
                <p>In reinforcement learning RL agents sequentially interact with changingenvironments while aiming to maximize the obtained rewards. Usually rewardsare observed only after acting and so the goal is to maximize the expectedcumulative reward. Yet in many practical settings reward information isobserved in advance -- prices are observed before performing transactionsnearby traffic information is partially known and goals are oftentimes givento agents prior to the interaction. In this work we aim to quantifiablyanalyze the value of such future reward information through the lens ofcompetitive analysis. In particular we measure the ratio between the value ofstandard RL agents and that of agents with partial future-reward lookahead. Wecharacterize the worst-case reward distribution and derive exact ratios for theworst-case reward expectations. Surprisingly the resulting ratios relate toknown quantities in offline RL and reward-free exploration. We further providetight bounds for the ratio given the worst-case dynamics. Our results cover thefull spectrum between observing the immediate rewards before acting toobserving all the rewards before the interaction starts.</p>
                <p>Last Updated: 2024-03-18 10:19:52 UTC</p>
                <button class="interpret-button" data-id="2403.11637v1">Interpret</button>
                <div id="interpretation-2403.11637v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)</h3>
                <p>Authors: Paul NovelloJoseba DalmauLéo Andeol</p>
                <p><a href="http://arxiv.org/abs/2403.11532v1">Link to paper</a></p>
                <p>Research on Out-Of-Distribution OOD detection focuses mainly on buildingscores that efficiently distinguish OOD data from In Distribution ID data. Onthe other hand Conformal Prediction CP uses non-conformity scores toconstruct prediction sets with probabilistic coverage guarantees. In this workwe propose to use CP to better assess the efficiency of OOD scores.Specifically we emphasize that in standard OOD benchmark settings evaluationmetrics can be overly optimistic due to the finite sample size of the testdataset. Based on the work of Bates et al. 2022 we define new conformalAUROC and conformal FRPTPR95 metrics which are corrections that provideprobabilistic conservativeness guarantees on the variability of these metrics.We show the effect of these corrections on two reference OOD and anomalydetection benchmarks OpenOOD Yang et al. 2022 and ADBench Han et al.2022. We also show that the benefits of using OOD together with CP apply theother way around by using OOD scores as non-conformity scores which results inimproving upon current CP methods. One of the key messages of thesecontributions is that since OOD is concerned with designing scores and CP withinterpreting these scores the two fields may be inherently intertwined.</p>
                <p>Last Updated: 2024-03-18 07:35:25 UTC</p>
                <button class="interpret-button" data-id="2403.11532v1">Interpret</button>
                <div id="interpretation-2403.11532v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards</h3>
                <p>Authors: Yuto TanimotoKenji Fukumizu</p>
                <p><a href="http://arxiv.org/abs/2403.11520v1">Link to paper</a></p>
                <p>While many multi-armed bandit algorithms assume that rewards for all arms areconstant across rounds this assumption does not hold in many real-worldscenarios. This paper considers the setting of recovering bandits Pike-Burke Grunewalder 2019 where the reward depends on the number of rounds elapsedsince the last time an arm was pulled. We propose a new reinforcement learningRL algorithm tailored to this setting named the State-Separate SARSASS-SARSA algorithm which treats rounds as states. The SS-SARSA algorithmachieves efficient learning by reducing the number of state combinationsrequired for Q-learning/SARSA which often suffers from combinatorial issuesfor large-scale RL problems. Additionally it makes minimal assumptions aboutthe reward structure and offers lower computational complexity. Furthermore weprove asymptotic convergence to an optimal policy under mild assumptions.Simulation studies demonstrate the superior performance of our algorithm acrossvarious settings.</p>
                <p>Last Updated: 2024-03-18 07:14:21 UTC</p>
                <button class="interpret-button" data-id="2403.11520v1">Interpret</button>
                <div id="interpretation-2403.11520v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</h3>
                <p>Authors: Ruyi XuYuan YaoZonghao GuoJunbo CuiZanlin NiChunjiang GeTat-Seng ChuaZhiyuan LiuMaosong SunGao Huang</p>
                <p><a href="http://arxiv.org/abs/2403.11703v1">Link to paper</a></p>
                <p>Visual encoding constitutes the basis of large multimodal models LMMs inunderstanding the visual world. Conventional LMMs process images in fixed sizesand limited resolutions while recent explorations in this direction arelimited in adaptivity efficiency and even correctness. In this work we firsttake GPT-4V and LLaVA-1.5 as representative examples and expose systematicflaws rooted in their visual encoding strategy. To address the challenges wepresent LLaVA-UHD a large multimodal model that can efficiently perceiveimages in any aspect ratio and high resolution. LLaVA-UHD includes three keycomponents: 1 An image modularization strategy that divides native-resolutionimages into smaller variable-sized slices for efficient and extensibleencoding 2 a compression module that further condenses image tokens fromvisual encoders and 3 a spatial schema to organize slice tokens for LLMs.Comprehensive experiments show that LLaVA-UHD outperforms established LMMstrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably ourmodel built on LLaVA-1.5 336x336 supports 6 times larger i.e. 672x1088resolution images using only 94 inference computation and achieves 6.4accuracy improvement on TextVQA. Moreover the model can be efficiently trainedin academic settings within 23 hours on 8 A100 GPUs vs. 26 hours ofLLaVA-1.5. We make the data and code publicly available athttps://github.com/thunlp/LLaVA-UHD.</p>
                <p>Last Updated: 2024-03-18 12:04:11 UTC</p>
                <button class="interpret-button" data-id="2403.11703v1">Interpret</button>
                <div id="interpretation-2403.11703v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>HDLdebugger: Streamlining HDL debugging with Large Language Models</h3>
                <p>Authors: Xufeng YaoHaoyang LiTsz Ho ChanWenyi XiaoMingxuan YuanYu HuangLei ChenBei Yu</p>
                <p><a href="http://arxiv.org/abs/2403.11671v1">Link to paper</a></p>
                <p>In the domain of chip design Hardware Description Languages HDLs play apivotal role. However due to the complex syntax of HDLs and the limitedavailability of online resources debugging HDL codes remains a difficult andtime-intensive task even for seasoned engineers. Consequently there is apressing need to develop automated HDL code debugging models which canalleviate the burden on hardware engineers. Despite the strong capabilities ofLarge Language Models LLMs in generating completing and debugging softwarecode their utilization in the specialized field of HDL debugging has beenlimited and to date has not yielded satisfactory results. In this paper wepropose an LLM-assisted HDL debugging framework namely HDLdebugger whichconsists of HDL debugging data generation via a reverse engineering approach asearch engine for retrieval-augmented generation and a retrieval-augmented LLMfine-tuning approach. Through the integration of these components HDLdebuggercan automate and streamline HDL debugging for chip design. Our comprehensiveexperiments conducted on an HDL code dataset sourced from Huawei reveal thatHDLdebugger outperforms 13 cutting-edge LLM baselines displaying exceptionaleffectiveness in HDL code debugging.</p>
                <p>Last Updated: 2024-03-18 11:19:37 UTC</p>
                <button class="interpret-button" data-id="2403.11671v1">Interpret</button>
                <div id="interpretation-2403.11671v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring</h3>
                <p>Authors: Andrei BuligaChiara Di FrancescomarinoChiara GhidiniIvan DonadelloFabrizio Maria Maggi</p>
                <p><a href="http://arxiv.org/abs/2403.11642v1">Link to paper</a></p>
                <p>Counterfactual explanations suggest what should be different in the inputinstance to change the outcome of an AI system. When dealing withcounterfactual explanations in the field of Predictive Process Monitoringhowever control flow relationships among events have to be carefullyconsidered. A counterfactual indeed should not violate control flowrelationships among activities temporal background knowledege. Within thefield of Explainability in Predictive Process Monitoring there have been aseries of works regarding counterfactual explanations for outcome-basedpredictions. However none of them consider the inclusion of temporalbackground knowledge when generating these counterfactuals. In this work weadapt state-of-the-art techniques for counterfactual generation in the domainof XAI that are based on genetic algorithms to consider a series of temporalconstraints at runtime. We assume that this temporal background knowledge isgiven and we adapt the fitness function as well as the crossover and mutationoperators to maintain the satisfaction of the constraints. The proposedmethods are evaluated with respect to state-of-the-art genetic algorithms forcounterfactual generation and the results are presented. We showcase that theinclusion of temporal background knowledge allows the generation ofcounterfactuals more conformant to the temporal background knowledge withouthowever losing in terms of the counterfactual traditional quality metrics.</p>
                <p>Last Updated: 2024-03-18 10:34:40 UTC</p>
                <button class="interpret-button" data-id="2403.11642v1">Interpret</button>
                <div id="interpretation-2403.11642v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation</h3>
                <p>Authors: Zhizhen ZhouYejing HuoGuoheng HuangAn ZengXuhang ChenLian HuangZinuo Li</p>
                <p><a href="http://arxiv.org/abs/2403.11626v1">Link to paper</a></p>
                <p>The study of music-generated dance is a novel and challenging Imagegeneration task. It aims to input a piece of music and seed motions thengenerate natural dance movements for the subsequent music. Transformer-basedmethods face challenges in time series prediction tasks related to humanmovements and music due to their struggle in capturing the nonlinearrelationship and temporal aspects. This can lead to issues like jointdeformation role deviation floating and inconsistencies in dance movementsgenerated in response to the music. In this paper we propose aQuaternion-Enhanced Attention Network QEAN for visual dance synthesis from aquaternion perspective which consists of a Spin Position Embedding SPEmodule and a Quaternion Rotary Attention QRA module. First SPE embedsposition information into self-attention in a rotational manner leading tobetter learning of features of movement sequences and audio sequences andimproved understanding of the connection between music and dance. Second QRArepresents and fuses 3D motion features and audio features in the form of aseries of quaternions enabling the model to better learn the temporalcoordination of music and dance under the complex temporal cycle conditions ofdance generation. Finally we conducted experiments on the dataset AIST andthe results show that our approach achieves better and more robust performancein generating accurate high-quality dance movements. Our source code anddataset can be available from https://github.com/MarasyZZ/QEAN andhttps://google.github.io/aistplusplus_dataset respectively.</p>
                <p>Last Updated: 2024-03-18 09:58:43 UTC</p>
                <button class="interpret-button" data-id="2403.11626v1">Interpret</button>
                <div id="interpretation-2403.11626v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors with 100+ Qubits</h3>
                <p>Authors: Irfansha ShaikJaco van de Pol</p>
                <p><a href="http://arxiv.org/abs/2403.11598v1">Link to paper</a></p>
                <p>Layout synthesis is mapping a quantum circuit to a quantum processor. SWAPgate insertions are needed for scheduling 2-qubit gates only on connectedphysical qubits. With the ever-increasing number of qubits in NISQ processorsscalable layout synthesis is of utmost importance. With large optimality gapsobserved in heuristic approaches scalable exact methods are needed. Whilerecent exact and near-optimal approaches scale to moderate circuits large deepcircuits are still out of scope.  In this work we propose a SAT encoding based on parallel plans that apply 1SWAP and a group of CNOTs at each time step. Using domain-specific informationwe maintain optimality in parallel plans while scaling to large and deepcircuits. From our results we show the scalability of our approach whichsignificantly outperforms leading exact and near-optimal approaches up to100x. For the first time we can optimally map several 8 14 and 16 qubitcircuits onto 54 80 and 127 qubit platforms with up to 17 SWAPs. While addingoptimal SWAPs we also report near-optimal depth in our mapped circuits.</p>
                <p>Last Updated: 2024-03-18 09:19:01 UTC</p>
                <button class="interpret-button" data-id="2403.11598v1">Interpret</button>
                <div id="interpretation-2403.11598v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation</h3>
                <p>Authors: Johannes PöppelbaumAndreas Schwung</p>
                <p><a href="http://arxiv.org/abs/2403.11722v1">Link to paper</a></p>
                <p>We propose a novel quaternionic time-series compression methodology where wedivide a long time-series into segments of data extract the min max mean andstandard deviation of these chunks as representative features and encapsulatethem in a quaternion yielding a quaternion valued time-series. Thistime-series is processed using quaternion valued neural network layers wherewe aim to preserve the relation between these features through the usage of theHamilton product. To train this quaternion neural network we derive quaternionbackpropagation employing the GHR calculus which is required for a validproduct and chain rule in quaternion space. Furthermore we investigate theconnection between the derived update rules and automatic differentiation. Weapply our proposed compression method on the Tennessee Eastman Dataset wherewe perform fault classification using the compressed data in two settings: afully supervised one and in a semi supervised contrastive learning setting.Both times we were able to outperform real valued counterparts as well as twobaseline models: one with the uncompressed time-series as the input and theother with a regular downsampling using the mean. Further we could improve theclassification benchmark set by SimCLR-TS from 81.43 to 83.90.</p>
                <p>Last Updated: 2024-03-18 12:22:11 UTC</p>
                <button class="interpret-button" data-id="2403.11722v1">Interpret</button>
                <div id="interpretation-2403.11722v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models</h3>
                <p>Authors: Emilian PostolacheGiorgio MarianiLuca CosmoEmmanouil BenetosEmanuele Rodolà</p>
                <p><a href="http://arxiv.org/abs/2403.11706v1">Link to paper</a></p>
                <p>Multi-Source Diffusion Models MSDM allow for compositional musicalgeneration tasks: generating a set of coherent sources creatingaccompaniments and performing source separation. Despite their versatilitythey require estimating the joint distribution over the sources necessitatingpre-separated musical data which is rarely available and fixing the numberand type of sources at training time. This paper generalizes MSDM to arbitrarytime-domain diffusion models conditioned on text embeddings. These models donot require separated data as they are trained on mixtures can parameterize anarbitrary number of sources and allow for rich semantic control. We propose aninference procedure enabling the coherent generation of sources andaccompaniments. Additionally we adapt the Dirac separator of MSDM to performsource separation. We experiment with diffusion models trained on Slakh2100 andMTG-Jamendo showcasing competitive generation and separation results in arelaxed data setting.</p>
                <p>Last Updated: 2024-03-18 12:08:01 UTC</p>
                <button class="interpret-button" data-id="2403.11706v1">Interpret</button>
                <div id="interpretation-2403.11706v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach</h3>
                <p>Authors: Yunhao FanSheng ZhangGia-Wei Chern</p>
                <p><a href="http://arxiv.org/abs/2403.11705v1">Link to paper</a></p>
                <p>Frustrated itinerant magnets often exhibit complex noncollinear ornoncoplanar magnetic orders which support topological electronic structures. Acanonical example is the anomalous quantum Hall state with a chiral spin orderstabilized by electron-spin interactions on a triangular lattice. While along-range magnetic order cannot survive thermal fluctuations in twodimensions the chiral order which results from the breaking of a discreteIsing symmetry persists even at finite temperatures. We present a scalablemachine learning ML framework to model the complex electron-mediatedspin-spin interactions that stabilize the chiral magnetic domains in atriangular lattice. Large-scale dynamical simulations enabled by the MLforce-field models are performed to investigate the coarsening of chiraldomains after a thermal quench. While the chiral phase is described by a brokenZ_2 Ising-type symmetry we find that the characteristic size of chiraldomains increases linearly with time in stark contrast to the expectedAllen-Cahn domain growth law for a non-conserved Ising order parameter field.The linear growth of the chiral domains is attributed to the orientationalanisotropy of domain boundaries. Our work also demonstrates the promisingpotential of ML models for large-scale spin dynamics of itinerant magnets.</p>
                <p>Last Updated: 2024-03-18 12:07:46 UTC</p>
                <button class="interpret-button" data-id="2403.11705v1">Interpret</button>
                <div id="interpretation-2403.11705v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Generalization error of spectral algorithms</h3>
                <p>Authors: Maksim VelikanovMaxim PanovDmitry Yarotsky</p>
                <p><a href="http://arxiv.org/abs/2403.11696v1">Link to paper</a></p>
                <p>The asymptotically precise estimation of the generalization of kernel methodshas recently received attention due to the parallels between neural networksand their associated kernels. However prior works derive such estimates fortraining by kernel ridge regression KRR whereas neural networks aretypically trained with gradient descent GD. In the present work we considerthe training of kernels with a family of textitspectral algorithmsspecified by profile hlambda and including KRR and GD as special cases.Then we derive the generalization error as a functional of learning profilehlambda for two data models: high-dimensional Gaussian and low-dimensionaltranslation-invariant model. Under power-law assumptions on the spectrum of thekernel and target we use our framework to i give full loss asymptotics forboth noisy and noiseless observations ii show that the loss localizes oncertain spectral scales giving a new perspective on the KRR saturationphenomenon iii conjecture and demonstrate for the considered data modelsthe universality of the loss w.r.t. non-spectral details of the problem butonly in case of noisy observation.</p>
                <p>Last Updated: 2024-03-18 11:52:33 UTC</p>
                <button class="interpret-button" data-id="2403.11696v1">Interpret</button>
                <div id="interpretation-2403.11696v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates</h3>
                <p>Authors: Riccardo GrazziMassimiliano PontilSaverio Salzo</p>
                <p><a href="http://arxiv.org/abs/2403.11687v1">Link to paper</a></p>
                <p>We study the problem of efficiently computing the derivative of thefixed-point of a parametric non-differentiable contraction map. This problemhas wide applications in machine learning including hyperparameteroptimization meta-learning and data poisoning attacks. We analyze two popularapproaches: iterative differentiation ITD and approximate implicitdifferentiation AID. A key challenge behind the nonsmooth setting is that thechain rule does not hold anymore. Building upon the recent work by Bolte et al.2022 who proved the linear convergence of non-differentiable ITD we providerefined linear convergence rates for both ITD and AID in the deterministiccase. We further introduce NSID a new method to compute the implicitderivative when the fixed point is defined as the composition of an outer mapand an inner map which is accessible only through a stochastic unbiasedestimator. We establish rates for the convergence of NSID to the truederivative encompassing the best available rates in the smooth setting. Wepresent illustrative experiments confirming our analysis.</p>
                <p>Last Updated: 2024-03-18 11:37:53 UTC</p>
                <button class="interpret-button" data-id="2403.11687v1">Interpret</button>
                <div id="interpretation-2403.11687v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification</h3>
                <p>Authors: Kaijie RenLei Zhang</p>
                <p><a href="http://arxiv.org/abs/2403.11708v1">Link to paper</a></p>
                <p>Visible-Infrared Person Re-identification VI-ReID is a challengingcross-modal pedestrian retrieval task due to significant intra-classvariations and cross-modal discrepancies among different cameras. Existingworks mainly focus on embedding images of different modalities into a unifiedspace to mine modality-shared features. They only seek distinctive informationwithin these shared features while ignoring the identity-aware usefulinformation that is implicit in the modality-specific features. To address thisissue we propose a novel Implicit Discriminative Knowledge Learning IDKLnetwork to uncover and leverage the implicit discriminative informationcontained within the modality-specific. First we extract modality-specific andmodality-shared features using a novel dual-stream network. Then themodality-specific features undergo purification to reduce their modality stylediscrepancies while preserving identity-aware discriminative knowledge.Subsequently this kind of implicit knowledge is distilled into themodality-shared feature to enhance its distinctiveness. Finally an alignmentloss is proposed to minimize modality discrepancy on enhanced modality-sharedfeatures. Extensive experiments on multiple public datasets demonstrate thesuperiority of IDKL network over the state-of-the-art methods. Code isavailable at https://github.com/1KK077/IDKL.</p>
                <p>Last Updated: 2024-03-18 12:12:45 UTC</p>
                <button class="interpret-button" data-id="2403.11708v1">Interpret</button>
                <div id="interpretation-2403.11708v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</h3>
                <p>Authors: Ruyi XuYuan YaoZonghao GuoJunbo CuiZanlin NiChunjiang GeTat-Seng ChuaZhiyuan LiuMaosong SunGao Huang</p>
                <p><a href="http://arxiv.org/abs/2403.11703v1">Link to paper</a></p>
                <p>Visual encoding constitutes the basis of large multimodal models LMMs inunderstanding the visual world. Conventional LMMs process images in fixed sizesand limited resolutions while recent explorations in this direction arelimited in adaptivity efficiency and even correctness. In this work we firsttake GPT-4V and LLaVA-1.5 as representative examples and expose systematicflaws rooted in their visual encoding strategy. To address the challenges wepresent LLaVA-UHD a large multimodal model that can efficiently perceiveimages in any aspect ratio and high resolution. LLaVA-UHD includes three keycomponents: 1 An image modularization strategy that divides native-resolutionimages into smaller variable-sized slices for efficient and extensibleencoding 2 a compression module that further condenses image tokens fromvisual encoders and 3 a spatial schema to organize slice tokens for LLMs.Comprehensive experiments show that LLaVA-UHD outperforms established LMMstrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably ourmodel built on LLaVA-1.5 336x336 supports 6 times larger i.e. 672x1088resolution images using only 94 inference computation and achieves 6.4accuracy improvement on TextVQA. Moreover the model can be efficiently trainedin academic settings within 23 hours on 8 A100 GPUs vs. 26 hours ofLLaVA-1.5. We make the data and code publicly available athttps://github.com/thunlp/LLaVA-UHD.</p>
                <p>Last Updated: 2024-03-18 12:04:11 UTC</p>
                <button class="interpret-button" data-id="2403.11703v1">Interpret</button>
                <div id="interpretation-2403.11703v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Spatial-Temporal Progressive Fusion Network for Breast Lesion Segmentation in Ultrasound Videos</h3>
                <p>Authors: Zhengzheng TuZigang ZhuYayang DuanBo JiangQishun WangChaoxue Zhang</p>
                <p><a href="http://arxiv.org/abs/2403.11699v1">Link to paper</a></p>
                <p>Ultrasound video-based breast lesion segmentation provides a valuableassistance in early breast lesion detection and treatment. However existingworks mainly focus on lesion segmentation based on ultrasound breast imageswhich usually can not be adapted well to obtain desirable results on ultrasoundvideos. The main challenge for ultrasound video-based breast lesionsegmentation is how to exploit the lesion cues of both intra-frame andinter-frame simultaneously. To address this problem we propose a novelSpatial-Temporal Progressive Fusion Network STPFNet for video based breastlesion segmentation problem. The main aspects of the proposed STPFNet arethreefold. First we propose to adopt a unified network architecture to captureboth spatial dependences within each ultrasound frame and temporal correlationsbetween different frames together for ultrasound data representation. Secondwe propose a new fusion module termed Multi-Scale Feature Fusion MSFF tofuse spatial and temporal cues together for lesion detection. MSFF can help todetermine the boundary contour of lesion region to overcome the issue of lesionboundary blurring. Third we propose to exploit the segmentation result ofprevious frame as the prior knowledge to suppress the noisy background andlearn more robust representation. In particular we introduce a new publiclyavailable ultrasound video breast lesion segmentation dataset termed UVBLS200which is specifically dedicated to breast lesion segmentation. It contains 200videos including 80 videos of benign lesions and 120 videos of malignantlesions. Experiments on the proposed dataset demonstrate that the proposedSTPFNet achieves better breast lesion detection performance thanstate-of-the-art methods.</p>
                <p>Last Updated: 2024-03-18 11:56:32 UTC</p>
                <button class="interpret-button" data-id="2403.11699v1">Interpret</button>
                <div id="interpretation-2403.11699v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Urban Scene Diffusion through Semantic Occupancy Map</h3>
                <p>Authors: Junge ZhangQihang ZhangLi ZhangRamana Rao KompellaGaowen LiuBolei Zhou</p>
                <p><a href="http://arxiv.org/abs/2403.11697v1">Link to paper</a></p>
                <p>Generating unbounded 3D scenes is crucial for large-scale scene understandingand simulation. Urban scenes unlike natural landscapes consist of variouscomplex man-made objects and structures such as roads traffic signs vehiclesand buildings. To create a realistic and detailed urban scene it is crucial toaccurately represent the geometry and semantics of the underlying objectsgoing beyond their visual appearance. In this work we propose UrbanDiffusiona 3D diffusion model that is conditioned on a Birds-Eye View BEV map andgenerates an urban scene with geometry and semantics in the form of semanticoccupancy map. Our model introduces a novel paradigm that learns the datadistribution of scene-level structures within a latent space and furtherenables the expansion of the synthesized scene into an arbitrary scale. Aftertraining on real-world driving datasets our model can generate a wide range ofdiverse urban scenes given the BEV maps from the held-out set and alsogeneralize to the synthesized maps from a driving simulator. We furtherdemonstrate its application to scene image synthesis with a pretrained imagegenerator as a prior.</p>
                <p>Last Updated: 2024-03-18 11:54:35 UTC</p>
                <button class="interpret-button" data-id="2403.11697v1">Interpret</button>
                <div id="interpretation-2403.11697v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction</h3>
                <p>Authors: Ali Asghar SharifiAli ZoljodiMasoud Daneshtalab</p>
                <p><a href="http://arxiv.org/abs/2403.11695v1">Link to paper</a></p>
                <p>Autonomous driving systems are a rapidly evolving technology that enablesdriverless car production. Trajectory prediction is a critical component ofautonomous driving systems enabling cars to anticipate the movements ofsurrounding objects for safe navigation. Trajectory prediction using Lidarpoint-cloud data performs better than 2D images due to providing 3Dinformation. However processing point-cloud data is more complicated andtime-consuming than 2D images. Hence state-of-the-art 3D trajectorypredictions using point-cloud data suffer from slow and erroneous predictions.This paper introduces TrajectoryNAS a pioneering method that focuses onutilizing point cloud data for trajectory prediction. By leveraging NeuralArchitecture Search NAS TrajectoryNAS automates the design of trajectoryprediction models encompassing object detection tracking and forecasting ina cohesive manner. This approach not only addresses the complexinterdependencies among these tasks but also emphasizes the importance ofaccuracy and efficiency in trajectory modeling. Through empirical studiesTrajectoryNAS demonstrates its effectiveness in enhancing the performance ofautonomous driving systems marking a significant advancement in thefield.Experimental results reveal that TrajcetoryNAS yield a minimum of 4.8higger accuracy and 1.1 lower latency over competing methods on the NuScenesdataset.</p>
                <p>Last Updated: 2024-03-18 11:48:41 UTC</p>
                <button class="interpret-button" data-id="2403.11695v1">Interpret</button>
                <div id="interpretation-2403.11695v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective</h3>
                <p>Authors: Muhammad Aneeq uz ZamanAlec KoppelMathieu LaurièreTamer Başar</p>
                <p><a href="http://arxiv.org/abs/2403.11345v1">Link to paper</a></p>
                <p>We address in this paper Reinforcement Learning RL among agents that aregrouped into teams such that there is cooperation within each team butgeneral-sum non-zero sum competition across different teams. To develop an RLmethod that provably achieves a Nash equilibrium we focus on alinear-quadratic structure. Moreover to tackle the non-stationarity induced bymulti-agent interactions in the finite population setting we consider the casewhere the number of agents within each team is infinite i.e. the mean-fieldsetting. This results in a General-Sum LQ Mean-Field Type Game GS-MFTGs. Wecharacterize the Nash equilibrium NE of the GS-MFTG under a standardinvertibility condition. This MFTG NE is then shown to be mathcalO1/M-NEfor the finite population game where M is a lower bound on the number ofagents in each team. These structural results motivate an algorithm calledMulti-player Receding-horizon Natural Policy Gradient MRPG where each teamminimizes its cumulative cost independently in a receding-horizon manner.Despite the non-convexity of the problem we establish that the resultingalgorithm converges to a global NE through a novel problem decomposition intosub-problems using backward recursive discrete-time Hamilton-Jacobi-IsaacsHJI equations in which independent natural policy gradient is shown toexhibit linear convergence under time-independent diagonal dominance.Experiments illuminate the merits of this approach in practice.</p>
                <p>Last Updated: 2024-03-17 21:11:55 UTC</p>
                <button class="interpret-button" data-id="2403.11345v1">Interpret</button>
                <div id="interpretation-2403.11345v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving</h3>
                <p>Authors: Steffen HagedornMarcel MilichAlexandru P. Condurache</p>
                <p><a href="http://arxiv.org/abs/2403.11304v1">Link to paper</a></p>
                <p>Planning the trajectory of the controlled ego vehicle is a key challenge inautomated driving. As for human drivers predicting the motions of surroundingvehicles is important to plan the own actions. Recent motion prediction methodsutilize equivariant neural networks to exploit geometric symmetries in thescene. However no existing method combines motion prediction and trajectoryplanning in a joint step while guaranteeing equivariance underroto-translations of the input space. We address this gap by proposing alightweight equivariant planning model that generates multi-modal jointpredictions for all vehicles and selects one mode as the ego plan. Theequivariant network design improves sample efficiency guarantees outputstability and reduces model parameters. We further propose equivariant routeattraction to guide the ego vehicle along a high-level route provided by anoff-the-shelf GPS navigation system. This module creates a momentum fromembedded vehicle positions toward the route in latent space while keeping theequivariance property. Route attraction enables goal-oriented behavior withoutforcing the vehicle to stick to the exact route. We conduct experiments on thechallenging nuScenes dataset to investigate the capability of our planner. Theresults show that the planned trajectory is stable under roto-translations ofthe input scene which demonstrates the equivariance of our model. Despite usingonly a small split of the dataset for training our method improves L2 distanceat 3 s by 20.6  and surpasses the state of the art.</p>
                <p>Last Updated: 2024-03-17 18:53:46 UTC</p>
                <button class="interpret-button" data-id="2403.11304v1">Interpret</button>
                <div id="interpretation-2403.11304v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment</h3>
                <p>Authors: Lance YingKunal JhaShivam AaryaJoshua B. TenenbaumAntonio TorralbaTianmin Shu</p>
                <p><a href="http://arxiv.org/abs/2403.11075v1">Link to paper</a></p>
                <p>Verbal communication plays a crucial role in human cooperation particularlywhen the partners only have incomplete information about the task environmentand each others mental state. In this paper we propose a novel cooperativecommunication framework Goal-Oriented Mental Alignment GOMA. GOMA formulatesverbal communication as a planning problem that minimizes the misalignmentbetween the parts of agents mental states that are relevant to the goals. Thisapproach enables an embodied assistant to reason about when and how toproactively initialize communication with humans verbally using naturallanguage to help achieve better cooperation. We evaluate our approach againststrong baselines in two challenging environments Overcooked a multiplayergame and VirtualHome a household simulator. Our experimental resultsdemonstrate that large language models struggle with generating meaningfulcommunication that is grounded in the social and physical context. In contrastour approach can successfully generate concise verbal communication for theembodied assistant to effectively boost the performance of the cooperation aswell as human users perception of the assistant.</p>
                <p>Last Updated: 2024-03-17 03:52:52 UTC</p>
                <button class="interpret-button" data-id="2403.11075v1">Interpret</button>
                <div id="interpretation-2403.11075v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Resilient Fleet Management for Energy-Aware Intra-Factory Logistics</h3>
                <p>Authors: Mithun GouthamStephanie Stockar</p>
                <p><a href="http://arxiv.org/abs/2403.11034v1">Link to paper</a></p>
                <p>This paper presents a novel fleet management strategy for battery-poweredrobot fleets tasked with intra-factory logistics in an autonomous manufacturingfacility. In this environment repetitive material handling operations aresubject to real-world uncertainties such as blocked passages and equipment orrobot malfunctions. In such cases centralized approaches enhance resilience byimmediately adjusting the task allocation between the robots. To overcome thecomputational expense a two-step methodology is proposed where the nominalproblem is solved a priori using a Monte Carlo Tree Search algorithm for taskallocation resulting in a nominal search tree. When a disruption occurs thenominal search tree is rapidly updated a posteriori with costs to the newproblem while simultaneously generating feasible solutions. Computationalexperiments prove the real-time capability of the proposed algorithm forvarious scenarios and compare it with the case where the search tree is notused and the decentralized approach that does not attempt task reassignment.</p>
                <p>Last Updated: 2024-03-16 22:46:12 UTC</p>
                <button class="interpret-button" data-id="2403.11034v1">Interpret</button>
                <div id="interpretation-2403.11034v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems</h3>
                <p>Authors: Chinmay Vilas SamakTanmay Vilas SamakVenkat Krovi</p>
                <p><a href="http://arxiv.org/abs/2403.10996v1">Link to paper</a></p>
                <p>This work presents a sustainable multi-agent deep reinforcement learningframework capable of selectively scaling parallelized training workloadson-demand and transferring the trained policies from simulation to realityusing minimal hardware resources. We introduce AutoDRIVE Ecosystem as anenabling digital twin framework to train deploy and transfer cooperative aswell as competitive multi-agent reinforcement learning policies from simulationto reality. Particularly we first investigate an intersection traversalproblem of 4 cooperative vehicles Nigel that share limited state informationin single as well as multi-agent learning settings using a common policyapproach. We then investigate an adversarial autonomous racing problem of 2vehicles F1TENTH using an individual policy approach. In either set ofexperiments a decentralized learning architecture was adopted which allowedrobust training and testing of the policies in stochastic environments. Theagents were provided with realistically sparse observation spaces and wererestricted to sample control actions that implicitly satisfied the imposedkinodynamic and safety constraints. The experimental results for both problemstatements are reported in terms of quantitative metrics and qualitativeremarks for training as well as deployment phases. We also discuss agent andenvironment parallelization techniques adopted to efficiently accelerate MARLtraining while analyzing their computational performance. Finally wedemonstrate a resource-aware transition of the trained policies from simulationto reality using the proposed digital twin framework.</p>
                <p>Last Updated: 2024-03-16 18:47:04 UTC</p>
                <button class="interpret-button" data-id="2403.10996v1">Interpret</button>
                <div id="interpretation-2403.10996v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-03-19</p>
        </div>
    
        </div>
    </body>
    </html>
    