
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>"It Might be Technically Impressive, But It's Practically Useless to Us": Practices, Challenges, and Opportunities for Cross-Functional Collaboration around AI within the News Industry</h3>
                <p>Authors: Qing XiaoXianzhe FanFelix M. SimonBingbing ZhangMotahhare Eslami</p>
                <p><a href="http://arxiv.org/abs/2409.12000v1">Link to paper</a></p>
                <p>Recently an increasing number of news organizations have integratedartificial intelligence AI into their workflows leading to a further influxof AI technologists and data workers into the news industry. This has initiatedcross-functional collaborations between these professionals and journalists.While prior research has explored the impact of AI-related roles entering thenews industry there is a lack of studies on how cross-functional collaborationunfolds between AI professionals and journalists. Through interviews with 17journalists 6 AI technologists and 3 AI workers with cross-functionalexperience from leading news organizations we investigate the currentpractices challenges and opportunities for cross-functional collaborationaround AI in todays news industry. We first study how journalists and AIprofessionals perceive existing cross-collaboration strategies. We furtherexplore the challenges of cross-functional collaboration and providerecommendations for enhancing future cross-functional collaboration around AIin the news industry.</p>
                <p>Last Updated: 2024-09-18 14:12:01 UTC</p>
                <button class="interpret-button" data-id="2409.12000v1">Interpret</button>
                <div id="interpretation-2409.12000v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Equimetrics -- Applying HAR principles to equestrian activities</h3>
                <p>Authors: Jonas PöhlerKristof Van Laerhoven</p>
                <p><a href="http://arxiv.org/abs/2409.11989v1">Link to paper</a></p>
                <p>This paper presents the Equimetrics data capture system. The primaryobjective is to apply HAR principles to enhance the understanding andoptimization of equestrian performance. By integrating data from strategicallyplaced sensors on the riders body and the horses limbs the system provides acomprehensive view of their interactions. Preliminary data collection hasdemonstrated the systems ability to accurately classify various equestrianactivities such as walking trotting cantering and jumping while alsodetecting subtle changes in rider posture and horse movement. The systemleverages open-source hardware and software to offer a cost-effectivealternative to traditional motion capture technologies making it accessiblefor researchers and trainers. The Equimetrics system represents a significantadvancement in equestrian performance analysis providing objectivedata-driven insights that can be used to enhance training and competitionoutcomes.</p>
                <p>Last Updated: 2024-09-18 13:55:54 UTC</p>
                <button class="interpret-button" data-id="2409.11989v1">Interpret</button>
                <div id="interpretation-2409.11989v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AI paintings vs. Human Paintings? Deciphering Public Interactions and Perceptions towards AI-Generated Paintings on TikTok</h3>
                <p>Authors: Jiajun WangXiangzhe YuanSiying HuZhicong Lu</p>
                <p><a href="http://arxiv.org/abs/2409.11911v1">Link to paper</a></p>
                <p>With the development of generative AI technology a vast array ofAI-generated paintings AIGP have gone viral on social media like TikTok.However some negative news about AIGP has also emerged. For example in 2022numerous painters worldwide organized a large-scale anti-AI movement because ofthe infringement in generative AI model training. This event reflected a socialissue that with the development and application of generative AI publicfeedback and feelings towards it may have been overlooked. Therefore toinvestigate public interactions and perceptions towards AIGP on social mediawe analyzed user engagement level and comment sentiment scores of AIGP usinghuman painting videos as a baseline. In analyzing user engagement we alsoconsidered the possible moderating effect of the aesthetic quality ofPaintings. Utilizing topic modeling we identified seven reasons includinglooks too real looks too scary ambivalence etc. leading to negative publicperceptions of AIGP. Our work may provide instructive suggestions for futuregenerative AI technology development and avoid potential crises in human-AIcollaboration.</p>
                <p>Last Updated: 2024-09-18 12:13:09 UTC</p>
                <button class="interpret-button" data-id="2409.11911v1">Interpret</button>
                <div id="interpretation-2409.11911v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for Large-Scale Product Retrieval Evaluation</h3>
                <p>Authors: Kasra HosseiniThomas KoberJosip KrapacRoland VollgrafWeiwei ChengAna Peleteiro Ramallo</p>
                <p><a href="http://arxiv.org/abs/2409.11860v1">Link to paper</a></p>
                <p>Evaluating production-level retrieval systems at scale is a crucial yetchallenging task due to the limited availability of a large pool ofwell-trained human annotators. Large Language Models LLMs have the potentialto address this scaling issue and offer a viable alternative to humans for thebulk of annotation tasks. In this paper we propose a framework for assessingthe product search engines in a large-scale e-commerce setting leveragingMultimodal LLMs for i generating tailored annotation guidelines forindividual queries and ii conducting the subsequent annotation task. Ourmethod validated through deployment on a large e-commerce platformdemonstrates comparable quality to human annotations significantly reducestime and cost facilitates rapid problem discovery and provides an effectivesolution for production-level quality control at scale.</p>
                <p>Last Updated: 2024-09-18 10:30:50 UTC</p>
                <button class="interpret-button" data-id="2409.11860v1">Interpret</button>
                <div id="interpretation-2409.11860v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>My Views Do Not Reflect Those of My Employer: Differences in Behavior of Organizations' Official and Personal Social Media Accounts</h3>
                <p>Authors: Esa PalosaariTed Hsuan Yun ChenArttu MalkamäkiMikko Kivelä</p>
                <p><a href="http://arxiv.org/abs/2409.11759v1">Link to paper</a></p>
                <p>On social media the boundaries between peoples private and public livesoften blur. The need to navigate both roles which are governed by distinctnorms impacts how individuals conduct themselves online and presentsmethodological challenges for researchers. We conduct a systematic explorationon how an organizations official Twitter accounts and its members personalaccounts differ. Using a climate change Twitter data set as our case we findsubstantial differences in activity and connectivity across the organizationallevels we examined. The levels differed considerably in their overall retweetnetwork structures and accounts within each level were more likely to havesimilar connections than accounts at different levels. We illustrate theimplications of these differences for applied research by showing that thelevels closer to the core of the organization display more sectoral homophilybut less triadic closure and how each level consists of very different groupstructures. Our results show that the common practice of solely analyzingaccounts from a single organizational level grouping together all levels orexcluding certain levels can lead to a skewed understanding of howorganizations are represented on social media.</p>
                <p>Last Updated: 2024-09-18 07:31:47 UTC</p>
                <button class="interpret-button" data-id="2409.11759v1">Interpret</button>
                <div id="interpretation-2409.11759v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</h3>
                <p>Authors: Zichen Jeff CuiHengkai PanAadhithya IyerSiddhant HaldarLerrel Pinto</p>
                <p><a href="http://arxiv.org/abs/2409.12192v1">Link to paper</a></p>
                <p>Imitation learning has proven to be a powerful tool for training complexvisuomotor policies. However current methods often require hundreds tothousands of expert demonstrations to handle high-dimensional visualobservations. A key reason for this poor data efficiency is that visualrepresentations are predominantly either pretrained on out-of-domain data ortrained directly through a behavior cloning objective. In this work we presentDynaMo a new in-domain self-supervised method for learning visualrepresentations. Given a set of expert demonstrations we jointly learn alatent inverse dynamics model and a forward dynamics model over a sequence ofimage embeddings predicting the next frame in latent space withoutaugmentations contrastive sampling or access to ground truth actions.Importantly DynaMo does not require any out-of-domain data such as Internetdatasets or cross-embodied datasets. On a suite of six simulated and realenvironments we show that representations learned with DynaMo significantlyimprove downstream imitation learning performance over prior self-supervisedlearning objectives and pretrained representations. Gains from using DynaMohold across policy classes such as Behavior Transformer Diffusion Policy MLPand nearest neighbors. Finally we ablate over key components of DynaMo andmeasure its impact on downstream policy performance. Robot videos are bestviewed at https://dynamo-ssl.github.io</p>
                <p>Last Updated: 2024-09-18 17:59:43 UTC</p>
                <button class="interpret-button" data-id="2409.12192v1">Interpret</button>
                <div id="interpretation-2409.12192v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Massively Multi-Person 3D Human Motion Forecasting with Scene Context</h3>
                <p>Authors: Felix B MuellerJulian TankeJuergen Gall</p>
                <p><a href="http://arxiv.org/abs/2409.12189v1">Link to paper</a></p>
                <p>Forecasting long-term 3D human motion is challenging: the stochasticity ofhuman behavior makes it hard to generate realistic human motion from the inputsequence alone. Information on the scene environment and the motion of nearbypeople can greatly aid the generation process. We propose a scene-aware socialtransformer model SAST to forecast long-term 10s human motion motion.Unlike previous models our approach can model interactions between both widelyvarying numbers of people and objects in a scene. We combine a temporalconvolutional encoder-decoder architecture with a Transformer-based bottleneckthat allows us to efficiently combine motion and scene information. We modelthe conditional motion distribution using denoising diffusion models. Webenchmark our approach on the Humans in Kitchens dataset which contains 1 to16 persons and 29 to 50 objects that are visible simultaneously. Our modeloutperforms other approaches in terms of realism and diversity on differentmetrics and in a user study. Code is available athttps://github.com/felixbmuller/SAST.</p>
                <p>Last Updated: 2024-09-18 17:58:51 UTC</p>
                <button class="interpret-button" data-id="2409.12189v1">Interpret</button>
                <div id="interpretation-2409.12189v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning</h3>
                <p>Authors: Zayne SpragueFangcong YinJuan Diego RodriguezDongwei JiangManya WadhwaPrasann SinghalXinyu ZhaoXi YeKyle MahowaldGreg Durrett</p>
                <p><a href="http://arxiv.org/abs/2409.12183v1">Link to paper</a></p>
                <p>Chain-of-thought CoT via prompting is the de facto method for elicitingreasoning capabilities from large language models LLMs. But for what kinds oftasks is this extra thinking really helpful To analyze this we conducteda quantitative meta-analysis covering over 100 papers using CoT and ran our ownevaluations of 20 datasets across 14 models. Our results show that CoT givesstrong performance benefits primarily on tasks involving math or logic withmuch smaller gains on other types of tasks. On MMLU directly generating theanswer without CoT leads to almost identical accuracy as CoT unless thequestion or models response contains an equals sign indicating symbolicoperations and reasoning. Following this finding we analyze the behavior ofCoT on these problems by separating planning and execution and comparingagainst tool-augmented LLMs. Much of CoTs gain comes from improving symbolicexecution but it underperforms relative to using a symbolic solver. Ourresults indicate that CoT can be applied selectively maintaining performancewhile saving inference costs. Furthermore they suggest a need to move beyondprompt-based CoT to new paradigms that better leverage intermediate computationacross the whole range of LLM applications.</p>
                <p>Last Updated: 2024-09-18 17:55:00 UTC</p>
                <button class="interpret-button" data-id="2409.12183v1">Interpret</button>
                <div id="interpretation-2409.12183v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Controlled Study on Long Context Extension and Generalization in LLMs</h3>
                <p>Authors: Yi LuJing Nathan YanSonglin YangJustin T. ChiuSiyu RenFei YuanWenting ZhaoZhiyong WuAlexander M. Rush</p>
                <p><a href="http://arxiv.org/abs/2409.12181v1">Link to paper</a></p>
                <p>Broad textual understanding and in-context learning require language modelsthat utilize full document contexts. Due to the implementation challengesassociated with directly training long-context models many methods have beenproposed for extending models to handle long contexts. However owing todifferences in data and model classes it has been challenging to compare theseapproaches leading to uncertainty as to how to evaluate long-contextperformance and whether it differs from standard evaluation. We implement acontrolled protocol for extension methods with a standardized evaluationutilizing consistent base models and extension data. Our study yields severalinsights into long-context behavior. First we reaffirm the critical role ofperplexity as a general-purpose performance indicator even in longer-contexttasks. Second we find that current approximate attention methodssystematically underperform across long-context tasks. Finally we confirm thatexact fine-tuning based methods are generally effective within the range oftheir extension whereas extrapolation remains challenging. All codebasesmodels and checkpoints will be made available open-source promotingtransparency and facilitating further research in this critical area of AIdevelopment.</p>
                <p>Last Updated: 2024-09-18 17:53:17 UTC</p>
                <button class="interpret-button" data-id="2409.12181v1">Interpret</button>
                <div id="interpretation-2409.12181v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Finetuning Language Models to Emit Linguistic Expressions of Uncertainty</h3>
                <p>Authors: Arslan ChaudhrySridhar ThiagarajanDilan Gorur</p>
                <p><a href="http://arxiv.org/abs/2409.12180v1">Link to paper</a></p>
                <p>Large language models LLMs are increasingly employed in information-seekingand decision-making tasks. Despite their broad utility LLMs tend to generateinformation that conflicts with real-world facts and their persuasive stylecan make these inaccuracies appear confident and convincing. As a resultend-users struggle to consistently align the confidence expressed by LLMs withthe accuracy of their predictions often leading to either blind trust in alloutputs or a complete disregard for their reliability. In this work we exploresupervised finetuning on uncertainty-augmented predictions as a method todevelop models that produce linguistic expressions of uncertainty.Specifically we measure the calibration of pre-trained models and thenfine-tune language models to generate calibrated linguistic expressions ofuncertainty. Through experiments on various question-answering datasets wedemonstrate that LLMs are well-calibrated in assessing their predictions andsupervised finetuning based on the models own confidence leads towell-calibrated expressions of uncertainty particularly for single-claimanswers.</p>
                <p>Last Updated: 2024-09-18 17:52:53 UTC</p>
                <button class="interpret-button" data-id="2409.12180v1">Interpret</button>
                <div id="interpretation-2409.12180v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Vista3D: Unravel the 3D Darkside of a Single Image</h3>
                <p>Authors: Qiuhong ShenXingyi YangMichael Bi MiXinchao Wang</p>
                <p><a href="http://arxiv.org/abs/2409.12193v1">Link to paper</a></p>
                <p>We embark on the age-old quest: unveiling the hidden dimensions of objectsfrom mere glimpses of their visible parts. To address this we present Vista3Da framework that realizes swift and consistent 3D generation within a mere 5minutes. At the heart of Vista3D lies a two-phase approach: the coarse phaseand the fine phase. In the coarse phase we rapidly generate initial geometrywith Gaussian Splatting from a single image. In the fine phase we extract aSigned Distance Function SDF directly from learned Gaussian Splattingoptimizing it with a differentiable isosurface representation. Furthermore itelevates the quality of generation by using a disentangled representation withtwo independent implicit functions to capture both visible and obscured aspectsof objects. Additionally it harmonizes gradients from 2D diffusion prior with3D-aware diffusion priors by angular diffusion prior composition. Throughextensive evaluation we demonstrate that Vista3D effectively sustains abalance between the consistency and diversity of the generated 3D objects.Demos and code will be available at https://github.com/florinshen/Vista3D.</p>
                <p>Last Updated: 2024-09-18 17:59:44 UTC</p>
                <button class="interpret-button" data-id="2409.12193v1">Interpret</button>
                <div id="interpretation-2409.12193v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</h3>
                <p>Authors: Zichen Jeff CuiHengkai PanAadhithya IyerSiddhant HaldarLerrel Pinto</p>
                <p><a href="http://arxiv.org/abs/2409.12192v1">Link to paper</a></p>
                <p>Imitation learning has proven to be a powerful tool for training complexvisuomotor policies. However current methods often require hundreds tothousands of expert demonstrations to handle high-dimensional visualobservations. A key reason for this poor data efficiency is that visualrepresentations are predominantly either pretrained on out-of-domain data ortrained directly through a behavior cloning objective. In this work we presentDynaMo a new in-domain self-supervised method for learning visualrepresentations. Given a set of expert demonstrations we jointly learn alatent inverse dynamics model and a forward dynamics model over a sequence ofimage embeddings predicting the next frame in latent space withoutaugmentations contrastive sampling or access to ground truth actions.Importantly DynaMo does not require any out-of-domain data such as Internetdatasets or cross-embodied datasets. On a suite of six simulated and realenvironments we show that representations learned with DynaMo significantlyimprove downstream imitation learning performance over prior self-supervisedlearning objectives and pretrained representations. Gains from using DynaMohold across policy classes such as Behavior Transformer Diffusion Policy MLPand nearest neighbors. Finally we ablate over key components of DynaMo andmeasure its impact on downstream policy performance. Robot videos are bestviewed at https://dynamo-ssl.github.io</p>
                <p>Last Updated: 2024-09-18 17:59:43 UTC</p>
                <button class="interpret-button" data-id="2409.12192v1">Interpret</button>
                <div id="interpretation-2409.12192v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution</h3>
                <p>Authors: Peng WangShuai BaiSinan TanShijie WangZhihao FanJinze BaiKeqin ChenXuejing LiuJialin WangWenbin GeYang FanKai DangMengfei DuXuancheng RenRui MenDayiheng LiuChang ZhouJingren ZhouJunyang Lin</p>
                <p><a href="http://arxiv.org/abs/2409.12191v1">Link to paper</a></p>
                <p>We present the Qwen2-VL Series an advanced upgrade of the previous Qwen-VLmodels that redefines the conventional predetermined-resolution approach invisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanismwhich enables the model to dynamically process images of varying resolutionsinto different numbers of visual tokens. This approach allows the model togenerate more efficient and accurate visual representations closely aligningwith human perceptual processes. The model also integrates Multimodal RotaryPosition Embedding M-RoPE facilitating the effective fusion of positionalinformation across text images and videos. We employ a unified paradigm forprocessing both images and videos enhancing the models visual perceptioncapabilities. To explore the potential of large multimodal models Qwen2-VLinvestigates the scaling laws for large vision-language models LVLMs. Byscaling both the model size-with versions at 2B 8B and 72B parameters-and theamount of training data the Qwen2-VL Series achieves highly competitiveperformance. Notably the Qwen2-VL-72B model achieves results comparable toleading models such as GPT-4o and Claude3.5-Sonnet across various multimodalbenchmarks outperforming other generalist models. Code is available aturlhttps://github.com/QwenLM/Qwen2-VL.</p>
                <p>Last Updated: 2024-09-18 17:59:32 UTC</p>
                <button class="interpret-button" data-id="2409.12191v1">Interpret</button>
                <div id="interpretation-2409.12191v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning</h3>
                <p>Authors: Zayne SpragueFangcong YinJuan Diego RodriguezDongwei JiangManya WadhwaPrasann SinghalXinyu ZhaoXi YeKyle MahowaldGreg Durrett</p>
                <p><a href="http://arxiv.org/abs/2409.12183v1">Link to paper</a></p>
                <p>Chain-of-thought CoT via prompting is the de facto method for elicitingreasoning capabilities from large language models LLMs. But for what kinds oftasks is this extra thinking really helpful To analyze this we conducteda quantitative meta-analysis covering over 100 papers using CoT and ran our ownevaluations of 20 datasets across 14 models. Our results show that CoT givesstrong performance benefits primarily on tasks involving math or logic withmuch smaller gains on other types of tasks. On MMLU directly generating theanswer without CoT leads to almost identical accuracy as CoT unless thequestion or models response contains an equals sign indicating symbolicoperations and reasoning. Following this finding we analyze the behavior ofCoT on these problems by separating planning and execution and comparingagainst tool-augmented LLMs. Much of CoTs gain comes from improving symbolicexecution but it underperforms relative to using a symbolic solver. Ourresults indicate that CoT can be applied selectively maintaining performancewhile saving inference costs. Furthermore they suggest a need to move beyondprompt-based CoT to new paradigms that better leverage intermediate computationacross the whole range of LLM applications.</p>
                <p>Last Updated: 2024-09-18 17:55:00 UTC</p>
                <button class="interpret-button" data-id="2409.12183v1">Interpret</button>
                <div id="interpretation-2409.12183v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Computational Dynamical Systems</h3>
                <p>Authors: Jordan CotlerSemon Rezchikov</p>
                <p><a href="http://arxiv.org/abs/2409.12179v1">Link to paper</a></p>
                <p>We study the computational complexity theory of smooth finite-dimensionaldynamical systems. Building off of previous work we give definitions for whatit means for a smooth dynamical system to simulate a Turing machine. We thenshow that chaotic dynamical systems more precisely Axiom A systems andintegrable dynamical systems more generally measure-preserving systemscannot robustly simulate universal Turing machines although such machines canbe robustly simulated by other kinds of dynamical systems. Subsequently weshow that any Turing machine that can be encoded into a structurally stableone-dimensional dynamical system must have a decidable halting problem andmoreover an explicit time complexity bound in instances where it does halt.More broadly our work elucidates what it means for one machine to simulateanother and emphasizes the necessity of defining low-complexity encoders anddecoders to translate between the dynamics of the simulation and the systembeing simulated. We highlight how the notion of a computational dynamicalsystem leads to questions at the intersection of computational complexitytheory dynamical systems theory and real algebraic geometry.</p>
                <p>Last Updated: 2024-09-18 17:51:48 UTC</p>
                <button class="interpret-button" data-id="2409.12179v1">Interpret</button>
                <div id="interpretation-2409.12179v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Gender Representation and Bias in Indian Civil Service Mock Interviews</h3>
                <p>Authors: Somonnoy BanerjeeSujan DuttaSoumyajit DattaAshiqur R. KhudaBukhsh</p>
                <p><a href="http://arxiv.org/abs/2409.12194v2">Link to paper</a></p>
                <p>This paper makes three key contributions. First via a substantial corpus of51278 interview questions sourced from 888 YouTube videos of mock interviewsof Indian civil service candidates we demonstrate stark gender bias in thebroad nature of questions asked to male and female candidates. Second ourexperiments with large language models show a strong presence of gender bias inexplanations provided by the LLMs on the gender inference task. Finally wepresent a novel dataset of 51278 interview questions that can inform futuresocial science studies.</p>
                <p>Last Updated: 2024-09-19 02:56:26 UTC</p>
                <button class="interpret-button" data-id="2409.12194v2">Interpret</button>
                <div id="interpretation-2409.12194v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution</h3>
                <p>Authors: Peng WangShuai BaiSinan TanShijie WangZhihao FanJinze BaiKeqin ChenXuejing LiuJialin WangWenbin GeYang FanKai DangMengfei DuXuancheng RenRui MenDayiheng LiuChang ZhouJingren ZhouJunyang Lin</p>
                <p><a href="http://arxiv.org/abs/2409.12191v1">Link to paper</a></p>
                <p>We present the Qwen2-VL Series an advanced upgrade of the previous Qwen-VLmodels that redefines the conventional predetermined-resolution approach invisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanismwhich enables the model to dynamically process images of varying resolutionsinto different numbers of visual tokens. This approach allows the model togenerate more efficient and accurate visual representations closely aligningwith human perceptual processes. The model also integrates Multimodal RotaryPosition Embedding M-RoPE facilitating the effective fusion of positionalinformation across text images and videos. We employ a unified paradigm forprocessing both images and videos enhancing the models visual perceptioncapabilities. To explore the potential of large multimodal models Qwen2-VLinvestigates the scaling laws for large vision-language models LVLMs. Byscaling both the model size-with versions at 2B 8B and 72B parameters-and theamount of training data the Qwen2-VL Series achieves highly competitiveperformance. Notably the Qwen2-VL-72B model achieves results comparable toleading models such as GPT-4o and Claude3.5-Sonnet across various multimodalbenchmarks outperforming other generalist models. Code is available aturlhttps://github.com/QwenLM/Qwen2-VL.</p>
                <p>Last Updated: 2024-09-18 17:59:32 UTC</p>
                <button class="interpret-button" data-id="2409.12191v1">Interpret</button>
                <div id="interpretation-2409.12191v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Qwen2.5-Coder Technical Report</h3>
                <p>Authors: Binyuan HuiJian YangZeyu CuiJiaxi YangDayiheng LiuLei ZhangTianyu LiuJiajun ZhangBowen YuKai DangAn YangRui MenFei HuangXingzhang RenXuancheng RenJingren ZhouJunyang Lin</p>
                <p><a href="http://arxiv.org/abs/2409.12186v1">Link to paper</a></p>
                <p>In this report we introduce the Qwen2.5-Coder series a significant upgradefrom its predecessor CodeQwen1.5. This series includes two models:Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific modelQwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrainedon a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaningscalable synthetic data generation and balanced data mixing Qwen2.5-Coderdemonstrates impressive code generation capabilities while retaining generalversatility. The model has been evaluated on a wide range of code-relatedtasks achieving state-of-the-art SOTA performance across more than 10benchmarks including code generation completion reasoning and repairconsistently outperforming larger models of the same model size. We believethat the release of the Qwen2.5-Coder series will not only push the boundariesof research in code intelligence but also through its permissive licensingencourage broader adoption by developers in real-world applications.</p>
                <p>Last Updated: 2024-09-18 17:57:57 UTC</p>
                <button class="interpret-button" data-id="2409.12186v1">Interpret</button>
                <div id="interpretation-2409.12186v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning</h3>
                <p>Authors: Zayne SpragueFangcong YinJuan Diego RodriguezDongwei JiangManya WadhwaPrasann SinghalXinyu ZhaoXi YeKyle MahowaldGreg Durrett</p>
                <p><a href="http://arxiv.org/abs/2409.12183v1">Link to paper</a></p>
                <p>Chain-of-thought CoT via prompting is the de facto method for elicitingreasoning capabilities from large language models LLMs. But for what kinds oftasks is this extra thinking really helpful To analyze this we conducteda quantitative meta-analysis covering over 100 papers using CoT and ran our ownevaluations of 20 datasets across 14 models. Our results show that CoT givesstrong performance benefits primarily on tasks involving math or logic withmuch smaller gains on other types of tasks. On MMLU directly generating theanswer without CoT leads to almost identical accuracy as CoT unless thequestion or models response contains an equals sign indicating symbolicoperations and reasoning. Following this finding we analyze the behavior ofCoT on these problems by separating planning and execution and comparingagainst tool-augmented LLMs. Much of CoTs gain comes from improving symbolicexecution but it underperforms relative to using a symbolic solver. Ourresults indicate that CoT can be applied selectively maintaining performancewhile saving inference costs. Furthermore they suggest a need to move beyondprompt-based CoT to new paradigms that better leverage intermediate computationacross the whole range of LLM applications.</p>
                <p>Last Updated: 2024-09-18 17:55:00 UTC</p>
                <button class="interpret-button" data-id="2409.12183v1">Interpret</button>
                <div id="interpretation-2409.12183v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Controlled Study on Long Context Extension and Generalization in LLMs</h3>
                <p>Authors: Yi LuJing Nathan YanSonglin YangJustin T. ChiuSiyu RenFei YuanWenting ZhaoZhiyong WuAlexander M. Rush</p>
                <p><a href="http://arxiv.org/abs/2409.12181v1">Link to paper</a></p>
                <p>Broad textual understanding and in-context learning require language modelsthat utilize full document contexts. Due to the implementation challengesassociated with directly training long-context models many methods have beenproposed for extending models to handle long contexts. However owing todifferences in data and model classes it has been challenging to compare theseapproaches leading to uncertainty as to how to evaluate long-contextperformance and whether it differs from standard evaluation. We implement acontrolled protocol for extension methods with a standardized evaluationutilizing consistent base models and extension data. Our study yields severalinsights into long-context behavior. First we reaffirm the critical role ofperplexity as a general-purpose performance indicator even in longer-contexttasks. Second we find that current approximate attention methodssystematically underperform across long-context tasks. Finally we confirm thatexact fine-tuning based methods are generally effective within the range oftheir extension whereas extrapolation remains challenging. All codebasesmodels and checkpoints will be made available open-source promotingtransparency and facilitating further research in this critical area of AIdevelopment.</p>
                <p>Last Updated: 2024-09-18 17:53:17 UTC</p>
                <button class="interpret-button" data-id="2409.12181v1">Interpret</button>
                <div id="interpretation-2409.12181v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning</h3>
                <p>Authors: Claude FormanekLouise BeyersCallum Rhys TilburyJonathan P. ShockArnu Pretorius</p>
                <p><a href="http://arxiv.org/abs/2409.12001v1">Link to paper</a></p>
                <p>Offline multi-agent reinforcement learning MARL is an exciting direction ofresearch that uses static datasets to find optimal control policies formulti-agent systems. Though the field is by definition data-driven effortshave thus far neglected data in their drive to achieve state-of-the-artresults. We first substantiate this claim by surveying the literature showinghow the majority of works generate their own datasets without consistentmethodology and provide sparse information about the characteristics of thesedatasets. We then show why neglecting the nature of the data is problematicthrough salient examples of how tightly algorithmic performance is coupled tothe dataset used necessitating a common foundation for experiments in thefield. In response we take a big step towards improving data usage and dataawareness in offline MARL with three key contributions: 1 a clear guidelinefor generating novel datasets 2 a standardisation of over 80 existingdatasets hosted in a publicly available repository using a consistent storageformat and easy-to-use API and 3 a suite of analysis tools that allow us tounderstand these datasets better aiding further development.</p>
                <p>Last Updated: 2024-09-18 14:13:24 UTC</p>
                <button class="interpret-button" data-id="2409.12001v1">Interpret</button>
                <div id="interpretation-2409.12001v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>XP-MARL: Auxiliary Prioritization in Multi-Agent Reinforcement Learning to Address Non-Stationarity</h3>
                <p>Authors: Jianye XuOmar SobhyBassam Alrifaee</p>
                <p><a href="http://arxiv.org/abs/2409.11852v1">Link to paper</a></p>
                <p>Non-stationarity poses a fundamental challenge in Multi-Agent ReinforcementLearning MARL arising from agents simultaneously learning and altering theirpolicies. This creates a non-stationary environment from the perspective ofeach individual agent often leading to suboptimal or even unconverged learningoutcomes. We propose an open-source framework named XP-MARL which augmentsMARL with auxiliary prioritization to address this challenge in cooperativesettings. XP-MARL is 1 founded upon our hypothesis that prioritizing agentsand letting higher-priority agents establish their actions first wouldstabilize the learning process and thus mitigate non-stationarity and 2enabled by our proposed mechanism called action propagation wherehigher-priority agents act first and communicate their actions providing amore stationary environment for others. Moreover instead of using a predefinedor heuristic priority assignment XP-MARL learns priority-assignment policieswith an auxiliary MARL problem leading to a joint learning scheme. Experimentsin a motion-planning scenario involving Connected and Automated Vehicles CAVsdemonstrate that XP-MARL improves the safety of a baseline model by 84.4 andoutperforms a state-of-the-art approach which improves the baseline by only12.8. Code: github.com/cas-lab-munich/sigmarl</p>
                <p>Last Updated: 2024-09-18 10:10:55 UTC</p>
                <button class="interpret-button" data-id="2409.11852v1">Interpret</button>
                <div id="interpretation-2409.11852v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>HARP: Human-Assisted Regrouping with Permutation Invariant Critic for Multi-Agent Reinforcement Learning</h3>
                <p>Authors: Huawen HuEnze ShiChenxi YueShuocun YangZihao WuYiwei LiTianyang ZhongTuo ZhangTianming LiuShu Zhang</p>
                <p><a href="http://arxiv.org/abs/2409.11741v1">Link to paper</a></p>
                <p>Human-in-the-loop reinforcement learning integrates human expertise toaccelerate agent learning and provide critical guidance and feedback in complexfields. However many existing approaches focus on single-agent tasks andrequire continuous human involvement during the training process significantlyincreasing the human workload and limiting scalability. In this paper wepropose HARP Human-Assisted Regrouping with Permutation Invariant Critic amulti-agent reinforcement learning framework designed for group-oriented tasks.HARP integrates automatic agent regrouping with strategic human assistanceduring deployment enabling and allowing non-experts to offer effectiveguidance with minimal intervention. During training agents dynamically adjusttheir groupings to optimize collaborative task completion. When deployed theyactively seek human assistance and utilize the Permutation Invariant GroupCritic to evaluate and refine human-proposed groupings allowing non-expertusers to contribute valuable suggestions. In multiple collaboration scenariosour approach is able to leverage limited guidance from non-experts and enhanceperformance. The project can be found at https://github.com/huawen-hu/HARP.</p>
                <p>Last Updated: 2024-09-18 06:54:36 UTC</p>
                <button class="interpret-button" data-id="2409.11741v1">Interpret</button>
                <div id="interpretation-2409.11741v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-robot connection towards collective obstacle field traversal</h3>
                <p>Authors: Haodi HuXingjue LiaoWuhao DuFeifei Qian</p>
                <p><a href="http://arxiv.org/abs/2409.11709v1">Link to paper</a></p>
                <p>Environments with large terrain height variations present great challengesfor legged robot locomotion. Drawing inspiration from fire ants collectiveassembly behavior we study strategies that can enable two connectablerobots to collectively navigate over bumpy terrains with height variationslarger than robot leg length. Each robot was designed to be extremely simplewith a cubical body and one rotary motor actuating four vertical peg legs thatmove in pairs. Two or more robots could physically connect to one another toenhance collective mobility. We performed locomotion experiments with atwo-robot group across an obstacle field filled with uniformly-distributedsemi-spherical boulders. Experimentally-measured robot speed suggested thatthe connection length between the robots has a significant effect on collectivemobility: connection length C in 0.86 0.9 robot unit body length UBL wereable to produce sustainable movements across the obstacle field whereasconnection length C in 0.63 0.84 and 0.92 1.1 UBL resulted in lowtraversability. An energy landscape based model revealed the underlyingmechanism of how connection length modulated collective mobility through thesystems potential energy landscape and informed adaptation strategies for thetwo-robot system to adapt their connection length for traversing obstaclefields with varying spatial frequencies. Our results demonstrated that byvarying the connection configuration between the robots the two-robot systemcould leverage mechanical intelligence to better utilize obstacle interactionforces and produce improved locomotion. Going forward we envision thatgeneralized principles of robot-environment coupling can inform design andcontrol strategies for a large group of small robots to achieve ant-likecollective environment negotiation.</p>
                <p>Last Updated: 2024-09-18 05:30:44 UTC</p>
                <button class="interpret-button" data-id="2409.11709v1">Interpret</button>
                <div id="interpretation-2409.11709v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Hypergraph-based Motion Generation with Multi-modal Interaction Relational Reasoning</h3>
                <p>Authors: Keshu WuYang ZhouHaotian ShiDominique LordBin RanXinyue Ye</p>
                <p><a href="http://arxiv.org/abs/2409.11676v1">Link to paper</a></p>
                <p>The intricate nature of real-world driving environments characterized bydynamic and diverse interactions among multiple vehicles and their possiblefuture states presents considerable challenges in accurately predicting themotion states of vehicles and handling the uncertainty inherent in thepredictions. Addressing these challenges requires comprehensive modeling andreasoning to capture the implicit relations among vehicles and thecorresponding diverse behaviors. This research introduces an integratedframework for autonomous vehicles AVs motion prediction to address thesecomplexities utilizing a novel Relational Hypergraph Interaction-informedNeural mOtion generator RHINO. RHINO leverages hypergraph-based relationalreasoning by integrating a multi-scale hypergraph neural network to modelgroup-wise interactions among multiple vehicles and their multi-modal drivingbehaviors thereby enhancing motion prediction accuracy and reliability.Experimental validation using real-world datasets demonstrates the superiorperformance of this framework in improving predictive accuracy and fosteringsocially aware automated driving in dynamic traffic scenarios.</p>
                <p>Last Updated: 2024-09-18 03:30:38 UTC</p>
                <button class="interpret-button" data-id="2409.11676v1">Interpret</button>
                <div id="interpretation-2409.11676v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Fitting Multilevel Factor Models</h3>
                <p>Authors: Tetiana ParshakovaTrevor HastieStephen Boyd</p>
                <p><a href="http://arxiv.org/abs/2409.12067v1">Link to paper</a></p>
                <p>We examine a special case of the multilevel factor model with covariancegiven by multilevel low rank MLR matrixciteparshakova2023factor. Wedevelop a novel fast implementation of the expectation-maximization EMalgorithm tailored for multilevel factor models to maximize the likelihood ofthe observed data. This method accommodates any hierarchical structure andmaintains linear time and storage complexities per iteration. This is achievedthrough a new efficient technique for computing the inverse of the positivedefinite MLR matrix. We show that the inverse of an invertible PSD MLR matrixis also an MLR matrix with the same sparsity in factors and we use therecursive Sherman-Morrison-Woodbury matrix identity to obtain the factors ofthe inverse. Additionally we present an algorithm that computes the Choleskyfactorization of an expanded matrix with linear time and space complexitiesyielding the covariance matrix as its Schur complement. This paper isaccompanied by an open-source package that implements the proposed methods.</p>
                <p>Last Updated: 2024-09-18 15:39:12 UTC</p>
                <button class="interpret-button" data-id="2409.12067v1">Interpret</button>
                <div id="interpretation-2409.12067v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Cartan moving frames and the data manifolds</h3>
                <p>Authors: Eliot TronRita FioresiNicolas CouellanStéphane Puechmorel</p>
                <p><a href="http://arxiv.org/abs/2409.12057v1">Link to paper</a></p>
                <p>The purpose of this paper is to employ the language of Cartan moving framesto study the geometry of the data manifolds and its Riemannian structure viathe data information metric and its curvature at data points. Using thisframework and through experiments explanations on the response of a neuralnetwork are given by pointing out the output classes that are easily reachablefrom a given input. This emphasizes how the proposed mathematical relationshipbetween the output of the network and the geometry of its inputs can beexploited as an explainable artificial intelligence tool.</p>
                <p>Last Updated: 2024-09-18 15:31:29 UTC</p>
                <button class="interpret-button" data-id="2409.12057v1">Interpret</button>
                <div id="interpretation-2409.12057v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Symmetry-Based Structured Matrices for Efficient Approximately Equivariant Networks</h3>
                <p>Authors: Ashwin SamudreMircea PetracheBrian D. NordShubhendu Trivedi</p>
                <p><a href="http://arxiv.org/abs/2409.11772v1">Link to paper</a></p>
                <p>There has been much recent interest in designing symmetry-aware neuralnetworks NNs exhibiting relaxed equivariance. Such NNs aim to interpolatebetween being exactly equivariant and being fully flexible affordingconsistent performance benefits. In a separate line of work certain structuredparameter matrices -- those with displacement structure characterized by lowdisplacement rank LDR -- have been used to design small-footprint NNs.Displacement structure enables fast function and gradient evaluation butpermits accurate approximations via compression primarily to classicalconvolutional neural networks CNNs. In this work we propose a generalframework -- based on a novel construction of symmetry-based structuredmatrices -- to build approximately equivariant NNs with significantly reducedparameter counts. Our framework integrates the two aforementioned lines of workvia the use of so-called Group Matrices GMs a forgotten precursor to themodern notion of regular representations of finite groups. GMs allow the designof structured matrices -- resembling LDR matrices -- which generalize thelinear operations of a classical CNN from cyclic groups to general finitegroups and their homogeneous spaces. We show that GMs can be employed to extendall the elementary operations of CNNs to general discrete groups. Further thetheory of structured matrices based on GMs provides a generalization of LDRtheory focussed on matrices with cyclic structure providing a tool forimplementing approximate equivariance for discrete groups. We test GM-basedarchitectures on a variety of tasks in the presence of relaxed symmetry. Wereport that our framework consistently performs competitively compared toapproximately equivariant NNs and other structured matrix-based compressionframeworks sometimes with a one or two orders of magnitude lower parametercount.</p>
                <p>Last Updated: 2024-09-18 07:52:33 UTC</p>
                <button class="interpret-button" data-id="2409.11772v1">Interpret</button>
                <div id="interpretation-2409.11772v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Recurrent Interpolants for Probabilistic Time Series Prediction</h3>
                <p>Authors: Yu ChenMarin BilošSarthak MittalWei DengKashif RasulAnderson Schneider</p>
                <p><a href="http://arxiv.org/abs/2409.11684v1">Link to paper</a></p>
                <p>Sequential models such as recurrent neural networks or transformer-basedmodels became textitde facto tools for multivariate time series forecastingin a probabilistic fashion with applications to a wide range of datasets suchas finance biology medicine etc. Despite their adeptness in capturingdependencies assessing prediction uncertainty and efficiency in trainingchallenges emerge in modeling high-dimensional complex distributions andcross-feature dependencies. To tackle these issues recent works delve intogenerative modeling by employing diffusion or flow-based models. Notably theintegration of stochastic differential equations or probability flowsuccessfully extends these methods to probabilistic time series imputation andforecasting. However scalability issues necessitate a computational-friendlyframework for large-scale generative model-based predictions. This workproposes a novel approach by blending the computational efficiency of recurrentneural networks with the high-quality probabilistic modeling of the diffusionmodel which addresses challenges and advances generative models applicationin time series forecasting. Our method relies on the foundation of stochasticinterpolants and the extension to a broader conditional generation frameworkwith additional control features offering insights for future developments inthis dynamic field.</p>
                <p>Last Updated: 2024-09-18 03:52:48 UTC</p>
                <button class="interpret-button" data-id="2409.11684v1">Interpret</button>
                <div id="interpretation-2409.11684v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PieClam: A Universal Graph Autoencoder Based on Overlapping Inclusive and Exclusive Communities</h3>
                <p>Authors: Daniel ZilbergRon Levie</p>
                <p><a href="http://arxiv.org/abs/2409.11618v1">Link to paper</a></p>
                <p>We propose PieClam Prior Inclusive Exclusive Cluster Affiliation Model: aprobabilistic graph model for representing any graph as overlapping generalizedcommunities. Our method can be interpreted as a graph autoencoder: nodes areembedded into a code space by an algorithm that maximizes the log-likelihood ofthe decoded graph given the input graph. PieClam is a community affiliationmodel that extends well-known methods like BigClam in two main manners. Firstinstead of the decoder being defined via pairwise interactions between thenodes in the code space we also incorporate a learned prior on thedistribution of nodes in the code space turning our method into a graphgenerative model. Secondly we generalize the notion of communities by allowingnot only sets of nodes with strong connectivity which we call inclusivecommunities but also sets of nodes with strong disconnection which we callexclusive communities. To model both types of communities we propose a newtype of decoder based the Lorentz inner product which we prove to be much moreexpressive than standard decoders based on standard inner products or normdistances. By introducing a new graph similarity measure that we call the logcut distance we show that PieClam is a universal autoencoder able touniformly approximately reconstruct any graph. Our method is shown to obtaincompetitive performance in graph anomaly detection benchmarks.</p>
                <p>Last Updated: 2024-09-18 00:49:42 UTC</p>
                <button class="interpret-button" data-id="2409.11618v1">Interpret</button>
                <div id="interpretation-2409.11618v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Vista3D: Unravel the 3D Darkside of a Single Image</h3>
                <p>Authors: Qiuhong ShenXingyi YangMichael Bi MiXinchao Wang</p>
                <p><a href="http://arxiv.org/abs/2409.12193v1">Link to paper</a></p>
                <p>We embark on the age-old quest: unveiling the hidden dimensions of objectsfrom mere glimpses of their visible parts. To address this we present Vista3Da framework that realizes swift and consistent 3D generation within a mere 5minutes. At the heart of Vista3D lies a two-phase approach: the coarse phaseand the fine phase. In the coarse phase we rapidly generate initial geometrywith Gaussian Splatting from a single image. In the fine phase we extract aSigned Distance Function SDF directly from learned Gaussian Splattingoptimizing it with a differentiable isosurface representation. Furthermore itelevates the quality of generation by using a disentangled representation withtwo independent implicit functions to capture both visible and obscured aspectsof objects. Additionally it harmonizes gradients from 2D diffusion prior with3D-aware diffusion priors by angular diffusion prior composition. Throughextensive evaluation we demonstrate that Vista3D effectively sustains abalance between the consistency and diversity of the generated 3D objects.Demos and code will be available at https://github.com/florinshen/Vista3D.</p>
                <p>Last Updated: 2024-09-18 17:59:44 UTC</p>
                <button class="interpret-button" data-id="2409.12193v1">Interpret</button>
                <div id="interpretation-2409.12193v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</h3>
                <p>Authors: Zichen Jeff CuiHengkai PanAadhithya IyerSiddhant HaldarLerrel Pinto</p>
                <p><a href="http://arxiv.org/abs/2409.12192v1">Link to paper</a></p>
                <p>Imitation learning has proven to be a powerful tool for training complexvisuomotor policies. However current methods often require hundreds tothousands of expert demonstrations to handle high-dimensional visualobservations. A key reason for this poor data efficiency is that visualrepresentations are predominantly either pretrained on out-of-domain data ortrained directly through a behavior cloning objective. In this work we presentDynaMo a new in-domain self-supervised method for learning visualrepresentations. Given a set of expert demonstrations we jointly learn alatent inverse dynamics model and a forward dynamics model over a sequence ofimage embeddings predicting the next frame in latent space withoutaugmentations contrastive sampling or access to ground truth actions.Importantly DynaMo does not require any out-of-domain data such as Internetdatasets or cross-embodied datasets. On a suite of six simulated and realenvironments we show that representations learned with DynaMo significantlyimprove downstream imitation learning performance over prior self-supervisedlearning objectives and pretrained representations. Gains from using DynaMohold across policy classes such as Behavior Transformer Diffusion Policy MLPand nearest neighbors. Finally we ablate over key components of DynaMo andmeasure its impact on downstream policy performance. Robot videos are bestviewed at https://dynamo-ssl.github.io</p>
                <p>Last Updated: 2024-09-18 17:59:43 UTC</p>
                <button class="interpret-button" data-id="2409.12192v1">Interpret</button>
                <div id="interpretation-2409.12192v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution</h3>
                <p>Authors: Peng WangShuai BaiSinan TanShijie WangZhihao FanJinze BaiKeqin ChenXuejing LiuJialin WangWenbin GeYang FanKai DangMengfei DuXuancheng RenRui MenDayiheng LiuChang ZhouJingren ZhouJunyang Lin</p>
                <p><a href="http://arxiv.org/abs/2409.12191v1">Link to paper</a></p>
                <p>We present the Qwen2-VL Series an advanced upgrade of the previous Qwen-VLmodels that redefines the conventional predetermined-resolution approach invisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanismwhich enables the model to dynamically process images of varying resolutionsinto different numbers of visual tokens. This approach allows the model togenerate more efficient and accurate visual representations closely aligningwith human perceptual processes. The model also integrates Multimodal RotaryPosition Embedding M-RoPE facilitating the effective fusion of positionalinformation across text images and videos. We employ a unified paradigm forprocessing both images and videos enhancing the models visual perceptioncapabilities. To explore the potential of large multimodal models Qwen2-VLinvestigates the scaling laws for large vision-language models LVLMs. Byscaling both the model size-with versions at 2B 8B and 72B parameters-and theamount of training data the Qwen2-VL Series achieves highly competitiveperformance. Notably the Qwen2-VL-72B model achieves results comparable toleading models such as GPT-4o and Claude3.5-Sonnet across various multimodalbenchmarks outperforming other generalist models. Code is available aturlhttps://github.com/QwenLM/Qwen2-VL.</p>
                <p>Last Updated: 2024-09-18 17:59:32 UTC</p>
                <button class="interpret-button" data-id="2409.12191v1">Interpret</button>
                <div id="interpretation-2409.12191v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Bundle Adjustment in the Eager Mode</h3>
                <p>Authors: Zitong ZhanHuan XuZihang FangXinpeng WeiYaoyu HuChen Wang</p>
                <p><a href="http://arxiv.org/abs/2409.12190v1">Link to paper</a></p>
                <p>Bundle adjustment BA is a critical technique in various roboticapplications such as simultaneous localization and mapping SLAM augmentedreality AR and photogrammetry. BA optimizes parameters such as camera posesand 3D landmarks to align them with observations. With the growing importanceof deep learning in perception systems there is an increasing need tointegrate BA with deep learning frameworks for enhanced reliability andperformance. However widely-used C-based BA frameworks such as GTSAMg2o and Ceres lack native integration with modern deep learning librarieslike PyTorch. This limitation affects their flexibility adaptability ease ofdebugging and overall implementation efficiency. To address this gap weintroduce an eager-mode BA framework seamlessly integrated with PyPoseproviding PyTorch-compatible interfaces with high efficiency. Our approachincludes GPU-accelerated differentiable and sparse operations designed for2nd-order optimization Lie group and Lie algebra operations and linearsolvers. Our eager-mode BA on GPU demonstrates substantial runtime efficiencyachieving an average speedup of 18.5times 22times and 23timescompared to GTSAM g2o and Ceres respectively.</p>
                <p>Last Updated: 2024-09-18 17:59:29 UTC</p>
                <button class="interpret-button" data-id="2409.12190v1">Interpret</button>
                <div id="interpretation-2409.12190v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Massively Multi-Person 3D Human Motion Forecasting with Scene Context</h3>
                <p>Authors: Felix B MuellerJulian TankeJuergen Gall</p>
                <p><a href="http://arxiv.org/abs/2409.12189v1">Link to paper</a></p>
                <p>Forecasting long-term 3D human motion is challenging: the stochasticity ofhuman behavior makes it hard to generate realistic human motion from the inputsequence alone. Information on the scene environment and the motion of nearbypeople can greatly aid the generation process. We propose a scene-aware socialtransformer model SAST to forecast long-term 10s human motion motion.Unlike previous models our approach can model interactions between both widelyvarying numbers of people and objects in a scene. We combine a temporalconvolutional encoder-decoder architecture with a Transformer-based bottleneckthat allows us to efficiently combine motion and scene information. We modelthe conditional motion distribution using denoising diffusion models. Webenchmark our approach on the Humans in Kitchens dataset which contains 1 to16 persons and 29 to 50 objects that are visible simultaneously. Our modeloutperforms other approaches in terms of realism and diversity on differentmetrics and in a user study. Code is available athttps://github.com/felixbmuller/SAST.</p>
                <p>Last Updated: 2024-09-18 17:58:51 UTC</p>
                <button class="interpret-button" data-id="2409.12189v1">Interpret</button>
                <div id="interpretation-2409.12189v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-09-21</p>
        </div>
    
        </div>
    </body>
    </html>
    