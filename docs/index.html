
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>A New Perspective on Shampoo's Preconditioner</h3>
                <p>Authors: Depen MorwaniItai ShapiraNikhil VyasEran MalachSham KakadeLucas Janson</p>
                <p><a href="http://arxiv.org/abs/2406.17748v1">Link to paper</a></p>
                <p>Shampoo a second-order optimization algorithm which uses a Kronecker productpreconditioner has recently garnered increasing attention from the machinelearning community. The preconditioner used by Shampoo can be viewed either asan approximation of the Gauss--Newton component of the Hessian or thecovariance matrix of the gradients maintained by Adagrad. We provide anexplicit and novel connection between the textitoptimal Kronecker productapproximation of these matrices and the approximation made by Shampoo. Ourconnection highlights a subtle but common misconception about Shampoosapproximation. In particular the textitsquare of the approximation usedby the Shampoo optimizer is equivalent to a single step of the power iterationalgorithm for computing the aforementioned optimal Kronecker productapproximation. Across a variety of datasets and architectures we empiricallydemonstrate that this is close to the optimal Kronecker product approximation.Additionally for the Hessian approximation viewpoint we empirically study theimpact of various practical tricks to make Shampoo more computationallyefficient such as using the batch gradient and the empirical Fisher on thequality of Hessian approximation.</p>
                <p>Last Updated: 2024-06-25 17:34:51 UTC</p>
                <button class="interpret-button" data-id="2406.17748v1">Interpret</button>
                <div id="interpretation-2406.17748v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Probing the effects of broken symmetries in machine learning</h3>
                <p>Authors: Marcel F. LangerSergey N. PozdnyakovMichele Ceriotti</p>
                <p><a href="http://arxiv.org/abs/2406.17747v1">Link to paper</a></p>
                <p>Symmetry is one of the most central concepts in physics and it is nosurprise that it has also been widely adopted as an inductive bias formachine-learning models applied to the physical sciences. This is especiallytrue for models targeting the properties of matter at the atomic scale. Bothestablished and state-of-the-art approaches with almost no exceptions arebuilt to be exactly equivariant to translations permutations and rotations ofthe atoms. Incorporating symmetries -- rotations in particular -- constrainsthe model design space and implies more complicated architectures that areoften also computationally demanding. There are indications that non-symmetricmodels can easily learn symmetries from data and that doing so can even bebeneficial for the accuracy of the model. We put a model that obeys rotationalinvariance only approximately to the test in realistic scenarios involvingsimulations of gas-phase liquid and solid water. We focus specifically onphysical observables that are likely to be affected -- directly or indirectly-- by symmetry breaking finding negligible consequences when the model is usedin an interpolative bulk regime. Even for extrapolative gas-phasepredictions the model remains very stable even though symmetry artifacts arenoticeable. We also discuss strategies that can be used to systematicallyreduce the magnitude of symmetry breaking when it occurs and assess theirimpact on the convergence of observables.</p>
                <p>Last Updated: 2024-06-25 17:34:09 UTC</p>
                <button class="interpret-button" data-id="2406.17747v1">Interpret</button>
                <div id="interpretation-2406.17747v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Identifying Nonstationary Causal Structures with High-Order Markov Switching Models</h3>
                <p>Authors: Carles Balsells-RodasYixin WangPedro A. M. MedianoYingzhen Li</p>
                <p><a href="http://arxiv.org/abs/2406.17698v1">Link to paper</a></p>
                <p>Causal discovery in time series is a rapidly evolving field with a widevariety of applications in other areas such as climate science andneuroscience. Traditional approaches assume a stationary causal graph whichcan be adapted to nonstationary time series with time-dependent effects orheterogeneous noise. In this work we address nonstationarity viaregime-dependent causal structures. We first establish identifiability forhigh-order Markov Switching Models which provide the foundations foridentifiable regime-dependent causal discovery. Our empirical studiesdemonstrate the scalability of our proposed approach for high-orderregime-dependent structure estimation and we illustrate its applicability onbrain activity data.</p>
                <p>Last Updated: 2024-06-25 16:38:27 UTC</p>
                <button class="interpret-button" data-id="2406.17698v1">Interpret</button>
                <div id="interpretation-2406.17698v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Diffusion-based Adversarial Purification for Intrusion Detection</h3>
                <p>Authors: Mohamed Amine MerzoukErwan BeurierReda YaichNora Boulahia-CuppensFrédéric Cuppens</p>
                <p><a href="http://arxiv.org/abs/2406.17606v1">Link to paper</a></p>
                <p>The escalating sophistication of cyberattacks has encouraged the integrationof machine learning techniques in intrusion detection systems but the rise ofadversarial examples presents a significant challenge. These craftedperturbations mislead ML models enabling attackers to evade detection ortrigger false alerts. As a reaction adversarial purification has emerged as acompelling solution particularly with diffusion models showing promisingresults. However their purification potential remains unexplored in thecontext of intrusion detection. This paper demonstrates the effectiveness ofdiffusion models in purifying adversarial examples in network intrusiondetection. Through a comprehensive analysis of the diffusion parameters weidentify optimal configurations maximizing adversarial robustness with minimalimpact on normal performance. Importantly this study reveals insights into therelationship between diffusion noise and diffusion steps representing a novelcontribution to the field. Our experiments are carried out on two datasets andagainst 5 adversarial attacks. The implementation code is publicly available.</p>
                <p>Last Updated: 2024-06-25 14:48:28 UTC</p>
                <button class="interpret-button" data-id="2406.17606v1">Interpret</button>
                <div id="interpretation-2406.17606v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Causal Responder Detection</h3>
                <p>Authors: Tzviel FrostigOshri MachlufAmitay KamberElad BerkmanRaviv Pryluk</p>
                <p><a href="http://arxiv.org/abs/2406.17571v1">Link to paper</a></p>
                <p>We introduce the causal responders detection CARD a novel method forresponder analysis that identifies treated subjects who significantly respondto a treatment. Leveraging recent advances in conformal prediction CARDemploys machine learning techniques to accurately identify responders whilecontrolling the false discovery rate in finite sample sizes. Additionally weincorporate a propensity score adjustment to mitigate bias arising fromnon-random treatment allocation enhancing the robustness of our method inobservational settings. Simulation studies demonstrate that CARD effectivelydetects responders with high power in diverse scenarios.</p>
                <p>Last Updated: 2024-06-25 14:09:06 UTC</p>
                <button class="interpret-button" data-id="2406.17571v1">Interpret</button>
                <div id="interpretation-2406.17571v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>ELIZA Reinterpreted: The world's first chatbot was not intended as a chatbot at all</h3>
                <p>Authors: Jeff Shrager</p>
                <p><a href="http://arxiv.org/abs/2406.17650v1">Link to paper</a></p>
                <p>ELIZA often considered the worlds first chatbot was written by JosephWeizenbaum in the early 1960s. Weizenbaum did not intend to invent the chatbotbut rather to build a platform for research into human-machine conversation andthe important cognitive processes of interpretation and misinterpretation. Hispurpose was obscured by ELIZAs fame resulting in large part from thefortuitous timing of its creation and its escape into the wild. In thispaper I provide a rich historical context for ELIZAs creation demonstratingthat ELIZA arose from the intersection of some of the central threads in thetechnical history of AI. I also briefly discuss how ELIZA escaped into theworld and how its accidental escape along with several coincidental turns ofthe programming language screws led both to the misapprehension that ELIZA wasintended as a chatbot and to the loss of the original ELIZA to history forover 50 years.</p>
                <p>Last Updated: 2024-06-25 15:41:40 UTC</p>
                <button class="interpret-button" data-id="2406.17650v1">Interpret</button>
                <div id="interpretation-2406.17650v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The experience of humans' and robots' mutual (im)politeness in enacted service scenarios: An empirical study</h3>
                <p>Authors: Victor KaptelininSuna BenschThomas HellströmPatrik BjörnfotShikhar Kumar</p>
                <p><a href="http://arxiv.org/abs/2406.17641v1">Link to paper</a></p>
                <p>The paper reports an empirical study of the effect of human treatment of arobot on the social perception of the robots behavior. The study employed anenacted interaction between an anthropomorphic waiter robot and twocustomers. The robot and one of the customers acted out by a researcher werefollowing four different interaction scripts representing all combinations ofmutual politeness and impoliteness of the robot and the customer. Theparticipants N24 within-subject design were assigned the role of anincluded observer that is a fellow customer who was present in thesituation without being actively involved in the interactions. The participantsassessed how they experienced the interaction scenarios by providing Likertscale scores and free-text responses. The results indicate that while impoliterobots behavior was generally assessed negatively it was commonly perceivedas more justifiable and fairer if the robot was treated impolitely by thehuman. Politeness reciprocity expectations in the context of the socialperception of robots are discussed.</p>
                <p>Last Updated: 2024-06-25 15:24:08 UTC</p>
                <button class="interpret-button" data-id="2406.17641v1">Interpret</button>
                <div id="interpretation-2406.17641v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness</h3>
                <p>Authors: Lucrezia GrassiCarmine Tommaso RecchiutoAntonio Sgorbissa</p>
                <p><a href="http://arxiv.org/abs/2406.17531v1">Link to paper</a></p>
                <p>This paper presents a system for diversity-aware autonomous conversationleveraging the capabilities of large language models LLMs. The system adaptsto diverse populations and individuals considering factors like backgroundpersonality age gender and culture. The conversation flow is guided by thestructure of the systems pre-established knowledge base while LLMs are taskedwith various functions including generating diversity-aware sentences.Achieving diversity-awareness involves providing carefully crafted prompts tothe models incorporating comprehensive information about users conversationhistory contextual details and specific guidelines. To assess the systemsperformance we conducted both controlled and real-world experiments measuringa wide range of performance indicators.</p>
                <p>Last Updated: 2024-06-25 13:15:36 UTC</p>
                <button class="interpret-button" data-id="2406.17531v1">Interpret</button>
                <div id="interpretation-2406.17531v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>FacePsy: An Open-Source Affective Mobile Sensing System -- Analyzing Facial Behavior and Head Gesture for Depression Detection in Naturalistic Settings</h3>
                <p>Authors: Rahul IslamSang Won Bae</p>
                <p><a href="http://arxiv.org/abs/2406.17181v1">Link to paper</a></p>
                <p>Depression a prevalent and complex mental health issue affecting millionsworldwide presents significant challenges for detection and monitoring. Whilefacial expressions have shown promise in laboratory settings for identifyingdepression their potential in real-world applications remains largelyunexplored due to the difficulties in developing efficient mobile systems. Inthis study we aim to introduce FacePsy an open-source mobile sensing systemdesigned to capture affective inferences by analyzing sophisticated featuresand generating real-time data on facial behavior landmarks eye movements andhead gestures -- all within the naturalistic context of smartphone usage with25 participants. Through rigorous development testing and optimization weidentified eye-open states head gestures smile expressions and specificAction Units 2 6 7 12 15 and 17 as significant indicators of depressiveepisodes AUROC81. Our regression model predicting PHQ-9 scores achievedmoderate accuracy with a Mean Absolute Error of 3.08. Our findings offervaluable insights and implications for enhancing deployable and usable mobileaffective sensing systems ultimately improving mental health monitoringprediction and just-in-time adaptive interventions for researchers anddevelopers in healthcare.</p>
                <p>Last Updated: 2024-06-24 23:40:19 UTC</p>
                <button class="interpret-button" data-id="2406.17181v1">Interpret</button>
                <div id="interpretation-2406.17181v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Toward Ubiquitous 3D Object Digitization: A Wearable Computing Framework for Non-Invasive Physical Property Acquisition</h3>
                <p>Authors: Yunxiang ZhangXin SunDengfeng LiXinge YuQi Sun</p>
                <p><a href="http://arxiv.org/abs/2406.17156v1">Link to paper</a></p>
                <p>Accurately digitizing physical objects is central to many applicationsincluding virtual/augmented reality industrial design and e-commerce. Priorresearch has demonstrated efficient and faithful reconstruction of objectsgeometric shapes and visual appearances which suffice for digitallyrepresenting rigid objects. In comparison physical properties such aselasticity and pressure are also indispensable to the behavioral fidelity ofdigitized deformable objects. However existing approaches to acquiring thesequantities either rely on invasive specimen collection or expensive/bulkylaboratory setups making them inapplicable to consumer-level usage.  To fill this gap we propose a wearable and non-invasive computing frameworkthat allows users to conveniently estimate the material elasticity and internalpressure of deformable objects through finger touches. This is achieved bymodeling their local surfaces as pressurized elastic shells and analyticallyderiving the two physical properties from finger-induced wrinkling patterns.Together with photogrammetry-reconstructed geometry and textures the twoestimated physical properties enable us to faithfully replicate the motion anddeformation behaviors of several deformable objects. For the pressureestimation our model achieves a relative error of 3.5. In the interactionexperiments the virtual-physical deformation discrepancy measures less than10.1. Generalization to objects of irregular shape further demonstrates thepotential of our approach in practical applications. We envision this work toprovide insights for and motivate research toward democratizing the ubiquitousand pervasive digitization of our physical surroundings in daily industrialand scientific scenarios.</p>
                <p>Last Updated: 2024-06-24 22:02:10 UTC</p>
                <button class="interpret-button" data-id="2406.17156v1">Interpret</button>
                <div id="interpretation-2406.17156v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Text-Animator: Controllable Visual Text Video Generation</h3>
                <p>Authors: Lin LiuQuande LiuShengju QianYuan ZhouWengang ZhouHouqiang LiLingxi XieQi Tian</p>
                <p><a href="http://arxiv.org/abs/2406.17777v1">Link to paper</a></p>
                <p>Video generation is a challenging yet pivotal task in various industriessuch as gaming e-commerce and advertising. One significant unresolved aspectwithin T2V is the effective visualization of text within generated videos.Despite the progress achieved in Text-to-VideoT2V generation currentmethods still cannot effectively visualize texts in videos directly as theymainly focus on summarizing semantic scene information understanding anddepicting actions. While recent advances in image-level visual text generationshow promise transitioning these techniques into the video domain facesproblems notably in preserving textual fidelity and motion coherence. In thispaper we propose an innovative approach termed Text-Animator for visual textvideo generation. Text-Animator contains a text embedding injection module toprecisely depict the structures of visual text in generated videos. Besides wedevelop a camera control module and a text refinement module to improve thestability of generated visual text by controlling the camera movement as wellas the motion of visualized text. Quantitative and qualitative experimentalresults demonstrate the superiority of our approach to the accuracy ofgenerated visual text over state-of-the-art video generation methods. Theproject page can be found at https://laulampaul.github.io/text-animator.html.</p>
                <p>Last Updated: 2024-06-25 17:59:41 UTC</p>
                <button class="interpret-button" data-id="2406.17777v1">Interpret</button>
                <div id="interpretation-2406.17777v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fast and Uncertainty-Aware SVBRDF Recovery from Multi-View Capture using Frequency Domain Analysis</h3>
                <p>Authors: Ruben WiersmaJulien PhilipMiloš HašanKrishna MulliaFujun LuanElmar EisemannValentin Deschaintre</p>
                <p><a href="http://arxiv.org/abs/2406.17774v1">Link to paper</a></p>
                <p>Relightable object acquisition is a key challenge in simplifying digitalasset creation. Complete reconstruction of an object typically requirescapturing hundreds to thousands of photographs under controlled illuminationwith specialized equipment. The recent progress in differentiable renderingimproved the quality and accessibility of inverse rendering optimization.Nevertheless under uncontrolled illumination and unstructured viewpointsthere is no guarantee that the observations contain enough information toreconstruct the appearance properties of the captured object.  We thus propose to consider the acquisition process from a signal-processingperspective. Given an objects geometry and a lighting environment we estimatethe properties of the materials on the objects surface in seconds. We do so byleveraging frequency domain analysis considering the recovery of materialproperties as a deconvolution enabling fast error estimation. We then quantifythe uncertainty of the estimation based on the available data highlightingthe areas for which priors or additional samples would be required for improvedacquisition quality. We compare our approach to previous work andquantitatively evaluate our results showing similar quality as previous workin a fraction of the time and providing key information about the certainty ofthe results.</p>
                <p>Last Updated: 2024-06-25 17:59:06 UTC</p>
                <button class="interpret-button" data-id="2406.17774v1">Interpret</button>
                <div id="interpretation-2406.17774v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning</h3>
                <p>Authors: Xiangyu ZhaoXiangtai LiHaodong DuanHaian HuangYining LiKai ChenHua Yang</p>
                <p><a href="http://arxiv.org/abs/2406.17770v1">Link to paper</a></p>
                <p>Multi-modal large language models MLLMs have made significant strides invarious visual understanding tasks. However the majority of these models areconstrained to process low-resolution images which limits their effectivenessin perception tasks that necessitate detailed visual information. In our studywe present MG-LLaVA an innovative MLLM that enhances the models visualprocessing capabilities by incorporating a multi-granularity vision flow whichincludes low-resolution high-resolution and object-centric features. Wepropose the integration of an additional high-resolution visual encoder tocapture fine-grained details which are then fused with base visual featuresthrough a Conv-Gate fusion network. To further refine the models objectrecognition abilities we incorporate object-level features derived frombounding boxes identified by offline detectors. Being trained solely onpublicly available multimodal data through instruction tuning MG-LLaVAdemonstrates exceptional perception skills. We instantiate MG-LLaVA with a widevariety of language encoders ranging from 3.8B to 34B to evaluate the modelsperformance comprehensively. Extensive evaluations across multiple benchmarksdemonstrate that MG-LLaVA outperforms existing MLLMs of comparable parametersizes showcasing its remarkable efficacy. The code will be available athttps://github.com/PhoenixZ810/MG-LLaVA.</p>
                <p>Last Updated: 2024-06-25 17:55:11 UTC</p>
                <button class="interpret-button" data-id="2406.17770v1">Interpret</button>
                <div id="interpretation-2406.17770v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DiffusionPDE: Generative PDE-Solving Under Partial Observation</h3>
                <p>Authors: Jiahe HuangGuandao YangZichen WangJeong Joon Park</p>
                <p><a href="http://arxiv.org/abs/2406.17763v1">Link to paper</a></p>
                <p>We introduce a general framework for solving partial differential equationsPDEs using generative diffusion models. In particular we focus on thescenarios where we do not have the full knowledge of the scene necessary toapply classical solvers. Most existing forward or inverse PDE approachesperform poorly when the observations on the data or the underlying coefficientsare incomplete which is a common assumption for real-world measurements. Inthis work we propose DiffusionPDE that can simultaneously fill in the missinginformation and solve a PDE by modeling the joint distribution of the solutionand coefficient spaces. We show that the learned generative priors lead to aversatile framework for accurately solving a wide range of PDEs under partialobservation significantly outperforming the state-of-the-art methods for bothforward and inverse directions.</p>
                <p>Last Updated: 2024-06-25 17:48:24 UTC</p>
                <button class="interpret-button" data-id="2406.17763v1">Interpret</button>
                <div id="interpretation-2406.17763v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MotionBooth: Motion-Aware Customized Text-to-Video Generation</h3>
                <p>Authors: Jianzong WuXiangtai LiYanhong ZengJiangning ZhangQianyu ZhouYining LiYunhai TongKai Chen</p>
                <p><a href="http://arxiv.org/abs/2406.17758v1">Link to paper</a></p>
                <p>In this work we present MotionBooth an innovative framework designed foranimating customized subjects with precise control over both object and cameramovements. By leveraging a few images of a specific object we efficientlyfine-tune a text-to-video model to capture the objects shape and attributesaccurately. Our approach presents subject region loss and video preservationloss to enhance the subjects learning performance along with a subject tokencross-attention loss to integrate the customized subject with motion controlsignals. Additionally we propose training-free techniques for managing subjectand camera motions during inference. In particular we utilize cross-attentionmap manipulation to govern subject motion and introduce a novel latent shiftmodule for camera movement control as well. MotionBooth excels in preservingthe appearance of subjects while simultaneously controlling the motions ingenerated videos. Extensive quantitative and qualitative evaluationsdemonstrate the superiority and effectiveness of our method. Our project pageis at https://jianzongwu.github.io/projects/motionbooth</p>
                <p>Last Updated: 2024-06-25 17:42:25 UTC</p>
                <button class="interpret-button" data-id="2406.17758v1">Interpret</button>
                <div id="interpretation-2406.17758v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot Skills from Offline Data</h3>
                <p>Authors: Jesse ZhangMinho HeoZuxin LiuErdem BiyikJoseph J LimYao LiuRasool Fakoor</p>
                <p><a href="http://arxiv.org/abs/2406.17768v1">Link to paper</a></p>
                <p>Most reinforcement learning RL methods focus on learning optimal policiesover low-level action spaces. While these methods can perform well in theirtraining environments they lack the flexibility to transfer to new tasks.Instead RL agents that can act over useful temporally extended skills ratherthan low-level actions can learn new tasks more easily. Prior work inskill-based RL either requires expert supervision to define useful skillswhich is hard to scale or learns a skill-space from offline data withheuristics that limit the adaptability of the skills making them difficult totransfer during downstream RL. Our approach EXTRACT instead utilizespre-trained vision language models to extract a discrete set of semanticallymeaningful skills from offline data each of which is parameterized bycontinuous arguments without human supervision. This skill parameterizationallows robots to learn new tasks by only needing to learn when to select aspecific skill and how to modify its arguments for the specific task. Wedemonstrate through experiments in sparse-reward image-based robotmanipulation environments that EXTRACT can more quickly learn new tasks thanprior works with major gains in sample efficiency and performance over priorskill-based RL. Website at https://www.jessezhang.net/projects/extract/.</p>
                <p>Last Updated: 2024-06-25 17:50:03 UTC</p>
                <button class="interpret-button" data-id="2406.17768v1">Interpret</button>
                <div id="interpretation-2406.17768v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning</h3>
                <p>Authors: Ercong NieBo ShaoZifeng DingMingyang WangHelmut SchmidHinrich Schütze</p>
                <p><a href="http://arxiv.org/abs/2406.17764v1">Link to paper</a></p>
                <p>Large language models LLMs possess extensive parametric knowledge but thisknowledge is difficult to update with new information because retraining isvery expensive and infeasible for closed-source models. Knowledge editing KEhas emerged as a viable solution for updating the knowledge of LLMs withoutcompromising their overall performance. On-the-fly KE methods inspired byin-context learning ICL have shown great promise and allow LLMs to betreated as black boxes. In the past KE was primarily employed in Englishcontexts whereas the potential for cross-lingual KE in current English-centricLLMs has not been fully explored. To foster more research in this direction weintroduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverselanguages across three KE task types. We also propose a gradient-free KE methodcalled Multilingual In-context Knowledge Editing MIKE and evaluate it onBMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in termsof reliability generality locality and portability offering valuableinsights and a framework for future research in cross-lingual KE. Our code anddata are publicly accessible via the anonymous repository athttps://anonymous.4open.science/r/MIKE.</p>
                <p>Last Updated: 2024-06-25 17:48:56 UTC</p>
                <button class="interpret-button" data-id="2406.17764v1">Interpret</button>
                <div id="interpretation-2406.17764v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DiffusionPDE: Generative PDE-Solving Under Partial Observation</h3>
                <p>Authors: Jiahe HuangGuandao YangZichen WangJeong Joon Park</p>
                <p><a href="http://arxiv.org/abs/2406.17763v1">Link to paper</a></p>
                <p>We introduce a general framework for solving partial differential equationsPDEs using generative diffusion models. In particular we focus on thescenarios where we do not have the full knowledge of the scene necessary toapply classical solvers. Most existing forward or inverse PDE approachesperform poorly when the observations on the data or the underlying coefficientsare incomplete which is a common assumption for real-world measurements. Inthis work we propose DiffusionPDE that can simultaneously fill in the missinginformation and solve a PDE by modeling the joint distribution of the solutionand coefficient spaces. We show that the learned generative priors lead to aversatile framework for accurately solving a wide range of PDEs under partialobservation significantly outperforming the state-of-the-art methods for bothforward and inverse directions.</p>
                <p>Last Updated: 2024-06-25 17:48:24 UTC</p>
                <button class="interpret-button" data-id="2406.17763v1">Interpret</button>
                <div id="interpretation-2406.17763v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Solving Hard Mizar Problems with Instantiation and Strategy Invention</h3>
                <p>Authors: Jan JakubůvMikoláš JanotaJosef Urban</p>
                <p><a href="http://arxiv.org/abs/2406.17762v1">Link to paper</a></p>
                <p>In this work we prove over 3000 previously ATP-unproved Mizar/MPTP problemsby using several ATP and AI methods raising the number of ATP-solved Mizarproblems from 75 to above 80. First we start to experiment with the cvc5SMT solver which uses several instantiation-based heuristics that differ fromthe superposition-based systems that were previously applied to Mizarand addmany new solutions. Then we use automated strategy invention to develop cvc5strategies that largely improve cvc5s performance on the hard problems. Inparticular the best invented strategy solves over 14 more problems than thebest previously available cvc5 strategy. We also show that differentclausification methods have a high impact on such instantiation-based methodsagain producing many new solutions. In total the methods solve 3021 21.3of the 14163 previously unsolved hard Mizar problems. This is a new milestoneover the Mizar large-theory benchmark and a large strengthening of the hammermethods for Mizar.</p>
                <p>Last Updated: 2024-06-25 17:47:13 UTC</p>
                <button class="interpret-button" data-id="2406.17762v1">Interpret</button>
                <div id="interpretation-2406.17762v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CaLMQA: Exploring culturally specific long-form question answering across 23 languages</h3>
                <p>Authors: Shane AroraMarzena KarpinskaHung-Ting ChenIpsita BhattacharjeeMohit IyyerEunsol Choi</p>
                <p><a href="http://arxiv.org/abs/2406.17761v1">Link to paper</a></p>
                <p>Large language models LLMs are commonly used for long-form questionanswering which requires them to generate paragraph-length answers to complexquestions. While long-form QA has been well-studied in English via manydifferent datasets and evaluation metrics this research has not been extendedto cover most other languages. To bridge this gap we introduce CaLMQA acollection of 2.6K complex questions spanning 23 languages includingunder-resourced rarely-studied languages such as Fijian and Kirundi. Ourdataset includes both naturally-occurring questions collected from communityweb forums as well as questions written by native speakers whom we hire forthis purpose. Our process yields diverse complex questions that reflectcultural topics e.g. traditions laws news and the language usage of nativespeakers. We conduct automatic evaluation across a suite of open- andclosed-source models using our novel metric CaLMScore which detects incorrectlanguage and token repetitions in answers and observe that the quality ofLLM-generated answers degrades significantly for some low-resource languages.We perform human evaluation on a subset of models and see that modelperformance is significantly worse for culturally specific questions than forculturally agnostic questions. Our findings highlight the need for furtherresearch in LLM multilingual capabilities and non-English LFQA evaluation.</p>
                <p>Last Updated: 2024-06-25 17:45:26 UTC</p>
                <button class="interpret-button" data-id="2406.17761v1">Interpret</button>
                <div id="interpretation-2406.17761v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning</h3>
                <p>Authors: Ercong NieBo ShaoZifeng DingMingyang WangHelmut SchmidHinrich Schütze</p>
                <p><a href="http://arxiv.org/abs/2406.17764v1">Link to paper</a></p>
                <p>Large language models LLMs possess extensive parametric knowledge but thisknowledge is difficult to update with new information because retraining isvery expensive and infeasible for closed-source models. Knowledge editing KEhas emerged as a viable solution for updating the knowledge of LLMs withoutcompromising their overall performance. On-the-fly KE methods inspired byin-context learning ICL have shown great promise and allow LLMs to betreated as black boxes. In the past KE was primarily employed in Englishcontexts whereas the potential for cross-lingual KE in current English-centricLLMs has not been fully explored. To foster more research in this direction weintroduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverselanguages across three KE task types. We also propose a gradient-free KE methodcalled Multilingual In-context Knowledge Editing MIKE and evaluate it onBMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in termsof reliability generality locality and portability offering valuableinsights and a framework for future research in cross-lingual KE. Our code anddata are publicly accessible via the anonymous repository athttps://anonymous.4open.science/r/MIKE.</p>
                <p>Last Updated: 2024-06-25 17:48:56 UTC</p>
                <button class="interpret-button" data-id="2406.17764v1">Interpret</button>
                <div id="interpretation-2406.17764v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CaLMQA: Exploring culturally specific long-form question answering across 23 languages</h3>
                <p>Authors: Shane AroraMarzena KarpinskaHung-Ting ChenIpsita BhattacharjeeMohit IyyerEunsol Choi</p>
                <p><a href="http://arxiv.org/abs/2406.17761v1">Link to paper</a></p>
                <p>Large language models LLMs are commonly used for long-form questionanswering which requires them to generate paragraph-length answers to complexquestions. While long-form QA has been well-studied in English via manydifferent datasets and evaluation metrics this research has not been extendedto cover most other languages. To bridge this gap we introduce CaLMQA acollection of 2.6K complex questions spanning 23 languages includingunder-resourced rarely-studied languages such as Fijian and Kirundi. Ourdataset includes both naturally-occurring questions collected from communityweb forums as well as questions written by native speakers whom we hire forthis purpose. Our process yields diverse complex questions that reflectcultural topics e.g. traditions laws news and the language usage of nativespeakers. We conduct automatic evaluation across a suite of open- andclosed-source models using our novel metric CaLMScore which detects incorrectlanguage and token repetitions in answers and observe that the quality ofLLM-generated answers degrades significantly for some low-resource languages.We perform human evaluation on a subset of models and see that modelperformance is significantly worse for culturally specific questions than forculturally agnostic questions. Our findings highlight the need for furtherresearch in LLM multilingual capabilities and non-English LFQA evaluation.</p>
                <p>Last Updated: 2024-06-25 17:45:26 UTC</p>
                <button class="interpret-button" data-id="2406.17761v1">Interpret</button>
                <div id="interpretation-2406.17761v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Accelerating Clinical Evidence Synthesis with Large Language Models</h3>
                <p>Authors: Zifeng WangLang CaoBenjamin DanekYichi ZhangQiao JinZhiyong LuJimeng Sun</p>
                <p><a href="http://arxiv.org/abs/2406.17755v1">Link to paper</a></p>
                <p>Automatic medical discovery by AI is a dream of many. One step toward thatgoal is to create an AI model to understand clinical studies and synthesizeclinical evidence from the literature. Clinical evidence synthesis currentlyrelies on systematic reviews of clinical trials and retrospective analyses frommedical literature. However the rapid expansion of publications presentschallenges in efficiently identifying summarizing and updating evidence. Weintroduce TrialMind a generative AI-based pipeline for conducting medicalsystematic reviews encompassing study search screening and data extractionphases. We utilize large language models LLMs to drive each pipelinecomponent while incorporating human expert oversight to minimize errors. Tofacilitate evaluation we also create a benchmark dataset TrialReviewBench acustom dataset with 870 annotated clinical studies from 25 meta-analysis papersacross various medical treatments. Our results demonstrate that TrialMindsignificantly improves the literature review process achieving high recallrates 0.897-1.000 in study searching from over 20 million PubMed studies andoutperforming traditional language model embeddings-based methods in screeningRecall20 of 0.227-0.246 vs. 0.000-0.102. Furthermore our approach surpassesdirect GPT-4 performance in result extraction with accuracy ranging from 0.65to 0.84. We also support clinical evidence synthesis in forest plots asvalidated by eight human annotators who preferred TrialMind over the GPT-4baseline with a winning rate of 62.5-100 across the involved reviews. Ourfindings suggest that an LLM-based clinical evidence synthesis approach suchas TrialMind can enable reliable and high-quality clinical evidence synthesisto improve clinical research efficiency.</p>
                <p>Last Updated: 2024-06-25 17:41:52 UTC</p>
                <button class="interpret-button" data-id="2406.17755v1">Interpret</button>
                <div id="interpretation-2406.17755v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language</h3>
                <p>Authors: Amalie Brogaard PauliIsabelle AugensteinIra Assent</p>
                <p><a href="http://arxiv.org/abs/2406.17753v1">Link to paper</a></p>
                <p>We are exposed to much information trying to influence us such as teasermessages debates politically framed news and propaganda - all of which usepersuasive language. With the recent interest in Large Language Models LLMswe study the ability of LLMs to produce persuasive text. As opposed to priorwork which focuses on particular domains or types of persuasion we conduct ageneral study across various domains to measure and benchmark to what degreeLLMs produce persuasive text - both when explicitly instructed to rewrite textto be more or less persuasive and when only instructed to paraphrase. To thisend we construct a new dataset Persuasive-Pairs of pairs each consisting ofa short text and of a text rewritten by an LLM to amplify or diminishpersuasive language. We multi-annotate the pairs on a relative scale forpersuasive language. This data is not only a valuable resource in itself butwe also show that it can be used to train a regression model to predict a scoreof persuasive language between text pairs. This model can score and benchmarknew LLMs across domains thereby facilitating the comparison of different LLMs.Finally we discuss effects observed for different system prompts. Notably wefind that different personas in the system prompt of LLaMA3 change thepersuasive language in the text substantially even when only instructed toparaphrase. These findings underscore the importance of investigatingpersuasive language in LLM generated text.</p>
                <p>Last Updated: 2024-06-25 17:40:47 UTC</p>
                <button class="interpret-button" data-id="2406.17753v1">Interpret</button>
                <div id="interpretation-2406.17753v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon</h3>
                <p>Authors: USVSN Sai PrashanthAlvin DengKyle O'BrienJyothir S VMohammad Aflah KhanJaydeep BorkarChristopher A. Choquette-ChooJacob Ray FuehneStella BidermanTracy KeKatherine LeeNaomi Saphra</p>
                <p><a href="http://arxiv.org/abs/2406.17746v1">Link to paper</a></p>
                <p>Memorization in language models is typically treated as a homogenousphenomenon neglecting the specifics of the memorized data. We instead modelmemorization as the effect of a set of complex factors that describe eachsample and relate it to the model and corpus. To build intuition around thesefactors we break memorization down into a taxonomy: recitation of highlyduplicated sequences reconstruction of inherently predictable sequences andrecollection of sequences that are neither. We demonstrate the usefulness ofour taxonomy by using it to construct a predictive model for memorization. Byanalyzing dependencies and inspecting the weights of the predictive model wefind that different factors influence the likelihood of memorizationdifferently depending on the taxonomic category.</p>
                <p>Last Updated: 2024-06-25 17:32:16 UTC</p>
                <button class="interpret-button" data-id="2406.17746v1">Interpret</button>
                <div id="interpretation-2406.17746v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot Skills from Offline Data</h3>
                <p>Authors: Jesse ZhangMinho HeoZuxin LiuErdem BiyikJoseph J LimYao LiuRasool Fakoor</p>
                <p><a href="http://arxiv.org/abs/2406.17768v1">Link to paper</a></p>
                <p>Most reinforcement learning RL methods focus on learning optimal policiesover low-level action spaces. While these methods can perform well in theirtraining environments they lack the flexibility to transfer to new tasks.Instead RL agents that can act over useful temporally extended skills ratherthan low-level actions can learn new tasks more easily. Prior work inskill-based RL either requires expert supervision to define useful skillswhich is hard to scale or learns a skill-space from offline data withheuristics that limit the adaptability of the skills making them difficult totransfer during downstream RL. Our approach EXTRACT instead utilizespre-trained vision language models to extract a discrete set of semanticallymeaningful skills from offline data each of which is parameterized bycontinuous arguments without human supervision. This skill parameterizationallows robots to learn new tasks by only needing to learn when to select aspecific skill and how to modify its arguments for the specific task. Wedemonstrate through experiments in sparse-reward image-based robotmanipulation environments that EXTRACT can more quickly learn new tasks thanprior works with major gains in sample efficiency and performance over priorskill-based RL. Website at https://www.jessezhang.net/projects/extract/.</p>
                <p>Last Updated: 2024-06-25 17:50:03 UTC</p>
                <button class="interpret-button" data-id="2406.17768v1">Interpret</button>
                <div id="interpretation-2406.17768v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DiffusionPDE: Generative PDE-Solving Under Partial Observation</h3>
                <p>Authors: Jiahe HuangGuandao YangZichen WangJeong Joon Park</p>
                <p><a href="http://arxiv.org/abs/2406.17763v1">Link to paper</a></p>
                <p>We introduce a general framework for solving partial differential equationsPDEs using generative diffusion models. In particular we focus on thescenarios where we do not have the full knowledge of the scene necessary toapply classical solvers. Most existing forward or inverse PDE approachesperform poorly when the observations on the data or the underlying coefficientsare incomplete which is a common assumption for real-world measurements. Inthis work we propose DiffusionPDE that can simultaneously fill in the missinginformation and solve a PDE by modeling the joint distribution of the solutionand coefficient spaces. We show that the learned generative priors lead to aversatile framework for accurately solving a wide range of PDEs under partialobservation significantly outperforming the state-of-the-art methods for bothforward and inverse directions.</p>
                <p>Last Updated: 2024-06-25 17:48:24 UTC</p>
                <button class="interpret-button" data-id="2406.17763v1">Interpret</button>
                <div id="interpretation-2406.17763v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Solving Hard Mizar Problems with Instantiation and Strategy Invention</h3>
                <p>Authors: Jan JakubůvMikoláš JanotaJosef Urban</p>
                <p><a href="http://arxiv.org/abs/2406.17762v1">Link to paper</a></p>
                <p>In this work we prove over 3000 previously ATP-unproved Mizar/MPTP problemsby using several ATP and AI methods raising the number of ATP-solved Mizarproblems from 75 to above 80. First we start to experiment with the cvc5SMT solver which uses several instantiation-based heuristics that differ fromthe superposition-based systems that were previously applied to Mizarand addmany new solutions. Then we use automated strategy invention to develop cvc5strategies that largely improve cvc5s performance on the hard problems. Inparticular the best invented strategy solves over 14 more problems than thebest previously available cvc5 strategy. We also show that differentclausification methods have a high impact on such instantiation-based methodsagain producing many new solutions. In total the methods solve 3021 21.3of the 14163 previously unsolved hard Mizar problems. This is a new milestoneover the Mizar large-theory benchmark and a large strengthening of the hammermethods for Mizar.</p>
                <p>Last Updated: 2024-06-25 17:47:13 UTC</p>
                <button class="interpret-button" data-id="2406.17762v1">Interpret</button>
                <div id="interpretation-2406.17762v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CaLMQA: Exploring culturally specific long-form question answering across 23 languages</h3>
                <p>Authors: Shane AroraMarzena KarpinskaHung-Ting ChenIpsita BhattacharjeeMohit IyyerEunsol Choi</p>
                <p><a href="http://arxiv.org/abs/2406.17761v1">Link to paper</a></p>
                <p>Large language models LLMs are commonly used for long-form questionanswering which requires them to generate paragraph-length answers to complexquestions. While long-form QA has been well-studied in English via manydifferent datasets and evaluation metrics this research has not been extendedto cover most other languages. To bridge this gap we introduce CaLMQA acollection of 2.6K complex questions spanning 23 languages includingunder-resourced rarely-studied languages such as Fijian and Kirundi. Ourdataset includes both naturally-occurring questions collected from communityweb forums as well as questions written by native speakers whom we hire forthis purpose. Our process yields diverse complex questions that reflectcultural topics e.g. traditions laws news and the language usage of nativespeakers. We conduct automatic evaluation across a suite of open- andclosed-source models using our novel metric CaLMScore which detects incorrectlanguage and token repetitions in answers and observe that the quality ofLLM-generated answers degrades significantly for some low-resource languages.We perform human evaluation on a subset of models and see that modelperformance is significantly worse for culturally specific questions than forculturally agnostic questions. Our findings highlight the need for furtherresearch in LLM multilingual capabilities and non-English LFQA evaluation.</p>
                <p>Last Updated: 2024-06-25 17:45:26 UTC</p>
                <button class="interpret-button" data-id="2406.17761v1">Interpret</button>
                <div id="interpretation-2406.17761v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Interpreting Attention Layer Outputs with Sparse Autoencoders</h3>
                <p>Authors: Connor KissaneRobert KrzyzanowskiJoseph Isaac BloomArthur ConmyNeel Nanda</p>
                <p><a href="http://arxiv.org/abs/2406.17759v1">Link to paper</a></p>
                <p>Decomposing model activations into interpretable components is a key openproblem in mechanistic interpretability. Sparse autoencoders SAEs are apopular method for decomposing the internal activations of trained transformersinto sparse interpretable features and have been applied to MLP layers andthe residual stream. In this work we train SAEs on attention layer outputs andshow that also here SAEs find a sparse interpretable decomposition. Wedemonstrate this on transformers from several model families and up to 2Bparameters.  We perform a qualitative study of the features computed by attention layersand find multiple families: long-range context short-range context andinduction features. We qualitatively study the role of every head in GPT-2Small and estimate that at least 90 of the heads are polysemantic i.e. havemultiple unrelated roles.  Further we show that Sparse Autoencoders are a useful tool that enableresearchers to explain model behavior in greater detail than prior work. Forexample we explore the mystery of why models have so many seemingly redundantinduction heads use SAEs to motivate the hypothesis that some are long-prefixwhereas others are short-prefix and confirm this with more rigorous analysis.We use our SAEs to analyze the computation performed by the Indirect ObjectIdentification circuit Wang et al. validating that the SAEs find causallymeaningful intermediate variables and deepening our understanding of thesemantics of the circuit. We open-source the trained SAEs and a tool forexploring arbitrary prompts through the lens of Attention Output SAEs.</p>
                <p>Last Updated: 2024-06-25 17:43:13 UTC</p>
                <button class="interpret-button" data-id="2406.17759v1">Interpret</button>
                <div id="interpretation-2406.17759v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Fairness in Social Influence Maximization via Optimal Transport</h3>
                <p>Authors: Shubham ChowdharyGiulia De PasqualeNicolas LanzettiAna-Andreea StoicaFlorian Dorfler</p>
                <p><a href="http://arxiv.org/abs/2406.17736v1">Link to paper</a></p>
                <p>We study fairness in social influence maximization whereby one seeks toselect seeds that spread a given information throughout a network ensuringbalanced outreach among different communities e.g. demographic groups. In theliterature fairness is often quantified in terms of the expected outreachwithin individual communities. In this paper we demonstrate that such fairnessmetrics can be misleading since they ignore the stochastic nature ofinformation diffusion processes. When information diffusion occurs in aprobabilistic manner multiple outreach scenarios can occur. As such outcomessuch as in 50 of the cases no one of group 1 receives the information andeveryone in group 2 receives it and in other 50 the opposite happens whichalways results in largely unfair outcomes are classified as fair by a varietyof fairness metrics in the literature. We tackle this problem by designing anew fairness metric mutual fairness that captures variability in outreachthrough optimal transport theory. We propose a new seed selection algorithmthat optimizes both outreach and mutual fairness and we show its efficacy onseveral real datasets. We find that our algorithm increases fairness with onlya minor decrease and at times even an increase in efficiency.</p>
                <p>Last Updated: 2024-06-25 17:24:01 UTC</p>
                <button class="interpret-button" data-id="2406.17736v1">Interpret</button>
                <div id="interpretation-2406.17736v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CuDA2: An approach for Incorporating Traitor Agents into Cooperative Multi-Agent Systems</h3>
                <p>Authors: Zhen ChenYong LiaoYoupeng ZhaoZipeng DaiJian Zhao</p>
                <p><a href="http://arxiv.org/abs/2406.17425v1">Link to paper</a></p>
                <p>Cooperative Multi-Agent Reinforcement Learning CMARL strategies are wellknown to be vulnerable to adversarial perturbations. Previous works onadversarial attacks have primarily focused on white-box attacks that directlyperturb the states or actions of victim agents often in scenarios with alimited number of attacks. However gaining complete access to victim agents inreal-world environments is exceedingly difficult. To create more realisticadversarial attacks we introduce a novel method that involves injectingtraitor agents into the CMARL system. We model this problem as a Traitor MarkovDecision Process TMDP where traitors cannot directly attack the victimagents but can influence their formation or positioning through collisions. InTMDP traitors are trained using the same MARL algorithm as the victim agentswith their reward function set as the negative of the victim agents reward.Despite this the training efficiency for traitors remains low because it ischallenging for them to directly associate their actions with the victimagents rewards. To address this issue we propose the Curiosity-DrivenAdversarial Attack CuDA2 framework. CuDA2 enhances the efficiency andaggressiveness of attacks on the specified victim agents policies whilemaintaining the optimal policy invariance of the traitors. Specifically weemploy a pre-trained Random Network Distillation RND module where the extrareward generated by the RND module encourages traitors to explore statesunencountered by the victim agents. Extensive experiments on various scenariosfrom SMAC demonstrate that our CuDA2 framework offers comparable or superioradversarial attack capabilities compared to other baselines.</p>
                <p>Last Updated: 2024-06-25 09:59:31 UTC</p>
                <button class="interpret-button" data-id="2406.17425v1">Interpret</button>
                <div id="interpretation-2406.17425v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learnings from Implementation of a BDI Agent-based Battery-less Wireless Sensor</h3>
                <p>Authors: Ganesh RamanathanAndres GomezSimon Mayer</p>
                <p><a href="http://arxiv.org/abs/2406.17303v1">Link to paper</a></p>
                <p>Battery-less embedded devices powered by energy harvesting are increasinglybeing used in wireless sensing applications. However their limited and oftenuncertain energy availability challenges designing application programs. Toexamine if BDI-based agent programming can address this challenge we used itfor a real-life application involving an environmental sensor that works onenergy harvested from ambient light. This yielded the first ever implementationof a BDI agent on a low-power battery-less and energy-harvesting embeddedsystem. Furthermore it uncovered conceptual integration challenges betweenembedded systems and BDI-based agent programming that if overcome willsimplify the deployment of more autonomous systems on low-power devices withnon-deterministic energy availability. Specifically we 1 mapped essentialdevice states to default textitinternal beliefs 2 recognized andaddressed the need for beliefs in general to be textitshort- ortextitlong-term and 3 propose dynamic annotation of intentions with theirrun-time energy impact. We show that incorporating these extensions not onlysimplified the programming but also improved code readability and understandingof its behavior.</p>
                <p>Last Updated: 2024-06-25 06:16:38 UTC</p>
                <button class="interpret-button" data-id="2406.17303v1">Interpret</button>
                <div id="interpretation-2406.17303v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Model Checking of vGOAL</h3>
                <p>Authors: Yi YangTom Holvoet</p>
                <p><a href="http://arxiv.org/abs/2406.17206v1">Link to paper</a></p>
                <p>Developing autonomous decision-making requires safety assurance. Agentprogramming languages like AgentSpeak and Gwendolen provide tools forprogramming autonomous decision-making. However despite numerous efforts toapply model checking to these languages challenges persist such as a faithfulsemantic mapping between agent programs and the generated models efficientmodel generation and efficient model checking.  As an extension of the agent programming language GOAL vGOAL has beenproposed to formally specify autonomous decisions with an emphasis on safety.This paper tackles the mentioned challenges through two automatedmodel-checking processes for vGOAL: one for Computation Tree Logic and anotherfor Probabilistic Computation Tree Logic. Compared with the existingmodel-checking approaches of agent programming languages it has three mainadvantages. First it efficiently performs automated model-checking analysisfor a given vGOAL specification including efficiently generating input modelsfor NuSMV and Storm and leveraging these efficient model checkers. Second thesemantic equivalence is established for both nondeterministic models andprobabilistic models of vGOAL: from vGOAL to transition systems or DTMCs.Third an algorithm is proposed for efficiently detecting errors which isparticularly useful for vGOAL specifications that describe complex scenarios.Validation and experiments in a real-world autonomous logistic system withthree autonomous mobile robots illustrate both the efficiency and practicalusability of the automated CTL and PCTL model-checking process for vGOAL.</p>
                <p>Last Updated: 2024-06-25 01:22:05 UTC</p>
                <button class="interpret-button" data-id="2406.17206v1">Interpret</button>
                <div id="interpretation-2406.17206v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Hierarchical Framework for Optimizing Wildfire Surveillance and Suppression using Human-Autonomous Teaming</h3>
                <p>Authors: Mahdi Al-HusseiniKyle WrayMykel Kochenderfer</p>
                <p><a href="http://arxiv.org/abs/2406.17189v1">Link to paper</a></p>
                <p>The integration of manned and unmanned aircraft can help improve wildfireresponse. Wildfire containment failures occur when resources available to firstresponders who execute the initial stages of wildfire management referred toas the initial attack are ineffective or insufficient. Initial attacksurveillance and suppression models have linked action spaces and objectivesmaking their optimization computationally challenging. The initial attack maybe formulated as a multi-agent partially observable Markov decision processMPOMDP. We divide the initial attack MPOMDP into surveillance and suppressionprocesses with their respective planners operating on different but constanttime scales. A hierarchical framework iterates between surveillance andsuppression planners while also providing collision avoidance. This frameworkis exemplified by a set of multi-rotor unmanned aircraft surveying an initialattack fire while a manned helicopter suppresses the fire with a water bucket.Wildfire-specific solver extensions are formulated to reduce the otherwise vastaction spaces. The hierarchical framework outperforms firefighting techniquesand a myopic baseline by up to 242 for moderate wildfires and 60 for rapidwildfires when simulated in abstracted and actual case studies. We alsovalidate the early dispatching of additional suppression assets usingregression models to ensure wildfire containment to thresholds established bywildfire agencies.</p>
                <p>Last Updated: 2024-06-25 00:07:33 UTC</p>
                <button class="interpret-button" data-id="2406.17189v1">Interpret</button>
                <div id="interpretation-2406.17189v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-06-26</p>
        </div>
    
        </div>
    </body>
    </html>
    