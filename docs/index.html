
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss</h3>
                <p>Authors: Ingvar ZiemannStephen TuGeorge J. PappasNikolai Matni</p>
                <p><a href="http://arxiv.org/abs/2402.05928v1">Link to paper</a></p>
                <p>In this work we study statistical learning with dependent beta-mixingdata and square loss in a hypothesis class mathscrFsubset L_Psi_pwhere Psi_p is the norm f_Psi_p triangleq sup_mgeq 1 m-1/pf_Lm  for some pin 2infty. Our inquiry is motivated by thesearch for a sharp noise interaction term or variance proxy in learning withdependent data. Absent any realizability assumption typical non-asymptoticresults exhibit variance proxies that are deflated emphmultiplicatively bythe mixing time of the underlying covariates process. We show that whenever thetopologies of L2 and Psi_p are comparable on our hypothesis classmathscrF -- that is mathscrF is a weakly sub-Gaussian class:f_Psi_p lesssim f_L2eta for some etain 01 -- theempirical risk minimizer achieves a rate that only depends on the complexity ofthe class and second order statistics in its leading term. Our result holdswhether the problem is realizable or not and we refer to this as a emphnearmixing-free rate since direct dependence on mixing is relegated to anadditive higher order term. We arrive at our result by combining the abovenotion of a weakly sub-Gaussian class with mixed tail generic chaining. Thiscombination allows us to compute sharp instance-optimal rates for a wide rangeof problems. Our approach reliant on mixed tail generic chaining allows usto obtain sharp instance-optimal rates. Examples that satisfy our frameworkinclude sub-Gaussian linear regression more general smoothly parameterizedfunction classes finite hypothesis classes and bounded smoothness classes.</p>
                <p>Last Updated: 2024-02-08 18:57:42 UTC</p>
                <button class="interpret-button" data-id="2402.05928v1">Interpret</button>
                <div id="interpretation-2402.05928v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits</h3>
                <p>Authors: Nicolas NguyenImad AoualiAndrás GyörgyClaire Vernade</p>
                <p><a href="http://arxiv.org/abs/2402.05878v1">Link to paper</a></p>
                <p>We study the problem of Bayesian fixed-budget best-arm identification BAIin structured bandits. We propose an algorithm that uses fixed allocationsbased on the prior information and the structure of the environment. We providetheoretical bounds on its performance across diverse models including thefirst prior-dependent upper bounds for linear and hierarchical BAI. Our keycontribution is introducing new proof methods that result in tighter bounds formulti-armed BAI compared to existing methods. We extensively compare ourapproach to other fixed-budget BAI methods demonstrating its consistent androbust performance in various settings. Our work improves our understanding ofBayesian fixed-budget BAI in structured bandits and highlights theeffectiveness of our approach in practical scenarios.</p>
                <p>Last Updated: 2024-02-08 18:13:26 UTC</p>
                <button class="interpret-button" data-id="2402.05878v1">Interpret</button>
                <div id="interpretation-2402.05878v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices</h3>
                <p>Authors: Jiin WooLaixi ShiGauri JoshiYuejie Chi</p>
                <p><a href="http://arxiv.org/abs/2402.05876v1">Link to paper</a></p>
                <p>Offline reinforcement learning RL which seeks to learn an optimal policyusing offline data has garnered significant interest due to its potential incritical applications where online data collection is infeasible or expensive.This work explores the benefit of federated learning for offline RL aiming atcollaboratively leveraging offline datasets at multiple agents. Focusing onfinite-horizon episodic tabular Markov decision processes MDPs we designFedLCB-Q a variant of the popular model-free Q-learning algorithm tailored forfederated offline RL. FedLCB-Q updates local Q-functions at agents with novellearning rate schedules and aggregates them at a central server usingimportance averaging and a carefully designed pessimistic penalty term. Oursample complexity analysis reveals that with appropriately chosen parametersand synchronization schedules FedLCB-Q achieves linear speedup in terms of thenumber of agents without requiring high-quality datasets at individual agentsas long as the local datasets collectively cover the state-action space visitedby the optimal policy highlighting the power of collaboration in the federatedsetting. In fact the sample complexity almost matches that of the single-agentcounterpart as if all the data are stored at a central location up topolynomial factors of the horizon length. Furthermore FedLCB-Q iscommunication-efficient where the number of communication rounds is onlylinear with respect to the horizon length up to logarithmic factors.</p>
                <p>Last Updated: 2024-02-08 18:09:17 UTC</p>
                <button class="interpret-button" data-id="2402.05876v1">Interpret</button>
                <div id="interpretation-2402.05876v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Let Your Graph Do the Talking: Encoding Structured Data for LLMs</h3>
                <p>Authors: Bryan PerozziBahare FatemiDustin ZelleAnton TsitsulinMehran KazemiRami Al-RfouJonathan Halcrow</p>
                <p><a href="http://arxiv.org/abs/2402.05862v1">Link to paper</a></p>
                <p>How can we best encode structured data into sequential form for use in largelanguage models LLMs In this work we introduce a parameter-efficient methodto explicitly represent structured data for LLMs. Our method GraphTokenlearns an encoding function to extend prompts with explicit structuredinformation. Unlike other work which focuses on limited domains e.g. knowledgegraph representation our work is the first effort focused on the generalencoding of structured data to be used for various reasoning tasks. We showthat explicitly representing the graph structure allows significantimprovements to graph reasoning tasks. Specifically we see across the boardimprovements - up to 73 points - on node edge and graph-level tasks from theGraphQA benchmark.</p>
                <p>Last Updated: 2024-02-08 17:51:44 UTC</p>
                <button class="interpret-button" data-id="2402.05862v1">Interpret</button>
                <div id="interpretation-2402.05862v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>How Much is Unseen Depends Chiefly on Information About the Seen</h3>
                <p>Authors: Seongmin LeeMarcel Böhme</p>
                <p><a href="http://arxiv.org/abs/2402.05835v1">Link to paper</a></p>
                <p>It might seem counter-intuitive at first: We find that in expectation theproportion of data points in an unknown population-that belong to classes thatdo not appear in the training data-is almost entirely determined by the numberf_k of classes that do appear in the training data the same number of times.While in theory we show that the difference of the induced estimator decaysexponentially in the size of the sample in practice the high variance preventsus from using it directly for an estimator of the sample coverage. However ourprecise characterization of the dependency between f_ks induces a largesearch space of different representations of the expected value which can bedeterministically instantiated as estimators. Hence we turn to optimizationand develop a genetic algorithm that given only the sample searches for anestimator with minimal mean-squared error MSE. In our experiments ourgenetic algorithm discovers estimators that have a substantially smaller MSEthan the state-of-the-art Good-Turing estimator. This holds for over 96 ofruns when there are at least as many samples as classes. Our estimators MSE isroughly 80 of the Good-Turing estimators.</p>
                <p>Last Updated: 2024-02-08 17:12:49 UTC</p>
                <button class="interpret-button" data-id="2402.05835v1">Interpret</button>
                <div id="interpretation-2402.05835v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>InstaGen: Enhancing Object Detection by Training on Synthetic Dataset</h3>
                <p>Authors: Chengjian FengYujie ZhongZequn JieWeidi XieLin Ma</p>
                <p><a href="http://arxiv.org/abs/2402.05937v1">Link to paper</a></p>
                <p>In this paper we introduce a novel paradigm to enhance the ability of objectdetector e.g. expanding categories or improving detection performance bytraining on synthetic dataset generated from diffusion models. Specifically weintegrate an instance-level grounding head into a pre-trained generativediffusion model to augment it with the ability of localising arbitraryinstances in the generated images. The grounding head is trained to align thetext embedding of category names with the regional visual feature of thediffusion model using supervision from an off-the-shelf object detector and anovel self-training scheme on novel categories not covered by the detector.This enhanced version of diffusion model termed as InstaGen can serve as adata synthesizer for object detection. We conduct thorough experiments to showthat object detector can be enhanced while training on the synthetic datasetfrom InstaGen demonstrating superior performance over existingstate-of-the-art methods in open-vocabulary 4.5 AP and data-sparse 1.2 to5.2 AP scenarios.</p>
                <p>Last Updated: 2024-02-08 18:59:53 UTC</p>
                <button class="interpret-button" data-id="2402.05937v1">Interpret</button>
                <div id="interpretation-2402.05937v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</h3>
                <p>Authors: Peng GaoRenrui ZhangChris LiuLongtian QiuSiyuan HuangWeifeng LinShitian ZhaoShijie GengZiyi LinPeng JinKaipeng ZhangWenqi ShaoChao XuConghui HeJunjun HeHao ShaoPan LuHongsheng LiYu Qiao</p>
                <p><a href="http://arxiv.org/abs/2402.05935v1">Link to paper</a></p>
                <p>We propose SPHINX-X an extensive Multimodality Large Language Model MLLMseries developed upon SPHINX. To improve the architecture and trainingefficiency we modify the SPHINX framework by removing redundant visualencoders bypassing fully-padded sub-images with skip tokens and simplifyingmulti-stage training into a one-stage all-in-one paradigm. To fully unleash thepotential of MLLMs we assemble a comprehensive multi-domain and multimodaldataset covering publicly available resources in language vision andvision-language tasks. We further enrich this collection with our curated OCRintensive and Set-of-Mark datasets extending the diversity and generality. Bytraining over different base LLMs including TinyLlama1.1B InternLM2-7BLLaMA2-13B and Mixtral8x7B we obtain a spectrum of MLLMs that vary inparameter size and multilingual capabilities. Comprehensive benchmarkingreveals a strong correlation between the multi-modal performance with the dataand parameter scales. Code and models are released athttps://github.com/Alpha-VLLM/LLaMA2-Accessory</p>
                <p>Last Updated: 2024-02-08 18:59:48 UTC</p>
                <button class="interpret-button" data-id="2402.05935v1">Interpret</button>
                <div id="interpretation-2402.05935v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</h3>
                <p>Authors: Xing Han LùZdeněk KasnerSiva Reddy</p>
                <p><a href="http://arxiv.org/abs/2402.05930v1">Link to paper</a></p>
                <p>We propose the problem of conversational web navigation where a digitalagent controls a web browser and follows user instructions to solve real-worldtasks in a multi-turn dialogue fashion. To support this problem we introduceWEBLINX - a large-scale benchmark of 100K interactions across 2300 expertdemonstrations of conversational web navigation. Our benchmark covers a broadrange of patterns on over 150 real-world websites and can be used to train andevaluate agents in diverse scenarios. Due to the magnitude of informationpresent Large Language Models LLMs cannot process entire web pages inreal-time. To solve this bottleneck we design a retrieval-inspired model thatefficiently prunes HTML pages by ranking relevant elements. We use the selectedelements along with screenshots and action history to assess a variety ofmodels for their ability to replicate human behavior when navigating the web.Our experiments span from small text-only to proprietary multimodal LLMs. Wefind that smaller finetuned decoders surpass the best zero-shot LLMs includingGPT-4V but also larger finetuned multimodal models which were explicitlypretrained on screenshots. However all finetuned models struggle to generalizeto unseen websites. Our findings highlight the need for large multimodal modelsthat can generalize to novel settings. Our code data and models are availablefor research: https://mcgill-nlp.github.io/weblinx</p>
                <p>Last Updated: 2024-02-08 18:58:02 UTC</p>
                <button class="interpret-button" data-id="2402.05930v1">Interpret</button>
                <div id="interpretation-2402.05930v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Collaborative Control for Geometry-Conditioned PBR Image Generation</h3>
                <p>Authors: Shimon VainerMark BossMathias PargerKonstantin KutsyDante De NigrisCiara RowlesNicolas PeronySimon Donné</p>
                <p><a href="http://arxiv.org/abs/2402.05919v1">Link to paper</a></p>
                <p>Current 3D content generation builds on generative models that output RGBimages. Modern graphics pipelines however require physically-based renderingPBR material properties. We propose to model the PBR image distributiondirectly to avoid photometric inaccuracies in RGB generation and the inherentambiguity in extracting PBR from RGB. Existing paradigms for cross-modalfinetuning are not suited for PBR generation due to a lack of data and the highdimensionality of the output modalities: we overcome both challenges byretaining a frozen RGB model and tightly linking a newly trained PBR modelusing a novel cross-network communication paradigm. As the base RGB model isfully frozen the proposed method does not risk catastrophic forgetting duringfinetuning and remains compatible with techniques such as IPAdapter pretrainedfor the base RGB model. We validate our design choices robustness to datasparsity and compare against existing paradigms with an extensive experimentalsection.</p>
                <p>Last Updated: 2024-02-08 18:53:21 UTC</p>
                <button class="interpret-button" data-id="2402.05919v1">Interpret</button>
                <div id="interpretation-2402.05919v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Point-VOS: Pointing Up Video Object Segmentation</h3>
                <p>Authors: Idil Esen ZulfikarSabarinath MahadevanPaul VoigtlaenderBastian Leibe</p>
                <p><a href="http://arxiv.org/abs/2402.05917v1">Link to paper</a></p>
                <p>Current state-of-the-art Video Object Segmentation VOS methods rely ondense per-object mask annotations both during training and testing. Thisrequires time-consuming and costly video annotation mechanisms. We propose anovel Point-VOS task with a spatio-temporally sparse point-wise annotationscheme that substantially reduces the annotation effort. We apply ourannotation scheme to two large-scale video datasets with text descriptions andannotate over 19M points across 133K objects in 32K videos. Based on ourannotations we propose a new Point-VOS benchmark and a correspondingpoint-based training mechanism which we use to establish strong baselineresults. We show that existing VOS methods can easily be adapted to leverageour point annotations during training and can achieve results close to thefully-supervised performance when trained on pseudo-masks generated from thesepoints. In addition we show that our data can be used to improve models thatconnect vision and language by evaluating it on the Video Narrative GroundingVNG task. We will make our code and annotations available athttps://pointvos.github.io.</p>
                <p>Last Updated: 2024-02-08 18:52:23 UTC</p>
                <button class="interpret-button" data-id="2402.05917v1">Interpret</button>
                <div id="interpretation-2402.05917v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</h3>
                <p>Authors: Peng GaoRenrui ZhangChris LiuLongtian QiuSiyuan HuangWeifeng LinShitian ZhaoShijie GengZiyi LinPeng JinKaipeng ZhangWenqi ShaoChao XuConghui HeJunjun HeHao ShaoPan LuHongsheng LiYu Qiao</p>
                <p><a href="http://arxiv.org/abs/2402.05935v1">Link to paper</a></p>
                <p>We propose SPHINX-X an extensive Multimodality Large Language Model MLLMseries developed upon SPHINX. To improve the architecture and trainingefficiency we modify the SPHINX framework by removing redundant visualencoders bypassing fully-padded sub-images with skip tokens and simplifyingmulti-stage training into a one-stage all-in-one paradigm. To fully unleash thepotential of MLLMs we assemble a comprehensive multi-domain and multimodaldataset covering publicly available resources in language vision andvision-language tasks. We further enrich this collection with our curated OCRintensive and Set-of-Mark datasets extending the diversity and generality. Bytraining over different base LLMs including TinyLlama1.1B InternLM2-7BLLaMA2-13B and Mixtral8x7B we obtain a spectrum of MLLMs that vary inparameter size and multilingual capabilities. Comprehensive benchmarkingreveals a strong correlation between the multi-modal performance with the dataand parameter scales. Code and models are released athttps://github.com/Alpha-VLLM/LLaMA2-Accessory</p>
                <p>Last Updated: 2024-02-08 18:59:48 UTC</p>
                <button class="interpret-button" data-id="2402.05935v1">Interpret</button>
                <div id="interpretation-2402.05935v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Time Series Diffusion in the Frequency Domain</h3>
                <p>Authors: Jonathan CrabbéNicolas HuynhJan StanczukMihaela van der Schaar</p>
                <p><a href="http://arxiv.org/abs/2402.05933v1">Link to paper</a></p>
                <p>Fourier analysis has been an instrumental tool in the development of signalprocessing. This leads us to wonder whether this framework could similarlybenefit generative modelling. In this paper we explore this question throughthe scope of time series diffusion models. More specifically we analyzewhether representing time series in the frequency domain is a useful inductivebias for score-based diffusion models. By starting from the canonical SDEformulation of diffusion in the time domain we show that a dual diffusionprocess occurs in the frequency domain with an important nuance: Brownianmotions are replaced by what we call mirrored Brownian motions characterizedby mirror symmetries among their components. Building on this insight we showhow to adapt the denoising score matching approach to implement diffusionmodels in the frequency domain. This results in frequency diffusion modelswhich we compare to canonical time diffusion models. Our empirical evaluationon real-world datasets covering various domains like healthcare and financeshows that frequency diffusion models better capture the training distributionthan time diffusion models. We explain this observation by showing that timeseries from these datasets tend to be more localized in the frequency domainthan in the time domain which makes them easier to model in the former case.All our observations point towards impactful synergies between Fourier analysisand diffusion models.</p>
                <p>Last Updated: 2024-02-08 18:59:05 UTC</p>
                <button class="interpret-button" data-id="2402.05933v1">Interpret</button>
                <div id="interpretation-2402.05933v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Driving Everywhere with Large Language Model Policy Adaptation</h3>
                <p>Authors: Boyi LiYue WangJiageng MaoBoris IvanovicSushant VeerKaren LeungMarco Pavone</p>
                <p><a href="http://arxiv.org/abs/2402.05932v1">Link to paper</a></p>
                <p>Adapting driving behavior to new environments customs and laws is along-standing problem in autonomous driving precluding the widespreaddeployment of autonomous vehicles AVs. In this paper we present LLaDA asimple yet powerful tool that enables human drivers and autonomous vehiclesalike to drive everywhere by adapting their tasks and motion plans to trafficrules in new locations. LLaDA achieves this by leveraging the impressivezero-shot generalizability of large language models LLMs in interpreting thetraffic rules in the local driver handbook. Through an extensive user study weshow that LLaDAs instructions are useful in disambiguating in-the-wildunexpected situations. We also demonstrate LLaDAs ability to adapt AV motionplanning policies in real-world datasets LLaDA outperforms baseline planningapproaches on all our metrics. Please check our website for more details:https://boyiliee.github.io/llada.</p>
                <p>Last Updated: 2024-02-08 18:59:03 UTC</p>
                <button class="interpret-button" data-id="2402.05932v1">Interpret</button>
                <div id="interpretation-2402.05932v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>An Interactive Agent Foundation Model</h3>
                <p>Authors: Zane DuranteBidipta SarkarRan GongRohan TaoriYusuke NodaPaul TangEhsan AdeliShrinidhi Kowshika LakshmikanthKevin SchulmanArnold MilsteinDemetri TerzopoulosAde FamotiNoboru KunoAshley LlorensHoi VoKatsu IkeuchiLi Fei-FeiJianfeng GaoNaoki WakeQiuyuan Huang</p>
                <p><a href="http://arxiv.org/abs/2402.05929v1">Link to paper</a></p>
                <p>The development of artificial intelligence systems is transitioning fromcreating static task-specific models to dynamic agent-based systems capableof performing well in a wide range of applications. We propose an InteractiveAgent Foundation Model that uses a novel multi-task agent training paradigm fortraining AI agents across a wide range of domains datasets and tasks. Ourtraining paradigm unifies diverse pre-training strategies including visualmasked auto-encoders language modeling and next-action prediction enabling aversatile and adaptable AI framework. We demonstrate the performance of ourframework across three separate domains -- Robotics Gaming AI and Healthcare.Our model demonstrates its ability to generate meaningful and contextuallyrelevant outputs in each area. The strength of our approach lies in itsgenerality leveraging a variety of data sources such as robotics sequencesgameplay data large-scale video datasets and textual information foreffective multimodal and multi-task learning. Our approach provides a promisingavenue for developing generalist action-taking multimodal systems.</p>
                <p>Last Updated: 2024-02-08 18:58:02 UTC</p>
                <button class="interpret-button" data-id="2402.05929v1">Interpret</button>
                <div id="interpretation-2402.05929v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games</h3>
                <p>Authors: Hafez GhaemiHamed KebriaeiAlireza Ramezani MoghaddamMajid Nili Ahamdabadi</p>
                <p><a href="http://arxiv.org/abs/2402.05906v1">Link to paper</a></p>
                <p>Classical multi-agent reinforcement learning MARL assumes risk neutralityand complete objectivity for agents. However in settings where agents need toconsider or model human economic or social preferences a notion of risk mustbe incorporated into the RL optimization problem. This will be of greaterimportance in MARL where other human or non-human agents are involved possiblywith their own risk-sensitive policies. In this work we considerrisk-sensitive and non-cooperative MARL with cumulative prospect theory CPTa non-convex risk measure and a generalization of coherent measures of risk.CPT is capable of explaining loss aversion in humans and their tendency tooverestimate/underestimate small/large probabilities. We propose a distributedsampling-based actor-critic AC algorithm with CPT risk for networkaggregative Markov games NAMGs which we call Distributed Nested CPT-AC.Under a set of assumptions we prove the convergence of the algorithm to asubjective notion of Markov perfect Nash equilibrium in NAMGs. The experimentalresults show that subjective CPT policies obtained by our algorithm can bedifferent from the risk-neutral ones and agents with a higher loss aversionare more inclined to socially isolate themselves in an NAMG.</p>
                <p>Last Updated: 2024-02-08 18:43:27 UTC</p>
                <button class="interpret-button" data-id="2402.05906v1">Interpret</button>
                <div id="interpretation-2402.05906v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs</h3>
                <p>Authors: Eun Cheol ChoiEmilio Ferrara</p>
                <p><a href="http://arxiv.org/abs/2402.05904v1">Link to paper</a></p>
                <p>Our society is facing rampant misinformation harming public health and trust.To address the societal challenge we introduce FACT-GPT a system leveragingLarge Language Models LLMs to automate the claim matching stage offact-checking. FACT-GPT trained on a synthetic dataset identifies socialmedia content that aligns with contradicts or is irrelevant to previouslydebunked claims. Our evaluation shows that our specialized LLMs can match theaccuracy of larger models in identifying related claims closely mirroringhuman judgment. This research provides an automated solution for efficientclaim matching demonstrates the potential of LLMs in supporting fact-checkersand offers valuable resources for further research in the field.</p>
                <p>Last Updated: 2024-02-08 18:43:05 UTC</p>
                <button class="interpret-button" data-id="2402.05904v1">Interpret</button>
                <div id="interpretation-2402.05904v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Personalizing Driver Safety Interfaces via Driver Cognitive Factors Inference</h3>
                <p>Authors: Emily S SumnerJonathan DeCastroJean CostaDeepak E GopinathEverlyne KimaniShabnam HakimiAllison MorganAndrew BestHieu NguyenDaniel J BrooksBassam ul HaqAndrew PatrikalakisHiroshi YasudaKate SieckAvinash BalachandranTiffany ChenGuy Rosman</p>
                <p><a href="http://arxiv.org/abs/2402.05893v1">Link to paper</a></p>
                <p>Recent advances in AI and intelligent vehicle technology hold promise torevolutionize mobility and transportation in the form of advanced drivingassistance ADAS interfaces. Although it is widely recognized that certaincognitive factors such as impulsivity and inhibitory control are related torisky driving behavior play a significant role in on-road risk-takingexisting systems fail to leverage such factors. Varying levels of thesecognitive factors could influence the effectiveness and acceptance of driversafety interfaces.  We demonstrate an approach for personalizing driver interaction via driversafety interfaces that are triggered based on a learned recurrent neuralnetwork. The network is trained from a population of human drivers to inferimpulsivity and inhibitory control from recent driving behavior. Using ahigh-fidelity vehicle motion simulator we demonstrate the ability to deducethese factors from driver behavior. We then use these inferred factors to makeinstantaneous determinations on whether or not to engage a driver safetyinterface. This interface aims to decrease a drivers speed during yellowlights and reduce their inclination to run through them.</p>
                <p>Last Updated: 2024-02-08 18:32:10 UTC</p>
                <button class="interpret-button" data-id="2402.05893v1">Interpret</button>
                <div id="interpretation-2402.05893v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GET-Tok: A GenAI-Enriched Multimodal TikTok Dataset Documenting the 2022 Attempted Coup in Peru</h3>
                <p>Authors: Gabriela PintoKeith BurghardtKristina LermanEmilio Ferrara</p>
                <p><a href="http://arxiv.org/abs/2402.05882v1">Link to paper</a></p>
                <p>TikTok is one of the largest and fastest-growing social media sites in theworld. TikTok features however such as voice transcripts are often missingand other important features such as OCR or video descriptions do not exist.We introduce the Generative AI Enriched TikTok GET-Tok data a pipeline forcollecting TikTok videos and enriched data by augmenting the TikTok ResearchAPI with generative AI models. As a case study we collect videos about theattempted coup in Peru initiated by its former President Pedro Castillo andits accompanying protests. The data includes information on 43697 videospublished from November 20 2022 to March 1 2023 102 days. Generative AIaugments the collected data via transcripts of TikTok videos text descriptionsof what is shown in the videos what text is displayed within the video andthe stances expressed in the video. Overall this pipeline will contribute to abetter understanding of online discussion in a multimodal setting withapplications of Generative AI especially outlining the utility of thispipeline in non-English-language social media. Our code used to produce thepipeline is in a public Github repository:https://github.com/gabbypinto/GET-Tok-Peru.</p>
                <p>Last Updated: 2024-02-08 18:16:47 UTC</p>
                <button class="interpret-button" data-id="2402.05882v1">Interpret</button>
                <div id="interpretation-2402.05882v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking</h3>
                <p>Authors: Nikhil SharmaQ. Vera LiaoZiang Xiao</p>
                <p><a href="http://arxiv.org/abs/2402.05880v1">Link to paper</a></p>
                <p>Large language models LLMs powered conversational search systems havealready been used by hundreds of millions of people and are believed to bringmany benefits over conventional search. However while decades of research andpublic discourse interrogated the risk of search systems in increasingselective exposure and creating echo chambers -- limiting exposure to diverseopinions and leading to opinion polarization little is known about such a riskof LLM-powered conversational search. We conduct two experiments toinvestigate: 1 whether and how LLM-powered conversational search increasesselective exposure compared to conventional search 2 whether and how LLMswith opinion biases that either reinforce or challenge the users view changethe effect. Overall we found that participants engaged in more biasedinformation querying with LLM-powered conversational search and an opinionatedLLM reinforcing their views exacerbated this bias. These results presentcritical implications for the development of LLMs and conversational searchsystems and the policy governing these technologies.</p>
                <p>Last Updated: 2024-02-08 18:14:33 UTC</p>
                <button class="interpret-button" data-id="2402.05880v1">Interpret</button>
                <div id="interpretation-2402.05880v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>"Can You Play Anything Else?" Understanding Play Style Flexibility in League of Legends</h3>
                <p>Authors: Emily ChenAlexander BisbergEmilio Ferrara</p>
                <p><a href="http://arxiv.org/abs/2402.05865v1">Link to paper</a></p>
                <p>This study investigates the concept of flexibility within League of Legendsa popular online multiplayer game focusing on the relationship between useradaptability and team success. Utilizing a dataset encompassing players ofvarying skill levels and play styles we calculate two measures of flexibilityfor each player: overall flexibility and temporal flexibility. Our findingssuggest that the flexibility of a user is dependent upon a users preferredplay style and flexibility does impact match outcome. This work also showsthat skill level not only indicates how willing a player is to adapt their playstyle but also how their adaptability changes over time. This paper highlightsthe the duality and balance of mastery versus flexibility providing insightsthat can inform strategic planning collaboration and resource allocation incompetitive environments.</p>
                <p>Last Updated: 2024-02-08 17:57:03 UTC</p>
                <button class="interpret-button" data-id="2402.05865v1">Interpret</button>
                <div id="interpretation-2402.05865v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</h3>
                <p>Authors: Peng GaoRenrui ZhangChris LiuLongtian QiuSiyuan HuangWeifeng LinShitian ZhaoShijie GengZiyi LinPeng JinKaipeng ZhangWenqi ShaoChao XuConghui HeJunjun HeHao ShaoPan LuHongsheng LiYu Qiao</p>
                <p><a href="http://arxiv.org/abs/2402.05935v1">Link to paper</a></p>
                <p>We propose SPHINX-X an extensive Multimodality Large Language Model MLLMseries developed upon SPHINX. To improve the architecture and trainingefficiency we modify the SPHINX framework by removing redundant visualencoders bypassing fully-padded sub-images with skip tokens and simplifyingmulti-stage training into a one-stage all-in-one paradigm. To fully unleash thepotential of MLLMs we assemble a comprehensive multi-domain and multimodaldataset covering publicly available resources in language vision andvision-language tasks. We further enrich this collection with our curated OCRintensive and Set-of-Mark datasets extending the diversity and generality. Bytraining over different base LLMs including TinyLlama1.1B InternLM2-7BLLaMA2-13B and Mixtral8x7B we obtain a spectrum of MLLMs that vary inparameter size and multilingual capabilities. Comprehensive benchmarkingreveals a strong correlation between the multi-modal performance with the dataand parameter scales. Code and models are released athttps://github.com/Alpha-VLLM/LLaMA2-Accessory</p>
                <p>Last Updated: 2024-02-08 18:59:48 UTC</p>
                <button class="interpret-button" data-id="2402.05935v1">Interpret</button>
                <div id="interpretation-2402.05935v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Classifying Nodes in Graphs without GNNs</h3>
                <p>Authors: Daniel WinterNiv CohenYedid Hoshen</p>
                <p><a href="http://arxiv.org/abs/2402.05934v1">Link to paper</a></p>
                <p>Graph neural networks GNNs are the dominant paradigm for classifying nodesin a graph but they have several undesirable attributes stemming from theirmessage passing architecture. Recently distillation methods succeeded ineliminating the use of GNNs at test time but they still require them duringtraining. We perform a careful analysis of the role that GNNs play indistillation methods. This analysis leads us to propose a fully GNN-freeapproach for node classification not requiring them at train or test time. Ourmethod consists of three key components: smoothness constraintspseudo-labeling iterations and neighborhood-label histograms. Our finalapproach can match the state-of-the-art accuracy on standard popular benchmarkssuch as citation and co-purchase networks without training a GNN.</p>
                <p>Last Updated: 2024-02-08 18:59:30 UTC</p>
                <button class="interpret-button" data-id="2402.05934v1">Interpret</button>
                <div id="interpretation-2402.05934v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Time Series Diffusion in the Frequency Domain</h3>
                <p>Authors: Jonathan CrabbéNicolas HuynhJan StanczukMihaela van der Schaar</p>
                <p><a href="http://arxiv.org/abs/2402.05933v1">Link to paper</a></p>
                <p>Fourier analysis has been an instrumental tool in the development of signalprocessing. This leads us to wonder whether this framework could similarlybenefit generative modelling. In this paper we explore this question throughthe scope of time series diffusion models. More specifically we analyzewhether representing time series in the frequency domain is a useful inductivebias for score-based diffusion models. By starting from the canonical SDEformulation of diffusion in the time domain we show that a dual diffusionprocess occurs in the frequency domain with an important nuance: Brownianmotions are replaced by what we call mirrored Brownian motions characterizedby mirror symmetries among their components. Building on this insight we showhow to adapt the denoising score matching approach to implement diffusionmodels in the frequency domain. This results in frequency diffusion modelswhich we compare to canonical time diffusion models. Our empirical evaluationon real-world datasets covering various domains like healthcare and financeshows that frequency diffusion models better capture the training distributionthan time diffusion models. We explain this observation by showing that timeseries from these datasets tend to be more localized in the frequency domainthan in the time domain which makes them easier to model in the former case.All our observations point towards impactful synergies between Fourier analysisand diffusion models.</p>
                <p>Last Updated: 2024-02-08 18:59:05 UTC</p>
                <button class="interpret-button" data-id="2402.05933v1">Interpret</button>
                <div id="interpretation-2402.05933v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>An Interactive Agent Foundation Model</h3>
                <p>Authors: Zane DuranteBidipta SarkarRan GongRohan TaoriYusuke NodaPaul TangEhsan AdeliShrinidhi Kowshika LakshmikanthKevin SchulmanArnold MilsteinDemetri TerzopoulosAde FamotiNoboru KunoAshley LlorensHoi VoKatsu IkeuchiLi Fei-FeiJianfeng GaoNaoki WakeQiuyuan Huang</p>
                <p><a href="http://arxiv.org/abs/2402.05929v1">Link to paper</a></p>
                <p>The development of artificial intelligence systems is transitioning fromcreating static task-specific models to dynamic agent-based systems capableof performing well in a wide range of applications. We propose an InteractiveAgent Foundation Model that uses a novel multi-task agent training paradigm fortraining AI agents across a wide range of domains datasets and tasks. Ourtraining paradigm unifies diverse pre-training strategies including visualmasked auto-encoders language modeling and next-action prediction enabling aversatile and adaptable AI framework. We demonstrate the performance of ourframework across three separate domains -- Robotics Gaming AI and Healthcare.Our model demonstrates its ability to generate meaningful and contextuallyrelevant outputs in each area. The strength of our approach lies in itsgenerality leveraging a variety of data sources such as robotics sequencesgameplay data large-scale video datasets and textual information foreffective multimodal and multi-task learning. Our approach provides a promisingavenue for developing generalist action-taking multimodal systems.</p>
                <p>Last Updated: 2024-02-08 18:58:02 UTC</p>
                <button class="interpret-button" data-id="2402.05929v1">Interpret</button>
                <div id="interpretation-2402.05929v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</h3>
                <p>Authors: Xing Han LùZdeněk KasnerSiva Reddy</p>
                <p><a href="http://arxiv.org/abs/2402.05930v1">Link to paper</a></p>
                <p>We propose the problem of conversational web navigation where a digitalagent controls a web browser and follows user instructions to solve real-worldtasks in a multi-turn dialogue fashion. To support this problem we introduceWEBLINX - a large-scale benchmark of 100K interactions across 2300 expertdemonstrations of conversational web navigation. Our benchmark covers a broadrange of patterns on over 150 real-world websites and can be used to train andevaluate agents in diverse scenarios. Due to the magnitude of informationpresent Large Language Models LLMs cannot process entire web pages inreal-time. To solve this bottleneck we design a retrieval-inspired model thatefficiently prunes HTML pages by ranking relevant elements. We use the selectedelements along with screenshots and action history to assess a variety ofmodels for their ability to replicate human behavior when navigating the web.Our experiments span from small text-only to proprietary multimodal LLMs. Wefind that smaller finetuned decoders surpass the best zero-shot LLMs includingGPT-4V but also larger finetuned multimodal models which were explicitlypretrained on screenshots. However all finetuned models struggle to generalizeto unseen websites. Our findings highlight the need for large multimodal modelsthat can generalize to novel settings. Our code data and models are availablefor research: https://mcgill-nlp.github.io/weblinx</p>
                <p>Last Updated: 2024-02-08 18:58:02 UTC</p>
                <button class="interpret-button" data-id="2402.05930v1">Interpret</button>
                <div id="interpretation-2402.05930v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</h3>
                <p>Authors: Peng GaoRenrui ZhangChris LiuLongtian QiuSiyuan HuangWeifeng LinShitian ZhaoShijie GengZiyi LinPeng JinKaipeng ZhangWenqi ShaoChao XuConghui HeJunjun HeHao ShaoPan LuHongsheng LiYu Qiao</p>
                <p><a href="http://arxiv.org/abs/2402.05935v1">Link to paper</a></p>
                <p>We propose SPHINX-X an extensive Multimodality Large Language Model MLLMseries developed upon SPHINX. To improve the architecture and trainingefficiency we modify the SPHINX framework by removing redundant visualencoders bypassing fully-padded sub-images with skip tokens and simplifyingmulti-stage training into a one-stage all-in-one paradigm. To fully unleash thepotential of MLLMs we assemble a comprehensive multi-domain and multimodaldataset covering publicly available resources in language vision andvision-language tasks. We further enrich this collection with our curated OCRintensive and Set-of-Mark datasets extending the diversity and generality. Bytraining over different base LLMs including TinyLlama1.1B InternLM2-7BLLaMA2-13B and Mixtral8x7B we obtain a spectrum of MLLMs that vary inparameter size and multilingual capabilities. Comprehensive benchmarkingreveals a strong correlation between the multi-modal performance with the dataand parameter scales. Code and models are released athttps://github.com/Alpha-VLLM/LLaMA2-Accessory</p>
                <p>Last Updated: 2024-02-08 18:59:48 UTC</p>
                <button class="interpret-button" data-id="2402.05935v1">Interpret</button>
                <div id="interpretation-2402.05935v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Driving Everywhere with Large Language Model Policy Adaptation</h3>
                <p>Authors: Boyi LiYue WangJiageng MaoBoris IvanovicSushant VeerKaren LeungMarco Pavone</p>
                <p><a href="http://arxiv.org/abs/2402.05932v1">Link to paper</a></p>
                <p>Adapting driving behavior to new environments customs and laws is along-standing problem in autonomous driving precluding the widespreaddeployment of autonomous vehicles AVs. In this paper we present LLaDA asimple yet powerful tool that enables human drivers and autonomous vehiclesalike to drive everywhere by adapting their tasks and motion plans to trafficrules in new locations. LLaDA achieves this by leveraging the impressivezero-shot generalizability of large language models LLMs in interpreting thetraffic rules in the local driver handbook. Through an extensive user study weshow that LLaDAs instructions are useful in disambiguating in-the-wildunexpected situations. We also demonstrate LLaDAs ability to adapt AV motionplanning policies in real-world datasets LLaDA outperforms baseline planningapproaches on all our metrics. Please check our website for more details:https://boyiliee.github.io/llada.</p>
                <p>Last Updated: 2024-02-08 18:59:03 UTC</p>
                <button class="interpret-button" data-id="2402.05932v1">Interpret</button>
                <div id="interpretation-2402.05932v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</h3>
                <p>Authors: Xing Han LùZdeněk KasnerSiva Reddy</p>
                <p><a href="http://arxiv.org/abs/2402.05930v1">Link to paper</a></p>
                <p>We propose the problem of conversational web navigation where a digitalagent controls a web browser and follows user instructions to solve real-worldtasks in a multi-turn dialogue fashion. To support this problem we introduceWEBLINX - a large-scale benchmark of 100K interactions across 2300 expertdemonstrations of conversational web navigation. Our benchmark covers a broadrange of patterns on over 150 real-world websites and can be used to train andevaluate agents in diverse scenarios. Due to the magnitude of informationpresent Large Language Models LLMs cannot process entire web pages inreal-time. To solve this bottleneck we design a retrieval-inspired model thatefficiently prunes HTML pages by ranking relevant elements. We use the selectedelements along with screenshots and action history to assess a variety ofmodels for their ability to replicate human behavior when navigating the web.Our experiments span from small text-only to proprietary multimodal LLMs. Wefind that smaller finetuned decoders surpass the best zero-shot LLMs includingGPT-4V but also larger finetuned multimodal models which were explicitlypretrained on screenshots. However all finetuned models struggle to generalizeto unseen websites. Our findings highlight the need for large multimodal modelsthat can generalize to novel settings. Our code data and models are availablefor research: https://mcgill-nlp.github.io/weblinx</p>
                <p>Last Updated: 2024-02-08 18:58:02 UTC</p>
                <button class="interpret-button" data-id="2402.05930v1">Interpret</button>
                <div id="interpretation-2402.05930v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On the Convergence of Zeroth-Order Federated Tuning in Large Language Models</h3>
                <p>Authors: Zhenqing LingDaoyuan ChenLiuyi YaoYaliang LiYing Shen</p>
                <p><a href="http://arxiv.org/abs/2402.05926v1">Link to paper</a></p>
                <p>The confluence of Federated Learning FL and Large Language Models LLMs isushering in a new era in privacy-preserving natural language processing.However the intensive memory requirements for fine-tuning LLMs posesignificant challenges especially when deploying on edge devices with limitedcomputational resources. To circumvent this we explore the novel integrationof Memory-efficient Zeroth-Order Optimization within a federated setting asynergy we denote as FedMeZO. Our study is the first to examine the theoreticalunderpinnings of FedMeZO in the context of LLMs tackling key questionsregarding the influence of large parameter spaces on optimization behavior theestablishment of convergence properties and the identification of criticalparameters for convergence to inform personalized federated strategies. Ourextensive empirical evidence supports the theory showing that FedMeZO not onlyconverges faster than traditional first-order methods such as SGD but alsosignificantly reduces GPU memory usage during training to levels comparable tothose during inference. Moreover the proposed personalized FL strategy that isbuilt upon the theoretical insights to customize the client-wise learning ratecan effectively accelerate loss reduction. We hope our work can help to bridgetheoretical and practical aspects of federated fine-tuning for LLMs andfacilitate further development and research.</p>
                <p>Last Updated: 2024-02-08 18:56:40 UTC</p>
                <button class="interpret-button" data-id="2402.05926v1">Interpret</button>
                <div id="interpretation-2402.05926v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Efficient Stagewise Pretraining via Progressive Subnetworks</h3>
                <p>Authors: Abhishek PanigrahiNikunj SaunshiKaifeng LyuSobhan MiryoosefiSashank ReddiSatyen KaleSanjiv Kumar</p>
                <p><a href="http://arxiv.org/abs/2402.05913v1">Link to paper</a></p>
                <p>Recent developments in large language models have sparked interest inefficient pretraining methods. A recent effective paradigm is to performstage-wise training where the size of the model is gradually increased overthe course of training e.g. gradual stacking Reddi et al. 2023. While theresource and wall-time savings are appealing it has limitations particularlythe inability to evaluate the full model during earlier stages and degradationin model quality due to smaller model capacity in the initial stages. In thiswork we propose an alternative framework progressive subnetwork trainingthat maintains the full model throughout training but only trains subnetworkswithin the model in each step. We focus on a simple instantiation of thisframework Random Path Training RaPTr that only trains a sub-path of layersin each step progressively increasing the path lengths in stages. RaPTrachieves better pre-training loss for BERT and UL2 language models whilerequiring 20-33 fewer FLOPs compared to standard training and is competitiveor better than other efficient training methods. Furthermore RaPTr showsbetter downstream performance on UL2 improving QA tasks and SuperGLUE by 1-5compared to standard training and stacking. Finally we provide a theoreticalbasis for RaPTr to justify a the increasing complexity of subnetworks instages and b the stability in loss across stage transitions due to residualconnections and layer norm.</p>
                <p>Last Updated: 2024-02-08 18:49:09 UTC</p>
                <button class="interpret-button" data-id="2402.05913v1">Interpret</button>
                <div id="interpretation-2402.05913v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Consensus-driven Deviated Pursuit for Guaranteed Simultaneous Interception of Moving Targets</h3>
                <p>Authors: Abhinav SinhaDwaipayan MukherjeeShashi Ranjan Kumar</p>
                <p><a href="http://arxiv.org/abs/2402.05918v1">Link to paper</a></p>
                <p>This work proposes a cooperative strategy that employs deviated pursuitguidance to simultaneously intercept a moving but not manoeuvring target. Asopposed to many existing cooperative guidance strategies which use estimates oftime-to-go based on proportional-navigation guidance the proposed strategyuses an exact expression for time-to-go to ensure simultaneous interception.The guidance design considers nonlinear engagement kinematics allowing theproposed strategy to remain effective over a large operating regime. Unlikeexisting strategies on simultaneous interception that achieve interception atthe average value of their initial time-to-go estimates this work providesflexibility in the choice of impact time. By judiciously choosing the edgeweights of the communication network a weighted consensus in time-to-go can beachieved. It has been shown that by allowing an edge weight to be negativeconsensus in time-to-go can even be achieved for an impact time that liesoutside the convex hull of the set of initial time-to-go values of theindividual interceptors. The bounds on such negative weights have been analysedfor some special graphs using Nyquist criterion. Simulations are provided tovindicate the efficacy of the proposed strategy.</p>
                <p>Last Updated: 2024-02-08 18:52:35 UTC</p>
                <button class="interpret-button" data-id="2402.05918v1">Interpret</button>
                <div id="interpretation-2402.05918v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games</h3>
                <p>Authors: Hafez GhaemiHamed KebriaeiAlireza Ramezani MoghaddamMajid Nili Ahamdabadi</p>
                <p><a href="http://arxiv.org/abs/2402.05906v1">Link to paper</a></p>
                <p>Classical multi-agent reinforcement learning MARL assumes risk neutralityand complete objectivity for agents. However in settings where agents need toconsider or model human economic or social preferences a notion of risk mustbe incorporated into the RL optimization problem. This will be of greaterimportance in MARL where other human or non-human agents are involved possiblywith their own risk-sensitive policies. In this work we considerrisk-sensitive and non-cooperative MARL with cumulative prospect theory CPTa non-convex risk measure and a generalization of coherent measures of risk.CPT is capable of explaining loss aversion in humans and their tendency tooverestimate/underestimate small/large probabilities. We propose a distributedsampling-based actor-critic AC algorithm with CPT risk for networkaggregative Markov games NAMGs which we call Distributed Nested CPT-AC.Under a set of assumptions we prove the convergence of the algorithm to asubjective notion of Markov perfect Nash equilibrium in NAMGs. The experimentalresults show that subjective CPT policies obtained by our algorithm can bedifferent from the risk-neutral ones and agents with a higher loss aversionare more inclined to socially isolate themselves in an NAMG.</p>
                <p>Last Updated: 2024-02-08 18:43:27 UTC</p>
                <button class="interpret-button" data-id="2402.05906v1">Interpret</button>
                <div id="interpretation-2402.05906v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Cutsets and EF1 Fair Division of Graphs</h3>
                <p>Authors: Jiehua ChenWilliam S. Zwicker</p>
                <p><a href="http://arxiv.org/abs/2402.05884v1">Link to paper</a></p>
                <p>In fair division of a connected graph G  V E each of n agentsreceives a share of Gs vertex set V. These shares partition V with eachshare required to induce a connected subgraph. Agents use their own valuationfunctions to determine the non-negative numerical values of the shares whichdetermine whether the allocation is fair in some specified sense. We introduceforbidden substructures called graph cutsets which block divisions that arefair in the EF1 envy-free up to one item sense by cutting the graph into toomany pieces. Two parameters - gap and valence - determine blocked values ofn. If G guarantees connected EF1 allocations for n agents with valuationsthat are CA common and additive then G contains no elementary cutset ofgap k ge 2 and valence in the interval n - k  1 n - 1. If Gguarantees connected EF1 allocations for n agents with valuations in thebroader CM common and monotone class then G contains no cutset of gap kge 2 and valence in the interval n - k  1 n - 1. These results ruleout the existence of connected EF1 allocations in a variety of situations. Forsome graphs G we can with help from some new positive results pin downGs spectrum - the list of exactly which values of n do/do not guaranteeconnected EF1 allocations. Examples suggest a conjectured common spectralpattern for all graphs. Further we show that it is NP-hard to determinewhether a graph admits a cutset. We also provide an example of anon-traceable graph on eight vertices that has no cutsets of gap ge 2 atall yet fails to guarantee connected EF1 allocations for three agents with CApreferences.</p>
                <p>Last Updated: 2024-02-08 18:18:29 UTC</p>
                <button class="interpret-button" data-id="2402.05884v1">Interpret</button>
                <div id="interpretation-2402.05884v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices</h3>
                <p>Authors: Jiin WooLaixi ShiGauri JoshiYuejie Chi</p>
                <p><a href="http://arxiv.org/abs/2402.05876v1">Link to paper</a></p>
                <p>Offline reinforcement learning RL which seeks to learn an optimal policyusing offline data has garnered significant interest due to its potential incritical applications where online data collection is infeasible or expensive.This work explores the benefit of federated learning for offline RL aiming atcollaboratively leveraging offline datasets at multiple agents. Focusing onfinite-horizon episodic tabular Markov decision processes MDPs we designFedLCB-Q a variant of the popular model-free Q-learning algorithm tailored forfederated offline RL. FedLCB-Q updates local Q-functions at agents with novellearning rate schedules and aggregates them at a central server usingimportance averaging and a carefully designed pessimistic penalty term. Oursample complexity analysis reveals that with appropriately chosen parametersand synchronization schedules FedLCB-Q achieves linear speedup in terms of thenumber of agents without requiring high-quality datasets at individual agentsas long as the local datasets collectively cover the state-action space visitedby the optimal policy highlighting the power of collaboration in the federatedsetting. In fact the sample complexity almost matches that of the single-agentcounterpart as if all the data are stored at a central location up topolynomial factors of the horizon length. Furthermore FedLCB-Q iscommunication-efficient where the number of communication rounds is onlylinear with respect to the horizon length up to logarithmic factors.</p>
                <p>Last Updated: 2024-02-08 18:09:17 UTC</p>
                <button class="interpret-button" data-id="2402.05876v1">Interpret</button>
                <div id="interpretation-2402.05876v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Analysing the Sample Complexity of Opponent Shaping</h3>
                <p>Authors: Kitty FungQizhen ZhangChris LuJia WanTimon WilliJakob Foerster</p>
                <p><a href="http://arxiv.org/abs/2402.05782v1">Link to paper</a></p>
                <p>Learning in general-sum games often yields collectively sub-optimal results.Addressing this opponent shaping OS methods actively guide the learningprocesses of other agents empirically leading to improved individual and groupperformances in many settings. Early OS methods use higher-order derivatives toshape the learning of co-players making them unsuitable for shaping multiplelearning steps. Follow-up work Model-free Opponent Shaping M-FOS addressesthese by reframing the OS problem as a meta-game. In contrast to early OSmethods there is little theoretical understanding of the M-FOS framework.Providing theoretical guarantees for M-FOS is hard because A there is littleliterature on theoretical sample complexity bounds for meta-reinforcementlearning B M-FOS operates in continuous state and action spaces sotheoretical analysis is challenging. In this work we present R-FOS a tabularversion of M-FOS that is more suitable for theoretical analysis. R-FOSdiscretises the continuous meta-game MDP into a tabular MDP. Within thisdiscretised MDP we adapt the R_max algorithm most prominently used toderive PAC-bounds for MDPs as the meta-learner in the R-FOS algorithm. Wederive a sample complexity bound that is exponential in the cardinality of theinner state and action space and the number of agents. Our bound guaranteesthat with high probability the final policy learned by an R-FOS agent isclose to the optimal policy apart from a constant factor. Finally weinvestigate how R-FOSs sample complexity scales in the size of state-actionspace. Our theoretical results on scaling are supported empirically in theMatching Pennies environment.</p>
                <p>Last Updated: 2024-02-08 16:17:18 UTC</p>
                <button class="interpret-button" data-id="2402.05782v1">Interpret</button>
                <div id="interpretation-2402.05782v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-02-11</p>
        </div>
    
        </div>
    </body>
    </html>
    