
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Innovation Resistance Theory in Action: Unveiling Barriers to Open Government Data Adoption by Public Organizations to Unlock Open Data Innovation</h3>
                <p>Authors: Anastasija NikiforovaAntoine ClarinvalAnneke ZuiderwijkDaniel RudmarkPetar MilicKatrin Rajamäe-Soosaar</p>
                <p><a href="http://arxiv.org/abs/2407.10883v1">Link to paper</a></p>
                <p>Open Government Data OGD plays a pivotal role in fostering data-driveninnovation and sustainability across various sectors. Despite its potentialmany public organizations are reluctant to share their data openly. Whileexisting research has explored factors impacting the public organizationsintention to share OGD there is a paucity of research applying theoreticalmodels to investigate the resistance by public organizations to makinggovernment data publicly available. This study addresses the gap by developingan Innovation Resistance Theory IRT model tailored to OGD that allowsidentifying predictors of resistance among public agencies. We develop aninitial model based on literature and refine it through interviews with 21public agencies across six countries. The final model describes 39 barriersrelated to usage value risks tradition and image. The findings contributeto the literature by adapting IRT to the context of OGD an area where itsapplication has been notably limited. As such this study addresses the growingdemand for novel theoretical frameworks to examine OGD adoption barriers.Practical insights are provided to support policymakers in creating dataecosystems that encourage data openness and address challenges in OGD adoption.</p>
                <p>Last Updated: 2024-07-15 16:35:38 UTC</p>
                <button class="interpret-button" data-id="2407.10883v1">Interpret</button>
                <div id="interpretation-2407.10883v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Random Channel Ablation for Robust Hand Gesture Classification with Multimodal Biosignals</h3>
                <p>Authors: Keshav BimbrawJing LiuYe WangToshiaki Koike-Akino</p>
                <p><a href="http://arxiv.org/abs/2407.10874v1">Link to paper</a></p>
                <p>Biosignal-based hand gesture classification is an important component ofeffective human-machine interaction. For multimodal biosignal sensing themodalities often face data loss due to missing channels in the data which canadversely affect the gesture classification performance. To make theclassifiers robust to missing channels in the data this paper proposes usingRandom Channel Ablation RChA during the training process. Ultrasound andforce myography FMG data were acquired from the forearm for 12 hand gesturesover 2 subjects. The resulting multimodal data had 16 total channels 8 foreach modality. The proposed method was applied to convolutional neural networkarchitecture and compared with baseline imputation and oracle methods. Using5-fold cross-validation for the two subjects on average 12.2 and 24.5improvement was observed for gesture classification with up to 4 and 8 missingchannels respectively compared to the baseline. Notably the proposed method isalso robust to an increase in the number of missing channels compared to othermethods. These results show the efficacy of using random channel ablation toimprove classifier robustness for multimodal and multi-channel biosignal-basedhand gesture classification.</p>
                <p>Last Updated: 2024-07-15 16:23:53 UTC</p>
                <button class="interpret-button" data-id="2407.10874v1">Interpret</button>
                <div id="interpretation-2407.10874v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM</h3>
                <p>Authors: Keshav BimbrawYe WangJing LiuToshiaki Koike-Akino</p>
                <p><a href="http://arxiv.org/abs/2407.10870v1">Link to paper</a></p>
                <p>Large vision-language models LVLMs such as the Generative Pre-trainedTransformer 4-omni GPT-4o are emerging multi-modal foundation models whichhave great potential as powerful artificial-intelligence AI assistance toolsfor a myriad of applications including healthcare industrial and academicsectors. Although such foundation models perform well in a wide range ofgeneral tasks their capability without fine-tuning is often limited inspecialized tasks. However full fine-tuning of large foundation models ischallenging due to enormous computation/memory/dataset requirements. We showthat GPT-4o can decode hand gestures from forearm ultrasound data even with nofine-tuning and improves with few-shot in-context learning.</p>
                <p>Last Updated: 2024-07-15 16:18:06 UTC</p>
                <button class="interpret-button" data-id="2407.10870v1">Interpret</button>
                <div id="interpretation-2407.10870v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Interactive Public Transport Infrastructure Analysis through Mobility Profiles: Making the Mobility Transition Transparent</h3>
                <p>Authors: Yannick MetzDennis AckermannDaniel A. KeimMaximilian T. Fischer</p>
                <p><a href="http://arxiv.org/abs/2407.10791v1">Link to paper</a></p>
                <p>Efficient public transport systems are crucial for sustainable urbandevelopment as cities face increasing mobility demands. Yet many publictransport networks struggle to meet diverse user needs due to historicaldevelopment urban constraints and financial limitations. Traditionallyplanning of transport network structure is often based on limited surveysexpert opinions or partial usage statistics. This provides an incomplete basisfor decision-making. We introduce an data-driven approach to public transportplanning and optimization calculating detailed accessibility measures at theindividual housing level. Our visual analytics workflow combinespopulation-group-based simulations with dynamic infrastructure analysisutilizing a scenario-based model to simulate daily travel patterns of varieddemographic groups including schoolchildren students workers andpensioners. These population groups each with unique mobility requirements androutines interact with the transport system under different scenariostraveling to and from Points of Interest POI assessed through travel timecalculations. Results are visualized through heatmaps density maps andnetwork overlays as well as detailed statistics. Our system allows us toanalyze both the underlying data and simulation results on multiple levels ofgranularity delivering both broad insights and granular details. Case studieswith the city of Konstanz Germany reveal key areas where public transport doesnot meet specific needs confirmed through a formative user study. Due to thehigh cost of changing legacy networks our analysis facilitates theidentification of strategic enhancements such as optimized schedules orrerouting and few targeted stop relocations highlighting consequentialvariations in accessibility to pinpointing critical service gaps.</p>
                <p>Last Updated: 2024-07-15 15:09:14 UTC</p>
                <button class="interpret-button" data-id="2407.10791v1">Interpret</button>
                <div id="interpretation-2407.10791v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>XEQ Scale for Evaluating XAI Experience Quality Grounded in Psychometric Theory</h3>
                <p>Authors: Anjana WijekoonNirmalie WiratungaDavid CorsarKyle MartinIkechukwu Nkisi-OrjiBelen Díaz-AgudoDerek Bridge</p>
                <p><a href="http://arxiv.org/abs/2407.10662v1">Link to paper</a></p>
                <p>Explainable Artificial Intelligence XAI aims to improve the transparency ofautonomous decision-making through explanations. Recent literature hasemphasised users need for holistic multi-shot explanations and the abilityto personalise their engagement with XAI systems. We refer to this user-centredinteraction as an XAI Experience. Despite advances in creating XAI experiencesevaluating them in a user-centred manner has remained challenging. To addressthis we introduce the XAI Experience Quality XEQ Scale pronounced SeekScale for evaluating the user-centred quality of XAI experiences.Furthermore XEQ quantifies the quality of experiences across four evaluationdimensions: learning utility fulfilment and engagement. These contributionsextend the state-of-the-art of XAI evaluation moving beyond theone-dimensional metrics frequently developed to assess single-shotexplanations. In this paper we present the XEQ scale development andvalidation process including content validation with XAI experts as well asdiscriminant and construct validation through a large-scale pilot study. Outpilot study results offer strong evidence that establishes the XEQ Scale as acomprehensive framework for evaluating user-centred XAI experiences.</p>
                <p>Last Updated: 2024-07-15 12:25:49 UTC</p>
                <button class="interpret-button" data-id="2407.10662v1">Interpret</button>
                <div id="interpretation-2407.10662v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</h3>
                <p>Authors: Hongyu WangShuming MaRuiping WangFuru Wei</p>
                <p><a href="http://arxiv.org/abs/2407.10969v1">Link to paper</a></p>
                <p>We introduce Q-Sparse a simple yet effective approach to trainingsparsely-activated large language models LLMs. Q-Sparse enables full sparsityof activations in LLMs which can bring significant efficiency gains ininference. This is achieved by applying top-K sparsification to the activationsand the straight-through-estimator to the training. The key results from thiswork are 1 Q-Sparse can achieve results comparable to those of baseline LLMswhile being much more efficient at inference time 2 We present aninference-optimal scaling law for sparsely-activated LLMs 3 Q-Sparse iseffective in different settings including training-from-scratchcontinue-training of off-the-shelf LLMs and finetuning 4 Q-Sparse works forboth full-precision and 1-bit LLMs e.g. BitNet b1.58. Particularly thesynergy of BitNet b1.58 and Q-Sparse can be equipped with MoE provides thecornerstone and a clear path to revolutionize the efficiency including costand energy consumption of future LLMs.</p>
                <p>Last Updated: 2024-07-15 17:59:29 UTC</p>
                <button class="interpret-button" data-id="2407.10969v1">Interpret</button>
                <div id="interpretation-2407.10969v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?</h3>
                <p>Authors: Ruisheng CaoFangyu LeiHaoyuan WuJixuan ChenYeqiao FuHongcheng GaoXinzhuang XiongHanchong ZhangYuchen MaoWenjing HuTianbao XieHongshen XuDanyang ZhangSida WangRuoxi SunPengcheng YinCaiming XiongAnsong NiQian LiuVictor ZhongLu ChenKai YuTao Yu</p>
                <p><a href="http://arxiv.org/abs/2407.10956v1">Link to paper</a></p>
                <p>Data science and engineering workflows often span multiple stages fromwarehousing to orchestration using tools like BigQuery dbt and Airbyte. Asvision language models VLMs advance in multimodal understanding and codegeneration VLM-based agents could potentially automate these workflows bygenerating SQL queries Python code and GUI operations. This automation canimprove the productivity of experts while democratizing access to large-scaledata analysis. In this paper we introduce Spider2-V the first multimodalagent benchmark focusing on professional data science and engineeringworkflows featuring 494 real-world tasks in authentic computer environmentsand incorporating 20 enterprise-level professional applications. These tasksderived from real-world use cases evaluate the ability of a multimodal agentto perform data-related tasks by writing code and managing the GUI inenterprise data software systems. To balance realistic simulation withevaluation simplicity we devote significant effort to developing automaticconfigurations for task setup and carefully crafting evaluation metrics foreach task. Furthermore we supplement multimodal agents with comprehensivedocuments of these enterprise data software systems. Our empirical evaluationreveals that existing state-of-the-art LLM/VLM-based agents do not reliablyautomate full data workflows 14.0 success. Even with step-by-step guidancethese agents still underperform in tasks that require fine-grainedknowledge-intensive GUI actions 16.2 and involve remote cloud-hostedworkspaces 10.6. We hope that Spider2-V paves the way for autonomousmultimodal agents to transform the automation of data science and engineeringworkflow. Our code and data are available at https://spider2-v.github.io.</p>
                <p>Last Updated: 2024-07-15 17:54:37 UTC</p>
                <button class="interpret-button" data-id="2407.10956v1">Interpret</button>
                <div id="interpretation-2407.10956v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models</h3>
                <p>Authors: Chengguang GanQingyu YinXinyang HeHanjun WeiYunhao LiangYounghun LimShijian WangHexiang HuangQinghao ZhangShiwen NiTatsunori Mori</p>
                <p><a href="http://arxiv.org/abs/2407.10953v1">Link to paper</a></p>
                <p>The Mutual Reinforcement Effect MRE represents a promising avenue ininformation extraction and multitasking research. Nevertheless itsapplicability has been constrained due to the exclusive availability of MRE mixdatasets in Japanese thereby limiting comprehensive exploration by the globalresearch community. To address this limitation we introduce a Multilingual MREmix dataset MMM that encompasses 21 sub-datasets in English Japanese andChinese. In this paper we also propose a method for dataset translationassisted by Large Language Models LLMs which significantly reduces themanual annotation time required for dataset construction by leveraging LLMs totranslate the original Japanese datasets. Additionally we have enriched thedataset by incorporating open-domain Named Entity Recognition NER andsentence classification tasks. Utilizing this expanded dataset we developed aunified input-output framework to train an Open-domain Information ExtractionLarge Language Model OIELLM. The OIELLM model demonstrates the capability toeffectively process novel MMM datasets exhibiting significant improvements inperformance.</p>
                <p>Last Updated: 2024-07-15 17:50:43 UTC</p>
                <button class="interpret-button" data-id="2407.10953v1">Interpret</button>
                <div id="interpretation-2407.10953v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Representing Rule-based Chatbots with Transformers</h3>
                <p>Authors: Dan FriedmanAbhishek PanigrahiDanqi Chen</p>
                <p><a href="http://arxiv.org/abs/2407.10949v1">Link to paper</a></p>
                <p>Transformer-based chatbots can conduct fluent natural-soundingconversations but we have limited understanding of the mechanisms underlyingtheir behavior. Prior work has taken a bottom-up approach to understandingTransformers by constructing Transformers for various synthetic and formallanguage tasks such as regular expressions and Dyck languages. However it isnot obvious how to extend this approach to understand more naturalisticconversational agents. In this work we take a step in this direction byconstructing a Transformer that implements the ELIZA program a classicrule-based chatbot. ELIZA illustrates some of the distinctive challenges of theconversational setting including both local pattern matching and long-termdialog state tracking. We build on constructions from prior work -- inparticular for simulating finite-state automata -- showing how simplerconstructions can be composed and extended to give rise to more sophisticatedbehavior. Next we train Transformers on a dataset of synthetically generatedELIZA conversations and investigate the mechanisms the models learn. Ouranalysis illustrates the kinds of mechanisms these models tend to prefer -- forexample models favor an induction head mechanism over a more precise positionbased copying mechanism and using intermediate generations to simulaterecurrent data structures like ELIZAs memory mechanisms. Overall by drawingan explicit connection between neural chatbots and interpretable symbolicmechanisms our results offer a new setting for mechanistic analysis ofconversational agents.</p>
                <p>Last Updated: 2024-07-15 17:45:53 UTC</p>
                <button class="interpret-button" data-id="2407.10949v1">Interpret</button>
                <div id="interpretation-2407.10949v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning from Naturally Occurring Feedback</h3>
                <p>Authors: Shachar Don-YehiyaLeshem ChoshenOmri Abend</p>
                <p><a href="http://arxiv.org/abs/2407.10944v1">Link to paper</a></p>
                <p>Human feedback data is a critical component in developing language models.However collecting this feedback is costly and ultimately not scalable. Wepropose a scalable method for extracting feedback that users naturally includewhen interacting with chat models and leveraging it for model training. We arefurther motivated by previous work that showed there are also qualitativeadvantages to using naturalistic rather than auto-generated feedback such asless hallucinations and biases. We manually annotated conversation data toconfirm the presence of naturally occurring feedback in a standard corpusfinding that as much as 30 of the chats include explicit feedback. We applyour method to over 1M conversations to obtain hundreds of thousands of feedbacksamples. Training with the extracted feedback shows significant performanceimprovements over baseline models demonstrating the efficacy of our approachin enhancing model alignment to human preferences.</p>
                <p>Last Updated: 2024-07-15 17:41:34 UTC</p>
                <button class="interpret-button" data-id="2407.10944v1">Interpret</button>
                <div id="interpretation-2407.10944v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>GuideLight: "Industrial Solution" Guidance for More Practical Traffic Signal Control Agents</h3>
                <p>Authors: Haoyuan JiangXuantang XiongZiyue LiHangyu MaoGuanghu SuiJingqing RuanYuheng ChengHua WeiWolfgang KetterRui Zhao</p>
                <p><a href="http://arxiv.org/abs/2407.10811v1">Link to paper</a></p>
                <p>Currently traffic signal control TSC methods based on reinforcementlearning RL have proven superior to traditional methods. However most RLmethods face difficulties when applied in the real world due to three factors:input output and the cycle-flow relation. The industrys observable input ismuch more limited than simulation-based RL methods. For real-world solutionsonly flow can be reliably collected whereas common RL methods need more. Forthe output action most RL methods focus on acyclic control which real-worldsignal controllers do not support. Most importantly industry standards requirea consistent cycle-flow relationship: non-decreasing and different responsestrategies for low medium and high-level flows which is ignored by the RLmethods. To narrow the gap between RL methods and industry standards weinnovatively propose to use industry solutions to guide the RL agent.Specifically we design behavior cloning and curriculum learning to guide theagent to mimic and meet industry requirements and at the same time leveragethe power of exploration and exploitation in RL for better performance. Wetheoretically prove that such guidance can largely decrease the samplecomplexity to polynomials in the horizon when searching for an optimal policy.Our rigid experiments show that our method has good cycle-flow relation andsuperior performance.</p>
                <p>Last Updated: 2024-07-15 15:26:10 UTC</p>
                <button class="interpret-button" data-id="2407.10811v1">Interpret</button>
                <div id="interpretation-2407.10811v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Communication- and Computation-Efficient Distributed Decision-Making in Multi-Robot Networks</h3>
                <p>Authors: Zirui XuSandilya Sai GarimellaVasileios Tzoumas</p>
                <p><a href="http://arxiv.org/abs/2407.10382v1">Link to paper</a></p>
                <p>We provide a distributed coordination paradigm that enables scalable andnear-optimal joint motion planning among multiple robots. Our coordinationparadigm contrasts with current paradigms that are either near-optimal butimpractical for replanning times or real-time but offer no near-optimalityguarantees. We are motivated by the future of collaborative mobile autonomywhere distributed teams of robots will coordinate via vehicle-to-vehicle v2vcommunication to execute information-heavy tasks like mapping surveillanceand target tracking. To enable rapid distributed coordination we must curtailthe explosion of information-sharing across the network thus limiting robotcoordination. However this can lead to suboptimal plans causing overlappingtrajectories instead of complementary ones. We make theoretical and algorithmiccontributions to balance the trade-off between decision speed and optimality.We introduce tools for distributed submodular optimization a diminishingreturns property in information-gathering tasks. Theoretically we analyze howlocal network topology affects near-optimality at the global level.Algorithmically we provide a communication- and computation-efficientcoordination algorithm for agents to balance the trade-off. Our algorithm is upto two orders faster than competitive near-optimal algorithms. In simulationsof surveillance tasks with up to 45 robots it enables real-time planning atthe order of 1 Hz with superior coverage performance. To enable thesimulations we provide a high-fidelity simulator that extends AirSim byintegrating a collaborative autonomy pipeline and simulating v2v communicationdelays.</p>
                <p>Last Updated: 2024-07-15 01:25:39 UTC</p>
                <button class="interpret-button" data-id="2407.10382v1">Interpret</button>
                <div id="interpretation-2407.10382v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding</h3>
                <p>Authors: Chang LeiHuan Lei</p>
                <p><a href="http://arxiv.org/abs/2407.10279v1">Link to paper</a></p>
                <p>Artificial intelligence for card games has long been a popular topic in AIresearch. In recent years complex card games like Mahjong and Texas Holdemhave been solved with corresponding AI programs reaching the level of humanexperts. However the game of Dou Di Zhu presents significant challenges due toits vast state/action space and unique characteristics involving reasoningabout competition and cooperation making the game extremely difficult tosolve.The RL model DouZero trained using the Deep Monte Carlo algorithmframework has shown excellent performance in DouDiZhu. However there aredifferences between its simplified game environment and the actual Dou Di Zhuenvironment and its performance is still a considerable distance from that ofhuman experts. This paper modifies the Deep Monte Carlo algorithm framework byusing reinforcement learning to obtain a neural network that simultaneouslyestimates win rates and expectations. The action space is pruned usingexpectations and strategies are generated based on win rates. This RL model istrained in a realistic DouDiZhu environment and achieves a state-of-the-artlevel among publicly available models.</p>
                <p>Last Updated: 2024-07-14 17:32:36 UTC</p>
                <button class="interpret-button" data-id="2407.10279v1">Interpret</button>
                <div id="interpretation-2407.10279v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning to Steer Markovian Agents under Model Uncertainty</h3>
                <p>Authors: Jiawei HuangVinzenz ThomaZebang ShenHeinrich H. NaxNiao He</p>
                <p><a href="http://arxiv.org/abs/2407.10207v1">Link to paper</a></p>
                <p>Designing incentives for an adapting population is a ubiquitous problem in awide array of economic applications and beyond. In this work we study how todesign additional rewards to steer multi-agent systems towards desired policiesemphwithout prior knowledge of the agents underlying learning dynamics. Weintroduce a model-based non-episodic Reinforcement Learning RL formulationfor our steering problem. Importantly we focus on learning aemphhistory-dependent steering strategy to handle the inherent modeluncertainty about the agents learning dynamics. We introduce a novel objectivefunction to encode the desiderata of achieving a good steering outcome withreasonable cost. Theoretically we identify conditions for the existence ofsteering strategies to guide agents to the desired policies. Complementing ourtheoretical contributions we provide empirical algorithms to approximatelysolve our objective which effectively tackles the challenge in learninghistory-dependent strategies. We demonstrate the efficacy of our algorithmsthrough empirical evaluations.</p>
                <p>Last Updated: 2024-07-14 14:01:38 UTC</p>
                <button class="interpret-button" data-id="2407.10207v1">Interpret</button>
                <div id="interpretation-2407.10207v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Revolutionizing Bridge Operation and maintenance with LLM-based Agents: An Overview of Applications and Insights</h3>
                <p>Authors: Xinyu-ChenYanwen-ZhuYang-HouLianzhen-Zhang</p>
                <p><a href="http://arxiv.org/abs/2407.10064v1">Link to paper</a></p>
                <p>In various industrial fields of human social development people have beenexploring methods aimed at freeing human labor. Constructing LLM-based agentsis considered to be one of the most effective tools to achieve this goal.Agent as a kind of human-like intelligent entity with the ability ofperception planning decision-making and action has created great productionvalue in many fields. However the bridge OM field shows a relatively lowlevel of intelligence compared to other industries. Nevertheless the bridgeOM field has developed numerous intelligent inspection devices machinelearning algorithms and autonomous evaluation and decision-making methodswhich provide a feasible basis for breakthroughs in artificial intelligence inthis field. The aim of this study is to explore the impact of AI bodies basedon large-scale language models on the field of bridge OM and to analyze thepotential challenges and opportunities it brings to the core tasks of bridgeOM. Through in-depth research and analysis this paper expects to provide amore comprehensive perspective for understanding the application ofintelligentsia in this field.</p>
                <p>Last Updated: 2024-07-14 03:31:33 UTC</p>
                <button class="interpret-button" data-id="2407.10064v1">Interpret</button>
                <div id="interpretation-2407.10064v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation</h3>
                <p>Authors: Bocheng ZouMu CaiJianrui ZhangYong Jae Lee</p>
                <p><a href="http://arxiv.org/abs/2407.10972v1">Link to paper</a></p>
                <p>In the realm of vision models the primary mode of representation is usingpixels to rasterize the visual world. Yet this is not always the best or uniqueway to represent visual content especially for designers and artists whodepict the world using geometry primitives such as polygons. Vector graphicsVG on the other hand offer a textual representation of visual contentwhich can be more concise and powerful for content like cartoons or sketches.Recent studies have shown promising results on processing vector graphics withcapable Large Language Models LLMs. However such works focus solely onqualitative results understanding or a specific type of vector graphics. Wepropose VGBench a comprehensive benchmark for LLMs on handling vector graphicsthrough diverse aspects including a both visual understanding andgeneration b evaluation of various vector graphics formats c diversequestion types d wide range of prompting techniques e under multipleLLMs. Evaluating on our collected 4279 understanding and 5845 generationsamples we find that LLMs show strong capability on both aspects whileexhibiting less desirable performance on low-level formats SVG. Both data andevaluation pipeline will be open-sourced at https://vgbench.github.io.</p>
                <p>Last Updated: 2024-07-15 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2407.10972v1">Interpret</button>
                <div id="interpretation-2407.10972v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Walking the Values in Bayesian Inverse Reinforcement Learning</h3>
                <p>Authors: Ondrej BajgarAlessandro AbateKonstantinos GatsisMichael A. Osborne</p>
                <p><a href="http://arxiv.org/abs/2407.10971v1">Link to paper</a></p>
                <p>The goal of Bayesian inverse reinforcement learning IRL is recovering aposterior distribution over reward functions using a set of demonstrations froman expert optimizing for a reward unknown to the learner. The resultingposterior over rewards can then be used to synthesize an apprentice policy thatperforms well on the same or a similar task. A key challenge in Bayesian IRL isbridging the computational gap between the hypothesis space of possible rewardsand the likelihood often defined in terms of Q values: vanilla Bayesian IRLneeds to solve the costly forward planning problem - going from rewards to theQ values - at every step of the algorithm which may need to be done thousandsof times. We propose to solve this by a simple change: instead of focusing onprimarily sampling in the space of rewards we can focus on primarily workingin the space of Q-values since the computation required to go from Q-values toreward is radically cheaper. Furthermore this reversion of the computationmakes it easy to compute the gradient allowing efficient sampling usingHamiltonian Monte Carlo. We propose ValueWalk - a new Markov chain Monte Carlomethod based on this insight - and illustrate its advantages on several tasks.</p>
                <p>Last Updated: 2024-07-15 17:59:52 UTC</p>
                <button class="interpret-button" data-id="2407.10971v1">Interpret</button>
                <div id="interpretation-2407.10971v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</h3>
                <p>Authors: Hongyu WangShuming MaRuiping WangFuru Wei</p>
                <p><a href="http://arxiv.org/abs/2407.10969v1">Link to paper</a></p>
                <p>We introduce Q-Sparse a simple yet effective approach to trainingsparsely-activated large language models LLMs. Q-Sparse enables full sparsityof activations in LLMs which can bring significant efficiency gains ininference. This is achieved by applying top-K sparsification to the activationsand the straight-through-estimator to the training. The key results from thiswork are 1 Q-Sparse can achieve results comparable to those of baseline LLMswhile being much more efficient at inference time 2 We present aninference-optimal scaling law for sparsely-activated LLMs 3 Q-Sparse iseffective in different settings including training-from-scratchcontinue-training of off-the-shelf LLMs and finetuning 4 Q-Sparse works forboth full-precision and 1-bit LLMs e.g. BitNet b1.58. Particularly thesynergy of BitNet b1.58 and Q-Sparse can be equipped with MoE provides thecornerstone and a clear path to revolutionize the efficiency including costand energy consumption of future LLMs.</p>
                <p>Last Updated: 2024-07-15 17:59:29 UTC</p>
                <button class="interpret-button" data-id="2407.10969v1">Interpret</button>
                <div id="interpretation-2407.10969v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning</h3>
                <p>Authors: Haohong LinWenhao DingJian ChenLaixi ShiJiacheng ZhuBo LiDing Zhao</p>
                <p><a href="http://arxiv.org/abs/2407.10967v1">Link to paper</a></p>
                <p>Offline model-based reinforcement learning MBRL enhances data efficiency byutilizing pre-collected datasets to learn models and policies especially inscenarios where exploration is costly or infeasible. Nevertheless itsperformance often suffers from the objective mismatch between model and policylearning resulting in inferior performance despite accurate model predictions.This paper first identifies the primary source of this mismatch comes from theunderlying confounders present in offline data for MBRL. Subsequently weintroduce textbfBilintextbfEar textbfCAUSalrtextbfEpresentationBECAUSE an algorithm to capture causalrepresentation for both states and actions to reduce the influence of thedistribution shift thus mitigating the objective mismatch problem.Comprehensive evaluations on 18 tasks that vary in data quality and environmentcontext demonstrate the superior performance of BECAUSE over existing offlineRL algorithms. We show the generalizability and robustness of BECAUSE underfewer samples or larger numbers of confounders. Additionally we offertheoretical analysis of BECAUSE to prove its error bound and sample efficiencywhen integrating causal representation into offline MBRL.</p>
                <p>Last Updated: 2024-07-15 17:59:23 UTC</p>
                <button class="interpret-button" data-id="2407.10967v1">Interpret</button>
                <div id="interpretation-2407.10967v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations</h3>
                <p>Authors: Walter SimonciniSpyros GidarisAndrei BursucYuki M. Asano</p>
                <p><a href="http://arxiv.org/abs/2407.10964v1">Link to paper</a></p>
                <p>This paper introduces FUNGI Features from UNsupervised GradIents a methodto enhance the features of vision encoders by leveraging self-supervisedgradients. Our method is simple: given any pretrained model we first computegradients from various self-supervised objectives for each input. These areprojected to a lower dimension and then concatenated with the modelsembedding. The resulting features are evaluated on k-nearest neighborclassification over 11 datasets from vision 5 from natural languageprocessing and 2 from audio. Across backbones spanning various sizes andpretraining strategies FUNGI features provide consistent performanceimprovements over the embeddings. We also show that using FUNGI features canbenefit linear classification and image retrieval and that they significantlyimprove the retrieval-based in-context scene understanding abilities ofpretrained models for example improving upon DINO by 17 for semanticsegmentation - without any training.</p>
                <p>Last Updated: 2024-07-15 17:58:42 UTC</p>
                <button class="interpret-button" data-id="2407.10964v1">Interpret</button>
                <div id="interpretation-2407.10964v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation</h3>
                <p>Authors: Bocheng ZouMu CaiJianrui ZhangYong Jae Lee</p>
                <p><a href="http://arxiv.org/abs/2407.10972v1">Link to paper</a></p>
                <p>In the realm of vision models the primary mode of representation is usingpixels to rasterize the visual world. Yet this is not always the best or uniqueway to represent visual content especially for designers and artists whodepict the world using geometry primitives such as polygons. Vector graphicsVG on the other hand offer a textual representation of visual contentwhich can be more concise and powerful for content like cartoons or sketches.Recent studies have shown promising results on processing vector graphics withcapable Large Language Models LLMs. However such works focus solely onqualitative results understanding or a specific type of vector graphics. Wepropose VGBench a comprehensive benchmark for LLMs on handling vector graphicsthrough diverse aspects including a both visual understanding andgeneration b evaluation of various vector graphics formats c diversequestion types d wide range of prompting techniques e under multipleLLMs. Evaluating on our collected 4279 understanding and 5845 generationsamples we find that LLMs show strong capability on both aspects whileexhibiting less desirable performance on low-level formats SVG. Both data andevaluation pipeline will be open-sourced at https://vgbench.github.io.</p>
                <p>Last Updated: 2024-07-15 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2407.10972v1">Interpret</button>
                <div id="interpretation-2407.10972v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations</h3>
                <p>Authors: Walter SimonciniSpyros GidarisAndrei BursucYuki M. Asano</p>
                <p><a href="http://arxiv.org/abs/2407.10964v1">Link to paper</a></p>
                <p>This paper introduces FUNGI Features from UNsupervised GradIents a methodto enhance the features of vision encoders by leveraging self-supervisedgradients. Our method is simple: given any pretrained model we first computegradients from various self-supervised objectives for each input. These areprojected to a lower dimension and then concatenated with the modelsembedding. The resulting features are evaluated on k-nearest neighborclassification over 11 datasets from vision 5 from natural languageprocessing and 2 from audio. Across backbones spanning various sizes andpretraining strategies FUNGI features provide consistent performanceimprovements over the embeddings. We also show that using FUNGI features canbenefit linear classification and image retrieval and that they significantlyimprove the retrieval-based in-context scene understanding abilities ofpretrained models for example improving upon DINO by 17 for semanticsegmentation - without any training.</p>
                <p>Last Updated: 2024-07-15 17:58:42 UTC</p>
                <button class="interpret-button" data-id="2407.10964v1">Interpret</button>
                <div id="interpretation-2407.10964v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>InVi: Object Insertion In Videos Using Off-the-Shelf Diffusion Models</h3>
                <p>Authors: Nirat SainiNavaneeth BodlaAshish ShrivastavaAvinash RavichandranXiao ZhangAbhinav ShrivastavaBharat Singh</p>
                <p><a href="http://arxiv.org/abs/2407.10958v1">Link to paper</a></p>
                <p>We introduce InVi an approach for inserting or replacing objects withinvideos referred to as inpainting using off-the-shelf text-to-image latentdiffusion models. InVi targets controlled manipulation of objects and blendingthem seamlessly into a background video unlike existing video editing methodsthat focus on comprehensive re-styling or entire scene alterations. To achievethis goal we tackle two key challenges. Firstly for high quality control andblending we employ a two-step process involving inpainting and matching. Thisprocess begins with inserting the object into a single frame using aControlNet-based inpainting diffusion model and then generating subsequentframes conditioned on features from an inpainted frame as an anchor to minimizethe domain gap between the background and the object. Secondly to ensuretemporal coherence we replace the diffusion models self-attention layers withextended-attention layers. The anchor frame features serve as the keys andvalues for these layers enhancing consistency across frames. Our approachremoves the need for video-specific fine-tuning presenting an efficient andadaptable solution. Experimental results demonstrate that InVi achievesrealistic object insertion with consistent blending and coherence acrossframes outperforming existing methods.</p>
                <p>Last Updated: 2024-07-15 17:55:09 UTC</p>
                <button class="interpret-button" data-id="2407.10958v1">Interpret</button>
                <div id="interpretation-2407.10958v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes</h3>
                <p>Authors: Yaoting WangPeiwen SunDongzhan ZhouGuangyao LiHonggang ZhangDi Hu</p>
                <p><a href="http://arxiv.org/abs/2407.10957v1">Link to paper</a></p>
                <p>Traditional reference segmentation tasks have predominantly focused on silentvisual scenes neglecting the integral role of multimodal perception andinteraction in human experiences. In this work we introduce a novel taskcalled Reference Audio-Visual Segmentation Ref-AVS which seeks to segmentobjects within the visual domain based on expressions containing multimodalcues. Such expressions are articulated in natural language forms but areenriched with multimodal cues including audio and visual descriptions. Tofacilitate this research we construct the first Ref-AVS benchmark whichprovides pixel-level annotations for objects described in correspondingmultimodal-cue expressions. To tackle the Ref-AVS task we propose a new methodthat adequately utilizes multimodal cues to offer precise segmentationguidance. Finally we conduct quantitative and qualitative experiments on threetest subsets to compare our approach with existing methods from related tasks.The results demonstrate the effectiveness of our method highlighting itscapability to precisely segment objects using multimodal-cue expressions.Dataset is available athrefhttps://gewu-lab.github.io/Ref-AVShttps://gewu-lab.github.io/Ref-AVS.</p>
                <p>Last Updated: 2024-07-15 17:54:45 UTC</p>
                <button class="interpret-button" data-id="2407.10957v1">Interpret</button>
                <div id="interpretation-2407.10957v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Can Textual Semantics Mitigate Sounding Object Segmentation Preference?</h3>
                <p>Authors: Yaoting WangPeiwen SunYuanchao LiHonggang ZhangDi Hu</p>
                <p><a href="http://arxiv.org/abs/2407.10947v1">Link to paper</a></p>
                <p>The Audio-Visual Segmentation AVS task aims to segment sounding objects inthe visual space using audio cues. However in this work it is recognized thatprevious AVS methods show a heavy reliance on detrimental segmentationpreferences related to audible objects rather than precise audio guidance. Weargue that the primary reason is that audio lacks robust semantics compared tovision especially in multi-source sounding scenes resulting in weak audioguidance over the visual space. Motivated by the the fact that text modality iswell explored and contains rich abstract semantics we propose leveraging textcues from the visual scene to enhance audio guidance with the semanticsinherent in text. Our approach begins by obtaining scene descriptions throughan off-the-shelf image captioner and prompting a frozen large language model todeduce potential sounding objects as text cues. Subsequently we introduce anovel semantics-driven audio modeling module with a dynamic mask to integrateaudio features with text cues leading to representative sounding objectfeatures. These features not only encompass audio cues but also possess vividsemantics providing clearer guidance in the visual space. Experimental resultson AVS benchmarks validate that our method exhibits enhanced sensitivity toaudio when aided by text cues achieving highly competitive performance on allthree subsets. Project page:hrefhttps://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preferencehttps://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference</p>
                <p>Last Updated: 2024-07-15 17:45:20 UTC</p>
                <button class="interpret-button" data-id="2407.10947v1">Interpret</button>
                <div id="interpretation-2407.10947v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion</h3>
                <p>Authors: Yongyuan LiangTingqiang XuKaizhe HuGuangqi JiangFurong HuangHuazhe Xu</p>
                <p><a href="http://arxiv.org/abs/2407.10973v1">Link to paper</a></p>
                <p>Can we generate a control policy for an agent using just one demonstration ofdesired behaviors as a prompt as effortlessly as creating an image from atextual description In this paper we present Make-An-Agent a novel policyparameter generator that leverages the power of conditional diffusion modelsfor behavior-to-policy generation. Guided by behavior embeddings that encodetrajectory information our policy generator synthesizes latent parameterrepresentations which can then be decoded into policy networks. Trained onpolicy network checkpoints and their corresponding trajectories our generationmodel demonstrates remarkable versatility and scalability on multiple tasks andhas a strong generalization ability on unseen tasks to output well-performedpolicies with only few-shot demonstrations as inputs. We showcase its efficacyand efficiency on various domains and tasks including varying objectivesbehaviors and even across different robot manipulators. Beyond simulation wedirectly deploy policies generated by Make-An-Agent onto real-world robots onlocomotion tasks.</p>
                <p>Last Updated: 2024-07-15 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2407.10973v1">Interpret</button>
                <div id="interpretation-2407.10973v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation</h3>
                <p>Authors: Bocheng ZouMu CaiJianrui ZhangYong Jae Lee</p>
                <p><a href="http://arxiv.org/abs/2407.10972v1">Link to paper</a></p>
                <p>In the realm of vision models the primary mode of representation is usingpixels to rasterize the visual world. Yet this is not always the best or uniqueway to represent visual content especially for designers and artists whodepict the world using geometry primitives such as polygons. Vector graphicsVG on the other hand offer a textual representation of visual contentwhich can be more concise and powerful for content like cartoons or sketches.Recent studies have shown promising results on processing vector graphics withcapable Large Language Models LLMs. However such works focus solely onqualitative results understanding or a specific type of vector graphics. Wepropose VGBench a comprehensive benchmark for LLMs on handling vector graphicsthrough diverse aspects including a both visual understanding andgeneration b evaluation of various vector graphics formats c diversequestion types d wide range of prompting techniques e under multipleLLMs. Evaluating on our collected 4279 understanding and 5845 generationsamples we find that LLMs show strong capability on both aspects whileexhibiting less desirable performance on low-level formats SVG. Both data andevaluation pipeline will be open-sourced at https://vgbench.github.io.</p>
                <p>Last Updated: 2024-07-15 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2407.10972v1">Interpret</button>
                <div id="interpretation-2407.10972v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes</h3>
                <p>Authors: Yaoting WangPeiwen SunDongzhan ZhouGuangyao LiHonggang ZhangDi Hu</p>
                <p><a href="http://arxiv.org/abs/2407.10957v1">Link to paper</a></p>
                <p>Traditional reference segmentation tasks have predominantly focused on silentvisual scenes neglecting the integral role of multimodal perception andinteraction in human experiences. In this work we introduce a novel taskcalled Reference Audio-Visual Segmentation Ref-AVS which seeks to segmentobjects within the visual domain based on expressions containing multimodalcues. Such expressions are articulated in natural language forms but areenriched with multimodal cues including audio and visual descriptions. Tofacilitate this research we construct the first Ref-AVS benchmark whichprovides pixel-level annotations for objects described in correspondingmultimodal-cue expressions. To tackle the Ref-AVS task we propose a new methodthat adequately utilizes multimodal cues to offer precise segmentationguidance. Finally we conduct quantitative and qualitative experiments on threetest subsets to compare our approach with existing methods from related tasks.The results demonstrate the effectiveness of our method highlighting itscapability to precisely segment objects using multimodal-cue expressions.Dataset is available athrefhttps://gewu-lab.github.io/Ref-AVShttps://gewu-lab.github.io/Ref-AVS.</p>
                <p>Last Updated: 2024-07-15 17:54:45 UTC</p>
                <button class="interpret-button" data-id="2407.10957v1">Interpret</button>
                <div id="interpretation-2407.10957v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?</h3>
                <p>Authors: Ruisheng CaoFangyu LeiHaoyuan WuJixuan ChenYeqiao FuHongcheng GaoXinzhuang XiongHanchong ZhangYuchen MaoWenjing HuTianbao XieHongshen XuDanyang ZhangSida WangRuoxi SunPengcheng YinCaiming XiongAnsong NiQian LiuVictor ZhongLu ChenKai YuTao Yu</p>
                <p><a href="http://arxiv.org/abs/2407.10956v1">Link to paper</a></p>
                <p>Data science and engineering workflows often span multiple stages fromwarehousing to orchestration using tools like BigQuery dbt and Airbyte. Asvision language models VLMs advance in multimodal understanding and codegeneration VLM-based agents could potentially automate these workflows bygenerating SQL queries Python code and GUI operations. This automation canimprove the productivity of experts while democratizing access to large-scaledata analysis. In this paper we introduce Spider2-V the first multimodalagent benchmark focusing on professional data science and engineeringworkflows featuring 494 real-world tasks in authentic computer environmentsand incorporating 20 enterprise-level professional applications. These tasksderived from real-world use cases evaluate the ability of a multimodal agentto perform data-related tasks by writing code and managing the GUI inenterprise data software systems. To balance realistic simulation withevaluation simplicity we devote significant effort to developing automaticconfigurations for task setup and carefully crafting evaluation metrics foreach task. Furthermore we supplement multimodal agents with comprehensivedocuments of these enterprise data software systems. Our empirical evaluationreveals that existing state-of-the-art LLM/VLM-based agents do not reliablyautomate full data workflows 14.0 success. Even with step-by-step guidancethese agents still underperform in tasks that require fine-grainedknowledge-intensive GUI actions 16.2 and involve remote cloud-hostedworkspaces 10.6. We hope that Spider2-V paves the way for autonomousmultimodal agents to transform the automation of data science and engineeringworkflow. Our code and data are available at https://spider2-v.github.io.</p>
                <p>Last Updated: 2024-07-15 17:54:37 UTC</p>
                <button class="interpret-button" data-id="2407.10956v1">Interpret</button>
                <div id="interpretation-2407.10956v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together</h3>
                <p>Authors: Dilara SoyluChristopher PottsOmar Khattab</p>
                <p><a href="http://arxiv.org/abs/2407.10930v1">Link to paper</a></p>
                <p>Natural Language Processing NLP systems are increasingly taking the form ofmulti-stage pipelines involving multiple distinct language models LMs andprompting strategies. Here we address the question of how to fine-tune suchsystems to improve their performance. We cast this as a problem of optimizingthe underlying LM weights and the prompting strategies together and consider achallenging but highly realistic scenario in which we have no gold labels forany intermediate stages in the pipeline. To address this challenge we evaluateapproximate optimization strategies in which we bootstrap training labels forall pipeline stages and use these to optimize the pipelines prompts andfine-tune its weights alternatingly. In experiments with multi-hop QAmathematical reasoning and feature-based classification we find that simpleapproaches for optimizing the prompts and weights together outperform directlyoptimizing weights alone and prompts alone by up to 65 and 5 respectivelyon average across LMs and tasks. We will release our new optimizers in DSPy athttp://dspy.ai</p>
                <p>Last Updated: 2024-07-15 17:30:31 UTC</p>
                <button class="interpret-button" data-id="2407.10930v1">Interpret</button>
                <div id="interpretation-2407.10930v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>A unified theory and statistical learning approach for traffic conflict detection</h3>
                <p>Authors: Yiru JiaoSimeon C. CalvertSander van CranenburghHans van Lint</p>
                <p><a href="http://arxiv.org/abs/2407.10959v1">Link to paper</a></p>
                <p>This study proposes a unified theory and statistical learning approach fortraffic conflict detection addressing the long-existing call for a consistentand comprehensive methodology to evaluate the collision risk emerged in roaduser interactions. The proposed theory assumes a context-dependentprobabilistic collision risk and frames conflict detection as estimating therisk by statistical learning from observed proximities and contextualvariables. Three primary tasks are integrated: representing interaction contextfrom selected observables inferring proximity distributions in differentcontexts and applying extreme value theory to relate conflict intensity withconflict probability. As a result this methodology is adaptable to variousroad users and interaction scenarios enhancing its applicability without theneed for pre-labelled conflict data. Demonstration experiments are executedusing real-world trajectory data with the unified metric trained onlane-changing interactions on German highways and applied to near-crash eventsfrom the 100-Car Naturalistic Driving Study in the U.S. The experimentsdemonstrate the methodologys ability to provide effective collision warningsgeneralise across different datasets and traffic environments cover a broadrange of conflicts and deliver a long-tailed distribution of conflictintensity. This study contributes to traffic safety by offering a consistentand explainable methodology for conflict detection applicable across variousscenarios. Its societal implications include enhanced safety evaluations oftraffic infrastructures more effective collision warning systems forautonomous and driving assistance systems and a deeper understanding of roaduser behaviour in different traffic conditions contributing to a potentialreduction in accident rates and improving overall traffic safety.</p>
                <p>Last Updated: 2024-07-15 17:55:36 UTC</p>
                <button class="interpret-button" data-id="2407.10959v1">Interpret</button>
                <div id="interpretation-2407.10959v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Enhancing Stochastic Optimization for Statistical Efficiency Using ROOT-SGD with Diminishing Stepsize</h3>
                <p>Authors: Tong ZhangChris Junchi Li</p>
                <p><a href="http://arxiv.org/abs/2407.10955v1">Link to paper</a></p>
                <p>In this paper we revisit textsfROOT-SGD an innovative method forstochastic optimization to bridge the gap between stochastic optimization andstatistical efficiency. The proposed method enhances the performance andreliability of textsfROOT-SGD by integrating a carefully designedemphdiminishing stepsize strategy. This approach addresses key challenges inoptimization providing robust theoretical guarantees and practical benefits.Our analysis demonstrates that textsfROOT-SGD with diminishing achievesoptimal convergence rates while maintaining computational efficiency. Bydynamically adjusting the learning rate textsfROOT-SGD ensures improvedstability and precision throughout the optimization process. The findings ofthis study offer valuable insights for developing advanced optimizationalgorithms that are both efficient and statistically robust.</p>
                <p>Last Updated: 2024-07-15 17:54:03 UTC</p>
                <button class="interpret-button" data-id="2407.10955v1">Interpret</button>
                <div id="interpretation-2407.10955v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SLIP: Securing LLMs IP Using Weights Decomposition</h3>
                <p>Authors: Yehonathan RefaelAdam HakimLev GreenbergTal AvivSatya LokamBen FishmanShachar Seidman</p>
                <p><a href="http://arxiv.org/abs/2407.10886v1">Link to paper</a></p>
                <p>Large language models LLMs have recently seen widespread adoption in bothacademia and industry. As these models grow they become valuable intellectualproperty IP reflecting enormous investments by their owners. Moreover thehigh cost of cloud-based deployment has driven interest towards deployment toedge devices yet this risks exposing valuable parameters to theft andunauthorized use. Current methods to protect models IP on the edge havelimitations in terms of practicality loss in accuracy or suitability torequirements. In this paper we introduce a novel hybrid inference algorithmnamed SLIP designed to protect edge-deployed models from theft. SLIP is thefirst hybrid protocol that is both practical for real-world applications andprovably secure while having zero accuracy degradation and minimal impact onlatency. It involves partitioning the model between two computing resourcesone secure but expensive and another cost-effective but vulnerable. This isachieved through matrix decomposition ensuring that the secure resourceretains a maximally sensitive portion of the models IP while performing aminimal amount of computations and vice versa for the vulnerable resource.Importantly the protocol includes security guarantees that prevent attackersfrom exploiting the partition to infer the secured information. Finally wepresent experimental results that show the robustness and effectiveness of ourmethod positioning it as a compelling solution for protecting LLMs.</p>
                <p>Last Updated: 2024-07-15 16:37:55 UTC</p>
                <button class="interpret-button" data-id="2407.10886v1">Interpret</button>
                <div id="interpretation-2407.10886v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Principal Component Flow Map Learning of PDEs from Incomplete, Limited, and Noisy Data</h3>
                <p>Authors: Victor Churchill</p>
                <p><a href="http://arxiv.org/abs/2407.10854v1">Link to paper</a></p>
                <p>We present a computational technique for modeling the evolution of dynamicalsystems in a reduced basis with a focus on the challenging problem of modelingpartially-observed partial differential equations PDEs on high-dimensionalnon-uniform grids. We address limitations of previous work on data-driven flowmap learning in the sense that we focus on noisy and limited data to movetoward data collection scenarios in real-world applications. Leveraging recentwork on modeling PDEs in modal and nodal spaces we present a neural networkstructure that is suitable for PDE modeling with noisy and limited dataavailable only on a subset of the state variables or computational domain. Inparticular spatial grid-point measurements are reduced using a learned lineartransformation after which the dynamics are learned in this reduced basisbefore being transformed back out to the nodal space. This approach yields adrastically reduced parameterization of the neural network compared withprevious flow map models for nodal space learning. This primarily allows forsmaller training data sets but also enables reduced training times.</p>
                <p>Last Updated: 2024-07-15 16:06:20 UTC</p>
                <button class="interpret-button" data-id="2407.10854v1">Interpret</button>
                <div id="interpretation-2407.10854v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler</h3>
                <p>Authors: Changhun KimTaewon KimSeungyeon WooJune Yong YangEunho Yang</p>
                <p><a href="http://arxiv.org/abs/2407.10784v1">Link to paper</a></p>
                <p>In real-world applications tabular data often suffer from distributionshifts due to their widespread and abundant nature leading to erroneouspredictions of pre-trained machine learning models. However addressing suchdistribution shifts in the tabular domain has been relatively underexplored dueto unique challenges such as varying attributes and dataset sizes as well asthe limited representation learning capabilities of deep learning models fortabular data. Particularly with the recent promising paradigm of test-timeadaptation TTA where we adapt the off-the-shelf model to the unlabeledtarget domain during the inference phase without accessing the source domainwe observe that directly adopting commonly used TTA methods from other domainsoften leads to model collapse. We systematically explore challenges in tabulardata test-time adaptation including skewed entropy complex latent spacedecision boundaries confidence calibration issues with both overconfident andunder-confident and model bias towards source label distributions along withclass imbalances. Based on these insights we introduce AdapTable a noveltabular test-time adaptation method that directly modifies output probabilitiesby estimating target label distributions and adjusting initial probabilitiesbased on calibrated uncertainty. Extensive experiments on both naturaldistribution shifts and synthetic corruptions demonstrate the adaptationefficacy of the proposed method.</p>
                <p>Last Updated: 2024-07-15 15:02:53 UTC</p>
                <button class="interpret-button" data-id="2407.10784v1">Interpret</button>
                <div id="interpretation-2407.10784v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-07-16</p>
        </div>
    
        </div>
    </body>
    </html>
    