
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Tighter Generalisation Bounds via Interpolation</h3>
                <p>Authors: Paul ViallardMaxime HaddoucheUmut ŞimşekliBenjamin Guedj</p>
                <p><a href="http://arxiv.org/abs/2402.05101v1">Link to paper</a></p>
                <p>This paper contains a recipe for deriving new PAC-Bayes generalisation boundsbased on the f Gamma-divergence and in addition presents PAC-Bayesgeneralisation bounds where we interpolate between a series of probabilitydivergences including but not limited to KL Wasserstein and totalvariation making the best out of many worlds depending on the posteriordistributions properties. We explore the tightness of these bounds and connectthem to earlier results from statistical learning which are specific cases. Wealso instantiate our bounds as training objectives yielding non-trivialguarantees and practical performances.</p>
                <p>Last Updated: 2024-02-07 18:55:22 UTC</p>
                <button class="interpret-button" data-id="2402.05101v1">Interpret</button>
                <div id="interpretation-2402.05101v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling</h3>
                <p>Authors: Marcin SenderaMinsu KimSarthak MittalPablo LemosLuca ScimecaJarrid Rector-BrooksAlexandre AdamYoshua BengioNikolay Malkin</p>
                <p><a href="http://arxiv.org/abs/2402.05098v1">Link to paper</a></p>
                <p>We study the problem of training diffusion models to sample from adistribution with a given unnormalized density or energy function. We benchmarkseveral diffusion-structured inference methods including simulation-basedvariational approaches and off-policy methods continuous generative flownetworks. Our results shed light on the relative advantages of existingalgorithms while bringing into question some claims from past work. We alsopropose a novel exploration strategy for off-policy methods based on localsearch in the target space with the use of a replay buffer and show that itimproves the quality of samples on a variety of target distributions. Our codefor the sampling methods and benchmarks studied is made public athttps://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusionmodels for amortized inference.</p>
                <p>Last Updated: 2024-02-07 18:51:49 UTC</p>
                <button class="interpret-button" data-id="2402.05098v1">Interpret</button>
                <div id="interpretation-2402.05098v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity</h3>
                <p>Authors: Ahmet AlacaogluDonghwan KimStephen J. Wright</p>
                <p><a href="http://arxiv.org/abs/2402.05071v1">Link to paper</a></p>
                <p>We focus on constrained L-smooth nonconvex-nonconcave min-max problemseither satisfying rho-cohypomonotonicity or admitting a solution to therho-weakly Minty Variational Inequality MVI where larger values of theparameter rho0 correspond to a greater degree of nonconvexity. Theseproblem classes include examples in two player reinforcement learninginteraction dominant min-max problems and certain synthetic test problems onwhich classical min-max algorithms fail. It has been conjectured thatfirst-order methods can tolerate value of rho no larger than frac1Lbut existing results in the literature have stagnated at the tighterrequirement rho  frac12L. With a simple argument we obtain optimal orbest-known complexity guarantees with cohypomonotonicity or weak MVI conditionsfor rho  frac1L. The algorithms we analyze are inexact variants ofHalpern and Krasnoselskiui-Mann KM iterations. We also providealgorithms and complexity guarantees in the stochastic case with the same rangeon rho. Our main insight for the improvements in the convergence analyses isto harness the recently proposed conic nonexpansiveness property ofoperators. As byproducts we provide a refined analysis for inexact Halperniteration and propose a stochastic KM iteration with a multilevel Monte Carloestimator.</p>
                <p>Last Updated: 2024-02-07 18:22:41 UTC</p>
                <button class="interpret-button" data-id="2402.05071v1">Interpret</button>
                <div id="interpretation-2402.05071v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Causal Representation Learning from Multiple Distributions: A General Setting</h3>
                <p>Authors: Kun ZhangShaoan XieIgnavier NgYujia Zheng</p>
                <p><a href="http://arxiv.org/abs/2402.05052v1">Link to paper</a></p>
                <p>In many problems the measured variables e.g. image pixels are justmathematical functions of the hidden causal variables e.g. the underlyingconcepts or objects. For the purpose of making predictions in changingenvironments or making proper changes to the system it is helpful to recoverthe hidden causal variables Z_i and their causal relations represented bygraph mathcalG_Z. This problem has recently been known as causalrepresentation learning. This paper is concerned with a general completelynonparametric setting of causal representation learning from multipledistributions arising from heterogeneous data or nonstationary time serieswithout assuming hard interventions behind distribution changes. We aim todevelop general solutions in this fundamental case as a by product this helpssee the unique benefit offered by other assumptions such as parametric causalmodels or hard interventions. We show that under the sparsity constraint on therecovered graph over the latent variables and suitable sufficient changeconditions on the causal influences interestingly one can recover themoralized graph of the underlying directed acyclic graph and the recoveredlatent variables and their relations are related to the underlying causal modelin a specific nontrivial way. In some cases each latent variable can even berecovered up to component-wise transformations. Experimental results verify ourtheoretical claims.</p>
                <p>Last Updated: 2024-02-07 17:51:38 UTC</p>
                <button class="interpret-button" data-id="2402.05052v1">Interpret</button>
                <div id="interpretation-2402.05052v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth</h3>
                <p>Authors: Kevin KöglerAlexander ShevchenkoHamed HassaniMarco Mondelli</p>
                <p><a href="http://arxiv.org/abs/2402.05013v1">Link to paper</a></p>
                <p>Autoencoders are a prominent model in many empirical branches of machinelearning and lossy data compression. However basic theoretical questionsremain unanswered even in a shallow two-layer setting. In particular to whatdegree does a shallow autoencoder capture the structure of the underlying datadistribution For the prototypical case of the 1-bit compression of sparseGaussian data we prove that gradient descent converges to a solution thatcompletely disregards the sparse structure of the input. Namely theperformance of the algorithm is the same as if it was compressing a Gaussiansource - with no sparsity. For general data distributions we give evidence ofa phase transition phenomenon in the shape of the gradient descent minimizeras a function of the data sparsity: below the critical sparsity level theminimizer is a rotation taken uniformly at random just like in the compressionof non-sparse data above the critical sparsity the minimizer is the identityup to a permutation. Finally by exploiting a connection with approximatemessage passing algorithms we show how to improve upon Gaussian performancefor the compression of sparse data: adding a denoising function to a shallowarchitecture already reduces the loss provably and a suitable multi-layerdecoder leads to a further improvement. We validate our findings on imagedatasets such as CIFAR-10 and MNIST.</p>
                <p>Last Updated: 2024-02-07 16:32:29 UTC</p>
                <button class="interpret-button" data-id="2402.05013v1">Interpret</button>
                <div id="interpretation-2402.05013v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Image captioning for Brazilian Portuguese using GRIT model</h3>
                <p>Authors: Rafael Silva de AlencarWilliam Alberto Cruz CastañedaMarcellus Amadeus</p>
                <p><a href="http://arxiv.org/abs/2402.05106v1">Link to paper</a></p>
                <p>This work presents the early development of a model of image captioning forthe Brazilian Portuguese language. We used the GRIT Grid - and Region-basedImage captioning Transformer model to accomplish this work. GRIT is aTransformer-only neural architecture that effectively utilizes two visualfeatures to generate better captions. The GRIT method emerged as a proposal tobe a more efficient way to generate image captioning. In this work we adaptthe GRIT model to be trained in a Brazilian Portuguese dataset to have an imagecaptioning method for the Brazilian Portuguese Language.</p>
                <p>Last Updated: 2024-02-07 18:57:37 UTC</p>
                <button class="interpret-button" data-id="2402.05106v1">Interpret</button>
                <div id="interpretation-2402.05106v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation</h3>
                <p>Authors: Dennis HoftijzerGertjan BurghoutsLuuk Spreeuwers</p>
                <p><a href="http://arxiv.org/abs/2402.05090v1">Link to paper</a></p>
                <p>Deep Reinforcement Learning DRL has shown great potential in enablingrobots to find certain objects e.g. find a fridge in environments likehomes or schools. This task is known as Object-Goal Navigation ObjectNav. DRLmethods are predominantly trained and evaluated using environment simulators.Although DRL has shown impressive results the simulators may be biased orlimited. This creates a risk of shortcut learning i.e. learning a policytailored to specific visual details of training environments. We aim to deepenour understanding of shortcut learning in ObjectNav its implications andpropose a solution. We design an experiment for inserting a shortcut bias inthe appearance of training environments. As a proof-of-concept we associateroom types to specific wall colors e.g. bedrooms with green walls andobserve poor generalization of a state-of-the-art SOTA ObjectNav method toenvironments where this is not the case e.g. bedrooms with blue walls. Wefind that shortcut learning is the root cause: the agent learns to navigate totarget objects by simply searching for the associated wall color of the targetobjects room. To solve this we propose Language-Based L-B augmentation. Ourkey insight is that we can leverage the multimodal feature space of aVision-Language Model VLM to augment visual representations directly at thefeature-level requiring no changes to the simulator and only an addition ofone layer to the model. Where the SOTA ObjectNav methods success rate drops69 our proposal has only a drop of 23.</p>
                <p>Last Updated: 2024-02-07 18:44:27 UTC</p>
                <button class="interpret-button" data-id="2402.05090v1">Interpret</button>
                <div id="interpretation-2402.05090v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation</h3>
                <p>Authors: Ziyang WangJian-Qing ZhengYichi ZhangGe CuiLei Li</p>
                <p><a href="http://arxiv.org/abs/2402.05079v1">Link to paper</a></p>
                <p>In recent advancements in medical image analysis Convolutional NeuralNetworks CNN and Vision Transformers ViT have set significant benchmarks.While the former excels in capturing local features through its convolutionoperations the latter achieves remarkable global context understanding byleveraging self-attention mechanisms. However both architectures exhibitlimitations in efficiently modeling long-range dependencies within medicalimages which is a critical aspect for precise segmentation. Inspired by theMamba architecture known for its proficiency in handling long sequences andglobal contextual information with enhanced computational efficiency as a StateSpace Model SSM we propose Mamba-UNet a novel architecture that synergizesthe U-Net in medical image segmentation with Mambas capability. Mamba-UNetadopts a pure Visual Mamba VMamba-based encoder-decoder structure infusedwith skip connections to preserve spatial information across different scalesof the network. This design facilitates a comprehensive feature learningprocess capturing intricate details and broader semantic contexts withinmedical images. We introduce a novel integration mechanism within the VMambablocks to ensure seamless connectivity and information flow between the encoderand decoder paths enhancing the segmentation performance. We conductedexperiments on publicly available MRI cardiac multi-structures segmentationdataset. The results show that Mamba-UNet outperforms UNet Swin-UNet inmedical image segmentation under the same hyper-parameter setting. The sourcecode and baseline implementations are available.</p>
                <p>Last Updated: 2024-02-07 18:33:04 UTC</p>
                <button class="interpret-button" data-id="2402.05079v1">Interpret</button>
                <div id="interpretation-2402.05079v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation</h3>
                <p>Authors: Jiaxiang TangZhaoxi ChenXiaokang ChenTengfei WangGang ZengZiwei Liu</p>
                <p><a href="http://arxiv.org/abs/2402.05054v1">Link to paper</a></p>
                <p>3D content creation has achieved significant progress in terms of bothquality and speed. Although current feed-forward models can produce 3D objectsin seconds their resolution is constrained by the intensive computationrequired during training. In this paper we introduce Large Multi-View GaussianModel LGM a novel framework designed to generate high-resolution 3D modelsfrom text prompts or single-view images. Our key insights are two-fold: 1 3DRepresentation: We propose multi-view Gaussian features as an efficient yetpowerful representation which can then be fused together for differentiablerendering. 2 3D Backbone: We present an asymmetric U-Net as a high-throughputbackbone operating on multi-view images which can be produced from text orsingle-view image input by leveraging multi-view diffusion models. Extensiveexperiments demonstrate the high fidelity and efficiency of our approach.Notably we maintain the fast speed to generate 3D objects within 5 secondswhile boosting the training resolution to 512 thereby achievinghigh-resolution 3D content generation.</p>
                <p>Last Updated: 2024-02-07 17:57:03 UTC</p>
                <button class="interpret-button" data-id="2402.05054v1">Interpret</button>
                <div id="interpretation-2402.05054v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Efficient Multi-Resolution Fusion for Remote Sensing Data with Label Uncertainty</h3>
                <p>Authors: Hersh VakhariaXiaoxiao Du</p>
                <p><a href="http://dx.doi.org/10.1109/IGARSS52108.2023.10282851">Link to paper</a></p>
                <p>Multi-modal sensor data fusion takes advantage of complementary orreinforcing information from each sensor and can boost overall performance inapplications such as scene classification and target detection. This paperpresents a new method for fusing multi-modal and multi-resolution remote sensordata without requiring pixel-level training labels which can be difficult toobtain. Previously we developed a Multiple Instance Multi-Resolution FusionMIMRF framework that addresses label uncertainty for fusion but it can beslow to train due to the large search space for the fuzzy measures used tointegrate sensor data sources. We propose a new method based on binary fuzzymeasures which reduces the search space and significantly improves theefficiency of the MIMRF framework. We present experimental results on syntheticdata and a real-world remote sensing detection task and show that the proposedMIMRF-BFM algorithm can effectively and efficiently perform multi-resolutionfusion given remote sensing data with uncertainty.</p>
                <p>Last Updated: 2024-02-07 17:34:32 UTC</p>
                <button class="interpret-button" data-id="2402.05045v1">Interpret</button>
                <div id="interpretation-2402.05045v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Edu-ConvoKit: An Open-Source Library for Education Conversation Data</h3>
                <p>Authors: Rose E. WangDorottya Demszky</p>
                <p><a href="http://arxiv.org/abs/2402.05111v1">Link to paper</a></p>
                <p>We introduce Edu-ConvoKit an open-source library designed to handlepre-processing annotation and analysis of conversation data in education.Resources for analyzing education conversation data are scarce making theresearch challenging to perform and therefore hard to access. We address thesechallenges with Edu-ConvoKit. Edu-ConvoKit is open-sourcehttps://github.com/stanfordnlp/edu-convokit  pip-installablehttps://pypi.org/project/edu-convokit/  with comprehensive documentationhttps://edu-convokit.readthedocs.io/en/latest/ . Our demo video is availableat: https://youtu.be/zdcI839vAkosih9qlnl76ucSuXb8- . We include additionalresources such as Colab applications of Edu-ConvoKit to three diverseeducation datasets and a repository of Edu-ConvoKit related papers that can befound in our GitHub repository.</p>
                <p>Last Updated: 2024-02-07 18:59:31 UTC</p>
                <button class="interpret-button" data-id="2402.05111v1">Interpret</button>
                <div id="interpretation-2402.05111v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Image captioning for Brazilian Portuguese using GRIT model</h3>
                <p>Authors: Rafael Silva de AlencarWilliam Alberto Cruz CastañedaMarcellus Amadeus</p>
                <p><a href="http://arxiv.org/abs/2402.05106v1">Link to paper</a></p>
                <p>This work presents the early development of a model of image captioning forthe Brazilian Portuguese language. We used the GRIT Grid - and Region-basedImage captioning Transformer model to accomplish this work. GRIT is aTransformer-only neural architecture that effectively utilizes two visualfeatures to generate better captions. The GRIT method emerged as a proposal tobe a more efficient way to generate image captioning. In this work we adaptthe GRIT model to be trained in a Brazilian Portuguese dataset to have an imagecaptioning method for the Brazilian Portuguese Language.</p>
                <p>Last Updated: 2024-02-07 18:57:37 UTC</p>
                <button class="interpret-button" data-id="2402.05106v1">Interpret</button>
                <div id="interpretation-2402.05106v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Roadmap to Pluralistic Alignment</h3>
                <p>Authors: Taylor SorensenJared MooreJillian FisherMitchell GordonNiloofar MireshghallahChristopher Michael RyttingAndre YeLiwei JiangXiming LuNouha DziriTim AlthoffYejin Choi</p>
                <p><a href="http://arxiv.org/abs/2402.05070v1">Link to paper</a></p>
                <p>With increased power and prevalence of AI systems it is ever more criticalthat AI systems are designed to serve all i.e. people with diverse values andperspectives. However aligning models to serve pluralistic human valuesremains an open research question. In this piece we propose a roadmap topluralistic alignment specifically using language models as a test bed. Weidentify and formalize three possible ways to define and operationalizepluralism in AI systems: 1 Overton pluralistic models that present a spectrumof reasonable responses 2 Steerably pluralistic models that can steer toreflect certain perspectives and 3 Distributionally pluralistic models thatare well-calibrated to a given population in distribution. We also propose andformalize three possible classes of pluralistic benchmarks: 1 Multi-objectivebenchmarks 2 Trade-off steerable benchmarks which incentivize models tosteer to arbitrary trade-offs and 3 Jury-pluralistic benchmarks whichexplicitly model diverse human ratings. We use this framework to argue thatcurrent alignment techniques may be fundamentally limited for pluralistic AIindeed we highlight empirical evidence both from our own experiments and fromother work that standard alignment procedures might reduce distributionalpluralism in models motivating the need for further research on pluralisticalignment.</p>
                <p>Last Updated: 2024-02-07 18:21:17 UTC</p>
                <button class="interpret-button" data-id="2402.05070v1">Interpret</button>
                <div id="interpretation-2402.05070v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>How VADER is your AI? Towards a definition of artificial intelligence systems appropriate for regulation</h3>
                <p>Authors: Leonardo C. T. BezerraAlexander E. I. BrownleeLuana Ferraz AlvarengaRenan Cipriano MoioliThais Vasconcelos Batista</p>
                <p><a href="http://arxiv.org/abs/2402.05048v1">Link to paper</a></p>
                <p>Artificial intelligence AI has driven many information and communicationtechnology ICT breakthroughs. Nonetheless the scope of ICT systems hasexpanded far beyond AI since the Turing test proposal. Critically recent AIregulation proposals adopt AI definitions affecting ICT techniques approachesand systems that are not AI. In some cases even works from mathematicsstatistics and engineering would be affected. Worryingly AI misdefinitionsare observed from Western societies to the Global South. In this paper wepropose a framework to score how textitvalidated as appropriately-defined forregulation VADER an AI definition is. Our online publicly-available VADERframework scores the coverage of premises that should underlie AI definitionsfor regulation which aim to i reproduce principles observed in othersuccessful technology regulations and ii include all AI techniques andapproaches while excluding non-AI works. Regarding the latter our score isbased on a dataset of representative AI non-AI ICT and non-ICT examples. Wedemonstrate our contribution by reviewing the AI regulation proposals of keyplayers namely the United States United Kingdom European Union and Brazil.Importantly none of the proposals assessed achieve the appropriateness scoreranging from a revision need to a concrete risk to ICT systems and works fromother fields.</p>
                <p>Last Updated: 2024-02-07 17:41:15 UTC</p>
                <button class="interpret-button" data-id="2402.05048v1">Interpret</button>
                <div id="interpretation-2402.05048v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models</h3>
                <p>Authors: Lijun LiBowen DongRuohui WangXuhao HuWangmeng ZuoDahua LinYu QiaoJing Shao</p>
                <p><a href="http://arxiv.org/abs/2402.05044v2">Link to paper</a></p>
                <p>In the rapidly evolving landscape of Large Language Models LLMs ensuringrobust safety measures is paramount. To meet this crucial need we proposeemphSALAD-Bench a safety benchmark specifically designed for evaluatingLLMs attack and defense methods. Distinguished by its breadth SALAD-Benchtranscends conventional benchmarks through its large scale rich diversityintricate taxonomy spanning three levels and versatilefunctionalities.SALAD-Bench is crafted with a meticulous array of questionsfrom standard queries to complex ones enriched with attack defensemodifications and multiple-choice. To effectively manage the inherentcomplexity we introduce an innovative evaluators: the LLM-based MD-Judge forQA pairs with a particular focus on attack-enhanced queries ensuring aseamless and reliable evaluation. Above components extend SALAD-Bench fromstandard LLM safety evaluation to both LLM attack and defense methodsevaluation ensuring the joint-purpose utility. Our extensive experiments shedlight on the resilience of LLMs against emerging threats and the efficacy ofcontemporary defense tactics. Data and evaluator are released underhttps://github.com/OpenSafetyLab/SALAD-BENCH.</p>
                <p>Last Updated: 2024-02-08 02:50:22 UTC</p>
                <button class="interpret-button" data-id="2402.05044v2">Interpret</button>
                <div id="interpretation-2402.05044v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>ARCollab: Towards Multi-User Interactive Cardiovascular Surgical Planning in Mobile Augmented Reality</h3>
                <p>Authors: Pratham MehtaHarsha KaranthHaoyang YangTimothy SlesnickFawwaz ShawDuen Horng Chau</p>
                <p><a href="http://arxiv.org/abs/2402.05075v1">Link to paper</a></p>
                <p>Surgical planning for congenital heart diseases requires a collaborativeapproach traditionally involving the 3D-printing of physical heart models forinspection by surgeons and cardiologists. Recent advancements in mobileaugmented reality AR technologies have offered a promising alternative notedfor their ease-of-use and portability. Despite this progress there remains agap in research exploring the use of multi-user mobile AR environments forfacilitating collaborative cardiovascular surgical planning. We are developingARCollab an iOS AR application designed to allow multiple surgeons andcardiologists to interact with patient-specific 3D heart models in a sharedenvironment. ARCollab allows surgeons and cardiologists to import heart modelsperform gestures to manipulate the heart and collaborate with other userswithout having to produce a physical heart model. We are excited by thepotential for ARCollab to make long-term real-world impact thanks to theubiquity of iOS devices that will allow for ARCollabs easy distributiondeployment and adoption.</p>
                <p>Last Updated: 2024-02-07 18:29:38 UTC</p>
                <button class="interpret-button" data-id="2402.05075v1">Interpret</button>
                <div id="interpretation-2402.05075v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>When the Body Became Data: Historical Data Cultures and Anatomical Illustration</h3>
                <p>Authors: Michael CorrellLaura A. Garrison</p>
                <p><a href="http://arxiv.org/abs/2402.05014v1">Link to paper</a></p>
                <p>With changing attitudes around knowledge medicine art and technology thehuman body has become a source of information and ultimately shareable andanalyzable data. Centuries of illustrations and visualizations of the bodyoccur within particular historical social and political contexts. Thesecontexts are enmeshed in different so-called data cultures: ways that dataknowledge and information are conceptualized and collected structured andshared. In this work we explore how information about the body was collectedas well as the circulation impact and persuasive force of the resultingimages. We show how mindfulness of data cultural influences remain crucial fortodays designers researchers and consumers of visualizations. We concludewith a call for the field to reflect on how visualizations are not timeless andcontextless mirrors on objective data but as much a product of our time andplace as the visualizations of the past.</p>
                <p>Last Updated: 2024-02-07 16:32:55 UTC</p>
                <button class="interpret-button" data-id="2402.05014v1">Interpret</button>
                <div id="interpretation-2402.05014v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Exploring the Opportunity of Augmented Reality (AR) in Supporting Older Adults Explore and Learn Smartphone Applications</h3>
                <p>Authors: Xiaofu JinWai TongXiaoying WeiXian WangEmily KuangXiaoyu MoHuamin QuMingming Fan</p>
                <p><a href="http://arxiv.org/abs/2402.04991v1">Link to paper</a></p>
                <p>The global aging trend compels older adults to navigate the evolving digitallandscape presenting a substantial challenge in mastering smartphoneapplications. While Augmented Reality AR holds promise for enhancing learningand user experience its role in aiding older adults smartphone appexploration remains insufficiently explored. Therefore we conducted atwo-phase study: 1 a workshop with 18 older adults to identify appexploration challenges and potential AR interventions and 2 tech-probeparticipatory design sessions with 15 participants to co-create AR supporttools. Our research highlights ARs effectiveness in reducing physical andcognitive strain among older adults during app exploration especially duringmulti-app usage and the trial-and-error learning process. We also examinedtheir interactional experiences with AR yielding design considerations ontailoring AR tools for smartphone app exploration. Ultimately our studyunveils the prospective landscape of AR in supporting the older demographicboth presently and in future scenarios.</p>
                <p>Last Updated: 2024-02-07 16:09:35 UTC</p>
                <button class="interpret-button" data-id="2402.04991v1">Interpret</button>
                <div id="interpretation-2402.04991v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12</h3>
                <p>Authors: Liuqing ChenShuhong XiaoYunnong ChenRuoyu WuYaxuan SongLingyun Sun</p>
                <p><a href="http://dx.doi.org/10.1145/3613904.3642229">Link to paper</a></p>
                <p>As Computational Thinking CT continues to permeate younger age groups inK-12 education established CT platforms such as Scratch face challenges incatering to these younger learners particularly those in the elementary schoolages 6-12. Through formative investigation with Scratch experts we uncoverthree key obstacles to childrens autonomous Scratch learning: artists blockin project planning bounded creativity in asset creation and inadequatecoding guidance during implementation. To address these barriers we introduceChatScratch an AI-augmented system to facilitate autonomous programminglearning for young children. ChatScratch employs structured interactivestoryboards and visual cues to overcome artists block integrates digitaldrawing and advanced image generation technologies to elevate creativity andleverages Scratch-specialized Large Language Models LLMs for professionalcoding guidance. Our study shows that compared to Scratch ChatScratchefficiently fosters autonomous programming learning and contributes to thecreation of high-quality personally meaningful Scratch projects for children.</p>
                <p>Last Updated: 2024-02-07 15:55:51 UTC</p>
                <button class="interpret-button" data-id="2402.04975v1">Interpret</button>
                <div id="interpretation-2402.04975v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems</h3>
                <p>Authors: Samuel Kernan FreireChaofan WangEvangelos Niforatos</p>
                <p><a href="http://arxiv.org/abs/2402.04955v1">Link to paper</a></p>
                <p>Cognitive assistants CA are chatbots that provide context-aware support tohuman workers in knowledge-intensive tasks. Traditionally cognitive assistantsrespond in specific ways to predefined user intents and conversation patterns.However this rigidness does not handle the diversity of natural language well.Recent advances in natural language processing NLP powering large languagemodels LLM such as GPT-4 Llama2 and Gemini could enable CAs to converse ina more flexible human-like manner. However the additional degrees of freedommay have unforeseen consequences especially in knowledge-intensive contextswhere accuracy is crucial. As a preliminary step to assessing the potential ofusing LLMs in these contexts we conducted a user study comparing an LLM-basedCA to an intent-based system regarding interaction efficiency user experienceworkload and usability. This revealed that LLM-based CAs exhibited better userexperience task completion rate usability and perceived performance thanintent-based systems suggesting that switching NLP techniques should beinvestigated further.</p>
                <p>Last Updated: 2024-02-07 15:39:07 UTC</p>
                <button class="interpret-button" data-id="2402.04955v1">Interpret</button>
                <div id="interpretation-2402.04955v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Opening the AI black box: program synthesis via mechanistic interpretability</h3>
                <p>Authors: Eric J. MichaudIsaac LiaoVedang LadZiming LiuAnish MudideChloe LoughridgeZifan Carl GuoTara Rezaei KheirkhahMateja VukelićMax Tegmark</p>
                <p><a href="http://arxiv.org/abs/2402.05110v1">Link to paper</a></p>
                <p>We present MIPS a novel method for program synthesis based on automatedmechanistic interpretability of neural networks trained to perform the desiredtask auto-distilling the learned algorithm into Python code. We test MIPS on abenchmark of 62 algorithmic tasks that can be learned by an RNN and find ithighly complementary to GPT-4: MIPS solves 32 of them including 13 that arenot solved by GPT-4 which also solves 30. MIPS uses an integer autoencoder toconvert the RNN into a finite state machine then applies Boolean or integersymbolic regression to capture the learned algorithm. As opposed to largelanguage models this program synthesis technique makes no use of and istherefore not limited by human training data such as algorithms and code fromGitHub. We discuss opportunities and challenges for scaling up this approach tomake machine-learned models more interpretable and trustworthy.</p>
                <p>Last Updated: 2024-02-07 18:59:12 UTC</p>
                <button class="interpret-button" data-id="2402.05110v1">Interpret</button>
                <div id="interpretation-2402.05110v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding</h3>
                <p>Authors: Zachary AnknerRishab ParthasarathyAniruddha NrusimhaChristopher RinardJonathan Ragan-KelleyWilliam Brandon</p>
                <p><a href="http://arxiv.org/abs/2402.05109v1">Link to paper</a></p>
                <p>To combat the memory bandwidth-bound nature of autoregressive LLM inferenceprevious research has proposed the speculative decoding framework. To performspeculative decoding a small draft model proposes candidate continuations ofthe input sequence that are then verified in parallel by the base model. Oneway to specify the draft model as used in the recent Medusa decodingframework is as a collection of light-weight heads called draft heads thatoperate on the base models hidden states. To date all existing draft headshave been sequentially independent meaning that they speculate tokens in thecandidate continuation independently of any preceding tokens in the candidatecontinuation. In this work we propose Hydra heads a sequentially dependentdrop-in replacement for standard draft heads that significantly improvesspeculation accuracy. Decoding with Hydra heads improves throughput compared toMedusa decoding with standard draft heads. We further explore the design spaceof Hydra head training objectives and architectures and propose acarefully-tuned Hydra head recipe which we call Hydra that improvesdecoding throughput by 1.31x and 2.71x compared to Medusa decoding andautoregressive decoding respectively. Overall Hydra heads are a simpleintervention on standard draft heads that significantly improve the end-to-endspeed of draft head based speculative decoding.</p>
                <p>Last Updated: 2024-02-07 18:58:50 UTC</p>
                <button class="interpret-button" data-id="2402.05109v1">Interpret</button>
                <div id="interpretation-2402.05109v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Tighter Generalisation Bounds via Interpolation</h3>
                <p>Authors: Paul ViallardMaxime HaddoucheUmut ŞimşekliBenjamin Guedj</p>
                <p><a href="http://arxiv.org/abs/2402.05101v1">Link to paper</a></p>
                <p>This paper contains a recipe for deriving new PAC-Bayes generalisation boundsbased on the f Gamma-divergence and in addition presents PAC-Bayesgeneralisation bounds where we interpolate between a series of probabilitydivergences including but not limited to KL Wasserstein and totalvariation making the best out of many worlds depending on the posteriordistributions properties. We explore the tightness of these bounds and connectthem to earlier results from statistical learning which are specific cases. Wealso instantiate our bounds as training objectives yielding non-trivialguarantees and practical performances.</p>
                <p>Last Updated: 2024-02-07 18:55:22 UTC</p>
                <button class="interpret-button" data-id="2402.05101v1">Interpret</button>
                <div id="interpretation-2402.05101v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Hydragen: High-Throughput LLM Inference with Shared Prefixes</h3>
                <p>Authors: Jordan JuravskyBradley BrownRyan EhrlichDaniel Y. FuChristopher RéAzalia Mirhoseini</p>
                <p><a href="http://arxiv.org/abs/2402.05099v1">Link to paper</a></p>
                <p>Transformer-based large language models LLMs are now deployed to hundredsof millions of users. LLM inference is commonly performed on batches ofsequences that share a prefix such as few-shot examples or a chatbot systemprompt. Decoding in this large-batch setting can be bottlenecked by theattention operation which reads large key-value KV caches from memory andcomputes inefficient matrix-vector products for every sequence in the batch. Inthis work we introduce Hydragen a hardware-aware exact implementation ofattention with shared prefixes. Hydragen computes attention over the sharedprefix and unique suffixes separately. This decomposition enables efficientprefix attention by batching queries together across sequences reducingredundant memory reads and enabling the use of hardware-friendly matrixmultiplications. Our method can improve end-to-end LLM throughput by up to 32xagainst competitive baselines with speedup growing with the batch size andshared prefix length. Hydragen also enables the use of very long sharedcontexts: with a high batch size increasing the prefix length from 1K to 16Ktokens decreases Hydragen throughput by less than 15 while the throughput ofbaselines drops by over 90. Hydragen generalizes beyond simple prefix-suffixdecomposition and can be applied to tree-based prompt sharing patternsallowing us to further reduce inference time on competitive programmingproblems by 55.</p>
                <p>Last Updated: 2024-02-07 18:53:01 UTC</p>
                <button class="interpret-button" data-id="2402.05099v1">Interpret</button>
                <div id="interpretation-2402.05099v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling</h3>
                <p>Authors: Marcin SenderaMinsu KimSarthak MittalPablo LemosLuca ScimecaJarrid Rector-BrooksAlexandre AdamYoshua BengioNikolay Malkin</p>
                <p><a href="http://arxiv.org/abs/2402.05098v1">Link to paper</a></p>
                <p>We study the problem of training diffusion models to sample from adistribution with a given unnormalized density or energy function. We benchmarkseveral diffusion-structured inference methods including simulation-basedvariational approaches and off-policy methods continuous generative flownetworks. Our results shed light on the relative advantages of existingalgorithms while bringing into question some claims from past work. We alsopropose a novel exploration strategy for off-policy methods based on localsearch in the target space with the use of a replay buffer and show that itimproves the quality of samples on a variety of target distributions. Our codefor the sampling methods and benchmarks studied is made public athttps://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusionmodels for amortized inference.</p>
                <p>Last Updated: 2024-02-07 18:51:49 UTC</p>
                <button class="interpret-button" data-id="2402.05098v1">Interpret</button>
                <div id="interpretation-2402.05098v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Edu-ConvoKit: An Open-Source Library for Education Conversation Data</h3>
                <p>Authors: Rose E. WangDorottya Demszky</p>
                <p><a href="http://arxiv.org/abs/2402.05111v1">Link to paper</a></p>
                <p>We introduce Edu-ConvoKit an open-source library designed to handlepre-processing annotation and analysis of conversation data in education.Resources for analyzing education conversation data are scarce making theresearch challenging to perform and therefore hard to access. We address thesechallenges with Edu-ConvoKit. Edu-ConvoKit is open-sourcehttps://github.com/stanfordnlp/edu-convokit  pip-installablehttps://pypi.org/project/edu-convokit/  with comprehensive documentationhttps://edu-convokit.readthedocs.io/en/latest/ . Our demo video is availableat: https://youtu.be/zdcI839vAkosih9qlnl76ucSuXb8- . We include additionalresources such as Colab applications of Edu-ConvoKit to three diverseeducation datasets and a repository of Edu-ConvoKit related papers that can befound in our GitHub repository.</p>
                <p>Last Updated: 2024-02-07 18:59:31 UTC</p>
                <button class="interpret-button" data-id="2402.05111v1">Interpret</button>
                <div id="interpretation-2402.05111v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Image captioning for Brazilian Portuguese using GRIT model</h3>
                <p>Authors: Rafael Silva de AlencarWilliam Alberto Cruz CastañedaMarcellus Amadeus</p>
                <p><a href="http://arxiv.org/abs/2402.05106v1">Link to paper</a></p>
                <p>This work presents the early development of a model of image captioning forthe Brazilian Portuguese language. We used the GRIT Grid - and Region-basedImage captioning Transformer model to accomplish this work. GRIT is aTransformer-only neural architecture that effectively utilizes two visualfeatures to generate better captions. The GRIT method emerged as a proposal tobe a more efficient way to generate image captioning. In this work we adaptthe GRIT model to be trained in a Brazilian Portuguese dataset to have an imagecaptioning method for the Brazilian Portuguese Language.</p>
                <p>Last Updated: 2024-02-07 18:57:37 UTC</p>
                <button class="interpret-button" data-id="2402.05106v1">Interpret</button>
                <div id="interpretation-2402.05106v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Roadmap to Pluralistic Alignment</h3>
                <p>Authors: Taylor SorensenJared MooreJillian FisherMitchell GordonNiloofar MireshghallahChristopher Michael RyttingAndre YeLiwei JiangXiming LuNouha DziriTim AlthoffYejin Choi</p>
                <p><a href="http://arxiv.org/abs/2402.05070v1">Link to paper</a></p>
                <p>With increased power and prevalence of AI systems it is ever more criticalthat AI systems are designed to serve all i.e. people with diverse values andperspectives. However aligning models to serve pluralistic human valuesremains an open research question. In this piece we propose a roadmap topluralistic alignment specifically using language models as a test bed. Weidentify and formalize three possible ways to define and operationalizepluralism in AI systems: 1 Overton pluralistic models that present a spectrumof reasonable responses 2 Steerably pluralistic models that can steer toreflect certain perspectives and 3 Distributionally pluralistic models thatare well-calibrated to a given population in distribution. We also propose andformalize three possible classes of pluralistic benchmarks: 1 Multi-objectivebenchmarks 2 Trade-off steerable benchmarks which incentivize models tosteer to arbitrary trade-offs and 3 Jury-pluralistic benchmarks whichexplicitly model diverse human ratings. We use this framework to argue thatcurrent alignment techniques may be fundamentally limited for pluralistic AIindeed we highlight empirical evidence both from our own experiments and fromother work that standard alignment procedures might reduce distributionalpluralism in models motivating the need for further research on pluralisticalignment.</p>
                <p>Last Updated: 2024-02-07 18:21:17 UTC</p>
                <button class="interpret-button" data-id="2402.05070v1">Interpret</button>
                <div id="interpretation-2402.05070v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models</h3>
                <p>Authors: Lijun LiBowen DongRuohui WangXuhao HuWangmeng ZuoDahua LinYu QiaoJing Shao</p>
                <p><a href="http://arxiv.org/abs/2402.05044v2">Link to paper</a></p>
                <p>In the rapidly evolving landscape of Large Language Models LLMs ensuringrobust safety measures is paramount. To meet this crucial need we proposeemphSALAD-Bench a safety benchmark specifically designed for evaluatingLLMs attack and defense methods. Distinguished by its breadth SALAD-Benchtranscends conventional benchmarks through its large scale rich diversityintricate taxonomy spanning three levels and versatilefunctionalities.SALAD-Bench is crafted with a meticulous array of questionsfrom standard queries to complex ones enriched with attack defensemodifications and multiple-choice. To effectively manage the inherentcomplexity we introduce an innovative evaluators: the LLM-based MD-Judge forQA pairs with a particular focus on attack-enhanced queries ensuring aseamless and reliable evaluation. Above components extend SALAD-Bench fromstandard LLM safety evaluation to both LLM attack and defense methodsevaluation ensuring the joint-purpose utility. Our extensive experiments shedlight on the resilience of LLMs against emerging threats and the efficacy ofcontemporary defense tactics. Data and evaluator are released underhttps://github.com/OpenSafetyLab/SALAD-BENCH.</p>
                <p>Last Updated: 2024-02-08 02:50:22 UTC</p>
                <button class="interpret-button" data-id="2402.05044v2">Interpret</button>
                <div id="interpretation-2402.05044v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual Language Models</h3>
                <p>Authors: Miriam CuscitoAlfio FerraraMartin Ruskov</p>
                <p><a href="http://arxiv.org/abs/2402.05034v1">Link to paper</a></p>
                <p>In this paper we explore the idea of analysing the historical bias ofcontextual language models based on BERT by measuring their adequacy withrespect to Early Modern EME and Modern ME English. In our preliminaryexperiments we perform fill-in-the-blank tests with 60 masked sentences 20EME-specific 20 ME-specific and 20 generic and three different models i.e.BERT Base MacBERTh English HLM. We then rate the model predictions accordingto a 5-point bipolar scale between the two language varieties and derive aweighted score to measure the adequacy of each model to EME and ME varieties ofEnglish.</p>
                <p>Last Updated: 2024-02-07 17:07:53 UTC</p>
                <button class="interpret-button" data-id="2402.05034v1">Interpret</button>
                <div id="interpretation-2402.05034v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Quantifying Population Exposure to Long-term PM10: A City-wide Agent-based Assessment</h3>
                <p>Authors: Hyesop Shin</p>
                <p><a href="http://arxiv.org/abs/2402.05029v1">Link to paper</a></p>
                <p>This study evaluates the health effects of long-term exposure to PM10 inSeoul. Building on the preliminary model Shin and Bithell 2019 an in-silicoagent-based model ABM is used to simulate the travel patterns of individualsaccording to their origins and destinations. During the simulation eachperson with their inherent socio-economic attributes and allocated origin anddestination location is assumed to commute to and from the same places for 10consecutive years. A nominal measure of their health is set to decreasewhenever the concentration of PM10 exceeds the national standard. Sensitivityanalysis on calibrated parameters reveals increased vulnerability among certaindemographic groups particularly those aged over 65 and under 15 with asignificant health decline associated with road proximity. The study reveals asubstantial health disparity after 7000 simulation ticks equivalent to 10years especially under scenarios of a 3 annual increase in pollution levels.Long-term exposure to PM10 has a significant impact on health vulnerabilitiesdespite initial resilience being minimal. The study emphasises the importanceof future research that takes into account different pollution thresholds aswell as more detailed models of population dynamics and pollution generation inorder to better understand and mitigate the health effects of air pollution ondiverse urban populations.</p>
                <p>Last Updated: 2024-02-07 16:56:45 UTC</p>
                <button class="interpret-button" data-id="2402.05029v1">Interpret</button>
                <div id="interpretation-2402.05029v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing</h3>
                <p>Authors: Jannis WeilZhenghua BaoOsama AbboudTobias Meuser</p>
                <p><a href="http://arxiv.org/abs/2402.05027v1">Link to paper</a></p>
                <p>Graph-based environments pose unique challenges to multi-agent reinforcementlearning. In decentralized approaches agents operate within a given graph andmake decisions based on partial or outdated observations. The size of theobserved neighborhood limits the generalizability to different graphs andaffects the reactivity of agents the quality of the selected actions and thecommunication overhead. This work focuses on generalizability and resolves thetrade-off in observed neighborhood size with a continuous information flow inthe whole graph. We propose a recurrent message-passing model that iterateswith the environments steps and allows nodes to create a global representationof the graph by exchanging messages with their neighbors. Agents receive theresulting learned graph observations based on their location in the graph. Ourapproach can be used in a decentralized manner at runtime and in combinationwith a reinforcement learning algorithm of choice. We evaluate our methodacross 1000 diverse graphs in the context of routing in communication networksand find that it enables agents to generalize and adapt to changes in thegraph.</p>
                <p>Last Updated: 2024-02-07 16:53:09 UTC</p>
                <button class="interpret-button" data-id="2402.05027v1">Interpret</button>
                <div id="interpretation-2402.05027v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>S-Agents: self-organizing agents in open-ended environment</h3>
                <p>Authors: Jiaqi ChenYuxian JiangJiachen LuLi Zhang</p>
                <p><a href="http://arxiv.org/abs/2402.04578v2">Link to paper</a></p>
                <p>Leveraging large language models LLMs autonomous agents have significantlyimproved gaining the ability to handle a variety of tasks. In open-endedsettings optimizing collaboration for efficiency and effectiveness demandsflexible adjustments. Despite this current research mainly emphasizes fixedtask-oriented workflows and overlooks agent-centric organizational structures.Drawing inspiration from human organizational behavior we introduce aself-organizing agent system S-Agents with a tree of agents structure fordynamic workflow an hourglass agent architecture for balancing informationpriorities and a non-obstructive collaboration method to allow asynchronoustask execution among agents. This structure can autonomously coordinate a groupof agents efficiently addressing the challenges of an open and dynamicenvironment without human intervention. Our experiments demonstrate thatS-Agents proficiently execute collaborative building tasks and resourcecollection in the Minecraft environment validating their effectiveness.</p>
                <p>Last Updated: 2024-02-08 17:01:00 UTC</p>
                <button class="interpret-button" data-id="2402.04578v2">Interpret</button>
                <div id="interpretation-2402.04578v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Human-guided Swarms: Impedance Control-inspired Influence in Virtual Reality Environments</h3>
                <p>Authors: Spencer BarclayKshitij Jerath</p>
                <p><a href="http://arxiv.org/abs/2402.04451v1">Link to paper</a></p>
                <p>Prior works in human-swarm interaction HSI have sought to guide swarmbehavior towards established objectives but may be unable to handle specificscenarios that require finer human supervision variable autonomy orapplication to large-scale swarms. In this paper we present an approach thatenables human supervisors to tune the level of swarm control and guide a largeswarm using an assistive control mechanism that does not significantly restrictemergent swarm behaviors. We develop this approach in a virtual reality VRenvironment using the HTC Vive and Unreal Engine 4 with AirSim plugin. Thenovel combination of an impedance control-inspired influence mechanism and a VRtest bed enables and facilitates the rapid design and test iterations toexamine trade-offs between swarming behavior and macroscopic-scale humaninfluence while circumventing flight duration limitations associated withbattery-powered small unmanned aerial system sUAS systems. The impedancecontrol-inspired mechanism was tested by a human supervisor to guide a virtualswarm consisting of 16 sUAS agents. Each test involved moving the swarmscenter of mass through narrow canyons which were not feasible for a swarm totraverse autonomously. Results demonstrate that integration of the influencemechanism enabled the successful manipulation of the macro-scale behavior ofthe swarm towards task completion while maintaining the innate swarmingbehavior.</p>
                <p>Last Updated: 2024-02-06 22:41:29 UTC</p>
                <button class="interpret-button" data-id="2402.04451v1">Interpret</button>
                <div id="interpretation-2402.04451v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit</h3>
                <p>Authors: Mengfan XuDiego Klabjan</p>
                <p><a href="http://arxiv.org/abs/2402.04417v1">Link to paper</a></p>
                <p>We study a robust multi-agent multi-armed bandit problem where multipleclients or participants are distributed on a fully decentralized blockchainwith the possibility of some being malicious. The rewards of arms arehomogeneous among the clients following time-invariant stochasticdistributions that are revealed to the participants only when the system issecure enough. The systems objective is to efficiently ensure the cumulativerewards gained by the honest participants. To this end and to the best of ourknowledge we are the first to incorporate advanced techniques fromblockchains as well as novel mechanisms into the system to design optimalstrategies for honest participants. This allows various malicious behaviors andthe maintenance of participant privacy. More specifically we randomly select apool of validators who have access to all participants design a brand-newconsensus mechanism based on digital signatures for these validators invent aUCB-based strategy that requires less information from participants throughsecure multi-party computation and design the chain-participant interactionand an incentive mechanism to encourage participants participation. Notablywe are the first to prove the theoretical guarantee of the proposed algorithmsby regret analyses in the context of optimality in blockchains. Unlike existingwork that integrates blockchains with learning problems such as federatedlearning which mainly focuses on numerical optimality we demonstrate that theregret of honest participants is upper bounded by logT. This is consistentwith the multi-agent multi-armed bandit problem without malicious participantsand the robust multi-agent multi-armed bandit problem with purely Byzantineattacks.</p>
                <p>Last Updated: 2024-02-06 21:33:34 UTC</p>
                <button class="interpret-button" data-id="2402.04417v1">Interpret</button>
                <div id="interpretation-2402.04417v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-02-09</p>
        </div>
    
        </div>
    </body>
    </html>
    