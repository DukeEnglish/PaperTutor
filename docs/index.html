
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health</h3>
                <p>Authors: Nikhil BehariEdwin ZhangYunfan ZhaoAparna TanejaDheeraj NagarajMilind Tambe</p>
                <p><a href="http://arxiv.org/abs/2402.14807v1">Link to paper</a></p>
                <p>Efforts to reduce maternal mortality rate a key UN Sustainable Developmenttarget SDG Target 3.1 rely largely on preventative care programs to spreadcritical health information to high-risk populations. These programs face twoimportant challenges: efficiently allocating limited health resources to largebeneficiary populations and adapting to evolving policy priorities. Whileprior works in restless multi-armed bandit RMAB demonstrated success inpublic health allocation tasks they lack flexibility to adapt to evolvingpolicy priorities. Concurrently Large Language Models LLMs have emerged asadept automated planners in various domains including robotic control andnavigation. In this paper we propose DLM: a Decision Language Model for RMABs.To enable dynamic fine-tuning of RMAB policies for challenging public healthsettings using human-language commands we propose using LLMs as automatedplanners to 1 interpret human policy preference prompts 2 propose codereward functions for a multi-agent RL environment for RMABs and 3 iterate onthe generated reward using feedback from RMAB simulations to effectively adaptpolicy outcomes. In collaboration with ARMMAN an India-based public healthorganization promoting preventative care for pregnant mothers we conduct asimulation study showing DLM can dynamically shape policy outcomes using onlyhuman language commands as input.</p>
                <p>Last Updated: 2024-02-22 18:58:27 UTC</p>
                <button class="interpret-button" data-id="2402.14807v1">Interpret</button>
                <div id="interpretation-2402.14807v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AgentScope: A Flexible yet Robust Multi-Agent Platform</h3>
                <p>Authors: Dawei GaoZitao LiWeirui KuangXuchen PanDaoyuan ChenZhijian MaBingchen QianLiuyi YaoLin ZhuChen ChengHongzhu ShiYaliang LiBolin DingJingren Zhou</p>
                <p><a href="http://arxiv.org/abs/2402.14034v1">Link to paper</a></p>
                <p>With the rapid advancement of Large Language Models LLMs significantprogress has been made in multi-agent applications. However the complexitiesin coordinating agents cooperation and LLMs erratic performance pose notablechallenges in developing robust and efficient multi-agent applications. Totackle these challenges we propose AgentScope a developer-centric multi-agentplatform with message exchange as its core communication mechanism. Togetherwith abundant syntactic tools built-in resources and user-friendlyinteractions our communication mechanism significantly reduces the barriers toboth development and understanding. Towards robust and flexible multi-agentapplication AgentScope provides both built-in and customizable fault tolerancemechanisms while it is also armed with system-level supports for multi-modaldata generation storage and transmission. Additionally we design anactor-based distribution framework enabling easy conversion between local anddistributed deployments and automatic parallel optimization without extraeffort. With these features AgentScope empowers developers to buildapplications that fully realize the potential of intelligent agents. We havereleased AgentScope at https://github.com/modelscope/agentscope and hopeAgentScope invites wider participation and innovation in this fast-movingfield.</p>
                <p>Last Updated: 2024-02-21 04:11:28 UTC</p>
                <button class="interpret-button" data-id="2402.14034v1">Interpret</button>
                <div id="interpretation-2402.14034v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies</h3>
                <p>Authors: Ammar N. AbbasChidera W. AmazuJoseph MietkiewiczHouda BriwaAndres Alonzo PerezGabriele BaldissoneMicaela DemichelaGeorgios G. ChasparisJohn D. KelleherMaria Chiara Leva</p>
                <p><a href="http://arxiv.org/abs/2402.13219v1">Link to paper</a></p>
                <p>In complex industrial and chemical process control rooms effectivedecision-making is crucial for safety and efficiency. The experiments in thispaper evaluate the impact and applications of an AI-based decision supportsystem integrated into an improved human-machine interface using dynamicinfluence diagrams a hidden Markov model and deep reinforcement learning. Theenhanced support system aims to reduce operator workload improve situationalawareness and provide different intervention strategies to the operatoradapted to the current state of both the system and human performance. Such asystem can be particularly useful in cases of information overload when manyalarms and inputs are presented all within the same time window or for junioroperators during training. A comprehensive cross-data analysis was conductedinvolving 47 participants and a diverse range of data sources such assmartwatch metrics eye-tracking data process logs and responses fromquestionnaires. The results indicate interesting insights regarding theeffectiveness of the approach in aiding decision-making decreasing perceivedworkload and increasing situational awareness for the scenarios considered.Additionally the results provide valuable insights to compare differencesbetween styles of information gathering when using the system by individualparticipants. These findings are particularly relevant when predicting theoverall performance of the individual participant and their capacity tosuccessfully handle a plant upset and the alarms connected to it using processand human-machine interaction logs in real-time. These predictions enable thedevelopment of more effective intervention strategies.</p>
                <p>Last Updated: 2024-02-20 18:31:27 UTC</p>
                <button class="interpret-button" data-id="2402.13219v1">Interpret</button>
                <div id="interpretation-2402.13219v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Conflict-Aware Optimal Goal Assignment Algorithm for Multi-Robot Systems</h3>
                <p>Authors: AakashIndranil Saha</p>
                <p><a href="http://arxiv.org/abs/2402.13292v1">Link to paper</a></p>
                <p>The fundamental goal assignment problem for a multi-robot application aims toassign a unique goal to each robot while ensuring collision-free pathsminimizing the total movement cost. A plausible algorithmic solution to thisNP-hard problem involves an iterative process that integrates a task planner tocompute the goal assignment while ignoring the collision possibilities amongthe robots and a multi-agent path-finding algorithm to find the collision-freetrajectories for a given assignment. This procedure involves a method forcomputing the next best assignment given the current best assignment. A naiveway of computing the next best assignment as done in the state-of-the-artsolutions becomes a roadblock to achieving scalability in solving the overallproblem. To obviate this bottleneck we propose an efficient conflict-guidedmethod to compute the next best assignment. Additionally we introduce two moreoptimizations to the algorithm -- first for avoiding the unconstrained pathcomputations between robot-goal pairs wherever possible and the second toprevent duplicate constrained path computations for multiple robot-goal pairs.We extensively evaluate our algorithm for up to a hundred robots on severalbenchmark workspaces. The results demonstrate that the proposed algorithmachieves nearly an order of magnitude speedup over the state-of-the-artalgorithm showcasing its efficacy in real-world scenarios.</p>
                <p>Last Updated: 2024-02-19 19:04:19 UTC</p>
                <button class="interpret-button" data-id="2402.13292v1">Interpret</button>
                <div id="interpretation-2402.13292v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents</h3>
                <p>Authors: Zengqing WuShuyuan ZhengQianying LiuXu HanBrian Inhyuk KwonMakoto OnizukaShaojie TangRun PengChuan Xiao</p>
                <p><a href="http://arxiv.org/abs/2402.12327v1">Link to paper</a></p>
                <p>Recent advancements have shown that agents powered by large language modelsLLMs possess capabilities to simulate human behaviors and societal dynamics.However the potential for LLM agents to spontaneously establish collaborativerelationships in the absence of explicit instructions has not been studied. Toaddress this gap we conduct three case studies revealing that LLM agents arecapable of spontaneously forming collaborations even within competitivesettings. This finding not only demonstrates the capacity of LLM agents tomimic competition and cooperation in human societies but also validates apromising vision of computational social science. Specifically it suggeststhat LLM agents could be utilized to model human social interactions includingthose with spontaneous collaborations thus offering insights into socialphenomena. The source codes for this study are available athttps://github.com/wuzengqing001225/SABM_ShallWeTalk .</p>
                <p>Last Updated: 2024-02-19 18:00:53 UTC</p>
                <button class="interpret-button" data-id="2402.12327v1">Interpret</button>
                <div id="interpretation-2402.12327v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging</h3>
                <p>Authors: Yuzhe YangYujia LiuXin LiuAvanti GulhaneDomenico MastrodicasaWei WuEdward J WangDushyant W SahaniShwetak Patel</p>
                <p><a href="http://arxiv.org/abs/2402.14815v1">Link to paper</a></p>
                <p>Advances in artificial intelligence AI have achieved expert-levelperformance in medical imaging applications. Notably self-supervisedvision-language foundation models can detect a broad spectrum of pathologieswithout relying on explicit training annotations. However it is crucial toensure that these AI models do not mirror or amplify human biases therebydisadvantaging historically marginalized groups such as females or Blackpatients. The manifestation of such biases could systematically delay essentialmedical care for certain patient subgroups. In this study we investigate thealgorithmic fairness of state-of-the-art vision-language foundation models inchest X-ray diagnosis across five globally-sourced datasets. Our findingsreveal that compared to board-certified radiologists these foundation modelsconsistently underdiagnose marginalized groups with even higher rates seen inintersectional subgroups such as Black female patients. Such demographicbiases present over a wide range of pathologies and demographic attributes.Further analysis of the model embedding uncovers its significant encoding ofdemographic information. Deploying AI systems with these biases in medicalimaging can intensify pre-existing care disparities posing potentialchallenges to equitable healthcare access and raising ethical questions abouttheir clinical application.</p>
                <p>Last Updated: 2024-02-22 18:59:53 UTC</p>
                <button class="interpret-button" data-id="2402.14815v1">Interpret</button>
                <div id="interpretation-2402.14815v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition</h3>
                <p>Authors: Lianghui ZhuJunwei ZhouYan LiuXin HaoWenyu LiuXinggang Wang</p>
                <p><a href="http://arxiv.org/abs/2402.14812v1">Link to paper</a></p>
                <p>Weakly supervised visual recognition using inexact supervision is a criticalyet challenging learning problem. It significantly reduces human labeling costsand traditionally relies on multi-instance learning and pseudo-labeling. Thispaper introduces WeakSAM and solves the weakly-supervised object detectionWSOD and segmentation by utilizing the pre-learned world knowledge containedin a vision foundation model i.e. the Segment Anything Model SAM. WeakSAMaddresses two critical limitations in traditional WSOD retraining i.e. pseudoground truth PGT incompleteness and noisy PGT instances through adaptive PGTgeneration and Region of Interest RoI drop regularization. It also addressesthe SAMs problems of requiring prompts and category unawareness for automaticobject detection and segmentation. Our results indicate that WeakSAMsignificantly surpasses previous state-of-the-art methods in WSOD and WSISbenchmarks with large margins i.e. average improvements of 7.4 and 8.5respectively. The code is available at urlhttps://github.com/hustvl/WeakSAM.</p>
                <p>Last Updated: 2024-02-22 18:59:24 UTC</p>
                <button class="interpret-button" data-id="2402.14812v1">Interpret</button>
                <div id="interpretation-2402.14812v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion</h3>
                <p>Authors: Xueyi LiuLi Yi</p>
                <p><a href="http://arxiv.org/abs/2402.14810v1">Link to paper</a></p>
                <p>In this work we tackle the challenging problem of denoising hand-objectinteractions HOI. Given an erroneous interaction sequence the objective isto refine the incorrect hand trajectory to remove interaction artifacts for aperceptually realistic sequence. This challenge involves intricate interactionnoise including unnatural hand poses and incorrect hand-object relationsalongside the necessity for robust generalization to new interactions anddiverse noise patterns. We tackle those challenges through a novel approachGeneOH Diffusion incorporating two key designs: an innovative contact-centricHOI representation named GeneOH and a new domain-generalizable denoisingscheme. The contact-centric representation GeneOH informatively parameterizesthe HOI process facilitating enhanced generalization across various HOIscenarios. The new denoising scheme consists of a canonical denoising modeltrained to project noisy data samples from a whitened noise space to a cleandata manifold and a denoising via diffusion strategy which can handle inputtrajectories with various noise patterns by first diffusing them to align withthe whitened noise space and cleaning via the canonical denoiser. Extensiveexperiments on four benchmarks with significant domain variations demonstratethe superior effectiveness of our method. GeneOH Diffusion also shows promisefor various downstream applications. Project website:https://meowuu7.github.io/GeneOH-Diffusion/.</p>
                <p>Last Updated: 2024-02-22 18:59:21 UTC</p>
                <button class="interpret-button" data-id="2402.14810v1">Interpret</button>
                <div id="interpretation-2402.14810v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CriticBench: Benchmarking LLMs for Critique-Correct Reasoning</h3>
                <p>Authors: Zicheng LinZhibin GouTian LiangRuilin LuoHaowei LiuYujiu Yang</p>
                <p><a href="http://arxiv.org/abs/2402.14809v1">Link to paper</a></p>
                <p>The ability of Large Language Models LLMs to critique and refine theirreasoning is crucial for their application in evaluation feedback provisionand self-improvement. This paper introduces CriticBench a comprehensivebenchmark designed to assess LLMs abilities to critique and rectify theirreasoning across a variety of tasks. CriticBench encompasses five reasoningdomains: mathematical commonsense symbolic coding and algorithmic. Itcompiles 15 datasets and incorporates responses from three LLM families.Utilizing CriticBench we evaluate and dissect the performance of 17 LLMs ingeneration critique and correction reasoning i.e. GQC reasoning. Ourfindings reveal: 1 a linear relationship in GQC capabilities withcritique-focused training markedly enhancing performance 2 a task-dependentvariation in correction effectiveness with logic-oriented tasks being moreamenable to correction 3 GQC knowledge inconsistencies that decrease asmodel size increases and 4 an intriguing inter-model critiquing dynamicwhere stronger models are better at critiquing weaker ones while weaker modelscan surprisingly surpass stronger ones in their self-critique. We hope theseinsights into the nuanced critique-correct reasoning of LLMs will fosterfurther research in LLM critique and self-improvement.</p>
                <p>Last Updated: 2024-02-22 18:59:02 UTC</p>
                <button class="interpret-button" data-id="2402.14809v1">Interpret</button>
                <div id="interpretation-2402.14809v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health</h3>
                <p>Authors: Nikhil BehariEdwin ZhangYunfan ZhaoAparna TanejaDheeraj NagarajMilind Tambe</p>
                <p><a href="http://arxiv.org/abs/2402.14807v1">Link to paper</a></p>
                <p>Efforts to reduce maternal mortality rate a key UN Sustainable Developmenttarget SDG Target 3.1 rely largely on preventative care programs to spreadcritical health information to high-risk populations. These programs face twoimportant challenges: efficiently allocating limited health resources to largebeneficiary populations and adapting to evolving policy priorities. Whileprior works in restless multi-armed bandit RMAB demonstrated success inpublic health allocation tasks they lack flexibility to adapt to evolvingpolicy priorities. Concurrently Large Language Models LLMs have emerged asadept automated planners in various domains including robotic control andnavigation. In this paper we propose DLM: a Decision Language Model for RMABs.To enable dynamic fine-tuning of RMAB policies for challenging public healthsettings using human-language commands we propose using LLMs as automatedplanners to 1 interpret human policy preference prompts 2 propose codereward functions for a multi-agent RL environment for RMABs and 3 iterate onthe generated reward using feedback from RMAB simulations to effectively adaptpolicy outcomes. In collaboration with ARMMAN an India-based public healthorganization promoting preventative care for pregnant mothers we conduct asimulation study showing DLM can dynamically shape policy outcomes using onlyhuman language commands as input.</p>
                <p>Last Updated: 2024-02-22 18:58:27 UTC</p>
                <button class="interpret-button" data-id="2402.14807v1">Interpret</button>
                <div id="interpretation-2402.14807v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Rao-Blackwellising Bayesian Causal Inference</h3>
                <p>Authors: Christian TothChristian KnollFranz PernkopfRobert Peharz</p>
                <p><a href="http://arxiv.org/abs/2402.14781v1">Link to paper</a></p>
                <p>Bayesian causal inference i.e. inferring a posterior over causal models forthe use in downstream causal reasoning tasks poses a hard computationalinference problem that is little explored in literature. In this work wecombine techniques from order-based MCMC structure learning with recentadvances in gradient-based graph learning into an effective Bayesian causalinference framework. Specifically we decompose the problem of inferring thecausal structure into i inferring a topological order over variables and iiinferring the parent sets for each variable. When limiting the number ofparents per variable we can exactly marginalise over the parent sets inpolynomial time. We further use Gaussian processes to model the unknown causalmechanisms which also allows their exact marginalisation. This introduces aRao-Blackwellization scheme where all components are eliminated from themodel except for the causal order for which we learn a distribution viagradient-based optimisation. The combination of Rao-Blackwellization with oursequential inference procedure for causal orders yields state-of-the-art onlinear and non-linear additive noise benchmarks with scale-free and Erdos-Renyigraph structures.</p>
                <p>Last Updated: 2024-02-22 18:39:24 UTC</p>
                <button class="interpret-button" data-id="2402.14781v1">Interpret</button>
                <div id="interpretation-2402.14781v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models</h3>
                <p>Authors: Alvaro RibotChandler SquiresCaroline Uhler</p>
                <p><a href="http://arxiv.org/abs/2402.14777v1">Link to paper</a></p>
                <p>We consider the task of causal imputation where we aim to predict theoutcomes of some set of actions across a wide range of possible contexts. As arunning example we consider predicting how different drugs affect cells fromdifferent cell types. We study the index-only setting where the actions andcontexts are categorical variables with a finite number of possible values.Even in this simple setting a practical challenge arises since often only asmall subset of possible action-context pairs have been studied. Thus modelsmust extrapolate to novel action-context pairs which can be framed as a formof matrix completion with rows indexed by actions columns indexed by contextsand matrix entries corresponding to outcomes. We introduce a novel SCM-basedmodel class where the outcome is expressed as a counterfactual actions areexpressed as interventions on an instrumental variable and contexts aredefined based on the initial state of the system. We show that under alinearity assumption this setup induces a latent factor model over the matrixof outcomes with an additional fixed effect term. To perform causal predictionbased on this model class we introduce simple extension to the SyntheticInterventions estimator Agarwal et al. 2020. We evaluate several matrixcompletion approaches on the PRISM drug repurposing dataset showing that ourmethod outperforms all other considered matrix completion approaches.</p>
                <p>Last Updated: 2024-02-22 18:37:33 UTC</p>
                <button class="interpret-button" data-id="2402.14777v1">Interpret</button>
                <div id="interpretation-2402.14777v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Batch and match: black-box variational inference with a score-based divergence</h3>
                <p>Authors: Diana CaiChirag ModiLoucas Pillaud-VivienCharles C. MargossianRobert M. GowerDavid M. BleiLawrence K. Saul</p>
                <p><a href="http://arxiv.org/abs/2402.14758v1">Link to paper</a></p>
                <p>Most leading implementations of black-box variational inference BBVI arebased on optimizing a stochastic evidence lower bound ELBO. But suchapproaches to BBVI often converge slowly due to the high variance of theirgradient estimates. In this work we propose batch and match BaM analternative approach to BBVI based on a score-based divergence. Notably thisscore-based divergence can be optimized by a closed-form proximal update forGaussian variational families with full covariance matrices. We analyze theconvergence of BaM when the target distribution is Gaussian and we prove thatin the limit of infinite batch size the variational parameter updates convergeexponentially quickly to the target mean and covariance. We also evaluate theperformance of BaM on Gaussian and non-Gaussian target distributions that arisefrom posterior inference in hierarchical and deep generative models. In theseexperiments we find that BaM typically converges in fewer and sometimessignificantly fewer gradient evaluations than leading implementations of BBVIbased on ELBO maximization.</p>
                <p>Last Updated: 2024-02-22 18:20:22 UTC</p>
                <button class="interpret-button" data-id="2402.14758v1">Interpret</button>
                <div id="interpretation-2402.14758v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>How Transformers Learn Causal Structure with Gradient Descent</h3>
                <p>Authors: Eshaan NichaniAlex DamianJason D. Lee</p>
                <p><a href="http://arxiv.org/abs/2402.14735v1">Link to paper</a></p>
                <p>The incredible success of transformers on sequence modeling tasks can belargely attributed to the self-attention mechanism which allows information tobe transferred between different parts of a sequence. Self-attention allowstransformers to encode causal structure which makes them particularly suitablefor sequence modeling. However the process by which transformers learn suchcausal structure via gradient-based training algorithms remains poorlyunderstood. To better understand this process we introduce an in-contextlearning task that requires learning latent causal structure. We prove thatgradient descent on a simplified two-layer transformer learns to solve thistask by encoding the latent causal graph in the first attention layer. The keyinsight of our proof is that the gradient of the attention matrix encodes themutual information between tokens. As a consequence of the data processinginequality the largest entries of this gradient correspond to edges in thelatent causal graph. As a special case when the sequences are generated fromin-context Markov chains we prove that transformers learn an induction headOlsson et al. 2022. We confirm our theoretical findings by showing thattransformers trained on our in-context learning task are able to recover a widevariety of causal structures.</p>
                <p>Last Updated: 2024-02-22 17:47:03 UTC</p>
                <button class="interpret-button" data-id="2402.14735v1">Interpret</button>
                <div id="interpretation-2402.14735v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Incorporating Expert Rules into Neural Networks in the Framework of Concept-Based Learning</h3>
                <p>Authors: Andrei V. KonstantinovLev V. Utkin</p>
                <p><a href="http://arxiv.org/abs/2402.14726v1">Link to paper</a></p>
                <p>A problem of incorporating the expert rules into machine learning models forextending the concept-based learning is formulated in the paper. It is proposedhow to combine logical rules and neural networks predicting the conceptprobabilities. The first idea behind the combination is to form constraints fora joint probability distribution over all combinations of concept values tosatisfy the expert rules. The second idea is to represent a feasible set ofprobability distributions in the form of a convex polytope and to use itsvertices or faces. We provide several approaches for solving the stated problemand for training neural networks which guarantee that the output probabilitiesof concepts would not violate the expert rules. The solution of the problem canbe viewed as a way for combining the inductive and deductive learning. Expertrules are used in a broader sense when any logical function that connectsconcepts and class labels or just concepts with each other can be regarded as arule. This feature significantly expands the class of the proposed results.Numerical examples illustrate the approaches. The code of proposed algorithmsis publicly available.</p>
                <p>Last Updated: 2024-02-22 17:33:49 UTC</p>
                <button class="interpret-button" data-id="2402.14726v1">Interpret</button>
                <div id="interpretation-2402.14726v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>PALO: A Polyglot Large Multimodal Model for 5B People</h3>
                <p>Authors: Muhammad MaazHanoona RasheedAbdelrahman ShakerSalman KhanHisham CholakalRao M. AnwerTim BaldwinMichael FelsbergFahad S. Khan</p>
                <p><a href="http://arxiv.org/abs/2402.14818v1">Link to paper</a></p>
                <p>In pursuit of more inclusive Vision-Language Models VLMs this studyintroduces a Large Multilingual Multimodal Model called textscPalo.textscPalo offers visual reasoning capabilities in 10 major languagesincluding English Chinese Hindi Spanish French Arabic Bengali RussianUrdu and Japanese that span a total of sim5B people 65 of the worldpopulation. Our approach involves a semi-automated translation approach toadapt the multimodal instruction dataset from English to the target languagesusing a fine-tuned Large Language Model thereby ensuring high linguisticfidelity while allowing scalability due to minimal manual effort. Theincorporation of diverse instruction sets helps us boost overall performanceacross multiple languages especially those that are underrepresented likeHindi Arabic Bengali and Urdu. The resulting models are trained across threescales 1.7B 7B and 13B parameters to show the generalization and scalabilitywhere we observe substantial improvements compared to strong baselines. We alsopropose the first multilingual multimodal benchmark for the forthcomingapproaches to evaluate their vision-language reasoning capabilities acrosslanguages. Code: https://github.com/mbzuai-oryx/PALO.</p>
                <p>Last Updated: 2024-02-22 18:59:58 UTC</p>
                <button class="interpret-button" data-id="2402.14818v1">Interpret</button>
                <div id="interpretation-2402.14818v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</h3>
                <p>Authors: Nikhil PrakashTamar Rott ShahamTal HaklayYonatan BelinkovDavid Bau</p>
                <p><a href="http://arxiv.org/abs/2402.14811v1">Link to paper</a></p>
                <p>Fine-tuning on generalized tasks such as instruction following codegeneration and mathematics has been shown to enhance language modelsperformance on a range of tasks. Nevertheless explanations of how suchfine-tuning influences the internal computations in these models remainelusive. We study how fine-tuning affects the internal mechanisms implementedin language models. As a case study we explore the property of entitytracking a crucial facet of language comprehension where models fine-tuned onmathematics have substantial performance gains. We identify the mechanism thatenables entity tracking and show that i in both the original model and itsfine-tuned versions primarily the same circuit implements entity tracking. Infact the entity tracking circuit of the original model on the fine-tunedversions performs better than the full original model. ii The circuits of allthe models implement roughly the same functionality: Entity tracking isperformed by tracking the position of the correct entity in both the originalmodel and its fine-tuned versions. iii Performance boost in the fine-tunedmodels is primarily attributed to its improved ability to handle the augmentedpositional information. To uncover these findings we employ: Patch PatchingDCM which automatically detects model components responsible for specificsemantics and CMAP a new approach for patching activations across models toreveal improved mechanisms. Our findings suggest that fine-tuning enhancesrather than fundamentally alters the mechanistic operation of the model.</p>
                <p>Last Updated: 2024-02-22 18:59:24 UTC</p>
                <button class="interpret-button" data-id="2402.14811v1">Interpret</button>
                <div id="interpretation-2402.14811v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CriticBench: Benchmarking LLMs for Critique-Correct Reasoning</h3>
                <p>Authors: Zicheng LinZhibin GouTian LiangRuilin LuoHaowei LiuYujiu Yang</p>
                <p><a href="http://arxiv.org/abs/2402.14809v1">Link to paper</a></p>
                <p>The ability of Large Language Models LLMs to critique and refine theirreasoning is crucial for their application in evaluation feedback provisionand self-improvement. This paper introduces CriticBench a comprehensivebenchmark designed to assess LLMs abilities to critique and rectify theirreasoning across a variety of tasks. CriticBench encompasses five reasoningdomains: mathematical commonsense symbolic coding and algorithmic. Itcompiles 15 datasets and incorporates responses from three LLM families.Utilizing CriticBench we evaluate and dissect the performance of 17 LLMs ingeneration critique and correction reasoning i.e. GQC reasoning. Ourfindings reveal: 1 a linear relationship in GQC capabilities withcritique-focused training markedly enhancing performance 2 a task-dependentvariation in correction effectiveness with logic-oriented tasks being moreamenable to correction 3 GQC knowledge inconsistencies that decrease asmodel size increases and 4 an intriguing inter-model critiquing dynamicwhere stronger models are better at critiquing weaker ones while weaker modelscan surprisingly surpass stronger ones in their self-critique. We hope theseinsights into the nuanced critique-correct reasoning of LLMs will fosterfurther research in LLM critique and self-improvement.</p>
                <p>Last Updated: 2024-02-22 18:59:02 UTC</p>
                <button class="interpret-button" data-id="2402.14809v1">Interpret</button>
                <div id="interpretation-2402.14809v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RelayAttention for Efficient Large Language Model Serving with Long System Prompts</h3>
                <p>Authors: Lei ZhuXinjiang WangWayne ZhangRynson W. H. Lau</p>
                <p><a href="http://arxiv.org/abs/2402.14808v1">Link to paper</a></p>
                <p>Practical large language model LLM services may involve a long systemprompt which specifies the instructions examples and knowledge documents ofthe task and is reused across numerous requests. However the long systemprompt causes throughput/latency bottlenecks as the cost of generating the nexttoken grows w.r.t. the sequence length. This paper aims to improve theefficiency of LLM services that involve long system prompts. Our keyobservation is that handling these system prompts requires heavily redundantmemory accesses in existing causal attention computation algorithms.Specifically for batched requests the cached hidden states i.e. key-valuepairs of system prompts are transferred from off-chip DRAM to on-chip SRAMmultiple times each corresponding to an individual request. To eliminate sucha redundancy we propose RelayAttention an attention algorithm that allowsreading these hidden states from DRAM exactly once for a batch of input tokens.RelayAttention is a free lunch: it maintains the generation quality whilerequiring no model retraining as it is based on a mathematical reformulationof causal attention.</p>
                <p>Last Updated: 2024-02-22 18:58:28 UTC</p>
                <button class="interpret-button" data-id="2402.14808v1">Interpret</button>
                <div id="interpretation-2402.14808v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Identifying Multiple Personalities in Large Language Models with External Evaluation</h3>
                <p>Authors: Xiaoyang SongYuta AdachiJessie FengMouwei LinLinhao YuFrank LiAkshat GuptaGopala AnumanchipalliSimerjot Kaur</p>
                <p><a href="http://arxiv.org/abs/2402.14805v1">Link to paper</a></p>
                <p>As Large Language Models LLMs are integrated with human daily applicationsrapidly many societal and ethical concerns are raised regarding the behaviorof LLMs. One of the ways to comprehend LLMs behavior is to analyze theirpersonalities. Many recent studies quantify LLMs personalities usingself-assessment tests that are created for humans. Yet many critiques questionthe applicability and reliability of these self-assessment tests when appliedto LLMs. In this paper we investigate LLM personalities using an alternatepersonality measurement method which we refer to as the external evaluationmethod where instead of prompting LLMs with multiple-choice questions in theLikert scale we evaluate LLMs personalities by analyzing their responsestoward open-ended situational questions using an external machine learningmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictorthat outperforms the state-of-the-art models as the tool to analyze LLMsresponses. Then we prompt the LLMs with situational questions and ask them togenerate Twitter posts and comments respectively in order to assess theirpersonalities when playing two different roles. Using the external personalityevaluation method we identify that the obtained personality types for LLMs aresignificantly different when generating posts versus comments whereas humansshow a consistent personality profile in these two different situations. Thisshows that LLMs can exhibit different personalities based on differentscenarios thus highlighting a fundamental difference between personality inLLMs and humans. With our work we call for a re-evaluation of personalitydefinition and measurement in LLMs.</p>
                <p>Last Updated: 2024-02-22 18:57:20 UTC</p>
                <button class="interpret-button" data-id="2402.14805v1">Interpret</button>
                <div id="interpretation-2402.14805v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>The European Commitment to Human-Centered Technology: The Integral Role of HCI in the EU AI Act's Success</h3>
                <p>Authors: André Calero ValdezMoreen HeineThomas FrankeNicole JochemsHans-Christian JetterTim Schrills</p>
                <p><a href="http://arxiv.org/abs/2402.14728v1">Link to paper</a></p>
                <p>The evolution of AI is set to profoundly reshape the future. The EuropeanUnion recognizing this impending prominence has enacted the AI Actregulating market access for AI-based systems. A salient feature of the Act isto guard democratic and humanistic values by focusing regulation ontransparency explainability and the human ability to understand and controlAI systems. Hereby the EU AI Act does not merely specify technologicalrequirements for AI systems. The EU issues a democratic call for human-centeredAI systems and in turn an interdisciplinary research agenda forhuman-centered innovation in AI development. Without robust methods to assessAI systems and their effect on individuals and society the EU AI Act may leadto repeating the mistakes of the General Data Protection Regulation of the EUand to rushed chaotic ad-hoc and ambiguous implementation causing moreconfusion than lending guidance. Moreover determined research activities inHuman-AI interaction will be pivotal for both regulatory compliance and theadvancement of AI in a manner that is both ethical and effective. Such anapproach will ensure that AI development aligns with human values and needsfostering a technology landscape that is innovative responsible and anintegral part of our society.</p>
                <p>Last Updated: 2024-02-22 17:35:29 UTC</p>
                <button class="interpret-button" data-id="2402.14728v1">Interpret</button>
                <div id="interpretation-2402.14728v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling</h3>
                <p>Authors: Baihan LinDjallel BouneffoufYulia LandaRachel JespersenCheryl CorcoranGuillermo Cecchi</p>
                <p><a href="http://arxiv.org/abs/2402.14701v1">Link to paper</a></p>
                <p>The therapeutic working alliance is a critical factor in predicting thesuccess of psychotherapy treatment. Traditionally working alliance assessmentrelies on questionnaires completed by both therapists and patients. In thispaper we present COMPASS a novel framework to directly infer the therapeuticworking alliance from the natural language used in psychotherapy sessions. Ourapproach utilizes advanced large language models to analyze transcripts ofpsychotherapy sessions and compare them with distributed representations ofstatements in the working alliance inventory. Analyzing a dataset of over 950sessions covering diverse psychiatric conditions we demonstrate theeffectiveness of our method in microscopically mapping patient-therapistalignment trajectories and providing interpretability for clinical psychiatryand in identifying emerging patterns related to the condition being treated. Byemploying various neural topic modeling techniques in combination withgenerative language prompting we analyze the topical characteristics ofdifferent psychiatric conditions and incorporate temporal modeling to capturethe evolution of topics at a turn-level resolution. This combined frameworkenhances the understanding of therapeutic interactions enabling timelyfeedback for therapists regarding conversation quality and providinginterpretable insights to improve the effectiveness of psychotherapy.</p>
                <p>Last Updated: 2024-02-22 16:56:44 UTC</p>
                <button class="interpret-button" data-id="2402.14701v1">Interpret</button>
                <div id="interpretation-2402.14701v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Doing AI: Algorithmic decision support as a human activity</h3>
                <p>Authors: Joachim Meyer</p>
                <p><a href="http://arxiv.org/abs/2402.14674v1">Link to paper</a></p>
                <p>Algorithmic decision support ADS using Machine-Learning-based AI isbecoming a major part of many processes. Organizations introduce ADS to improvedecision-making and make optimal use of data thereby possibly avoidingdeviations from the normative homo economicus and the biases thatcharacterize human decision-making. A closer look at the development process ofADS systems reveals that ADS itself results from a series of largelyunspecified human decisions. They begin with deliberations for which decisionsto use ADS continue with choices while developing the ADS and end with usingthe ADS output for decisions. Finally conclusions are implemented inorganizational settings often without analyzing the implications of thedecision support. The paper explores some issues in developing and using ADSpointing to behavioral aspects that should be considered when implementing ADSin organizational settings. It points out directions for further researchwhich is essential for gaining an informed understanding of the processes andtheir vulnerabilities.</p>
                <p>Last Updated: 2024-02-22 16:29:31 UTC</p>
                <button class="interpret-button" data-id="2402.14674v1">Interpret</button>
                <div id="interpretation-2402.14674v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GazeTrak: Exploring Acoustic-based Eye Tracking on a Glass Frame</h3>
                <p>Authors: Ke LiRuidong ZhangBoao ChenSiyuan ChenSicheng YinSaif MahmudQikang LiangFrançois GuimbretièreCheng Zhang</p>
                <p><a href="http://dx.doi.org/10.1145/3636534.3649376">Link to paper</a></p>
                <p>In this paper we present GazeTrak the first acoustic-based eye trackingsystem on glasses. Our system only needs one speaker and four microphonesattached to each side of the glasses. These acoustic sensors capture theformations of the eyeballs and the surrounding areas by emitting encodedinaudible sound towards eyeballs and receiving the reflected signals. Thesereflected signals are further processed to calculate the echo profiles whichare fed to a customized deep learning pipeline to continuously infer the gazeposition. In a user study with 20 participants GazeTrak achieves an accuracyof 3.6deg within the same remounting session and 4.9deg across differentsessions with a refreshing rate of 83.3 Hz and a power signature of 287.9 mW.Furthermore we report the performance of our gaze tracking system fullyimplemented on an MCU with a low-power CNN accelerator MAX78002. In thisconfiguration the system runs at up to 83.3 Hz and has a total power signatureof 95.4 mW with a 30 Hz FPS.</p>
                <p>Last Updated: 2024-02-22 15:28:26 UTC</p>
                <button class="interpret-button" data-id="2402.14634v1">Interpret</button>
                <div id="interpretation-2402.14634v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Non-Contact Acquisition of PPG Signal using Chest Movement-Modulated Radio Signals</h3>
                <p>Authors: Israel Jesus Santos FilhoMuhammad Mahboob Ur RahmanTaous-Meriem Laleg-KiratiTareq Al-Naffouri</p>
                <p><a href="http://arxiv.org/abs/2402.14565v1">Link to paper</a></p>
                <p>We present for the first time a novel method that utilizes the chestmovement-modulated radio signals for non-contact acquisition of thephotoplethysmography PPG signal. Under the proposed method asoftware-defined radio SDR exposes the chest of a subject sitting nearby toan orthogonal frequency division multiplexing signal with 64 sub-carriers at acenter frequency 5.24 GHz while another SDR in the close vicinity collects themodulated radio signal reflected off the chest. This way we construct a customdataset by collecting 160 minutes of labeled data both raw radio data as wellas the reference PPG signal from 16 healthy young subjects. With this wefirst utilize principal component analysis for dimensionality reduction of theradio data. Next we denoise the radio signal and reference PPG signal usingwavelet technique followed by segmentation and Z-score normalization. We thensynchronize the radio and PPG segments using cross-correlation method. Finallywe proceed to the waveform translation regression task whereby we firstconvert the radio and PPG segments into frequency domain using discrete cosinetransform DCT and then learn the non-linear regression between them.Eventually we reconstruct the synthetic PPG signal by taking inverse DCT ofthe output of regression block with a mean absolute error of 8.1294. Thesynthetic PPG waveform has a great clinical significance as it could be usedfor non-contact performance assessment of cardiovascular and respiratorysystems of patients suffering from infectious diseases e.g. covid19.</p>
                <p>Last Updated: 2024-02-22 14:04:13 UTC</p>
                <button class="interpret-button" data-id="2402.14565v1">Interpret</button>
                <div id="interpretation-2402.14565v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Cameras as Rays: Pose Estimation via Ray Diffusion</h3>
                <p>Authors: Jason Y. ZhangAmy LinMoneish KumarTzu-Hsuan YangDeva RamananShubham Tulsiani</p>
                <p><a href="http://arxiv.org/abs/2402.14817v1">Link to paper</a></p>
                <p>Estimating camera poses is a fundamental task for 3D reconstruction andremains challenging given sparse views 10. In contrast to existingapproaches that pursue top-down prediction of global parametrizations of cameraextrinsics we propose a distributed representation of camera pose that treatsa camera as a bundle of rays. This representation allows for a tight couplingwith spatial image features improving pose precision. We observe that thisrepresentation is naturally suited for set-level level transformers and developa regression-based approach that maps image patches to corresponding rays. Tocapture the inherent uncertainties in sparse-view pose inference we adapt thisapproach to learn a denoising diffusion model which allows us to sampleplausible modes while improving performance. Our proposed methods bothregression- and diffusion-based demonstrate state-of-the-art performance oncamera pose estimation on CO3D while generalizing to unseen object categoriesand in-the-wild captures.</p>
                <p>Last Updated: 2024-02-22 18:59:56 UTC</p>
                <button class="interpret-button" data-id="2402.14817v1">Interpret</button>
                <div id="interpretation-2402.14817v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging</h3>
                <p>Authors: Yuzhe YangYujia LiuXin LiuAvanti GulhaneDomenico MastrodicasaWei WuEdward J WangDushyant W SahaniShwetak Patel</p>
                <p><a href="http://arxiv.org/abs/2402.14815v1">Link to paper</a></p>
                <p>Advances in artificial intelligence AI have achieved expert-levelperformance in medical imaging applications. Notably self-supervisedvision-language foundation models can detect a broad spectrum of pathologieswithout relying on explicit training annotations. However it is crucial toensure that these AI models do not mirror or amplify human biases therebydisadvantaging historically marginalized groups such as females or Blackpatients. The manifestation of such biases could systematically delay essentialmedical care for certain patient subgroups. In this study we investigate thealgorithmic fairness of state-of-the-art vision-language foundation models inchest X-ray diagnosis across five globally-sourced datasets. Our findingsreveal that compared to board-certified radiologists these foundation modelsconsistently underdiagnose marginalized groups with even higher rates seen inintersectional subgroups such as Black female patients. Such demographicbiases present over a wide range of pathologies and demographic attributes.Further analysis of the model embedding uncovers its significant encoding ofdemographic information. Deploying AI systems with these biases in medicalimaging can intensify pre-existing care disparities posing potentialchallenges to equitable healthcare access and raising ethical questions abouttheir clinical application.</p>
                <p>Last Updated: 2024-02-22 18:59:53 UTC</p>
                <button class="interpret-button" data-id="2402.14815v1">Interpret</button>
                <div id="interpretation-2402.14815v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</h3>
                <p>Authors: Nikhil PrakashTamar Rott ShahamTal HaklayYonatan BelinkovDavid Bau</p>
                <p><a href="http://arxiv.org/abs/2402.14811v1">Link to paper</a></p>
                <p>Fine-tuning on generalized tasks such as instruction following codegeneration and mathematics has been shown to enhance language modelsperformance on a range of tasks. Nevertheless explanations of how suchfine-tuning influences the internal computations in these models remainelusive. We study how fine-tuning affects the internal mechanisms implementedin language models. As a case study we explore the property of entitytracking a crucial facet of language comprehension where models fine-tuned onmathematics have substantial performance gains. We identify the mechanism thatenables entity tracking and show that i in both the original model and itsfine-tuned versions primarily the same circuit implements entity tracking. Infact the entity tracking circuit of the original model on the fine-tunedversions performs better than the full original model. ii The circuits of allthe models implement roughly the same functionality: Entity tracking isperformed by tracking the position of the correct entity in both the originalmodel and its fine-tuned versions. iii Performance boost in the fine-tunedmodels is primarily attributed to its improved ability to handle the augmentedpositional information. To uncover these findings we employ: Patch PatchingDCM which automatically detects model components responsible for specificsemantics and CMAP a new approach for patching activations across models toreveal improved mechanisms. Our findings suggest that fine-tuning enhancesrather than fundamentally alters the mechanistic operation of the model.</p>
                <p>Last Updated: 2024-02-22 18:59:24 UTC</p>
                <button class="interpret-button" data-id="2402.14811v1">Interpret</button>
                <div id="interpretation-2402.14811v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion</h3>
                <p>Authors: Xueyi LiuLi Yi</p>
                <p><a href="http://arxiv.org/abs/2402.14810v1">Link to paper</a></p>
                <p>In this work we tackle the challenging problem of denoising hand-objectinteractions HOI. Given an erroneous interaction sequence the objective isto refine the incorrect hand trajectory to remove interaction artifacts for aperceptually realistic sequence. This challenge involves intricate interactionnoise including unnatural hand poses and incorrect hand-object relationsalongside the necessity for robust generalization to new interactions anddiverse noise patterns. We tackle those challenges through a novel approachGeneOH Diffusion incorporating two key designs: an innovative contact-centricHOI representation named GeneOH and a new domain-generalizable denoisingscheme. The contact-centric representation GeneOH informatively parameterizesthe HOI process facilitating enhanced generalization across various HOIscenarios. The new denoising scheme consists of a canonical denoising modeltrained to project noisy data samples from a whitened noise space to a cleandata manifold and a denoising via diffusion strategy which can handle inputtrajectories with various noise patterns by first diffusing them to align withthe whitened noise space and cleaning via the canonical denoiser. Extensiveexperiments on four benchmarks with significant domain variations demonstratethe superior effectiveness of our method. GeneOH Diffusion also shows promisefor various downstream applications. Project website:https://meowuu7.github.io/GeneOH-Diffusion/.</p>
                <p>Last Updated: 2024-02-22 18:59:21 UTC</p>
                <button class="interpret-button" data-id="2402.14810v1">Interpret</button>
                <div id="interpretation-2402.14810v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CriticBench: Benchmarking LLMs for Critique-Correct Reasoning</h3>
                <p>Authors: Zicheng LinZhibin GouTian LiangRuilin LuoHaowei LiuYujiu Yang</p>
                <p><a href="http://arxiv.org/abs/2402.14809v1">Link to paper</a></p>
                <p>The ability of Large Language Models LLMs to critique and refine theirreasoning is crucial for their application in evaluation feedback provisionand self-improvement. This paper introduces CriticBench a comprehensivebenchmark designed to assess LLMs abilities to critique and rectify theirreasoning across a variety of tasks. CriticBench encompasses five reasoningdomains: mathematical commonsense symbolic coding and algorithmic. Itcompiles 15 datasets and incorporates responses from three LLM families.Utilizing CriticBench we evaluate and dissect the performance of 17 LLMs ingeneration critique and correction reasoning i.e. GQC reasoning. Ourfindings reveal: 1 a linear relationship in GQC capabilities withcritique-focused training markedly enhancing performance 2 a task-dependentvariation in correction effectiveness with logic-oriented tasks being moreamenable to correction 3 GQC knowledge inconsistencies that decrease asmodel size increases and 4 an intriguing inter-model critiquing dynamicwhere stronger models are better at critiquing weaker ones while weaker modelscan surprisingly surpass stronger ones in their self-critique. We hope theseinsights into the nuanced critique-correct reasoning of LLMs will fosterfurther research in LLM critique and self-improvement.</p>
                <p>Last Updated: 2024-02-22 18:59:02 UTC</p>
                <button class="interpret-button" data-id="2402.14809v1">Interpret</button>
                <div id="interpretation-2402.14809v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>PALO: A Polyglot Large Multimodal Model for 5B People</h3>
                <p>Authors: Muhammad MaazHanoona RasheedAbdelrahman ShakerSalman KhanHisham CholakalRao M. AnwerTim BaldwinMichael FelsbergFahad S. Khan</p>
                <p><a href="http://arxiv.org/abs/2402.14818v1">Link to paper</a></p>
                <p>In pursuit of more inclusive Vision-Language Models VLMs this studyintroduces a Large Multilingual Multimodal Model called textscPalo.textscPalo offers visual reasoning capabilities in 10 major languagesincluding English Chinese Hindi Spanish French Arabic Bengali RussianUrdu and Japanese that span a total of sim5B people 65 of the worldpopulation. Our approach involves a semi-automated translation approach toadapt the multimodal instruction dataset from English to the target languagesusing a fine-tuned Large Language Model thereby ensuring high linguisticfidelity while allowing scalability due to minimal manual effort. Theincorporation of diverse instruction sets helps us boost overall performanceacross multiple languages especially those that are underrepresented likeHindi Arabic Bengali and Urdu. The resulting models are trained across threescales 1.7B 7B and 13B parameters to show the generalization and scalabilitywhere we observe substantial improvements compared to strong baselines. We alsopropose the first multilingual multimodal benchmark for the forthcomingapproaches to evaluate their vision-language reasoning capabilities acrosslanguages. Code: https://github.com/mbzuai-oryx/PALO.</p>
                <p>Last Updated: 2024-02-22 18:59:58 UTC</p>
                <button class="interpret-button" data-id="2402.14818v1">Interpret</button>
                <div id="interpretation-2402.14818v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Cameras as Rays: Pose Estimation via Ray Diffusion</h3>
                <p>Authors: Jason Y. ZhangAmy LinMoneish KumarTzu-Hsuan YangDeva RamananShubham Tulsiani</p>
                <p><a href="http://arxiv.org/abs/2402.14817v1">Link to paper</a></p>
                <p>Estimating camera poses is a fundamental task for 3D reconstruction andremains challenging given sparse views 10. In contrast to existingapproaches that pursue top-down prediction of global parametrizations of cameraextrinsics we propose a distributed representation of camera pose that treatsa camera as a bundle of rays. This representation allows for a tight couplingwith spatial image features improving pose precision. We observe that thisrepresentation is naturally suited for set-level level transformers and developa regression-based approach that maps image patches to corresponding rays. Tocapture the inherent uncertainties in sparse-view pose inference we adapt thisapproach to learn a denoising diffusion model which allows us to sampleplausible modes while improving performance. Our proposed methods bothregression- and diffusion-based demonstrate state-of-the-art performance oncamera pose estimation on CO3D while generalizing to unseen object categoriesand in-the-wild captures.</p>
                <p>Last Updated: 2024-02-22 18:59:56 UTC</p>
                <button class="interpret-button" data-id="2402.14817v1">Interpret</button>
                <div id="interpretation-2402.14817v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging</h3>
                <p>Authors: Yuzhe YangYujia LiuXin LiuAvanti GulhaneDomenico MastrodicasaWei WuEdward J WangDushyant W SahaniShwetak Patel</p>
                <p><a href="http://arxiv.org/abs/2402.14815v1">Link to paper</a></p>
                <p>Advances in artificial intelligence AI have achieved expert-levelperformance in medical imaging applications. Notably self-supervisedvision-language foundation models can detect a broad spectrum of pathologieswithout relying on explicit training annotations. However it is crucial toensure that these AI models do not mirror or amplify human biases therebydisadvantaging historically marginalized groups such as females or Blackpatients. The manifestation of such biases could systematically delay essentialmedical care for certain patient subgroups. In this study we investigate thealgorithmic fairness of state-of-the-art vision-language foundation models inchest X-ray diagnosis across five globally-sourced datasets. Our findingsreveal that compared to board-certified radiologists these foundation modelsconsistently underdiagnose marginalized groups with even higher rates seen inintersectional subgroups such as Black female patients. Such demographicbiases present over a wide range of pathologies and demographic attributes.Further analysis of the model embedding uncovers its significant encoding ofdemographic information. Deploying AI systems with these biases in medicalimaging can intensify pre-existing care disparities posing potentialchallenges to equitable healthcare access and raising ethical questions abouttheir clinical application.</p>
                <p>Last Updated: 2024-02-22 18:59:53 UTC</p>
                <button class="interpret-button" data-id="2402.14815v1">Interpret</button>
                <div id="interpretation-2402.14815v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition</h3>
                <p>Authors: Lianghui ZhuJunwei ZhouYan LiuXin HaoWenyu LiuXinggang Wang</p>
                <p><a href="http://arxiv.org/abs/2402.14812v1">Link to paper</a></p>
                <p>Weakly supervised visual recognition using inexact supervision is a criticalyet challenging learning problem. It significantly reduces human labeling costsand traditionally relies on multi-instance learning and pseudo-labeling. Thispaper introduces WeakSAM and solves the weakly-supervised object detectionWSOD and segmentation by utilizing the pre-learned world knowledge containedin a vision foundation model i.e. the Segment Anything Model SAM. WeakSAMaddresses two critical limitations in traditional WSOD retraining i.e. pseudoground truth PGT incompleteness and noisy PGT instances through adaptive PGTgeneration and Region of Interest RoI drop regularization. It also addressesthe SAMs problems of requiring prompts and category unawareness for automaticobject detection and segmentation. Our results indicate that WeakSAMsignificantly surpasses previous state-of-the-art methods in WSOD and WSISbenchmarks with large margins i.e. average improvements of 7.4 and 8.5respectively. The code is available at urlhttps://github.com/hustvl/WeakSAM.</p>
                <p>Last Updated: 2024-02-22 18:59:24 UTC</p>
                <button class="interpret-button" data-id="2402.14812v1">Interpret</button>
                <div id="interpretation-2402.14812v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion</h3>
                <p>Authors: Xueyi LiuLi Yi</p>
                <p><a href="http://arxiv.org/abs/2402.14810v1">Link to paper</a></p>
                <p>In this work we tackle the challenging problem of denoising hand-objectinteractions HOI. Given an erroneous interaction sequence the objective isto refine the incorrect hand trajectory to remove interaction artifacts for aperceptually realistic sequence. This challenge involves intricate interactionnoise including unnatural hand poses and incorrect hand-object relationsalongside the necessity for robust generalization to new interactions anddiverse noise patterns. We tackle those challenges through a novel approachGeneOH Diffusion incorporating two key designs: an innovative contact-centricHOI representation named GeneOH and a new domain-generalizable denoisingscheme. The contact-centric representation GeneOH informatively parameterizesthe HOI process facilitating enhanced generalization across various HOIscenarios. The new denoising scheme consists of a canonical denoising modeltrained to project noisy data samples from a whitened noise space to a cleandata manifold and a denoising via diffusion strategy which can handle inputtrajectories with various noise patterns by first diffusing them to align withthe whitened noise space and cleaning via the canonical denoiser. Extensiveexperiments on four benchmarks with significant domain variations demonstratethe superior effectiveness of our method. GeneOH Diffusion also shows promisefor various downstream applications. Project website:https://meowuu7.github.io/GeneOH-Diffusion/.</p>
                <p>Last Updated: 2024-02-22 18:59:21 UTC</p>
                <button class="interpret-button" data-id="2402.14810v1">Interpret</button>
                <div id="interpretation-2402.14810v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-02-26</p>
        </div>
    
        </div>
    </body>
    </html>
    