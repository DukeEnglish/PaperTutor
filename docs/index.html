
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Robi Butler: Remote Multimodal Interactions with Household Robot Assistant</h3>
                <p>Authors: Anxing XiaoNuwan JanakaTianrun HuAnshul GuptaKaixin LiCunjun YuDavid Hsu</p>
                <p><a href="http://arxiv.org/abs/2409.20548v1">Link to paper</a></p>
                <p>In this paper we introduce Robi Butler a novel household robotic systemthat enables multimodal interactions with remote users. Building on theadvanced communication interfaces Robi Butler allows users to monitor therobots status send text or voice instructions and select target objects byhand pointing. At the core of our system is a high-level behavior modulepowered by Large Language Models LLMs that interprets multimodalinstructions to generate action plans. These plans are composed of a set ofopen vocabulary primitives supported by Vision Language Models VLMs thathandle both text and pointing queries. The integration of the above componentsallows Robi Butler to ground remote multimodal instructions in the real-worldhome environment in a zero-shot manner. We demonstrate the effectiveness andefficiency of this system using a variety of daily household tasks that involveremote users giving multimodal instructions. Additionally we conducted a userstudy to analyze how multimodal interactions affect efficiency and userexperience during remote human-robot interaction and discuss the potentialimprovements.</p>
                <p>Last Updated: 2024-09-30 17:49:09 UTC</p>
                <button class="interpret-button" data-id="2409.20548v1">Interpret</button>
                <div id="interpretation-2409.20548v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Does Positive Reinforcement Work?: A Quasi-Experimental Study of the Effects of Positive Feedback on Reddit</h3>
                <p>Authors: Charlotte LambertKoustuv SahaEshwar Chandrasekharan</p>
                <p><a href="http://arxiv.org/abs/2409.20410v1">Link to paper</a></p>
                <p>Social media platform design often incorporates explicit signals of positivefeedback. Some moderators provide positive feedback with the goal of positivereinforcement but are often unsure of their ability to actually influence userbehavior. Despite its widespread use and theory touting positive feedback ascrucial for user motivation its effect on recipients is relatively unknown.This paper examines how positive feedback impacts Reddit users and evaluatesits differential effects to understand who benefits most from receivingpositive feedback. Through a causal inference study of 11M posts across 4months we find that users who received positive feedback made more frequent2 per day and higher quality 57 higher score 2 fewer removals per dayposts compared to a set of matched control users. Our findings highlight theneed for platforms and communities to expand their perspective on moderationand complement punitive approaches with positive reinforcement strategies.</p>
                <p>Last Updated: 2024-09-30 15:38:33 UTC</p>
                <button class="interpret-button" data-id="2409.20410v1">Interpret</button>
                <div id="interpretation-2409.20410v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Automation from the Worker's Perspective</h3>
                <p>Authors: Ben ArmstrongValerie K. ChenAlex CuellarAlexandra Forsey-SmerekJulie A. Shah</p>
                <p><a href="http://arxiv.org/abs/2409.20387v1">Link to paper</a></p>
                <p>Common narratives about automation often pit new technologies againstworkers. The introduction of advanced machine tools industrial robots and AIhave all been met with concern that technological progress will mean fewerjobs. However workers themselves offer a more optimistic nuanced perspective.Drawing on a far-reaching 2024 survey of more than 9000 workers across ninecountries this paper finds that more workers report potential benefits fromnew technologies like robots and AI for their safety and comfort at work theirpay and their autonomy on the job than report potential costs. Workers withjobs that ask them to solve complex problems workers who feel valued by theiremployers and workers who are motivated to move up in their careers are allmore likely to see new technologies as beneficial. In contrast to assumptionsin previous research more formal education is in some cases associated withmore negative attitudes toward automation and its impact on work. In anexperimental setting the prospect of financial incentives for workers improvetheir perceptions of automation technologies whereas the prospect of increasedinput about how new technologies are used does not have a significant effect onworkers attitudes toward automation.</p>
                <p>Last Updated: 2024-09-30 15:21:08 UTC</p>
                <button class="interpret-button" data-id="2409.20387v1">Interpret</button>
                <div id="interpretation-2409.20387v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Computer-mediated therapies for stroke rehabilitation: a systematic review and meta-Analysis</h3>
                <p>Authors: Stanley Mugisha. Mirko Job. Matteo ZoppiMarco TestaRezia Molfino</p>
                <p><a href="http://dx.doi.org/10.1016/j.jstrokecerebrovasdis.2022.106454">Link to paper</a></p>
                <p>OBJECTIVE: To evaluate the efficacy of different forms of virtual realityVR treatments as either immersive virtual reality IVR or non-immersivevirtual reality NIVR in comparison to conventional therapy CT in improvingphysical and psychological status among stroke patients. METHODS: Theliterature search was conducted on seven databases. ACM Digital LibraryMedline via PubMed Cochrane IEEE Xplore Web of Science and Scopus. Theeffect sizes of the main outcomes were calculated using Cohens d. Pooledresults were used to present an overall estimate of the treatment effect usinga random-effects model. RESULTS: A total of 22 randomized controlled trialswere evaluated. 3 trials demonstrated that immersive virtual reality improvedupper limb activity function and activity of daily life in a way comparable toCT. 18 trials showed that NIVR had similar benefits to CT for upper limbactivity and function balance and mobility activities of daily living andparticipation. A comparison between the different forms of VR showed that IVRmay be more beneficial than NIVR for upper limb training and activities ofdaily life. CONCLUSIONS: This study found out that IVR therapies may be moreeffective than NIVR but not CT to improve upper limb activity function anddaily life activities. However there is no evidence of the durability of IVRtreatment. More research involving studies with larger samples is needed toassess the long-term effects and promising benefits of immersive virtualreality technology.</p>
                <p>Last Updated: 2024-09-30 12:50:46 UTC</p>
                <button class="interpret-button" data-id="2409.20260v1">Interpret</button>
                <div id="interpretation-2409.20260v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Investigating Creation Perspectives and Icon Placement Preferences for On-Body Menus in Virtual Reality</h3>
                <p>Authors: Xiang LiWei HeShan JinJan GugenheimerPan HuiHai-Ning LiangPer Ola Kristensson</p>
                <p><a href="http://dx.doi.org/10.1145/3698136">Link to paper</a></p>
                <p>On-body menus present a novel interaction paradigm within Virtual RealityVR environments by embedding virtual interfaces directly onto the usersbody. Unlike traditional screen-based interfaces on-body menus enable users tointeract with virtual options or icons visually attached to their physicalform. In this paper We investigated the impact of the creation process on theeffectiveness of on-body menus comparing first-person third-person andmirror perspectives. Our first study N  12 revealed that the mirrorperspective led to faster creation times and more accurate recall compared tothe other two perspectives. To further explore user preferences we conducted asecond study N  18 utilizing a VR system with integrated body tracking. Bycombining distributions of icons from both studies N  30 we confirmedsignificant preferences in on-body menu placement based on icon category e.g.Social Media icons were consistently placed on forearms. We also discoveredassociations between categories such as Leisure and Social Media iconsfrequently co-occurring. Our findings highlight the importance of the creationprocess uncover user preferences for on-body menu organization and provideinsights to guide the development of intuitive and effective on-bodyinteractions within virtual environments.</p>
                <p>Last Updated: 2024-09-30 12:23:24 UTC</p>
                <button class="interpret-button" data-id="2409.20238v1">Interpret</button>
                <div id="interpretation-2409.20238v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Continuously Improving Mobile Manipulation with Autonomous Real-World RL</h3>
                <p>Authors: Russell MendoncaEmmanuel PanovBernadette BucherJiuguang WangDeepak Pathak</p>
                <p><a href="http://arxiv.org/abs/2409.20568v1">Link to paper</a></p>
                <p>We present a fully autonomous real-world RL framework for mobile manipulationthat can learn policies without extensive instrumentation or human supervision.This is enabled by 1 task-relevant autonomy which guides exploration towardsobject interactions and prevents stagnation near goal states 2 efficientpolicy learning by leveraging basic task knowledge in behavior priors and 3formulating generic rewards that combine human-interpretable semanticinformation with low-level fine-grained observations. We demonstrate that ourapproach allows Spot robots to continually improve their performance on a setof four challenging mobile manipulation tasks obtaining an average successrate of 80 across tasks a 3-4 improvement over existing approaches. Videoscan be found at https://continual-mobile-manip.github.io/</p>
                <p>Last Updated: 2024-09-30 17:59:50 UTC</p>
                <button class="interpret-button" data-id="2409.20568v1">Interpret</button>
                <div id="interpretation-2409.20568v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning</h3>
                <p>Authors: Haotian ZhangMingfei GaoZhe GanPhilipp DufterNina WenzelForrest HuangDhruti ShahXianzhi DuBowen ZhangYanghao LiSam DodgeKeen YouZhen YangAleksei TimofeevMingze XuHong-You ChenJean-Philippe FauconnierZhengfeng LaiHaoxuan YouZirui WangAfshin DehghanPeter GraschYinfei Yang</p>
                <p><a href="http://arxiv.org/abs/2409.20566v1">Link to paper</a></p>
                <p>We present MM1.5 a new family of multimodal large language models MLLMsdesigned to enhance capabilities in text-rich image understanding visualreferring and grounding and multi-image reasoning. Building upon the MM1architecture MM1.5 adopts a data-centric approach to model trainingsystematically exploring the impact of diverse data mixtures across the entiremodel training lifecycle. This includes high-quality OCR data and syntheticcaptions for continual pre-training as well as an optimized visualinstruction-tuning data mixture for supervised fine-tuning. Our models rangefrom 1B to 30B parameters encompassing both dense and mixture-of-experts MoEvariants and demonstrate that careful data curation and training strategiescan yield strong performance even at small scales 1B and 3B. Additionally weintroduce two specialized variants: MM1.5-Video designed for videounderstanding and MM1.5-UI tailored for mobile UI understanding. Throughextensive empirical studies and ablations we provide detailed insights intothe training processes and decisions that inform our final designs offeringvaluable guidance for future research in MLLM development.</p>
                <p>Last Updated: 2024-09-30 17:59:34 UTC</p>
                <button class="interpret-button" data-id="2409.20566v1">Interpret</button>
                <div id="interpretation-2409.20566v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SpaceMesh: A Continuous Representation for Learning Manifold Surface Meshes</h3>
                <p>Authors: Tianchang ShenZhaoshuo LiMarc LawMatan AtzmonSanja FidlerJames LucasJun GaoNicholas Sharp</p>
                <p><a href="http://dx.doi.org/10.1145/3680528.3687634">Link to paper</a></p>
                <p>Meshes are ubiquitous in visual computing and simulation yet most existingmachine learning techniques represent meshes only indirectly e.g. as the levelset of a scalar field or deformation of a template or as a disordered trianglesoup lacking local structure. This work presents a scheme to directly generatemanifold polygonal meshes of complex connectivity as the output of a neuralnetwork. Our key innovation is to define a continuous latent connectivity spaceat each mesh vertex which implies the discrete mesh. In particular our vertexembeddings generate cyclic neighbor relationships in a halfedge meshrepresentation which gives a guarantee of edge-manifoldness and the ability torepresent general polygonal meshes. This representation is well-suited tomachine learning and stochastic optimization without restriction onconnectivity or topology. We first explore the basic properties of thisrepresentation then use it to fit distributions of meshes from large datasets.The resulting models generate diverse meshes with tessellation structurelearned from the dataset population with concise details and high-quality meshelements. In applications this approach not only yields high-quality outputsfrom generative models but also enables directly learning challenging geometryprocessing tasks such as mesh repair.</p>
                <p>Last Updated: 2024-09-30 17:59:03 UTC</p>
                <button class="interpret-button" data-id="2409.20562v1">Interpret</button>
                <div id="interpretation-2409.20562v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner</h3>
                <p>Authors: Xiaopan ZhangHao QinFuquan WangYue DongJiachen Li</p>
                <p><a href="http://arxiv.org/abs/2409.20560v1">Link to paper</a></p>
                <p>Language models LMs possess a strong capability to comprehend naturallanguage making them effective in translating human instructions into detailedplans for simple robot tasks. Nevertheless it remains a significant challengeto handle long-horizon tasks especially in subtask identification andallocation for cooperative heterogeneous robot teams. To address this issue wepropose a Language Model-Driven Multi-Agent PDDL Planner LaMMA-P a novelmulti-agent task planning framework that achieves state-of-the-art performanceon long-horizon tasks. LaMMA-P integrates the strengths of the LMs reasoningcapability and the traditional heuristic search planner to achieve a highsuccess rate and efficiency while demonstrating strong generalization acrosstasks. Additionally we create MAT-THOR a comprehensive benchmark thatfeatures household tasks with two different levels of complexity based on theAI2-THOR environment. The experimental results demonstrate that LaMMA-Pachieves a 105 higher success rate and 36 higher efficiency than existingLM-based multi-agent planners. The experimental videos code and datasets ofthis work as well as the detailed prompts used in each module are available athttps://lamma-p.github.io.</p>
                <p>Last Updated: 2024-09-30 17:58:18 UTC</p>
                <button class="interpret-button" data-id="2409.20560v1">Interpret</button>
                <div id="interpretation-2409.20560v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Supervised Multi-Modal Fission Learning</h3>
                <p>Authors: Lingchao MaoQi wangYi SuFleming LureJing Li</p>
                <p><a href="http://arxiv.org/abs/2409.20559v1">Link to paper</a></p>
                <p>Learning from multimodal datasets can leverage complementary information andimprove performance in prediction tasks. A commonly used strategy to accountfor feature correlations in high-dimensional datasets is the latent variableapproach. Several latent variable methods have been proposed for multimodaldatasets. However these methods either focus on extracting the sharedcomponent across all modalities or on extracting both a shared component andindividual components specific to each modality. To address this gap wepropose a Multi-Modal Fission Learning MMFL model that simultaneouslyidentifies globally joint partially joint and individual componentsunderlying the features of multimodal datasets. Unlike existing latent variablemethods MMFL uses supervision from the response variable to identifypredictive latent components and has a natural extension for incorporatingincomplete multimodal data. Through simulation studies we demonstrate thatMMFL outperforms various existing multimodal algorithms in both complete andincomplete modality settings. We applied MMFL to a real-world case study forearly prediction of Alzheimers Disease using multimodal neuroimaging andgenomics data from the Alzheimers Disease Neuroimaging Initiative ADNIdataset. MMFL provided more accurate predictions and better insights intowithin- and across-modality correlations compared to existing methods.</p>
                <p>Last Updated: 2024-09-30 17:58:03 UTC</p>
                <button class="interpret-button" data-id="2409.20559v1">Interpret</button>
                <div id="interpretation-2409.20559v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Continuously Improving Mobile Manipulation with Autonomous Real-World RL</h3>
                <p>Authors: Russell MendoncaEmmanuel PanovBernadette BucherJiuguang WangDeepak Pathak</p>
                <p><a href="http://arxiv.org/abs/2409.20568v1">Link to paper</a></p>
                <p>We present a fully autonomous real-world RL framework for mobile manipulationthat can learn policies without extensive instrumentation or human supervision.This is enabled by 1 task-relevant autonomy which guides exploration towardsobject interactions and prevents stagnation near goal states 2 efficientpolicy learning by leveraging basic task knowledge in behavior priors and 3formulating generic rewards that combine human-interpretable semanticinformation with low-level fine-grained observations. We demonstrate that ourapproach allows Spot robots to continually improve their performance on a setof four challenging mobile manipulation tasks obtaining an average successrate of 80 across tasks a 3-4 improvement over existing approaches. Videoscan be found at https://continual-mobile-manip.github.io/</p>
                <p>Last Updated: 2024-09-30 17:59:50 UTC</p>
                <button class="interpret-button" data-id="2409.20568v1">Interpret</button>
                <div id="interpretation-2409.20568v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner</h3>
                <p>Authors: Xiaopan ZhangHao QinFuquan WangYue DongJiachen Li</p>
                <p><a href="http://arxiv.org/abs/2409.20560v1">Link to paper</a></p>
                <p>Language models LMs possess a strong capability to comprehend naturallanguage making them effective in translating human instructions into detailedplans for simple robot tasks. Nevertheless it remains a significant challengeto handle long-horizon tasks especially in subtask identification andallocation for cooperative heterogeneous robot teams. To address this issue wepropose a Language Model-Driven Multi-Agent PDDL Planner LaMMA-P a novelmulti-agent task planning framework that achieves state-of-the-art performanceon long-horizon tasks. LaMMA-P integrates the strengths of the LMs reasoningcapability and the traditional heuristic search planner to achieve a highsuccess rate and efficiency while demonstrating strong generalization acrosstasks. Additionally we create MAT-THOR a comprehensive benchmark thatfeatures household tasks with two different levels of complexity based on theAI2-THOR environment. The experimental results demonstrate that LaMMA-Pachieves a 105 higher success rate and 36 higher efficiency than existingLM-based multi-agent planners. The experimental videos code and datasets ofthis work as well as the detailed prompts used in each module are available athttps://lamma-p.github.io.</p>
                <p>Last Updated: 2024-09-30 17:58:18 UTC</p>
                <button class="interpret-button" data-id="2409.20560v1">Interpret</button>
                <div id="interpretation-2409.20560v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Maia-2: A Unified Model for Human-AI Alignment in Chess</h3>
                <p>Authors: Zhenwei TangDifan JiaoReid McIlroy-YoungJon KleinbergSiddhartha SenAshton Anderson</p>
                <p><a href="http://arxiv.org/abs/2409.20553v1">Link to paper</a></p>
                <p>There are an increasing number of domains in which artificial intelligenceAI systems both surpass human ability and accurately model human behavior.This introduces the possibility of algorithmically-informed teaching in thesedomains through more relatable AI partners and deeper insights into humandecision-making. Critical to achieving this goal however is coherentlymodeling human behavior at various skill levels. Chess is an ideal model systemfor conducting research into this kind of human-AI alignment with its richhistory as a pivotal testbed for AI research mature superhuman AI systems likeAlphaZero and precise measurements of skill via chess rating systems. Previouswork in modeling human decision-making in chess uses completely independentmodels to capture human style at different skill levels meaning they lackcoherence in their ability to adapt to the full spectrum of human improvementand are ultimately limited in their effectiveness as AI partners and teachingtools. In this work we propose a unified modeling approach for human-AIalignment in chess that coherently captures human style across different skilllevels and directly captures how people improve. Recognizing the complexnon-linear nature of human learning we introduce a skill-aware attentionmechanism to dynamically integrate players strengths with encoded chesspositions enabling our model to be sensitive to evolving player skill. Ourexperimental results demonstrate that this unified framework significantlyenhances the alignment between AI and human players across a diverse range ofexpertise levels paving the way for deeper insights into human decision-makingand AI-guided teaching tools.</p>
                <p>Last Updated: 2024-09-30 17:54:23 UTC</p>
                <button class="interpret-button" data-id="2409.20553v1">Interpret</button>
                <div id="interpretation-2409.20553v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation</h3>
                <p>Authors: Ziyao ZhangYanlin WangChong WangJiachi ChenZibin Zheng</p>
                <p><a href="http://arxiv.org/abs/2409.20550v1">Link to paper</a></p>
                <p>Code generation aims to automatically generate code from input requirementssignificantly enhancing development efficiency. Recent large language modelsLLMs based approaches have shown promising results and revolutionized codegeneration task. Despite the promising performance LLMs often generatecontents with hallucinations especially for the code generation scenariorequiring the handling of complex contextual dependencies in practicaldevelopment process. Although previous study has analyzed hallucinations inLLM-powered code generation the study is limited to standalone functiongeneration. In this paper we conduct an empirical study to study thephenomena mechanism and mitigation of LLM hallucinations within morepractical and complex development contexts in repository-level generationscenario. First we manually examine the code generation results from sixmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.Next we elaborate on the phenomenon of hallucinations analyze theirdistribution across different models. We then analyze causes of hallucinationsand identify four potential factors contributing to hallucinations. Finally wepropose an RAG-based mitigation method which demonstrates consistenteffectiveness in all studied LLMs. The replication package including codedata and experimental results is available athttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination</p>
                <p>Last Updated: 2024-09-30 17:51:15 UTC</p>
                <button class="interpret-button" data-id="2409.20550v1">Interpret</button>
                <div id="interpretation-2409.20550v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Robi Butler: Remote Multimodal Interactions with Household Robot Assistant</h3>
                <p>Authors: Anxing XiaoNuwan JanakaTianrun HuAnshul GuptaKaixin LiCunjun YuDavid Hsu</p>
                <p><a href="http://arxiv.org/abs/2409.20548v1">Link to paper</a></p>
                <p>In this paper we introduce Robi Butler a novel household robotic systemthat enables multimodal interactions with remote users. Building on theadvanced communication interfaces Robi Butler allows users to monitor therobots status send text or voice instructions and select target objects byhand pointing. At the core of our system is a high-level behavior modulepowered by Large Language Models LLMs that interprets multimodalinstructions to generate action plans. These plans are composed of a set ofopen vocabulary primitives supported by Vision Language Models VLMs thathandle both text and pointing queries. The integration of the above componentsallows Robi Butler to ground remote multimodal instructions in the real-worldhome environment in a zero-shot manner. We demonstrate the effectiveness andefficiency of this system using a variety of daily household tasks that involveremote users giving multimodal instructions. Additionally we conducted a userstudy to analyze how multimodal interactions affect efficiency and userexperience during remote human-robot interaction and discuss the potentialimprovements.</p>
                <p>Last Updated: 2024-09-30 17:49:09 UTC</p>
                <button class="interpret-button" data-id="2409.20548v1">Interpret</button>
                <div id="interpretation-2409.20548v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning</h3>
                <p>Authors: Haotian ZhangMingfei GaoZhe GanPhilipp DufterNina WenzelForrest HuangDhruti ShahXianzhi DuBowen ZhangYanghao LiSam DodgeKeen YouZhen YangAleksei TimofeevMingze XuHong-You ChenJean-Philippe FauconnierZhengfeng LaiHaoxuan YouZirui WangAfshin DehghanPeter GraschYinfei Yang</p>
                <p><a href="http://arxiv.org/abs/2409.20566v1">Link to paper</a></p>
                <p>We present MM1.5 a new family of multimodal large language models MLLMsdesigned to enhance capabilities in text-rich image understanding visualreferring and grounding and multi-image reasoning. Building upon the MM1architecture MM1.5 adopts a data-centric approach to model trainingsystematically exploring the impact of diverse data mixtures across the entiremodel training lifecycle. This includes high-quality OCR data and syntheticcaptions for continual pre-training as well as an optimized visualinstruction-tuning data mixture for supervised fine-tuning. Our models rangefrom 1B to 30B parameters encompassing both dense and mixture-of-experts MoEvariants and demonstrate that careful data curation and training strategiescan yield strong performance even at small scales 1B and 3B. Additionally weintroduce two specialized variants: MM1.5-Video designed for videounderstanding and MM1.5-UI tailored for mobile UI understanding. Throughextensive empirical studies and ablations we provide detailed insights intothe training processes and decisions that inform our final designs offeringvaluable guidance for future research in MLLM development.</p>
                <p>Last Updated: 2024-09-30 17:59:34 UTC</p>
                <button class="interpret-button" data-id="2409.20566v1">Interpret</button>
                <div id="interpretation-2409.20566v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments</h3>
                <p>Authors: Iker De la IglesiaIakes GoenagaJohanna Ramirez-RomeroJose Maria Villa-GonzalezJosu GoikoetxeaAnder Barrena</p>
                <p><a href="http://arxiv.org/abs/2409.20565v1">Link to paper</a></p>
                <p>Evaluating LLM-generated text has become a key challenge especially indomain-specific contexts like the medical field. This work introduces a novelevaluation methodology for LLM-generated medical explanatory arguments relyingon Proxy Tasks and rankings to closely align results with human evaluationcriteria overcoming the biases typically seen in LLMs used as judges. Wedemonstrate that the proposed evaluators are robust against adversarialattacks including the assessment of non-argumentative text. Additionally thehuman-crafted arguments needed to train the evaluators are minimized to justone example per Proxy Task. By examining multiple LLM-generated arguments weestablish a methodology for determining whether a Proxy Task is suitable forevaluating LLM-generated medical explanatory arguments requiring only fiveexamples and two human experts.</p>
                <p>Last Updated: 2024-09-30 17:59:33 UTC</p>
                <button class="interpret-button" data-id="2409.20565v1">Interpret</button>
                <div id="interpretation-2409.20565v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation</h3>
                <p>Authors: Ziyao ZhangYanlin WangChong WangJiachi ChenZibin Zheng</p>
                <p><a href="http://arxiv.org/abs/2409.20550v1">Link to paper</a></p>
                <p>Code generation aims to automatically generate code from input requirementssignificantly enhancing development efficiency. Recent large language modelsLLMs based approaches have shown promising results and revolutionized codegeneration task. Despite the promising performance LLMs often generatecontents with hallucinations especially for the code generation scenariorequiring the handling of complex contextual dependencies in practicaldevelopment process. Although previous study has analyzed hallucinations inLLM-powered code generation the study is limited to standalone functiongeneration. In this paper we conduct an empirical study to study thephenomena mechanism and mitigation of LLM hallucinations within morepractical and complex development contexts in repository-level generationscenario. First we manually examine the code generation results from sixmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.Next we elaborate on the phenomenon of hallucinations analyze theirdistribution across different models. We then analyze causes of hallucinationsand identify four potential factors contributing to hallucinations. Finally wepropose an RAG-based mitigation method which demonstrates consistenteffectiveness in all studied LLMs. The replication package including codedata and experimental results is available athttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination</p>
                <p>Last Updated: 2024-09-30 17:51:15 UTC</p>
                <button class="interpret-button" data-id="2409.20550v1">Interpret</button>
                <div id="interpretation-2409.20550v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Word Sense Disambiguation in Native Spanish: A Comprehensive Lexical Evaluation Resource</h3>
                <p>Authors: Pablo OrtegaJordi LuqueLuis LamiableRodrigo LópezRichard Benjamins</p>
                <p><a href="http://arxiv.org/abs/2409.20524v1">Link to paper</a></p>
                <p>Human language while aimed at conveying meaning inherently carriesambiguity. It poses challenges for speech and language processing but alsoserves crucial communicative functions. Efficiently solve ambiguity is both adesired and a necessary characteristic. The lexical meaning of a word incontext can be determined automatically by Word Sense Disambiguation WSDalgorithms that rely on external knowledge often limited and biased towardEnglish. When adapting content to other languages automated translations arefrequently inaccurate and a high degree of expert human validation is necessaryto ensure both accuracy and understanding. The current study addresses previouslimitations by introducing a new resource for Spanish WSD. It includes a senseinventory and a lexical dataset sourced from the Diccionario de la LenguaEspanola which is maintained by the Real Academia Espanola. We also reviewcurrent resources for Spanish and report metrics on them by a state-of-the-artsystem.</p>
                <p>Last Updated: 2024-09-30 17:22:33 UTC</p>
                <button class="interpret-button" data-id="2409.20524v1">Interpret</button>
                <div id="interpretation-2409.20524v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Enhancing Romanian Offensive Language Detection through Knowledge Distillation, Multi-Task Learning, and Data Augmentation</h3>
                <p>Authors: Vlad-Cristian MateiIulian-Marius TăiatuRăzvan-Alexandru SmăduDumitru-Clementin Cercel</p>
                <p><a href="http://dx.doi.org/10.1007/978-3-031-70239-6_22">Link to paper</a></p>
                <p>This paper highlights the significance of natural language processing NLPwithin artificial intelligence underscoring its pivotal role in comprehendingand modeling human language. Recent advancements in NLP particularly inconversational bots have garnered substantial attention and adoption amongdevelopers. This paper explores advanced methodologies for attaining smallerand more efficient NLP models. Specifically we employ three key approaches:1 training a Transformer-based neural network to detect offensive language2 employing data augmentation and knowledge distillation techniques toincrease performance and 3 incorporating multi-task learning with knowledgedistillation and teacher annealing using diverse datasets to enhanceefficiency. The culmination of these methods has yielded demonstrably improvedoutcomes.</p>
                <p>Last Updated: 2024-09-30 16:59:48 UTC</p>
                <button class="interpret-button" data-id="2409.20498v1">Interpret</button>
                <div id="interpretation-2409.20498v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner</h3>
                <p>Authors: Xiaopan ZhangHao QinFuquan WangYue DongJiachen Li</p>
                <p><a href="http://arxiv.org/abs/2409.20560v1">Link to paper</a></p>
                <p>Language models LMs possess a strong capability to comprehend naturallanguage making them effective in translating human instructions into detailedplans for simple robot tasks. Nevertheless it remains a significant challengeto handle long-horizon tasks especially in subtask identification andallocation for cooperative heterogeneous robot teams. To address this issue wepropose a Language Model-Driven Multi-Agent PDDL Planner LaMMA-P a novelmulti-agent task planning framework that achieves state-of-the-art performanceon long-horizon tasks. LaMMA-P integrates the strengths of the LMs reasoningcapability and the traditional heuristic search planner to achieve a highsuccess rate and efficiency while demonstrating strong generalization acrosstasks. Additionally we create MAT-THOR a comprehensive benchmark thatfeatures household tasks with two different levels of complexity based on theAI2-THOR environment. The experimental results demonstrate that LaMMA-Pachieves a 105 higher success rate and 36 higher efficiency than existingLM-based multi-agent planners. The experimental videos code and datasets ofthis work as well as the detailed prompts used in each module are available athttps://lamma-p.github.io.</p>
                <p>Last Updated: 2024-09-30 17:58:18 UTC</p>
                <button class="interpret-button" data-id="2409.20560v1">Interpret</button>
                <div id="interpretation-2409.20560v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MARLadona -- Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning</h3>
                <p>Authors: Zichong LiFilip BjelonicVictor KlemmMarco Hutter</p>
                <p><a href="http://arxiv.org/abs/2409.20326v1">Link to paper</a></p>
                <p>Robot soccer in its full complexity poses an unsolved research challenge.Current solutions heavily rely on engineered heuristic strategies which lackrobustness and adaptability. Deep reinforcement learning has gained significanttraction in various complex robotics tasks such as locomotion manipulationand competitive games e.g. AlphaZero OpenAI Five making it a promisingsolution to the robot soccer problem. This paper introduces MARLadona. Adecentralized multi-agent reinforcement learning MARL training pipelinecapable of producing agents with sophisticated team play behavior bridging theshortcomings of heuristic methods. Further we created an open-sourcemulti-agent soccer environment based on Isaac Gym. Utilizing our MARL frameworkand a modified a global entity encoder as our core architecture our approachachieves a 66.8 win rate against HELIOS agent which employs astate-of-the-art heuristic strategy. Furthermore we provided an in-depthanalysis of the policy behavior and interpreted the agents intention using thecritic network.</p>
                <p>Last Updated: 2024-09-30 14:26:53 UTC</p>
                <button class="interpret-button" data-id="2409.20326v1">Interpret</button>
                <div id="interpretation-2409.20326v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Can We Break the Curse of Multiagency in Robust Multi-Agent Reinforcement Learning?</h3>
                <p>Authors: Laixi ShiJingchu GaiEric MazumdarYuejie ChiAdam Wierman</p>
                <p><a href="http://arxiv.org/abs/2409.20067v1">Link to paper</a></p>
                <p>Standard multi-agent reinforcement learning MARL algorithms are vulnerableto sim-to-real gaps. To address this distributionally robust Markov gamesRMGs have been proposed to enhance robustness in MARL by optimizing theworst-case performance when game dynamics shift within a prescribed uncertaintyset. Solving RMGs remains under-explored from problem formulation to thedevelopment of sample-efficient algorithms. A notorious yet open challenge isif RMGs can escape the curse of multiagency where the sample complexity scalesexponentially with the number of agents. In this work we propose a naturalclass of RMGs where the uncertainty set of each agent is shaped by both theenvironment and other agents strategies in a best-response manner. We firstestablish the well-posedness of these RMGs by proving the existence ofgame-theoretic solutions such as robust Nash equilibria and coarse correlatedequilibria CCE. Assuming access to a generative model we then introduce asample-efficient algorithm for learning the CCE whose sample complexity scalespolynomially with all relevant parameters. To the best of our knowledge thisis the first algorithm to break the curse of multiagency for RMGs.</p>
                <p>Last Updated: 2024-09-30 08:09:41 UTC</p>
                <button class="interpret-button" data-id="2409.20067v1">Interpret</button>
                <div id="interpretation-2409.20067v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fuel tax loss in a world of electric mobility: A window of opportunity for congestion pricing</h3>
                <p>Authors: Thi Ngoc NguyenFelix Muesgens</p>
                <p><a href="http://arxiv.org/abs/2409.20033v1">Link to paper</a></p>
                <p>The continued transition towards electric mobility will decrease energy taxrevenues worldwide which has substantial implications for government funds. Atthe same time demand for transportation is ever increasing which in turnincreases congestion problems. Combining both challenges this paper assessesthe effectiveness of congestion pricing as a sustainable revenue stream tooffset fuel tax loss in 2030 while simultaneously enhancing efficiency in thetransport sector. A congestion-based toll that is road-and-time-variant issimulated for the greater Berlin area in Germany using the multi-agenttransport simulation MATSim software. Through the simulation results thispaper quantifies the impacts of the toll on the governmental revenue trafficmanagement environment social welfare and the distribution effects. We findthat the revenue from congestion tolls in a metropolitan area can compensatethe reduction in passenger car fuel tax. Furthermore a remarkable welfaresurplus is observed. The toll also successfully incentivises transport users toadjust their travel behaviour which reduces traffic delay time by 28. CO2emissions as a key metric for decarbonisation of the transport sector decreaseby more than 5. The analysis of the distribution effects suggests that aredistribution plan with a focus on the middle-low-income residents and theouter boroughs could help the policy gain more public acceptance.</p>
                <p>Last Updated: 2024-09-30 07:39:29 UTC</p>
                <button class="interpret-button" data-id="2409.20033v1">Interpret</button>
                <div id="interpretation-2409.20033v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Variational Auto-encoder Based Solutions to Interactive Dynamic Influence Diagrams</h3>
                <p>Authors: Yinghui PanBiyang MaHanyi ZhangYifeng Zeng</p>
                <p><a href="http://arxiv.org/abs/2409.19965v1">Link to paper</a></p>
                <p>Addressing multiagent decision problems in AI especially those involvingcollaborative or competitive agents acting concurrently in a partiallyobservable and stochastic environment remains a formidable challenge. WhileInteractive Dynamic Influence DiagramsI-DIDs have offered a promisingdecision framework for such problems they encounter limitations when thesubject agent encounters unknown behaviors exhibited by other agents that arenot explicitly modeled within the I-DID. This can lead to sub-optimal responsesfrom the subject agent. In this paper we propose a novel data-driven approachthat utilizes an encoder-decoder architecture particularly a variationalautoencoder to enhance I-DID solutions. By integrating a perplexity-based treeloss function into the optimization algorithm of the variational autoencodercoupled with the advantages of Zig-Zag One-Hot encoding and decoding wegenerate potential behaviors of other agents within the I-DID that are morelikely to contain their true behaviors even from limited interactions. Thisnew approach enables the subject agent to respond more appropriately to unknownbehaviors thus improving its decision quality. We empirically demonstrate theeffectiveness of the proposed approach in two well-established problem domainshighlighting its potential for handling multi-agent decision problems withunknown behaviors. This work is the first time of using neural networks basedapproaches to deal with the I-DID challenge in agent planning and learningproblems.</p>
                <p>Last Updated: 2024-09-30 05:35:18 UTC</p>
                <button class="interpret-button" data-id="2409.19965v1">Interpret</button>
                <div id="interpretation-2409.19965v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Annealing Flow Generative Model Towards Sampling High-Dimensional and Multi-Modal Distributions</h3>
                <p>Authors: Dongze WuYao Xie</p>
                <p><a href="http://arxiv.org/abs/2409.20547v1">Link to paper</a></p>
                <p>Sampling from high-dimensional multi-modal distributions remains afundamental challenge across domains such as statistical Bayesian inference andphysics-based machine learning. In this paper we propose Annealing Flow AFa continuous normalizing flow-based approach designed to sample fromhigh-dimensional and multi-modal distributions. The key idea is to learn acontinuous normalizing flow-based transport map guided by annealing totransition samples from an easy-to-sample distribution to the targetdistribution facilitating effective exploration of modes in high-dimensionalspaces. Unlike many existing methods AF training does not rely on samples fromthe target distribution. AF ensures effective and balanced mode explorationachieves linear complexity in sample size and dimensions and circumventsinefficient mixing times. We demonstrate the superior performance of AFcompared to state-of-the-art methods through extensive experiments on variouschallenging distributions and real-world datasets particularly inhigh-dimensional and multi-modal settings. We also highlight the potential ofAF for sampling the least favorable distributions.</p>
                <p>Last Updated: 2024-09-30 17:48:22 UTC</p>
                <button class="interpret-button" data-id="2409.20547v1">Interpret</button>
                <div id="interpretation-2409.20547v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Upper and Lower Bounds for Distributionally Robust Off-Dynamics Reinforcement Learning</h3>
                <p>Authors: Zhishuai LiuWeixin WangPan Xu</p>
                <p><a href="http://arxiv.org/abs/2409.20521v1">Link to paper</a></p>
                <p>We study off-dynamics Reinforcement Learning RL where the policy trainingand deployment environments are different. To deal with this environmentalperturbation we focus on learning policies robust to uncertainties intransition dynamics under the framework of distributionally robust Markovdecision processes DRMDPs where the nominal and perturbed dynamics arelinear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U thatenjoys an average suboptimality widetildemathcalObigd H cdot min1/rho H/sqrtK big where K is the number of episodes H isthe horizon length d is the feature dimension and rho is the uncertaintylevel. This result improves the state-of-the-art bymathcalOdH/min1/rhoH. We also construct a novel hard instance andderive the first information-theoretic lower bound in this setting whichindicates our algorithm is near-optimal up to mathcalOsqrtH for anyuncertainty level rhoin01. Our algorithm also enjoys a rare-switchingdesign and thus only requires mathcalOdHlog1H2K policy switchesand mathcalOd2Hlog1H2K calls for oracle to solve dual optimizationproblems which significantly improves the computational efficiency of existingalgorithms for DRMDPs whose policy switch and oracle complexities are bothmathcalOK.</p>
                <p>Last Updated: 2024-09-30 17:21:15 UTC</p>
                <button class="interpret-button" data-id="2409.20521v1">Interpret</button>
                <div id="interpretation-2409.20521v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits</h3>
                <p>Authors: Mengmeng LiDaniel KuhnBahar Taskesen</p>
                <p><a href="http://arxiv.org/abs/2409.20440v1">Link to paper</a></p>
                <p>Follow-The-Regularized-Leader FTRL algorithms often enjoy optimal regretfor adversarial as well as stochastic bandit problems and allow for astreamlined analysis. Nonetheless FTRL algorithms require the solution of anoptimization problem in every iteration and are thus computationallychallenging. In contrast Follow-The-Perturbed-Leader FTPL algorithms achievecomputational efficiency by perturbing the estimates of the rewards of thearms but their regret analysis is cumbersome. We propose a new FTPL algorithmthat generates optimal policies for both adversarial and stochastic multi-armedbandits. Like FTRL our algorithm admits a unified regret analysis and similarto FTPL it offers low computational costs. Unlike existing FTPL algorithmsthat rely on independent additive disturbances governed by a textitknowndistribution we allow for disturbances governed by an textitambiguousdistribution that is only known to belong to a given set and propose aprinciple of optimism in the face of ambiguity. Consequently our frameworkgeneralizes existing FTPL algorithms. It also encapsulates a broad range ofFTRL methods as special cases including several optimal ones which appears tobe impossible with current FTPL methods. Finally we use techniques fromdiscrete choice theory to devise an efficient bisection algorithm for computingthe optimistic arm sampling probabilities. This algorithm is up to 104 timesfaster than standard FTRL algorithms that solve an optimization problem inevery iteration. Our results not only settle existing conjectures but alsoprovide new insights into the impact of perturbations by mapping FTRL to FTPL.</p>
                <p>Last Updated: 2024-09-30 16:00:23 UTC</p>
                <button class="interpret-button" data-id="2409.20440v1">Interpret</button>
                <div id="interpretation-2409.20440v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Sufficient and Necessary Explanations (and What Lies in Between)</h3>
                <p>Authors: Beepul BhartiPaul YiJeremias Sulam</p>
                <p><a href="http://arxiv.org/abs/2409.20427v1">Link to paper</a></p>
                <p>As complex machine learning models continue to find applications inhigh-stakes decision-making scenarios it is crucial that we can explain andunderstand their predictions. Post-hoc explanation methods provide usefulinsights by identifying important features in an input mathbfx withrespect to the model output fmathbfx. In this work we formalize andstudy two precise notions of feature importance for general machine learningmodels: sufficiency and necessity. We demonstrate how these two types ofexplanations albeit intuitive and simple can fall short in providing acomplete picture of which features a model finds important. To this end wepropose a unified notion of importance that circumvents these limitations byexploring a continuum along a necessity-sufficiency axis. Our unified notionwe show has strong ties to other popular definitions of feature importancelike those based on conditional independence and game-theoretic quantities likeShapley values. Crucially we demonstrate how a unified perspective allows usto detect important features that could be missed by either of the previousapproaches alone.</p>
                <p>Last Updated: 2024-09-30 15:50:57 UTC</p>
                <button class="interpret-button" data-id="2409.20427v1">Interpret</button>
                <div id="interpretation-2409.20427v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Stream-level flow matching from a Bayesian decision theoretic perspective</h3>
                <p>Authors: Ganchao WeiLi Ma</p>
                <p><a href="http://arxiv.org/abs/2409.20423v1">Link to paper</a></p>
                <p>Flow matching FM is a family of training algorithms for fitting continuousnormalizing flows CNFs. A standard approach to FM called conditional flowmatching CFM exploits the fact that the marginal vector field of a CNF canbe learned by fitting least-square regression to the so-called conditionalvector field specified given one or both ends of the flow path. We show thatviewing CFM training from a Bayesian decision theoretic perspective onparameter estimation opens the door to generalizations of CFM algorithms. Wepropose one such extension by introducing a CFM algorithm based on definingconditional probability paths given what we refer to as streams instancesof latent stochastic paths that connect pairs of noise and observed data.Further we advocates the modeling of these latent streams using Gaussianprocesses GPs. The unique distributional properties of GPs and in particularthe fact that the velocities of a GP is still a GP allows drawing samples fromthe resulting stream-augmented conditional probability path without simulatingthe actual streams and hence the simulation-free nature of CFM training ispreserved. We show that this generalization of the CFM can substantially reducethe variance in the estimated marginal vector field at a moderate computationalcost thereby improving the quality of the generated samples under commonmetrics. Additionally we show that adopting the GP on the streams allows forflexibly linking multiple related training data points e.g. time series andincorporating additional prior information. We empirically validate our claimthrough both simulations and applications to two hand-written image datasets.</p>
                <p>Last Updated: 2024-09-30 15:47:22 UTC</p>
                <button class="interpret-button" data-id="2409.20423v1">Interpret</button>
                <div id="interpretation-2409.20423v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Continuously Improving Mobile Manipulation with Autonomous Real-World RL</h3>
                <p>Authors: Russell MendoncaEmmanuel PanovBernadette BucherJiuguang WangDeepak Pathak</p>
                <p><a href="http://arxiv.org/abs/2409.20568v1">Link to paper</a></p>
                <p>We present a fully autonomous real-world RL framework for mobile manipulationthat can learn policies without extensive instrumentation or human supervision.This is enabled by 1 task-relevant autonomy which guides exploration towardsobject interactions and prevents stagnation near goal states 2 efficientpolicy learning by leveraging basic task knowledge in behavior priors and 3formulating generic rewards that combine human-interpretable semanticinformation with low-level fine-grained observations. We demonstrate that ourapproach allows Spot robots to continually improve their performance on a setof four challenging mobile manipulation tasks obtaining an average successrate of 80 across tasks a 3-4 improvement over existing approaches. Videoscan be found at https://continual-mobile-manip.github.io/</p>
                <p>Last Updated: 2024-09-30 17:59:50 UTC</p>
                <button class="interpret-button" data-id="2409.20568v1">Interpret</button>
                <div id="interpretation-2409.20568v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning</h3>
                <p>Authors: Haotian ZhangMingfei GaoZhe GanPhilipp DufterNina WenzelForrest HuangDhruti ShahXianzhi DuBowen ZhangYanghao LiSam DodgeKeen YouZhen YangAleksei TimofeevMingze XuHong-You ChenJean-Philippe FauconnierZhengfeng LaiHaoxuan YouZirui WangAfshin DehghanPeter GraschYinfei Yang</p>
                <p><a href="http://arxiv.org/abs/2409.20566v1">Link to paper</a></p>
                <p>We present MM1.5 a new family of multimodal large language models MLLMsdesigned to enhance capabilities in text-rich image understanding visualreferring and grounding and multi-image reasoning. Building upon the MM1architecture MM1.5 adopts a data-centric approach to model trainingsystematically exploring the impact of diverse data mixtures across the entiremodel training lifecycle. This includes high-quality OCR data and syntheticcaptions for continual pre-training as well as an optimized visualinstruction-tuning data mixture for supervised fine-tuning. Our models rangefrom 1B to 30B parameters encompassing both dense and mixture-of-experts MoEvariants and demonstrate that careful data curation and training strategiescan yield strong performance even at small scales 1B and 3B. Additionally weintroduce two specialized variants: MM1.5-Video designed for videounderstanding and MM1.5-UI tailored for mobile UI understanding. Throughextensive empirical studies and ablations we provide detailed insights intothe training processes and decisions that inform our final designs offeringvaluable guidance for future research in MLLM development.</p>
                <p>Last Updated: 2024-09-30 17:59:34 UTC</p>
                <button class="interpret-button" data-id="2409.20566v1">Interpret</button>
                <div id="interpretation-2409.20566v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DressRecon: Freeform 4D Human Reconstruction from Monocular Video</h3>
                <p>Authors: Jeff TanDonglai XiangShubham TulsianiDeva RamananGengshan Yang</p>
                <p><a href="http://arxiv.org/abs/2409.20563v1">Link to paper</a></p>
                <p>We present a method to reconstruct time-consistent human body models frommonocular videos focusing on extremely loose clothing or handheld objectinteractions. Prior work in human reconstruction is either limited to tightclothing with no object interactions or requires calibrated multi-viewcaptures or personalized template scans which are costly to collect at scale.Our key insight for high-quality yet flexible reconstruction is the carefulcombination of generic human priors about articulated body shape learned fromlarge-scale training data with video-specific articulated bag-of-bonesdeformation fit to a single video via test-time optimization. We accomplishthis by learning a neural implicit model that disentangles body versus clothingdeformations as separate motion model layers. To capture subtle geometry ofclothing we leverage image-based priors such as human body pose surfacenormals and optical flow during optimization. The resulting neural fields canbe extracted into time-consistent meshes or further optimized as explicit 3DGaussians for high-fidelity interactive rendering. On datasets with highlychallenging clothing deformations and object interactions DressRecon yieldshigher-fidelity 3D reconstructions than prior art. Project page:https://jefftan969.github.io/dressrecon/</p>
                <p>Last Updated: 2024-09-30 17:59:15 UTC</p>
                <button class="interpret-button" data-id="2409.20563v1">Interpret</button>
                <div id="interpretation-2409.20563v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SpaceMesh: A Continuous Representation for Learning Manifold Surface Meshes</h3>
                <p>Authors: Tianchang ShenZhaoshuo LiMarc LawMatan AtzmonSanja FidlerJames LucasJun GaoNicholas Sharp</p>
                <p><a href="http://dx.doi.org/10.1145/3680528.3687634">Link to paper</a></p>
                <p>Meshes are ubiquitous in visual computing and simulation yet most existingmachine learning techniques represent meshes only indirectly e.g. as the levelset of a scalar field or deformation of a template or as a disordered trianglesoup lacking local structure. This work presents a scheme to directly generatemanifold polygonal meshes of complex connectivity as the output of a neuralnetwork. Our key innovation is to define a continuous latent connectivity spaceat each mesh vertex which implies the discrete mesh. In particular our vertexembeddings generate cyclic neighbor relationships in a halfedge meshrepresentation which gives a guarantee of edge-manifoldness and the ability torepresent general polygonal meshes. This representation is well-suited tomachine learning and stochastic optimization without restriction onconnectivity or topology. We first explore the basic properties of thisrepresentation then use it to fit distributions of meshes from large datasets.The resulting models generate diverse meshes with tessellation structurelearned from the dataset population with concise details and high-quality meshelements. In applications this approach not only yields high-quality outputsfrom generative models but also enables directly learning challenging geometryprocessing tasks such as mesh repair.</p>
                <p>Last Updated: 2024-09-30 17:59:03 UTC</p>
                <button class="interpret-button" data-id="2409.20562v1">Interpret</button>
                <div id="interpretation-2409.20562v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner</h3>
                <p>Authors: Xiaopan ZhangHao QinFuquan WangYue DongJiachen Li</p>
                <p><a href="http://arxiv.org/abs/2409.20560v1">Link to paper</a></p>
                <p>Language models LMs possess a strong capability to comprehend naturallanguage making them effective in translating human instructions into detailedplans for simple robot tasks. Nevertheless it remains a significant challengeto handle long-horizon tasks especially in subtask identification andallocation for cooperative heterogeneous robot teams. To address this issue wepropose a Language Model-Driven Multi-Agent PDDL Planner LaMMA-P a novelmulti-agent task planning framework that achieves state-of-the-art performanceon long-horizon tasks. LaMMA-P integrates the strengths of the LMs reasoningcapability and the traditional heuristic search planner to achieve a highsuccess rate and efficiency while demonstrating strong generalization acrosstasks. Additionally we create MAT-THOR a comprehensive benchmark thatfeatures household tasks with two different levels of complexity based on theAI2-THOR environment. The experimental results demonstrate that LaMMA-Pachieves a 105 higher success rate and 36 higher efficiency than existingLM-based multi-agent planners. The experimental videos code and datasets ofthis work as well as the detailed prompts used in each module are available athttps://lamma-p.github.io.</p>
                <p>Last Updated: 2024-09-30 17:58:18 UTC</p>
                <button class="interpret-button" data-id="2409.20560v1">Interpret</button>
                <div id="interpretation-2409.20560v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-10-02</p>
        </div>
    
        </div>
    </body>
    </html>
    