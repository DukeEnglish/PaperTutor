
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>MobA: A Two-Level Agent System for Efficient Mobile Task Automation</h3>
                <p>Authors: Zichen ZhuHao TangYansi LiKunyao LanYixuan JiangHao ZhouYixiao WangSituo ZhangLiangtai SunLu ChenKai Yu</p>
                <p><a href="http://arxiv.org/abs/2410.13757v1">Link to paper</a></p>
                <p>Current mobile assistants are limited by dependence on system APIs orstruggle with complex user instructions and diverse interfaces due torestricted comprehension and decision-making abilities. To address thesechallenges we propose MobA a novel Mobile phone Agent powered by multimodallarge language models that enhances comprehension and planning capabilitiesthrough a sophisticated two-level agent architecture. The high-level GlobalAgent GA is responsible for understanding user commands tracking historymemories and planning tasks. The low-level Local Agent LA predicts detailedactions in the form of function calls guided by sub-tasks and memory from theGA. Integrating a Reflection Module allows for efficient task completion andenables the system to handle previously unseen complex tasks. MobA demonstratessignificant improvements in task execution efficiency and completion rate inreal-life evaluations underscoring the potential of MLLM-empowered mobileassistants.</p>
                <p>Last Updated: 2024-10-17 16:53:50 UTC</p>
                <button class="interpret-button" data-id="2410.13757v1">Interpret</button>
                <div id="interpretation-2410.13757v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Scaling Wearable Foundation Models</h3>
                <p>Authors: Girish NarayanswamyXin LiuKumar AyushYuzhe YangXuhai XuShun LiaoJake GarrisonShyam TailorJake SunshineYun LiuTim AlthoffShrikanth NarayananPushmeet KohliJiening ZhanMark MalhotraShwetak PatelSamy Abdel-GhaffarDaniel McDuff</p>
                <p><a href="http://arxiv.org/abs/2410.13638v1">Link to paper</a></p>
                <p>Wearable sensors have become ubiquitous thanks to a variety of healthtracking features. The resulting continuous and longitudinal measurements fromeveryday life generate large volumes of data however making sense of theseobservations for scientific and actionable insights is non-trivial. Inspired bythe empirical success of generative modeling where large neural networks learnpowerful representations from vast amounts of text image video or audiodata we investigate the scaling properties of sensor foundation models acrosscompute data and model size. Using a dataset of up to 40 million hours ofin-situ heart rate heart rate variability electrodermal activityaccelerometer skin temperature and altimeter per-minute data from over165000 people we create LSM a multimodal foundation model built on thelargest wearable-signals dataset with the most extensive range of sensormodalities to date. Our results establish the scaling laws of LSM for taskssuch as imputation interpolation and extrapolation both across time andsensor modalities. Moreover we highlight how LSM enables sample-efficientdownstream learning for tasks like exercise and activity recognition.</p>
                <p>Last Updated: 2024-10-17 15:08:21 UTC</p>
                <button class="interpret-button" data-id="2410.13638v1">Interpret</button>
                <div id="interpretation-2410.13638v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Neural Correlates of Augmented Reality Safety Warnings: EEG Analysis of Situational Awareness and Cognitive Performance in Roadway Work Zones</h3>
                <p>Authors: Fatemeh Banani ArdecaniAmit KumarSepehr SabetiOmidreza Shoghli</p>
                <p><a href="http://arxiv.org/abs/2410.13623v1">Link to paper</a></p>
                <p>Despite the research and implementation efforts involving various safetystrategies protocols and technologies work zone crashes and fatalitiescontinue to occur at an alarming rate each year. This study investigates theneurophysiological responses to Augmented Reality safety warnings in roadwaywork zones under varying workload conditions. Using electroencephalogram EEGtechnology we objectively assessed situational awareness attention andcognitive load in simulated low-intensity LA and moderate-intensity MA workactivities. The research analyzed key EEG indicators including beta gammaalpha and theta waves as well as various combined wave ratios. Resultsrevealed that AR warnings effectively triggered neurological responsesassociated with increased situational awareness and attention across bothworkload conditions. However significant differences were observed in thetiming and intensity of these responses. In the LA condition peak responsesoccurred earlier within 125 ms post-warning and were more pronouncedsuggesting a more robust cognitive response when physical demands were lower.Conversely the MA condition showed delayed peak responses 125-250 mspost-warning and more gradual changes indicating a potential impact ofincreased physical activity on cognitive processing speed. These findingsunderscore the importance of considering physical workload when designingAR-based safety systems for roadway work zones. The research contributes to theunderstanding of how AR can enhance worker safety and provides insights fordeveloping more effective context-aware safety interventions in high-risk workenvironments.</p>
                <p>Last Updated: 2024-10-17 14:56:44 UTC</p>
                <button class="interpret-button" data-id="2410.13623v1">Interpret</button>
                <div id="interpretation-2410.13623v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Co-creation and evaluation of an app to support reminiscence therapy interventions for older people with dementia</h3>
                <p>Authors: Iván De-Rosende-CeleiroVirginia Francisco-GilmartínSusana Bautista-BlascoAdriana Ávila-Álvarez</p>
                <p><a href="http://dx.doi.org/10.1177/20552076241261849">Link to paper</a></p>
                <p>Objective: The objectives encompassed 1 the creation of Recuerdame adigital app specifically designed for occupational therapists aiming tosupport these professionals in the processes of planning organizingdeveloping and documenting reminiscence therapies for older people withdementia and 2 the evaluation of the designed prototype through aparticipatory and user-centered design approach exploring the perceptions ofend-users. Methods: This exploratory research used a mixed-methods design. Theapp was developed in two phases. In the first phase the research teamidentified the requirements and designed a prototype. In the second phaseexperienced occupational therapists evaluated the prototype. Results: Theresearch team determined the apps required functionalities grouped into eightmajor themes: register related persons and caregivers record the patientslife story memories prepare a reminiscence therapy session conduct a sessionend a session assess the patient automatically generate a life story otherrequirements. The first phase ended with the development of a prototype. In thesecond phase eight occupational therapists performed a series of tasks usingall the applications functionalities. Most of these tasks were very easySingle Ease Question. The level of usability was considered excellent SystemUsability Scale. Participants believed that the app would save practitionerstime enrich therapy sessions and improve their effectiveness. The qualitativeresults were summarized in two broad themes: a acceptability of the app andb areas for improvement.ConclusionsParticipating occupational therapistsgenerally agreed that the co-designed app appears to be a versatile tool thatempowers these professionals to manage reminiscence interventions.</p>
                <p>Last Updated: 2024-10-17 13:55:32 UTC</p>
                <button class="interpret-button" data-id="2410.13556v1">Interpret</button>
                <div id="interpretation-2410.13556v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RAMPA: Robotic Augmented Reality for Machine Programming and Automation</h3>
                <p>Authors: Fatih DogangunSerdar BaharYigit YildirimBora Toprak TemirEmre UgurMustafa Doga Dogan</p>
                <p><a href="http://arxiv.org/abs/2410.13412v1">Link to paper</a></p>
                <p>As robotics continue to enter various sectors beyond traditional industrialapplications the need for intuitive robot training and interaction systemsbecomes increasingly more important. This paper introduces Robotic AugmentedReality for Machine Programming RAMPA a system that utilizes thecapabilities of state-of-the-art and commercially available AR headsets e.g.Meta Quest 3 to facilitate the application of Programming from DemonstrationPfD approaches on industrial robotic arms such as Universal Robots UR10. Ourapproach enables in-situ data recording visualization and fine-tuning ofskill demonstrations directly within the users physical environment. RAMPAaddresses critical challenges of PfD such as safety concerns programmingbarriers and the inefficiency of collecting demonstrations on the actualhardware. The performance of our system is evaluated against the traditionalmethod of kinesthetic control in teaching three different robotic manipulationtasks and analyzed with quantitative metrics measuring task performance andcompletion time trajectory smoothness system usability user experience andtask load using standardized surveys. Our findings indicate a substantialadvancement in how robotic tasks are taught and refined promising improvementsin operational safety efficiency and user engagement in robotic programming.</p>
                <p>Last Updated: 2024-10-17 10:21:28 UTC</p>
                <button class="interpret-button" data-id="2410.13412v1">Interpret</button>
                <div id="interpretation-2410.13412v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Transformer Guided Coevolution: Improved Team Formation in Multiagent Adversarial Games</h3>
                <p>Authors: Pranav RajbhandariPrithviraj DasguptaDonald Sofge</p>
                <p><a href="http://arxiv.org/abs/2410.13769v1">Link to paper</a></p>
                <p>We consider the problem of team formation within multiagent adversarialgames. We propose BERTeam a novel algorithm that uses a transformer-based deepneural network with Masked Language Model training to select the best team ofplayers from a trained population. We integrate this with coevolutionary deepreinforcement learning which trains a diverse set of individual players tochoose teams from. We test our algorithm in the multiagent adversarial gameMarine Capture-The-Flag and we find that BERTeam learns non-trivial teamcompositions that perform well against unseen opponents. For this game we findthat BERTeam outperforms MCAA an algorithm that similarly optimizes teamformation.</p>
                <p>Last Updated: 2024-10-17 17:06:41 UTC</p>
                <button class="interpret-button" data-id="2410.13769v1">Interpret</button>
                <div id="interpretation-2410.13769v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Rapid and Automated Alloy Design with Graph Neural Network-Powered LLM-Driven Multi-Agent Systems</h3>
                <p>Authors: Alireza GhafarollahiMarkus J. Buehler</p>
                <p><a href="http://arxiv.org/abs/2410.13768v1">Link to paper</a></p>
                <p>A multi-agent AI model is used to automate the discovery of new metallicalloys integrating multimodal data and external knowledge including insightsfrom physics via atomistic simulations. Our multi-agent system features threekey components: a a suite of LLMs responsible for tasks such as reasoning andplanning b a group of AI agents with distinct roles and expertise thatdynamically collaborate and c a newly developed graph neural network GNNmodel for rapid retrieval of key physical properties. A set of LLM-driven AIagents collaborate to automate the exploration of the vast design space ofMPEAs guided by predictions from the GNN. We focus on the NbMoTa family ofbody-centered cubic bcc alloys modeled using an ML-based interatomicpotential and target two key properties: the Peierls barrier and solute/screwdislocation interaction energy. Our GNN model accurately predicts theseatomic-scale properties providing a faster alternative to costly brute-forcecalculations and reducing the computational burden on multi-agent systems forphysics retrieval. This AI system revolutionizes materials discovery byreducing reliance on human expertise and overcoming the limitations of directall-atom simulations. By synergizing the predictive power of GNNs with thedynamic collaboration of LLM-based agents the system autonomously navigatesvast alloy design spaces identifying trends in atomic-scale materialproperties and predicting macro-scale mechanical strength as demonstrated byseveral computational experiments. This approach accelerates the discovery ofadvanced alloys and holds promise for broader applications in other complexsystems marking a significant step forward in automated materials design.</p>
                <p>Last Updated: 2024-10-17 17:06:26 UTC</p>
                <button class="interpret-button" data-id="2410.13768v1">Interpret</button>
                <div id="interpretation-2410.13768v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MobA: A Two-Level Agent System for Efficient Mobile Task Automation</h3>
                <p>Authors: Zichen ZhuHao TangYansi LiKunyao LanYixuan JiangHao ZhouYixiao WangSituo ZhangLiangtai SunLu ChenKai Yu</p>
                <p><a href="http://arxiv.org/abs/2410.13757v1">Link to paper</a></p>
                <p>Current mobile assistants are limited by dependence on system APIs orstruggle with complex user instructions and diverse interfaces due torestricted comprehension and decision-making abilities. To address thesechallenges we propose MobA a novel Mobile phone Agent powered by multimodallarge language models that enhances comprehension and planning capabilitiesthrough a sophisticated two-level agent architecture. The high-level GlobalAgent GA is responsible for understanding user commands tracking historymemories and planning tasks. The low-level Local Agent LA predicts detailedactions in the form of function calls guided by sub-tasks and memory from theGA. Integrating a Reflection Module allows for efficient task completion andenables the system to handle previously unseen complex tasks. MobA demonstratessignificant improvements in task execution efficiency and completion rate inreal-life evaluations underscoring the potential of MLLM-empowered mobileassistants.</p>
                <p>Last Updated: 2024-10-17 16:53:50 UTC</p>
                <button class="interpret-button" data-id="2410.13757v1">Interpret</button>
                <div id="interpretation-2410.13757v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>EFX Exists for Three Types of Agents</h3>
                <p>Authors: Vishwa Prakash H. V.Pratik GhosalPrajakta NimbhorkarNithin Varma</p>
                <p><a href="http://arxiv.org/abs/2410.13580v1">Link to paper</a></p>
                <p>In this paper we study the problem of finding an envy-free allocation ofindivisible goods among multiple agents. EFX which stands for envy-freeness upto any good is a well-studied relaxation of the envy-free allocation problemand has been shown to exist for specific scenarios. For instance EFX is knownto exist when there are only three agents Chaudhury et al EC 2020 and forany number of agents when there are only two types of valuations MaharaDiscret. Appl. Math 2023.  We show that EFX allocations exist for any number of agents when there are atmost three types of additive valuations.</p>
                <p>Last Updated: 2024-10-17 14:15:08 UTC</p>
                <button class="interpret-button" data-id="2410.13580v1">Interpret</button>
                <div id="interpretation-2410.13580v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Byzantine-Resilient Output Optimization of Multiagent via Self-Triggered Hybrid Detection Approach</h3>
                <p>Authors: Chenhang YanLiping YanYuezu LvBolei DongYuanqing Xia</p>
                <p><a href="http://arxiv.org/abs/2410.13454v1">Link to paper</a></p>
                <p>How to achieve precise distributed optimization despite unknown attacksespecially the Byzantine attacks is one of the critical challenges formultiagent systems. This paper addresses a distributed resilient optimizationfor linear heterogeneous multi-agent systems faced with adversarial threats. Weestablish a framework aimed at realizing resilient optimization forcontinuous-time systems by incorporating a novel self-triggered hybriddetection approach. The proposed hybrid detection approach is able to identifyattacks on neighbors using both error thresholds and triggering intervalsthereby optimizing the balance between effective attack detection and thereduction of excessive communication triggers. Through using an edge-basedadaptive self-triggered approach each agent can receive its neighborsinformation and determine whether these information is valid. If any neighborprove invalid each normal agent will isolate that neighbor by disconnectingcommunication along that specific edge. Importantly our adaptive algorithmguarantees the accuracy of the optimization solution even when an agent isisolated by its neighbors.</p>
                <p>Last Updated: 2024-10-17 11:31:22 UTC</p>
                <button class="interpret-button" data-id="2410.13454v1">Interpret</button>
                <div id="interpretation-2410.13454v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs</h3>
                <p>Authors: Guhao FengKai YangYuntian GuXinyue AiShengjie LuoJiacheng SunDi HeZhenguo LiLiwei Wang</p>
                <p><a href="http://arxiv.org/abs/2410.13857v1">Link to paper</a></p>
                <p>Despite the remarkable success of Transformer-based Large Language ModelsLLMs across various domains understanding and enhancing their mathematicalcapabilities remains a significant challenge. In this paper we conduct arigorous theoretical analysis of LLMs mathematical abilities with a specificfocus on their arithmetic performances. We identify numerical precision as akey factor that influences their effectiveness in mathematical tasks. Ourresults show that Transformers operating with low numerical precision fail toaddress arithmetic tasks such as iterated addition and integer multiplicationunless the model size grows super-polynomially with respect to the inputlength. In contrast Transformers with standard numerical precision canefficiently handle these tasks with significantly smaller model sizes. Wefurther support our theoretical findings through empirical experiments thatexplore the impact of varying numerical precision on arithmetic tasksproviding valuable insights for improving the mathematical reasoningcapabilities of LLMs.</p>
                <p>Last Updated: 2024-10-17 17:59:35 UTC</p>
                <button class="interpret-button" data-id="2410.13857v1">Interpret</button>
                <div id="interpretation-2410.13857v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>From Gradient Clipping to Normalization for Heavy Tailed SGD</h3>
                <p>Authors: Florian HüblerIlyas FatkhullinNiao He</p>
                <p><a href="http://arxiv.org/abs/2410.13849v1">Link to paper</a></p>
                <p>Recent empirical evidence indicates that many machine learning applicationsinvolve heavy-tailed gradient noise which challenges the standard assumptionsof bounded variance in stochastic optimization. Gradient clipping has emergedas a popular tool to handle this heavy-tailed noise as it achieves goodperformance in this setting both theoretically and practically. However ourcurrent theoretical understanding of non-convex gradient clipping has threemain shortcomings. First the theory hinges on large increasing clippingthresholds which are in stark contrast to the small constant clippingthresholds employed in practice. Second clipping thresholds require knowledgeof problem-dependent parameters to guarantee convergence. Lastly even withthis knowledge current sampling complexity upper bounds for the method aresub-optimal in nearly all parameters. To address these issues we studyconvergence of Normalized SGD NSGD. First we establish a parameter-freesample complexity for NSGD ofmathcalOleftvarepsilon-frac2pp-1right to find anvarepsilon-stationary point. Furthermore we prove tightness of this resultby providing a matching algorithm-specific lower bound. In the setting whereall problem parameters are known we show this complexity is improved tomathcalOleftvarepsilon-frac3p-2p-1right matching thepreviously known lower bound for all first-order methods in all problemdependent parameters. Finally we establish high-probability convergence ofNSGD with a mild logarithmic dependence on the failure probability. Our workcomplements the studies of gradient clipping under heavy tailed noise improvingthe sample complexities of existing algorithms and offering an alternativemechanism to achieve high probability convergence.</p>
                <p>Last Updated: 2024-10-17 17:59:01 UTC</p>
                <button class="interpret-button" data-id="2410.13849v1">Interpret</button>
                <div id="interpretation-2410.13849v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Artificial Kuramoto Oscillatory Neurons</h3>
                <p>Authors: Takeru MiyatoSindy LöweAndreas GeigerMax Welling</p>
                <p><a href="http://arxiv.org/abs/2410.13821v1">Link to paper</a></p>
                <p>It has long been known in both neuroscience and AI that binding betweenneurons leads to a form of competitive learning where representations arecompressed in order to represent more abstract concepts in deeper layers of thenetwork. More recently it was also hypothesized that dynamic spatiotemporalrepresentations play an important role in both neuroscience and AI. Building onthese ideas we introduce Artificial Kuramoto Oscillatory Neurons AKOrN as adynamical alternative to threshold units which can be combined with arbitraryconnectivity designs such as fully connected convolutional or attentivemechanisms. Our generalized Kuramoto updates bind neurons together throughtheir synchronization dynamics. We show that this idea provides performanceimprovements across a wide spectrum of tasks such as unsupervised objectdiscovery adversarial robustness calibrated uncertainty quantification andreasoning. We believe that these empirical results show the importance ofrethinking our assumptions at the most basic neuronal level of neuralrepresentation and in particular show the importance of dynamicalrepresentations.</p>
                <p>Last Updated: 2024-10-17 17:47:54 UTC</p>
                <button class="interpret-button" data-id="2410.13821v1">Interpret</button>
                <div id="interpretation-2410.13821v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Discrete distributions are learnable from metastable samples</h3>
                <p>Authors: Abhijith JayakumarAndrey Y. LokhovSidhant MisraMarc Vuffray</p>
                <p><a href="http://arxiv.org/abs/2410.13800v1">Link to paper</a></p>
                <p>Markov chain samplers designed to sample from multi-variable distributionsoften undesirably get stuck in specific regions of their state space. Thiscauses such samplers to approximately sample from a metastable distributionwhich is usually quite different from the desired stationary distribution ofthe chain. We show that single-variable conditionals of metastabledistributions of reversible Markov chain samplers that satisfy a strongmetastability condition are on average very close to those of the truedistribution. This holds even when the metastable distribution is far away fromthe true model in terms of global metrics like Kullback-Leibler divergence ortotal variation distance. This property allows us to learn the true model usinga conditional likelihood based estimator even when the samples come from ametastable distribution concentrated in a small region of the state space.Explicit examples of such metastable states can be constructed from regionsthat effectively bottleneck the probability flow and cause poor mixing of theMarkov chain. For specific cases of binary pairwise undirected graphicalmodels we extend our results to further rigorously show that data coming frommetastable states can be used to learn the parameters of the energy functionand recover the structure of the model.</p>
                <p>Last Updated: 2024-10-17 17:38:44 UTC</p>
                <button class="interpret-button" data-id="2410.13800v1">Interpret</button>
                <div id="interpretation-2410.13800v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree</h3>
                <p>Authors: Michelangelo Olmo Nogara NotarianniFilippo LeveniDiego StucchiLuca FrittoliGiacomo Boracchi</p>
                <p><a href="http://arxiv.org/abs/2410.13778v1">Link to paper</a></p>
                <p>We present Kernel-QuantTree Exponentially Weighted Moving Average KQT-EWMAa non-parametric change-detection algorithm that combines the Kernel-QuantTreeKQT histogram and the EWMA statistic to monitor multivariate data streamsonline. The resulting monitoring scheme is very flexible since histograms canbe used to model any stationary distribution and practical since thedistribution of test statistics does not depend on the distribution ofdatastream in stationary conditions non-parametric monitoring. KQT-EWMAenables controlling false alarms by operating at a pre-determined Average RunLength ARL_0 which measures the expected number of stationary samples tobe monitored before triggering a false alarm. The latter peculiarity is incontrast with most non-parametric change-detection tests which rarely cancontrol the ARL_0 a priori. Our experiments on synthetic and real-worlddatasets demonstrate that KQT-EWMA can control ARL_0 while achievingdetection delays comparable to or lower than state-of-the-art methods designedto work in the same conditions.</p>
                <p>Last Updated: 2024-10-17 17:17:38 UTC</p>
                <button class="interpret-button" data-id="2410.13778v1">Interpret</button>
                <div id="interpretation-2410.13778v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens</h3>
                <p>Authors: Lijie FanTianhong LiSiyang QinYuanzhen LiChen SunMichael RubinsteinDeqing SunKaiming HeYonglong Tian</p>
                <p><a href="http://arxiv.org/abs/2410.13863v1">Link to paper</a></p>
                <p>Scaling up autoregressive models in vision has not proven as beneficial as inlarge language models. In this work we investigate this scaling problem in thecontext of text-to-image generation focusing on two critical factors: whethermodels use discrete or continuous tokens and whether tokens are generated in arandom or fixed raster order using BERT- or GPT-like transformer architectures.Our empirical results show that while all models scale effectively in terms ofvalidation loss their evaluation performance -- measured by FID GenEvalscore and visual quality -- follows different trends. Models based oncontinuous tokens achieve significantly better visual quality than those usingdiscrete tokens. Furthermore the generation order and attention mechanismssignificantly affect the GenEval score: random-order models achieve notablybetter GenEval scores compared to raster-order models. Inspired by thesefindings we train Fluid a random-order autoregressive model on continuoustokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16on MS-COCO 30K and 0.69 overall score on the GenEval benchmark. We hope ourfindings and results will encourage future efforts to further bridge thescaling gap between vision and language models.</p>
                <p>Last Updated: 2024-10-17 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2410.13863v1">Interpret</button>
                <div id="interpretation-2410.13863v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>UniDrive: Towards Universal Driving Perception Across Camera Configurations</h3>
                <p>Authors: Ye LiWenzhao ZhengXiaonan HuangKurt Keutzer</p>
                <p><a href="http://arxiv.org/abs/2410.13864v1">Link to paper</a></p>
                <p>Vision-centric autonomous driving has demonstrated excellent performance witheconomical sensors. As the fundamental step 3D perception aims to infer 3Dinformation from 2D images based on 3D-2D projection. This makes drivingperception models susceptible to sensor configuration e.g. camera intrinsicsand extrinsics variations. However generalizing across camera configurationsis important for deploying autonomous driving models on different car models.In this paper we present UniDrive a novel framework for vision-centricautonomous driving to achieve universal perception across cameraconfigurations. We deploy a set of unified virtual cameras and propose aground-aware projection method to effectively transform the original imagesinto these unified virtual views. We further propose a virtual configurationoptimization method by minimizing the expected projection error betweenoriginal cameras and virtual cameras. The proposed virtual camera projectioncan be applied to existing 3D perception methods as a plug-and-play module tomitigate the challenges posed by camera parameter variability resulting inmore adaptable and reliable driving perception models. To evaluate theeffectiveness of our framework we collect a dataset on Carla by driving thesame routes while only modifying the camera configurations. Experimentalresults demonstrate that our method trained on one specific cameraconfiguration can generalize to varying configurations with minor performancedegradation.</p>
                <p>Last Updated: 2024-10-17 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2410.13864v1">Interpret</button>
                <div id="interpretation-2410.13864v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DepthSplat: Connecting Gaussian Splatting and Depth</h3>
                <p>Authors: Haofei XuSongyou PengFangjinhua WangHermann BlumDaniel BarathAndreas GeigerMarc Pollefeys</p>
                <p><a href="http://arxiv.org/abs/2410.13862v1">Link to paper</a></p>
                <p>Gaussian splatting and single/multi-view depth estimation are typicallystudied in isolation. In this paper we present DepthSplat to connect Gaussiansplatting and depth estimation and study their interactions. More specificallywe first contribute a robust multi-view depth model by leveraging pre-trainedmonocular depth features leading to high-quality feed-forward 3D Gaussiansplatting reconstructions. We also show that Gaussian splatting can serve as anunsupervised pre-training objective for learning powerful depth models fromlarge-scale unlabelled datasets. We validate the synergy between Gaussiansplatting and depth estimation through extensive ablation and cross-tasktransfer experiments. Our DepthSplat achieves state-of-the-art performance onScanNet RealEstate10K and DL3DV datasets in terms of both depth estimation andnovel view synthesis demonstrating the mutual benefits of connecting bothtasks. Our code models and video results are available athttps://haofeixu.github.io/depthsplat/.</p>
                <p>Last Updated: 2024-10-17 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2410.13862v1">Interpret</button>
                <div id="interpretation-2410.13862v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</h3>
                <p>Authors: Rongyao FangChengqi DuanKun WangHao LiHao TianXingyu ZengRui ZhaoJifeng DaiHongsheng LiXihui Liu</p>
                <p><a href="http://arxiv.org/abs/2410.13861v1">Link to paper</a></p>
                <p>Recent advancements in multimodal foundation models have yielded significantprogress in vision-language understanding. Initial attempts have also exploredthe potential of multimodal large language models MLLMs for visual contentgeneration. However existing works have insufficiently addressed the varyinggranularity demands of different image generation tasks within a unified MLLMparadigm - from the diversity required in text-to-image generation to theprecise controllability needed in image manipulation. In this work we proposePUMA emPowering Unified MLLM with Multi-grAnular visual generation. PUMAunifies multi-granular visual features as both inputs and outputs of MLLMselegantly addressing the different granularity requirements of various imagegeneration tasks within a unified MLLM framework. Following multimodalpretraining and task-specific instruction tuning PUMA demonstrates proficiencyin a wide range of multimodal tasks. This work represents a significant steptowards a truly unified MLLM capable of adapting to the granularity demands ofvarious visual tasks. The code and model will be released inhttps://github.com/rongyaofang/PUMA.</p>
                <p>Last Updated: 2024-10-17 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2410.13861v1">Interpret</button>
                <div id="interpretation-2410.13861v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding</h3>
                <p>Authors: Runsen XuZhiwei HuangTai WangYilun ChenJiangmiao PangDahua Lin</p>
                <p><a href="http://arxiv.org/abs/2410.13860v1">Link to paper</a></p>
                <p>3D visual grounding is crucial for robots requiring integration of naturallanguage and 3D scene understanding. Traditional methods depending onsupervised learning with 3D point clouds are limited by scarce datasets.Recently zero-shot methods leveraging LLMs have been proposed to address thedata issue. While effective these methods only use object-centric informationlimiting their ability to handle complex queries. In this work we presentVLM-Grounder a novel framework using vision-language models VLMs forzero-shot 3D visual grounding based solely on 2D images. VLM-Grounderdynamically stitches image sequences employs a grounding and feedback schemeto find the target object and uses a multi-view ensemble projection toaccurately estimate 3D bounding boxes. Experiments on ScanRefer and Nr3Ddatasets show VLM-Grounder outperforms previous zero-shot methods achieving51.6 Acc0.25 on ScanRefer and 48.0 Acc on Nr3D without relying on 3Dgeometry or object priors. Codes are available athttps://github.com/OpenRobotLab/VLM-Grounder .</p>
                <p>Last Updated: 2024-10-17 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2410.13860v1">Interpret</button>
                <div id="interpretation-2410.13860v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs</h3>
                <p>Authors: Guhao FengKai YangYuntian GuXinyue AiShengjie LuoJiacheng SunDi HeZhenguo LiLiwei Wang</p>
                <p><a href="http://arxiv.org/abs/2410.13857v1">Link to paper</a></p>
                <p>Despite the remarkable success of Transformer-based Large Language ModelsLLMs across various domains understanding and enhancing their mathematicalcapabilities remains a significant challenge. In this paper we conduct arigorous theoretical analysis of LLMs mathematical abilities with a specificfocus on their arithmetic performances. We identify numerical precision as akey factor that influences their effectiveness in mathematical tasks. Ourresults show that Transformers operating with low numerical precision fail toaddress arithmetic tasks such as iterated addition and integer multiplicationunless the model size grows super-polynomially with respect to the inputlength. In contrast Transformers with standard numerical precision canefficiently handle these tasks with significantly smaller model sizes. Wefurther support our theoretical findings through empirical experiments thatexplore the impact of varying numerical precision on arithmetic tasksproviding valuable insights for improving the mathematical reasoningcapabilities of LLMs.</p>
                <p>Last Updated: 2024-10-17 17:59:35 UTC</p>
                <button class="interpret-button" data-id="2410.13857v1">Interpret</button>
                <div id="interpretation-2410.13857v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Can MLLMs Understand the Deep Implication Behind Chinese Images?</h3>
                <p>Authors: Chenhao ZhangXi FengYuelin BaiXinrun DuJinchang HouKaixin DengGuangzeng HanQinrui LiBingli WangJiaheng LiuXingwei QuYifei ZhangQixuan ZhaoYiming LiangZiqiang LiuFeiteng FangMin YangWenhao HuangChenghua LinGe ZhangShiwen Ni</p>
                <p><a href="http://arxiv.org/abs/2410.13854v1">Link to paper</a></p>
                <p>As the capabilities of Multimodal Large Language Models MLLMs continue toimprove the need for higher-order capability evaluation of MLLMs isincreasing. However there is a lack of work evaluating MLLM for higher-orderperception and understanding of Chinese visual content. To fill the gap weintroduce the Chinese Image Implication understandingBenchmark CII-Bench which aims to assess the higher-order perceptionand understanding capabilities of MLLMs for Chinese images. CII-Bench standsout in several ways compared to existing benchmarks. Firstly to ensure theauthenticity of the Chinese context images in CII-Bench are sourced from theChinese Internet and manually reviewed with corresponding answers alsomanually crafted. Additionally CII-Bench incorporates images that representChinese traditional culture such as famous Chinese traditional paintingswhich can deeply reflect the models understanding of Chinese traditionalculture. Through extensive experiments on CII-Bench across multiple MLLMs wehave made significant findings. Initially a substantial gap is observedbetween the performance of MLLMs and humans on CII-Bench. The highest accuracyof MLLMs attains 64.4 where as human accuracy averages 78.2 peaking at animpressive 81.0. Subsequently MLLMs perform worse on Chinese traditionalculture images suggesting limitations in their ability to understandhigh-level semantics and lack a deep knowledge base of Chinese traditionalculture. Finally it is observed that most models exhibit enhanced accuracywhen image emotion hints are incorporated into the prompts. We believe thatCII-Bench will enable MLLMs to gain a better understanding of Chinese semanticsand Chinese-specific images advancing the journey towards expert artificialgeneral intelligence AGI. Our project is publicly available athttps://cii-bench.github.io/.</p>
                <p>Last Updated: 2024-10-17 17:59:24 UTC</p>
                <button class="interpret-button" data-id="2410.13854v1">Interpret</button>
                <div id="interpretation-2410.13854v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Retrospective Learning from Interactions</h3>
                <p>Authors: Zizhao ChenMustafa Omer GulYiwei ChenGloria GengAnne WuYoav Artzi</p>
                <p><a href="http://arxiv.org/abs/2410.13852v1">Link to paper</a></p>
                <p>Multi-turn interactions between large language models LLMs and usersnaturally include implicit feedback signals. If an LLM responds in anunexpected way to an instruction the user is likely to signal it by rephrasingthe request expressing frustration or pivoting to an alternative task. Suchsignals are task-independent and occupy a relatively constrained subspace oflanguage allowing the LLM to identify them even if it fails on the actualtask. This creates an avenue for continually learning from interactions withoutadditional annotations. We introduce ReSpect a method to learn from suchsignals in past interactions via retrospection. We deploy ReSpect in a newmultimodal interaction scenario where humans instruct an LLM to solve anabstract reasoning task with a combinatorial solution space. Through thousandsof interactions with humans we show how ReSpect gradually improves taskcompletion rate from 31 to 82 all without any external annotation.</p>
                <p>Last Updated: 2024-10-17 17:59:03 UTC</p>
                <button class="interpret-button" data-id="2410.13852v1">Interpret</button>
                <div id="interpretation-2410.13852v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Influence Functions for Scalable Data Attribution in Diffusion Models</h3>
                <p>Authors: Bruno MlodozeniecRuna EschenhagenJuhan BaeAlexander ImmerDavid KruegerRichard Turner</p>
                <p><a href="http://arxiv.org/abs/2410.13850v1">Link to paper</a></p>
                <p>Diffusion models have led to significant advancements in generativemodelling. Yet their widespread adoption poses challenges regarding dataattribution and interpretability. In this paper we aim to help address suchchallenges in diffusion models by developing an textitinfluence functionsframework. Influence function-based data attribution methods approximate how amodels output would have changed if some training data were removed. Insupervised learning this is usually used for predicting how the loss on aparticular example would change. For diffusion models we focus on predictingthe change in the probability of generating a particular example via severalproxy measurements. We show how to formulate influence functions for suchquantities and how previously proposed methods can be interpreted as particulardesign choices in our framework. To ensure scalability of the Hessiancomputations in influence functions we systematically develop K-FACapproximations based on generalised Gauss-Newton matrices specifically tailoredto diffusion models. We recast previously proposed methods as specific designchoices in our framework and show that our recommended method outperformsprevious data attribution approaches on common evaluations such as the LinearData-modelling Score LDS or retraining without top influences without theneed for method-specific hyperparameter tuning.</p>
                <p>Last Updated: 2024-10-17 17:59:02 UTC</p>
                <button class="interpret-button" data-id="2410.13850v1">Interpret</button>
                <div id="interpretation-2410.13850v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</h3>
                <p>Authors: Chengyue WuXiaokang ChenZhiyu WuYiyang MaXingchao LiuZizheng PanWen LiuZhenda XieXingkai YuChong RuanPing Luo</p>
                <p><a href="http://arxiv.org/abs/2410.13848v1">Link to paper</a></p>
                <p>In this paper we introduce Janus an autoregressive framework that unifiesmultimodal understanding and generation. Prior research often relies on asingle visual encoder for both tasks such as Chameleon. However due to thediffering levels of information granularity required by multimodalunderstanding and generation this approach can lead to suboptimal performanceparticularly in multimodal understanding. To address this issue we decouplevisual encoding into separate pathways while still leveraging a singleunified transformer architecture for processing. The decoupling not onlyalleviates the conflict between the visual encoders roles in understanding andgeneration but also enhances the frameworks flexibility. For instance boththe multimodal understanding and generation components can independently selecttheir most suitable encoding methods. Experiments show that Janus surpassesprevious unified model and matches or exceeds the performance of task-specificmodels. The simplicity high flexibility and effectiveness of Janus make it astrong candidate for next-generation unified multimodal models.</p>
                <p>Last Updated: 2024-10-17 17:58:37 UTC</p>
                <button class="interpret-button" data-id="2410.13848v1">Interpret</button>
                <div id="interpretation-2410.13848v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs</h3>
                <p>Authors: Guhao FengKai YangYuntian GuXinyue AiShengjie LuoJiacheng SunDi HeZhenguo LiLiwei Wang</p>
                <p><a href="http://arxiv.org/abs/2410.13857v1">Link to paper</a></p>
                <p>Despite the remarkable success of Transformer-based Large Language ModelsLLMs across various domains understanding and enhancing their mathematicalcapabilities remains a significant challenge. In this paper we conduct arigorous theoretical analysis of LLMs mathematical abilities with a specificfocus on their arithmetic performances. We identify numerical precision as akey factor that influences their effectiveness in mathematical tasks. Ourresults show that Transformers operating with low numerical precision fail toaddress arithmetic tasks such as iterated addition and integer multiplicationunless the model size grows super-polynomially with respect to the inputlength. In contrast Transformers with standard numerical precision canefficiently handle these tasks with significantly smaller model sizes. Wefurther support our theoretical findings through empirical experiments thatexplore the impact of varying numerical precision on arithmetic tasksproviding valuable insights for improving the mathematical reasoningcapabilities of LLMs.</p>
                <p>Last Updated: 2024-10-17 17:59:35 UTC</p>
                <button class="interpret-button" data-id="2410.13857v1">Interpret</button>
                <div id="interpretation-2410.13857v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Can MLLMs Understand the Deep Implication Behind Chinese Images?</h3>
                <p>Authors: Chenhao ZhangXi FengYuelin BaiXinrun DuJinchang HouKaixin DengGuangzeng HanQinrui LiBingli WangJiaheng LiuXingwei QuYifei ZhangQixuan ZhaoYiming LiangZiqiang LiuFeiteng FangMin YangWenhao HuangChenghua LinGe ZhangShiwen Ni</p>
                <p><a href="http://arxiv.org/abs/2410.13854v1">Link to paper</a></p>
                <p>As the capabilities of Multimodal Large Language Models MLLMs continue toimprove the need for higher-order capability evaluation of MLLMs isincreasing. However there is a lack of work evaluating MLLM for higher-orderperception and understanding of Chinese visual content. To fill the gap weintroduce the Chinese Image Implication understandingBenchmark CII-Bench which aims to assess the higher-order perceptionand understanding capabilities of MLLMs for Chinese images. CII-Bench standsout in several ways compared to existing benchmarks. Firstly to ensure theauthenticity of the Chinese context images in CII-Bench are sourced from theChinese Internet and manually reviewed with corresponding answers alsomanually crafted. Additionally CII-Bench incorporates images that representChinese traditional culture such as famous Chinese traditional paintingswhich can deeply reflect the models understanding of Chinese traditionalculture. Through extensive experiments on CII-Bench across multiple MLLMs wehave made significant findings. Initially a substantial gap is observedbetween the performance of MLLMs and humans on CII-Bench. The highest accuracyof MLLMs attains 64.4 where as human accuracy averages 78.2 peaking at animpressive 81.0. Subsequently MLLMs perform worse on Chinese traditionalculture images suggesting limitations in their ability to understandhigh-level semantics and lack a deep knowledge base of Chinese traditionalculture. Finally it is observed that most models exhibit enhanced accuracywhen image emotion hints are incorporated into the prompts. We believe thatCII-Bench will enable MLLMs to gain a better understanding of Chinese semanticsand Chinese-specific images advancing the journey towards expert artificialgeneral intelligence AGI. Our project is publicly available athttps://cii-bench.github.io/.</p>
                <p>Last Updated: 2024-10-17 17:59:24 UTC</p>
                <button class="interpret-button" data-id="2410.13854v1">Interpret</button>
                <div id="interpretation-2410.13854v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Retrospective Learning from Interactions</h3>
                <p>Authors: Zizhao ChenMustafa Omer GulYiwei ChenGloria GengAnne WuYoav Artzi</p>
                <p><a href="http://arxiv.org/abs/2410.13852v1">Link to paper</a></p>
                <p>Multi-turn interactions between large language models LLMs and usersnaturally include implicit feedback signals. If an LLM responds in anunexpected way to an instruction the user is likely to signal it by rephrasingthe request expressing frustration or pivoting to an alternative task. Suchsignals are task-independent and occupy a relatively constrained subspace oflanguage allowing the LLM to identify them even if it fails on the actualtask. This creates an avenue for continually learning from interactions withoutadditional annotations. We introduce ReSpect a method to learn from suchsignals in past interactions via retrospection. We deploy ReSpect in a newmultimodal interaction scenario where humans instruct an LLM to solve anabstract reasoning task with a combinatorial solution space. Through thousandsof interactions with humans we show how ReSpect gradually improves taskcompletion rate from 31 to 82 all without any external annotation.</p>
                <p>Last Updated: 2024-10-17 17:59:03 UTC</p>
                <button class="interpret-button" data-id="2410.13852v1">Interpret</button>
                <div id="interpretation-2410.13852v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</h3>
                <p>Authors: Chengyue WuXiaokang ChenZhiyu WuYiyang MaXingchao LiuZizheng PanWen LiuZhenda XieXingkai YuChong RuanPing Luo</p>
                <p><a href="http://arxiv.org/abs/2410.13848v1">Link to paper</a></p>
                <p>In this paper we introduce Janus an autoregressive framework that unifiesmultimodal understanding and generation. Prior research often relies on asingle visual encoder for both tasks such as Chameleon. However due to thediffering levels of information granularity required by multimodalunderstanding and generation this approach can lead to suboptimal performanceparticularly in multimodal understanding. To address this issue we decouplevisual encoding into separate pathways while still leveraging a singleunified transformer architecture for processing. The decoupling not onlyalleviates the conflict between the visual encoders roles in understanding andgeneration but also enhances the frameworks flexibility. For instance boththe multimodal understanding and generation components can independently selecttheir most suitable encoding methods. Experiments show that Janus surpassesprevious unified model and matches or exceeds the performance of task-specificmodels. The simplicity high flexibility and effectiveness of Janus make it astrong candidate for next-generation unified multimodal models.</p>
                <p>Last Updated: 2024-10-17 17:58:37 UTC</p>
                <button class="interpret-button" data-id="2410.13848v1">Interpret</button>
                <div id="interpretation-2410.13848v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction</h3>
                <p>Authors: Xuan ZhangCunxiao DuChao DuTianyu PangWei GaoMin Lin</p>
                <p><a href="http://arxiv.org/abs/2410.13846v1">Link to paper</a></p>
                <p>Recent advancements in large language models LLMs have extended theircapabilities to handle long contexts. However increasing the number of modellayers and the length of input sequences significantly escalates the memoryrequired to store key-value KV cache posing challenges for efficientinference. To mitigate this issue we present SimLayerKV a simple yeteffective method that reduces inter-layer KV cache redundancies by selectivelydropping cache in identified lazy layers. Our approach is based on theobservation that certain layers in long-context LLMs exhibit lazy behaviorcontributing less to modeling long-range dependencies compared to non-lazylayers. By analyzing attention weight patterns we find that the behavior ofthese lazy layers is consistent across tokens during generation for a giveninput. This insight motivates our SimLayerKV which identifies lazy layers andreduces their KV cache accordingly. SimLayerKV is training-free generalizableand can be implemented with only seven lines of code. We conduct extensiveexperiments on three representative LLMs e.g. LLaMA2-7B LLaMA3-8B andMistral-7B across 16 tasks from the LongBench benchmark. The resultsdemonstrate that SimLayerKV achieves a KV cache compression ratio of 5timeswith only a 1.2 performance drop when combined with 4-bit quantization. Ourcode is available at https://github.com/sail-sg/SimLayerKV.</p>
                <p>Last Updated: 2024-10-17 17:58:14 UTC</p>
                <button class="interpret-button" data-id="2410.13846v1">Interpret</button>
                <div id="interpretation-2410.13846v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens</h3>
                <p>Authors: Lijie FanTianhong LiSiyang QinYuanzhen LiChen SunMichael RubinsteinDeqing SunKaiming HeYonglong Tian</p>
                <p><a href="http://arxiv.org/abs/2410.13863v1">Link to paper</a></p>
                <p>Scaling up autoregressive models in vision has not proven as beneficial as inlarge language models. In this work we investigate this scaling problem in thecontext of text-to-image generation focusing on two critical factors: whethermodels use discrete or continuous tokens and whether tokens are generated in arandom or fixed raster order using BERT- or GPT-like transformer architectures.Our empirical results show that while all models scale effectively in terms ofvalidation loss their evaluation performance -- measured by FID GenEvalscore and visual quality -- follows different trends. Models based oncontinuous tokens achieve significantly better visual quality than those usingdiscrete tokens. Furthermore the generation order and attention mechanismssignificantly affect the GenEval score: random-order models achieve notablybetter GenEval scores compared to raster-order models. Inspired by thesefindings we train Fluid a random-order autoregressive model on continuoustokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16on MS-COCO 30K and 0.69 overall score on the GenEval benchmark. We hope ourfindings and results will encourage future efforts to further bridge thescaling gap between vision and language models.</p>
                <p>Last Updated: 2024-10-17 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2410.13863v1">Interpret</button>
                <div id="interpretation-2410.13863v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs</h3>
                <p>Authors: Guhao FengKai YangYuntian GuXinyue AiShengjie LuoJiacheng SunDi HeZhenguo LiLiwei Wang</p>
                <p><a href="http://arxiv.org/abs/2410.13857v1">Link to paper</a></p>
                <p>Despite the remarkable success of Transformer-based Large Language ModelsLLMs across various domains understanding and enhancing their mathematicalcapabilities remains a significant challenge. In this paper we conduct arigorous theoretical analysis of LLMs mathematical abilities with a specificfocus on their arithmetic performances. We identify numerical precision as akey factor that influences their effectiveness in mathematical tasks. Ourresults show that Transformers operating with low numerical precision fail toaddress arithmetic tasks such as iterated addition and integer multiplicationunless the model size grows super-polynomially with respect to the inputlength. In contrast Transformers with standard numerical precision canefficiently handle these tasks with significantly smaller model sizes. Wefurther support our theoretical findings through empirical experiments thatexplore the impact of varying numerical precision on arithmetic tasksproviding valuable insights for improving the mathematical reasoningcapabilities of LLMs.</p>
                <p>Last Updated: 2024-10-17 17:59:35 UTC</p>
                <button class="interpret-button" data-id="2410.13857v1">Interpret</button>
                <div id="interpretation-2410.13857v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Diffusing States and Matching Scores: A New Framework for Imitation Learning</h3>
                <p>Authors: Runzhe WuYiding ChenGokul SwamyKianté BrantleyWen Sun</p>
                <p><a href="http://arxiv.org/abs/2410.13855v1">Link to paper</a></p>
                <p>Adversarial Imitation Learning is traditionally framed as a two-playerzero-sum game between a learner and an adversarially chosen cost function andcan therefore be thought of as the sequential generalization of a GenerativeAdversarial Network GAN. A prominent example of this framework is GenerativeAdversarial Imitation Learning GAIL. However in recent years diffusionmodels have emerged as a non-adversarial alternative to GANs that merelyrequire training a score function via regression yet produce generations of ahigher quality. In response we investigate how to lift insights from diffusionmodeling to the sequential setting. We propose diffusing states and performingscore-matching along diffused states to measure the discrepancy between theexperts and learners states. Thus our approach only requires training scorefunctions to predict noises via standard regression making it significantlyeasier and more stable to train than adversarial methods. Theoretically weprove first- and second-order instance-dependent bounds with linear scaling inthe horizon proving that our approach avoids the compounding errors thatstymie offline approaches to imitation learning. Empirically we show ourapproach outperforms GAN-style imitation learning baselines across variouscontinuous control problems including complex tasks like controlling humanoidsto walk sit and crawl.</p>
                <p>Last Updated: 2024-10-17 17:59:25 UTC</p>
                <button class="interpret-button" data-id="2410.13855v1">Interpret</button>
                <div id="interpretation-2410.13855v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AutoAL: Automated Active Learning with Differentiable Query Strategy Search</h3>
                <p>Authors: Yifeng WangXueying ZhanSiyu Huang</p>
                <p><a href="http://arxiv.org/abs/2410.13853v1">Link to paper</a></p>
                <p>As deep learning continues to evolve the need for data efficiency becomesincreasingly important. Considering labeling large datasets is bothtime-consuming and expensive active learning AL provides a promisingsolution to this challenge by iteratively selecting the most informativesubsets of examples to train deep neural networks thereby reducing thelabeling cost. However the effectiveness of different AL algorithms can varysignificantly across data scenarios and determining which AL algorithm bestfits a given task remains a challenging problem. This work presents the firstdifferentiable AL strategy search method named AutoAL which is designed ontop of existing AL sampling strategies. AutoAL consists of two neural netsnamed SearchNet and FitNet which are optimized concurrently under adifferentiable bi-level optimization framework. For any given task SearchNetand FitNet are iteratively co-optimized using the labeled data learning howwell a set of candidate AL algorithms perform on that task. With the optimal ALstrategies identified SearchNet selects a small subset from the unlabeled poolfor querying their annotations enabling efficient training of the task model.Experimental results demonstrate that AutoAL consistently achieves superioraccuracy compared to all candidate AL algorithms and other selective ALapproaches showcasing its potential for adapting and integrating multipleexisting AL methods across diverse tasks and domains. Code will be availableat: https://github.com/haizailache999/AutoAL.</p>
                <p>Last Updated: 2024-10-17 17:59:09 UTC</p>
                <button class="interpret-button" data-id="2410.13853v1">Interpret</button>
                <div id="interpretation-2410.13853v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Retrospective Learning from Interactions</h3>
                <p>Authors: Zizhao ChenMustafa Omer GulYiwei ChenGloria GengAnne WuYoav Artzi</p>
                <p><a href="http://arxiv.org/abs/2410.13852v1">Link to paper</a></p>
                <p>Multi-turn interactions between large language models LLMs and usersnaturally include implicit feedback signals. If an LLM responds in anunexpected way to an instruction the user is likely to signal it by rephrasingthe request expressing frustration or pivoting to an alternative task. Suchsignals are task-independent and occupy a relatively constrained subspace oflanguage allowing the LLM to identify them even if it fails on the actualtask. This creates an avenue for continually learning from interactions withoutadditional annotations. We introduce ReSpect a method to learn from suchsignals in past interactions via retrospection. We deploy ReSpect in a newmultimodal interaction scenario where humans instruct an LLM to solve anabstract reasoning task with a combinatorial solution space. Through thousandsof interactions with humans we show how ReSpect gradually improves taskcompletion rate from 31 to 82 all without any external annotation.</p>
                <p>Last Updated: 2024-10-17 17:59:03 UTC</p>
                <button class="interpret-button" data-id="2410.13852v1">Interpret</button>
                <div id="interpretation-2410.13852v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-10-20</p>
        </div>
    
        </div>
    </body>
    </html>
    