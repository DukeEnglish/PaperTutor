
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Auctioning Escape Permits for Multiple Correlated Pollutants Using CMRA</h3>
                <p>Authors: Keshav GoyalSooraj SathishShrisha Rao</p>
                <p><a href="http://arxiv.org/abs/2408.10148v1">Link to paper</a></p>
                <p>In the context of increasingly complex environmental challenges effectivepollution control mechanisms are crucial. By extending the state of the artauction mechanisms we aim to develop an efficient approach for allocatingpollution abatement resources in a multi-pollutant setting with pollutantsaffecting each others reduction costs. We modify the Combinatorial Multi-RoundAscending Auction for the auction of escape permits of pollutants withco-dependent reduction processes specifically greenhouse gas emissions andnutrient runoff in Finnish agriculture. We show the significant advantages ofthis mechanism in pollution control through experiments on the bid prices andamount of escape permits sold in multiple auction simulations.</p>
                <p>Last Updated: 2024-08-19 16:49:19 UTC</p>
                <button class="interpret-button" data-id="2408.10148v1">Interpret</button>
                <div id="interpretation-2408.10148v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Synthesis of Reward Machines for Multi-Agent Equilibrium Design (Full Version)</h3>
                <p>Authors: Muhammad NajibGiuseppe Perelli</p>
                <p><a href="http://arxiv.org/abs/2408.10074v1">Link to paper</a></p>
                <p>Mechanism design is a well-established game-theoretic paradigm for designinggames to achieve desired outcomes. This paper addresses a closely related butdistinct concept equilibrium design. Unlike mechanism design the designersauthority in equilibrium design is more constrained she can only modify theincentive structures in a given game to achieve certain outcomes without theability to create the game from scratch. We study the problem of equilibriumdesign using dynamic incentive structures known as reward machines. We useweighted concurrent game structures for the game model with goals for theplayers and the designer defined as mean-payoff objectives. We show how rewardmachines can be used to represent dynamic incentives that allocate rewards in amanner that optimises the designers goal. We also introduce the main decisionproblem within our framework the payoff improvement problem. This problemessentially asks whether there exists a dynamic incentive represented by somereward machine that can improve the designers payoff by more than a giventhreshold value. We present two variants of the problem: strong and weak. Wedemonstrate that both can be solved in polynomial time using a Turing machineequipped with an NP oracle. Furthermore we also establish that these variantsare either NP-hard or coNP-hard. Finally we show how to synthesise thecorresponding reward machine if it exists.</p>
                <p>Last Updated: 2024-08-19 15:17:58 UTC</p>
                <button class="interpret-button" data-id="2408.10074v1">Interpret</button>
                <div id="interpretation-2408.10074v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MegaAgent: A Practical Framework for Autonomous Cooperation in Large-Scale LLM Agent Systems</h3>
                <p>Authors: Qian WangTianyu WangQinbin LiJingsheng LiangBingsheng He</p>
                <p><a href="http://arxiv.org/abs/2408.09955v1">Link to paper</a></p>
                <p>With the emergence of large language models LLMs LLM-powered multi-agentsystems LLM-MA systems have been proposed to tackle real-world tasks.However their agents mostly follow predefined Standard Operating ProceduresSOPs that remain unchanged across the whole interaction lacking autonomy andscalability. Additionally current solutions often overlook the necessity foreffective agent cooperation. To address the above limitations we proposeMegaAgent a practical framework designed for autonomous cooperation inlarge-scale LLM Agent systems. MegaAgent leverages the autonomy of agents todynamically generate agents based on task requirements incorporating featuressuch as automatically dividing tasks systematic planning and monitoring ofagent activities and managing concurrent operations. In addition MegaAgent isdesigned with a hierarchical structure and employs system-level parallelism toenhance performance and boost communication. We demonstrate the effectivenessof MegaAgent through Gobang game development showing that it outperformspopular LLM-MA systems and national policy simulation demonstrating its highautonomy and potential to rapidly scale up to 590 agents while ensuringeffective cooperation among them. Our results indicate that MegaAgent is thefirst autonomous large-scale LLM-MA system with no pre-defined SOPs higheffectiveness and scalability paving the way for further research in thisfield. Our code is at https://anonymous.4open.science/r/MegaAgent-81F3.</p>
                <p>Last Updated: 2024-08-19 12:55:16 UTC</p>
                <button class="interpret-button" data-id="2408.09955v1">Interpret</button>
                <div id="interpretation-2408.09955v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Algorithmic Contract Design with Reinforcement Learning Agents</h3>
                <p>Authors: David Molina ConchaKyeonghyeon ParkHyun-Rok LeeTaesik LeeChi-Guhn Lee</p>
                <p><a href="http://arxiv.org/abs/2408.09686v1">Link to paper</a></p>
                <p>We introduce a novel problem setting for algorithmic contract design namedthe principal-MARL contract design problem. This setting extends traditionalcontract design to account for dynamic and stochastic environments using MarkovGames and Multi-Agent Reinforcement Learning. To tackle this problem wepropose a Multi-Objective Bayesian Optimization MOBO framework namedConstrained Pareto Maximum Entropy Search cPMES. Our approach integrates MOBOand MARL to explore the highly constrained contract design space identifyingpromising incentive and recruitment decisions. cPMES transforms theprincipal-MARL contract design problem into an unconstrained multi-objectiveproblem leveraging the probability of feasibility as part of the objectivesand ensuring promising designs predicted on the feasibility border are includedin the Pareto front. By focusing the entropy prediction on designs within thePareto set cPMES mitigates the risk of the search strategy being overwhelmedby entropy from constraints. We demonstrate the effectiveness of cPMES throughextensive benchmark studies in synthetic and simulated environments showingits ability to find feasible contract designs that maximize the principalsobjectives. Additionally we provide theoretical support with a sub-linearregret bound concerning the number of iterations.</p>
                <p>Last Updated: 2024-08-19 03:48:38 UTC</p>
                <button class="interpret-button" data-id="2408.09686v1">Interpret</button>
                <div id="interpretation-2408.09686v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey</h3>
                <p>Authors: Ruiqi ZhangJing HouFlorian WalterShangding GuJiayi GuanFlorian RÃ¶hrbeinYali DuPanpan CaiGuang ChenAlois Knoll</p>
                <p><a href="http://arxiv.org/abs/2408.09675v1">Link to paper</a></p>
                <p>Reinforcement Learning RL is a potent tool for sequential decision-makingand has achieved performance surpassing human capabilities across manychallenging real-world tasks. As the extension of RL in the multi-agent systemdomain multi-agent RL MARL not only need to learn the control policy butalso requires consideration regarding interactions with all other agents in theenvironment mutual influences among different system components and thedistribution of computational resources. This augments the complexity ofalgorithmic design and poses higher requirements on computational resources.Simultaneously simulators are crucial to obtain realistic data which is thefundamentals of RL. In this paper we first propose a series of metrics ofsimulators and summarize the features of existing benchmarks. Second to easecomprehension we recall the foundational knowledge and then synthesize therecently advanced studies of MARL-related autonomous driving and intelligenttransportation systems. Specifically we examine their environmental modelingstate representation perception units and algorithm design. Conclusively wediscuss open challenges as well as prospects and opportunities. We hope thispaper can help the researchers integrate MARL technologies and trigger moreinsightful ideas toward the intelligent and autonomous driving.</p>
                <p>Last Updated: 2024-08-19 03:31:20 UTC</p>
                <button class="interpret-button" data-id="2408.09675v1">Interpret</button>
                <div id="interpretation-2408.09675v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Criticality Leveraged Adversarial Training (CLAT) for Boosted Performance via Parameter Efficiency</h3>
                <p>Authors: Bhavna GopalHuanrui YangJingyang ZhangMark HortonYiran Chen</p>
                <p><a href="http://arxiv.org/abs/2408.10204v1">Link to paper</a></p>
                <p>Adversarial training enhances neural network robustness but suffers from atendency to overfit and increased generalization errors on clean data. Thiswork introduces CLAT an innovative approach that mitigates adversarialoverfitting by introducing parameter efficiency into the adversarial trainingprocess improving both clean accuracy and adversarial robustness. Instead oftuning the entire model CLAT identifies and fine-tunes robustness-criticallayers - those predominantly learning non-robust features - while freezing theremaining model to enhance robustness. It employs dynamic critical layerselection to adapt to changes in layer criticality throughout the fine-tuningprocess. Empirically CLAT can be applied on top of existing adversarialtraining methods significantly reduces the number of trainable parameters byapproximately 95 and achieves more than a 2 improvement in adversarialrobustness compared to baseline methods.</p>
                <p>Last Updated: 2024-08-19 17:58:03 UTC</p>
                <button class="interpret-button" data-id="2408.10204v1">Interpret</button>
                <div id="interpretation-2408.10204v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP</h3>
                <p>Authors: Yusuke HirotaMin-Hung ChenChien-Yi WangYuta NakashimaYu-Chiang Frank WangRyo Hachiuma</p>
                <p><a href="http://arxiv.org/abs/2408.10202v1">Link to paper</a></p>
                <p>Large-scale vision-language models such as CLIP are known to containharmful societal bias regarding protected attributes e.g. gender and age. Inthis paper we aim to address the problems of societal bias in CLIP. Althoughprevious studies have proposed to debias societal bias through adversariallearning or test-time projecting our comprehensive study of these worksidentifies two critical limitations: 1 loss of attribute information when itis explicitly disclosed in the input and 2 use of the attribute annotationsduring debiasing process. To mitigate societal bias in CLIP and overcome theselimitations simultaneously we introduce a simple-yet-effective debiasingmethod called SANER societal attribute neutralizer that eliminates attributeinformation from CLIP text features only of attribute-neutral descriptions.Experimental results show that SANER which does not require attributeannotations and preserves original information for attribute-specificdescriptions demonstrates superior debiasing ability than the existingmethods.</p>
                <p>Last Updated: 2024-08-19 17:57:28 UTC</p>
                <button class="interpret-button" data-id="2408.10202v1">Interpret</button>
                <div id="interpretation-2408.10202v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model</h3>
                <p>Authors: Minghua LiuChong ZengXinyue WeiRuoxi ShiLinghao ChenChao XuMengqi ZhangZhaoning WangXiaoshuai ZhangIsabella LiuHongzhi WuHao Su</p>
                <p><a href="http://arxiv.org/abs/2408.10198v1">Link to paper</a></p>
                <p>Open-world 3D reconstruction models have recently garnered significantattention. However without sufficient 3D inductive bias existing methodstypically entail expensive training costs and struggle to extract high-quality3D meshes. In this work we introduce MeshFormer a sparse-view reconstructionmodel that explicitly leverages 3D native structure input guidance andtraining supervision. Specifically instead of using a triplane representationwe store features in 3D sparse voxels and combine transformers with 3Dconvolutions to leverage an explicit 3D structure and projective bias. Inaddition to sparse-view RGB input we require the network to take input andgenerate corresponding normal maps. The input normal maps can be predicted by2D diffusion models significantly aiding in the guidance and refinement of thegeometrys learning. Moreover by combining Signed Distance Function SDFsupervision with surface rendering we directly learn to generate high-qualitymeshes without the need for complex multi-stage training processes. Byincorporating these explicit 3D biases MeshFormer can be trained efficientlyand deliver high-quality textured meshes with fine-grained geometric details.It can also be integrated with 2D diffusion models to enable fastsingle-image-to-3D and text-to-3D tasks. Project page:https://meshformer3d.github.io</p>
                <p>Last Updated: 2024-08-19 17:55:17 UTC</p>
                <button class="interpret-button" data-id="2408.10198v1">Interpret</button>
                <div id="interpretation-2408.10198v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views</h3>
                <p>Authors: Chao XuAng LiLinghao ChenYulin LiuRuoxi ShiHao SuMinghua Liu</p>
                <p><a href="http://arxiv.org/abs/2408.10195v1">Link to paper</a></p>
                <p>Open-world 3D generation has recently attracted considerable attention. Whilemany single-image-to-3D methods have yielded visually appealing outcomes theyoften lack sufficient controllability and tend to produce hallucinated regionsthat may not align with users expectations. In this paper we explore animportant scenario in which the input consists of one or a few unposed 2Dimages of a single object with little or no overlap. We propose a novelmethod SpaRP to reconstruct a 3D textured mesh and estimate the relativecamera poses for these sparse-view images. SpaRP distills knowledge from 2Ddiffusion models and finetunes them to implicitly deduce the 3D spatialrelationships between the sparse views. The diffusion model is trained tojointly predict surrogate representations for camera poses and multi-viewimages of the object under known poses integrating all information from theinput sparse views. These predictions are then leveraged to accomplish 3Dreconstruction and pose estimation and the reconstructed 3D model can be usedto further refine the camera poses of input views. Through extensiveexperiments on three datasets we demonstrate that our method not onlysignificantly outperforms baseline methods in terms of 3D reconstructionquality and pose prediction accuracy but also exhibits strong efficiency. Itrequires only about 20 seconds to produce a textured mesh and camera poses forthe input views. Project page: https://chaoxu.xyz/sparp.</p>
                <p>Last Updated: 2024-08-19 17:53:10 UTC</p>
                <button class="interpret-button" data-id="2408.10195v1">Interpret</button>
                <div id="interpretation-2408.10195v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LongVILA: Scaling Long-Context Visual Language Models for Long Videos</h3>
                <p>Authors: Fuzhao XueYukang ChenDacheng LiQinghao HuLigeng ZhuXiuyu LiYunhao FangHaotian TangShang YangZhijian LiuEthan HeHongxu YinPavlo MolchanovJan KautzLinxi FanYuke ZhuYao LuSong Han</p>
                <p><a href="http://arxiv.org/abs/2408.10188v1">Link to paper</a></p>
                <p>Long-context capability is critical for multi-modal foundation models. Weintroduce LongVILA a full-stack solution for long-context vision-languagemodels including system model training and dataset development. On thesystem side we introduce the first Multi-Modal Sequence Parallelism MM-SPsystem that enables long-context training and inference enabling 2M contextlength training on 256 GPUs. MM-SP is also efficient being 2.1x - 5.7x fasterthan Ring-Style Sequence Parallelism and 1.1x - 1.4x faster than Megatron-LM intext-only settings. Moreover it seamlessly integrates with Hugging FaceTransformers. For model training we propose a five-stage pipeline comprisingalignment pre-training context extension and long-short joint supervisedfine-tuning. Regarding datasets we meticulously construct large-scale visuallanguage pre-training datasets and long video instruction-following datasets tosupport our multi-stage training process. The full-stack solution extends thefeasible frame number of VILA by a factor of 128 from 8 to 1024 frames andimproves long video captioning score from 2.00 to 3.26 1.6x achieving 99.5accuracy in 1400-frames video 274k context length needle in a haystack.LongVILA-8B also demonstrates a consistent improvement in performance on longvideos within the VideoMME benchmark as the video frames increase.</p>
                <p>Last Updated: 2024-08-19 17:48:08 UTC</p>
                <button class="interpret-button" data-id="2408.10188v1">Interpret</button>
                <div id="interpretation-2408.10188v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>KAN 2.0: Kolmogorov-Arnold Networks Meet Science</h3>
                <p>Authors: Ziming LiuPingchuan MaYixuan WangWojciech MatusikMax Tegmark</p>
                <p><a href="http://arxiv.org/abs/2408.10205v1">Link to paper</a></p>
                <p>A major challenge of AI  Science lies in their inherent incompatibility:todays AI is primarily based on connectionism while science depends onsymbolism. To bridge the two worlds we propose a framework to seamlesslysynergize Kolmogorov-Arnold Networks KANs and science. The frameworkhighlights KANs usage for three aspects of scientific discovery: identifyingrelevant features revealing modular structures and discovering symbolicformulas. The synergy is bidirectional: science to KAN incorporatingscientific knowledge into KANs and KAN to science extracting scientificinsights from KANs. We highlight major new functionalities in the pykanpackage: 1 MultKAN: KANs with multiplication nodes. 2 kanpiler: a KANcompiler that compiles symbolic formulas into KANs. 3 tree converter: convertKANs or any neural networks to tree graphs. Based on these tools wedemonstrate KANs capability to discover various types of physical lawsincluding conserved quantities Lagrangians symmetries and constitutive laws.</p>
                <p>Last Updated: 2024-08-19 17:59:04 UTC</p>
                <button class="interpret-button" data-id="2408.10205v1">Interpret</button>
                <div id="interpretation-2408.10205v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Criticality Leveraged Adversarial Training (CLAT) for Boosted Performance via Parameter Efficiency</h3>
                <p>Authors: Bhavna GopalHuanrui YangJingyang ZhangMark HortonYiran Chen</p>
                <p><a href="http://arxiv.org/abs/2408.10204v1">Link to paper</a></p>
                <p>Adversarial training enhances neural network robustness but suffers from atendency to overfit and increased generalization errors on clean data. Thiswork introduces CLAT an innovative approach that mitigates adversarialoverfitting by introducing parameter efficiency into the adversarial trainingprocess improving both clean accuracy and adversarial robustness. Instead oftuning the entire model CLAT identifies and fine-tunes robustness-criticallayers - those predominantly learning non-robust features - while freezing theremaining model to enhance robustness. It employs dynamic critical layerselection to adapt to changes in layer criticality throughout the fine-tuningprocess. Empirically CLAT can be applied on top of existing adversarialtraining methods significantly reduces the number of trainable parameters byapproximately 95 and achieves more than a 2 improvement in adversarialrobustness compared to baseline methods.</p>
                <p>Last Updated: 2024-08-19 17:58:03 UTC</p>
                <button class="interpret-button" data-id="2408.10204v1">Interpret</button>
                <div id="interpretation-2408.10204v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification</h3>
                <p>Authors: Jing Li</p>
                <p><a href="http://arxiv.org/abs/2408.10193v1">Link to paper</a></p>
                <p>Evaluation Metrics is an important question for model evaluation and modelselection in binary classification tasks. This study investigates howconsistent metrics are at evaluating different models under different datascenarios. Analyzing over 150 data scenarios and 18 model evaluation metricsusing statistical simulation I find that for binary classification tasksevaluation metrics that are less influenced by prevalence offer more consistentranking of a set of different models. In particular Area Under the ROC CurveAUC has smallest variance in ranking of different models. Matthewscorrelation coefficient as a more strict measure of model performance has thesecond smallest variance. These patterns holds across a rich set of datascenarios and five commonly used machine learning models as well as a naiverandom guess model. The results have significant implications for modelevaluation and model selection in binary classification tasks.</p>
                <p>Last Updated: 2024-08-19 17:52:38 UTC</p>
                <button class="interpret-button" data-id="2408.10193v1">Interpret</button>
                <div id="interpretation-2408.10193v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models</h3>
                <p>Authors: Aviv BickKevin Y. LiEric P. XingJ. Zico KolterAlbert Gu</p>
                <p><a href="http://arxiv.org/abs/2408.10189v1">Link to paper</a></p>
                <p>Transformer architectures have become a dominant paradigm for domains likelanguage modeling but suffer in many inference settings due to theirquadratic-time self-attention. Recently proposed subquadratic architecturessuch as Mamba have shown promise but have been pretrained with substantiallyless computational resources than the strongest Transformer models. In thiswork we present a method that is able to distill a pretrained Transformerarchitecture into alternative architectures such as state space models SSMs.The key idea to our approach is that we can view both Transformers and SSMs asapplying different forms of mixing matrices over the token sequences. We canthus progressively distill the Transformer architecture by matching differentdegrees of granularity in the SSM: first matching the mixing matricesthemselves then the hidden units at each block and finally the end-to-endpredictions. Our method called MOHAWK is able to distill a Mamba-2 variantbased on the Phi-1.5 architecture Phi-Mamba using only 3B tokens and a hybridversion Hybrid Phi-Mamba using 5B tokens. Despite using less than 1 of thetraining data typically used to train models from scratch Phi-Mamba boastssubstantially stronger performance compared to all past open-sourcenon-Transformer models. MOHAWK allows models like SSMs to leveragecomputational resources invested in training Transformer-based architectureshighlighting a new avenue for building such models.</p>
                <p>Last Updated: 2024-08-19 17:48:11 UTC</p>
                <button class="interpret-button" data-id="2408.10189v1">Interpret</button>
                <div id="interpretation-2408.10189v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models</h3>
                <p>Authors: Anke TangLi ShenYong LuoShuai XieHan HuLefei ZhangBo DuDacheng Tao</p>
                <p><a href="http://arxiv.org/abs/2408.10174v1">Link to paper</a></p>
                <p>Deep model training on extensive datasets is increasingly becomingcost-prohibitive prompting the widespread adoption of deep model fusiontechniques to leverage knowledge from pre-existing models. From simple weightaveraging to more sophisticated methods like AdaMerging model fusioneffectively improves model performance and accelerates the development of newmodels. However potential interference between parameters of individual modelsand the lack of interpretability in the fusion progress remain significantchallenges. Existing methods often try to resolve the parameter interferenceissue by evaluating attributes of parameters such as their magnitude or signor by parameter pruning. In this study we begin by examining the fine-tuningof linear layers through the lens of subspace analysis and explicitly defineparameter interference as an optimization problem to shed light on thissubject. Subsequently we introduce an innovative approach to model fusioncalled zero-shot Sparse MIxture of Low-rank Experts SMILE construction whichallows for the upscaling of source models into an MoE model without extra dataor further training. Our approach relies on the observation that fine-tuningmostly keeps the important parts from the pre-training but it uses lesssignificant or unused areas to adapt to new tasks. Also the issue of parameterinterference which is intrinsically intractable in the original parameterspace can be managed by expanding the dimensions. We conduct extensiveexperiments across diverse scenarios such as image classification and textgeneralization tasks using full fine-tuning and LoRA fine-tuning and we applyour method to large language models CLIP models Flan-T5 models andMistral-7B models highlighting the adaptability and scalability of SMILE.Code is available at https://github.com/tanganke/fusion_bench</p>
                <p>Last Updated: 2024-08-19 17:32:15 UTC</p>
                <button class="interpret-button" data-id="2408.10174v1">Interpret</button>
                <div id="interpretation-2408.10174v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>A Graph-based Approach to Human Activity Recognition</h3>
                <p>Authors: Thomas PeroutkaIlir MurturiPraveen Kumar DontaSchahram Dustdar</p>
                <p><a href="http://arxiv.org/abs/2408.10191v1">Link to paper</a></p>
                <p>Advanced wearable sensor devices have enabled the recording of vast amountsof movement data from individuals regarding their physical activities. Thisdata offers valuable insights that enhance our understanding of how physicalactivities contribute to improved physical health and overall quality of life.Consequently there is a growing need for efficient methods to extractsignificant insights from these rapidly expanding real-time datasets. Thispaper presents a methodology to efficiently extract substantial insights fromthese expanding datasets focusing on professional sports but applicable tovarious human activities. By utilizing data from Inertial Measurement UnitsIMU and Global Navigation Satellite Systems GNSS receivers athleticperformance can be analyzed using directed graphs to encode knowledge ofcomplex movements. Our approach is demonstrated on biathlon data and detectsspecific points of interest and complex movement sequences facilitating thecomparison and analysis of human physical performance.</p>
                <p>Last Updated: 2024-08-19 17:51:00 UTC</p>
                <button class="interpret-button" data-id="2408.10191v1">Interpret</button>
                <div id="interpretation-2408.10191v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Envisioning Possibilities and Challenges of AI for Personalized Cancer Care</h3>
                <p>Authors: Elaine KongKuo-TingHuangAakash Gautam</p>
                <p><a href="http://dx.doi.org/10.1145/3678884.3681885">Link to paper</a></p>
                <p>The use of Artificial Intelligence AI in healthcare including in caringfor cancer survivors has gained significant interest. However gaps remain inour understanding of how such AI systems can provide care especially forethnic and racial minority groups who continue to face care disparities.Through interviews with six cancer survivors we identify critical gaps incurrent healthcare systems such as a lack of personalized care and insufficientcultural and linguistic accommodation. AI when applied to care was seen as away to address these issues by enabling real-time culturally aligned andlinguistically appropriate interactions. We also uncovered concerns about theimplications of AI-driven personalization such as data privacy loss of humantouch in caregiving and the risk of echo chambers that limit exposure todiverse information. We conclude by discussing the trade-offs betweenAI-enhanced personalization and the need for structural changes in healthcarethat go beyond technological solutions leading us to argue that we shouldbegin by asking Why personalization</p>
                <p>Last Updated: 2024-08-19 15:55:46 UTC</p>
                <button class="interpret-button" data-id="2408.10108v1">Interpret</button>
                <div id="interpretation-2408.10108v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Working in Extended Reality in the Wild: Worker and Bystander Experiences of XR Virtual Displays in Real-World Settings</h3>
                <p>Authors: Leonardo PavanattoVerena BienerJennifer ChandranSnehanjali KalamkarFeiyu LuJohn J. DudleyJinghui HuG. Nikki Ramirez-SaffyPer Ola KristenssonAlexander GiovannelliLuke SchlueterJÃ¶rg MÃ¼llerJens GrubertDoug A. Bowman</p>
                <p><a href="http://arxiv.org/abs/2408.10000v1">Link to paper</a></p>
                <p>Although access to sufficient screen space is crucial to knowledge workworkers often find themselves with limited access to display infrastructure inremote or public settings. While virtual displays can be used to extend theavailable screen space through extended reality XR head-worn displays HWDwe must better understand the implications of working with them in publicsettings from both users and bystanders viewpoints. To this end we conductedtwo user studies. We first explored the usage of a hybrid AR display acrossreal-world settings and tasks. We focused on how users take advantage ofvirtual displays and what social and environmental factors impact their usageof the system. A second study investigated the differences between working witha laptop an AR system or a VR system in public. We focused on a singlelocation and participants performed a predefined task to enable directcomparisons between the conditions while also gathering data from bystanders.The combined results suggest a positive acceptance of XR technology in publicsettings and show that virtual displays can be used to accompany existingdevices. We highlighted some environmental and social factors. We saw thatprevious XR experience and personality can influence how people perceive theuse of XR in public. In addition we confirmed that using XR in public stillmakes users stand out and that bystanders are curious about the devices yethave no clear understanding of how they can be used.</p>
                <p>Last Updated: 2024-08-19 13:53:35 UTC</p>
                <button class="interpret-button" data-id="2408.10000v1">Interpret</button>
                <div id="interpretation-2408.10000v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WoW -- A System for Self-Service Collaborative Design Workshops</h3>
                <p>Authors: Ilyasse BelkacemVasile CiornaFrank PetryMohammad Ghoniem</p>
                <p><a href="http://arxiv.org/abs/2408.09926v1">Link to paper</a></p>
                <p>In many working environments users have to solve complex problems relying onlarge and multi-source data. Such problems require several experts tocollaborate on solving them or a single analyst to reconcile multiplecomplementary standpoints. Previous research has shown that wall-sized displayssupports different collaboration styles based most often on abstract tasks asproxies of real work. We present the design and implementation of WoW shortfor Workspace on Wall a multi-user Web-based portal for collaborativemeetings and workshops in multi-surface environments. We report on a two-yeareffort spanning context inquiry studies system design iterations developmentand real testing rounds targeting design engineers in the tire industry. Thepneumatic tires found on the market result from a highly collaborative anditerative development process that reconciles conflicting constraints through aseries of product design workshops. WoW was found to be a flexible solution tobuild multi-view set-ups in a self-service manner and an effective means toaccess more content at once. Our users also felt more engaged in theircollaborative problem-solving work using WoW than in conventional meetingrooms.</p>
                <p>Last Updated: 2024-08-19 12:03:51 UTC</p>
                <button class="interpret-button" data-id="2408.09926v1">Interpret</button>
                <div id="interpretation-2408.09926v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LCE: A Framework for Explainability of DNNs for Ultrasound Image Based on Concept Discovery</h3>
                <p>Authors: Weiji KongXun GongJuan Wang</p>
                <p><a href="http://arxiv.org/abs/2408.09899v1">Link to paper</a></p>
                <p>Explaining the decisions of Deep Neural Networks DNNs for medical imageshas become increasingly important. Existing attribution methods have difficultyexplaining the meaning of pixels while existing concept-based methods arelimited by additional annotations or specific model structures that aredifficult to apply to ultrasound images. In this paper we propose the LesionConcept Explainer LCE framework which combines attribution methods withconcept-based methods. We introduce the Segment Anything Model SAMfine-tuned on a large number of medical images for concept discovery to enablea meaningful explanation of ultrasound image DNNs. The proposed framework isevaluated in terms of both faithfulness and understandability. We point outdeficiencies in the popular faithfulness evaluation metrics and propose a newevaluation metric. Our evaluation of public and private breast ultrasounddatasets BUSI and FG-US-B shows that LCE performs well compared tocommonly-used explainability methods. Finally we also validate that LCE canconsistently provide reliable explanations for more meaningful fine-graineddiagnostic tasks in breast ultrasound.</p>
                <p>Last Updated: 2024-08-19 11:13:49 UTC</p>
                <button class="interpret-button" data-id="2408.09899v1">Interpret</button>
                <div id="interpretation-2408.09899v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>LongVILA: Scaling Long-Context Visual Language Models for Long Videos</h3>
                <p>Authors: Fuzhao XueYukang ChenDacheng LiQinghao HuLigeng ZhuXiuyu LiYunhao FangHaotian TangShang YangZhijian LiuEthan HeHongxu YinPavlo MolchanovJan KautzLinxi FanYuke ZhuYao LuSong Han</p>
                <p><a href="http://arxiv.org/abs/2408.10188v1">Link to paper</a></p>
                <p>Long-context capability is critical for multi-modal foundation models. Weintroduce LongVILA a full-stack solution for long-context vision-languagemodels including system model training and dataset development. On thesystem side we introduce the first Multi-Modal Sequence Parallelism MM-SPsystem that enables long-context training and inference enabling 2M contextlength training on 256 GPUs. MM-SP is also efficient being 2.1x - 5.7x fasterthan Ring-Style Sequence Parallelism and 1.1x - 1.4x faster than Megatron-LM intext-only settings. Moreover it seamlessly integrates with Hugging FaceTransformers. For model training we propose a five-stage pipeline comprisingalignment pre-training context extension and long-short joint supervisedfine-tuning. Regarding datasets we meticulously construct large-scale visuallanguage pre-training datasets and long video instruction-following datasets tosupport our multi-stage training process. The full-stack solution extends thefeasible frame number of VILA by a factor of 128 from 8 to 1024 frames andimproves long video captioning score from 2.00 to 3.26 1.6x achieving 99.5accuracy in 1400-frames video 274k context length needle in a haystack.LongVILA-8B also demonstrates a consistent improvement in performance on longvideos within the VideoMME benchmark as the video frames increase.</p>
                <p>Last Updated: 2024-08-19 17:48:08 UTC</p>
                <button class="interpret-button" data-id="2408.10188v1">Interpret</button>
                <div id="interpretation-2408.10188v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models</h3>
                <p>Authors: Amey HenglePrasoon BajpaiSoham DanTanmoy Chakraborty</p>
                <p><a href="http://arxiv.org/abs/2408.10151v1">Link to paper</a></p>
                <p>While recent large language models LLMs demonstrate remarkable abilities inresponding to queries in diverse languages their ability to handle longmultilingual contexts is unexplored. As such a systematic evaluation of thelong-context capabilities of LLMs in multilingual settings is crucialspecifically in the context of information retrieval. To address this gap weintroduce the MultiLingual Needle-in-a-Haystack MLNeedle test designed toassess a models ability to retrieve relevant information the needle from acollection of multilingual distractor texts the haystack. This test serves asan extension of the multilingual question-answering task encompassing bothmonolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMson MLNeedle. Our findings reveal that model performance can vary significantlywith language and needle position. Specifically we observe that modelperformance is the lowest when the needle is i in a language outside theEnglish language family and ii located in the middle of the input context.Furthermore although some models claim a context size of 8k tokens orgreater none demonstrate satisfactory cross-lingual retrieval performance asthe context length increases. Our analysis provides key insights into thelong-context behavior of LLMs in multilingual settings to guide futureevaluation protocols. To our knowledge this is the first study to investigatethe multilingual long-context behavior of LLMs.</p>
                <p>Last Updated: 2024-08-19 17:02:06 UTC</p>
                <button class="interpret-button" data-id="2408.10151v1">Interpret</button>
                <div id="interpretation-2408.10151v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>In-Context Learning with Representations: Contextual Generalization of Trained Transformers</h3>
                <p>Authors: Tong YangYu HuangYingbin LiangYuejie Chi</p>
                <p><a href="http://arxiv.org/abs/2408.10147v1">Link to paper</a></p>
                <p>In-context learning ICL refers to a remarkable capability of pretrainedlarge language models which can learn a new task given a few examples duringinference. However theoretical understanding of ICL is largely under-exploredparticularly whether transformers can be trained to generalize to unseenexamples in a prompt which will require the model to acquire contextualknowledge of the prompt for generalization. This paper investigates thetraining dynamics of transformers by gradient descent through the lens ofnon-linear regression tasks. The contextual generalization here can be attainedvia learning the template function for each task in-context where all templatefunctions lie in a linear space with m basis functions. We analyze thetraining dynamics of one-layer multi-head transformers to in-contextly predictunlabeled inputs given partially labeled prompts where the labels containGaussian noise and the number of examples in each prompt are not sufficient todetermine the template. Under mild assumptions we show that the training lossfor a one-layer multi-head transformer converges linearly to a global minimum.Moreover the transformer effectively learns to perform ridge regression overthe basis functions. To our knowledge this study is the first provabledemonstration that transformers can learn contextual i.e. templateinformation to generalize to both unseen examples and tasks when promptscontain only a small number of query-answer pairs.</p>
                <p>Last Updated: 2024-08-19 16:47:46 UTC</p>
                <button class="interpret-button" data-id="2408.10147v1">Interpret</button>
                <div id="interpretation-2408.10147v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Instruction Finetuning for Leaderboard Generation from Empirical AI Research</h3>
                <p>Authors: Salomon KabongoJennifer D'Souza</p>
                <p><a href="http://arxiv.org/abs/2408.10141v1">Link to paper</a></p>
                <p>This study demonstrates the application of instruction finetuning ofpretrained Large Language Models LLMs to automate the generation of AIresearch leaderboards extracting Task Dataset Metric Score quadruplesfrom articles. It aims to streamline the dissemination of advancements in AIresearch by transitioning from traditional manual community curation orotherwise taxonomy-constrained natural language inference NLI models to anautomated generative LLM-based approach. Utilizing the FLAN-T5 model thisresearch enhances LLMs adaptability and reliability in information extractionoffering a novel method for structured knowledge representation.</p>
                <p>Last Updated: 2024-08-19 16:41:07 UTC</p>
                <button class="interpret-button" data-id="2408.10141v1">Interpret</button>
                <div id="interpretation-2408.10141v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Rhyme-aware Chinese lyric generator based on GPT</h3>
                <p>Authors: Yixiao YuanYangchen HuangYu MaXinjin LiZhenglin LiYiming ShiHuapeng Zhou</p>
                <p><a href="http://arxiv.org/abs/2408.10130v1">Link to paper</a></p>
                <p>Neural language representation models such as GPT pre-trained on large-scalecorpora can effectively capture rich semantic patterns from plain text and befine-tuned to consistently improve natural language generation performance.However existing pre-trained language models used to generate lyrics rarelyconsider rhyme information which is crucial in lyrics. Using a pre-trainedmodel directly results in poor performance. To enhance the rhyming quality ofgenerated lyrics we incorporate integrated rhyme information into our modelthereby improving lyric generation performance.</p>
                <p>Last Updated: 2024-08-19 16:17:20 UTC</p>
                <button class="interpret-button" data-id="2408.10130v1">Interpret</button>
                <div id="interpretation-2408.10130v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification</h3>
                <p>Authors: Jing Li</p>
                <p><a href="http://arxiv.org/abs/2408.10193v1">Link to paper</a></p>
                <p>Evaluation Metrics is an important question for model evaluation and modelselection in binary classification tasks. This study investigates howconsistent metrics are at evaluating different models under different datascenarios. Analyzing over 150 data scenarios and 18 model evaluation metricsusing statistical simulation I find that for binary classification tasksevaluation metrics that are less influenced by prevalence offer more consistentranking of a set of different models. In particular Area Under the ROC CurveAUC has smallest variance in ranking of different models. Matthewscorrelation coefficient as a more strict measure of model performance has thesecond smallest variance. These patterns holds across a rich set of datascenarios and five commonly used machine learning models as well as a naiverandom guess model. The results have significant implications for modelevaluation and model selection in binary classification tasks.</p>
                <p>Last Updated: 2024-08-19 17:52:38 UTC</p>
                <button class="interpret-button" data-id="2408.10193v1">Interpret</button>
                <div id="interpretation-2408.10193v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Robust spectral clustering with rank statistics</h3>
                <p>Authors: Joshua CapeXianshi YuJonquil Z. Liao</p>
                <p><a href="http://arxiv.org/abs/2408.10136v1">Link to paper</a></p>
                <p>This paper analyzes the statistical performance of a robust spectralclustering method for latent structure recovery in noisy data matrices. Weconsider eigenvector-based clustering applied to a matrix of nonparametric rankstatistics that is derived entrywise from the raw original data matrix. Thisapproach is robust in the sense that unlike traditional spectral clusteringprocedures it can provably recover population-level latent block structureeven when the observed data matrix includes heavy-tailed entries and has aheterogeneous variance profile.  Our main theoretical contributions are threefold and hold under flexible datagenerating conditions. First we establish that robust spectral clustering withrank statistics can consistently recover latent block structure viewed ascommunities of nodes in a graph in the sense that unobserved communitymemberships for all but a vanishing fraction of nodes are correctly recoveredwith high probability when the data matrix is large. Second we refine theformer result and further establish that under certain conditions thecommunity membership of any individual specified node of interest can beasymptotically exactly recovered with probability tending to one in thelarge-data limit. Third we establish asymptotic normality results associatedwith the truncated eigenstructure of matrices whose entries are rankstatistics made possible by synthesizing contemporary entrywise matrixperturbation analysis with the classical nonparametric theory of so-calledsimple linear rank statistics. Collectively these results demonstrate thestatistical utility of rank-based data transformations when paired withspectral techniques for dimensionality reduction. Additionally for a datasetof human connectomes our approach yields parsimonious dimensionality reductionand improved recovery of ground-truth neuroanatomical cluster structure.</p>
                <p>Last Updated: 2024-08-19 16:33:44 UTC</p>
                <button class="interpret-button" data-id="2408.10136v1">Interpret</button>
                <div id="interpretation-2408.10136v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples in Constrained Access Environments</h3>
                <p>Authors: Heeyoung LeeHoyoon ByunChangdae OhJinYeong BakKyungwoo Song</p>
                <p><a href="http://arxiv.org/abs/2408.10107v1">Link to paper</a></p>
                <p>Accessing machine learning models through remote APIs has been gainingprevalence following the recent trend of scaling up model parameters forincreased performance. Even though these models exhibit remarkable abilitydetecting out-of-distribution OOD samples remains a crucial safety concernfor end users as these samples may induce unreliable outputs from the model. Inthis work we propose an OOD detection framework MixDiff that is applicableeven when the models parameters or its activations are not accessible to theend user. To bypass the access restriction MixDiff applies an identicalinput-level perturbation to a given target sample and a similar in-distributionID sample then compares the relative difference in the model outputs ofthese two samples. MixDiff is model-agnostic and compatible with existingoutput-based OOD detection methods. We provide theoretical analysis toillustrate MixDiffs effectiveness in discerning OOD samples that induceoverconfident outputs from the model and empirically demonstrate that MixDiffconsistently enhances the OOD detection performance on various datasets invision and text domains.</p>
                <p>Last Updated: 2024-08-19 15:51:31 UTC</p>
                <button class="interpret-button" data-id="2408.10107v1">Interpret</button>
                <div id="interpretation-2408.10107v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Parseval Convolution Operators and Neural Networks</h3>
                <p>Authors: Michael UnserStanislas Ducotterd</p>
                <p><a href="http://arxiv.org/abs/2408.09981v1">Link to paper</a></p>
                <p>We first establish a kernel theorem that characterizes all linearshift-invariant LSI operators acting on discrete multicomponent signals. Thisresult naturally leads to the identification of the Parseval convolutionoperators as the class of energy-preserving filterbanks. We then present aconstructive approach for the design/specification of such filterbanks via thechaining of elementary Parseval modules each of which being parameterized byan orthogonal matrix or a 1-tight frame. Our analysis is complemented withexplicit formulas for the Lipschitz constant of all the components of aconvolutional neural network CNN which gives us a handle on their stability.Finally we demonstrate the usage of those tools with the design of a CNN-basedalgorithm for the iterative reconstruction of biomedical images. Our algorithmfalls within the plug-and-play framework for the resolution of inverseproblems. It yields better-quality results than the sparsity-based methods usedin compressed sensing while offering essentially the same convergence androbustness guarantees.</p>
                <p>Last Updated: 2024-08-19 13:31:16 UTC</p>
                <button class="interpret-button" data-id="2408.09981v1">Interpret</button>
                <div id="interpretation-2408.09981v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Predicting path-dependent processes by deep learning</h3>
                <p>Authors: Xudong ZhengYuecai Han</p>
                <p><a href="http://arxiv.org/abs/2408.09941v1">Link to paper</a></p>
                <p>In this paper we investigate a deep learning method for predictingpath-dependent processes based on discretely observed historical information.This method is implemented by considering the prediction as a nonparametricregression and obtaining the regression function through simulated samples anddeep neural networks. When applying this method to fractional Brownian motionand the solutions of some stochastic differential equations driven by it wetheoretically proved that the L_2 errors converge to 0 and we furtherdiscussed the scope of the method. With the frequency of discrete observationstending to infinity the predictions based on discrete observations converge tothe predictions based on continuous observations which implies that we canmake approximations by the method. We apply the method to the fractionalBrownian motion and the fractional Ornstein-Uhlenbeck process as examples.Comparing the results with the theoretical optimal predictions and taking themean square error as a measure the numerical simulations demonstrate that themethod can generate accurate results. We also analyze the impact of factorssuch as prediction period Hurst index etc. on the accuracy.</p>
                <p>Last Updated: 2024-08-19 12:24:25 UTC</p>
                <button class="interpret-button" data-id="2408.09941v1">Interpret</button>
                <div id="interpretation-2408.09941v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>KAN 2.0: Kolmogorov-Arnold Networks Meet Science</h3>
                <p>Authors: Ziming LiuPingchuan MaYixuan WangWojciech MatusikMax Tegmark</p>
                <p><a href="http://arxiv.org/abs/2408.10205v1">Link to paper</a></p>
                <p>A major challenge of AI  Science lies in their inherent incompatibility:todays AI is primarily based on connectionism while science depends onsymbolism. To bridge the two worlds we propose a framework to seamlesslysynergize Kolmogorov-Arnold Networks KANs and science. The frameworkhighlights KANs usage for three aspects of scientific discovery: identifyingrelevant features revealing modular structures and discovering symbolicformulas. The synergy is bidirectional: science to KAN incorporatingscientific knowledge into KANs and KAN to science extracting scientificinsights from KANs. We highlight major new functionalities in the pykanpackage: 1 MultKAN: KANs with multiplication nodes. 2 kanpiler: a KANcompiler that compiles symbolic formulas into KANs. 3 tree converter: convertKANs or any neural networks to tree graphs. Based on these tools wedemonstrate KANs capability to discover various types of physical lawsincluding conserved quantities Lagrangians symmetries and constitutive laws.</p>
                <p>Last Updated: 2024-08-19 17:59:04 UTC</p>
                <button class="interpret-button" data-id="2408.10205v1">Interpret</button>
                <div id="interpretation-2408.10205v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Demystifying the Communication Characteristics for Distributed Transformer Models</h3>
                <p>Authors: Quentin AnthonyBenjamin MichalowiczJacob HatefLang XuMustafa AbduljabbarAamir ShafiHari SubramoniDhabaleswar Panda</p>
                <p><a href="http://arxiv.org/abs/2408.10197v1">Link to paper</a></p>
                <p>Deep learning DL models based on the transformer architecture haverevolutionized many DL applications such as large language models LLMsvision transformers audio generation and time series prediction. Much of thisprogress has been fueled by distributed training yet distributed communicationremains a substantial bottleneck to training progress. This paper examines thecommunication behavior of transformer models - that is how differentparallelism schemes used in multi-node/multi-GPU DL Training communicate datain the context of transformers. We use GPT-based language models as a casestudy of the transformer architecture due to their ubiquity. We validate theempirical results obtained from our communication logs using analytical models.At a high level our analysis reveals a need to optimize small messagepoint-to-point communication further correlations between sequence lengthper-GPU throughput model size and optimizations used and where topotentially guide further optimizations in framework and HPC middleware designand optimization.</p>
                <p>Last Updated: 2024-08-19 17:54:29 UTC</p>
                <button class="interpret-button" data-id="2408.10197v1">Interpret</button>
                <div id="interpretation-2408.10197v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views</h3>
                <p>Authors: Chao XuAng LiLinghao ChenYulin LiuRuoxi ShiHao SuMinghua Liu</p>
                <p><a href="http://arxiv.org/abs/2408.10195v1">Link to paper</a></p>
                <p>Open-world 3D generation has recently attracted considerable attention. Whilemany single-image-to-3D methods have yielded visually appealing outcomes theyoften lack sufficient controllability and tend to produce hallucinated regionsthat may not align with users expectations. In this paper we explore animportant scenario in which the input consists of one or a few unposed 2Dimages of a single object with little or no overlap. We propose a novelmethod SpaRP to reconstruct a 3D textured mesh and estimate the relativecamera poses for these sparse-view images. SpaRP distills knowledge from 2Ddiffusion models and finetunes them to implicitly deduce the 3D spatialrelationships between the sparse views. The diffusion model is trained tojointly predict surrogate representations for camera poses and multi-viewimages of the object under known poses integrating all information from theinput sparse views. These predictions are then leveraged to accomplish 3Dreconstruction and pose estimation and the reconstructed 3D model can be usedto further refine the camera poses of input views. Through extensiveexperiments on three datasets we demonstrate that our method not onlysignificantly outperforms baseline methods in terms of 3D reconstructionquality and pose prediction accuracy but also exhibits strong efficiency. Itrequires only about 20 seconds to produce a textured mesh and camera poses forthe input views. Project page: https://chaoxu.xyz/sparp.</p>
                <p>Last Updated: 2024-08-19 17:53:10 UTC</p>
                <button class="interpret-button" data-id="2408.10195v1">Interpret</button>
                <div id="interpretation-2408.10195v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models</h3>
                <p>Authors: Aviv BickKevin Y. LiEric P. XingJ. Zico KolterAlbert Gu</p>
                <p><a href="http://arxiv.org/abs/2408.10189v1">Link to paper</a></p>
                <p>Transformer architectures have become a dominant paradigm for domains likelanguage modeling but suffer in many inference settings due to theirquadratic-time self-attention. Recently proposed subquadratic architecturessuch as Mamba have shown promise but have been pretrained with substantiallyless computational resources than the strongest Transformer models. In thiswork we present a method that is able to distill a pretrained Transformerarchitecture into alternative architectures such as state space models SSMs.The key idea to our approach is that we can view both Transformers and SSMs asapplying different forms of mixing matrices over the token sequences. We canthus progressively distill the Transformer architecture by matching differentdegrees of granularity in the SSM: first matching the mixing matricesthemselves then the hidden units at each block and finally the end-to-endpredictions. Our method called MOHAWK is able to distill a Mamba-2 variantbased on the Phi-1.5 architecture Phi-Mamba using only 3B tokens and a hybridversion Hybrid Phi-Mamba using 5B tokens. Despite using less than 1 of thetraining data typically used to train models from scratch Phi-Mamba boastssubstantially stronger performance compared to all past open-sourcenon-Transformer models. MOHAWK allows models like SSMs to leveragecomputational resources invested in training Transformer-based architectureshighlighting a new avenue for building such models.</p>
                <p>Last Updated: 2024-08-19 17:48:11 UTC</p>
                <button class="interpret-button" data-id="2408.10189v1">Interpret</button>
                <div id="interpretation-2408.10189v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Imbalance-Aware Culvert-Sewer Defect Segmentation Using an Enhanced Feature Pyramid Network</h3>
                <p>Authors: Rasha AlshawiMd Meftahul FerdausMahdi AbdelguerfiKendall NilesKen PathakSteve Sloan</p>
                <p><a href="http://arxiv.org/abs/2408.10181v1">Link to paper</a></p>
                <p>Imbalanced datasets are a significant challenge in real-world scenarios. Theylead to models that underperform on underrepresented classes which is acritical issue in infrastructure inspection. This paper introduces the EnhancedFeature Pyramid Network E-FPN a deep learning model for the semanticsegmentation of culverts and sewer pipes within imbalanced datasets. The E-FPNincorporates architectural innovations like sparsely connected blocks anddepth-wise separable convolutions to improve feature extraction and handleobject variations. To address dataset imbalance the model employs strategieslike class decomposition and data augmentation. Experimental results on theculvert-sewer defects dataset and a benchmark aerial semantic segmentationdrone dataset show that the E-FPN outperforms state-of-the-art methodsachieving an average Intersection over Union IoU improvement of 13.8 and27.2 respectively. Additionally class decomposition and data augmentationtogether boost the models performance by approximately 6.9 IoU. The proposedE-FPN presents a promising solution for enhancing object segmentation inchallenging multi-class real-world datasets with potential applicationsextending beyond culvert-sewer defect detection.</p>
                <p>Last Updated: 2024-08-19 17:40:18 UTC</p>
                <button class="interpret-button" data-id="2408.10181v1">Interpret</button>
                <div id="interpretation-2408.10181v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-08-20</p>
        </div>
    
        </div>
    </body>
    </html>
    