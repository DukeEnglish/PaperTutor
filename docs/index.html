
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Linear causal disentanglement via higher-order cumulants</h3>
                <p>Authors: Paula Leyes CarrenoChiara MeroniAnna Seigal</p>
                <p><a href="http://arxiv.org/abs/2407.04605v1">Link to paper</a></p>
                <p>Linear causal disentanglement is a recent method in causal representationlearning to describe a collection of observed variables via latent variableswith causal dependencies between them. It can be viewed as a generalization ofboth independent component analysis and linear structural equation models. Westudy the identifiability of linear causal disentanglement assuming access todata under multiple contexts each given by an intervention on a latentvariable. We show that one perfect intervention on each latent variable issufficient and in the worst case necessary to recover parameters under perfectinterventions generalizing previous work to allow more latent than observedvariables. We give a constructive proof that computes parameters via a coupledtensor decomposition. For soft interventions we find the equivalence class oflatent graphs and parameters that are consistent with observed data via thestudy of a system of polynomial equations. Our results hold assuming theexistence of non-zero higher-order cumulants which implies non-Gaussianity ofvariables.</p>
                <p>Last Updated: 2024-07-05 15:53:16 UTC</p>
                <button class="interpret-button" data-id="2407.04605v1">Interpret</button>
                <div id="interpretation-2407.04605v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Understanding the Gains from Repeated Self-Distillation</h3>
                <p>Authors: Divyansh PareekSimon S. DuSewoong Oh</p>
                <p><a href="http://arxiv.org/abs/2407.04600v1">Link to paper</a></p>
                <p>Self-Distillation is a special type of knowledge distillation where thestudent model has the same architecture as the teacher model. Despite using thesame architecture and the same training data self-distillation has beenempirically observed to improve performance especially when appliedrepeatedly. For such a process there is a fundamental question of interest:How much gain is possible by applying multiple steps of self-distillation Toinvestigate this relative gain we propose studying the simple but canonicaltask of linear regression. Our analysis shows that the excess risk achieved bymulti-step self-distillation can significantly improve upon a single step ofself-distillation reducing the excess risk by a factor as large as d whered is the input dimension. Empirical results on regression tasks from the UCIrepository show a reduction in the learnt models risk MSE by up to 47.</p>
                <p>Last Updated: 2024-07-05 15:48:34 UTC</p>
                <button class="interpret-button" data-id="2407.04600v1">Interpret</button>
                <div id="interpretation-2407.04600v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Speed-accuracy trade-off for the diffusion models: Wisdom from nonequlibrium thermodynamics and optimal transport</h3>
                <p>Authors: Kotaro IkedaTomoya UdaDaisuke OkanoharaSosuke Ito</p>
                <p><a href="http://arxiv.org/abs/2407.04495v1">Link to paper</a></p>
                <p>We discuss a connection between a generative model called the diffusionmodel and nonequilibrium thermodynamics for the Fokker-Planck equation calledstochastic thermodynamics. Based on the techniques of stochasticthermodynamics we derive the speed-accuracy trade-off for the diffusionmodels which is a trade-off relationship between the speed and accuracy ofdata generation in diffusion models. Our result implies that the entropyproduction rate in the forward process affects the errors in data generation.From a stochastic thermodynamic perspective our results provide quantitativeinsight into how best to generate data in diffusion models. The optimallearning protocol is introduced by the conservative force in stochasticthermodynamics and the geodesic of space by the 2-Wasserstein distance inoptimal transport theory. We numerically illustrate the validity of thespeed-accuracy trade-off for the diffusion models with different noiseschedules such as the cosine schedule the conditional optimal transport andthe optimal transport.</p>
                <p>Last Updated: 2024-07-05 13:35:14 UTC</p>
                <button class="interpret-button" data-id="2407.04495v1">Interpret</button>
                <div id="interpretation-2407.04495v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Machine Learning for Complex Systems with Abnormal Pattern by Exception Maximization Outlier Detection Method</h3>
                <p>Authors: Zhikun ZhangYiting DuanXiangjun WangMingyuan Zhang</p>
                <p><a href="http://arxiv.org/abs/2407.04248v1">Link to paper</a></p>
                <p>This paper proposes a novel fast online methodology for outlier detectioncalled the exception maximization outlier detection methodEMODM whichemploys probabilistic models and statistical algorithms to detect abnormalpatterns from the outputs of complex systems. The EMODM is based on a two-stateGaussian mixture model and demonstrates strong performance in probabilityanomaly detection working on real-time raw data rather than using special priordistribution information. We confirm this using the synthetic data from twonumerical cases. For the real-world data we have detected the short circuitpattern of the circuit system using EMODM by the current and voltage output ofa three-phase inverter. The EMODM also found an abnormal period due to COVID-19in the insured unemployment data of 53 regions in the United States from 2000to 2024. The application of EMODM to these two real-life datasets demonstratedthe effectiveness and accuracy of our algorithm.</p>
                <p>Last Updated: 2024-07-05 04:30:41 UTC</p>
                <button class="interpret-button" data-id="2407.04248v1">Interpret</button>
                <div id="interpretation-2407.04248v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Quantifying Prediction Consistency Under Model Multiplicity in Tabular LLMs</h3>
                <p>Authors: Faisal HammanPasan DissanayakeSaumitra MishraFreddy LecueSanghamitra Dutta</p>
                <p><a href="http://arxiv.org/abs/2407.04173v1">Link to paper</a></p>
                <p>Fine-tuning large language models LLMs on limited tabular data forclassification tasks can lead to textitfine-tuning multiplicity whereequally well-performing models make conflicting predictions on the same inputsdue to variations in the training process i.e. seed random weightinitialization retraining on additional or deleted samples. This raisescritical concerns about the robustness and reliability of Tabular LLMsparticularly when deployed for high-stakes decision-making such as financehiring education healthcare etc. This work formalizes the challenge offine-tuning multiplicity in Tabular LLMs and proposes a novel metric toquantify the robustness of individual predictions without expensive modelretraining. Our metric quantifies a predictions stability by analyzingsampling the models local behavior around the input in the embedding space.Interestingly we show that sampling in the local neighborhood can be leveragedto provide probabilistic robustness guarantees against a broad class offine-tuned models. By leveraging Bernsteins Inequality we show thatpredictions with sufficiently high robustness as defined by our measure willremain consistent with high probability. We also provide empirical evaluationon real-world datasets to support our theoretical results. Our work highlightsthe importance of addressing fine-tuning instabilities to enable trustworthydeployment of LLMs in high-stakes and safety-critical applications.</p>
                <p>Last Updated: 2024-07-04 22:22:09 UTC</p>
                <button class="interpret-button" data-id="2407.04173v1">Interpret</button>
                <div id="interpretation-2407.04173v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Gamification of Motor Imagery Brain-Computer Interface Training Protocols: a systematic review</h3>
                <p>Authors: Fred AtillaMarie PostmaMaryam Alimardani</p>
                <p><a href="http://arxiv.org/abs/2407.04610v1">Link to paper</a></p>
                <p>Current Motor Imagery Brain-Computer Interfaces MI-BCI require a lengthyand monotonous training procedure to train both the system and the user.Considering many users struggle with effective control of MI-BCI systems amore user-centered approach to training might help motivate users andfacilitate learning alleviating inefficiency of the BCI system. With theincrease of BCI-controlled games researchers have suggested using gameprinciples for BCI training as games are naturally centered on the player.This review identifies and evaluates the application of game design elements toMI-BCI training a process known as gamification. Through a systematicliterature search we examined how MI-BCI training protocols have been gamifiedand how specific game elements impacted the training outcomes. We identified 86studies that employed gamified MI-BCI protocols in the past decade. Theprevalence and reported effects of individual game elements on user experienceand performance were extracted and synthesized. Results reveal that MI-BCItraining protocols are most often gamified by having users move an avatar in avirtual environment that provides visual feedback. Furthermore in thesevirtual environments users were provided with goals that guided their actions.Using gamification the reviewed protocols allowed users to reach effectiveMI-BCI control with studies reporting positive effects of four individualelements on user performance and experience namely: feedback avatarsassistance and social interaction. Based on these elements this review makescurrent and future recommendations for effective gamification such as the useof virtual reality and adaptation of game difficulty to user skill level.</p>
                <p>Last Updated: 2024-07-05 16:03:58 UTC</p>
                <button class="interpret-button" data-id="2407.04610v1">Interpret</button>
                <div id="interpretation-2407.04610v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Enabling On-Device LLMs Personalization with Smartphone Sensing</h3>
                <p>Authors: Shiquan ZhangYing MaLe FangHong JiaSimon D'AlfonsoVassilis Kostakos</p>
                <p><a href="http://arxiv.org/abs/2407.04418v1">Link to paper</a></p>
                <p>This demo presents a novel end-to-end framework that combines on-device largelanguage models LLMs with smartphone sensing technologies to achievecontext-aware and personalized services. The framework addresses criticallimitations of current personalization solutions via cloud-based LLMs such asprivacy concerns latency and cost and limited personal sensor data. Toachieve this we innovatively proposed deploying LLMs on smartphones withmultimodal sensor data and customized prompt engineering ensuring privacy andenhancing personalization performance through context-aware sensing. A casestudy involving a university student demonstrated the proposed frameworkscapability to provide tailored recommendations. In addition we show that theproposed framework achieves the best trade-off in privacy performancelatency cost battery and energy consumption between on-device and cloud LLMs.Future work aims to integrate more diverse sensor data and conduct large-scaleuser studies to further refine the personalization. We envision the proposedframework could significantly improve user experiences in various domains suchas healthcare productivity and entertainment by providing securecontext-aware and efficient interactions directly on users devices.</p>
                <p>Last Updated: 2024-07-05 11:09:05 UTC</p>
                <button class="interpret-button" data-id="2407.04418v1">Interpret</button>
                <div id="interpretation-2407.04418v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials</h3>
                <p>Authors: Shuoyang ZhengAnna Xambó SedóNick Bryan-Kinns</p>
                <p><a href="http://arxiv.org/abs/2407.04379v1">Link to paper</a></p>
                <p>This paper presents a mapping strategy for interacting with the latent spacesof generative AI models. Our approach involves using unsupervised featurelearning to encode a human control space and mapping it to an audio synthesismodels latent space. To demonstrate how this mapping strategy can turnhigh-dimensional sensor data into control mechanisms of a deep generativemodel we present a proof-of-concept system that uses visual sketches tocontrol an audio synthesis model. We draw on emerging discourses in XAIxArts todiscuss how this approach can contribute to XAI in artistic and creativecontexts we also discuss its current limitations and propose future researchdirections.</p>
                <p>Last Updated: 2024-07-05 09:32:44 UTC</p>
                <button class="interpret-button" data-id="2407.04379v1">Interpret</button>
                <div id="interpretation-2407.04379v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Towards Context-aware Support for Color Vision Deficiency: An Approach Integrating LLM and AR</h3>
                <p>Authors: Shogo MoritaYan ZhangTakuto YamauchiSinan ChenJialong LiKenji Tei</p>
                <p><a href="http://arxiv.org/abs/2407.04362v1">Link to paper</a></p>
                <p>People with color vision deficiency often face challenges in distinguishingcolors such as red and green which can complicate daily tasks and require theuse of assistive tools or environmental adjustments. Current support toolsmainly focus on presentation-based aids like the color vision modes found iniPhone accessibility settings. However offering context-aware support likeindicating the doneness of meat remains a challenge since task-specificsolutions are not cost-effective for all possible scenarios. To address thisour paper proposes an application that provides contextual and autonomousassistance. This application is mainly composed of: i an augmented realityinterface that efficiently captures context and ii a multi-modal largelanguage model-based reasoner that serves to cognitize the context and thenreason about the appropriate support contents. Preliminary user experimentswith two color vision deficient users across five different scenarios havedemonstrated the effectiveness and universality of our application.</p>
                <p>Last Updated: 2024-07-05 09:03:52 UTC</p>
                <button class="interpret-button" data-id="2407.04362v1">Interpret</button>
                <div id="interpretation-2407.04362v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>UpStory: the Uppsala Storytelling dataset</h3>
                <p>Authors: Marc FraileNatalia Calvo-BarajasAnastasia Sophia ApeironGiovanna VarniJoakim LindbladNataša SladojeGinevra Castellano</p>
                <p><a href="http://arxiv.org/abs/2407.04352v1">Link to paper</a></p>
                <p>Friendship and rapport play an important role in the formation ofconstructive social interactions and have been widely studied in educationalsettings due to their impact on student outcomes. Given the growing interest inautomating the analysis of such phenomena through Machine Learning ML accessto annotated interaction datasets is highly valuable. However no dataset ondyadic child-child interactions explicitly capturing rapport currently exists.Moreover despite advances in the automatic analysis of human behaviour noprevious work has addressed the prediction of rapport in child-child dyadicinteractions in educational settings. We present UpStory -- the UppsalaStorytelling dataset: a novel dataset of naturalistic dyadic interactionsbetween primary school aged children with an experimental manipulation ofrapport. Pairs of children aged 8-10 participate in a task-oriented activity:designing a story together while being allowed free movement within the playarea. We promote balanced collection of different levels of rapport by using awithin-subjects design: self-reported friendships are used to pair each childtwice either minimizing or maximizing pair separation in the friendshipnetwork. The dataset contains data for 35 pairs totalling 3h 40m of audio andvideo recordings. It includes two video sources covering the play area as wellas separate voice recordings for each child. An anonymized version of thedataset is made publicly available containing per-frame head pose body poseand face features as well as per-pair information including the level ofrapport. Finally we provide ML baselines for the prediction of rapport.</p>
                <p>Last Updated: 2024-07-05 08:46:16 UTC</p>
                <button class="interpret-button" data-id="2407.04352v1">Interpret</button>
                <div id="interpretation-2407.04352v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>LaRa: Efficient Large-Baseline Radiance Fields</h3>
                <p>Authors: Anpei ChenHaofei XuStefano EspositoSiyu TangAndreas Geiger</p>
                <p><a href="http://arxiv.org/abs/2407.04699v1">Link to paper</a></p>
                <p>Radiance field methods have achieved photorealistic novel view synthesis andgeometry reconstruction. But they are mostly applied in per-scene optimizationor small-baseline settings. While several recent works investigate feed-forwardreconstruction with large baselines by utilizing transformers they all operatewith a standard global attention mechanism and hence ignore the local nature of3D reconstruction. We propose a method that unifies local and global reasoningin transformer layers resulting in improved quality and faster convergence.Our model represents scenes as Gaussian Volumes and combines this with an imageencoder and Group Attention Layers for efficient feed-forward reconstruction.Experimental results demonstrate that our model trained for two days on fourGPUs demonstrates high fidelity in reconstructing 360deg radiance fields androbustness to zero-shot and out-of-domain testing.</p>
                <p>Last Updated: 2024-07-05 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2407.04699v1">Interpret</button>
                <div id="interpretation-2407.04699v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VCoME: Verbal Video Composition with Multimodal Editing Effects</h3>
                <p>Authors: Weibo GongXiaojie JinXin LiDongliang HeXinglong Wu</p>
                <p><a href="http://arxiv.org/abs/2407.04697v1">Link to paper</a></p>
                <p>Verbal videos featuring voice-overs or text overlays provide valuablecontent but present significant challenges in composition especially whenincorporating editing effects to enhance clarity and visual appeal. In thispaper we introduce the novel task of verbal video composition with editingeffects. This task aims to generate coherent and visually appealing verbalvideos by integrating multimodal editing effects across textual visual andaudio categories. To achieve this we curate a large-scale dataset of videoeffects compositions from publicly available sources. We then formulate thistask as a generative problem involving the identification of appropriatepositions in the verbal content and the recommendation of editing effects forthese positions. To address this task we propose VCoME a general frameworkthat employs a large multimodal model to generate editing effects for videocomposition. Specifically VCoME takes in the multimodal video context andautoregressively outputs where to apply effects within the verbal content andwhich effects are most appropriate for each position. VCoME also supportsprompt-based control of composition density and style providing substantialflexibility for diverse applications. Through extensive quantitative andqualitative evaluations we clearly demonstrate the effectiveness of VCoME. Acomprehensive user study shows that our method produces videos of professionalquality while being 85times more efficient than professional editors.</p>
                <p>Last Updated: 2024-07-05 17:59:02 UTC</p>
                <button class="interpret-button" data-id="2407.04697v1">Interpret</button>
                <div id="interpretation-2407.04697v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation</h3>
                <p>Authors: Yuxuan KuangJunjie YeHaoran GengJiageng MaoCongyue DengLeonidas GuibasHe WangYue Wang</p>
                <p><a href="http://arxiv.org/abs/2407.04689v1">Link to paper</a></p>
                <p>This work proposes a retrieve-and-transfer framework for zero-shot roboticmanipulation dubbed RAM featuring generalizability across various objectsenvironments and embodiments. Unlike existing approaches that learnmanipulation from expensive in-domain demonstrations RAM capitalizes on aretrieval-based affordance transfer paradigm to acquire versatile manipulationcapabilities from abundant out-of-domain data. First RAM extracts unifiedaffordance at scale from diverse sources of demonstrations including roboticdata human-object interaction HOI data and custom data to construct acomprehensive affordance memory. Then given a language instruction RAMhierarchically retrieves the most similar demonstration from the affordancememory and transfers such out-of-domain 2D affordance to in-domain 3Dexecutable affordance in a zero-shot and embodiment-agnostic manner. Extensivesimulation and real-world evaluations demonstrate that our RAM consistentlyoutperforms existing works in diverse daily tasks. Additionally RAM showssignificant potential for downstream applications such as automatic andefficient data collection one-shot visual imitation and LLM/VLM-integratedlong-horizon manipulation. For more details please check our website athttps://yxkryptonite.github.io/RAM/.</p>
                <p>Last Updated: 2024-07-05 17:50:38 UTC</p>
                <button class="interpret-button" data-id="2407.04689v1">Interpret</button>
                <div id="interpretation-2407.04689v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Enhancing Vehicle Re-identification and Matching for Weaving Analysis</h3>
                <p>Authors: Mei QiuWei LinStanley ChienLauren ChristopherYaobin ChenShu Hu</p>
                <p><a href="http://arxiv.org/abs/2407.04688v1">Link to paper</a></p>
                <p>Vehicle weaving on highways contributes to traffic congestion raises safetyissues and underscores the need for sophisticated traffic management systems.Current tools are inadequate in offering precise and comprehensive data onlane-specific weaving patterns. This paper introduces an innovative method forcollecting non-overlapping video data in weaving zones enabling the generationof quantitative insights into lane-specific weaving behaviors. Our experimentalresults confirm the efficacy of this approach delivering critical data thatcan assist transportation authorities in enhancing traffic control and roadwayinfrastructure.</p>
                <p>Last Updated: 2024-07-05 17:50:35 UTC</p>
                <button class="interpret-button" data-id="2407.04688v1">Interpret</button>
                <div id="interpretation-2407.04688v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Embracing Massive Medical Data</h3>
                <p>Authors: Yu-Cheng ChouZongwei ZhouAlan Yuille</p>
                <p><a href="http://arxiv.org/abs/2407.04687v1">Link to paper</a></p>
                <p>As massive medical data become available with an increasing number of scansexpanding classes and varying sources prevalent training paradigms -- whereAI is trained with multiple passes over fixed finite datasets -- facesignificant challenges. First training AI all at once on such massive data isimpractical as new scans/sources/classes continuously arrive. Second trainingAI continuously on new scans/sources/classes can lead to catastrophicforgetting where AI forgets old data as it learns new data and vice versa. Toaddress these two challenges we propose an online learning method that enablestraining AI from massive medical data. Instead of repeatedly training AI onrandomly selected data samples our method identifies the most significantsamples for the current AI model based on their data uniqueness and predictionuncertainty then trains the AI on these selective data samples. Compared withprevalent training paradigms our method not only improves data efficiency byenabling training on continual data streams but also mitigates catastrophicforgetting by selectively training AI on significant data samples that mightotherwise be forgotten outperforming by 15 in Dice score for multi-organ andtumor segmentation.  The code is available at https://github.com/MrGiovanni/OnlineLearning</p>
                <p>Last Updated: 2024-07-05 17:50:30 UTC</p>
                <button class="interpret-button" data-id="2407.04687v1">Interpret</button>
                <div id="interpretation-2407.04687v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>LaRa: Efficient Large-Baseline Radiance Fields</h3>
                <p>Authors: Anpei ChenHaofei XuStefano EspositoSiyu TangAndreas Geiger</p>
                <p><a href="http://arxiv.org/abs/2407.04699v1">Link to paper</a></p>
                <p>Radiance field methods have achieved photorealistic novel view synthesis andgeometry reconstruction. But they are mostly applied in per-scene optimizationor small-baseline settings. While several recent works investigate feed-forwardreconstruction with large baselines by utilizing transformers they all operatewith a standard global attention mechanism and hence ignore the local nature of3D reconstruction. We propose a method that unifies local and global reasoningin transformer layers resulting in improved quality and faster convergence.Our model represents scenes as Gaussian Volumes and combines this with an imageencoder and Group Attention Layers for efficient feed-forward reconstruction.Experimental results demonstrate that our model trained for two days on fourGPUs demonstrates high fidelity in reconstructing 360deg radiance fields androbustness to zero-shot and out-of-domain testing.</p>
                <p>Last Updated: 2024-07-05 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2407.04699v1">Interpret</button>
                <div id="interpretation-2407.04699v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs</h3>
                <p>Authors: Rudolf LaineBilal ChughtaiJan BetleyKaivalya HariharanJeremy ScheurerMikita BalesniMarius HobbhahnAlexander MeinkeOwain Evans</p>
                <p><a href="http://arxiv.org/abs/2407.04694v1">Link to paper</a></p>
                <p>AI assistants such as ChatGPT are trained to respond to users by saying Iam a large language model. This raises questions. Do such models know thatthey are LLMs and reliably act on this knowledge Are they aware of theircurrent circumstances such as being deployed to the public We refer to amodels knowledge of itself and its circumstances as situational awareness. Toquantify situational awareness in LLMs we introduce a range of behavioraltests based on question answering and instruction following. These tests formthe textbfSituational Awareness Dataset SAD a benchmark comprising 7task categories and over 13000 questions. The benchmark tests numerousabilities including the capacity of LLMs to i recognize their own generatedtext ii predict their own behavior iii determine whether a prompt is frominternal evaluation or real-world deployment and iv follow instructions thatdepend on self-knowledge.  We evaluate 16 LLMs on SAD including both base pretrained and chat models.While all models perform better than chance even the highest-scoring modelClaude 3 Opus is far from a human baseline on certain tasks. We also observethat performance on SAD is only partially predicted by metrics of generalknowledge e.g. MMLU. Chat models which are finetuned to serve as AIassistants outperform their corresponding base models on SAD but not ongeneral knowledge tasks. The purpose of SAD is to facilitate scientificunderstanding of situational awareness in LLMs by breaking it down intoquantitative abilities. Situational awareness is important because it enhancesa models capacity for autonomous planning and action. While this has potentialbenefits for automation it also introduces novel risks related to AI safetyand control. Code and latest results available athttps://situational-awareness-dataset.org .</p>
                <p>Last Updated: 2024-07-05 17:57:02 UTC</p>
                <button class="interpret-button" data-id="2407.04694v1">Interpret</button>
                <div id="interpretation-2407.04694v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models</h3>
                <p>Authors: Yuzhe GuZiwei JiWenwei ZhangChengqi LyuDahua LinKai Chen</p>
                <p><a href="http://arxiv.org/abs/2407.04693v1">Link to paper</a></p>
                <p>Large language models LLMs exhibit hallucinations in long-formquestion-answering tasks across various domains and wide applications. Currenthallucination detection and mitigation datasets are limited in domains andsizes which struggle to scale due to prohibitive labor costs and insufficientreliability of existing hallucination annotators. To facilitate the scalableoversight of LLM hallucinations this paper introduces an iterativeself-training framework that simultaneously and progressively scales up thehallucination annotation dataset and improves the accuracy of the hallucinationannotator. Based on the Expectation Maximization EM algorithm in eachiteration the framework first applies a hallucination annotation pipeline toannotate a scaled dataset and then trains a more accurate hallucinationannotator on the dataset. This new hallucination annotator is adopted in thehallucination annotation pipeline used for the next iteration. Extensiveexperimental results demonstrate that the finally obtained hallucinationannotator with only 7B parameters surpasses the performance of GPT-4 andobtains new state-of-the-art hallucination detection results on HaluEval andHalluQA by zero-shot inference. Such an annotator can not only evaluate thehallucination levels of various LLMs on the large-scale dataset but also helpto mitigate the hallucination of LLMs generations with the Natural LanguageInference NLI metric increasing from 25 to 37 on HaluEval.</p>
                <p>Last Updated: 2024-07-05 17:56:38 UTC</p>
                <button class="interpret-button" data-id="2407.04693v1">Interpret</button>
                <div id="interpretation-2407.04693v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge</h3>
                <p>Authors: Yuanze LinYunsheng LiDongdong ChenWeijian XuRonald ClarkPhilip TorrLu Yuan</p>
                <p><a href="http://arxiv.org/abs/2407.04681v1">Link to paper</a></p>
                <p>In recent years multimodal large language models MLLMs have madesignificant strides by training on vast high-quality image-text datasetsenabling them to generally understand images well. However the inherentdifficulty in explicitly conveying fine-grained or spatially dense informationin text such as masks poses a challenge for MLLMs limiting their ability toanswer questions requiring an understanding of detailed or localized visualelements. Drawing inspiration from the Retrieval-Augmented Generation RAGconcept this paper proposes a new visual prompt approach to integratefine-grained external knowledge gleaned from specialized vision models e.g.instance segmentation/OCR models into MLLMs. This is a promising yetunderexplored direction for enhancing MLLMs performance. Our approach divergesfrom concurrent works which transform external knowledge into additional textprompts necessitating the model to indirectly learn the correspondence betweenvisual content and text coordinates. Instead we propose embedding fine-grainedknowledge information directly into a spatial embedding map as a visual prompt.This design can be effortlessly incorporated into various MLLMs such as LLaVAand Mipha considerably improving their visual understanding performance.Through rigorous experiments we demonstrate that our method can enhance MLLMperformance across nine benchmarks amplifying their fine-grained context-awarecapabilities.</p>
                <p>Last Updated: 2024-07-05 17:43:30 UTC</p>
                <button class="interpret-button" data-id="2407.04681v1">Interpret</button>
                <div id="interpretation-2407.04681v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Lost in Translation: The Algorithmic Gap Between LMs and the Brain</h3>
                <p>Authors: Tommaso TosatoPascal Jr Tikeng NotsawoSaskia HelblingIrina RishGuillaume Dumas</p>
                <p><a href="http://arxiv.org/abs/2407.04680v1">Link to paper</a></p>
                <p>Language Models LMs have achieved impressive performance on variouslinguistic tasks but their relationship to human language processing in thebrain remains unclear. This paper examines the gaps and overlaps between LMsand the brain at different levels of analysis emphasizing the importance oflooking beyond input-output behavior to examine and compare the internalprocesses of these systems. We discuss how insights from neuroscience such assparsity modularity internal states and interactive learning can inform thedevelopment of more biologically plausible language models. Furthermore weexplore the role of scaling laws in bridging the gap between LMs and humancognition highlighting the need for efficiency constraints analogous to thosein biological systems. By developing LMs that more closely mimic brainfunction we aim to advance both artificial intelligence and our understandingof human cognition.</p>
                <p>Last Updated: 2024-07-05 17:43:16 UTC</p>
                <button class="interpret-button" data-id="2407.04680v1">Interpret</button>
                <div id="interpretation-2407.04680v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs</h3>
                <p>Authors: Rudolf LaineBilal ChughtaiJan BetleyKaivalya HariharanJeremy ScheurerMikita BalesniMarius HobbhahnAlexander MeinkeOwain Evans</p>
                <p><a href="http://arxiv.org/abs/2407.04694v1">Link to paper</a></p>
                <p>AI assistants such as ChatGPT are trained to respond to users by saying Iam a large language model. This raises questions. Do such models know thatthey are LLMs and reliably act on this knowledge Are they aware of theircurrent circumstances such as being deployed to the public We refer to amodels knowledge of itself and its circumstances as situational awareness. Toquantify situational awareness in LLMs we introduce a range of behavioraltests based on question answering and instruction following. These tests formthe textbfSituational Awareness Dataset SAD a benchmark comprising 7task categories and over 13000 questions. The benchmark tests numerousabilities including the capacity of LLMs to i recognize their own generatedtext ii predict their own behavior iii determine whether a prompt is frominternal evaluation or real-world deployment and iv follow instructions thatdepend on self-knowledge.  We evaluate 16 LLMs on SAD including both base pretrained and chat models.While all models perform better than chance even the highest-scoring modelClaude 3 Opus is far from a human baseline on certain tasks. We also observethat performance on SAD is only partially predicted by metrics of generalknowledge e.g. MMLU. Chat models which are finetuned to serve as AIassistants outperform their corresponding base models on SAD but not ongeneral knowledge tasks. The purpose of SAD is to facilitate scientificunderstanding of situational awareness in LLMs by breaking it down intoquantitative abilities. Situational awareness is important because it enhancesa models capacity for autonomous planning and action. While this has potentialbenefits for automation it also introduces novel risks related to AI safetyand control. Code and latest results available athttps://situational-awareness-dataset.org .</p>
                <p>Last Updated: 2024-07-05 17:57:02 UTC</p>
                <button class="interpret-button" data-id="2407.04694v1">Interpret</button>
                <div id="interpretation-2407.04694v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models</h3>
                <p>Authors: Yuzhe GuZiwei JiWenwei ZhangChengqi LyuDahua LinKai Chen</p>
                <p><a href="http://arxiv.org/abs/2407.04693v1">Link to paper</a></p>
                <p>Large language models LLMs exhibit hallucinations in long-formquestion-answering tasks across various domains and wide applications. Currenthallucination detection and mitigation datasets are limited in domains andsizes which struggle to scale due to prohibitive labor costs and insufficientreliability of existing hallucination annotators. To facilitate the scalableoversight of LLM hallucinations this paper introduces an iterativeself-training framework that simultaneously and progressively scales up thehallucination annotation dataset and improves the accuracy of the hallucinationannotator. Based on the Expectation Maximization EM algorithm in eachiteration the framework first applies a hallucination annotation pipeline toannotate a scaled dataset and then trains a more accurate hallucinationannotator on the dataset. This new hallucination annotator is adopted in thehallucination annotation pipeline used for the next iteration. Extensiveexperimental results demonstrate that the finally obtained hallucinationannotator with only 7B parameters surpasses the performance of GPT-4 andobtains new state-of-the-art hallucination detection results on HaluEval andHalluQA by zero-shot inference. Such an annotator can not only evaluate thehallucination levels of various LLMs on the large-scale dataset but also helpto mitigate the hallucination of LLMs generations with the Natural LanguageInference NLI metric increasing from 25 to 37 on HaluEval.</p>
                <p>Last Updated: 2024-07-05 17:56:38 UTC</p>
                <button class="interpret-button" data-id="2407.04693v1">Interpret</button>
                <div id="interpretation-2407.04693v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for Interpreting Neural Networks</h3>
                <p>Authors: Aaron Mueller</p>
                <p><a href="http://arxiv.org/abs/2407.04690v1">Link to paper</a></p>
                <p>Interpretability research takes counterfactual theories of causality forgranted. Most causal methods rely on counterfactual interventions to inputs orthe activations of particular model components followed by observations of thechange in models output logits or behaviors. While this yields more faithfulevidence than correlational methods counterfactuals nonetheless have keyproblems that bias our findings in specific and predictable ways. Specificallyi counterfactual theories do not effectively capture multiple independentlysufficient causes of the same effect which leads us to miss certain causesentirely and ii counterfactual dependencies in neural networks are generallynot transitive which complicates methods for extracting and interpretingcausal graphs from neural networks. We discuss the implications of thesechallenges for interpretability researchers and propose concrete suggestionsfor future work.</p>
                <p>Last Updated: 2024-07-05 17:53:03 UTC</p>
                <button class="interpret-button" data-id="2407.04690v1">Interpret</button>
                <div id="interpretation-2407.04690v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge</h3>
                <p>Authors: Yuanze LinYunsheng LiDongdong ChenWeijian XuRonald ClarkPhilip TorrLu Yuan</p>
                <p><a href="http://arxiv.org/abs/2407.04681v1">Link to paper</a></p>
                <p>In recent years multimodal large language models MLLMs have madesignificant strides by training on vast high-quality image-text datasetsenabling them to generally understand images well. However the inherentdifficulty in explicitly conveying fine-grained or spatially dense informationin text such as masks poses a challenge for MLLMs limiting their ability toanswer questions requiring an understanding of detailed or localized visualelements. Drawing inspiration from the Retrieval-Augmented Generation RAGconcept this paper proposes a new visual prompt approach to integratefine-grained external knowledge gleaned from specialized vision models e.g.instance segmentation/OCR models into MLLMs. This is a promising yetunderexplored direction for enhancing MLLMs performance. Our approach divergesfrom concurrent works which transform external knowledge into additional textprompts necessitating the model to indirectly learn the correspondence betweenvisual content and text coordinates. Instead we propose embedding fine-grainedknowledge information directly into a spatial embedding map as a visual prompt.This design can be effortlessly incorporated into various MLLMs such as LLaVAand Mipha considerably improving their visual understanding performance.Through rigorous experiments we demonstrate that our method can enhance MLLMperformance across nine benchmarks amplifying their fine-grained context-awarecapabilities.</p>
                <p>Last Updated: 2024-07-05 17:43:30 UTC</p>
                <button class="interpret-button" data-id="2407.04681v1">Interpret</button>
                <div id="interpretation-2407.04681v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Lost in Translation: The Algorithmic Gap Between LMs and the Brain</h3>
                <p>Authors: Tommaso TosatoPascal Jr Tikeng NotsawoSaskia HelblingIrina RishGuillaume Dumas</p>
                <p><a href="http://arxiv.org/abs/2407.04680v1">Link to paper</a></p>
                <p>Language Models LMs have achieved impressive performance on variouslinguistic tasks but their relationship to human language processing in thebrain remains unclear. This paper examines the gaps and overlaps between LMsand the brain at different levels of analysis emphasizing the importance oflooking beyond input-output behavior to examine and compare the internalprocesses of these systems. We discuss how insights from neuroscience such assparsity modularity internal states and interactive learning can inform thedevelopment of more biologically plausible language models. Furthermore weexplore the role of scaling laws in bridging the gap between LMs and humancognition highlighting the need for efficiency constraints analogous to thosein biological systems. By developing LMs that more closely mimic brainfunction we aim to advance both artificial intelligence and our understandingof human cognition.</p>
                <p>Last Updated: 2024-07-05 17:43:16 UTC</p>
                <button class="interpret-button" data-id="2407.04680v1">Interpret</button>
                <div id="interpretation-2407.04680v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs</h3>
                <p>Authors: Rudolf LaineBilal ChughtaiJan BetleyKaivalya HariharanJeremy ScheurerMikita BalesniMarius HobbhahnAlexander MeinkeOwain Evans</p>
                <p><a href="http://arxiv.org/abs/2407.04694v1">Link to paper</a></p>
                <p>AI assistants such as ChatGPT are trained to respond to users by saying Iam a large language model. This raises questions. Do such models know thatthey are LLMs and reliably act on this knowledge Are they aware of theircurrent circumstances such as being deployed to the public We refer to amodels knowledge of itself and its circumstances as situational awareness. Toquantify situational awareness in LLMs we introduce a range of behavioraltests based on question answering and instruction following. These tests formthe textbfSituational Awareness Dataset SAD a benchmark comprising 7task categories and over 13000 questions. The benchmark tests numerousabilities including the capacity of LLMs to i recognize their own generatedtext ii predict their own behavior iii determine whether a prompt is frominternal evaluation or real-world deployment and iv follow instructions thatdepend on self-knowledge.  We evaluate 16 LLMs on SAD including both base pretrained and chat models.While all models perform better than chance even the highest-scoring modelClaude 3 Opus is far from a human baseline on certain tasks. We also observethat performance on SAD is only partially predicted by metrics of generalknowledge e.g. MMLU. Chat models which are finetuned to serve as AIassistants outperform their corresponding base models on SAD but not ongeneral knowledge tasks. The purpose of SAD is to facilitate scientificunderstanding of situational awareness in LLMs by breaking it down intoquantitative abilities. Situational awareness is important because it enhancesa models capacity for autonomous planning and action. While this has potentialbenefits for automation it also introduces novel risks related to AI safetyand control. Code and latest results available athttps://situational-awareness-dataset.org .</p>
                <p>Last Updated: 2024-07-05 17:57:02 UTC</p>
                <button class="interpret-button" data-id="2407.04694v1">Interpret</button>
                <div id="interpretation-2407.04694v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for Interpreting Neural Networks</h3>
                <p>Authors: Aaron Mueller</p>
                <p><a href="http://arxiv.org/abs/2407.04690v1">Link to paper</a></p>
                <p>Interpretability research takes counterfactual theories of causality forgranted. Most causal methods rely on counterfactual interventions to inputs orthe activations of particular model components followed by observations of thechange in models output logits or behaviors. While this yields more faithfulevidence than correlational methods counterfactuals nonetheless have keyproblems that bias our findings in specific and predictable ways. Specificallyi counterfactual theories do not effectively capture multiple independentlysufficient causes of the same effect which leads us to miss certain causesentirely and ii counterfactual dependencies in neural networks are generallynot transitive which complicates methods for extracting and interpretingcausal graphs from neural networks. We discuss the implications of thesechallenges for interpretability researchers and propose concrete suggestionsfor future work.</p>
                <p>Last Updated: 2024-07-05 17:53:03 UTC</p>
                <button class="interpret-button" data-id="2407.04690v1">Interpret</button>
                <div id="interpretation-2407.04690v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge</h3>
                <p>Authors: Yuanze LinYunsheng LiDongdong ChenWeijian XuRonald ClarkPhilip TorrLu Yuan</p>
                <p><a href="http://arxiv.org/abs/2407.04681v1">Link to paper</a></p>
                <p>In recent years multimodal large language models MLLMs have madesignificant strides by training on vast high-quality image-text datasetsenabling them to generally understand images well. However the inherentdifficulty in explicitly conveying fine-grained or spatially dense informationin text such as masks poses a challenge for MLLMs limiting their ability toanswer questions requiring an understanding of detailed or localized visualelements. Drawing inspiration from the Retrieval-Augmented Generation RAGconcept this paper proposes a new visual prompt approach to integratefine-grained external knowledge gleaned from specialized vision models e.g.instance segmentation/OCR models into MLLMs. This is a promising yetunderexplored direction for enhancing MLLMs performance. Our approach divergesfrom concurrent works which transform external knowledge into additional textprompts necessitating the model to indirectly learn the correspondence betweenvisual content and text coordinates. Instead we propose embedding fine-grainedknowledge information directly into a spatial embedding map as a visual prompt.This design can be effortlessly incorporated into various MLLMs such as LLaVAand Mipha considerably improving their visual understanding performance.Through rigorous experiments we demonstrate that our method can enhance MLLMperformance across nine benchmarks amplifying their fine-grained context-awarecapabilities.</p>
                <p>Last Updated: 2024-07-05 17:43:30 UTC</p>
                <button class="interpret-button" data-id="2407.04681v1">Interpret</button>
                <div id="interpretation-2407.04681v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The diameter of a stochastic matrix: A new measure for sensitivity analysis in Bayesian networks</h3>
                <p>Authors: Manuele LeonelliJim Q. SmithSophia K. Wright</p>
                <p><a href="http://arxiv.org/abs/2407.04667v1">Link to paper</a></p>
                <p>Bayesian networks are one of the most widely used classes of probabilisticmodels for risk management and decision support because of theirinterpretability and flexibility in including heterogeneous pieces ofinformation. In any applied modelling it is critical to assess how robust theinferences on certain target variables are to changes in the model. In Bayesiannetworks these analyses fall under the umbrella of sensitivity analysis whichis most commonly carried out by quantifying dissimilarities usingKullback-Leibler information measures. In this paper we argue that robustnessmethods based instead on the familiar total variation distance provide simpleand more valuable bounds on robustness to misspecification which are bothformally justifiable and transparent. We introduce a novel measure ofdependence in conditional probability tables called the diameter to derive suchbounds. This measure quantifies the strength of dependence between a variableand its parents. We demonstrate how such formal robustness considerations canbe embedded in building a Bayesian network.</p>
                <p>Last Updated: 2024-07-05 17:22:12 UTC</p>
                <button class="interpret-button" data-id="2407.04667v1">Interpret</button>
                <div id="interpretation-2407.04667v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Unsupervised 4D Cardiac Motion Tracking with Spatiotemporal Optical Flow Networks</h3>
                <p>Authors: Long TengWei FengMenglong ZhuXinchao Li</p>
                <p><a href="http://arxiv.org/abs/2407.04663v1">Link to paper</a></p>
                <p>Cardiac motion tracking from echocardiography can be used to estimate andquantify myocardial motion within a cardiac cycle. It is a cost-efficient andeffective approach for assessing myocardial function. However ultrasoundimaging has the inherent characteristics of spatially low resolution andtemporally random noise which leads to difficulties in obtaining reliableannotation. Thus it is difficult to perform supervised learning for motiontracking. In addition there is no end-to-end unsupervised method currently inthe literature. This paper presents a motion tracking method where unsupervisedoptical flow networks are designed with spatial reconstruction loss andtemporal-consistency loss. Our proposed loss functions make use of thepair-wise and temporal correlation to estimate cardiac motion from noisybackground. Experiments using a synthetic 4D echocardiography dataset has shownthe effectiveness of our approach and its superiority over existing methods onboth accuracy and running speed. To the best of our knowledge this is thefirst work performed that uses unsupervised end-to-end deep learning opticalflow network for 4D cardiac motion tracking.</p>
                <p>Last Updated: 2024-07-05 17:18:46 UTC</p>
                <button class="interpret-button" data-id="2407.04663v1">Interpret</button>
                <div id="interpretation-2407.04663v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>A Multi-Player Potential Game Approach for Sensor Network Localization with Noisy Measurements</h3>
                <p>Authors: Gehui XuGuanpu ChenBaris FidanYiguang HongHongsheng QiThomas ParisiniKarl H. Johansson</p>
                <p><a href="http://arxiv.org/abs/2407.04608v1">Link to paper</a></p>
                <p>Sensor network localization SNL is a challenging problem due to itsinherent non-convexity and the effects of noise in inter-node rangingmeasurements and anchor node position. We formulate a non-convex SNL problem asa multi-player non-convex potential game and investigate the existence anduniqueness of a Nash equilibrium NE in both the ideal setting withoutmeasurement noise and the practical setting with measurement noise. We firstshow that the NE exists and is unique in the noiseless case and corresponds tothe precise network localization. Then we study the SNL for the case witherrors affecting the anchor node position and the inter-node distancemeasurements. Specifically we establish that in case these errors aresufficiently small the NE exists and is unique. It is shown that the NE is anapproximate solution to the SNL problem and that the position errors can bequantified accordingly. Based on these findings we apply the results to casestudies involving only inter-node distance measurement errors and only anchorposition information inaccuracies.</p>
                <p>Last Updated: 2024-07-05 16:00:19 UTC</p>
                <button class="interpret-button" data-id="2407.04608v1">Interpret</button>
                <div id="interpretation-2407.04608v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions</h3>
                <p>Authors: Jérémy PerezCorentin LégerGrgur KovačCédric ColasGaia MolinaroMaxime DerexPierre-Yves OudeyerClément Moulin-Frier</p>
                <p><a href="http://arxiv.org/abs/2407.04503v1">Link to paper</a></p>
                <p>As large language models LLMs start interacting with each other andgenerating an increasing amount of text online it becomes crucial to betterunderstand how information is transformed as it passes from one LLM to thenext. While significant research has examined individual LLM behaviorsexisting studies have largely overlooked the collective behaviors andinformation distortions arising from iterated LLM interactions. Small biasesnegligible at the single output level risk being amplified in iteratedinteractions potentially leading the content to evolve towards attractorstates. In a series of telephone game experiments we apply a transmissionchain design borrowed from the human cultural evolution literature: LLM agentsiteratively receive produce and transmit texts from the previous to the nextagent in the chain. By tracking the evolution of text toxicity positivitydifficulty and length across transmission chains we uncover the existence ofbiases and attractors and study their dependence on the initial text theinstructions language model and model size. For instance we find that moreopen-ended instructions lead to stronger attraction effects compared to moreconstrained tasks. We also find that different text properties displaydifferent sensitivity to attraction effects with toxicity leading to strongerattractors than length. These findings highlight the importance of accountingfor multi-step transmission dynamics and represent a first step towards a morecomprehensive understanding of LLM cultural dynamics.</p>
                <p>Last Updated: 2024-07-05 13:44:09 UTC</p>
                <button class="interpret-button" data-id="2407.04503v1">Interpret</button>
                <div id="interpretation-2407.04503v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Solving Zebra Puzzles Using Constraint-Guided Multi-Agent Systems</h3>
                <p>Authors: Shmuel BermanBaishakhi RayKathleen McKeown</p>
                <p><a href="http://arxiv.org/abs/2407.03956v1">Link to paper</a></p>
                <p>Prior research has enhanced the ability of Large Language Models LLMs tosolve logic puzzles using techniques such as chain-of-thought prompting orintroducing a symbolic representation. These frameworks are still usuallyinsufficient to solve complicated logical problems such as Zebra puzzles dueto the inherent complexity of translating natural language clues into logicalstatements. We introduce a multi-agent system ZPS that integrates LLMs withan off the shelf theorem prover. This system tackles the complex puzzle-solvingtask by breaking down the problem into smaller manageable parts generatingSMT Satisfiability Modulo Theories code to solve them with a theorem proverand using feedback between the agents to repeatedly improve their answers. Wealso introduce an automated grid puzzle grader to assess the correctness of ourpuzzle solutions and show that the automated grader is reliable by evaluatingit in a user-study. Our approach shows improvement in all three LLMs we testedwith GPT-4 showing 166 improvement in the number of fully correct solutions.</p>
                <p>Last Updated: 2024-07-04 14:22:25 UTC</p>
                <button class="interpret-button" data-id="2407.03956v1">Interpret</button>
                <div id="interpretation-2407.03956v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A multi-objective combinatorial optimisation framework for large scale hierarchical population synthesis</h3>
                <p>Authors: Imran MahmoodNicholas BishopAnisoara CalinescuMichael WooldridgeIoannis Zachos</p>
                <p><a href="http://arxiv.org/abs/2407.03180v1">Link to paper</a></p>
                <p>In agent-based simulations synthetic populations of agents are commonly usedto represent the structure behaviour and interactions of individuals.However generating a synthetic population that accurately reflects realpopulation statistics is a challenging task particularly when performed atscale. In this paper we propose a multi objective combinatorial optimisationtechnique for large scale population synthesis. We demonstrate theeffectiveness of our approach by generating a synthetic population for selectedregions and validating it on contingency tables from real population data. Ourapproach supports complex hierarchical structures between individuals andhouseholds is scalable to large populations and achieves minimal contigencytable reconstruction error. Hence it provides a useful tool for policymakersand researchers for simulating the dynamics of complex populations.</p>
                <p>Last Updated: 2024-07-03 15:01:12 UTC</p>
                <button class="interpret-button" data-id="2407.03180v1">Interpret</button>
                <div id="interpretation-2407.03180v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Wildfire Autonomous Response and Prediction Using Cellular Automata (WARP-CA)</h3>
                <p>Authors: Abdelrahman Ramadan</p>
                <p><a href="http://arxiv.org/abs/2407.02613v1">Link to paper</a></p>
                <p>Wildfires pose a severe challenge to ecosystems and human settlementsexacerbated by climate change and environmental factors. Traditional wildfiremodeling while useful often fails to adapt to the rapid dynamics of suchevents. This report introduces the Wildfire Autonomous Response and PredictionUsing Cellular Automata WARP-CA model a novel approach that integratesterrain generation using Perlin noise with the dynamism of Cellular AutomataCA to simulate wildfire spread. We explore the potential of Multi-AgentReinforcement Learning MARL to manage wildfires by simulating autonomousagents such as UAVs and UGVs within a collaborative framework. Ourmethodology combines world simulation techniques and investigates emergentbehaviors in MARL focusing on efficient wildfire suppression and consideringcritical environmental factors like wind patterns and terrain features.</p>
                <p>Last Updated: 2024-07-02 19:01:59 UTC</p>
                <button class="interpret-button" data-id="2407.02613v1">Interpret</button>
                <div id="interpretation-2407.02613v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-07-08</p>
        </div>
    
        </div>
    </body>
    </html>
    