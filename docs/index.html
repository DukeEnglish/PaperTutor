
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Stochastic Semi-Gradient Descent for Learning Mean Field Games with Population-Aware Function Approximation</h3>
                <p>Authors: Chenyu ZhangXu ChenXuan Di</p>
                <p><a href="http://arxiv.org/abs/2408.08192v1">Link to paper</a></p>
                <p>Mean field games MFGs model the interactions within a large-populationmulti-agent system using the population distribution. Traditional learningmethods for MFGs are based on fixed-point iteration FPI which calculatesbest responses and induced population distribution separately and sequentially.However FPI-type methods suffer from inefficiency and instability due tooscillations caused by the forward-backward procedure. This paper considers anonline learning method for MFGs where an agent updates its policy andpopulation estimates simultaneously and fully asynchronously resulting in asimple stochastic gradient descent SGD type method called SemiSGD. Not onlydoes SemiSGD exhibit numerical stability and efficiency but it also provides anovel perspective by treating the value function and population distribution asa unified parameter. We theoretically show that SemiSGD directs this unifiedparameter along a descent direction to the mean field equilibrium. Motivated bythis perspective we develop a linear function approximation LFA for both thevalue function and the population distribution resulting in the firstpopulation-aware LFA for MFGs on continuous state-action space. Finite-timeconvergence and approximation error analysis are provided for SemiSGD equippedwith population-aware LFA.</p>
                <p>Last Updated: 2024-08-15 14:51:50 UTC</p>
                <button class="interpret-button" data-id="2408.08192v1">Interpret</button>
                <div id="interpretation-2408.08192v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>EmBARDiment: an Embodied AI Agent for Productivity in XR</h3>
                <p>Authors: Riccardo BovoSteven AbreuKaran AhujaEric J GonzalezLi-Te ChengMar Gonzalez-Franco</p>
                <p><a href="http://arxiv.org/abs/2408.08158v1">Link to paper</a></p>
                <p>XR devices running chat-bots powered by Large Language Models LLMs havetremendous potential as always-on agents that can enable much betterproductivity scenarios. However screen based chat-bots do not take advantageof the the full-suite of natural inputs available in XR including inwardfacing sensor data instead they over-rely on explicit voice or text promptssometimes paired with multi-modal data dropped as part of the query. We proposea solution that leverages an attention framework that derives contextimplicitly from user actions eye-gaze and contextual memory within the XRenvironment. This minimizes the need for engineered explicit prompts fosteringgrounded and intuitive interactions that glean user insights for the chat-bot.Our user studies demonstrate the imminent feasibility and transformativepotential of our approach to streamline user interaction in XR with chat-botswhile offering insights for the design of future XR-embodied LLM agents.</p>
                <p>Last Updated: 2024-08-15 13:48:44 UTC</p>
                <button class="interpret-button" data-id="2408.08158v1">Interpret</button>
                <div id="interpretation-2408.08158v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Independent Policy Mirror Descent for Markov Potential Games: Scaling to Large Number of Players</h3>
                <p>Authors: Pragnya AlaturAnas BarakatNiao He</p>
                <p><a href="http://arxiv.org/abs/2408.08075v1">Link to paper</a></p>
                <p>Markov Potential Games MPGs form an important sub-class of Markov gameswhich are a common framework to model multi-agent reinforcement learningproblems. In particular MPGs include as a special case the identical-interestsetting where all the agents share the same reward function. Scaling theperformance of Nash equilibrium learning algorithms to a large number of agentsis crucial for multi-agent systems. To address this important challenge wefocus on the independent learning setting where agents can only have access totheir local information to update their own policy. In prior work on MPGs theiteration complexity for obtaining epsilon-Nash regret scales linearly withthe number of agents N. In this work we investigate the iteration complexityof an independent policy mirror descent PMD algorithm for MPGs. We show thatPMD with KL regularization also known as natural policy gradient enjoys abetter sqrtN dependence on the number of agents improving over PMD withEuclidean regularization and prior work. Furthermore the iteration complexityis also independent of the sizes of the agents action spaces.</p>
                <p>Last Updated: 2024-08-15 11:02:05 UTC</p>
                <button class="interpret-button" data-id="2408.08075v1">Interpret</button>
                <div id="interpretation-2408.08075v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Time-Ordered Ad-hoc Resource Sharing for Independent Robotic Agents</h3>
                <p>Authors: Arjo ChakravartyMichael X. GreyM. A. Viraj J. MuthugalaMohan Rajesh Elara</p>
                <p><a href="http://arxiv.org/abs/2408.07942v1">Link to paper</a></p>
                <p>Resource sharing is a crucial part of a multi-robot system. We propose aBoolean satisfiability based approach to resource sharing. Our keycontributions are an algorithm for converting any constrained assignment to aweighted-SAT based optimization. We propose a theorem that allows optimalresource assignment problems to be solved via repeated application of a SATsolver. Additionally we show a way to encode continuous time orderingconstraints using Conjunctive Normal Form CNF. We benchmark our newalgorithms and show that they can be used in an ad-hoc setting. We test ouralgorithms on a fleet of simulated and real world robots and show that thealgorithms are able to handle real world situations. Our algorithms and testharnesses are opensource and build on Open-RMFs fleet management system.</p>
                <p>Last Updated: 2024-08-15 05:34:17 UTC</p>
                <button class="interpret-button" data-id="2408.07942v1">Interpret</button>
                <div id="interpretation-2408.07942v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Nah Bandit: Modeling User Non-compliance in Recommendation Systems</h3>
                <p>Authors: Tianyue ZhouJung-Hoon ChoCathy Wu</p>
                <p><a href="http://arxiv.org/abs/2408.07897v1">Link to paper</a></p>
                <p>Recommendation systems now pervade the digital world ranging fromadvertising to entertainment. However it remains challenging to implementeffective recommendation systems in the physical world such as in mobility orhealth. This work focuses on a key challenge: in the physical world it isoften easy for the user to opt out of taking any recommendation if they are notto her liking and to fall back to her baseline behavior. It is thus crucial incyber-physical recommendation systems to operate with an interaction model thatis aware of such user behavior lest the user abandon the recommendationsaltogether. This paper thus introduces the Nah Bandit a tongue-in-cheekreference to describe a Bandit problem where users can say nah to therecommendation and opt for their preferred option instead. As such thisproblem lies in between a typical bandit setup and supervised learning. Wemodel the user non-compliance by parameterizing an anchoring effect ofrecommendations on users. We then propose the Expert with Clustering EWCalgorithm a hierarchical approach that incorporates feedback from bothrecommended and non-recommended options to accelerate user preference learning.In a recommendation scenario with N users T rounds per user and Kclusters EWC achieves a regret bound of ONsqrtTlog K  NT achievingsuperior theoretical performance in the short term compared to LinUCBalgorithm. Experimental results also highlight that EWC outperforms bothsupervised learning and traditional contextual bandit approaches. Thisadvancement reveals that effective use of non-compliance feedback canaccelerate preference learning and improve recommendation accuracy. This worklays the foundation for future research in Nah Bandit providing a robustframework for more effective recommendation systems.</p>
                <p>Last Updated: 2024-08-15 03:01:02 UTC</p>
                <button class="interpret-button" data-id="2408.07897v1">Interpret</button>
                <div id="interpretation-2408.07897v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Can Large Language Models Understand Symbolic Graphics Programs?</h3>
                <p>Authors: Zeju QiuWeiyang LiuHaiwen FengZhen LiuTim Z. XiaoKatherine M. CollinsJoshua B. TenenbaumAdrian WellerMichael J. BlackBernhard Schölkopf</p>
                <p><a href="http://arxiv.org/abs/2408.08313v1">Link to paper</a></p>
                <p>Assessing the capabilities of large language models LLMs is oftenchallenging in part because it is hard to find tasks to which they have notbeen exposed during training. We take one step to address this challenge byturning to a new task: focusing on symbolic graphics programs which are apopular representation for graphics content that procedurally generates visualdata. LLMs have shown exciting promise towards program synthesis but do theyunderstand symbolic graphics programs Unlike conventional programs symbolicgraphics programs can be translated to graphics content. Here we characterizean LLMs understanding of symbolic programs in terms of their ability to answerquestions related to the graphics content. This task is challenging as thequestions are difficult to answer from the symbolic programs alone -- yet theywould be easy to answer from the corresponding graphics content as we verifythrough a human experiment. To understand symbolic programs LLMs may need topossess the ability to imagine how the corresponding graphics content wouldlook without directly accessing the rendered visual content. We use this taskto evaluate LLMs by creating a large benchmark for the semantic understandingof symbolic graphics programs. This benchmark is built via program-graphicscorrespondence hence requiring minimal human efforts. We evaluate current LLMson our benchmark to elucidate a preliminary assessment of their ability toreason about visual scenes from programs. We find that this task distinguishesexisting LLMs and models considered good at reasoning perform better. Lastlywe introduce Symbolic Instruction Tuning SIT to improve this ability.Specifically we query GPT4-o with questions and images generated by symbolicprograms. Such data are then used to finetune an LLM. We also find that SITdata can improve the general instruction following ability of LLMs.</p>
                <p>Last Updated: 2024-08-15 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2408.08313v1">Interpret</button>
                <div id="interpretation-2408.08313v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Understanding the Local Geometry of Generative Model Manifolds</h3>
                <p>Authors: Ahmed Imtiaz HumayunIbtihel AmaraCandice SchumannGolnoosh FarnadiNegar RostamzadehMohammad Havaei</p>
                <p><a href="http://arxiv.org/abs/2408.08307v1">Link to paper</a></p>
                <p>Deep generative models learn continuous representations of complex datamanifolds using a finite number of samples during training. For a pre-trainedgenerative model the common way to evaluate the quality of the manifoldrepresentation learned is by computing global metrics like Frechet InceptionDistance using a large number of generated and real samples. Howevergenerative model performance is not uniform across the learned manifold e.g.for textitfoundation models like Stable Diffusion generation performance canvary significantly based on the conditioning or initial noise vector beingdenoised. In this paper we study the relationship between the textitlocalgeometry of the learned manifold and downstream generation. Based on thetheory of continuous piecewise-linear CPWL generators we use three geometricdescriptors - scaling psi rank nu and complexity delta - tocharacterize a pre-trained generative model manifold locally. We providequantitative and qualitative evidence showing that for a given latent thelocal descriptors are correlated with generation aesthetics artifactsuncertainty and even memorization. Finally we demonstrate that training atextitreward model on the local geometry can allow controlling thelikelihood of a generated sample under the learned distribution.</p>
                <p>Last Updated: 2024-08-15 17:59:06 UTC</p>
                <button class="interpret-button" data-id="2408.08307v1">Interpret</button>
                <div id="interpretation-2408.08307v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Towards Flexible Visual Relationship Segmentation</h3>
                <p>Authors: Fangrui ZhuJianwei YangHuaizu Jiang</p>
                <p><a href="http://arxiv.org/abs/2408.08305v1">Link to paper</a></p>
                <p>Visual relationship understanding has been studied separately in human-objectinteractionHOI detection scene graph generationSGG and referringrelationshipsRR tasks. Given the complexity and interconnectedness of thesetasks it is crucial to have a flexible framework that can effectively addressthese tasks in a cohesive manner. In this work we propose FleVRS a singlemodel that seamlessly integrates the above three aspects in standard andpromptable visual relationship segmentation and further possesses thecapability for open-vocabulary segmentation to adapt to novel scenarios. FleVRSleverages the synergy between text and image modalities to ground varioustypes of relationships from images and use textual features fromvision-language models to visual conceptual understanding. Empirical validationacross various datasets demonstrates that our framework outperforms existingmodels in standard promptable and open-vocabulary tasks e.g. 1.9 mAP onHICO-DET 11.4 Acc on VRD 4.7 mAP on unseen HICO-DET. Our FleVRSrepresents a significant step towards a more intuitive comprehensive andscalable understanding of visual relationships.</p>
                <p>Last Updated: 2024-08-15 17:57:38 UTC</p>
                <button class="interpret-button" data-id="2408.08305v1">Interpret</button>
                <div id="interpretation-2408.08305v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training</h3>
                <p>Authors: Gengwei ZhangLiyuan WangGuoliang KangLing ChenYunchao Wei</p>
                <p><a href="http://arxiv.org/abs/2408.08295v1">Link to paper</a></p>
                <p>In recent years continual learning with pre-training CLPT has receivedwidespread interest instead of its traditional focus of training from scratch.The use of strong pre-trained models PTMs can greatly facilitate knowledgetransfer and alleviate catastrophic forgetting but also suffers fromprogressive overfitting of pre-trained knowledge into specific downstreamtasks. A majority of current efforts often keep the PTMs frozen and incorporatetask-specific prompts to instruct representation learning coupled with aprompt selection process for inference. However due to the limited capacity ofprompt parameters this strategy demonstrates only sub-optimal performance incontinual learning. In comparison tuning all parameters of PTMs often providesthe greatest potential for representation learning making sequentialfine-tuning Seq FT a fundamental baseline that has been overlooked in CLPT.To this end we present an in-depth analysis of the progressive overfittingproblem from the lens of Seq FT. Considering that the overly fastrepresentation learning and the biased classification layer constitute thisparticular problem we introduce the advanced Slow Learner with ClassifierAlignment SLCA framework to unleash the power of Seq FT serving as astrong baseline approach for CLPT. Our approach involves a Slow Learner toselectively reduce the learning rate of backbone parameters and a ClassifierAlignment to align the disjoint classification layers in a post-hoc fashion. Wefurther enhance the efficacy of SL with a symmetric cross-entropy loss as wellas employ a parameter-efficient strategy to implement Seq FT with SLCA.Across a variety of continual learning scenarios on image classificationbenchmarks our approach provides substantial improvements and outperformsstate-of-the-art methods by a large margin. Code:https://github.com/GengDavid/SLCA.</p>
                <p>Last Updated: 2024-08-15 17:50:07 UTC</p>
                <button class="interpret-button" data-id="2408.08295v1">Interpret</button>
                <div id="interpretation-2408.08295v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>HeightLane: BEV Heightmap guided 3D Lane Detection</h3>
                <p>Authors: Chaesong ParkEunbin SeoJongwoo Lim</p>
                <p><a href="http://arxiv.org/abs/2408.08270v1">Link to paper</a></p>
                <p>Accurate 3D lane detection from monocular images presents significantchallenges due to depth ambiguity and imperfect ground modeling. Previousattempts to model the ground have often used a planar ground assumption withlimited degrees of freedom making them unsuitable for complex roadenvironments with varying slopes. Our study introduces HeightLane aninnovative method that predicts a height map from monocular images by creatinganchors based on a multi-slope assumption. This approach provides a detailedand accurate representation of the ground. HeightLane employs the predictedheightmap along with a deformable attention-based spatial feature transformframework to efficiently convert 2D image features into 3D birds eye viewBEV features enhancing spatial understanding and lane structure recognition.Additionally the heightmap is used for the positional encoding of BEVfeatures further improving their spatial accuracy. This explicit viewtransformation bridges the gap between front-view perceptions and spatiallyaccurate BEV representations significantly improving detection performance. Toaddress the lack of the necessary ground truth GT height map in the originalOpenLane dataset we leverage the Waymo dataset and accumulate its LiDAR datato generate a height map for the drivable area of each scene. The GT heightmapsare used to train the heightmap extraction module from monocular images.Extensive experiments on the OpenLane validation set show that HeightLaneachieves state-of-the-art performance in terms of F-score highlighting itspotential in real-world applications.</p>
                <p>Last Updated: 2024-08-15 17:14:57 UTC</p>
                <button class="interpret-button" data-id="2408.08270v1">Interpret</button>
                <div id="interpretation-2408.08270v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Can Large Language Models Understand Symbolic Graphics Programs?</h3>
                <p>Authors: Zeju QiuWeiyang LiuHaiwen FengZhen LiuTim Z. XiaoKatherine M. CollinsJoshua B. TenenbaumAdrian WellerMichael J. BlackBernhard Schölkopf</p>
                <p><a href="http://arxiv.org/abs/2408.08313v1">Link to paper</a></p>
                <p>Assessing the capabilities of large language models LLMs is oftenchallenging in part because it is hard to find tasks to which they have notbeen exposed during training. We take one step to address this challenge byturning to a new task: focusing on symbolic graphics programs which are apopular representation for graphics content that procedurally generates visualdata. LLMs have shown exciting promise towards program synthesis but do theyunderstand symbolic graphics programs Unlike conventional programs symbolicgraphics programs can be translated to graphics content. Here we characterizean LLMs understanding of symbolic programs in terms of their ability to answerquestions related to the graphics content. This task is challenging as thequestions are difficult to answer from the symbolic programs alone -- yet theywould be easy to answer from the corresponding graphics content as we verifythrough a human experiment. To understand symbolic programs LLMs may need topossess the ability to imagine how the corresponding graphics content wouldlook without directly accessing the rendered visual content. We use this taskto evaluate LLMs by creating a large benchmark for the semantic understandingof symbolic graphics programs. This benchmark is built via program-graphicscorrespondence hence requiring minimal human efforts. We evaluate current LLMson our benchmark to elucidate a preliminary assessment of their ability toreason about visual scenes from programs. We find that this task distinguishesexisting LLMs and models considered good at reasoning perform better. Lastlywe introduce Symbolic Instruction Tuning SIT to improve this ability.Specifically we query GPT4-o with questions and images generated by symbolicprograms. Such data are then used to finetune an LLM. We also find that SITdata can improve the general instruction following ability of LLMs.</p>
                <p>Last Updated: 2024-08-15 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2408.08313v1">Interpret</button>
                <div id="interpretation-2408.08313v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Understanding the Local Geometry of Generative Model Manifolds</h3>
                <p>Authors: Ahmed Imtiaz HumayunIbtihel AmaraCandice SchumannGolnoosh FarnadiNegar RostamzadehMohammad Havaei</p>
                <p><a href="http://arxiv.org/abs/2408.08307v1">Link to paper</a></p>
                <p>Deep generative models learn continuous representations of complex datamanifolds using a finite number of samples during training. For a pre-trainedgenerative model the common way to evaluate the quality of the manifoldrepresentation learned is by computing global metrics like Frechet InceptionDistance using a large number of generated and real samples. Howevergenerative model performance is not uniform across the learned manifold e.g.for textitfoundation models like Stable Diffusion generation performance canvary significantly based on the conditioning or initial noise vector beingdenoised. In this paper we study the relationship between the textitlocalgeometry of the learned manifold and downstream generation. Based on thetheory of continuous piecewise-linear CPWL generators we use three geometricdescriptors - scaling psi rank nu and complexity delta - tocharacterize a pre-trained generative model manifold locally. We providequantitative and qualitative evidence showing that for a given latent thelocal descriptors are correlated with generation aesthetics artifactsuncertainty and even memorization. Finally we demonstrate that training atextitreward model on the local geometry can allow controlling thelikelihood of a generated sample under the learned distribution.</p>
                <p>Last Updated: 2024-08-15 17:59:06 UTC</p>
                <button class="interpret-button" data-id="2408.08307v1">Interpret</button>
                <div id="interpretation-2408.08307v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Benchmarking the Capabilities of Large Language Models in Transportation System Engineering: Accuracy, Consistency, and Reasoning Behaviors</h3>
                <p>Authors: Usman SyedEthan LightXingang GuoHuan ZhangLianhui QinYanfeng OuyangBin Hu</p>
                <p><a href="http://arxiv.org/abs/2408.08302v1">Link to paper</a></p>
                <p>In this paper we explore the capabilities of state-of-the-art large languagemodels LLMs such as GPT-4 GPT-4o Claude 3.5 Sonnet Claude 3 Opus Gemini1.5 Pro Llama 3 and Llama 3.1 in solving some selected undergraduate-leveltransportation engineering problems. We introduce TransportBench a benchmarkdataset that includes a sample of transportation engineering problems on a widerange of subjects in the context of planning design management and controlof transportation systems. This dataset is used by human experts to evaluatethe capabilities of various commercial and open-sourced LLMs especially theiraccuracy consistency and reasoning behaviors in solving transportationengineering problems. Our comprehensive analysis uncovers the unique strengthsand limitations of each LLM e.g. our analysis shows the impressive accuracyand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solvingTransportBench problems. Our study marks a thrilling first step towardharnessing artificial general intelligence for complex transportationchallenges.</p>
                <p>Last Updated: 2024-08-15 17:55:45 UTC</p>
                <button class="interpret-button" data-id="2408.08302v1">Interpret</button>
                <div id="interpretation-2408.08302v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>HELP: Hierarchical Embeddings-based Log Parsing</h3>
                <p>Authors: Andy XuArno Gau</p>
                <p><a href="http://arxiv.org/abs/2408.08300v1">Link to paper</a></p>
                <p>Logs are a first-hand source of information for software maintenance andfailure diagnosis. Log parsing which converts semi-structured log messagesinto structured templates is a prerequisite for automated log analysis taskssuch as anomaly detection troubleshooting and root cause analysis. Howeverexisting log parsers fail in real-world systems for three main reasons. Firsttraditional heuristics-based parsers require handcrafted features and domainknowledge which are difficult to generalize at scale. Second existing largelanguage model-based parsers rely on periodic offline processing limitingtheir effectiveness in real-time use cases. Third existing online parsingalgorithms are susceptible to log drift where slight log changes create falsepositives that drown out real anomalies. To address these challenges wepropose HELP a Hierarchical Embeddings-based Log Parser. HELP is the firstonline semantic-based parser to leverage LLMs for performant and cost-effectivelog parsing. We achieve this through a novel hierarchical embeddings modulewhich fine-tunes a text embedding model to cluster logs before parsingreducing querying costs by multiple orders of magnitude. To combat log driftwe also develop an iterative rebalancing module which periodically updatesexisting log groupings. We evaluate HELP extensively on 14 public large-scaledatasets showing that HELP achieves significantly higher F1-weighted groupingand parsing accuracy than current state-of-the-art online log parsers. We alsoimplement HELP into Iudexs production observability platform confirmingHELPs practicality in a production environment. Our results show that HELP iseffective and efficient for high-throughput real-world log parsing.</p>
                <p>Last Updated: 2024-08-15 17:54:31 UTC</p>
                <button class="interpret-button" data-id="2408.08300v1">Interpret</button>
                <div id="interpretation-2408.08300v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training</h3>
                <p>Authors: Gengwei ZhangLiyuan WangGuoliang KangLing ChenYunchao Wei</p>
                <p><a href="http://arxiv.org/abs/2408.08295v1">Link to paper</a></p>
                <p>In recent years continual learning with pre-training CLPT has receivedwidespread interest instead of its traditional focus of training from scratch.The use of strong pre-trained models PTMs can greatly facilitate knowledgetransfer and alleviate catastrophic forgetting but also suffers fromprogressive overfitting of pre-trained knowledge into specific downstreamtasks. A majority of current efforts often keep the PTMs frozen and incorporatetask-specific prompts to instruct representation learning coupled with aprompt selection process for inference. However due to the limited capacity ofprompt parameters this strategy demonstrates only sub-optimal performance incontinual learning. In comparison tuning all parameters of PTMs often providesthe greatest potential for representation learning making sequentialfine-tuning Seq FT a fundamental baseline that has been overlooked in CLPT.To this end we present an in-depth analysis of the progressive overfittingproblem from the lens of Seq FT. Considering that the overly fastrepresentation learning and the biased classification layer constitute thisparticular problem we introduce the advanced Slow Learner with ClassifierAlignment SLCA framework to unleash the power of Seq FT serving as astrong baseline approach for CLPT. Our approach involves a Slow Learner toselectively reduce the learning rate of backbone parameters and a ClassifierAlignment to align the disjoint classification layers in a post-hoc fashion. Wefurther enhance the efficacy of SL with a symmetric cross-entropy loss as wellas employ a parameter-efficient strategy to implement Seq FT with SLCA.Across a variety of continual learning scenarios on image classificationbenchmarks our approach provides substantial improvements and outperformsstate-of-the-art methods by a large margin. Code:https://github.com/GengDavid/SLCA.</p>
                <p>Last Updated: 2024-08-15 17:50:07 UTC</p>
                <button class="interpret-button" data-id="2408.08295v1">Interpret</button>
                <div id="interpretation-2408.08295v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Marker or Markerless? Mode-Switchable Optical Tactile Sensing for Diverse Robot Tasks</h3>
                <p>Authors: Ni OuZhuo ChenShan Luo</p>
                <p><a href="http://arxiv.org/abs/2408.08276v1">Link to paper</a></p>
                <p>Optical tactile sensors play a pivotal role in robot perception andmanipulation tasks. The membrane of these sensors can be painted with markersor remain markerless enabling them to function in either marker or markerlessmode. However this uni-modal selection means the sensor is only suitable foreither manipulation or perception tasks. While markers are vital formanipulation they can also obstruct the camera thereby impeding perception.The dilemma of selecting between marker and markerless modes presents asignificant obstacle. To address this issue we propose a novel mode-switchableoptical tactile sensing approach that facilitates transitions between the twomodes. The marker-to-markerless transition is achieved through a generativemodel whereas its inverse transition is realized using a sparsely supervisedregressive model. Our approach allows a single-mode optical sensor to operateeffectively in both marker and markerless modes without the need for additionalhardware making it well-suited for both perception and manipulation tasks.Extensive experiments validate the effectiveness of our method. For perceptiontasks our approach decreases the number of categories that includemisclassified samples by 2 and improves contact area segmentation IoU by 3.53.For manipulation tasks our method attains a high success rate of 92.59 inslip detection. Code dataset and demo videos are available at the projectwebsite: https://gitouni.github.io/Marker-Markerless-Transition/</p>
                <p>Last Updated: 2024-08-15 17:21:21 UTC</p>
                <button class="interpret-button" data-id="2408.08276v1">Interpret</button>
                <div id="interpretation-2408.08276v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ESCape the ClassRoom</h3>
                <p>Authors: John O'Connor</p>
                <p><a href="http://arxiv.org/abs/2408.08273v1">Link to paper</a></p>
                <p>Educational Escape Rooms EERs through their use of immersive storytellingand practical application of abstract concepts present a novel new techniquefor engaging learners in a variety of subjects. However there is a significanttime and materials investment required to build new physical Escape Rooms andprior attempts to create digital escape rooms have resulted in games that lackthe immersive qualities that make physical escape rooms so compelling. Thispaper presents ESCape the Classroom a web framework for creating virtualreality educational escape rooms VR EERs that can be delivered to anyweb-connected device. The framework is equipped with essential tools to designand deploy intricate multi-room VR escape experiences using HTML andWeb-Components. It is designed to be used by educators with rudimentaryprogramming skills eliminating the need for advanced game programming ordevelopment expertise. VR EERs created with this platform can be publishedonline as WebXR sites that are compatible with a broad spectrum of VR hardwareincluding the Meta Quest 3 allowing educators to share the experiences theycreate while bypassing the need for additional software installations ondevices. This paper will present the design and implementation of ESCape theClassroom and discuss the potential for this platform to be used ineducational settings.</p>
                <p>Last Updated: 2024-08-15 17:18:33 UTC</p>
                <button class="interpret-button" data-id="2408.08273v1">Interpret</button>
                <div id="interpretation-2408.08273v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>"I Try to Represent Myself as I Am": Self-Presentation Preferences of People with Invisible Disabilities through Embodied Social VR Avatars</h3>
                <p>Authors: Ria J. GualanoLucy JiangKexin ZhangTanisha ShendeAndrea Stevenson WonShiri Azenkot</p>
                <p><a href="http://dx.doi.org/10.1145/3663548.3675620">Link to paper</a></p>
                <p>With the increasing adoption of social virtual reality VR it is criticalto design inclusive avatars. While researchers have investigated how and whyblind and d/Deaf people wish to disclose their disabilities in VR little isknown about the preferences of many others with invisible disabilities e.g.ADHD dyslexia chronic conditions. We filled this gap by interviewing 15participants each with one to three invisible disabilities who represented 22different invisible disabilities in total. We found that invisibly disabledpeople approached avatar-based disclosure through contextualized considerationsinformed by their prior experiences. For example some wished to use VRsembodied affordances such as facial expressions and body language todynamically represent their energy level or willingness to engage with otherswhile others preferred not to disclose their disability identity in anycontext. We define a binary framework for embodied invisible disabilityexpression public and private and discuss three disclosure patternsActivists Non-Disclosers and Situational Disclosers to inform the design offuture inclusive VR experiences.</p>
                <p>Last Updated: 2024-08-15 14:53:48 UTC</p>
                <button class="interpret-button" data-id="2408.08193v1">Interpret</button>
                <div id="interpretation-2408.08193v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>EmBARDiment: an Embodied AI Agent for Productivity in XR</h3>
                <p>Authors: Riccardo BovoSteven AbreuKaran AhujaEric J GonzalezLi-Te ChengMar Gonzalez-Franco</p>
                <p><a href="http://arxiv.org/abs/2408.08158v1">Link to paper</a></p>
                <p>XR devices running chat-bots powered by Large Language Models LLMs havetremendous potential as always-on agents that can enable much betterproductivity scenarios. However screen based chat-bots do not take advantageof the the full-suite of natural inputs available in XR including inwardfacing sensor data instead they over-rely on explicit voice or text promptssometimes paired with multi-modal data dropped as part of the query. We proposea solution that leverages an attention framework that derives contextimplicitly from user actions eye-gaze and contextual memory within the XRenvironment. This minimizes the need for engineered explicit prompts fosteringgrounded and intuitive interactions that glean user insights for the chat-bot.Our user studies demonstrate the imminent feasibility and transformativepotential of our approach to streamline user interaction in XR with chat-botswhile offering insights for the design of future XR-embodied LLM agents.</p>
                <p>Last Updated: 2024-08-15 13:48:44 UTC</p>
                <button class="interpret-button" data-id="2408.08158v1">Interpret</button>
                <div id="interpretation-2408.08158v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Confidence-weighted integration of human and machine judgments for superior decision-making</h3>
                <p>Authors: Felipe YáñezXiaoliang LuoOmar Valerio MineroBradley C. Love</p>
                <p><a href="http://arxiv.org/abs/2408.08083v1">Link to paper</a></p>
                <p>Large language models LLMs have emerged as powerful tools in variousdomains. Recent studies have shown that LLMs can surpass humans in certaintasks such as predicting the outcomes of neuroscience studies. What role doesthis leave for humans in the overall decision process One possibility is thathumans despite performing worse than LLMs can still add value when teamedwith them. A human and machine team can surpass each individual teammate whenteam members confidence is well-calibrated and team members diverge in whichtasks they find difficult i.e. calibration and diversity are needed. Wesimplified and extended a Bayesian approach to combining judgments using alogistic regression framework that integrates confidence-weighted judgments forany number of team members. Using this straightforward method we demonstratedin a neuroscience forecasting task that even when humans were inferior toLLMs their combination with one or more LLMs consistently improved teamperformance. Our hope is that this simple and effective strategy forintegrating the judgments of humans and machines will lead to productivecollaborations.</p>
                <p>Last Updated: 2024-08-15 11:16:21 UTC</p>
                <button class="interpret-button" data-id="2408.08083v1">Interpret</button>
                <div id="interpretation-2408.08083v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Can Large Language Models Understand Symbolic Graphics Programs?</h3>
                <p>Authors: Zeju QiuWeiyang LiuHaiwen FengZhen LiuTim Z. XiaoKatherine M. CollinsJoshua B. TenenbaumAdrian WellerMichael J. BlackBernhard Schölkopf</p>
                <p><a href="http://arxiv.org/abs/2408.08313v1">Link to paper</a></p>
                <p>Assessing the capabilities of large language models LLMs is oftenchallenging in part because it is hard to find tasks to which they have notbeen exposed during training. We take one step to address this challenge byturning to a new task: focusing on symbolic graphics programs which are apopular representation for graphics content that procedurally generates visualdata. LLMs have shown exciting promise towards program synthesis but do theyunderstand symbolic graphics programs Unlike conventional programs symbolicgraphics programs can be translated to graphics content. Here we characterizean LLMs understanding of symbolic programs in terms of their ability to answerquestions related to the graphics content. This task is challenging as thequestions are difficult to answer from the symbolic programs alone -- yet theywould be easy to answer from the corresponding graphics content as we verifythrough a human experiment. To understand symbolic programs LLMs may need topossess the ability to imagine how the corresponding graphics content wouldlook without directly accessing the rendered visual content. We use this taskto evaluate LLMs by creating a large benchmark for the semantic understandingof symbolic graphics programs. This benchmark is built via program-graphicscorrespondence hence requiring minimal human efforts. We evaluate current LLMson our benchmark to elucidate a preliminary assessment of their ability toreason about visual scenes from programs. We find that this task distinguishesexisting LLMs and models considered good at reasoning perform better. Lastlywe introduce Symbolic Instruction Tuning SIT to improve this ability.Specifically we query GPT4-o with questions and images generated by symbolicprograms. Such data are then used to finetune an LLM. We also find that SITdata can improve the general instruction following ability of LLMs.</p>
                <p>Last Updated: 2024-08-15 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2408.08313v1">Interpret</button>
                <div id="interpretation-2408.08313v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws</h3>
                <p>Authors: Ruihang LiYixuan WeiMiaosen ZhangNenghai YuHan HuHouwen Peng</p>
                <p><a href="http://arxiv.org/abs/2408.08310v1">Link to paper</a></p>
                <p>High-quality data is crucial for the pre-training performance of largelanguage models. Unfortunately existing quality filtering methods rely on aknown high-quality dataset as reference which can introduce potential bias andcompromise diversity. In this paper we propose ScalingFilter a novel approachthat evaluates text quality based on the perplexity difference between twolanguage models trained on the same data thereby eliminating the influence ofthe reference dataset in the filtering process. An theoretical analysis showsthat ScalingFilter is equivalent to an inverse utilization of scaling laws.Through training models with 1.3B parameters on the same data source processedby various quality filters we find ScalingFilter can improve zero-shotperformance of pre-trained models in downstream tasks. To assess the biasintroduced by quality filtering we introduce semantic diversity a metric ofutilizing text embedding models for semantic representations. Extensiveexperiments reveal that semantic diversity is a reliable indicator of datasetdiversity and ScalingFilter achieves an optimal balance between downstreamperformance and semantic diversity.</p>
                <p>Last Updated: 2024-08-15 17:59:30 UTC</p>
                <button class="interpret-button" data-id="2408.08310v1">Interpret</button>
                <div id="interpretation-2408.08310v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Benchmarking the Capabilities of Large Language Models in Transportation System Engineering: Accuracy, Consistency, and Reasoning Behaviors</h3>
                <p>Authors: Usman SyedEthan LightXingang GuoHuan ZhangLianhui QinYanfeng OuyangBin Hu</p>
                <p><a href="http://arxiv.org/abs/2408.08302v1">Link to paper</a></p>
                <p>In this paper we explore the capabilities of state-of-the-art large languagemodels LLMs such as GPT-4 GPT-4o Claude 3.5 Sonnet Claude 3 Opus Gemini1.5 Pro Llama 3 and Llama 3.1 in solving some selected undergraduate-leveltransportation engineering problems. We introduce TransportBench a benchmarkdataset that includes a sample of transportation engineering problems on a widerange of subjects in the context of planning design management and controlof transportation systems. This dataset is used by human experts to evaluatethe capabilities of various commercial and open-sourced LLMs especially theiraccuracy consistency and reasoning behaviors in solving transportationengineering problems. Our comprehensive analysis uncovers the unique strengthsand limitations of each LLM e.g. our analysis shows the impressive accuracyand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solvingTransportBench problems. Our study marks a thrilling first step towardharnessing artificial general intelligence for complex transportationchallenges.</p>
                <p>Last Updated: 2024-08-15 17:55:45 UTC</p>
                <button class="interpret-button" data-id="2408.08302v1">Interpret</button>
                <div id="interpretation-2408.08302v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The ShareLM Collection and Plugin: Contributing Human-Model Chats for the Benefit of the Community</h3>
                <p>Authors: Shachar Don-YehiyaLeshem ChoshenOmri Abend</p>
                <p><a href="http://arxiv.org/abs/2408.08291v1">Link to paper</a></p>
                <p>Human-model conversations provide a window into users real-world scenariosbehavior and needs and thus are a valuable resource for model development andresearch. While for-profit companies collect user data through the APIs oftheir models using it internally to improve their own models the open sourceand research community lags behind.  We introduce the ShareLM collection a unified set of human conversationswith large language models and its accompanying plugin a Web extension forvoluntarily contributing user-model conversations. Where few platforms sharetheir chats the ShareLM plugin adds this functionality thus allowing usersto share conversations from most platforms. The plugin allows the user to ratetheir conversations both at the conversation and the response levels anddelete conversations they prefer to keep private before they ever leave theusers local storage. We release the plugin conversations as part of theShareLM collection and call for more community effort in the field of openhuman-model data.  The code plugin and data are available.</p>
                <p>Last Updated: 2024-08-15 17:46:54 UTC</p>
                <button class="interpret-button" data-id="2408.08291v1">Interpret</button>
                <div id="interpretation-2408.08291v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>mhGPT: A Lightweight Generative Pre-Trained Transformer for Mental Health Text Analysis</h3>
                <p>Authors: Dae-young KimRebecca HwaMuhammad Mahbubur Rahman</p>
                <p><a href="http://arxiv.org/abs/2408.08261v1">Link to paper</a></p>
                <p>This paper introduces mhGPT a lightweight generative pre-trained transformertrained on mental health-related social media and PubMed articles. Fine-tunedfor specific mental health tasks mhGPT was evaluated under limited hardwareconstraints and compared with state-of-the-art models like MentaLLaMA andGemma. Despite having only 1.98 billion parameters and using just 5 of thedataset mhGPT outperformed larger models and matched the performance of modelstrained on significantly more data. The key contributions include integratingdiverse mental health data creating a custom tokenizer and optimizing asmaller architecture for low-resource settings. This research could advanceAI-driven mental health care especially in areas with limited computing power.</p>
                <p>Last Updated: 2024-08-15 17:01:57 UTC</p>
                <button class="interpret-button" data-id="2408.08261v1">Interpret</button>
                <div id="interpretation-2408.08261v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Aliasing and Label-Independent Decomposition of Risk: Beyond the bias-variance trade-off</h3>
                <p>Authors: Mark K. TranstrumGus L. W. HartTyler J. JarvisJared P. Whitehead</p>
                <p><a href="http://arxiv.org/abs/2408.08294v1">Link to paper</a></p>
                <p>A central problem in data science is to use potentially noisy samples of anunknown function to predict function values for unseen inputs. In classicalstatistics the predictive error is understood as a trade-off between the biasand the variance that balances model simplicity with its ability to fit complexfunctions. However over-parameterized models exhibit counter-intuitivebehaviors such as double descent in which models of increasing complexityexhibit decreasing generalization error. We introduce an alternative paradigmcalled the generalized aliasing decomposition. We explain the asymptoticallysmall error of complex models as a systematic de-aliasing that occurs in theover-parameterized regime. In the limit of large models the contribution dueto aliasing vanishes leaving an expression for the asymptotic total error wecall the invertibility failure of very large models on few training points.Because the generalized aliasing decomposition can be explicitly calculatedfrom the relationship between model class and samples without seeing any datalabels it can answer questions related to experimental design and modelselection before collecting data or performing experiments. We demonstrate thisapproach using several examples including classical regression problems and acluster expansion model used in materials science.</p>
                <p>Last Updated: 2024-08-15 17:49:24 UTC</p>
                <button class="interpret-button" data-id="2408.08294v1">Interpret</button>
                <div id="interpretation-2408.08294v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding</h3>
                <p>Authors: Xiner LiYulai ZhaoChenyu WangGabriele ScaliaGokcen EraslanSurag NairTommaso BiancalaniAviv RegevSergey LevineMasatoshi Uehara</p>
                <p><a href="http://arxiv.org/abs/2408.08252v1">Link to paper</a></p>
                <p>Diffusion models excel at capturing the natural design spaces of imagesmolecules DNA RNA and protein sequences. However rather than merelygenerating designs that are natural we often aim to optimize downstream rewardfunctions while preserving the naturalness of these design spaces. Existingmethods for achieving this goal often require differentiable proxy modelstextite.g. classifier guidance or DPS or involve computationallyexpensive fine-tuning of diffusion models textite.g. classifier-freeguidance RL-based fine-tuning. In our work we propose a new method toaddress these challenges. Our algorithm is an iterative sampling method thatintegrates soft value functions which looks ahead to how intermediate noisystates lead to high rewards in the future into the standard inferenceprocedure of pre-trained diffusion models. Notably our approach avoidsfine-tuning generative models and eliminates the need to constructdifferentiable models. This enables us to 1 directly utilizenon-differentiable features/reward feedback commonly used in many scientificdomains and 2 apply our method to recent discrete diffusion models in aprincipled way. Finally we demonstrate the effectiveness of our algorithmacross several domains including image generation molecule generation andDNA/RNA sequence generation. The code is available athrefhttps://github.com/masa-ue/SVDDhttps://github.com/masa-ue/SVDD.</p>
                <p>Last Updated: 2024-08-15 16:47:59 UTC</p>
                <button class="interpret-button" data-id="2408.08252v1">Interpret</button>
                <div id="interpretation-2408.08252v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Localized Sparse Principal Component Analysis of Multivariate Time Series in Frequency Domain</h3>
                <p>Authors: Jamshid NamdariAmita ManatungaFabio FerrarelliRobert Krafty</p>
                <p><a href="http://arxiv.org/abs/2408.08177v1">Link to paper</a></p>
                <p>Principal component analysis has been a main tool in multivariate analysisfor estimating a low dimensional linear subspace that explains most of thevariability in the data. However in high-dimensional regimes naive estimatesof the principal loadings are not consistent and difficult to interpret. In thecontext of time series principal component analysis of spectral densitymatrices can provide valuable parsimonious information about the behavior ofthe underlying process particularly if the principal components areinterpretable in that they are sparse in coordinates and localized in frequencybands. In this paper we introduce a formulation and consistent estimationprocedure for interpretable principal component analysis for high-dimensionaltime series in the frequency domain. An efficient frequency-sequentialalgorithm is developed to compute sparse-localized estimates of thelow-dimensional principal subspaces of the signal process. The method ismotivated by and used to understand neurological mechanisms from high-densityresting-state EEG in a study of first episode psychosis.</p>
                <p>Last Updated: 2024-08-15 14:30:34 UTC</p>
                <button class="interpret-button" data-id="2408.08177v1">Interpret</button>
                <div id="interpretation-2408.08177v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Extracting Sentence Embeddings from Pretrained Transformer Models</h3>
                <p>Authors: Lukas StankevičiusMantas Lukoševičius</p>
                <p><a href="http://arxiv.org/abs/2408.08073v1">Link to paper</a></p>
                <p>Background/introduction: Pre-trained transformer models shine in many naturallanguage processing tasks and therefore are expected to bear the representationof the input sentence or text meaning. These sentence-level embeddings are alsoimportant in retrieval-augmented generation. But do commonly used plainaveraging or prompt templates surface it enough  Methods: Given 110M parameters BERTs hidden representations from multiplelayers and multiple tokens we tried various ways to extract optimal sentencerepresentations. We tested various token aggregation and representationpost-processing techniques. We also tested multiple ways of using a generalWikitext dataset to complement BERTs sentence representations. All methods weretested on 8 Semantic Textual Similarity STS 6 short text clustering and 12classification tasks. We also evaluated our representation-shaping techniqueson other static models including random token representations.  Results: Proposed representation extraction methods improved the performanceon STS and clustering tasks for all models considered. Very high improvementsfor static token-based models especially random embeddings for STS tasksalmost reach the performance of BERT-derived representations.  Conclusions: Our work shows that for multiple tasks simple baselines withrepresentation shaping techniques reach or even outperform more complexBERT-based models or are able to contribute to their performance.</p>
                <p>Last Updated: 2024-08-15 10:54:55 UTC</p>
                <button class="interpret-button" data-id="2408.08073v1">Interpret</button>
                <div id="interpretation-2408.08073v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>BINDy -- Bayesian identification of nonlinear dynamics with reversible-jump Markov-chain Monte-Carlo</h3>
                <p>Authors: Max D. ChampneysTimothy J. Rogers</p>
                <p><a href="http://arxiv.org/abs/2408.08062v1">Link to paper</a></p>
                <p>Model parsimony is an important emphcognitive bias in data-drivenmodelling that aids interpretability and helps to prevent over-fitting. Sparseidentification of nonlinear dynamics SINDy methods are able to learn sparserepresentations of complex dynamics directly from data given a basis oflibrary functions. In this work a novel Bayesian treatment of dictionarylearning system identification as an alternative to SINDy is envisaged. Theproposed method -- Bayesian identification of nonlinear dynamics BINDy -- isdistinct from previous approaches in that it targets the full joint posteriordistribution over both the terms in the library and their parameterisation inthe model. This formulation confers the advantage that an arbitrary prior maybe placed over the model structure to produce models that are sparse in themodel space rather than in parameter space. Because this posterior is definedover parameter vectors that can change in dimension the inference cannot beperformed by standard techniques. Instead a Gibbs sampler based onreversible-jump Markov-chain Monte-Carlo is proposed. BINDy is shown to comparefavourably to ensemble SINDy in three benchmark case-studies. In particular itis seen that the proposed method is better able to assign high probability tocorrect model terms.</p>
                <p>Last Updated: 2024-08-15 10:03:30 UTC</p>
                <button class="interpret-button" data-id="2408.08062v1">Interpret</button>
                <div id="interpretation-2408.08062v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Can Large Language Models Understand Symbolic Graphics Programs?</h3>
                <p>Authors: Zeju QiuWeiyang LiuHaiwen FengZhen LiuTim Z. XiaoKatherine M. CollinsJoshua B. TenenbaumAdrian WellerMichael J. BlackBernhard Schölkopf</p>
                <p><a href="http://arxiv.org/abs/2408.08313v1">Link to paper</a></p>
                <p>Assessing the capabilities of large language models LLMs is oftenchallenging in part because it is hard to find tasks to which they have notbeen exposed during training. We take one step to address this challenge byturning to a new task: focusing on symbolic graphics programs which are apopular representation for graphics content that procedurally generates visualdata. LLMs have shown exciting promise towards program synthesis but do theyunderstand symbolic graphics programs Unlike conventional programs symbolicgraphics programs can be translated to graphics content. Here we characterizean LLMs understanding of symbolic programs in terms of their ability to answerquestions related to the graphics content. This task is challenging as thequestions are difficult to answer from the symbolic programs alone -- yet theywould be easy to answer from the corresponding graphics content as we verifythrough a human experiment. To understand symbolic programs LLMs may need topossess the ability to imagine how the corresponding graphics content wouldlook without directly accessing the rendered visual content. We use this taskto evaluate LLMs by creating a large benchmark for the semantic understandingof symbolic graphics programs. This benchmark is built via program-graphicscorrespondence hence requiring minimal human efforts. We evaluate current LLMson our benchmark to elucidate a preliminary assessment of their ability toreason about visual scenes from programs. We find that this task distinguishesexisting LLMs and models considered good at reasoning perform better. Lastlywe introduce Symbolic Instruction Tuning SIT to improve this ability.Specifically we query GPT4-o with questions and images generated by symbolicprograms. Such data are then used to finetune an LLM. We also find that SITdata can improve the general instruction following ability of LLMs.</p>
                <p>Last Updated: 2024-08-15 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2408.08313v1">Interpret</button>
                <div id="interpretation-2408.08313v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>HyperTaxel: Hyper-Resolution for Taxel-Based Tactile Signals Through Contrastive Learning</h3>
                <p>Authors: Hongyu LiSnehal DikhaleJinda CuiSoshi IbaNawid Jamali</p>
                <p><a href="http://arxiv.org/abs/2408.08312v1">Link to paper</a></p>
                <p>To achieve dexterity comparable to that of humans robots must intelligentlyprocess tactile sensor data. Taxel-based tactile signals often have lowspatial-resolution with non-standardized representations. In this paper wepropose a novel framework HyperTaxel for learning a geometrically-informedrepresentation of taxel-based tactile signals to address challenges associatedwith their spatial resolution. We use this representation and a contrastivelearning objective to encode and map sparse low-resolution taxel signals tohigh-resolution contact surfaces. To address the uncertainty inherent in thesesignals we leverage joint probability distributions across multiplesimultaneous contacts to improve taxel hyper-resolution. We evaluate ourrepresentation by comparing it with two baselines and present results thatsuggest our representation outperforms the baselines. Furthermore we presentqualitative results that demonstrate the learned representation captures thegeometric features of the contact surface such as flatness curvature andedges and generalizes across different objects and sensor configurations.Moreover we present results that suggest our representation improves theperformance of various downstream tasks such as surface classification 6Din-hand pose estimation and sim-to-real transfer.</p>
                <p>Last Updated: 2024-08-15 17:59:53 UTC</p>
                <button class="interpret-button" data-id="2408.08312v1">Interpret</button>
                <div id="interpretation-2408.08312v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Benchmarking the Capabilities of Large Language Models in Transportation System Engineering: Accuracy, Consistency, and Reasoning Behaviors</h3>
                <p>Authors: Usman SyedEthan LightXingang GuoHuan ZhangLianhui QinYanfeng OuyangBin Hu</p>
                <p><a href="http://arxiv.org/abs/2408.08302v1">Link to paper</a></p>
                <p>In this paper we explore the capabilities of state-of-the-art large languagemodels LLMs such as GPT-4 GPT-4o Claude 3.5 Sonnet Claude 3 Opus Gemini1.5 Pro Llama 3 and Llama 3.1 in solving some selected undergraduate-leveltransportation engineering problems. We introduce TransportBench a benchmarkdataset that includes a sample of transportation engineering problems on a widerange of subjects in the context of planning design management and controlof transportation systems. This dataset is used by human experts to evaluatethe capabilities of various commercial and open-sourced LLMs especially theiraccuracy consistency and reasoning behaviors in solving transportationengineering problems. Our comprehensive analysis uncovers the unique strengthsand limitations of each LLM e.g. our analysis shows the impressive accuracyand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solvingTransportBench problems. Our study marks a thrilling first step towardharnessing artificial general intelligence for complex transportationchallenges.</p>
                <p>Last Updated: 2024-08-15 17:55:45 UTC</p>
                <button class="interpret-button" data-id="2408.08302v1">Interpret</button>
                <div id="interpretation-2408.08302v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training</h3>
                <p>Authors: Gengwei ZhangLiyuan WangGuoliang KangLing ChenYunchao Wei</p>
                <p><a href="http://arxiv.org/abs/2408.08295v1">Link to paper</a></p>
                <p>In recent years continual learning with pre-training CLPT has receivedwidespread interest instead of its traditional focus of training from scratch.The use of strong pre-trained models PTMs can greatly facilitate knowledgetransfer and alleviate catastrophic forgetting but also suffers fromprogressive overfitting of pre-trained knowledge into specific downstreamtasks. A majority of current efforts often keep the PTMs frozen and incorporatetask-specific prompts to instruct representation learning coupled with aprompt selection process for inference. However due to the limited capacity ofprompt parameters this strategy demonstrates only sub-optimal performance incontinual learning. In comparison tuning all parameters of PTMs often providesthe greatest potential for representation learning making sequentialfine-tuning Seq FT a fundamental baseline that has been overlooked in CLPT.To this end we present an in-depth analysis of the progressive overfittingproblem from the lens of Seq FT. Considering that the overly fastrepresentation learning and the biased classification layer constitute thisparticular problem we introduce the advanced Slow Learner with ClassifierAlignment SLCA framework to unleash the power of Seq FT serving as astrong baseline approach for CLPT. Our approach involves a Slow Learner toselectively reduce the learning rate of backbone parameters and a ClassifierAlignment to align the disjoint classification layers in a post-hoc fashion. Wefurther enhance the efficacy of SL with a symmetric cross-entropy loss as wellas employ a parameter-efficient strategy to implement Seq FT with SLCA.Across a variety of continual learning scenarios on image classificationbenchmarks our approach provides substantial improvements and outperformsstate-of-the-art methods by a large margin. Code:https://github.com/GengDavid/SLCA.</p>
                <p>Last Updated: 2024-08-15 17:50:07 UTC</p>
                <button class="interpret-button" data-id="2408.08295v1">Interpret</button>
                <div id="interpretation-2408.08295v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Autonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model</h3>
                <p>Authors: Jin WangArturo LaurenziNikos Tsagarakis</p>
                <p><a href="http://arxiv.org/abs/2408.08282v1">Link to paper</a></p>
                <p>Enabling humanoid robots to perform autonomously loco-manipulation inunstructured environments is crucial and highly challenging for achievingembodied intelligence. This involves robots being able to plan their actionsand behaviors in long-horizon tasks while using multi-modality to perceivedeviations between task execution and high-level planning. Recently largelanguage models LLMs have demonstrated powerful planning and reasoningcapabilities for comprehension and processing of semantic information throughrobot control tasks as well as the usability of analytical judgment anddecision-making for multi-modal inputs. To leverage the power of LLMs towardshumanoid loco-manipulation we propose a novel language-model based frameworkthat enables robots to autonomously plan behaviors and low-level executionunder given textual instructions while observing and correcting failures thatmay occur during task execution. To systematically evaluate this framework ingrounding LLMs we created the robot action and sensing behavior libraryfor task planning and conducted mobile manipulation tasks and experiments inboth simulated and real environments using the CENTAURO robot and verified theeffectiveness and application of this approach in robotic tasks with autonomousbehavioral planning.</p>
                <p>Last Updated: 2024-08-15 17:33:32 UTC</p>
                <button class="interpret-button" data-id="2408.08282v1">Interpret</button>
                <div id="interpretation-2408.08282v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-08-17</p>
        </div>
    
        </div>
    </body>
    </html>
    