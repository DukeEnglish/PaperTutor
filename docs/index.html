
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>X-VILA: Cross-Modality Alignment for Large Language Model</h3>
                <p>Authors: Hanrong YeDe-An HuangYao LuZhiding YuWei PingAndrew TaoJan KautzSong HanDan XuPavlo MolchanovHongxu Yin</p>
                <p><a href="http://arxiv.org/abs/2405.19335v1">Link to paper</a></p>
                <p>We introduce X-VILA an omni-modality model designed to extend thecapabilities of large language models LLMs by incorporating image video andaudio modalities. By aligning modality-specific encoders with LLM inputs anddiffusion decoders with LLM outputs X-VILA achieves cross-modalityunderstanding reasoning and generation. To facilitate this cross-modalityalignment we curate an effective interleaved any-to-any modalityinstruction-following dataset. Furthermore we identify a significant problemwith the current cross-modality alignment method which results in visualinformation loss. To address the issue we propose a visual alignment mechanismwith a visual embedding highway module. We then introduce a resource-efficientrecipe for training X-VILA that exhibits proficiency in any-to-any modalityconversation surpassing previous approaches by large margins. X-VILA alsoshowcases emergent properties across modalities even in the absence of similartraining data. The project will be made open-source.</p>
                <p>Last Updated: 2024-05-29 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2405.19335v1">Interpret</button>
                <div id="interpretation-2405.19335v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLMs Meet Multimodal Generation and Editing: A Survey</h3>
                <p>Authors: Yingqing HeZhaoyang LiuJingye ChenZeyue TianHongyu LiuXiaowei ChiRuntao LiuRuibin YuanYazhou XingWenhai WangJifeng DaiYong ZhangWei XueQifeng LiuYike GuoQifeng Chen</p>
                <p><a href="http://arxiv.org/abs/2405.19334v1">Link to paper</a></p>
                <p>With the recent advancement in large language models LLMs there is agrowing interest in combining LLMs with multimodal learning. Previous surveysof multimodal large language models MLLMs mainly focus on understanding. Thissurvey elaborates on multimodal generation across different domains includingimage video 3D and audio where we highlight the notable advancements withmilestone works in these fields. Specifically we exhaustively investigate thekey technical components behind methods and multimodal datasets utilized inthese studies. Moreover we dig into tool-augmented multimodal agents that canuse existing generative models for human-computer interaction. Lastly we alsocomprehensively discuss the advancement in AI safety and investigate emergingapplications as well as future prospects. Our work provides a systematic andinsightful overview of multimodal generation which is expected to advance thedevelopment of Artificial Intelligence for Generative Content AIGC and worldmodels. A curated list of all related papers can be found athttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation</p>
                <p>Last Updated: 2024-05-29 17:59:20 UTC</p>
                <button class="interpret-button" data-id="2405.19334v1">Interpret</button>
                <div id="interpretation-2405.19334v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series</h3>
                <p>Authors: Ge ZhangScott QuJiaheng LiuChenchen ZhangChenghua LinChou Leuang YuDanny PanEsther ChengJie LiuQunshu LinRaven YuanTuney ZhengWei PangXinrun DuYiming LiangYinghao MaYizhi LiZiyang MaBill LinEmmanouil BenetosHuan YangJunting ZhouKaijing MaMinghao LiuMorry NiuNoah WangQuehry QueRuibo LiuSine LiuShawn GuoSoren GaoWangchunshu ZhouXinyue ZhangYizhi ZhouYubo WangYuelin BaiYuhan ZhangYuxiang ZhangZenith WangZhenzhu YangZijian ZhaoJiajun ZhangWanli OuyangWenhao HuangWenhu Chen</p>
                <p><a href="http://arxiv.org/abs/2405.19327v1">Link to paper</a></p>
                <p>Large Language Models LLMs have made great strides in recent years toachieve unprecedented performance across different tasks. However due tocommercial interest the most competitive models like GPT Gemini and Claudehave been gated behind proprietary interfaces without disclosing the trainingdetails. Recently many institutions have open-sourced several strong LLMs likeLLaMA-3 comparable to existing closed-source LLMs. However only the modelsweights are provided with most details e.g. intermediate checkpointspre-training corpus and training code etc. being undisclosed. To improve thetransparency of LLMs the research community has formed to open-source trulyopen LLMs e.g. Pythia Amber OLMo where more details e.g. pre-trainingcorpus and training code are being provided. These models have greatlyadvanced the scientific study of these large models including their strengthsweaknesses biases and risks. However we observe that the existing truly openLLMs on reasoning knowledge and coding tasks are still inferior to existingstate-of-the-art LLMs with similar model sizes. To this end we open-sourceMAP-Neo a highly capable and transparent bilingual language model with 7Bparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is thefirst fully open-sourced bilingual LLM with comparable performance compared toexisting state-of-the-art LLMs. Moreover we open-source all details toreproduce our MAP-Neo where the cleaned pre-training corpus data cleaningpipeline checkpoints and well-optimized training/evaluation framework areprovided. Finally we hope our MAP-Neo will enhance and strengthen the openresearch community and inspire more innovations and creativities to facilitatethe further improvements of LLMs.</p>
                <p>Last Updated: 2024-05-29 17:57:16 UTC</p>
                <button class="interpret-button" data-id="2405.19327v1">Interpret</button>
                <div id="interpretation-2405.19327v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Nearest Neighbor Speculative Decoding for LLM Generation and Attribution</h3>
                <p>Authors: Minghan LiXilun ChenAri HoltzmanBeidi ChenJimmy LinWen-tau YihXi Victoria Lin</p>
                <p><a href="http://arxiv.org/abs/2405.19325v1">Link to paper</a></p>
                <p>Large language models LLMs often hallucinate and lack the ability toprovide attribution for their generations. Semi-parametric LMs such as kNN-LMapproach these limitations by refining the output of an LM for a given promptusing its nearest neighbor matches in a non-parametric data store. Howeverthese models often exhibit slow inference speeds and produce non-fluent texts.In this paper we introduce Nearest Neighbor Speculative Decoding NEST anovel semi-parametric language modeling approach that is capable ofincorporating real-world text spans of arbitrary length into the LM generationsand providing attribution to their sources. NEST performs token-level retrievalat each inference step to compute a semi-parametric mixture distribution andidentify promising span continuations in a corpus. It then uses an approximatespeculative decoding procedure that accepts a prefix of the retrieved span orgenerates a new token. NEST significantly enhances the generation quality andattribution rate of the base LM across a variety of knowledge-intensive taskssurpassing the conventional kNN-LM method and performing competitively within-context retrieval augmentation. In addition NEST substantially improves thegeneration speed achieving a 1.8x speedup in inference time when applied toLlama-2-Chat 70B.</p>
                <p>Last Updated: 2024-05-29 17:55:03 UTC</p>
                <button class="interpret-button" data-id="2405.19325v1">Interpret</button>
                <div id="interpretation-2405.19325v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Are Large Language Models Chameleons?</h3>
                <p>Authors: Mingmeng GengSihong HeRoberto Trotta</p>
                <p><a href="http://arxiv.org/abs/2405.19323v1">Link to paper</a></p>
                <p>Do large language models LLMs have their own worldviews and personalitytendencies Simulations in which an LLM was asked to answer subjectivequestions were conducted more than 1 million times. Comparison of the responsesfrom different LLMs with real data from the European Social Survey ESSsuggests that the effect of prompts on bias and variability is fundamentalhighlighting major cultural age and gender biases. Methods for measuring thedifference between LLMs and survey data are discussed such as calculatingweighted means and a new proposed measure inspired by Jaccard similarity. Weconclude that it is important to analyze the robustness and variability ofprompts before using LLMs to model individual decisions or collective behavioras their imitation abilities are approximate at best.</p>
                <p>Last Updated: 2024-05-29 17:54:22 UTC</p>
                <button class="interpret-button" data-id="2405.19323v1">Interpret</button>
                <div id="interpretation-2405.19323v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF</h3>
                <p>Authors: Shicong CenJincheng MeiKatayoon GoshvadiHanjun DaiTong YangSherry YangDale SchuurmansYuejie ChiBo Dai</p>
                <p><a href="http://arxiv.org/abs/2405.19320v1">Link to paper</a></p>
                <p>Reinforcement learning from human feedback RLHF has demonstrated greatpromise in aligning large language models LLMs with human preference.Depending on the availability of preference data both online and offline RLHFare active areas of investigation. A key bottleneck is understanding how toincorporate uncertainty estimation in the reward function learned from thepreference data for RLHF regardless of how the preference data is collected.While the principles of optimism or pessimism under uncertainty arewell-established in standard reinforcement learning RL apractically-implementable and theoretically-grounded form amenable to largelanguage models is not yet available as standard techniques for constructingconfidence intervals become intractable under arbitrary policyparameterizations.  In this paper we introduce a unified approach to online and offline RLHF --value-incentivized preference optimization VPO -- which regularizes themaximum-likelihood estimate of the reward function with the corresponding valuefunction modulated by a textitsign to indicate whether the optimism orpessimism is chosen. VPO also directly optimizes the policy with implicitreward modeling and therefore shares a simpler RLHF pipeline similar to directpreference optimization. Theoretical guarantees of VPO are provided for bothonline and offline settings matching the rates of their standard RLcounterparts. Moreover experiments on text summarization and dialog verify thepracticality and effectiveness of VPO.</p>
                <p>Last Updated: 2024-05-29 17:51:42 UTC</p>
                <button class="interpret-button" data-id="2405.19320v1">Interpret</button>
                <div id="interpretation-2405.19320v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Adaptive Generalized Neyman Allocation: Local Asymptotic Minimax Optimal Best Arm Identification</h3>
                <p>Authors: Masahiro Kato</p>
                <p><a href="http://arxiv.org/abs/2405.19317v1">Link to paper</a></p>
                <p>This study investigates a local asymptotic minimax optimal strategy forfixed-budget best arm identification BAI. We propose the Adaptive GeneralizedNeyman Allocation AGNA strategy and show that its worst-case upper bound ofthe probability of misidentifying the best arm aligns with the worst-case lowerbound under the small-gap regime where the gap between the expected outcomesof the best and suboptimal arms is small. Our strategy corresponds to ageneralization of the Neyman allocation for two-armed bandits Neyman 1934Kaufmann et al. 2016 and a refinement of existing strategies such as the onesproposed by Glynn  Juneja 2004 and Shin et al. 2018. Compared to Komiyamaet al. 2022 which proposes a minimax rate-optimal strategy our proposedstrategy has a tighter upper bound that exactly matches the lower boundincluding the constant terms by restricting the class of distributions to theclass of small-gap distributions. Our result contributes to the longstandingopen issue about the existence of asymptotically optimal strategies infixed-budget BAI by presenting the local asymptotic minimax optimal strategy.</p>
                <p>Last Updated: 2024-05-29 17:43:13 UTC</p>
                <button class="interpret-button" data-id="2405.19317v1">Interpret</button>
                <div id="interpretation-2405.19317v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Valid Conformal Prediction for Dynamic GNNs</h3>
                <p>Authors: Ed DavisIan GallagherDaniel John LawsonPatrick Rubin-Delanchy</p>
                <p><a href="http://arxiv.org/abs/2405.19230v1">Link to paper</a></p>
                <p>Graph neural networks GNNs are powerful black-box models which have shownimpressive empirical performance. However without any form of uncertaintyquantification it can be difficult to trust such models in high-riskscenarios. Conformal prediction aims to address this problem however anassumption of exchangeability is required for its validity which has limitedits applicability to static graphs and transductive regimes. We propose to useunfolding which allows any existing static GNN to output a dynamic graphembedding with exchangeability properties. Using this we extend the validityof conformal prediction to dynamic GNNs in both transductive and semi-inductiveregimes. We provide a theoretical guarantee of valid conformal prediction inthese cases and demonstrate the empirical validity as well as the performancegains of unfolded GNNs against standard GNN architectures on both simulatedand real datasets.</p>
                <p>Last Updated: 2024-05-29 16:07:39 UTC</p>
                <button class="interpret-button" data-id="2405.19230v1">Interpret</button>
                <div id="interpretation-2405.19230v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Matrix Manifold Neural Networks++</h3>
                <p>Authors: Xuan Son NguyenShuo YangAymeric Histace</p>
                <p><a href="http://arxiv.org/abs/2405.19206v1">Link to paper</a></p>
                <p>Deep neural networks DNNs on Riemannian manifolds have garnered increasinginterest in various applied areas. For instance DNNs on spherical andhyperbolic manifolds have been designed to solve a wide range of computervision and nature language processing tasks. One of the key factors thatcontribute to the success of these networks is that spherical and hyperbolicmanifolds have the rich algebraic structures of gyrogroups and gyrovectorspaces. This enables principled and effective generalizations of the mostsuccessful DNNs to these manifolds. Recently some works have shown that manyconcepts in the theory of gyrogroups and gyrovector spaces can also begeneralized to matrix manifolds such as Symmetric Positive Definite SPD andGrassmann manifolds. As a result some building blocks for SPD and Grassmannneural networks e.g. isometric models and multinomial logistic regressionMLR can be derived in a way that is fully analogous to their spherical andhyperbolic counterparts. Building upon these works we design fully-connectedFC and convolutional layers for SPD neural networks. We also develop MLR onSymmetric Positive Semi-definite SPSD manifolds and propose a method forperforming backpropagation with the Grassmann logarithmic map in the projectorperspective. We demonstrate the effectiveness of the proposed approach in thehuman action recognition and node classification tasks.</p>
                <p>Last Updated: 2024-05-29 15:47:35 UTC</p>
                <button class="interpret-button" data-id="2405.19206v1">Interpret</button>
                <div id="interpretation-2405.19206v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Online Linear Regression in Dynamic Environments via Discounting</h3>
                <p>Authors: Andrew JacobsenAshok Cutkosky</p>
                <p><a href="http://arxiv.org/abs/2405.19175v1">Link to paper</a></p>
                <p>We develop algorithms for online linear regression which achieve optimalstatic and dynamic regret guarantees empheven in the complete absence ofprior knowledge. We present a novel analysis showing that a discounted variantof the Vovk-Azoury-Warmuth forecaster achieves dynamic regret of the formR_Tvecule OleftdlogTveesqrtdP_TgammavecuTright where P_Tgammavecu is ameasure of variability of the comparator sequence and show that the discountfactor achieving this result can be learned on-the-fly. We show that thisresult is optimal by providing a matching lower bound. We also extend ourresults to emphstrongly-adaptive guarantees which hold over everysub-interval absubseteq1T simultaneously.</p>
                <p>Last Updated: 2024-05-29 15:17:53 UTC</p>
                <button class="interpret-button" data-id="2405.19175v1">Interpret</button>
                <div id="interpretation-2405.19175v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>X-VILA: Cross-Modality Alignment for Large Language Model</h3>
                <p>Authors: Hanrong YeDe-An HuangYao LuZhiding YuWei PingAndrew TaoJan KautzSong HanDan XuPavlo MolchanovHongxu Yin</p>
                <p><a href="http://arxiv.org/abs/2405.19335v1">Link to paper</a></p>
                <p>We introduce X-VILA an omni-modality model designed to extend thecapabilities of large language models LLMs by incorporating image video andaudio modalities. By aligning modality-specific encoders with LLM inputs anddiffusion decoders with LLM outputs X-VILA achieves cross-modalityunderstanding reasoning and generation. To facilitate this cross-modalityalignment we curate an effective interleaved any-to-any modalityinstruction-following dataset. Furthermore we identify a significant problemwith the current cross-modality alignment method which results in visualinformation loss. To address the issue we propose a visual alignment mechanismwith a visual embedding highway module. We then introduce a resource-efficientrecipe for training X-VILA that exhibits proficiency in any-to-any modalityconversation surpassing previous approaches by large margins. X-VILA alsoshowcases emergent properties across modalities even in the absence of similartraining data. The project will be made open-source.</p>
                <p>Last Updated: 2024-05-29 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2405.19335v1">Interpret</button>
                <div id="interpretation-2405.19335v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLMs Meet Multimodal Generation and Editing: A Survey</h3>
                <p>Authors: Yingqing HeZhaoyang LiuJingye ChenZeyue TianHongyu LiuXiaowei ChiRuntao LiuRuibin YuanYazhou XingWenhai WangJifeng DaiYong ZhangWei XueQifeng LiuYike GuoQifeng Chen</p>
                <p><a href="http://arxiv.org/abs/2405.19334v1">Link to paper</a></p>
                <p>With the recent advancement in large language models LLMs there is agrowing interest in combining LLMs with multimodal learning. Previous surveysof multimodal large language models MLLMs mainly focus on understanding. Thissurvey elaborates on multimodal generation across different domains includingimage video 3D and audio where we highlight the notable advancements withmilestone works in these fields. Specifically we exhaustively investigate thekey technical components behind methods and multimodal datasets utilized inthese studies. Moreover we dig into tool-augmented multimodal agents that canuse existing generative models for human-computer interaction. Lastly we alsocomprehensively discuss the advancement in AI safety and investigate emergingapplications as well as future prospects. Our work provides a systematic andinsightful overview of multimodal generation which is expected to advance thedevelopment of Artificial Intelligence for Generative Content AIGC and worldmodels. A curated list of all related papers can be found athttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation</p>
                <p>Last Updated: 2024-05-29 17:59:20 UTC</p>
                <button class="interpret-button" data-id="2405.19334v1">Interpret</button>
                <div id="interpretation-2405.19334v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-Modal Generative Embedding Model</h3>
                <p>Authors: Feipeng MaHongwei XueGuangting WangYizhou ZhouFengyun RaoShilin YanYueyi ZhangSiying WuMike Zheng ShouXiaoyan Sun</p>
                <p><a href="http://arxiv.org/abs/2405.19333v1">Link to paper</a></p>
                <p>Most multi-modal tasks can be formulated into problems of either generationor embedding. Existing models usually tackle these two types of problems bydecoupling language modules into a text decoder for generation and a textencoder for embedding. To explore the minimalism of multi-modal paradigms weattempt to achieve only one model per modality in this work. We propose aMulti-Modal Generative Embedding Model MM-GEM whereby the generative andembedding objectives are encapsulated in one Large Language Model. We alsopropose a PoolAggregator to boost efficiency and enable the ability offine-grained embedding and generation. A surprising finding is that these twoobjectives do not significantly conflict with each other. For example MM-GEMinstantiated from ViT-Large and TinyLlama shows competitive performance onbenchmarks for multimodal embedding models such as cross-modal retrieval andzero-shot classification while has good ability of image captioning.Additionally MM-GEM can seamlessly execute region-level image captiongeneration and retrieval tasks. Besides the advanced text model in MM-GEMbrings over 5 improvement in Recall1 for long text and image retrieval.</p>
                <p>Last Updated: 2024-05-29 17:59:10 UTC</p>
                <button class="interpret-button" data-id="2405.19333v1">Interpret</button>
                <div id="interpretation-2405.19333v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>NPGA: Neural Parametric Gaussian Avatars</h3>
                <p>Authors: Simon GiebenhainTobias KirschsteinMartin RünzLourdes AgapitoMatthias Nießner</p>
                <p><a href="http://arxiv.org/abs/2405.19331v1">Link to paper</a></p>
                <p>The creation of high-fidelity digital versions of human heads is animportant stepping stone in the process of further integrating virtualcomponents into our everyday lives. Constructing such avatars is a challengingresearch problem due to a high demand for photo-realism and real-timerendering performance. In this work we propose Neural Parametric GaussianAvatars NPGA a data-driven approach to create high-fidelity controllableavatars from multi-view video recordings. We build our method around 3DGaussian Splatting for its highly efficient rendering and to inherit thetopological flexibility of point clouds. In contrast to previous work wecondition our avatars dynamics on the rich expression space of neuralparametric head models NPHM instead of mesh-based 3DMMs. To this end wedistill the backward deformation field of our underlying NPHM into forwarddeformations which are compatible with rasterization-based rendering. Allremaining fine-scale expression-dependent details are learned from themulti-view videos. To increase the representational capacity of our avatars weaugment the canonical Gaussian point cloud using per-primitive latent featureswhich govern its dynamic behavior. To regularize this increased dynamicexpressivity we propose Laplacian terms on the latent features and predicteddynamics. We evaluate our method on the public NeRSemble dataset demonstratingthat NPGA significantly outperforms the previous state-of-the-art avatars onthe self-reenactment task by 2.6 PSNR. Furthermore we demonstrate accurateanimation capabilities from real-world monocular videos.</p>
                <p>Last Updated: 2024-05-29 17:58:09 UTC</p>
                <button class="interpret-button" data-id="2405.19331v1">Interpret</button>
                <div id="interpretation-2405.19331v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models</h3>
                <p>Authors: Tianrun ChenChunan YuJing LiJianqi ZhangLanyun ZhuDeyi JiYong ZhangYing ZangZejian LiLingyun Sun</p>
                <p><a href="http://arxiv.org/abs/2405.19326v1">Link to paper</a></p>
                <p>In this paper we introduce a new task: Zero-Shot 3D Reasoning Segmentationfor parts searching and localization for objects which is a new paradigm to 3Dsegmentation that transcends limitations for previous category-specific 3Dsemantic segmentation 3D instance segmentation and open-vocabulary 3Dsegmentation. We design a simple baseline method Reasoning3D with thecapability to understand and execute complex commands for fine-grainedsegmenting specific parts for 3D meshes with contextual awareness and reasonedanswers for interactive segmentation. Specifically Reasoning3D leverages anoff-the-shelf pre-trained 2D segmentation network powered by Large LanguageModels LLMs to interpret user input queries in a zero-shot manner. Previousresearch have shown that extensive pre-training endows foundation models withprior world knowledge enabling them to comprehend complex commands acapability we can harness to segment anything in 3D with limited 3D datasetssource efficient. Experimentation reveals that our approach is generalizableand can effectively localize and highlight parts of 3D objects in 3D meshbased on implicit textual queries including these articulated 3d objects andreal-world scanned data. Our method can also generate natural languageexplanations corresponding to these 3D models and the decomposition. Moreoverour training-free approach allows rapid deployment and serves as a viableuniversal baseline for future research of part-level 3d semantic objectunderstanding in various fields including robotics object manipulation partassembly autonomous driving applications augment reality and virtual realityAR/VR and medical applications. The code the model weight the deploymentguide and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/</p>
                <p>Last Updated: 2024-05-29 17:56:07 UTC</p>
                <button class="interpret-button" data-id="2405.19326v1">Interpret</button>
                <div id="interpretation-2405.19326v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models</h3>
                <p>Authors: Tianrun ChenChunan YuJing LiJianqi ZhangLanyun ZhuDeyi JiYong ZhangYing ZangZejian LiLingyun Sun</p>
                <p><a href="http://arxiv.org/abs/2405.19326v1">Link to paper</a></p>
                <p>In this paper we introduce a new task: Zero-Shot 3D Reasoning Segmentationfor parts searching and localization for objects which is a new paradigm to 3Dsegmentation that transcends limitations for previous category-specific 3Dsemantic segmentation 3D instance segmentation and open-vocabulary 3Dsegmentation. We design a simple baseline method Reasoning3D with thecapability to understand and execute complex commands for fine-grainedsegmenting specific parts for 3D meshes with contextual awareness and reasonedanswers for interactive segmentation. Specifically Reasoning3D leverages anoff-the-shelf pre-trained 2D segmentation network powered by Large LanguageModels LLMs to interpret user input queries in a zero-shot manner. Previousresearch have shown that extensive pre-training endows foundation models withprior world knowledge enabling them to comprehend complex commands acapability we can harness to segment anything in 3D with limited 3D datasetssource efficient. Experimentation reveals that our approach is generalizableand can effectively localize and highlight parts of 3D objects in 3D meshbased on implicit textual queries including these articulated 3d objects andreal-world scanned data. Our method can also generate natural languageexplanations corresponding to these 3D models and the decomposition. Moreoverour training-free approach allows rapid deployment and serves as a viableuniversal baseline for future research of part-level 3d semantic objectunderstanding in various fields including robotics object manipulation partassembly autonomous driving applications augment reality and virtual realityAR/VR and medical applications. The code the model weight the deploymentguide and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/</p>
                <p>Last Updated: 2024-05-29 17:56:07 UTC</p>
                <button class="interpret-button" data-id="2405.19326v1">Interpret</button>
                <div id="interpretation-2405.19326v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Future of Child Development in the AI Era. Cross-Disciplinary Perspectives Between AI and Child Development Experts</h3>
                <p>Authors: Mathilde Neugnot-CerioliOlga Muss Laurenty</p>
                <p><a href="http://arxiv.org/abs/2405.19275v1">Link to paper</a></p>
                <p>This report explores the potential implications of rapidly integratingArtificial Intelligence AI applications into childrens environments. Theintroduction of AI in our daily lives necessitates scrutiny considering thesignificant role of the environment in shaping cognition socio-emotionalskills and behaviors especially during the first 25 years of cerebraldevelopment. As AI becomes prevalent in educational and leisure activities itwill significantly modify the experiences of children and adolescentspresenting both challenges and opportunities for their developmentaltrajectories. This analysis was informed by consulting with 15 experts frompertinent disciplines AI product development child development andneurosciences along with a comprehensive review of scientific literature onchildren development and child-technology interactions. Overall AI expertsanticipate that AI will transform leisure activities revolutionize educationand redefine human-machine interactions. While AI offers substantial benefitsin fostering interactive engagement it also poses risks that require carefulconsiderations especially during sensitive developmental periods. The reportadvocates for proactive international collaboration across multiple disciplinesand increased research into how technological innovations affect childdevelopment. Such efforts are crucial for designing a sustainable and ethicalfuture for the next generation through specific child-centered regulations andhelping to educate all potential stakeholders regulators developers parentsand educators children about responsible AI use and its potential impacts onchild development.</p>
                <p>Last Updated: 2024-05-29 17:07:02 UTC</p>
                <button class="interpret-button" data-id="2405.19275v1">Interpret</button>
                <div id="interpretation-2405.19275v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Personalized Interiors at Scale: Leveraging AI for Efficient and Customizable Design Solutions</h3>
                <p>Authors: Kaiwen ZhouTianyu Wang</p>
                <p><a href="http://arxiv.org/abs/2405.19188v1">Link to paper</a></p>
                <p>In this paper we introduce an innovative application of artificialintelligence in the realm of interior design through the integration of StableDiffusion and Dreambooth models. This paper explores the potential of theseadvanced generative models to streamline and democratize the process of roominterior generation offering a significant departure from conventionallabor-intensive techniques. Our approach leverages the capabilities of StableDiffusion for generating high-quality images and Dreambooth for rapidcustomization with minimal training data addressing the need for efficiencyand personalization in the design industry. We detail a comprehensivemethodology that combines these models providing a robust framework for thecreation of tailored room interiors that reflect individual tastes andfunctional requirements. We presents an extensive evaluation of our methodsupported by experimental results that demonstrate its effectiveness and aseries of case studies that illustrate its practical application in interiordesign projects. Our study contributes to the ongoing discourse on the role ofAI in creative fields highlighting the benefits of leveraging generativemodels to enhance creativity and reshape the future of interior design.</p>
                <p>Last Updated: 2024-05-29 15:29:21 UTC</p>
                <button class="interpret-button" data-id="2405.19188v1">Interpret</button>
                <div id="interpretation-2405.19188v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Alt4Blind: A User Interface to Simplify Charts Alt-Text Creation</h3>
                <p>Authors: Omar MouredShahid Ali FarooquiKarin MullerSharifeh FadaeijouybariThorsten SchwarzMohammed JavedRainer Stiefelhagen</p>
                <p><a href="http://arxiv.org/abs/2405.19111v1">Link to paper</a></p>
                <p>Alternative Texts Alt-Text for chart images are essential for makinggraphics accessible to people with blindness and visual impairments.Traditionally Alt-Text is manually written by authors but often encountersissues such as oversimplification or complication. Recent trends have seen theuse of AI for Alt-Text generation. However existing models are susceptible toproducing inaccurate or misleading information. We address this challenge byretrieving high-quality alt-texts from similar chart images serving as areference for the user when creating alt-texts. Our three contributions are asfollows: 1 we introduce a new benchmark comprising 5000 real images withsemantically labeled high-quality Alt-Texts collected from Human ComputerInteraction venues. 2 We developed a deep learning-based model to rank andretrieve similar chart images that share the same visual and textual semantics.3 We designed a user interface UI to facilitate the alt-text creationprocess. Our preliminary interviews and investigations highlight the usabilityof our UI. For the dataset and further details please refer to our projectpage: https://moured.github.io/alt4blind/.</p>
                <p>Last Updated: 2024-05-29 14:19:57 UTC</p>
                <button class="interpret-button" data-id="2405.19111v1">Interpret</button>
                <div id="interpretation-2405.19111v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Towards Standardizing AI Bias Exploration</h3>
                <p>Authors: Emmanouil KrasanakisSymeon Papadopoulos</p>
                <p><a href="http://arxiv.org/abs/2405.19022v1">Link to paper</a></p>
                <p>Creating fair AI systems is a complex problem that involves the assessment ofcontext-dependent bias concerns. Existing research and programming librariesexpress specific concerns as measures of bias that they aim to constrain ormitigate. In practice one should explore a wide variety of sometimesincompatible measures before deciding which ones warrant corrective actionbut their narrow scope means that most new situations can only be examinedafter devising new measures. In this work we present a mathematical frameworkthat distils literature measures of bias into building blocks herebyfacilitating new combinations to cover a wide range of fairness concerns suchas classification or recommendation differences across multiple multi-valuesensitive attributes e.g. many genders and races and their intersections.We show how this framework generalizes existing concepts and present frequentlyused blocks. We provide an open-source implementation of our framework as aPython library called FairBench that facilitates systematic and extensibleexploration of potential bias concerns.</p>
                <p>Last Updated: 2024-05-29 12:03:45 UTC</p>
                <button class="interpret-button" data-id="2405.19022v1">Interpret</button>
                <div id="interpretation-2405.19022v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>LLMs Meet Multimodal Generation and Editing: A Survey</h3>
                <p>Authors: Yingqing HeZhaoyang LiuJingye ChenZeyue TianHongyu LiuXiaowei ChiRuntao LiuRuibin YuanYazhou XingWenhai WangJifeng DaiYong ZhangWei XueQifeng LiuYike GuoQifeng Chen</p>
                <p><a href="http://arxiv.org/abs/2405.19334v1">Link to paper</a></p>
                <p>With the recent advancement in large language models LLMs there is agrowing interest in combining LLMs with multimodal learning. Previous surveysof multimodal large language models MLLMs mainly focus on understanding. Thissurvey elaborates on multimodal generation across different domains includingimage video 3D and audio where we highlight the notable advancements withmilestone works in these fields. Specifically we exhaustively investigate thekey technical components behind methods and multimodal datasets utilized inthese studies. Moreover we dig into tool-augmented multimodal agents that canuse existing generative models for human-computer interaction. Lastly we alsocomprehensively discuss the advancement in AI safety and investigate emergingapplications as well as future prospects. Our work provides a systematic andinsightful overview of multimodal generation which is expected to advance thedevelopment of Artificial Intelligence for Generative Content AIGC and worldmodels. A curated list of all related papers can be found athttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation</p>
                <p>Last Updated: 2024-05-29 17:59:20 UTC</p>
                <button class="interpret-button" data-id="2405.19334v1">Interpret</button>
                <div id="interpretation-2405.19334v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Self-Exploring Language Models: Active Preference Elicitation for Online Alignment</h3>
                <p>Authors: Shenao ZhangDonghan YuHiteshi SharmaZiyi YangShuohang WangHany HassanZhaoran Wang</p>
                <p><a href="http://arxiv.org/abs/2405.19332v1">Link to paper</a></p>
                <p>Preference optimization particularly through Reinforcement Learning fromHuman Feedback RLHF has achieved significant success in aligning LargeLanguage Models LLMs to adhere to human intentions. Unlike offline alignmentwith a fixed dataset online feedback collection from humans or AI on modelgenerations typically leads to more capable reward models and better-alignedLLMs through an iterative process. However achieving a globally accuratereward model requires systematic exploration to generate diverse responses thatspan the vast space of natural language. Random sampling from standardreward-maximizing LLMs alone is insufficient to fulfill this requirement. Toaddress this issue we propose a bilevel objective optimistically biasedtowards potentially high-reward responses to actively exploreout-of-distribution regions. By solving the inner-level problem with thereparameterized reward function the resulting algorithm named Self-ExploringLanguage Models SELM eliminates the need for a separate RM and iterativelyupdates the LLM with a straightforward objective. Compared to Direct PreferenceOptimization DPO the SELM objective reduces indiscriminate favor of unseenextrapolations and enhances exploration efficiency. Our experimental resultsdemonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instructmodels SELM significantly boosts the performance on instruction-followingbenchmarks such as MT-Bench and AlpacaEval 2.0 as well as various standardacademic benchmarks in different settings. Our code and models are available athttps://github.com/shenao-zhang/SELM.</p>
                <p>Last Updated: 2024-05-29 17:59:07 UTC</p>
                <button class="interpret-button" data-id="2405.19332v1">Interpret</button>
                <div id="interpretation-2405.19332v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>NPGA: Neural Parametric Gaussian Avatars</h3>
                <p>Authors: Simon GiebenhainTobias KirschsteinMartin RünzLourdes AgapitoMatthias Nießner</p>
                <p><a href="http://arxiv.org/abs/2405.19331v1">Link to paper</a></p>
                <p>The creation of high-fidelity digital versions of human heads is animportant stepping stone in the process of further integrating virtualcomponents into our everyday lives. Constructing such avatars is a challengingresearch problem due to a high demand for photo-realism and real-timerendering performance. In this work we propose Neural Parametric GaussianAvatars NPGA a data-driven approach to create high-fidelity controllableavatars from multi-view video recordings. We build our method around 3DGaussian Splatting for its highly efficient rendering and to inherit thetopological flexibility of point clouds. In contrast to previous work wecondition our avatars dynamics on the rich expression space of neuralparametric head models NPHM instead of mesh-based 3DMMs. To this end wedistill the backward deformation field of our underlying NPHM into forwarddeformations which are compatible with rasterization-based rendering. Allremaining fine-scale expression-dependent details are learned from themulti-view videos. To increase the representational capacity of our avatars weaugment the canonical Gaussian point cloud using per-primitive latent featureswhich govern its dynamic behavior. To regularize this increased dynamicexpressivity we propose Laplacian terms on the latent features and predicteddynamics. We evaluate our method on the public NeRSemble dataset demonstratingthat NPGA significantly outperforms the previous state-of-the-art avatars onthe self-reenactment task by 2.6 PSNR. Furthermore we demonstrate accurateanimation capabilities from real-world monocular videos.</p>
                <p>Last Updated: 2024-05-29 17:58:09 UTC</p>
                <button class="interpret-button" data-id="2405.19331v1">Interpret</button>
                <div id="interpretation-2405.19331v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series</h3>
                <p>Authors: Ge ZhangScott QuJiaheng LiuChenchen ZhangChenghua LinChou Leuang YuDanny PanEsther ChengJie LiuQunshu LinRaven YuanTuney ZhengWei PangXinrun DuYiming LiangYinghao MaYizhi LiZiyang MaBill LinEmmanouil BenetosHuan YangJunting ZhouKaijing MaMinghao LiuMorry NiuNoah WangQuehry QueRuibo LiuSine LiuShawn GuoSoren GaoWangchunshu ZhouXinyue ZhangYizhi ZhouYubo WangYuelin BaiYuhan ZhangYuxiang ZhangZenith WangZhenzhu YangZijian ZhaoJiajun ZhangWanli OuyangWenhao HuangWenhu Chen</p>
                <p><a href="http://arxiv.org/abs/2405.19327v1">Link to paper</a></p>
                <p>Large Language Models LLMs have made great strides in recent years toachieve unprecedented performance across different tasks. However due tocommercial interest the most competitive models like GPT Gemini and Claudehave been gated behind proprietary interfaces without disclosing the trainingdetails. Recently many institutions have open-sourced several strong LLMs likeLLaMA-3 comparable to existing closed-source LLMs. However only the modelsweights are provided with most details e.g. intermediate checkpointspre-training corpus and training code etc. being undisclosed. To improve thetransparency of LLMs the research community has formed to open-source trulyopen LLMs e.g. Pythia Amber OLMo where more details e.g. pre-trainingcorpus and training code are being provided. These models have greatlyadvanced the scientific study of these large models including their strengthsweaknesses biases and risks. However we observe that the existing truly openLLMs on reasoning knowledge and coding tasks are still inferior to existingstate-of-the-art LLMs with similar model sizes. To this end we open-sourceMAP-Neo a highly capable and transparent bilingual language model with 7Bparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is thefirst fully open-sourced bilingual LLM with comparable performance compared toexisting state-of-the-art LLMs. Moreover we open-source all details toreproduce our MAP-Neo where the cleaned pre-training corpus data cleaningpipeline checkpoints and well-optimized training/evaluation framework areprovided. Finally we hope our MAP-Neo will enhance and strengthen the openresearch community and inspire more innovations and creativities to facilitatethe further improvements of LLMs.</p>
                <p>Last Updated: 2024-05-29 17:57:16 UTC</p>
                <button class="interpret-button" data-id="2405.19327v1">Interpret</button>
                <div id="interpretation-2405.19327v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Are Large Language Models Chameleons?</h3>
                <p>Authors: Mingmeng GengSihong HeRoberto Trotta</p>
                <p><a href="http://arxiv.org/abs/2405.19323v1">Link to paper</a></p>
                <p>Do large language models LLMs have their own worldviews and personalitytendencies Simulations in which an LLM was asked to answer subjectivequestions were conducted more than 1 million times. Comparison of the responsesfrom different LLMs with real data from the European Social Survey ESSsuggests that the effect of prompts on bias and variability is fundamentalhighlighting major cultural age and gender biases. Methods for measuring thedifference between LLMs and survey data are discussed such as calculatingweighted means and a new proposed measure inspired by Jaccard similarity. Weconclude that it is important to analyze the robustness and variability ofprompts before using LLMs to model individual decisions or collective behavioras their imitation abilities are approximate at best.</p>
                <p>Last Updated: 2024-05-29 17:54:22 UTC</p>
                <button class="interpret-button" data-id="2405.19323v1">Interpret</button>
                <div id="interpretation-2405.19323v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Normative Modules: A Generative Agent Architecture for Learning Norms that Supports Multi-Agent Cooperation</h3>
                <p>Authors: Atrisha SarkarAndrei Ioan MuresanuCarter BlairAaryam SharmaRakshit S TrivediGillian K Hadfield</p>
                <p><a href="http://arxiv.org/abs/2405.19328v1">Link to paper</a></p>
                <p>Generative agents which implement behaviors using a large language modelLLM to interpret and evaluate an environment has demonstrated the capacityto solve complex tasks across many social and technological domains. Howeverwhen these agents interact with other agents and humans in presence of socialstructures such as existing norms fostering cooperation between them is afundamental challenge. In this paper we develop the framework of a NormativeModule: an architecture designed to enhance cooperation by enabling agents torecognize and adapt to the normative infrastructure of a given environment. Wefocus on the equilibrium selection aspect of the cooperation problem and informour agent design based on the existence of classification institutions thatimplement correlated equilibrium to provide effective resolution of theequilibrium selection problem. Specifically the normative module enablesagents to learn through peer interactions which of multiple candidateinstitutions in the environment does a group treat as authoritative. Byenabling normative competence in this sense agents gain ability to coordinatetheir sanctioning behaviour coordinated sanctioning behaviour in turn shapesprimary behaviour within a social environment leading to higher averagewelfare. We design a new environment that supports institutions and evaluatethe proposed framework based on two key criteria derived from agentinteractions with peers and institutions: i the agents ability to disregardnon-authoritative institutions and ii the agents ability to identifyauthoritative institutions among several options. We show that thesecapabilities allow the agent to achieve more stable cooperative outcomescompared to baseline agents without the normative module paving the way forresearch in a new avenue of designing environments and agents that account fornormative infrastructure.</p>
                <p>Last Updated: 2024-05-29 17:57:30 UTC</p>
                <button class="interpret-button" data-id="2405.19328v1">Interpret</button>
                <div id="interpretation-2405.19328v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Act Natural! Projecting Autonomous System Trajectories Into Naturalistic Behavior Sets</h3>
                <p>Authors: Hamzah I. KhanAdam J. ThorpeDavid Fridovich-Keil</p>
                <p><a href="http://arxiv.org/abs/2405.19292v1">Link to paper</a></p>
                <p>Autonomous agents operating around human actors must consider how theirbehaviors might affect those humans even when not directly interacting withthem. To this end it is often beneficial to be predictable and appearnaturalistic. Existing methods to address this problem use human actor intentmodeling or imitation learning techniques but these approaches rarely captureall possible motivations for human behavior or require significant amounts ofdata. In contrast we propose a technique for modeling naturalistic behavior asa set of convex hulls computed over a relatively small dataset of humanbehavior. Given this set we design an optimization-based filter which projectsarbitrary trajectories into it to make them more naturalistic for autonomousagents to execute while also satisfying dynamics constraints. We demonstrateour methods on real-world human driving data from the inD intersection datasetBock et al. 2020.</p>
                <p>Last Updated: 2024-05-29 17:21:25 UTC</p>
                <button class="interpret-button" data-id="2405.19292v1">Interpret</button>
                <div id="interpretation-2405.19292v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory</h3>
                <p>Authors: Mustafa Mert ÇelikokFrans A. OliehoekJan-Willem van de Meent</p>
                <p><a href="http://arxiv.org/abs/2405.19024v1">Link to paper</a></p>
                <p>We consider inverse reinforcement learning problems with concave utilities.Concave Utility Reinforcement Learning CURL is a generalisation of thestandard RL objective which employs a concave function of the state occupancymeasure rather than a linear function. CURL has garnered recent attention forits ability to represent instances of many important applications including thestandard RL such as imitation learning pure exploration constrained MDPsoffline RL human-regularized RL and others. Inverse reinforcement learning isa powerful paradigm that focuses on recovering an unknown reward function thatcan rationalize the observed behaviour of an agent. There has been recenttheoretical advances in inverse RL where the problem is formulated asidentifying the set of feasible reward functions. However inverse RL for CURLproblems has not been considered previously. In this paper we show that most ofthe standard IRL results do not apply to CURL in general since CURLinvalidates the classical Bellman equations. This calls for a new theoreticalframework for the inverse CURL problem. Using a recent equivalence resultbetween CURL and Mean-field Games we propose a new definition for the feasiblerewards for I-CURL by proving that this problem is equivalent to an inversegame theory problem in a subclass of mean-field games. We present initial queryand sample complexity results for the I-CURL problem under assumptions such asLipschitz-continuity. Finally we outline future directions and applications inhuman--AI collaboration enabled by our results.</p>
                <p>Last Updated: 2024-05-29 12:07:17 UTC</p>
                <button class="interpret-button" data-id="2405.19024v1">Interpret</button>
                <div id="interpretation-2405.19024v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Distributed Management of Fluctuating Energy Resources in Dynamic Networked Systems</h3>
                <p>Authors: Xiaotong ChengIoannis TsetisSetareh Maghsudi</p>
                <p><a href="http://arxiv.org/abs/2405.19015v1">Link to paper</a></p>
                <p>Modern power systems integrate renewable distributed energy resources DERsas an environment-friendly enhancement to meet the ever-increasing demands.However the inherent unreliability of renewable energy renders developing DERmanagement algorithms imperative. We study the energy-sharing problem in asystem consisting of several DERs. Each agent harvests and distributesrenewable energy in its neighborhood to optimize the networks performancewhile minimizing energy waste. We model this problem as a bandit convexoptimization problem with constraints that correspond to each nodeslimitations for energy production. We propose distributed decision-makingpolicies to solve the formulated problem where we utilize the notion ofdynamic regret as the performance metric. We also include an adjustmentstrategy in our developed algorithm to reduce the constraint violations.Besides we design a policy that deals with the non-stationary environment.Theoretical analysis shows the effectiveness of our proposed algorithm.Numerical experiments using a real-world dataset show superior performance ofour proposal compared to state-of-the-art methods.</p>
                <p>Last Updated: 2024-05-29 11:54:11 UTC</p>
                <button class="interpret-button" data-id="2405.19015v1">Interpret</button>
                <div id="interpretation-2405.19015v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Resilient Average Consensus with Adversaries via Distributed Detection and Recovery</h3>
                <p>Authors: Liwei YuanHideaki Ishii</p>
                <p><a href="http://arxiv.org/abs/2405.18752v1">Link to paper</a></p>
                <p>We study the problem of resilient average consensus in multi-agent systemswhere some of the agents are subject to failures or attacks. The objective ofresilient average consensus is for non-faulty/normal agents to converge to theaverage of their initial values despite the erroneous effects from maliciousagents. To this end we propose a successful distributed iterative resilientaverage consensus algorithm for the multi-agent networks with general directedtopologies. The proposed algorithm has two parts at each iteration: detectionand averaging. For the detection part we propose two distributed algorithmsand one of them can detect malicious agents with only the information fromdirect in-neighbors. For the averaging part we extend the applicability of anexisting averaging algorithm where normal agents can remove the effects frommalicious agents so far after they are detected. Another important feature ofour method is that it can handle the case where malicious agents areneighboring and collaborating with each other to mislead the normal ones fromaveraging. This case cannot be solved by existing detection approaches inrelated literature. Moreover our algorithm is efficient in storage usageespecially for large-scale networks as each agent only requires the values ofneighbors within two hops. Lastly numerical examples are given to verify theefficacy of the proposed algorithms.</p>
                <p>Last Updated: 2024-05-29 04:32:28 UTC</p>
                <button class="interpret-button" data-id="2405.18752v1">Interpret</button>
                <div id="interpretation-2405.18752v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>X-VILA: Cross-Modality Alignment for Large Language Model</h3>
                <p>Authors: Hanrong YeDe-An HuangYao LuZhiding YuWei PingAndrew TaoJan KautzSong HanDan XuPavlo MolchanovHongxu Yin</p>
                <p><a href="http://arxiv.org/abs/2405.19335v1">Link to paper</a></p>
                <p>We introduce X-VILA an omni-modality model designed to extend thecapabilities of large language models LLMs by incorporating image video andaudio modalities. By aligning modality-specific encoders with LLM inputs anddiffusion decoders with LLM outputs X-VILA achieves cross-modalityunderstanding reasoning and generation. To facilitate this cross-modalityalignment we curate an effective interleaved any-to-any modalityinstruction-following dataset. Furthermore we identify a significant problemwith the current cross-modality alignment method which results in visualinformation loss. To address the issue we propose a visual alignment mechanismwith a visual embedding highway module. We then introduce a resource-efficientrecipe for training X-VILA that exhibits proficiency in any-to-any modalityconversation surpassing previous approaches by large margins. X-VILA alsoshowcases emergent properties across modalities even in the absence of similartraining data. The project will be made open-source.</p>
                <p>Last Updated: 2024-05-29 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2405.19335v1">Interpret</button>
                <div id="interpretation-2405.19335v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Self-Exploring Language Models: Active Preference Elicitation for Online Alignment</h3>
                <p>Authors: Shenao ZhangDonghan YuHiteshi SharmaZiyi YangShuohang WangHany HassanZhaoran Wang</p>
                <p><a href="http://arxiv.org/abs/2405.19332v1">Link to paper</a></p>
                <p>Preference optimization particularly through Reinforcement Learning fromHuman Feedback RLHF has achieved significant success in aligning LargeLanguage Models LLMs to adhere to human intentions. Unlike offline alignmentwith a fixed dataset online feedback collection from humans or AI on modelgenerations typically leads to more capable reward models and better-alignedLLMs through an iterative process. However achieving a globally accuratereward model requires systematic exploration to generate diverse responses thatspan the vast space of natural language. Random sampling from standardreward-maximizing LLMs alone is insufficient to fulfill this requirement. Toaddress this issue we propose a bilevel objective optimistically biasedtowards potentially high-reward responses to actively exploreout-of-distribution regions. By solving the inner-level problem with thereparameterized reward function the resulting algorithm named Self-ExploringLanguage Models SELM eliminates the need for a separate RM and iterativelyupdates the LLM with a straightforward objective. Compared to Direct PreferenceOptimization DPO the SELM objective reduces indiscriminate favor of unseenextrapolations and enhances exploration efficiency. Our experimental resultsdemonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instructmodels SELM significantly boosts the performance on instruction-followingbenchmarks such as MT-Bench and AlpacaEval 2.0 as well as various standardacademic benchmarks in different settings. Our code and models are available athttps://github.com/shenao-zhang/SELM.</p>
                <p>Last Updated: 2024-05-29 17:59:07 UTC</p>
                <button class="interpret-button" data-id="2405.19332v1">Interpret</button>
                <div id="interpretation-2405.19332v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series</h3>
                <p>Authors: Ge ZhangScott QuJiaheng LiuChenchen ZhangChenghua LinChou Leuang YuDanny PanEsther ChengJie LiuQunshu LinRaven YuanTuney ZhengWei PangXinrun DuYiming LiangYinghao MaYizhi LiZiyang MaBill LinEmmanouil BenetosHuan YangJunting ZhouKaijing MaMinghao LiuMorry NiuNoah WangQuehry QueRuibo LiuSine LiuShawn GuoSoren GaoWangchunshu ZhouXinyue ZhangYizhi ZhouYubo WangYuelin BaiYuhan ZhangYuxiang ZhangZenith WangZhenzhu YangZijian ZhaoJiajun ZhangWanli OuyangWenhao HuangWenhu Chen</p>
                <p><a href="http://arxiv.org/abs/2405.19327v1">Link to paper</a></p>
                <p>Large Language Models LLMs have made great strides in recent years toachieve unprecedented performance across different tasks. However due tocommercial interest the most competitive models like GPT Gemini and Claudehave been gated behind proprietary interfaces without disclosing the trainingdetails. Recently many institutions have open-sourced several strong LLMs likeLLaMA-3 comparable to existing closed-source LLMs. However only the modelsweights are provided with most details e.g. intermediate checkpointspre-training corpus and training code etc. being undisclosed. To improve thetransparency of LLMs the research community has formed to open-source trulyopen LLMs e.g. Pythia Amber OLMo where more details e.g. pre-trainingcorpus and training code are being provided. These models have greatlyadvanced the scientific study of these large models including their strengthsweaknesses biases and risks. However we observe that the existing truly openLLMs on reasoning knowledge and coding tasks are still inferior to existingstate-of-the-art LLMs with similar model sizes. To this end we open-sourceMAP-Neo a highly capable and transparent bilingual language model with 7Bparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is thefirst fully open-sourced bilingual LLM with comparable performance compared toexisting state-of-the-art LLMs. Moreover we open-source all details toreproduce our MAP-Neo where the cleaned pre-training corpus data cleaningpipeline checkpoints and well-optimized training/evaluation framework areprovided. Finally we hope our MAP-Neo will enhance and strengthen the openresearch community and inspire more innovations and creativities to facilitatethe further improvements of LLMs.</p>
                <p>Last Updated: 2024-05-29 17:57:16 UTC</p>
                <button class="interpret-button" data-id="2405.19327v1">Interpret</button>
                <div id="interpretation-2405.19327v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Are Large Language Models Chameleons?</h3>
                <p>Authors: Mingmeng GengSihong HeRoberto Trotta</p>
                <p><a href="http://arxiv.org/abs/2405.19323v1">Link to paper</a></p>
                <p>Do large language models LLMs have their own worldviews and personalitytendencies Simulations in which an LLM was asked to answer subjectivequestions were conducted more than 1 million times. Comparison of the responsesfrom different LLMs with real data from the European Social Survey ESSsuggests that the effect of prompts on bias and variability is fundamentalhighlighting major cultural age and gender biases. Methods for measuring thedifference between LLMs and survey data are discussed such as calculatingweighted means and a new proposed measure inspired by Jaccard similarity. Weconclude that it is important to analyze the robustness and variability ofprompts before using LLMs to model individual decisions or collective behavioras their imitation abilities are approximate at best.</p>
                <p>Last Updated: 2024-05-29 17:54:22 UTC</p>
                <button class="interpret-button" data-id="2405.19323v1">Interpret</button>
                <div id="interpretation-2405.19323v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF</h3>
                <p>Authors: Shicong CenJincheng MeiKatayoon GoshvadiHanjun DaiTong YangSherry YangDale SchuurmansYuejie ChiBo Dai</p>
                <p><a href="http://arxiv.org/abs/2405.19320v1">Link to paper</a></p>
                <p>Reinforcement learning from human feedback RLHF has demonstrated greatpromise in aligning large language models LLMs with human preference.Depending on the availability of preference data both online and offline RLHFare active areas of investigation. A key bottleneck is understanding how toincorporate uncertainty estimation in the reward function learned from thepreference data for RLHF regardless of how the preference data is collected.While the principles of optimism or pessimism under uncertainty arewell-established in standard reinforcement learning RL apractically-implementable and theoretically-grounded form amenable to largelanguage models is not yet available as standard techniques for constructingconfidence intervals become intractable under arbitrary policyparameterizations.  In this paper we introduce a unified approach to online and offline RLHF --value-incentivized preference optimization VPO -- which regularizes themaximum-likelihood estimate of the reward function with the corresponding valuefunction modulated by a textitsign to indicate whether the optimism orpessimism is chosen. VPO also directly optimizes the policy with implicitreward modeling and therefore shares a simpler RLHF pipeline similar to directpreference optimization. Theoretical guarantees of VPO are provided for bothonline and offline settings matching the rates of their standard RLcounterparts. Moreover experiments on text summarization and dialog verify thepracticality and effectiveness of VPO.</p>
                <p>Last Updated: 2024-05-29 17:51:42 UTC</p>
                <button class="interpret-button" data-id="2405.19320v1">Interpret</button>
                <div id="interpretation-2405.19320v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-05-30</p>
        </div>
    
        </div>
    </body>
    </html>
    