
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>From Zero to Hero: Cold-Start Anomaly Detection</h3>
                <p>Authors: Tal ReissGeorge KourNaama ZwerdlingAteret Anaby-TavorYedid Hoshen</p>
                <p><a href="http://arxiv.org/abs/2405.20341v1">Link to paper</a></p>
                <p>When first deploying an anomaly detection system e.g. to detectout-of-scope queries in chatbots there are no observed data makingdata-driven approaches ineffective. Zero-shot anomaly detection methods offer asolution to such cold-start cases but unfortunately they are often notaccurate enough. This paper studies the realistic but underexplored cold-startsetting where an anomaly detection model is initialized using zero-shotguidance but subsequently receives a small number of contaminated observationsnamely that may include anomalies. The goal is to make efficient use of boththe zero-shot guidance and the observations. We propose ColdFusion a methodthat effectively adapts the zero-shot anomaly detector to contaminatedobservations. To support future development of this new setting we propose anevaluation suite consisting of evaluation protocols and metrics.</p>
                <p>Last Updated: 2024-05-30 17:59:51 UTC</p>
                <button class="interpret-button" data-id="2405.20341v1">Interpret</button>
                <div id="interpretation-2405.20341v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Xwin-LM: Strong and Scalable Alignment Practice for LLMs</h3>
                <p>Authors: Bolin NiJingCheng HuYixuan WeiHouwen PengZheng ZhangGaofeng MengHan Hu</p>
                <p><a href="http://arxiv.org/abs/2405.20335v1">Link to paper</a></p>
                <p>In this work we present Xwin-LM a comprehensive suite of alignmentmethodologies for large language models LLMs. This suite encompasses severalkey techniques including supervised finetuning SFT reward modeling RMrejection sampling finetuning RS and direct preference optimization DPO.The key components are as follows: 1 Xwin-LM-SFT models initially finetunedwith high-quality instruction data 2 Xwin-Pair a large-scale multi-turnpreference dataset meticulously annotated using GPT-4 3 Xwin-RM rewardmodels trained on Xwin-Pair developed at scales of 7B 13B and 70Bparameters 4 Xwin-Set a multiwise preference dataset in which each promptis linked to 64 unique responses generated by Xwin-LM-SFT and scored byXwin-RM 5 Xwin-LM-RS models finetuned with the highest-scoring responsesfrom Xwin-Set 6 Xwin-LM-DPO models further optimized on Xwin-Set using theDPO algorithm. Our evaluations on AlpacaEval and MT-bench demonstrateconsistent and significant improvements across the pipeline demonstrating thestrength and scalability of Xwin-LM. The repositoryhttps://github.com/Xwin-LM/Xwin-LM will be continually updated to fostercommunity research.</p>
                <p>Last Updated: 2024-05-30 17:59:31 UTC</p>
                <button class="interpret-button" data-id="2405.20335v1">Interpret</button>
                <div id="interpretation-2405.20335v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CausalQuest: Collecting Natural Causal Questions for AI Agents</h3>
                <p>Authors: Roberto CeraoloDmitrii KharlapenkoAmélie ReymondRada MihalceaMrinmaya SachanBernhard SchölkopfZhijing Jin</p>
                <p><a href="http://arxiv.org/abs/2405.20318v1">Link to paper</a></p>
                <p>Humans have an innate drive to seek out causality. Whether fuelled bycuriosity or specific goals we constantly question why things happen how theyare interconnected and many other related phenomena. To develop AI agentscapable of addressing this natural human quest for causality we urgently needa comprehensive dataset of natural causal questions. Unfortunately existingdatasets either contain only artificially-crafted questions that do not reflectreal AI usage scenarios or have limited coverage of questions from specificsources. To address this gap we present CausalQuest a dataset of 13500naturally occurring questions sourced from social networks search engines andAI assistants. We formalize the definition of causal questions and establish ataxonomy for finer-grained classification. Through a combined effort of humanannotators and large language models LLMs we carefully label the dataset. Wefind that 42 of the questions humans ask are indeed causal with the majorityseeking to understand the causes behind given effects. Using this dataset wetrain efficient classifiers up to 2.85B parameters for the binary task ofidentifying causal questions achieving high performance with F1 scores of upto 0.877. We conclude with a rich set of future research directions that canbuild upon our data and models.</p>
                <p>Last Updated: 2024-05-30 17:55:28 UTC</p>
                <button class="interpret-button" data-id="2405.20318v1">Interpret</button>
                <div id="interpretation-2405.20318v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ANAH: Analytical Annotation of Hallucinations in Large Language Models</h3>
                <p>Authors: Ziwei JiYuzhe GuWenwei ZhangChengqi LyuDahua LinKai Chen</p>
                <p><a href="http://arxiv.org/abs/2405.20315v1">Link to paper</a></p>
                <p>Reducing the textithallucination problem of Large Language ModelsLLMs is crucial for their wide applications. A comprehensive and fine-grainedmeasurement of the hallucination is the first key step for the governance ofthis issue but is under-explored in the community. Thus we presenttextbfANAH a bilingual dataset that offers textbfANalyticaltextbfAnnotation of textbfHallucinations in LLMs within GenerativeQuestion Answering. Each answer sentence in our dataset undergoes rigorousannotation involving the retrieval of a reference fragment the judgment ofthe hallucination type and the correction of hallucinated content. ANAHconsists of 12k sentence-level annotations for 4.3k LLM responses coveringover 700 topics constructed by a human-in-the-loop pipeline. Thanks to thefine granularity of the hallucination annotations we can quantitativelyconfirm that the hallucinations of LLMs progressively accumulate in the answerand use ANAH to train and evaluate hallucination annotators. We conductextensive experiments on studying generative and discriminative annotators andshow that although current open-source LLMs have difficulties in fine-grainedhallucination annotation the generative annotator trained with ANAH cansurpass all open-source LLMs and GPT-3.5 obtain performance competitive withGPT-4 and exhibits better generalization ability on unseen questions.</p>
                <p>Last Updated: 2024-05-30 17:54:40 UTC</p>
                <button class="interpret-button" data-id="2405.20315v1">Interpret</button>
                <div id="interpretation-2405.20315v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs</h3>
                <p>Authors: Wei ZhongManasa Bharadwaj</p>
                <p><a href="http://arxiv.org/abs/2405.20314v1">Link to paper</a></p>
                <p>Speculative decoding SD has attracted a significant amount of researchattention due to the substantial speedup it can achieve for LLM inference.However despite the high speedups they offer speculative decoding methodsoften achieve optimal performance on high-end devices or with a substantial GPUmemory overhead. Given limited memory and the necessity of quantization ahigh-performing model on a high-end GPU can slow down by up to 7 times. To thisend we propose Skippy Simultaneous Speculative Decoding or S3D acost-effective self-speculative SD method based on simultaneous multi-tokendecoding and mid-layer skipping. When compared against recent effectiveopen-source SD systems our method has achieved one of the topperformance-memory ratios while requiring minimal architecture changes andtraining data. Leveraging our memory efficiency we created a smaller yet moreeffective SD model based on Phi-3. It is 1.4 to 2 times faster than thequantized EAGLE model and operates in half-precision while using less VRAM.</p>
                <p>Last Updated: 2024-05-30 17:54:35 UTC</p>
                <button class="interpret-button" data-id="2405.20314v1">Interpret</button>
                <div id="interpretation-2405.20314v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Disentangling and Mitigating the Impact of Task Similarity for Continual Learning</h3>
                <p>Authors: Naoki Hiratani</p>
                <p><a href="http://arxiv.org/abs/2405.20236v1">Link to paper</a></p>
                <p>Continual learning of partially similar tasks poses a challenge forartificial neural networks as task similarity presents both an opportunity forknowledge transfer and a risk of interference and catastrophic forgetting.However it remains unclear how task similarity in input features and readoutpatterns influences knowledge transfer and forgetting as well as how theyinteract with common algorithms for continual learning. Here we develop alinear teacher-student model with latent structure and show analytically thathigh input feature similarity coupled with low readout similarity iscatastrophic for both knowledge transfer and retention. Conversely theopposite scenario is relatively benign. Our analysis further reveals thattask-dependent activity gating improves knowledge retention at the expense oftransfer while task-dependent plasticity gating does not affect eitherretention or transfer performance at the over-parameterized limit. In contrastweight regularization based on the Fisher information metric significantlyimproves retention regardless of task similarity without compromisingtransfer performance. Nevertheless its diagonal approximation andregularization in the Euclidean space are much less robust against tasksimilarity. We demonstrate consistent results in a permuted MNIST task withlatent variables. Overall this work provides insights into when continuallearning is difficult and how to mitigate it.</p>
                <p>Last Updated: 2024-05-30 16:40:07 UTC</p>
                <button class="interpret-button" data-id="2405.20236v1">Interpret</button>
                <div id="interpretation-2405.20236v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof</h3>
                <p>Authors: Derek LimMoe PuttermanRobin WaltersHaggai MaronStefanie Jegelka</p>
                <p><a href="http://arxiv.org/abs/2405.20231v1">Link to paper</a></p>
                <p>Many algorithms and observed phenomena in deep learning appear to be affectedby parameter symmetries -- transformations of neural network parameters that donot change the underlying neural network function. These include linear modeconnectivity model merging Bayesian neural network inference metanetworksand several other characteristics of optimization or loss-landscapes. Howevertheoretical analysis of the relationship between parameter space symmetries andthese phenomena is difficult. In this work we empirically investigate theimpact of neural parameter symmetries by introducing new neural networkarchitectures that have reduced parameter space symmetries. We develop twomethods with some provable guarantees of modifying standard neural networksto reduce parameter space symmetries. With these new methods we conduct acomprehensive experimental study consisting of multiple tasks aimed atassessing the effect of removing parameter symmetries. Our experiments revealseveral interesting observations on the empirical impact of parametersymmetries for instance we observe linear mode connectivity between ournetworks without alignment of weight spaces and we find that our networksallow for faster and more effective Bayesian neural network training.</p>
                <p>Last Updated: 2024-05-30 16:32:31 UTC</p>
                <button class="interpret-button" data-id="2405.20231v1">Interpret</button>
                <div id="interpretation-2405.20231v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Randomized Exploration for Reinforcement Learning with Multinomial Logistic Function Approximation</h3>
                <p>Authors: Wooseong ChoTaehyun HwangJoongkyu LeeMin-hwan Oh</p>
                <p><a href="http://arxiv.org/abs/2405.20165v1">Link to paper</a></p>
                <p>We study reinforcement learning with multinomial logistic MNL functionapproximation where the underlying transition probability kernel of the Markovdecision processes MDPs is parametrized by an unknown transition core withfeatures of state and action. For the finite horizon episodic setting withinhomogeneous state transitions we propose provably efficient algorithms withrandomized exploration having frequentist regret guarantees. For our firstalgorithm textttRRL-MNL we adapt optimistic sampling to ensure theoptimism of the estimated value function with sufficient frequency andestablish that textttRRL-MNL is both statistically and computationallyefficient achieving a tildeOkappa-1 dfrac32 Hfrac32sqrtT frequentist regret bound with constant-time computational cost perepisode. Here d is the dimension of the transition core H is the horizonlength T is the total number of steps and kappa is a problem-dependentconstant. Despite the simplicity and practicality of textttRRL-MNL itsregret bound scales with kappa-1 which is potentially large in the worstcase. To improve the dependence on kappa-1 we proposetextttORRL-MNL which estimates the value function using local gradientinformation of the MNL transition model. We show that its frequentist regretbound is tildeOdfrac32 Hfrac32 sqrtT  kappa-1 d2H2. To the best of our knowledge these are the first randomized RLalgorithms for the MNL transition model that achieve both computational andstatistical efficiency. Numerical experiments demonstrate the superiorperformance of the proposed algorithms.</p>
                <p>Last Updated: 2024-05-30 15:39:19 UTC</p>
                <button class="interpret-button" data-id="2405.20165v1">Interpret</button>
                <div id="interpretation-2405.20165v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Geometric Unification of Distributionally Robust Covariance Estimators: Shrinking the Spectrum by Inflating the Ambiguity Set</h3>
                <p>Authors: Man-Chung YueYves RychenerDaniel KuhnViet Anh Nguyen</p>
                <p><a href="http://arxiv.org/abs/2405.20124v1">Link to paper</a></p>
                <p>The state-of-the-art methods for estimating high-dimensional covariancematrices all shrink the eigenvalues of the sample covariance matrix towards adata-insensitive shrinkage target. The underlying shrinkage transformation iseither chosen heuristically - without compelling theoretical justification - oroptimally in view of restrictive distributional assumptions. In this paper wepropose a principled approach to construct covariance estimators withoutimposing restrictive assumptions. That is we study distributionally robustcovariance estimation problems that minimize the worst-case Frobenius errorwith respect to all data distributions close to a nominal distribution wherethe proximity of distributions is measured via a divergence on the space ofcovariance matrices. We identify mild conditions on this divergence under whichthe resulting minimizers represent shrinkage estimators. We show that thecorresponding shrinkage transformations are intimately related to thegeometrical properties of the underlying divergence. We also prove that ourrobust estimators are efficiently computable and asymptotically consistent andthat they enjoy finite-sample performance guarantees. We exemplify our generalmethodology by synthesizing explicit estimators induced by theKullback-Leibler Fisher-Rao and Wasserstein divergences. Numericalexperiments based on synthetic and real data show that our robust estimatorsare competitive with state-of-the-art estimators.</p>
                <p>Last Updated: 2024-05-30 15:01:18 UTC</p>
                <button class="interpret-button" data-id="2405.20124v1">Interpret</button>
                <div id="interpretation-2405.20124v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Near Optimal Decentralized Optimization with Compression and Momentum Tracking</h3>
                <p>Authors: Rustem IslamovYuan GaoSebastian U. Stich</p>
                <p><a href="http://arxiv.org/abs/2405.20114v1">Link to paper</a></p>
                <p>Communication efficiency has garnered significant attention as it isconsidered the main bottleneck for large-scale decentralized Machine Learningapplications in distributed and federated settings. In this regime clients arerestricted to transmitting small amounts of quantized information to theirneighbors over a communication graph. Numerous endeavors have been made toaddress this challenging problem by developing algorithms with compressedcommunication for decentralized non-convex optimization problems. Despiteconsiderable efforts the current results suffer from various issues such asnon-scalability with the number of clients requirements for large batches orbounded gradient assumption. In this paper we introduce MoTEF a novelapproach that integrates communication compression with Momentum Tracking andError Feedback. Our analysis demonstrates that MoTEF achieves most of thedesired properties and significantly outperforms existing methods underarbitrary data heterogeneity. We provide numerical experiments to validate ourtheoretical findings and confirm the practical superiority of MoTEF.</p>
                <p>Last Updated: 2024-05-30 14:51:57 UTC</p>
                <button class="interpret-button" data-id="2405.20114v1">Interpret</button>
                <div id="interpretation-2405.20114v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image</h3>
                <p>Authors: Kailu WuFangfu LiuZhihan CaiRunjie YanHanyang WangYating HuYueqi DuanKaisheng Ma</p>
                <p><a href="http://arxiv.org/abs/2405.20343v1">Link to paper</a></p>
                <p>In this work we introduce Unique3D a novel image-to-3D framework forefficiently generating high-quality 3D meshes from single-view imagesfeaturing state-of-the-art generation fidelity and strong generalizability.Previous methods based on Score Distillation Sampling SDS can producediversified 3D results by distilling 3D knowledge from large 2D diffusionmodels but they usually suffer from long per-case optimization time withinconsistent issues. Recent works address the problem and generate better 3Dresults either by finetuning a multi-view diffusion model or training a fastfeed-forward model. However they still lack intricate textures and complexgeometries due to inconsistency and limited generated resolution. Tosimultaneously achieve high fidelity consistency and efficiency in singleimage-to-3D we propose a novel framework Unique3D that includes a multi-viewdiffusion model with a corresponding normal diffusion model to generatemulti-view images with their normal maps a multi-level upscale process toprogressively improve the resolution of generated orthographic multi-views aswell as an instant and consistent mesh reconstruction algorithm called ISOMERwhich fully integrates the color and geometric priors into mesh results.Extensive experiments demonstrate that our Unique3D significantly outperformsother image-to-3D baselines in terms of geometric and textural details.</p>
                <p>Last Updated: 2024-05-30 17:59:54 UTC</p>
                <button class="interpret-button" data-id="2405.20343v1">Interpret</button>
                <div id="interpretation-2405.20343v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MotionLLM: Understanding Human Behaviors from Human Motions and Videos</h3>
                <p>Authors: Ling-Hao ChenShunlin LuAiling ZengHao ZhangBenyou WangRuimao ZhangLei Zhang</p>
                <p><a href="http://arxiv.org/abs/2405.20340v1">Link to paper</a></p>
                <p>This study delves into the realm of multi-modality i.e. video and motionmodalities human behavior understanding by leveraging the powerfulcapabilities of Large Language Models LLMs. Diverging from recent LLMsdesigned for video-only or motion-only understanding we argue thatunderstanding human behavior necessitates joint modeling from both videos andmotion sequences e.g. SMPL sequences to capture nuanced body part dynamicsand semantics effectively. In light of this we present MotionLLM astraightforward yet effective framework for human motion understandingcaptioning and reasoning. Specifically MotionLLM adopts a unifiedvideo-motion training strategy that leverages the complementary advantages ofexisting coarse video-text data and fine-grained motion-text data to glean richspatial-temporal insights. Furthermore we collect a substantial datasetMoVid comprising diverse videos motions captions and instructions.Additionally we propose the MoVid-Bench with carefully manual annotationsfor better evaluation of human behavior understanding on video and motion.Extensive experiments show the superiority of MotionLLM in the captionspatial-temporal comprehension and reasoning ability.</p>
                <p>Last Updated: 2024-05-30 17:59:50 UTC</p>
                <button class="interpret-button" data-id="2405.20340v1">Interpret</button>
                <div id="interpretation-2405.20340v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Visual Perception by Large Language Model's Weights</h3>
                <p>Authors: Feipeng MaHongwei XueGuangting WangYizhou ZhouFengyun RaoShilin YanYueyi ZhangSiying WuMike Zheng ShouXiaoyan Sun</p>
                <p><a href="http://arxiv.org/abs/2405.20339v1">Link to paper</a></p>
                <p>Existing Multimodal Large Language Models MLLMs follow the paradigm thatperceives visual information by aligning visual features with the input spaceof Large Language Models LLMs and concatenating visual tokens with texttokens to form a unified sequence input for LLMs. These methods demonstratepromising results on various vision-language tasks but are limited by the highcomputational effort due to the extended input sequence resulting from theinvolvement of visual tokens. In this paper instead of input space alignmentwe propose a novel parameter space alignment paradigm that represents visualinformation as model weights. For each input image we use a vision encoder toextract visual features convert features into perceptual weights and mergethe perceptual weights with LLMs weights. In this way the input of LLM doesnot require visual tokens which reduces the length of the input sequence andgreatly improves efficiency. Following this paradigm we propose VLoRA with theperceptual weights generator. The perceptual weights generator is designed toconvert visual features to perceptual weights with low-rank propertyexhibiting a form similar to LoRA. The experimental results show that our VLoRAachieves comparable performance on various benchmarks for MLLMs whilesignificantly reducing the computational costs for both training and inference.The code and models will be made open-source.</p>
                <p>Last Updated: 2024-05-30 17:59:47 UTC</p>
                <button class="interpret-button" data-id="2405.20339v1">Interpret</button>
                <div id="interpretation-2405.20339v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving</h3>
                <p>Authors: Lening WangWenzhao ZhengYilong RenHan JiangZhiyong CuiHaiyang YuJiwen Lu</p>
                <p><a href="http://arxiv.org/abs/2405.20337v1">Link to paper</a></p>
                <p>Understanding the evolution of 3D scenes is important for effectiveautonomous driving. While conventional methods mode scene development with themotion of individual instances world models emerge as a generative frameworkto describe the general scene dynamics. However most existing methods adopt anautoregressive framework to perform next-token prediction which suffer frominefficiency in modeling long-term temporal evolutions. To address this wepropose a diffusion-based 4D occupancy generation model OccSora to simulatethe development of the 3D world for autonomous driving. We employ a 4D scenetokenizer to obtain compact discrete spatial-temporal representations for 4Doccupancy input and achieve high-quality reconstruction for long-sequenceoccupancy videos. We then learn a diffusion transformer on the spatial-temporalrepresentations and generate 4D occupancy conditioned on a trajectory prompt.We conduct extensive experiments on the widely used nuScenes dataset with Occ3Doccupancy annotations. OccSora can generate 16s-videos with authentic 3D layoutand temporal consistency demonstrating its ability to understand the spatialand temporal distributions of driving scenes. With trajectory-aware 4Dgeneration OccSora has the potential to serve as a world simulator for thedecision-making of autonomous driving. Code is available at:https://github.com/wzzheng/OccSora.</p>
                <p>Last Updated: 2024-05-30 17:59:42 UTC</p>
                <button class="interpret-button" data-id="2405.20337v1">Interpret</button>
                <div id="interpretation-2405.20337v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text</h3>
                <p>Authors: Jiaben ChenXin YanYihang ChenSiyuan CenQinwei MaHaoyu ZhenKaizhi QianLie LuChuang Gan</p>
                <p><a href="http://arxiv.org/abs/2405.20336v1">Link to paper</a></p>
                <p>In this work we introduce a challenging task for simultaneously generating3D holistic body motions and singing vocals directly from textual lyricsinputs advancing beyond existing works that typically address these twomodalities in isolation. To facilitate this we first collect the RapVersedataset a large dataset containing synchronous rapping vocals lyrics andhigh-quality 3D holistic body meshes. With the RapVerse dataset we investigatethe extent to which scaling autoregressive multimodal transformers acrosslanguage audio and motion can enhance the coherent and realistic generationof vocals and whole-body human motions. For modality unification avector-quantized variational autoencoder is employed to encode whole-bodymotion sequences into discrete motion tokens while a vocal-to-unit model isleveraged to obtain quantized audio tokens preserving content prosodicinformation and singer identity. By jointly performing transformer modeling onthese three modalities in a unified way our framework ensures a seamless andrealistic blend of vocals and human motions. Extensive experiments demonstratethat our unified generation framework not only produces coherent and realisticsinging vocals alongside human motions directly from textual inputs but alsorivals the performance of specialized single-modality generation systemsestablishing new benchmarks for joint vocal-motion generation. The project pageis available for research purposes at https://vis-www.cs.umass.edu/RapVerse.</p>
                <p>Last Updated: 2024-05-30 17:59:39 UTC</p>
                <button class="interpret-button" data-id="2405.20336v1">Interpret</button>
                <div id="interpretation-2405.20336v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>ParSEL: Parameterized Shape Editing with Language</h3>
                <p>Authors: Aditya GaneshanRyan Y. HuangXianghao XuR. Kenny JonesDaniel Ritchie</p>
                <p><a href="http://arxiv.org/abs/2405.20319v1">Link to paper</a></p>
                <p>The ability to edit 3D assets from natural language presents a compellingparadigm to aid in the democratization of 3D content creation. However whilenatural language is often effective at communicating general intent it ispoorly suited for specifying precise manipulation. To address this gap weintroduce ParSEL a system that enables controllable editing of high-quality 3Dassets from natural language. Given a segmented 3D mesh and an editing requestParSEL produces a parameterized editing program. Adjusting the programparameters allows users to explore shape variations with a precise control overthe magnitudes of edits. To infer editing programs which align with an inputedit request we leverage the abilities of large-language models LLMs.However while we find that LLMs excel at identifying initial edit operationsthey often fail to infer complete editing programs and produce outputs thatviolate shape semantics. To overcome this issue we introduce Analytical EditPropagation AEP an algorithm which extends a seed edit with additionaloperations until a complete editing program has been formed. Unlike priormethods AEP searches for analytical editing operations compatible with a rangeof possible user edits through the integration of computer algebra systems forgeometric analysis. Experimentally we demonstrate ParSELs effectiveness inenabling controllable editing of 3D objects through natural language requestsover alternative system designs.</p>
                <p>Last Updated: 2024-05-30 17:55:46 UTC</p>
                <button class="interpret-button" data-id="2405.20319v1">Interpret</button>
                <div id="interpretation-2405.20319v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Conversational Agents to Facilitate Deliberation on Harmful Content in WhatsApp Groups</h3>
                <p>Authors: Dhruv AgarwalFarhana ShahidAditya Vashistha</p>
                <p><a href="http://arxiv.org/abs/2405.20254v1">Link to paper</a></p>
                <p>WhatsApp groups have become a hotbed for the propagation of harmful contentincluding misinformation hate speech polarizing content and rumorsespecially in Global South countries. Given the platforms end-to-endencryption moderation responsibilities lie on group admins and members whorarely contest such content. Another approach is fact-checking which isunscalable and can only contest factual content e.g. misinformation but notsubjective content e.g. hate speech. Drawing on recent literature weexplore deliberation -- open and inclusive discussion -- as an alternative. Weinvestigate the role of a conversational agent in facilitating deliberation onharmful content in WhatsApp groups. We conducted semi-structured interviewswith 21 Indian WhatsApp users employing a design probe to showcase an exampleagent. Participants expressed the need for anonymity and recommended AIassistance to reduce the effort required in deliberation. They appreciated theagents neutrality but pointed out the futility of deliberation in echo chambergroups. Our findings highlight design tensions for such an agent includingprivacy versus group dynamics and freedom of speech in private spaces. Wediscuss the efficacy of deliberation using deliberative theory as a lenscompare deliberation with moderation and fact-checking and provide designrecommendations for future such systems. Ultimately this work advances CSCW byoffering insights into designing deliberative systems for combating harmfulcontent in private group chats on social media.</p>
                <p>Last Updated: 2024-05-30 17:07:07 UTC</p>
                <button class="interpret-button" data-id="2405.20254v1">Interpret</button>
                <div id="interpretation-2405.20254v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Using Large Language Models for Humanitarian Frontline Negotiation: Opportunities and Considerations</h3>
                <p>Authors: Zilin Ma1 SusannahSuNathan ZhaoLinn BieskeBlake BullwinkelYanyi ZhangSophiaYangZiqing LuoSiyao LiGekai LiaoBoxiang WangJinglun GaoZihan WenClaude BruderleinWeiwei Pan</p>
                <p><a href="http://arxiv.org/abs/2405.20195v1">Link to paper</a></p>
                <p>Humanitarian negotiations in conflict zones called emphfrontlinenegotiation are often highly adversarial complex and high-risk. Severalbest-practices have emerged over the years that help negotiators extractinsights from large datasets to navigate nuanced and rapidly evolvingscenarios. Recent advances in large language models LLMs have sparkedinterest in the potential for AI to aid decision making in frontlinenegotiation. Through in-depth interviews with 13 experienced frontlinenegotiators we identified their needs for AI-assisted case analysis andcreativity support as well as concerns surrounding confidentiality and modelbias. We further explored the potential for AI augmentation of three standardtools used in frontline negotiation planning. We evaluated the quality andstability of our ChatGPT-based negotiation tools in the context of two realcases. Our findings highlight the potential for LLMs to enhance humanitariannegotiations and underscore the need for careful ethical and practicalconsiderations.</p>
                <p>Last Updated: 2024-05-30 15:58:49 UTC</p>
                <button class="interpret-button" data-id="2405.20195v1">Interpret</button>
                <div id="interpretation-2405.20195v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Visual Attention Analysis in Online Learning</h3>
                <p>Authors: Navarro MiriamBecerra ÁlvaroDaza RobertoCobos RuthMorales AythamiFierrez Julian</p>
                <p><a href="http://arxiv.org/abs/2405.20091v1">Link to paper</a></p>
                <p>In this paper we present an approach in the Multimodal Learning Analyticsfield. Within this approach we have developed a tool to visualize and analyzeeye movement data collected during learning sessions in online courses. Thetool is named VAAD an acronym for Visual Attention Analysis Dashboard. Theseeye movement data have been gathered using an eye-tracker and subsequentlyprocessed and visualized for interpretation. The purpose of the tool is toconduct a descriptive analysis of the data by facilitating its visualizationenabling the identification of differences and learning patterns among variouslearner populations. Additionally it integrates a predictive module capable ofanticipating learner activities during a learning session. Consequently VAADholds the potential to offer valuable insights into online learning behaviorsfrom both descriptive and predictive perspectives.</p>
                <p>Last Updated: 2024-05-30 14:27:40 UTC</p>
                <button class="interpret-button" data-id="2405.20091v1">Interpret</button>
                <div id="interpretation-2405.20091v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PixelsDB: Serverless and Natural-Language-Aided Data Analytics with Flexible Service Levels and Prices</h3>
                <p>Authors: Haoqiong BianDongyang GengHaoyang LiAnastasia Ailamaki</p>
                <p><a href="http://arxiv.org/abs/2405.19784v1">Link to paper</a></p>
                <p>Serverless query processing has become increasingly popular due to itsadvantages including automated hardware and software management highelasticity and pay-as-you-go pricing. For users who are not system expertsserverless query processing greatly reduces the cost of owning a data analyticsystem. However it is still a significant challenge for non-expert users totransform their complex and evolving data analytic needs into proper SQLqueries and select a serverless query engine that delivers satisfactoryperformance and price for each type of query.  This paper presents PixelsDB an open-source data analytic system that allowsusers who lack system or SQL expertise to explore data efficiently. It allowsusers to generate and debug SQL queries using a natural language interfacepowered by fine-tuned language models. The queries are then executed by aserverless query engine that offers varying prices for different service levelson query urgency. The service levels are natively supported by dedicatedarchitecture design and heterogeneous resource scheduling that can applycost-efficient resources to process non-urgent queries. We envision that thecombination of a serverless paradigm a natural-language-aided interface andflexible service levels and prices will substantially improve the userexperience in data analysis.</p>
                <p>Last Updated: 2024-05-30 07:48:43 UTC</p>
                <button class="interpret-button" data-id="2405.19784v1">Interpret</button>
                <div id="interpretation-2405.19784v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving</h3>
                <p>Authors: Lening WangWenzhao ZhengYilong RenHan JiangZhiyong CuiHaiyang YuJiwen Lu</p>
                <p><a href="http://arxiv.org/abs/2405.20337v1">Link to paper</a></p>
                <p>Understanding the evolution of 3D scenes is important for effectiveautonomous driving. While conventional methods mode scene development with themotion of individual instances world models emerge as a generative frameworkto describe the general scene dynamics. However most existing methods adopt anautoregressive framework to perform next-token prediction which suffer frominefficiency in modeling long-term temporal evolutions. To address this wepropose a diffusion-based 4D occupancy generation model OccSora to simulatethe development of the 3D world for autonomous driving. We employ a 4D scenetokenizer to obtain compact discrete spatial-temporal representations for 4Doccupancy input and achieve high-quality reconstruction for long-sequenceoccupancy videos. We then learn a diffusion transformer on the spatial-temporalrepresentations and generate 4D occupancy conditioned on a trajectory prompt.We conduct extensive experiments on the widely used nuScenes dataset with Occ3Doccupancy annotations. OccSora can generate 16s-videos with authentic 3D layoutand temporal consistency demonstrating its ability to understand the spatialand temporal distributions of driving scenes. With trajectory-aware 4Dgeneration OccSora has the potential to serve as a world simulator for thedecision-making of autonomous driving. Code is available at:https://github.com/wzzheng/OccSora.</p>
                <p>Last Updated: 2024-05-30 17:59:42 UTC</p>
                <button class="interpret-button" data-id="2405.20337v1">Interpret</button>
                <div id="interpretation-2405.20337v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CoSy: Evaluating Textual Explanations of Neurons</h3>
                <p>Authors: Laura KopfPhiline Lou BommerAnna HedströmSebastian LapuschkinMarina M. -C. HöhneKirill Bykov</p>
                <p><a href="http://arxiv.org/abs/2405.20331v1">Link to paper</a></p>
                <p>A crucial aspect of understanding the complex nature of Deep Neural NetworksDNNs is the ability to explain learned concepts within their latentrepresentations. While various methods exist to connect neurons to textualdescriptions of human-understandable concepts evaluating the quality of theseexplanation methods presents a major challenge in the field due to a lack ofunified general-purpose quantitative evaluation. In this work we introduceCoSy Concept Synthesis -- a novel architecture-agnostic framework toevaluate the quality of textual explanations for latent neurons. Given textualexplanations our proposed framework leverages a generative model conditionedon textual input to create data points representing the textual explanation.Then the neurons response to these explanation data points is compared withthe response to control data points providing a quality estimate of the givenexplanation. We ensure the reliability of our proposed framework in a series ofmeta-evaluation experiments and demonstrate practical value through insightsfrom benchmarking various concept-based textual explanation methods forComputer Vision tasks showing that tested explanation methods significantlydiffer in quality.</p>
                <p>Last Updated: 2024-05-30 17:59:04 UTC</p>
                <button class="interpret-button" data-id="2405.20331v1">Interpret</button>
                <div id="interpretation-2405.20331v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>4DHands: Reconstructing Interactive Hands in 4D with Transformers</h3>
                <p>Authors: Dixuan LinYuxiang ZhangMengcheng LiYebin LiuWei JingQi YanQianying WangHongwen Zhang</p>
                <p><a href="http://arxiv.org/abs/2405.20330v1">Link to paper</a></p>
                <p>In this paper we introduce 4DHands a robust approach to recoveringinteractive hand meshes and their relative movement from monocular inputs. Ourapproach addresses two major limitations of previous methods: lacking a unifiedsolution for handling various hand image inputs and neglecting the positionalrelationship of two hands within images. To overcome these challenges wedevelop a transformer-based architecture with novel tokenization and featurefusion strategies. Specifically we propose a Relation-aware Two-HandTokenization RAT method to embed positional relation information into thehand tokens. In this way our network can handle both single-hand and two-handinputs and explicitly leverage relative hand positions facilitating thereconstruction of intricate hand interactions in real-world scenarios. As suchtokenization indicates the relative relationship of two hands it also supportsmore effective feature fusion. To this end we further develop aSpatio-temporal Interaction Reasoning SIR module to fuse hand tokens in 4Dwith attention and decode them into 3D hand meshes and relative temporalmovements. The efficacy of our approach is validated on several benchmarkdatasets. The results on in-the-wild videos and real-world scenariosdemonstrate the superior performances of our approach for interactive handreconstruction. More video results can be found on the project page:https://4dhands.github.io.</p>
                <p>Last Updated: 2024-05-30 17:59:02 UTC</p>
                <button class="interpret-button" data-id="2405.20330v1">Interpret</button>
                <div id="interpretation-2405.20330v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>$\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving</h3>
                <p>Authors: Nan HuangXiaobao WeiWenzhao ZhengPengju AnMing LuWei ZhanMasayoshi TomizukaKurt KeutzerShanghang Zhang</p>
                <p><a href="http://arxiv.org/abs/2405.20323v1">Link to paper</a></p>
                <p>Photorealistic 3D reconstruction of street scenes is a critical technique fordeveloping real-world simulators for autonomous driving. Despite the efficacyof Neural Radiance Fields NeRF for driving scenes 3D Gaussian Splatting3DGS emerges as a promising direction due to its faster speed and moreexplicit representation. However most existing street 3DGS methods requiretracked 3D vehicle bounding boxes to decompose the static and dynamic elementsfor effective reconstruction limiting their applications for in-the-wildscenarios. To facilitate efficient 3D scene reconstruction without costlyannotations we propose a self-supervised street GaussiantextitS3Gaussian method to decompose dynamic and static elements from4D consistency. We represent each scene with 3D Gaussians to preserve theexplicitness and further accompany them with a spatial-temporal field networkto compactly model the 4D dynamics. We conduct extensive experiments on thechallenging Waymo-Open dataset to evaluate the effectiveness of our method. OurtextitS3Gaussian demonstrates the ability to decompose static and dynamicscenes and achieves the best performance without using 3D annotations. Code isavailable at: https://github.com/nnanhuang/S3Gaussian/.</p>
                <p>Last Updated: 2024-05-30 17:57:08 UTC</p>
                <button class="interpret-button" data-id="2405.20323v1">Interpret</button>
                <div id="interpretation-2405.20323v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Improving the Training of Rectified Flows</h3>
                <p>Authors: Sangyun LeeZinan LinGiulia Fanti</p>
                <p><a href="http://arxiv.org/abs/2405.20320v1">Link to paper</a></p>
                <p>Diffusion models have shown great promise for image and video generation butsampling from state-of-the-art models requires expensive numerical integrationof a generative ODE. One approach for tackling this problem is rectified flowswhich iteratively learn smooth ODE paths that are less susceptible totruncation error. However rectified flows still require a relatively largenumber of function evaluations NFEs. In this work we propose improvedtechniques for training rectified flows allowing them to compete withknowledge distillation methods even in the low NFE setting. Our main insight isthat under realistic settings a single iteration of the Reflow algorithm fortraining rectified flows is sufficient to learn nearly straight trajectorieshence the current practice of using multiple Reflow iterations is unnecessary.We thus propose techniques to improve one-round training of rectified flowsincluding a U-shaped timestep distribution and LPIPS-Huber premetric. Withthese techniques we improve the FID of the previous 2-rectified flow by up to72 in the 1 NFE setting on CIFAR-10. On ImageNet 64times64 our improvedrectified flow outperforms the state-of-the-art distillation methods such asconsistency distillation and progressive distillation in both one-step andtwo-step settings and rivals the performance of improved consistency trainingiCT in FID. Code is available at https://github.com/sangyun884/rfpp.</p>
                <p>Last Updated: 2024-05-30 17:56:04 UTC</p>
                <button class="interpret-button" data-id="2405.20320v1">Interpret</button>
                <div id="interpretation-2405.20320v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Distributed maze exploration using multiple agents and optimal goal assignment</h3>
                <p>Authors: Manousos LinardakisIraklis VarlamisGeorgios Th. Papadopoulos</p>
                <p><a href="http://arxiv.org/abs/2405.20232v1">Link to paper</a></p>
                <p>Robotic exploration has long captivated researchers aiming to map complexenvironments efficiently. Techniques such as potential fields and frontierexploration have traditionally been employed in this pursuit primarilyfocusing on solitary agents. Recent advancements have shifted towardsoptimizing exploration efficiency through multiagent systems. However manyexisting approaches overlook critical real-world factors such as broadcastrange limitations communication costs and coverage overlap. This paperaddresses these gaps by proposing a distributed maze exploration strategyCU-LVP that assumes constrained broadcast ranges and utilizes Voronoidiagrams for better area partitioning. By adapting traditional multiagentmethods to distributed environments with limited broadcast ranges this studyevaluates their performance across diverse maze topologies demonstrating theefficacy and practical applicability of the proposed method. The code andexperimental results supporting this study are available in the followingrepository: https://github.com/manouslinard/multiagent-exploration/.</p>
                <p>Last Updated: 2024-05-30 16:33:01 UTC</p>
                <button class="interpret-button" data-id="2405.20232v1">Interpret</button>
                <div id="interpretation-2405.20232v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Soft Partitioning of Latent Space for Semantic Channel Equalization</h3>
                <p>Authors: Tomás HuttebrauckerMohamed SanaEmilio Calvanese Strinati</p>
                <p><a href="http://arxiv.org/abs/2405.20085v1">Link to paper</a></p>
                <p>Semantic channel equalization has emerged as a solution to address languagemismatch in multi-user semantic communications. This approach aims to align thelatent spaces of an encoder and a decoder which were not jointly trained and itrelies on a partition of the semantic latent space into atoms based on thethe semantic meaning. In this work we explore the role of the semantic spacepartition in scenarios where the task structure involves a one-to-many mappingbetween the semantic space and the action space. In such scenariospartitioning based on hard inference results results in loss of informationwhich degrades the equalization performance. We propose a soft criterion toderive the atoms of the partition which leverages the soft decoders output andoffers a more comprehensive understanding of the semantic spaces structure.Through empirical validation we demonstrate that soft partitioning yields amore descriptive and regular partition of the space consequently enhancing theperformance of the equalization algorithm.</p>
                <p>Last Updated: 2024-05-30 14:16:19 UTC</p>
                <button class="interpret-button" data-id="2405.20085v1">Interpret</button>
                <div id="interpretation-2405.20085v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Safe Multi-agent Reinforcement Learning with Natural Language Constraints</h3>
                <p>Authors: Ziyan WangMeng FangTristan TomilinFei FangYali Du</p>
                <p><a href="http://arxiv.org/abs/2405.20018v1">Link to paper</a></p>
                <p>The role of natural language constraints in Safe Multi-agent ReinforcementLearning MARL is crucial yet often overlooked. While Safe MARL has vastpotential especially in fields like robotics and autonomous vehicles its fullpotential is limited by the need to define constraints in pre-designedmathematical terms which requires extensive domain expertise and reinforcementlearning knowledge hindering its broader adoption. To address this limitationand make Safe MARL more accessible and adaptable we propose a novel approachnamed Safe Multi-agent Reinforcement Learning with Natural Language constraintsSMALL. Our method leverages fine-tuned language models to interpret andprocess free-form textual constraints converting them into semantic embeddingsthat capture the essence of prohibited states and behaviours. These embeddingsare then integrated into the multi-agent policy learning process enablingagents to learn policies that minimize constraint violations while optimizingrewards. To evaluate the effectiveness of SMALL we introduce the LaMaSafe amulti-task benchmark designed to assess the performance of multiple agents inadhering to natural language constraints. Empirical evaluations across variousenvironments demonstrate that SMALL achieves comparable rewards andsignificantly fewer constraint violations highlighting its effectiveness inunderstanding and enforcing natural language constraints.</p>
                <p>Last Updated: 2024-05-30 12:57:35 UTC</p>
                <button class="interpret-button" data-id="2405.20018v1">Interpret</button>
                <div id="interpretation-2405.20018v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning</h3>
                <p>Authors: Hyungho NaIl-chul Moon</p>
                <p><a href="http://arxiv.org/abs/2405.19998v1">Link to paper</a></p>
                <p>In cooperative multi-agent reinforcement learning MARL agents collaborateto achieve common goals such as defeating enemies and scoring a goal. Howeverlearning goal-reaching paths toward such a semantic goal takes a considerableamount of time in complex tasks and the trained model often fails to find suchpaths. To address this we present LAtent Goal-guided Multi-Agent reinforcementlearning LAGMA which generates a goal-reaching trajectory in latent spaceand provides a latent goal-guided incentive to transitions toward thisreference trajectory. LAGMA consists of three major components: a quantizedlatent space constructed via a modified VQ-VAE for efficient sampleutilization b goal-reaching trajectory generation via extended VQ codebookand c latent goal-guided intrinsic reward generation to encourage transitionstowards the sampled goal-reaching path. The proposed method is evaluated byStarCraft II with both dense and sparse reward settings and Google ResearchFootball. Empirical results show further performance improvement overstate-of-the-art baselines.</p>
                <p>Last Updated: 2024-05-30 12:34:58 UTC</p>
                <button class="interpret-button" data-id="2405.19998v1">Interpret</button>
                <div id="interpretation-2405.19998v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Dispersion of personal spaces</h3>
                <p>Authors: Jaroslav HoráčekMiroslav Rada</p>
                <p><a href="http://arxiv.org/abs/2405.19895v1">Link to paper</a></p>
                <p>There are many entities that disseminate in the physical space - informationgossip mood innovation etc. Personal spaces are also entities that disperseand interplay. In this work we study the emergence of configurations formed byparticipants when choosing a place to sit in a rectangular auditorium. Based onexperimental questionnaire data we design several models and assess theirrelevancy to a real time-lapse footage of lecture hall being filled up. Themain focus is to compare the evolution of entropy of occupied seatconfigurations in time. Even though the process of choosing a seat is complexand could depend on various properties of participants or environment some ofthe developed models can capture at least basic essence of the real processes.After introducing the problem of seat selection and related results in closeresearch areas we introduce preliminary collected data and build models ofseat selection based on them. We compare the resulting models to the realobservational data and discuss areas of future research directions.</p>
                <p>Last Updated: 2024-05-30 09:52:27 UTC</p>
                <button class="interpret-button" data-id="2405.19895v1">Interpret</button>
                <div id="interpretation-2405.19895v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image</h3>
                <p>Authors: Kailu WuFangfu LiuZhihan CaiRunjie YanHanyang WangYating HuYueqi DuanKaisheng Ma</p>
                <p><a href="http://arxiv.org/abs/2405.20343v1">Link to paper</a></p>
                <p>In this work we introduce Unique3D a novel image-to-3D framework forefficiently generating high-quality 3D meshes from single-view imagesfeaturing state-of-the-art generation fidelity and strong generalizability.Previous methods based on Score Distillation Sampling SDS can producediversified 3D results by distilling 3D knowledge from large 2D diffusionmodels but they usually suffer from long per-case optimization time withinconsistent issues. Recent works address the problem and generate better 3Dresults either by finetuning a multi-view diffusion model or training a fastfeed-forward model. However they still lack intricate textures and complexgeometries due to inconsistency and limited generated resolution. Tosimultaneously achieve high fidelity consistency and efficiency in singleimage-to-3D we propose a novel framework Unique3D that includes a multi-viewdiffusion model with a corresponding normal diffusion model to generatemulti-view images with their normal maps a multi-level upscale process toprogressively improve the resolution of generated orthographic multi-views aswell as an instant and consistent mesh reconstruction algorithm called ISOMERwhich fully integrates the color and geometric priors into mesh results.Extensive experiments demonstrate that our Unique3D significantly outperformsother image-to-3D baselines in terms of geometric and textural details.</p>
                <p>Last Updated: 2024-05-30 17:59:54 UTC</p>
                <button class="interpret-button" data-id="2405.20343v1">Interpret</button>
                <div id="interpretation-2405.20343v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>From Zero to Hero: Cold-Start Anomaly Detection</h3>
                <p>Authors: Tal ReissGeorge KourNaama ZwerdlingAteret Anaby-TavorYedid Hoshen</p>
                <p><a href="http://arxiv.org/abs/2405.20341v1">Link to paper</a></p>
                <p>When first deploying an anomaly detection system e.g. to detectout-of-scope queries in chatbots there are no observed data makingdata-driven approaches ineffective. Zero-shot anomaly detection methods offer asolution to such cold-start cases but unfortunately they are often notaccurate enough. This paper studies the realistic but underexplored cold-startsetting where an anomaly detection model is initialized using zero-shotguidance but subsequently receives a small number of contaminated observationsnamely that may include anomalies. The goal is to make efficient use of boththe zero-shot guidance and the observations. We propose ColdFusion a methodthat effectively adapts the zero-shot anomaly detector to contaminatedobservations. To support future development of this new setting we propose anevaluation suite consisting of evaluation protocols and metrics.</p>
                <p>Last Updated: 2024-05-30 17:59:51 UTC</p>
                <button class="interpret-button" data-id="2405.20341v1">Interpret</button>
                <div id="interpretation-2405.20341v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CoSy: Evaluating Textual Explanations of Neurons</h3>
                <p>Authors: Laura KopfPhiline Lou BommerAnna HedströmSebastian LapuschkinMarina M. -C. HöhneKirill Bykov</p>
                <p><a href="http://arxiv.org/abs/2405.20331v1">Link to paper</a></p>
                <p>A crucial aspect of understanding the complex nature of Deep Neural NetworksDNNs is the ability to explain learned concepts within their latentrepresentations. While various methods exist to connect neurons to textualdescriptions of human-understandable concepts evaluating the quality of theseexplanation methods presents a major challenge in the field due to a lack ofunified general-purpose quantitative evaluation. In this work we introduceCoSy Concept Synthesis -- a novel architecture-agnostic framework toevaluate the quality of textual explanations for latent neurons. Given textualexplanations our proposed framework leverages a generative model conditionedon textual input to create data points representing the textual explanation.Then the neurons response to these explanation data points is compared withthe response to control data points providing a quality estimate of the givenexplanation. We ensure the reliability of our proposed framework in a series ofmeta-evaluation experiments and demonstrate practical value through insightsfrom benchmarking various concept-based textual explanation methods forComputer Vision tasks showing that tested explanation methods significantlydiffer in quality.</p>
                <p>Last Updated: 2024-05-30 17:59:04 UTC</p>
                <button class="interpret-button" data-id="2405.20331v1">Interpret</button>
                <div id="interpretation-2405.20331v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Don't drop your samples! Coherence-aware training benefits Conditional diffusion</h3>
                <p>Authors: Nicolas DufourVictor BesnierVicky KalogeitonDavid Picard</p>
                <p><a href="http://arxiv.org/abs/2405.20324v1">Link to paper</a></p>
                <p>Conditional diffusion models are powerful generative models that can leveragevarious types of conditional information such as class labels segmentationmasks or text captions. However in many real-world scenarios conditionalinformation may be noisy or unreliable due to human annotation errors or weakalignment. In this paper we propose the Coherence-Aware Diffusion CAD anovel method that integrates coherence in conditional information intodiffusion models allowing them to learn from noisy annotations withoutdiscarding data. We assume that each data point has an associated coherencescore that reflects the quality of the conditional information. We thencondition the diffusion model on both the conditional information and thecoherence score. In this way the model learns to ignore or discount theconditioning when the coherence is low. We show that CAD is theoretically soundand empirically effective on various conditional generation tasks. Moreover weshow that leveraging coherence generates realistic and diverse samples thatrespect conditional information better than models trained on cleaned datasetswhere samples with low coherence have been discarded.</p>
                <p>Last Updated: 2024-05-30 17:57:26 UTC</p>
                <button class="interpret-button" data-id="2405.20324v1">Interpret</button>
                <div id="interpretation-2405.20324v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Vision-based Manipulation from Single Human Video with Open-World Object Graphs</h3>
                <p>Authors: Yifeng ZhuArisrei LimPeter StoneYuke Zhu</p>
                <p><a href="http://arxiv.org/abs/2405.20321v1">Link to paper</a></p>
                <p>We present an object-centric approach to empower robots to learn vision-basedmanipulation skills from human videos. We investigate the problem of imitatingrobot manipulation from a single human video in the open-world setting where arobot must learn to manipulate novel objects from one video demonstration. Weintroduce ORION an algorithm that tackles the problem by extracting anobject-centric manipulation plan from a single RGB-D video and deriving apolicy that conditions on the extracted plan. Our method enables the robot tolearn from videos captured by daily mobile devices such as an iPad andgeneralize the policies to deployment environments with varying visualbackgrounds camera angles spatial layouts and novel object instances. Wesystematically evaluate our method on both short-horizon and long-horizontasks demonstrating the efficacy of ORION in learning from a single humanvideo in the open world. Videos can be found in the project websitehttps://ut-austin-rpl.github.io/ORION-release.</p>
                <p>Last Updated: 2024-05-30 17:56:54 UTC</p>
                <button class="interpret-button" data-id="2405.20321v1">Interpret</button>
                <div id="interpretation-2405.20321v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-05-31</p>
        </div>
    
        </div>
    </body>
    </html>
    