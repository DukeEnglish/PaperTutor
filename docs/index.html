
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>SoundScape: A Human-AI Co-Creation System Making Your Memories Heard</h3>
                <p>Authors: Chongjun ZhongJiaxing YuYingping CaoSongruoyao WuWenqi WuKejun Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.08136v1">Link to paper</a></p>
                <p>Sound plays a significant role in human memory yet it is often overlooked bymainstream life-recording methods. Most current UGC User-Generated Contentcreation tools emphasize visual content while lacking user-friendly sounddesign features. This paper introduces SoundScape a human-AI co-creationsystem that allows users to easily create sound memories on mobile devicesthrough innovative interaction. By integrating sound effects and music withvisual scenes SoundScape encourages users to enrich their creations withimmersive sound elements enhancing the atmosphere of their works. To supportpublic creation SoundScape incorporates a conversational agent and AI musicgeneration technology. User studies indicate that our approach is effective forsound memory creation with SoundScape outperforming existing tools in terms ofuser experience and the perceived quality of produced works.</p>
                <p>Last Updated: 2024-10-10 17:19:50 UTC</p>
                <button class="interpret-button" data-id="2410.08136v1">Interpret</button>
                <div id="interpretation-2410.08136v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Crossing Margins: Intersectional Users' Ethical Concerns about Software</h3>
                <p>Authors: Lauren OlsonTom P. HumbertRicarda Anna-Lena FischerBob WesterveldFlorian KunnemanEmitzá Guzmán</p>
                <p><a href="http://arxiv.org/abs/2410.08090v1">Link to paper</a></p>
                <p>Many modern software applications present numerous ethical concerns due toconflicts between users values and companies priorities. Intersectionalcommunities those with multiple marginalized identities aredisproportionately affected by these ethical issues leading to legalfinancial and reputational issues for software companies as well asreal-world harm for intersectional users. Historically the voices ofintersectional communities have been systematically marginalized and excludedfrom contributing their unique perspectives to software design perpetuatingsoftware-related ethical concerns.  This work aims to fill the gap in research on intersectional userssoftware-related perspectives and provide software practitioners with astarting point to address their ethical concerns. We aggregated and analyzedthe intersectional users ethical concerns over time and developed aprioritization method to identify critical concerns. To achieve this wecollected posts from over 700 intersectional subreddits discussing softwareapplications utilized deep learning to identify ethical concerns in theseposts and employed state-of-the-art techniques to analyze their content inrelation to time and priority. Our findings revealed that intersectionalcommunities report textitcritical complaints related to cyberbullyinginappropriate content and discrimination highlighting significant flaws inmodern software particularly for intersectional users. Based on thesefindings we discuss how to better address the ethical concerns ofintersectional users in software development.</p>
                <p>Last Updated: 2024-10-10 16:33:05 UTC</p>
                <button class="interpret-button" data-id="2410.08090v1">Interpret</button>
                <div id="interpretation-2410.08090v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>APOLLO: A GPT-based tool to detect phishing emails and generate explanations that warn users</h3>
                <p>Authors: Giuseppe DesoldaFrancesco GrecoLuca Viganò</p>
                <p><a href="http://arxiv.org/abs/2410.07997v1">Link to paper</a></p>
                <p>Phishing is one of the most prolific cybercriminal activities with attacksbecoming increasingly sophisticated. It is therefore imperative to explorenovel technologies to improve user protection across both technical and humandimensions. Large Language Models LLMs offer significant promise for textprocessing in various domains but their use for defense against phishingattacks still remains scarcely explored. In this paper we present APOLLO atool based on OpenAIs GPT-4o to detect phishing emails and generateexplanation messages to users about why a specific email is dangerous thusimproving their decision-making capabilities. We have evaluated the performanceof APOLLO in classifying phishing emails the results show that the LLM modelshave exemplary capabilities in classifying phishing emails 97 percent accuracyin the case of GPT-4o and that this performance can be further improved byintegrating data from third-party services resulting in a near-perfectclassification rate 99 percent accuracy. To assess the perception of theexplanations generated by this tool we also conducted a study with 20participants comparing four different explanations presented as phishingwarnings. We compared the LLM-generated explanations to four baselines: amanually crafted warning and warnings from Chrome Firefox and Edge browsers.The results show that not only the LLM-generated explanations were perceived ashigh quality but also that they can be more understandable interesting andtrustworthy than the baselines. These findings suggest that using LLMs as adefense against phishing is a very promising approach with APOLLO representinga proof of concept in this research direction.</p>
                <p>Last Updated: 2024-10-10 14:53:39 UTC</p>
                <button class="interpret-button" data-id="2410.07997v1">Interpret</button>
                <div id="interpretation-2410.07997v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets</h3>
                <p>Authors: Tommaso GiorgiLorenzo CimaTiziano FagniMarco AvvenutiStefano Cresci</p>
                <p><a href="http://arxiv.org/abs/2410.07991v1">Link to paper</a></p>
                <p>The rise of online platforms exacerbated the spread of hate speech demandingscalable and effective detection. However the accuracy of hate speechdetection systems heavily relies on human-labeled data which is inherentlysusceptible to biases. While previous work has examined the issue theinterplay between the characteristics of the annotator and those of the targetof the hate are still unexplored. We fill this gap by leveraging an extensivedataset with rich socio-demographic information of both annotators and targetsuncovering how human biases manifest in relation to the targets attributes.Our analysis surfaces the presence of widespread biases which wequantitatively describe and characterize based on their intensity andprevalence revealing marked differences. Furthermore we compare human biaseswith those exhibited by persona-based LLMs. Our findings indicate that whilepersona-based LLMs do exhibit biases these differ significantly from those ofhuman annotators. Overall our work offers new and nuanced results on humanbiases in hate speech annotations as well as fresh insights into the design ofAI-driven hate speech detection systems.</p>
                <p>Last Updated: 2024-10-10 14:48:57 UTC</p>
                <button class="interpret-button" data-id="2410.07991v1">Interpret</button>
                <div id="interpretation-2410.07991v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Post-Training Quantization in Brain-Computer Interfaces based on Event-Related Potential Detection</h3>
                <p>Authors: Hubert CecottiDalvir DhaliwalHardip SinghYogesh Kumar Meena</p>
                <p><a href="http://arxiv.org/abs/2410.07920v1">Link to paper</a></p>
                <p>Post-training quantization PTQ is a technique used to optimize and reducethe memory footprint and computational requirements of machine learning models.It has been used primarily for neural networks. For Brain-Computer InterfacesBCI that are fully portable and usable in various situations it is necessaryto provide approaches that are lightweight for storage and computation. In thispaper we propose the evaluation of post-training quantization onstate-of-the-art approaches in brain-computer interfaces and assess theirimpact on accuracy. We evaluate the performance of the single-trial detectionof event-related potentials representing one major BCI paradigm. The area underthe receiver operating characteristic curve drops from 0.861 to 0.825 with PTQwhen applied on both spatial filters and the classifier while reducing thesize of the model by about times 15. The results support the conclusion thatPTQ can substantially reduce the memory footprint of the models while keepingroughly the same level of accuracy.</p>
                <p>Last Updated: 2024-10-10 13:47:30 UTC</p>
                <button class="interpret-button" data-id="2410.07920v1">Interpret</button>
                <div id="interpretation-2410.07920v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Agent-based modeling for realistic reproduction of human mobility and contact behavior to evaluate test and isolation strategies in epidemic infectious disease spread</h3>
                <p>Authors: David KerkmannSascha KorfKhoa NguyenDaniel AbeleAlain SchengenCarlotta GersteinJens Henrik GöbbertAchim BasermannMartin J. KühnMichael Meyer-Hermann</p>
                <p><a href="http://arxiv.org/abs/2410.08050v1">Link to paper</a></p>
                <p>Agent-based models have proven to be useful tools in supportingdecision-making processes in different application domains. The advent ofmodern computers and supercomputers has enabled these bottom-up approaches torealistically model human mobility and contact behavior. The COVID-19 pandemicshowcased the urgent need for detailed and informative models that can answerresearch questions on transmission dynamics. We present a sophisticatedagent-based model to simulate the spread of respiratory diseases. The model ishighly modularized and can be used on various scales from a small collectionof buildings up to cities or countries. Although not being the focus of thispaper the model has undergone performance engineering on a single core andprovides an efficient intra- and inter-simulation parallelization fortime-critical decision-making processes.  In order to allow answering research questions on individual levelresolution nonpharmaceutical intervention strategies such as face masks orvenue closures can be implemented for particular locations or agents. Inparticular we allow for sophisticated testing and isolation strategies tostudy the effects of minimal-invasive infectious disease mitigation. Withrealistic human mobility patterns for the region of Brunswick Germany westudy the effects of different interventions between March 1st and May 30 2021in the SARS-CoV-2 pandemic. Our analyses suggest that symptom-independenttesting has limited impact on the mitigation of disease dynamics if the darkfigure in symptomatic cases is high. Furthermore we found that quarantinelength is more important than quarantine efficiency but that with sufficientsymptomatic control also short quarantines can have a substantial effect.</p>
                <p>Last Updated: 2024-10-10 15:45:27 UTC</p>
                <button class="interpret-button" data-id="2410.08050v1">Interpret</button>
                <div id="interpretation-2410.08050v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Strategic Classification With Externalities</h3>
                <p>Authors: Yiling ChenSafwan HossainEvi MichaAriel Procaccia</p>
                <p><a href="http://arxiv.org/abs/2410.08032v1">Link to paper</a></p>
                <p>We propose a new variant of the strategic classification problem: a principalreveals a classifier and n agents report their possibly manipulatedfeatures to be classified. Motivated by real-world applications our modelcrucially allows the manipulation of one agent to affect another that is itexplicitly captures inter-agent externalities. The principal-agent interactionsare formally modeled as a Stackelberg game with the resulting agentmanipulation dynamics captured as a simultaneous game. We show that undercertain assumptions the pure Nash Equilibrium of this agent manipulation gameis unique and can be efficiently computed. Leveraging this result PAC learningguarantees are established for the learner: informally we show that it ispossible to learn classifiers that minimize loss on the distribution even whena random number of agents are manipulating their way to a pure NashEquilibrium. We also comment on the optimization of such classifiers throughgradient-based approaches. This work sets the theoretical foundations for amore realistic analysis of classifiers that are robust against multiplestrategic actors interacting in a common environment.</p>
                <p>Last Updated: 2024-10-10 15:28:04 UTC</p>
                <button class="interpret-button" data-id="2410.08032v1">Interpret</button>
                <div id="interpretation-2410.08032v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Dynamic Programming based Local Search approaches for Multi-Agent Path Finding problems on Directed Graphs</h3>
                <p>Authors: Irene SaccaniStefano ArdizzoniLuca ConsoliniMarco Locatelli</p>
                <p><a href="http://arxiv.org/abs/2410.07954v1">Link to paper</a></p>
                <p>Among sub-optimal Multi-Agent Path Finding MAPF solvers rule-basedalgorithms are particularly appealing since they are complete. Even in crowdedscenarios they allow finding a feasible solution that brings each agent to itstarget preventing deadlock situations. However generally rule-basedalgorithms provide much longer solutions than the shortest one. The maincontribution of this paper is introducing a new local search procedure forimproving a known feasible solution. We start from a feasible sub-optimalsolution and perform a local search in a neighborhood of this solution. If weare able to find a shorter solution we repeat this procedure until thesolution cannot be shortened anymore. At the end we obtain a solution that isstill sub-optimal but generally of much better quality than the initial one.We propose two different local search policies. In the first we explore allpaths in which the agents positions remain in a neighborhood of thecorresponding positions of the reference solution. In the second we set anupper limit to the number of agents that can change their path with respect tothe reference solution. These two different policies can also be alternated. Weexplore the neighborhoods by dynamic programming. The fact that our search islocal is fundamental in terms of time complexity. Indeed if the dynamicprogramming approach is applied to the full MAPF problem the number ofexplored states grows exponentially with the number of agents. Instead theintroduction of a locality constraint allows exploring the neghborhoods in atime that grows polynomially with respect to the number of agents.</p>
                <p>Last Updated: 2024-10-10 14:20:19 UTC</p>
                <button class="interpret-button" data-id="2410.07954v1">Interpret</button>
                <div id="interpretation-2410.07954v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Benchmarking Agentic Workflow Generation</h3>
                <p>Authors: Shuofei QiaoRunnan FangZhisong QiuXiaobin WangNingyu ZhangYong JiangPengjun XieFei HuangHuajun Chen</p>
                <p><a href="http://arxiv.org/abs/2410.07869v1">Link to paper</a></p>
                <p>Large Language Models LLMs with their exceptional ability to handle a widerange of tasks have driven significant advancements in tackling reasoning andplanning tasks wherein decomposing complex problems into executable workflowsis a crucial step in this process. Existing workflow evaluation frameworkseither focus solely on holistic performance or suffer from limitations such asrestricted scenario coverage simplistic workflow structures and laxevaluation standards. To this end we introduce WorFBench a unified workflowgeneration benchmark with multi-faceted scenarios and intricate graph workflowstructures. Additionally we present WorFEval a systemic evaluation protocolutilizing subsequence and subgraph matching algorithms to accurately quantifythe LLM agents workflow generation capabilities. Through comprehensiveevaluations across different types of LLMs we discover distinct gaps betweenthe sequence planning capabilities and graph planning capabilities of LLMagents with even GPT-4 exhibiting a gap of around 15. We also train twoopen-source models and evaluate their generalization abilities on held-outtasks. Furthermore we observe that the generated workflows can enhancedownstream tasks enabling them to achieve superior performance with less timeduring inference. Code and dataset will be available athttps://github.com/zjunlp/WorFBench.</p>
                <p>Last Updated: 2024-10-10 12:41:19 UTC</p>
                <button class="interpret-button" data-id="2410.07869v1">Interpret</button>
                <div id="interpretation-2410.07869v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Hate Speech Moderated Chat Application: Use Case for GDPR and DSA Compliance</h3>
                <p>Authors: Jan FilliesTheodoros MitsikasRalph SchäfermeierAdrian Paschke</p>
                <p><a href="http://arxiv.org/abs/2410.07713v1">Link to paper</a></p>
                <p>The detection of hate speech or toxic content online is a complex andsensitive issue. While the identification itself is highly dependent on thecontext of the situation sensitive personal attributes such as age languageand nationality are rarely available due to privacy concerns. Additionallyplatforms struggle with a wide range of local jurisdictions regarding onlinehate speech and the evaluation of content based on their internal ethicalnorms. This research presents a novel approach that demonstrates aGDPR-compliant application capable of implementing legal and ethical reasoninginto the content moderation process. The application increases theexplainability of moderation decisions by utilizing user information. Two usecases fundamental to online communication are presented and implemented usingtechnologies such as GPT-3.5 Solid Pods and the rule language Prova. Thefirst use case demonstrates the scenario of a platform aiming to protectadolescents from potentially harmful content by limiting the ability to postcertain content when minors are present. The second use case aims to identifyand counter problematic statements online by providing counter hate speech. Thecounter hate speech is generated using personal attributes to appeal to theuser. This research lays the groundwork for future DSA compliance of onlineplatforms. The work proposes a novel approach to reason within different legaland ethical definitions of hate speech and plan the fitting counter hatespeech. Overall the platform provides a fitted protection to users and a moreexplainable and individualized response. The hate speech detection service thechat platform and the reasoning in Prova are discussed and the potentialbenefits for content moderation and algorithmic hate speech detection areoutlined. A selection of important aspects for DSA compliance is outlined.</p>
                <p>Last Updated: 2024-10-10 08:28:38 UTC</p>
                <button class="interpret-button" data-id="2410.07713v1">Interpret</button>
                <div id="interpretation-2410.07713v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Features are fate: a theory of transfer learning in high-dimensional regression</h3>
                <p>Authors: Javan TahirSurya GanguliGrant M. Rotskoff</p>
                <p><a href="http://arxiv.org/abs/2410.08194v1">Link to paper</a></p>
                <p>With the emergence of large-scale pre-trained neural networks methods toadapt such foundation models to data-limited downstream tasks have become anecessity. Fine-tuning preference optimization and transfer learning have allbeen successfully employed for these purposes when the target task closelyresembles the source task but a precise theoretical understanding of tasksimilarity is still lacking. While conventional wisdom suggests that simplemeasures of similarity between source and target distributions such asphi-divergences or integral probability metrics can directly predict thesuccess of transfer we prove the surprising fact that in general this is notthe case. We adopt instead a feature-centric viewpoint on transfer learningand establish a number of theoretical results that demonstrate that when thetarget task is well represented by the feature space of the pre-trained modeltransfer learning outperforms training from scratch. We study deep linearnetworks as a minimal model of transfer learning in which we can analyticallycharacterize the transferability phase diagram as a function of the targetdataset size and the feature space overlap. For this model we establishrigorously that when the feature space overlap between the source and targettasks is sufficiently strong both linear transfer and fine-tuning improveperformance especially in the low data limit. These results build on anemerging understanding of feature learning dynamics in deep linear networksand we demonstrate numerically that the rigorous results we derive for thelinear case also apply to nonlinear networks.</p>
                <p>Last Updated: 2024-10-10 17:58:26 UTC</p>
                <button class="interpret-button" data-id="2410.08194v1">Interpret</button>
                <div id="interpretation-2410.08194v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Generalizing Stochastic Smoothing for Differentiation and Gradient Estimation</h3>
                <p>Authors: Felix PetersenChristian BorgeltAashwin MishraStefano Ermon</p>
                <p><a href="http://arxiv.org/abs/2410.08125v1">Link to paper</a></p>
                <p>We deal with the problem of gradient estimation for stochastic differentiablerelaxations of algorithms operators simulators and other non-differentiablefunctions. Stochastic smoothing conventionally perturbs the input of anon-differentiable function with a differentiable density distribution withfull support smoothing it and enabling gradient estimation. Our theory startsat first principles to derive stochastic smoothing with reduced assumptionswithout requiring a differentiable density nor full support and we present ageneral framework for relaxation and gradient estimation of non-differentiableblack-box functions f:mathbbRntomathbbRm. We develop variancereduction for gradient estimation from 3 orthogonal perspectives. Empiricallywe benchmark 6 distributions and up to 24 variance reduction strategies fordifferentiable sorting and ranking differentiable shortest-paths on graphsdifferentiable rendering for pose estimation as well as differentiable cryo-ETsimulations.</p>
                <p>Last Updated: 2024-10-10 17:10:00 UTC</p>
                <button class="interpret-button" data-id="2410.08125v1">Interpret</button>
                <div id="interpretation-2410.08125v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Active Fourier Auditor for Estimating Distributional Properties of ML Models</h3>
                <p>Authors: Ayoub AjarraBishwamittra GhoshDebabrota Basu</p>
                <p><a href="http://arxiv.org/abs/2410.08111v1">Link to paper</a></p>
                <p>With the pervasive deployment of Machine Learning ML models in real-worldapplications verifying and auditing properties of ML models have become acentral concern. In this work we focus on three properties: robustnessindividual fairness and group fairness. We discuss two approaches for auditingML model properties: estimation with and without reconstruction of the targetmodel under audit. Though the first approach is studied in the literature thesecond approach remains unexplored. For this purpose we develop a newframework that quantifies different properties in terms of the Fouriercoefficients of the ML model under audit but does not parametricallyreconstruct it. We propose the Active Fourier Auditor AFA which queriessample points according to the Fourier coefficients of the ML model andfurther estimates the properties. We derive high probability error bounds onAFAs estimates along with the worst-case lower bounds on the samplecomplexity to audit them. Numerically we demonstrate on multiple datasets andmodels that AFA is more accurate and sample-efficient to estimate theproperties of interest than the baselines.</p>
                <p>Last Updated: 2024-10-10 16:57:01 UTC</p>
                <button class="interpret-button" data-id="2410.08111v1">Interpret</button>
                <div id="interpretation-2410.08111v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Noether's razor: Learning Conserved Quantities</h3>
                <p>Authors: Tycho F. A. van der OuderaaMark van der WilkPim de Haan</p>
                <p><a href="http://arxiv.org/abs/2410.08087v1">Link to paper</a></p>
                <p>Symmetries have proven useful in machine learning models improvinggeneralisation and overall performance. At the same time recent advancementsin learning dynamical systems rely on modelling the underlying Hamiltonian toguarantee the conservation of energy. These approaches can be connected via aseminal result in mathematical physics: Noethers theorem which states thatsymmetries in a dynamical system correspond to conserved quantities. This workuses Noethers theorem to parameterise symmetries as learnable conservedquantities. We then allow conserved quantities and associated symmetries to belearned directly from train data through approximate Bayesian model selectionjointly with the regular training procedure. As training objective we derive avariational lower bound to the marginal likelihood. The objective automaticallyembodies an Occams Razor effect that avoids collapse of conservation laws tothe trivial constant without the need to manually add and tune additionalregularisers. We demonstrate a proof-of-principle on n-harmonic oscillatorsand n-body systems. We find that our method correctly identifies the correctconserved quantities and Un and SEn symmetry groups improving overallperformance and predictive accuracy on test data.</p>
                <p>Last Updated: 2024-10-10 16:29:49 UTC</p>
                <button class="interpret-button" data-id="2410.08087v1">Interpret</button>
                <div id="interpretation-2410.08087v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Gaussian Process Thompson Sampling via Rootfinding</h3>
                <p>Authors: Taiwo A. AdebiyiBach DoRuda Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.08071v1">Link to paper</a></p>
                <p>Thompson sampling TS is a simple effective stochastic policy in Bayesiandecision making. It samples the posterior belief about the reward profile andoptimizes the sample to obtain a candidate decision. In continuousoptimization the posterior of the objective function is often a Gaussianprocess GP whose sample paths have numerous local optima making theirglobal optimization challenging. In this work we introduce an efficient globaloptimization strategy for GP-TS that carefully selects starting points forgradient-based multi-start optimizers. It identifies all local optima of theprior sample via univariate global rootfinding and optimizes the posteriorsample using a differentiable decoupled representation. We demonstrateremarkable improvement in the global optimization of GP posterior samplesespecially in high dimensions. This leads to dramatic improvements in theoverall performance of Bayesian optimization using GP-TS acquisition functionssurprisingly outperforming alternatives like GP-UCB and EI.</p>
                <p>Last Updated: 2024-10-10 16:06:45 UTC</p>
                <button class="interpret-button" data-id="2410.08071v1">Interpret</button>
                <div id="interpretation-2410.08071v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts</h3>
                <p>Authors: Anh-Quan CaoMaximilian JaritzMatthieu GuillauminRaoul de CharetteLoris Bazzani</p>
                <p><a href="http://arxiv.org/abs/2410.08211v1">Link to paper</a></p>
                <p>Large-scale vision-language pre-trained VLP models e.g. CLIP arerenowned for their versatility as they can be applied to diverse applicationsin a zero-shot setup. However when these models are used in specific domainstheir performance often falls short due to domain gaps or theunder-representation of these domains in the training data. While fine-tuningVLP models on custom datasets with human-annotated labels can address thisissue annotating even a small-scale dataset e.g. 100k samples can be anexpensive endeavor often requiring expert annotators if the task is complex.To address these challenges we propose LatteCLIP an unsupervised method forfine-tuning CLIP models on classification with known class names in customdomains without relying on human annotations. Our method leverages LargeMultimodal Models LMMs to generate expressive textual descriptions for bothindividual images and groups of images. These provide additional contextualinformation to guide the fine-tuning process in the custom domains. SinceLMM-generated descriptions are prone to hallucination or missing details weintroduce a novel strategy to distill only the useful information and stabilizethe training. Specifically we learn rich per-class prototype representationsfrom noisy generated texts and dual pseudo-labels. Our experiments on 10domain-specific datasets show that LatteCLIP outperforms pre-trained zero-shotmethods by an average improvement of 4.74 points in top-1 accuracy and otherstate-of-the-art unsupervised methods by 3.45 points.</p>
                <p>Last Updated: 2024-10-10 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2410.08211v1">Interpret</button>
                <div id="interpretation-2410.08211v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented Object Detection</h3>
                <p>Authors: Botao RenXue YangYi YuJunwei LuoZhidong Deng</p>
                <p><a href="http://arxiv.org/abs/2410.08210v1">Link to paper</a></p>
                <p>Single point supervised oriented object detection has gained attention andmade initial progress within the community. Diverse from those approachesrelying on one-shot samples or powerful pretrained models e.g. SAM PointOBBhas shown promise due to its prior-free feature. In this paper we proposePointOBB-v2 a simpler faster and stronger method to generate pseudo rotatedboxes from points without relying on any other prior. Specifically we firstgenerate a Class Probability Map CPM by training the network with non-uniformpositive and negative sampling. We show that the CPM is able to learn theapproximate object regions and their contours. Then Principal ComponentAnalysis PCA is applied to accurately estimate the orientation and theboundary of objects. By further incorporating a separation mechanism weresolve the confusion caused by the overlapping on the CPM enabling itsoperation in high-density scenarios. Extensive comparisons demonstrate that ourmethod achieves a training speed 15.58x faster and an accuracy improvement of11.60/25.15/21.19 on the DOTA-v1.0/v1.5/v2.0 datasets compared to theprevious state-of-the-art PointOBB. This significantly advances the cuttingedge of single point supervised oriented detection in the modular track.</p>
                <p>Last Updated: 2024-10-10 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2410.08210v1">Interpret</button>
                <div id="interpretation-2410.08210v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision</h3>
                <p>Authors: Shengcao CaoLiang-Yan GuiYu-Xiong Wang</p>
                <p><a href="http://arxiv.org/abs/2410.08209v1">Link to paper</a></p>
                <p>Current large multimodal models LMMs face challenges in grounding whichrequires the model to relate language components to visual entities. Contraryto the common practice that fine-tunes LMMs with additional groundingsupervision we find that the grounding ability can in fact emerge in LMMstrained without explicit grounding supervision. To reveal this emerginggrounding we introduce an attend-and-segment method which leveragesattention maps from standard LMMs to perform pixel-level segmentation.Furthermore to enhance the grounding ability we propose DIFFLMM an LMMutilizing a diffusion-based visual encoder as opposed to the standard CLIPvisual encoder and trained with the same weak supervision. Without beingconstrained by the biases and limited scale of grounding-specific supervisiondata our approach is more generalizable and scalable. We achieve competitiveperformance on both grounding-specific and general visual question answeringbenchmarks compared with grounding LMMs and generalist LMMs respectively.Notably we achieve a 44.2 grounding mask recall on grounded conversationgeneration without any grounding supervision outperforming the extensivelysupervised model GLaMM. Project page: https://groundLMM.github.io.</p>
                <p>Last Updated: 2024-10-10 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2410.08209v1">Interpret</button>
                <div id="interpretation-2410.08209v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SPA: 3D Spatial-Awareness Enables Effective Embodied Representation</h3>
                <p>Authors: Haoyi ZhuHonghui YangYating WangJiange YangLimin WangTong He</p>
                <p><a href="http://arxiv.org/abs/2410.08208v1">Link to paper</a></p>
                <p>In this paper we introduce SPA a novel representation learning frameworkthat emphasizes the importance of 3D spatial awareness in embodied AI. Ourapproach leverages differentiable neural rendering on multi-view images toendow a vanilla Vision Transformer ViT with intrinsic spatial understanding.We present the most comprehensive evaluation of embodied representationlearning to date covering 268 tasks across 8 simulators with diverse policiesin both single-task and language-conditioned multi-task scenarios. The resultsare compelling: SPA consistently outperforms more than 10 state-of-the-artrepresentation methods including those specifically designed for embodied AIvision-centric tasks and multi-modal applications while using less trainingdata. Furthermore we conduct a series of real-world experiments to confirm itseffectiveness in practical scenarios. These results highlight the critical roleof 3D spatial awareness for embodied representation learning. Our strongestmodel takes more than 6000 GPU hours to train and we are committed toopen-sourcing all code and model weights to foster future research in embodiedrepresentation learning. Project Page: https://haoyizhu.github.io/spa/.</p>
                <p>Last Updated: 2024-10-10 17:59:51 UTC</p>
                <button class="interpret-button" data-id="2410.08208v1">Interpret</button>
                <div id="interpretation-2410.08208v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models</h3>
                <p>Authors: Xiaoxiao HeLigong HanQuan DaoSong WenMinhao BaiDi LiuHan ZhangMartin Renqiang MinFelix Juefei-XuChaowei TanBo LiuKang LiHongdong LiJunzhou HuangFaez AhmedAkash SrivastavaDimitris Metaxas</p>
                <p><a href="http://arxiv.org/abs/2410.08207v1">Link to paper</a></p>
                <p>Discrete diffusion models have achieved success in tasks like imagegeneration and masked language modeling but face limitations in controlledcontent editing. We introduce DICE Discrete Inversion for ControllableEditing the first approach to enable precise inversion for discrete diffusionmodels including multinomial diffusion and masked generative models. Byrecording noise sequences and masking patterns during the reverse diffusionprocess DICE enables accurate reconstruction and flexible editing of discretedata without the need for predefined masks or attention manipulation. Wedemonstrate the effectiveness of DICE across both image and text domainsevaluating it on models such as VQ-Diffusion Paella and RoBERTa. Our resultsshow that DICE preserves high data fidelity while enhancing editingcapabilities offering new opportunities for fine-grained content manipulationin discrete spaces. For project webpage seehttps://hexiaoxiao-cs.github.io/DICE/.</p>
                <p>Last Updated: 2024-10-10 17:59:48 UTC</p>
                <button class="interpret-button" data-id="2410.08207v1">Interpret</button>
                <div id="interpretation-2410.08207v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts</h3>
                <p>Authors: Anh-Quan CaoMaximilian JaritzMatthieu GuillauminRaoul de CharetteLoris Bazzani</p>
                <p><a href="http://arxiv.org/abs/2410.08211v1">Link to paper</a></p>
                <p>Large-scale vision-language pre-trained VLP models e.g. CLIP arerenowned for their versatility as they can be applied to diverse applicationsin a zero-shot setup. However when these models are used in specific domainstheir performance often falls short due to domain gaps or theunder-representation of these domains in the training data. While fine-tuningVLP models on custom datasets with human-annotated labels can address thisissue annotating even a small-scale dataset e.g. 100k samples can be anexpensive endeavor often requiring expert annotators if the task is complex.To address these challenges we propose LatteCLIP an unsupervised method forfine-tuning CLIP models on classification with known class names in customdomains without relying on human annotations. Our method leverages LargeMultimodal Models LMMs to generate expressive textual descriptions for bothindividual images and groups of images. These provide additional contextualinformation to guide the fine-tuning process in the custom domains. SinceLMM-generated descriptions are prone to hallucination or missing details weintroduce a novel strategy to distill only the useful information and stabilizethe training. Specifically we learn rich per-class prototype representationsfrom noisy generated texts and dual pseudo-labels. Our experiments on 10domain-specific datasets show that LatteCLIP outperforms pre-trained zero-shotmethods by an average improvement of 4.74 points in top-1 accuracy and otherstate-of-the-art unsupervised methods by 3.45 points.</p>
                <p>Last Updated: 2024-10-10 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2410.08211v1">Interpret</button>
                <div id="interpretation-2410.08211v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented Object Detection</h3>
                <p>Authors: Botao RenXue YangYi YuJunwei LuoZhidong Deng</p>
                <p><a href="http://arxiv.org/abs/2410.08210v1">Link to paper</a></p>
                <p>Single point supervised oriented object detection has gained attention andmade initial progress within the community. Diverse from those approachesrelying on one-shot samples or powerful pretrained models e.g. SAM PointOBBhas shown promise due to its prior-free feature. In this paper we proposePointOBB-v2 a simpler faster and stronger method to generate pseudo rotatedboxes from points without relying on any other prior. Specifically we firstgenerate a Class Probability Map CPM by training the network with non-uniformpositive and negative sampling. We show that the CPM is able to learn theapproximate object regions and their contours. Then Principal ComponentAnalysis PCA is applied to accurately estimate the orientation and theboundary of objects. By further incorporating a separation mechanism weresolve the confusion caused by the overlapping on the CPM enabling itsoperation in high-density scenarios. Extensive comparisons demonstrate that ourmethod achieves a training speed 15.58x faster and an accuracy improvement of11.60/25.15/21.19 on the DOTA-v1.0/v1.5/v2.0 datasets compared to theprevious state-of-the-art PointOBB. This significantly advances the cuttingedge of single point supervised oriented detection in the modular track.</p>
                <p>Last Updated: 2024-10-10 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2410.08210v1">Interpret</button>
                <div id="interpretation-2410.08210v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision</h3>
                <p>Authors: Shengcao CaoLiang-Yan GuiYu-Xiong Wang</p>
                <p><a href="http://arxiv.org/abs/2410.08209v1">Link to paper</a></p>
                <p>Current large multimodal models LMMs face challenges in grounding whichrequires the model to relate language components to visual entities. Contraryto the common practice that fine-tunes LMMs with additional groundingsupervision we find that the grounding ability can in fact emerge in LMMstrained without explicit grounding supervision. To reveal this emerginggrounding we introduce an attend-and-segment method which leveragesattention maps from standard LMMs to perform pixel-level segmentation.Furthermore to enhance the grounding ability we propose DIFFLMM an LMMutilizing a diffusion-based visual encoder as opposed to the standard CLIPvisual encoder and trained with the same weak supervision. Without beingconstrained by the biases and limited scale of grounding-specific supervisiondata our approach is more generalizable and scalable. We achieve competitiveperformance on both grounding-specific and general visual question answeringbenchmarks compared with grounding LMMs and generalist LMMs respectively.Notably we achieve a 44.2 grounding mask recall on grounded conversationgeneration without any grounding supervision outperforming the extensivelysupervised model GLaMM. Project page: https://groundLMM.github.io.</p>
                <p>Last Updated: 2024-10-10 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2410.08209v1">Interpret</button>
                <div id="interpretation-2410.08209v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SPA: 3D Spatial-Awareness Enables Effective Embodied Representation</h3>
                <p>Authors: Haoyi ZhuHonghui YangYating WangJiange YangLimin WangTong He</p>
                <p><a href="http://arxiv.org/abs/2410.08208v1">Link to paper</a></p>
                <p>In this paper we introduce SPA a novel representation learning frameworkthat emphasizes the importance of 3D spatial awareness in embodied AI. Ourapproach leverages differentiable neural rendering on multi-view images toendow a vanilla Vision Transformer ViT with intrinsic spatial understanding.We present the most comprehensive evaluation of embodied representationlearning to date covering 268 tasks across 8 simulators with diverse policiesin both single-task and language-conditioned multi-task scenarios. The resultsare compelling: SPA consistently outperforms more than 10 state-of-the-artrepresentation methods including those specifically designed for embodied AIvision-centric tasks and multi-modal applications while using less trainingdata. Furthermore we conduct a series of real-world experiments to confirm itseffectiveness in practical scenarios. These results highlight the critical roleof 3D spatial awareness for embodied representation learning. Our strongestmodel takes more than 6000 GPU hours to train and we are committed toopen-sourcing all code and model weights to foster future research in embodiedrepresentation learning. Project Page: https://haoyizhu.github.io/spa/.</p>
                <p>Last Updated: 2024-10-10 17:59:51 UTC</p>
                <button class="interpret-button" data-id="2410.08208v1">Interpret</button>
                <div id="interpretation-2410.08208v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions</h3>
                <p>Authors: Changle QuSunhao DaiXiaochi WeiHengyi CaiShuaiqiang WangDawei YinJun XuJi-Rong Wen</p>
                <p><a href="http://arxiv.org/abs/2410.08197v1">Link to paper</a></p>
                <p>Tool learning enables Large Language Models LLMs to interact with externalenvironments by invoking tools serving as an effective strategy to mitigatethe limitations inherent in their pre-training data. In this process tooldocumentation plays a crucial role by providing usage instructions for LLMsthereby facilitating effective tool utilization. This paper concentrates on thecritical challenge of bridging the comprehension gap between LLMs and externaltools due to the inadequacies and inaccuracies inherent in existinghuman-centric tool documentation. We propose a novel framework DRAFT aimed atDynamically Refining tool documentation through the Analysis of Feedback andTrails emanating from LLMs interactions with external tools. This methodologypivots on an innovative trial-and-error approach consisting of three distinctlearning phases: experience gathering learning from experience anddocumentation rewriting to iteratively enhance the tool documentation. Thisprocess is further optimized by implementing a diversity-promoting explorationstrategy to ensure explorative diversity and a tool-adaptive terminationmechanism to prevent overfitting while enhancing efficiency. Extensiveexperiments on multiple datasets demonstrate that DRAFTs iterativefeedback-based refinement significantly ameliorates documentation qualityfostering a deeper comprehension and more effective utilization of tools byLLMs. Notably our analysis reveals that the tool documentation refined via ourapproach demonstrates robust cross-model generalization capabilities.</p>
                <p>Last Updated: 2024-10-10 17:58:44 UTC</p>
                <button class="interpret-button" data-id="2410.08197v1">Interpret</button>
                <div id="interpretation-2410.08197v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts</h3>
                <p>Authors: Anh-Quan CaoMaximilian JaritzMatthieu GuillauminRaoul de CharetteLoris Bazzani</p>
                <p><a href="http://arxiv.org/abs/2410.08211v1">Link to paper</a></p>
                <p>Large-scale vision-language pre-trained VLP models e.g. CLIP arerenowned for their versatility as they can be applied to diverse applicationsin a zero-shot setup. However when these models are used in specific domainstheir performance often falls short due to domain gaps or theunder-representation of these domains in the training data. While fine-tuningVLP models on custom datasets with human-annotated labels can address thisissue annotating even a small-scale dataset e.g. 100k samples can be anexpensive endeavor often requiring expert annotators if the task is complex.To address these challenges we propose LatteCLIP an unsupervised method forfine-tuning CLIP models on classification with known class names in customdomains without relying on human annotations. Our method leverages LargeMultimodal Models LMMs to generate expressive textual descriptions for bothindividual images and groups of images. These provide additional contextualinformation to guide the fine-tuning process in the custom domains. SinceLMM-generated descriptions are prone to hallucination or missing details weintroduce a novel strategy to distill only the useful information and stabilizethe training. Specifically we learn rich per-class prototype representationsfrom noisy generated texts and dual pseudo-labels. Our experiments on 10domain-specific datasets show that LatteCLIP outperforms pre-trained zero-shotmethods by an average improvement of 4.74 points in top-1 accuracy and otherstate-of-the-art unsupervised methods by 3.45 points.</p>
                <p>Last Updated: 2024-10-10 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2410.08211v1">Interpret</button>
                <div id="interpretation-2410.08211v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training</h3>
                <p>Authors: Gen LuoXue YangWenhan DouZhaokai WangJifeng DaiYu QiaoXizhou Zhu</p>
                <p><a href="http://arxiv.org/abs/2410.08202v1">Link to paper</a></p>
                <p>The rapid advancement of Large Language Models LLMs has led to an influx ofefforts to extend their capabilities to multimodal tasks. Among them growingattention has been focused on monolithic Multimodal Large Language ModelsMLLMs that integrate visual encoding and language decoding into a single LLM.Despite the structural simplicity and deployment-friendliness training amonolithic MLLM with promising performance still remains challenging. Inparticular the popular approaches adopt continuous pre-training to extend apre-trained LLM to a monolithic MLLM which suffers from catastrophicforgetting and leads to performance degeneration. In this paper we aim toovercome this limitation from the perspective of delta tuning. Specificallyour core idea is to embed visual parameters into a pre-trained LLM therebyincrementally learning visual knowledge from massive data via delta tuningi.e. freezing the LLM when optimizing the visual parameters. Based on thisprinciple we present Mono-InternVL a novel monolithic MLLM that seamlesslyintegrates a set of visual experts via a multimodal mixture-of-expertsstructure. Moreover we propose an innovative pre-training strategy to maximizethe visual capability of Mono-InternVL namely Endogenous Visual Pre-trainingEViP. In particular EViP is designed as a progressive learning process forvisual experts which aims to fully exploit the visual knowledge from noisydata to high-quality data. To validate our approach we conduct extensiveexperiments on 16 benchmarks. Experimental results not only validate thesuperior performance of Mono-InternVL compared to the state-of-the-art MLLM on6 multimodal benchmarks e.g. 113 points over InternVL-1.5 on OCRBench butalso confirm its better deployment efficiency with first token latency reducedby up to 67.</p>
                <p>Last Updated: 2024-10-10 17:59:22 UTC</p>
                <button class="interpret-button" data-id="2410.08202v1">Interpret</button>
                <div id="interpretation-2410.08202v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions</h3>
                <p>Authors: Changle QuSunhao DaiXiaochi WeiHengyi CaiShuaiqiang WangDawei YinJun XuJi-Rong Wen</p>
                <p><a href="http://arxiv.org/abs/2410.08197v1">Link to paper</a></p>
                <p>Tool learning enables Large Language Models LLMs to interact with externalenvironments by invoking tools serving as an effective strategy to mitigatethe limitations inherent in their pre-training data. In this process tooldocumentation plays a crucial role by providing usage instructions for LLMsthereby facilitating effective tool utilization. This paper concentrates on thecritical challenge of bridging the comprehension gap between LLMs and externaltools due to the inadequacies and inaccuracies inherent in existinghuman-centric tool documentation. We propose a novel framework DRAFT aimed atDynamically Refining tool documentation through the Analysis of Feedback andTrails emanating from LLMs interactions with external tools. This methodologypivots on an innovative trial-and-error approach consisting of three distinctlearning phases: experience gathering learning from experience anddocumentation rewriting to iteratively enhance the tool documentation. Thisprocess is further optimized by implementing a diversity-promoting explorationstrategy to ensure explorative diversity and a tool-adaptive terminationmechanism to prevent overfitting while enhancing efficiency. Extensiveexperiments on multiple datasets demonstrate that DRAFTs iterativefeedback-based refinement significantly ameliorates documentation qualityfostering a deeper comprehension and more effective utilization of tools byLLMs. Notably our analysis reveals that the tool documentation refined via ourapproach demonstrates robust cross-model generalization capabilities.</p>
                <p>Last Updated: 2024-10-10 17:58:44 UTC</p>
                <button class="interpret-button" data-id="2410.08197v1">Interpret</button>
                <div id="interpretation-2410.08197v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code</h3>
                <p>Authors: Zimu LuAojun ZhouKe WangHouxing RenWeikang ShiJunting PanMingjie ZhanHongsheng Li</p>
                <p><a href="http://arxiv.org/abs/2410.08196v1">Link to paper</a></p>
                <p>Code has been shown to be effective in enhancing the mathematical reasoningabilities of large language models due to its precision and accuracy. Previousworks involving continued mathematical pretraining often include code thatutilizes math-related packages which are primarily designed for fields such asengineering machine learning signal processing or module testing ratherthan being directly focused on mathematical reasoning. In this paper weintroduce a novel method for generating mathematical code accompanied withcorresponding reasoning steps for continued pretraining. Our approach beginswith the construction of a high-quality mathematical continued pretrainingdataset by incorporating math-related web data code using mathematicalpackages math textbooks and synthetic data. Next we construct reasoningsteps by extracting LaTeX expressions the conditions needed for theexpressions and the results of the expressions from the previously collecteddataset. Based on this extracted information we generate corresponding code toaccurately capture the mathematical reasoning process. Appending the generatedcode to each reasoning step results in data consisting of paired naturallanguage reasoning steps and their corresponding code. Combining this data withthe original dataset results in a 19.2B-token high-performing mathematicalpretraining corpus which we name MathCode-Pile. Training several popular basemodels with this corpus significantly improves their mathematical abilitiesleading to the creation of the MathCoder2 family of models. All of our dataprocessing and training code is open-sourced ensuring full transparency andeasy reproducibility of the entire data collection and training pipeline. Thecode is released at https://github.com/mathllm/MathCoder2 .</p>
                <p>Last Updated: 2024-10-10 17:58:40 UTC</p>
                <button class="interpret-button" data-id="2410.08196v1">Interpret</button>
                <div id="interpretation-2410.08196v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment</h3>
                <p>Authors: Yuancheng XuUdari Madhushani SehwagAlec KoppelSicheng ZhuBang AnFurong HuangSumitra Ganesh</p>
                <p><a href="http://arxiv.org/abs/2410.08193v1">Link to paper</a></p>
                <p>Large Language Models LLMs exhibit impressive capabilities but requirecareful alignment with human preferences. Traditional training-time methodsfinetune LLMs using human preference datasets but incur significant trainingcosts and require repeated training to handle diverse user preferences.Test-time alignment methods address this by using reward models RMs to guidefrozen LLMs without retraining. However existing test-time approaches rely ontrajectory-level RMs which are designed to evaluate complete responses makingthem unsuitable for autoregressive text generation that requires computingnext-token rewards from partial responses. To address this we introduceGenARM a test-time alignment approach that leverages the Autoregressive RewardModel--a novel reward parametrization designed to predict next-token rewardsfor efficient and effective autoregressive generation. Theoretically wedemonstrate that this parametrization can provably guide frozen LLMs toward anydistribution achievable by traditional RMs within the KL-regularizedreinforcement learning framework. Experimental results show that GenARMsignificantly outperforms prior test-time alignment baselines and matches theperformance of training-time methods. Additionally GenARM enables efficientweak-to-strong guidance aligning larger LLMs with smaller RMs without the highcosts of training larger models. Furthermore GenARM supports multi-objectivealignment allowing real-time trade-offs between preference dimensions andcatering to diverse user preferences without retraining.</p>
                <p>Last Updated: 2024-10-10 17:58:24 UTC</p>
                <button class="interpret-button" data-id="2410.08193v1">Interpret</button>
                <div id="interpretation-2410.08193v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision</h3>
                <p>Authors: Shengcao CaoLiang-Yan GuiYu-Xiong Wang</p>
                <p><a href="http://arxiv.org/abs/2410.08209v1">Link to paper</a></p>
                <p>Current large multimodal models LMMs face challenges in grounding whichrequires the model to relate language components to visual entities. Contraryto the common practice that fine-tunes LMMs with additional groundingsupervision we find that the grounding ability can in fact emerge in LMMstrained without explicit grounding supervision. To reveal this emerginggrounding we introduce an attend-and-segment method which leveragesattention maps from standard LMMs to perform pixel-level segmentation.Furthermore to enhance the grounding ability we propose DIFFLMM an LMMutilizing a diffusion-based visual encoder as opposed to the standard CLIPvisual encoder and trained with the same weak supervision. Without beingconstrained by the biases and limited scale of grounding-specific supervisiondata our approach is more generalizable and scalable. We achieve competitiveperformance on both grounding-specific and general visual question answeringbenchmarks compared with grounding LMMs and generalist LMMs respectively.Notably we achieve a 44.2 grounding mask recall on grounded conversationgeneration without any grounding supervision outperforming the extensivelysupervised model GLaMM. Project page: https://groundLMM.github.io.</p>
                <p>Last Updated: 2024-10-10 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2410.08209v1">Interpret</button>
                <div id="interpretation-2410.08209v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SPA: 3D Spatial-Awareness Enables Effective Embodied Representation</h3>
                <p>Authors: Haoyi ZhuHonghui YangYating WangJiange YangLimin WangTong He</p>
                <p><a href="http://arxiv.org/abs/2410.08208v1">Link to paper</a></p>
                <p>In this paper we introduce SPA a novel representation learning frameworkthat emphasizes the importance of 3D spatial awareness in embodied AI. Ourapproach leverages differentiable neural rendering on multi-view images toendow a vanilla Vision Transformer ViT with intrinsic spatial understanding.We present the most comprehensive evaluation of embodied representationlearning to date covering 268 tasks across 8 simulators with diverse policiesin both single-task and language-conditioned multi-task scenarios. The resultsare compelling: SPA consistently outperforms more than 10 state-of-the-artrepresentation methods including those specifically designed for embodied AIvision-centric tasks and multi-modal applications while using less trainingdata. Furthermore we conduct a series of real-world experiments to confirm itseffectiveness in practical scenarios. These results highlight the critical roleof 3D spatial awareness for embodied representation learning. Our strongestmodel takes more than 6000 GPU hours to train and we are committed toopen-sourcing all code and model weights to foster future research in embodiedrepresentation learning. Project Page: https://haoyizhu.github.io/spa/.</p>
                <p>Last Updated: 2024-10-10 17:59:51 UTC</p>
                <button class="interpret-button" data-id="2410.08208v1">Interpret</button>
                <div id="interpretation-2410.08208v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models</h3>
                <p>Authors: Xiaoxiao HeLigong HanQuan DaoSong WenMinhao BaiDi LiuHan ZhangMartin Renqiang MinFelix Juefei-XuChaowei TanBo LiuKang LiHongdong LiJunzhou HuangFaez AhmedAkash SrivastavaDimitris Metaxas</p>
                <p><a href="http://arxiv.org/abs/2410.08207v1">Link to paper</a></p>
                <p>Discrete diffusion models have achieved success in tasks like imagegeneration and masked language modeling but face limitations in controlledcontent editing. We introduce DICE Discrete Inversion for ControllableEditing the first approach to enable precise inversion for discrete diffusionmodels including multinomial diffusion and masked generative models. Byrecording noise sequences and masking patterns during the reverse diffusionprocess DICE enables accurate reconstruction and flexible editing of discretedata without the need for predefined masks or attention manipulation. Wedemonstrate the effectiveness of DICE across both image and text domainsevaluating it on models such as VQ-Diffusion Paella and RoBERTa. Our resultsshow that DICE preserves high data fidelity while enhancing editingcapabilities offering new opportunities for fine-grained content manipulationin discrete spaces. For project webpage seehttps://hexiaoxiao-cs.github.io/DICE/.</p>
                <p>Last Updated: 2024-10-10 17:59:48 UTC</p>
                <button class="interpret-button" data-id="2410.08207v1">Interpret</button>
                <div id="interpretation-2410.08207v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Efficient Dictionary Learning with Switch Sparse Autoencoders</h3>
                <p>Authors: Anish MudideJoshua EngelsEric J. MichaudMax TegmarkChristian Schroeder de Witt</p>
                <p><a href="http://arxiv.org/abs/2410.08201v1">Link to paper</a></p>
                <p>Sparse autoencoders SAEs are a recent technique for decomposing neuralnetwork activations into human-interpretable features. However in order forSAEs to identify all features represented in frontier models it will benecessary to scale them up to very high width posing a computationalchallenge. In this work we introduce Switch Sparse Autoencoders a novel SAEarchitecture aimed at reducing the compute cost of training SAEs. Inspired bysparse mixture of experts models Switch SAEs route activation vectors betweensmaller expert SAEs enabling SAEs to efficiently scale to many morefeatures. We present experiments comparing Switch SAEs with other SAEarchitectures and find that Switch SAEs deliver a substantial Paretoimprovement in the reconstruction vs. sparsity frontier for a given fixedtraining compute budget. We also study the geometry of features across expertsanalyze features duplicated across experts and verify that Switch SAE featuresare as interpretable as features found by other SAE architectures.</p>
                <p>Last Updated: 2024-10-10 17:59:11 UTC</p>
                <button class="interpret-button" data-id="2410.08201v1">Interpret</button>
                <div id="interpretation-2410.08201v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity</h3>
                <p>Authors: Shuo XieMohamad Amin MohamadiZhiyuan Li</p>
                <p><a href="http://arxiv.org/abs/2410.08198v1">Link to paper</a></p>
                <p>Adam outperforms SGD when training language models. Yet this advantage is notwell-understood theoretically -- previous convergence analysis for Adam and SGDmainly focuses on the number of steps T and is already minimax-optimal innon-convex cases which are both widetildeOT-1/4. In this work weargue that the exploitation of nice ell_infty-geometry is the key advantageof Adam over SGD. More specifically we give a new convergence analysis forAdam under novel assumptions that loss is smooth under ell_infty-geometryrather than the more common ell_2-geometry which yields a much betterempirical smoothness constant for GPT-2 and ResNet models. Our experimentsconfirm that Adam performs much worse when the favorable ell_infty-geometryis changed while SGD provably remains unaffected. We also extend theconvergence analysis to blockwise Adam under novel blockwise smoothnessassumptions.</p>
                <p>Last Updated: 2024-10-10 17:58:53 UTC</p>
                <button class="interpret-button" data-id="2410.08198v1">Interpret</button>
                <div id="interpretation-2410.08198v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-10-13</p>
        </div>
    
        </div>
    </body>
    </html>
    