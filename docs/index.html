
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Pre-training and in-context learning IS Bayesian inference a la De Finetti</h3>
                <p>Authors: Naimeng YeHanming YangAndrew SiahHongseok Namkoong</p>
                <p><a href="http://arxiv.org/abs/2408.03307v1">Link to paper</a></p>
                <p>Accurately gauging uncertainty on the underlying environment is alongstanding goal of intelligent systems. We characterize which latent conceptspre-trained sequence models are naturally able to reason with. We go back to DeFinettis predictive view of Bayesian reasoning: instead of modeling latentparameters through priors and likelihoods like topic models do De Finetti haslong advocated for modeling exchangeable permutation invariant sequences ofobservables. According to this view pre-training autoregressive modelsformulates informed beliefs based on prior observations empirical Bayesand forward generation is a simulated instantiation of an environmentposterior inference. This connection allows extending in-context learningICL beyond predictive settings highlighting sequence models ability toperform explicit statistical inference. In particular we show the sequenceprediction loss over exchangeable documents controls performance on downstreamtasks where uncertainty quantification is key. Empirically we propose anddemonstrate several approaches for encoding exchangeability in sequence modelarchitectures: data augmentation regularization and causal masking.</p>
                <p>Last Updated: 2024-08-06 17:16:10 UTC</p>
                <button class="interpret-button" data-id="2408.03307v1">Interpret</button>
                <div id="interpretation-2408.03307v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Active Learning for Level Set Estimation Using Randomized Straddle Algorithms</h3>
                <p>Authors: Yu InatsuShion TakenoKentaro KutsukakeIchiro Takeuchi</p>
                <p><a href="http://arxiv.org/abs/2408.03144v1">Link to paper</a></p>
                <p>Level set estimation LSE the problem of identifying the set of inputpoints where a function takes value above or below a given threshold isimportant in practical applications. When the function is expensive-to-evaluateand black-box the textitstraddle algorithm which is a representativeheuristic for LSE based on Gaussian process models and its extensions havingtheoretical guarantees have been developed. However many of existing methodsinclude a confidence parameter beta1/2_t that must be specified by theuser and methods that choose beta1/2_t heuristically do not providetheoretical guarantees. In contrast theoretically guaranteed values ofbeta1/2_t need to be increased depending on the number of iterations andcandidate points and are conservative and not good for practical performance.In this study we propose a novel method the textitrandomized straddlealgorithm in which beta_t in the straddle algorithm is replaced by a randomsample from the chi-squared distribution with two degrees of freedom. Theconfidence parameter in the proposed method has the advantages of not needingadjustment not depending on the number of iterations and candidate points andnot being conservative. Furthermore we show that the proposed method hastheoretical guarantees that depend on the sample complexity and the number ofiterations. Finally we confirm the usefulness of the proposed method throughnumerical experiments using synthetic and real data.</p>
                <p>Last Updated: 2024-08-06 12:39:12 UTC</p>
                <button class="interpret-button" data-id="2408.03144v1">Interpret</button>
                <div id="interpretation-2408.03144v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Predictive Performance Test based on the Exhaustive Nested Cross-Validation for High-dimensional data</h3>
                <p>Authors: Iris Ivy GauranHernando OmbaoZhaoxia Yu</p>
                <p><a href="http://arxiv.org/abs/2408.03138v1">Link to paper</a></p>
                <p>It is crucial to assess the predictive performance of a model in order toestablish its practicality and relevance in real-world scenarios particularlyfor high-dimensional data analysis. Among data splitting or resampling methodscross-validation CV is extensively used for several tasks such as estimatingthe prediction error tuning the regularization parameter and selecting themost suitable predictive model among competing alternatives. The K-foldcross-validation is a popular CV method but its limitation is that the riskestimates are highly dependent on the partitioning of the data for trainingand testing. Here the issues regarding the reproducibility of the K-fold CVestimator is demonstrated in hypothesis testing wherein different partitionslead to notably disparate conclusions. This study presents an alternative novelpredictive performance test and valid confidence intervals based on exhaustivenested cross-validation for determining the difference in prediction errorbetween two model-fitting algorithms. A naive implementation of the exhaustivenested cross-validation is computationally costly. Here we address concernsregarding computational complexity by devising a computationally tractableclosed-form expression for the proposed cross-validation estimator using ridgeregularization. Our study also investigates strategies aimed at enhancingstatistical power within high-dimensional scenarios while controlling the TypeI error rate. To illustrate the practical utility of our method we apply it toan RNA sequencing study and demonstrate its effectiveness in the context ofbiological data analysis.</p>
                <p>Last Updated: 2024-08-06 12:28:16 UTC</p>
                <button class="interpret-button" data-id="2408.03138v1">Interpret</button>
                <div id="interpretation-2408.03138v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Evaluating Posterior Probabilities: Decision Theory, Proper Scoring Rules, and Calibration</h3>
                <p>Authors: Luciana FerrerDaniel Ramos</p>
                <p><a href="http://arxiv.org/abs/2408.02841v1">Link to paper</a></p>
                <p>Most machine learning classifiers are designed to output posteriorprobabilities for the classes given the input sample. These probabilities maybe used to make the categorical decision on the class of the sample providedas input to a downstream system or provided to a human for interpretation.Evaluating the quality of the posteriors generated by these system is anessential problem which was addressed decades ago with the invention of properscoring rules PSRs. Unfortunately much of the recent machine learningliterature uses calibration metrics -- most commonly the expected calibrationerror ECE -- as a proxy to assess posterior performance. The problem withthis approach is that calibration metrics reflect only one aspect of thequality of the posteriors ignoring the discrimination performance. For thisreason we argue that calibration metrics should play no role in the assessmentof posterior quality. Expected PSRs should instead be used for this jobpreferably normalized for ease of interpretation. In this work we first give abrief review of PSRs from a practical perspective motivating their definitionusing Bayes decision theory. We discuss why expected PSRs provide a principledmeasure of the quality of a systems posteriors and why calibration metrics arenot the right tool for this job. We argue that calibration metrics while notuseful for performance assessment may be used as diagnostic tools duringsystem development. With this purpose in mind we discuss a simple andpractical calibration metric called calibration loss derived from adecomposition of expected PSRs. We compare this metric with the ECE and withthe expected score divergence calibration metric from the PSR literature andargue using theoretical and empirical evidence that calibration loss issuperior to these two metrics.</p>
                <p>Last Updated: 2024-08-05 21:35:51 UTC</p>
                <button class="interpret-button" data-id="2408.02841v1">Interpret</button>
                <div id="interpretation-2408.02841v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Optimizing Cox Models with Stochastic Gradient Descent: Theoretical Foundations and Practical Guidances</h3>
                <p>Authors: Lang ZengWeijing TangZhao RenYing Ding</p>
                <p><a href="http://arxiv.org/abs/2408.02839v1">Link to paper</a></p>
                <p>Optimizing Cox regression and its neural network variants poses substantialcomputational challenges in large-scale studies. Stochastic gradient descentSGD known for its scalability in model optimization has recently beenadapted to optimize Cox models. Unlike its conventional application whichtypically targets a sum of independent individual loss SGD for Cox modelsupdates parameters based on the partial likelihood of a subset of data. Despiteits empirical success the theoretical foundation for optimizing Cox partiallikelihood with SGD is largely underexplored. In this work we demonstrate thatthe SGD estimator targets an objective function that is batch-size-dependent.We establish that the SGD estimator for the Cox neural network Cox-NN isconsistent and achieves the optimal minimax convergence rate up to apolylogarithmic factor. For Cox regression we further prove thesqrtn-consistency and asymptotic normality of the SGD estimator withvariance depending on the batch size. Furthermore we quantify the impact ofbatch size on Cox-NN training and its effect on the SGD estimators asymptoticefficiency in Cox regression. These findings are validated by extensivenumerical experiments and provide guidance for selecting batch sizes in SGDapplications. Finally we demonstrate the effectiveness of SGD in a real-worldapplication where GD is unfeasible due to the large scale of data.</p>
                <p>Last Updated: 2024-08-05 21:25:10 UTC</p>
                <button class="interpret-button" data-id="2408.02839v1">Interpret</button>
                <div id="interpretation-2408.02839v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>LLaVA-OneVision: Easy Visual Task Transfer</h3>
                <p>Authors: Bo LiYuanhan ZhangDong GuoRenrui ZhangFeng LiHao ZhangKaichen ZhangYanwei LiZiwei LiuChunyuan Li</p>
                <p><a href="http://arxiv.org/abs/2408.03326v1">Link to paper</a></p>
                <p>We present LLaVA-OneVision a family of open large multimodal models LMMsdeveloped by consolidating our insights into data models and visualrepresentations in the LLaVA-NeXT blog series. Our experimental resultsdemonstrate that LLaVA-OneVision is the first single model that cansimultaneously push the performance boundaries of open LMMs in three importantcomputer vision scenarios: single-image multi-image and video scenarios.Importantly the design of LLaVA-OneVision allows strong transfer learningacross different modalities/scenarios yielding new emerging capabilities. Inparticular strong video understanding and cross-scenario capabilities aredemonstrated through task transfer from images to videos.</p>
                <p>Last Updated: 2024-08-06 17:59:44 UTC</p>
                <button class="interpret-button" data-id="2408.03326v1">Interpret</button>
                <div id="interpretation-2408.03326v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CoverBench: A Challenging Benchmark for Complex Claim Verification</h3>
                <p>Authors: Alon JacoviMoran AmbarEyal Ben-DavidUri ShahamAmir FederMor GevaDror MarcusAvi Caciularu</p>
                <p><a href="http://arxiv.org/abs/2408.03325v1">Link to paper</a></p>
                <p>There is a growing line of research on verifying the correctness of languagemodels outputs. At the same time LMs are being used to tackle complex queriesthat require reasoning. We introduce CoverBench a challenging benchmarkfocused on verifying LM outputs in complex reasoning settings. Datasets thatcan be used for this purpose are often designed for other complex reasoningtasks e.g. QA targeting specific use-cases e.g. financial tablesrequiring transformations negative sampling and selection of hard examples tocollect such a benchmark. CoverBench provides a diversified evaluation forcomplex claim verification in a variety of domains types of reasoningrelatively long inputs and a variety of standardizations such as multiplerepresentations for tables where available and a consistent schema. Wemanually vet the data for quality to ensure low levels of label noise. Finallywe report a variety of competitive baseline results to show CoverBench ischallenging and has very significant headroom. The data is available athttps://huggingface.co/datasets/google/coverbench .</p>
                <p>Last Updated: 2024-08-06 17:58:53 UTC</p>
                <button class="interpret-button" data-id="2408.03325v1">Interpret</button>
                <div id="interpretation-2408.03325v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Training LLMs to Recognize Hedges in Spontaneous Narratives</h3>
                <p>Authors: Amie J. PaigeAdil SoubkiJohn MurzakuOwen RambowSusan E. Brennan</p>
                <p><a href="http://arxiv.org/abs/2408.03319v1">Link to paper</a></p>
                <p>Hedges allow speakers to mark utterances as provisional whether to signalnon-prototypicality or fuzziness to indicate a lack of commitment to anutterance to attribute responsibility for a statement to someone else toinvite input from a partner or to soften critical feedback in the service offace-management needs. Here we focus on hedges in an experimentallyparameterized corpus of 63 Roadrunner cartoon narratives spontaneously producedfrom memory by 21 speakers for co-present addressees transcribed to textGalati and Brennan 2010. We created a gold standard of hedges annotated byhuman coders the Roadrunner-Hedge corpus and compared three LLM-basedapproaches for hedge detection: fine-tuning BERT and zero and few-shotprompting with GPT-4o and LLaMA-3. The best-performing approach was afine-tuned BERT model followed by few-shot GPT-4o. After an error analysis onthe top performing approaches we used an LLM-in-the-Loop approach to improvethe gold standard coding as well as to highlight cases in which hedges areambiguous in linguistically interesting ways that will guide future research.This is the first step in our research program to train LLMs to interpret andgenerate collateral signals appropriately and meaningfully in conversation.</p>
                <p>Last Updated: 2024-08-06 17:51:42 UTC</p>
                <button class="interpret-button" data-id="2408.03319v1">Interpret</button>
                <div id="interpretation-2408.03319v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</h3>
                <p>Authors: Charlie SnellJaehoon LeeKelvin XuAviral Kumar</p>
                <p><a href="http://arxiv.org/abs/2408.03314v1">Link to paper</a></p>
                <p>Enabling LLMs to improve their outputs by using more test-time computation isa critical step towards building generally self-improving agents that canoperate on open-ended natural language. In this paper we study the scaling ofinference-time computation in LLMs with a focus on answering the question: ifan LLM is allowed to use a fixed but non-trivial amount of inference-timecompute how much can it improve its performance on a challenging promptAnswering this question has implications not only on the achievable performanceof LLMs but also on the future of LLM pretraining and how one should tradeoffinference-time and pre-training compute. Despite its importance littleresearch attempted to understand the scaling behaviors of various test-timeinference methods. Moreover current work largely provides negative results fora number of these strategies. In this work we analyze two primary mechanismsto scale test-time computation: 1 searching against dense process-basedverifier reward models and 2 updating the models distribution over aresponse adaptively given the prompt at test time. We find that in both casesthe effectiveness of different approaches to scaling test-time computecritically varies depending on the difficulty of the prompt. This observationmotivates applying a compute-optimal scaling strategy which acts to mosteffectively allocate test-time compute adaptively per prompt. Using thiscompute-optimal strategy we can improve the efficiency of test-time computescaling by more than 4x compared to a best-of-N baseline. Additionally in aFLOPs-matched evaluation we find that on problems where a smaller base modelattains somewhat non-trivial success rates test-time compute can be used tooutperform a 14x larger model.</p>
                <p>Last Updated: 2024-08-06 17:35:05 UTC</p>
                <button class="interpret-button" data-id="2408.03314v1">Interpret</button>
                <div id="interpretation-2408.03314v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models</h3>
                <p>Authors: Ruizhe ZhangYongxin XuYuzhen XiaoRunchuan ZhuXinke JiangXu ChuJunfeng ZhaoYasha Wang</p>
                <p><a href="http://arxiv.org/abs/2408.03297v1">Link to paper</a></p>
                <p>By integrating external knowledge Retrieval-Augmented Generation RAG hasbecome an effective strategy for mitigating the hallucination problems thatlarge language models LLMs encounter when dealing with knowledge-intensivetasks. However in the process of integrating external non-parametricsupporting evidence with internal parametric knowledge inevitable knowledgeconflicts may arise leading to confusion in the models responses. To enhancethe knowledge selection of LLMs in various contexts some research has focusedon refining their behavior patterns through instruction-tuning. Nonethelessdue to the absence of explicit negative signals and comparative objectivesmodels fine-tuned in this manner may still exhibit undesirable behaviors in theintricate and realistic retrieval scenarios. To this end we propose aKnowledge-aware Preference Optimization dubbed KaPO aimed at achievingcontrollable knowledge selection in real retrieval scenarios. Concretely weexplore and simulate error types across diverse context combinations and learnhow to avoid these negative signals through preference optimization methods.Simultaneously by adjusting the balance between response length and theproportion of preference data representing different behavior patterns weenhance the adherence capabilities and noise robustness of LLMs in a balancedmanner. Experimental results show that KaPO outperforms previous methods forhandling knowledge conflicts by over 37 while also exhibiting robustgeneralization across various out-of-distribution datasets.</p>
                <p>Last Updated: 2024-08-06 16:55:54 UTC</p>
                <button class="interpret-button" data-id="2408.03297v1">Interpret</button>
                <div id="interpretation-2408.03297v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Heterogeneous graph attention network improves cancer multiomics integration</h3>
                <p>Authors: Sina TabakhiCharlotte VandermeulenIan SudberyHaiping Lu</p>
                <p><a href="http://arxiv.org/abs/2408.02845v1">Link to paper</a></p>
                <p>The increase in high-dimensional multiomics data demands advanced integrationmodels to capture the complexity of human diseases. Graph-based deep learningintegration models despite their promise struggle with small patient cohortsand high-dimensional features often applying independent feature selectionwithout modeling relationships among omics. Furthermore conventionalgraph-based omics models focus on homogeneous graphs lacking multiple types ofnodes and edges to capture diverse structures. We introduce a HeterogeneousGraph ATtention network for omics integration HeteroGATomics to improvecancer diagnosis. HeteroGATomics performs joint feature selection through amulti-agent system creating dedicated networks of feature and patientsimilarity for each omic modality. These networks are then combined into oneheterogeneous graph for learning holistic omic-specific representations andintegrating predictions across modalities. Experiments on three cancermultiomics datasets demonstrate HeteroGATomics superior performance in cancerdiagnosis. Moreover HeteroGATomics enhances interpretability by identifyingimportant biomarkers contributing to the diagnosis outcomes.</p>
                <p>Last Updated: 2024-08-05 22:01:13 UTC</p>
                <button class="interpret-button" data-id="2408.02845v1">Interpret</button>
                <div id="interpretation-2408.02845v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Assessing the Effects of Container Handling Strategies on Enhancing Freight Throughput</h3>
                <p>Authors: Sarita RattanakunuprakarnMingzhou JinMustafa Can CamurXueping Li</p>
                <p><a href="http://arxiv.org/abs/2408.02768v1">Link to paper</a></p>
                <p>As global supply chains and freight volumes grow the U.S. faces escalatingtransportation demands. The heavy reliance on road transport coupled with theunderutilization of the railway system results in congested highwaysprolonged transportation times higher costs and increased carbon emissions.Californias San Pedro Port Complex SPPC the nations busiest incurs asignificant share of these challenges. We utilize an agent-based simulation toreplicate real-world scenarios focusing on the intricacies of interactions ina modified intermodal inbound freight system for the SPPC. This involvesrelocating container classification to potential warehouses in CaliforniaUtah Arizona and Nevada rather than exclusively at port areas. Our primaryaim is to evaluate the proposed systems efficiency considering cost andfreight throughput while also examining the effects of workforce shortages.Computational analysis suggests that strategically installing intermodalcapabilities in select warehouses can reduce transportation costs boostthroughput and foster resour</p>
                <p>Last Updated: 2024-08-05 18:38:27 UTC</p>
                <button class="interpret-button" data-id="2408.02768v1">Interpret</button>
                <div id="interpretation-2408.02768v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems</h3>
                <p>Authors: Andrew ZhuLiam DuganChris Callison-Burch</p>
                <p><a href="http://arxiv.org/abs/2408.02248v1">Link to paper</a></p>
                <p>Recently there has been increasing interest in using Large Language ModelsLLMs to construct complex multi-agent systems to perform tasks such ascompiling literature reviews drafting consumer reports and planningvacations. Many tools and libraries exist for helping create such systemshowever none support recursive multi-agent systems -- where the modelsthemselves flexibly decide when to delegate tasks and how to organize theirdelegation structure. In this work we introduce ReDel: a toolkit for recursivemulti-agent systems that supports custom tool-use delegation schemesevent-based logging and interactive replay in an easy-to-use web interface. Weshow that using ReDel we are able to achieve significant performance gains onagentic benchmarks and easily identify potential areas of improvements throughthe visualization and debugging tools. Our code documentation and PyPIpackage are open-source and free to use under the MIT license.</p>
                <p>Last Updated: 2024-08-05 05:43:23 UTC</p>
                <button class="interpret-button" data-id="2408.02248v1">Interpret</button>
                <div id="interpretation-2408.02248v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Environment Complexity and Nash Equilibria in a Sequential Social Dilemma</h3>
                <p>Authors: Mustafa YasirAndrew HowesVasilios MavroudisChris Hicks</p>
                <p><a href="http://arxiv.org/abs/2408.02148v1">Link to paper</a></p>
                <p>Multi-agent reinforcement learning MARL methods while effective inzero-sum or positive-sum games often yield suboptimal outcomes in general-sumgames where cooperation is essential for achieving globally optimal outcomes.Matrix game social dilemmas which abstract key aspects of general-suminteractions such as cooperation risk and trust fail to model the temporaland spatial dynamics characteristic of real-world scenarios. In response ourstudy extends matrix game social dilemmas into more complex higher-dimensionalMARL environments. We adapt a gridworld implementation of the Stag Hunt dilemmato more closely match the decision-space of a one-shot matrix game while alsointroducing variable environment complexity. Our findings indicate that ascomplexity increases MARL agents trained in these environments converge tosuboptimal strategies consistent with the risk-dominant Nash equilibriastrategies found in matrix games. Our work highlights the impact of environmentcomplexity on achieving optimal outcomes in higher-dimensional game-theoreticMARL environments.</p>
                <p>Last Updated: 2024-08-04 21:27:36 UTC</p>
                <button class="interpret-button" data-id="2408.02148v1">Interpret</button>
                <div id="interpretation-2408.02148v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Value-Based Rationales Improve Social Experience: A Multiagent Simulation Study</h3>
                <p>Authors: Sz-Ting TzengNirav AjmeriMunindar P. Singh</p>
                <p><a href="http://arxiv.org/abs/2408.02117v1">Link to paper</a></p>
                <p>We propose Exanna a framework to realize agents that incorporate values indecision making. An Exannaagent considers the values of itself and others whenproviding rationales for its actions and evaluating the rationales provided byothers. Via multiagent simulation we demonstrate that considering values indecision making and producing rationales especially for norm-deviatingactions leads to 1 higher conflict resolution 2 better social experience3 higher privacy and 4 higher flexibility.</p>
                <p>Last Updated: 2024-08-04 19:14:36 UTC</p>
                <button class="interpret-button" data-id="2408.02117v1">Interpret</button>
                <div id="interpretation-2408.02117v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>LLaVA-OneVision: Easy Visual Task Transfer</h3>
                <p>Authors: Bo LiYuanhan ZhangDong GuoRenrui ZhangFeng LiHao ZhangKaichen ZhangYanwei LiZiwei LiuChunyuan Li</p>
                <p><a href="http://arxiv.org/abs/2408.03326v1">Link to paper</a></p>
                <p>We present LLaVA-OneVision a family of open large multimodal models LMMsdeveloped by consolidating our insights into data models and visualrepresentations in the LLaVA-NeXT blog series. Our experimental resultsdemonstrate that LLaVA-OneVision is the first single model that cansimultaneously push the performance boundaries of open LMMs in three importantcomputer vision scenarios: single-image multi-image and video scenarios.Importantly the design of LLaVA-OneVision allows strong transfer learningacross different modalities/scenarios yielding new emerging capabilities. Inparticular strong video understanding and cross-scenario capabilities aredemonstrated through task transfer from images to videos.</p>
                <p>Last Updated: 2024-08-06 17:59:44 UTC</p>
                <button class="interpret-button" data-id="2408.03326v1">Interpret</button>
                <div id="interpretation-2408.03326v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Training LLMs to Recognize Hedges in Spontaneous Narratives</h3>
                <p>Authors: Amie J. PaigeAdil SoubkiJohn MurzakuOwen RambowSusan E. Brennan</p>
                <p><a href="http://arxiv.org/abs/2408.03319v1">Link to paper</a></p>
                <p>Hedges allow speakers to mark utterances as provisional whether to signalnon-prototypicality or fuzziness to indicate a lack of commitment to anutterance to attribute responsibility for a statement to someone else toinvite input from a partner or to soften critical feedback in the service offace-management needs. Here we focus on hedges in an experimentallyparameterized corpus of 63 Roadrunner cartoon narratives spontaneously producedfrom memory by 21 speakers for co-present addressees transcribed to textGalati and Brennan 2010. We created a gold standard of hedges annotated byhuman coders the Roadrunner-Hedge corpus and compared three LLM-basedapproaches for hedge detection: fine-tuning BERT and zero and few-shotprompting with GPT-4o and LLaMA-3. The best-performing approach was afine-tuned BERT model followed by few-shot GPT-4o. After an error analysis onthe top performing approaches we used an LLM-in-the-Loop approach to improvethe gold standard coding as well as to highlight cases in which hedges areambiguous in linguistically interesting ways that will guide future research.This is the first step in our research program to train LLMs to interpret andgenerate collateral signals appropriately and meaningfully in conversation.</p>
                <p>Last Updated: 2024-08-06 17:51:42 UTC</p>
                <button class="interpret-button" data-id="2408.03319v1">Interpret</button>
                <div id="interpretation-2408.03319v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks</h3>
                <p>Authors: Rafael SterzingerChristian StippelRobert Sablatnig</p>
                <p><a href="http://arxiv.org/abs/2408.03304v1">Link to paper</a></p>
                <p>Etruscan mirrors constitute a significant category in Etruscan artcharacterized by elaborate figurative illustrations featured on their backside.A laborious and costly aspect of their analysis and documentation is the taskof manually tracing these illustrations. In previous work a methodology hasbeen proposed to automate this process involving photometric-stereo scanningin combination with deep neural networks. While achieving quantitativeperformance akin to an expert annotator some results still lack qualitativeprecision and thus require annotators for inspection and potentialcorrection maintaining resource intensity. In response we propose a deepneural network trained to interactively refine existing annotations based onhuman guidance. Our human-in-the-loop approach streamlines annotationachieving equal quality with up to 75 less manual input required. Moreoverduring the refinement process the relative improvement of our methodology overpure manual labeling reaches peak values of up to 26 attaining drasticallybetter quality quicker. By being tailored to the complex task of segmentingintricate lines specifically distinguishing it from previous methods ourapproach offers drastic improvements in efficacy transferable to a broadspectrum of applications beyond Etruscan mirrors.</p>
                <p>Last Updated: 2024-08-06 17:11:40 UTC</p>
                <button class="interpret-button" data-id="2408.03304v1">Interpret</button>
                <div id="interpretation-2408.03304v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges</h3>
                <p>Authors: Jonggi HongHernisa Kacorri</p>
                <p><a href="http://dx.doi.org/10.1145/3663548.3675635">Link to paper</a></p>
                <p>Object recognition technologies hold the potential to support blind andlow-vision people in navigating the world around them. However the gap betweenbenchmark performances and practical usability remains a significant challenge.This paper presents a study aimed at understanding blind users interactionwith object recognition systems for identifying and avoiding errors. Leveraginga pre-existing object recognition system URCam fine-tuned for our experimentwe conducted a user study involving 12 blind and low-vision participants.Through in-depth interviews and hands-on error identification tasks we gainedinsights into users experiences challenges and strategies for identifyingerrors in camera-based assistive technologies and object recognition systems.During interviews many participants preferred independent error review whileexpressing apprehension toward misrecognitions. In the error identificationtask participants varied viewpoints backgrounds and object sizes in theirimages to avoid and overcome errors. Even after repeating the taskparticipants identified only half of the errors and the proportion of errorsidentified did not significantly differ from their first attempts. Based onthese insights we offer implications for designing accessible interfacestailored to the needs of blind and low-vision users in identifying objectrecognition errors.</p>
                <p>Last Updated: 2024-08-06 17:09:56 UTC</p>
                <button class="interpret-button" data-id="2408.03303v1">Interpret</button>
                <div id="interpretation-2408.03303v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models</h3>
                <p>Authors: Ruizhe ZhangYongxin XuYuzhen XiaoRunchuan ZhuXinke JiangXu ChuJunfeng ZhaoYasha Wang</p>
                <p><a href="http://arxiv.org/abs/2408.03297v1">Link to paper</a></p>
                <p>By integrating external knowledge Retrieval-Augmented Generation RAG hasbecome an effective strategy for mitigating the hallucination problems thatlarge language models LLMs encounter when dealing with knowledge-intensivetasks. However in the process of integrating external non-parametricsupporting evidence with internal parametric knowledge inevitable knowledgeconflicts may arise leading to confusion in the models responses. To enhancethe knowledge selection of LLMs in various contexts some research has focusedon refining their behavior patterns through instruction-tuning. Nonethelessdue to the absence of explicit negative signals and comparative objectivesmodels fine-tuned in this manner may still exhibit undesirable behaviors in theintricate and realistic retrieval scenarios. To this end we propose aKnowledge-aware Preference Optimization dubbed KaPO aimed at achievingcontrollable knowledge selection in real retrieval scenarios. Concretely weexplore and simulate error types across diverse context combinations and learnhow to avoid these negative signals through preference optimization methods.Simultaneously by adjusting the balance between response length and theproportion of preference data representing different behavior patterns weenhance the adherence capabilities and noise robustness of LLMs in a balancedmanner. Experimental results show that KaPO outperforms previous methods forhandling knowledge conflicts by over 37 while also exhibiting robustgeneralization across various out-of-distribution datasets.</p>
                <p>Last Updated: 2024-08-06 16:55:54 UTC</p>
                <button class="interpret-button" data-id="2408.03297v1">Interpret</button>
                <div id="interpretation-2408.03297v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>ClassiFIM: An Unsupervised Method To Detect Phase Transitions</h3>
                <p>Authors: Victor KasatkinEvgeny MozgunovNicholas EzzellUtkarsh MishraItay HenDaniel Lidar</p>
                <p><a href="http://arxiv.org/abs/2408.03323v1">Link to paper</a></p>
                <p>Estimation of the Fisher Information Metric FIM-estimation is an importanttask that arises in unsupervised learning of phase transitions a problemproposed by physicists. This work completes the definition of the task bydefining rigorous evaluation metrics distMSE distMSEPS and distRE andintroduces ClassiFIM a novel machine learning method designed to solve theFIM-estimation task. Unlike existing methods for unsupervised learning of phasetransitions ClassiFIM directly estimates a well-defined quantity the FIMallowing it to be rigorously compared to any present and future other methodsthat estimate the same. ClassiFIM transforms a dataset for the FIM-estimationtask into a dataset for an auxiliary binary classification task and involvesselecting and training a model for the latter. We prove that the output ofClassiFIM approaches the exact FIM in the limit of infinite dataset size andunder certain regularity conditions. We implement ClassiFIM on multipledatasets including datasets describing classical and quantum phasetransitions and find that it achieves a good ground truth approximation withmodest computational resources. Furthermore we independently implement twoalternative state-of-the-art methods for unsupervised estimation of phasetransition locations on the same datasets and find that ClassiFIM predicts suchlocations at least as well as these other methods. To emphasize the generalityof our method we also propose and generate the MNIST-CNN dataset whichconsists of the output of CNNs trained on MNIST for different hyperparameterchoices. Using ClassiFIM on this dataset suggests there is a phase transitionin the distribution of image-prediction pairs for CNNs trained on MNISTdemonstrating the broad scope of FIM-estimation beyond physics.</p>
                <p>Last Updated: 2024-08-06 17:58:29 UTC</p>
                <button class="interpret-button" data-id="2408.03323v1">Interpret</button>
                <div id="interpretation-2408.03323v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Hedge Fund Portfolio Construction Using PolyModel Theory and iTransformer</h3>
                <p>Authors: Siqiao ZhaoZhikang DongZeyu CaoRaphael Douady</p>
                <p><a href="http://arxiv.org/abs/2408.03320v1">Link to paper</a></p>
                <p>When constructing portfolios a key problem is that a lot of financial timeseries data are sparse making it challenging to apply machine learningmethods. Polymodel theory can solve this issue and demonstrate superiority inportfolio construction from various aspects. To implement the PolyModel theoryfor constructing a hedge fund portfolio we begin by identifying an asset poolutilizing over 10000 hedge funds for the past 29 years data. PolyModel theoryalso involves choosing a wide-ranging set of risk factors which includesvarious financial indices currencies and commodity prices. This comprehensiveselection mirrors the complexities of the real-world environment. Leveraging onthe PolyModel theory we create quantitative measures such as Long-term AlphaLong-term Ratio and SVaR. We also use more classical measures like the Sharperatio or Morningstars MRAR. To enhance the performance of the constructedportfolio we also employ the latest deep learning techniques iTransformer tocapture the upward trend while efficiently controlling the downside using allthe features. The iTransformer model is specifically designed to address thechallenges in high-dimensional time series forecasting and could largelyimprove our strategies. More precisely our strategies achieve better Sharperatio and annualized return. The above process enables us to create multipleportfolio strategies aiming for high returns and low risks when compared tovarious benchmarks.</p>
                <p>Last Updated: 2024-08-06 17:55:58 UTC</p>
                <button class="interpret-button" data-id="2408.03320v1">Interpret</button>
                <div id="interpretation-2408.03320v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</h3>
                <p>Authors: Charlie SnellJaehoon LeeKelvin XuAviral Kumar</p>
                <p><a href="http://arxiv.org/abs/2408.03314v1">Link to paper</a></p>
                <p>Enabling LLMs to improve their outputs by using more test-time computation isa critical step towards building generally self-improving agents that canoperate on open-ended natural language. In this paper we study the scaling ofinference-time computation in LLMs with a focus on answering the question: ifan LLM is allowed to use a fixed but non-trivial amount of inference-timecompute how much can it improve its performance on a challenging promptAnswering this question has implications not only on the achievable performanceof LLMs but also on the future of LLM pretraining and how one should tradeoffinference-time and pre-training compute. Despite its importance littleresearch attempted to understand the scaling behaviors of various test-timeinference methods. Moreover current work largely provides negative results fora number of these strategies. In this work we analyze two primary mechanismsto scale test-time computation: 1 searching against dense process-basedverifier reward models and 2 updating the models distribution over aresponse adaptively given the prompt at test time. We find that in both casesthe effectiveness of different approaches to scaling test-time computecritically varies depending on the difficulty of the prompt. This observationmotivates applying a compute-optimal scaling strategy which acts to mosteffectively allocate test-time compute adaptively per prompt. Using thiscompute-optimal strategy we can improve the efficiency of test-time computescaling by more than 4x compared to a best-of-N baseline. Additionally in aFLOPs-matched evaluation we find that on problems where a smaller base modelattains somewhat non-trivial success rates test-time compute can be used tooutperform a 14x larger model.</p>
                <p>Last Updated: 2024-08-06 17:35:05 UTC</p>
                <button class="interpret-button" data-id="2408.03314v1">Interpret</button>
                <div id="interpretation-2408.03314v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Pre-training and in-context learning IS Bayesian inference a la De Finetti</h3>
                <p>Authors: Naimeng YeHanming YangAndrew SiahHongseok Namkoong</p>
                <p><a href="http://arxiv.org/abs/2408.03307v1">Link to paper</a></p>
                <p>Accurately gauging uncertainty on the underlying environment is alongstanding goal of intelligent systems. We characterize which latent conceptspre-trained sequence models are naturally able to reason with. We go back to DeFinettis predictive view of Bayesian reasoning: instead of modeling latentparameters through priors and likelihoods like topic models do De Finetti haslong advocated for modeling exchangeable permutation invariant sequences ofobservables. According to this view pre-training autoregressive modelsformulates informed beliefs based on prior observations empirical Bayesand forward generation is a simulated instantiation of an environmentposterior inference. This connection allows extending in-context learningICL beyond predictive settings highlighting sequence models ability toperform explicit statistical inference. In particular we show the sequenceprediction loss over exchangeable documents controls performance on downstreamtasks where uncertainty quantification is key. Empirically we propose anddemonstrate several approaches for encoding exchangeability in sequence modelarchitectures: data augmentation regularization and causal masking.</p>
                <p>Last Updated: 2024-08-06 17:16:10 UTC</p>
                <button class="interpret-button" data-id="2408.03307v1">Interpret</button>
                <div id="interpretation-2408.03307v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks</h3>
                <p>Authors: Rafael SterzingerChristian StippelRobert Sablatnig</p>
                <p><a href="http://arxiv.org/abs/2408.03304v1">Link to paper</a></p>
                <p>Etruscan mirrors constitute a significant category in Etruscan artcharacterized by elaborate figurative illustrations featured on their backside.A laborious and costly aspect of their analysis and documentation is the taskof manually tracing these illustrations. In previous work a methodology hasbeen proposed to automate this process involving photometric-stereo scanningin combination with deep neural networks. While achieving quantitativeperformance akin to an expert annotator some results still lack qualitativeprecision and thus require annotators for inspection and potentialcorrection maintaining resource intensity. In response we propose a deepneural network trained to interactively refine existing annotations based onhuman guidance. Our human-in-the-loop approach streamlines annotationachieving equal quality with up to 75 less manual input required. Moreoverduring the refinement process the relative improvement of our methodology overpure manual labeling reaches peak values of up to 26 attaining drasticallybetter quality quicker. By being tailored to the complex task of segmentingintricate lines specifically distinguishing it from previous methods ourapproach offers drastic improvements in efficacy transferable to a broadspectrum of applications beyond Etruscan mirrors.</p>
                <p>Last Updated: 2024-08-06 17:11:40 UTC</p>
                <button class="interpret-button" data-id="2408.03304v1">Interpret</button>
                <div id="interpretation-2408.03304v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks</h3>
                <p>Authors: Rafael SterzingerChristian StippelRobert Sablatnig</p>
                <p><a href="http://arxiv.org/abs/2408.03304v1">Link to paper</a></p>
                <p>Etruscan mirrors constitute a significant category in Etruscan artcharacterized by elaborate figurative illustrations featured on their backside.A laborious and costly aspect of their analysis and documentation is the taskof manually tracing these illustrations. In previous work a methodology hasbeen proposed to automate this process involving photometric-stereo scanningin combination with deep neural networks. While achieving quantitativeperformance akin to an expert annotator some results still lack qualitativeprecision and thus require annotators for inspection and potentialcorrection maintaining resource intensity. In response we propose a deepneural network trained to interactively refine existing annotations based onhuman guidance. Our human-in-the-loop approach streamlines annotationachieving equal quality with up to 75 less manual input required. Moreoverduring the refinement process the relative improvement of our methodology overpure manual labeling reaches peak values of up to 26 attaining drasticallybetter quality quicker. By being tailored to the complex task of segmentingintricate lines specifically distinguishing it from previous methods ourapproach offers drastic improvements in efficacy transferable to a broadspectrum of applications beyond Etruscan mirrors.</p>
                <p>Last Updated: 2024-08-06 17:11:40 UTC</p>
                <button class="interpret-button" data-id="2408.03304v1">Interpret</button>
                <div id="interpretation-2408.03304v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges</h3>
                <p>Authors: Jonggi HongHernisa Kacorri</p>
                <p><a href="http://dx.doi.org/10.1145/3663548.3675635">Link to paper</a></p>
                <p>Object recognition technologies hold the potential to support blind andlow-vision people in navigating the world around them. However the gap betweenbenchmark performances and practical usability remains a significant challenge.This paper presents a study aimed at understanding blind users interactionwith object recognition systems for identifying and avoiding errors. Leveraginga pre-existing object recognition system URCam fine-tuned for our experimentwe conducted a user study involving 12 blind and low-vision participants.Through in-depth interviews and hands-on error identification tasks we gainedinsights into users experiences challenges and strategies for identifyingerrors in camera-based assistive technologies and object recognition systems.During interviews many participants preferred independent error review whileexpressing apprehension toward misrecognitions. In the error identificationtask participants varied viewpoints backgrounds and object sizes in theirimages to avoid and overcome errors. Even after repeating the taskparticipants identified only half of the errors and the proportion of errorsidentified did not significantly differ from their first attempts. Based onthese insights we offer implications for designing accessible interfacestailored to the needs of blind and low-vision users in identifying objectrecognition errors.</p>
                <p>Last Updated: 2024-08-06 17:09:56 UTC</p>
                <button class="interpret-button" data-id="2408.03303v1">Interpret</button>
                <div id="interpretation-2408.03303v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>JetUnit: Rendering Diverse Force Feedback in Virtual Reality Using Water Jets</h3>
                <p>Authors: Zining ZhangJiasheng LiZeyu YanJun NishidaHuaishu Peng</p>
                <p><a href="http://dx.doi.org/10.1145/3654777.3676440">Link to paper</a></p>
                <p>We propose JetUnit a water-based VR haptic system designed to produce forcefeedback with a wide spectrum of intensities and frequencies through waterjets. The key challenge in designing this system lies in optimizing parametersto enable the haptic device to generate force feedback that closely replicatesthe most intense force produced by direct water jets while ensuring the userremains dry. In this paper we present the key design parameters of the JetUnitwearable device determined through a set of quantitative experiments and aperception study. We further conducted a user study to assess the impact ofintegrating our haptic solutions into virtual reality experiences. The resultsrevealed that by adhering to the design principles of JetUnit the water-basedhaptic system is capable of delivering diverse force feedback sensationssignificantly enhancing the immersive experience in virtual reality.</p>
                <p>Last Updated: 2024-08-06 16:33:35 UTC</p>
                <button class="interpret-button" data-id="2408.03285v1">Interpret</button>
                <div id="interpretation-2408.03285v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments</h3>
                <p>Authors: Angie BoggustVenkatesh SivaramanYannick AssogbaDonghao RenDominik MoritzFred Hohman</p>
                <p><a href="http://arxiv.org/abs/2408.03274v1">Link to paper</a></p>
                <p>To deploy machine learning models on-device practitioners use compressionalgorithms to shrink and speed up models while maintaining their high-qualityoutput. A critical aspect of compression in practice is model comparisonincluding tracking many compression experiments identifying subtle changes inmodel behavior and negotiating complex accuracy-efficiency trade-offs.However existing compression tools poorly support comparison leading totedious and sometimes incomplete analyses spread across disjoint tools. Tosupport real-world comparative workflows we develop an interactive visualsystem called Compress and Compare. Within a single interface Compress andCompare surfaces promising compression strategies by visualizing provenancerelationships between compressed models and reveals compression-inducedbehavior changes by comparing models predictions weights and activations. Wedemonstrate how Compress and Compare supports common compression analysis tasksthrough two case studies debugging failed compression on generative languagemodels and identifying compression artifacts in image classification models. Wefurther evaluate Compress and Compare in a user study with eight compressionexperts illustrating its potential to provide structure to compressionworkflows help practitioners build intuition about compression and encouragethorough analysis of compressions effect on model behavior. Through theseevaluations we identify compression-specific challenges that future visualanalytics tools should consider and Compress and Compare visualizations thatmay generalize to broader model comparison tasks.</p>
                <p>Last Updated: 2024-08-06 16:17:51 UTC</p>
                <button class="interpret-button" data-id="2408.03274v1">Interpret</button>
                <div id="interpretation-2408.03274v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Connections Beyond Data: Exploring Homophily With Visualizations</h3>
                <p>Authors: Poorna Talkad SukumarMaurizio PorfiriOded Nov</p>
                <p><a href="http://arxiv.org/abs/2408.03269v1">Link to paper</a></p>
                <p>Homophily refers to the tendency of individuals to associate with others whoare similar to them in characteristics such as race ethnicity age genderor interests. In this paper we investigate if individuals exhibit racialhomophily when viewing visualizations using mass shooting data in the UnitedStates as the example topic. We conducted a crowdsourced experiment N450where each participant was shown a visualization displaying the counts of massshooting victims highlighting the counts for one of three racial groupsWhite Black or Hispanic. Participants were assigned to view visualizationshighlighting their own race or a different race to assess the influence ofracial concordance on changes in affect emotion and attitude towards guncontrol. While we did not find evidence of homophily the results showed asignificant negative shift in affect across all visualization conditions.Notably political ideology significantly impacted changes in affect with moreliberal views correlating with a more negative affect change. Our findingsunderscore the complexity of reactions to mass shooting visualizations andsuggest that future research should consider various methodologicalimprovements to better assess homophily effects.</p>
                <p>Last Updated: 2024-08-06 15:59:53 UTC</p>
                <button class="interpret-button" data-id="2408.03269v1">Interpret</button>
                <div id="interpretation-2408.03269v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>LLaVA-OneVision: Easy Visual Task Transfer</h3>
                <p>Authors: Bo LiYuanhan ZhangDong GuoRenrui ZhangFeng LiHao ZhangKaichen ZhangYanwei LiZiwei LiuChunyuan Li</p>
                <p><a href="http://arxiv.org/abs/2408.03326v1">Link to paper</a></p>
                <p>We present LLaVA-OneVision a family of open large multimodal models LMMsdeveloped by consolidating our insights into data models and visualrepresentations in the LLaVA-NeXT blog series. Our experimental resultsdemonstrate that LLaVA-OneVision is the first single model that cansimultaneously push the performance boundaries of open LMMs in three importantcomputer vision scenarios: single-image multi-image and video scenarios.Importantly the design of LLaVA-OneVision allows strong transfer learningacross different modalities/scenarios yielding new emerging capabilities. Inparticular strong video understanding and cross-scenario capabilities aredemonstrated through task transfer from images to videos.</p>
                <p>Last Updated: 2024-08-06 17:59:44 UTC</p>
                <button class="interpret-button" data-id="2408.03326v1">Interpret</button>
                <div id="interpretation-2408.03326v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Segment Anything in Medical Images and Videos: Benchmark and Deployment</h3>
                <p>Authors: Jun MaSumin KimFeifei LiMohammed BaharoonReza AsakerehHongwei LyuBo Wang</p>
                <p><a href="http://arxiv.org/abs/2408.03322v1">Link to paper</a></p>
                <p>Recent advances in segmentation foundation models have enabled accurate andefficient segmentation across a wide range of natural images and videos buttheir utility to medical data remains unclear. In this work we first present acomprehensive benchmarking of the Segment Anything Model 2 SAM2 across 11medical image modalities and videos and point out its strengths and weaknessesby comparing it to SAM1 and MedSAM. Then we develop a transfer learningpipeline and demonstrate SAM2 can be quickly adapted to medical domain byfine-tuning. Furthermore we implement SAM2 as a 3D slicer plugin and GradioAPI for efficient 3D image and video segmentation. The code has been madepublicly available at urlhttps://github.com/bowang-lab/MedSAM.</p>
                <p>Last Updated: 2024-08-06 17:58:18 UTC</p>
                <button class="interpret-button" data-id="2408.03322v1">Interpret</button>
                <div id="interpretation-2408.03322v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation</h3>
                <p>Authors: Xiaofeng MaoZhengkai JiangQilin WangChencan FuJiangning ZhangJiafu WuYabiao WangChengjie WangWei LiMingmin Chi</p>
                <p><a href="http://dx.doi.org/10.1145/3664647.3680684">Link to paper</a></p>
                <p>Recent advancements in the field of Diffusion Transformers have substantiallyimproved the generation of high-quality 2D images 3D videos and 3D shapes.However the effectiveness of the Transformer architecture in the domain ofco-speech gesture generation remains relatively unexplored as priormethodologies have predominantly employed the Convolutional Neural NetworkCNNs or simple a few transformer layers. In an attempt to bridge thisresearch gap we introduce a novel Masked Diffusion Transformer for co-speechgesture generation referred to as MDT-A2G which directly implements thedenoising process on gesture sequences. To enhance the contextual reasoningcapability of temporally aligned speech-driven gestures we incorporate a novelMasked Diffusion Transformer. This model employs a mask modeling schemespecifically designed to strengthen temporal relation learning among sequencegestures thereby expediting the learning process and leading to coherent andrealistic motions. Apart from audio Our MDT-A2G model also integratesmulti-modal information encompassing text emotion and identity. Furthermorewe propose an efficient inference strategy that diminishes the denoisingcomputation by leveraging previously calculated results thereby achieving aspeedup with negligible performance degradation. Experimental resultsdemonstrate that MDT-A2G excels in gesture generation boasting a learningspeed that is over 6times faster than traditional diffusion transformers andan inference speed that is 5.7times than the standard diffusion model.</p>
                <p>Last Updated: 2024-08-06 17:29:01 UTC</p>
                <button class="interpret-button" data-id="2408.03312v1">Interpret</button>
                <div id="interpretation-2408.03312v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks</h3>
                <p>Authors: Rafael SterzingerChristian StippelRobert Sablatnig</p>
                <p><a href="http://arxiv.org/abs/2408.03304v1">Link to paper</a></p>
                <p>Etruscan mirrors constitute a significant category in Etruscan artcharacterized by elaborate figurative illustrations featured on their backside.A laborious and costly aspect of their analysis and documentation is the taskof manually tracing these illustrations. In previous work a methodology hasbeen proposed to automate this process involving photometric-stereo scanningin combination with deep neural networks. While achieving quantitativeperformance akin to an expert annotator some results still lack qualitativeprecision and thus require annotators for inspection and potentialcorrection maintaining resource intensity. In response we propose a deepneural network trained to interactively refine existing annotations based onhuman guidance. Our human-in-the-loop approach streamlines annotationachieving equal quality with up to 75 less manual input required. Moreoverduring the refinement process the relative improvement of our methodology overpure manual labeling reaches peak values of up to 26 attaining drasticallybetter quality quicker. By being tailored to the complex task of segmentingintricate lines specifically distinguishing it from previous methods ourapproach offers drastic improvements in efficacy transferable to a broadspectrum of applications beyond Etruscan mirrors.</p>
                <p>Last Updated: 2024-08-06 17:11:40 UTC</p>
                <button class="interpret-button" data-id="2408.03304v1">Interpret</button>
                <div id="interpretation-2408.03304v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TextIM: Part-aware Interactive Motion Synthesis from Text</h3>
                <p>Authors: Siyuan FanBo DuXiantao CaiBo PengLongling Sun</p>
                <p><a href="http://arxiv.org/abs/2408.03302v1">Link to paper</a></p>
                <p>In this work we propose TextIM a novel framework for synthesizingTEXT-driven human Interactive Motions with a focus on the precise alignment ofpart-level semantics. Existing methods often overlook the critical roles ofinteractive body parts and fail to adequately capture and align part-levelsemantics resulting in inaccuracies and even erroneous movement outcomes. Toaddress these issues TextIM utilizes a decoupled conditional diffusionframework to enhance the detailed alignment between interactive movements andcorresponding semantic intents from textual descriptions. Our approachleverages large language models functioning as a human brain to identifyinteracting human body parts and to comprehend interaction semantics togenerate complicated and subtle interactive motion. Guided by the refinedmovements of the interacting parts TextIM further extends these movements intoa coherent whole-body motion. We design a spatial coherence module tocomplement the entire body movements while maintaining consistency and harmonyacross body parts using a part graph convolutional network. For training andevaluation we carefully selected and re-labeled interactive motions fromHUMANML3D to develop a specialized dataset. Experimental results demonstratethat TextIM produces semantically accurate human interactive motionssignificantly enhancing the realism and applicability of synthesizedinteractive motions in diverse scenarios even including interactions withdeformable and dynamically changing objects.</p>
                <p>Last Updated: 2024-08-06 17:08:05 UTC</p>
                <button class="interpret-button" data-id="2408.03302v1">Interpret</button>
                <div id="interpretation-2408.03302v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-08-07</p>
        </div>
    
        </div>
    </body>
    </html>
    