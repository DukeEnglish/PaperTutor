
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>The Computational Complexity of Learning Gaussian Single-Index Models</h3>
                <p>Authors: Alex DamianLoucas Pillaud-VivienJason D. LeeJoan Bruna</p>
                <p><a href="http://arxiv.org/abs/2403.05529v1">Link to paper</a></p>
                <p>Single-Index Models are high-dimensional regression problems with plantedstructure whereby labels depend on an unknown one-dimensional projection ofthe input via a generic non-linear and potentially non-deterministictransformation. As such they encompass a broad class of statistical inferencetasks and provide a rich template to study statistical and computationaltrade-offs in the high-dimensional regime.  While the information-theoretic sample complexity to recover the hiddendirection is linear in the dimension d we show that computationallyefficient algorithms both within the Statistical Query SQ and the Low-DegreePolynomial LDP framework necessarily require Omegadkstar/2samples where kstar is a generative exponent associated with the modelthat we explicitly characterize. Moreover we show that this sample complexityis also sufficient by establishing matching upper bounds using a partial-tracealgorithm. Therefore our results provide evidence of a sharpcomputational-to-statistical gap under both the SQ and LDP class wheneverkstar2. To complete the study we provide examples of smooth and Lipschitzdeterministic target functions with arbitrarily large generative exponentskstar.</p>
                <p>Last Updated: 2024-03-08 18:50:19 UTC</p>
                <button class="interpret-button" data-id="2403.05529v1">Interpret</button>
                <div id="interpretation-2403.05529v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Poly-View Contrastive Learning</h3>
                <p>Authors: Amitis ShidaniDevon HjelmJason RamapuramRuss WebbEeshan Gunesh DhekaneDan Busbridge</p>
                <p><a href="http://arxiv.org/abs/2403.05490v1">Link to paper</a></p>
                <p>Contrastive learning typically matches pairs of related views among a numberof unrelated negative views. Views can be generated e.g. by augmentations orbe observed. We investigate matching when there are more than two related viewswhich we call poly-view tasks and derive new representation learningobjectives using information maximization and sufficient statistics. We showthat with unlimited computation one should maximize the number of relatedviews and with a fixed compute budget it is beneficial to decrease the numberof unique samples whilst increasing the number of views of those samples. Inparticular poly-view contrastive models trained for 128 epochs with batch size256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1kchallenging the belief that contrastive models require large batch sizes andmany training epochs.</p>
                <p>Last Updated: 2024-03-08 17:55:41 UTC</p>
                <button class="interpret-button" data-id="2403.05490v1">Interpret</button>
                <div id="interpretation-2403.05490v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>An Improved Algorithm for Learning Drifting Discrete Distributions</h3>
                <p>Authors: Alessio Mazzetto</p>
                <p><a href="http://arxiv.org/abs/2403.05446v1">Link to paper</a></p>
                <p>We present a new adaptive algorithm for learning discrete distributions underdistribution drift. In this setting we observe a sequence of independentsamples from a discrete distribution that is changing over time and the goalis to estimate the current distribution. Since we have access to only a singlesample for each time step a good estimation requires a careful choice of thenumber of past samples to use. To use more samples we must resort to samplesfurther in the past and we incur a drift error due to the bias introduced bythe change in distribution. On the other hand if we use a small number of pastsamples we incur a large statistical error as the estimation has a highvariance. We present a novel adaptive algorithm that can solve this trade-offwithout any prior knowledge of the drift. Unlike previous adaptive results ouralgorithm characterizes the statistical error using data-dependent bounds. Thistechnicality enables us to overcome the limitations of the previous work thatrequire a fixed finite support whose size is known in advance and that cannotchange over time. Additionally we can obtain tighter bounds depending on thecomplexity of the drifting distribution and also consider distributions withinfinite support.</p>
                <p>Last Updated: 2024-03-08 16:54:27 UTC</p>
                <button class="interpret-button" data-id="2403.05446v1">Interpret</button>
                <div id="interpretation-2403.05446v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>An Adaptive Dimension Reduction Estimation Method for High-dimensional Bayesian Optimization</h3>
                <p>Authors: Shouri HuJiawei LiZhibo Cai</p>
                <p><a href="http://arxiv.org/abs/2403.05425v1">Link to paper</a></p>
                <p>Bayesian optimization BO has shown impressive results in a variety ofapplications within low-to-moderate dimensional Euclidean spaces. Howeverextending BO to high-dimensional settings remains a significant challenge. Weaddress this challenge by proposing a two-step optimization framework.Initially we identify the effective dimension reduction EDR subspace for theobjective function using the minimum average variance estimation MAVE method.Subsequently we construct a Gaussian process model within this EDR subspaceand optimize it using the expected improvement criterion. Our algorithm offersthe flexibility to operate these steps either concurrently or in sequence. Inthe sequential approach we meticulously balance the exploration-exploitationtrade-off by distributing the sampling budget between subspace estimation andfunction optimization and the convergence rate of our algorithm inhigh-dimensional contexts has been established. Numerical experiments validatethe efficacy of our method in challenging scenarios.</p>
                <p>Last Updated: 2024-03-08 16:21:08 UTC</p>
                <button class="interpret-button" data-id="2403.05425v1">Interpret</button>
                <div id="interpretation-2403.05425v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Variational Inference of Parameters in Opinion Dynamics Models</h3>
                <p>Authors: Jacopo LentiFabrizio SilvestriGianmarco De Francisci Morales</p>
                <p><a href="http://arxiv.org/abs/2403.05358v1">Link to paper</a></p>
                <p>Despite the frequent use of agent-based models ABMs for studying socialphenomena parameter estimation remains a challenge often relying on costlysimulation-based heuristics. This work uses variational inference to estimatethe parameters of an opinion dynamics ABM by transforming the estimationproblem into an optimization task that can be solved directly.  Our proposal relies on probabilistic generative ABMs PGABMs: we start bysynthesizing a probabilistic generative model from the ABM rules. Then wetransform the inference process into an optimization problem suitable forautomatic differentiation. In particular we use the Gumbel-Softmaxreparameterization for categorical agent attributes and stochastic variationalinference for parameter estimation. Furthermore we explore the trade-offs ofusing variational distributions with different complexity: normal distributionsand normalizing flows.  We validate our method on a bounded confidence model with agent rolesleaders and followers. Our approach estimates both macroscopic boundedconfidence intervals and backfire thresholds and microscopic 200categorical agent-level roles more accurately than simulation-based and MCMCmethods. Consequently our technique enables experts to tune and validate theirABMs against real-world observations thus providing insights into humanbehavior in social systems via data-driven analysis.</p>
                <p>Last Updated: 2024-03-08 14:45:18 UTC</p>
                <button class="interpret-button" data-id="2403.05358v1">Interpret</button>
                <div id="interpretation-2403.05358v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos</h3>
                <p>Authors: Tarun KalluriBodhisattwa Prasad MajumderManmohan Chandraker</p>
                <p><a href="http://arxiv.org/abs/2403.05535v1">Link to paper</a></p>
                <p>We introduce LaGTran a novel framework that utilizes readily available oreasily acquired text descriptions to guide robust transfer of discriminativeknowledge from labeled source to unlabeled target data with domain shifts.While unsupervised adaptation methods have been established to address thisproblem they show limitations in handling challenging domain shifts due totheir exclusive operation within the pixel-space. Motivated by our observationthat semantically richer text modality has more favorable transfer propertieswe devise a transfer mechanism to use a source-trained text-classifier togenerate predictions on the target text descriptions and utilize thesepredictions as supervision for the corresponding images. Our approach driven bylanguage guidance is surprisingly easy and simple yet significantlyoutperforms all prior approaches on challenging datasets like GeoNet andDomainNet validating its extreme effectiveness. To further extend the scope ofour study beyond images we introduce a new benchmark to study ego-exo transferin videos and find that our language-aided LaGTran yields significant gains inthis highly challenging and non-trivial transfer setting. Code models andproposed datasets are publicly available athttps://tarun005.github.io/lagtran/.</p>
                <p>Last Updated: 2024-03-08 18:58:46 UTC</p>
                <button class="interpret-button" data-id="2403.05535v1">Interpret</button>
                <div id="interpretation-2403.05535v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</h3>
                <p>Authors: Machel ReidNikolay SavinovDenis TeplyashinDmitry LepikhinTimothy LillicrapJean-baptiste AlayracRadu SoricutAngeliki LazaridouOrhan FiratJulian SchrittwieserIoannis AntonoglouRohan AnilSebastian BorgeaudAndrew DaiKatie MillicanEthan DyerMia GlaeseThibault SottiauxBenjamin LeeFabio ViolaMalcolm ReynoldsYuanzhong XuJames MolloyJilin ChenMichael IsardPaul BarhamTom HenniganRoss McIlroyMelvin JohnsonJohan SchalkwykEli CollinsEliza RutherfordErica MoreiraKareem AyoubMegha GoelClemens MeyerGregory ThorntonZhen YangHenryk MichalewskiZaheer AbbasNathan SchucherAnkesh AnandRichard IvesJames KeelingKarel LencSalem HaykalSiamak ShakeriPranav ShyamAakanksha ChowdheryRoman RingStephen SpencerEren SezenerLuke VilnisOscar ChangNobuyuki MoriokaGeorge TuckerCe ZhengOliver WoodmanNithya AttaluriTomas KociskyEvgenii EltyshevXi ChenTimothy ChungVittorio SeloSiddhartha BrahmaPetko GeorgievAmbrose SloneZhenkai ZhuJames LottesSiyuan QiaoBen CaineSebastian RiedelAlex TomalaMartin ChadwickJuliette LovePeter ChoySid MittalNeil HoulsbyYunhao TangMatthew LammLibin BaiQiao ZhangLuheng HeYong ChengPeter HumphreysYujia LiSergey BrinAlbin CassirerYingjie MiaoLukas ZilkaTaylor TobinKelvin XuLev ProleevDaniel SohnAlberto MagniLisa Anne HendricksIsabel GaoSantiago OntañónOskar BunyanNathan ByrdAbhanshu SharmaBiao ZhangMario PintoRishika SinhaHarsh MehtaDawei JiaSergi CaellesAlbert WebsonAlex MorrisBecca RoelofsYifan DingRobin StrudelXuehan XiongMarvin RitterMostafa DehghaniRahma ChaabouniAbhijit KarmarkarGuangda LaiFabian MentzerBibo XuYaGuang LiYujing ZhangTom Le PaineAlex GoldinBehnam NeyshaburKate BaumliAnselm LevskayaMichael LaskinWenhao JiaJack W. RaeKefan XiaoAntoine HeSkye GiordanoLakshman YagatiJean-Baptiste LespiauPaul NatsevSanjay GanapathyFangyu LiuDanilo MartinsNanxin ChenYunhan XuMegan BarnesRhys MayArpi VezerJunhyuk OhKen FrankoSophie BridgersRuizhe ZhaoBoxi WuBasil MustafaSean SechristEmilio ParisottoThanumalayan Sankaranarayana PillaiChris LarkinChenjie GuChristina SorokinMaxim KrikunAlexey GuseynovJessica LandonRomina DattaAlexander PritzelPhoebe ThackerFan YangKevin HuiAnja HauthChih-Kuan YehDavid BarkerJustin Mao-JonesSophia AustinHannah SheahanParker SchuhJames SvenssonRohan JainVinay RamaseshAnton BriukhovDa-Woon ChungTamara von GlehnChristina ButterfieldPriya JhakraMatthew WiethoffJustin FryeJordan GrimstadBeer ChangpinyoCharline Le LanAnna BortsovaYonghui WuPaul VoigtlaenderTara SainathCharlotte SmithWill HawkinsKris CaoJames BesleySrivatsan SrinivasanMark OmernickColin GaffneyGabriela SuritaRyan BurnellBogdan DamocJunwhan AhnAndrew BrockMantas PajarskasAnastasia PetrushkinaSeb NouryLorenzo BlancoKevin SwerskyArun AhujaThi AvrahamiVedant MisraRaoul de LiedekerkeMariko IinumaAlex PolozovSarah YorkGeorge van den DriesschePaul MichelJustin ChiuRory BlevinsZach GleicherAdrià RecasensAlban RrustemiElena GribovskayaAurko RoyWiktor GworekSéb ArnoldLisa LeeJames Lee-ThorpMarcello MaggioniEnrique PiquerasKartikeya BadolaSharad VikramLucas GonzalezAnirudh BaddepudiEvan SenterJacob DevlinJames QinMichael AzzamMaja TrebaczMartin PolacekKashyap KrishnakumarShuo-yiin ChangMatthew TungIvo PenchevRishabh JoshiKate OlszewskaCarrie MuirMateo WirthAle Jakse HartmanJosh NewlanSheleem KashemVijay BolinaElahe DabirJoost van AmersfoortZafarali AhmedJames Cobon-KerrAishwarya KamathArnar Mar HrafnkelssonLe HouIan MackinnonAlexandre FrechetteEric NolandXiance SiEmanuel TaropaDong LiPhil CroneAnmol GulatiSébastien CeveyJonas AdlerAda MaDavid SilverSimon TokumineRichard PowellStephan LeeMichael ChangSamer HassanDiana MincuAntoine YangNir LevineJenny BrennanMingqiu WangSarah HodkinsonJeffrey ZhaoJosh LipschultzAedan PopeMichael B. ChangCheng LiLaurent El ShafeyMichela PaganiniSholto DouglasBernd BohnetFabio PardoSeth OdoomMihaela RoscaCicero Nogueira dos SantosKedar SoparkarArthur GuezTom HudsonSteven HansenChulayuth AsawaroengchaiRavi AddankiTianhe YuWojciech StokowiecMina KhanJustin GilmerJaehoon LeeCarrie Grimes BostockKeran RongJonathan CatonPedram PejmanFilip PaveticGeoff BrownVivek SharmaMario LučićRajkumar SamuelJosip DjolongaAmol MandhaneLars Lowe SjösundElena BuchatskayaElspeth WhiteNatalie ClayJiepu JiangHyeontaek LimRoss HemsleyJane LabanowskiNicola De CaoDavid SteinerSayed Hadi HashemiJacob AustinAnita GergelyTim BlythJoe StantonKaushik ShivakumarAditya SiddhantAnders AndreassenCarlos ArayaNikhil SethiRakesh ShivannaSteven HandAnkur BapnaAli KhodaeiAntoine MiechGarrett TanzerAndy SwingShantanu ThakoorZhufeng PanZachary NadoStephanie WinklerDian YuMohammad SalehLoren MaggioreIain BarrMinh GiangThais KagoharaIvo DanihelkaAmit MaratheVladimir FeinbergMohamed ElhawatyNimesh GhelaniDan HorganHelen MillerLexi WalkerRichard TanburnMukarram TariqDisha ShrivastavaFei XiaChung-Cheng ChiuZoe AshwoodKhuslen BaatarsukhSina SamangooeiFred AlcoberAxel StjerngrenPaul KomarekKaterina TsihlasAnudhyan BoralRamona ComanescuJeremy ChenRuibo LiuDawn BloxwichCharlie ChenYanhua SunFangxiaoyu FengMatthew MaugerXerxes DotiwallaVincent HellendoornMichael SharmanIvy ZhengKrishna HaridasanGabe Barth-MaronCraig SwansonDominika RogozińskaAlek AndreevPaul Kishan RubensteinRuoxin SangDan HurtGamaleldin ElsayedRenshen WangDave LaceyAnastasija IlićYao ZhaoLora AroyoChimezie IwuanyanwuVitaly NikolaevBalaji LakshminarayananSadegh JazayeriRaphaël Lopez KaufmanMani VaradarajanChetan TekurDoug FritzMisha KhalmanDavid ReitterKingshuk DasguptaShourya SarcarTina OrnduffJavier SnaiderFantine HuotJohnson JiaRupert KempNejc TrdinAnitha VijayakumarLucy KimChristof AngermuellerLi LaoTianqi LiuHaibin ZhangDavid EngelSomer GreeneAnaïs WhiteJessica AustinLilly TaylorShereen AshrafDangyi LiuMaria GeorgakiIrene CaiYana KulizhskayaSonam GoenkaBrennan SaetaKiran VodrahalliChristian FrankDario de CesareBrona RobenekHarry RichardsonMahmoud AlnahlawiChristopher YewPriya PonnapalliMarco TagliasacchiAlex KorchemniyYelin KimDinghua LiBill RosgenZoe AshwoodKyle LevinJeremy WiesnerPraseem BanzalPraveen SrinivasanHongkun YuÇağlar ÜnlüDavid ReidZora TungDaniel FinchelsteinRavin KumarAndre ElisseeffJin HuangMing ZhangRui ZhuRicardo AguilarMai GiménezJiawei XiaOlivier DousseWilli GierkeSoheil Hassas YeganehDamion YatesKomal JalanLu LiEri Latorre-ChimotoDuc Dung NguyenKen DurdenPraveen KallakuriYaxin LiuMatthew JohnsonTomy TsaiAlice TalbertJasmine LiuAlexander NeitzChen ElkindMarco SelviMimi JasarevicLivio Baldini SoaresAlbert CuiPidong WangAlek Wenjiao WangXinyu YeKrystal KallarackalLucia LoherHoi LamJosef BroderDan Holtmann-RiceNina MartinBramandia RamadhanaDaniel ToyamaMrinal ShuklaSujoy BasuAbhi MohanNick FernandoNoah FiedelKim PatersonHui LiAnkush GargJane ParkDongHyun ChoiDiane WuSankalp SinghZhishuai ZhangAmir GlobersonLily YuJohn CarpenterFélix de Chaumont QuitryCarey RadebaughChu-Cheng LinAlex TudorPrakash ShroffDrew GarmonDayou DuNeera VatsHan LuShariq IqbalAlex YakubovichNilesh TripuraneniJames ManyikaHaroon QureshiNan HuaChristel NganiMaria Abi RaadHannah ForbesAnna BulanovaJeff StanwayMukund SundararajanVictor UngureanuColton BishopYunjie LiBalaji VenkatramanBo LiChloe ThorntonSalvatore ScellatoNishesh GuptaYicheng WangIan TenneyXihui WuAshish ShenoyGabriel CarvajalDiana Gage WrightBen BariachZhuyun XiaoPeter HawkinsSid DalmiaClement FarabetPedro ValenzuelaQuan YuanChris WeltyAnanth AgarwalMia ChenWooyeol KimBrice HulseNandita DukkipatiAdam PaszkeAndrew BoltElnaz DavoodiKiam ChooJennifer BeattieJennifer PrendkiHarsha VashishtRebeca Santamaria-FernandezLuis C. CoboJarek WilkiewiczDavid MadrasAli ElqurshGrant UyKevin RamirezMatt HarveyTyler LiechtyHeiga ZenJeff SeibertClara Huiyi HuMohamed ElhawatyAndrey KhorlinMaigo LeAsaf AharoniMegan LiLily WangSandeep KumarAlejandro LinceNorman CasagrandeJay HooverDalia El BadawyDavid SoergelDenis VnukovMatt MiecnikowskiJiri SimsaAnna KoopPraveen KumarThibault SellamDaniel VlasicSamira DarukiNir ShabatJohn ZhangGuolong SuJiageng ZhangJeremiah LiuYi SunEvan PalmerAlireza GhaffarkhahXi XiongVictor CotrutaMichael FinkLucas DixonAshwin SreevatsaAdrian GoedeckemeyerAlek DimitrievMohsen JafariRemi CrockerNicholas FitzGeraldAviral KumarSanjay GhemawatIvan PhilipsFrederick LiuYannie LiangRachel SterneckAlena RepinaMarcus WuLaura KnightMarin GeorgievHyo LeeHarry AskhamAbhishek ChakladarAnnie LouisCarl CrousHardie CateDessie PetrovaMichael QuinnDenese Owusu-AfriyieAchintya SinghalNan WeiSolomon KimDamien VincentMilad NasrChristopher A. Choquette-ChooReiko TojoShawn LuDiego de Las CasasYuchung ChengTolga BolukbasiKatherine LeeSaaber FatehiRajagopal AnanthanarayananMiteyan PatelCharbel KaedJing LiJakub SygnowskiShreyas Rammohan BelleZhe ChenJaclyn KonzelmannSiim PõderRoopal GargVinod KoverkathuAdam BrownChris DyerRosanne LiuAzade NovaJun XuSlav PetrovDemis HassabisKoray KavukcuogluJeffrey DeanOriol Vinyals</p>
                <p><a href="http://arxiv.org/abs/2403.05530v1">Link to paper</a></p>
                <p>In this report we present the latest model of the Gemini family Gemini 1.5Pro a highly compute-efficient multimodal mixture-of-experts model capable ofrecalling and reasoning over fine-grained information from millions of tokensof context including multiple long documents and hours of video and audio.Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasksacross modalities improves the state-of-the-art in long-document QAlong-video QA and long-context ASR and matches or surpasses Gemini 1.0 Ultrasstate-of-the-art performance across a broad set of benchmarks. Studying thelimits of Gemini 1.5 Pros long-context ability we find continued improvementin next-token prediction and near-perfect retrieval 99 up to at least 10Mtokens a generational leap over existing models such as Claude 2.1 200k andGPT-4 Turbo 128k. Finally we highlight surprising new capabilities of largelanguage models at the frontier when given a grammar manual for Kalamang alanguage with fewer than 200 speakers worldwide the model learns to translateEnglish to Kalamang at a similar level to a person who learned from the samecontent.</p>
                <p>Last Updated: 2024-03-08 18:54:20 UTC</p>
                <button class="interpret-button" data-id="2403.05530v1">Interpret</button>
                <div id="interpretation-2403.05530v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM</h3>
                <p>Authors: Hao KangQingru ZhangSouvik KunduGeonhwa JeongZaoxing LiuTushar KrishnaTuo Zhao</p>
                <p><a href="http://arxiv.org/abs/2403.05527v1">Link to paper</a></p>
                <p>Key-value KV caching has become the de-facto to accelerate generation speedfor large language models LLMs inference. However the growing cache demandwith increasing sequence length has transformed LLM inference to be a memorybound problem significantly constraining the system throughput. Existingmethods rely on dropping unimportant tokens or quantizing all entriesuniformly. Such methods however often incur high approximation errors torepresent the compressed matrices. The autoregressive decoding process furthercompounds the error of each step resulting in critical deviation in modelgeneration and deterioration of performance. To tackle this challenge wepropose GEAR an efficient KV cache compression framework that achievesnear-lossless high-ratio compression. GEAR first applies quantization tomajority of entries of similar magnitudes to ultra-low precision. It thenemploys a low rank matrix to approximate the quantization error and a sparsematrix to remedy individual errors from outlier entries. By adeptly integratingthree techniques GEAR is able to fully exploit their synergistic potentials.Our experiments demonstrate that compared to alternatives GEAR achievesnear-lossless 4-bit KV cache compression with up to 2.38x throughputimprovement while reducing peak-memory size up to 2.29x. Our code is publiclyavailable at https://github.com/HaoKang-Timmy/GEAR.</p>
                <p>Last Updated: 2024-03-08 18:48:30 UTC</p>
                <button class="interpret-button" data-id="2403.05527v1">Interpret</button>
                <div id="interpretation-2403.05527v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DeepSeek-VL: Towards Real-World Vision-Language Understanding</h3>
                <p>Authors: Haoyu LuWen LiuBo ZhangBingxuan WangKai DongBo LiuJingxiang SunTongzheng RenZhuoshu LiYaofeng SunChengqi DengHanwei XuZhenda XieChong Ruan</p>
                <p><a href="http://arxiv.org/abs/2403.05525v1">Link to paper</a></p>
                <p>We present DeepSeek-VL an open-source Vision-Language VL Model designedfor real-world vision and language understanding applications. Our approach isstructured around three key dimensions:  We strive to ensure our data is diverse scalable and extensively coversreal-world scenarios including web screenshots PDFs OCR charts andknowledge-based content aiming for a comprehensive representation of practicalcontexts. Further we create a use case taxonomy from real user scenarios andconstruct an instruction tuning dataset accordingly. The fine-tuning with thisdataset substantially improves the models user experience in practicalapplications. Considering efficiency and the demands of most real-worldscenarios DeepSeek-VL incorporates a hybrid vision encoder that efficientlyprocesses high-resolution images 1024 x 1024 while maintaining a relativelylow computational overhead. This design choice ensures the models ability tocapture critical semantic and detailed information across various visual tasks.We posit that a proficient Vision-Language Model should foremost possessstrong language abilities. To ensure the preservation of LLM capabilitiesduring pretraining we investigate an effective VL pretraining strategy byintegrating LLM training from the beginning and carefully managing thecompetitive dynamics observed between vision and language modalities.  The DeepSeek-VL family both 1.3B and 7B models showcases superior userexperiences as a vision-language chatbot in real-world applications achievingstate-of-the-art or competitive performance across a wide range ofvisual-language benchmarks at the same model size while maintaining robustperformance on language-centric benchmarks. We have made both 1.3B and 7Bmodels publicly accessible to foster innovations based on this foundationmodel.</p>
                <p>Last Updated: 2024-03-08 18:46:00 UTC</p>
                <button class="interpret-button" data-id="2403.05525v1">Interpret</button>
                <div id="interpretation-2403.05525v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought</h3>
                <p>Authors: James ChuaEdward ReesHunar BatraSamuel R. BowmanJulian MichaelEthan PerezMiles Turpin</p>
                <p><a href="http://arxiv.org/abs/2403.05518v1">Link to paper</a></p>
                <p>While chain-of-thought prompting CoT has the potential to improve theexplainability of language model reasoning it can systematically misrepresentthe factors influencing models behavior--for example rationalizing answers inline with a users opinion without mentioning this bias. To mitigate thisbiased reasoning problem we introduce bias-augmented consistency trainingBCT an unsupervised fine-tuning scheme that trains models to give consistentreasoning across prompts with and without biasing features. We construct asuite testing nine forms of biased reasoning on seven question-answering tasksand find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate ofbiased reasoning by 86 on held-out tasks. Moreover this model generalizes toother forms of bias reducing biased reasoning on held-out biases by an averageof 37. As BCT generalizes to held-out biases and does not require gold labelsthis method may hold promise for reducing biased reasoning from as-of-yetunknown biases and on tasks where supervision for ground truth reasoning isunavailable.</p>
                <p>Last Updated: 2024-03-08 18:41:42 UTC</p>
                <button class="interpret-button" data-id="2403.05518v1">Interpret</button>
                <div id="interpretation-2403.05518v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos</h3>
                <p>Authors: Tarun KalluriBodhisattwa Prasad MajumderManmohan Chandraker</p>
                <p><a href="http://arxiv.org/abs/2403.05535v1">Link to paper</a></p>
                <p>We introduce LaGTran a novel framework that utilizes readily available oreasily acquired text descriptions to guide robust transfer of discriminativeknowledge from labeled source to unlabeled target data with domain shifts.While unsupervised adaptation methods have been established to address thisproblem they show limitations in handling challenging domain shifts due totheir exclusive operation within the pixel-space. Motivated by our observationthat semantically richer text modality has more favorable transfer propertieswe devise a transfer mechanism to use a source-trained text-classifier togenerate predictions on the target text descriptions and utilize thesepredictions as supervision for the corresponding images. Our approach driven bylanguage guidance is surprisingly easy and simple yet significantlyoutperforms all prior approaches on challenging datasets like GeoNet andDomainNet validating its extreme effectiveness. To further extend the scope ofour study beyond images we introduce a new benchmark to study ego-exo transferin videos and find that our language-aided LaGTran yields significant gains inthis highly challenging and non-trivial transfer setting. Code models andproposed datasets are publicly available athttps://tarun005.github.io/lagtran/.</p>
                <p>Last Updated: 2024-03-08 18:58:46 UTC</p>
                <button class="interpret-button" data-id="2403.05535v1">Interpret</button>
                <div id="interpretation-2403.05535v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets</h3>
                <p>Authors: Lorenzo BrigatoStavroula Mougiakakou</p>
                <p><a href="http://arxiv.org/abs/2403.05532v1">Link to paper</a></p>
                <p>We introduce Tune without Validation Twin a pipeline for tuning learningrate and weight decay without validation sets. We leverage a recent theoreticalframework concerning learning phases in hypothesis space to devise a heuristicthat predicts what hyper-parameter HP combinations yield bettergeneralization. Twin performs a grid search of trials according to anearly-/non-early-stopping scheduler and then segments the region that providesthe best results in terms of training loss. Among these trials the weight normstrongly correlates with predicting generalization. To assess the effectivenessof Twin we run extensive experiments on 20 image classification datasets andtrain several families of deep networks including convolutional transformerand feed-forward models. We demonstrate proper HP selection when training fromscratch and fine-tuning emphasizing small-sample scenarios.</p>
                <p>Last Updated: 2024-03-08 18:57:00 UTC</p>
                <button class="interpret-button" data-id="2403.05532v1">Interpret</button>
                <div id="interpretation-2403.05532v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapola</h3>
                <p>Authors: Yijiang LiSucheng RenWeipeng DengYuzhi XuYing GaoEdith NgaiHaohan Wang</p>
                <p><a href="http://arxiv.org/abs/2403.05523v1">Link to paper</a></p>
                <p>Out-of-distribution OOD generalization is a favorable yet challengingproperty for deep neural networks. The core challenges lie in the limitedavailability of source domains that help models learn an invariantrepresentation from the spurious features. Various domain augmentation havebeen proposed but largely rely on interpolating existing domains and frequentlyface difficulties in creating truly novel domains. Humans on the other handcan easily extrapolate novel domains thus an intriguing question arises: Howcan neural networks extrapolate like humans and achieve OOD generalization  We introduce a novel approach to domain extrapolation that leveragesreasoning ability and the extensive knowledge encapsulated within largelanguage models LLMs to synthesize entirely new domains. Starting with theclass of interest we query the LLMs to extract relevant knowledge for thesenovel domains. We then bridge the gap between the text-centric knowledgederived from LLMs and the pixel input space of the model using text-to-imagegeneration techniques. By augmenting the training set of domain generalizationdatasets with high-fidelity photo-realistic images of these new domains weachieve significant improvements over all existing methods as demonstrated inboth single and multi-domain generalization across various benchmarks.  With the ability to extrapolate any domains for any class our method has thepotential to learn a generalized model for any task without any data. Toillustrate we put forth a much more difficult setting termed data-free domaingeneralization that aims to learn a generalized model in the absence of anycollected data. Our empirical findings support the above argument and ourmethods exhibit commendable performance in this setting even surpassing thesupervised setting by approximately 1-2 on datasets such as VLCS.</p>
                <p>Last Updated: 2024-03-08 18:44:23 UTC</p>
                <button class="interpret-button" data-id="2403.05523v1">Interpret</button>
                <div id="interpretation-2403.05523v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Probabilistic Image-Driven Traffic Modeling via Remote Sensing</h3>
                <p>Authors: Scott WorkmanArmin Hadzic</p>
                <p><a href="http://arxiv.org/abs/2403.05521v1">Link to paper</a></p>
                <p>This work addresses the task of modeling spatiotemporal traffic patternsdirectly from overhead imagery which we refer to as image-driven trafficmodeling. We extend this line of work and introduce a multi-modal multi-tasktransformer-based segmentation architecture that can be used to create densecity-scale traffic models. Our approach includes a geo-temporal positionalencoding module for integrating geo-temporal context and a probabilisticobjective function for estimating traffic speeds that naturally models temporalvariations. We evaluate our method extensively using the Dynamic Traffic SpeedsDTS benchmark dataset and significantly improve the state-of-the-art.Finally we introduce the DTS dataset to support mobility-related locationadaptation experiments.</p>
                <p>Last Updated: 2024-03-08 18:43:28 UTC</p>
                <button class="interpret-button" data-id="2403.05521v1">Interpret</button>
                <div id="interpretation-2403.05521v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Poly-View Contrastive Learning</h3>
                <p>Authors: Amitis ShidaniDevon HjelmJason RamapuramRuss WebbEeshan Gunesh DhekaneDan Busbridge</p>
                <p><a href="http://arxiv.org/abs/2403.05490v1">Link to paper</a></p>
                <p>Contrastive learning typically matches pairs of related views among a numberof unrelated negative views. Views can be generated e.g. by augmentations orbe observed. We investigate matching when there are more than two related viewswhich we call poly-view tasks and derive new representation learningobjectives using information maximization and sufficient statistics. We showthat with unlimited computation one should maximize the number of relatedviews and with a fixed compute budget it is beneficial to decrease the numberof unique samples whilst increasing the number of views of those samples. Inparticular poly-view contrastive models trained for 128 epochs with batch size256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1kchallenging the belief that contrastive models require large batch sizes andmany training epochs.</p>
                <p>Last Updated: 2024-03-08 17:55:41 UTC</p>
                <button class="interpret-button" data-id="2403.05490v1">Interpret</button>
                <div id="interpretation-2403.05490v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Engineering consensus in static networks with unknown disruptors</h3>
                <p>Authors: Agathe BouisChristopher LoweRuaridh A. ClarkMalcolm Macdonald</p>
                <p><a href="http://arxiv.org/abs/2403.05272v1">Link to paper</a></p>
                <p>Distributed control increases system scalability flexibility andredundancy. Foundational to such decentralisation is consensus formation bywhich decision-making and coordination are achieved. However decentralisedmulti-agent systems are inherently vulnerable to disruption. To develop aresilient consensus approach inspiration is taken from the study of socialsystems and their dynamics specifically the Deffuant Model. A dynamicalgorithm is presented enabling efficient consensus to be reached with anunknown number of disruptors present within a multi-agent system. By invertingtypical social tolerance agents filter out extremist non-standard opinionsthat would drive them away from consensus. This approach allows distributedsystems to deal with unknown disruptions without knowledge of the networktopology or the numbers and behaviours of the disruptors. A disruptor-agnosticalgorithm is particularly suitable to real-world applications where thisinformation is typically unknown. Faster and tighter convergence can beachieved across a range of scenarios with the social dynamics inspiredalgorithm compared with standard Mean-Subsequence-Reduced-type methods.</p>
                <p>Last Updated: 2024-03-08 12:51:10 UTC</p>
                <button class="interpret-button" data-id="2403.05272v1">Interpret</button>
                <div id="interpretation-2403.05272v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Control-based Graph Embeddings with Data Augmentation for Contrastive Learning</h3>
                <p>Authors: Obaid Ullah AhmadAnwar SaidMudassir ShabbirWaseem AbbasXenofon Koutsoukos</p>
                <p><a href="http://arxiv.org/abs/2403.04923v1">Link to paper</a></p>
                <p>In this paper we study the problem of unsupervised graph representationlearning by harnessing the control properties of dynamical networks defined ongraphs. Our approach introduces a novel framework for contrastive learning awidely prevalent technique for unsupervised representation learning. A crucialstep in contrastive learning is the creation of augmented graphs from theinput graphs. Though different from the original graphs these augmented graphsretain the original graphs structural characteristics. Here we propose aunique method for generating these augmented graphs by leveraging the controlproperties of networks. The core concept revolves around perturbing theoriginal graph to create a new one while preserving the controllabilityproperties specific to networks and graphs. Compared to the existing methodswe demonstrate that this innovative approach enhances the effectiveness ofcontrastive learning frameworks leading to superior results regarding theaccuracy of the classification tasks. The key innovation lies in our ability todecode the network structure using these control properties opening newavenues for unsupervised graph representation learning.</p>
                <p>Last Updated: 2024-03-07 22:14:04 UTC</p>
                <button class="interpret-button" data-id="2403.04923v1">Interpret</button>
                <div id="interpretation-2403.04923v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Stochastic Games for Interactive Manipulation Domains</h3>
                <p>Authors: Karan MuvvalaAndrew M. WellsMorteza LahijanianLydia E. KavrakiMoshe Y. Vardi</p>
                <p><a href="http://arxiv.org/abs/2403.04910v1">Link to paper</a></p>
                <p>As robots become more prevalent the complexity of robot-robot robot-humanand robot-environment interactions increases. In these interactions a robotneeds to consider not only the effects of its own actions but also the effectsof other agents actions and the possible interactions between agents. Previousworks have considered reactive synthesis where the human/environment ismodeled as a deterministic adversarial agent as well as probabilisticsynthesis where the human/environment is modeled via a Markov chain. Whilethey provide strong theoretical frameworks there are still many aspects ofhuman-robot interaction that cannot be fully expressed and many assumptionsthat must be made in each model. In this work we propose stochastic games as ageneral model for human-robot interaction which subsumes the expressivity ofall previous representations. In addition it allows us to make fewer modelingassumptions and leads to more natural and powerful models of interaction. Weintroduce the semantics of this abstraction and show how existing tools can beutilized to synthesize strategies to achieve complex tasks with guarantees.Further we discuss the current computational limitations and improve thescalability by two orders of magnitude by a new way of constructing models forPRISM-games.</p>
                <p>Last Updated: 2024-03-07 21:46:18 UTC</p>
                <button class="interpret-button" data-id="2403.04910v1">Interpret</button>
                <div id="interpretation-2403.04910v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Distributed Multi-objective Optimization in Cyber-Physical Energy Systems</h3>
                <p>Authors: Sanja StarkEmilie FrostMarvin Nebel-Wenner</p>
                <p><a href="http://arxiv.org/abs/2403.04627v1">Link to paper</a></p>
                <p>Managing complex Cyber-Physical Energy Systems CPES requires solvingvarious optimization problems with multiple objectives and constraints. Asdistributed control architectures are becoming more popular in CPES for certaintasks due to their flexibility robustness and privacy protectionmulti-objective optimization must also be distributed. For this purpose wepresent MO-COHDA a fully distributed agent-based algorithm for solvingmulti-objective optimization problems of CPES. MO-COHDA allows an easy andflexible adaptation to different use cases and integration of customfunctionality. To evaluate the effectiveness of MO-COHDA we compare it to acentral NSGA-2 algorithm using multi-objective benchmark functions from the ZDTproblem suite. The results show that MO-COHDA can approximate the referencefront of the benchmark problems well and is suitable for solvingmulti-objective optimization problems. In addition an example use case ofscheduling a group of generation units while optimizing three differentobjectives was evaluated to show how MO-COHDA can be easily applied toreal-world optimization problems in CPES.</p>
                <p>Last Updated: 2024-03-07 16:12:54 UTC</p>
                <button class="interpret-button" data-id="2403.04627v1">Interpret</button>
                <div id="interpretation-2403.04627v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Cooperative Bayesian Optimization for Imperfect Agents</h3>
                <p>Authors: Ali KhoshvishkaiePetrus MikkolaPierre-Alexandre MurenaSamuel Kaski</p>
                <p><a href="http://dx.doi.org/10.1007/978-3-031-43412-9_28">Link to paper</a></p>
                <p>We introduce a cooperative Bayesian optimization problem for optimizingblack-box functions of two variables where two agents choose together at whichpoints to query the function but have only control over one variable each. Thissetting is inspired by human-AI teamwork where an AI-assistant helps its humanuser solve a problem in this simplest case collaborative optimization. Weformulate the solution as sequential decision-making where the agent wecontrol models the user as a computationally rational agent with priorknowledge about the function. We show that strategic planning of the queriesenables better identification of the global maximum of the function as long asthe user avoids excessive exploration. This planning is made possible by usingBayes Adaptive Monte Carlo planning and by endowing the agent with a user modelthat accounts for conservative belief updates and exploratory sampling of thepoints to query.</p>
                <p>Last Updated: 2024-03-07 12:16:51 UTC</p>
                <button class="interpret-button" data-id="2403.04442v1">Interpret</button>
                <div id="interpretation-2403.04442v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets</h3>
                <p>Authors: Lorenzo BrigatoStavroula Mougiakakou</p>
                <p><a href="http://arxiv.org/abs/2403.05532v1">Link to paper</a></p>
                <p>We introduce Tune without Validation Twin a pipeline for tuning learningrate and weight decay without validation sets. We leverage a recent theoreticalframework concerning learning phases in hypothesis space to devise a heuristicthat predicts what hyper-parameter HP combinations yield bettergeneralization. Twin performs a grid search of trials according to anearly-/non-early-stopping scheduler and then segments the region that providesthe best results in terms of training loss. Among these trials the weight normstrongly correlates with predicting generalization. To assess the effectivenessof Twin we run extensive experiments on 20 image classification datasets andtrain several families of deep networks including convolutional transformerand feed-forward models. We demonstrate proper HP selection when training fromscratch and fine-tuning emphasizing small-sample scenarios.</p>
                <p>Last Updated: 2024-03-08 18:57:00 UTC</p>
                <button class="interpret-button" data-id="2403.05532v1">Interpret</button>
                <div id="interpretation-2403.05532v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Computational Complexity of Learning Gaussian Single-Index Models</h3>
                <p>Authors: Alex DamianLoucas Pillaud-VivienJason D. LeeJoan Bruna</p>
                <p><a href="http://arxiv.org/abs/2403.05529v1">Link to paper</a></p>
                <p>Single-Index Models are high-dimensional regression problems with plantedstructure whereby labels depend on an unknown one-dimensional projection ofthe input via a generic non-linear and potentially non-deterministictransformation. As such they encompass a broad class of statistical inferencetasks and provide a rich template to study statistical and computationaltrade-offs in the high-dimensional regime.  While the information-theoretic sample complexity to recover the hiddendirection is linear in the dimension d we show that computationallyefficient algorithms both within the Statistical Query SQ and the Low-DegreePolynomial LDP framework necessarily require Omegadkstar/2samples where kstar is a generative exponent associated with the modelthat we explicitly characterize. Moreover we show that this sample complexityis also sufficient by establishing matching upper bounds using a partial-tracealgorithm. Therefore our results provide evidence of a sharpcomputational-to-statistical gap under both the SQ and LDP class wheneverkstar2. To complete the study we provide examples of smooth and Lipschitzdeterministic target functions with arbitrarily large generative exponentskstar.</p>
                <p>Last Updated: 2024-03-08 18:50:19 UTC</p>
                <button class="interpret-button" data-id="2403.05529v1">Interpret</button>
                <div id="interpretation-2403.05529v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM</h3>
                <p>Authors: Hao KangQingru ZhangSouvik KunduGeonhwa JeongZaoxing LiuTushar KrishnaTuo Zhao</p>
                <p><a href="http://arxiv.org/abs/2403.05527v1">Link to paper</a></p>
                <p>Key-value KV caching has become the de-facto to accelerate generation speedfor large language models LLMs inference. However the growing cache demandwith increasing sequence length has transformed LLM inference to be a memorybound problem significantly constraining the system throughput. Existingmethods rely on dropping unimportant tokens or quantizing all entriesuniformly. Such methods however often incur high approximation errors torepresent the compressed matrices. The autoregressive decoding process furthercompounds the error of each step resulting in critical deviation in modelgeneration and deterioration of performance. To tackle this challenge wepropose GEAR an efficient KV cache compression framework that achievesnear-lossless high-ratio compression. GEAR first applies quantization tomajority of entries of similar magnitudes to ultra-low precision. It thenemploys a low rank matrix to approximate the quantization error and a sparsematrix to remedy individual errors from outlier entries. By adeptly integratingthree techniques GEAR is able to fully exploit their synergistic potentials.Our experiments demonstrate that compared to alternatives GEAR achievesnear-lossless 4-bit KV cache compression with up to 2.38x throughputimprovement while reducing peak-memory size up to 2.29x. Our code is publiclyavailable at https://github.com/HaoKang-Timmy/GEAR.</p>
                <p>Last Updated: 2024-03-08 18:48:30 UTC</p>
                <button class="interpret-button" data-id="2403.05527v1">Interpret</button>
                <div id="interpretation-2403.05527v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Poly-View Contrastive Learning</h3>
                <p>Authors: Amitis ShidaniDevon HjelmJason RamapuramRuss WebbEeshan Gunesh DhekaneDan Busbridge</p>
                <p><a href="http://arxiv.org/abs/2403.05490v1">Link to paper</a></p>
                <p>Contrastive learning typically matches pairs of related views among a numberof unrelated negative views. Views can be generated e.g. by augmentations orbe observed. We investigate matching when there are more than two related viewswhich we call poly-view tasks and derive new representation learningobjectives using information maximization and sufficient statistics. We showthat with unlimited computation one should maximize the number of relatedviews and with a fixed compute budget it is beneficial to decrease the numberof unique samples whilst increasing the number of views of those samples. Inparticular poly-view contrastive models trained for 128 epochs with batch size256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1kchallenging the belief that contrastive models require large batch sizes andmany training epochs.</p>
                <p>Last Updated: 2024-03-08 17:55:41 UTC</p>
                <button class="interpret-button" data-id="2403.05490v1">Interpret</button>
                <div id="interpretation-2403.05490v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference</h3>
                <p>Authors: Akshat RamachandranZishen WanGeonhwa JeongJohn GustafsonTushar Krishna</p>
                <p><a href="http://arxiv.org/abs/2403.05465v1">Link to paper</a></p>
                <p>Traditional Deep Neural Network DNN quantization methods using integerfixed-point or floating-point data types struggle to capture diverse DNNparameter distributions at low precision and often require large siliconoverhead and intensive quantization-aware training. In this study we introduceLogarithmic Posits LP an adaptive hardware-friendly data type inspired byposits that dynamically adapts to DNN weight/activation distributions byparameterizing LP bit fields. We also develop a novel genetic-algorithm basedframework LP Quantization LPQ to find optimal layer-wise LP parameterswhile reducing representational divergence between quantized and full-precisionmodels through a novel global-local contrastive objective. Additionally wedesign a unified mixed-precision LP accelerator LPA architecture comprisingof processing elements PEs incorporating LP in the computational datapath.Our algorithm-hardware co-design demonstrates on average 1 drop in top-1accuracy across various CNN and ViT models. It also achieves  2x improvementsin performance per unit area and 2.2x gains in energy efficiency compared tostate-of-the-art quantization accelerators using different data types.</p>
                <p>Last Updated: 2024-03-08 17:28:49 UTC</p>
                <button class="interpret-button" data-id="2403.05465v1">Interpret</button>
                <div id="interpretation-2403.05465v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos</h3>
                <p>Authors: Tarun KalluriBodhisattwa Prasad MajumderManmohan Chandraker</p>
                <p><a href="http://arxiv.org/abs/2403.05535v1">Link to paper</a></p>
                <p>We introduce LaGTran a novel framework that utilizes readily available oreasily acquired text descriptions to guide robust transfer of discriminativeknowledge from labeled source to unlabeled target data with domain shifts.While unsupervised adaptation methods have been established to address thisproblem they show limitations in handling challenging domain shifts due totheir exclusive operation within the pixel-space. Motivated by our observationthat semantically richer text modality has more favorable transfer propertieswe devise a transfer mechanism to use a source-trained text-classifier togenerate predictions on the target text descriptions and utilize thesepredictions as supervision for the corresponding images. Our approach driven bylanguage guidance is surprisingly easy and simple yet significantlyoutperforms all prior approaches on challenging datasets like GeoNet andDomainNet validating its extreme effectiveness. To further extend the scope ofour study beyond images we introduce a new benchmark to study ego-exo transferin videos and find that our language-aided LaGTran yields significant gains inthis highly challenging and non-trivial transfer setting. Code models andproposed datasets are publicly available athttps://tarun005.github.io/lagtran/.</p>
                <p>Last Updated: 2024-03-08 18:58:46 UTC</p>
                <button class="interpret-button" data-id="2403.05535v1">Interpret</button>
                <div id="interpretation-2403.05535v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Bayesian Preference Elicitation with Language Models</h3>
                <p>Authors: Kunal HandaYarin GalEllie PavlickNoah GoodmanJacob AndreasAlex TamkinBelinda Z. Li</p>
                <p><a href="http://arxiv.org/abs/2403.05534v1">Link to paper</a></p>
                <p>Aligning AI systems to users interests requires understanding andincorporating humans complex values and preferences. Recently language modelsLMs have been used to gather information about the preferences of humanusers. This preference data can be used to fine-tune or guide other LMs and/orAI systems. However LMs have been shown to struggle with crucial aspects ofpreference learning: quantifying uncertainty modeling human mental states andasking informative questions. These challenges have been addressed in otherareas of machine learning such as Bayesian Optimal Experimental Design BOEDwhich focus on designing informative queries within a well-defined featurespace. But these methods in turn are difficult to scale and apply toreal-world problems where simply identifying the relevant features can bedifficult. We introduce OPEN Optimal Preference Elicitation with Naturallanguage a framework that uses BOED to guide the choice of informativequestions and an LM to extract features and translate abstract BOED queriesinto natural language questions. By combining the flexibility of LMs with therigor of BOED OPEN can optimize the informativity of queries while remainingadaptable to real-world domains. In user studies we find that OPEN outperformsexisting LM- and BOED-based methods for preference elicitation.</p>
                <p>Last Updated: 2024-03-08 18:57:52 UTC</p>
                <button class="interpret-button" data-id="2403.05534v1">Interpret</button>
                <div id="interpretation-2403.05534v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</h3>
                <p>Authors: Machel ReidNikolay SavinovDenis TeplyashinDmitry LepikhinTimothy LillicrapJean-baptiste AlayracRadu SoricutAngeliki LazaridouOrhan FiratJulian SchrittwieserIoannis AntonoglouRohan AnilSebastian BorgeaudAndrew DaiKatie MillicanEthan DyerMia GlaeseThibault SottiauxBenjamin LeeFabio ViolaMalcolm ReynoldsYuanzhong XuJames MolloyJilin ChenMichael IsardPaul BarhamTom HenniganRoss McIlroyMelvin JohnsonJohan SchalkwykEli CollinsEliza RutherfordErica MoreiraKareem AyoubMegha GoelClemens MeyerGregory ThorntonZhen YangHenryk MichalewskiZaheer AbbasNathan SchucherAnkesh AnandRichard IvesJames KeelingKarel LencSalem HaykalSiamak ShakeriPranav ShyamAakanksha ChowdheryRoman RingStephen SpencerEren SezenerLuke VilnisOscar ChangNobuyuki MoriokaGeorge TuckerCe ZhengOliver WoodmanNithya AttaluriTomas KociskyEvgenii EltyshevXi ChenTimothy ChungVittorio SeloSiddhartha BrahmaPetko GeorgievAmbrose SloneZhenkai ZhuJames LottesSiyuan QiaoBen CaineSebastian RiedelAlex TomalaMartin ChadwickJuliette LovePeter ChoySid MittalNeil HoulsbyYunhao TangMatthew LammLibin BaiQiao ZhangLuheng HeYong ChengPeter HumphreysYujia LiSergey BrinAlbin CassirerYingjie MiaoLukas ZilkaTaylor TobinKelvin XuLev ProleevDaniel SohnAlberto MagniLisa Anne HendricksIsabel GaoSantiago OntañónOskar BunyanNathan ByrdAbhanshu SharmaBiao ZhangMario PintoRishika SinhaHarsh MehtaDawei JiaSergi CaellesAlbert WebsonAlex MorrisBecca RoelofsYifan DingRobin StrudelXuehan XiongMarvin RitterMostafa DehghaniRahma ChaabouniAbhijit KarmarkarGuangda LaiFabian MentzerBibo XuYaGuang LiYujing ZhangTom Le PaineAlex GoldinBehnam NeyshaburKate BaumliAnselm LevskayaMichael LaskinWenhao JiaJack W. RaeKefan XiaoAntoine HeSkye GiordanoLakshman YagatiJean-Baptiste LespiauPaul NatsevSanjay GanapathyFangyu LiuDanilo MartinsNanxin ChenYunhan XuMegan BarnesRhys MayArpi VezerJunhyuk OhKen FrankoSophie BridgersRuizhe ZhaoBoxi WuBasil MustafaSean SechristEmilio ParisottoThanumalayan Sankaranarayana PillaiChris LarkinChenjie GuChristina SorokinMaxim KrikunAlexey GuseynovJessica LandonRomina DattaAlexander PritzelPhoebe ThackerFan YangKevin HuiAnja HauthChih-Kuan YehDavid BarkerJustin Mao-JonesSophia AustinHannah SheahanParker SchuhJames SvenssonRohan JainVinay RamaseshAnton BriukhovDa-Woon ChungTamara von GlehnChristina ButterfieldPriya JhakraMatthew WiethoffJustin FryeJordan GrimstadBeer ChangpinyoCharline Le LanAnna BortsovaYonghui WuPaul VoigtlaenderTara SainathCharlotte SmithWill HawkinsKris CaoJames BesleySrivatsan SrinivasanMark OmernickColin GaffneyGabriela SuritaRyan BurnellBogdan DamocJunwhan AhnAndrew BrockMantas PajarskasAnastasia PetrushkinaSeb NouryLorenzo BlancoKevin SwerskyArun AhujaThi AvrahamiVedant MisraRaoul de LiedekerkeMariko IinumaAlex PolozovSarah YorkGeorge van den DriesschePaul MichelJustin ChiuRory BlevinsZach GleicherAdrià RecasensAlban RrustemiElena GribovskayaAurko RoyWiktor GworekSéb ArnoldLisa LeeJames Lee-ThorpMarcello MaggioniEnrique PiquerasKartikeya BadolaSharad VikramLucas GonzalezAnirudh BaddepudiEvan SenterJacob DevlinJames QinMichael AzzamMaja TrebaczMartin PolacekKashyap KrishnakumarShuo-yiin ChangMatthew TungIvo PenchevRishabh JoshiKate OlszewskaCarrie MuirMateo WirthAle Jakse HartmanJosh NewlanSheleem KashemVijay BolinaElahe DabirJoost van AmersfoortZafarali AhmedJames Cobon-KerrAishwarya KamathArnar Mar HrafnkelssonLe HouIan MackinnonAlexandre FrechetteEric NolandXiance SiEmanuel TaropaDong LiPhil CroneAnmol GulatiSébastien CeveyJonas AdlerAda MaDavid SilverSimon TokumineRichard PowellStephan LeeMichael ChangSamer HassanDiana MincuAntoine YangNir LevineJenny BrennanMingqiu WangSarah HodkinsonJeffrey ZhaoJosh LipschultzAedan PopeMichael B. ChangCheng LiLaurent El ShafeyMichela PaganiniSholto DouglasBernd BohnetFabio PardoSeth OdoomMihaela RoscaCicero Nogueira dos SantosKedar SoparkarArthur GuezTom HudsonSteven HansenChulayuth AsawaroengchaiRavi AddankiTianhe YuWojciech StokowiecMina KhanJustin GilmerJaehoon LeeCarrie Grimes BostockKeran RongJonathan CatonPedram PejmanFilip PaveticGeoff BrownVivek SharmaMario LučićRajkumar SamuelJosip DjolongaAmol MandhaneLars Lowe SjösundElena BuchatskayaElspeth WhiteNatalie ClayJiepu JiangHyeontaek LimRoss HemsleyJane LabanowskiNicola De CaoDavid SteinerSayed Hadi HashemiJacob AustinAnita GergelyTim BlythJoe StantonKaushik ShivakumarAditya SiddhantAnders AndreassenCarlos ArayaNikhil SethiRakesh ShivannaSteven HandAnkur BapnaAli KhodaeiAntoine MiechGarrett TanzerAndy SwingShantanu ThakoorZhufeng PanZachary NadoStephanie WinklerDian YuMohammad SalehLoren MaggioreIain BarrMinh GiangThais KagoharaIvo DanihelkaAmit MaratheVladimir FeinbergMohamed ElhawatyNimesh GhelaniDan HorganHelen MillerLexi WalkerRichard TanburnMukarram TariqDisha ShrivastavaFei XiaChung-Cheng ChiuZoe AshwoodKhuslen BaatarsukhSina SamangooeiFred AlcoberAxel StjerngrenPaul KomarekKaterina TsihlasAnudhyan BoralRamona ComanescuJeremy ChenRuibo LiuDawn BloxwichCharlie ChenYanhua SunFangxiaoyu FengMatthew MaugerXerxes DotiwallaVincent HellendoornMichael SharmanIvy ZhengKrishna HaridasanGabe Barth-MaronCraig SwansonDominika RogozińskaAlek AndreevPaul Kishan RubensteinRuoxin SangDan HurtGamaleldin ElsayedRenshen WangDave LaceyAnastasija IlićYao ZhaoLora AroyoChimezie IwuanyanwuVitaly NikolaevBalaji LakshminarayananSadegh JazayeriRaphaël Lopez KaufmanMani VaradarajanChetan TekurDoug FritzMisha KhalmanDavid ReitterKingshuk DasguptaShourya SarcarTina OrnduffJavier SnaiderFantine HuotJohnson JiaRupert KempNejc TrdinAnitha VijayakumarLucy KimChristof AngermuellerLi LaoTianqi LiuHaibin ZhangDavid EngelSomer GreeneAnaïs WhiteJessica AustinLilly TaylorShereen AshrafDangyi LiuMaria GeorgakiIrene CaiYana KulizhskayaSonam GoenkaBrennan SaetaKiran VodrahalliChristian FrankDario de CesareBrona RobenekHarry RichardsonMahmoud AlnahlawiChristopher YewPriya PonnapalliMarco TagliasacchiAlex KorchemniyYelin KimDinghua LiBill RosgenZoe AshwoodKyle LevinJeremy WiesnerPraseem BanzalPraveen SrinivasanHongkun YuÇağlar ÜnlüDavid ReidZora TungDaniel FinchelsteinRavin KumarAndre ElisseeffJin HuangMing ZhangRui ZhuRicardo AguilarMai GiménezJiawei XiaOlivier DousseWilli GierkeSoheil Hassas YeganehDamion YatesKomal JalanLu LiEri Latorre-ChimotoDuc Dung NguyenKen DurdenPraveen KallakuriYaxin LiuMatthew JohnsonTomy TsaiAlice TalbertJasmine LiuAlexander NeitzChen ElkindMarco SelviMimi JasarevicLivio Baldini SoaresAlbert CuiPidong WangAlek Wenjiao WangXinyu YeKrystal KallarackalLucia LoherHoi LamJosef BroderDan Holtmann-RiceNina MartinBramandia RamadhanaDaniel ToyamaMrinal ShuklaSujoy BasuAbhi MohanNick FernandoNoah FiedelKim PatersonHui LiAnkush GargJane ParkDongHyun ChoiDiane WuSankalp SinghZhishuai ZhangAmir GlobersonLily YuJohn CarpenterFélix de Chaumont QuitryCarey RadebaughChu-Cheng LinAlex TudorPrakash ShroffDrew GarmonDayou DuNeera VatsHan LuShariq IqbalAlex YakubovichNilesh TripuraneniJames ManyikaHaroon QureshiNan HuaChristel NganiMaria Abi RaadHannah ForbesAnna BulanovaJeff StanwayMukund SundararajanVictor UngureanuColton BishopYunjie LiBalaji VenkatramanBo LiChloe ThorntonSalvatore ScellatoNishesh GuptaYicheng WangIan TenneyXihui WuAshish ShenoyGabriel CarvajalDiana Gage WrightBen BariachZhuyun XiaoPeter HawkinsSid DalmiaClement FarabetPedro ValenzuelaQuan YuanChris WeltyAnanth AgarwalMia ChenWooyeol KimBrice HulseNandita DukkipatiAdam PaszkeAndrew BoltElnaz DavoodiKiam ChooJennifer BeattieJennifer PrendkiHarsha VashishtRebeca Santamaria-FernandezLuis C. CoboJarek WilkiewiczDavid MadrasAli ElqurshGrant UyKevin RamirezMatt HarveyTyler LiechtyHeiga ZenJeff SeibertClara Huiyi HuMohamed ElhawatyAndrey KhorlinMaigo LeAsaf AharoniMegan LiLily WangSandeep KumarAlejandro LinceNorman CasagrandeJay HooverDalia El BadawyDavid SoergelDenis VnukovMatt MiecnikowskiJiri SimsaAnna KoopPraveen KumarThibault SellamDaniel VlasicSamira DarukiNir ShabatJohn ZhangGuolong SuJiageng ZhangJeremiah LiuYi SunEvan PalmerAlireza GhaffarkhahXi XiongVictor CotrutaMichael FinkLucas DixonAshwin SreevatsaAdrian GoedeckemeyerAlek DimitrievMohsen JafariRemi CrockerNicholas FitzGeraldAviral KumarSanjay GhemawatIvan PhilipsFrederick LiuYannie LiangRachel SterneckAlena RepinaMarcus WuLaura KnightMarin GeorgievHyo LeeHarry AskhamAbhishek ChakladarAnnie LouisCarl CrousHardie CateDessie PetrovaMichael QuinnDenese Owusu-AfriyieAchintya SinghalNan WeiSolomon KimDamien VincentMilad NasrChristopher A. Choquette-ChooReiko TojoShawn LuDiego de Las CasasYuchung ChengTolga BolukbasiKatherine LeeSaaber FatehiRajagopal AnanthanarayananMiteyan PatelCharbel KaedJing LiJakub SygnowskiShreyas Rammohan BelleZhe ChenJaclyn KonzelmannSiim PõderRoopal GargVinod KoverkathuAdam BrownChris DyerRosanne LiuAzade NovaJun XuSlav PetrovDemis HassabisKoray KavukcuogluJeffrey DeanOriol Vinyals</p>
                <p><a href="http://arxiv.org/abs/2403.05530v1">Link to paper</a></p>
                <p>In this report we present the latest model of the Gemini family Gemini 1.5Pro a highly compute-efficient multimodal mixture-of-experts model capable ofrecalling and reasoning over fine-grained information from millions of tokensof context including multiple long documents and hours of video and audio.Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasksacross modalities improves the state-of-the-art in long-document QAlong-video QA and long-context ASR and matches or surpasses Gemini 1.0 Ultrasstate-of-the-art performance across a broad set of benchmarks. Studying thelimits of Gemini 1.5 Pros long-context ability we find continued improvementin next-token prediction and near-perfect retrieval 99 up to at least 10Mtokens a generational leap over existing models such as Claude 2.1 200k andGPT-4 Turbo 128k. Finally we highlight surprising new capabilities of largelanguage models at the frontier when given a grammar manual for Kalamang alanguage with fewer than 200 speakers worldwide the model learns to translateEnglish to Kalamang at a similar level to a person who learned from the samecontent.</p>
                <p>Last Updated: 2024-03-08 18:54:20 UTC</p>
                <button class="interpret-button" data-id="2403.05530v1">Interpret</button>
                <div id="interpretation-2403.05530v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM</h3>
                <p>Authors: Hao KangQingru ZhangSouvik KunduGeonhwa JeongZaoxing LiuTushar KrishnaTuo Zhao</p>
                <p><a href="http://arxiv.org/abs/2403.05527v1">Link to paper</a></p>
                <p>Key-value KV caching has become the de-facto to accelerate generation speedfor large language models LLMs inference. However the growing cache demandwith increasing sequence length has transformed LLM inference to be a memorybound problem significantly constraining the system throughput. Existingmethods rely on dropping unimportant tokens or quantizing all entriesuniformly. Such methods however often incur high approximation errors torepresent the compressed matrices. The autoregressive decoding process furthercompounds the error of each step resulting in critical deviation in modelgeneration and deterioration of performance. To tackle this challenge wepropose GEAR an efficient KV cache compression framework that achievesnear-lossless high-ratio compression. GEAR first applies quantization tomajority of entries of similar magnitudes to ultra-low precision. It thenemploys a low rank matrix to approximate the quantization error and a sparsematrix to remedy individual errors from outlier entries. By adeptly integratingthree techniques GEAR is able to fully exploit their synergistic potentials.Our experiments demonstrate that compared to alternatives GEAR achievesnear-lossless 4-bit KV cache compression with up to 2.38x throughputimprovement while reducing peak-memory size up to 2.29x. Our code is publiclyavailable at https://github.com/HaoKang-Timmy/GEAR.</p>
                <p>Last Updated: 2024-03-08 18:48:30 UTC</p>
                <button class="interpret-button" data-id="2403.05527v1">Interpret</button>
                <div id="interpretation-2403.05527v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT</h3>
                <p>Authors: Aisha KhatunAnisur RahmanMd Saiful IslamHemayet Ahmed ChowdhuryAyesha Tasnim</p>
                <p><a href="http://dx.doi.org/10.1145/3530691">Link to paper</a></p>
                <p>Authorship Attribution is the task of creating an appropriatecharacterization of text that captures the authors writing style to identifythe original author of a given piece of text. With increased anonymity on theinternet this task has become increasingly crucial in various security andplagiarism detection fields. Despite significant advancements in otherlanguages such as English Spanish and Chinese Bangla lacks comprehensiveresearch in this field due to its complex linguistic feature and sentencestructure. Moreover existing systems are not scalable when the number ofauthor increases and the performance drops for small number of samples perauthor. In this paper we propose the use of Average-Stochastic GradientDescent Weight-Dropped Long Short-Term Memory AWD-LSTM architecture and aneffective transfer learning approach that addresses the problem of complexlinguistic features extraction and scalability for authorship attribution inBangla Literature AABL. We analyze the effect of different tokenization suchas word sub-word and character level tokenization and demonstrate theeffectiveness of these tokenizations in the proposed model. Moreover weintroduce the publicly available Bangla Authorship Attribution Dataset of 16authors BAAD16 containing 17966 sample texts and 13.4 million words tosolve the standard dataset scarcity problem and release six variations ofpre-trained language models for use in any Bangla NLP downstream task. Forevaluation we used our developed BAAD16 dataset as well as other publiclyavailable datasets. Empirically our proposed model outperformedstate-of-the-art models and achieved 99.8 accuracy in the BAAD16 dataset.Furthermore we showed that the proposed system scales much better even with anincreasing number of authors and performance remains steady despite fewtraining samples.</p>
                <p>Last Updated: 2024-03-08 18:42:59 UTC</p>
                <button class="interpret-button" data-id="2403.05519v1">Interpret</button>
                <div id="interpretation-2403.05519v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Enabling Developers, Protecting Users: Investigating Harassment and Safety in VR</h3>
                <p>Authors: Abhinaya S. B.Aafaq SabirAnupam Das</p>
                <p><a href="http://arxiv.org/abs/2403.05499v1">Link to paper</a></p>
                <p>Virtual Reality VR has witnessed a rising issue of harassment promptingthe integration of safety controls like muting and blocking in VR applications.However the lack of standardized safety measures across VR applicationshinders their universal effectiveness especially across contexts likesocializing gaming and streaming. While prior research has studied safetycontrols in social VR applications our user study n  27 takes amulti-perspective approach examining both users perceptions of safety controlusability and effectiveness as well as the challenges that developers face indesigning and deploying VR safety controls. We identify challenges VR usersface while employing safety controls such as finding users in crowded virtualspaces to block them. VR users also find controls ineffective in addressingharassment for instance they fail to eliminate the harassers presence fromthe environment. Further VR users find the current methods of submittingevidence for reports time-consuming and cumbersome. Improvements desired byusers include live moderation and behavior tracking across VR apps howeverdevelopers cite technological financial and legal obstacles to implementingsuch solutions often due to a lack of awareness and high development costs. Weemphasize the importance of establishing technical and legal guidelines toenhance user safety in virtual environments.</p>
                <p>Last Updated: 2024-03-08 18:15:53 UTC</p>
                <button class="interpret-button" data-id="2403.05499v1">Interpret</button>
                <div id="interpretation-2403.05499v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Comparison of Spatial Visualization Techniques for Radiation in Augmented Reality</h3>
                <p>Authors: Fintan McGeeRoderick McCallJoan Baixauli</p>
                <p><a href="http://dx.doi.org/10.1145/3613904.3642646">Link to paper</a></p>
                <p>Augmented Reality AR provides a safe and low-cost option for hazardoussafety training that allows for the visualization of aspects that may beinvisible such as radiation. Effectively visually communicating such threatsin the environment around the user is not straightforward. This work describesvisually encoding radiation using the spatial awareness mesh of an AR HeadMounted Display. We leverage the AR devices GPUs to develop a real timesolution that accumulates multiple dynamic sources and uses stencils to preventan environment being over saturated with a visualization as well as supportingthe encoding of direction explicitly in the visualization. We perform a userstudy 25 participants of different visualizations and obtain user feedback.Results show that there are complex interactions and while no visualrepresentation was statistically superior or inferior user opinions varywidely. We also discuss the evaluation approaches and provide recommendations.</p>
                <p>Last Updated: 2024-03-08 15:59:26 UTC</p>
                <button class="interpret-button" data-id="2403.05403v1">Interpret</button>
                <div id="interpretation-2403.05403v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WatChat: Explaining perplexing programs by debugging mental models</h3>
                <p>Authors: Kartik ChandraTzu-Mao LiRachit NigamJoshua TenenbaumJonathan Ragan-Kelley</p>
                <p><a href="http://arxiv.org/abs/2403.05334v1">Link to paper</a></p>
                <p>Often a good explanation for a programs unexpected behavior is a bug in theprogrammers code. But sometimes an even better explanation is a bug in theprogrammers mental model of the language they are using. Instead of merelydebugging our current code giving the programmer a fish what if our toolscould directly debug our mental models teaching the programmer to fish Inthis paper we apply ideas from computational cognitive science to do exactlythat. Given a perplexing program we use program synthesis techniques toautomatically infer potential misconceptions that might cause the user to besurprised by the programs behavior. By analyzing these misconceptions weprovide succinct useful explanations of the programs behavior. Our methodscan even be inverted to synthesize pedagogical example programs for diagnosingand correcting misconceptions in students.</p>
                <p>Last Updated: 2024-03-08 14:10:25 UTC</p>
                <button class="interpret-button" data-id="2403.05334v1">Interpret</button>
                <div id="interpretation-2403.05334v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Direction of slip modulates the perception of slip distance and slip speed</h3>
                <p>Authors: Ayesha Tooba KhanDeepak JoshiBiswarup Mukherjee</p>
                <p><a href="http://arxiv.org/abs/2403.05316v1">Link to paper</a></p>
                <p>Purpose: The purpose of this study was to investigate the psychophysicalunderstanding of the slip stimulus. We emphasized that the perception of slipand its characteristics such as slip distance and slip speed depend on theinteraction between slip direction slip distance as well as slip speed.Methods: We developed a novel slip induction device to simulate the artificialsense of slip. We conducted a psychophysical experiment on eight healthysubjects. The experiment was designed to evaluate the effect of slip directionon slip perception as well as on the perception of slip distance and slipspeed. A series of psychophysical questions were asked at the end of the slipstimulation to record the subjective responses of the participants. The averagesuccess rate  was used to quantify the subject responses. Results: Wedemonstrated that the perception of slip is independent of slip directionhowever perception of slip distance and slip speed are significantly modulatedby slip direction. We also observed that a significant interaction existsbetween slip distance and slip speed in the upward slip direction. It was alsoobserved that the average success rate was significantly different for variouscombinations of slip distance and slip speed in the upward slip direction.Conclusions: Our study clearly establishes a significant interaction betweenthe slip direction slip distance and slip speed for psychophysicalunderstanding of the perception of slip distance and slip speed.</p>
                <p>Last Updated: 2024-03-08 13:47:32 UTC</p>
                <button class="interpret-button" data-id="2403.05316v1">Interpret</button>
                <div id="interpretation-2403.05316v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Sparse Wearable Sonomyography Sensor-based Proprioceptive Proportional Control Across Multiple Gestures</h3>
                <p>Authors: Anne Tryphosa KamathamKavita SharmaSrikumar VenkataramanBiswarup Mukherjee</p>
                <p><a href="http://arxiv.org/abs/2403.05308v1">Link to paper</a></p>
                <p>Sonomyography SMG is a non-invasive technique that uses ultrasound imagingto detect the dynamic activity of muscles. Wearable SMG systems have recentlygained popularity due to their potential as human-computer interfaces for theirsuperior performance compared to conventional methods. This paper demonstratesreal-time positional proportional control of multiple gestures using amultiplexed 8-channel wearable SMG system. The amplitude-mode ultrasoundsignals from the SMG system were utilized to detect muscle activity from theforearm of 8 healthy individuals. The derived signals were used to control theon-screen movement of the cursor. A target achievement task was performed toanalyze the performance of our SMG-based human-machine interface. Our wearableSMG system provided accurate stable and intuitive control in real-time byachieving an average success rate greater than 80 with all gestures.Furthermore the wearable SMG systems abilities to detect volitional movementand decode movement kinematic information from SMG trajectories using standardperformance metrics were evaluated. Our results provide insights to validateSMG as an intuitive human-machine interface.</p>
                <p>Last Updated: 2024-03-08 13:38:07 UTC</p>
                <button class="interpret-button" data-id="2403.05308v1">Interpret</button>
                <div id="interpretation-2403.05308v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-03-11</p>
        </div>
    
        </div>
    </body>
    </html>
    