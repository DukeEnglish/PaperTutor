
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</h3>
                <p>Authors: Piotr NawrotAdrian ŁańcuckiMarcin ChochowskiDavid TarjanEdoardo M. Ponti</p>
                <p><a href="http://arxiv.org/abs/2403.09636v1">Link to paper</a></p>
                <p>Transformers have emerged as the backbone of large language models LLMs.However generation remains inefficient due to the need to store in memory acache of key-value representations for past tokens whose size scales linearlywith the input sequence length and batch size. As a solution we proposeDynamic Memory Compression DMC a method for on-line key-value cachecompression at inference time. Most importantly the model learns to applydifferent compression rates in different heads and layers. We retrofitpre-trained LLMs such as Llama 2 7B 13B and 70B into DMC Transformersachieving up to 3.7x throughput increase in auto-regressive inference on aNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligiblepercentage of the original data without adding any extra parameters. We findthat DMC preserves the original downstream performance with up to 4x cachecompression outperforming up-trained grouped-query attention GQA. GQA andDMC can be even combined to obtain compounded gains. As a result DMC fitslonger contexts and larger batches within any given memory budget.</p>
                <p>Last Updated: 2024-03-14 17:59:26 UTC</p>
                <button class="interpret-button" data-id="2403.09636v1">Interpret</button>
                <div id="interpretation-2403.09636v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models</h3>
                <p>Authors: Akhil KediaMohd Abbas ZaidiSushil KhyaliaJungho JungHarshith GokaHaejun Lee</p>
                <p><a href="http://arxiv.org/abs/2403.09635v1">Link to paper</a></p>
                <p>In spite of their huge success transformer models remain difficult to scalein depth. In this work we develop a unified signal propagation theory andprovide formulae that govern the moments of the forward and backward signalthrough the transformer model. Our framework can be used to understand andmitigate vanishing/exploding gradients rank collapse and instabilityassociated with high attention scores. We also propose DeepScaleLM aninitialization and scaling scheme that conserves unit output/gradient momentsthroughout the model enabling the training of very deep models with 100s oflayers. We find that transformer models could be much deeper - our deep modelswith fewer parameters outperform shallow models in Language Modeling SpeechTranslation and Image Classification across Encoder-only Decoder-only andEncoder-Decoder variants for both Pre-LN and Post-LN transformers formultiple datasets and model sizes. These improvements also translate intoimproved performance on downstream Question Answering tasks and improvedrobustness for image classification.</p>
                <p>Last Updated: 2024-03-14 17:59:14 UTC</p>
                <button class="interpret-button" data-id="2403.09635v1">Interpret</button>
                <div id="interpretation-2403.09635v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>3D-VLA: A 3D Vision-Language-Action Generative World Model</h3>
                <p>Authors: Haoyu ZhenXiaowen QiuPeihao ChenJincheng YangXin YanYilun DuYining HongChuang Gan</p>
                <p><a href="http://arxiv.org/abs/2403.09631v1">Link to paper</a></p>
                <p>Recent vision-language-action VLA models rely on 2D inputs lackingintegration with the broader realm of the 3D physical world. Furthermore theyperform action prediction by learning a direct mapping from perception toaction neglecting the vast dynamics of the world and the relations betweenactions and dynamics. In contrast human beings are endowed with world modelsthat depict imagination about future scenarios to plan actions accordingly. Tothis end we propose 3D-VLA by introducing a new family of embodied foundationmodels that seamlessly link 3D perception reasoning and action through agenerative world model. Specifically 3D-VLA is built on top of a 3D-basedlarge language model LLM and a set of interaction tokens is introduced toengage with the embodied environment. Furthermore to inject generationabilities into the model we train a series of embodied diffusion models andalign them into the LLM for predicting the goal images and point clouds. Totrain our 3D-VLA we curate a large-scale 3D embodied instruction dataset byextracting vast 3D-related information from existing robotics datasets. Ourexperiments on held-in datasets demonstrate that 3D-VLA significantly improvesthe reasoning multimodal generation and planning capabilities in embodiedenvironments showcasing its potential in real-world applications.</p>
                <p>Last Updated: 2024-03-14 17:58:41 UTC</p>
                <button class="interpret-button" data-id="2403.09631v1">Interpret</button>
                <div id="interpretation-2403.09631v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</h3>
                <p>Authors: Eric ZelikmanGeorges HarikYijia ShaoVaruna JayasiriNick HaberNoah D. Goodman</p>
                <p><a href="http://arxiv.org/abs/2403.09629v1">Link to paper</a></p>
                <p>When writing and talking people sometimes pause to think. Althoughreasoning-focused works have often framed reasoning as a method of answeringquestions or completing agentic tasks reasoning is implicit in almost allwritten text. For example this applies to the steps not stated between thelines of a proof or to the theory of mind underlying a conversation. In theSelf-Taught Reasoner STaR Zelikman et al. 2022 useful thinking is learnedby inferring rationales from few-shot examples in question-answering andlearning from those that lead to a correct answer. This is a highly constrainedsetting -- ideally a language model could instead learn to infer unstatedrationales in arbitrary text. We present Quiet-STaR a generalization of STaRin which LMs learn to generate rationales at each token to explain future textimproving their predictions. We address key challenges including 1 thecomputational cost of generating continuations 2 the fact that the LM doesnot initially know how to generate or use internal thoughts and 3 the need topredict beyond individual next tokens. To resolve these we propose a tokenwiseparallel sampling algorithm using learnable tokens indicating a thoughtsstart and end and an extended teacher-forcing technique. Encouraginglygenerated rationales disproportionately help model difficult-to-predict tokensand improve the LMs ability to directly answer difficult questions. Inparticular after continued pretraining of an LM on a corpus of internet textwith Quiet-STaR we find zero-shot improvements on GSM8K5.9rightarrow10.9 and CommonsenseQA 36.3rightarrow47.2 andobserve a perplexity improvement of difficult tokens in natural text.Crucially these improvements require no fine-tuning on these tasks. Quiet-STaRmarks a step towards LMs that can learn to reason in a more general andscalable way.</p>
                <p>Last Updated: 2024-03-14 17:58:16 UTC</p>
                <button class="interpret-button" data-id="2403.09629v1">Interpret</button>
                <div id="interpretation-2403.09629v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training</h3>
                <p>Authors: Yanlai YangMatt JonesMichael C. MozerMengye Ren</p>
                <p><a href="http://arxiv.org/abs/2403.09613v1">Link to paper</a></p>
                <p>We explore the training dynamics of neural networks in a structured non-IIDsetting where documents are presented cyclically in a fixed repeated sequence.Typically networks suffer from catastrophic interference when training on asequence of documents however we discover a curious and remarkable propertyof LLMs fine-tuned sequentially in this setting: they exhibit anticipatorybehavior recovering from the forgetting on documents before encountering themagain. The behavior emerges and becomes more robust as the architecture scalesup its number of parameters. Through comprehensive experiments andvisualizations we uncover new insights into training over-parameterizednetworks in structured environments.</p>
                <p>Last Updated: 2024-03-14 17:51:54 UTC</p>
                <button class="interpret-button" data-id="2403.09613v1">Interpret</button>
                <div id="interpretation-2403.09613v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation</h3>
                <p>Authors: Yuhan GuoHanning ShaoCan LiuKai XuXiaoru Yuan</p>
                <p><a href="http://arxiv.org/abs/2403.09615v1">Link to paper</a></p>
                <p>Generative text-to-image models which allow users to create appealing imagesthrough a text prompt have seen a dramatic increase in popularity in recentyears. However most users have a limited understanding of how such models workand it often requires many trials and errors to achieve satisfactory results.The prompt history contains a wealth of information that could provide userswith insights into what have been explored and how the prompt changes impactthe output image yet little research attention has been paid to the visualanalysis of such process to support users. We propose the Image Variant Grapha novel visual representation designed to support comparing prompt-image pairsand exploring the editing history. The Image Variant Graph models promptdifferences as edges between corresponding images and presents the distancesbetween images through projection. Based on the graph we developed thePrompTHis system through co-design with artists. Besides Image Variant GraphPrompTHis also incorporates a detailed prompt-image history and a navigationmini-map. Based on the review and analysis of the prompting history users canbetter understand the impact of prompt changes and have a more effectivecontrol of image generation. A quantitative user study with eleven amateurparticipants and qualitative interviews with five professionals and one amateuruser were conducted to evaluate the effectiveness of PrompTHis. The resultsdemonstrate PrompTHis can help users review the prompt history make sense ofthe model and plan their creative process.</p>
                <p>Last Updated: 2024-03-14 17:52:17 UTC</p>
                <button class="interpret-button" data-id="2403.09615v1">Interpret</button>
                <div id="interpretation-2403.09615v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication</h3>
                <p>Authors: Evgeny StemasovSimon DemharterMax RädlerJan GugenheimerEnrico Rukzio</p>
                <p><a href="http://dx.doi.org/10.1145/3613904.3642083">Link to paper</a></p>
                <p>Extended Reality XR allows in-situ previewing of designs to be manufacturedthrough Personal Fabrication PF. These in-situ interactions exhibitadvantages for PF like incorporating the environment into the design process.However design-for-fabrication in XR often happens through either highlycomplex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourcedmodels. We present pARam a tool combining parametric designs PDs and XRenabling in-situ configuration of artifacts for PF. In contrast to modeling- orsearch-focused approaches pARam supports customization through embodied andpractical inputs e.g. gestures recommendations and evaluation e.g.lighting estimation without demanding complex 3D-modeling skills. Weimplemented pARam for HoloLens 2 and evaluated it n20 comparing XR anddesktop conditions. Users succeeded in choosing context-related parameters andtook their environment into account for their configuration using pARam. Wereflect on the prospects and challenges of PDs in XR to streamline complexdesign methods for PF while retaining suitable expressivity.</p>
                <p>Last Updated: 2024-03-14 17:48:30 UTC</p>
                <button class="interpret-button" data-id="2403.09607v1">Interpret</button>
                <div id="interpretation-2403.09607v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DungeonMaker: Embedding Tangible Creation and Destruction in Hybrid Board Games through Personal Fabrication Technology</h3>
                <p>Authors: Evgeny StemasovTobias WagnerAli AskariJessica JanekOmid RajabiAnja SchikorrJulian FrommelJan GugenheimerEnrico Rukzio</p>
                <p><a href="http://dx.doi.org/10.1145/3613904.3642243">Link to paper</a></p>
                <p>Hybrid board games HBGs augment their analog origins digitally e.g.through apps and are an increasingly popular pastime activity. Continuousworld and character development and customization known to facilitateengagement in video games remain rare in HBGs. If present they happendigitally or imaginarily often leaving physical aspects generic. We developedDungeonMaker a fabrication-augmented HBG bridging physical and digital gameelements: 1 the setup narrates a story and projects a digital game board ontoa laser cutter 2 DungeonMaker assesses player-crafted artifacts 3DungeonMakers modified laser head senses and moves player- and non-playerfigures and 4 can physically damage figures. An evaluation n4x3 indicatedthat DungeonMaker provides an engaging experience may support playersconnection to their figures and potentially spark novices interest infabrication. DungeonMaker provides a rich constellation to play HBGs byblending aspects of craft and automation to couple the physical and digitalelements of an HBG tightly.</p>
                <p>Last Updated: 2024-03-14 17:35:19 UTC</p>
                <button class="interpret-button" data-id="2403.09592v1">Interpret</button>
                <div id="interpretation-2403.09592v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>"Are You Really Sure?" Understanding the Effects of Human Self-Confidence Calibration in AI-Assisted Decision Making</h3>
                <p>Authors: Shuai MaXinru WangYing LeiChuhan ShiMing YinXiaojuan Ma</p>
                <p><a href="http://arxiv.org/abs/2403.09552v1">Link to paper</a></p>
                <p>In AI-assisted decision-making it is crucial but challenging for humans toachieve appropriate reliance on AI. This paper approaches this problem from ahuman-centered perspective human self-confidence calibration. We begin byproposing an analytical framework to highlight the importance of calibratedhuman self-confidence. In our first study we explore the relationship betweenhuman self-confidence appropriateness and reliance appropriateness. Then in oursecond study We propose three calibration mechanisms and compare their effectson humans self-confidence and user experience. Subsequently our third studyinvestigates the effects of self-confidence calibration on AI-assisteddecision-making. Results show that calibrating human self-confidence enhanceshuman-AI team performance and encourages more rational reliance on AI in someaspects compared to uncalibrated baselines. Finally we discuss our mainfindings and provide implications for designing future AI-assisteddecision-making interfaces.</p>
                <p>Last Updated: 2024-03-14 16:39:23 UTC</p>
                <button class="interpret-button" data-id="2403.09552v1">Interpret</button>
                <div id="interpretation-2403.09552v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Effect of external characteristics of a virtual human being during the use of a computer-assisted therapy tool</h3>
                <p>Authors: Navid AshrafiVanessa NeuhausFrancesco VonaNicolina Laura PeperkornYoussef ShibanJan-Niklas Voigt-Antons</p>
                <p><a href="http://arxiv.org/abs/2403.09544v1">Link to paper</a></p>
                <p>Identification within media whether with real or fictional characterssignificantly impacts users shaping their behavior and enriching their socialand emotional experiences. Immersive media like video games utilize virtualentities such as agents avatars or NPCs to connect users with virtual worldsfostering a heightened sense of immersion and identification. Howeverchallenges arise in visually representing these entities with design decisionscrucial for enhancing user interaction. Recent research highlights thepotential of user-defined design or customization which goes beyond merevisual resemblance to the user. Understanding how identification with virtualavatars influences user experiences especially in psychological interventionsis pivotal. In a study exploring this 22 participants created virtual agentseither similar or dissimilar to themselves which then addressed theirdysfunctional thoughts. Results indicate that similarity between users andvirtual agents not only boosts identification but also positively impactsemotions and motivation enhancing interest and enjoyment. This study shedslight on the significance of customization and identification particularly incomputer-assisted therapy tools underscoring the importance of visual designfor optimizing user experiences.</p>
                <p>Last Updated: 2024-03-14 16:31:18 UTC</p>
                <button class="interpret-button" data-id="2403.09544v1">Interpret</button>
                <div id="interpretation-2403.09544v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning</h3>
                <p>Authors: Zhishuai LiuPan Xu</p>
                <p><a href="http://arxiv.org/abs/2403.09621v1">Link to paper</a></p>
                <p>Distributionally robust offline reinforcement learning RL which seeksrobust policy training against environment perturbation by modeling dynamicsuncertainty calls for function approximations when facing large state-actionspaces. However the consideration of dynamics uncertainty introduces essentialnonlinearity and computational burden posing unique challenges for analyzingand practically employing function approximation. Focusing on a basic settingwhere the nominal model and perturbed models are linearly parameterized wepropose minimax optimal and computationally efficient algorithms realizingfunction approximation and initiate the study on instance-dependentsuboptimality analysis in the context of robust offline RL. Our results uncoverthat function approximation in robust offline RL is essentially distinct fromand probably harder than that in standard offline RL. Our algorithms andtheoretical results crucially depend on a variety of new techniques involvinga novel function approximation mechanism incorporating variance information anew procedure of suboptimality and estimation uncertainty decomposition aquantification of the robust value function shrinkage and a meticulouslydesigned family of hard instances which might be of independent interest.</p>
                <p>Last Updated: 2024-03-14 17:55:10 UTC</p>
                <button class="interpret-button" data-id="2403.09621v1">Interpret</button>
                <div id="interpretation-2403.09621v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Extremal graphical modeling with latent variables</h3>
                <p>Authors: Sebastian EngelkeArmeen Taeb</p>
                <p><a href="http://arxiv.org/abs/2403.09604v1">Link to paper</a></p>
                <p>Extremal graphical models encode the conditional independence structure ofmultivariate extremes and provide a powerful tool for quantifying the risk ofrare events. Prior work on learning these graphs from data has focused on thesetting where all relevant variables are observed. For the popular class ofHusler-Reiss models we propose the texttteglatent method a tractableconvex program for learning extremal graphical models in the presence of latentvariables. Our approach decomposes the Husler-Reiss precision matrix into asparse component encoding the graphical structure among the observed variablesafter conditioning on the latent variables and a low-rank component encodingthe effect of a few latent variables on the observed variables. We providefinite-sample guarantees of texttteglatent and show that it consistentlyrecovers the conditional graph as well as the number of latent variables. Wehighlight the improved performances of our approach on synthetic and real data.</p>
                <p>Last Updated: 2024-03-14 17:45:24 UTC</p>
                <button class="interpret-button" data-id="2403.09604v1">Interpret</button>
                <div id="interpretation-2403.09604v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Variational Inference with Sequential Sample-Average Approximations</h3>
                <p>Authors: Heiko ZimmermannChristian A. NaessethJan-Willem van de Meent</p>
                <p><a href="http://arxiv.org/abs/2403.09429v1">Link to paper</a></p>
                <p>We present variational inference with sequential sample-average approximationVISA a method for approximate inference in computationally intensive modelssuch as those based on numerical simulations. VISA extends importance-weightedforward-KL variational inference by employing a sequence of sample-averageapproximations which are considered valid inside a trust region. This makes itpossible to reuse model evaluations across multiple gradient steps therebyreducing computational cost. We perform experiments on high-dimensionalGaussians Lotka-Volterra dynamics and a Pickover attractor which demonstratethat VISA can achieve comparable approximation accuracy to standardimportance-weighted forward-KL variational inference with computational savingsof a factor two or more for conservatively chosen learning rates.</p>
                <p>Last Updated: 2024-03-14 14:20:22 UTC</p>
                <button class="interpret-button" data-id="2403.09429v1">Interpret</button>
                <div id="interpretation-2403.09429v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Scalability of Metropolis-within-Gibbs schemes for high-dimensional Bayesian models</h3>
                <p>Authors: Filippo AscolaniGareth O. RobertsGiacomo Zanella</p>
                <p><a href="http://arxiv.org/abs/2403.09416v1">Link to paper</a></p>
                <p>We study general coordinate-wise MCMC schemes such asMetropolis-within-Gibbs samplers which are commonly used to fit Bayesiannon-conjugate hierarchical models. We relate their convergence properties tothe ones of the corresponding potentially not implementable Gibbs samplerthrough the notion of conditional conductance. This allows us to study theperformances of popular Metropolis-within-Gibbs schemes for non-conjugatehierarchical models in high-dimensional regimes where both number ofdatapoints and parameters increase. Given random data-generating assumptionswe establish dimension-free convergence results which are in close accordancewith numerical evidences. Applications to Bayesian models for binary regressionwith unknown hyperparameters and discretely observed diffusions are alsodiscussed. Motivated by such statistical applications auxiliary results ofindependent interest on approximate conductances and perturbation of Markovoperators are provided.</p>
                <p>Last Updated: 2024-03-14 14:04:44 UTC</p>
                <button class="interpret-button" data-id="2403.09416v1">Interpret</button>
                <div id="interpretation-2403.09416v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Pantypes: Diverse Representatives for Self-Explainable Models</h3>
                <p>Authors: Rune KjærsgaardAhcène BoubekkiLine Clemmensen</p>
                <p><a href="http://arxiv.org/abs/2403.09383v1">Link to paper</a></p>
                <p>Prototypical self-explainable classifiers have emerged to meet the growingdemand for interpretable AI systems. These classifiers are designed toincorporate high transparency in their decisions by basing inference onsimilarity with learned prototypical objects. While these models are designedwith diversity in mind the learned prototypes often do not sufficientlyrepresent all aspects of the input distribution particularly those in lowdensity regions. Such lack of sufficient data representation known asrepresentation bias has been associated with various detrimental propertiesrelated to machine learning diversity and fairness. In light of this weintroduce pantypes a new family of prototypical objects designed to capturethe full diversity of the input distribution through a sparse set of objects.We show that pantypes can empower prototypical self-explainable models byoccupying divergent regions of the latent space and thus fostering highdiversity interpretability and fairness.</p>
                <p>Last Updated: 2024-03-14 13:34:30 UTC</p>
                <button class="interpret-button" data-id="2403.09383v1">Interpret</button>
                <div id="interpretation-2403.09383v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models</h3>
                <p>Authors: Akhil KediaMohd Abbas ZaidiSushil KhyaliaJungho JungHarshith GokaHaejun Lee</p>
                <p><a href="http://arxiv.org/abs/2403.09635v1">Link to paper</a></p>
                <p>In spite of their huge success transformer models remain difficult to scalein depth. In this work we develop a unified signal propagation theory andprovide formulae that govern the moments of the forward and backward signalthrough the transformer model. Our framework can be used to understand andmitigate vanishing/exploding gradients rank collapse and instabilityassociated with high attention scores. We also propose DeepScaleLM aninitialization and scaling scheme that conserves unit output/gradient momentsthroughout the model enabling the training of very deep models with 100s oflayers. We find that transformer models could be much deeper - our deep modelswith fewer parameters outperform shallow models in Language Modeling SpeechTranslation and Image Classification across Encoder-only Decoder-only andEncoder-Decoder variants for both Pre-LN and Post-LN transformers formultiple datasets and model sizes. These improvements also translate intoimproved performance on downstream Question Answering tasks and improvedrobustness for image classification.</p>
                <p>Last Updated: 2024-03-14 17:59:14 UTC</p>
                <button class="interpret-button" data-id="2403.09635v1">Interpret</button>
                <div id="interpretation-2403.09635v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>3D-VLA: A 3D Vision-Language-Action Generative World Model</h3>
                <p>Authors: Haoyu ZhenXiaowen QiuPeihao ChenJincheng YangXin YanYilun DuYining HongChuang Gan</p>
                <p><a href="http://arxiv.org/abs/2403.09631v1">Link to paper</a></p>
                <p>Recent vision-language-action VLA models rely on 2D inputs lackingintegration with the broader realm of the 3D physical world. Furthermore theyperform action prediction by learning a direct mapping from perception toaction neglecting the vast dynamics of the world and the relations betweenactions and dynamics. In contrast human beings are endowed with world modelsthat depict imagination about future scenarios to plan actions accordingly. Tothis end we propose 3D-VLA by introducing a new family of embodied foundationmodels that seamlessly link 3D perception reasoning and action through agenerative world model. Specifically 3D-VLA is built on top of a 3D-basedlarge language model LLM and a set of interaction tokens is introduced toengage with the embodied environment. Furthermore to inject generationabilities into the model we train a series of embodied diffusion models andalign them into the LLM for predicting the goal images and point clouds. Totrain our 3D-VLA we curate a large-scale 3D embodied instruction dataset byextracting vast 3D-related information from existing robotics datasets. Ourexperiments on held-in datasets demonstrate that 3D-VLA significantly improvesthe reasoning multimodal generation and planning capabilities in embodiedenvironments showcasing its potential in real-world applications.</p>
                <p>Last Updated: 2024-03-14 17:58:41 UTC</p>
                <button class="interpret-button" data-id="2403.09631v1">Interpret</button>
                <div id="interpretation-2403.09631v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</h3>
                <p>Authors: Eric ZelikmanGeorges HarikYijia ShaoVaruna JayasiriNick HaberNoah D. Goodman</p>
                <p><a href="http://arxiv.org/abs/2403.09629v1">Link to paper</a></p>
                <p>When writing and talking people sometimes pause to think. Althoughreasoning-focused works have often framed reasoning as a method of answeringquestions or completing agentic tasks reasoning is implicit in almost allwritten text. For example this applies to the steps not stated between thelines of a proof or to the theory of mind underlying a conversation. In theSelf-Taught Reasoner STaR Zelikman et al. 2022 useful thinking is learnedby inferring rationales from few-shot examples in question-answering andlearning from those that lead to a correct answer. This is a highly constrainedsetting -- ideally a language model could instead learn to infer unstatedrationales in arbitrary text. We present Quiet-STaR a generalization of STaRin which LMs learn to generate rationales at each token to explain future textimproving their predictions. We address key challenges including 1 thecomputational cost of generating continuations 2 the fact that the LM doesnot initially know how to generate or use internal thoughts and 3 the need topredict beyond individual next tokens. To resolve these we propose a tokenwiseparallel sampling algorithm using learnable tokens indicating a thoughtsstart and end and an extended teacher-forcing technique. Encouraginglygenerated rationales disproportionately help model difficult-to-predict tokensand improve the LMs ability to directly answer difficult questions. Inparticular after continued pretraining of an LM on a corpus of internet textwith Quiet-STaR we find zero-shot improvements on GSM8K5.9rightarrow10.9 and CommonsenseQA 36.3rightarrow47.2 andobserve a perplexity improvement of difficult tokens in natural text.Crucially these improvements require no fine-tuning on these tasks. Quiet-STaRmarks a step towards LMs that can learn to reason in a more general andscalable way.</p>
                <p>Last Updated: 2024-03-14 17:58:16 UTC</p>
                <button class="interpret-button" data-id="2403.09629v1">Interpret</button>
                <div id="interpretation-2403.09629v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning</h3>
                <p>Authors: Zhishuai LiuPan Xu</p>
                <p><a href="http://arxiv.org/abs/2403.09621v1">Link to paper</a></p>
                <p>Distributionally robust offline reinforcement learning RL which seeksrobust policy training against environment perturbation by modeling dynamicsuncertainty calls for function approximations when facing large state-actionspaces. However the consideration of dynamics uncertainty introduces essentialnonlinearity and computational burden posing unique challenges for analyzingand practically employing function approximation. Focusing on a basic settingwhere the nominal model and perturbed models are linearly parameterized wepropose minimax optimal and computationally efficient algorithms realizingfunction approximation and initiate the study on instance-dependentsuboptimality analysis in the context of robust offline RL. Our results uncoverthat function approximation in robust offline RL is essentially distinct fromand probably harder than that in standard offline RL. Our algorithms andtheoretical results crucially depend on a variety of new techniques involvinga novel function approximation mechanism incorporating variance information anew procedure of suboptimality and estimation uncertainty decomposition aquantification of the robust value function shrinkage and a meticulouslydesigned family of hard instances which might be of independent interest.</p>
                <p>Last Updated: 2024-03-14 17:55:10 UTC</p>
                <button class="interpret-button" data-id="2403.09621v1">Interpret</button>
                <div id="interpretation-2403.09621v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey</h3>
                <p>Authors: Xiaoyu LiuPaiheng XuJunda WuJiaxin YuanYifan YangYuhang ZhouFuxiao LiuTianrui GuanHaoliang WangTong YuJulian McAuleyWei AiFurong Huang</p>
                <p><a href="http://arxiv.org/abs/2403.09606v1">Link to paper</a></p>
                <p>Causal inference has shown potential in enhancing the predictive accuracyfairness robustness and explainability of Natural Language Processing NLPmodels by capturing causal relationships among variables. The emergence ofgenerative Large Language Models LLMs has significantly impacted various NLPdomains particularly through their advanced reasoning capabilities. Thissurvey focuses on evaluating and improving LLMs from a causal view in thefollowing areas: understanding and improving the LLMs reasoning capacityaddressing fairness and safety issues in LLMs complementing LLMs withexplanations and handling multimodality. Meanwhile LLMs strong reasoningcapacities can in turn contribute to the field of causal inference by aidingcausal relationship discovery and causal effect estimations. This reviewexplores the interplay between causal inference frameworks and LLMs from bothperspectives emphasizing their collective potential to further the developmentof more advanced and equitable artificial intelligence systems.</p>
                <p>Last Updated: 2024-03-14 17:47:20 UTC</p>
                <button class="interpret-button" data-id="2403.09606v1">Interpret</button>
                <div id="interpretation-2403.09606v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models</h3>
                <p>Authors: Akhil KediaMohd Abbas ZaidiSushil KhyaliaJungho JungHarshith GokaHaejun Lee</p>
                <p><a href="http://arxiv.org/abs/2403.09635v1">Link to paper</a></p>
                <p>In spite of their huge success transformer models remain difficult to scalein depth. In this work we develop a unified signal propagation theory andprovide formulae that govern the moments of the forward and backward signalthrough the transformer model. Our framework can be used to understand andmitigate vanishing/exploding gradients rank collapse and instabilityassociated with high attention scores. We also propose DeepScaleLM aninitialization and scaling scheme that conserves unit output/gradient momentsthroughout the model enabling the training of very deep models with 100s oflayers. We find that transformer models could be much deeper - our deep modelswith fewer parameters outperform shallow models in Language Modeling SpeechTranslation and Image Classification across Encoder-only Decoder-only andEncoder-Decoder variants for both Pre-LN and Post-LN transformers formultiple datasets and model sizes. These improvements also translate intoimproved performance on downstream Question Answering tasks and improvedrobustness for image classification.</p>
                <p>Last Updated: 2024-03-14 17:59:14 UTC</p>
                <button class="interpret-button" data-id="2403.09635v1">Interpret</button>
                <div id="interpretation-2403.09635v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</h3>
                <p>Authors: Eric ZelikmanGeorges HarikYijia ShaoVaruna JayasiriNick HaberNoah D. Goodman</p>
                <p><a href="http://arxiv.org/abs/2403.09629v1">Link to paper</a></p>
                <p>When writing and talking people sometimes pause to think. Althoughreasoning-focused works have often framed reasoning as a method of answeringquestions or completing agentic tasks reasoning is implicit in almost allwritten text. For example this applies to the steps not stated between thelines of a proof or to the theory of mind underlying a conversation. In theSelf-Taught Reasoner STaR Zelikman et al. 2022 useful thinking is learnedby inferring rationales from few-shot examples in question-answering andlearning from those that lead to a correct answer. This is a highly constrainedsetting -- ideally a language model could instead learn to infer unstatedrationales in arbitrary text. We present Quiet-STaR a generalization of STaRin which LMs learn to generate rationales at each token to explain future textimproving their predictions. We address key challenges including 1 thecomputational cost of generating continuations 2 the fact that the LM doesnot initially know how to generate or use internal thoughts and 3 the need topredict beyond individual next tokens. To resolve these we propose a tokenwiseparallel sampling algorithm using learnable tokens indicating a thoughtsstart and end and an extended teacher-forcing technique. Encouraginglygenerated rationales disproportionately help model difficult-to-predict tokensand improve the LMs ability to directly answer difficult questions. Inparticular after continued pretraining of an LM on a corpus of internet textwith Quiet-STaR we find zero-shot improvements on GSM8K5.9rightarrow10.9 and CommonsenseQA 36.3rightarrow47.2 andobserve a perplexity improvement of difficult tokens in natural text.Crucially these improvements require no fine-tuning on these tasks. Quiet-STaRmarks a step towards LMs that can learn to reason in a more general andscalable way.</p>
                <p>Last Updated: 2024-03-14 17:58:16 UTC</p>
                <button class="interpret-button" data-id="2403.09629v1">Interpret</button>
                <div id="interpretation-2403.09629v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation</h3>
                <p>Authors: Fangfu LiuHanyang WangWeiliang ChenHaowen SunYueqi Duan</p>
                <p><a href="http://arxiv.org/abs/2403.09625v1">Link to paper</a></p>
                <p>Recent years have witnessed the strong power of 3D generation models whichoffer a new level of creative flexibility by allowing users to guide the 3Dcontent generation process through a single image or natural language. Howeverit remains challenging for existing 3D generation methods to createsubject-driven 3D content across diverse prompts. In this paper we introduce anovel 3D customization method dubbed Make-Your-3D that can personalizehigh-fidelity and consistent 3D content from only a single image of a subjectwith text description within 5 minutes. Our key insight is to harmonize thedistributions of a multi-view diffusion model and an identity-specific 2Dgenerative model aligning them with the distribution of the desired 3Dsubject. Specifically we design a co-evolution framework to reduce thevariance of distributions where each model undergoes a process of learningfrom the other through identity-aware optimization and subject-prioroptimization respectively. Extensive experiments demonstrate that our methodcan produce high-quality consistent and subject-specific 3D content withtext-driven modifications that are unseen in subject image.</p>
                <p>Last Updated: 2024-03-14 17:57:04 UTC</p>
                <button class="interpret-button" data-id="2403.09625v1">Interpret</button>
                <div id="interpretation-2403.09625v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning</h3>
                <p>Authors: Zhishuai LiuPan Xu</p>
                <p><a href="http://arxiv.org/abs/2403.09621v1">Link to paper</a></p>
                <p>Distributionally robust offline reinforcement learning RL which seeksrobust policy training against environment perturbation by modeling dynamicsuncertainty calls for function approximations when facing large state-actionspaces. However the consideration of dynamics uncertainty introduces essentialnonlinearity and computational burden posing unique challenges for analyzingand practically employing function approximation. Focusing on a basic settingwhere the nominal model and perturbed models are linearly parameterized wepropose minimax optimal and computationally efficient algorithms realizingfunction approximation and initiate the study on instance-dependentsuboptimality analysis in the context of robust offline RL. Our results uncoverthat function approximation in robust offline RL is essentially distinct fromand probably harder than that in standard offline RL. Our algorithms andtheoretical results crucially depend on a variety of new techniques involvinga novel function approximation mechanism incorporating variance information anew procedure of suboptimality and estimation uncertainty decomposition aquantification of the robust value function shrinkage and a meticulouslydesigned family of hard instances which might be of independent interest.</p>
                <p>Last Updated: 2024-03-14 17:55:10 UTC</p>
                <button class="interpret-button" data-id="2403.09621v1">Interpret</button>
                <div id="interpretation-2403.09621v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training</h3>
                <p>Authors: Yanlai YangMatt JonesMichael C. MozerMengye Ren</p>
                <p><a href="http://arxiv.org/abs/2403.09613v1">Link to paper</a></p>
                <p>We explore the training dynamics of neural networks in a structured non-IIDsetting where documents are presented cyclically in a fixed repeated sequence.Typically networks suffer from catastrophic interference when training on asequence of documents however we discover a curious and remarkable propertyof LLMs fine-tuned sequentially in this setting: they exhibit anticipatorybehavior recovering from the forgetting on documents before encountering themagain. The behavior emerges and becomes more robust as the architecture scalesup its number of parameters. Through comprehensive experiments andvisualizations we uncover new insights into training over-parameterizednetworks in structured environments.</p>
                <p>Last Updated: 2024-03-14 17:51:54 UTC</p>
                <button class="interpret-button" data-id="2403.09613v1">Interpret</button>
                <div id="interpretation-2403.09613v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding</h3>
                <p>Authors: Chengyao WangLi JiangXiaoyang WuZhuotao TianBohao PengHengshuang ZhaoJiaya Jia</p>
                <p><a href="http://arxiv.org/abs/2403.09639v1">Link to paper</a></p>
                <p>Self-supervised 3D representation learning aims to learn effectiverepresentations from large-scale unlabeled point clouds. Most existingapproaches adopt point discrimination as the pretext task which assignsmatched points in two distinct views as positive pairs and unmatched points asnegative pairs. However this approach often results in semantically identicalpoints having dissimilar representations leading to a high number of falsenegatives and introducing a semantic conflict problem. To address this issuewe propose GroupContrast a novel approach that combines segment grouping andsemantic-aware contrastive learning. Segment grouping partitions points intosemantically meaningful regions which enhances semantic coherence and providessemantic guidance for the subsequent contrastive representation learning.Semantic-aware contrastive learning augments the semantic information extractedfrom segment grouping and helps to alleviate the issue of semantic conflict.We conducted extensive experiments on multiple 3D scene understanding tasks.The results demonstrate that GroupContrast learns semantically meaningfulrepresentations and achieves promising transfer learning performance.</p>
                <p>Last Updated: 2024-03-14 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2403.09639v1">Interpret</button>
                <div id="interpretation-2403.09639v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior</h3>
                <p>Authors: Huan-ang GaoMingju GaoJiaju LiWenyi LiRong ZhiHao TangHao Zhao</p>
                <p><a href="http://arxiv.org/abs/2403.09638v1">Link to paper</a></p>
                <p>Semantic image synthesis SIS shows good promises for sensor simulation.However current best practices in this field based on GANs have not yetreached the desired level of quality. As latent diffusion models makesignificant strides in image generation we are prompted to evaluateControlNet a notable method for its dense control capabilities. Ourinvestigation uncovered two primary issues with its results: the presence ofweird sub-structures within large semantic areas and the misalignment ofcontent with the semantic mask. Through empirical study we pinpointed thecause of these problems as a mismatch between the noised training datadistribution and the standard normal prior applied at the inference stage. Toaddress this challenge we developed specific noise priors for SISencompassing spatial categorical and a novel spatial-categorical joint priorfor inference. This approach which we have named SCP-Diff has yieldedexceptional results achieving an FID of 10.53 on Cityscapes and 12.66 onADE20K.The code and models can be accessed via the project page.</p>
                <p>Last Updated: 2024-03-14 17:59:55 UTC</p>
                <button class="interpret-button" data-id="2403.09638v1">Interpret</button>
                <div id="interpretation-2403.09638v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping</h3>
                <p>Authors: Yuhang ZhengXiangyu ChenYupeng ZhengSongen GuRunyi YangBu JinPengfei LiChengliang ZhongZengmao WangLina LiuChao YangDawei WangZhen ChenXiaoxiao LongMeiqing Wang</p>
                <p><a href="http://arxiv.org/abs/2403.09637v1">Link to paper</a></p>
                <p>Constructing a 3D scene capable of accommodating open-ended language queriesis a pivotal pursuit particularly within the domain of robotics. Suchtechnology facilitates robots in executing object manipulations based on humanlanguage directives. To tackle this challenge some research efforts have beendedicated to the development of language-embedded implicit fields. Howeverimplicit fields e.g. NeRF encounter limitations due to the necessity ofprocessing a large number of input views for reconstruction coupled with theirinherent inefficiencies in inference. Thus we present the GaussianGrasperwhich utilizes 3D Gaussian Splatting to explicitly represent the scene as acollection of Gaussian primitives. Our approach takes a limited set of RGB-Dviews and employs a tile-based splatting technique to create a feature field.In particular we propose an Efficient Feature Distillation EFD module thatemploys contrastive learning to efficiently and accurately distill languageembeddings derived from foundational models. With the reconstructed geometry ofthe Gaussian field our method enables the pre-trained grasping model togenerate collision-free grasp pose candidates. Furthermore we propose anormal-guided grasp module to select the best grasp pose. Through comprehensivereal-world experiments we demonstrate that GaussianGrasper enables robots toaccurately query and grasp objects with language instructions providing a newsolution for language-guided manipulation tasks. Data and codes can beavailable at https://github.com/MrSecant/GaussianGrasper.</p>
                <p>Last Updated: 2024-03-14 17:59:46 UTC</p>
                <button class="interpret-button" data-id="2403.09637v1">Interpret</button>
                <div id="interpretation-2403.09637v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models</h3>
                <p>Authors: Akhil KediaMohd Abbas ZaidiSushil KhyaliaJungho JungHarshith GokaHaejun Lee</p>
                <p><a href="http://arxiv.org/abs/2403.09635v1">Link to paper</a></p>
                <p>In spite of their huge success transformer models remain difficult to scalein depth. In this work we develop a unified signal propagation theory andprovide formulae that govern the moments of the forward and backward signalthrough the transformer model. Our framework can be used to understand andmitigate vanishing/exploding gradients rank collapse and instabilityassociated with high attention scores. We also propose DeepScaleLM aninitialization and scaling scheme that conserves unit output/gradient momentsthroughout the model enabling the training of very deep models with 100s oflayers. We find that transformer models could be much deeper - our deep modelswith fewer parameters outperform shallow models in Language Modeling SpeechTranslation and Image Classification across Encoder-only Decoder-only andEncoder-Decoder variants for both Pre-LN and Post-LN transformers formultiple datasets and model sizes. These improvements also translate intoimproved performance on downstream Question Answering tasks and improvedrobustness for image classification.</p>
                <p>Last Updated: 2024-03-14 17:59:14 UTC</p>
                <button class="interpret-button" data-id="2403.09635v1">Interpret</button>
                <div id="interpretation-2403.09635v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning</h3>
                <p>Authors: Lingyi HongShilin YanRenrui ZhangWanyun LiXinyu ZhouPinxue GuoKaixun JiangYiting ChenJinglun LiZhaoyu ChenWenqiang Zhang</p>
                <p><a href="http://arxiv.org/abs/2403.09634v1">Link to paper</a></p>
                <p>Visual object tracking aims to localize the target object of each frame basedon its initial appearance in the first frame. Depending on the input modilitytracking tasks can be divided into RGB tracking and RGBX e.g. RGBN andRGBD tracking. Despite the different input modalities the core aspect oftracking is the temporal matching. Based on this common ground we present ageneral framework to unify various tracking tasks termed as OneTracker.OneTracker first performs a large-scale pre-training on a RGB tracker calledFoundation Tracker. This pretraining phase equips the Foundation Tracker with astable ability to estimate the location of the target object. Then we regardother modality information as prompt and build Prompt Tracker upon FoundationTracker. Through freezing the Foundation Tracker and only adjusting someadditional trainable parameters Prompt Tracker inhibits the stronglocalization ability from Foundation Tracker and achieves parameter-efficientfinetuning on downstream RGBX tracking tasks. To evaluate the effectiveness ofour general framework OneTracker which is consisted of Foundation Tracker andPrompt Tracker we conduct extensive experiments on 6 popular tracking tasksacross 11 benchmarks and our OneTracker outperforms other models and achievesstate-of-the-art performance.</p>
                <p>Last Updated: 2024-03-14 17:59:13 UTC</p>
                <button class="interpret-button" data-id="2403.09634v1">Interpret</button>
                <div id="interpretation-2403.09634v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Trust AI Regulation? Discerning users are vital to build trust and effective AI regulation</h3>
                <p>Authors: Zainab AlalawiPaolo BovaTheodor CimpeanuAlessandro Di StefanoManh Hong DuongElias Fernandez DomingosThe Anh HanMarcus KrellnerBianca OgboSimon T. PowersFilippo Zimmaro</p>
                <p><a href="http://arxiv.org/abs/2403.09510v1">Link to paper</a></p>
                <p>There is general agreement that some form of regulation is necessary both forAI creators to be incentivised to develop trustworthy systems and for users toactually trust those systems. But there is much debate about what form theseregulations should take and how they should be implemented. Most work in thisarea has been qualitative and has not been able to make formal predictions.Here we propose that evolutionary game theory can be used to quantitativelymodel the dilemmas faced by users AI creators and regulators and provideinsights into the possible effects of different regulatory regimes. We showthat creating trustworthy AI and user trust requires regulators to beincentivised to regulate effectively. We demonstrate the effectiveness of twomechanisms that can achieve this. The first is where governments can recogniseand reward regulators that do a good job. In that case if the AI system is nottoo risky for users then some level of trustworthy development and user trustevolves. We then consider an alternative solution where users can conditiontheir trust decision on the effectiveness of the regulators. This leads toeffective regulation and consequently the development of trustworthy AI anduser trust provided that the cost of implementing regulations is not too high.Our findings highlight the importance of considering the effect of differentregulatory regimes from an evolutionary game theoretic perspective.</p>
                <p>Last Updated: 2024-03-14 15:56:39 UTC</p>
                <button class="interpret-button" data-id="2403.09510v1">Interpret</button>
                <div id="interpretation-2403.09510v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Safe Road-Crossing by Autonomous Wheelchairs: a Novel Dataset and its Experimental Evaluation</h3>
                <p>Authors: Carlo GrigioniFranca CorradiniAlessandro AntonucciJérôme GuzziFrancesco Flammini</p>
                <p><a href="http://arxiv.org/abs/2403.08984v1">Link to paper</a></p>
                <p>Safe road-crossing by self-driving vehicles is a crucial problem to addressin smart-cities. In this paper we introduce a multi-sensor fusion approach tosupport road-crossing decisions in a system composed by an autonomouswheelchair and a flying drone featuring a robust sensory system made of diverseand redundant components. To that aim we designed an analytical dangerfunction based on explainable physical conditions evaluated by single sensorsincluding those using machine learning and artificial vision. As aproof-of-concept we provide an experimental evaluation in a laboratoryenvironment showing the advantages of using multiple sensors which canimprove decision accuracy and effectively support safety assessment. We madethe dataset available to the scientific community for further experimentation.The work has been developed in the context of an European project namedREXASI-PRO which aims to develop trustworthy artificial intelligence forsocial navigation of people with reduced mobility.</p>
                <p>Last Updated: 2024-03-13 22:19:06 UTC</p>
                <button class="interpret-button" data-id="2403.08984v1">Interpret</button>
                <div id="interpretation-2403.08984v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning</h3>
                <p>Authors: Peihong YuManav MishraAlec KoppelCarl BusartPriya NarayanDinesh ManochaAmrit BediPratap Tokekar</p>
                <p><a href="http://arxiv.org/abs/2403.08936v1">Link to paper</a></p>
                <p>Multi-Agent Reinforcement Learning MARL algorithms face the challenge ofefficient exploration due to the exponential increase in the size of the jointstate-action space. While demonstration-guided learning has proven beneficialin single-agent settings its direct applicability to MARL is hindered by thepractical difficulty of obtaining joint expert demonstrations. In this work weintroduce a novel concept of personalized expert demonstrations tailored foreach individual agent or more broadly each individual type of agent within aheterogeneous team. These demonstrations solely pertain to single-agentbehaviors and how each agent can achieve personal goals without encompassingany cooperative elements thus naively imitating them will not achievecooperation due to potential conflicts. To this end we propose an approachthat selectively utilizes personalized expert demonstrations as guidance andallows agents to learn to cooperate namely personalized expert-guided MARLPegMARL. This algorithm utilizes two discriminators: the first providesincentives based on the alignment of policy behavior with demonstrations andthe second regulates incentives based on whether the behavior leads to thedesired objective. We evaluate PegMARL using personalized demonstrations inboth discrete and continuous environments. The results demonstrate that PegMARLlearns near-optimal policies even when provided with suboptimal demonstrationsand outperforms state-of-the-art MARL algorithms in solving coordinated tasks.We also showcase PegMARLs capability to leverage joint demonstrations in theStarCraft scenario and converge effectively even with demonstrations fromnon-co-trained policies.</p>
                <p>Last Updated: 2024-03-13 20:11:20 UTC</p>
                <button class="interpret-button" data-id="2403.08936v1">Interpret</button>
                <div id="interpretation-2403.08936v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Cultural evolution in populations of Large Language Models</h3>
                <p>Authors: Jérémy PerezCorentin LégerMarcela Ovando-TellezChris FoulonJoan DussauldPierre-Yves OudeyerClément Moulin-Frier</p>
                <p><a href="http://arxiv.org/abs/2403.08882v1">Link to paper</a></p>
                <p>Research in cultural evolution aims at providing causal explanations for thechange of culture over time. Over the past decades this field has generated animportant body of knowledge using experimental historical and computationalmethods. While computational models have been very successful at generatingtestable hypotheses about the effects of several factors such as populationstructure or transmission biases some phenomena have so far been more complexto capture using agent-based and formal models. This is in particular the casefor the effect of the transformations of social information induced by evolvedcognitive mechanisms. We here propose that leveraging the capacity of LargeLanguage Models LLMs to mimic human behavior may be fruitful to address thisgap. On top of being an useful approximation of human cultural dynamicsmulti-agents models featuring generative agents are also important to study fortheir own sake. Indeed as artificial agents are bound to participate more andmore to the evolution of culture it is crucial to better understand thedynamics of machine-generated cultural evolution. We here present a frameworkfor simulating cultural evolution in populations of LLMs allowing themanipulation of variables known to be important in cultural evolution such asnetwork structure personality and the way social information is aggregatedand transformed. The software we developed for conducting these simulations isopen-source and features an intuitive user-interface which we hope will helpto build bridges between the fields of cultural evolution and generativeartificial intelligence.</p>
                <p>Last Updated: 2024-03-13 18:11:17 UTC</p>
                <button class="interpret-button" data-id="2403.08882v1">Interpret</button>
                <div id="interpretation-2403.08882v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning</h3>
                <p>Authors: Jing TanRamin KhaliliHolger Karl</p>
                <p><a href="http://arxiv.org/abs/2403.08879v1">Link to paper</a></p>
                <p>The Intelligent Transportation System ITS environment is known to bedynamic and distributed where participants vehicle users operators etc.have multiple changing and possibly conflicting objectives. AlthoughReinforcement Learning RL algorithms are commonly applied to optimize ITSapplications such as resource management and offloading most RL algorithmsfocus on single objectives. In many situations converting a multi-objectiveproblem into a single-objective one is impossible intractable or insufficientmaking such RL algorithms inapplicable. We propose a multi-objectivemulti-agent reinforcement learning MARL algorithm with high learningefficiency and low computational requirements which automatically triggersadaptive few-shot learning in a dynamic distributed and noisy environment withsparse and delayed reward. We test our algorithm in an ITS environment withedge cloud computing. Empirical results show that the algorithm is quick toadapt to new environments and performs better in all individual and systemmetrics compared to the state-of-the-art benchmark. Our algorithm alsoaddresses various practical concerns with its modularized and asynchronousonline training method. In addition to the cloud simulation we test ouralgorithm on a single-board computer and show that it can make inference in 6milliseconds.</p>
                <p>Last Updated: 2024-03-13 18:05:16 UTC</p>
                <button class="interpret-button" data-id="2403.08879v1">Interpret</button>
                <div id="interpretation-2403.08879v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-03-17</p>
        </div>
    
        </div>
    </body>
    </html>
    